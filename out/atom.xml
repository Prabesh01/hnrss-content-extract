<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-26T10:39:59.598939+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45375845</id><title>Improved Gemini 2.5 Flash and Flash-Lite</title><updated>2025-09-26T10:40:10.298907+00:00</updated><content>&lt;doc fingerprint="bf7738879939489d"&gt;
  &lt;main&gt;
    &lt;p&gt;Today, we are releasing updated versions of Gemini 2.5 Flash and 2.5 Flash-Lite, available on Google AI Studio and Vertex AI, aimed at continuing to deliver better quality while also improving the efficiency.&lt;/p&gt;
    &lt;p&gt;The latest version of Gemini 2.5 Flash-Lite was trained and built based on three key themes:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;You can start testing this version today using the following model string: &lt;code&gt;gemini-2.5-flash-lite-preview-09-2025&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This latest 2.5 Flash model comes with improvements in two key areas we heard consistent feedback on:&lt;/p&gt;
    &lt;p&gt;We’re already seeing positive feedback from early testers. As Yichao ‘Peak’ Ji, Co-Founder &amp;amp; Chief Scientist at Manus, an autonomous AI agent, noted: “The new Gemini 2.5 Flash model offers a remarkable blend of speed and intelligence. Our evaluation on internal benchmarks revealed a 15% leap in performance for long-horizon agentic tasks. Its outstanding cost-efficiency enables Manus to scale to unprecedented levels—advancing our mission to Extend Human Reach.”&lt;/p&gt;
    &lt;p&gt;You can start testing this preview version today by using the following model string: &lt;code&gt;gemini-2.5-flash-preview-09-2025&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Over the last year, we’ve learned that shipping preview versions of our models allows you to test our latest improvements and innovations, provide feedback, and build production-ready experiences with the best of Gemini. Today’s releases are not intended to graduate to a new, stable version but will help us shape our future stable releases, and allow us to continue iterating and bring you the best of Gemini.&lt;/p&gt;
    &lt;p&gt;To make it even easier to access our latest models while also reducing the need to keep track of long model string names, we are also introducing a &lt;code&gt;-latest&lt;/code&gt; alias for each model family. This alias always points to our most recent model versions, allowing you to experiment with new features without needing to update your code for each release. You can access the new previews using:&lt;/p&gt;
    &lt;code&gt;gemini-flash-latest&lt;/code&gt;
    &lt;code&gt;gemini-flash-lite-latest&lt;/code&gt;
    &lt;p&gt;&lt;lb/&gt;To ensure you have time to test new models, we will always provide a 2-week notice (via email) before we make updates or deprecate a specific version behind &lt;code&gt;-latest&lt;/code&gt;. These are just model aliases so the rate limits, cost, and features available may fluctuate between releases.&lt;/p&gt;
    &lt;p&gt;For applications that require more stability, continue to use &lt;code&gt;gemini-2.5-flash&lt;/code&gt; and &lt;code&gt;gemini-2.5-flash-lite&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We continue to push the frontier of what is possible with Gemini and this release is just another step in that direction. We will have more to share soon, but in the meantime, happy building!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/"/><published>2025-09-25T17:20:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45376605</id><title>Athlon 64: How AMD turned the tables on Intel</title><updated>2025-09-26T10:40:09.587754+00:00</updated><content>&lt;doc fingerprint="e70d5115fd2265ff"&gt;
  &lt;main&gt;
    &lt;p&gt;22 years ago, on September 23, 2003, AMD changed the game for x86 once and for all. They released the Athlon 64 CPU, a chip that did something Intel didn’t want. Intel didn’t want to extend x86 to 64 bits. But when AMD did it, it forced Intel to clone AMD, rather than the other way around.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Intel didn’t want to go 64-bit&lt;/head&gt;
    &lt;p&gt;Even in 2001, x86 had decades of baggage attached to it. It was a 32-bit architecture that had been extended from a 16-bit architecture. But that in turn had been extended from an 8-bit CPU design from 1972 that, believe it or not, originated at Datapoint, not Intel.&lt;/p&gt;
    &lt;p&gt;This was great for backward compatibility. 8-bit applications were very easy to port to x86 in the early 1980s, and those early DOS applications still ran flawlessly on modern systems 30 years later. For that matter, it’s not impossible to get them running even today.&lt;/p&gt;
    &lt;p&gt;Removal of the ability to run 16-bit applications in 64-bit Windows was a design decision, not a technical limitation.&lt;/p&gt;
    &lt;p&gt;Intel wanted to start over to go 64-bit. Without having to worry about backward compatibility, they could design something that would be faster and more efficient. In theory at least, it would be able to scale higher in clock speed. And there was no question a new design would outperform a theoretical 64-bit x86 when running at the same speed because of efficiency.&lt;/p&gt;
    &lt;p&gt;And if you are cynical, there was one more motivation. If Intel could start over, they wouldn’t have to worry about competing CPU designs, at least not for a very long time. The new design would be encumbered with so many patents, it might be 20 years before someone could clone it.&lt;/p&gt;
    &lt;p&gt;Keep in mind that in 2003, not only was AMD in the picture, but Transmeta was still in the picture, and Cyrix was fading but not completely gone.&lt;/p&gt;
    &lt;p&gt;Starting over with a new CPU architecture outright was massively attractive to Intel.&lt;/p&gt;
    &lt;p&gt;This new 64-bit architecture wasn’t theoretical, either. Intel was producing it. It was called Itanium, and Intel first released it in June 2001.&lt;/p&gt;
    &lt;head rend="h2"&gt;AMD’s risky bet and why they made it&lt;/head&gt;
    &lt;p&gt;AMD was well aware of the shortcomings of extending x86 to 64 bits. And they did it anyway. For them, the stakes were completely different.&lt;/p&gt;
    &lt;p&gt;AMD knew that if Itanium caught on, that would be the end for them as a CPU company, unless maybe they wanted to become just another ARM licensee. Being just another ARM licensee is more attractive in 2025 than it was in 2003.&lt;/p&gt;
    &lt;p&gt;But they could see Itanium wasn’t catching on. It had its uses, and it was doing well enough in those niches, but Windows on Itanium was a non-starter. So much so, The Register called it “Itanic.”&lt;/p&gt;
    &lt;p&gt;AMD bet that there would be appeal in a 64-bit architecture that was fully backward compatible with x86 and natively ran 32-bit applications at full speed. People would be able to run 32-bit Windows and 32-bit applications on it if they needed to, and then when they were ready for 64-bit software, the hardware was there and ready to go. And they could continue to run 32-bit apps in 64-bit operating systems as long as needed to ease the transition.&lt;/p&gt;
    &lt;p&gt;The transition to 32 bits took a decade. AMD reasoned more people would be willing to upgrade to 64 bits if they made that transition as similar as the transition from the 286 to the 386 as possible.&lt;/p&gt;
    &lt;p&gt;They believed the market would willingly trade lower 64-bit performance in the long term for better 32-bit performance right away. They also believed that if Microsoft was willing to build Windows on Itanium, they would be willing to take a chance on 64-bit x86 as well.&lt;/p&gt;
    &lt;p&gt;So on September 23, 2003, AMD launched its Athlon 64, the first 64-bit x86 CPU.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why the Athlon 64 was a hit&lt;/head&gt;
    &lt;p&gt;AMD64 was everything AMD hoped it would be. It was backward compatible with 32-bit x86. The 64-bit builds of Windows weren’t available immediately, and they didn’t catch on immediately, but you cannot say nobody used them. People did, in fact, use them. In late 2005, I was in charge of administering the complimentary antivirus software that Charter Communications provided to its subscribers. I’m not going to say say someone called me every day wanting 64-bit antivirus for 64-bit Windows. But it did happen once a week.&lt;/p&gt;
    &lt;p&gt;The transition took at least as long as AMD expected. When I finally bought an Athlon 64 in 2011, I found native 64-bit software was still scarce. I’m an outspoken Firefox fan; the reason I briefly switched to Google Chrome was to get a 64-bit web browser.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Athlon 64 in the enterprise&lt;/head&gt;
    &lt;p&gt;A few months later, I got a better job with more pay and better growth potential. I can’t talk a lot about the job, but I was administering a mission critical system that ran on Windows, mostly on Dell hardware. I mention Dell because they were exclusively an Intel vendor for years. Cofounder and longtime AMD CEO Jerry Sanders once said of Michael Dell, “I can’t sell him a[n AMD] K6 no matter what I do.”&lt;/p&gt;
    &lt;p&gt;It was the Athlon 64 that made Dell relent and finally start using AMD CPUs. Not only were they using them on desktop systems, but they were putting AMD CPUs in servers, an idea that would have been extremely controversial 5 years before. At least in the circles I ran in.&lt;/p&gt;
    &lt;p&gt;The Athlon 64 caught on because, in spite of its name, it was an outstanding 32-bit CPU. It was faster than an Intel CPU running at the same clock rate, and it used less power as well. The power consumption was the key to getting into the data center. The Intel name was a security blanket, even though AMD had been making x86 CPUs exactly as long as Intel. But certain decision makers bought Intel marketing and saw AMD as a second tier brand.&lt;/p&gt;
    &lt;p&gt;The thing is, when you have a data center with hundreds of systems in it, the money you save on a more efficient CPU really talks.&lt;/p&gt;
    &lt;p&gt;Replacing Intel Prescott-based servers with AMD64 servers was not a universally popular idea. But you could tell a difference when you were standing behind a rack full of Intel-based servers versus a rack full of AMD based servers. The Intels ran hotter.&lt;/p&gt;
    &lt;p&gt;From an uptime perspective, we couldn’t see a difference. The performance metrics I collected showed there was a slight difference, and that difference was in AMD’s favor. So the AMD critics quickly ate their words.&lt;/p&gt;
    &lt;head rend="h3"&gt;Intel giving in and cloning AMD64&lt;/head&gt;
    &lt;p&gt;In 2004, Intel wrote off the Itanium and cloned AMD64. They called it Intel64, but it was a blatant copy of the AMD implementation. A quirk in the agreements that allowed AMD to use the x86 instruction set also gave Intel the rights to use the AMD64 instructions. So there was nothing illegal about what Intel did. Itanium continued to see use in specialized applications, but Intel quietly discontinued it in 2020.&lt;/p&gt;
    &lt;p&gt;AMD and Intel have been chasing and catching each other ever since. One of them will pass the other for a CPU generation or two, and then they will change positions. It’s not terribly different from the situation in 1999 with the original Athlon, when AMD outperformed Intel for the first time. The question in everyone’s mind was whether they would do it a second time. The Athlon 64 was the second time.&lt;/p&gt;
    &lt;p&gt;It was a big step forward. Eight years before, AMD was trying to pass off a high-clocked 486 as a Pentium equivalent. With the Athlon 64, AMD was innovating.&lt;/p&gt;
    &lt;p&gt;David Farquhar is a computer security professional, entrepreneur, and author. He has written professionally about computers since 1991, so he was writing about retro computers when they were still new. He has been working in IT professionally since 1994 and has specialized in vulnerability management since 2013. He holds Security+ and CISSP certifications. Today he blogs five times a week, mostly about retro computers and retro gaming covering the time period from 1975 to 2000.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dfarq.homeip.net/athlon-64-how-amd-turned-the-tables-on-intel/"/><published>2025-09-25T18:09:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45377641</id><title>Ollama Web Search</title><updated>2025-09-26T10:40:09.428238+00:00</updated><content>&lt;doc fingerprint="2d52dc131ca4ae0e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Web search&lt;/head&gt;
    &lt;head rend="h2"&gt;September 24, 2025&lt;/head&gt;
    &lt;p&gt;A new web search API is now available in Ollama. Ollama provides a generous free tier of web searches for individuals to use, and higher rate limits are available via Ollama’s cloud.&lt;/p&gt;
    &lt;p&gt;This web search capability can augment models with the latest information from the web to reduce hallucinations and improve accuracy.&lt;/p&gt;
    &lt;p&gt;Web search is provided as a REST API with deeper tool integrations in Ollama’s Python and JavaScript libraries. This also enables models such as OpenAI’s &lt;code&gt;gpt-oss&lt;/code&gt; models to conduct long-running research tasks.&lt;/p&gt;
    &lt;head rend="h3"&gt;Get started&lt;/head&gt;
    &lt;p&gt;Create an API key from your Ollama account.&lt;/p&gt;
    &lt;code&gt;export OLLAMA_API_KEY="your_api_key"
&lt;/code&gt;
    &lt;head rend="h4"&gt;cURL&lt;/head&gt;
    &lt;code&gt;curl https://ollama.com/api/web_search \
  --header "Authorization: Bearer $OLLAMA_API_KEY" \
  -d '{
    "query": "what is ollama?"
  }'
&lt;/code&gt;
    &lt;p&gt;Example output&lt;/p&gt;
    &lt;code&gt;{
  "results": [
    {
      "title": "Ollama",
      "url": "https://ollama.com/",
      "content": "Cloud models are now available..."
    },
    {
      "title": "What is Ollama? Introduction to the AI model management tool",
      "url": "https://www.hostinger.com/tutorials/what-is-ollama",
      "content": "Ariffud M. 6min Read..."
    },
    {
      "title": "Ollama Explained: Transforming AI Accessibility and Language ...",
      "url": "https://www.geeksforgeeks.org/artificial-intelligence/ollama-explained-transforming-ai-accessibility-and-language-processing/",
      "content": "Data Science Data Science Projects Data Analysis..."
    }
  ]
}
&lt;/code&gt;
    &lt;head rend="h4"&gt;Python&lt;/head&gt;
    &lt;p&gt;Install and run Ollama’s Python library&lt;/p&gt;
    &lt;code&gt;pip install 'ollama&amp;gt;=0.6.0'
&lt;/code&gt;
    &lt;p&gt;Then make a request using &lt;code&gt;ollama.web_search&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;import ollama
response = ollama.web_search("What is Ollama?")
print(response)
&lt;/code&gt;
    &lt;p&gt;Example output&lt;/p&gt;
    &lt;code&gt;results = [
    {
        "title": "Ollama",
        "url": "https://ollama.com/",
        "content": "Cloud models are now available in Ollama..."
    },
    {
        "title": "What is Ollama? Features, Pricing, and Use Cases - Walturn",
        "url": "https://www.walturn.com/insights/what-is-ollama-features-pricing-and-use-cases",
        "content": "Our services..."
    },
    {
        "title": "Complete Ollama Guide: Installation, Usage &amp;amp; Code Examples",
        "url": "https://collabnix.com/complete-ollama-guide-installation-usage-code-examples",
        "content": "Join our Discord Server..."
    }
]
&lt;/code&gt;
    &lt;head rend="h4"&gt;JavaScript&lt;/head&gt;
    &lt;p&gt;Install and run Ollama’s JavaScript library&lt;/p&gt;
    &lt;code&gt;npm install 'ollama@&amp;gt;=0.6.0'
&lt;/code&gt;
    &lt;code&gt;import { Ollama } from "ollama";

const client = new Ollama();
const results = await client.webSearch({ query: "what is ollama?" });
console.log(JSON.stringify(results, null, 2));
&lt;/code&gt;
    &lt;p&gt;Example output&lt;/p&gt;
    &lt;code&gt;{
  "results": [
    {
      "title": "Ollama",
      "url": "https://ollama.com/",
      "content": "Cloud models are now available..."
    },
    {
      "title": "What is Ollama? Introduction to the AI model management tool",
      "url": "https://www.hostinger.com/tutorials/what-is-ollama",
      "content": "Ollama is an open-source tool..."
    },
    {
      "title": "Ollama Explained: Transforming AI Accessibility and Language Processing",
      "url": "https://www.geeksforgeeks.org/artificial-intelligence/ollama-explained-transforming-ai-accessibility-and-language-processing/",
      "content": "Ollama is a groundbreaking..."
    }
  ]
}

&lt;/code&gt;
    &lt;head rend="h3"&gt;Building a search agent&lt;/head&gt;
    &lt;p&gt;Use Ollama’s web search as a tool to build a mini search agent.&lt;/p&gt;
    &lt;p&gt;The example uses Alibaba’s Qwen 3 model with 4B parameters.&lt;/p&gt;
    &lt;code&gt;ollama pull qwen3:4b
&lt;/code&gt;
    &lt;code&gt;from ollama import chat, web_fetch, web_search

available_tools = {'web_search': web_search, 'web_fetch': web_fetch}

messages = [{'role': 'user', 'content': "what is ollama's new engine"}]

while True:
  response = chat(
    model='qwen3:4b',
    messages=messages,
    tools=[web_search, web_fetch],
    think=True
    )
  if response.message.thinking:
    print('Thinking: ', response.message.thinking)
  if response.message.content:
    print('Content: ', response.message.content)
  messages.append(response.message)
  if response.message.tool_calls:
    print('Tool calls: ', response.message.tool_calls)
    for tool_call in response.message.tool_calls:
      function_to_call = available_tools.get(tool_call.function.name)
      if function_to_call:
        args = tool_call.function.arguments
        result = function_to_call(**args)
        print('Result: ', str(result)[:200]+'...')
        # Result is truncated for limited context lengths
        messages.append({'role': 'tool', 'content': str(result)[:2000 * 4], 'tool_name': tool_call.function.name})
      else:
        messages.append({'role': 'tool', 'content': f'Tool {tool_call.function.name} not found', 'tool_name': tool_call.function.name})
  else:
    break
&lt;/code&gt;
    &lt;p&gt;Result&lt;/p&gt;
    &lt;code&gt;Thinking:  Okay, the user is asking about Ollama's new engine. I need to figure out what they're referring to. Ollama is a company that develops large language models, so maybe they've released a new model or an updated version of their existing engine...

Tool calls:  [ToolCall(function=Function(name='web_search', arguments={'max_results': 3, 'query': 'Ollama new engine'}))]
Result:  results=[WebSearchResult(content='# New model scheduling\n\n## September 23, 2025\n\nOllama now includes a significantly improved model scheduling system. Ahead of running a model, Ollama’s new engine...

Thinking:  Okay, the user asked about Ollama's new engine. Let me look at the search results.

First result is from September 23, 2025, talking about new model scheduling. It mentions improved memory management, reduced crashes, better GPU utilization, and multi-GPU performance. Examples show speed improvements and accurate memory reporting. Supported models include gemma3, llama4, qwen3, etc...

Content:  Ollama has introduced two key updates to its engine, both released in 2025:

1. **Enhanced Model Scheduling (September 23, 2025)**
   - **Precision Memory Management**: Exact memory allocation reduces out-of-memory crashes and optimizes GPU utilization.
   - **Performance Gains**: Examples show significant speed improvements (e.g., 85.54 tokens/s vs 52.02 tokens/s) and full GPU layer utilization.
   - **Multi-GPU Support**: Improved efficiency across multiple GPUs, with accurate memory reporting via tools like `nvidia-smi`.
   - **Supported Models**: Includes `gemma3`, `llama4`, `qwen3`, `mistral-small3.2`, and more.

2. **Multimodal Engine (May 15, 2025)**
   - **Vision Support**: First-class support for vision models, including `llama4:scout` (109B parameters), `gemma3`, `qwen2.5vl`, and `mistral-small3.1`.
   - **Multimodal Tasks**: Examples include identifying animals in multiple images, answering location-based questions from videos, and document scanning.

These updates highlight Ollama's focus on efficiency, performance, and expanded capabilities for both text and vision tasks.
&lt;/code&gt;
    &lt;p&gt;Recommended models:&lt;/p&gt;
    &lt;p&gt;These models have great tool-use capabilities and are able to have multi-turn interactions with the user and tools to get to a final result.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;qwen3&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;gpt-oss&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Recommended cloud models:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;qwen3:480b-cloud&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;gpt-oss:120b-cloud&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;deepseek-v3.1-cloud&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The &lt;code&gt;web_search&lt;/code&gt; and &lt;code&gt;web_fetch&lt;/code&gt; tools can return thousands of tokens. It is recommended to increase the context length of the model to ~32000 tokens for reasonable performance. Search agents work best with full context length.&lt;/p&gt;
    &lt;head rend="h3"&gt;Fetching page results&lt;/head&gt;
    &lt;p&gt;To fetch individual pages (e.g. when a user provides a url in the prompt), use the new web fetch API.&lt;/p&gt;
    &lt;head rend="h4"&gt;Python library&lt;/head&gt;
    &lt;code&gt;from ollama import web_fetch

result = web_fetch('https://ollama.com')
print(result)
&lt;/code&gt;
    &lt;p&gt;Result&lt;/p&gt;
    &lt;code&gt;WebFetchResponse(
    title='Ollama',
    content='[Cloud models](https://ollama.com/blog/cloud-models) are now available in Ollama\n\n**Chat &amp;amp; build
with open models**\n\n[Download](https://ollama.com/download) [Explore
models](https://ollama.com/models)\n\nAvailable for macOS, Windows, and Linux',
    links=['https://ollama.com/', 'https://ollama.com/models', 'https://github.com/ollama/ollama']
)
&lt;/code&gt;
    &lt;p&gt;Example Python code is available on GitHub.&lt;/p&gt;
    &lt;head rend="h4"&gt;JavaScript library&lt;/head&gt;
    &lt;code&gt;import { Ollama } from "ollama";

const client = new Ollama();
const fetchResult = await client.webFetch({ url: "https://ollama.com" });
console.log(JSON.stringify(fetchResult, null, 2));
&lt;/code&gt;
    &lt;p&gt;Result&lt;/p&gt;
    &lt;code&gt;{
  "title": "Ollama",
  "content": "[Cloud models](https://ollama.com/blog/cloud-models) are now available in Ollama...",
  "links": [
    "https://ollama.com/",
    "https://ollama.com/models",
    "https://github.com/ollama/ollama"
  ]
}
&lt;/code&gt;
    &lt;p&gt;Example JavaScript code is available on GitHub.&lt;/p&gt;
    &lt;head rend="h4"&gt;cURL&lt;/head&gt;
    &lt;code&gt;curl --request POST \
  --url https://ollama.com/api/web_fetch \
  --header "Authorization: Bearer $OLLAMA_API_KEY" \
  --header 'Content-Type: application/json' \
  --data '{
      "url": "ollama.com"
}'
&lt;/code&gt;
    &lt;p&gt;Result&lt;/p&gt;
    &lt;code&gt;{
  "title": "Ollama",
  "content": "[Cloud models](https://ollama.com/blog/cloud-models) are now available in Ollama...",
  "links": [
    "http://ollama.com/",
    "http://ollama.com/models",
    "https://github.com/ollama/ollama"
  ]
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Integrations&lt;/head&gt;
    &lt;head rend="h3"&gt;MCP Server (Model Context Protocol server)&lt;/head&gt;
    &lt;p&gt;You can enable web search in any MCP client through the Python MCP server.&lt;/p&gt;
    &lt;head rend="h4"&gt;Cline&lt;/head&gt;
    &lt;p&gt;To integrate with Cline, configure MCP servers in its settings.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Manage MCP Servers &amp;gt; Configure MCP Servers &amp;gt; Add the configuration below&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;{
  "mcpServers": {
    "web_search_and_fetch": {
      "type": "stdio",
      "command": "uv",
      "args": ["run", "path/to/web-search-mcp.py"],
      "env": { "OLLAMA_API_KEY": "your_api_key_here" }
    }
  }
}
&lt;/code&gt;
    &lt;head rend="h4"&gt;Codex&lt;/head&gt;
    &lt;p&gt;Add the following configuration to &lt;code&gt;~/.codex/config.toml&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;[mcp_servers.web_search]
command = "uv"
args = ["run", "path/to/web-search-mcp.py"]
env = { "OLLAMA_API_KEY" = "your_api_key_here" }
&lt;/code&gt;
    &lt;head rend="h4"&gt;Goose&lt;/head&gt;
    &lt;p&gt;You can integrate with Ollama via Goose’s extensions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Get started&lt;/head&gt;
    &lt;p&gt;Web search is included with a free Ollama account, with much higher rate limits available by upgrading your Ollama subscription.&lt;/p&gt;
    &lt;p&gt;To get started, sign up for an Ollama account!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ollama.com/blog/web-search"/><published>2025-09-25T19:21:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45377748</id><title>Can a model trained on satellite data really find brambles on the ground?</title><updated>2025-09-26T10:40:08.882865+00:00</updated><content>&lt;doc fingerprint="3d09f083bdbd4a9a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Can a model trained on satellite data really find brambles on the ground?&lt;/head&gt;
    &lt;p&gt;Over the summer Gabriel Mahler has been conducting research on hedgehog habitat mapping using Agent Based Models (ABMs) and remote sensing. Hedgehogs seem to like brambles and so as part of his work he has produced a bramble map. He did this by combining the TESSERA earth representation embeddings (using the geotessera library) with data from iNaturalist. The current model is an ensemble of logistic regression and a knn classifier.&lt;/p&gt;
    &lt;p&gt;Can we really see brambles from space? What better way to test the model than a quick field trip around Cambridge. Gabriel, Anil, Shane and I did just that today.&lt;/p&gt;
    &lt;p&gt;We started at Milton Community Centre, as the model was relatively confident there were brambles near the car park and along the path to Milton Park. It took us about 20 seconds to find the first one in an area indicated by the model.&lt;/p&gt;
    &lt;p&gt;So it turns out that there's a lot of bramble between the community center and entrance to Milton Country Park. We stopped six or seven times before reaching the park entrance. While the model predicted we'd find brambles all over the park, we went for the few areas of very high confidence near the entrance. In every place we checked, we found pretty significant amounts of bramble.&lt;/p&gt;
    &lt;p&gt;We collected photos of all the places we stopped, as well as recording our GPS location. One thought while out exploring is that the model did a great job predicting where we would find very large quantities of bramble without any cover. It didn't have high confidence in other areas where we found smaller brambles under partial cover. Since TESSERA is learned representation from remote sensing data (Sentinel 1 and 2), it would make sense that bramble partially obscured from above might be harder to spot. This is something we can potentially tease apart when we have more validation data.&lt;/p&gt;
    &lt;p&gt;Finally, we were satisfied the model was doing a good job in the park area and decided to pick a hotspot the model was predicting in part of a residential street. We drove over to find an empty plot that did indeed have a lot of bramble!&lt;/p&gt;
    &lt;p&gt;Another hotspot was on Fen Road and we stopped by to find this absolute unit:&lt;/p&gt;
    &lt;p&gt;Finally, we headed back in to Cambridge to see what one of the big hotspots in North Cambridge was like. To our amusement we ended up at the local nature reserve Bramblefields, which, true to its name, has a lot of bramble.&lt;/p&gt;
    &lt;p&gt;I was pleasantly surprised by how good Gabriel's model was for its simplicity. Great work!&lt;/p&gt;
    &lt;p&gt;We had hoped to actually re-run the model based on the data we were gathering but that proved tricky on a laptop, in a park. Given the richness of the TESSERA embeddings and the simplicity of the classifiers being used, a mobile phone-based human-in-the-loop active learning setup could be practical..&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://toao.com/blog/can-we-really-see-brambles-from-space"/><published>2025-09-25T19:28:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45379325</id><title>RedoxFS is the default filesystem of Redox OS, inspired by ZFS</title><updated>2025-09-26T10:40:08.535093+00:00</updated><content>&lt;doc fingerprint="1c99101b8162c9f1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;RedoxFS&lt;/head&gt;
    &lt;p&gt;This is the default filesystem of Redox OS, inspired by ZFS and adapted to a microkernel architecture.&lt;/p&gt;
    &lt;p&gt;Redox had a read-only ZFS driver but it was abandoned because of the monolithic nature of ZFS that created problems with the Redox microkernel design.&lt;/p&gt;
    &lt;p&gt;(It's a replacement for TFS)&lt;/p&gt;
    &lt;p&gt;Current features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Compatible with Redox and Linux (FUSE)&lt;/item&gt;
      &lt;item&gt;Copy-on-write&lt;/item&gt;
      &lt;item&gt;Data/metadata checksums&lt;/item&gt;
      &lt;item&gt;Transparent encryption&lt;/item&gt;
      &lt;item&gt;Standard Unix file attributes&lt;/item&gt;
      &lt;item&gt;File/directory size limit up to 193TiB (212TB)&lt;/item&gt;
      &lt;item&gt;File/directory quantity limit up to 4 billion per 193TiB (2^32 - 1 = 4294967295)&lt;/item&gt;
      &lt;item&gt;Disk encryption fully supported by the Redox bootloader, letting it load the kernel off an encrypted partition.&lt;/item&gt;
      &lt;item&gt;MIT licensed&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Being MIT licensed, RedoxFS can be bundled on GPL-licensed operating systems (Linux, for example).&lt;/p&gt;
    &lt;head rend="h2"&gt;Tooling&lt;/head&gt;
    &lt;p&gt;RedoxFS tooling can be used to create, mount and edit contents of an &lt;code&gt;.img&lt;/code&gt; file containing RedoxFS. It can be installed with:&lt;/p&gt;
    &lt;code&gt;cargo install redoxfs
&lt;/code&gt;
    &lt;p&gt;If you found errors while installing it, make sure to install &lt;code&gt;fuse3&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Create a disk&lt;/head&gt;
    &lt;p&gt;You can create an empty, non bootable RedoxFS by allocating an empty file with &lt;code&gt;fallocate&lt;/code&gt; then run &lt;code&gt;redoxfs-mkfs&lt;/code&gt; to initialize the whole image as &lt;code&gt;RedoxFS&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;fallocate -l 1G redox.img
&lt;/code&gt;
    &lt;code&gt;redoxfs-mkfs redox.img
&lt;/code&gt;
    &lt;head rend="h3"&gt;Mount a disk&lt;/head&gt;
    &lt;p&gt;To mount the disk, run &lt;code&gt;redoxfs [image] [directory]&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;mkdir ./redox-img
&lt;/code&gt;
    &lt;code&gt;redoxfs redox.img ./redox-img
&lt;/code&gt;
    &lt;p&gt;It will mount the disk using FUSE underneath.&lt;/p&gt;
    &lt;head rend="h3"&gt;Unmount&lt;/head&gt;
    &lt;p&gt;Unmount the disk using FUSE unmount binary:&lt;/p&gt;
    &lt;code&gt;fusermount3 ./redox-img
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://doc.redox-os.org/book/redoxfs.html"/><published>2025-09-25T21:25:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45380699</id><title>Redis is fast – I'll cache in Postgres</title><updated>2025-09-26T10:40:08.308361+00:00</updated><content>&lt;doc fingerprint="6770f5d56a0059d1"&gt;
  &lt;main&gt;
    &lt;p&gt;There are books &amp;amp; many articles online, like this one arguing for using Postgres for everything. I thought I’d take a look at one use case - using Postgres instead of Redis for caching. I work with APIs quite a bit, so I’d build a super simple HTTP server that responds with data from that cache. I’d start from Redis as this is something I frequently encounter at work, switch it out to Postgres using unlogged tables and see if there’s a difference.&lt;/p&gt;
    &lt;head rend="h2"&gt;The setup&lt;/head&gt;
    &lt;p&gt;I’ll run the experiment on my homelab’s k8s cluster. The idea is to run Postgres or Redis on one node, limiting it to 2CPUs via k8s limits, as well as 8GiB of memory. On another node, I’ll run the web server itself and then spin a pod for the benchmark executed via k6 on the third.&lt;/p&gt;
    &lt;p&gt;Both postgres and redis are used with the out of the box settings for the following images:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Postgres - &lt;code&gt;postgres:17.6&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Redis - &lt;code&gt;redis:8.2&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I wrote a simple webserver, with 2 endpoints, a cache and a “Session” struct which we’ll store in the cache:&lt;/p&gt;
    &lt;code&gt;var ErrCacheMiss = errors.New("cache miss")

type Cache interface {
	Get(ctx context.Context, key string) (string, error)
	Set(ctx context.Context, key string, value string) error
}

type Session struct {
	ID string
}


func serveHTTP(c Cache) {
	http.HandleFunc("/get", getHandler(c))
	http.HandleFunc("/set", setHandler(c))

	port := os.Getenv("PORT")
	if port == "" {
		port = "8080"
	}

	fmt.Println("Server starting on http://0.0.0.0:" + port)

	server := &amp;amp;http.Server{Addr: "0.0.0.0:" + port}

	go func() {
		if err := server.ListenAndServe(); err != nil &amp;amp;&amp;amp; err != http.ErrServerClosed {
			fmt.Println("Error starting server:", err)
		}
	}()

	quit := make(chan os.Signal, 1)
	signal.Notify(quit, os.Interrupt)
	&amp;lt;-quit

	fmt.Println("Shutting down server...")

	if err := server.Close(); err != nil {
		fmt.Println("Error shutting down server:", err)
	}
}&lt;/code&gt;
    &lt;p&gt;For redis, I’ve implemented the cache using &lt;code&gt;github.com/redis/go-redis/v9&lt;/code&gt; as follows:&lt;/p&gt;
    &lt;code&gt;type RedisCache struct {
	client *redis.Client
}

func NewRedisCache() *RedisCache {
	redisURL := os.Getenv("REDIS_URL")
	if redisURL == "" {
		redisURL = "localhost:6379"
	}

	fmt.Println("Connecting to Redis at", redisURL)

	client := redis.NewClient(&amp;amp;redis.Options{
		Addr:     redisURL,
		Password: "",
		DB:       0,
	})

	return &amp;amp;RedisCache{
		client: client,
	}
}

func (r *RedisCache) Get(ctx context.Context, key string) (string, error) {
	val, err := r.client.Get(ctx, key).Result()
	if err == redis.Nil {
		return "", ErrCacheMiss
	}
	if err != nil {
		return "", err
	}
	return val, nil
}

func (r *RedisCache) Set(ctx context.Context, key string, value string) error {
	return r.client.Set(ctx, key, value, 0).Err()
}&lt;/code&gt;
    &lt;p&gt;The postgres cache is implemented using the &lt;code&gt;github.com/jackc/pgx/v5&lt;/code&gt; library:
&lt;/p&gt;
    &lt;code&gt;type PostgresCache struct {
	db *pgxpool.Pool
}

func NewPostgresCache() (*PostgresCache, error) {
	pgDSN := os.Getenv("POSTGRES_DSN")
	if pgDSN == "" {
		pgDSN = "postgres://user:password@localhost:5432/mydb"
	}

	cfg, err := pgxpool.ParseConfig(pgDSN)
	if err != nil {
		return nil, err
	}

	cfg.MaxConns = 50
	cfg.MinConns = 10

	pool, err := pgxpool.NewWithConfig(context.Background(), cfg)
	if err != nil {
		return nil, err
	}

	_, err = pool.Exec(context.Background(), `
		CREATE UNLOGGED TABLE IF NOT EXISTS cache (
			key VARCHAR(255) PRIMARY KEY,
			value TEXT
		);
	`)
	if err != nil {
		return nil, err
	}

	return &amp;amp;PostgresCache{
		db: pool,
	}, nil
}

func (p *PostgresCache) Get(ctx context.Context, key string) (string, error) {
	var content string
	err := p.db.QueryRow(ctx, `SELECT value FROM cache WHERE key = $1`, key).Scan(&amp;amp;content)
	if err == pgx.ErrNoRows {
		return "", ErrCacheMiss
	}
	if err != nil {
		return "", err
	}
	return content, nil
}

func (p *PostgresCache) Set(ctx context.Context, key string, value string) error {
	_, err := p.db.Exec(ctx, `INSERT INTO cache (key, value) VALUES ($1, $2) ON CONFLICT (key) DO UPDATE SET value = $2`, key, value)
	return err
}&lt;/code&gt;
    &lt;p&gt;I’ll seed the redis and postgres with 30 million entries each, keeping record of the inserted uuids. From there, I’ll generate a subset of existing uuids to use while benchmarking. This allows for simulating both hits and misses.&lt;/p&gt;
    &lt;p&gt;I’ll do a few runs of benchmarks for gets first, then sets and then a mixed run. Each run will execute for 2 minutes. I’ll look at the number of operations per second, latencies as well as memory and CPU usage during those times.&lt;/p&gt;
    &lt;p&gt;To simulate a somewhat real scenario where only a subset of keys exist in the cache the set benchmark will have a 10% chance to update an existing key, whereas the get will have an 80% chance of picking an existing key. The mixed workload will have a 20% chance to execute a set scenario and 80% for the get scenario.&lt;/p&gt;
    &lt;head rend="h2"&gt;The results&lt;/head&gt;
    &lt;head rend="h3"&gt;Getting values from cache&lt;/head&gt;
    &lt;p&gt;Redis performed better than Postgres, which did not surprise me at all. The bottleneck was actually the HTTP server. The machine running the http server maxed out on CPU, with redis running comfortably with ~1280mCPU - short of the 2000mCPU limit imposed. Redis used ~3800MiB of RAM, which stayed flat across the runs.&lt;/p&gt;
    &lt;p&gt;For postgres, the bottleneck was the CPU on postgres side. It consistently maxed out the 2 cores dedicated to it, while also using ~5000MiB of RAM.&lt;/p&gt;
    &lt;p&gt;Redis also did better when it comes to latencies of the HTTP responses:&lt;/p&gt;
    &lt;head rend="h3"&gt;Setting values in cache&lt;/head&gt;
    &lt;p&gt;Once again Redis performed better. The CPU usage stayed roughly the same as in the case of the GET experiment, with the RAM usage growing to ~4300MiB due to the new keys being inserted. The bottleneck stayed on the HTTP server side, with Redis using ~1280mCPU once again.&lt;/p&gt;
    &lt;p&gt;Postgres once again was bottlenecked by the CPU, constantly using 100% of the 2 cores it was limited to. During the course of the run, the memory usage grew to ~5500MiB.&lt;/p&gt;
    &lt;p&gt;During the test, the endpoints with the Redis cache implementation also had better latencies:&lt;/p&gt;
    &lt;head rend="h3"&gt;Read/write performance&lt;/head&gt;
    &lt;p&gt;The mixed benchmark also returned the predictable result of Redis reigning superior. As has been the story so far, the CPU stayed put at ~1280mCPU, RAM usage grew a bit due to the new keys being inserted.&lt;/p&gt;
    &lt;p&gt;Postgres maxed out the two cores and reached around 6GiB of memory used.&lt;/p&gt;
    &lt;p&gt;Latencies once again were better when using redis:&lt;/p&gt;
    &lt;head rend="h3"&gt;Unlogged tables&lt;/head&gt;
    &lt;p&gt;In the benchmark, I’ve used an unlogged table for postgres but this has not seemed to help, or has it? If I rerun the same benchmark with a normal(logged) table we can look at the numbers.&lt;/p&gt;
    &lt;p&gt;The unlogged table makes a huge difference for the write benchmark and a somewhat smaller but still significant one for the mixed workload. This is because the unlogged tables skip the write ahead log making them a lot faster for writes. There’s very little difference for the read performance though and I expect more runs would show the two test cases converging.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Redis is faster than postgres when it comes to caching, there’s no doubt about it. It conveniently comes with a bunch of other useful functionality that one would expect from a cache, such as TTLs. It was also bottlenecked by the hardware, my service or a combination of both and could definitely show better numbers. Surely, we should all use Redis for our caching needs then, right? Well, I think I’ll still use postgres. Almost always, my projects need a database. Not having to add another dependency comes with its own benefits. If I need my keys to expire, I’ll add a column for it, and a cron job to remove those keys from the table. As far as speed goes - 7425 requests per second is still a lot. That’s more than half a billion requests per day. All on hardware that’s 10 years old and using laptop CPUs. Not many projects will reach this scale and if they do I can just upgrade the postgres instance or if need be spin up a redis then. Having an interface for your cache so you can easily switch out the underlying store is definitely something I’ll keep doing exactly for this purpose.&lt;/p&gt;
    &lt;p&gt;Thanks for reading!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dizzy.zone/2025/09/24/Redis-is-fast-Ill-cache-in-Postgres/"/><published>2025-09-25T23:34:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45381010</id><title>Investigating a Forged PDF</title><updated>2025-09-26T10:40:07.314841+00:00</updated><content>&lt;doc fingerprint="767cce1f3dd3c327"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Captcha Check&lt;/head&gt;
    &lt;p&gt;Hello, you've been (semi-randomly) selected to take a CAPTCHA to validate your requests. Please complete it below and hit the button!&lt;/p&gt;
    &lt;p&gt;Hello, you've been (semi-randomly) selected to take a CAPTCHA to validate your requests. Please complete it below and hit the button!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mjg59.dreamwidth.org/73317.html"/><published>2025-09-26T00:14:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45381590</id><title>Exploit allows for takeover of fleets of Unitree robots</title><updated>2025-09-26T10:40:06.921012+00:00</updated><content>&lt;doc fingerprint="b4089113aaa71981"&gt;
  &lt;main&gt;
    &lt;p&gt;A critical vulnerability in the Bluetooth Low Energy (BLE) Wi-Fi configuration interface used by several different Unitree robots can result in a root level takeover by an attacker, security researchers disclosed on 20 September. The exploit impacts Unitree’s Go2 and B2 quadrupeds and G1 and H1 humanoids. Because the vulnerability is wireless, and the resulting access to the affected platform is complete, the vulnerability becomes wormable, say the researchers, meaning “an infected robot can simply scan for other Unitree robots in BLE range and automatically compromise them, creating a robot botnet that spreads without user intervention.”&lt;/p&gt;
    &lt;p&gt;Initially discovered by security researchers Andreas Makris and Kevin Finisterre, UniPwn takes advantage of several security lapses that are still present in the firmware of Unitree robots as of 20 September, 2025. As far as IEEE Spectrum is aware, this is the first major public exploit of a commercial humanoid platform.&lt;/p&gt;
    &lt;head rend="h2"&gt;Unitree Robots’ BLE Security Flaw Exposed&lt;/head&gt;
    &lt;p&gt;Like many robots, Unitree’s robots use an initial BLE connection to make it easier for a user to set up a Wi-Fi network connection. The BLE packets that the robot accepts are encrypted, but those encryption keys are hardcoded and were published on X (formerly Twitter) by Makris in July. Although the robot does validate the contents of the BLE packets to make sure that the user is authenticated, the researchers say that all it takes to become an authenticated user is to encrypt the string ‘unitree’ with the hardcoded keys and the robot will let someone in. From there, an attacker can inject arbitrary code masquerading as the Wi-Fi SSID and password, and when the robot attempts to connect to Wi-Fi, it will execute that code without any validation and with root privileges.&lt;/p&gt;
    &lt;p&gt;“A simple attack might be just to reboot the robot, which we published as a proof-of-concept,” explains Makris. “But an attacker could do much more sophisticated things: It would be possible to have a trojan implanted into your robot’s startup routine to exfiltrate data while disabling the ability to install new firmware without the user knowing. And as the vulnerability uses BLE, the robots can easily infect each other, and from there the attacker might have access to an army of robots.”&lt;/p&gt;
    &lt;p&gt;Makris and Finisterre first contacted Unitree in May in an attempt to responsibly disclose this vulnerability. After some back and forth with little progress, Unitree stopped responding to the researchers in July, and the decision was made to make the vulnerability public. “We have had some bad experiences communicating with them,” Makris tells us, citing an earlier backdoor vulnerability he discovered with the Unitree Go1. “So we need to ask ourselves—are they introducing vulnerabilities like this on purpose, or is it sloppy development? Both answers are equally bad.” Unitree has not responded to a request for comment from IEEE Spectrum as of press time.&lt;/p&gt;
    &lt;p&gt;“Unitree, as other manufacturers do, has simply ignored prior security disclosures and repeated outreach attempts,” says Víctor Mayoral-Vilches, the founder of robotics cybersecurity company Alias Robotics. “This is not the right way to cooperate with security researchers.” Mayoral-Vilches was not involved in publishing the UniPwn exploit, but he has found other security issues with Unitree robots, including undisclosed streaming of telemetry data to servers in China which could potentially include audio, visual, and spatial data.&lt;/p&gt;
    &lt;p&gt;Mayoral-Vilches explains that security researchers are focusing on Unitree primarily because the robots are available and affordable. This makes them not just more accessible for the researchers, but also more relevant, since Unitree’s robots are already being deployed by users around the world who are likely not aware of the security risks. For example, Makris is concerned that the Nottinghamshire Police in the UK have begun testing a Unitree Go2, which can be exploited by UniPwn. “We tried contacting them and would have disclosed the vulnerability upfront to them before going public, but they ignored us. What would happen if an attacker implanted themselves into one of these police dogs?”&lt;/p&gt;
    &lt;head rend="h2"&gt;How to Secure Unitree Robots&lt;/head&gt;
    &lt;p&gt;In the short term, Mayoral-Vilches suggests that people using Unitree robots can protect themselves by only connecting the robots to isolated Wi-Fi networks and disabling their Bluetooth connectivity. “You need to hack the robot to secure it for real,” he says. “This is not uncommon and why security research in robotics is so important.”&lt;/p&gt;
    &lt;p&gt;Both Mayoral-Vilches and Makris believe that fundamentally it’s up to Unitree to make their robots secure in the long term, and that the company needs to be much more responsive to users and security researchers. But Makris says: “There will never be a 100 percent secure system.”&lt;/p&gt;
    &lt;p&gt;Mayoral-Vilches agrees. “Robots are very complex systems, with wide attack surfaces to protect, and a state-of-the-art humanoid exemplifies that complexity.”&lt;/p&gt;
    &lt;p&gt;Unitree, of course, is not the only company offering complex state-of-the-art quadrupeds and humanoids, and it seems likely (if not inevitable) that similar exploits will be discovered in other platforms. The potential consequences here can’t be overstated—the idea that robots can be taken over and used for nefarious purposes is already a science fiction trope, but the impact of a high-profile robot hack on the reputation of the commercial robotics industry is unclear. Robots companies are barely talking about security in public, despite how damaging even the perception of an unsecured robot might be. A robot that is not under control has the potential to be a real physical danger.&lt;/p&gt;
    &lt;p&gt;At the IEEE Humanoids Conference in Seoul from 30 September to 2 October, Mayoral-Vilches has organized a workshop on Cybersecurity for Humanoids, where he will present a brief (co-authored with Makris and Finisterre) titled Humanoid Robots as Attack Vectors. Despite the title, their intent is not to overhype the problem but instead to encourage roboticists (and robotics companies) to take security seriously, and not treat it as an afterthought. As Mayoral-Vilches points out, “robots are only safe if secure.”&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This Robotics Startup Wants to Be the Boston Dynamics of China ›&lt;/item&gt;
      &lt;item&gt;Unitree’s New Go2 Is One Dynamic Quadruped ›&lt;/item&gt;
      &lt;item&gt;Unitree’s Go1 Robot Dog Looks Pretty Great, Costs Just USD $2700 ›&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Evan Ackerman is a senior editor at IEEE Spectrum. Since 2007, he has written over 6,000 articles on robotics and technology. He has a degree in Martian geology and is excellent at playing bagpipes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://spectrum.ieee.org/unitree-robot-exploit"/><published>2025-09-26T01:38:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45381631</id><title>Bit is all we need: binary normalized neural networks</title><updated>2025-09-26T10:40:06.655972+00:00</updated><content>&lt;doc fingerprint="853f05b8443e96ce"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Machine Learning&lt;/head&gt;&lt;p&gt; [Submitted on 7 Sep 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:1 bit is all we need: binary normalized neural networks&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:The increasing size of large neural network models, specifically language models and foundational image models, poses deployment challenges, prompting efforts to reduce memory requirements and enhance computational efficiency. These efforts are critical to ensure practical deployment and effective utilization of these models across various applications. In this work, a novel type of neural network layers and models is developed that uses only single-bit parameters. In this novel type of models all parameters of all layers, including kernel weights and biases, only have values equal to zero or one. This novel type of models uses layers named as binary normalized layer. These binary normalized layers can be of any type, such as fully connected, convolutional, attention, etc., and they consist of slight variations of the corresponding conventional layers. To show the effectiveness of the binary normalized layers, two different models are configured to solve a multiclass image classification problem and a language decoder to predict the next token of a sequence. The model to solve the image classification has convolutional and fully connected layers, and the language model is composed of transformer blocks with multi-head attention. The results show that models with binary normalized layers present almost the same results obtained by equivalent models with real 32-bit parameters. The binary normalized layers allow to develop models that use 32 times less memory than current models and have equivalent performance. Besides, the binary normalized layers can be easily implemented on current computers using 1-bit arrays, and do not require the development of dedicated electronic hardware. This novel type of layers opens a new era for large neural network models with reduced memory requirements that can be deployed using simple and cheap hardware, such as mobile devices or only cpus.&lt;/quote&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2509.07025"/><published>2025-09-26T01:43:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45381813</id><title>Writing Memory Safe JIT Compilers</title><updated>2025-09-26T10:40:06.457667+00:00</updated><content>&lt;doc fingerprint="8e229c19fc33becc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Writing Truly Memory Safe JIT Compilers&lt;/head&gt;
    &lt;head rend="h2"&gt;How to kill off a top source of browser exploits&lt;/head&gt;
    &lt;p&gt;Last month the V8 team published an excellent blog post on what they call the V8 Sandbox. This isn’t a sandbox for your JavaScript code — it’s intended to mitigate browser exploits caused by bugs in the JIT compiler itself. That’s important work because they report that most Chrome exploits start with a V8 memory safety bug.&lt;/p&gt;
    &lt;p&gt;V8 is written in C++, so it may seem like these are the sort of bugs you’d expect from working in a memory-unsafe language. Unfortunately the situation is more complex. Why? The team explain:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;There is a catch: V8 vulnerabilities are rarely “classic” memory corruption bugs (use-after-frees, out-of-bounds accesses, etc.) but instead subtle logic issues which can in turn be exploited to corrupt memory. As such, existing memory safety solutions are, for the most part, not applicable to V8. In particular, neither switching to a memory safe language, such as Rust, nor using current or future hardware memory safety features, such as memory tagging, can help with the security challenges faced by V8 today.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;They give an example bug that can cause memory corruption without the engine itself containing any normal memory safety problems, as VM intrinsics or JIT compiled machine code itself may accidentally rely on invalid assumptions about memory.&lt;/p&gt;
    &lt;p&gt;It would be nice if there was a rigorous approach to writing language runtimes that eliminated such bugs by design.&lt;/p&gt;
    &lt;p&gt;GraalVM has a JavaScript engine called GraalJS. It’s written in Java using the Truffle language framework. Its peak performance is competitive with V8 and on a few benchmarks (such as ray tracing) is actually faster!&lt;/p&gt;
    &lt;p&gt;Although being written in Java does improve memory safety, we just saw that rewriting V8 in a safe language wouldn’t help with the types of bugs V8 is trying to solve and so we would intuitively expect that GraalJS must suffer from the same classes of bugs. Yet, it doesn’t. Let’s take a look at why not. Along the way we’ll explore the first Futamura projection, the core theoretical idea underpinning Truffle.&lt;/p&gt;
    &lt;p&gt;All fast language VMs work the same way. A program is loaded from disk into in-memory data structures representing the program, either an abstract syntax tree or byte code. The program starts running in an interpreter. Parts are soon discovered to be hot spots, i.e. the program spends much more time there than in other parts. Those hot spots are passed to a just-in-time compiler that converts them to optimized machine code, and execution then jumps back and forth between the interpreter and the collection of compiled program fragments. This gives a big performance boost.&lt;/p&gt;
    &lt;p&gt;This architecture is standard — both the JVM and V8 use it — but viewed from a security perspective the design has a flaw: it’s error prone. The language semantics are implemented twice, once for the interpreter and again for the JIT compiler. It’s critical not only that both places are fully correct but also that they exactly match. Otherwise, the VM becomes exploitable.&lt;/p&gt;
    &lt;p&gt;Truffle is a Java library that helps you build advanced, high performance language runtimes. VMs built using the Truffle framework operate in a fundamentally different way to conventional VMs, one that not only makes them much easier to write but which also eliminates memory safety bugs by design. It all starts with you writing an interpreter for your language in Java. This doesn’t mean compiling your target language to JVM bytecode — in fact bytecode won’t feature anywhere in this story. You just write an ordinary interpreter. Because the interpreter’s code is garbage collected and bounds-checked, malicious user code can’t use memory safety bugs to exploit it.&lt;/p&gt;
    &lt;p&gt;If you think about conventional Java then this may sound quite slow — isn’t Java itself interpreted until it gets JIT compiled? Are we … interpreting an interpreter? Fortunately not because you can ship your Truffle-based language runtime as a native executable, meaning it’s compiled ahead of time to fully native code using the Graal compiler (from which the wider umbrella project takes its name).&lt;/p&gt;
    &lt;p&gt;So at the start of the user’s program their JavaScript is running in a regular interpreter shipped as a normal executable binary or DLL, but which still benefits from the safety properties of a Java program. Soon some methods get hot. At this point something unconventional happens. The Truffle framework is keeping track of which functions are hot for you and will decide to schedule JIT compilations. But unlike in a conventional VM design, you don’t write your own JIT compiler. Instead your user’s code is automatically compiled by the same general-purpose Graal compiler that was used to convert your interpreter to native code, and execution will start automatically switching back and forth between the interpreter and compiled functions. This is possible thanks to an unusual technique called partial evaluation (or the first Futamura projection).&lt;/p&gt;
    &lt;p&gt;You might not have encountered Futamura projections or partial evaluation before, so what is this strange sounding thing?&lt;/p&gt;
    &lt;p&gt;The core idea is to automatically transform the code of your interpreter to create individual JIT compiled user methods. Instead of needing to carefully implement the language semantics in two places (interpreter and hand-crafted JIT), it’s sufficient to implement it just once. As the interpreter is memory safe and the transform preserves interpreter semantics, the compiled version of the user’s code is guaranteed to match the interpreter’s behavior and is therefore also automatically memory safe. This makes it much harder to slip up and write an exploitable VM.&lt;/p&gt;
    &lt;p&gt;There are several tricks that make this possible. The most important is a new form of constant-ness, added to Java using annotations. In normal programming a variable is either mutable or immutable. An immutable variable is marked with a special keyword such as&lt;code&gt;final&lt;/code&gt; or &lt;code&gt;const&lt;/code&gt; and must be set only once, at the declaration site. Constants are great for compilers because they can be folded, meaning that references to them can be replaced with their value. Consider the following bit of code:&lt;/p&gt;
    &lt;code&gt;class Example {&lt;lb/&gt;    private static final int A = 1;&lt;lb/&gt;    private static final int B = 2;&lt;lb/&gt;&lt;lb/&gt;    static int answer() {&lt;lb/&gt;        return A - B;&lt;lb/&gt;    }&lt;lb/&gt;&lt;lb/&gt;    static String doSomething() {&lt;lb/&gt;        if (answer() &amp;lt; 0) &lt;lb/&gt;            return "OK" &lt;lb/&gt;        else &lt;lb/&gt;            throw new IllegalStateException();&lt;lb/&gt;    }&lt;lb/&gt;}&lt;/code&gt;
    &lt;p&gt;It’s easy to see the &lt;code&gt;answer()&lt;/code&gt; method will always return the same number. A good compiler will substitute 1 and 2 into the expression yielding &lt;code&gt;return 1 — 2&lt;/code&gt; and pre-compute the answer. Then it will inline any calls to answer (i.e. copy/paste the implementation into the call site), substituting those with -1 and thus removing the call overhead as well. That in turn may trigger even more constant folding, such as in the &lt;code&gt;doSomething&lt;/code&gt; method where the compiler will prove that the exception can never be thrown and delete it entirely. Having done that, &lt;code&gt;doSomething&lt;/code&gt; can also be optimized out by simply replacing it with “OK”, and so on.&lt;/p&gt;
    &lt;p&gt;That’s neat, but every compiler can do that … as long as the constant values are known at compile time. Truffle changes that by introducing a third kind of const-ness called compilation final. If in your interpreter implementation you declare a variable like this:&lt;/p&gt;
    &lt;code&gt;@CompilationFinal private int a = 1;&lt;/code&gt;
    &lt;p&gt;then it will change its const-ness depending on when it’s being accessed. From inside your interpreter, it’s mutable. You will use such variables to implement your interpreter. They’ll be set when you load your user’s program and maybe also whilst it runs. Once a function in the user’s script becomes hot, Truffle will work together with the Graal compiler to recompile the parts of the interpreter corresponding to the user’s code, and this time &lt;code&gt;a&lt;/code&gt;will be treated as if it was a constant, i.e. the same as the literal value &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This works for any kind of data, including complex objects. Consider the following highly simplified pseudocode:&lt;/p&gt;
    &lt;code&gt;import com.oracle.truffle.api.nodes.Node;&lt;lb/&gt;&lt;lb/&gt;class JavaScriptFunction extends Node {&lt;lb/&gt;    @CompilationFinal Node[] statements;&lt;lb/&gt;&lt;lb/&gt;    Object execute() {&lt;lb/&gt;        for (var statement : statements) statement.execute();&lt;lb/&gt;    } &lt;lb/&gt;}&lt;/code&gt;
    &lt;p&gt;This is the sort of class you might find in a typical abstract syntax tree interpreter. The &lt;code&gt;statements&lt;/code&gt; array is marked compilation-final. When the program is first loaded we can initialize the array with objects representing the different things a user’s JavaScript function is doing, because it’s mutable. Now imagine that the function represented by this object gets hot. Truffle will start a special compilation of the &lt;code&gt;execute()&lt;/code&gt; method in which Graal is told that the &lt;code&gt;this&lt;/code&gt; pointer should be treated implicitly as compilation-final. Because the object is treated as constant, so can &lt;code&gt;this.statements&lt;/code&gt; also be treated as constant. It’ll be substituted with the exact contents of a specific &lt;code&gt;JavaScriptFunction&lt;/code&gt; object on the interpreter heap enabling the compiler to unroll the loop inside &lt;code&gt;execute&lt;/code&gt;, transforming it to look like this:&lt;/p&gt;
    &lt;code&gt;Object execute() {&lt;lb/&gt;    this.statements[0].execute();&lt;lb/&gt;    this.statements[1].execute();&lt;lb/&gt;    this.statements[2].execute();&lt;lb/&gt;}&lt;/code&gt;
    &lt;p&gt;Here &lt;code&gt;Node&lt;/code&gt; is a superclass and &lt;code&gt;execute()&lt;/code&gt; is virtual, but that doesn’t matter. Because the list is compilation-final the individual objects in the list are also constant folded, so the &lt;code&gt;execute&lt;/code&gt; method can be de-virtualized (resolved to whatever concrete type it really is) and then inlined as well.&lt;/p&gt;
    &lt;p&gt;And on and on we go. At the end the compiler generates a native function which matches the semantics of the user’s JavaScript (or Python or C++ or whatever language we’re implementing). Invocations of the specific &lt;code&gt;JavaScriptFunction.execute()&lt;/code&gt; method that were compiled are diverted, so when the interpreter invokes it, there will be a transition from interpreter to native code and back. If your interpreter realizes it needs to change a &lt;code&gt;@CompilationFinal&lt;/code&gt; field, for example because the program changes its behavior and invalidates an optimistic assumption you made, that's absolutely fine. Truffle will let you do that and "deoptimizes" the program back to the interpreter for you. Deoptimization (tech talk) is an advanced technique that's normally very hard to implement securely, as it means mapping the optimized CPU state back to the interpreter state and once again, any mistakes can be exploitable (you may be seeing a theme here). But you don’t have to write any of this. It’s all done for you by Truffle.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why does this work?&lt;/head&gt;
    &lt;p&gt;It might not be obvious why partial evaluation actually makes things faster.&lt;/p&gt;
    &lt;p&gt;Interpreters are slow because they have to make a lot of decisions. The user’s program could do anything, so interpreters must constantly check for many possibilities to find out what the program is trying to do at that exact moment. Because branches and memory loads are difficult for the CPU to execute quickly, the whole program ends up being slow. This technique of compiling an interpreter with enhanced constant folding eliminates branches and loads. On top of this, Truffle builds an API that makes it easy to implement advanced optimizations and features for JavaScript or indeed, for any other language you have an interpreter for. For example, it offers a simple API for using assumptions — a way to JIT compile code that executes faster by not including code for handling edge cases. If such an edge case is hit then the compiled code can be thrown away and regenerated to take into account that the edge case was observed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Recompilation&lt;/head&gt;
    &lt;p&gt;Above we briefly mentioned “recompilation”, but glossed over how that’s possible. We said the interpreter is just native code, right?&lt;/p&gt;
    &lt;p&gt;When the interpreter was compiled ahead of time with the &lt;code&gt;native-image&lt;/code&gt; in preparation for shipping to the user’s computer, the Graal compiler recognized that it was compiling a Truffle-using program. Graal and Truffle are co-developed, so although they can be used independently, when used together they recognize each other and collaborate.&lt;/p&gt;
    &lt;p&gt;Graal changes its behavior in a couple of ways when it notices it’s compiling a Truffle language ahead-of-time. Firstly, it adds a copy of itself to the output program. Interpreter methods are then discovered by doing a static analysis of the program and then stored in the resulting executable, but with a twist: they’re stored more than once. One version is directly executable machine code. That’s your regular generic interpreter. Another is a carefully encoded form of Graal’s intermediate representation (or IR). An IR is sort of half way between the source code you write and the machine code that eventually executes (Graal’s IR is an object graph). Graal also compiles in a garbage collector, either the advanced and mature G1 collector (if you use Oracle GraalVM) or a simpler GC written in pure Java (if you use the GraalVM Community Edition).&lt;/p&gt;
    &lt;p&gt;When a user function gets hot, Truffle looks up the embedded IR for the “execute a user function” node and partially evaluates it. The evaluation is interleaved with the parsing of the graph IR to ensure that the process is as efficient as possible — if something won’t be executed because constant folding already proved it can’t be reached it won’t even be decoded or seen by the compiler. This also ensures that memory usage during the compile is kept low.&lt;/p&gt;
    &lt;head rend="h2"&gt;My only friend, the end&lt;/head&gt;
    &lt;p&gt;And that’s it! That’s how an entire class of subtle safety bugs is eliminated in GraalJS: because the semantics of the language are defined by the memory-safe interpreter and then partially evaluated, the generated machine code is also memory safe by construction.&lt;/p&gt;
    &lt;p&gt;What about the V8 sandbox that the original blog post is about? Expressing pointers as offsets from a heap base is a great idea that’s already used in GraalVM natively compiled binaries. However this is done for performance, as the other memory safety mechanisms mean there’s no need for mitigating heap overwrites.&lt;/p&gt;
    &lt;p&gt;None of the above is in any way specific to JavaScript, and nor are Truffle’s benefits limited to security and performance. In fact Truffle automatically adds many other features to your language, such as debugging (through Chrome Debugger’s wire protocol), language interop with both Java/Kotlin/etc and any other Truffle language, a fast regular expression engine, a fast foreign function interface, profiling tools, heap snapshotting and much more. Truffle has been used to build over 30 language VMs for dozens of languages, including languages you wouldn’t expect to have such features such as the recent Pkl configuration language from Apple.&lt;/p&gt;
    &lt;p&gt;If this article has whetted your appetite to learn more, take a look at the documentation or this tech talk on how it all works.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://medium.com/graalvm/writing-truly-memory-safe-jit-compilers-f79ad44558dd"/><published>2025-09-26T02:10:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45382397</id><title>My Deus Ex lipsyncing fix mod</title><updated>2025-09-26T10:40:05.969428+00:00</updated><content>&lt;doc fingerprint="43251212cb9cd376"&gt;
  &lt;main&gt;
    &lt;p&gt;Back in 2021 I made a mod for Deus Ex 1 that fixes the lipsyncing and blinking, which, I betcha didn’t know, was broken since ship. Everything I wrote about it is on Twitter, and it oughta be somewhere else, so here’s a post about it. The mod itself can be downloaded here.&lt;/p&gt;
    &lt;p&gt;I guess I was playing DX1 and thinking, geez, was this lipsync always this bad? In a weird way? It’s insta-snapping mouth shapes, but they’re not always the same mouth shapes. Is this broken? I couldn’t find anything online about it, but I did find this article: an interview with Chris Norden, a coder on DX, where he goes into the lipsyncing and how it was, at one point, super elaborate and amazing, and they had to pare it back for performance reasons. I thought I’d check how much of this was done in Unrealscript (since the C++ source for DX is nowhere) and whether I could un-pare it. It turns out it was an extremely simple fix to get it as good as I got it, and I think that’s as good as you can get it until someone leaks the source code.&lt;/p&gt;
    &lt;p&gt;I’d messed around with lipsyncing stuff before and was familiar with the broad strokes of how it tends to work via my intense familiarity with Half-Life 2: you figure out, hopefully automatically, the sounds (phonemes) present in a sound file (“oo”, “ah”, whatever) and map those to mouth shapes (visemes), then when the audio plays, move the mouth into the right shape for the phoneme we’re in at this moment. The figuring-out process is called “phoneme extraction”, at least by Valve, and Valve do this offline, because it takes a sec. In Valve’s case they append this phoneme information to the end of the .wav file, and it looks like this:&lt;/p&gt;
    &lt;code&gt;PLAINTEXT
{
Okay, I don't blame you for hesitating, but if we're gonna do this thing, then let's just get through it. 
}
WORDS
{
WORD Okay 0.064 0.224
{
111 ow 0.014 0.096 1.000
107 k 0.096 0.142 1.000
101 ey 0.142 0.220 1.000
}
WORD I 0.224 0.352
{
593 ay 0.220 0.310 1.000
105 iy 0.310 0.364 1.000
}
WORD don't 0.352 0.496
{
100 d 0.364 0.396 1.000
111 ow 0.396 0.456 1.000
110 n 0.456 0.496 1.000
}
&lt;/code&gt;
    &lt;p&gt;, etc. Phonemes, start times, end times. Easy!&lt;/p&gt;
    &lt;p&gt;My assumption is that the reason Deus Ex’s super cool lipsyncing was too expensive to ship was, they don’t seem to save this information anywhere, so I guess they were figuring out the phonemes in realtime. If correct, this is sort of a bummer – doing what Valve did would have scooped the whole cost out. Maybe there was more to it.&lt;/p&gt;
    &lt;p&gt;Anyway, the Unrealscript. Deus Ex is pre-Unreal having skeletal animation, it’s all vertex animation. The character heads have a few: relevant here, 7 visemes and a blink. &lt;code&gt;nextphoneme&lt;/code&gt; is set from somewhere outside this code (probably a cpp audio system I can’t access) to A, E, F, M, O, T or U, which it doesn’t matter which is which and I don’t remember, or X, which is nothing (close mouth). Then this Unrealscript on the character sets the head’s anim sequence to the appropriate pose. This all happens on tick, but only if &lt;code&gt;IsSpeaking&lt;/code&gt;. We have a &lt;code&gt;tweentime&lt;/code&gt; we’re using to blend between these poses, so we should be seeing nice smooth blending, the lack of which is why I’m here in the first place! So what’s the problem?&lt;/p&gt;
    &lt;p&gt;The main thing is a dodgy frame rate check:&lt;/p&gt;
    &lt;code&gt;// update the animation timers that we are using
	animTimer[0] += deltaTime;
	animTimer[1] += deltaTime;
	animTimer[2] += deltaTime;

	if (bIsSpeaking)
	{
		// if our framerate is high enough (&amp;gt;20fps), tween the lips smoothly
		if (Level.TimeSeconds - animTimer[3]  &amp;lt; 0.05)
			tweentime = 0;
		else
			tweentime = 0.1;
&lt;/code&gt;
    &lt;p&gt;“tweentime” is how long it takes to blend to the next viseme in seconds; if 0, it’s an instant snap. The intent here is to skip blending entirely if our framerate is so low that it looks better snapping the lips around than showing any in-between poses, only it doesn’t work. The code is keeping &lt;code&gt;Level.TimeSeconds&lt;/code&gt; from the previous frame and subtracting that from the current &lt;code&gt;Level.TimeSeconds&lt;/code&gt; to get deltatime, which if it’s less than 0.05, we’re assumed to be getting less than 20fps. So it’s flipped.&lt;/p&gt;
    &lt;p&gt;Also, 0.1 is just way too fast a value, which I suspect a reason for that I’ll come back to*. I increased it to 0.35 to make the blends take long enough to really see.&lt;/p&gt;
    &lt;p&gt;With that fixed, the lipsync is smooth! Hooray! But it’s not perfect: at the end of a line, when the audio finishes, we don’t smoothly close the mouth; we snap the mouth shut instantly. This is because we’re only doing any blending if &lt;code&gt;bIsSpeaking=true&lt;/code&gt;, which it suddenly isn’t. The perf hit of this function no longer matters at all, so I just skip that check too: every character always gets to run lipsync. Tweentime is also local to this function and initialises at 0, so I had to set it to 0.3 to get blending even when we have no phoneme.&lt;/p&gt;
    &lt;p&gt;Blinking was also way too fast, so fast as to be invisible, so I slowed it down a ton. Now you can see ’em blinkin’.&lt;/p&gt;
    &lt;p&gt;So now we have nice blinking and smooth mouth movement, but there’s one thing that still sucks: presumably as part of the optimisation that made this ship at all, &lt;code&gt;nextphoneme&lt;/code&gt; does not update every tick, or anywhere near every tick. It doesn’t even update at a fixed rate – sometimes you’ll get a good amount of updates in a sentence, sometimes one or two. This means that all the smooth blending in the world won’t get you a correct result unless you happen to get lucky: JC can be speaking the M in “a bomb” and you’re still back on the “a”. As far as I can tell there’s no way to fix this right now – the code that updates the phonemes just needs to do it every tick, and it don’t, and it’s not Unrealscript so I can’t touch it. If the time between phoneme updates was at least consistent, you could set tweentime to that duration and make your blend take as long as it takes for a new phoneme to show up, but it ain’t. So close!&lt;/p&gt;
    &lt;p&gt;*In the interview where Norden alludes to this amazing lipsync demo they had going on before they optimised it down, I assume it was initially getting a new phoneme every tick, and that is probably when they set 0.1 seconds as a blend duration. If you’re getting constant new phonemes, blending super fast to the next one makes sense; it’s only when you’re not that a slower blend time looks good.&lt;/p&gt;
    &lt;p&gt;There’s a lot of jank to this code. The silliest thing about it might be that it lives in &lt;code&gt;ScriptedPawn&lt;/code&gt;, Deus Ex’s NPC class, which does not share an immediate parent with the player character, so this whole function is just duplicated between the two classes.&lt;/p&gt;
    &lt;p&gt;Anyway, here’s the whole function after I futzed with it.&lt;/p&gt;
    &lt;code&gt;// lip synching support - DEUS_EX CNN
//
function LipSynch(float deltaTime)
{
	local name animseq;
	local float rnd;
	local float tweentime;

	// update the animation timers that we are using
	animTimer[0] += deltaTime;
	animTimer[1] += deltaTime;
	animTimer[2] += deltaTime;

	if (bIsSpeaking)
	{
		// if our framerate is high enough (&amp;gt;20fps), tween the lips smoothly
		
//JOE CHANGE: 
//This used to set tweentime to 0 (no blend) if it thought FPS was low, else 0.1. It was 
//backwards though, the result was the opposite. 
//Even 0.1 is too fast to look good though. Anyway, skip the check, we don't care
//
//		if (Level.TimeSeconds - animTimer[3]  &amp;lt; 0.05)
//			tweentime = 0.4;
//		else
			tweentime = 0.36;

//Also, ideally tweentime would be the duration until the next time we get a phoneme update?
//But I don't know where that update comes from at the moment

		// the last animTimer slot is used to check framerate
		animTimer[3] = Level.TimeSeconds;

		if (nextPhoneme == "A")
			animseq = 'MouthA';
		else if (nextPhoneme == "E")
			animseq = 'MouthE';
		else if (nextPhoneme == "F")
			animseq = 'MouthF';
		else if (nextPhoneme == "M")
			animseq = 'MouthM';
		else if (nextPhoneme == "O")
			animseq = 'MouthO';
		else if (nextPhoneme == "T")
			animseq = 'MouthT';
		else if (nextPhoneme == "U")
			animseq = 'MouthU';
		else if (nextPhoneme == "X")
			animseq = 'MouthClosed';

		if (animseq != '')
		{
					if (lastPhoneme != nextPhoneme)
			{
				lastPhoneme = nextPhoneme;
				TweenBlendAnim(animseq, tweentime);
				TimeLastPhoneme = Level.TimeSeconds;
			}
		
		}
		

//		if ((Level.TimeSeconds - TimeLastPhoneme) &amp;gt;= tweentime*0.8 &amp;amp;&amp;amp; TimeLastPhoneme != 0)
//		{
//		TweenBlendAnim('MouthClosed', 0.2);
//		nextPhoneme = "X";
//		lastPhoneme = "A";
//		TimeLastPhoneme = Level.TimeSeconds;
//		}
	}
	else
	if (bWasSpeaking)
	{
		bWasSpeaking = false;
		
//JOE: I added this tweentime set. Without it it was 0 as initialised, so the jaw snapped shut

		tweentime = 0.3;
		TweenBlendAnim('MouthClosed', tweentime);
	}

	// blink randomly
	if (animTimer[0] &amp;gt; 0.5)
	{
		animTimer[0] = 0;
		if (FRand() &amp;lt; 0.4)
			PlayBlendAnim('Blink', 0.2, 0.1, 1);
	}

	LoopHeadConvoAnim();
	LoopBaseConvoAnim();
}
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.joewintergreen.com/my-deus-ex-lipsyncing-fix-mod-making-of/"/><published>2025-09-26T03:45:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45382434</id><title>Evanston orders Flock to remove reinstalled cameras</title><updated>2025-09-26T10:40:05.807620+00:00</updated><content>&lt;doc fingerprint="6f8091763aa212fe"&gt;
  &lt;main&gt;
    &lt;p&gt;Private surveillance vendor Flock Safety reinstalled all of its stationary license plate cameras in Evanston that had previously been removed, apparently doing so without authorization from the city, which sent the company a cease-and-desist order Tuesday afternoon demanding that the cams be taken back down.&lt;/p&gt;
    &lt;p&gt;The city previously ordered Flock to shut down 19 automated license plate readers (18 stationary and one flex camera that can be attached to a squad car) provided by the company and put its contract with Flock on a 30-day termination notice on Aug. 26.&lt;/p&gt;
    &lt;p&gt;This decision came after Illinois Secretary of State Alexi Giannoulias discovered that Flock had allowed U.S. Customs and Border Protection to access Illinois cameras in a “pilot program” against state law, and after the RoundTable reported in June that out-of-state law enforcement agencies were able to search Flock’s data for assistance in immigration cases.&lt;/p&gt;
    &lt;p&gt;Flock had removed 15 of the 18 stationary cameras by Sept. 8, only to reinstall each one at or near its prior location by Tuesday. City spokesperson Cynthia Vargas said in a written statement that the city has not deviated from or made any changes to its policies “since the earlier contract termination, meaning Flock reinstalled the cameras without the city’s permission.”&lt;/p&gt;
    &lt;p&gt;“Recently, we became aware that Flock has reinstalled the physical cameras that they had previously taken down,” Vargas wrote. “We immediately issued a cease-and-desist order to Flock. Earlier this afternoon, Flock committed to promptly removing the cameras.”&lt;/p&gt;
    &lt;p&gt;Flock did not immediately respond to a request for comment from the RoundTable on Tuesday night.&lt;/p&gt;
    &lt;p&gt;The city first installed Flock cameras in late 2022 and early 2023 as part of two separate one-year contracts, and City Council later approved a single five-year contract extension in January 2024.&lt;/p&gt;
    &lt;p&gt;The city has paid the first two years of that extension but would still owe $145,500 for the final three years if the contract is upheld. The city intends to terminate the contract on Sept. 26 under its notice to Flock, but the company is challenging that termination, and the dispute could escalate to litigation.&lt;/p&gt;
    &lt;head rend="h4"&gt;Same spots, with some different models&lt;/head&gt;
    &lt;p&gt;The RoundTable mapped and photographed each of the 18 stationary cameras in June, and site visits on Sept. 8 confirmed that all but three had been removed by Flock. The last three, which appear to have never been removed, are the north-facing cameras at Howard Street’s intersections with Chicago, Ridge and Dodge avenues.&lt;/p&gt;
    &lt;p&gt;Further site visits Tuesday confirmed that the 15 removed cameras had been replaced at the same locations. Most of them were banded back onto public streetlight fixtures where they were placed before, while five located on east-west streets along McCormick Boulevard had individual poles reinstalled into the ground. Near three of these pole mounts were freshly spray painted lines, the word “FLOCK” and numbers appearing to designate the cameras individually.&lt;/p&gt;
    &lt;p&gt;A Reddit user posted a photo to the r/Evanston subreddit on Monday evening showing a worker installing one of these pole mounts and its camera earlier that morning at the corner of McCormick and Main Street.&lt;/p&gt;
    &lt;p&gt;The worker is seen on a ladder holding the camera’s solar panel in front of the pole mount, and behind them is an Enterprise-branded rental van parked on the sidewalk in front of the sign for the Skokie Northshore Channel Park. Although this camera and the one at McCormick and Oakton Street are installed outside of Evanston’s city limits, they both fall under Evanston’s contract with Flock, rather than Skokie’s.&lt;/p&gt;
    &lt;p&gt;Click on the images in the gallery above to see them full screen.&lt;/p&gt;
    &lt;p&gt;Additionally, not all of the reinstalled cameras were “Falcon” models — the long, oval-shaped camera with a solar panel and battery packs that was previously used in every location.&lt;/p&gt;
    &lt;p&gt;At five locations, there was instead a stubbier camera that looks similar to the “Standard” model currently advertised on Flock’s website, except with an extra attachment under the main body. These five also appear to lack solar panels, instead attaching to several previously unseen boxes, and at least one camera is attached to a wire connected to the city-owned light post it’s mounted to, suggesting it may draw power from the city’s grid.&lt;/p&gt;
    &lt;p&gt;Click on the images in the gallery above to see them full screen.&lt;/p&gt;
    &lt;head rend="h4"&gt;Analysis: Flock’s data suggests cams could be active&lt;/head&gt;
    &lt;p&gt;Even before any cameras were initially removed, none of them were supposed to be collecting any data. The city wrote in its Aug. 26 announcement that the 19 cameras were “no longer collecting or providing license plate reader data to the Flock network,” and EPD Cmdr. Scott Sophier reconfirmed this to the RoundTable on Sept. 8.&lt;/p&gt;
    &lt;p&gt;“The last read on an Evanston Flock camera was logged shortly before 1:00 p.m. on August 26th, which is consistent with the City’s request for de-activation,” Sophier said at the time.&lt;/p&gt;
    &lt;p&gt;However, Flock’s own publicly available data suggests that may not be the case.&lt;/p&gt;
    &lt;p&gt;The company maintains a “transparency portal” webpage for Evanston that updates daily with basic data on the cameras’ operations, including “Number of LPR [license plate readers] and other cameras” and “Vehicles detected in the last 30 days.” The RoundTable has tracked this page since shortly after the city’s shutdown order, logging the data and archiving updates on most days.&lt;/p&gt;
    &lt;p&gt;The “Number of LPR and other cameras” figure was at 19 when the shutdown was ordered, matching Evanston’s 19 cameras, but it later dropped to 10 on Aug. 30. Rather than falling to zero, however, the figure stayed at 10 until Sept. 16, when it increased to 12, eventually returning to 19 on Sept. 23, matching the reinstallation of all the cameras.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the “Vehicles detected in the last 30 days” number has steadily decreased since the shutdown order, with each passing update rolling off another day when the cameras were known to be active. However, the figure has not decreased enough over time to actually reach zero once 30 days have passed.&lt;/p&gt;
    &lt;p&gt;When the RoundTable began tracking this figure on Aug. 28, it stood at 439,542 vehicles detected over approximately 28 days of active cameras. To reach zero by 30 days post-shutdown, the figure would need to drop by an average of around 15,700 each day, because every new day added to the data should have included zero new vehicles detected.&lt;/p&gt;
    &lt;p&gt;Based on the city’s Aug. 26 termination notice, there should only be two full days’ worth of vehicle detections left on Flock’s data portal as of late Tuesday, Sept. 23. But the page still reports 155,507 vehicles detected in the last 30 days, yielding a reduction of 284,035 vehicles over 26 days, or around 10,924 per day — well below the reduction rate needed to reach zero.&lt;/p&gt;
    &lt;p&gt;This trend means that on Friday, Sept. 26, when more than 30 days will have passed since the city’s cameras were supposed to be shut down, Flock will still report some number of vehicles as being detected in the prior 30 days. That suggests some number of cameras may have remained active and logging vehicles after Aug. 26, in violation of the city’s order and without the city’s knowledge, as indicated by Sophier’s response to the RoundTable on Sept. 8.&lt;/p&gt;
    &lt;p&gt;“Flock has not indicated to the City in direct communications that any ALPR’s are active or have been re-activated,” Sophier wrote. “There is no indication that Flock did not honor/fulfill the City’s request and also no indication on the City’s end to show any plate reads since the aforementioned date/time.”&lt;/p&gt;
    &lt;p&gt;Flock did not answer questions about this data sent by the RoundTable on Sept. 8. Site visits by the RoundTable that day confirmed that the 15 aforementioned cameras had been removed by that time.&lt;/p&gt;
    &lt;p&gt;Update: The City of Evanston has covered up the Flock cameras while waiting for their removal.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://evanstonroundtable.com/2025/09/24/flock-safety-reinstalls-evanston-cameras/"/><published>2025-09-26T03:51:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45382645</id><title>A platform-jumping prince – History of Prince of Persia's 1990s Ports</title><updated>2025-09-26T10:40:05.272239+00:00</updated><content>&lt;doc fingerprint="a7c6792f1b92179c"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;A platform-jumping prince&lt;/head&gt;
    &lt;p&gt;"Which is your favorite/definitive version of the original Prince of Persia game?"&lt;/p&gt;
    &lt;p&gt;I get this question surprisingly often, considering it's been 35 years. I figured it deserves a blog post.&lt;/p&gt;
    &lt;head rend="h4"&gt;Apple II&lt;/head&gt;
    &lt;p&gt;The Apple II version was the original. It's the only version I programmed myself; Prince of Persia's gameplay, graphics, animation and music were all created on the Apple II. I spent three years sweating over every byte (from 1986 to 1989), so it's close to my heart in a way no other version can be. That said...&lt;/p&gt;
    &lt;head rend="h4"&gt;DOS/Windows&lt;/head&gt;
    &lt;p&gt;The 1990 PC version, developed in parallel with the Apple II and shipped a few months later, took advantage of the PC's improved graphics and sound capabilities to deliver the Prince of Persia most players remember (in CGA, EGA, or VGA). My dad, Francis Mechner, re-orchestrated his music (previously limited by the Apple II's tinny built-in speaker) for MIDI synthesizers. The Broderbund in-house team, led by programmer Lance Groody, with Leila Joslyn on art, Tom Rettig on sound, and me as director, stayed faithful to the Apple game while upping the quality in every dimension. The digitized spike and slicer sound effects that traumatized many an elementary-school gamer originated with the PC version. If someone asked me the best way to play old-school PoP online today, I'd likely recommend the DOS version.&lt;/p&gt;
    &lt;p&gt;In 1990, C-family programming languages were the future, 6502 machine language the past. For good reasons, nearly all subsequent ports of PoP took the PC version as their starting point, rather than the Apple II.&lt;/p&gt;
    &lt;head rend="h4"&gt;Amiga&lt;/head&gt;
    &lt;p&gt;The Amiga port was developed by Dan Gorlin (of Choplifter fame), in parallel with the PC version, using the graphics and sound assets developed by the Broderbund team.&lt;/p&gt;
    &lt;p&gt;Danny was one of my game-author heroes. Playing Choplifter, as a 17-year-old college freshman in 1982, blew me away and set me on the creative path that would lead to Karateka. I was star-struck that he agreed to port PoP to Amiga. He did an impeccable job, working alone at home, using the state-of-the-art development system he'd built for his games Airheart and Typhoon Thompson.&lt;/p&gt;
    &lt;p&gt;In a detail perhaps mainly interesting to lawyers, Amiga was one of three PoP versions (Apple II and Macintosh were the others) that I was contractually responsible for delivering to Broderbund, rather than their doing the development. This meant me driving to Danny's house for meetings instead of to Broderbund, and that I was on the hook in case the project fell behind schedule or something went wrong. Fortunately, with Danny, all was smooth sailing.&lt;/p&gt;
    &lt;head rend="h4"&gt;Commodore 64&lt;/head&gt;
    &lt;p&gt;One port that didn't get greenlit was the Commodore 64. Like the Apple II, the C64 had its heyday in the mid-1980s. By 1990, Broderbund (and most U.S. retailers) considered the C64 and Apple II outdated platforms; sales numbers were dwindling by the month. Broderbund couldn't escape publishing PoP on Apple, since it was the lead platform I created the game on, but they had little interest in a C64 version. It would have been a tough port in any case. To fit PoP into 64K of memory, with the Commodore's technical limitations, needed an ace 6502 programmer.&lt;/p&gt;
    &lt;p&gt;In a twist I'd never have predicted, an unofficial, fan-made C64 port was finally done in 2011, over 20 years later, and a Commodore Plus/4 port just last year. I hope my Apple II source code was helpful.&lt;/p&gt;
    &lt;head rend="h4"&gt;Macintosh&lt;/head&gt;
    &lt;p&gt;In 1984, Apple unveiled the Macintosh computer (with a now-legendary Super Bowl ad). Still in college, and flush with Karateka royalties, I took advantage of the student discount to purchase a 128K Mac — keeping my Apple IIe for games. (A computer with no lowercase, and enough RAM to hold four pages of text, isn't ideal for writing term papers.) I loved my Mac, and faithfully upgraded my system every time they did: Mac Plus, SE, II, IIci, LC. By 1990, I was proudly Mac-only.&lt;/p&gt;
    &lt;p&gt;But the games market was overwhelmingly PC. Broderbund estimated Mac's games market share as 5% of DOS/Windows. Since I believed in the Mac more than they did, it made sense for me to take on the port, as I'd done with Amiga. I subcontracted it to Presage Software, a group of ex-Broderbund programmers I'd known since Karateka days.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Fun fact: the previous occupant of Presage's San Rafael office was George Lucas's Industrial Light &amp;amp; Magic.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Presage had an excellent, seasoned lead Mac programmer in Scott Shumway; but whereas Danny met his Amiga milestones promptly, Scott's Mac milestones receded like the horizon as they approached. With each new Mac model release — black-and-white, then color, then a different-sized screen — Presage had to redo the bit-mapped PoP graphics for the new configuration. While Prince of Persia's Apple, Amiga and PC versions languished on store shelves (the game wasn't a hit in its first two years), the Mac release date slipped from 1990 to 1991, then to 1992.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Fun fact #2: the young graphic artist who up-rezzed the Mac sprites, Mike Kennedy, went on to found the comics imprint Magnetic Press. We met again in 2024, when Magnetic published my graphic novel Monte Cristo.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Ironically, the Mac delays turned out to be a blessing in disguise. By the time the port was finally finished, almost two years late, Broderbund marketing had noticed that despite PoP's lackluster U.S. sales, its overseas and console versions were doing surprisingly well. Maybe the game had untapped potential?&lt;/p&gt;
    &lt;p&gt;Broderbund took the gamble of combining PoP's Mac release with a PC re-release in a bigger, hourglass-shaped "candy box" designed by the San Francisco firm Wong &amp;amp; Yeo. The dual Mac-PC release in the new box turned the prince's fortunes around. PoP not only became the #1-selling Mac game, it went from ice-cold to hot on PC as well. To 1992 Mac owners who'd been using their machines mainly for work, a game like PoP was a welcome diversion.&lt;/p&gt;
    &lt;p&gt;The Mac port was terrific. A sign of its quality is that we adopted its revamped prince (sporting a vest, turban and shoes) for the sequel, Prince of Persia 2: The Shadow and The Flame.&lt;/p&gt;
    &lt;p&gt;But I still think the original Apple and PC graphics play best. The CRT blur and fat pixels smoothed over animated glitches, enhancing the illusion of life. Higher resolution leaves less to the imagination. (The same can be said of photography and cinema.)&lt;/p&gt;
    &lt;head rend="h4"&gt;Other ports&lt;/head&gt;
    &lt;p&gt;Between 1990 and 1993, more computer and console ports of PoP than I can list — Nintendo NES, Game Boy, SEGA Game Gear, Genesis, Master System, Amstrad CPC, Atari ST, NEC PC-9801, FM Towns, Sam Coupé — were developed by teams in Japan, Europe, and elsewhere. Usually, by the time someone handed me a controller to playtest a build, it was too late for my feedback to matter, so I rarely played beyond the first level or two. I don't remember enough specifics of those versions to compare them; I'll leave that to players who know them better.&lt;/p&gt;
    &lt;p&gt;There is one unforgettable exception.&lt;/p&gt;
    &lt;head rend="h4"&gt;Super Nintendo&lt;/head&gt;
    &lt;p&gt;In March 1992, I moved to Paris for a year (to learn French and 16mm filmmaking). Soon after my arrival, a colleague at Activision invited me to visit their office. They showed me the Super Nintendo version of PoP, developed by Arsys and published by NCS in Japan. Activision was lobbying Broderbund for the rights to publish it in Europe and the U.S. It wasn't my call, but they hoped I'd put in a word.&lt;/p&gt;
    &lt;p&gt;I wrote in my journal that day:&lt;/p&gt;
    &lt;p&gt;"Wow! It was like a brand new game. For the first time I felt what it's really like to play Prince of Persia, when you're not the author and don't already know by rote what's lurking around every corner."&lt;/p&gt;
    &lt;p&gt;Arsys had done more than a straight port; they'd expanded the game from 12 levels to 20, adding new enemies, traps, setpieces, and new music. I didn't play all the way through — a half-hour in Activision's office only scratched the surface — but I'll never forget the delighted thrill of being surprised playing my own game. You can see and play it in your browser here.&lt;/p&gt;
    &lt;p&gt;Elaborate production values and doubled playtime helped make SNES PoP a huge hit. I especially loved the fantastic box artwork by Katsuya Terada.&lt;/p&gt;
    &lt;p&gt;A recent feature article in Time Extension revealed behind-the-scenes details about the SNES development that I hadn't known — including that game producer Keiichi Onogi traveled to the U.S. to visit Broderbund in 1991, hoping to get my feedback. (I missed his visit.) The article is a fascinating time capsule and testament to how special that port was.&lt;/p&gt;
    &lt;head rend="h4"&gt;...And onwards&lt;/head&gt;
    &lt;p&gt;The SNES, so different from the original Apple/DOS version, gave me my first taste of a feeling I would grow used to in decades to come: playing and enjoying new Prince of Persia games that were made by others. With the exception of The Sands of Time (2003), where I was part of a Ubisoft Montreal team, the more recent modern PoP games don't have my fingerprints on them.&lt;/p&gt;
    &lt;p&gt;I suspect that for many reading this post, your answer to "Which is your favorite PoP?" will be the same as mine: Whichever version we played, for hours on end, at a formative age when playing and finishing a game mattered intensely. The real value is in the ingenuity and imagination you brought to the effort, and in your own memories tied to that time.&lt;/p&gt;
    &lt;p&gt;Thanks for reading this post. If you'd like a deeper dive into the story behind Prince of Persia's creation, I've published two books on the subject: my old journals (1985-1993), and my new graphic novel Replay. You can check them out here. Archival materials about PoP (including the Apple II source code) can be found in this website's Library.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.jordanmechner.com/en/latest-news/#a-platform-jumping-prince"/><published>2025-09-26T04:29:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45382755</id><title>No reachable chess position with more than 218 moves</title><updated>2025-09-26T10:40:04.491231+00:00</updated><content>&lt;doc fingerprint="13011afe410486f7"&gt;
  &lt;main&gt;&lt;p&gt;Created by the author using GIMP and freely available images.&lt;/p&gt;&lt;head rend="h1"&gt;There is no reachable chess position with more than 218 moves.&lt;/head&gt;Stop searching, we had it right for 60 years.&lt;p&gt;Ever since Nenad Petrović, grandmaster of chess composition, published his 218 move composition in 1964, people have tried to come up with a better one. Last month, I joined the hunt and, being a computer scientist, I decided to settle this question once and for all, using computers. You can give it a try yourself. Try to find a position with more moves than the one below.&lt;/p&gt;&lt;p&gt;Spoiler: You won't.&lt;/p&gt;&lt;p&gt;&lt;lb/&gt;Reachable chess position with 218 moves for White, published by Petrović in 1964.&lt;/p&gt;&lt;p&gt;...but how can we know for sure?&lt;/p&gt;&lt;p&gt;By checking all approximately 8.7x10^45 reachable chess positions?&lt;lb/&gt;Yeah that's not gonna happen...&lt;/p&gt;&lt;p&gt;That's 8.7 billion billion billion billion billion and it's enough to scare even the mightiest of supercomputers. In fact, cracking AES-128 encryption would be easier.&lt;/p&gt;&lt;p&gt;Fortunately, we can use the power of Math!&lt;/p&gt;&lt;p&gt;Follow me along and perhaps you can compute yourself a world record afterwards :)&lt;/p&gt;&lt;head rend="h2"&gt;Using the power of Math&lt;/head&gt;&lt;p&gt;&lt;lb/&gt;Red is the official color of Math, at least in my elementary school.&lt;/p&gt;&lt;p&gt;We're gonna tackle this problem from the white side, i.e., it is White to move. Except for rare exceptions regarding the reachability of positions, this is equivalent.&lt;/p&gt;&lt;p&gt;Since proving that a position is reachable is complicated, we're gonna search through all ways of placing pieces on the board and filter out the non-reachable ones later on, if needed.&lt;/p&gt;&lt;p&gt;Obviously, 99.9% of the positions suck at having lots of moves, they are not even close to 218. We just need a way to skip them and pray that there exists enough electricity to check the rest.&lt;/p&gt;&lt;p&gt;Let's begin with a couple of useful observations.&lt;/p&gt;&lt;head rend="h4"&gt;Useless pieces&lt;/head&gt;&lt;p&gt;A black piece does not improve the number of moves, most of the time. Its existence only benefits the number of moves if it increases White's number of moves, i.e., at least one of the following is true:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;It can be taken by a white pawn, giving said pawn more moves&lt;/item&gt;&lt;item&gt;It protects the black king from check, making an otherwise illegal position with lots of moves legal&lt;/item&gt;&lt;item&gt;It frees a white piece from being pinned to the white king, thus giving it more moves&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Otherwise, it is useless, at best, and thus can be removed.&lt;/p&gt;&lt;p&gt;&lt;lb/&gt;I inserted this picture for the sole purpose of making this article look less text-heavy.&lt;/p&gt;&lt;head rend="h4"&gt;Too powerful pieces&lt;/head&gt;&lt;p&gt;Next, we observe that, if piece counts permit, we can always replace a black piece, with the exception of the black king, with a strictly less powerful one. That is, a piece that has a subset of the moves of the original piece. For example, a queen with a pawn/bishop/rook or a bishop with a pawn. Except if a pawn is on the seventh rank, since in that case, each of its moves to the promotion square counts as 4 separata moves. The only way for Black's moves to affect White's number of moves is by pinning White's pieces or preventing the white king from stepping on a certain square. Both of these things can't get worse if the black piece has less moves than before.&lt;/p&gt;&lt;head rend="h4"&gt;Too weak pieces?&lt;/head&gt;&lt;p&gt;The other way around, it is not so easy, however. You'd think that you can just replace white rooks and white bishops with white queens if counts permit, but the problem is this: How do you ensure that you cannot capture the black king, making the position illegal? Maybe the optimal solution has a rook instead of a queen to avoid just that?&lt;/p&gt;&lt;p&gt;Well, you might say "Let's just place a black piece in between then".&lt;/p&gt;&lt;p&gt;And you would be wrong unless you can tell me why you haven't just blocked some other white piece and thus reduced the number of moves in total. Or what your plan is, if there is no space to put something in between. You just made the position illegal, duh!&lt;/p&gt;&lt;head rend="h4"&gt;No checks, thank you&lt;/head&gt;&lt;p&gt;Finally, we can get rid of checks. If the black king is in check, it means that the position is illegal, since it is White's turn to move and they could just capture the black king. Not okay! So that can't be it. On the other hand, if the white king is in check, then the number of White's moves is severely restricted and we can easily prove that we cannot reach 218 moves. There are three ways to get out of check:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Move the king&lt;/item&gt;&lt;item&gt;Capture the attacker&lt;/item&gt;&lt;item&gt;Block the attack&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Moving the king gives 8 moves at best. Since any square can be reached by at most 16 pieces at the same time (8 knights and 8 other pieces straight or diagonally), capturing gives 16 moves at best. We can have at most 6 squares to block an attack, so that's an additional 6 x 16 = 96 moves. So at best 8+16+96 = 120 moves, far less than 218, and thus we do not need to consider positions in which either king is in check.&lt;/p&gt;&lt;p&gt;So is this it Tobi? Can we now call NASA and ask for their supercomputer?&lt;/p&gt;&lt;p&gt;Nope, it's still absolutely hopeless, there are waaaaay too many positions left.&lt;lb/&gt;We have to skip even more positions.&lt;/p&gt;&lt;head rend="h2"&gt;Introducing Chess with Cheating: Partial Pieces and Moves&lt;/head&gt;&lt;p&gt;&lt;lb/&gt;Reminder: Replace rook pictures by something more interesting.&lt;/p&gt;&lt;p&gt;While searching through all possible piece configurations, we would ideally like to have some provably correct way of telling whether we can still reach 218 moves, so we can stop trying and save ourselves an astronomically large amount of work. The better the method is, the more work we save. But it also needs to be fast, since we need to run it millions and billions of times.&lt;/p&gt;&lt;p&gt;A common technique in optimization is to allow fractional decisions. Instead of a piece being either on e4 or not, it can be 27.3% there and 72.7% not there. This enables us to just "swim" through the solution space towards the optimal solution instead of trying all combinations. The drawback is that we usually end up with a way too good solution with most decisions being fractional. But if that doesn't get us beyond 218 moves, we know we can stop trying.&lt;/p&gt;&lt;p&gt;Obviously, these kinds of algorithms are already implemented in state-of-the-art solvers like Gurobi, so all we need to do is model our problem to its likings (as a so-called integer programming problem), tell it to maximize the number of moves and pray. Finally, after ~55 000 seconds, it crashed.&lt;lb/&gt;Not enough memory, but also hopelessly far away from completing the proof. Extrapolating the runtime led to an estimated runtime of ~6 years. Yeah, let's not go there. Going back to making more observations:&lt;/p&gt;&lt;head rend="h4"&gt;Checking more positions to make things less slow&lt;/head&gt;&lt;p&gt;To reduce the size of the model, I bent some chess rules, simplifying the search space:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;I allowed castling if king and rook were on the right squares, not requiring anything else&lt;/item&gt;&lt;item&gt;I stopped caring about pieces moving despite being pinned&lt;/item&gt;&lt;item&gt;I stopped caring whether the white king is in check or walks into check&lt;/item&gt;&lt;item&gt;I allowed white pawns to always capture when standing on the fifth rank so I'd not have to check en passant.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;All of these things are unlikely to happen in 218+ move positions and after a solution has been found, I can still check and discard it if it has less moves than it claims to have.&lt;/p&gt;&lt;p&gt;I started again and this time, memory wasn't a problem, but the progress was awfully slow with ~29 days remaining. The world could not wait this long and neither could I. I pressed cancel.&lt;/p&gt;&lt;head rend="h4"&gt;Preventing white magic&lt;/head&gt;&lt;p&gt;&lt;lb/&gt;The optimal fractional solution for the empty board. Most pieces are spread out over multiple squares and have fractional numbers of moves available that sum to ~305. This does prove that the real solution can't have more than 305 moves. Still quite a bit from proving 218 to be the optimum.&lt;/p&gt;&lt;p&gt;Our current way of cheating is too different from reality, causing the solver to have to search through way too many board configurations. In our case, the optimal solution to the easy problem flooded the center with half queens and half knights sharing the same squares, having half pieces move through other half pieces with half moves. Gurobi thinks that the above position has 305 moves in total, which is far away from 218 and a bad upper bound.&lt;/p&gt;&lt;p&gt;In order to cut off this crazy solution, I added a "redundant" constraint, saying that at most one piece in total can move from one direction onto a particular square. Which got us a new hallucination...&lt;/p&gt;&lt;p&gt;&lt;lb/&gt;The second try. This one has 271 2/3 moves which proves that there is no solution with more than 271 moves.&lt;/p&gt;&lt;p&gt;Now we have 271 2/3 moves, which is much better. It's a bit like having to try all passwords with 53 characters vs. all passwords with 87 characters, the latter being many magnitudes harder.&lt;/p&gt;&lt;p&gt;Wait a minute, White can't have 4 kings.&lt;/p&gt;&lt;p&gt;White does not have four kings. White has 3 times 24.6% kings and one 26.2% king. Also, the queen on g3 barely exists, it's 0.8% but apparently it somehow contributes to the total sum of moves :)&lt;/p&gt;&lt;p&gt;With this improved model, I tried again and after ~23 000 seconds, Gurobi solved it to optimality!&lt;/p&gt;&lt;head rend="h2"&gt;Results&lt;/head&gt;&lt;p&gt;Sadly, instead of finding me a fancy position with 219 moves and making my name immortal, Gurobi gave me the following 12 representative positions (of 40,000 in total) with 218 moves each:&lt;/p&gt;&lt;p&gt;All 12 positions seem trivially reachable. I only constructed a proof game for one of the positions, since that is sufficient for proving our claim. If you don't believe that this is possible, keep in mind that White's last or second last move might have been a capture. Or click on the link :)&lt;/p&gt;&lt;p&gt;Sadly not a world record, but at least we now know for certain that 218 is the limit.&lt;/p&gt;&lt;p&gt;And you smart chess move 3.7 bit compression people and chess engine developers, you can finally stop worrying. 256 moves will be enough. You're welcome :-)&lt;/p&gt;&lt;p&gt;Except if you allow non-reachable positions, in which case you might want to read on :P&lt;/p&gt;&lt;head rend="h2"&gt;Other stuff solved along the way&lt;/head&gt;&lt;p&gt;I also confirmed the optimality of the 144 move record without promotions. Since Gurobi did not find any position with more than 144 moves, that means that there also is no reachable position with more than 144 moves. Hence, 144 moves is the best we can do and "Jenő Bán", a chess composer from Hungary, found one in 1960 already:&lt;/p&gt;&lt;p&gt;&lt;lb/&gt;144 moves for White, no promotions. Created by Hungarian chess composer "Jenő Bán". Here is a proof game, demonstrating that the position is reachable.&lt;/p&gt;&lt;p&gt;Also, I confirmed the optimality of the following illegal position, which has 288 moves for White.&lt;/p&gt;&lt;p&gt;&lt;lb/&gt;Illegal position with 288 moves for White. Corner queens can be replaced with bishops.&lt;/p&gt;&lt;p&gt;Now have a guess at what the best non-reachable legal position looks like ;)&lt;lb/&gt;Yep, you're right, cramming the kings into the corners for 271 moves.&lt;/p&gt;&lt;p&gt;&lt;lb/&gt;Legal but non-reachable position with 271 moves for White. Corner queens can be replaced with bishops.&lt;/p&gt;&lt;head rend="h2"&gt;Future plans&lt;/head&gt;&lt;p&gt;My code snippet is freely available at Github. If you manage to do something cool based on it, please let the world know :)&lt;/p&gt;&lt;p&gt;Fun problems enthusiasts could try to tackle next using similar techniques:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Most captures&lt;/item&gt;&lt;item&gt;Most stalemates&lt;/item&gt;&lt;item&gt;Most checks&lt;/item&gt;&lt;item&gt;Most checkmates&lt;/item&gt;&lt;item&gt;Most mates in two&lt;/item&gt;&lt;item&gt;...&lt;/item&gt;&lt;item&gt;Most ... under &amp;lt;condition&amp;gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Some of these might be hard, extremely hard or practically impossible to solve with current technology. I don't think that integer linear programming is a suitable approach for all of them, one likely has to develop a custom algorithm for computing good upper bounds, based on creative mathematical insights.&lt;/p&gt;&lt;p&gt;Good luck to those who dare to try solving one of these ^^&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lichess.org/@/Tobs40/blog/there-is-no-reachable-chess-position-with-more-than-218-moves/a5xdxeqs"/><published>2025-09-26T04:47:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45383612</id><title>Kapa.ai (YC S23) Is Hiring a Customer Solutions Engineer (EU Remote)</title><updated>2025-09-26T10:40:03.899804+00:00</updated><content>&lt;doc fingerprint="3d459b337e3b4fd0"&gt;
  &lt;main&gt;
    &lt;p&gt;The fastest way to build AI assistants on technical content&lt;/p&gt;
    &lt;p&gt;Kapa makes technical knowledge instantly accessible through AI assistants. As a customer solutions engineer you will work work closely with our 200+ customers to help them deploy and manage both customer-facing and employee-facing AI assistants. Check out Docker’s documentation (https://docs.docker.com) for a live example of what Kapa is (look for the “Ask AI” button).&lt;/p&gt;
    &lt;p&gt;In this role, you will:&lt;/p&gt;
    &lt;p&gt;You may be a good fit if you have*:&lt;/p&gt;
    &lt;p&gt;* This is neither an exhaustive nor necessary set of attributes. Even if none of these apply to you, but you believe you will contribute to kapa.ai, please reach out.&lt;/p&gt;
    &lt;p&gt;We make it easy for technical companies to build AI assistants. Companies like Docker, Grafana and Mixpanel deploy kapa in the following ways:&lt;/p&gt;
    &lt;p&gt;We leverage companies existing technical knowledge sources including documentation, tutorials, forum posts, Slack channels, GitHub issues and many more to generate AI assistants that can handle complicated technical questions. More than 200 companies use kapa and we have answered more than 10 million questions to date.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/kapa-ai/jobs/mHIFJVz-support-engineer"/><published>2025-09-26T07:01:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45383637</id><title>Translating a Fortran F-16 Simulator to Unity3D</title><updated>2025-09-26T10:40:03.331081+00:00</updated><content>&lt;doc fingerprint="3633da10e06c133a"&gt;
  &lt;main&gt;&lt;p&gt;I recently purchased the textbook “Aircraft Control and Simulation” by Brian L. Stevens, Frank L. Lewis, and Eric N. Johnson1. This book covers the control and simulation of aircraft. It’s really dense and frankly hard to understand. As far as aerodynamics texts go, it’s pretty typical.&lt;/p&gt;&lt;p&gt;One interesting item in the appendices of the book is the source code for the simulation of an F-16. It has a flight model, based on scale model wind tunnel data. The flight model consists of a dozen lookup tables and the math equations to make it fly.&lt;/p&gt;&lt;p&gt;The only problem: it’s written entirely in Fortran.&lt;/p&gt;&lt;p&gt;The source code is available on Github.&lt;/p&gt;&lt;p&gt;You can play the finished project right now on itch.io&lt;/p&gt;&lt;p&gt;Or watch the demo on Youtube:&lt;/p&gt;&lt;p&gt;While I am a professional software engineer and I have worked in the aerospace industry, that doesn’t mean that I understand what I’m doing.&lt;/p&gt;&lt;p&gt;Table of Contents&lt;/p&gt;&lt;head rend="h1"&gt;Introduction&lt;/head&gt;&lt;p&gt;In previous posts on this blog2 3 4, I covered the development of a flight simulator based on the lift equation and hand-tuned parameters. This gives the game designer direct control over a lot of flight parameters. For example, you can directly choose the turn rate and the G-limit of the aircraft, allowing the designer to easily tune the corner speed. This works well for game development, since the designer, and ultimately the player, care more about these high-level parameters.&lt;/p&gt;&lt;p&gt;But real aircraft are designed from the other direction, starting from low-level parameters such as the size, shape, and position of airfoils. Engineers tune every aspect of the aircraft in order to reach those high-level behaviors. But every design decision has trade offs and reaching the goal for one parameter means compromising another. An airliner is designed very differently from a fighter jet because of this.&lt;/p&gt;&lt;p&gt;Simulating all of the low-level parameters is difficult. It’s possible to simulate air flow over the vehicle using computational fluid dynamics (CFD), but this kind of software is difficult to write and even more difficult to verify.&lt;/p&gt;&lt;p&gt;The F-16 flight model from the textbook does not simulate the low-level parameters, but it also doesn’t simulate the high-level parameters either. It sits somewhere in between, so it serves as a useful stepping stone from my previous projects. This project will explore more advanced flight dynamics and explain the limitations of the old flight model as well as the new one.&lt;/p&gt;&lt;head rend="h1"&gt;Aerospace Conventions&lt;/head&gt;&lt;head rend="h2"&gt;Coordinate System&lt;/head&gt;&lt;p&gt;Before we can write any code, we need to understand the conventions used for mathematically modeling aircraft that are used in the aerospace industry. The textbook uses aerospace conventions and to use them in this project, we must convert them to Unity conventions.&lt;/p&gt;&lt;p&gt;The first convention is the coordinate system axes. If you’ve ever visited a graphics programming forum, you might have seen people arguing over how the X, Y, and Z axes should be arranged in their game. Especially whether to use a right handed or left handed, and Y-up or Z-up coordinate system.&lt;/p&gt;&lt;p&gt;This chart by Freya Holmer5 shows the axis choices made by a variety of 3D software tools.&lt;/p&gt;&lt;p&gt;The aerospace industry takes a different path. The most common coordinate system for aircraft is right handed, X forward, Y right, and Z down. This is completely different from every tool shown above. The textbook defines all of it’s math using this convention.&lt;/p&gt;&lt;p&gt;Luckily, translating between two coordinate systems is easy. You just swap the components around and then add or remove minus signs until it all works. Every calculation made by the textbook’s code can be easily translated into Unity’s coordinate system and vice versa.&lt;/p&gt;&lt;p&gt;Writing functions to do this is simple:&lt;/p&gt;&lt;quote&gt;public static Vector3 ConvertVectorToAerospace(Vector3 vector) { return new Vector3(vector.z, vector.x, -vector.y); } public static Vector3 ConvertVectorToUnity(Vector3 vector) { return new Vector3(vector.y, -vector.z, vector.x); }&lt;/quote&gt;&lt;p&gt;When translating euler angles, torque, or other angular values, one additional negation is needed:&lt;/p&gt;&lt;quote&gt;public static Vector3 ConvertAngleToAerospace(Vector3 angle) { // negate when switching handedness return -ConvertVectorToAerospace(angle); } public static Vector3 ConvertAngleToUnity(Vector3 angle) { // negate when switching handedness return -ConvertVectorToUnity(angle); }&lt;/quote&gt;&lt;head rend="h2"&gt;Units&lt;/head&gt;&lt;p&gt;For completely inscrutable reasons, American aerospace texts (and the industry!) insist on using US customary units for everything. All math is defined with these units. Distance is measured in feet. Mass is measured in slugs.&lt;/p&gt;&lt;p&gt;What the hell is a slug? A slug is the unit of mass in the US system. This is the equivalent unit of the kilogram in the metric system. Remember that weight and mass are not the same thing.&lt;/p&gt;\(1 \, \text{kg} * 9.81 \, \text{m/s}^2 = 9.81 \, \text{N}\) \(1 \, \text{slug} * 32.17 \, \text{ft/s}^2 = 32.17 \, \text{lb}\)&lt;p&gt;Mass is the measure of how much an object resists linear force. Moment of inertia is how much the object resists rotational force or torque. The unit for moment of inertia in metric is kg-m2. Thus, the equivalent unit in customary is slug-ft2.&lt;/p&gt;&lt;p&gt;You want to measure how much air mass is in a given volume? That’s gonna be slugs/ft3.&lt;/p&gt;&lt;p&gt;Speed is mostly measured in feet per second, unless you want to know the speed of the aircraft. Then you use knots, which means nautical miles per hour. Importantly, a nautical mile is not the same as a regular mile. A regular mile is 5,280 feet. A nautical mile is ~6,076 feet or exactly 1,852 meters (???).&lt;/p&gt;&lt;p&gt;Do you want to know how fast your ship is sailing? Just throw out this piece of wood tied to a spool of rope. The rope has knots tied at regular intervals. Count the number of knots that unspool in a given time frame. That’s how many knots your ship is making.&lt;/p&gt;&lt;p&gt;Finally, temperature is measured in degrees Rankine. You know how the Kelvin scale is just the Celsius scale adjusted so that 0 Kelvin equals absolute zero? Well Rankine is the same concept applied to Fahrenheit.&lt;/p&gt;\(0 \, \text{R} = \text{absolute zero} \\ 534 \, \text{R} = 75 \, \text{F} = \text{room temperature}\)&lt;p&gt;How the fuck did we ever build the SR-71? 💀&lt;/p&gt;&lt;p&gt;Unity by convention uses metric for all physics units. The flight model in the textbook uses US customary units, so every input and output of this system has to be converted. So we have to add functions to handle converting to and from US customary units. This is easy enough since conversion is just a multiplication or division operation.&lt;/p&gt;&lt;head rend="h2"&gt;Terminology&lt;/head&gt;&lt;p&gt;There are several terms used in aerospace that I need to define. I have used equivalent terms in the previous project, but I will clarify them here.&lt;/p&gt;&lt;p&gt;Alpha (α) refers to the angle of attack.&lt;/p&gt;&lt;p&gt;Beta (β) refers to the angle of side slip.&lt;/p&gt;&lt;p&gt;Longitudinal axis is the X-axis, from tail to nose.&lt;/p&gt;&lt;p&gt;Normal axis is the Z- axis, the vertical axis pointing downwards.&lt;/p&gt;&lt;p&gt;Lateral axis is the Y-axis, or side axis, pointing right.&lt;/p&gt;&lt;p&gt;Phi (φ) is the aircraft’s roll around the X axis.&lt;/p&gt;&lt;p&gt;Theta (θ) is the aircraft’s pitch around the Y axis.&lt;/p&gt;&lt;p&gt;Psi (ψ) is the aircraft’s yaw around the Z axis.&lt;/p&gt;&lt;p&gt;P, Q, and R refer to the angular velocity around the X, Y, and Z axes respectively.&lt;/p&gt;&lt;p&gt;In general you will find that aerodynamics texts are allergic to good variable names. I suspect this is a form of gatekeeping. Or perhaps the authors have to pay by the letter to publish.&lt;/p&gt;&lt;head rend="h1"&gt;Air Data&lt;/head&gt;&lt;p&gt;Airplanes need air to fly [citation needed]. Every behavior of a plane is determined by the movement of air. Therefore it is critically important, for real and simulated planes, to be able to measure the air flowing around it.&lt;/p&gt;&lt;p&gt;Real planes need to measure static and dynamic air pressure to determine how fast the plane is moving. Static pressure is measured by a static pressure port. It’s the pressure that you would measure if you just lifted a pressure meter to the same altitude as the plane. Static pressure decreases with altitude.&lt;/p&gt;&lt;p&gt;Dynamic pressure measures the pressure added by the plane’s forward motion. This requires a pitot tube to measure. As the plane moves forward it rams air into the pitot tube and increases the pressure above the static pressure. The pitot measures the total pressure of the air. By subtracting the static pressure, we can obtain the dynamic pressure.&lt;/p&gt;&lt;p&gt;The static and dynamic pressures can then be used to calculate many of the variables the pilot needs to fly. Most important are the airspeed and altitude of the aircraft. Specifically, these values can be used to calculate the indicated airspeed of the aircraft. Indicated airspeed is calculated directly from the dynamic pressure.&lt;/p&gt;&lt;p&gt;At most subsonic speeds, the dynamic pressure of air flowing over the wings is the most important variable in flight. A plane’s performance can be defined in terms of indicated airspeed. For example, a plane may have a stall speed of 100 knots indicated airspeed. This means that no matter what altitude the plane is at, the indicated airspeed will be 100 when the plane stalls.&lt;/p&gt;&lt;p&gt;This is important since it gives a consistent number for the stall speed regardless of atmospheric conditions. The pressure and density of the air can vary based on weather, temperature, and other factors. So the true airspeed when a stall occurs can be very inconsistent. But as long as the pilot knows the indicated airspeed, they know how their plane will behave.&lt;/p&gt;&lt;p&gt;For this simulator, the calculation has to work backwards. We know the true airspeed and altitude of the plane from the velocity and position of the rigidbody. From that, we can calculate the dynamic pressure. This dynamic pressure is then used for later calculations in the flight model. Additionally, the plane’s speed in mach is calculated here as well.&lt;/p&gt;&lt;p&gt;The original Fortran source code is given:&lt;/p&gt;&lt;quote&gt;SUBROUTINE ADC(VT,ALT,AMACH,QBAR) DATA R0/2.377E-3/ TFAC = 1.0 - 0.703E-5 * ALT T = 519.0 * TFAC IF (ALT .GE. 35000.0) T= 390.0 RHO = R0 * (TFAC**4.14) AMACH= VT/SQRT(1.4*1716.3*T) QBAR = 0.5*RHO*VT*VT C PS = 1715.0 * RHO * T RETURN END&lt;/quote&gt;&lt;p&gt;It turns out, Fortran is actually pretty good at translating formulas. So this code is not as difficult to read as I expected.&lt;/p&gt;&lt;p&gt;To translate this to Unity, we create a class AirDataComputer to perform these calculations. The output of the calculation is the AirData struct.&lt;/p&gt;&lt;quote&gt;public struct AirData { public float altitudeMach; public float qBar; } public class AirDataComputer { /// &amp;lt;summary&amp;gt; /// Density in slugs/ft^3 /// &amp;lt;/summary&amp;gt; public const float SeaLevelDensity = 2.377e-3f; public const float MaxAltitude = 35000.0f; /// &amp;lt;summary&amp;gt; /// Calculates air data based on velocity and altitude /// &amp;lt;/summary&amp;gt; /// &amp;lt;param name="velocity"&amp;gt;Velocity in ft/s&amp;lt;/param&amp;gt; /// &amp;lt;param name="altitude"&amp;gt;Altitude in ft&amp;lt;/param&amp;gt; /// &amp;lt;returns&amp;gt;Air data&amp;lt;/returns&amp;gt; public AirData CalculateAirData(float velocity, float altitude) { ... } }&lt;/quote&gt;&lt;p&gt;Here we can see where the US customary units are used. The density of air at sea level is defined in slugs/ft3. The altitude is defined in feet. Theoretically, these values could be defined using metric. But the implementation of the function depends on even more values defined in customary.&lt;/p&gt;&lt;quote&gt;const float baseTemperature = 519.0f; // sea level temp in R const float minTemperature = 390.0f; // minimum temp in R const float temperatureGradient = 0.703e-5f; // gradient in R / ft altitude = Mathf.Clamp(altitude, 0, MaxAltitude); // calculate temperature in Rankine float temperatureFactor = 1.0f - (temperatureGradient * altitude); float T = Mathf.Max(minTemperature, baseTemperature * temperatureFactor);&lt;/quote&gt;&lt;p&gt;These calculations simulate the change in atmospheric conditions at different altitudes. Particularly important is how the temperature drops at higher altitudes. The temperature gradient approximates the decreases in temperature (in Rankine) as altitude increases.&lt;/p&gt;&lt;p&gt;This flight model supports altitudes up to 35,000 ft. Altitudes above this are not supported. At any altitude above this, the plane will behave as if it were at 35,000 ft. This is because the temperatures at this altitude no longer consistently decrease, as it does in the lower atmosphere. A more advanced atmosphere model would need to be used.&lt;/p&gt;&lt;p&gt;Temperature factor does not drop below about 0.75 in this range, so the resulting temperature T does not fall below 390 R.&lt;/p&gt;&lt;quote&gt;const float gamma = 1.4f; // ratio of specific heats const float gasConstant = 1716.3f; float speedOfSound = Mathf.Sqrt(gamma * gasConstant * T); float altitudeMach = velocity / speedOfSound;&lt;/quote&gt;&lt;p&gt;Now we can calculate the speed of sound at the plane’s current altitude and use it to find the plane’s Mach number. The speed of sound varies with density, which varies with temperature. The speed of sound is equal to the square root of the ratio of specific heat, called gamma, times the gas constant, gasConstant, times the absolute temperature, T.7&lt;/p&gt;&lt;p&gt;Once the speed of sound is known, calculating the Mach number is just a simple division.&lt;/p&gt;&lt;quote&gt;const float densityPower = 4.14f; float rho = SeaLevelDensity * Mathf.Pow(temperatureFactor, densityPower); float qBar = 0.5f * rho * velocity * velocity;&lt;/quote&gt;&lt;p&gt;And finally the dynamic pressure is calculated from the temperature factor. I’ll admit, I don’t understand why exactly the formula is designed this way. It seems to calculate a density factor, called rho, based solely on the temperature factor, raised to an arbitrary value, densityPower.&lt;/p&gt;&lt;p&gt;The NASA reference provides a similar formula using metric units and using a different arbitrary power. I guess this value is just what results from using customary🤷♂️&lt;/p&gt;&lt;p&gt;In any case, this gives us the two air data values we need for the rest of the simulation, dynamic pressure and mach number.&lt;/p&gt;&lt;head rend="h1"&gt;Table Interpolation&lt;/head&gt;&lt;p&gt;Throughout this flight model, various forms of table lookups are used to determine the aircraft’s behavior. Lookup tables are commonly used in flight simulators to represent complex curves and functions. In fact, Unity’s AnimationCurve class in the previous project is used to define a few lookup tables, such as lift coefficient.&lt;/p&gt;&lt;p&gt;This animation curve serves as a 1 dimensional lookup table. The input dimension is AOA and the output value is lift coefficient.&lt;/p&gt;&lt;p&gt;Fortran code doesn’t have the luxury of using AnimationCurves, but a simple table of values with an interpolation function is almost as powerful.&lt;/p&gt;&lt;head rend="h2"&gt;1D Lookup Table&lt;/head&gt;&lt;p&gt;The interpolation functions provided by the textbook look something like this:&lt;/p&gt;&lt;quote&gt;FUNCTION LOOKUP(ALPHA, RESULT) REAL A(-2:9) C DATA A / .770,.241,-.100,-.416,-.731,-1.053, &amp;amp; -1.366,-1.646,-1.917,-2.120,-2.248,-2.229 / C S = 0.2*ALPHA K = INT(S) IF(K.LE.-2) K=-1 IF(K.GE.9) K=8 DA = S - FLOAT(K) L = K + INT(SIGN(1.1,DA)) RESULT = A(K) + ABS(DA)*(A(L)-A(K)) RETURN END&lt;/quote&gt;&lt;p&gt;This function takes alpha (AOA) and uses it to lookup a value from the table. Alpha is a float that can have any value from [-10, 45]. The table “A” represents values for every 5 degree increment of alpha. Note that Fortran supports arrays with an arbitrary starting index, in this case -2. So this table supports indices in the range [-2, 9].&lt;/p&gt;&lt;p&gt;This first step is multiplying alpha by a scaling value to create a float S, which maps alpha to the range [-2, 9]. An integer index K is created from S and then clamped to values one less than the table’s index range. The value DA is calculated as the difference between S and K.&lt;/p&gt;&lt;p&gt;The value L is calculated to be one index away from K, in the same direction as S. So now we have two indices to the table, K and L, which we use to read two values from the table, A(K) and A(L). DA is then used to blend between these table values and produce the final result.&lt;/p&gt;&lt;p&gt;This has two effects. The first is the simplest to understand. If alpha falls within the input range of the table, L and K are selected as the closest table values. For example, if alpha is 12, the two indices would be 2 and 3. The difference between S and K would be less than 1. The values A(2) and A(3) can be read from the table and then interpolated based on the value of DA. This is a fairly normal interpolation calculation.&lt;/p&gt;&lt;p&gt;The other effect is what happens when alpha is outside of the input range of the table. K is guaranteed to not be the first or last index, and L is allowed to be one index off of K. L and K are still valid indices, but the value of DA may be larger than 1. This means when we interpolate between A(L) and A(K), we can extrapolate values for inputs beyond the range of the table.&lt;/p&gt;&lt;p&gt;This means our lookup table can handle values outside of it’s input range. But there is still a limitation. As the input value gets further away from the input range, the extrapolated values will become more and more unrealistic. This allows our plane to fly slightly outside the flight envelope of the lookup tables.&lt;/p&gt;&lt;p&gt;I translated this function into C# like this:&lt;/p&gt;&lt;quote&gt;public static float ReadTable(float[] table, int i, int start) { return table[i - start]; } public static (int k0, int k1, float t) GetLookUpIndex(float value, float scale, int min, int max) { float scaled = value * scale; int K0 = Mathf.Clamp((int)scaled, min, max); float T = scaled - K0; int K1 = K0 + (int)Mathf.Sign(T); return (K0, K1, T); } public static float LinearLookup(float value, float scale, float[] table, int min, int max) { (int k0, int k1, float kT) = GetLookUpIndex(value, scale, min + 1, max - 1); float T = ReadTable(table, k0, min); float U = ReadTable(table, k1, min); float result = T + Math.Abs(kT) * (U - T); return result; }&lt;/quote&gt;&lt;p&gt;GetLookUpIndex calculates K, L, and DA. These variables are renamed to k0, k1, and kT respectively.&lt;/p&gt;&lt;p&gt;ReadTable is a function that maps array indices to a new range, to support arbitrary starting indices like Fortran. (C# surprisingly supports this feature natively, but who actually uses that?)&lt;/p&gt;&lt;p&gt;LinearLookup reads the k0 and k1 values from the array and performs the interpolation. This allows us to calculate values for any input to the lookup table.&lt;/p&gt;&lt;p&gt;Note that the expression “T + Math.Abs(kT) * (U – T)” is effectively equivalent to Mathf.LerpUnclamped.&lt;/p&gt;&lt;head rend="h2"&gt;2D Lookup Table&lt;/head&gt;&lt;p&gt;All of the above code is needed to perform a one dimensional table lookup. Performing this kind of table lookup with two input dimensions is called a bilinear interpolation. Extending this to two dimensions is not that much more complicated.&lt;/p&gt;&lt;p&gt;The two input values to the table form a two dimensional space. Our input values form a two dimensional point. Instead of selecting two array indices K and L, we need to select four array indices. These four indices form a box around our input point. We simply perform 2 one dimensional lookups, and then interpolate between them to produce the final value.&lt;/p&gt;&lt;p&gt;The 2 one dimensional lookups are marked in red. The final interpolation is marked in blue.&lt;/p&gt;&lt;p&gt;Implementing this in C# is a simple extension of the LinearLookup function:&lt;/p&gt;&lt;quote&gt;public static float BilinearLookup(float xValue, float xScale, float yValue, float yScale, float[,] table, int xMin, int xMax, int yMin, int yMax) { (int x0, int x1, float xT) = GetLookUpIndex(xValue, xScale, xMin + 1, xMax - 1); (int y0, int y1, float yT) = GetLookUpIndex(yValue, yScale, yMin + 1, yMax - 1); float T = ReadTable(table, x0, y0, xMin, yMin); float U = ReadTable(table, x0, y1, xMin, yMin); float V = T + Math.Abs(xT) * (ReadTable(table, x1, y0, xMin, yMin) - T); float W = U + Math.Abs(xT) * (ReadTable(table, x1, y1, xMin, yMin) - U); float result = V + (W - V) * Math.Abs(yT); return result; }&lt;/quote&gt;&lt;p&gt;A bilinear interpolation is a very common operation in computer graphics. This is how textures are sampled when placed on 3D geometry.&lt;/p&gt;&lt;p&gt;In the next section, we will see that the engine thrust calculation interpolates between the output of 2 two dimensional tables. Adding this third interpolation means this calculation is now a trilinear interpolation. Interpolating between two tables is how mipmaps are blended together in computer graphics. How neat is that?&lt;/p&gt;&lt;head rend="h1"&gt;Engine&lt;/head&gt;&lt;p&gt;The next system we’re going to add is the engine. In my previous project, the engine was dead simple. The player selected a throttle value from [0, 1], which is multiplied by the plane’s total thrust. This works fine for that simulation and even gives us the ability to reduce thrust to zero, so the plane becomes a glider.&lt;/p&gt;&lt;p&gt;However, it is not a realistic simulation of how a jet engine works. In reality, a jet engine still produces some thrust at idle throttle. And there are more factors that affect thrust output than just the throttle setting.&lt;/p&gt;&lt;p&gt;The thrust output of a jet engine decreases with altitude and increases with speed. As altitude increases, the air gets thinner and the jet engine becomes weaker. But as speed increases, dynamic pressure, and thus pressure in the engine, increases and the engine becomes stronger. These two effects need to be considered at the same time to find the thrust output at any given moment.&lt;/p&gt;&lt;p&gt;Additionally, we have to consider how jet engines behave in terms of RPM. Just like piston engines (like in a typical car), jet engines have rotating components whose speed increases with throttle. The max RPM of a jet is much higher than a piston engine, however the range of possible RPM is smaller.&lt;/p&gt;&lt;p&gt;The engine in an F-16 has a maximum RPM of about 14,000. This is at the maximum non-afterburner power, called military power. When throttle is reduced to the lowest setting, idle, the RPM falls to about 8,400 RPM or about 60% of the max. Planes of course do not have a transmission like a car does, so this range of RPM also covers the range of thrust needed at all stages of flight.&lt;/p&gt;&lt;p&gt;At idle throttle, the engine runs at 60% max RPM, but only produces 8% of max thrust. At military power, the engine runs at 100% RPM and produces 100% thrust.&lt;/p&gt;&lt;p&gt;Military power is selected when the pilot moves the throttle lever to 77% of it’s max setting. Pushing the throttle beyond that engages the afterburner and produces even more thrust. Setting the throttle lever to 100% is called max power. Max power provides about 57% more thrust than military power. Engine RPM does not increase when using afterburner.&lt;/p&gt;&lt;p&gt;A significant difference between a piston engine and a jet engine is how fast the engine can change RPM. In a car, you can put the transmission in neutral and rev the engine up and down very quickly. But a jet engine is much slower to respond to changes in throttle, regardless of how fast the pilot moves the throttle lever. Generally, it can take several seconds to go from idle to military power or vice versa.&lt;/p&gt;&lt;p&gt;The reasons why jet engines are slower to change RPM are complicated. The change in throttle is managed by a computer to avoid compressor stall, which can cause damage or shut down of the engine. This computer will change engine parameters slowly to avoid compressor stall or any other problems that might be caused by moving the throttle too quickly.&lt;/p&gt;&lt;head rend="h2"&gt;Power&lt;/head&gt;&lt;p&gt;The behavior of the jet engine is included in the textbook’s flight model. RPM is not explicitly modeled, but is abstracted as power. The pilot chooses a commanded power level and the engine’s current power setting will move towards this over time. This behavior is spring-like, thus a larger difference will cause the current power setting to change faster. It takes about 2 seconds to increase from idle to military power in this flight model.&lt;/p&gt;&lt;p&gt;The first step is to translate the player’s throttle setting into engine power. This is a fairly simple function that maps military power, or 77% throttle, to 50% power. Full afterburner, or 100%, is mapped to 100% power. This is called the “throttle gearing”, but don’t confuse that with a car’s gearing. It’s much simpler.&lt;/p&gt;&lt;quote&gt;FUNCTION TGEAR(THTL) ! Power command v. thtl. relationship IF(THTL.LE.0.77) THEN TGEAR = 64.94*THTL ELSE TGEAR = 217.38*THTL-117.38 END IF RETURN END&lt;/quote&gt;&lt;p&gt;In C#, this is translated as:&lt;/p&gt;&lt;quote&gt;public static float CalculateThrottleGear(float throttle) { // maps throttle 0 - 0.77 to power 0% - 50% // maps throttle 0.77 - 1.0 to power 50% - 100% float power; if (throttle &amp;lt;= militaryPowerThrottle) { power = 64.94f * throttle; } else { power = 217.38f * throttle - 117.38f; } return power; }&lt;/quote&gt;&lt;p&gt;Those constants might seem weird, but they just define two lines with different slopes. The two lines intersect when the throttle is 0.77.&lt;/p&gt;&lt;p&gt;The player’s throttle setting is used to calculate the commanded power level. The rate of change of engine power also depends on the current power level. This rate is calculated in the functions PDOT and RTAU:&lt;/p&gt;&lt;quote&gt;FUNCTION PDOT(P3,P1) ! PDOT= rate of change of power IF (P1.GE.50.0) THEN ! P3= actual power, P1= power command IF (P3.GE.50.0) THEN T=5.0 P2=P1 ELSE P2=60.0 T=RTAU(P2-P3) END IF ELSE IF (P3.GE.50.0) THEN T=5.0 P2=40.0 ELSE P2=P1 T=RTAU(P2-P3) END IF END IF PDOT=T*(P2-P3) RETURN END FUNCTION RTAU(DP) ! used by function PDOT IF (DP.LE.25.0) THEN RTAU=1.0 ! reciprocal time constant ELSE IF (DP.GE.50.0)THEN RTAU=0.1 ELSE RTAU=1.9-.036*DP END IF RETURN END&lt;/quote&gt;&lt;p&gt;PDOT means power rate of change. In C#, this is translated as:&lt;/p&gt;&lt;quote&gt;float CalculatePowerRateOfChange(float actualPower, float commandPower) { // calculates how fast power output should change based on commanded power float T; float p2; if (commandPower &amp;gt;= 50.0) { if (actualPower &amp;gt;= 50.0) { T = 5.0f; p2 = commandPower; } else { p2 = 60.0f; T = CalculateRTau(p2 - actualPower); } } else { if (actualPower &amp;gt;= 50.0) { T = 5.0f; p2 = 40.0f; } else { p2 = commandPower; T = CalculateRTau(p2 - actualPower); } } float pdot = T * (p2 - actualPower); return pdot; } float CalculateRTau(float deltaPower) { float rTau; if (dp &amp;lt;= 25.0) { rTau = 1.0f; } else if (dp &amp;gt;= 50.0) { rTau = 0.1f; } else { rTau = 1.9f - 0.036f * dp; } return rTau; }&lt;/quote&gt;&lt;p&gt;Power rate of change is the velocity of the power level. The most important line is this:&lt;/p&gt;&lt;quote&gt;float pdot = T * (p2 - actualPower);&lt;/quote&gt;&lt;p&gt;The velocity depends on the quantity (p2 – actualPower). Let’s call this value deltaPower. A larger deltaPower means a larger velocity. This is scaled by the factor T. The complexity comes from selecting the values for p2 and T. p2 is sometimes the commandPower value. T is sometimes the result of calling CalculateRTau.&lt;/p&gt;&lt;p&gt;These values are selected by the if statements above. These check for two conditions, the commandedPower being above 50%, and the actualPower being above 50%. This is checking whether the afterburner is being requested, and whether the afterburner is currently active. Remember that afterburner starts at 77% throttle, but 50% power.&lt;/p&gt;&lt;p&gt;If the afterburner is not active, then the T is given the value of CalculateRTau. If it is active, then T is given the constant value of 5.0. This matches with our expectation of how the engine’s RPM changes. When not in afterburner, the engine RPM should change slowly, thus power changes slowly. When in afterburner, fuel flow into the afterburner can change quickly, thus power changes quickly.&lt;/p&gt;&lt;p&gt;If we look at the function CalculateRTau, we can see that T can vary in the range [0.1, 1.0]. This depends on deltaPower. When the engine is not in afterburner, T can be at most 1.0. In afterburner, T is 5.0. That means power can change about 5 times faster when in afterburner. When multiplied with deltaPower, pdot can be as large as 250% per second.&lt;/p&gt;&lt;p&gt;The smallest value of T occurs when deltaPower is 50 or greater. This occurs when actualPower is 0 and commanded power is 50%, for example. This will cause the power rate of change to be quite small at only 6% per second. Note that this is simply the instantaneous rate of change. As the actual power rises, T will become larger and the rate of change will increase.&lt;/p&gt;&lt;p&gt;Now the reason why p2 is used instead of commandedPower is to handle the case where commandedPower is over 50% and actualPower is below 50%, or vice versa. The pilot is requesting afterburner, but the engine has not reached military power yet. In that case, deltaPower would become very large and the simulation would change power levels too quickly. To avoid this, an arbitrary constant is chosen that is on the opposite side of 50%, but not very far.&lt;/p&gt;&lt;p&gt;So if the actualPower is 0%, but commandedPower is 100%, p2 is set to the value of 60. This limits deltaPower to a maximum value of 60, instead of 100. And in the case where actualPower is 100% and commandedPower is 0%, deltaPower is limited to -60.&lt;/p&gt;&lt;p&gt;Another behavior of this code is that CalculateRTau does not handle cases where deltaPower is negative. In this case, the function returns 1, the highest value it can return. This means that the power can decrease 10 times faster than it can increase, in the most extreme case.&lt;/p&gt;&lt;p&gt;I don’t know if this is an intentional effect. This may match the behavior of real jet engines, or it may be an oversight by the authors. You can play with the behavior by adding a few calls to Mathf.Abs().&lt;/p&gt;&lt;p&gt;The practical effect of all this is that the plane’s power will lag behind the player’s throttle setting. The pilot needs to make sure that they provide enough time for the power level to change when moving the throttle.&lt;/p&gt;&lt;p&gt;The HUD for this project is mostly reused from the previous flight sim project. But the throttle indicator must be updated, since it can’t show the difference between commanded power and current power.&lt;/p&gt;&lt;p&gt;Previously, the red bar used to show the player’s throttle setting. This worked fine since power lag was not modeled. In this project, the red bar shows the engine’s current power level. I added a triangle marker to show the commanded power setting.&lt;/p&gt;&lt;p&gt;As you move the throttle, you’ll see that current power level changes quickly when there is a large difference from commanded power, and it slows down as it approaches. And when the engine enters afterburner, the power level changes very quickly.&lt;/p&gt;&lt;head rend="h2"&gt;Thrust&lt;/head&gt;&lt;p&gt;Engine power is a fairly abstract variable in this flight model. It doesn’t really correspond to any physical variable. Once we calculate the current power, we use it to find the thrust generated by the engine. Thrust in this flight model is defined in terms of pounds-force (lbf).&lt;/p&gt;&lt;p&gt;Thrust is defined by a group of look up tables. Each table has two dimensions as input, mach number and altitude, and the output is thrust. This gives us different thrust values in different flight conditions. Mach is input as 0.0 to 1.0 mach, in increments of 0.2 mach. Altitude is input as 0 to 50,000 ft, in increments of 10,000 ft. In other words, the table has dimensions 6×6.&lt;/p&gt;&lt;p&gt;The lookup tables in this flight model correspond to idle power, military power, and max power (full afterburner). The engine’s power value is used to perform a third interpolation between the output values of these tables. This makes the thrust calculation a trilinear interpolation.&lt;/p&gt;&lt;p&gt;At idle throttle, the thrust output has 100% influence from the idle table. When the throttle is halfway to military power, the output has 50% influence from the idle table and 50% influence from the military table. Above military power, the output will have some influence from the military power table and the max power table.&lt;/p&gt;&lt;p&gt;The code to read one table in Fortran is given:&lt;/p&gt;&lt;quote&gt;DATA A/ [IDLE TABLE OMITTED] DATA B/ [MIL TABLE OMITTED] DATA C/ [MAX TABLE OMITTED] H=0.0001*ALT I=INT(H) IF (I.GE.5) I=4 DH=H-FLOAT(I) RM=5.*RMACH M=INT(RM) IF (M.GE.5) M=4 DM=RM-FLOAT(M) CDH=1.0-DH&lt;/quote&gt;&lt;p&gt;These parameters are used to perform the table lookups:&lt;/p&gt;&lt;quote&gt;TMIL= S + (T-S)*DM IF (POW.LT.50.0) THEN S= A(I,M)*CDH + A(I+1,M)*DH T= A(I,M+1)*CDH + A(I+1,M+1)*DH TIDL= S + (T-S)*DM THRUST= TIDL + (TMIL-TIDL)*POW/50.0 ELSE S= C(I,M)*CDH + C(I+1,M)*DH T= C(I,M+1)*CDH + C(I+1,M+1)*DH TMAX= S + (T-S)*DM THRUST= TMIL + (TMAX-TMIL)*(POW-50.0)*0.02 END IF&lt;/quote&gt;&lt;p&gt;The output of the military power table, TMIL, is always calculated. If the power level is under 50, then the idle table is calculated as well, TIDL. Otherwise the max table is calculated, TMAX. The output of the two table lookups is then interpolated again to calculate the final thrust value, THRUST.&lt;/p&gt;&lt;p&gt;Altogether, this forms a trilinear lookup. To translate this to C#, we call BilinearLookup twice. Then those two results are interpolated based on the power level:&lt;/p&gt;&lt;quote&gt;float InterpolateThrust(float thrust1, float thrust2, float power) { float result = Mathf.LerpUnclamped(thrust1, thrust2, power * 0.02f); return result; } float CalculateThrust(float power, float altitude, float rMach) { float a = Mathf.Max(0, altitude); float m = Mathf.Max(0, rMach); float thrust; float thrustMilitary = Table.BilinearLookup(a, 0.0001f, m, 5, militaryPowerTable, 0, 6, 0, 6); // perform trilinear interpolation if (power &amp;lt; 50.0) { float thrustIdle = Table.BilinearLookup(a, 0.0001f, m, 5, idlePowerTable, 0, 6, 0, 6); thrust = InterpolateThrust(thrustIdle, thrustMilitary, power); } else { float thrustMax = Table.BilinearLookup(a, 0.0001f, m, 5, maxPowerTable, 0, 6, 0, 6); thrust = InterpolateThrust(thrustMilitary, thrustMax, power - 50.0f); } return thrust; }&lt;/quote&gt;&lt;p&gt;The output of this calculation is the plane’s thrust in pounds-force. A simple unit conversion allows us to apply it in newtons to a Unity rigidbody:&lt;/p&gt;&lt;quote&gt;void UpdateThrust(float dt) { engine.ThrottleCommand = Throttle; engine.Mach = Mach; engine.Altitude = AltitudeFeet; engine.Update(dt); Rigidbody.AddRelativeForce(new Vector3(0, 0, engine.Thrust * poundsForceToNewtons)); }&lt;/quote&gt;&lt;head rend="h1"&gt;Forces&lt;/head&gt;&lt;head rend="h2"&gt;Lift force vs Normal force&lt;/head&gt;&lt;p&gt;In the previous flight sim project, we calculated a plane’s lift force using the angle of attack and an AnimationCurve. This is the very core of the flight simulator and is what enables flight. The flight model from the textbook does not calculate lift force.&lt;/p&gt;&lt;p&gt;Instead what this flight model calculates is normal force. Recall that lift force is perpendicular to the aircraft’s velocity vector. Normal force is perpendicular to the aircraft’s nose. This distinction is subtle at a low angle of attack, but it becomes significant at a high angle of attack.&lt;/p&gt;&lt;p&gt;There are two more analogous forces to consider, drag and axial force. Drag is always exactly opposite to the aircraft’s velocity vector while axial force is opposite the aircraft’s nose. Lift and drag are perpendicular to each other and form one set of forces. Normal and axial form another perpendicular set. It’s important to understand that these two sets of forces are equally valid. In fact, they are simply the consequence of choosing different basis vectors for measuring force.&lt;/p&gt;&lt;p&gt;And of course there is the side force that points to the right. These forces are applied on the normal, side, and longitudinal (axial) axes, which are equivalent to the X, Y, and Z axes.&lt;/p&gt;&lt;p&gt;Imagine all of the forces being produced by the aircraft are summed into a single force vector. This vector would be strongly vertical, because the plane is generating enough lift to support it’s own weight, and somewhat backwards because of drag. When this vector is projected onto the lift vector, the result is the lift force. When it’s projected onto the normal vector, the result is the normal force.&lt;/p&gt;&lt;p&gt;Choosing to represent these forces as lift/drag or normal/axial is arbitrary. The textbook flight model only deals with normal/axial force. I suspect that’s because it’s easier to measure the physical forces when using normal/axial forces in a wind tunnel, since those are always aligned with the plane’s local axes.&lt;/p&gt;&lt;p&gt;The normal force is very similar to lift for low angles of attack. Lift force peaks at the stall AOA and then declines. Normal force similarly peaks at stall AOA, but it then increases again to peak at 90 AOA, with an even higher force. 90 degrees AOA means the plane is falling downwards belly first, so it’s no longer producing lift over the wings. Instead the normal vector and the drag vector are now aligned. All of the drag force projected onto the normal vector results in a large normal force.&lt;/p&gt;&lt;p&gt;We can calculate the lift force from the normal and axial force. Both normal and axial force may contribute to the lift force, so a complete projection needs to use both. This is the formula:&lt;/p&gt;\(\text{Lift} = \text{normal} * \cos{(\text{alpha})} – \text{axial} * \sin{(\text{alpha})}\)&lt;p&gt;When we apply this formula to the normal force from the textbook, this is the result:&lt;/p&gt;&lt;p&gt;Oh wait, that’s upside down. Recall that the Z axis points downward in this coordinate system. So a negative Z value is an upwards force. Still though, the chart is a little confusing. I inverted the values below to make it more intuitive.&lt;/p&gt;&lt;p&gt;We can see at 90 degrees AOA, the normal force stays relatively high while the lift force drops to zero. This roughly matches with the chart from aerospaceweb.org above.&lt;/p&gt;&lt;p&gt;Also note that the textbook only provides table values up to 45 degrees AOA. The extrapolation of the table lookup function is what allows us to have normal force values up to 90 degrees AOA. Additionally, the table only goes down to -10 degrees AOA. We can extrapolate further, but the data will be inaccurate by -30 degrees AOA. Large negative AOA values will quickly become inaccurate. So when you’re flying, don’t do that.&lt;/p&gt;&lt;p&gt;Anyways, adding these forces to our simulator is easy. The functions are fairly simple. They are called CZ, CY, and CX. These calculate the coefficients of force on the Z, Y, and X axes respectively. Note that these functions are the coefficients, not the force values themselves. They are used to calculate the force later on.&lt;/p&gt;&lt;p&gt;CZ or the normal coefficient is calculated like this:&lt;/p&gt;&lt;quote&gt;FUNCTION CZ(ALPHA,BETA,EL) REAL A(-2:9) C DATA A/ [TABLE OMITTED] C S = 0.2*ALPHA K = INT(S) IF(K.LE.-2) K=-1 IF(K.GE.9) K=8 DA = S - FLOAT(K) L = K + INT(SIGN(1.1,DA)) S = A(K) + ABS(DA)*(A(L)-A(K)) CZ = S*(1-(BETA/57.3)**2) - .19*(EL/25.0) C RETURN END&lt;/quote&gt;&lt;p&gt;The bulk of this code is just the table interpolation function. The table only depends on ALPHA and the output is S. The only new part here is the last line, where CZ is assigned a value. S is reduced based on the value of BETA and another term is subtracted based on EL, the elevator angle.&lt;/p&gt;&lt;p&gt;This is very easy to translate to C#:&lt;/p&gt;&lt;quote&gt;float GetZAxisForceCoefficient(float alpha, float beta, float elevator) { float S = Table.LinearLookup(alpha, 0.2f, zAxisTable, -2, 10); float CZ = S * (1 - Mathf.Pow(beta * Mathf.Deg2Rad, 2)) - 0.19f * (elevator / 25.0f); return CZ; }&lt;/quote&gt;&lt;p&gt;CY or the side coefficient is even simpler. It doesn’t even have a lookup table. Side force is perpendicular to both normal and axial force.&lt;/p&gt;&lt;quote&gt;FUNCTION CY(BETA,AIL,RDR) CY = -.02*BETA + .021*(AIL/20.0) + .086*(RDR/30.0) C RETURN END&lt;/quote&gt;&lt;p&gt;Side coefficient depends solely on beta, aileron angle, and rudder angle.&lt;/p&gt;&lt;p&gt;In C#:&lt;/p&gt;&lt;quote&gt;float GetYAxisForceCoefficient(float beta, float aileron, float rudder) { float CY = -0.02f * beta + 0.021f * (aileron / 20.0f) + 0.086f * (rudder / 30.0f); return CY; }&lt;/quote&gt;&lt;p&gt;CX or the axial coefficient is basically what creates drag on the aircraft. This function is a little more complicated since it performs a bilinear interpolation, with alpha and elevator angle as the inputs.&lt;/p&gt;&lt;quote&gt;FUNCTION CX(ALPHA,EL) REAL A(-2:9,-2:2) C DATA A/ [TABLE OMITTED] C S = 0.2*ALPHA K = INT(S) IF(K.LE.-2) K=-1 IF(K.GE.9) K=8 DA = S - FLOAT(K) L = K + INT(SIGN(1.1,DA)) S = EL/12.0 M = INT(S) IF(M.LE.-2) M=-1 IF(M.GE.2) M=1 DE = S - FLOAT(M) N = M + INT(SIGN(1.1,DE)) V = A(K,M) + ABS(DA)*(A(L,M)-A(K,M)) W = A(K,N) + ABS(DA)*(A(L,N)-A(K,N)) CX = V + (W-V)*ABS(DE) C RETURN END&lt;/quote&gt;&lt;p&gt;Thanks to the table lookup functions, this is easy to translate to C#:&lt;/p&gt;&lt;quote&gt;float GetXAxisForceCoefficient(float alpha, float elevator) { float result = Table.BilinearLookup(alpha, 0.2f, elevator, 1f / 12f, xAxisTable, -2, 9, -2, 2); return result; }&lt;/quote&gt;&lt;p&gt;These three functions define all of the linear force coefficients applied to the aircraft during flight. None of these will rotate the aircraft. That is handled by a different and more complicated set of calculations.&lt;/p&gt;&lt;head rend="h1"&gt;Moments&lt;/head&gt;&lt;p&gt;Moment is another word for torque. (There is a subtle difference, but who cares?🤓) The F-16 flight model uses another set of look up tables to compute the moment for the aircraft.&lt;/p&gt;&lt;p&gt;In the previous flight sim, torque was not actually calculated. Instead, the flight model calculates the angular acceleration directly. This ignores the mass of the plane when applying the torque. This is a simplification. A more realistic flight model would take into account the mass of the aircraft when applying torque.&lt;/p&gt;&lt;p&gt;Note that mass is not sufficient to model rotations. When it comes to rotation, the analogy to mass is called moment of inertia. Just like mass is the property that measures an object’s resistance to force, moment of inertia is the resistance to torque. But unlike mass, moment of inertia can differ on all 3 axes. This means a torque on the X axis will result in a different angular acceleration than the same torque on the Y axis, for example.&lt;/p&gt;&lt;p&gt;Moment of inertia is a four-dimensional value, called AXX, AYY, AZZ, and AXZ in the textbook code. This flight model contains it’s own calculations for angular velocity using these moment of inertia values.&lt;/p&gt;&lt;p&gt;The flight model contains several functions that calculate the moment of the aircraft. The three basic functions are called CM, CL, and CN. These calculate the moments around the Y, X, and Z axes, AKA pitch, roll, and yaw, respectively. (L stands for longitudinal, which is the X axis. N stands for normal, which is the Z axis. M stands for… something)&lt;/p&gt;&lt;p&gt;These functions are all simple look up tables. CM (pitch) uses alpha and elevator angle as the input. CL (roll) and CN (yaw) use alpha and beta as the input. CM is basically the same as the other lookup table functions. CL and CN are similar to each other since they both use a symmetric table. This is because the plane is symmetric on the lateral axis, so a single table can represent the left and right sides. Their final output is then multiplied by the sign of beta.&lt;/p&gt;&lt;quote&gt;FUNCTION CL(ALPHA,BETA) REAL A(-2:9,0:6) DATA A/ [DATA OMITTED] S = 0.2*ALPHA K = INT(S) IF(K.LE.-2) K=-1 IF(K.GE.9) K=8 DA = S - FLOAT(K) L = K + INT(SIGN(1.1,DA)) S = .2*ABS(BETA) M = INT(S) IF(M.EQ.0) M=1 IF(M.GE.6) M=5 DB = S - FLOAT(M) N = M + INT(SIGN(1.1,DB)) T = A(K,M) U = A(K,N) V = T + ABS(DA)*(A(L,M) - T) W = U + ABS(DA)*(A(L,N) - U) DUM = V + (W - V) * ABS(DB) CL = DUM + SIGN(1.0,BETA) RETURN END&lt;/quote&gt;&lt;p&gt;Note that there is an error in the textbook code. The final operation “CL = DUM + SIGN(…)” should use multiplication instead of addition. Otherwise this operation doesn’t make any sense.&lt;/p&gt;&lt;p&gt;When translated into C#:&lt;/p&gt;&lt;quote&gt;float GetYAxisMoment(float alpha, float elevator) { float result = Table.BilinearLookup(alpha, 0.2f, elevator, 1f / 12f, yMomentTable, -2, 9, -2, 2); return result; } float GetXAxisMoment(float alpha, float beta) { float DUM = Table.BilinearLookup(alpha, 0.2f, Mathf.Abs(beta), 0.2f, xMomentTable, -2, 9, 0, 7); float CL = DUM * Mathf.Sign(beta); return CL; } float GetZAxisMoment(float alpha, float beta) { float DUM = Table.BilinearLookup(alpha, 0.2f, Mathf.Abs(beta), 0.2f, zMomentTable, -2, 9, 0, 7); float CL = DUM * Mathf.Sign(beta); return CL; }&lt;/quote&gt;&lt;p&gt;Notice that CM takes “elevator” as an argument, so this is where the elevator’s turning effect is calculated. But CL and CN do not take any control surface as an argument. These functions only apply moment based on alpha and beta. For example, at high angles of sideslip, the plane tends to roll. On real planes, this is caused by wing sweep. In this flight model, it’s caused by the CL function.&lt;/p&gt;&lt;p&gt;Elevators are applied in CM, but rudder and ailerons are not. Those are actually handled by four more functions, called DLDA, DLDR, DNDA, and DNDR. The names are cryptic, but it just means which axis is affected from which control surface.&lt;/p&gt;&lt;p&gt;The “L” stands for longitudinal, so DLDA is the longitudinal moment from the ailerons, A. DLDR is the longitudinal moment from the rudder, R. The “N” stands for normal, so those functions are the normal axis moment from aileron and rudders.&lt;/p&gt;&lt;p&gt;These four functions are eventually summed with the CL and CN functions above. These functions mean that roll is affected by aileron and rudder, and yaw is affected by aileron and rudder.&lt;/p&gt;&lt;head rend="h2"&gt;Damping&lt;/head&gt;&lt;p&gt;There is one more set of coefficients that must be calculated. These are the damping coefficients and they depend solely on alpha. These values are stored in 9 distinct 1D lookup tables. The code for these lookups is the same as the other lookup code.&lt;/p&gt;&lt;p&gt;These values are stored in an array of length 9 called D.&lt;/p&gt;&lt;p&gt;Damping is the moment that opposes the angular velocity of an aircraft, essentially angular drag. They affect the other moment values in somewhat complex ways. For example, some of them are combined with the plane’s current bank value to affect the roll moment.&lt;/p&gt;&lt;p&gt;What isn’t clear is what the damping values actually represent. In the C# code, I added these comments explaining their meaning:&lt;/p&gt;&lt;quote&gt;// D[0] = CXq // D[1] = CYr // D[2] = CYp // D[3] = CZq // D[4] = Clr // D[5] = Clp // D[6] = Cmq // D[7] = Cnr // D[8] = Cnp&lt;/quote&gt;&lt;p&gt;Hope this helps!&lt;/p&gt;&lt;p&gt;The best I can tell is that “CXq” is the damping moment on the X axis relative to q, which is the angular velocity around the Y axis. The other damping values follow this naming scheme.&lt;/p&gt;&lt;p&gt;This is yet another example of aerodynamics texts with poor variable names.&lt;/p&gt;&lt;head rend="h1"&gt;Complete Flight Model&lt;/head&gt;&lt;p&gt;With all of the individual coefficients defined, we can now implement the complete flight model for the F-16. This flight model actually contains it’s own physics integrator. The text provides it’s own code for calculating velocity and angular velocity from the aerodynamic forces.&lt;/p&gt;&lt;p&gt;Strictly speaking, we don’t need to use this code since Unity allows us to provide those same forces and then performs the physics calculation for us. Setting the mass is easy enough, we just have to convert slugs to kilograms. The textbook code calculates acceleration by dividing the force by the aircraft mass. We simply omit this division, convert the forces to newtons, and apply it to the rigidbody.&lt;/p&gt;&lt;p&gt;However the moment of inertia is more complicated. The textbook provides the 4 dimensional MOI values, but Unity expects a 3 dimensional inertia tensor. That inertia tensor is then rotated by a quaternion called “inertiaTensorRotation”. I have no idea how to calculate this quaternion from the textbook’s provided value.&lt;/p&gt;&lt;p&gt;Therefore, we continue to use the textbook’s code for applying moment and simply apply the resulting angular acceleration to the rigidbody.&lt;/p&gt;&lt;p&gt;The Fortran code for the flight model is concise, yet scrutable. The first step is to read the plane’s current state from the input state vector X. This is simply an array that contains all of the relevant data for this frame.&lt;/p&gt;&lt;quote&gt;VT= X(1); ALPHA= X(2)*RTOD; BETA= X(3)*RTOD PHI=X(4); THETA= X(5); PSI= X(6) P= X(7); Q= X(8); R= X(9); ALT= X(12); POW= X(13)&lt;/quote&gt;&lt;p&gt;VT is the plane’s velocity in feet per second.&lt;/p&gt;&lt;p&gt;ALPHA and BETA are the plane’s AOA and AOS in degrees. RTOD is the constant to convert from radians to degrees.&lt;/p&gt;&lt;p&gt;PHI, THETA, and PSI are the plane’s roll, pitch, and yaw in radians.&lt;/p&gt;&lt;p&gt;P, Q, and R are the plane’s angular velocities (or roll rate, pitch rate, and yaw rate) in radians per second.&lt;/p&gt;&lt;p&gt;ALT is the altitude in feet.&lt;/p&gt;&lt;p&gt;POW is the current power level of the engine (0 – 100).&lt;/p&gt;&lt;p&gt;The air data computer (ADC) and engine model are then called using these variables:&lt;/p&gt;&lt;quote&gt;CALL ADC(VT,ALT,AMACH,QBAR); CPOW= TGEAR(THTL) XD(13) = PDOT(POW,CPOW); T= THRUST(POW,ALT,AMACH)&lt;/quote&gt;&lt;p&gt;The ADC function populates the variables AMACH and QBAR, which are the altitude mach and dynamic pressure.&lt;/p&gt;&lt;p&gt;CPOW is the pilot’s commanded power setting. That is, the power level returned by calling the throttle gear function, TGEAR, on the throttle lever position, THTL.&lt;/p&gt;&lt;p&gt;The array XD is the output state vector. Specifically, it holds the calculated derivative for every input value. XD(13) is set to the value calculated by PDOT, which is the velocity of the power level.&lt;/p&gt;&lt;p&gt;T is the thrust in pounds-force output by the engine, calculated using the power level, altitude, and altitude mach.&lt;/p&gt;&lt;p&gt;Then the aerodynamic coefficients are calculated using the force and moment functions:&lt;/p&gt;&lt;quote&gt;CXT = CX (ALPHA,EL) CYT = CY (BETA,AIL,RDR) CZT = CZ (ALPHA,BETA,EL) DAIL= AIL/20.0; DRDR= RDR/30.0 CLT = CL(ALPHA,BETA) + DLDA(ALPHA,BETA)*DAIL &amp;amp; + DLDR(ALPHA,BETA)*DRDR CMT = CM(ALPHA,EL) CNT = CN(ALPHA,BETA) + DNDA(ALPHA,BETA)*DAIL &amp;amp; + DNDR(ALPHA,BETA)*DRDR&lt;/quote&gt;&lt;p&gt;The values CXT, CYT, and CZT are the coefficients on the X, Y, and Z axes, calculated by calling their respective coefficient functions.&lt;/p&gt;&lt;p&gt;EL, AIL, and RDR are the current position of the elevators, ailerons, and rudder in degrees. DAIL and DRDR are simply the angle of these surfaces divided by the max angle. Their range is [-1, 1].&lt;/p&gt;&lt;p&gt;The values CLT, CMT, and CNT are the moment coefficients on the longitudinal, m’lateral, and normal axes. Note that CM calculates moment caused by the elevator position. The effects of the other control surfaces are calculated in the DLDA, DLDR, DNDA, and DNDR functions.&lt;/p&gt;&lt;p&gt;Then some other values are calculated and the damping coefficients are added to the above values:&lt;/p&gt;&lt;quote&gt;TVT= 0.5/VT; B2V= B*TVT; CQ= CBAR*Q*TVT CALL DAMP(ALPHA,D) CXT= CXT + CQ * D(1) CYT= CYT + B2V * ( D(2)*R + D(3)*P ) CZT= CZT + CQ * D(4) CLT= CLT + B2V * ( D(5)*R + D(6)*P ) CMT= CMT + CQ * D(7) + CZT * (XCGR-XCG) CNT= CNT + B2V*(D(8)*R + D(9)*P) - CYT*(XCGR-XCG) * CBAR/B&lt;/quote&gt;&lt;p&gt;I’ll be honest, I straight up don’t know what any of these values are or why they are being applied like this. The effect appears to be angular damping (AKA angular drag) which opposes the plane’s angular velocity.&lt;/p&gt;&lt;p&gt;The value (XCGR – XCG) is the center of gravity reference minus the current center of gravity. This allows us to alter the center of gravity of the aircraft and see how that affects stability.&lt;/p&gt;&lt;p&gt;XCGR is 0.35 for this flight model. XCG is 0.35 by default. XCG is the normalized position of the center of gravity, with a possible range of [0, 1]. This means that when XCG is 0.35, the term (XCGR – XCG) becomes zero and the aircraft is balanced around it’s center of gravity.&lt;/p&gt;&lt;p&gt;The center of gravity term affects CMT and CNT, which are the pitch and yaw axes. The roll axis is not affected.&lt;/p&gt;&lt;p&gt;The next block of code is fun:&lt;/p&gt;&lt;quote&gt;CBTA = COS(X(3)); U=VT*COS(X(2))*CBTA V= VT * SIN(X(3)); W=VT*SIN(X(2))*CBTA STH = SIN(THETA); CTH= COS(THETA); SPH= SIN(PHI) CPH = COS(PHI) ; SPSI= SIN(PSI); CPSI= COS(PSI) QS = QBAR * S ; QSB= QS * B; RMQS= QS/MASS GCTH = GD * CTH ; QSPH= Q * SPH AY = RMQS*CYT ; AZ= RMQS * CZT&lt;/quote&gt;&lt;p&gt;This writhing mass of arithmetic is simply pre-calculating a lot of the values that are used to calculate the aerodynamic forces. Some of these values are used in multiple places, so to avoid repeating them, they are pulled out of those equations and placed here.&lt;/p&gt;&lt;p&gt;This is essentially the “common subexpression” optimization pass of a compiler, but applied manually.&lt;/p&gt;&lt;p&gt;The important variables are U, V, and W, which is the plane’s velocity on the X, Y, and Z axes respectively.&lt;/p&gt;&lt;p&gt;QS is QBAR (dynamic pressure) times S (wing area).&lt;/p&gt;&lt;p&gt;Now the aerodynamic forces are calculated:&lt;/p&gt;&lt;quote&gt;UDOT = R*V - Q*W - GD*STH + (QS * CXT + T)/MASS VDOT = P*W - R*U + GCTH * SPH + AY WDOT = Q*U - P*V + GCTH * CPH + AZ DUM = (U*U + W*W) xd(1) = (U*UDOT + V*VDOT + W*WDOT)/VT xd(2) = (U*WDOT - W*UDOT) / DUM xd(3) = (VT*VDOT- V*XD(1)) * CBTA / DUM&lt;/quote&gt;&lt;p&gt;Once again, I don’t actually understand what I’m reading. UDOT etc are the accelerations on each axis. These values are then used to update the output state vector xd(1), xd(2), and xd(3), which are the VT, ALPHA, and BETA that will be used in the next frame.&lt;/p&gt;&lt;p&gt;It appears that this flight model is calculating the change in alpha and beta directly from the change in velocity. This is not necessary in C#, since we can calculate alpha and beta fresh in each frame.&lt;/p&gt;&lt;p&gt;But I don’t fully understand how UDOT is calculated. R and Q are angular velocities, so multiplying them with linear velocity doesn’t make any sense. Perhaps this is some physics equation that I’m not familiar with.&lt;/p&gt;&lt;p&gt;GD * STH is the gravity acceleration times sin(theta). This is simply how gravity is applied. When the plane is level (theta = 0), sin(theta) is 0. The plane experiences no gravity acceleration on the X axis (the forward axis). When the plane is pointed straight down, sin(theta) = 1, so the plane experiences the full force of gravity pulling on the X axis.&lt;/p&gt;&lt;p&gt;A similar calculation is made for every axis.&lt;/p&gt;&lt;p&gt;For UDOT, the final term is (QS * CXT + T) / MASS. This is the coefficient CXT plus the thrust from the engine, divided by mass. VDOT and WDOT have similar final terms, made more difficult to read by the common subexpression optimization.&lt;/p&gt;&lt;p&gt;Ignoring the other terms, the 3 accelerations can be written:&lt;/p&gt;&lt;quote&gt;UDOT = (QS * CXT + T)/MASS VDOT = AY WDOT = AZ&lt;/quote&gt;&lt;p&gt;Then the variables can be expanded and rewritten:&lt;/p&gt;&lt;quote&gt;UDOT = (QBAR * S * CXT + T) / MASS VDOT = (QBAR * S * CYT) / MASS WDOT = (QBAR * S * CZT) / MASS&lt;/quote&gt;&lt;p&gt;This is simply the force coefficient times QBAR (dynamic pressure) times S (wing area). Then thrust is added to the X axis. This is how all forces are applied to the aircraft.&lt;/p&gt;&lt;p&gt;Recall the lift equation from my previous project:&lt;/p&gt;\(L=\frac12\times A\times\rho\times C_L\times v^2\)&lt;list rend="ul"&gt;&lt;item&gt;L is the resulting lift force&lt;/item&gt;&lt;item&gt;A is the surface area&lt;/item&gt;&lt;item&gt;ρ (rho) is the air density&lt;/item&gt;&lt;item&gt;CL is the coefficient of lift&lt;/item&gt;&lt;item&gt;v is the velocity&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The surface area A is equivalent to the wing area S in the Fortran code. CL is equivalent to the variables CXT, CYT, or CZT. The factor ρ * v2 is equivalent to QBAR. Thus we are essentially calculating a lift force on all three axes. But remember that we are specifically calculating normal force, not lift force.&lt;/p&gt;&lt;p&gt;The roll, pitch, and yaw state vectors are then updated:&lt;/p&gt;&lt;quote&gt;xd(4) = P + (STH/CTH)*(QSPH + R*CPH) xd(5) = Q*CPH - R*SPH xd(6) = (QSPH + R*CPH)/CTH&lt;/quote&gt;&lt;p&gt;Once again, these equations make zero sense to me🤷♂️. It’s important for the Fortran code, but we will be calculating roll, pitch, and yaw differently in C#.&lt;/p&gt;&lt;p&gt;Aerodynamic moment is about to be calculated. However this depends on the moment of inertia values and some more values derived from those:&lt;/p&gt;&lt;quote&gt;PARAMETER (AXX=9496.0, AYY= 55814.0, AZZ=63100.0, AXZ= 982.0) PARAMETER (AXZS=AXZ**2, XPQ=AXZ*(AXX-AYY+AZZ),GAM=AXX*AZZ-AXZ**2) PARAMETER (XQR= AZZ*(AZZ-AYY)+AXZS, ZPQ=(AXX-AYY)*AXX+AXZS) PARAMETER ( YPR= AZZ - AXX )&lt;/quote&gt;&lt;p&gt;Now the aerodynamic moment is calculated:&lt;/p&gt;&lt;quote&gt;ROLL = QSB*CLT PITCH = QS *CBAR*CMT YAW = QSB*CNT PQ = p*Q QR = Q*R QHX = Q*HX xd(7) = ( XPQ*PQ - XQR*QR + AZZ*ROLL + AXZ*(YAW + QHX) )/GAM xd(8) = ( YPR*P*R - AXZ*(P**2 - R**2) + PITCH - R*HX )/AYY xd(9) = ( ZPQ*PQ - XPQ*QR + AXZ*ROLL + AXX*(YAW + QHX) )/GAM&lt;/quote&gt;&lt;p&gt;There’s a lot of stuff going on here. The output state vectors are updated using the moment of inertia values as well as HX, which is the angular momentum of the spinning engine mass. I don’t know enough about physics to fully understand why these equations are defined like this.&lt;/p&gt;&lt;p&gt;But we can at least see how the moment coefficients are used if we expand the ROLL, PITCH, and YAW variables:&lt;/p&gt;&lt;quote&gt;ROLL = QBAR * S * B * CLT PITCH = QBAR * S * CBAR * CMT YAW = QBAR * S * B * CNT&lt;/quote&gt;&lt;p&gt;ROLL and YAW depend on B, the wingspan of the plane. Pitch depends on CBAR, the mean aerodynamic chord.&lt;/p&gt;&lt;p&gt;The final step is to calculate the world space position of the aircraft. Since we are using Unity rigidbodies to implement the flight model, this step is not translated to C#. But for reference:&lt;/p&gt;&lt;quote&gt;T1= SPH * CPSI; T2= CPH * STH; T3= SPH * SPSI S1= CTH * CPSI; S2= CTH * SPSI; S3= T1 * STH - CPH * SPSI S4= T3 * STH + CPH * CPSI; S5= SPH * CTH; S6= T2*CPSI + T3 S7= T2 * SPSI - T1; S8= CPH * CTH xd(10) = U * S1 + V * S3 + W * S6 ! North speed xd(11) = U * S2 + V * S4 + W * S7 ! East speed xd(12) = U * STH -V * S5 - W * S8 ! Vertical speed AN = -AZ/GD; ALAT= AY/GD;&lt;/quote&gt;&lt;p&gt;Now we can start translating this into C# using Unity’s physics engine to replace some parts.&lt;/p&gt;&lt;p&gt;A lot of the code can be reused from the previous flight sim project. Using it for this new flight model only requires some conversion into customary units and back. The main class that controls everything is Plane. This class contains instances of the AirDataComputer, Engine, and Aerodynamics, which is where the translated Fortran code lives.&lt;/p&gt;&lt;p&gt;One simplification can be made since we are using Unity physics. We do not need to calculate the acceleration of the aircraft manually. That can be done automatically by the physics engine. However, the moment calculation needs to be copied more or less directly from the textbook.&lt;/p&gt;&lt;p&gt;The air data computer needs to be called:&lt;/p&gt;&lt;quote&gt;void UpdateAirData() { float speed = LocalVelocity.magnitude; // m/s float speedFeet = speed * metersToFeet; AltitudeFeet = Rigidbody.position.y * metersToFeet; airData = airDataComputer.CalculateAirData(speedFeet, AltitudeFeet); }&lt;/quote&gt;&lt;p&gt;Then the engine needs to be updated and the thrust force applied:&lt;/p&gt;&lt;quote&gt;void UpdateThrust(float dt) { engine.ThrottleCommand = Throttle; engine.Mach = Mach; engine.Altitude = AltitudeFeet; engine.Update(dt); Rigidbody.AddRelativeForce(new Vector3(0, 0, engine.Thrust * poundsForceToNewtons)); }&lt;/quote&gt;&lt;p&gt;For the aerodynamics class, a struct with all relevant aerodynamic state is passed, similar to the state vector in the Fortran code.&lt;/p&gt;&lt;quote&gt;public struct AerodynamicState { public Vector4 inertiaTensor; public Vector3 velocity; public Vector3 angularVelocity; public AirData airData; public float altitude; public float alpha; public float beta; public float xcg; public ControlSurfaces controlSurfaces; }&lt;/quote&gt;&lt;p&gt;This is populated by the Plane class, which also handles unit conversions:&lt;/p&gt;&lt;quote&gt;AerodynamicState currentState = new AerodynamicState { inertiaTensor = inertiaTensor, velocity = ConvertVectorToAerospace(LocalVelocity) * metersToFeet, angularVelocity = ConvertAngleToAerospace(LocalAngularVelocity), airData = airData, alpha = alpha, beta = beta, xcg = centerOfGravityPosition, controlSurfaces = ControlSurfaces }; var newState = aerodynamics.CalculateAerodynamics(currentState);&lt;/quote&gt;&lt;p&gt;All of the flight model code is located inside the Aerodynamics class.&lt;/p&gt;&lt;p&gt;First step is to call the aerodynamic coefficient functions from above:&lt;/p&gt;&lt;quote&gt;Vector3 GetForceCoefficient(float alpha, float beta, float aileron, float rudder, float elevator) { return new Vector3( GetXAxisForceCoefficient(alpha, elevator), GetYAxisForceCoefficient(beta, aileron, rudder), GetZAxisForceCoefficient(alpha, beta, elevator) ); } Vector3 GetMomentCoefficient(float alpha, float beta, float elevator) { return new Vector3( GetXAxisMomentCoefficient(alpha, beta), GetYAxisMomentCoefficient(alpha, elevator), GetZAxisMomentCoefficient(alpha, beta) ); } ... public AerodynamicForces CalculateAerodynamics(AerodynamicState currentState) { Vector3 forceCoefficient = GetForceCoefficient( currentState.alpha, currentState.beta, currentState.controlSurfaces.aileron, currentState.controlSurfaces.rudder, currentState.controlSurfaces.elevator ); Vector3 momentCoefficient = GetMomentCoefficient( currentState.alpha, currentState.beta, currentState.controlSurfaces.elevator ); }&lt;/quote&gt;&lt;p&gt;Then we calculate the damping values. This function simply performs the 9 table lookups.&lt;/p&gt;&lt;quote&gt;void CalculateDampingValues(float alpha) { float S = 0.2f * alpha; int K = Mathf.Clamp((int)S, -1, 8); float DA = S - K; int L = K + (int)Mathf.Sign(DA); for (int i = 0; i &amp;lt; 9; i++) { dampingTable[i] = ReadDampTable(dampTable, K, i) + Math.Abs(DA) * (ReadDampTable(dampTable, L, i) - ReadDampTable(dampTable, K, i)); } }&lt;/quote&gt;&lt;p&gt;Then the variables we need later are calculated:&lt;/p&gt;&lt;quote&gt;// calculate variables float P = currentState.angularVelocity.x; // roll rate float Q = currentState.angularVelocity.y; // pitch rate float R = currentState.angularVelocity.z; // yaw rate float airspeed = Mathf.Max(1, currentState.velocity.magnitude); float TVT = 0.5f / airspeed; float B2V = wingSpanFt * TVT; float CQ = CBAR * Q * TVT; float DAIL = currentState.controlSurfaces.aileron / 20.0f; float DRDR = currentState.controlSurfaces.rudder / 30.0f; float QS = currentState.airData.qBar * wingAreaFtSquared; float QSB = QS * wingSpanFt;&lt;/quote&gt;&lt;p&gt;Then damping is applied to the force and moment coefficients:&lt;/p&gt;&lt;quote&gt;// damping float CXT = forceCoefficient.x + CQ * dampingTable[0]; float CYT = forceCoefficient.y + B2V * (dampingTable[1] * R + dampingTable[2] * P); float CZT = forceCoefficient.z + CQ * dampingTable[3]; float CLT = momentCoefficient.x + B2V * (dampingTable[4] * R + dampingTable[5] * P); CLT += GetDLDA(currentState.alpha, currentState.beta) * DAIL; CLT += GetDLDR(currentState.alpha, currentState.beta) * DRDR; float CMT = momentCoefficient.y + CQ * dampingTable[6] + CZT * (XCGR - currentState.xcg); float CNT = momentCoefficient.z + B2V * (dampingTable[7] * R + dampingTable[8] * P) - CYT * (XCGR - currentState.xcg) * CBAR / wingSpanFt; CNT += GetDNDA(currentState.alpha, currentState.beta) * DAIL; CNT += GetDNDR(currentState.alpha, currentState.beta) * DRDR;&lt;/quote&gt;&lt;p&gt;Note that the damping array in Fortran is 1 based, while the same array in C# is 0 based.&lt;/p&gt;&lt;p&gt;Forces are calculated from the force coefficients. Since we are using Unity’s physics to apply gravity, the gravity terms are not included here. The force from engine thrust is applied outside of this class. And force is applied to a rigidbody, so acceleration does not need to be calculated manually. So the force calculations are now very simple:&lt;/p&gt;&lt;quote&gt;// forces // Acceleration in original text. Need to calculate force instead of acceleration float UDOT = QS * CXT; float VDOT = QS * CYT; float WDOT = QS * CZT;&lt;/quote&gt;&lt;p&gt;Moments are calculated using largely the same code as the textbook:&lt;/p&gt;&lt;quote&gt;// moments float ROLL = QSB * CLT; float PITCH = QS * CBAR * CMT; float YAW = QSB * CNT; float PQ = P * Q; float QR = Q * R; float QHX = Q * HX; // calculate inertia values float AXX = currentState.inertiaTensor.x; float AYY = currentState.inertiaTensor.y; float AZZ = currentState.inertiaTensor.z; float AXZ = currentState.inertiaTensor.w; float AXZS = AXZ * AXZ; float XPQ = AXZ * (AXX - AYY + AZZ); float GAM = AXX * AZZ - AXZS; float XQR = AZZ * (AZZ - AYY) + AXZS; float ZPQ = (AZZ - AYY) * AXX + AXZS; float YPR = AZZ - AXX; float rollAccel = ((XPQ * PQ) - (XQR * QR) + (AZZ * ROLL) + (AXZ * (YAW + QHX))) / GAM; float pitchAccel = ((YPR * P * R) - (AXZ * (P * P - R * R)) + PITCH - (R * HX)) / AYY; float yawAccel = ((ZPQ * PQ) - (XPQ * QR) + (AXZ * ROLL) + (AXX * (YAW + QHX))) / GAM;&lt;/quote&gt;&lt;p&gt;Finally, the force and angular acceleration is returned:&lt;/p&gt;&lt;quote&gt;public AerodynamicForces CalculateAerodynamics(AerodynamicState currentState) { ... result.force = new Vector3(UDOT, VDOT, WDOT); result.angularAcceleration = new Vector3(rollAccel, pitchAccel, yawAccel); return result; }&lt;/quote&gt;&lt;p&gt;Then in the Plane class, the force and angular acceleration can be applied to the rigidbody:&lt;/p&gt;&lt;quote&gt;// aeroForces in pounds var forces = ConvertVectorToUnity(aeroForces) * poundsForceToNewtons; Rigidbody.AddRelativeForce(forces); // aeroAngularAcceleration changes angular velocity directly Vector3 avCorrection = ConvertAngleToUnity(aeroAngularAcceleration); Rigidbody.AddRelativeTorque(avCorrection, ForceMode.Acceleration); lastAngularAcceleration = avCorrection;&lt;/quote&gt;&lt;p&gt;The plane is now able to fly. But if you try flying it right now, you will quickly find that it is impossible to fly by hand.&lt;/p&gt;&lt;head rend="h1"&gt;Stability&lt;/head&gt;&lt;p&gt;One important aerodynamic effect not modeled in my previous flight sim project is stability. Stability is the behavior of an aircraft when it’s disturbed from it’s flight path. More specifically, it’s how the aircraft behaves when it’s nose vector doesn’t match it’s velocity vector. Stability is the force that pulls the nose vector back towards the velocity vector.&lt;/p&gt;&lt;p&gt;For most aircraft, stability is created by the stabilizers in the tail. A stabilizer is simply a small airfoil (wing). Even without the pilot giving input, the stabilizers act like the fins of a dart. As the plane increases it’s Angle of Attack, the horizontal stabilizer will produce a lift force at the rear of the plane. This creates a torque that pulls the plane’s nose back towards the velocity vector, thus reducing the AOA. Likewise, the vertical stabilizers will create a torque that reduces Angle of Slip.&lt;/p&gt;&lt;p&gt;Keep in mind that stability depends on AOA, just as lift does. When the aircraft has a large AOA, the wings produce lift, which brings the velocity vector towards the nose vector. The stabilizers create torque which brings the nose vector towards the velocity vector. These two forces balance out somewhere and the aircraft will take a new attitude with a new velocity.&lt;/p&gt;&lt;p&gt;However, the aircraft needs to maintain a non-zero AOA to create enough lift to fly straight and level. How does it maintain this AOA when stability works to reduce AOA to zero? The stabilizers can be trimmed to hold a specific AOA. This means that the stabilizers produce zero torque at this particular non-zero AOA. In some planes this must be done manually by the pilot, but in the F-16 this is done automatically by the FCS.&lt;/p&gt;&lt;p&gt;The two forces of lift and stability combine to produce the “feel” of an aircraft’s controls. The tendency for the nose to be pulled towards the velocity vector is called positive stability. Most non-fighter aircraft are designed to have positive stability to maximize safety and ease of flying.&lt;/p&gt;&lt;p&gt;But fighter aircraft like the F-16 are different. These aircraft are often designed to have neutral or even negative stability. Neutral stability means that the aircraft will hold it’s current attitude. Negative stability means that the aircraft will rotate even further away from the velocity vector, at an increasing rate.&lt;/p&gt;&lt;p&gt;The previous flight sim does not model this at all. There is no torque that changes the plane’s attitude except for the steering force. So the behavior is best described as neutrally stable.&lt;/p&gt;&lt;p&gt;This F-16 flight model does include stability. But keep in mind that the real F-16 was designed to have relaxed static stability. This means that it is positively stable, but weakly so. This makes the aircraft more maneuverable and better at retaining energy while turning. But flying an aircraft like this is difficult or even impossible for a human pilot. The plane will depart from steady flight from the smallest stick input or wind gust. The only way a human can handle an aircraft like this during long and stressful missions is with a computerized flight control system.&lt;/p&gt;&lt;head rend="h1"&gt;Flight Control System&lt;/head&gt;&lt;p&gt;A flight control system (FCS) is a computer located between the flight stick and the control surfaces. This computer translates the pilot’s input on the flight stick into control surface movement. It can react to disturbances in the plane’s attitude more quickly and precisely than a human can.&lt;/p&gt;&lt;p&gt;In my previous flight sim project, the control surfaces were purely cosmetic. The actual method used to turn the vehicle was by applying torque directly to the center of mass. That torque was calculated to create a certain amount of angular acceleration without exceeding the plane’s turn rate limit.&lt;/p&gt;&lt;p&gt;For example, the plane had a turn rate on the pitch axis of 60 degrees per second and an acceleration of 120 degrees per second per second. The plane’s turn rate never leaves the range [-60, 60]. Actually, no torque is ever calculated. Unity provides a function to apply angular acceleration directly, ignoring moment of inertia. I chose this behavior to make it easy to both understand the code and to fly the plane.&lt;/p&gt;&lt;p&gt;But this F-16 simulator does depend on the position of the control surfaces. Instead of specifying the acceleration directly, this simulator specifies the torque (moment) and calculates the resulting acceleration. This is more accurate to how serious simulators work and how real planes fly, but this makes controlling the plane more difficult.&lt;/p&gt;&lt;p&gt;The steering system in the previous flight sim project was essentially a perfect FCS that could always achieve the turn rate chosen by the pilot. This is helped by the fact that that simulator does not model aerodynamic stability or instability at all. Spinning out of control was simply not possible.&lt;/p&gt;&lt;p&gt;This F-16 simulator is more difficult to control both because of the more accurate control surfaces and because of the modeled stability. You can actually try to fly this F-16 manually, by disabling the FCS in the config menu.&lt;/p&gt;&lt;p&gt;You will quickly find that the F-16 is almost impossible to fly manually. Every small disturbance from straight and level flight will create small torques that turn your plane unexpectedly. If you try to correct it with the control stick, you will almost certainly overcorrect and send the plane into a new and exciting attitude. This is called pilot induced oscillation.&lt;/p&gt;&lt;p&gt;It simply isn’t possible for a human to react quickly and precisely enough to fly this aircraft. You may be able to fly straight and level with some effort, but you will quickly lose control if you attempt any maneuver. This is indeed a property of the real F-16.&lt;/p&gt;&lt;p&gt;The textbook provides no Fortran code for the FCS. From here on out, it’s my own original code.&lt;/p&gt;&lt;head rend="h2"&gt;PID Controllers&lt;/head&gt;&lt;p&gt;The steering system from the previous project cannot be reused. The solution is to use PID controllers, a topic I’ve covered on this blog before.11&lt;/p&gt;&lt;p&gt;To be more specific, steering in the previous flight sim was easy because we could read the angular velocity of the aircraft and apply a torque that directly countered any undesired movement. This F-16 flight model does not allow us to apply torques directly. We can only set the angle of the control surfaces. This is the problem that PID controllers are good at solving.&lt;/p&gt;&lt;p&gt;Adding the PID controllers is simple. The pilot’s control input is used to select a target angular velocity for the plane, for the 3 axes of rotation. This is given to three independent PID controllers. The output of the PID controllers set the target position for the control surface.&lt;/p&gt;&lt;p&gt;The control surface positions are then passed into the flight model inside AerodynamicState.&lt;/p&gt;&lt;quote&gt;Vector3 targetAV = Vector3.Scale(controlInput, steeringSpeed * steeringSpeedFactor); var accel = lastAngularAcceleration * Mathf.Rad2Deg * dt; controlSurfaceTarget = new Vector3( pitchController.Calculate(dt, av.x, accel.x, targetAV.x), -yawController.Calculate(dt, av.y, accel.y, targetAV.y), rollController.Calculate(dt, av.z, accel.z, targetAV.z) ); var current = ControlSurfaces; ControlSurfaces = new ControlSurfaces( Utilities.MoveTo(current.elevator, controlSurfaceTarget.x, elevatorSpeed, dt, -elevatorRange, elevatorRange), Utilities.MoveTo(current.rudder, controlSurfaceTarget.y, rudderSpeed, dt, -rudderRange, rudderRange), Utilities.MoveTo(current.aileron, controlSurfaceTarget.z, aileronSpeed, dt, -aileronRange, aileronRange) ); ... AerodynamicState currentState = new AerodynamicState { controlSurfaces = ControlSurfaces }; var newState = aerodynamics.CalculateAerodynamics(currentState);&lt;/quote&gt;&lt;p&gt;Here the PIDs are named “pitchController”, “yawController”, and “rollController”. They are all tuned separately to handle a single axis.&lt;/p&gt;&lt;p&gt;When the player releases the stick, the PID controllers will attempt to hold an angular velocity of zero. This makes the aircraft feel like it’s neutrally stable. This also acts as a way to trim the aircraft, so that level flight can be maintained without needing to constantly pull the stick. The PID controller will detect an undesired rotation and move the elevators at a slight angle to counter it.&lt;/p&gt;&lt;p&gt;These PID controllers only add a small amount of complexity to the code, but they achieve similar results as the perfect FCS from the previous project. But there are still limitations that prevent it from being a perfect FCS.&lt;/p&gt;&lt;p&gt;First, the PID controllers must be tuned. The output has to be strong enough to quickly respond to pilot inputs, while avoiding oscillation. This is of course a limitation of any PID control system.&lt;/p&gt;&lt;p&gt;Second, the control surfaces move at a finite speed. This means that it will take some time for the control surface to match the FCS’s commands. So the commands will be imperfectly applied to the aircraft.&lt;/p&gt;&lt;p&gt;Third, unlike the previous flight sim, the three axes of rotation are not independent. For example, a large angle of slip will cause the plane to roll. This is due to the swept wings of the F-16. The roll controller will cancel this out somewhat, but a large enough AOS will result in an uncommanded roll.&lt;/p&gt;&lt;p&gt;Even with these limitations, the PID controllers work fairly well at keeping the plane in control.&lt;/p&gt;&lt;p&gt;Additionally, I use a technique called gain scheduling to change the gain parameters of the roll controller. Because roll performance increases with airspeed, we need a way to limit the amount of aileron movement at high speed. I add two animation curves, which take speed as input, and give the P and D gain of the roll controller as output.&lt;/p&gt;&lt;quote&gt;rollController.P = rollPSchedule.Evaluate(Mathf.Max(0, LocalVelocity.z)); rollController.D = rollDSchedule.Evaluate(Mathf.Max(0, LocalVelocity.z)); Vector3 fcsTarget = new Vector3( pitchController.Calculate(dt, av.x, accel.x, targetAV.x), -yawController.Calculate(dt, av.y, accel.y, targetAV.y), rollController.Calculate(dt, av.z, accel.z, targetAV.z) );&lt;/quote&gt;&lt;p&gt;This allows us to change the strength of the roll controller at different speeds. A more advanced FCS might have a gain schedule for each controller, possibly using more inputs than just airspeed. In fact, if there were multiple inputs, we would need a 2D lookup table to calculate the gain schedule.&lt;/p&gt;&lt;p&gt;Because the flight model is a complete description of how the aircraft will respond at different combinations of AOA, AOS, and control input, it is theoretically possible to design an FCS system that perfectly counters all of the unwanted tendencies. However, that is beyond my understanding of aerodynamics and control theory.&lt;/p&gt;&lt;head rend="h2"&gt;G and AOA Limiter&lt;/head&gt;&lt;p&gt;The weakness of PID controllers is that they only control the angular velocity of the plane. This is not sufficient to control the plane. The previous project has a G limiter, which is simple since steering torque is applied directly to the aircraft. Adding a G limiter is more complicated with this F-16 flight model.&lt;/p&gt;&lt;p&gt;Additionally, a critical part of the FCS on a real F-16 is the AOA limiter. Just like the G limiter prevents the pilot from creating excessive G-forces while maneuvering, the AOA limiter prevents excessive AOA. This is because the aircraft becomes so unstable at about 28 degrees AOA that even the FCS can not compensate. And importantly, our flight model only supports a limited range of AOA (up to 45 degrees), so if the pilot goes beyond that, the behavior of the simulator becomes nonsensical. So limiting the AOA to about 25 degrees is important for maintaining stable flight.&lt;/p&gt;&lt;p&gt;The previous flight sim project did not have anything like an AOA limiter. I simply tuned the steering strength so that AOA would not exceed about 15 degrees (unless stalling). And even then, there is no instability caused by high AOA, so nothing bad happens if the pilot exceeds that.&lt;/p&gt;&lt;p&gt;We need a system that prevents the pilot from exceeding 25 degrees AOA. This would be implemented as a multiplier on the pilot’s stick input, just like a G limiter. Since there are two limits, we simply select the more restrictive limit using min(). So if the G limiter says to limit input to 0.75 and the AOA limiter says to limit input to 0.5, the value 0.5 is chosen.&lt;/p&gt;&lt;p&gt;Because this flight model uses lookup tables, there is no simple formula for calculating either the G limiter or AOA limiter. The G limiter from the previous project won’t work here. Additionally, the relationship between steering input and AOA is not simple. There is a feedback loop between AOA and lift. As AOA increases, lift increases. But as lift increases, AOA decreases since lift pulls the plane onto a new velocity vector. Not to mention lift also depends on airspeed and altitude.&lt;/p&gt;&lt;p&gt;Luckily, the F-16 flight model is completely disconnected from Unity’s physics system. We can actually run the flight model as much as we want with any inputs, and use the outputs for any purpose. There is the “main” flight model that syncs with a Unity rigidbody. But we can create “side” flight models to predict future behavior of the plane.&lt;/p&gt;&lt;p&gt;I chose to implement the G and AOA limiters by running a side flight model. This side model takes the pilot’s inputs and simulates the aircraft in a simplified world state. In a single physics update, the main flight model runs once, but the side flight model runs multiple times to predict movement several seconds into the future. Because running the flight model is a few lookups and math operations, running multiple times per frame is dirt cheap.&lt;/p&gt;&lt;p&gt;By running this side model, we can determine how the plane would behave if it flew without any limiters. So if the plane is flying fast enough to pull 16 Gs, the side model will report that. We can use that information to calculate the G limiter for the main model.&lt;/p&gt;&lt;p&gt;The side model is contained in the class SimpleTrimmer. The main function Trim looks like this:&lt;/p&gt;&lt;quote&gt;public SimulatedState Trim(float dt, float timeMax, SimulatedState initialState) { float time = 0; while (time &amp;lt; timeMax) { AerodynamicState aeroState = new AerodynamicState() { ... }; var aeroForces = aerodynamics.CalculateAerodynamics(aeroState); … time += dt; } return state; }&lt;/quote&gt;&lt;p&gt;It just calls CalculateAerodynamics in a loop with it’s own time variable. The timestep can also be different from the main FixedUpdate loop time step. The variable timeMax controls how far into the future the prediction runs. For example, this side model can run at 0.1 second time steps for 5 seconds total.&lt;/p&gt;&lt;p&gt;After one step of the simulation is run, the state variables are updated and fed back into the next step. The maximum G force and AOA of the whole simulation is recorded.&lt;/p&gt;&lt;quote&gt;// rotate velocity by pitchDelta Quaternion newRotation = Quaternion.Euler(0, pitchDelta, 0); Vector3 newVelocity = newRotation * state.velocity; newVelocity.y = 0; newVelocity.z += gravity * dt; Vector3 velNormalized = newVelocity.normalized; // assume airspeed magnitude does not change (no drag, no thrust) state.velocity = velNormalized * airspeed; state.alpha = Mathf.Atan2(velNormalized.z, velNormalized.x) * Mathf.Rad2Deg; state.maxAlpha = Mathf.Max(state.maxAlpha, state.alpha); state.maxAccelerationZ = Mathf.Min(state.maxAccelerationZ, state.acceleration.z);&lt;/quote&gt;&lt;p&gt;This simulation is highly simplified compared to the main flight model. It ignores the pilot’s input except pitch. It ignores angular velocity except for pitch rate. It does not apply drag or any other force that changes airspeed or altitude. It ignores any change to the aircraft’s pitch. Note that the flight model does not care about the orientation of the aircraft to begin with.&lt;/p&gt;&lt;p&gt;The Trim function assumes the pilot is giving a full pitch up or pitch down input and takes the pitch PID controller as a parameter. So this side flight model uses the same PID values as the main model, to prevent the simulation from turning faster than the max turn rate. Since the I term is not used on the PID controller, we can use it without worrying about state.&lt;/p&gt;&lt;p&gt;Gravity as a single float value is also passed as a parameter. This allows the simulation to know how much gravity is affecting the turn on the pitch axis. If the plane is level, this value is 1. If the plane is rolled 90 degrees to the side, this value is 0. If upside down, this value is -1. Gravity on the other axes is ignored.&lt;/p&gt;&lt;p&gt;The larger time step and reduced complexity of simulation means that the side model is not completely accurate to how the plane will fly. But that’s acceptable since we are only using this to estimate the maximum G force and AOA that a turn might create.&lt;/p&gt;&lt;p&gt;After running through a few seconds of simulation on the flight model, the Trim function returns with the max G force and AOA. The FCS then uses these values to calculate the limiting factors for the pilot’s pitch input.&lt;/p&gt;&lt;quote&gt;SimpleTrimmer.SimulatedState state = simpleTrimmer.Trim( trimmerTimeStep, trimmerTime, initialState, maxAV.x * Mathf.Deg2Rad, gravityFactor * metersToFeet, pitchController, centerOfGravityPosition ); float predictedAlpha = state.maxAlpha; float predictedG = -state.maxAccelerationZ * feetToMeters; float aoaPitchMult = CalculateAOALimiter(predictedAlpha); float gLimit = gLimitPitch; // pitch up limit (ie 8G) if (controlInput.x &amp;gt; 0) { gLimit = this.gLimit; // pitch down limit (ie 4G) } float gPitchMult = CalculateGLimiter(predictedG, gLimit);&lt;/quote&gt;&lt;p&gt;The limiting factor for AOA and G force is calculated with a simple function:&lt;/p&gt;&lt;quote&gt;float ApplyLimiter(float value, float limit, float limitStrength) { if (limit &amp;lt;= 0) return 1; if (value &amp;lt; limit) return 1; float error = value - limit; error *= limitStrength; return limit / (limit + error); }&lt;/quote&gt;&lt;p&gt;ApplyLimiter returns a factor in the range [0, 1], which is eventually multiplied with the pilot’s control input. This function then used in the limiter functions:&lt;/p&gt;&lt;quote&gt;float CalculateGLimiter(float predictedG, float gLimit) { float gForce = predictedG / 9.81f; float gPitchMult = ApplyLimiter(gForce, gLimit, gLimitStrength); return gPitchMult; }&lt;/quote&gt;&lt;p&gt;The variable gForce is the predicted max G force from the side model. gLimit is the value chosen as the max G force, for example, 8. If the predicted value is 12, then the variable error will be 12 – 8 = 4. The returned factor would be 8 / (8 + 4) = 8 / 12 = 0.66. limitStrength is used to tune how strongly the error affects the returned limit factor.&lt;/p&gt;&lt;p&gt;If the value is below the limit, the returned factor is 1.&lt;/p&gt;&lt;p&gt;The AOA limiter uses the same function to calculate two limiting factors which are combined:&lt;/p&gt;&lt;quote&gt;float CalculateAOALimiter(float predictedAlpha) { float aoaPitchMult = 1.0f; aoaPitchMult *= ApplyLimiter(predictedAlpha, predictedAoaLimitMax, predictedAoaLimitStrength); float realAOA = AngleOfAttack * Mathf.Rad2Deg; aoaPitchMult *= ApplyLimiter(realAOA, feedbackAoaLimitMax, feedbackAoaLimitStrength); return aoaPitchMult; }&lt;/quote&gt;&lt;p&gt;One limit factor depends on the predicted alpha from the SimpleTrimmer class. The other factor depends on the actual alpha value the plane currently has. This can handle cases where the real alpha is much larger than the predicted value, such as when the plane is already stalling.&lt;/p&gt;&lt;p&gt;Then the AOA and G limiter factors are applied to the pilot’s input:&lt;/p&gt;&lt;quote&gt;float aoaPitchMult = CalculateAOALimiter(predictedAlpha); float gPitchMult = CalculateGLimiter(predictedG, gLimitPitch); float pitchMult = Mathf.Min(aoaPitchMult, gPitchMult); // select whichever limiter is stronger float rollMult = rollPitchFactor.Evaluate(Mathf.Abs(controlInput.x)) * rollAOAFactor.Evaluate(AngleOfAttack * Mathf.Rad2Deg); Vector3 limitedInput = Vector3.Scale(controlInput, new Vector3(pitchMult, 1, rollMult)); Vector3 targetAV = Vector3.Scale(limitedInput, steeringSpeed * steeringSpeedFactor);&lt;/quote&gt;&lt;p&gt;The min() function is used to select whichever limiter factor is strongest. Since I am designing these systems myself, I can tell you there is not a strong reason why I chose min() instead of another multiplication. This is just the formula that felt right when I was testing it.&lt;/p&gt;&lt;p&gt;In fact there are many different ways that the limiting factors could be calculated and combined. I designed the ApplyLimiter function primarily to be easy to tune. These allow me to have separate variables for tuning predicted G, predicted AOA, and feedback AOA limiters.&lt;/p&gt;&lt;p&gt;There is one final limiter above, rollMult. This is controlled by two AnimationCurves, rollPitchFactor and rollAOAFactor. These curves reduce the strength of roll input when the pilot is commanding a strong pitch rotation and when the plane has a high AOA. I added this because rolls felt too sensitive when in a high G or high AOA turn. Tune these to your own taste.&lt;/p&gt;&lt;head rend="h2"&gt;Stick Pusher&lt;/head&gt;&lt;p&gt;The final system to add is a stick pusher. A stick pusher is a device some aircraft have that physically pushes the stick forward to avoid a stall. This doesn’t exist in the real F-16, even digitally, but who cares? It was quick and easy to write.&lt;/p&gt;&lt;p&gt;If the AOA exceeds some threshold, a bias value is added to the pilot’s stick input to push the nose down. This is different from the AOA limiter above, which multiplies the input by a factor [0, 1]. If the pilot is giving an input of 0, then the AOA limiter has no effect. The stick pusher adds the bias value to the pilot’s input, so it will work even when the pilot gives no input.&lt;/p&gt;&lt;p&gt;The stick pusher will apply when the plane is stalling or if the AOA limiter fails to keep the AOA in the safe range.&lt;/p&gt;&lt;p&gt;The code for this is incredibly simple in concept and implementation:&lt;/p&gt;&lt;quote&gt;float CalculateAOAPusher() { float bias = 0.0f; float aoa = AngleOfAttack * Mathf.Rad2Deg; if (aoa &amp;gt; stickPusherThreshold) { float error = aoa - stickPusherThreshold; bias = stickPusherCurve.Evaluate(error); } return Mathf.Min(stickPusherMax, bias); }&lt;/quote&gt;&lt;p&gt;If the AOA is over the stickPushThreshold, add a bias to the player’s input. The more it exceeds the threshold, the stronger the bias. At max strength, the stick pusher can give a full nose down input that can’t be overridden by the pilot.&lt;/p&gt;&lt;p&gt;This value is summed with the pilot’s input before running the PID controllers.&lt;/p&gt;&lt;quote&gt;Vector3 stickPusher = new Vector3(CalculateAOAPusher(), 0, 0); Vector3 limitedInput = Vector3.Scale(controlInput, new Vector3(pitchMult, 1, rollMult)) + stickPusher; Vector3 targetAV = Vector3.Scale(limitedInput, steeringSpeed * steeringSpeedFactor);&lt;/quote&gt;&lt;p&gt;With all of these systems added to the FCS, the plane should be very stable to fly now. Since the side model simulation is simplified, the G and AOA limiters are not perfect. They will sometimes result in those parameters being limited at a value too high or too low. But these systems do work accurately enough to keep the plane stable.&lt;/p&gt;&lt;head rend="h1"&gt;Testing&lt;/head&gt;&lt;p&gt;Of course any implementation can have bugs. We need to test the flight model to make sure it works. This includes the translation of the Fortran flight model, and the code that implements all of this in Unity.&lt;/p&gt;&lt;head rend="h2"&gt;Unit Testing&lt;/head&gt;&lt;p&gt;Because the flight model is separate from Unity’s physics engine, we can actually test it using normal unit testing techniques. Unity provides a unit testing framework based on NUnit, so testing is pretty typical for C#.&lt;/p&gt;&lt;p&gt;The authors of the textbook also helpfully provide a test case to use. They give the inputs to the model (airspeed, control surface position, throttle, etc) and the expected output of the model (forces, moment, angular velocity, etc). This lets us validate that the model is implemented correctly by running a single step of simulation.&lt;/p&gt;&lt;quote&gt;// Textbook provides a table of input values and the expected output // Index Param Input State (X) Output (XD) // 1 0.4 (XCG) 0.9 (throttle) 500 (vt) -75.23724 // 2 20 (elevator) 0.5 (alpha) -0.8813419 // 3 -15 (aileron) -0.2 (beta) -0.4759990 // 4 -20 (rudder) -1 (phi) // 5 1 (theta) // 6 -1 (psi) // 7 0.7 (P) 12.62679 // 8 -0.8 (Q) 0.9649671 // 9 0.9 (R) 0.5809759 // 10 1000 (north) // 11 900 (east) // 12 10000 (alt) 248.1241 // 13 90 (power) -58.68999&lt;/quote&gt;&lt;p&gt;Note that the output values for roll, pitch, and yaw, and north, east, and altitude, are not checked in this test. We are using the Unity rigidbody to handle these, so these values are not even calculated in C#.&lt;/p&gt;&lt;p&gt;Additionally, the table lookup operations are fairly simple C# code, so these functions can also be unit tested. I caught a few bugs in the flight model by adding these tests.&lt;/p&gt;&lt;p&gt;All of the tests are in the ModelTestCase class.&lt;/p&gt;&lt;head rend="h2"&gt;Flight Testing&lt;/head&gt;&lt;p&gt;Of course unit testing can only cover so much. The whole point of this project is to create a flight simulator. The only way to know how the flight model really feels is to fly it. So get out there and start flying it!&lt;/p&gt;&lt;p&gt;I have caught a few bugs in the implementation just by flying it and realizing that some aspect felt weird.&lt;/p&gt;&lt;p&gt;In the aerospace industry, test flights are thoroughly instrumented to gather as much data as possible. Force on every axis, angular velocity, pilot input, GPS track, etc is all recorded and stored for future analysis. It’s possible to write automated tests that read this data and check that values stay within expected bounds.&lt;/p&gt;&lt;p&gt;I have done none of that here. Just have fun flying 🙂&lt;/p&gt;&lt;head rend="h1"&gt;Limitations&lt;/head&gt;&lt;p&gt;The flight model defined in the textbook has several limitations.&lt;/p&gt;&lt;p&gt;The effects of alpha on the flight model is only modeled for the range [-10, 45]. Beta is only modeled for the range [-30, 30]. The flight model supports extrapolating data tables beyond their defined ranges, but the returned values will quickly become nonsensical. This means that if you manage to fly the F-16 beyond the provided ranges for alpha and beta, the flight model will break down and begin behaving non-physically.&lt;/p&gt;&lt;p&gt;In some cases, the aircraft will eventually return to controlled flight. But in other cases, one bad data value used to query the tables will cause increasingly bad data to be stored to the plane’s state. These bad values will quickly grow until the plane is thrown to infinity.&lt;/p&gt;&lt;p&gt;Hopefully, this is not possible when using FCS that I’ve written. But I encourage any readers to try breaking it themselves.&lt;/p&gt;&lt;p&gt;You can also turn off parts of the FCS using the config menu in the top left corner. This allows you to fly the plane completely manually, turn the engine off, or alter the center of gravity.&lt;/p&gt;&lt;p&gt;If the flight model doesn’t bug out from extreme values, then you can actually perform a backflip or a “Kulbit” maneuver with the F-16. I recommend turning off only the pitch axis FCS if you want to try that.&lt;/p&gt;&lt;p&gt;Another limitation is that flaps and slats are not defined in the flight model. The real F-16 uses a single control surface called a “flaperon” that works as both a flap and an aileron. When more lift is needed at low speeds, both flaperons deflect downwards like traditional flaps. Leading edge slats also deflect downwards to increase lift.&lt;/p&gt;&lt;p&gt;The textbook flight model only considers these control surfaces to be ailerons. That is, they always deflect in opposite directions in order to create a roll moment. Only a single “aileron” value is used to represent both left and right, so they cannot be used as flaps. If they were to be used as flaps, then there would need to be a left aileron and right aileron value and the Z axis force coefficient would depend on flaperon position.&lt;/p&gt;&lt;p&gt;The effect of slat position is blended into the existing tables, so there is some effect of slats on Z axis force. But the slat position cannot be animated on the plane’s 3D model since no variable for it exists.&lt;/p&gt;&lt;p&gt;This means that there are reduced high lift devices on the aircraft. The extra lift from flaps cannot be modeled. So the plane’s takeoff speed is much higher than you might expect from the F-16. The textbook only defines a model for flight, not for taxiing or takeoff. Landings feel quite bad because of this.&lt;/p&gt;&lt;p&gt;Another limitation is the lack of landing gear simulation. The landing gear is implemented exactly the same as the previous project: three capsule colliders. There is no simulation of wheel, tire, or suspension behavior. Again, this makes takeoff and landing feel kind of weird. But I have no idea how to write a system like that and it’s out of scope for this project anyways.🤷♂️&lt;/p&gt;&lt;p&gt;Another limitation of the flight model is the inaccuracy when flying super sonic. With real planes, lift and drag forces change drastically as you approach Mach 1. Air accelerates as it passes over the wing. Even while the plane remains subsonic, some parts of the air flow are forced to accelerate above Mach 1. When this air reaches supersonic speeds, shockwaves form over the wing which alters the way air flows around it.&lt;/p&gt;&lt;p&gt;This region, where some air is supersonic and some is not, is called the transonic region. This has a drastic effect on the aircraft’s performance and handling. In particular, the coefficient of drag increases, creating the “sound barrier” effect. The position of lift force on the wing changes, which will change how the plane handles.&lt;/p&gt;&lt;p&gt;None of these effects are included in the textbook’s flight model. These could be modeled by adding another input dimension to the force and moment tables. I suspect these were omitted to keep the flight model simple.&lt;/p&gt;&lt;p&gt;The practical effect is that the flight model only works up to about Mach 0.7. Above that, all of the forces on the aircraft become unrealistic. The behavior of the plane continues to increase smoothly with airspeed as if supersonic effects don’t exist.&lt;/p&gt;&lt;head rend="h1"&gt;Conclusion&lt;/head&gt;&lt;p&gt;I started this project after I got a job in the aerospace industry. The textbook was recommended by my manager, since I was working on real flight control systems. In a way, this article is a summary of everything I’ve learned about flying and software engineering in that job.&lt;/p&gt;&lt;p&gt;The way this F-16 flight model is implemented is very different from my previous project. It is actually close to how professional level sims are written, though simplified to fit in a textbook. Even so, there are still plenty of limitations in the flight model which means the simulation will behave unrealistically beyond the intended flight envelope.&lt;/p&gt;&lt;p&gt;The authors of the textbook based their flight model on a NASA paper12 which measured the aerodynamic properties of a scale model in a wind tunnel. The Nasa paper provides 50 lookup tables. The textbook simplified, approximated, and combined these into only 13 lookup tables.&lt;/p&gt;&lt;p&gt;With only a little more effort, you could write a simulator that uses many more tables to cover a larger flight envelope with more detail. The only limit is the data you have access to and your understanding of aerodynamics.&lt;/p&gt;&lt;p&gt;The FCS I’ve written is much simpler than the real FCS. Theoretically, it would be possible to write code that models the real F-16 FCS and apply it to this flight simulator. But how could you even get that information and who would be crazy enough to try that?&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;“Aircraft Control and Simulation” by Brian L. Stevens, Frank L. Lewis, and Eric N. Johnson ↩︎&lt;/item&gt;&lt;item&gt;https://vazgriz.com/346/flight-simulator-in-unity3d-part-1/ ↩︎&lt;/item&gt;&lt;item&gt;https://vazgriz.com/467/flight-simulator-in-unity3d-part-2/ ↩︎&lt;/item&gt;&lt;item&gt;https://vazgriz.com/503/creating-a-flight-simulator-in-unity3d-part-3/ ↩︎&lt;/item&gt;&lt;item&gt;https://twitter.com/FreyaHolmer/status/1325556229410861056 ↩︎&lt;/item&gt;&lt;item&gt;https://commons.wikimedia.org/wiki/File:Speyer_Handlog.jpg ↩︎&lt;/item&gt;&lt;item&gt;https://www.grc.nasa.gov/www/k-12/VirtualAero/BottleRocket/airplane/sound.html ↩︎&lt;/item&gt;&lt;item&gt;https://en.wikipedia.org/wiki/Bilinear_interpolation ↩︎&lt;/item&gt;&lt;item&gt;https://en.wikipedia.org/wiki/Trilinear_interpolation ↩︎&lt;/item&gt;&lt;item&gt;https://aerospaceweb.org/question/aerodynamics/q0194.shtml ↩︎&lt;/item&gt;&lt;item&gt;https://vazgriz.com/621/pid-controllers/ ↩︎&lt;/item&gt;&lt;item&gt;Simulator study of stall/post-stall characteristics of a fighter airplane with relaxed longitudinal static stability, Nyugen et al, 1979 (https://ntrs.nasa.gov/citations/19800005879) ↩︎&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://vazgriz.com/762/f-16-flight-sim-in-unity-3d/"/><published>2025-09-26T07:06:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45383851</id><title>Ode to Libraries from a Remote Worker</title><updated>2025-09-26T10:40:02.899670+00:00</updated><content>&lt;doc fingerprint="32141477ad8d23ee"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;ode to libraries (the book ones)&lt;/head&gt;
    &lt;p&gt;Most of the audience reading this post think of some random new programming tool dropped recently when the word library comes up. Let’s leave our déformation professionnelle in the door and think about the libraries. Book ones. This post is an ode to them.&lt;/p&gt;
    &lt;p&gt;I am not sure if libraries work the same way everywhere around the world but in my country they are completely free, you can sit for hours, borrow books, again, for free.&lt;/p&gt;
    &lt;p&gt;The idea of borrowing something from an institution without owing them anything in this time we’re living in amazes me. I mean who gives you free stuff (except free pain)? This fact itself is enough for me to think libraries are amazing. I consider myself a decent book reader and it’s been a long long time since I had to pay for a book.&lt;/p&gt;
    &lt;p&gt;But this isn’t the only think I like about my libraries.&lt;/p&gt;
    &lt;p&gt;So the story begins with me, a remote working software developer getting married to a remote working software developer.&lt;/p&gt;
    &lt;p&gt;My husband and I are a newly wed couple with all the fun and silliness but life (work) gets in the way naturally. Traditional couples see each other a few hours every day, and usually spend the weekends together to make up the lost time. We don’t have the lost time. We’re together, always. Working in the same room (our home office) resting in the same room etc. and god, we annoy each other. Love only goes so far, folks. Love is great for your daily life, for your home. But what if your home is also your work? And what if your work needs some limits on the PDA?&lt;/p&gt;
    &lt;p&gt;You can (ideally) leave your work stress and burden when you leave company building. However, when remote working, you’re not leaving work at all and even if unintentionally, that mood rubs off on your partner, who has the same tension from work.&lt;/p&gt;
    &lt;p&gt;I’ve spent all of my teen years and a couple of my twenties in student dormitories which I had to share my room with at least 4 other girls. That fact alone justifies that I love and need my personal space and time in my home. And my partner also needs that, even if he doesn’t say so, I know he needs. And we respect each other on this and try to leave the house for a couple hours every now and then. Again, any traditional couple may not need this kind of arrangement, one or both of them having to leave for work, but we’re not like the other couples.&lt;/p&gt;
    &lt;p&gt;It’s not always related to your partner / roommates / family. Even if you’re having enough personal space, enough distance from others, home office still can be life sucking.&lt;/p&gt;
    &lt;p&gt;I’ve spared the biggest and prettiest room of my house as the home office. I’m talking about wall-size windows, glittering sea in front of me, big sky and trees. There is a real view I can’t describe enough. (It’s a house that’s a few decades old, and we are just tenants so don’t think too much of me lol) And I’ve decorated the office pretty nice, spaced, refreshing. Even with that perks, being at home all day just consumes my soul. I can't focus after a couple of hours. Especially if I haven’t left the house all day, rolling out of bed straight to the desk, it becomes too draining. I need the concept of leaving home for work, and I need the feeling of “arriving home”. Getting cozy again after a long day, hiding from darkness of the outside world (work)&lt;/p&gt;
    &lt;p&gt;Being at home while working AND while resting, my brain just can’t switch off between work and rest, drawing a line between bad and good.&lt;/p&gt;
    &lt;p&gt;I’ve talked about my feelings, but no need to mention here, you’ve probably read all about health and psychological problems that remote work causes.&lt;/p&gt;
    &lt;p&gt;With all these said, it’s almost mandatory for me to adapt a hybrid work approach.&lt;/p&gt;
    &lt;p&gt;You may think, just go back to the office. No, no. This is not an option. First I don’t even live in the same city with my office building. Even if I did, it’s too late for me to adapt to office work. My work life started remote, even my internship was remote. I don't have office experience. I don’t know what am I going to do if I have to work in an office job in the future. Leaving your warm bed, rising before the sun in the morning (and I’m a morning person believe it or not), getting ready, commuting, traffic, thinking about what to eat in the office, commuting again, leaving you no time to live your life.. No. I don’t want to think about that possibility. It’s not for me.&lt;/p&gt;
    &lt;p&gt;Also having a home office is not bad at all. Somedays you just don’t want to leave the house, maybe you’re a bit sick, or just not feel like it. It’s good to have the option to stay in bed while working. In other words, stay in bed and get paid. I just need a context switch for a couple days in a week.&lt;/p&gt;
    &lt;p&gt;So what to do? Go to nearest coffee shop. You’re lucky if they don’t play tasteless trendy music. You’re lucky if a waiter doesn’t keep asking if you need anything, isn’t intrusive, and doesn’t subtly let you know when it’s time to leave by checking on you constantly . You’re lucky if no teenagers talking loudly about their-whatever-teens-talk-about-these-days. Oh, by the way, pay a lot of money to be here and to drink a nice cup of burnt coffe.&lt;/p&gt;
    &lt;p&gt;Working in the coffee shop might feel cool just for one day, just for the vibes. Not maintainable.&lt;/p&gt;
    &lt;p&gt;Then rent an office or subscribe to a co-working space? What? I get paid to work, not to pay for it.&lt;/p&gt;
    &lt;p&gt;And here comes the raison d’être of our ode, libraries.&lt;/p&gt;
    &lt;p&gt;Go to your local library. Set up your working tools. That’s it. No one will talk to you, if you’re in a relatively small one, chances are no one will be there. Just you, the nice vintage smell of the books surrounding all over, and a nice vibe.&lt;/p&gt;
    &lt;p&gt;You’re tired, want to take a break, you’ll walk around book shelves and you’ll see there are very odd books on very specific topics. Take it, or laugh at the title, free amusement. Share it on twitter, free likes. Find some hard-cover, old, brown page books. Smell them, feel good. Free joy.&lt;/p&gt;
    &lt;p&gt;Get back to working. Stay focused, finish your tasks without your bed luring you to itself. No distractions, no funny businesses.&lt;/p&gt;
    &lt;p&gt;I use a technique that maybe we can call laptop-driven-development: working until my laptop does not feel like it anymore. I don’t plug my laptop so my work has a natural deadline for the day. I need to stay focused and get things done, or I’m gonna have to get back home with leftover tasks.&lt;/p&gt;
    &lt;p&gt;Fall is on the way, maybe it’s raining outside. With big windows all around you, overthink your life while waiting for the rain to stop before heading back home..&lt;/p&gt;
    &lt;p&gt;This is my call to all the remote workers or students or any other people staying home too much: Go to your local library, enjoy it, support it. Cherish it.&lt;/p&gt;
    &lt;p&gt;Not just for working. Use it for thinking, for reconnecting, writing or creating. The library is a sanctuary where your mind slows down, even though the world outside keeps running. Maybe you’ve been postponing to think, to reflect, to spend time by yourself, like I did before I discovered my local library. You’ll find some kind of joy and productivity in there. We don’t have so many pure-good organizations in our world. We should appreciate them while we still have them.&lt;/p&gt;
    &lt;p&gt;I’ll admit that this post can be a little biased, because my local library isn’t exactly a popular spot. Most of the time, I’m the only one in a room, and there are only 3–4 people in the rest of the library. It’s just a 20-minute walk from my home, so I get to walk my daily steps in on the way there and back. And some of the rooms look like this:&lt;/p&gt;
    &lt;p&gt;Yes, I’m lucky on that. Yes, this is where I write this blog post.&lt;/p&gt;
    &lt;p&gt;This post has been discussed on Hacker News, you can join the conversation there.&lt;/p&gt;
    &lt;p&gt;Thanks for reading. If you have any feedback or would like to discuss further, I would be happy to hear from you.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://sibervepunk.com/ode-to-libraries/"/><published>2025-09-26T07:39:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45383960</id><title>The great sameness: a comic on how AI makes us more alike</title><updated>2025-09-26T10:40:02.724979+00:00</updated><content>&lt;doc fingerprint="98034b29d9372164"&gt;
  &lt;main&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Words&lt;/item&gt;
      &lt;item rend="dd-1"&gt;Liz Gorny&lt;/item&gt;
      &lt;item rend="dt-2"&gt;Comic&lt;/item&gt;
      &lt;item rend="dd-2"&gt;Jordon Bolton&lt;/item&gt;
      &lt;item rend="dt-3"&gt;—&lt;/item&gt;
      &lt;item rend="dt-3"&gt;Date&lt;/item&gt;
      &lt;item rend="dd-3"&gt;22 September 2025&lt;/item&gt;
      &lt;item rend="dt-4"&gt;Tags&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;The great sameness: a comic on how AI makes us more alike&lt;/head&gt;
    &lt;p&gt;Studies say that using generative AI flattens our collective creative individuality. But how does this happen? Comic artist Jordan Bolton shows us, so we might escape the same fate.&lt;/p&gt;
    &lt;p&gt;Share&lt;/p&gt;
    &lt;p&gt;Share&lt;/p&gt;
    &lt;p&gt;Light and Shade is a new series exploring the challenges at the heart of the AI-creative conversation. As AI becomes increasingly present across the creative industries, the series examines the opportunities and dilemmas our community grapples with. It is grounded in interviews with technologists, researchers, artists, designers, creative founders, writers, lecturers and environmental and computational experts, offering a fuller view of the many sides of the story of AI’s creative influence.&lt;/p&gt;
    &lt;p&gt;Déjà-vu is a familiar sensation in graphic design. There is always a popular template, plug-in or feature that becomes trendy, encouraging a flurry of similar-looking work, before the inevitable drop off. For years, these systems have allowed designers to make stylistic leaps quicker, to make sense of partially formed ideas faster. The resulting flatness is merely a side effect of multiple people harnessing the same tools, for the same steps in the design process. Not evil, but inevitable.&lt;/p&gt;
    &lt;p&gt;AI, like any tool in a designer’s belt which introduces automation, can also increase similarity. The artist, writer and technologist James Bridle likens this to the similarities in the appearance of buildings – a result of the default settings in the design software used by architects. But, unlike the tools designers and architects have used in the past, generative AI tends to nudge us further in the “correct direction”. In fact, it’s often described as a creative collaborator, feeding in personalised responses to your unique creative problem, making its voice just a little louder than your typical digital design tool.&lt;/p&gt;
    &lt;p&gt;We’ve seen in some initial research studies how this kind of relationship between AI and creative can increase homogenisation. In 2024, an online experiment with 36 ChatGPT users found that use of AI can expand how many creative ideas an individual user has, but at a group level, “users tended to produce less semantically distinct ideas”. That same year, another study reached similar results; this time it was a group of short-story writers who each managed to independently write a “more creative” story by using AI, but produced a narrower scope of novel content overall.&lt;/p&gt;
    &lt;p&gt;It’s odd: sameness has become a side effect of AI, though it needn’t be. While some tools currently used by designers or illustrators might offer only marginal variations, the range of inputs AI allows for are technically far greater. So long as the tool is steerable enough, and the designer has enough time to explore, AI should increase our capacity for creative expression, not stunt it. The question becomes, how do we ensure our dealings with AI expand our individual creative potential, without flattening our collective richness?&lt;/p&gt;
    &lt;p&gt;To improve our working relationship with AI, we must first understand how sameness happens. The comic below, created by Jordan Bolton, explores this topic through a single character who, under the weight of everyday pressures, comes face to face with the threat of a great homogenisation. Our hope is that it offers a pause; a chance to consider what tasks are worth templating, and when it’s best to trust our instincts.&lt;/p&gt;
    &lt;head rend="h2"&gt;Uncover the full Light and Shade series&lt;/head&gt;
    &lt;p&gt;Explore the challenges at the heart of the AI-creative conversation with our series of insights-driven articles below.&lt;/p&gt;
    &lt;head rend="h2"&gt;Brought to you by&lt;/head&gt;
    &lt;head rend="h3"&gt;Insights&lt;/head&gt;
    &lt;p&gt;Insights is a visual research department within It’s Nice That helping creative teams with sticking points. We deliver research on cultural landscapes, audience tastes, communities and talent to unlock your creative approach.&lt;/p&gt;
    &lt;head rend="h3"&gt;Share Article&lt;/head&gt;
    &lt;head rend="h3"&gt;Further Info&lt;/head&gt;
    &lt;head rend="h3"&gt;About the Author&lt;/head&gt;
    &lt;p&gt;—&lt;/p&gt;
    &lt;p&gt;Liz (she/they) is associate editor at Insights, a research-driven department within It's Nice That. They previously ran the news section of the website. Get in contact with them for potential Insights collaborations or to discuss Insights’ fortnightly column, POV.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.itsnicethat.com/features/the-great-sameness-light-and-shade-digital-220925"/><published>2025-09-26T07:59:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45384481</id><title>Pop OS is getting beta</title><updated>2025-09-26T10:40:02.297981+00:00</updated><content>&lt;doc fingerprint="d3f093e026c52dc0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Pop!_OS 24.04 LTS Beta Download&lt;/head&gt;
    &lt;head rend="h2"&gt;Pop!_OS is Getting Beta&lt;/head&gt;
    &lt;p&gt;Pop!_OS 24.04 LTS with the new COSMIC DE, developed by System76, is coming with many new features to explore and discover. Test out the beta as we fine-tune for release.&lt;/p&gt;
    &lt;head rend="h3"&gt;Upgrading from Pop!_OS 22.04 LTS to Pop!_OS 24.04 LTS Beta&lt;/head&gt;
    &lt;head rend="h3"&gt;Pop!_OS 24.04 LTS Beta&lt;/head&gt;
    &lt;p&gt;Filesize: GB&lt;/p&gt;
    &lt;head rend="h3"&gt;Pop!_OS 24.04 LTS Beta with NVIDIA&lt;/head&gt;
    &lt;p&gt;Filesize: GB&lt;/p&gt;
    &lt;head rend="h2"&gt;Release Notes&lt;/head&gt;
    &lt;p&gt;- Pop!_OS 24.04 LTS Beta includes the new COSMIC Desktop Environment designed and developed by System76. COSMIC DE is largely feature complete for the first release and development focus has turned to bug fixes for the final release. - This is a beta release and some bugs are expected. - On occasion, the installer does not start in a virtual machine. Press Super to activate the Launcher and search for "Installer". - Some GNOME apps are replaced by COSMIC apps - GNOME Files (Nautilus) &amp;gt; COSMIC Files - GNOME Terminal &amp;gt; COSMIC Terminal - GNOME Text Editor &amp;gt; COSMIC Text Editor - GNOME Media Player (Totem) &amp;gt; COSMIC Media Player - Pop!_Shop is replaced by COSMIC Store - Key components - COSMIC Epoch 1 Beta - Linux kernel 6.16.3 - Mesa 25.1.5-1 - NVIDIA Driver 580 - libwayland/libwayland-client 1.23.1-3 - libdrm 2.4.125-1 - Dragging and Dropping files from Wayland apps to X11 apps is not currently supported. For instance dragging files from COSMIC Files to Slack. Use the applications upload option as a work-around until the feature is added. - On distributions other than Pop!_OS, Firefox may need a configuration flag set to match COSMIC theming - Go to **```about:config```** and set **```widget.gtk.libadwaita-colors.enabled```** to **```false```** - Google Chrome based browsers - As of Google Chrome version 140, no configuration is necessary for Wayland - For versions prior to 140 and other Chrome based browsers that aren’t updated, setting the **```ozone-platform-hint```** is necessary. Go to chrome://flags in a tab, search for **```ozone-platform-hint```** and change the setting to “auto”. Restart the browser. - Gaming is working well but we expect to need more fixes in our xwayland implementation for the Release Candidate. - Some games may start partially off screen. Press F11 or Super+F11 to full screen the game (Goat Simulator is one example) - Display toggle hotkeys and an on-screen-display is not supported yet. - The “Accent hint” around windows doesn’t match the roundness style setting in Appearance. This is expected for at least COSMIC apps for the Release Candidate. - COSMIC has a built-in screenshot tool. If you require annotations, we recommend Flameshot which can be installed from Flathub via COSMIC Store. Version 13.1 or higher is required for COSMIC. - COSMIC Store doesn’t currently display Flatpak suggested addons for apps. This is planned for the Release Candidate. - Accessibility: The screen reader may not read all COSMIC apps widgets or may read them in an unintuitive direction. We’re working on screen reader flow and navigation for the Release Candidate. - Some application indicators do not appear in the Notification Tray applet. - Switching to an application using its application indicator does not currently work. - Printing support in COSMIC Text Editor is planned for the release candidate - Additional features and bugs expected to be fixed are triaged in the RC column on the [project board](https://github.com/orgs/pop-os/projects/23/views/1)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://system76.com/pop/pop-beta/"/><published>2025-09-26T09:20:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45384617</id><title>Show HN: The Little Notebook for Learning Linear Algebra with Python</title><updated>2025-09-26T10:40:00.061034+00:00</updated><content>&lt;doc fingerprint="951d5b6e170e15bc"&gt;
  &lt;main&gt;
    &lt;code&gt;import numpy as np&lt;/code&gt;
    &lt;head rend="h1"&gt;The LAB&lt;/head&gt;
    &lt;head rend="h2"&gt;Chapter 1. Vectors, scalars, and geometry&lt;/head&gt;
    &lt;head rend="h3"&gt;1. Scalars, Vectors, and Coordinate Systems&lt;/head&gt;
    &lt;p&gt;Let’s get our hands dirty! This lab is about playing with the building blocks of linear algebra: scalars and vectors. Think of a scalar as just a plain number, like &lt;code&gt;3&lt;/code&gt; or &lt;code&gt;-1.5&lt;/code&gt;. A vector is a small list of numbers, which you can picture as an arrow in space.&lt;/p&gt;
    &lt;p&gt;We’ll use Python (with NumPy) to explore them. Don’t worry if this is your first time with NumPy - we’ll go slowly.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;p&gt;That’s it - we’re ready! NumPy is the main tool we’ll use for linear algebra.&lt;/p&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;p&gt;Scalars are just numbers.&lt;/p&gt;
    &lt;code&gt;= 5       # a scalar
 a = -2.5    # another scalar
 b 
print(a + b)   # add them
print(a * b)   # multiply them&lt;/code&gt;
    &lt;code&gt;2.5
-12.5&lt;/code&gt;
    &lt;p&gt;Vectors are lists of numbers.&lt;/p&gt;
    &lt;code&gt;= np.array([2, 3])      # a vector in 2D
 v = np.array([1, -1, 4])  # a vector in 3D
 w 
print(v)
print(w)&lt;/code&gt;
    &lt;code&gt;[2 3]
[ 1 -1  4]&lt;/code&gt;
    &lt;p&gt;Coordinates tell us where we are. Think of &lt;code&gt;[2, 3]&lt;/code&gt; as “go 2 steps in the x-direction, 3 steps in the y-direction.”&lt;/p&gt;
    &lt;p&gt;We can even draw it:&lt;/p&gt;
    &lt;code&gt;import matplotlib.pyplot as plt

# plot vector v
0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='r')
 plt.quiver(0, 4)
 plt.xlim(0, 4)
 plt.ylim(
 plt.grid() plt.show()&lt;/code&gt;
    &lt;p&gt;This makes a little arrow from the origin &lt;code&gt;(0,0)&lt;/code&gt; to &lt;code&gt;(2,3)&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Change the vector &lt;code&gt;v&lt;/code&gt;to&lt;code&gt;[4, 1]&lt;/code&gt;. Where does the arrow point now?&lt;/item&gt;
      &lt;item&gt;Try making a 3D vector with 4 numbers, like &lt;code&gt;[1, 2, 3, 4]&lt;/code&gt;. What happens?&lt;/item&gt;
      &lt;item&gt;Replace &lt;code&gt;np.array([2,3])&lt;/code&gt;with&lt;code&gt;np.array([0,0])&lt;/code&gt;. What does the arrow look like?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;2. Vector Notation, Components, and Arrows&lt;/head&gt;
    &lt;p&gt;In this lab, we’ll practice reading, writing, and visualizing vectors in different ways. A vector can look simple at first - just a list of numbers - but how we write it and how we interpret it really matters. This is where notation and components come into play.&lt;/p&gt;
    &lt;p&gt;A vector has:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A symbol (we might call it &lt;code&gt;v&lt;/code&gt;,&lt;code&gt;w&lt;/code&gt;, or even&lt;code&gt;→AB&lt;/code&gt;in geometry).&lt;/item&gt;
      &lt;item&gt;Components (the individual numbers, like &lt;code&gt;2&lt;/code&gt;and&lt;code&gt;3&lt;/code&gt;in&lt;code&gt;[2, 3]&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;An arrow picture (a geometric way to see the vector as a directed line segment).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s see all three in action with Python.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
import matplotlib.pyplot as plt&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Writing vectors in Python&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Two-dimensional vector
= np.array([2, 3])
 v 
# Three-dimensional vector
= np.array([1, -1, 4])
 w 
print("v =", v)
print("w =", w)&lt;/code&gt;
    &lt;code&gt;v = [2 3]
w = [ 1 -1  4]&lt;/code&gt;
    &lt;p&gt;Here &lt;code&gt;v&lt;/code&gt; has components &lt;code&gt;(2, 3)&lt;/code&gt; and &lt;code&gt;w&lt;/code&gt; has components &lt;code&gt;(1, -1, 4)&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Accessing components Each number in the vector is a component. We can pick them out using indexing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;print("First component of v:", v[0])
print("Second component of v:", v[1])&lt;/code&gt;
    &lt;code&gt;First component of v: 2
Second component of v: 3&lt;/code&gt;
    &lt;p&gt;Notice: in Python, indices start at &lt;code&gt;0&lt;/code&gt;, so &lt;code&gt;v[0]&lt;/code&gt; is the first component.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visualizing vectors as arrows In 2D, it’s easy to draw a vector from the origin &lt;code&gt;(0,0)&lt;/code&gt;to its endpoint&lt;code&gt;(x,y)&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='r')
 plt.quiver(-1, 4)
 plt.xlim(-2, 4)
 plt.ylim(0, color='black', linewidth=0.5)
 plt.axhline(0, color='black', linewidth=0.5)
 plt.axvline(
 plt.grid() plt.show()&lt;/code&gt;
    &lt;p&gt;This shows vector v as a red arrow from &lt;code&gt;(0,0)&lt;/code&gt; to &lt;code&gt;(2,3)&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Drawing multiple vectors We can plot several arrows at once to compare them.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([3, 1])
 u = np.array([-1, 2])
 z 
# Draw v, u, z in different colors
0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='r', label='v')
 plt.quiver(0, 0, u[0], u[1], angles='xy', scale_units='xy', scale=1, color='b', label='u')
 plt.quiver(0, 0, z[0], z[1], angles='xy', scale_units='xy', scale=1, color='g', label='z')
 plt.quiver(
-2, 4)
 plt.xlim(-2, 4)
 plt.ylim(0, color='black', linewidth=0.5)
 plt.axhline(0, color='black', linewidth=0.5)
 plt.axvline(
 plt.grid() plt.show()&lt;/code&gt;
    &lt;p&gt;Now you’ll see three arrows starting at the same point, each pointing in a different direction.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Change &lt;code&gt;v&lt;/code&gt;to&lt;code&gt;[5, 0]&lt;/code&gt;. What does the arrow look like now?&lt;/item&gt;
      &lt;item&gt;Try a vector like &lt;code&gt;[0, -3]&lt;/code&gt;. Which axis does it line up with?&lt;/item&gt;
      &lt;item&gt;Make a new vector &lt;code&gt;q = np.array([2, 0, 0])&lt;/code&gt;. What happens if you try to plot it with&lt;code&gt;plt.quiver&lt;/code&gt;in 2D?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;3. Vector Addition and Scalar Multiplication&lt;/head&gt;
    &lt;p&gt;In this lab, we’ll explore the two most fundamental operations you can perform with vectors: adding them together and scaling them by a number (a scalar). These operations form the basis of everything else in linear algebra, from geometry to machine learning. Understanding how they work, both in code and visually, is key to building intuition.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
import matplotlib.pyplot as plt&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Vector addition When you add two vectors, you simply add their components one by one.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([2, 3])
 v = np.array([1, -1])
 u 
= v + u
 sum_vector print("v + u =", sum_vector)&lt;/code&gt;
    &lt;code&gt;v + u = [3 2]&lt;/code&gt;
    &lt;p&gt;Here, &lt;code&gt;(2,3) + (1,-1) = (3,2)&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visualizing vector addition (tip-to-tail method) Graphically, vector addition means placing the tail of one vector at the head of the other. The resulting vector goes from the start of the first to the end of the second.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='r', label='v')
 plt.quiver(0], v[1], u[0], u[1], angles='xy', scale_units='xy', scale=1, color='b', label='u placed at end of v')
 plt.quiver(v[0, 0, sum_vector[0], sum_vector[1], angles='xy', scale_units='xy', scale=1, color='g', label='v + u')
 plt.quiver(
-1, 5)
 plt.xlim(-2, 5)
 plt.ylim(0, color='black', linewidth=0.5)
 plt.axhline(0, color='black', linewidth=0.5)
 plt.axvline(
 plt.grid() plt.show()&lt;/code&gt;
    &lt;p&gt;The green arrow is the result of adding &lt;code&gt;v&lt;/code&gt; and &lt;code&gt;u&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Scalar multiplication Multiplying a vector by a scalar stretches or shrinks it. If the scalar is negative, the vector flips direction.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= 2
 c = c * v
 scaled_v print("2 * v =", scaled_v)

= -1
 d = d * v
 scaled_v_neg print("-1 * v =", scaled_v_neg)&lt;/code&gt;
    &lt;code&gt;2 * v = [4 6]
-1 * v = [-2 -3]&lt;/code&gt;
    &lt;p&gt;So &lt;code&gt;2 * (2,3) = (4,6)&lt;/code&gt; and &lt;code&gt;-1 * (2,3) = (-2,-3)&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visualizing scalar multiplication&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='r', label='v')
 plt.quiver(0, 0, scaled_v[0], scaled_v[1], angles='xy', scale_units='xy', scale=1, color='b', label='2 * v')
 plt.quiver(0, 0, scaled_v_neg[0], scaled_v_neg[1], angles='xy', scale_units='xy', scale=1, color='g', label='-1 * v')
 plt.quiver(
-5, 5)
 plt.xlim(-5, 7)
 plt.ylim(0, color='black', linewidth=0.5)
 plt.axhline(0, color='black', linewidth=0.5)
 plt.axvline(
 plt.grid() plt.show()&lt;/code&gt;
    &lt;p&gt;Here, the blue arrow is twice as long as the red arrow, while the green arrow points in the opposite direction.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Combining both operations We can scale vectors and then add them. This is called a linear combination (and it’s the foundation for the next section).&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= 3*v + (-2)*u
 combo print("3*v - 2*u =", combo)&lt;/code&gt;
    &lt;code&gt;3*v - 2*u = [ 4 11]&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Replace &lt;code&gt;c = 2&lt;/code&gt;with&lt;code&gt;c = 0.5&lt;/code&gt;. What happens to the vector?&lt;/item&gt;
      &lt;item&gt;Try adding three vectors: &lt;code&gt;v + u + np.array([-1,2])&lt;/code&gt;. Can you predict the result before printing?&lt;/item&gt;
      &lt;item&gt;Visualize &lt;code&gt;3*v + 2*u&lt;/code&gt;using arrows. How does it compare to just&lt;code&gt;v + u&lt;/code&gt;?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;4. Linear Combinations and Span&lt;/head&gt;
    &lt;p&gt;Now that we know how to add vectors and scale them, we can combine these two moves to create linear combinations. A linear combination is just a recipe: multiply vectors by scalars, then add them together. The set of all possible results you can get from such recipes is called the span.&lt;/p&gt;
    &lt;p&gt;This idea is powerful because span tells us what directions and regions of space we can reach using given vectors.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
import matplotlib.pyplot as plt&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Linear combinations in Python&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([2, 1])
 v = np.array([1, 3])
 u 
= 2*v + 3*u
 combo1 = -1*v + 4*u
 combo2 
print("2*v + 3*u =", combo1)
print("-v + 4*u =", combo2)&lt;/code&gt;
    &lt;code&gt;2*v + 3*u = [ 7 11]
-v + 4*u = [ 2 11]&lt;/code&gt;
    &lt;p&gt;Here, we multiplied and added vectors using scalars. Each result is a new vector.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visualizing linear combinations Let’s plot &lt;code&gt;v&lt;/code&gt;,&lt;code&gt;u&lt;/code&gt;, and their combinations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='r', label='v')
 plt.quiver(0, 0, u[0], u[1], angles='xy', scale_units='xy', scale=1, color='b', label='u')
 plt.quiver(0, 0, combo1[0], combo1[1], angles='xy', scale_units='xy', scale=1, color='g', label='2v + 3u')
 plt.quiver(0, 0, combo2[0], combo2[1], angles='xy', scale_units='xy', scale=1, color='m', label='-v + 4u')
 plt.quiver(
-5, 10)
 plt.xlim(-5, 10)
 plt.ylim(0, color='black', linewidth=0.5)
 plt.axhline(0, color='black', linewidth=0.5)
 plt.axvline(
 plt.grid() plt.show()&lt;/code&gt;
    &lt;p&gt;This shows how new arrows can be generated from scaling and adding the original ones.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Exploring the span The span of two 2D vectors is either:&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A line (if one is a multiple of the other).&lt;/item&gt;
      &lt;item&gt;The whole 2D plane (if they are independent).&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Generate many combinations
= range(-5, 6)
 coeffs = []
 points for a in coeffs:
for b in coeffs:
     = a*v + b*u
         point 
         points.append(point)
= np.array(points)
 points 
0], points[:,1], s=10, color='gray')
 plt.scatter(points[:,0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='r')
 plt.quiver(0, 0, u[0], u[1], angles='xy', scale_units='xy', scale=1, color='b')
 plt.quiver(
-10, 10)
 plt.xlim(-10, 10)
 plt.ylim(0, color='black', linewidth=0.5)
 plt.axhline(0, color='black', linewidth=0.5)
 plt.axvline(
 plt.grid() plt.show()&lt;/code&gt;
    &lt;p&gt;The gray dots show all reachable points with combinations of &lt;code&gt;v&lt;/code&gt; and &lt;code&gt;u&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Special case: dependent vectors&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([4, 2])  # notice w = 2*v
 w = range(-5, 6)
 coeffs = []
 points for a in coeffs:
for b in coeffs:
     *v + b*w)
         points.append(a
= np.array(points)
 points 
0], points[:,1], s=10, color='gray')
 plt.scatter(points[:,0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='r')
 plt.quiver(0, 0, w[0], w[1], angles='xy', scale_units='xy', scale=1, color='b')
 plt.quiver(
-10, 10)
 plt.xlim(-10, 10)
 plt.ylim(0, color='black', linewidth=0.5)
 plt.axhline(0, color='black', linewidth=0.5)
 plt.axvline(
 plt.grid() plt.show()&lt;/code&gt;
    &lt;p&gt;Here, the span collapses to a line because &lt;code&gt;w&lt;/code&gt; is just a scaled copy of &lt;code&gt;v&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Replace &lt;code&gt;u = [1,3]&lt;/code&gt;with&lt;code&gt;u = [-1,2]&lt;/code&gt;. What does the span look like?&lt;/item&gt;
      &lt;item&gt;Try three vectors in 2D (e.g., &lt;code&gt;v, u, w&lt;/code&gt;). Do you get more than the whole plane?&lt;/item&gt;
      &lt;item&gt;Experiment with 3D vectors. Use &lt;code&gt;np.array([x,y,z])&lt;/code&gt;and check whether different vectors span a plane or all of space.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;5. Length (Norm) and Distance&lt;/head&gt;
    &lt;p&gt;In this lab, we’ll measure how big a vector is (its length, also called its norm) and how far apart two vectors are (their distance). These ideas connect algebra to geometry: when we compute a norm, we’re measuring the size of an arrow; when we compute a distance, we’re measuring the gap between two points in space.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
import matplotlib.pyplot as plt&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Vector length (norm) in 2D The length of a vector is computed using the Pythagorean theorem. For a vector &lt;code&gt;(x, y)&lt;/code&gt;, the length is&lt;code&gt;sqrt(x² + y²)&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([3, 4])
 v = np.linalg.norm(v)
 length print("Length of v =", length)&lt;/code&gt;
    &lt;code&gt;Length of v = 5.0&lt;/code&gt;
    &lt;p&gt;This prints &lt;code&gt;5.0&lt;/code&gt;, because &lt;code&gt;(3,4)&lt;/code&gt; forms a right triangle with sides 3 and 4, and &lt;code&gt;sqrt(3²+4²)=5&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Manual calculation vs NumPy&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= (v[0]**2 + v[1]**2)**0.5
 manual_length print("Manual length =", manual_length)
print("NumPy length =", np.linalg.norm(v))&lt;/code&gt;
    &lt;code&gt;Manual length = 5.0
NumPy length = 5.0&lt;/code&gt;
    &lt;p&gt;Both give the same result.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visualizing vector length&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='r')
 plt.quiver(0, 5)
 plt.xlim(0, 5)
 plt.ylim(0, color='black', linewidth=0.5)
 plt.axhline(0, color='black', linewidth=0.5)
 plt.axvline(0]/2, v[1]/2, f"Length={length}", fontsize=10, color='blue')
 plt.text(v[
 plt.grid() plt.show()&lt;/code&gt;
    &lt;p&gt;You’ll see the arrow &lt;code&gt;(3,4)&lt;/code&gt; with its length labeled.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Distance between two vectors The distance between &lt;code&gt;v&lt;/code&gt;and another vector&lt;code&gt;u&lt;/code&gt;is the length of their difference:&lt;code&gt;‖v - u‖&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([0, 0])   # the origin
 u = np.linalg.norm(v - u)
 dist print("Distance between v and u =", dist)&lt;/code&gt;
    &lt;code&gt;Distance between v and u = 5.0&lt;/code&gt;
    &lt;p&gt;Since &lt;code&gt;u&lt;/code&gt; is the origin, this is just the length of &lt;code&gt;v&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A more interesting distance&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([1, 1])
 u = np.linalg.norm(v - u)
 dist print("Distance between v and u =", dist)&lt;/code&gt;
    &lt;code&gt;Distance between v and u = 3.605551275463989&lt;/code&gt;
    &lt;p&gt;This measures how far &lt;code&gt;(3,4)&lt;/code&gt; is from &lt;code&gt;(1,1)&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visualizing distance between points&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;0], u[0]], [v[1], u[1]], color=['red','blue'])
 plt.scatter([v[0], u[0]], [v[1], u[1]], 'k--')
 plt.plot([v[0], v[1], 'v', fontsize=12, color='red')
 plt.text(v[0], u[1], 'u', fontsize=12, color='blue')
 plt.text(u[
 plt.grid() plt.show()&lt;/code&gt;
    &lt;p&gt;The dashed line shows the distance between the two points.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Higher-dimensional vectors Norms and distances work the same way in any dimension:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([1,2,3])
 a = np.array([4,0,8])
 b print("‖a‖ =", np.linalg.norm(a))
print("‖b‖ =", np.linalg.norm(b))
print("Distance between a and b =", np.linalg.norm(a-b))&lt;/code&gt;
    &lt;code&gt;‖a‖ = 3.7416573867739413
‖b‖ = 8.94427190999916
Distance between a and b = 6.164414002968976&lt;/code&gt;
    &lt;p&gt;Even though we can’t draw 3D easily on paper, the formulas still apply.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute the length of &lt;code&gt;np.array([5,12])&lt;/code&gt;. What do you expect?&lt;/item&gt;
      &lt;item&gt;Find the distance between &lt;code&gt;(2,3)&lt;/code&gt;and&lt;code&gt;(7,7)&lt;/code&gt;. Can you sketch it by hand and check?&lt;/item&gt;
      &lt;item&gt;In 3D, try vectors &lt;code&gt;(1,1,1)&lt;/code&gt;and&lt;code&gt;(2,2,2)&lt;/code&gt;. Why is the distance exactly&lt;code&gt;sqrt(3)&lt;/code&gt;?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;6. Dot Product&lt;/head&gt;
    &lt;p&gt;The dot product is one of the most important operations in linear algebra. It takes two vectors and gives you a single number. That number combines both the lengths of the vectors and how much they point in the same direction. In this lab, we’ll calculate dot products in several ways, see how they relate to geometry, and visualize their meaning.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
import matplotlib.pyplot as plt&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Algebraic definition The dot product of two vectors is the sum of the products of their components:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([2, 3])
 v = np.array([4, -1])
 u 
= v[0]*u[0] + v[1]*u[1]
 dot_manual = np.dot(v, u)
 dot_numpy 
print("Manual dot product:", dot_manual)
print("NumPy dot product:", dot_numpy)&lt;/code&gt;
    &lt;code&gt;Manual dot product: 5
NumPy dot product: 5&lt;/code&gt;
    &lt;p&gt;Here, &lt;code&gt;(2*4) + (3*-1) = 8 - 3 = 5&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Geometric definition The dot product also equals the product of the lengths of the vectors times the cosine of the angle between them:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ v \cdot u = \|v\| \|u\| \cos \theta \]&lt;/p&gt;
    &lt;p&gt;We can compute the angle:&lt;/p&gt;
    &lt;code&gt;= np.linalg.norm(v)
 norm_v = np.linalg.norm(u)
 norm_u 
= np.dot(v, u) / (norm_v * norm_u)
 cos_theta = np.arccos(cos_theta)
 theta 
print("cos(theta) =", cos_theta)
print("theta (in radians) =", theta)
print("theta (in degrees) =", np.degrees(theta))&lt;/code&gt;
    &lt;code&gt;cos(theta) = 0.33633639699815626
theta (in radians) = 1.2277723863741932
theta (in degrees) = 70.3461759419467&lt;/code&gt;
    &lt;p&gt;This gives the angle between &lt;code&gt;v&lt;/code&gt; and &lt;code&gt;u&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visualizing the dot product Let’s draw the two vectors:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;0,0,v[0],v[1],angles='xy',scale_units='xy',scale=1,color='r',label='v')
 plt.quiver(0,0,u[0],u[1],angles='xy',scale_units='xy',scale=1,color='b',label='u')
 plt.quiver(-1,5)
 plt.xlim(-2,4)
 plt.ylim(0,color='black',linewidth=0.5)
 plt.axhline(0,color='black',linewidth=0.5)
 plt.axvline(
 plt.grid() plt.show()&lt;/code&gt;
    &lt;p&gt;The dot product is positive if the angle is less than 90°, negative if greater than 90°, and zero if the vectors are perpendicular.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Projections and dot product The dot product lets us compute how much of one vector lies in the direction of another.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.dot(v, u) / np.linalg.norm(u)
 proj_length print("Projection length of v onto u:", proj_length)&lt;/code&gt;
    &lt;code&gt;Projection length of v onto u: 1.212678125181665&lt;/code&gt;
    &lt;p&gt;This is the length of the shadow of &lt;code&gt;v&lt;/code&gt; onto &lt;code&gt;u&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Special cases&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If vectors point in the same direction, the dot product is large and positive.&lt;/item&gt;
      &lt;item&gt;If vectors are perpendicular, the dot product is zero.&lt;/item&gt;
      &lt;item&gt;If vectors point in opposite directions, the dot product is negative.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([1,0])
 a = np.array([0,1])
 b = np.array([-1,0])
 c 
print("a · b =", np.dot(a,b))   # perpendicular
print("a · a =", np.dot(a,a))   # length squared
print("a · c =", np.dot(a,c))   # opposite&lt;/code&gt;
    &lt;code&gt;a · b = 0
a · a = 1
a · c = -1&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute the dot product of &lt;code&gt;(3,4)&lt;/code&gt;with&lt;code&gt;(4,3)&lt;/code&gt;. Is the result larger or smaller than the product of their lengths?&lt;/item&gt;
      &lt;item&gt;Try &lt;code&gt;(1,2,3) · (4,5,6)&lt;/code&gt;. Does the geometric meaning still work in 3D?&lt;/item&gt;
      &lt;item&gt;Create two perpendicular vectors (e.g. &lt;code&gt;(2,0)&lt;/code&gt;and&lt;code&gt;(0,5)&lt;/code&gt;). Verify the dot product is zero.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;7. Angles Between Vectors and Cosine&lt;/head&gt;
    &lt;p&gt;In this lab, we’ll go deeper into the connection between vectors and geometry by calculating angles. Angles tell us how much two vectors “point in the same direction.” The bridge between algebra and geometry here is the cosine formula, which comes directly from the dot product.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
import matplotlib.pyplot as plt&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Formula for the angle The angle \(\theta\) between two vectors \(v\) and \(u\) is given by:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ \cos \theta = \frac{v \cdot u}{\|v\| \, \|u\|} \]&lt;/p&gt;
    &lt;p&gt;This means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If \(\cos \theta = 1\), the vectors point in exactly the same direction.&lt;/item&gt;
      &lt;item&gt;If \(\cos \theta = 0\), they are perpendicular.&lt;/item&gt;
      &lt;item&gt;If \(\cos \theta = -1\), they point in opposite directions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Computing the angle in Python&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([2, 3])
 v = np.array([3, -1])
 u 
= np.dot(v, u)
 dot = np.linalg.norm(v)
 norm_v = np.linalg.norm(u)
 norm_u 
= dot / (norm_v * norm_u)
 cos_theta = np.arccos(cos_theta)
 theta 
print("cos(theta) =", cos_theta)
print("theta in radians =", theta)
print("theta in degrees =", np.degrees(theta))&lt;/code&gt;
    &lt;code&gt;cos(theta) = 0.2631174057921088
theta in radians = 1.3045442776439713
theta in degrees = 74.74488129694222&lt;/code&gt;
    &lt;p&gt;This gives both the cosine value and the actual angle.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visualizing the vectors&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;0,0,v[0],v[1],angles='xy',scale_units='xy',scale=1,color='r',label='v')
 plt.quiver(0,0,u[0],u[1],angles='xy',scale_units='xy',scale=1,color='b',label='u')
 plt.quiver(
-1,4)
 plt.xlim(-2,4)
 plt.ylim(0,color='black',linewidth=0.5)
 plt.axhline(0,color='black',linewidth=0.5)
 plt.axvline(
 plt.grid() plt.show()&lt;/code&gt;
    &lt;p&gt;You can see the angle between &lt;code&gt;v&lt;/code&gt; and &lt;code&gt;u&lt;/code&gt; as the gap between the red and blue arrows.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Checking special cases&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([1,0])
 a = np.array([0,1])
 b = np.array([-1,0])
 c 
print("Angle between a and b =", np.degrees(np.arccos(np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b)))))
print("Angle between a and c =", np.degrees(np.arccos(np.dot(a,c)/(np.linalg.norm(a)*np.linalg.norm(c)))))&lt;/code&gt;
    &lt;code&gt;Angle between a and b = 90.0
Angle between a and c = 180.0&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Angle between &lt;code&gt;(1,0)&lt;/code&gt;and&lt;code&gt;(0,1)&lt;/code&gt;is 90°.&lt;/item&gt;
      &lt;item&gt;Angle between &lt;code&gt;(1,0)&lt;/code&gt;and&lt;code&gt;(-1,0)&lt;/code&gt;is 180°.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Using cosine as a similarity measure In data science and machine learning, people often use cosine similarity instead of raw angles. It’s just the cosine value itself:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.dot(v,u)/(np.linalg.norm(v)*np.linalg.norm(u))
 cosine_similarity print("Cosine similarity =", cosine_similarity)&lt;/code&gt;
    &lt;code&gt;Cosine similarity = 0.2631174057921088&lt;/code&gt;
    &lt;p&gt;Values close to &lt;code&gt;1&lt;/code&gt; mean vectors are aligned, values near &lt;code&gt;0&lt;/code&gt; mean unrelated, and values near &lt;code&gt;-1&lt;/code&gt; mean opposite.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Create two random vectors with &lt;code&gt;np.random.randn(3)&lt;/code&gt;and compute the angle between them.&lt;/item&gt;
      &lt;item&gt;Verify that swapping the vectors gives the same angle (symmetry).&lt;/item&gt;
      &lt;item&gt;Find two vectors where cosine similarity is exactly &lt;code&gt;0&lt;/code&gt;. Can you come up with an example in 2D?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;8. Projections and Decompositions&lt;/head&gt;
    &lt;p&gt;In this lab, we’ll learn how to split one vector into parts: one part that lies along another vector, and one part that is perpendicular. This process is called projection and decomposition. Projections let us measure “how much of a vector points in a given direction,” and decompositions give us a way to break vectors into useful components.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
import matplotlib.pyplot as plt&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Projection formula The projection of vector \(v\) onto vector \(u\) is:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ \text{proj}_u(v) = \frac{v \cdot u}{u \cdot u} \, u \]&lt;/p&gt;
    &lt;p&gt;This gives the component of \(v\) that points in the direction of \(u\).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Computing projection in Python&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([3, 2])
 v = np.array([2, 0])
 u 
= (np.dot(v, u) / np.dot(u, u)) * u
 proj_u_v print("Projection of v onto u:", proj_u_v)&lt;/code&gt;
    &lt;code&gt;Projection of v onto u: [3. 0.]&lt;/code&gt;
    &lt;p&gt;Here, \(v = (3,2)\) and \(u = (2,0)\). The projection of &lt;code&gt;v&lt;/code&gt; onto &lt;code&gt;u&lt;/code&gt; is a vector pointing along the x-axis.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Decomposing into parallel and perpendicular parts&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can write:&lt;/p&gt;
    &lt;p&gt;\[ v = \text{proj}_u(v) + (v - \text{proj}_u(v)) \]&lt;/p&gt;
    &lt;p&gt;The first part is parallel to &lt;code&gt;u&lt;/code&gt;, the second part is perpendicular.&lt;/p&gt;
    &lt;code&gt;= v - proj_u_v
 perp print("Parallel part:", proj_u_v)
print("Perpendicular part:", perp)&lt;/code&gt;
    &lt;code&gt;Parallel part: [3. 0.]
Perpendicular part: [0. 2.]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visualizing projection and decomposition&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='r', label='v')
 plt.quiver(0, 0, u[0], u[1], angles='xy', scale_units='xy', scale=1, color='b', label='u')
 plt.quiver(0, 0, proj_u_v[0], proj_u_v[1], angles='xy', scale_units='xy', scale=1, color='g', label='proj_u(v)')
 plt.quiver(0], proj_u_v[1], perp[0], perp[1], angles='xy', scale_units='xy', scale=1, color='m', label='perpendicular')
 plt.quiver(proj_u_v[
-1, 5)
 plt.xlim(-1, 4)
 plt.ylim(0, color='black', linewidth=0.5)
 plt.axhline(0, color='black', linewidth=0.5)
 plt.axvline(
 plt.grid() plt.show()&lt;/code&gt;
    &lt;p&gt;You’ll see &lt;code&gt;v&lt;/code&gt; (red), &lt;code&gt;u&lt;/code&gt; (blue), the projection (green), and the perpendicular remainder (magenta).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Projection in higher dimensions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This formula works in any dimension:&lt;/p&gt;
    &lt;code&gt;= np.array([1,2,3])
 a = np.array([0,1,0])
 b 
= (np.dot(a,b)/np.dot(b,b)) * b
 proj = a - proj
 perp 
print("Projection of a onto b:", proj)
print("Perpendicular component:", perp)&lt;/code&gt;
    &lt;code&gt;Projection of a onto b: [0. 2. 0.]
Perpendicular component: [1. 0. 3.]&lt;/code&gt;
    &lt;p&gt;Even in 3D or higher, projections are about splitting into “along” and “across.”&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Try projecting &lt;code&gt;(2,3)&lt;/code&gt;onto&lt;code&gt;(0,5)&lt;/code&gt;. Where does it land?&lt;/item&gt;
      &lt;item&gt;Take a 3D vector like &lt;code&gt;(4,2,6)&lt;/code&gt;and project it onto&lt;code&gt;(1,0,0)&lt;/code&gt;. What does this give you?&lt;/item&gt;
      &lt;item&gt;Change the base vector &lt;code&gt;u&lt;/code&gt;to something not aligned with the axes, like&lt;code&gt;(1,1)&lt;/code&gt;. Does the projection still work?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;9. Cauchy–Schwarz and Triangle Inequalities&lt;/head&gt;
    &lt;p&gt;This lab introduces two fundamental inequalities in linear algebra. They may look abstract at first, but they provide guarantees that always hold true for vectors. We’ll explore them with small examples in Python to see why they matter.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Cauchy–Schwarz inequality&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The inequality states:&lt;/p&gt;
    &lt;p&gt;\[ |v \cdot u| \leq \|v\| \, \|u\| \]&lt;/p&gt;
    &lt;p&gt;It means the dot product is never “bigger” than the product of the vector lengths. Equality happens only if the two vectors are pointing in exactly the same (or opposite) direction.&lt;/p&gt;
    &lt;code&gt;= np.array([3, 4])
 v = np.array([1, 2])
 u 
= abs(np.dot(v, u))
 lhs = np.linalg.norm(v) * np.linalg.norm(u)
 rhs 
print("Left-hand side (|v·u|):", lhs)
print("Right-hand side (‖v‖‖u‖):", rhs)
print("Inequality holds?", lhs &amp;lt;= rhs)&lt;/code&gt;
    &lt;code&gt;Left-hand side (|v·u|): 11
Right-hand side (‖v‖‖u‖): 11.180339887498949
Inequality holds? True&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Testing Cauchy–Schwarz with different vectors&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= [
 pairs 1,0]), np.array([0,1])),  # perpendicular
     (np.array([2,3]), np.array([4,6])),  # multiples
     (np.array([-1,2]), np.array([3,-6])) # opposite multiples
     (np.array([
 ]
for v,u in pairs:
= abs(np.dot(v, u))
     lhs = np.linalg.norm(v) * np.linalg.norm(u)
     rhs print(f"v={v}, u={u} -&amp;gt; |v·u|={lhs}, ‖v‖‖u‖={rhs}, holds={lhs&amp;lt;=rhs}")     &lt;/code&gt;
    &lt;code&gt;v=[1 0], u=[0 1] -&amp;gt; |v·u|=0, ‖v‖‖u‖=1.0, holds=True
v=[2 3], u=[4 6] -&amp;gt; |v·u|=26, ‖v‖‖u‖=25.999999999999996, holds=False
v=[-1  2], u=[ 3 -6] -&amp;gt; |v·u|=15, ‖v‖‖u‖=15.000000000000002, holds=True&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Perpendicular vectors give &lt;code&gt;|v·u| = 0&lt;/code&gt;, far less than the product of norms.&lt;/item&gt;
      &lt;item&gt;Multiples give equality (&lt;code&gt;lhs = rhs&lt;/code&gt;).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Triangle inequality&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The triangle inequality states:&lt;/p&gt;
    &lt;p&gt;\[ \|v + u\| \leq \|v\| + \|u\| \]&lt;/p&gt;
    &lt;p&gt;Geometrically, the length of one side of a triangle can never be longer than the sum of the other two sides.&lt;/p&gt;
    &lt;code&gt;= np.array([3, 4])
 v = np.array([1, 2])
 u 
= np.linalg.norm(v + u)
 lhs = np.linalg.norm(v) + np.linalg.norm(u)
 rhs 
print("‖v+u‖ =", lhs)
print("‖v‖ + ‖u‖ =", rhs)
print("Inequality holds?", lhs &amp;lt;= rhs)&lt;/code&gt;
    &lt;code&gt;‖v+u‖ = 7.211102550927978
‖v‖ + ‖u‖ = 7.23606797749979
Inequality holds? True&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visual demonstration with a triangle&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;import matplotlib.pyplot as plt

= np.array([0,0])
 origin = np.array([origin, v, v+u, origin])
 points 
0], points[:,1], 'ro-')  # triangle outline
 plt.plot(points[:,0], v[1], 'v')
 plt.text(v[0]+u[0], v[1]+u[1], 'v+u')
 plt.text(v[0], u[1], 'u')
 plt.text(u[

 plt.grid()0,color='black',linewidth=0.5)
 plt.axhline(0,color='black',linewidth=0.5)
 plt.axvline('equal')
 plt.axis( plt.show()&lt;/code&gt;
    &lt;p&gt;This triangle shows why the inequality is called the “triangle” inequality.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Testing triangle inequality with random vectors&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;for _ in range(5):
= np.random.randn(2)
     v = np.random.randn(2)
     u = np.linalg.norm(v+u)
     lhs = np.linalg.norm(v) + np.linalg.norm(u)
     rhs print(f"‖v+u‖={lhs:.3f}, ‖v‖+‖u‖={rhs:.3f}, holds={lhs &amp;lt;= rhs}")     &lt;/code&gt;
    &lt;code&gt;‖v+u‖=0.778, ‖v‖+‖u‖=2.112, holds=True
‖v+u‖=1.040, ‖v‖+‖u‖=2.621, holds=True
‖v+u‖=1.632, ‖v‖+‖u‖=2.482, holds=True
‖v+u‖=1.493, ‖v‖+‖u‖=2.250, holds=True
‖v+u‖=2.653, ‖v‖+‖u‖=2.692, holds=True&lt;/code&gt;
    &lt;p&gt;No matter what vectors you try, the inequality always holds.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cauchy–Schwarz: The dot product is always bounded by the product of vector lengths.&lt;/item&gt;
      &lt;item&gt;Triangle inequality: The length of one side of a triangle can’t exceed the sum of the other two.&lt;/item&gt;
      &lt;item&gt;These inequalities form the backbone of geometry, analysis, and many proofs in linear algebra.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;10. Orthonormal Sets in ℝ²/ℝ³&lt;/head&gt;
    &lt;p&gt;In this lab, we’ll explore orthonormal sets - collections of vectors that are both orthogonal (perpendicular) and normalized (length = 1). These sets are the “nicest” possible bases for vector spaces. In 2D and 3D, they correspond to the coordinate axes we already know, but we can also construct and test new ones.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
import matplotlib.pyplot as plt&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Orthogonal vectors Two vectors are orthogonal if their dot product is zero.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([1, 0])
 x_axis = np.array([0, 1])
 y_axis 
print("x_axis · y_axis =", np.dot(x_axis, y_axis))  # should be 0&lt;/code&gt;
    &lt;code&gt;x_axis · y_axis = 0&lt;/code&gt;
    &lt;p&gt;So the standard axes are orthogonal.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Normalizing vectors Normalization means dividing a vector by its length to make its norm equal to 1.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([3, 4])
 v = v / np.linalg.norm(v)
 v_normalized 
print("Original v:", v)
print("Normalized v:", v_normalized)
print("Length of normalized v:", np.linalg.norm(v_normalized))&lt;/code&gt;
    &lt;code&gt;Original v: [3 4]
Normalized v: [0.6 0.8]
Length of normalized v: 1.0&lt;/code&gt;
    &lt;p&gt;Now &lt;code&gt;v_normalized&lt;/code&gt; points in the same direction as &lt;code&gt;v&lt;/code&gt; but has unit length.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Building an orthonormal set in 2D&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([1, 0])
 u1 = np.array([0, 1])
 u2 
print("u1 length:", np.linalg.norm(u1))
print("u2 length:", np.linalg.norm(u2))
print("u1 · u2 =", np.dot(u1,u2))&lt;/code&gt;
    &lt;code&gt;u1 length: 1.0
u2 length: 1.0
u1 · u2 = 0&lt;/code&gt;
    &lt;p&gt;Both have length 1, and their dot product is 0. That makes &lt;code&gt;{u1, u2}&lt;/code&gt; an orthonormal set in 2D.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visualizing 2D orthonormal vectors&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;0,0,u1[0],u1[1],angles='xy',scale_units='xy',scale=1,color='r')
 plt.quiver(0,0,u2[0],u2[1],angles='xy',scale_units='xy',scale=1,color='b')
 plt.quiver(
-1.5,1.5)
 plt.xlim(-1.5,1.5)
 plt.ylim(0,color='black',linewidth=0.5)
 plt.axhline(0,color='black',linewidth=0.5)
 plt.axvline(
 plt.grid() plt.show()&lt;/code&gt;
    &lt;p&gt;You’ll see the red and blue arrows at right angles, each of length 1.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Orthonormal set in 3D In 3D, the standard basis vectors are:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([1,0,0])
 i = np.array([0,1,0])
 j = np.array([0,0,1])
 k 
print("‖i‖ =", np.linalg.norm(i))
print("‖j‖ =", np.linalg.norm(j))
print("‖k‖ =", np.linalg.norm(k))
print("i·j =", np.dot(i,j))
print("j·k =", np.dot(j,k))
print("i·k =", np.dot(i,k))&lt;/code&gt;
    &lt;code&gt;‖i‖ = 1.0
‖j‖ = 1.0
‖k‖ = 1.0
i·j = 0
j·k = 0
i·k = 0&lt;/code&gt;
    &lt;p&gt;Lengths are all 1, and dot products are 0. So &lt;code&gt;{i, j, k}&lt;/code&gt; is an orthonormal set in ℝ³.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Testing if a set is orthonormal We can write a helper function:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;def is_orthonormal(vectors):
for i in range(len(vectors)):
     for j in range(len(vectors)):
         = np.dot(vectors[i], vectors[j])
             dot if i == j:
             if not np.isclose(dot, 1): return False
                 else:
             if not np.isclose(dot, 0): return False
                 return True
     
print(is_orthonormal([i, j, k]))  # True&lt;/code&gt;
    &lt;code&gt;True&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Constructing a new orthonormal pair Not all orthonormal sets look like the axes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([1,1]) / np.sqrt(2)
 u1 = np.array([-1,1]) / np.sqrt(2)
 u2 
print("u1·u2 =", np.dot(u1,u2))
print("‖u1‖ =", np.linalg.norm(u1))
print("‖u2‖ =", np.linalg.norm(u2))&lt;/code&gt;
    &lt;code&gt;u1·u2 = 0.0
‖u1‖ = 0.9999999999999999
‖u2‖ = 0.9999999999999999&lt;/code&gt;
    &lt;p&gt;This gives a rotated orthonormal basis in 2D.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Normalize &lt;code&gt;(2,2,1)&lt;/code&gt;to make it a unit vector.&lt;/item&gt;
      &lt;item&gt;Test whether the set &lt;code&gt;{[1,0,0], [0,2,0], [0,0,3]}&lt;/code&gt;is orthonormal.&lt;/item&gt;
      &lt;item&gt;Construct two vectors in 2D that are not perpendicular. Normalize them and check if the dot product is still zero.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Chapter 2. Matrices and basic operations&lt;/head&gt;
    &lt;head rend="h3"&gt;11. Matrices as Tables and as Machines&lt;/head&gt;
    &lt;p&gt;Matrices can feel mysterious at first, but there are two simple ways to think about them:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;As tables of numbers - just a grid you can store and manipulate.&lt;/item&gt;
      &lt;item&gt;As machines - something that takes a vector in and spits a new vector out.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In this lab, we’ll explore both views and see how they connect.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A matrix as a table of numbers&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([
 A 1, 2, 3],
     [4, 5, 6]
     [
 ])
print("Matrix A:\n", A)
print("Shape of A:", A.shape)&lt;/code&gt;
    &lt;code&gt;Matrix A:
 [[1 2 3]
 [4 5 6]]
Shape of A: (2, 3)&lt;/code&gt;
    &lt;p&gt;Here, &lt;code&gt;A&lt;/code&gt; is a 2×3 matrix (2 rows, 3 columns).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rows = horizontal slices → &lt;code&gt;[1,2,3]&lt;/code&gt;and&lt;code&gt;[4,5,6]&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Columns = vertical slices → &lt;code&gt;[1,4]&lt;/code&gt;,&lt;code&gt;[2,5]&lt;/code&gt;,&lt;code&gt;[3,6]&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Accessing rows and columns&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= A[0]        # row 0
 first_row = A[:,1]  # column 1
 second_column 
print("First row:", first_row)
print("Second column:", second_column)&lt;/code&gt;
    &lt;code&gt;First row: [1 2 3]
Second column: [2 5]&lt;/code&gt;
    &lt;p&gt;Rows are whole vectors too, and so are columns.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A matrix as a machine&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A matrix can “act” on a vector. If &lt;code&gt;x = [x1, x2, x3]&lt;/code&gt;, then &lt;code&gt;A·x&lt;/code&gt; is computed by taking linear combinations of the columns of &lt;code&gt;A&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;= np.array([1, 0, -1])  # a 3D vector
 x = A.dot(x)
 result 
print("A·x =", result)&lt;/code&gt;
    &lt;code&gt;A·x = [-2 -2]&lt;/code&gt;
    &lt;p&gt;Interpretation: multiply &lt;code&gt;A&lt;/code&gt; by &lt;code&gt;x&lt;/code&gt; = combine columns of &lt;code&gt;A&lt;/code&gt; with weights from &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;\[ A \cdot x = 1 \cdot \text{(col 1)} + 0 \cdot \text{(col 2)} + (-1) \cdot \text{(col 3)} \]&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verifying column combination view&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= A[:,0]
 col1 = A[:,1]
 col2 = A[:,2]
 col3 
= 1*col1 + 0*col2 + (-1)*col3
 manual print("Manual combination:", manual)
print("A·x result:", result)&lt;/code&gt;
    &lt;code&gt;Manual combination: [-2 -2]
A·x result: [-2 -2]&lt;/code&gt;
    &lt;p&gt;They match exactly. This shows the “machine” interpretation is just a shortcut for column combinations.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Geometric intuition (2D example)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([
 B 2, 0],
     [0, 1]
     [
 ])
= np.array([1,2])
 v print("B·v =", B.dot(v))&lt;/code&gt;
    &lt;code&gt;B·v = [2 2]&lt;/code&gt;
    &lt;p&gt;Here, &lt;code&gt;B&lt;/code&gt; scales the x-direction by 2 while leaving the y-direction alone. So &lt;code&gt;(1,2)&lt;/code&gt; becomes &lt;code&gt;(2,2)&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Create a 3×3 identity matrix with &lt;code&gt;np.eye(3)&lt;/code&gt;and multiply it by different vectors. What happens?&lt;/item&gt;
      &lt;item&gt;Build a matrix &lt;code&gt;[[0,-1],[1,0]]&lt;/code&gt;. Try multiplying it by&lt;code&gt;(1,0)&lt;/code&gt;and&lt;code&gt;(0,1)&lt;/code&gt;. What transformation is this?&lt;/item&gt;
      &lt;item&gt;Create your own 2×2 matrix that flips vectors across the x-axis. Test it on &lt;code&gt;(1,2)&lt;/code&gt;and&lt;code&gt;(−3,4)&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A matrix is both a grid of numbers and a machine that transforms vectors.&lt;/item&gt;
      &lt;item&gt;Matrix–vector multiplication is the same as combining columns with given weights.&lt;/item&gt;
      &lt;item&gt;Thinking of matrices as machines helps build intuition for rotations, scalings, and other transformations later.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;12. Matrix Shapes, Indexing, and Block Views&lt;/head&gt;
    &lt;p&gt;Matrices come in many shapes, and learning to read their structure is essential. Shape tells us how many rows and columns a matrix has. Indexing lets us grab specific entries, rows, or columns. Block views let us zoom in on submatrices, which is extremely useful for both theory and computation.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Matrix shapes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The shape of a matrix is &lt;code&gt;(rows, columns)&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;= np.array([
 A 1, 2, 3],
     [4, 5, 6],
     [7, 8, 9]
     [
 ])
print("Matrix A:\n", A)
print("Shape of A:", A.shape)&lt;/code&gt;
    &lt;code&gt;Matrix A:
 [[1 2 3]
 [4 5 6]
 [7 8 9]]
Shape of A: (3, 3)&lt;/code&gt;
    &lt;p&gt;Here, &lt;code&gt;A&lt;/code&gt; is a 3×3 matrix.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Indexing elements&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In NumPy, rows and columns are 0-based. The first entry is &lt;code&gt;A[0,0]&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;print("A[0,0] =", A[0,0])  # top-left element
print("A[1,2] =", A[1,2])  # second row, third column&lt;/code&gt;
    &lt;code&gt;A[0,0] = 1
A[1,2] = 6&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Extracting rows and columns&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= A[0]       # first row
 row1 = A[:,1]     # second column
 col2 
print("First row:", row1)
print("Second column:", col2)&lt;/code&gt;
    &lt;code&gt;First row: [1 2 3]
Second column: [2 5 8]&lt;/code&gt;
    &lt;p&gt;Notice: &lt;code&gt;A[i]&lt;/code&gt; gives a row, &lt;code&gt;A[:,j]&lt;/code&gt; gives a column.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Slicing submatrices (block view)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can slice multiple rows and columns to form a smaller matrix.&lt;/p&gt;
    &lt;code&gt;= A[0:2, 1:3]  # rows 0–1, columns 1–2
 block print("Block submatrix:\n", block)&lt;/code&gt;
    &lt;code&gt;Block submatrix:
 [[2 3]
 [5 6]]&lt;/code&gt;
    &lt;p&gt;This block is:&lt;/p&gt;
    &lt;p&gt;\[ \begin{bmatrix} 2 &amp;amp; 3 \\ 5 &amp;amp; 6 \end{bmatrix} \]&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Modifying parts of a matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;0,0] = 99
 A[print("Modified A:\n", A)

1,:] = [10, 11, 12]   # replace row 1
 A[print("After replacing row 1:\n", A)&lt;/code&gt;
    &lt;code&gt;Modified A:
 [[99  2  3]
 [ 4  5  6]
 [ 7  8  9]]
After replacing row 1:
 [[99  2  3]
 [10 11 12]
 [ 7  8  9]]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Non-square matrices&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Not all matrices are square. Shapes can be rectangular, too.&lt;/p&gt;
    &lt;code&gt;= np.array([
 B 1, 2],
     [3, 4],
     [5, 6]
     [
 ])
print("Matrix B:\n", B)
print("Shape of B:", B.shape)&lt;/code&gt;
    &lt;code&gt;Matrix B:
 [[1 2]
 [3 4]
 [5 6]]
Shape of B: (3, 2)&lt;/code&gt;
    &lt;p&gt;Here, &lt;code&gt;B&lt;/code&gt; is 3×2 (3 rows, 2 columns).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Block decomposition idea&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can think of large matrices as made of smaller blocks. This is common in linear algebra proofs and algorithms.&lt;/p&gt;
    &lt;code&gt;= np.array([
 C 1,2,3,4],
     [5,6,7,8],
     [9,10,11,12],
     [13,14,15,16]
     [
 ])
= C[0:2, 0:2]
 top_left = C[2:4, 2:4]
 bottom_right 
print("Top-left block:\n", top_left)
print("Bottom-right block:\n", bottom_right)&lt;/code&gt;
    &lt;code&gt;Top-left block:
 [[1 2]
 [5 6]]
Bottom-right block:
 [[11 12]
 [15 16]]&lt;/code&gt;
    &lt;p&gt;This is the start of block matrix notation.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Create a 4×5 matrix with values 1–20 using &lt;code&gt;np.arange(1,21).reshape(4,5)&lt;/code&gt;. Find its shape.&lt;/item&gt;
      &lt;item&gt;Extract the middle row and last column.&lt;/item&gt;
      &lt;item&gt;Cut it into four 2×2 blocks. Can you reassemble them in a different order?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;13. Matrix Addition and Scalar Multiplication&lt;/head&gt;
    &lt;p&gt;Now that we understand matrix shapes and indexing, let’s practice two of the simplest but most important operations: adding matrices and scaling them with numbers (scalars). These operations extend the rules we already know for vectors.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Adding two matrices You can add two matrices if (and only if) they have the same shape. Addition happens entry by entry.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([
 A 1, 2],
     [3, 4]
     [
 ])
= np.array([
 B 5, 6],
     [7, 8]
     [
 ])
= A + B
 C print("A + B =\n", C)&lt;/code&gt;
    &lt;code&gt;A + B =
 [[ 6  8]
 [10 12]]&lt;/code&gt;
    &lt;p&gt;Each element in &lt;code&gt;C&lt;/code&gt; is the sum of corresponding elements in &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Scalar multiplication Multiplying a matrix by a scalar multiplies every entry by that number.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= 3
 k = k * A
 D print("3 * A =\n", D)&lt;/code&gt;
    &lt;code&gt;3 * A =
 [[ 3  6]
 [ 9 12]]&lt;/code&gt;
    &lt;p&gt;Here, each element of &lt;code&gt;A&lt;/code&gt; is tripled.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Combining both operations We can mix addition and scaling, just like with vectors.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= 2*A - B
 combo print("2A - B =\n", combo)&lt;/code&gt;
    &lt;code&gt;2A - B =
 [[-3 -2]
 [-1  0]]&lt;/code&gt;
    &lt;p&gt;This creates new matrices as linear combinations of others.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Zero matrix A matrix of all zeros acts like “nothing happens” for addition.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.zeros((2,2))
 zero print("Zero matrix:\n", zero)
print("A + Zero =\n", A + zero)&lt;/code&gt;
    &lt;code&gt;Zero matrix:
 [[0. 0.]
 [0. 0.]]
A + Zero =
 [[1. 2.]
 [3. 4.]]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Shape mismatch (what fails) If shapes don’t match, NumPy throws an error.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([
 X 1,2,3],
     [4,5,6]
     [
 ])
try:
print(A + X)
     except ValueError as e:
print("Error:", e)     &lt;/code&gt;
    &lt;code&gt;Error: operands could not be broadcast together with shapes (2,2) (2,3) &lt;/code&gt;
    &lt;p&gt;This shows why shape consistency matters.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Create two random 3×3 matrices with &lt;code&gt;np.random.randint(0,10,(3,3))&lt;/code&gt;and add them.&lt;/item&gt;
      &lt;item&gt;Multiply a 4×4 matrix by &lt;code&gt;-1&lt;/code&gt;. What happens to its entries?&lt;/item&gt;
      &lt;item&gt;Compute &lt;code&gt;3A + 2B&lt;/code&gt;with the matrices from above. Compare with doing each step manually.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;14. Matrix–Vector Product (Linear Combinations of Columns)&lt;/head&gt;
    &lt;p&gt;This lab introduces the matrix–vector product, one of the most important operations in linear algebra. Multiplying a matrix by a vector doesn’t just crunch numbers - it produces a new vector by combining the matrix’s columns in a weighted way.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A simple matrix and vector&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([
 A 1, 2],
     [3, 4],
     [5, 6]
     [# 3×2 matrix
 ])  
= np.array([2, -1])  # 2D vector x &lt;/code&gt;
    &lt;p&gt;Here, &lt;code&gt;A&lt;/code&gt; has 2 columns, so we can multiply it by a 2D vector &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Matrix–vector multiplication in NumPy&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= A.dot(x)
 y print("A·x =", y)&lt;/code&gt;
    &lt;code&gt;A·x = [0 2 4]&lt;/code&gt;
    &lt;p&gt;Result: a 3D vector.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Interpreting the result as linear combinations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Matrix &lt;code&gt;A&lt;/code&gt; has two columns:&lt;/p&gt;
    &lt;code&gt;= A[:,0]   # first column
 col1 = A[:,1]   # second column
 col2 
= 2*col1 + (-1)*col2
 manual print("Manual linear combination:", manual)&lt;/code&gt;
    &lt;code&gt;Manual linear combination: [0 2 4]&lt;/code&gt;
    &lt;p&gt;This matches &lt;code&gt;A·x&lt;/code&gt;. In words: multiply each column by the corresponding entry of &lt;code&gt;x&lt;/code&gt; and then add them up.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Another example (geometry)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([
 B 2, 0],
     [0, 1]
     [# stretches x-axis by 2
 ])  
= np.array([1, 3])
 v print("B·v =", B.dot(v))&lt;/code&gt;
    &lt;code&gt;B·v = [2 3]&lt;/code&gt;
    &lt;p&gt;Here, &lt;code&gt;(1,3)&lt;/code&gt; becomes &lt;code&gt;(2,3)&lt;/code&gt;. The x-component was doubled, while y stayed the same.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visualization of matrix action&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;import matplotlib.pyplot as plt

# draw original vector
0,0,v[0],v[1],angles='xy',scale_units='xy',scale=1,color='r',label='v')
 plt.quiver(
# draw transformed vector
= B.dot(v)
 v_transformed 0,0,v_transformed[0],v_transformed[1],angles='xy',scale_units='xy',scale=1,color='b',label='B·v')
 plt.quiver(
-1,4)
 plt.xlim(-1,4)
 plt.ylim(0,color='black',linewidth=0.5)
 plt.axhline(0,color='black',linewidth=0.5)
 plt.axvline(
 plt.grid() plt.show()&lt;/code&gt;
    &lt;p&gt;Red arrow = original vector, blue arrow = transformed vector.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Multiply&lt;/p&gt;
        &lt;p&gt;\[ A = \begin{bmatrix}1 &amp;amp; 0 \\ 0 &amp;amp; 1 \\ -1 &amp;amp; 2\end{bmatrix},\; x = [3,1] \]&lt;/p&gt;
        &lt;p&gt;What’s the result?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Replace&lt;/p&gt;&lt;code&gt;B&lt;/code&gt;with&lt;code&gt;[[0,-1],[1,0]]&lt;/code&gt;. Multiply it by&lt;code&gt;(1,0)&lt;/code&gt;and&lt;code&gt;(0,1)&lt;/code&gt;. What geometric transformation does this represent?&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;For a 4×4 identity matrix (&lt;/p&gt;&lt;code&gt;np.eye(4)&lt;/code&gt;), try multiplying by any 4D vector. What do you observe?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;15. Matrix–Matrix Product (Composition of Linear Steps)&lt;/head&gt;
    &lt;p&gt;Matrix–matrix multiplication is how we combine two linear transformations into one. Instead of applying one transformation, then another, we can multiply their matrices and get a single matrix that does both at once.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Matrix–matrix multiplication in NumPy&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([
 A 1, 2],
     [3, 4]
     [# 2×2
 ])  
= np.array([
 B 2, 0],
     [1, 2]
     [# 2×2
 ])  
= A.dot(B)   # or A @ B
 C print("A·B =\n", C)&lt;/code&gt;
    &lt;code&gt;A·B =
 [[ 4  4]
 [10  8]]&lt;/code&gt;
    &lt;p&gt;The result &lt;code&gt;C&lt;/code&gt; is another 2×2 matrix.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Manual computation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each entry of &lt;code&gt;C&lt;/code&gt; is computed as a row of A dotted with a column of B:&lt;/p&gt;
    &lt;code&gt;= A[0,:].dot(B[:,0])
 c11 = A[0,:].dot(B[:,1])
 c12 = A[1,:].dot(B[:,0])
 c21 = A[1,:].dot(B[:,1])
 c22 
print("Manual C =\n", np.array([[c11,c12],[c21,c22]]))&lt;/code&gt;
    &lt;code&gt;Manual C =
 [[ 4  4]
 [10  8]]&lt;/code&gt;
    &lt;p&gt;This should match &lt;code&gt;A·B&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Geometric interpretation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s see how two transformations combine.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Matrix &lt;code&gt;B&lt;/code&gt;scales x by 2 and stretches y by 2.&lt;/item&gt;
      &lt;item&gt;Matrix &lt;code&gt;A&lt;/code&gt;applies another linear transformation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Together, &lt;code&gt;C = A·B&lt;/code&gt; does both in one step.&lt;/p&gt;
    &lt;code&gt;= np.array([1,1])
 v 
print("First apply B:", B.dot(v))
print("Then apply A:", A.dot(B.dot(v)))
print("Directly with C:", C.dot(v))&lt;/code&gt;
    &lt;code&gt;First apply B: [2 3]
Then apply A: [ 8 18]
Directly with C: [ 8 18]&lt;/code&gt;
    &lt;p&gt;The result is the same: applying &lt;code&gt;B&lt;/code&gt; then &lt;code&gt;A&lt;/code&gt; is equivalent to applying &lt;code&gt;C&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Non-square matrices&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Matrix multiplication also works for rectangular matrices, as long as the inner dimensions match.&lt;/p&gt;
    &lt;code&gt;= np.array([
 M 1, 0, 2],
     [0, 1, 3]
     [# 2×3
 ])  
= np.array([
 N 1, 2],
     [0, 1],
     [4, 0]
     [# 3×2
 ])  
= M.dot(N)  # result is 2×2
 P print("M·N =\n", P)&lt;/code&gt;
    &lt;code&gt;M·N =
 [[ 9  2]
 [12  1]]&lt;/code&gt;
    &lt;p&gt;Shape rule: &lt;code&gt;(2×3)·(3×2) = (2×2)&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Associativity (but not commutativity)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Matrix multiplication is associative: &lt;code&gt;(A·B)·C = A·(B·C)&lt;/code&gt;. But it’s not commutative: in general, &lt;code&gt;A·B ≠ B·A&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;= np.array([[1,2],[3,4]])
 A = np.array([[0,1],[1,0]])
 B 
print("A·B =\n", A.dot(B))
print("B·A =\n", B.dot(A))&lt;/code&gt;
    &lt;code&gt;A·B =
 [[2 1]
 [4 3]]
B·A =
 [[3 4]
 [1 2]]&lt;/code&gt;
    &lt;p&gt;The two results are different.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Multiply&lt;/p&gt;&lt;p&gt;\[ A = \begin{bmatrix}1 &amp;amp; 0 \\ 0 &amp;amp; 1\end{bmatrix},\; B = \begin{bmatrix}0 &amp;amp; -1 \\ 1 &amp;amp; 0\end{bmatrix} \]&lt;/p&gt;&lt;p&gt;What transformation does&lt;/p&gt;&lt;code&gt;A·B&lt;/code&gt;represent?&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Create a random 3×2 matrix and a 2×4 matrix. Multiply them. What shape is the result?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Verify with Python that&lt;/p&gt;&lt;code&gt;(A·B)·C = A·(B·C)&lt;/code&gt;for some 3×3 random matrices.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;16. Identity, Inverse, and Transpose&lt;/head&gt;
    &lt;p&gt;In this lab, we’ll meet three special matrix operations and objects: the identity matrix, the inverse, and the transpose. These are the building blocks of matrix algebra, each with a simple meaning but deep importance.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Identity matrix The identity matrix is like the number &lt;code&gt;1&lt;/code&gt;for matrices: multiplying by it changes nothing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.eye(3)  # 3×3 identity matrix
 I print("Identity matrix:\n", I)

= np.array([
 A 2, 1, 0],
     [0, 1, 3],
     [4, 0, 1]
     [
 ])
print("A·I =\n", A.dot(I))
print("I·A =\n", I.dot(A))&lt;/code&gt;
    &lt;code&gt;Identity matrix:
 [[1. 0. 0.]
 [0. 1. 0.]
 [0. 0. 1.]]
A·I =
 [[2. 1. 0.]
 [0. 1. 3.]
 [4. 0. 1.]]
I·A =
 [[2. 1. 0.]
 [0. 1. 3.]
 [4. 0. 1.]]&lt;/code&gt;
    &lt;p&gt;Both equal &lt;code&gt;A&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Transpose The transpose flips rows and columns.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([
 B 1, 2, 3],
     [4, 5, 6]
     [
 ])
print("B:\n", B)
print("B.T:\n", B.T)&lt;/code&gt;
    &lt;code&gt;B:
 [[1 2 3]
 [4 5 6]]
B.T:
 [[1 4]
 [2 5]
 [3 6]]&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Original: 2×3&lt;/item&gt;
      &lt;item&gt;Transpose: 3×2&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Geometrically, transpose swaps the axes when vectors are viewed in row/column form.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Inverse The inverse matrix is like dividing by a number: multiplying a matrix by its inverse gives the identity.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([
 C 2, 1],
     [5, 3]
     [
 ])
= np.linalg.inv(C)
 C_inv print("Inverse of C:\n", C_inv)

print("C·C_inv =\n", C.dot(C_inv))
print("C_inv·C =\n", C_inv.dot(C))&lt;/code&gt;
    &lt;code&gt;Inverse of C:
 [[ 3. -1.]
 [-5.  2.]]
C·C_inv =
 [[ 1.00000000e+00  2.22044605e-16]
 [-8.88178420e-16  1.00000000e+00]]
C_inv·C =
 [[1.00000000e+00 3.33066907e-16]
 [0.00000000e+00 1.00000000e+00]]&lt;/code&gt;
    &lt;p&gt;Both products are (approximately) the identity.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Matrices that don’t have inverses Not every matrix is invertible. If a matrix is singular (determinant = 0), it has no inverse.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([
 D 1, 2],
     [2, 4]
     [
 ])
try:

     np.linalg.inv(D)except np.linalg.LinAlgError as e:
print("Error:", e)     &lt;/code&gt;
    &lt;code&gt;Error: Singular matrix&lt;/code&gt;
    &lt;p&gt;Here, the second row is a multiple of the first, so &lt;code&gt;D&lt;/code&gt; can’t be inverted.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Transpose and inverse together For invertible matrices,&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ (A^T)^{-1} = (A^{-1})^T \]&lt;/p&gt;
    &lt;p&gt;We can check this numerically:&lt;/p&gt;
    &lt;code&gt;= np.array([
 A 1, 2],
     [3, 5]
     [
 ])
= np.linalg.inv(A.T)
 lhs = np.linalg.inv(A).T
 rhs 
print("Do they match?", np.allclose(lhs, rhs))&lt;/code&gt;
    &lt;code&gt;Do they match? True&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Create a 4×4 identity matrix. Multiply it by any 4×1 vector. Does it change?&lt;/item&gt;
      &lt;item&gt;Take a random 2×2 matrix with &lt;code&gt;np.random.randint&lt;/code&gt;. Compute its inverse and check if multiplying gives identity.&lt;/item&gt;
      &lt;item&gt;Pick a rectangular 3×2 matrix. What happens when you try &lt;code&gt;np.linalg.inv&lt;/code&gt;? Why?&lt;/item&gt;
      &lt;item&gt;Compute &lt;code&gt;(A.T).T&lt;/code&gt;for some matrix&lt;code&gt;A&lt;/code&gt;. What do you notice?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;17. Symmetric, Diagonal, Triangular, and Permutation Matrices&lt;/head&gt;
    &lt;p&gt;In this lab, we’ll meet four important families of special matrices. They have patterns that make them easier to understand, compute with, and use in algorithms.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Symmetric matrices A matrix is symmetric if it equals its transpose: \(A = A^T\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([
 A 2, 3, 4],
     [3, 5, 6],
     [4, 6, 8]
     [
 ])
print("A:\n", A)
print("A.T:\n", A.T)
print("Is symmetric?", np.allclose(A, A.T))&lt;/code&gt;
    &lt;code&gt;A:
 [[2 3 4]
 [3 5 6]
 [4 6 8]]
A.T:
 [[2 3 4]
 [3 5 6]
 [4 6 8]]
Is symmetric? True&lt;/code&gt;
    &lt;p&gt;Symmetric matrices appear in physics, optimization, and statistics (e.g., covariance matrices).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Diagonal matrices A diagonal matrix has nonzero entries only on the main diagonal.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.diag([1, 5, 9])
 D print("Diagonal matrix:\n", D)

= np.array([2, 3, 4])
 x print("D·x =", D.dot(x))  # scales each component&lt;/code&gt;
    &lt;code&gt;Diagonal matrix:
 [[1 0 0]
 [0 5 0]
 [0 0 9]]
D·x = [ 2 15 36]&lt;/code&gt;
    &lt;p&gt;Diagonal multiplication simply scales each coordinate separately.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Triangular matrices Upper triangular: all entries below the diagonal are zero. Lower triangular: all entries above the diagonal are zero.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([
 U 1, 2, 3],
     [0, 4, 5],
     [0, 0, 6]
     [
 ])
= np.array([
 L 7, 0, 0],
     [8, 9, 0],
     [1, 2, 3]
     [
 ])
print("Upper triangular U:\n", U)
print("Lower triangular L:\n", L)&lt;/code&gt;
    &lt;code&gt;Upper triangular U:
 [[1 2 3]
 [0 4 5]
 [0 0 6]]
Lower triangular L:
 [[7 0 0]
 [8 9 0]
 [1 2 3]]&lt;/code&gt;
    &lt;p&gt;These are important in solving linear systems (e.g., Gaussian elimination).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Permutation matrices A permutation matrix rearranges the order of coordinates. Each row and each column has exactly one &lt;code&gt;1&lt;/code&gt;, everything else is&lt;code&gt;0&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([
 P 0, 1, 0],
     [0, 0, 1],
     [1, 0, 0]
     [
 ])
print("Permutation matrix P:\n", P)

= np.array([10, 20, 30])
 v print("P·v =", P.dot(v))&lt;/code&gt;
    &lt;code&gt;Permutation matrix P:
 [[0 1 0]
 [0 0 1]
 [1 0 0]]
P·v = [20 30 10]&lt;/code&gt;
    &lt;p&gt;Here, &lt;code&gt;P&lt;/code&gt; cycles &lt;code&gt;(10,20,30)&lt;/code&gt; into &lt;code&gt;(20,30,10)&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Checking properties&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;def is_symmetric(M): return np.allclose(M, M.T)
def is_diagonal(M): return np.count_nonzero(M - np.diag(np.diag(M))) == 0
def is_upper_triangular(M): return np.allclose(M, np.triu(M))
def is_lower_triangular(M): return np.allclose(M, np.tril(M))

print("A symmetric?", is_symmetric(A))
print("D diagonal?", is_diagonal(D))
print("U upper triangular?", is_upper_triangular(U))
print("L lower triangular?", is_lower_triangular(L))&lt;/code&gt;
    &lt;code&gt;A symmetric? True
D diagonal? True
U upper triangular? True
L lower triangular? True&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Create a random symmetric matrix by generating any matrix &lt;code&gt;M&lt;/code&gt;and computing&lt;code&gt;(M + M.T)/2&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Build a 4×4 diagonal matrix with diagonal entries &lt;code&gt;[2,4,6,8]&lt;/code&gt;and multiply it by&lt;code&gt;[1,1,1,1]&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Make a permutation matrix that swaps the first and last components of a 3D vector.&lt;/item&gt;
      &lt;item&gt;Check whether the identity matrix is diagonal, symmetric, upper triangular, and lower triangular all at once.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;18. Trace and Basic Matrix Properties&lt;/head&gt;
    &lt;p&gt;In this lab, we’ll introduce the trace of a matrix and a few quick properties that often appear in proofs, algorithms, and applications. The trace is simple to compute but surprisingly powerful.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;What is the trace? The trace of a square matrix is the sum of its diagonal entries:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ \text{tr}(A) = \sum_i A_{ii} \]&lt;/p&gt;
    &lt;code&gt;= np.array([
 A 2, 1, 3],
     [0, 4, 5],
     [7, 8, 6]
     [
 ])
= np.trace(A)
 trace_A print("Matrix A:\n", A)
print("Trace of A =", trace_A)&lt;/code&gt;
    &lt;code&gt;Matrix A:
 [[2 1 3]
 [0 4 5]
 [7 8 6]]
Trace of A = 12&lt;/code&gt;
    &lt;p&gt;Here, trace = \(2 + 4 + 6 = 12\).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Trace is linear For matrices &lt;code&gt;A&lt;/code&gt;and&lt;code&gt;B&lt;/code&gt;:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ \text{tr}(A+B) = \text{tr}(A) + \text{tr}(B) \]&lt;/p&gt;
    &lt;p&gt;\[ \text{tr}(cA) = c \cdot \text{tr}(A) \]&lt;/p&gt;
    &lt;code&gt;= np.array([
 B 1, 0, 0],
     [0, 2, 0],
     [0, 0, 3]
     [
 ])
print("tr(A+B) =", np.trace(A+B))
print("tr(A) + tr(B) =", np.trace(A) + np.trace(B))

print("tr(3A) =", np.trace(3*A))
print("3 * tr(A) =", 3*np.trace(A))&lt;/code&gt;
    &lt;code&gt;tr(A+B) = 18
tr(A) + tr(B) = 18
tr(3A) = 36
3 * tr(A) = 36&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Trace of a product One important property is:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ \text{tr}(AB) = \text{tr}(BA) \]&lt;/p&gt;
    &lt;code&gt;= np.array([
 C 0,1],
     [2,3]
     [
 ])
= np.array([
 D 4,5],
     [6,7]
     [
 ])
print("tr(CD) =", np.trace(C.dot(D)))
print("tr(DC) =", np.trace(D.dot(C)))&lt;/code&gt;
    &lt;code&gt;tr(CD) = 37
tr(DC) = 37&lt;/code&gt;
    &lt;p&gt;Both are equal, even though &lt;code&gt;CD&lt;/code&gt; and &lt;code&gt;DC&lt;/code&gt; are different matrices.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Trace and eigenvalues The trace equals the sum of eigenvalues of a matrix (counting multiplicities).&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.linalg.eig(A)
 vals, vecs print("Eigenvalues:", vals)
print("Sum of eigenvalues =", np.sum(vals))
print("Trace =", np.trace(A))&lt;/code&gt;
    &lt;code&gt;Eigenvalues: [12.83286783  2.13019807 -2.9630659 ]
Sum of eigenvalues = 12.000000000000007
Trace = 12&lt;/code&gt;
    &lt;p&gt;The results should match (within rounding error).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Quick invariants&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Trace doesn’t change under transpose: &lt;code&gt;tr(A) = tr(A.T)&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Trace doesn’t change under similarity transforms: &lt;code&gt;tr(P^-1 A P) = tr(A)&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;print("tr(A) =", np.trace(A))
print("tr(A.T) =", np.trace(A.T))&lt;/code&gt;
    &lt;code&gt;tr(A) = 12
tr(A.T) = 12&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Create a 2×2 rotation matrix for 90°:&lt;/p&gt;
        &lt;p&gt;\[ R = \begin{bmatrix}0 &amp;amp; -1 \\ 1 &amp;amp; 0\end{bmatrix} \]&lt;/p&gt;
        &lt;p&gt;What is its trace? What does that tell you about its eigenvalues?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Make a random 3×3 matrix and compare&lt;/p&gt;&lt;code&gt;tr(A)&lt;/code&gt;with the sum of eigenvalues.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Test&lt;/p&gt;&lt;code&gt;tr(AB)&lt;/code&gt;and&lt;code&gt;tr(BA)&lt;/code&gt;with a rectangular matrix&lt;code&gt;A&lt;/code&gt;(e.g. 2×3) and&lt;code&gt;B&lt;/code&gt;(3×2). Do they still match?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;19. Affine Transforms and Homogeneous Coordinates&lt;/head&gt;
    &lt;p&gt;Affine transformations let us do more than just linear operations - they include translations (shifting points), which ordinary matrices can’t handle alone. To unify rotations, scalings, reflections, and translations, we use homogeneous coordinates.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
import matplotlib.pyplot as plt&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Linear transformations vs affine transformations&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A linear transformation can rotate, scale, or shear, but always keeps the origin fixed.&lt;/item&gt;
      &lt;item&gt;An affine transformation allows translation as well.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example, shifting every point by &lt;code&gt;(2,3)&lt;/code&gt; is affine but not linear.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Homogeneous coordinates idea We add an extra coordinate (usually &lt;code&gt;1&lt;/code&gt;) to vectors.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A 2D point &lt;code&gt;(x,y)&lt;/code&gt;becomes&lt;code&gt;(x,y,1)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;A 3D point &lt;code&gt;(x,y,z)&lt;/code&gt;becomes&lt;code&gt;(x,y,z,1)&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This trick lets us represent translations using matrix multiplication.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;2D translation matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ T = \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; t_x \\ 0 &amp;amp; 1 &amp;amp; t_y \\ 0 &amp;amp; 0 &amp;amp; 1 \end{bmatrix} \]&lt;/p&gt;
    &lt;code&gt;= np.array([
 T 1, 0, 2],
     [0, 1, 3],
     [0, 0, 1]
     [
 ])
= np.array([1, 1, 1])  # point at (1,1)
 p = T.dot(p)
 p_translated 
print("Original point:", p)
print("Translated point:", p_translated)&lt;/code&gt;
    &lt;code&gt;Original point: [1 1 1]
Translated point: [3 4 1]&lt;/code&gt;
    &lt;p&gt;This shifts &lt;code&gt;(1,1)&lt;/code&gt; to &lt;code&gt;(3,4)&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Combining rotation and translation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A 90° rotation around the origin in 2D:&lt;/p&gt;
    &lt;code&gt;= np.array([
 R 0, -1, 0],
     [1,  0, 0],
     [0,  0, 1]
     [
 ])
= T.dot(R)  # rotate then translate
 M print("Combined transform:\n", M)

= np.array([1, 0, 1])
 p print("Rotated + translated point:", M.dot(p))&lt;/code&gt;
    &lt;code&gt;Combined transform:
 [[ 0 -1  2]
 [ 1  0  3]
 [ 0  0  1]]
Rotated + translated point: [2 4 1]&lt;/code&gt;
    &lt;p&gt;Now we can apply rotation and translation in one step.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visualization of translation&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([
 points 0,0,1],
     [1,0,1],
     [1,1,1],
     [0,1,1]
     [# a unit square
 ])  
= points.dot(T.T)
 transformed 
0], points[:,1], color='r', label='original')
 plt.scatter(points[:,0], transformed[:,1], color='b', label='translated')
 plt.scatter(transformed[:,
for i in range(len(points)):
0], points[i,1],
     plt.arrow(points[i,0]-points[i,0],
               transformed[i,1]-points[i,1],
               transformed[i,=0.05, color='gray')
               head_width

 plt.legend()'equal')
 plt.axis(
 plt.grid() plt.show()&lt;/code&gt;
    &lt;p&gt;You’ll see the red unit square moved to a blue unit square shifted by &lt;code&gt;(2,3)&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Extending to 3D In 3D, homogeneous coordinates use 4×4 matrices. Translations, rotations, and scalings all fit the same framework.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([
 T3 1,0,0,5],
     [0,1,0,-2],
     [0,0,1,3],
     [0,0,0,1]
     [
 ])
= np.array([1,2,3,1])
 p3 print("Translated 3D point:", T3.dot(p3))&lt;/code&gt;
    &lt;code&gt;Translated 3D point: [6 0 6 1]&lt;/code&gt;
    &lt;p&gt;This shifts &lt;code&gt;(1,2,3)&lt;/code&gt; to &lt;code&gt;(6,0,6)&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build a scaling matrix in homogeneous coordinates that doubles both x and y, and apply it to &lt;code&gt;(1,1)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Create a 2D transform that rotates by 90° and then shifts by &lt;code&gt;(−2,1)&lt;/code&gt;. Apply it to&lt;code&gt;(0,2)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;In 3D, translate &lt;code&gt;(0,0,0)&lt;/code&gt;by&lt;code&gt;(10,10,10)&lt;/code&gt;. What homogeneous matrix did you use?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;20. Computing with Matrices (Cost Counts and Simple Speedups)&lt;/head&gt;
    &lt;p&gt;Working with matrices is not just about theory - in practice, we care about how much computation it takes to perform operations, and how we can make them faster. This lab introduces basic cost analysis (counting operations) and demonstrates simple NumPy optimizations.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
import time&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Counting operations (matrix–vector multiply)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If &lt;code&gt;A&lt;/code&gt; is an \(m \times n\) matrix and &lt;code&gt;x&lt;/code&gt; is an \(n\)-dimensional vector, computing &lt;code&gt;A·x&lt;/code&gt; takes about \(m \times n\) multiplications and the same number of additions.&lt;/p&gt;
    &lt;code&gt;= 3, 4
 m, n = np.random.randint(1,10,(m,n))
 A = np.random.randint(1,10,n)
 x 
print("Matrix A:\n", A)
print("Vector x:", x)
print("A·x =", A.dot(x))&lt;/code&gt;
    &lt;code&gt;Matrix A:
 [[6 6 6 2]
 [1 1 1 1]
 [1 8 7 4]]
Vector x: [6 5 4 5]
A·x = [100  20  94]&lt;/code&gt;
    &lt;p&gt;Here the cost is \(3 \times 4 = 12\) multiplications + 12 additions.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Counting operations (matrix–matrix multiply)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For an \(m \times n\) times \(n \times p\) multiplication, the cost is about \(m \times n \times p\).&lt;/p&gt;
    &lt;code&gt;= 3, 4, 2
 m, n, p = np.random.randint(1,10,(m,n))
 A = np.random.randint(1,10,(n,p))
 B 
= A.dot(B)
 C print("A·B =\n", C)&lt;/code&gt;
    &lt;code&gt;A·B =
 [[ 59  92]
 [ 43  81]
 [ 65 102]]&lt;/code&gt;
    &lt;p&gt;Here the cost is \(3 \times 4 \times 2 = 24\) multiplications + 24 additions.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Timing with NumPy (vectorized vs loop)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;NumPy is optimized in C and Fortran under the hood. Let’s compare matrix multiplication with and without vectorization.&lt;/p&gt;
    &lt;code&gt;= 50
 n = np.random.randn(n,n)
 A = np.random.randn(n,n)
 B 
# Vectorized
= time.time()
 start = A.dot(B)
 C1 = time.time()
 end print("Vectorized dot:", round(end-start,3), "seconds")

# Manual loops
= np.zeros((n,n))
 C2 = time.time()
 start for i in range(n):
for j in range(n):
     for k in range(n):
         += A[i,k]*B[k,j]
             C2[i,j] = time.time()
 end print("Triple loop:", round(end-start,3), "seconds")&lt;/code&gt;
    &lt;code&gt;Vectorized dot: 0.0 seconds
Triple loop: 0.026 seconds&lt;/code&gt;
    &lt;p&gt;The vectorized version should be thousands of times faster.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Broadcasting tricks&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;NumPy lets us avoid loops by broadcasting operations across entire rows or columns.&lt;/p&gt;
    &lt;code&gt;= np.array([
 A 1,2,3],
     [4,5,6]
     [
 ])
# Add 10 to every entry
print("A+10 =\n", A+10)

# Multiply each row by a different scalar
= np.array([1,10])[:,None]
 scales print("Row-scaled A =\n", A*scales)&lt;/code&gt;
    &lt;code&gt;A+10 =
 [[11 12 13]
 [14 15 16]]
Row-scaled A =
 [[ 1  2  3]
 [40 50 60]]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Memory and data types&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For large computations, data type matters.&lt;/p&gt;
    &lt;code&gt;= np.random.randn(1000,1000).astype(np.float32)  # 32-bit floats
 A = np.random.randn(1000,1000).astype(np.float32)
 B 
= time.time()
 start = A.dot(B)
 C print("Result shape:", C.shape, "dtype:", C.dtype)
print("Time:", round(time.time()-start,3), "seconds")&lt;/code&gt;
    &lt;code&gt;Result shape: (1000, 1000) dtype: float32
Time: 0.002 seconds&lt;/code&gt;
    &lt;p&gt;Using &lt;code&gt;float32&lt;/code&gt; instead of &lt;code&gt;float64&lt;/code&gt; halves memory use and can speed up computation (at the cost of some precision).&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute the cost of multiplying a 200×500 matrix with a 500×1000 matrix. How many multiplications are needed?&lt;/item&gt;
      &lt;item&gt;Time matrix multiplication for sizes 100, 500, 1000 in NumPy. How does the time scale?&lt;/item&gt;
      &lt;item&gt;Experiment with &lt;code&gt;float32&lt;/code&gt;vs&lt;code&gt;float64&lt;/code&gt;in NumPy. How do speed and memory change?&lt;/item&gt;
      &lt;item&gt;Try broadcasting: multiply each column of a matrix by &lt;code&gt;[1,2,3,...]&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Matrix operations have predictable computational costs: &lt;code&gt;A·x&lt;/code&gt;~ \(m \times n\),&lt;code&gt;A·B&lt;/code&gt;~ \(m \times n \times p\).&lt;/item&gt;
      &lt;item&gt;Vectorized NumPy operations are vastly faster than Python loops.&lt;/item&gt;
      &lt;item&gt;Broadcasting and choosing the right data type are simple speedups every beginner should learn.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Chapter 3. Linear Systems and Elimination&lt;/head&gt;
    &lt;head rend="h3"&gt;21. From Equations to Matrices (Augmenting and Encoding)&lt;/head&gt;
    &lt;p&gt;Linear algebra often begins with solving systems of linear equations. For example:&lt;/p&gt;
    &lt;p&gt;\[ \begin{cases} x + 2y = 5 \\ 3x - y = 4 \end{cases} \]&lt;/p&gt;
    &lt;p&gt;Instead of juggling symbols, we can encode the entire system into a matrix. This is the key idea that lets computers handle thousands or millions of equations efficiently.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Write a system of equations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We’ll use this small example:&lt;/p&gt;
    &lt;p&gt;\[ \begin{cases} 2x + y = 8 \\ -3x + 4y = -11 \end{cases} \]&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Encode coefficients and constants&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Coefficient matrix \(A\): numbers multiplying variables.&lt;/item&gt;
      &lt;item&gt;Variable vector \(x\): unknowns &lt;code&gt;[x, y]&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Constant vector \(b\): right-hand side.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([
 A 2, 1],
     [-3, 4]
     [
 ])
= np.array([8, -11])
 b 
print("Coefficient matrix A:\n", A)
print("Constants vector b:", b)&lt;/code&gt;
    &lt;code&gt;Coefficient matrix A:
 [[ 2  1]
 [-3  4]]
Constants vector b: [  8 -11]&lt;/code&gt;
    &lt;p&gt;So the system is \(A·x = b\).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Augmented matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can bundle the system into one compact matrix:&lt;/p&gt;
    &lt;p&gt;\[ [A|b] = \begin{bmatrix}2 &amp;amp; 1 &amp;amp; | &amp;amp; 8 \\ -3 &amp;amp; 4 &amp;amp; | &amp;amp; -11 \end{bmatrix} \]&lt;/p&gt;
    &lt;code&gt;= np.column_stack((A, b))
 augmented print("Augmented matrix:\n", augmented)&lt;/code&gt;
    &lt;code&gt;Augmented matrix:
 [[  2   1   8]
 [ -3   4 -11]]&lt;/code&gt;
    &lt;p&gt;This format is useful for elimination algorithms.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Solving directly with NumPy&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.linalg.solve(A, b)
 solution print("Solution (x,y):", solution)&lt;/code&gt;
    &lt;code&gt;Solution (x,y): [3.90909091 0.18181818]&lt;/code&gt;
    &lt;p&gt;Here NumPy solves the system using efficient algorithms.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Checking the solution&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Always verify:&lt;/p&gt;
    &lt;code&gt;= A.dot(solution)
 check print("A·x =", check, "should equal b =", b)&lt;/code&gt;
    &lt;code&gt;A·x = [  8. -11.] should equal b = [  8 -11]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Another example (3 variables)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ \begin{cases} x + y + z = 6 \\ 2x - y + z = 3 \\ - x + 2y - z = 2 \end{cases} \]&lt;/p&gt;
    &lt;code&gt;= np.array([
 A 1, 1, 1],
     [2, -1, 1],
     [-1, 2, -1]
     [
 ])
= np.array([6, 3, 2])
 b 
print("Augmented matrix:\n", np.column_stack((A, b)))
print("Solution:", np.linalg.solve(A, b))&lt;/code&gt;
    &lt;code&gt;Augmented matrix:
 [[ 1  1  1  6]
 [ 2 -1  1  3]
 [-1  2 -1  2]]
Solution: [2.33333333 2.66666667 1.        ]&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Encode the system:&lt;/p&gt;&lt;p&gt;\[ \begin{cases} 2x - y = 1 \\ x + 3y = 7 \end{cases} \]&lt;/p&gt;&lt;p&gt;Write&lt;/p&gt;&lt;code&gt;A&lt;/code&gt;and&lt;code&gt;b&lt;/code&gt;, then solve.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;For a 3×3 system, try creating a random coefficient matrix with&lt;/p&gt;&lt;code&gt;np.random.randint(-5,5,(3,3))&lt;/code&gt;and a random&lt;code&gt;b&lt;/code&gt;. Use&lt;code&gt;np.linalg.solve&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Modify the constants&lt;/p&gt;&lt;code&gt;b&lt;/code&gt;slightly and see how the solution changes. This introduces the idea of sensitivity.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Systems of linear equations can be neatly written as \(A·x = b\).&lt;/item&gt;
      &lt;item&gt;The augmented matrix \([A|b]\) is a compact way to set up elimination.&lt;/item&gt;
      &lt;item&gt;This matrix encoding transforms algebra problems into matrix problems - the gateway to all of linear algebra.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;22. Row Operations (Legal Moves That Keep Solutions)&lt;/head&gt;
    &lt;p&gt;When solving linear systems, we don’t want to change the solutions - just simplify the system into an easier form. This is where row operations come in. They are the “legal moves” we can do on an augmented matrix \([A|b]\) without changing the solution set.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Three legal row operations&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Swap two rows \((R_i \leftrightarrow R_j)\)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multiply a row by a nonzero scalar \((R_i \to c·R_i)\)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Replace a row with itself plus a multiple of another row \((R_i \to R_i + c·R_j)\)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These preserve the solution set.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Start with an augmented matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;System:&lt;/p&gt;
    &lt;p&gt;\[ \begin{cases} x + 2y = 5 \\ 3x + 4y = 6 \end{cases} \]&lt;/p&gt;
    &lt;code&gt;= np.array([
 A 1, 2, 5],
     [3, 4, 6]
     [=float)
 ], dtype
print("Initial augmented matrix:\n", A)&lt;/code&gt;
    &lt;code&gt;Initial augmented matrix:
 [[1. 2. 5.]
 [3. 4. 6.]]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Row swap&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Swap row 0 and row 1.&lt;/p&gt;
    &lt;code&gt;0,1]] = A[[1,0]]
 A[[print("After swapping rows:\n", A)&lt;/code&gt;
    &lt;code&gt;After swapping rows:
 [[3. 4. 6.]
 [1. 2. 5.]]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Multiply a row by a scalar&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Make the pivot in row 0 equal to 1.&lt;/p&gt;
    &lt;code&gt;0] = A[0] / A[0,0]
 A[print("After scaling first row:\n", A)&lt;/code&gt;
    &lt;code&gt;After scaling first row:
 [[1.         1.33333333 2.        ]
 [1.         2.         5.        ]]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Add a multiple of another row&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Eliminate the first column of row 1.&lt;/p&gt;
    &lt;code&gt;1] = A[1] - 3*A[0]
 A[print("After eliminating x from second row:\n", A)&lt;/code&gt;
    &lt;code&gt;After eliminating x from second row:
 [[ 1.          1.33333333  2.        ]
 [-2.         -2.         -1.        ]]&lt;/code&gt;
    &lt;p&gt;Now the system is simpler: second row has only &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Solving from the new system&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= A[1,2] / A[1,1]
 y = (A[0,2] - A[0,1]*y) / A[0,0]
 x print("Solution: x =", x, ", y =", y)&lt;/code&gt;
    &lt;code&gt;Solution: x = 1.3333333333333335 , y = 0.5&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Using NumPy step-by-step vs solver&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[1,2],[3,4]])
 coeff = np.array([5,6])
 const print("np.linalg.solve result:", np.linalg.solve(coeff,const))&lt;/code&gt;
    &lt;code&gt;np.linalg.solve result: [-4.   4.5]&lt;/code&gt;
    &lt;p&gt;Both methods give the same solution.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Take the system:&lt;/p&gt;
        &lt;p&gt;\[ \begin{cases} 2x + y = 7 \\ x - y = 1 \end{cases} \]&lt;/p&gt;
        &lt;p&gt;Write its augmented matrix, then:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Swap rows.&lt;/item&gt;
          &lt;item&gt;Scale the first row.&lt;/item&gt;
          &lt;item&gt;Eliminate one variable.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Create a random 3×3 system with integers between -5 and 5. Perform at least one of each row operation manually in code.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Experiment with multiplying a row by&lt;/p&gt;&lt;code&gt;0&lt;/code&gt;. What happens, and why is this not allowed as a legal operation?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The three legal row operations are row swap, row scaling, and row replacement.&lt;/item&gt;
      &lt;item&gt;These steps preserve the solution set while moving toward a simpler form.&lt;/item&gt;
      &lt;item&gt;They are the foundation of Gaussian elimination, the standard algorithm for solving linear systems.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;23. Row-Echelon and Reduced Row-Echelon Forms (Target Shapes)&lt;/head&gt;
    &lt;p&gt;When solving systems, our goal is to simplify the augmented matrix into a standard shape where the solutions are easy to read. These shapes are called row-echelon form (REF) and reduced row-echelon form (RREF).&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;p&gt;We’ll use NumPy for basic work and SymPy for exact RREF (since NumPy doesn’t have it built-in).&lt;/p&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Row-Echelon Form (REF)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All nonzero rows are above any zero rows.&lt;/item&gt;
      &lt;item&gt;Each leading entry (pivot) is to the right of the pivot in the row above.&lt;/item&gt;
      &lt;item&gt;Pivots are usually scaled to 1, but not strictly required.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example system:&lt;/p&gt;
    &lt;p&gt;\[ \begin{cases} x + 2y + z = 7 \\ 2x + 4y + z = 12 \\ 3x + 6y + 2z = 17 \end{cases} \]&lt;/p&gt;
    &lt;code&gt;= np.array([
 A 1, 2, 1, 7],
     [2, 4, 1, 12],
     [3, 6, 2, 17]
     [=float)
 ], dtype
print("Augmented matrix:\n", A)&lt;/code&gt;
    &lt;code&gt;Augmented matrix:
 [[ 1.  2.  1.  7.]
 [ 2.  4.  1. 12.]
 [ 3.  6.  2. 17.]]&lt;/code&gt;
    &lt;p&gt;Perform elimination manually:&lt;/p&gt;
    &lt;code&gt;# eliminate first column entries below pivot
1] = A[1] - 2*A[0]
 A[2] = A[2] - 3*A[0]
 A[print("After eliminating first column:\n", A)&lt;/code&gt;
    &lt;code&gt;After eliminating first column:
 [[ 1.  2.  1.  7.]
 [ 0.  0. -1. -2.]
 [ 0.  0. -1. -4.]]&lt;/code&gt;
    &lt;p&gt;Now the pivots move diagonally across the matrix - this is row-echelon form.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Reduced Row-Echelon Form (RREF) In RREF, we go further:&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Every pivot = 1.&lt;/item&gt;
      &lt;item&gt;Every pivot is the only nonzero in its column.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Instead of coding manually, we’ll let SymPy handle it:&lt;/p&gt;
    &lt;code&gt;= Matrix([
 M 1, 2, 1, 7],
     [2, 4, 1, 12],
     [3, 6, 2, 17]
     [
 ])
= M.rref()
 M_rref print("RREF form:\n", M_rref[0])&lt;/code&gt;
    &lt;code&gt;RREF form:
 Matrix([[1, 2, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])&lt;/code&gt;
    &lt;p&gt;SymPy shows the final canonical form.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Reading solutions from RREF&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If the RREF looks like:&lt;/p&gt;
    &lt;p&gt;\[ \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; a &amp;amp; b \\ 0 &amp;amp; 1 &amp;amp; c &amp;amp; d \\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \end{bmatrix} \]&lt;/p&gt;
    &lt;p&gt;It means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The first two variables are leading (pivots).&lt;/item&gt;
      &lt;item&gt;The third variable is free.&lt;/item&gt;
      &lt;item&gt;Solutions can be written in terms of the free variable.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A quick example with free variables&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;System:&lt;/p&gt;
    &lt;p&gt;\[ x + y + z = 3 \\ 2x + y - z = 0 \]&lt;/p&gt;
    &lt;code&gt;= Matrix([
 M2 1,1,1,3],
     [2,1,-1,0]
     [
 ])
= M2.rref()
 M2_rref print("RREF form:\n", M2_rref[0])&lt;/code&gt;
    &lt;code&gt;RREF form:
 Matrix([[1, 0, -2, -3], [0, 1, 3, 6]])&lt;/code&gt;
    &lt;p&gt;Here, one column will not have a pivot → that variable is free.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Take the system:&lt;/p&gt;
        &lt;p&gt;\[ 2x + 3y = 6, \quad 4x + 6y = 12 \]&lt;/p&gt;
        &lt;p&gt;Write the augmented matrix and compute its RREF. What does it tell you about solutions?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Create a random 3×4 matrix in NumPy. Use SymPy’s&lt;/p&gt;&lt;code&gt;Matrix.rref()&lt;/code&gt;to compute its reduced form. Identify the pivot columns.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;For the system:&lt;/p&gt;
        &lt;p&gt;\[ x + 2y + 3z = 4, \quad 2x + 4y + 6z = 8 \]&lt;/p&gt;
        &lt;p&gt;Check if the equations are independent or multiples of each other by looking at the RREF.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;REF organizes equations into a staircase shape.&lt;/item&gt;
      &lt;item&gt;RREF goes further, making each pivot the only nonzero in its column.&lt;/item&gt;
      &lt;item&gt;These canonical forms make it easy to identify pivot variables, free variables, and the solution set structure.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;24. Pivots, Free Variables, and Leading Ones (Reading Solutions)&lt;/head&gt;
    &lt;p&gt;Once a matrix is in row-echelon or reduced row-echelon form, the solutions to the system become visible. The key is identifying pivots, leading ones, and free variables.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;What are pivots?&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A pivot is the first nonzero entry in a row (after elimination).&lt;/item&gt;
      &lt;item&gt;In RREF, pivots are scaled to &lt;code&gt;1&lt;/code&gt;and are called leading ones.&lt;/item&gt;
      &lt;item&gt;Pivot columns correspond to basic variables.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Example system&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ \begin{cases} x + y + z = 6 \\ 2x + 3y + z = 10 \end{cases} \]&lt;/p&gt;
    &lt;code&gt;= Matrix([
 M 1,1,1,6],
     [2,3,1,10]
     [
 ])
= M.rref()
 M_rref print("RREF form:\n", M_rref[0])&lt;/code&gt;
    &lt;code&gt;RREF form:
 Matrix([[1, 0, 2, 8], [0, 1, -1, -2]])&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Interpreting the RREF&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Suppose the RREF comes out as:&lt;/p&gt;
    &lt;p&gt;\[ \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; -2 &amp;amp; 4 \\ 0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 2 \end{bmatrix} \]&lt;/p&gt;
    &lt;p&gt;This means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Pivot columns: 1 and 2 → variables \(x\) and \(y\) are basic.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Free variable: \(z\).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Equations:&lt;/p&gt;
        &lt;p&gt;\[ x - 2z = 4, \quad y + z = 2 \]&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Solution in terms of \(z\):&lt;/p&gt;
        &lt;p&gt;\[ x = 4 + 2z, \quad y = 2 - z, \quad z = z \]&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Coding the solution extraction&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= M_rref
 rref_matrix, pivots print("Pivot columns:", pivots)

# free variables are the columns not in pivots
= set(range(rref_matrix.shape[1]-1))  # exclude last column (constants)
 all_vars = all_vars - set(pivots)
 free_vars print("Free variable indices:", free_vars)&lt;/code&gt;
    &lt;code&gt;Pivot columns: (0, 1)
Free variable indices: {2}&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Another example with infinitely many solutions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ x + 2y + 3z = 4, \quad 2x + 4y + 6z = 8 \]&lt;/p&gt;
    &lt;code&gt;= Matrix([
 M2 1,2,3,4],
     [2,4,6,8]
     [
 ])
= M2.rref()
 M2_rref print("RREF form:\n", M2_rref[0])&lt;/code&gt;
    &lt;code&gt;RREF form:
 Matrix([[1, 2, 3, 4], [0, 0, 0, 0]])&lt;/code&gt;
    &lt;p&gt;The second row becomes all zeros, showing redundancy. Pivot in column 1, free variables in columns 2 and 3.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Solving underdetermined systems&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you have more variables than equations, expect free variables. Example:&lt;/p&gt;
    &lt;p&gt;\[ x + y = 3 \]&lt;/p&gt;
    &lt;code&gt;= Matrix([[1,1,3]])
 M3 print("RREF form:\n", M3.rref()[0])&lt;/code&gt;
    &lt;code&gt;RREF form:
 Matrix([[1, 1, 3]])&lt;/code&gt;
    &lt;p&gt;Here, \(x = 3 - y\). Variable \(y\) is free.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Take the system:&lt;/p&gt;
        &lt;p&gt;\[ x + y + z = 2, \quad y + z = 1 \]&lt;/p&gt;
        &lt;p&gt;Compute its RREF and identify pivot and free variables.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Create a random 3×4 system and compute its pivots. How many free variables do you get?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;For the system:&lt;/p&gt;
        &lt;p&gt;\[ x - y = 0, \quad 2x - 2y = 0 \]&lt;/p&gt;
        &lt;p&gt;Verify that the system has infinitely many solutions and describe them in terms of a free variable.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pivots / leading ones mark the basic variables.&lt;/item&gt;
      &lt;item&gt;Free variables correspond to non-pivot columns.&lt;/item&gt;
      &lt;item&gt;Solutions are written in terms of free variables, showing whether the system has a unique, infinite, or no solution.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;25. Solving Consistent Systems (Unique vs. Infinite Solutions)&lt;/head&gt;
    &lt;p&gt;Now that we can spot pivots and free variables, we can classify systems of equations as having a unique solution or infinitely many solutions (assuming they’re consistent). In this lab, we’ll practice solving both types.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Unique solution example&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;System:&lt;/p&gt;
    &lt;p&gt;\[ x + y = 3, \quad 2x - y = 0 \]&lt;/p&gt;
    &lt;code&gt;from sympy import Matrix

= Matrix([
 M 1, 1, 3],
     [2, -1, 0]
     [
 ])
= M.rref()
 M_rref print("RREF form:\n", M_rref[0])

# Split into coefficient matrix A and right-hand side b
= M[:, :2]
 A = M[:, 2]
 b 
= A.solve_least_squares(b)
 solution print("Solution:", solution)&lt;/code&gt;
    &lt;code&gt;RREF form:
 Matrix([[1, 0, 1], [0, 1, 2]])
Solution: Matrix([[1], [2]])&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Infinite solution example&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;System:&lt;/p&gt;
    &lt;p&gt;\[ x + y + z = 2, \quad 2x + 2y + 2z = 4 \]&lt;/p&gt;
    &lt;code&gt;= Matrix([
 M2 1, 1, 1, 2],
     [2, 2, 2, 4]
     [
 ])
= M2.rref()
 M2_rref print("RREF form:\n", M2_rref[0])&lt;/code&gt;
    &lt;code&gt;RREF form:
 Matrix([[1, 1, 1, 2], [0, 0, 0, 0]])&lt;/code&gt;
    &lt;p&gt;Only one pivot → two free variables.&lt;/p&gt;
    &lt;p&gt;Interpretation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(x = 2 - y - z\)&lt;/item&gt;
      &lt;item&gt;\(y, z\) are free&lt;/item&gt;
      &lt;item&gt;Infinite solutions described by parameters.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Classifying consistency&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A system is consistent if the RREF does not have a row like:&lt;/p&gt;
    &lt;p&gt;\[ [0, 0, 0, c] \quad (c \neq 0) \]&lt;/p&gt;
    &lt;p&gt;Example consistent system:&lt;/p&gt;
    &lt;code&gt;= Matrix([
 M3 1, 2, 3],
     [0, 1, 4]
     [
 ])print("RREF:\n", M3.rref()[0])&lt;/code&gt;
    &lt;code&gt;RREF:
 Matrix([[1, 0, -5], [0, 1, 4]])&lt;/code&gt;
    &lt;p&gt;Example inconsistent system (no solution):&lt;/p&gt;
    &lt;code&gt;= Matrix([
 M4 1, 1, 2],
     [2, 2, 5]
     [
 ])print("RREF:\n", M4.rref()[0])&lt;/code&gt;
    &lt;code&gt;RREF:
 Matrix([[1, 1, 0], [0, 0, 1]])&lt;/code&gt;
    &lt;p&gt;The second one ends with &lt;code&gt;[0,0,1]&lt;/code&gt;, meaning contradiction (0 = 1).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Quick NumPy comparison&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For systems with unique solutions:&lt;/p&gt;
    &lt;code&gt;= np.array([[1,1],[2,-1]], dtype=float)
 A = np.array([3,0], dtype=float)
 b print("Unique solution with np.linalg.solve:", np.linalg.solve(A,b))&lt;/code&gt;
    &lt;code&gt;Unique solution with np.linalg.solve: [1. 2.]&lt;/code&gt;
    &lt;p&gt;For systems with infinite solutions, &lt;code&gt;np.linalg.solve&lt;/code&gt; will fail, but SymPy handles parametric solutions.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Solve:&lt;/p&gt;
        &lt;p&gt;\[ x + y + z = 1, \quad 2x + 3y + z = 2 \]&lt;/p&gt;
        &lt;p&gt;Is the solution unique or infinite?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Check consistency of:&lt;/p&gt;
        &lt;p&gt;\[ x + 2y = 3, \quad 2x + 4y = 8 \]&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Build a random 3×4 augmented matrix and compute its RREF. Identify:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Does it have a unique solution, infinitely many, or none?&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unique solution: pivot in every variable column.&lt;/item&gt;
      &lt;item&gt;Infinite solutions: free variables remain, system is still consistent.&lt;/item&gt;
      &lt;item&gt;No solution: an inconsistent row appears.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Understanding pivots and free variables gives a complete picture of the solution set.&lt;/p&gt;
    &lt;head rend="h3"&gt;26. Detecting Inconsistency (When No Solution Exists)&lt;/head&gt;
    &lt;p&gt;Not all systems of linear equations can be solved. Some are inconsistent, meaning the equations contradict each other. In this lab, we’ll learn how to recognize inconsistency using augmented matrices and RREF.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;An inconsistent system&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ x + y = 2, \quad 2x + 2y = 5 \]&lt;/p&gt;
    &lt;p&gt;Notice the second equation looks like a multiple of the first, but the constant doesn’t match - contradiction.&lt;/p&gt;
    &lt;code&gt;= Matrix([
 M 1, 1, 2],
     [2, 2, 5]
     [
 ])
= M.rref()
 M_rref print("RREF:\n", M_rref[0])&lt;/code&gt;
    &lt;code&gt;RREF:
 Matrix([[1, 1, 0], [0, 0, 1]])&lt;/code&gt;
    &lt;p&gt;RREF gives:&lt;/p&gt;
    &lt;p&gt;\[ \begin{bmatrix} 1 &amp;amp; 1 &amp;amp; 2 \\ 0 &amp;amp; 0 &amp;amp; 1 \end{bmatrix} \]&lt;/p&gt;
    &lt;p&gt;The last row means \(0 = 1\), so no solution exists.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A consistent system (for contrast)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ x + y = 2, \quad 2x + 2y = 4 \]&lt;/p&gt;
    &lt;code&gt;= Matrix([
 M2 1, 1, 2],
     [2, 2, 4]
     [
 ])
print("RREF:\n", M2.rref()[0])&lt;/code&gt;
    &lt;code&gt;RREF:
 Matrix([[1, 1, 2], [0, 0, 0]])&lt;/code&gt;
    &lt;p&gt;This reduces to one equation and a redundant row of zeros → infinitely many solutions.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visualizing inconsistency (2D case)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;System:&lt;/p&gt;
    &lt;p&gt;\[ x + y = 2 \quad \text{and} \quad x + y = 3 \]&lt;/p&gt;
    &lt;p&gt;These are parallel lines that never meet.&lt;/p&gt;
    &lt;code&gt;import matplotlib.pyplot as plt

= np.linspace(-1, 3, 100)
 x_vals = 2 - x_vals
 y1 = 3 - x_vals
 y2 
="x+y=2")
 plt.plot(x_vals, y1, label="x+y=3")
 plt.plot(x_vals, y2, label

 plt.legend()0,color='black',linewidth=0.5)
 plt.axhline(0,color='black',linewidth=0.5)
 plt.axvline(
 plt.grid() plt.show()&lt;/code&gt;
    &lt;p&gt;The two lines are parallel → no solution.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Detecting inconsistency automatically&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can scan the RREF for a row of the form \([0, 0, …, c]\) with \(c \neq 0\).&lt;/p&gt;
    &lt;code&gt;def is_inconsistent(M):
= M.rref()
     rref_matrix, _ for row in rref_matrix.tolist():
     if all(v == 0 for v in row[:-1]) and row[-1] != 0:
         return True
             return False
     
print("System 1 inconsistent?", is_inconsistent(M))
print("System 2 inconsistent?", is_inconsistent(M2))&lt;/code&gt;
    &lt;code&gt;System 1 inconsistent? True
System 2 inconsistent? False&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Test the system:&lt;/p&gt;
        &lt;p&gt;\[ x + 2y = 4, \quad 2x + 4y = 10 \]&lt;/p&gt;
        &lt;p&gt;Write the augmented matrix and check if it’s inconsistent.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Build a random 2×3 augmented matrix with integer entries. Use&lt;/p&gt;&lt;code&gt;is_inconsistent&lt;/code&gt;to check.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Plot two linear equations in 2D. Adjust constants to see when they intersect (consistent) vs when they are parallel (inconsistent).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;A system is inconsistent if RREF contains a row like \([0,0,…,c]\) with \(c \neq 0\).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Geometrically, this means the equations describe parallel lines (2D), parallel planes (3D), or higher-dimensional contradictions.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Recognizing inconsistency quickly saves time and avoids chasing impossible solutions.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;27. Gaussian Elimination by Hand (A Disciplined Procedure)&lt;/head&gt;
    &lt;p&gt;Gaussian elimination is the systematic way to solve linear systems using row operations. It transforms the augmented matrix into row-echelon form (REF) and then uses back substitution to find solutions. In this lab, we’ll walk step by step through the process.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Example system&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ \begin{cases} x + y + z = 6 \\ 2x + 3y + z = 14 \\ x + 2y + 3z = 14 \end{cases} \]&lt;/p&gt;
    &lt;code&gt;= np.array([
 A 1, 1, 1, 6],
     [2, 3, 1, 14],
     [1, 2, 3, 14]
     [=float)
 ], dtype
print("Initial augmented matrix:\n", A)&lt;/code&gt;
    &lt;code&gt;Initial augmented matrix:
 [[ 1.  1.  1.  6.]
 [ 2.  3.  1. 14.]
 [ 1.  2.  3. 14.]]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Step 1: Get a pivot in the first column&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Make the pivot at (0,0) into 1 (it already is). Now eliminate below it.&lt;/p&gt;
    &lt;code&gt;1] = A[1] - 2*A[0]   # Row2 → Row2 - 2*Row1
 A[2] = A[2] - A[0]     # Row3 → Row3 - Row1
 A[print("After eliminating first column:\n", A)&lt;/code&gt;
    &lt;code&gt;After eliminating first column:
 [[ 1.  1.  1.  6.]
 [ 0.  1. -1.  2.]
 [ 0.  1.  2.  8.]]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Step 2: Pivot in the second column&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Make the pivot in row 1, col 1 equal to 1.&lt;/p&gt;
    &lt;code&gt;1] = A[1] / A[1,1]
 A[print("After scaling second row:\n", A)&lt;/code&gt;
    &lt;code&gt;After scaling second row:
 [[ 1.  1.  1.  6.]
 [ 0.  1. -1.  2.]
 [ 0.  1.  2.  8.]]&lt;/code&gt;
    &lt;p&gt;Now eliminate below:&lt;/p&gt;
    &lt;code&gt;2] = A[2] - A[2,1]*A[1]
 A[print("After eliminating second column:\n", A)&lt;/code&gt;
    &lt;code&gt;After eliminating second column:
 [[ 1.  1.  1.  6.]
 [ 0.  1. -1.  2.]
 [ 0.  0.  3.  6.]]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Step 3: Pivot in the third column&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Make the bottom-right entry into 1.&lt;/p&gt;
    &lt;code&gt;2] = A[2] / A[2,2]
 A[print("After scaling third row:\n", A)&lt;/code&gt;
    &lt;code&gt;After scaling third row:
 [[ 1.  1.  1.  6.]
 [ 0.  1. -1.  2.]
 [ 0.  0.  1.  2.]]&lt;/code&gt;
    &lt;p&gt;At this point, the matrix is in row-echelon form (REF).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Back substitution&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now solve from the bottom up:&lt;/p&gt;
    &lt;code&gt;= A[2,3]
 z = A[1,3] - A[1,2]*z
 y = A[0,3] - A[0,1]*y - A[0,2]*z
 x 
print(f"Solution: x={x}, y={y}, z={z}")&lt;/code&gt;
    &lt;code&gt;Solution: x=0.0, y=4.0, z=2.0&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verification&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([
 coeff 1,1,1],
     [2,3,1],
     [1,2,3]
     [=float)
 ], dtype= np.array([6,14,14], dtype=float)
 const 
print("Check with np.linalg.solve:", np.linalg.solve(coeff,const))&lt;/code&gt;
    &lt;code&gt;Check with np.linalg.solve: [0. 4. 2.]&lt;/code&gt;
    &lt;p&gt;The results match.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Solve:&lt;/p&gt;
        &lt;p&gt;\[ 2x + y = 5, \quad 4x - 6y = -2 \]&lt;/p&gt;
        &lt;p&gt;using Gaussian elimination manually in code.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Create a random 3×4 augmented matrix and reduce it step by step, printing after each row operation.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Compare your manual elimination to SymPy’s RREF with&lt;/p&gt;&lt;code&gt;Matrix.rref()&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gaussian elimination is a disciplined sequence of row operations.&lt;/item&gt;
      &lt;item&gt;It reduces the matrix to row-echelon form, from which back substitution is straightforward.&lt;/item&gt;
      &lt;item&gt;This method is the backbone of solving systems by hand and underlies many numerical algorithms.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;28. Back Substitution and Solution Sets (Finishing Cleanly)&lt;/head&gt;
    &lt;p&gt;Once Gaussian elimination reduces a system to row-echelon form (REF), the final step is back substitution. This means solving variables starting from the last equation and working upward. In this lab, we’ll practice both unique and infinite solution cases.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Unique solution example&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;System:&lt;/p&gt;
    &lt;p&gt;\[ x + y + z = 6, \quad 2y + 5z = -4, \quad z = 3 \]&lt;/p&gt;
    &lt;p&gt;Row-echelon form looks like:&lt;/p&gt;
    &lt;p&gt;\[ \begin{bmatrix} 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 6 \\ 0 &amp;amp; 2 &amp;amp; 5 &amp;amp; -4 \\ 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 3 \end{bmatrix} \]&lt;/p&gt;
    &lt;p&gt;Solve bottom-up:&lt;/p&gt;
    &lt;code&gt;= 3
 z = (-4 - 5*z)/2
 y = 6 - y - z
 x print(f"Solution: x={x}, y={y}, z={z}")&lt;/code&gt;
    &lt;code&gt;Solution: x=12.5, y=-9.5, z=3&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Infinite solution example&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;System:&lt;/p&gt;
    &lt;p&gt;\[ x + y + z = 2, \quad 2x + 2y + 2z = 4 \]&lt;/p&gt;
    &lt;p&gt;After elimination:&lt;/p&gt;
    &lt;p&gt;\[ \begin{bmatrix} 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 2 \\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \end{bmatrix} \]&lt;/p&gt;
    &lt;p&gt;This means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Equation: \(x + y + z = 2\).&lt;/item&gt;
      &lt;item&gt;Free variables: choose \(y\) and \(z\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let \(y = s, z = t\). Then:&lt;/p&gt;
    &lt;p&gt;\[ x = 2 - s - t \]&lt;/p&gt;
    &lt;p&gt;So the solution set is:&lt;/p&gt;
    &lt;code&gt;from sympy import symbols
= symbols('s t')
 s, t = 2 - s - t
 x = s
 y = t
 z print("General solution:")
print("x =", x, ", y =", y, ", z =", z)&lt;/code&gt;
    &lt;code&gt;General solution:
x = -s - t + 2 , y = s , z = t&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Consistency check with RREF&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can use SymPy to confirm solution sets:&lt;/p&gt;
    &lt;code&gt;= Matrix([
 M 1,1,1,2],
     [2,2,2,4]
     [
 ])
print("RREF form:\n", M.rref()[0])&lt;/code&gt;
    &lt;code&gt;RREF form:
 Matrix([[1, 1, 1, 2], [0, 0, 0, 0]])&lt;/code&gt;
    &lt;p&gt;The second row disappears, showing infinite solutions.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Encoding solution sets&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;General solutions are often written in parametric vector form.&lt;/p&gt;
    &lt;p&gt;For the infinite solution above:&lt;/p&gt;
    &lt;p&gt;\[ (x,y,z) = (2,0,0) + s(-1,1,0) + t(-1,0,1) \]&lt;/p&gt;
    &lt;p&gt;This shows the solution space is a plane in \(\mathbb{R}^3\).&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Solve:&lt;/p&gt;
        &lt;p&gt;\[ x + 2y = 5, \quad y = 1 \]&lt;/p&gt;
        &lt;p&gt;Do back substitution by hand and check with NumPy.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Take the system:&lt;/p&gt;
        &lt;p&gt;\[ x + y + z = 1, \quad 2x + 2y + 2z = 2 \]&lt;/p&gt;
        &lt;p&gt;Write its solution set in parametric form.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Use&lt;/p&gt;&lt;code&gt;Matrix.rref()&lt;/code&gt;on a 3×4 random augmented matrix. Identify pivot and free variables, then describe the solution set.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Back substitution is the cleanup step after Gaussian elimination.&lt;/item&gt;
      &lt;item&gt;It reveals whether the system has a unique solution or infinitely many.&lt;/item&gt;
      &lt;item&gt;Solutions can be expressed explicitly (unique case) or parametrically (infinite case).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;29. Rank and Its First Meaning (Pivots as Information)&lt;/head&gt;
    &lt;p&gt;The rank of a matrix tells us how much independent information it contains. Rank is one of the most important concepts in linear algebra because it connects to pivots, independence, dimension, and the number of solutions to a system.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Rank definition The rank is the number of pivots (leading ones) in the row-echelon form of a matrix.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;= Matrix([
 A 1, 2, 3],
     [2, 4, 6],
     [1, 1, 1]
     [
 ])
print("RREF:\n", A.rref()[0])
print("Rank of A:", A.rank())&lt;/code&gt;
    &lt;code&gt;RREF:
 Matrix([[1, 0, -1], [0, 1, 2], [0, 0, 0]])
Rank of A: 2&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The second row is a multiple of the first, so the rank is less than 3.&lt;/item&gt;
      &lt;item&gt;Only two independent rows → rank = 2.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Rank and solutions to \(A·x = b\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Consider:&lt;/p&gt;
    &lt;p&gt;\[ \begin{cases} x + y + z = 3 \\ 2x + 2y + 2z = 6 \\ x - y = 0 \end{cases} \]&lt;/p&gt;
    &lt;code&gt;= Matrix([
 M 1, 1, 1, 3],
     [2, 2, 2, 6],
     [1, -1, 0, 0]
     [
 ])
print("RREF:\n", M.rref()[0])
print("Rank of coefficient matrix:", M[:, :-1].rank())
print("Rank of augmented matrix:", M.rank())&lt;/code&gt;
    &lt;code&gt;RREF:
 Matrix([[1, 0, 1/2, 3/2], [0, 1, 1/2, 3/2], [0, 0, 0, 0]])
Rank of coefficient matrix: 2
Rank of augmented matrix: 2&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If rank(A) = rank([A|b]) = number of variables → unique solution.&lt;/item&gt;
      &lt;item&gt;If rank(A) = rank([A|b]) &amp;lt; number of variables → infinite solutions.&lt;/item&gt;
      &lt;item&gt;If rank(A) &amp;lt; rank([A|b]) → no solution.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;NumPy comparison&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([
 A 1, 2, 3],
     [2, 4, 6],
     [1, 1, 1]
     [=float)
 ], dtype
print("Rank with NumPy:", np.linalg.matrix_rank(A))&lt;/code&gt;
    &lt;code&gt;Rank with NumPy: 2&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Rank as “dimension of information”&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The rank equals:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The number of independent rows.&lt;/item&gt;
      &lt;item&gt;The number of independent columns.&lt;/item&gt;
      &lt;item&gt;The dimension of the column space.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([
 B 1,2],
     [2,4],
     [3,6]
     [
 ])
print("Rank of B:", B.rank())&lt;/code&gt;
    &lt;code&gt;Rank of B: 1&lt;/code&gt;
    &lt;p&gt;All columns are multiples → only one independent direction → rank = 1.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Compute the rank of:&lt;/p&gt;
        &lt;p&gt;\[ \begin{bmatrix} 1 &amp;amp; 2 &amp;amp; 3 \\ 2 &amp;amp; 4 &amp;amp; 6 \\ 3 &amp;amp; 6 &amp;amp; 9 \end{bmatrix} \]&lt;/p&gt;
        &lt;p&gt;What do you expect?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Create a random 4×4 matrix with&lt;/p&gt;&lt;code&gt;np.random.randint&lt;/code&gt;. Compute its rank with both SymPy and NumPy.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Test solution consistency using rank: build a system where rank(A) ≠ rank([A|b]) and show it has no solution.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rank = number of pivots = dimension of independent information.&lt;/item&gt;
      &lt;item&gt;Rank reveals whether a system has no solution, one solution, or infinitely many.&lt;/item&gt;
      &lt;item&gt;Rank connects algebra (pivots) with geometry (dimension of subspaces).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;30. LU Factorization (Elimination Captured as L and U)&lt;/head&gt;
    &lt;p&gt;Gaussian elimination can be recorded in a neat factorization:&lt;/p&gt;
    &lt;p&gt;\[ A = LU \]&lt;/p&gt;
    &lt;p&gt;where \(L\) is a lower triangular matrix (recording the multipliers we used) and \(U\) is an upper triangular matrix (the result of elimination). This is called LU factorization. It’s a powerful tool for solving systems efficiently.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from scipy.linalg import lu&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Example matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([
 A 2, 3, 1],
     [4, 7, 7],
     [6, 18, 22]
     [=float)
 ], dtype
print("Matrix A:\n", A)&lt;/code&gt;
    &lt;code&gt;Matrix A:
 [[ 2.  3.  1.]
 [ 4.  7.  7.]
 [ 6. 18. 22.]]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;LU decomposition with SciPy&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= lu(A)
 P, L, U 
print("Permutation matrix P:\n", P)
print("Lower triangular L:\n", L)
print("Upper triangular U:\n", U)&lt;/code&gt;
    &lt;code&gt;Permutation matrix P:
 [[0. 0. 1.]
 [0. 1. 0.]
 [1. 0. 0.]]
Lower triangular L:
 [[1.         0.         0.        ]
 [0.66666667 1.         0.        ]
 [0.33333333 0.6        1.        ]]
Upper triangular U:
 [[ 6.         18.         22.        ]
 [ 0.         -5.         -7.66666667]
 [ 0.          0.         -1.73333333]]&lt;/code&gt;
    &lt;p&gt;Here, \(P\) handles row swaps (partial pivoting), \(L\) is lower triangular, and \(U\) is upper triangular.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verifying the factorization&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= P @ L @ U
 reconstructed print("Does P·L·U equal A?\n", np.allclose(reconstructed, A))&lt;/code&gt;
    &lt;code&gt;Does P·L·U equal A?
 True&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Solving a system with LU&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Suppose we want to solve \(Ax = b\). Instead of working directly with \(A\), we solve in two steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Solve \(Ly = Pb\) (forward substitution).&lt;/item&gt;
      &lt;item&gt;Solve \(Ux = y\) (back substitution).&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([1, 2, 3], dtype=float)
 b 
# Step 1: Pb
= P @ b
 Pb 
# Step 2: forward substitution Ly = Pb
= np.linalg.solve(L, Pb)
 y 
# Step 3: back substitution Ux = y
= np.linalg.solve(U, y)
 x 
print("Solution x:", x)&lt;/code&gt;
    &lt;code&gt;Solution x: [ 0.5 -0.  -0. ]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Efficiency advantage&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If we have to solve many systems with the same \(A\) but different \(b\), we only compute \(LU\) once, then reuse it. This saves a lot of computation.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;NumPy’s built-in rank-revealing factorization&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While NumPy doesn’t have &lt;code&gt;lu&lt;/code&gt; directly, it works seamlessly with SciPy. For large matrices, LU decomposition is the backbone of solvers like &lt;code&gt;np.linalg.solve&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Compute LU decomposition for&lt;/p&gt;
        &lt;p&gt;\[ A = \begin{bmatrix} 1 &amp;amp; 2 &amp;amp; 0 \\ 3 &amp;amp; 4 &amp;amp; 4 \\ 5 &amp;amp; 6 &amp;amp; 3 \end{bmatrix} \]&lt;/p&gt;
        &lt;p&gt;Verify \(P·L·U = A\).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Solve \(Ax = b\) with&lt;/p&gt;
        &lt;p&gt;\[ b = [3,7,8] \]&lt;/p&gt;
        &lt;p&gt;using LU factorization.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Compare solving with LU factorization vs directly using&lt;/p&gt;&lt;code&gt;np.linalg.solve(A,b)&lt;/code&gt;. Are the answers the same?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;LU factorization captures Gaussian elimination in matrix form: \(A = P·L·U\).&lt;/item&gt;
      &lt;item&gt;It allows fast repeated solving of systems with different right-hand sides.&lt;/item&gt;
      &lt;item&gt;LU decomposition is a core technique in numerical linear algebra and the basis of many solvers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Chapter 4. Vector Spaces and Subspaces&lt;/head&gt;
    &lt;head rend="h3"&gt;31. Axioms of Vector Spaces (What “Space” Really Means)&lt;/head&gt;
    &lt;p&gt;Vector spaces generalize what we’ve been doing with vectors and matrices. Instead of just \(\mathbb{R}^n\), a vector space is any collection of objects (vectors) where addition and scalar multiplication follow specific axioms (rules). In this lab, we’ll explore these axioms concretely with Python.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Vector space example: \(\mathbb{R}^2\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s check two rules (axioms): closure under addition and scalar multiplication.&lt;/p&gt;
    &lt;code&gt;= np.array([1, 2])
 u = np.array([3, -1])
 v 
# Closure under addition
print("u + v =", u + v)

# Closure under scalar multiplication
= 5
 k print("k * u =", k * u)&lt;/code&gt;
    &lt;code&gt;u + v = [4 1]
k * u = [ 5 10]&lt;/code&gt;
    &lt;p&gt;Both results are still in \(\mathbb{R}^2\).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Zero vector and additive inverses&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Every vector space must contain a zero vector, and every vector must have an additive inverse.&lt;/p&gt;
    &lt;code&gt;= np.array([0, 0])
 zero = -u
 inverse_u print("Zero vector:", zero)
print("u + (-u) =", u + inverse_u)&lt;/code&gt;
    &lt;code&gt;Zero vector: [0 0]
u + (-u) = [0 0]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Distributive and associative properties&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Check:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(a(u+v) = au + av\)&lt;/item&gt;
      &lt;item&gt;\((a+b)u = au + bu\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= 2, 3
 a, b 
= a * (u + v)
 lhs1 = a*u + a*v
 rhs1 print("a(u+v) =", lhs1, ", au+av =", rhs1)

= (a+b) * u
 lhs2 = a*u + b*u
 rhs2 print("(a+b)u =", lhs2, ", au+bu =", rhs2)&lt;/code&gt;
    &lt;code&gt;a(u+v) = [8 2] , au+av = [8 2]
(a+b)u = [ 5 10] , au+bu = [ 5 10]&lt;/code&gt;
    &lt;p&gt;Both equalities hold → distributive laws confirmed.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A set that fails to be a vector space&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Consider only positive numbers with normal addition and scalar multiplication.&lt;/p&gt;
    &lt;code&gt;= [1, 2, 3]
 positive_numbers try:
print("Closure under negatives?", -1 * np.array(positive_numbers))
     except Exception as e:
print("Error:", e)     &lt;/code&gt;
    &lt;code&gt;Closure under negatives? [-1 -2 -3]&lt;/code&gt;
    &lt;p&gt;Negative results leave the set → not a vector space.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Python helper to check axioms&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can quickly check if a set of vectors is closed under addition and scalar multiplication.&lt;/p&gt;
    &lt;code&gt;def check_closure(vectors, scalars):
for v in vectors:
     for u in vectors:
         if not any(np.array_equal(v+u, w) for w in vectors):
             return False
                 for k in scalars:
         if not any(np.array_equal(k*v, w) for w in vectors):
             return False
                 return True
     
= [np.array([0,0]), np.array([1,0]), np.array([0,1]), np.array([1,1])]
 vectors = [0,1,-1]
 scalars print("Closed under addition and scalar multiplication?", check_closure(vectors, scalars))&lt;/code&gt;
    &lt;code&gt;Closed under addition and scalar multiplication? False&lt;/code&gt;
    &lt;p&gt;This small set is closed → it forms a vector space (a subspace of \(\mathbb{R}^2\)).&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verify that \(\mathbb{R}^3\) satisfies the vector space axioms using random vectors.&lt;/item&gt;
      &lt;item&gt;Test whether the set of all 2×2 matrices forms a vector space under normal addition and scalar multiplication.&lt;/item&gt;
      &lt;item&gt;Find an example of a set that fails closure (e.g., integers under division).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A vector space is any set where addition and scalar multiplication satisfy 10 standard axioms.&lt;/item&gt;
      &lt;item&gt;These rules ensure consistent algebraic behavior.&lt;/item&gt;
      &lt;item&gt;Many objects beyond arrows in \(\mathbb{R}^n\) (like polynomials or matrices) are vector spaces too.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;32. Subspaces, Column Space, and Null Space (Where Solutions Live)&lt;/head&gt;
    &lt;p&gt;A subspace is a smaller vector space sitting inside a bigger one. For matrices, two subspaces show up all the time:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Column space: all combinations of the matrix’s columns (possible outputs of \(Ax\)).&lt;/item&gt;
      &lt;item&gt;Null space: all vectors \(x\) such that \(Ax = 0\) (inputs that vanish).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This lab explores both in Python.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Column space basics&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Take:&lt;/p&gt;
    &lt;p&gt;\[ A = \begin{bmatrix} 1 &amp;amp; 2 \\ 2 &amp;amp; 4 \\ 3 &amp;amp; 6 \end{bmatrix} \]&lt;/p&gt;
    &lt;code&gt;= Matrix([
 A 1,2],
     [2,4],
     [3,6]
     [
 ])
print("Matrix A:\n", A)
print("Column space basis:\n", A.columnspace())
print("Rank (dimension of column space):", A.rank())&lt;/code&gt;
    &lt;code&gt;Matrix A:
 Matrix([[1, 2], [2, 4], [3, 6]])
Column space basis:
 [Matrix([
[1],
[2],
[3]])]
Rank (dimension of column space): 1&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The second column is a multiple of the first → column space has dimension 1.&lt;/item&gt;
      &lt;item&gt;All outputs of \(Ax\) lie on a line in \(\mathbb{R}^3\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Null space basics&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;print("Null space basis:\n", A.nullspace())&lt;/code&gt;
    &lt;code&gt;Null space basis:
 [Matrix([
[-2],
[ 1]])]&lt;/code&gt;
    &lt;p&gt;The null space contains all \(x\) where \(Ax=0\). Here, the null space is 1-dimensional (vectors like \([-2,1]\)).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A full-rank example&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([
 B 1,0,0],
     [0,1,0],
     [0,0,1]
     [
 ])
print("Column space basis:\n", B.columnspace())
print("Null space basis:\n", B.nullspace())&lt;/code&gt;
    &lt;code&gt;Column space basis:
 [Matrix([
[1],
[0],
[0]]), Matrix([
[0],
[1],
[0]]), Matrix([
[0],
[0],
[1]])]
Null space basis:
 []&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Column space = all of \(\mathbb{R}^3\).&lt;/item&gt;
      &lt;item&gt;Null space = only the zero vector.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Geometry link&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For \(A\) (rank 1, 2 columns):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Column space: line in \(\mathbb{R}^3\).&lt;/item&gt;
      &lt;item&gt;Null space: line in \(\mathbb{R}^2\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Together they explain the system \(Ax = b\):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If \(b\) is outside the column space, no solution exists.&lt;/item&gt;
      &lt;item&gt;If \(b\) is inside, solutions differ by a vector in the null space.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Quick NumPy version&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;NumPy doesn’t directly give null space, but we can compute it with SVD.&lt;/p&gt;
    &lt;code&gt;from numpy.linalg import svd

= np.array([[1,2],[2,4],[3,6]], dtype=float)
 A = svd(A)
 U, S, Vt 
= 1e-10
 tol = (S &amp;lt;= tol)
 null_mask = Vt.T[:, null_mask]
 null_space print("Null space (via SVD):\n", null_space)&lt;/code&gt;
    &lt;code&gt;Null space (via SVD):
 [[-0.89442719]
 [ 0.4472136 ]]&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Find the column space and null space of&lt;/p&gt;
        &lt;p&gt;\[ \begin{bmatrix} 1 &amp;amp; 1 \\ 0 &amp;amp; 1 \\ 0 &amp;amp; 0 \end{bmatrix} \]&lt;/p&gt;
        &lt;p&gt;How many dimensions does each have?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Generate a random 3×3 matrix. Compute its rank, column space, and null space.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Solve \(Ax = b\) with&lt;/p&gt;
        &lt;p&gt;\[ A = \begin{bmatrix} 1 &amp;amp; 2 \\ 2 &amp;amp; 4 \\ 3 &amp;amp; 6 \end{bmatrix}, \quad b = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} \]&lt;/p&gt;
        &lt;p&gt;and describe why it has infinitely many solutions.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The column space = all possible outputs of a matrix.&lt;/item&gt;
      &lt;item&gt;The null space = all inputs that map to zero.&lt;/item&gt;
      &lt;item&gt;These subspaces give the complete picture of what a matrix does.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;33. Span and Generating Sets (Coverage of a Space)&lt;/head&gt;
    &lt;p&gt;The span of a set of vectors is all the linear combinations you can make from them. If a set of vectors can “cover” a whole space, we call it a generating set. This lab shows how to compute and visualize spans.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix
import matplotlib.pyplot as plt&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Span in \(\mathbb{R}^2\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Two vectors that aren’t multiples span the whole plane.&lt;/p&gt;
    &lt;code&gt;= np.array([1, 0])
 u = np.array([0, 1])
 v 
= Matrix.hstack(Matrix(u), Matrix(v))
 M print("Rank:", M.rank())&lt;/code&gt;
    &lt;code&gt;Rank: 2&lt;/code&gt;
    &lt;p&gt;Rank = 2 → the span of \(\{u,v\}\) is all of \(\mathbb{R}^2\).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Dependent vectors (smaller span)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([1, 2])
 u = np.array([2, 4])
 v 
= Matrix.hstack(Matrix(u), Matrix(v))
 M print("Rank:", M.rank())&lt;/code&gt;
    &lt;code&gt;Rank: 1&lt;/code&gt;
    &lt;p&gt;Rank = 1 → these vectors only span a line.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visualizing a span&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s see what the span of two vectors looks like.&lt;/p&gt;
    &lt;code&gt;= np.array([1, 2])
 u = np.array([2, 1])
 v 
= np.linspace(-2, 2, 11)
 coeffs = []
 points for a in coeffs:
for b in coeffs:
     *u + b*v)
         points.append(a= np.array(points)
 points 
0], points[:,1], s=10)
 plt.scatter(points[:,0,color='black',linewidth=0.5)
 plt.axhline(0,color='black',linewidth=0.5)
 plt.axvline("Span of {u,v}")
 plt.title(
 plt.grid() plt.show()&lt;/code&gt;
    &lt;p&gt;You’ll see a filled grid - the entire plane, because the two vectors are independent.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Generating set of a space&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For \(\mathbb{R}^3\):&lt;/p&gt;
    &lt;code&gt;= [Matrix([1,0,0]), Matrix([0,1,0]), Matrix([0,0,1])]
 basis = Matrix.hstack(*basis)
 M print("Rank:", M.rank())&lt;/code&gt;
    &lt;code&gt;Rank: 3&lt;/code&gt;
    &lt;p&gt;Rank = 3 → this set spans the whole space.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Testing if a vector is in the span&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example: Is \([3,5]\) in the span of \([1,2]\) and \([2,1]\)?&lt;/p&gt;
    &lt;code&gt;= Matrix([1,2])
 u = Matrix([2,1])
 v = Matrix([3,5])
 target 
= Matrix.hstack(u,v)
 M = M.gauss_jordan_solve(target)
 solution print("Coefficients (a,b):", solution)&lt;/code&gt;
    &lt;code&gt;Coefficients (a,b): (Matrix([
[7/3],
[1/3]]), Matrix(0, 1, []))&lt;/code&gt;
    &lt;p&gt;If a solution exists, the target is in the span.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Test if \([4,6]\) is in the span of \([1,2]\).&lt;/item&gt;
      &lt;item&gt;Visualize the span of \([1,0,0]\) and \([0,1,0]\) in \(\mathbb{R}^3\). What does it look like?&lt;/item&gt;
      &lt;item&gt;Create a random 3×3 matrix. Use &lt;code&gt;rank()&lt;/code&gt;to check if its columns span \(\mathbb{R}^3\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Span = all linear combinations of a set of vectors.&lt;/item&gt;
      &lt;item&gt;Independent vectors span bigger spaces; dependent ones collapse to smaller spaces.&lt;/item&gt;
      &lt;item&gt;Generating sets are the foundation of bases and coordinate systems.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;34. Linear Independence and Dependence (No Redundancy vs. Redundancy)&lt;/head&gt;
    &lt;p&gt;A set of vectors is linearly independent if none of them can be written as a combination of the others. If at least one can, the set is dependent. This distinction tells us whether a set of vectors has redundancy.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Independent vectors example&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([1, 0, 0])
 v1 = Matrix([0, 1, 0])
 v2 = Matrix([0, 0, 1])
 v3 
= Matrix.hstack(v1, v2, v3)
 M print("Rank:", M.rank(), " Number of vectors:", M.shape[1])&lt;/code&gt;
    &lt;code&gt;Rank: 3  Number of vectors: 3&lt;/code&gt;
    &lt;p&gt;Rank = 3, number of vectors = 3 → all independent.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Dependent vectors example&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([1, 2, 3])
 v1 = Matrix([2, 4, 6])
 v2 = Matrix([3, 6, 9])
 v3 
= Matrix.hstack(v1, v2, v3)
 M print("Rank:", M.rank(), " Number of vectors:", M.shape[1])&lt;/code&gt;
    &lt;code&gt;Rank: 1  Number of vectors: 3&lt;/code&gt;
    &lt;p&gt;Rank = 1, number of vectors = 3 → they’re dependent (multiples of each other).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Checking dependence automatically&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A quick test: if rank &amp;lt; number of vectors → dependent.&lt;/p&gt;
    &lt;code&gt;def check_independence(vectors):
= Matrix.hstack(*vectors)
     M return M.rank() == M.shape[1]
     
print("Independent?", check_independence([Matrix([1,0]), Matrix([0,1])]))
print("Independent?", check_independence([Matrix([1,2]), Matrix([2,4])]))&lt;/code&gt;
    &lt;code&gt;Independent? True
Independent? False&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Solving for dependence relation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If vectors are dependent, we can find coefficients \(c_1, c_2, …\) such that&lt;/p&gt;
    &lt;p&gt;\[ c_1 v_1 + c_2 v_2 + … + c_k v_k = 0 \]&lt;/p&gt;
    &lt;p&gt;with some \(c_i \neq 0\).&lt;/p&gt;
    &lt;code&gt;= Matrix.hstack(Matrix([1,2]), Matrix([2,4]))
 M = M.nullspace()
 null_space print("Dependence relation (coefficients):", null_space)&lt;/code&gt;
    &lt;code&gt;Dependence relation (coefficients): [Matrix([
[-2],
[ 1]])]&lt;/code&gt;
    &lt;p&gt;This shows the exact linear relation.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Random example&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;0)
 np.random.seed(= Matrix(np.random.randint(-3, 4, (3,3)))
 R print("Random matrix:\n", R)
print("Rank:", R.rank())&lt;/code&gt;
    &lt;code&gt;Random matrix:
 Matrix([[1, 2, -3], [0, 0, 0], [-2, 0, 2]])
Rank: 2&lt;/code&gt;
    &lt;p&gt;Depending on the rank, the columns may be independent (rank = 3) or dependent (rank &amp;lt; 3).&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Test if \([1,1,0], [0,1,1], [1,2,1]\) are independent.&lt;/item&gt;
      &lt;item&gt;Generate 4 random vectors in \(\mathbb{R}^3\). Can they ever be independent? Why or why not?&lt;/item&gt;
      &lt;item&gt;Find the dependence relation for \([2,4], [3,6]\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Independent set: no redundancy, each vector adds a new direction.&lt;/item&gt;
      &lt;item&gt;Dependent set: at least one vector is unnecessary (it lies in the span of others).&lt;/item&gt;
      &lt;item&gt;Independence is the key to defining basis and dimension.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;35. Basis and Coordinates (Naming Every Vector Uniquely)&lt;/head&gt;
    &lt;p&gt;A basis is a set of independent vectors that span a space. It’s like choosing a coordinate system: every vector in the space can be expressed uniquely as a combination of basis vectors. In this lab, we’ll see how to find bases and compute coordinates relative to them.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Standard basis in \(\mathbb{R}^3\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([1,0,0])
 e1 = Matrix([0,1,0])
 e2 = Matrix([0,0,1])
 e3 
= Matrix.hstack(e1, e2, e3)
 M print("Rank:", M.rank())&lt;/code&gt;
    &lt;code&gt;Rank: 3&lt;/code&gt;
    &lt;p&gt;These three independent vectors form the standard basis of \(\mathbb{R}^3\). Any vector like \([2,5,-1]\) can be expressed as&lt;/p&gt;
    &lt;p&gt;\[ 2e_1 + 5e_2 - 1e_3 \]&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Finding a basis from dependent vectors&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([1,2,3])
 v1 = Matrix([2,4,6])
 v2 = Matrix([1,0,1])
 v3 
= Matrix.hstack(v1,v2,v3)
 M print("Column space basis:", M.columnspace())&lt;/code&gt;
    &lt;code&gt;Column space basis: [Matrix([
[1],
[2],
[3]]), Matrix([
[1],
[0],
[1]])]&lt;/code&gt;
    &lt;p&gt;SymPy extracts independent columns automatically. This gives a basis for the column space.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Coordinates relative to a basis&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Suppose basis = \(\{ [1,0], [1,1] \}\). Express vector \([3,5]\) in this basis.&lt;/p&gt;
    &lt;code&gt;= Matrix.hstack(Matrix([1,0]), Matrix([1,1]))
 B = Matrix([3,5])
 target 
= B.solve_least_squares(target)
 coords print("Coordinates in basis B:", coords)&lt;/code&gt;
    &lt;code&gt;Coordinates in basis B: Matrix([[-2], [5]])&lt;/code&gt;
    &lt;p&gt;So \([3,5] = 3·[1,0] + 2·[1,1]\).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Basis change&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If we switch to a different basis, coordinates change but the vector stays the same.&lt;/p&gt;
    &lt;code&gt;= Matrix.hstack(Matrix([2,1]), Matrix([1,2]))
 new_basis = new_basis.solve_least_squares(target)
 coords_new print("Coordinates in new basis:", coords_new)&lt;/code&gt;
    &lt;code&gt;Coordinates in new basis: Matrix([[1/3], [7/3]])&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Random example&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Generate 3 random vectors in \(\mathbb{R}^3\). Check if they form a basis.&lt;/p&gt;
    &lt;code&gt;1)
 np.random.seed(= Matrix(np.random.randint(-3,4,(3,3)))
 R print("Random matrix:\n", R)
print("Rank:", R.rank())&lt;/code&gt;
    &lt;code&gt;Random matrix:
 Matrix([[2, 0, 1], [-3, -2, 0], [2, -3, -3]])
Rank: 3&lt;/code&gt;
    &lt;p&gt;If rank = 3 → basis for \(\mathbb{R}^3\). Otherwise, only span a subspace.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Check if \([1,2], [3,4]\) form a basis of \(\mathbb{R}^2\).&lt;/item&gt;
      &lt;item&gt;Express vector \([7,5]\) in that basis.&lt;/item&gt;
      &lt;item&gt;Create 4 random vectors in \(\mathbb{R}^3\). Find a basis for their span.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A basis = minimal set of vectors that span a space.&lt;/item&gt;
      &lt;item&gt;Every vector has a unique coordinate representation in a given basis.&lt;/item&gt;
      &lt;item&gt;Changing bases changes the coordinates, not the vector itself.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;36. Dimension (How Many Directions)&lt;/head&gt;
    &lt;p&gt;The dimension of a vector space is the number of independent directions it has. Formally, it’s the number of vectors in any basis of the space. Dimension tells us the “size” of a space in terms of degrees of freedom.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Dimension of \(\mathbb{R}^n\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The dimension of \(\mathbb{R}^n\) is \(n\).&lt;/p&gt;
    &lt;code&gt;= 4
 n = [Matrix.eye(n)[:,i] for i in range(n)]
 basis print("Basis for R^4:", basis)
print("Dimension of R^4:", len(basis))&lt;/code&gt;
    &lt;code&gt;Basis for R^4: [Matrix([
[1],
[0],
[0],
[0]]), Matrix([
[0],
[1],
[0],
[0]]), Matrix([
[0],
[0],
[1],
[0]]), Matrix([
[0],
[0],
[0],
[1]])]
Dimension of R^4: 4&lt;/code&gt;
    &lt;p&gt;Each standard unit vector adds one independent direction → dimension = 4.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Dimension via rank&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The rank of a matrix equals the dimension of its column space.&lt;/p&gt;
    &lt;code&gt;= Matrix([
 A 1,2,3],
     [2,4,6],
     [1,0,1]
     [
 ])
print("Rank (dimension of column space):", A.rank())&lt;/code&gt;
    &lt;code&gt;Rank (dimension of column space): 2&lt;/code&gt;
    &lt;p&gt;Here, rank = 2 → the column space is a 2D plane inside \(\mathbb{R}^3\).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Null space dimension&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The null space dimension is given by:&lt;/p&gt;
    &lt;p&gt;\[ \text{dim(Null(A))} = \#\text{variables} - \text{rank(A)} \]&lt;/p&gt;
    &lt;code&gt;print("Null space basis:", A.nullspace())
print("Dimension of null space:", len(A.nullspace()))&lt;/code&gt;
    &lt;code&gt;Null space basis: [Matrix([
[-1],
[-1],
[ 1]])]
Dimension of null space: 1&lt;/code&gt;
    &lt;p&gt;This is the number of free variables in a solution.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Dimension in practice&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A line through the origin in \(\mathbb{R}^3\) has dimension 1.&lt;/item&gt;
      &lt;item&gt;A plane through the origin has dimension 2.&lt;/item&gt;
      &lt;item&gt;The whole \(\mathbb{R}^3\) has dimension 3.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;= Matrix([1,2,3])
 v1 = Matrix([2,4,6])
 v2 = Matrix.hstack(v1,v2)
 span print("Dimension of span:", span.rank())&lt;/code&gt;
    &lt;code&gt;Dimension of span: 1&lt;/code&gt;
    &lt;p&gt;Result = 1 → they only generate a line.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Random example&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;2)
 np.random.seed(= Matrix(np.random.randint(-3,4,(4,4)))
 R print("Random 4x4 matrix:\n", R)
print("Column space dimension:", R.rank())&lt;/code&gt;
    &lt;code&gt;Random 4x4 matrix:
 Matrix([[-3, 2, -3, 3], [0, -1, 0, -3], [-1, -2, 0, 2], [-1, 1, 1, 1]])
Column space dimension: 4&lt;/code&gt;
    &lt;p&gt;Rank may be 4 (full space) or smaller (collapsed).&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Find the dimension of the column space of&lt;/p&gt;
        &lt;p&gt;\[ \begin{bmatrix} 1 &amp;amp; 1 &amp;amp; 1 \\ 0 &amp;amp; 1 &amp;amp; 1 \\ 0 &amp;amp; 0 &amp;amp; 0 \end{bmatrix} \]&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Compute the dimension of the null space of a 3×3 singular matrix.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Generate a 5×3 random matrix and compute its column space dimension.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dimension = number of independent directions.&lt;/item&gt;
      &lt;item&gt;Found by counting basis vectors (or rank).&lt;/item&gt;
      &lt;item&gt;Dimensions describe lines (1D), planes (2D), and higher subspaces inside larger spaces.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;37. Rank–Nullity Theorem (Dimensions That Add Up)&lt;/head&gt;
    &lt;p&gt;The rank–nullity theorem ties together the dimension of the column space and the null space of a matrix. It says:&lt;/p&gt;
    &lt;p&gt;\[ \text{rank}(A) + \text{nullity}(A) = \text{number of columns of } A \]&lt;/p&gt;
    &lt;p&gt;This is a powerful consistency check in linear algebra.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Simple 3×3 example&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([
 A 1, 2, 3],
     [2, 4, 6],
     [1, 0, 1]
     [
 ])
= A.rank()
 rank = len(A.nullspace())
 nullity print("Rank:", rank)
print("Nullity:", nullity)
print("Rank + Nullity =", rank + nullity)
print("Number of columns =", A.shape[1])&lt;/code&gt;
    &lt;code&gt;Rank: 2
Nullity: 1
Rank + Nullity = 3
Number of columns = 3&lt;/code&gt;
    &lt;p&gt;You should see that rank + nullity = 3, the number of columns.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Full-rank case&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([
 B 1,0,0],
     [0,1,0],
     [0,0,1]
     [
 ])
print("Rank:", B.rank())
print("Nullity:", len(B.nullspace()))&lt;/code&gt;
    &lt;code&gt;Rank: 3
Nullity: 0&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rank = 3 (all independent).&lt;/item&gt;
      &lt;item&gt;Nullity = 0 (only zero solution to \(Bx=0\)).&lt;/item&gt;
      &lt;item&gt;Rank + Nullity = 3 columns.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Wide matrix (more columns than rows)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([
 C 1,2,3,4],
     [0,1,1,2],
     [0,0,0,0]
     [
 ])
= C.rank()
 rank = len(C.nullspace())
 nullity print("Rank:", rank, " Nullity:", nullity, " Columns:", C.shape[1])&lt;/code&gt;
    &lt;code&gt;Rank: 2  Nullity: 2  Columns: 4&lt;/code&gt;
    &lt;p&gt;Here, nullity &amp;gt; 0 because there are more variables than independent equations.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verifying with random matrices&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;3)
 np.random.seed(= Matrix(np.random.randint(-3,4,(4,5)))
 R print("Random 4x5 matrix:\n", R)
print("Rank + Nullity =", R.rank() + len(R.nullspace()))
print("Number of columns =", R.shape[1])&lt;/code&gt;
    &lt;code&gt;Random 4x5 matrix:
 Matrix([[-1, -3, -2, 0, -3], [-3, -3, 2, 2, 0], [-1, 0, -2, -2, -1], [2, 3, -3, 1, 1]])
Rank + Nullity = 5
Number of columns = 5&lt;/code&gt;
    &lt;p&gt;Always consistent: rank + nullity = number of columns.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Geometric interpretation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For an \(m \times n\) matrix:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rank(A) = dimension of outputs (column space).&lt;/item&gt;
      &lt;item&gt;Nullity(A) = dimension of hidden directions that collapse to 0.&lt;/item&gt;
      &lt;item&gt;Together, they use up all the “input dimensions” (n).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Compute rank and nullity of&lt;/p&gt;
        &lt;p&gt;\[ \begin{bmatrix} 1 &amp;amp; 1 &amp;amp; 1 \\ 0 &amp;amp; 1 &amp;amp; 1 \end{bmatrix} \]&lt;/p&gt;
        &lt;p&gt;Check the theorem.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Create a 2×4 random integer matrix. Confirm that rank + nullity = 4.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Explain why a tall full-rank \(5 \times 3\) matrix must have nullity = 0.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rank + Nullity = number of columns (always true).&lt;/item&gt;
      &lt;item&gt;Rank measures independent outputs; nullity measures hidden freedom.&lt;/item&gt;
      &lt;item&gt;This theorem connects solutions of \(Ax=0\) with the structure of \(A\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;38. Coordinates Relative to a Basis (Changing the “Ruler”)&lt;/head&gt;
    &lt;p&gt;Once we choose a basis, every vector can be described with coordinates relative to that basis. This is like changing the “ruler” we use to measure vectors. In this lab, we’ll practice computing coordinates in different bases.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Standard basis coordinates&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Vector \(v = [4,5]\) in \(\mathbb{R}^2\):&lt;/p&gt;
    &lt;code&gt;= Matrix([4,5])
 v = Matrix([1,0])
 e1 = Matrix([0,1])
 e2 
= Matrix.hstack(e1,e2)
 B = B.solve_least_squares(v)
 coords print("Coordinates in standard basis:", coords)&lt;/code&gt;
    &lt;code&gt;Coordinates in standard basis: Matrix([[4], [5]])&lt;/code&gt;
    &lt;p&gt;Result is just \([4,5]\). Easy - the standard basis matches the components directly.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Non-standard basis&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Suppose basis = \(\{ [1,1], [1,-1] \}\). Express \(v = [4,5]\) in this basis.&lt;/p&gt;
    &lt;code&gt;= Matrix.hstack(Matrix([1,1]), Matrix([1,-1]))
 B2 = B2.solve_least_squares(v)
 coords2 print("Coordinates in new basis:", coords2)&lt;/code&gt;
    &lt;code&gt;Coordinates in new basis: Matrix([[9/2], [-1/2]])&lt;/code&gt;
    &lt;p&gt;Now \(v\) has different coordinates.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Changing coordinates back&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To reconstruct the vector from coordinates:&lt;/p&gt;
    &lt;code&gt;= B2 * coords2
 reconstructed print("Reconstructed vector:", reconstructed)&lt;/code&gt;
    &lt;code&gt;Reconstructed vector: Matrix([[4], [5]])&lt;/code&gt;
    &lt;p&gt;It matches the original \([4,5]\).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Random basis in \(\mathbb{R}^3\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix.hstack(
 basis 1,0,1]),
     Matrix([0,1,1]),
     Matrix([1,1,0])
     Matrix([
 )= Matrix([2,3,4])
 v 
= basis.solve_least_squares(v)
 coords print("Coordinates of v in random basis:", coords)&lt;/code&gt;
    &lt;code&gt;Coordinates of v in random basis: Matrix([[3/2], [5/2], [1/2]])&lt;/code&gt;
    &lt;p&gt;Any independent set of 3 vectors in \(\mathbb{R}^3\) works as a basis.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visualization in 2D&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s compare coordinates in two bases.&lt;/p&gt;
    &lt;code&gt;import matplotlib.pyplot as plt

= np.array([4,5])
 v = np.array([1,1])
 b1 = np.array([1,-1])
 b2 
0,0,v[0],v[1],angles='xy',scale_units='xy',scale=1,color='blue',label='v')
 plt.quiver(0,0,b1[0],b1[1],angles='xy',scale_units='xy',scale=1,color='red',label='basis1')
 plt.quiver(0,0,b2[0],b2[1],angles='xy',scale_units='xy',scale=1,color='green',label='basis2')
 plt.quiver(
-1,6)
 plt.xlim(-6,6)
 plt.ylim(
 plt.legend()
 plt.grid() plt.show()&lt;/code&gt;
    &lt;p&gt;Even though the basis vectors look different, they span the same space, and \(v\) can be expressed in terms of them.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Express \([7,3]\) in the basis \(\{[2,0], [0,3]\}\).&lt;/item&gt;
      &lt;item&gt;Pick three independent random vectors in \(\mathbb{R}^3\). Write down the coordinates of \([1,2,3]\) in that basis.&lt;/item&gt;
      &lt;item&gt;Verify that reconstructing always gives the original vector.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A basis provides a coordinate system for vectors.&lt;/item&gt;
      &lt;item&gt;Coordinates depend on the basis, but the underlying vector doesn’t change.&lt;/item&gt;
      &lt;item&gt;Changing the basis is like changing the “ruler” you measure vectors with.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;39. Change-of-Basis Matrices (Moving Between Coordinate Systems)&lt;/head&gt;
    &lt;p&gt;When we switch from one basis to another, we need a change-of-basis matrix. This matrix acts like a translator: it converts coordinates in one system to coordinates in another.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Two bases in \(\mathbb{R}^2\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s define:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Basis \(B = \{ [1,0], [0,1] \}\) (standard basis).&lt;/item&gt;
      &lt;item&gt;Basis \(C = \{ [1,1], [1,-1] \}\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix.hstack(Matrix([1,0]), Matrix([0,1]))
 B = Matrix.hstack(Matrix([1,1]), Matrix([1,-1])) C &lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Change-of-basis matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The matrix that converts C-coordinates → standard coordinates is just \(C\).&lt;/p&gt;
    &lt;code&gt;print("C (basis matrix):\n", C)&lt;/code&gt;
    &lt;code&gt;C (basis matrix):
 Matrix([[1, 1], [1, -1]])&lt;/code&gt;
    &lt;p&gt;To go the other way (standard → C), we compute the inverse of \(C\).&lt;/p&gt;
    &lt;code&gt;= C.inv()
 C_inv print("C inverse:\n", C_inv)&lt;/code&gt;
    &lt;code&gt;C inverse:
 Matrix([[1/2, 1/2], [1/2, -1/2]])&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Converting coordinates&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Vector \(v = [4,5]\).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In standard basis:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([4,5])
 v = v
 coords_in_standard print("Coordinates in standard basis:", coords_in_standard)&lt;/code&gt;
    &lt;code&gt;Coordinates in standard basis: Matrix([[4], [5]])&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In basis \(C\):&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= C_inv * v
 coords_in_C print("Coordinates in C basis:", coords_in_C)&lt;/code&gt;
    &lt;code&gt;Coordinates in C basis: Matrix([[9/2], [-1/2]])&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Convert back:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= C * coords_in_C
 reconstructed print("Reconstructed vector:", reconstructed)&lt;/code&gt;
    &lt;code&gt;Reconstructed vector: Matrix([[4], [5]])&lt;/code&gt;
    &lt;p&gt;The reconstruction matches the original vector.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;General formula&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If \(P\) is the change-of-basis matrix from basis \(B\) to basis \(C\):&lt;/p&gt;
    &lt;p&gt;\[ [v]_C = P^{-1}[v]_B \]&lt;/p&gt;
    &lt;p&gt;\[ [v]_B = P[v]_C \]&lt;/p&gt;
    &lt;p&gt;Here, \(P\) is the matrix of new basis vectors written in terms of the old basis.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Random 3D example&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix.eye(3)  # standard basis
 B = Matrix.hstack(
 C 1,0,1]),
     Matrix([0,1,1]),
     Matrix([1,1,0])
     Matrix([
 )
= Matrix([2,3,4])
 v 
= C.inv()
 C_inv = C_inv * v
 coords_in_C print("Coordinates in new basis C:", coords_in_C)

print("Back to standard:", C * coords_in_C)&lt;/code&gt;
    &lt;code&gt;Coordinates in new basis C: Matrix([[3/2], [5/2], [1/2]])
Back to standard: Matrix([[2], [3], [4]])&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Convert \([7,3]\) from the standard basis to the basis \(\{[2,0], [0,3]\}\).&lt;/item&gt;
      &lt;item&gt;Pick a random invertible 3×3 matrix as a basis. Write a vector in that basis, then convert it back to the standard basis.&lt;/item&gt;
      &lt;item&gt;Prove that converting back and forth always returns the same vector.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A change-of-basis matrix converts coordinates between bases.&lt;/item&gt;
      &lt;item&gt;Going from new basis → old basis uses the basis matrix.&lt;/item&gt;
      &lt;item&gt;Going from old basis → new basis requires its inverse.&lt;/item&gt;
      &lt;item&gt;The vector itself never changes - only the description of it does.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;40. Affine Subspaces (Lines and Planes Not Through the Origin)&lt;/head&gt;
    &lt;p&gt;So far, subspaces always passed through the origin. But many familiar objects - like lines offset from the origin or planes floating in space - are affine subspaces. They look like subspaces, just shifted away from zero.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
import matplotlib.pyplot as plt
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Line through the origin (a subspace)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ L = \{ t \cdot [1,2] : t \in \mathbb{R} \} \]&lt;/p&gt;
    &lt;code&gt;= np.linspace(-3,3,20)
 t = np.array([t, 2*t]).T
 line_origin 0], line_origin[:,1], label="Through origin") plt.plot(line_origin[:,&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Line not through the origin (affine subspace)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ L' = \{ [3,1] + t \cdot [1,2] : t \in \mathbb{R} \} \]&lt;/p&gt;
    &lt;code&gt;= np.array([3,1])
 point = np.array([1,2])
 direction = np.array([point + k*direction for k in t])
 line_shifted 0], line_shifted[:,1], label="Shifted line") plt.plot(line_shifted[:,&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visualizing together&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;*point, color="red", label="Shift point")
 plt.scatter(0,color='black',linewidth=0.5)
 plt.axhline(0,color='black',linewidth=0.5)
 plt.axvline(
 plt.legend()
 plt.grid() plt.show()&lt;/code&gt;
    &lt;p&gt;One line passes through the origin, the other is parallel but shifted.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Plane example&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A plane in \(\mathbb{R}^3\):&lt;/p&gt;
    &lt;p&gt;\[ P = \{ [1,2,3] + s[1,0,0] + t[0,1,0] : s,t \in \mathbb{R} \} \]&lt;/p&gt;
    &lt;p&gt;This is an affine plane parallel to the \(xy\)-plane, but shifted.&lt;/p&gt;
    &lt;code&gt;= np.linspace(-2,2,10)
 s_vals = np.linspace(-2,2,10)
 t_vals 
= []
 points for s in s_vals:
for t in t_vals:
     1,2,3] + s*np.array([1,0,0]) + t*np.array([0,1,0]))
         points.append([
= np.array(points)
 points 
from mpl_toolkits.mplot3d import Axes3D
= plt.figure()
 fig = fig.add_subplot(111, projection='3d')
 ax 0], points[:,1], points[:,2])
 ax.scatter(points[:,"Affine plane in R^3")
 ax.set_title( plt.show()&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Algebraic difference&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A subspace must satisfy closure under addition and scalar multiplication, and must include 0.&lt;/item&gt;
      &lt;item&gt;An affine subspace is just a subspace plus a fixed shift vector.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Define a line in \(\mathbb{R}^2\):&lt;/p&gt;
        &lt;p&gt;\[ (x,y) = (2,3) + t(1,-1) \]&lt;/p&gt;
        &lt;p&gt;Plot it and compare with the subspace spanned by \((1,-1)\).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Construct an affine plane in \(\mathbb{R}^3\) shifted by vector \((5,5,5)\).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Show algebraically that subtracting the shift point turns an affine subspace back into a regular subspace.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Subspaces go through the origin.&lt;/item&gt;
      &lt;item&gt;Affine subspaces are shifted copies of subspaces.&lt;/item&gt;
      &lt;item&gt;They’re essential in geometry, computer graphics, and optimization (e.g., feasible regions in linear programming).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Chapter 5. Linear Transformation and Structure&lt;/head&gt;
    &lt;head rend="h3"&gt;41. Linear Transformations (Preserving Lines and Sums)&lt;/head&gt;
    &lt;p&gt;A linear transformation is a function between vector spaces that preserves two key properties:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Additivity: \(T(u+v) = T(u) + T(v)\)&lt;/item&gt;
      &lt;item&gt;Homogeneity: \(T(cu) = cT(u)\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In practice, every linear transformation can be represented by a matrix. This lab will help you understand and experiment with linear transformations in Python.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
import matplotlib.pyplot as plt&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Simple linear transformation (scaling)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s scale vectors by 2 in the x-direction and by 0.5 in the y-direction.&lt;/p&gt;
    &lt;code&gt;= np.array([
 A 2, 0],
     [0, 0.5]
     [
 ])
= np.array([1, 2])
 v = A @ v
 Tv print("Original v:", v)
print("Transformed Tv:", Tv)&lt;/code&gt;
    &lt;code&gt;Original v: [1 2]
Transformed Tv: [2. 1.]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visualizing multiple vectors&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= [np.array([1,1]), np.array([2,0]), np.array([-1,2])]
 vectors 
for v in vectors:
= A @ v
     Tv 0,0,v[0],v[1],head_width=0.1,color='blue',length_includes_head=True)
     plt.arrow(0,0,Tv[0],Tv[1],head_width=0.1,color='red',length_includes_head=True)
     plt.arrow(
0,color='black',linewidth=0.5)
 plt.axhline(0,color='black',linewidth=0.5)
 plt.axvline(-3,5)
 plt.xlim(-1,5)
 plt.ylim(
 plt.grid()"Blue = original, Red = transformed")
 plt.title( plt.show()&lt;/code&gt;
    &lt;p&gt;Blue arrows are the original vectors; red arrows are the transformed ones. Notice how the transformation stretches and compresses consistently.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Rotation as a linear transformation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Rotating vectors by \(\theta = 90^\circ\):&lt;/p&gt;
    &lt;code&gt;= np.pi/2
 theta = np.array([
 R -np.sin(theta)],
     [np.cos(theta), 
     [np.sin(theta),  np.cos(theta)]
 ])
= np.array([1,0])
 v print("Rotate [1,0] by 90°:", R @ v)&lt;/code&gt;
    &lt;code&gt;Rotate [1,0] by 90°: [6.123234e-17 1.000000e+00]&lt;/code&gt;
    &lt;p&gt;The result is \([0,1]\), a perfect rotation.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Checking linearity&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([1,2])
 u = np.array([3,4])
 v = 5
 c 
= A @ (u+v)
 lhs = A@u + A@v
 rhs print("Additivity holds?", np.allclose(lhs,rhs))

= A @ (c*u)
 lhs = c*(A@u)
 rhs print("Homogeneity holds?", np.allclose(lhs,rhs))&lt;/code&gt;
    &lt;code&gt;Additivity holds? True
Homogeneity holds? True&lt;/code&gt;
    &lt;p&gt;Both checks return &lt;code&gt;True&lt;/code&gt;, proving \(T\) is linear.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Non-linear example (for contrast)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A transformation like \(T(x,y) = (x^2, y)\) is not linear.&lt;/p&gt;
    &lt;code&gt;def nonlinear(v):
return np.array([v[0]**2, v[1]])
     
print("T([2,3]) =", nonlinear(np.array([2,3])))
print("Check additivity:", nonlinear(np.array([1,2])+np.array([3,4])) == (nonlinear([1,2])+nonlinear([3,4])))&lt;/code&gt;
    &lt;code&gt;T([2,3]) = [4 3]
Check additivity: [False  True]&lt;/code&gt;
    &lt;p&gt;This fails the additivity test, so it’s not linear.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Define a shear matrix&lt;/p&gt;
        &lt;p&gt;\[ S = \begin{bmatrix} 1 &amp;amp; 1 \\ 0 &amp;amp; 1 \end{bmatrix} \]&lt;/p&gt;
        &lt;p&gt;Apply it to vectors and plot before/after.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Verify linearity for rotation by 45°.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Test whether \(T(x,y) = (x+y, y)\) is linear.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A linear transformation preserves vector addition and scalar multiplication.&lt;/item&gt;
      &lt;item&gt;Every linear transformation can be represented by a matrix.&lt;/item&gt;
      &lt;item&gt;Visualizing with arrows helps build geometric intuition: stretching, rotating, and shearing are all linear.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;42. Matrix Representation of a Linear Map (Choosing a Basis)&lt;/head&gt;
    &lt;p&gt;Every linear transformation can be written as a matrix, but the exact matrix depends on the basis you choose. This lab shows how to build and interpret matrix representations.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;From transformation to matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Suppose \(T: \mathbb{R}^2 \to \mathbb{R}^2\) is defined by:&lt;/p&gt;
    &lt;p&gt;\[ T(x,y) = (2x + y, \; x - y) \]&lt;/p&gt;
    &lt;p&gt;To find its matrix in the standard basis, apply \(T\) to each basis vector:&lt;/p&gt;
    &lt;code&gt;= Matrix([1,0])
 e1 = Matrix([0,1])
 e2 
def T(v):
= v
     x, y return Matrix([2*x + y, x - y])
     
print("T(e1):", T(e1))
print("T(e2):", T(e2))&lt;/code&gt;
    &lt;code&gt;T(e1): Matrix([[2], [1]])
T(e2): Matrix([[1], [-1]])&lt;/code&gt;
    &lt;p&gt;Stacking results as columns gives the matrix:&lt;/p&gt;
    &lt;code&gt;= Matrix.hstack(T(e1), T(e2))
 A print("Matrix representation in standard basis:\n", A)&lt;/code&gt;
    &lt;code&gt;Matrix representation in standard basis:
 Matrix([[2, 1], [1, -1]])&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Using the matrix for computations&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([3,4])
 v print("T(v) via definition:", T(v))
print("T(v) via matrix:", A*v)&lt;/code&gt;
    &lt;code&gt;T(v) via definition: Matrix([[10], [-1]])
T(v) via matrix: Matrix([[10], [-1]])&lt;/code&gt;
    &lt;p&gt;Both methods match.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Matrix in a different basis&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now suppose we use basis&lt;/p&gt;
    &lt;p&gt;\[ B = \{ [1,1], [1,-1] \} \]&lt;/p&gt;
    &lt;p&gt;To represent \(T\) in this basis:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build the change-of-basis matrix \(P\).&lt;/item&gt;
      &lt;item&gt;Compute \(A_B = P^{-1}AP\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix.hstack(Matrix([1,1]), Matrix([1,-1]))
 B = B
 P = P.inv() * A * P
 A_B print("Matrix representation in new basis:\n", A_B)&lt;/code&gt;
    &lt;code&gt;Matrix representation in new basis:
 Matrix([[3/2, 3/2], [3/2, -1/2]])&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Interpretation&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In standard basis, \(A\) tells us how \(T\) acts on unit vectors.&lt;/item&gt;
      &lt;item&gt;In basis \(B\), \(A_B\) shows how \(T\) looks when described using different coordinates.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Random linear map in \(\mathbb{R}^3\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;1)
 np.random.seed(= Matrix(np.random.randint(-3,4,(3,3)))
 A3 print("Random transformation matrix:\n", A3)

= Matrix.hstack(Matrix([1,0,1]), Matrix([0,1,1]), Matrix([1,1,0]))
 B3 = B3.inv() * A3 * B3
 A3_B print("Representation in new basis:\n", A3_B)&lt;/code&gt;
    &lt;code&gt;Random transformation matrix:
 Matrix([[2, 0, 1], [-3, -2, 0], [2, -3, -3]])
Representation in new basis:
 Matrix([[5/2, -3/2, 3], [-7/2, -9/2, -4], [1/2, 5/2, -1]])&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Define \(T(x,y) = (x+2y, 3x+y)\). Find its matrix in the standard basis.&lt;/item&gt;
      &lt;item&gt;Use a new basis \(\{[2,0],[0,3]\}\). Compute the representation \(A_B\).&lt;/item&gt;
      &lt;item&gt;Verify that applying \(T\) directly to a vector matches computing via \(A_B\) and change-of-basis.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A linear transformation becomes a matrix representation once a basis is chosen.&lt;/item&gt;
      &lt;item&gt;Columns of the matrix = images of basis vectors.&lt;/item&gt;
      &lt;item&gt;Changing the basis changes the matrix, but the transformation itself stays the same.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;43. Kernel and Image (Inputs That Vanish; Outputs We Can Reach)&lt;/head&gt;
    &lt;p&gt;Two fundamental subspaces describe any linear transformation \(T(x) = Ax\):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kernel (null space): all vectors \(x\) such that \(Ax = 0\).&lt;/item&gt;
      &lt;item&gt;Image (column space): all possible outputs \(Ax\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The kernel tells us what inputs collapse to zero, while the image tells us what outputs are achievable.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Kernel of a matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Consider&lt;/p&gt;
    &lt;p&gt;\[ A = \begin{bmatrix} 1 &amp;amp; 2 &amp;amp; 3 \\ 2 &amp;amp; 4 &amp;amp; 6 \end{bmatrix} \]&lt;/p&gt;
    &lt;code&gt;= Matrix([
 A 1,2,3],
     [2,4,6]
     [
 ])
print("Null space (kernel):", A.nullspace())&lt;/code&gt;
    &lt;code&gt;Null space (kernel): [Matrix([
[-2],
[ 1],
[ 0]]), Matrix([
[-3],
[ 0],
[ 1]])]&lt;/code&gt;
    &lt;p&gt;The null space basis shows dependencies among columns. Here, the kernel is 2-dimensional because columns are dependent.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Image (column space)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;print("Column space (image):", A.columnspace())
print("Rank (dimension of image):", A.rank())&lt;/code&gt;
    &lt;code&gt;Column space (image): [Matrix([
[1],
[2]])]
Rank (dimension of image): 1&lt;/code&gt;
    &lt;p&gt;The image is spanned by \([1,2]^T\). So all outputs of \(A\) are multiples of this vector.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Interpretation&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kernel vectors → directions that map to zero.&lt;/item&gt;
      &lt;item&gt;Image vectors → directions we can actually reach in the output space.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If \(x \in \ker(A)\), then \(Ax = 0\). If \(b\) is not in the image, the system \(Ax = b\) has no solution.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Example with full rank&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([
 B 1,0,0],
     [0,1,0],
     [0,0,1]
     [
 ])
print("Kernel of B:", B.nullspace())
print("Image of B:", B.columnspace())&lt;/code&gt;
    &lt;code&gt;Kernel of B: []
Image of B: [Matrix([
[1],
[0],
[0]]), Matrix([
[0],
[1],
[0]]), Matrix([
[0],
[0],
[1]])]&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kernel = only zero vector.&lt;/item&gt;
      &lt;item&gt;Image = all of \(\mathbb{R}^3\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;NumPy version (image via column space)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[1,2,3],[2,4,6]], dtype=float)
 A = np.linalg.matrix_rank(A)
 rank print("Rank with NumPy:", rank)&lt;/code&gt;
    &lt;code&gt;Rank with NumPy: 1&lt;/code&gt;
    &lt;p&gt;NumPy doesn’t compute null spaces directly, but we can use SVD for that if needed.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Compute kernel and image for&lt;/p&gt;
        &lt;p&gt;\[ \begin{bmatrix} 1 &amp;amp; 1 \\ 1 &amp;amp; 1 \end{bmatrix} \]&lt;/p&gt;
        &lt;p&gt;What do they look like?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Take a random 3×4 matrix and find its kernel and image dimensions.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Solve \(Ax = b\) for a matrix \(A\). Try two different \(b\): one inside the image, one outside. Observe the difference.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kernel = inputs that vanish under \(A\).&lt;/item&gt;
      &lt;item&gt;Image = outputs that can be reached by \(A\).&lt;/item&gt;
      &lt;item&gt;Together, they fully describe what a linear map does: what it “kills” and what it “produces.”&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;44. Invertibility and Isomorphisms (Perfectly Reversible Maps)&lt;/head&gt;
    &lt;p&gt;A matrix (or linear map) is invertible if it has an inverse \(A^{-1}\) such that&lt;/p&gt;
    &lt;p&gt;\[ A^{-1}A = I \quad \text{and} \quad AA^{-1} = I \]&lt;/p&gt;
    &lt;p&gt;An invertible map is also called an isomorphism, because it preserves all information - every input has exactly one output, and every output comes from exactly one input.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Checking invertibility&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([
 A 2,1],
     [5,3]
     [
 ])
print("Determinant:", A.det())
print("Is invertible?", A.det() != 0)&lt;/code&gt;
    &lt;code&gt;Determinant: 1
Is invertible? True&lt;/code&gt;
    &lt;p&gt;If determinant ≠ 0 → invertible.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Computing the inverse&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= A.inv()
 A_inv print("Inverse matrix:\n", A_inv)

print("Check A*A_inv = I:\n", A * A_inv)&lt;/code&gt;
    &lt;code&gt;Inverse matrix:
 Matrix([[3, -1], [-5, 2]])
Check A*A_inv = I:
 Matrix([[1, 0], [0, 1]])&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Solving systems with inverses&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For \(Ax = b\), if \(A\) is invertible:&lt;/p&gt;
    &lt;code&gt;= Matrix([1,2])
 b = A_inv * b
 x print("Solution x:", x)&lt;/code&gt;
    &lt;code&gt;Solution x: Matrix([[1], [-1]])&lt;/code&gt;
    &lt;p&gt;This is equivalent to &lt;code&gt;A.solve(b)&lt;/code&gt; in SymPy or &lt;code&gt;np.linalg.solve&lt;/code&gt; in NumPy.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Non-invertible (singular) example&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([
 B 1,2],
     [2,4]
     [
 ])
print("Determinant:", B.det())
print("Is invertible?", B.det() != 0)&lt;/code&gt;
    &lt;code&gt;Determinant: 0
Is invertible? False&lt;/code&gt;
    &lt;p&gt;Determinant = 0 → no inverse. The matrix collapses space onto a line, losing information.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;NumPy version&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[2,1],[5,3]], dtype=float)
 A print("Determinant:", np.linalg.det(A))
print("Inverse:\n", np.linalg.inv(A))&lt;/code&gt;
    &lt;code&gt;Determinant: 1.0000000000000002
Inverse:
 [[ 3. -1.]
 [-5.  2.]]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Geometric intuition&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Invertible transformation = reversible (like rotating, scaling by nonzero).&lt;/item&gt;
      &lt;item&gt;Non-invertible transformation = squashing space into a lower dimension (like flattening a plane onto a line).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Test whether&lt;/p&gt;
        &lt;p&gt;\[ \begin{bmatrix} 1 &amp;amp; 0 \\ 0 &amp;amp; 1 \end{bmatrix} \]&lt;/p&gt;
        &lt;p&gt;is invertible and find its inverse.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Compute the determinant of a 3×3 random integer matrix. If it’s nonzero, find its inverse.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Create a singular 3×3 matrix (make one row a multiple of another). Confirm it has no inverse.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Invertible matrix ↔︎ isomorphism: perfectly reversible, no information lost.&lt;/item&gt;
      &lt;item&gt;Determinant ≠ 0 → invertible; determinant = 0 → singular.&lt;/item&gt;
      &lt;item&gt;Inverses are useful conceptually, but in computation we usually solve systems directly instead of calculating \(A^{-1}\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;45. Composition, Powers, and Iteration (Doing It Again and Again)&lt;/head&gt;
    &lt;p&gt;Linear transformations can be chained together. Applying one after another is called composition, and in matrix form this becomes multiplication. Repeated application of the same transformation leads to powers of a matrix.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Composition of transformations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Suppose we have two linear maps:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(T_1\): rotate by 90°&lt;/item&gt;
      &lt;item&gt;\(T_2\): scale x by 2&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.pi/2
 theta = np.array([
 R -np.sin(theta)],
     [np.cos(theta), 
     [np.sin(theta),  np.cos(theta)]
 ])= np.array([
 S 2,0],
     [0,1]
     [
 ])
# Compose: apply R then S
= S @ R
 C print("Composite matrix:\n", C)&lt;/code&gt;
    &lt;code&gt;Composite matrix:
 [[ 1.2246468e-16 -2.0000000e+00]
 [ 1.0000000e+00  6.1232340e-17]]&lt;/code&gt;
    &lt;p&gt;Applying the composite matrix is equivalent to applying both maps in sequence.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verifying with a vector&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([1,1])
 v = R @ v
 step1 = S @ step1
 step2 = C @ v
 composite 
print("Step-by-step:", step2)
print("Composite:", composite)&lt;/code&gt;
    &lt;code&gt;Step-by-step: [-2.  1.]
Composite: [-2.  1.]&lt;/code&gt;
    &lt;p&gt;Both results are the same → composition = matrix multiplication.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Powers of a matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Repeatedly applying a transformation corresponds to matrix powers.&lt;/p&gt;
    &lt;p&gt;Example: scaling by 2.&lt;/p&gt;
    &lt;code&gt;= np.array([[2,0],[0,2]])
 A = np.array([1,1])
 v 
print("A @ v =", A @ v)
print("A^2 @ v =", np.linalg.matrix_power(A,2) @ v)
print("A^5 @ v =", np.linalg.matrix_power(A,5) @ v)&lt;/code&gt;
    &lt;code&gt;A @ v = [2 2]
A^2 @ v = [4 4]
A^5 @ v = [32 32]&lt;/code&gt;
    &lt;p&gt;Each step doubles the scaling effect.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Iteration dynamics&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s iterate a transformation many times and see what happens.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;p&gt;\[ A = \begin{bmatrix} 0.5 &amp;amp; 0 \\ 0 &amp;amp; 0.5 \end{bmatrix} \]&lt;/p&gt;
    &lt;code&gt;= np.array([[0.5,0],[0,0.5]])
 A = np.array([4,4])
 v 
for i in range(5):
= A @ v
     v print(f"Step {i+1}:", v)     &lt;/code&gt;
    &lt;code&gt;Step 1: [2. 2.]
Step 2: [1. 1.]
Step 3: [0.5 0.5]
Step 4: [0.25 0.25]
Step 5: [0.125 0.125]&lt;/code&gt;
    &lt;p&gt;Each step shrinks the vector → iteration can reveal stability.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Random example&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;0)
 np.random.seed(= np.random.randint(-2,3,(2,2))
 M print("Random matrix:\n", M)

print("M^2:\n", np.linalg.matrix_power(M,2))
print("M^3:\n", np.linalg.matrix_power(M,3))&lt;/code&gt;
    &lt;code&gt;Random matrix:
 [[ 2 -2]
 [ 1  1]]
M^2:
 [[ 2 -6]
 [ 3 -1]]
M^3:
 [[ -2 -10]
 [  5  -7]]&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Create two transformations: reflection across x-axis and scaling by 3. Compose them.&lt;/item&gt;
      &lt;item&gt;Take a shear matrix and compute \(A^5\). What happens to a vector after repeated application?&lt;/item&gt;
      &lt;item&gt;Experiment with a rotation matrix raised to higher powers. What cycle do you see?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Composition of linear maps = matrix multiplication.&lt;/item&gt;
      &lt;item&gt;Powers of a matrix represent repeated application.&lt;/item&gt;
      &lt;item&gt;Iteration reveals long-term dynamics: shrinking, growing, or oscillating behavior.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;46. Similarity and Conjugation (Same Action, Different Basis)&lt;/head&gt;
    &lt;p&gt;Two matrices \(A\) and \(B\) are called similar if there exists an invertible matrix \(P\) such that&lt;/p&gt;
    &lt;p&gt;\[ B = P^{-1} A P \]&lt;/p&gt;
    &lt;p&gt;This means \(A\) and \(B\) represent the same linear transformation, but in different bases. This lab explores similarity and why it matters.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Example with a change of basis&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([
 A 2,1],
     [0,2]
     [
 ])
= Matrix([
 P 1,1],
     [0,1]
     [
 ])
= P.inv() * A * P
 B print("Original A:\n", A)
print("Similar matrix B:\n", B)&lt;/code&gt;
    &lt;code&gt;Original A:
 Matrix([[2, 1], [0, 2]])
Similar matrix B:
 Matrix([[2, 1], [0, 2]])&lt;/code&gt;
    &lt;p&gt;Here, \(A\) and \(B\) are similar: they describe the same transformation in different coordinates.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Eigenvalues stay the same&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Similarity preserves eigenvalues.&lt;/p&gt;
    &lt;code&gt;print("Eigenvalues of A:", A.eigenvals())
print("Eigenvalues of B:", B.eigenvals())&lt;/code&gt;
    &lt;code&gt;Eigenvalues of A: {2: 2}
Eigenvalues of B: {2: 2}&lt;/code&gt;
    &lt;p&gt;Both matrices have the same eigenvalues, even though their entries differ.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Similarity and diagonalization&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If a matrix is diagonalizable, there exists \(P\) such that&lt;/p&gt;
    &lt;p&gt;\[ D = P^{-1} A P \]&lt;/p&gt;
    &lt;p&gt;where \(D\) is diagonal.&lt;/p&gt;
    &lt;code&gt;= Matrix([
 C 4,1],
     [0,2]
     [
 ])
= C.diagonalize()
 P, D print("Diagonal form D:\n", D)
print("Check similarity (P^-1 C P = D):\n", P.inv()*C*P)&lt;/code&gt;
    &lt;code&gt;Diagonal form D:
 Matrix([[2, 0], [0, 4]])
Check similarity (P^-1 C P = D):
 Matrix([[2, 0], [0, 4]])&lt;/code&gt;
    &lt;p&gt;Diagonalization is a special case of similarity, where the new matrix is as simple as possible.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;NumPy version&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[2,1],[0,2]], dtype=float)
 A = np.linalg.eig(A)
 eigvals, eigvecs print("Eigenvalues:", eigvals)
print("Eigenvectors (basis P):\n", eigvecs)&lt;/code&gt;
    &lt;code&gt;Eigenvalues: [2. 2.]
Eigenvectors (basis P):
 [[ 1.0000000e+00 -1.0000000e+00]
 [ 0.0000000e+00  4.4408921e-16]]&lt;/code&gt;
    &lt;p&gt;Here, eigenvectors form the change-of-basis matrix \(P\).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Geometric interpretation&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Similar matrices = same transformation, different “ruler” (basis).&lt;/item&gt;
      &lt;item&gt;Diagonalization = finding a ruler that makes the transformation look like pure stretching along axes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Take&lt;/p&gt;
        &lt;p&gt;\[ A = \begin{bmatrix} 1 &amp;amp; 1 \\ 0 &amp;amp; 1 \end{bmatrix} \]&lt;/p&gt;
        &lt;p&gt;and find a matrix \(P\) that gives a similar \(B\).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Show that two similar matrices have the same determinant and trace.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;For a random 3×3 matrix, check if it is diagonalizable using SymPy’s&lt;/p&gt;&lt;code&gt;.diagonalize()&lt;/code&gt;method.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Similarity = same linear map, different basis.&lt;/item&gt;
      &lt;item&gt;Similar matrices share eigenvalues, determinant, and trace.&lt;/item&gt;
      &lt;item&gt;Diagonalization is the simplest similarity form, making repeated computations (like powers) much easier.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;47. Projections and Reflections (Idempotent and Involutive Maps)&lt;/head&gt;
    &lt;p&gt;Two very common geometric linear maps are projections and reflections. They show up in graphics, physics, and optimization.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A projection squashes vectors onto a subspace (like dropping a shadow).&lt;/item&gt;
      &lt;item&gt;A reflection flips vectors across a line or plane (like a mirror).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix
import matplotlib.pyplot as plt&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Projection onto a line&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If we want to project onto the line spanned by \(u\), the projection matrix is:&lt;/p&gt;
    &lt;p&gt;\[ P = \frac{uu^T}{u^T u} \]&lt;/p&gt;
    &lt;code&gt;= np.array([2,1], dtype=float)
 u = u / np.linalg.norm(u)   # normalize
 u = np.outer(u,u)
 P 
print("Projection matrix:\n", P)&lt;/code&gt;
    &lt;code&gt;Projection matrix:
 [[0.8 0.4]
 [0.4 0.2]]&lt;/code&gt;
    &lt;p&gt;Apply projection:&lt;/p&gt;
    &lt;code&gt;= np.array([3,4], dtype=float)
 v = P @ v
 proj_v print("Original v:", v)
print("Projection of v onto u:", proj_v)&lt;/code&gt;
    &lt;code&gt;Original v: [3. 4.]
Projection of v onto u: [4. 2.]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visualization of projection&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;0,0,v[0],v[1],head_width=0.1,color="blue",length_includes_head=True)
 plt.arrow(0,0,proj_v[0],proj_v[1],head_width=0.1,color="red",length_includes_head=True)
 plt.arrow(0],proj_v[1],v[0]-proj_v[0],v[1]-proj_v[1],head_width=0.1,color="gray",linestyle="dashed")
 plt.arrow(proj_v[
0,color='black',linewidth=0.5)
 plt.axhline(0,color='black',linewidth=0.5)
 plt.axvline(
 plt.grid()"Blue = original, Red = projection, Gray = error vector")
 plt.title( plt.show()&lt;/code&gt;
    &lt;p&gt;The projection is the closest point on the line to the original vector.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Reflection across a line&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The reflection matrix across the line spanned by \(u\) is:&lt;/p&gt;
    &lt;p&gt;\[ R = 2P - I \]&lt;/p&gt;
    &lt;code&gt;= np.eye(2)
 I = 2*P - I
 R 
= R @ v
 reflect_v print("Reflection of v across line u:", reflect_v)&lt;/code&gt;
    &lt;code&gt;Reflection of v across line u: [ 5.0000000e+00 -4.4408921e-16]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Checking algebraic properties&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Projection: \(P^2 = P\) (idempotent).&lt;/item&gt;
      &lt;item&gt;Reflection: \(R^2 = I\) (involutive).&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;print("P^2 =\n", P @ P)
print("R^2 =\n", R @ R)&lt;/code&gt;
    &lt;code&gt;P^2 =
 [[0.8 0.4]
 [0.4 0.2]]
R^2 =
 [[ 1.00000000e+00 -1.59872116e-16]
 [-1.59872116e-16  1.00000000e+00]]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Projection in higher dimensions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Project onto the plane spanned by two vectors in \(\mathbb{R}^3\).&lt;/p&gt;
    &lt;code&gt;= np.array([1,0,0], dtype=float)
 u1 = np.array([0,1,0], dtype=float)
 u2 
= np.column_stack((u1,u2))   # basis for plane
 U = U @ np.linalg.inv(U.T @ U) @ U.T
 P_plane 
= np.array([1,2,3], dtype=float)
 v = P_plane @ v
 proj_plane print("Projection onto xy-plane:", proj_plane)&lt;/code&gt;
    &lt;code&gt;Projection onto xy-plane: [1. 2. 0.]&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Project \([4,5]\) onto the x-axis and verify the result.&lt;/item&gt;
      &lt;item&gt;Reflect \([1,2]\) across the line \(y=x\).&lt;/item&gt;
      &lt;item&gt;Create a random 3D vector and project it onto the plane spanned by \([1,1,0]\) and \([0,1,1]\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Projection: idempotent (\(P^2 = P\)), finds the closest vector in a subspace.&lt;/item&gt;
      &lt;item&gt;Reflection: involutive (\(R^2 = I\)), flips across a line/plane but preserves lengths.&lt;/item&gt;
      &lt;item&gt;Both are simple but powerful examples of linear transformations with clear geometry.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;48. Rotations and Shear (Geometric Intuition)&lt;/head&gt;
    &lt;p&gt;Two transformations often used in geometry, graphics, and physics are rotations and shears. Both are linear maps, but they behave differently:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rotation preserves lengths and angles.&lt;/item&gt;
      &lt;item&gt;Shear preserves area (in 2D) but distorts shapes, turning squares into parallelograms.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
import matplotlib.pyplot as plt&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Rotation in 2D&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The rotation matrix by angle \(\theta\) is:&lt;/p&gt;
    &lt;p&gt;\[ R(\theta) = \begin{bmatrix} \cos\theta &amp;amp; -\sin\theta \\ \sin\theta &amp;amp; \cos\theta \end{bmatrix} \]&lt;/p&gt;
    &lt;code&gt;def rotation_matrix(theta):
return np.array([
     -np.sin(theta)],
         [np.cos(theta), 
         [np.sin(theta),  np.cos(theta)]
     ])
= np.pi/4   # 45 degrees
 theta = rotation_matrix(theta)
 R 
= np.array([2,1])
 v = R @ v
 rotated_v print("Original v:", v)
print("Rotated v (45°):", rotated_v)&lt;/code&gt;
    &lt;code&gt;Original v: [2 1]
Rotated v (45°): [0.70710678 2.12132034]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visualizing rotation&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;0,0,v[0],v[1],head_width=0.1,color="blue",length_includes_head=True)
 plt.arrow(0,0,rotated_v[0],rotated_v[1],head_width=0.1,color="red",length_includes_head=True)
 plt.arrow(
0,color='black',linewidth=0.5)
 plt.axhline(0,color='black',linewidth=0.5)
 plt.axvline(
 plt.grid()"Blue = original, Red = rotated (45°)")
 plt.title("equal")
 plt.axis( plt.show()&lt;/code&gt;
    &lt;p&gt;The vector rotates counterclockwise by 45°.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Shear in 2D&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A shear along the x-axis by factor \(k\):&lt;/p&gt;
    &lt;p&gt;\[ S = \begin{bmatrix} 1 &amp;amp; k \\ 0 &amp;amp; 1 \end{bmatrix} \]&lt;/p&gt;
    &lt;code&gt;= 1.0
 k = np.array([
 S 1,k],
     [0,1]
     [
 ])
= S @ v
 sheared_v print("Sheared v:", sheared_v)&lt;/code&gt;
    &lt;code&gt;Sheared v: [3. 1.]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visualizing shear&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;0,0,v[0],v[1],head_width=0.1,color="blue",length_includes_head=True)
 plt.arrow(0,0,sheared_v[0],sheared_v[1],head_width=0.1,color="green",length_includes_head=True)
 plt.arrow(
0,color='black',linewidth=0.5)
 plt.axhline(0,color='black',linewidth=0.5)
 plt.axvline(
 plt.grid()"Blue = original, Green = sheared")
 plt.title("equal")
 plt.axis( plt.show()&lt;/code&gt;
    &lt;p&gt;The shear moves the vector sideways, distorting its angle.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Properties check&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rotation preserves length:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;print("||v|| =", np.linalg.norm(v))
print("||R v|| =", np.linalg.norm(rotated_v))&lt;/code&gt;
    &lt;code&gt;||v|| = 2.23606797749979
||R v|| = 2.2360679774997894&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Shear preserves area (determinant = 1):&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;print("det(S) =", np.linalg.det(S))&lt;/code&gt;
    &lt;code&gt;det(S) = 1.0&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Rotate \([1,0]\) by 90° and check it becomes \([0,1]\).&lt;/item&gt;
      &lt;item&gt;Apply shear with \(k=2\) to a square (points \((0,0),(1,0),(1,1),(0,1)\)) and plot before/after.&lt;/item&gt;
      &lt;item&gt;Combine rotation and shear: apply shear first, then rotation. What happens?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rotation: length- and angle-preserving, determinant = 1.&lt;/item&gt;
      &lt;item&gt;Shear: shape-distorting but area-preserving, determinant = 1.&lt;/item&gt;
      &lt;item&gt;Both are linear maps that provide geometric intuition and real-world modeling tools.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;49. Rank and Operator Viewpoint (Rank Beyond Elimination)&lt;/head&gt;
    &lt;p&gt;The rank of a matrix tells us how much “information” a linear map carries. Algebraically, it is the dimension of the image (column space). Geometrically, it measures how many independent directions survive the transformation.&lt;/p&gt;
    &lt;p&gt;From the operator viewpoint:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A matrix \(A\) is not just a table of numbers - it is a linear operator that maps vectors to other vectors.&lt;/item&gt;
      &lt;item&gt;The rank is the dimension of the output space that \(A\) actually reaches.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Rank via elimination (SymPy)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([
 A 1,2,3],
     [2,4,6],
     [1,1,1]
     [
 ])
print("Matrix A:\n", A)
print("Rank of A:", A.rank())&lt;/code&gt;
    &lt;code&gt;Matrix A:
 Matrix([[1, 2, 3], [2, 4, 6], [1, 1, 1]])
Rank of A: 2&lt;/code&gt;
    &lt;p&gt;Here, the second row is a multiple of the first → less independence → rank &amp;lt; 3.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Rank via NumPy&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[1,2,3],[2,4,6],[1,1,1]], dtype=float)
 A_np print("Rank (NumPy):", np.linalg.matrix_rank(A_np))&lt;/code&gt;
    &lt;code&gt;Rank (NumPy): 2&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Operator viewpoint&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s apply \(A\) to random vectors:&lt;/p&gt;
    &lt;code&gt;for v in [np.array([1,0,0]), np.array([0,1,0]), np.array([0,0,1])]:
print("A @", v, "=", A_np @ v)     &lt;/code&gt;
    &lt;code&gt;A @ [1 0 0] = [1. 2. 1.]
A @ [0 1 0] = [2. 4. 1.]
A @ [0 0 1] = [3. 6. 1.]&lt;/code&gt;
    &lt;p&gt;Even though we started in 3D, all outputs lie in a plane in \(\mathbb{R}^3\). That’s why rank = 2.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Full rank vs reduced rank&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Full rank: the transformation preserves dimension (no collapse).&lt;/item&gt;
      &lt;item&gt;Reduced rank: the transformation collapses onto a lower-dimensional subspace.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example full-rank:&lt;/p&gt;
    &lt;code&gt;= Matrix([
 B 1,0,0],
     [0,1,0],
     [0,0,1]
     [
 ])
print("Rank of B:", B.rank())&lt;/code&gt;
    &lt;code&gt;Rank of B: 3&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Connection to nullity&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The rank-nullity theorem:&lt;/p&gt;
    &lt;p&gt;\[ \text{rank}(A) + \text{nullity}(A) = \text{number of columns of } A \]&lt;/p&gt;
    &lt;p&gt;Check with SymPy:&lt;/p&gt;
    &lt;code&gt;print("Null space (basis):", A.nullspace())
print("Nullity:", len(A.nullspace()))
print("Rank + Nullity =", A.rank() + len(A.nullspace()))&lt;/code&gt;
    &lt;code&gt;Null space (basis): [Matrix([
[ 1],
[-2],
[ 1]])]
Nullity: 1
Rank + Nullity = 3&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Take&lt;/p&gt;
        &lt;p&gt;\[ \begin{bmatrix} 1 &amp;amp; 1 \\ 1 &amp;amp; 1 \end{bmatrix} \]&lt;/p&gt;
        &lt;p&gt;and compute its rank. Why is it 1?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;For a random 4×4 matrix, use&lt;/p&gt;&lt;code&gt;np.linalg.matrix_rank&lt;/code&gt;to check if it’s invertible.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Verify rank-nullity theorem for a 3×5 random integer matrix.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rank = dimension of the image (how many independent outputs a transformation has).&lt;/item&gt;
      &lt;item&gt;Operator viewpoint: rank shows how much of the input space survives after transformation.&lt;/item&gt;
      &lt;item&gt;Rank-nullity links the image and kernel - together they fully describe a linear operator.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;50. Block Matrices and Block Maps (Divide and Conquer Structure)&lt;/head&gt;
    &lt;p&gt;Sometimes matrices can be arranged in blocks (submatrices). Treating a big matrix as smaller pieces helps simplify calculations, especially in systems with structure (networks, coupled equations, or partitioned variables).&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Constructing block matrices&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can build a block matrix from smaller pieces:&lt;/p&gt;
    &lt;code&gt;= Matrix([[1,2],[3,4]])
 A11 = Matrix([[5,6],[7,8]])
 A12 = Matrix([[9,10]])
 A21 = Matrix([[11,12]])
 A22 
# Combine into a block matrix
= Matrix.vstack(
 A 
     Matrix.hstack(A11, A12),
     Matrix.hstack(A21, A22)
 )print("Block matrix A:\n", A)&lt;/code&gt;
    &lt;code&gt;Block matrix A:
 Matrix([[1, 2, 5, 6], [3, 4, 7, 8], [9, 10, 11, 12]])&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Block multiplication&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If a matrix is partitioned into blocks, multiplication follows block rules:&lt;/p&gt;
    &lt;p&gt;\[ \begin{bmatrix} A &amp;amp; B \\ C &amp;amp; D \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} Ax + By \\ Cx + Dy \end{bmatrix} \]&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;= Matrix([
 A 1,2,5,6],
     [3,4,7,8],
     [9,10,11,12]
     [
 ])
= Matrix([1,1,2,2])
 x print("A * x =", A*x)&lt;/code&gt;
    &lt;code&gt;A * x = Matrix([[25], [37], [65]])&lt;/code&gt;
    &lt;p&gt;Here the vector is split into blocks \([x,y]\).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Block diagonal matrices&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Block diagonal = independent subproblems:&lt;/p&gt;
    &lt;code&gt;= Matrix([[2,0],[0,2]])
 B1 = Matrix([[3,1],[0,3]])
 B2 
= Matrix([
 BlockDiag 2,0,0,0],
     [0,2,0,0],
     [0,0,3,1],
     [0,0,0,3]
     [
 ])
print("Block diagonal matrix:\n", BlockDiag)&lt;/code&gt;
    &lt;code&gt;Block diagonal matrix:
 Matrix([[2, 0, 0, 0], [0, 2, 0, 0], [0, 0, 3, 1], [0, 0, 0, 3]])&lt;/code&gt;
    &lt;p&gt;Applying this matrix acts separately on each block - like running two smaller transformations in parallel.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Inverse of block diagonal&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The inverse of a block diagonal is just the block diagonal of inverses:&lt;/p&gt;
    &lt;code&gt;= B1.inv()
 B1_inv = B2.inv()
 B2_inv = Matrix([
 BlockDiagInv 0,0],0,0,0],
     [B1_inv[0,B1_inv[1,1],0,0],
     [0,0,B2_inv[0,0],B2_inv[0,1]],
     [0,0,B2_inv[1,0],B2_inv[1,1]]
     [
 ])print("Inverse block diag:\n", BlockDiagInv)&lt;/code&gt;
    &lt;code&gt;Inverse block diag:
 Matrix([[1/2, 0, 0, 0], [0, 1/2, 0, 0], [0, 0, 1/3, -1/9], [0, 0, 0, 1/3]])&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Practical example - coupled equations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Suppose we have two independent systems:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;System 1: \(Ax = b\)&lt;/item&gt;
      &lt;item&gt;System 2: \(Cy = d\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can represent both together:&lt;/p&gt;
    &lt;p&gt;\[ \begin{bmatrix} A &amp;amp; 0 \\ 0 &amp;amp; C \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} b \\ d \end{bmatrix} \]&lt;/p&gt;
    &lt;p&gt;This shows how block matrices organize multiple systems in one big equation.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build a block diagonal matrix with three 2×2 blocks. Apply it to a vector.&lt;/item&gt;
      &lt;item&gt;Verify block multiplication rule by manually computing \(Ax + By\) and \(Cx + Dy\).&lt;/item&gt;
      &lt;item&gt;Write two small systems of equations and combine them into one block system.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Block matrices let us break down big systems into smaller parts.&lt;/item&gt;
      &lt;item&gt;Block diagonal matrices = independent subsystems.&lt;/item&gt;
      &lt;item&gt;Thinking in blocks simplifies algebra, programming, and numerical computation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Chapter 6. Determinants and volume&lt;/head&gt;
    &lt;head rend="h3"&gt;51. Areas, Volumes, and Signed Scale Factors (Geometric Entry Point)&lt;/head&gt;
    &lt;p&gt;The determinant of a matrix has a deep geometric meaning: it tells us how a linear transformation scales area (in 2D), volume (in 3D), or higher-dimensional content. It can also flip orientation (sign).&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Determinant in 2D (area scaling)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s take a matrix that stretches and shears:&lt;/p&gt;
    &lt;code&gt;= Matrix([
 A 2,1],
     [1,1]
     [
 ])
print("Determinant:", A.det())&lt;/code&gt;
    &lt;code&gt;Determinant: 1&lt;/code&gt;
    &lt;p&gt;The determinant = 1 → areas are preserved, even though the shape is distorted.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Unit square under transformation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Transform the square with corners \((0,0),(1,0),(1,1),(0,1)\):&lt;/p&gt;
    &lt;code&gt;= Matrix([
 square 0,0],
     [1,0],
     [1,1],
     [0,1]
     [
 ])
= (A * square.T).T
 transformed print("Original square:\n", square)
print("Transformed square:\n", transformed)&lt;/code&gt;
    &lt;code&gt;Original square:
 Matrix([[0, 0], [1, 0], [1, 1], [0, 1]])
Transformed square:
 Matrix([[0, 0], [2, 1], [3, 2], [1, 1]])&lt;/code&gt;
    &lt;p&gt;The area of the transformed shape equals \(|\det(A)|\).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Determinant in 3D (volume scaling)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([
 B 1,2,0],
     [0,1,0],
     [0,0,3]
     [
 ])
print("Determinant:", B.det())&lt;/code&gt;
    &lt;code&gt;Determinant: 3&lt;/code&gt;
    &lt;p&gt;\(\det(B)=3\) means that volumes are scaled by 3.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Negative determinant = orientation flip&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([
 C 0,1],
     [1,0]
     [
 ])
print("Determinant:", C.det())&lt;/code&gt;
    &lt;code&gt;Determinant: -1&lt;/code&gt;
    &lt;p&gt;The determinant = -1 → area preserved but orientation flipped (like a mirror reflection).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;NumPy version&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[2,1],[1,1]], dtype=float)
 A print("Det (NumPy):", np.linalg.det(A))&lt;/code&gt;
    &lt;code&gt;Det (NumPy): 1.0&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Take&lt;/p&gt;
        &lt;p&gt;\[ \begin{bmatrix} 3 &amp;amp; 0 \\ 0 &amp;amp; 2 \end{bmatrix} \]&lt;/p&gt;
        &lt;p&gt;and compute the determinant. Verify it scales areas by 6.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Build a 3×3 shear matrix and check how it affects volume.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Test a reflection matrix and confirm that the determinant is negative.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Determinant measures how a linear map scales area, volume, or hypervolume.&lt;/item&gt;
      &lt;item&gt;Positive determinant = preserves orientation; negative = flips it.&lt;/item&gt;
      &lt;item&gt;Magnitude of determinant = scaling factor of geometric content.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;52. Determinant via Linear Rules (Multilinearity, Sign, Normalization)&lt;/head&gt;
    &lt;p&gt;The determinant isn’t just a formula; it’s defined by three elegant rules that make it unique. These rules capture its geometric meaning as a volume-scaling factor.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Multilinearity: Linear in each row (or column).&lt;/item&gt;
      &lt;item&gt;Sign Change: Swapping two rows flips the sign.&lt;/item&gt;
      &lt;item&gt;Normalization: The determinant of the identity matrix is 1.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Multilinearity&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If one row is scaled, the determinant scales the same way.&lt;/p&gt;
    &lt;code&gt;= Matrix([[1,2],[3,4]])
 A print("det(A):", A.det())

= Matrix([[2,4],[3,4]])  # first row doubled
 B print("det(B):", B.det())&lt;/code&gt;
    &lt;code&gt;det(A): -2
det(B): -4&lt;/code&gt;
    &lt;p&gt;You’ll see &lt;code&gt;det(B) = 2 * det(A)&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Sign change by row swap&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([[1,2],[3,4]])
 C = Matrix([[3,4],[1,2]])
 C_swapped 
print("det(C):", C.det())
print("det(C_swapped):", C_swapped.det())&lt;/code&gt;
    &lt;code&gt;det(C): -2
det(C_swapped): 2&lt;/code&gt;
    &lt;p&gt;Swapping rows flips the sign of the determinant.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Normalization rule&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix.eye(3)
 I print("det(I):", I.det())&lt;/code&gt;
    &lt;code&gt;det(I): 1&lt;/code&gt;
    &lt;p&gt;The determinant of the identity is always 1 - this fixes the scaling baseline.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Combining rules (example in 3×3)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([[1,2,3],[4,5,6],[7,8,9]])
 M print("det(M):", M.det())&lt;/code&gt;
    &lt;code&gt;det(M): 0&lt;/code&gt;
    &lt;p&gt;Here, rows are linearly dependent, so the determinant is 0 - consistent with multilinearity (since one row can be written as a combo of others).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;NumPy check&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[1,2],[3,4]], dtype=float)
 A print("det(A) NumPy:", np.linalg.det(A))&lt;/code&gt;
    &lt;code&gt;det(A) NumPy: -2.0000000000000004&lt;/code&gt;
    &lt;p&gt;Both SymPy and NumPy confirm the same result.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Scale a row of a 3×3 matrix by 3. Confirm the determinant scales by 3.&lt;/item&gt;
      &lt;item&gt;Swap two rows twice in a row - does the determinant return to its original value?&lt;/item&gt;
      &lt;item&gt;Compute determinant of a triangular matrix. What pattern do you see?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Determinant is defined by multilinearity, sign change, and normalization.&lt;/item&gt;
      &lt;item&gt;These rules uniquely pin down the determinant’s behavior.&lt;/item&gt;
      &lt;item&gt;Every formula (cofactor expansion, row-reduction method, etc.) comes from these core principles.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;53. Determinant and Row Operations (How Each Move Changes det)&lt;/head&gt;
    &lt;p&gt;Row operations are at the heart of Gaussian elimination, and the determinant has simple, predictable reactions to them. Understanding these reactions gives both computational shortcuts and geometric intuition.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Three Key Rules&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Row swap: Swapping two rows flips the sign of the determinant.&lt;/item&gt;
      &lt;item&gt;Row scaling: Multiplying a row by a scalar \(c\) multiplies the determinant by \(c\).&lt;/item&gt;
      &lt;item&gt;Row replacement: Adding a multiple of one row to another leaves the determinant unchanged.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Row swap&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([[1,2],[3,4]])
 A print("det(A):", A.det())

= Matrix([[3,4],[1,2]])
 A_swapped print("det(after swap):", A_swapped.det())&lt;/code&gt;
    &lt;code&gt;det(A): -2
det(after swap): 2&lt;/code&gt;
    &lt;p&gt;The result flips sign.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Row scaling&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([[1,2],[3,4]])
 B = Matrix([[2,4],[3,4]])  # first row × 2
 B_scaled 
print("det(B):", B.det())
print("det(after scaling row 1 by 2):", B_scaled.det())&lt;/code&gt;
    &lt;code&gt;det(B): -2
det(after scaling row 1 by 2): -4&lt;/code&gt;
    &lt;p&gt;Determinant is multiplied by 2.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Row replacement (no change)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([[1,2],[3,4]])
 C = Matrix([[1,2],[3-2*1, 4-2*2]])  # row2 → row2 - 2*row1
 C_replaced 
print("det(C):", C.det())
print("det(after row replacement):", C_replaced.det())&lt;/code&gt;
    &lt;code&gt;det(C): -2
det(after row replacement): -2&lt;/code&gt;
    &lt;p&gt;Determinant stays the same.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Triangular form shortcut&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since elimination only uses row replacement (which doesn’t change the determinant) and row swaps/scales (which we can track), the determinant of a triangular matrix is just the product of its diagonal entries.&lt;/p&gt;
    &lt;code&gt;= Matrix([[2,1,3],[0,4,5],[0,0,6]])
 D print("det(D):", D.det())
print("Product of diagonals:", 2*4*6)&lt;/code&gt;
    &lt;code&gt;det(D): 48
Product of diagonals: 48&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;NumPy confirmation&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[1,2,3],[0,4,5],[1,0,6]], dtype=float)
 A print("det(A):", np.linalg.det(A))&lt;/code&gt;
    &lt;code&gt;det(A): 22.000000000000004&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Take&lt;/p&gt;
        &lt;p&gt;\[ \begin{bmatrix} 2 &amp;amp; 3 \\ 4 &amp;amp; 6 \end{bmatrix} \]&lt;/p&gt;
        &lt;p&gt;and scale the second row by \(\tfrac{1}{2}\). Compare determinants before and after.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Do Gaussian elimination on a 3×3 matrix, and track how each row operation changes the determinant.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Compute determinant by reducing to triangular form and compare with SymPy’s&lt;/p&gt;&lt;code&gt;.det()&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Determinant reacts predictably to row operations.&lt;/item&gt;
      &lt;item&gt;Row replacement is “safe” (no change), scaling multiplies by the factor, and swapping flips the sign.&lt;/item&gt;
      &lt;item&gt;This makes elimination not just a solving tool, but also a method to compute determinants efficiently.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;54. Triangular Matrices and Product of Diagonals (Fast Wins)&lt;/head&gt;
    &lt;p&gt;For triangular matrices (upper or lower), the determinant is simply the product of the diagonal entries. This rule is one of the biggest shortcuts in linear algebra - no expansion or elimination needed.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why It Works&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Triangular matrices already look like the end result of Gaussian elimination.&lt;/item&gt;
      &lt;item&gt;Since row replacement operations don’t change the determinant, what’s left is just the product of the diagonal.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Upper triangular example&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([
 A 2,1,3],
     [0,4,5],
     [0,0,6]
     [
 ])
print("det(A):", A.det())
print("Product of diagonals:", 2*4*6)&lt;/code&gt;
    &lt;code&gt;det(A): 48
Product of diagonals: 48&lt;/code&gt;
    &lt;p&gt;Both values match exactly.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Lower triangular example&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([
 B 7,0,0],
     [2,5,0],
     [3,4,9]
     [
 ])
print("det(B):", B.det())
print("Product of diagonals:", 7*5*9)&lt;/code&gt;
    &lt;code&gt;det(B): 315
Product of diagonals: 315&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Diagonal matrix (special case)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For diagonal matrices, determinant = product of diagonal entries directly.&lt;/p&gt;
    &lt;code&gt;= Matrix.diag(3,5,7)
 C print("det(C):", C.det())
print("Product of diagonals:", 3*5*7)&lt;/code&gt;
    &lt;code&gt;det(C): 105
Product of diagonals: 105&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;NumPy version&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[2,1,3],[0,4,5],[0,0,6]], dtype=float)
 A print("det(A):", np.linalg.det(A))
print("Product of diagonals:", np.prod(np.diag(A)))&lt;/code&gt;
    &lt;code&gt;det(A): 47.999999999999986
Product of diagonals: 48.0&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Quick elimination to triangular form&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Even for non-triangular matrices, elimination reduces them to triangular form, where this rule applies.&lt;/p&gt;
    &lt;code&gt;= Matrix([[1,2,3],[4,5,6],[7,8,10]])
 D print("det(D) via SymPy:", D.det())
print("det(D) via LU decomposition:", D.LUdecomposition()[0].det() * D.LUdecomposition()[1].det())&lt;/code&gt;
    &lt;code&gt;det(D) via SymPy: -3
det(D) via LU decomposition: -3&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute the determinant of a 4×4 diagonal matrix quickly.&lt;/item&gt;
      &lt;item&gt;Verify that triangular matrices with a zero on the diagonal always have determinant 0.&lt;/item&gt;
      &lt;item&gt;Use SymPy to check that elimination to triangular form preserves determinant (except for swaps/scales).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;For triangular (and diagonal) matrices:&lt;/p&gt;
        &lt;p&gt;\[ \det(A) = \prod_{i} a_{ii} \]&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This shortcut makes determinant computation trivial.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Gaussian elimination leverages this fact: once reduced to triangular form, the determinant is just the product of pivots (with sign adjustments for swaps).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;55. det(AB) = det(A)det(B) (Multiplicative Magic)&lt;/head&gt;
    &lt;p&gt;One of the most elegant properties of determinants is multiplicativity:&lt;/p&gt;
    &lt;p&gt;\[ \det(AB) = \det(A)\,\det(B) \]&lt;/p&gt;
    &lt;p&gt;This rule is powerful because it connects algebra (matrix multiplication) with geometry (volume scaling).&lt;/p&gt;
    &lt;head rend="h4"&gt;Geometric Intuition&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If \(A\) scales volumes by factor \(\det(A)\), and \(B\) scales them by \(\det(B)\), then applying \(B\) followed by \(A\) scales volumes by \(\det(A)\det(B)\).&lt;/item&gt;
      &lt;item&gt;This property works in all dimensions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;2×2 example&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([[2,1],[0,3]])
 A = Matrix([[1,4],[2,5]])
 B 
= A.det()
 detA = B.det()
 detB = (A*B).det()
 detAB 
print("det(A):", detA)
print("det(B):", detB)
print("det(AB):", detAB)
print("det(A)*det(B):", detA*detB)&lt;/code&gt;
    &lt;code&gt;det(A): 6
det(B): -3
det(AB): -18
det(A)*det(B): -18&lt;/code&gt;
    &lt;p&gt;The two results match.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;3×3 random matrix check&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;1)
 np.random.seed(= Matrix(np.random.randint(-3,4,(3,3)))
 A = Matrix(np.random.randint(-3,4,(3,3)))
 B 
print("det(A):", A.det())
print("det(B):", B.det())
print("det(AB):", (A*B).det())
print("det(A)*det(B):", A.det()*B.det())&lt;/code&gt;
    &lt;code&gt;det(A): 25
det(B): -15
det(AB): -375
det(A)*det(B): -375&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Special cases&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If \(\det(A)=0\), then \(\det(AB)=0\).&lt;/item&gt;
      &lt;item&gt;If \(\det(A)=\pm1\), it acts like a “volume-preserving” transformation (rotation/reflection).&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([[1,0],[0,0]])  # singular
 A = Matrix([[2,3],[4,5]])
 B 
print("det(A):", A.det())
print("det(AB):", (A*B).det())&lt;/code&gt;
    &lt;code&gt;det(A): 0
det(AB): 0&lt;/code&gt;
    &lt;p&gt;Both are 0.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;NumPy version&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[2,1],[0,3]], dtype=float)
 A = np.array([[1,4],[2,5]], dtype=float)
 B 
= np.linalg.det(A @ B)
 lhs = np.linalg.det(A) * np.linalg.det(B)
 rhs 
print("det(AB) =", lhs)
print("det(A)*det(B) =", rhs)&lt;/code&gt;
    &lt;code&gt;det(AB) = -17.999999999999996
det(A)*det(B) = -17.999999999999996&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Construct two triangular matrices and verify multiplicativity (diagonal products multiply too).&lt;/item&gt;
      &lt;item&gt;Test the property with an orthogonal matrix \(Q\) (\(\det(Q)=\pm 1\)). What happens?&lt;/item&gt;
      &lt;item&gt;Try with one matrix singular - confirm the product is always singular.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Determinant is multiplicative, not additive.&lt;/item&gt;
      &lt;item&gt;\(\det(AB) = \det(A)\det(B)\) is a cornerstone identity in linear algebra.&lt;/item&gt;
      &lt;item&gt;This property connects geometry (volume scaling) with algebra (matrix multiplication).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;56. Invertibility and Zero Determinant (Flat vs. Full Volume)&lt;/head&gt;
    &lt;p&gt;The determinant gives a quick test for invertibility:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If \(\det(A) \neq 0\), the matrix is invertible.&lt;/item&gt;
      &lt;item&gt;If \(\det(A) = 0\), the matrix is singular (non-invertible).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Geometrically:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Nonzero determinant → transformation keeps full dimension (no collapse).&lt;/item&gt;
      &lt;item&gt;Zero determinant → transformation flattens space into a lower dimension (volume = 0).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix
from sympy.matrices.common import NonInvertibleMatrixError&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Invertible example&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([[2,1],[5,3]])
 A print("det(A):", A.det())
print("Inverse exists?", A.det() != 0)
print("A inverse:\n", A.inv())&lt;/code&gt;
    &lt;code&gt;det(A): 1
Inverse exists? True
A inverse:
 Matrix([[3, -1], [-5, 2]])&lt;/code&gt;
    &lt;p&gt;The determinant is nonzero → invertible.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Singular example (zero determinant)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([[1,2],[2,4]])
 B print("det(B):", B.det())
print("Inverse exists?", B.det() != 0)&lt;/code&gt;
    &lt;code&gt;det(B): 0
Inverse exists? False&lt;/code&gt;
    &lt;p&gt;Since the second row is a multiple of the first, determinant = 0 → no inverse.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Solving systems with determinant check&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If \(\det(A)=0\), the system \(Ax=b\) may have no solutions or infinitely many.&lt;/p&gt;
    &lt;code&gt;# 3. Solving systems with determinant check
= Matrix([1,2])
 b try:
print("Solve Ax=b with singular B:", B.solve(b))
     except NonInvertibleMatrixError as e:
print("Error when solving Ax=b:", e)     &lt;/code&gt;
    &lt;code&gt;Error when solving Ax=b: Matrix det == 0; not invertible.&lt;/code&gt;
    &lt;p&gt;SymPy indicates inconsistency or multiple solutions.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Higher-dimensional example&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([
 C 1,0,0],
     [0,2,0],
     [0,0,3]
     [
 ])print("det(C):", C.det())
print("Invertible?", C.det() != 0)&lt;/code&gt;
    &lt;code&gt;det(C): 6
Invertible? True&lt;/code&gt;
    &lt;p&gt;Diagonal entries all nonzero → invertible.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;NumPy version&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[2,1],[5,3]], dtype=float)
 A print("det(A):", np.linalg.det(A))
print("Inverse:\n", np.linalg.inv(A))

= np.array([[1,2],[2,4]], dtype=float)
 B print("det(B):", np.linalg.det(B))
# np.linalg.inv(B) would fail because det=0&lt;/code&gt;
    &lt;code&gt;det(A): 1.0000000000000002
Inverse:
 [[ 3. -1.]
 [-5.  2.]]
det(B): 0.0&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build a 3×3 matrix with determinant 0 by making one row a multiple of another. Confirm singularity.&lt;/item&gt;
      &lt;item&gt;Generate a random 4×4 matrix and check whether it’s invertible using &lt;code&gt;.det()&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Test if two different 2×2 matrices are invertible, then multiply them together - is the product invertible too?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(\det(A) \neq 0 \implies\) invertible (full volume).&lt;/item&gt;
      &lt;item&gt;\(\det(A) = 0 \implies\) singular (space collapsed).&lt;/item&gt;
      &lt;item&gt;Determinant gives both algebraic and geometric insight into when a matrix is reversible.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;57. Cofactor Expansion (Laplace’s Method)&lt;/head&gt;
    &lt;p&gt;The cofactor expansion is a systematic way to compute determinants using minors. It’s not efficient for large matrices, but it reveals the recursive structure of determinants.&lt;/p&gt;
    &lt;head rend="h4"&gt;Definition&lt;/head&gt;
    &lt;p&gt;For an \(n \times n\) matrix \(A\),&lt;/p&gt;
    &lt;p&gt;\[ \det(A) = \sum_{j=1}^{n} (-1)^{i+j} a_{ij} \det(M_{ij}) \]&lt;/p&gt;
    &lt;p&gt;where:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(i\) = chosen row (or column),&lt;/item&gt;
      &lt;item&gt;\(M_{ij}\) = minor matrix after removing row \(i\), column \(j\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix, symbols&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;2×2 case (base rule)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# declare symbols
= symbols('a b c d')
 a, b, c, d 
# build the matrix
= Matrix([[a, b],[c, d]])
 A 
# compute determinant
= A.det()
 detA print("Determinant 2x2:", detA)&lt;/code&gt;
    &lt;code&gt;Determinant 2x2: a*d - b*c&lt;/code&gt;
    &lt;p&gt;Formula: \(\det(A) = ad - bc\).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;3×3 example using cofactor expansion&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([
 A 1,2,3],
     [4,5,6],
     [7,8,9]
     [
 ])
= A.det()
 detA print("Determinant via SymPy:", detA)&lt;/code&gt;
    &lt;code&gt;Determinant via SymPy: 0&lt;/code&gt;
    &lt;p&gt;Let’s compute manually along the first row:&lt;/p&gt;
    &lt;code&gt;= (
 cofactor_expansion 1 * Matrix([[5,6],[8,9]]).det()
     - 2 * Matrix([[4,6],[7,9]]).det()
     + 3 * Matrix([[4,5],[7,8]]).det()
     
 )print("Cofactor expansion result:", cofactor_expansion)&lt;/code&gt;
    &lt;code&gt;Cofactor expansion result: 0&lt;/code&gt;
    &lt;p&gt;Both match (here = 0 because rows are dependent).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Expansion along different rows/columns&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The result is the same no matter which row/column you expand along.&lt;/p&gt;
    &lt;code&gt;= (
 cofactor_col1 1 * Matrix([[2,3],[8,9]]).det()
     - 4 * Matrix([[2,3],[5,6]]).det()
     + 7 * Matrix([[2,3],[5,6]]).det()
     
 )print("Expansion along col1:", cofactor_col1)&lt;/code&gt;
    &lt;code&gt;Expansion along col1: -15&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Larger example (4×4)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([
 B 2,0,1,3],
     [1,2,0,4],
     [0,1,1,0],
     [3,0,2,1]
     [
 ])
print("Determinant 4x4:", B.det())&lt;/code&gt;
    &lt;code&gt;Determinant 4x4: -15&lt;/code&gt;
    &lt;p&gt;SymPy handles it directly, but conceptually it’s still the same recursive expansion.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;NumPy vs SymPy&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[2,0,1,3],[1,2,0,4],[0,1,1,0],[3,0,2,1]], dtype=float)
 B_np print("NumPy determinant:", np.linalg.det(B_np))&lt;/code&gt;
    &lt;code&gt;NumPy determinant: -15.0&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute a 3×3 determinant manually using cofactor expansion and confirm with &lt;code&gt;.det()&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Expand along a different row and check that the result is unchanged.&lt;/item&gt;
      &lt;item&gt;Build a 4×4 diagonal matrix and expand it - what simplification do you notice?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cofactor expansion defines determinant recursively.&lt;/item&gt;
      &lt;item&gt;Works on any row or column, with consistent results.&lt;/item&gt;
      &lt;item&gt;Important for proofs and theory, though not practical for computation on large matrices.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;58. Permutations and Sign (The Combinatorial Core)&lt;/head&gt;
    &lt;p&gt;The determinant can also be defined using permutations of indices. This looks abstract, but it’s the most fundamental definition:&lt;/p&gt;
    &lt;p&gt;\[ \det(A) = \sum_{\sigma \in S_n} \text{sgn}(\sigma) \prod_{i=1}^n a_{i,\sigma(i)} \]&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(S_n\) = set of all permutations of \(\{1,\dots,n\}\)&lt;/item&gt;
      &lt;item&gt;\(\text{sgn}(\sigma)\) = +1 if the permutation is even, -1 if odd&lt;/item&gt;
      &lt;item&gt;Each term = one product of entries, one from each row and column&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This formula explains why determinants mix signs, why row swaps flip the determinant, and why dependence kills it.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import itertools
import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Determinant by permutation expansion (3×3)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;def determinant_permutation(A):
= A.shape[0]
     n = 0
     total for perm in itertools.permutations(range(n)):
     = (-1)**(sum(1 for i in range(n) for j in range(i) if perm[j] &amp;gt; perm[i]))
         sign = 1
         product for i in range(n):
         *= A[i, perm[i]]
             product += sign * product
         total return total
     
= np.array([[1,2,3],
 A 4,5,6],
               [7,8,9]])
               [
print("Permutation formula det:", determinant_permutation(A))
print("NumPy det:", np.linalg.det(A))&lt;/code&gt;
    &lt;code&gt;Permutation formula det: 0
NumPy det: -9.51619735392994e-16&lt;/code&gt;
    &lt;p&gt;Both results ≈ 0, since rows are dependent.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Count permutations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For \(n=3\), there are \(3! = 6\) terms:&lt;/p&gt;
    &lt;p&gt;\[ \det(A) = a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} - a_{13}a_{22}a_{31} - a_{11}a_{23}a_{32} - a_{12}a_{21}a_{33} \]&lt;/p&gt;
    &lt;p&gt;You can see the alternating signs explicitly.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verification with SymPy&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([[2,1,0],
 M 1,3,4],
             [0,2,5]])
             [print("SymPy det:", M.det())&lt;/code&gt;
    &lt;code&gt;SymPy det: 9&lt;/code&gt;
    &lt;p&gt;Matches the permutation expansion.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Growth of terms&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2×2 → 2 terms&lt;/item&gt;
      &lt;item&gt;3×3 → 6 terms&lt;/item&gt;
      &lt;item&gt;4×4 → 24 terms&lt;/item&gt;
      &lt;item&gt;\(n\) → \(n!\) terms (factorial growth!)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is why cofactor or LU is preferred computationally.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Write out the 2×2 permutation formula explicitly and check it equals \(ad - bc\).&lt;/item&gt;
      &lt;item&gt;Expand a 3×3 determinant by hand using the six terms.&lt;/item&gt;
      &lt;item&gt;Modify the code to count how many multiplications are required for a 5×5 matrix using the permutation definition.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Determinant = signed sum over all permutations.&lt;/item&gt;
      &lt;item&gt;Signs come from permutation parity (even/odd swaps).&lt;/item&gt;
      &lt;item&gt;This definition is the combinatorial foundation that unifies all determinant properties.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;59. Cramer’s Rule (Solving with Determinants, and When Not to Use It)&lt;/head&gt;
    &lt;p&gt;Cramer’s Rule gives an explicit formula for solving a system of linear equations \(Ax = b\) using determinants. It is elegant but inefficient for large systems.&lt;/p&gt;
    &lt;p&gt;For \(A \in \mathbb{R}^{n \times n}\) with \(\det(A) \neq 0\):&lt;/p&gt;
    &lt;p&gt;\[ x_i = \frac{\det(A_i)}{\det(A)} \]&lt;/p&gt;
    &lt;p&gt;where \(A_i\) is \(A\) with its \(i\)-th column replaced by \(b\).&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Simple 2×2 example&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Solve:&lt;/p&gt;
    &lt;p&gt;\[ \begin{cases} 2x + y = 5 \\ x - y = 1 \end{cases} \]&lt;/p&gt;
    &lt;code&gt;= Matrix([[2,1],[1,-1]])
 A = Matrix([5,1])
 b 
= A.det()
 detA print("det(A):", detA)

# Replace columns
= A.copy()
 A1 0] = b
 A1[:,= A.copy()
 A2 1] = b
 A2[:,
= A1.det() / detA
 x1 = A2.det() / detA
 x2 print("Solution via Cramer's Rule:", [x1, x2])

# Check with built-in solver
print("SymPy solve:", A.LUsolve(b))&lt;/code&gt;
    &lt;code&gt;det(A): -3
Solution via Cramer's Rule: [2, 1]
SymPy solve: Matrix([[2], [1]])&lt;/code&gt;
    &lt;p&gt;Both give the same solution.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;3×3 example&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([
 A 1,2,3],
     [0,1,4],
     [5,6,0]
     [
 ])= Matrix([7,8,9])
 b 
= A.det()
 detA print("det(A):", detA)

= []
 solutions for i in range(A.shape[1]):
= A.copy()
     Ai = b
     Ai[:,i] /detA)
     solutions.append(Ai.det()
print("Solution via Cramer's Rule:", solutions)
print("SymPy solve:", A.LUsolve(b))&lt;/code&gt;
    &lt;code&gt;det(A): 1
Solution via Cramer's Rule: [21, -16, 6]
SymPy solve: Matrix([[21], [-16], [6]])&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;NumPy version (inefficient but illustrative)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[2,1],[1,-1]], dtype=float)
 A = np.array([5,1], dtype=float)
 b 
= np.linalg.det(A)
 detA 
= []
 solutions for i in range(A.shape[1]):
= A.copy()
     Ai = b
     Ai[:,i] /detA)
     solutions.append(np.linalg.det(Ai)
print("Solution:", solutions)&lt;/code&gt;
    &lt;code&gt;Solution: [np.float64(2.0000000000000004), np.float64(1.0)]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Why not use it in practice?&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Requires computing \(n+1\) determinants.&lt;/item&gt;
      &lt;item&gt;Determinant computation via cofactor expansion is factorial-time.&lt;/item&gt;
      &lt;item&gt;Gaussian elimination or LU is far more efficient.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Solve a 3×3 system using Cramer’s Rule and confirm with &lt;code&gt;A.solve(b)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Try Cramer’s Rule when \(\det(A)=0\). What happens?&lt;/item&gt;
      &lt;item&gt;Compare runtime of Cramer’s Rule vs LU for a random 5×5 matrix.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cramer’s Rule gives explicit formulas for solutions using determinants.&lt;/item&gt;
      &lt;item&gt;Beautiful for theory, useful for small cases, but not computationally practical.&lt;/item&gt;
      &lt;item&gt;It highlights the deep connection between determinants and solving linear systems.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;60. Computing Determinants in Practice (Use LU, Mind Stability)&lt;/head&gt;
    &lt;p&gt;While definitions like cofactor expansion and permutations are beautiful, they are too slow for large matrices. In practice, determinants are computed using row reduction or LU decomposition, with careful attention to numerical stability.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Cofactor expansion is too slow&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([
 A 1,2,3],
     [4,5,6],
     [7,8,10]
     [
 ])print("det via cofactor expansion:", A.det())&lt;/code&gt;
    &lt;code&gt;det via cofactor expansion: -3&lt;/code&gt;
    &lt;p&gt;This works for 3×3, but complexity grows factorially.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Determinant via triangular form (LU decomposition)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;LU decomposition factorizes \(A = LU\), where \(L\) is lower triangular and \(U\) is upper triangular. Determinant = product of diagonals of \(U\), up to sign corrections for row swaps.&lt;/p&gt;
    &lt;code&gt;= A.LUdecomposition()
 L, U, perm = A.det()
 detA print("L:\n", L)
print("U:\n", U)
print("Permutation matrix:\n", perm)
print("det via LU product:", detA)&lt;/code&gt;
    &lt;code&gt;L:
 Matrix([[1, 0, 0], [4, 1, 0], [7, 2, 1]])
U:
 Matrix([[1, 2, 3], [0, -3, -6], [0, 0, 1]])
Permutation matrix:
 []
det via LU product: -3&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;NumPy efficient method&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[1,2,3],[4,5,6],[7,8,10]], dtype=float)
 A_np print("NumPy det:", np.linalg.det(A_np))&lt;/code&gt;
    &lt;code&gt;NumPy det: -3.000000000000001&lt;/code&gt;
    &lt;p&gt;NumPy uses optimized routines (LAPACK under the hood).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Large random matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;0)
 np.random.seed(= np.random.rand(5,5)
 B print("NumPy det (5x5):", np.linalg.det(B))&lt;/code&gt;
    &lt;code&gt;NumPy det (5x5): 0.009658225505885114&lt;/code&gt;
    &lt;p&gt;Computes quickly even for larger matrices.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Stability issues&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Determinants of large or ill-conditioned matrices can suffer from floating-point errors. For example, if rows are nearly dependent:&lt;/p&gt;
    &lt;code&gt;= np.array([[1,2,3],[2,4.0000001,6],[3,6,9]], dtype=float)
 C print("det(C):", np.linalg.det(C))&lt;/code&gt;
    &lt;code&gt;det(C): -4.996003624823549e-23&lt;/code&gt;
    &lt;p&gt;The result may not be exactly 0 due to floating-point approximations.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute the determinant of a random 10×10 matrix with &lt;code&gt;np.linalg.det&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Compare results between SymPy (exact rational arithmetic) and NumPy (floating-point).&lt;/item&gt;
      &lt;item&gt;Test determinant of a nearly singular matrix - notice numerical instability.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Determinants in practice are computed with LU decomposition or equivalent.&lt;/item&gt;
      &lt;item&gt;Always be mindful of numerical stability - small errors matter when determinant ≈ 0.&lt;/item&gt;
      &lt;item&gt;For exact answers (small cases), use symbolic tools like SymPy; for speed, use NumPy.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Chapter 7. Eigenvalues, Eigenvectors, and Dynamics&lt;/head&gt;
    &lt;head rend="h3"&gt;61. Eigenvalues and Eigenvectors (Directions That Stay Put)&lt;/head&gt;
    &lt;p&gt;An eigenvector of a matrix \(A\) is a special vector that doesn’t change direction when multiplied by \(A\). Instead, it only gets stretched or shrunk by a scalar called the eigenvalue.&lt;/p&gt;
    &lt;p&gt;Formally:&lt;/p&gt;
    &lt;p&gt;\[ A v = \lambda v \]&lt;/p&gt;
    &lt;p&gt;where \(v\) is an eigenvector and \(\lambda\) is the eigenvalue.&lt;/p&gt;
    &lt;p&gt;Geometrically: eigenvectors are “preferred directions” of a linear transformation.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A simple 2×2 example&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([
 A 2,1],
     [1,2]
     [
 ])
= A.eigenvects()
 eigs print("Eigenvalues and eigenvectors:", eigs)&lt;/code&gt;
    &lt;code&gt;Eigenvalues and eigenvectors: [(1, 1, [Matrix([
[-1],
[ 1]])]), (3, 1, [Matrix([
[1],
[1]])])]&lt;/code&gt;
    &lt;p&gt;This outputs eigenvalues and their associated eigenvectors.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verify the eigen equation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Pick one eigenpair \((\lambda, v)\):&lt;/p&gt;
    &lt;code&gt;= eigs[0][0]
 lam = eigs[0][2][0]
 v print("Check Av = λv:", A*v, lam*v)&lt;/code&gt;
    &lt;code&gt;Check Av = λv: Matrix([[-1], [1]]) Matrix([[-1], [1]])&lt;/code&gt;
    &lt;p&gt;Both sides match → confirming the eigenpair.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;NumPy version&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[2,1],[1,2]], dtype=float)
 A_np = np.linalg.eig(A_np)
 eigvals, eigvecs 
print("Eigenvalues:", eigvals)
print("Eigenvectors:\n", eigvecs)&lt;/code&gt;
    &lt;code&gt;Eigenvalues: [3. 1.]
Eigenvectors:
 [[ 0.70710678 -0.70710678]
 [ 0.70710678  0.70710678]]&lt;/code&gt;
    &lt;p&gt;Columns of &lt;code&gt;eigvecs&lt;/code&gt; are eigenvectors.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Geometric interpretation (plot)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;import matplotlib.pyplot as plt

= np.array(eigvecs[:,0])
 v1 = np.array(eigvecs[:,1])
 v2 
0,0,v1[0],v1[1],head_width=0.1,color="blue",length_includes_head=True)
 plt.arrow(0,0,v2[0],v2[1],head_width=0.1,color="red",length_includes_head=True)
 plt.arrow(
0,color="black",linewidth=0.5)
 plt.axhline(0,color="black",linewidth=0.5)
 plt.axvline("equal")
 plt.axis(
 plt.grid()"Eigenvectors: directions that stay put")
 plt.title( plt.show()&lt;/code&gt;
    &lt;p&gt;Both eigenvectors define directions where the transformation acts by scaling only.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Random 3×3 matrix example&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;1)
 np.random.seed(= Matrix(np.random.randint(-2,3,(3,3)))
 B print("Matrix B:\n", B)
print("Eigenvalues/vectors:", B.eigenvects())&lt;/code&gt;
    &lt;code&gt;Matrix B:
 Matrix([[1, 2, -2], [-1, 1, -2], [-2, -1, 2]])
Eigenvalues/vectors: [(4/3 + (-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3) + 13/(9*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)), 1, [Matrix([
[ -16/27 - 91/(81*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)) + (4/3 + (-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3) + 13/(9*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)))**2/9 - 7*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)/9],
[50/27 + 5*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)/9 - 2*(4/3 + (-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3) + 13/(9*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)))**2/9 + 65/(81*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3))],
[                                                                                                                                                                                                                                                              1]])]), (4/3 + 13/(9*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)) + (-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3), 1, [Matrix([
[ -16/27 - 7*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)/9 + (4/3 + 13/(9*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)) + (-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3))**2/9 - 91/(81*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3))],
[50/27 + 65/(81*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)) - 2*(4/3 + 13/(9*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)) + (-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3))**2/9 + 5*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)/9],
[                                                                                                                                                                                                                                                              1]])]), (13/(9*(2*sqrt(43)/3 + 127/27)**(1/3)) + 4/3 + (2*sqrt(43)/3 + 127/27)**(1/3), 1, [Matrix([
[  -7*(2*sqrt(43)/3 + 127/27)**(1/3)/9 - 16/27 - 91/(81*(2*sqrt(43)/3 + 127/27)**(1/3)) + (13/(9*(2*sqrt(43)/3 + 127/27)**(1/3)) + 4/3 + (2*sqrt(43)/3 + 127/27)**(1/3))**2/9],
[-2*(13/(9*(2*sqrt(43)/3 + 127/27)**(1/3)) + 4/3 + (2*sqrt(43)/3 + 127/27)**(1/3))**2/9 + 65/(81*(2*sqrt(43)/3 + 127/27)**(1/3)) + 5*(2*sqrt(43)/3 + 127/27)**(1/3)/9 + 50/27],
[                                                                                                                                                                           1]])])]&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Compute eigenvalues and eigenvectors of&lt;/p&gt;
        &lt;p&gt;\[ \begin{bmatrix} 3 &amp;amp; 0 \\ 0 &amp;amp; 2 \end{bmatrix} \]&lt;/p&gt;
        &lt;p&gt;and verify that they match the diagonal entries.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Use NumPy to find eigenvectors of a rotation matrix by 90°. What do you notice?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;For a singular matrix, check if 0 is an eigenvalue.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Eigenvalues = scale factors; eigenvectors = directions that stay put.&lt;/item&gt;
      &lt;item&gt;The eigen equation \(Av=\lambda v\) captures the essence of a matrix’s action.&lt;/item&gt;
      &lt;item&gt;They form the foundation for deeper topics like diagonalization, stability, and dynamics.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;62. Characteristic Polynomial (Where Eigenvalues Come From)&lt;/head&gt;
    &lt;p&gt;Eigenvalues don’t appear out of thin air - they come from the characteristic polynomial of a matrix. For a square matrix \(A\),&lt;/p&gt;
    &lt;p&gt;\[ p(\lambda) = \det(A - \lambda I) \]&lt;/p&gt;
    &lt;p&gt;The roots of this polynomial are the eigenvalues of \(A\).&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix, symbols&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;2×2 example&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ A = \begin{bmatrix} 2 &amp;amp; 1 \\ 1 &amp;amp; 2 \end{bmatrix} \]&lt;/p&gt;
    &lt;code&gt;= symbols('λ')
 λ = Matrix([[2,1],[1,2]])
 A = A.charpoly(λ)
 char_poly print("Characteristic polynomial:", char_poly.as_expr())
print("Eigenvalues (roots):", char_poly.all_roots())&lt;/code&gt;
    &lt;code&gt;Characteristic polynomial: λ**2 - 4*λ + 3
Eigenvalues (roots): [1, 3]&lt;/code&gt;
    &lt;p&gt;Polynomial: \(\lambda^2 - 4\lambda + 3\). Roots: \(\lambda = 3, 1\).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verify with eigen computation&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;print("Eigenvalues directly:", A.eigenvals())&lt;/code&gt;
    &lt;code&gt;Eigenvalues directly: {3: 1, 1: 1}&lt;/code&gt;
    &lt;p&gt;Matches the roots of the polynomial.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;3×3 example&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([
 B 1,2,3],
     [0,1,4],
     [5,6,0]
     [
 ])
= B.charpoly(λ)
 char_poly_B print("Characteristic polynomial of B:", char_poly_B.as_expr())
print("Eigenvalues of B:", char_poly_B.all_roots())&lt;/code&gt;
    &lt;code&gt;Characteristic polynomial of B: λ**3 - 2*λ**2 - 38*λ - 1
Eigenvalues of B: [CRootOf(x**3 - 2*x**2 - 38*x - 1, 0), CRootOf(x**3 - 2*x**2 - 38*x - 1, 1), CRootOf(x**3 - 2*x**2 - 38*x - 1, 2)]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;NumPy version&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;NumPy doesn’t give the polynomial directly, but eigenvalues can be checked:&lt;/p&gt;
    &lt;code&gt;= np.array([[1,2,3],[0,1,4],[5,6,0]], dtype=float)
 B_np = np.linalg.eigvals(B_np)
 eigvals print("NumPy eigenvalues:", eigvals)&lt;/code&gt;
    &lt;code&gt;NumPy eigenvalues: [-5.2296696  -0.02635282  7.25602242]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Relation to trace and determinant&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For a 2×2 matrix&lt;/p&gt;
    &lt;p&gt;\[ A = \begin{bmatrix} a &amp;amp; b \\ c &amp;amp; d \end{bmatrix}, \]&lt;/p&gt;
    &lt;p&gt;the characteristic polynomial is&lt;/p&gt;
    &lt;p&gt;\[ \lambda^2 - (a+d)\lambda + (ad - bc). \]&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Coefficient of \(\lambda\): \(-\text{trace}(A)\).&lt;/item&gt;
      &lt;item&gt;Constant term: \(\det(A)\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;print("Trace:", A.trace())
print("Determinant:", A.det())&lt;/code&gt;
    &lt;code&gt;Trace: 4
Determinant: 3&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Compute the characteristic polynomial of&lt;/p&gt;
        &lt;p&gt;\[ \begin{bmatrix} 4 &amp;amp; 0 \\ 0 &amp;amp; 5 \end{bmatrix} \]&lt;/p&gt;
        &lt;p&gt;and confirm eigenvalues are 4 and 5.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Check the relationship between polynomial coefficients, trace, and determinant for a 3×3 case.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Verify with NumPy that the roots of the polynomial equal the eigenvalues.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The characteristic polynomial encodes eigenvalues as its roots.&lt;/item&gt;
      &lt;item&gt;Coefficients are tied to invariants: trace and determinant.&lt;/item&gt;
      &lt;item&gt;This polynomial viewpoint is the bridge from algebraic formulas to geometric eigen-behavior.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;63. Algebraic vs. Geometric Multiplicity (How Many and How Independent)&lt;/head&gt;
    &lt;p&gt;Eigenvalues can repeat, and when they do, two notions of multiplicity arise:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Algebraic multiplicity: how many times the eigenvalue appears as a root of the characteristic polynomial.&lt;/item&gt;
      &lt;item&gt;Geometric multiplicity: the dimension of the eigenspace (number of independent eigenvectors).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Always:&lt;/p&gt;
    &lt;p&gt;\[ 1 \leq \text{geometric multiplicity} \leq \text{algebraic multiplicity} \]&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Matrix with repeated eigenvalue&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([
 A 2,1],
     [0,2]
     [
 ])
print("Eigenvalues and algebraic multiplicity:", A.eigenvals())
print("Eigenvectors:", A.eigenvects())&lt;/code&gt;
    &lt;code&gt;Eigenvalues and algebraic multiplicity: {2: 2}
Eigenvectors: [(2, 2, [Matrix([
[1],
[0]])])]&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Eigenvalue 2 has algebraic multiplicity = 2.&lt;/item&gt;
      &lt;item&gt;But only 1 independent eigenvector → geometric multiplicity = 1.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Diagonal matrix with repetition&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([
 B 3,0,0],
     [0,3,0],
     [0,0,3]
     [
 ])
print("Eigenvalues:", B.eigenvals())
print("Eigenvectors:", B.eigenvects())&lt;/code&gt;
    &lt;code&gt;Eigenvalues: {3: 3}
Eigenvectors: [(3, 3, [Matrix([
[1],
[0],
[0]]), Matrix([
[0],
[1],
[0]]), Matrix([
[0],
[0],
[1]])])]&lt;/code&gt;
    &lt;p&gt;Here, eigenvalue 3 has algebraic multiplicity = 3, and geometric multiplicity = 3.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;NumPy check&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[2,1],[0,2]], dtype=float)
 A_np = np.linalg.eig(A_np)
 eigvals, eigvecs print("Eigenvalues:", eigvals)
print("Eigenvectors:\n", eigvecs)&lt;/code&gt;
    &lt;code&gt;Eigenvalues: [2. 2.]
Eigenvectors:
 [[ 1.0000000e+00 -1.0000000e+00]
 [ 0.0000000e+00  4.4408921e-16]]&lt;/code&gt;
    &lt;p&gt;NumPy won’t show multiplicities directly, but you can see repeated eigenvalues.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Comparing two cases&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Defective matrix: Algebraic &amp;gt; geometric (like the upper triangular \(A\)).&lt;/item&gt;
      &lt;item&gt;Diagonalizable matrix: Algebraic = geometric (like \(B\)).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This distinction determines whether a matrix can be diagonalized.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Compute algebraic and geometric multiplicities of&lt;/p&gt;
        &lt;p&gt;\[ \begin{bmatrix} 1 &amp;amp; 1 \\ 0 &amp;amp; 1 \end{bmatrix} \]&lt;/p&gt;
        &lt;p&gt;(hint: only one eigenvector).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Take a diagonal matrix with repeated entries - what happens to multiplicities?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Test a random 3×3 singular matrix. Does 0 have algebraic multiplicity &amp;gt; 1?&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Algebraic multiplicity = count of root in characteristic polynomial.&lt;/item&gt;
      &lt;item&gt;Geometric multiplicity = dimension of eigenspace.&lt;/item&gt;
      &lt;item&gt;If they match for all eigenvalues → matrix is diagonalizable.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;64. Diagonalization (When a Matrix Becomes Simple)&lt;/head&gt;
    &lt;p&gt;A matrix \(A\) is diagonalizable if it can be written as&lt;/p&gt;
    &lt;p&gt;\[ A = P D P^{-1} \]&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(D\) is diagonal (containing eigenvalues).&lt;/item&gt;
      &lt;item&gt;Columns of \(P\) are the eigenvectors.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This means \(A\) acts like simple scaling in a “better” coordinate system.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A diagonalizable 2×2 matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([
 A 4,1],
     [2,3]
     [
 ])
= A.diagonalize()
 P, D print("P (eigenvectors):")
print(P)
print("D (eigenvalues on diagonal):")
print(D)

# Verify A = P D P^-1
print("Check:", P*D*P.inv())&lt;/code&gt;
    &lt;code&gt;P (eigenvectors):
Matrix([[-1, 1], [2, 1]])
D (eigenvalues on diagonal):
Matrix([[2, 0], [0, 5]])
Check: Matrix([[4, 1], [2, 3]])&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A non-diagonalizable matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([
 B 2,1],
     [0,2]
     [
 ])
try:
= B.diagonalize()
     P, D print("Diagonalization successful")
     except Exception as e:
print("Not diagonalizable:", e)     &lt;/code&gt;
    &lt;code&gt;Not diagonalizable: Matrix is not diagonalizable&lt;/code&gt;
    &lt;p&gt;This fails because eigenvalue 2 has algebraic multiplicity 2 but geometric multiplicity 1.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Diagonalization with NumPy&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;NumPy doesn’t diagonalize explicitly, but we can build \(P\) and \(D\) ourselves:&lt;/p&gt;
    &lt;code&gt;= np.array([[4,1],[2,3]], dtype=float)
 A_np = np.linalg.eig(A_np)
 eigvals, eigvecs 
= eigvecs
 P = np.diag(eigvals)
 D = np.linalg.inv(P)
 Pinv 
print("Check A = PDP^-1:\n", P @ D @ Pinv)&lt;/code&gt;
    &lt;code&gt;Check A = PDP^-1:
 [[4. 1.]
 [2. 3.]]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Powers of a diagonalizable matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;One reason diagonalization is powerful:&lt;/p&gt;
    &lt;p&gt;\[ A^k = P D^k P^{-1} \]&lt;/p&gt;
    &lt;p&gt;Since \(D^k\) is trivial (just raise each diagonal entry to power \(k\)).&lt;/p&gt;
    &lt;code&gt;= 5
 k 
= np.linalg.matrix_power(A, k)
 A_power = np.linalg.matrix_power(D, k)
 D_power = P @ D_power @ np.linalg.inv(P)
 A_via_diag 
print("A^5 via diagonalization:\n", A_via_diag)
print("Direct A^5:\n", A_power)&lt;/code&gt;
    &lt;code&gt;A^5 via diagonalization:
 [[2094. 1031.]
 [2062. 1063.]]
Direct A^5:
 [[2094 1031]
 [2062 1063]]&lt;/code&gt;
    &lt;p&gt;Both match.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Check whether&lt;/p&gt;
        &lt;p&gt;\[ \begin{bmatrix} 5 &amp;amp; 0 \\ 0 &amp;amp; 5 \end{bmatrix} \]&lt;/p&gt;
        &lt;p&gt;is diagonalizable.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Try diagonalizing a rotation matrix by 90°. Do you get complex eigenvalues?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Verify the formula \(A^k = P D^k P^{-1}\) for a 3×3 diagonalizable matrix.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Diagonalization rewrites a matrix in its simplest form.&lt;/item&gt;
      &lt;item&gt;Works if there are enough independent eigenvectors.&lt;/item&gt;
      &lt;item&gt;It makes powers of \(A\) easy, and is the gateway to analyzing dynamics.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;65. Powers of a Matrix (Long-Term Behavior via Eigenvalues)&lt;/head&gt;
    &lt;p&gt;One of the most useful applications of eigenvalues and diagonalization is computing powers of a matrix:&lt;/p&gt;
    &lt;p&gt;\[ A^k = P D^k P^{-1} \]&lt;/p&gt;
    &lt;p&gt;where \(D\) is diagonal with eigenvalues of \(A\). Each eigenvalue \(\lambda\) raised to \(k\) dictates how its eigenvector direction grows, decays, or oscillates over time.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Simple diagonal matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If \(D = \text{diag}(2,3)\):&lt;/p&gt;
    &lt;code&gt;= Matrix([[2,0],[0,3]])
 D print("D^5 =")
print(D**5)&lt;/code&gt;
    &lt;code&gt;D^5 =
Matrix([[32, 0], [0, 243]])&lt;/code&gt;
    &lt;p&gt;Eigenvalues are 2 and 3. Raising to the 5th power just raises each eigenvalue to the 5th: \(2^5, 3^5\).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Non-diagonal matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([
 A 4,1],
     [2,3]
     [
 ])
= A.diagonalize()
 P, D print("D (eigenvalues):")
print(D)

# Compute A^10 via diagonalization
= P * (D**10) * P.inv()
 A10 print("A^10 =")
print(A10)&lt;/code&gt;
    &lt;code&gt;D (eigenvalues):
Matrix([[2, 0], [0, 5]])
A^10 =
Matrix([[6510758, 3254867], [6509734, 3255891]])&lt;/code&gt;
    &lt;p&gt;Much easier than multiplying \(A\) ten times!&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;NumPy version&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[4,1],[2,3]], dtype=float)
 A_np = np.linalg.eig(A_np)
 eigvals, eigvecs 
= 10
 k = np.diag(eigvals**k)
 D_power = eigvecs @ D_power @ np.linalg.inv(eigvecs)
 A10_np 
print("A^10 via eigen-decomposition:\n", A10_np)&lt;/code&gt;
    &lt;code&gt;A^10 via eigen-decomposition:
 [[6510758. 3254867.]
 [6509734. 3255891.]]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Long-term behavior&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Eigenvalues tell us what happens as \(k \to \infty\):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If \(|\lambda| &amp;lt; 1\) → decay to 0.&lt;/item&gt;
      &lt;item&gt;If \(|\lambda| &amp;gt; 1\) → grows unbounded.&lt;/item&gt;
      &lt;item&gt;If \(|\lambda| = 1\) → oscillates or stabilizes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([
 B 0.5,0],
     [0,1.2]
     [
 ])
= B.diagonalize()
 P, D print("Eigenvalues:", D)
print("B^20:", P*(D**20)*P.inv())&lt;/code&gt;
    &lt;code&gt;Eigenvalues: Matrix([[0.500000000000000, 0], [0, 1.20000000000000]])
B^20: Matrix([[9.53674316406250e-7, 0], [0, 38.3375999244747]])&lt;/code&gt;
    &lt;p&gt;Here, the component along eigenvalue 0.5 decays, while eigenvalue 1.2 grows.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute \(A^{50}\) for a diagonal matrix with eigenvalues 0.9 and 1.1. Which component dominates?&lt;/item&gt;
      &lt;item&gt;Take a stochastic (Markov) matrix and compute powers. Do the rows stabilize?&lt;/item&gt;
      &lt;item&gt;Experiment with complex eigenvalues (like a rotation) and check if the powers oscillate.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Matrix powers are simple when using eigenvalues.&lt;/item&gt;
      &lt;item&gt;Long-term dynamics are controlled by eigenvalue magnitudes.&lt;/item&gt;
      &lt;item&gt;This insight is critical in Markov chains, stability analysis, and dynamical systems.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;66. Real vs. Complex Spectra (Rotations and Oscillations)&lt;/head&gt;
    &lt;p&gt;Not all eigenvalues are real. Some matrices, especially those involving rotations, have complex eigenvalues. Complex eigenvalues often describe oscillations or rotations in systems.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Rotation matrix in 2D&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A 90° rotation matrix:&lt;/p&gt;
    &lt;p&gt;\[ R = \begin{bmatrix} 0 &amp;amp; -1 \\ 1 &amp;amp; 0 \end{bmatrix} \]&lt;/p&gt;
    &lt;code&gt;= Matrix([[0, -1],
 R 1,  0]])
             [
print("Characteristic polynomial:", R.charpoly())
print("Eigenvalues:", R.eigenvals())&lt;/code&gt;
    &lt;code&gt;Characteristic polynomial: PurePoly(lambda**2 + 1, lambda, domain='ZZ')
Eigenvalues: {-I: 1, I: 1}&lt;/code&gt;
    &lt;p&gt;Result: eigenvalues are \(i\) and \(-i\) (purely imaginary).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verify eigen-equation with complex numbers&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= R.eigenvects()
 eigs for eig in eigs:
= eig[0]
     lam = eig[2][0]
     v print(f"λ = {lam}, Av = {R*v}, λv = {lam*v}")     &lt;/code&gt;
    &lt;code&gt;λ = -I, Av = Matrix([[-1], [-I]]), λv = Matrix([[-1], [-I]])
λ = I, Av = Matrix([[-1], [I]]), λv = Matrix([[-1], [I]])&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;NumPy version&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[0,-1],[1,0]], dtype=float)
 R_np = np.linalg.eig(R_np)
 eigvals, eigvecs print("Eigenvalues:", eigvals)
print("Eigenvectors:\n", eigvecs)&lt;/code&gt;
    &lt;code&gt;Eigenvalues: [0.+1.j 0.-1.j]
Eigenvectors:
 [[0.70710678+0.j         0.70710678-0.j        ]
 [0.        -0.70710678j 0.        +0.70710678j]]&lt;/code&gt;
    &lt;p&gt;NumPy shows complex eigenvalues with &lt;code&gt;j&lt;/code&gt; (Python’s imaginary unit).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Rotation by arbitrary angle&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;General 2D rotation:&lt;/p&gt;
    &lt;p&gt;\[ R(\theta) = \begin{bmatrix} \cos\theta &amp;amp; -\sin\theta \\ \sin\theta &amp;amp; \cos\theta \end{bmatrix} \]&lt;/p&gt;
    &lt;p&gt;Eigenvalues:&lt;/p&gt;
    &lt;p&gt;\[ \lambda = e^{\pm i\theta} = \cos\theta \pm i\sin\theta \]&lt;/p&gt;
    &lt;code&gt;= np.pi/4  # 45 degrees
 theta = np.array([[np.cos(theta), -np.sin(theta)],
 R_theta 
                     [np.sin(theta),  np.cos(theta)]])
= np.linalg.eig(R_theta)
 eigvals, eigvecs print("Eigenvalues (rotation 45°):", eigvals)&lt;/code&gt;
    &lt;code&gt;Eigenvalues (rotation 45°): [0.70710678+0.70710678j 0.70710678-0.70710678j]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Oscillation insight&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Complex eigenvalues with \(|\lambda|=1\) → pure oscillation (no growth).&lt;/item&gt;
      &lt;item&gt;If \(|\lambda|&amp;lt;1\) → decaying spiral.&lt;/item&gt;
      &lt;item&gt;If \(|\lambda|&amp;gt;1\) → growing spiral.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;= np.array([[0.8, -0.6],
 A 0.6,  0.8]])
               [
= np.linalg.eig(A)
 eigvals, _ print("Eigenvalues:", eigvals)&lt;/code&gt;
    &lt;code&gt;Eigenvalues: [0.8+0.6j 0.8-0.6j]&lt;/code&gt;
    &lt;p&gt;These eigenvalues lie inside the unit circle → spiral decay.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute eigenvalues of a 180° rotation. What happens?&lt;/item&gt;
      &lt;item&gt;Modify the rotation matrix to include scaling (e.g., multiply by 1.1). Do the eigenvalues lie outside the unit circle?&lt;/item&gt;
      &lt;item&gt;Plot the trajectory of repeatedly applying a rotation matrix to a vector.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Complex eigenvalues naturally appear in rotations and oscillatory systems.&lt;/item&gt;
      &lt;item&gt;Their magnitude controls growth or decay; their angle controls oscillation.&lt;/item&gt;
      &lt;item&gt;This is a key link between linear algebra and dynamics in physics and engineering.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;67. Defective Matrices and a Peek at Jordan Form (When Diagonalization Fails)&lt;/head&gt;
    &lt;p&gt;Not every matrix has enough independent eigenvectors to be diagonalized. Such matrices are called defective. To handle them, mathematicians use the Jordan normal form, which extends diagonalization with extra structure.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A defective example&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ A = \begin{bmatrix} 2 &amp;amp; 1 \\ 0 &amp;amp; 2 \end{bmatrix} \]&lt;/p&gt;
    &lt;code&gt;= Matrix([[2,1],
 A 0,2]])
             [
print("Eigenvalues:", A.eigenvals())
print("Eigenvectors:", A.eigenvects())&lt;/code&gt;
    &lt;code&gt;Eigenvalues: {2: 2}
Eigenvectors: [(2, 2, [Matrix([
[1],
[0]])])]&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Eigenvalue 2 has algebraic multiplicity = 2.&lt;/item&gt;
      &lt;item&gt;Only 1 eigenvector exists → geometric multiplicity = 1.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thus \(A\) is defective, not diagonalizable.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Attempt diagonalization&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;try:
= A.diagonalize()
     P, D print("Diagonal form:", D)
     except Exception as e:
print("Diagonalization failed:", e)     &lt;/code&gt;
    &lt;code&gt;Diagonalization failed: Matrix is not diagonalizable&lt;/code&gt;
    &lt;p&gt;You’ll see an error - confirming \(A\) is not diagonalizable.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Jordan form in SymPy&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= A.jordan_form()
 J, P print("Jordan form J:")
print(J)
print("P (generalized eigenvectors):")
print(P)&lt;/code&gt;
    &lt;code&gt;Jordan form J:
Matrix([[1, 0], [0, 1]])
P (generalized eigenvectors):
Matrix([[2, 1], [0, 2]])&lt;/code&gt;
    &lt;p&gt;The Jordan form shows a Jordan block:&lt;/p&gt;
    &lt;p&gt;\[ J = \begin{bmatrix} 2 &amp;amp; 1 \\ 0 &amp;amp; 2 \end{bmatrix} \]&lt;/p&gt;
    &lt;p&gt;This block structure represents the failure of diagonalization.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;NumPy perspective&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;NumPy doesn’t compute Jordan form, but you can see repeated eigenvalues and lack of eigenvectors:&lt;/p&gt;
    &lt;code&gt;= np.array([[2,1],[0,2]], dtype=float)
 A_np = np.linalg.eig(A_np)
 eigvals, eigvecs print("Eigenvalues:", eigvals)
print("Eigenvectors:\n", eigvecs)&lt;/code&gt;
    &lt;code&gt;Eigenvalues: [2. 2.]
Eigenvectors:
 [[ 1.0000000e+00 -1.0000000e+00]
 [ 0.0000000e+00  4.4408921e-16]]&lt;/code&gt;
    &lt;p&gt;The eigenvectors matrix has fewer independent columns than expected.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Generalized eigenvectors&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Jordan form introduces generalized eigenvectors, which satisfy:&lt;/p&gt;
    &lt;p&gt;\[ (A - \lambda I)^k v = 0 \quad \text{for some } k&amp;gt;1 \]&lt;/p&gt;
    &lt;p&gt;They “fill the gap” when ordinary eigenvectors are insufficient.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Test diagonalizability of&lt;/p&gt;
        &lt;p&gt;\[ \begin{bmatrix} 3 &amp;amp; 1 \\ 0 &amp;amp; 3 \end{bmatrix} \]&lt;/p&gt;
        &lt;p&gt;and compare with its Jordan form.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Try a 3×3 defective matrix with one Jordan block of size 3.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Verify that Jordan blocks still capture the correct eigenvalues.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Defective matrices lack enough eigenvectors for diagonalization.&lt;/item&gt;
      &lt;item&gt;Jordan form replaces diagonalization with blocks, keeping eigenvalues on the diagonal.&lt;/item&gt;
      &lt;item&gt;Understanding Jordan blocks is essential for advanced linear algebra and differential equations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;68. Stability and Spectral Radius (Grow, Decay, or Oscillate)&lt;/head&gt;
    &lt;p&gt;The spectral radius of a matrix \(A\) is defined as&lt;/p&gt;
    &lt;p&gt;\[ \rho(A) = \max_i |\lambda_i| \]&lt;/p&gt;
    &lt;p&gt;where \(\lambda_i\) are the eigenvalues. It tells us the long-term behavior of repeated applications of \(A\):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If \(\rho(A) &amp;lt; 1\) → powers of \(A\) tend to 0 (stable/decay).&lt;/item&gt;
      &lt;item&gt;If \(\rho(A) = 1\) → powers neither blow up nor vanish (neutral, may oscillate).&lt;/item&gt;
      &lt;item&gt;If \(\rho(A) &amp;gt; 1\) → powers diverge (unstable/growth).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Stable matrix (\(\rho &amp;lt; 1\))&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[0.5, 0],
 A 0, 0.3]])
               [
= np.linalg.eigvals(A)
 eigvals = max(abs(eigvals))
 spectral_radius 
print("Eigenvalues:", eigvals)
print("Spectral radius:", spectral_radius)

print("A^10:\n", np.linalg.matrix_power(A, 10))&lt;/code&gt;
    &lt;code&gt;Eigenvalues: [0.5 0.3]
Spectral radius: 0.5
A^10:
 [[9.765625e-04 0.000000e+00]
 [0.000000e+00 5.904900e-06]]&lt;/code&gt;
    &lt;p&gt;All entries shrink toward zero.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Unstable matrix (\(\rho &amp;gt; 1\))&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[1.2, 0],
 B 0, 0.9]])
               [
= np.linalg.eigvals(B)
 eigvals print("Eigenvalues:", eigvals, "Spectral radius:", max(abs(eigvals)))
print("B^10:\n", np.linalg.matrix_power(B, 10))&lt;/code&gt;
    &lt;code&gt;Eigenvalues: [1.2 0.9] Spectral radius: 1.2
B^10:
 [[6.19173642 0.        ]
 [0.         0.34867844]]&lt;/code&gt;
    &lt;p&gt;The component along eigenvalue 1.2 grows quickly.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Neutral/oscillatory case (\(\rho = 1\))&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;90° rotation matrix:&lt;/p&gt;
    &lt;code&gt;= np.array([[0, -1],
 R 1,  0]])
               [
= np.linalg.eigvals(R)
 eigvals print("Eigenvalues:", eigvals)
print("Spectral radius:", max(abs(eigvals)))
print("R^4:\n", np.linalg.matrix_power(R, 4))&lt;/code&gt;
    &lt;code&gt;Eigenvalues: [0.+1.j 0.-1.j]
Spectral radius: 1.0
R^4:
 [[1 0]
 [0 1]]&lt;/code&gt;
    &lt;p&gt;Eigenvalues are ±i, with modulus 1 → pure oscillation.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Spectral radius with SymPy&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([[2,1],[1,2]])
 M = M.eigenvals()
 eigs print("Eigenvalues:", eigs)
print("Spectral radius:", max(abs(ev) for ev in eigs))&lt;/code&gt;
    &lt;code&gt;Eigenvalues: {3: 1, 1: 1}
Spectral radius: 3&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build a diagonal matrix with entries 0.8, 1.0, and 1.1. Predict which direction dominates as powers grow.&lt;/item&gt;
      &lt;item&gt;Apply a random matrix repeatedly to a vector. Does it shrink, grow, or oscillate?&lt;/item&gt;
      &lt;item&gt;Check if a Markov chain transition matrix always has spectral radius 1.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The spectral radius is the key number that predicts growth, decay, or oscillation.&lt;/item&gt;
      &lt;item&gt;Long-term stability in dynamical systems is governed entirely by eigenvalue magnitudes.&lt;/item&gt;
      &lt;item&gt;This connects linear algebra directly to control theory, Markov chains, and differential equations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;69. Markov Chains and Steady States (Probabilities as Linear Algebra)&lt;/head&gt;
    &lt;p&gt;A Markov chain is a process that moves between states according to probabilities. The transitions are encoded in a stochastic matrix \(P\):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Each entry \(p_{ij} \geq 0\)&lt;/item&gt;
      &lt;item&gt;Each row sums to 1&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If we start with a probability vector \(v_0\), then after \(k\) steps:&lt;/p&gt;
    &lt;p&gt;\[ v_k = v_0 P^k \]&lt;/p&gt;
    &lt;p&gt;A steady state is a probability vector \(v\) such that \(vP = v\). It corresponds to eigenvalue \(\lambda = 1\).&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Simple two-state chain&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([
 P 0.9, 0.1],
     [0.5, 0.5]
     [
 ])
= np.array([1.0, 0.0])  # start in state 1
 v0 for k in [1, 2, 5, 10, 50]:
= v0 @ np.linalg.matrix_power(P, k)
     vk print(f"Step {k}: {vk}")     &lt;/code&gt;
    &lt;code&gt;Step 1: [0.9 0.1]
Step 2: [0.86 0.14]
Step 5: [0.83504 0.16496]
Step 10: [0.83335081 0.16664919]
Step 50: [0.83333333 0.16666667]&lt;/code&gt;
    &lt;p&gt;The distribution stabilizes as \(k\) increases.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Steady state via eigenvector&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Find eigenvector for eigenvalue 1:&lt;/p&gt;
    &lt;code&gt;= np.linalg.eig(P.T)
 eigvals, eigvecs = eigvecs[:, np.isclose(eigvals, 1)]
 steady_state = steady_state / steady_state.sum()
 steady_state print("Steady state:", steady_state.real.flatten())&lt;/code&gt;
    &lt;code&gt;Steady state: [0.83333333 0.16666667]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;SymPy exact check&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([[0.9,0.1],[0.5,0.5]])
 P_sym = P_sym.eigenvects()
 steady print("Eigen info:", steady)&lt;/code&gt;
    &lt;code&gt;Eigen info: [(1.00000000000000, 1, [Matrix([
[0.707106781186548],
[0.707106781186547]])]), (0.400000000000000, 1, [Matrix([
[-0.235702260395516],
[  1.17851130197758]])])]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A 3-state example&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([
 Q 0.3, 0.7, 0.0],
     [0.2, 0.5, 0.3],
     [0.1, 0.2, 0.7]
     [
 ])
= np.linalg.eig(Q.T)
 eigvals, eigvecs = eigvecs[:, np.isclose(eigvals, 1)]
 steady = steady / steady.sum()
 steady print("Steady state for Q:", steady.real.flatten())&lt;/code&gt;
    &lt;code&gt;Steady state for Q: [0.17647059 0.41176471 0.41176471]&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Create a transition matrix where one state is absorbing (e.g., row = [0,0,1]). What happens to the steady state?&lt;/item&gt;
      &lt;item&gt;Simulate a random walk on 3 states. Does the steady state distribute evenly?&lt;/item&gt;
      &lt;item&gt;Compare long-run simulation with eigenvector computation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Markov chains evolve by repeated multiplication with a stochastic matrix.&lt;/item&gt;
      &lt;item&gt;Steady states are eigenvectors with eigenvalue 1.&lt;/item&gt;
      &lt;item&gt;This framework powers real applications like PageRank, weather models, and queuing systems.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;70. Linear Differential Systems (Solutions via Eigen-Decomposition)&lt;/head&gt;
    &lt;p&gt;Linear differential equations often reduce to systems of the form:&lt;/p&gt;
    &lt;p&gt;\[ \frac{d}{dt}x(t) = A x(t) \]&lt;/p&gt;
    &lt;p&gt;where \(A\) is a matrix and \(x(t)\) is a vector of functions. The solution is given by the matrix exponential:&lt;/p&gt;
    &lt;p&gt;\[ x(t) = e^{At} x(0) \]&lt;/p&gt;
    &lt;p&gt;If \(A\) is diagonalizable, this becomes simple using eigenvalues and eigenvectors.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from sympy import Matrix, exp, symbols
from scipy.linalg import expm&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Simple system with diagonal matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ A = \begin{bmatrix} -1 &amp;amp; 0 \\ 0 &amp;amp; 2 \end{bmatrix} \]&lt;/p&gt;
    &lt;code&gt;= Matrix([[-1,0],
 A 0, 2]])
             [= symbols('t')
 t = (A*t).exp()
 expAt print("e^{At} =")
print(expAt)&lt;/code&gt;
    &lt;code&gt;e^{At} =
Matrix([[exp(-t), 0], [0, exp(2*t)]])&lt;/code&gt;
    &lt;p&gt;Solution:&lt;/p&gt;
    &lt;p&gt;\[ x(t) = \begin{bmatrix} e^{-t} &amp;amp; 0 \\ 0 &amp;amp; e^{2t} \end{bmatrix} x(0) \]&lt;/p&gt;
    &lt;p&gt;One component decays, the other grows.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Non-diagonal example&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Matrix([[0,1],
 B -2,-3]])
             [= (B*t).exp()
 expBt print("e^{Bt} =")
print(expBt)&lt;/code&gt;
    &lt;code&gt;e^{Bt} =
Matrix([[2*exp(-t) - exp(-2*t), exp(-t) - exp(-2*t)], [-2*exp(-t) + 2*exp(-2*t), -exp(-t) + 2*exp(-2*t)]])&lt;/code&gt;
    &lt;p&gt;Here the solution involves exponentials and possibly sines/cosines (oscillatory behavior).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Numeric computation with SciPy&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;import numpy as np
from scipy.linalg import expm

= np.array([[-1,0],[0,2]], dtype=float)
 A = 1.0
 t print("Matrix exponential e^{At} at t=1:\n", expm(A*t))&lt;/code&gt;
    &lt;code&gt;Matrix exponential e^{At} at t=1:
 [[0.36787944 0.        ]
 [0.         7.3890561 ]]&lt;/code&gt;
    &lt;p&gt;This computes \(e^{At}\) numerically.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Simulation of a trajectory&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([1.0, 1.0])
 x0 for t in [0, 0.5, 1, 2]:
= expm(A*t) @ x0
     xt print(f"x({t}) = {xt}")     &lt;/code&gt;
    &lt;code&gt;x(0) = [1. 1.]
x(0.5) = [0.60653066 2.71828183]
x(1) = [0.36787944 7.3890561 ]
x(2) = [ 0.13533528 54.59815003]&lt;/code&gt;
    &lt;p&gt;One coordinate decays, the other explodes with time.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Solve the system \(\dot{x} = \begin{bmatrix} 0 &amp;amp; 1 \\ -1 &amp;amp; 0 \end{bmatrix}x\). What kind of motion do you see?&lt;/item&gt;
      &lt;item&gt;Use SciPy to simulate a system with eigenvalues less than 0. Does it always decay?&lt;/item&gt;
      &lt;item&gt;Try an unstable system with eigenvalues &amp;gt; 0 and watch how trajectories diverge.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Linear systems \(\dot{x} = Ax\) are solved via the matrix exponential.&lt;/item&gt;
      &lt;item&gt;Eigenvalues determine stability: negative real parts = stable, positive = unstable, imaginary = oscillations.&lt;/item&gt;
      &lt;item&gt;This ties linear algebra directly to differential equations and dynamical systems.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Chapter 8. Orthogonality, least squars, and QR&lt;/head&gt;
    &lt;head rend="h3"&gt;71. Inner Products Beyond Dot Product (Custom Notions of Angle)&lt;/head&gt;
    &lt;p&gt;The dot product is the standard inner product in \(\mathbb{R}^n\), but linear algebra allows us to define more general inner products that measure length and angle in different ways.&lt;/p&gt;
    &lt;p&gt;An inner product on a vector space is a function \(\langle u, v \rangle\) that satisfies:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Linearity in the first argument.&lt;/item&gt;
      &lt;item&gt;Symmetry: \(\langle u, v \rangle = \langle v, u \rangle\).&lt;/item&gt;
      &lt;item&gt;Positive definiteness: \(\langle v, v \rangle \geq 0\) and equals 0 only if \(v=0\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Standard dot product&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([1,2,3])
 u = np.array([4,5,6])
 v 
print("Dot product:", np.dot(u,v))&lt;/code&gt;
    &lt;code&gt;Dot product: 32&lt;/code&gt;
    &lt;p&gt;This is the familiar formula: \(1·4 + 2·5 + 3·6 = 32\).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Weighted inner product&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can define:&lt;/p&gt;
    &lt;p&gt;\[ \langle u, v \rangle_W = u^T W v \]&lt;/p&gt;
    &lt;p&gt;where \(W\) is a positive definite matrix.&lt;/p&gt;
    &lt;code&gt;= np.array([[2,0,0],
 W 0,1,0],
               [0,0,3]])
               [
def weighted_inner(u,v,W):
return u.T @ W @ v
     
print("Weighted inner product:", weighted_inner(u,v,W))&lt;/code&gt;
    &lt;code&gt;Weighted inner product: 72&lt;/code&gt;
    &lt;p&gt;Here, some coordinates “count more” than others.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Check symmetry and positivity&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;print("⟨u,v⟩ == ⟨v,u⟩ ?", weighted_inner(u,v,W) == weighted_inner(v,u,W))
print("⟨u,u⟩ (should be &amp;gt;0):", weighted_inner(u,u,W))&lt;/code&gt;
    &lt;code&gt;⟨u,v⟩ == ⟨v,u⟩ ? True
⟨u,u⟩ (should be &amp;gt;0): 33&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Angle with weighted inner product&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ \cos\theta = \frac{\langle u,v \rangle_W}{\|u\|_W \, \|v\|_W} \]&lt;/p&gt;
    &lt;code&gt;def weighted_norm(u,W):
return np.sqrt(weighted_inner(u,u,W))
     
= weighted_inner(u,v,W) / (weighted_norm(u,W) * weighted_norm(v,W))
 cos_theta print("Cosine of angle (weighted):", cos_theta)&lt;/code&gt;
    &lt;code&gt;Cosine of angle (weighted): 0.97573875381809&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Custom example: correlation inner product&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For statistics, an inner product can be defined as covariance or correlation. Example with mean-centered vectors:&lt;/p&gt;
    &lt;code&gt;= np.array([2,4,6])
 x = np.array([1,3,5])
 y 
= x - x.mean()
 x_centered = y - y.mean()
 y_centered 
= np.dot(x_centered,y_centered)
 corr_inner print("Correlation-style inner product:", corr_inner)&lt;/code&gt;
    &lt;code&gt;Correlation-style inner product: 8.0&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Define a custom inner product with \(W = \text{diag}(1,10,100)\). How does it change angles between vectors?&lt;/item&gt;
      &lt;item&gt;Verify positivity: compute \(\langle v, v \rangle_W\) for a random vector \(v\).&lt;/item&gt;
      &lt;item&gt;Compare dot product vs weighted inner product on the same pair of vectors.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Inner products generalize the dot product to new “geometries.”&lt;/item&gt;
      &lt;item&gt;By changing the weight matrix \(W\), you change how lengths and angles are measured.&lt;/item&gt;
      &lt;item&gt;This flexibility is essential in statistics, optimization, and machine learning.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;72. Orthogonality and Orthonormal Bases (Perpendicular Power)&lt;/head&gt;
    &lt;p&gt;Two vectors are orthogonal if their inner product is zero:&lt;/p&gt;
    &lt;p&gt;\[ \langle u, v \rangle = 0 \]&lt;/p&gt;
    &lt;p&gt;If, in addition, each vector has length 1, the set is orthonormal. Orthonormal bases are extremely useful because they simplify computations: projections, decompositions, and coordinate changes all become clean.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Check orthogonality&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([1, -1])
 u = np.array([1, 1])
 v 
print("Dot product:", np.dot(u,v))&lt;/code&gt;
    &lt;code&gt;Dot product: 0&lt;/code&gt;
    &lt;p&gt;Since the dot product is 0, they’re orthogonal.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Normalizing vectors&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ \hat{u} = \frac{u}{\|u\|} \]&lt;/p&gt;
    &lt;code&gt;def normalize(vec):
return vec / np.linalg.norm(vec)
     
= normalize(u)
 u_norm = normalize(v)
 v_norm 
print("Normalized u:", u_norm)
print("Normalized v:", v_norm)&lt;/code&gt;
    &lt;code&gt;Normalized u: [ 0.70710678 -0.70710678]
Normalized v: [0.70710678 0.70710678]&lt;/code&gt;
    &lt;p&gt;Now both have length 1.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Form an orthonormal basis&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.column_stack((u_norm, v_norm))
 basis print("Orthonormal basis:\n", basis)

print("Check inner products:\n", basis.T @ basis)&lt;/code&gt;
    &lt;code&gt;Orthonormal basis:
 [[ 0.70710678  0.70710678]
 [-0.70710678  0.70710678]]
Check inner products:
 [[ 1.00000000e+00 -2.23711432e-17]
 [-2.23711432e-17  1.00000000e+00]]&lt;/code&gt;
    &lt;p&gt;The result is the identity matrix → perfectly orthonormal.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Apply to coordinates&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If \(x = [2,3]\), coordinates in the orthonormal basis are:&lt;/p&gt;
    &lt;code&gt;= np.array([2,3])
 x = basis.T @ x
 coords print("Coordinates in new basis:", coords)
print("Reconstruction:", basis @ coords)&lt;/code&gt;
    &lt;code&gt;Coordinates in new basis: [-0.70710678  3.53553391]
Reconstruction: [2. 3.]&lt;/code&gt;
    &lt;p&gt;It reconstructs exactly.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Random example with QR&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Any set of linearly independent vectors can be orthonormalized (Gram–Schmidt, or QR decomposition):&lt;/p&gt;
    &lt;code&gt;= np.random.rand(3,3)
 M = np.linalg.qr(M)
 Q, R print("Q (orthonormal basis):\n", Q)
print("Check Q^T Q = I:\n", Q.T @ Q)&lt;/code&gt;
    &lt;code&gt;Q (orthonormal basis):
 [[-0.37617518  0.91975919 -0.111961  ]
 [-0.82070726 -0.38684608 -0.42046368]
 [-0.430037   -0.06628079  0.90037494]]
Check Q^T Q = I:
 [[1.00000000e+00 5.55111512e-17 5.55111512e-17]
 [5.55111512e-17 1.00000000e+00 3.47849792e-17]
 [5.55111512e-17 3.47849792e-17 1.00000000e+00]]&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Create two 3D vectors and check if they’re orthogonal.&lt;/item&gt;
      &lt;item&gt;Normalize them to form an orthonormal set.&lt;/item&gt;
      &lt;item&gt;Use &lt;code&gt;np.linalg.qr&lt;/code&gt;on a 4×3 random matrix and verify that the columns of \(Q\) are orthonormal.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Orthogonality means perpendicularity; orthonormality adds unit length.&lt;/item&gt;
      &lt;item&gt;Orthonormal bases simplify coordinate systems, making inner products and projections easy.&lt;/item&gt;
      &lt;item&gt;QR decomposition is the practical tool to generate orthonormal bases in higher dimensions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;73. Gram–Schmidt Process (Constructing Orthonormal Bases)&lt;/head&gt;
    &lt;p&gt;The Gram–Schmidt process takes a set of linearly independent vectors and turns them into an orthonormal basis. This is crucial for working with subspaces, projections, and numerical stability.&lt;/p&gt;
    &lt;p&gt;Given vectors \(v_1, v_2, \dots, v_n\):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Set \(u_1 = v_1\).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Subtract projections to make each new vector orthogonal to the earlier ones:&lt;/p&gt;
        &lt;p&gt;\[ u_k = v_k - \sum_{j=1}^{k-1} \frac{\langle v_k, u_j \rangle}{\langle u_j, u_j \rangle} u_j \]&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Normalize:&lt;/p&gt;
        &lt;p&gt;\[ e_k = \frac{u_k}{\|u_k\|} \]&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The set \(\{e_1, e_2, \dots, e_n\}\) is orthonormal.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Define vectors&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([1.0, 1.0, 0.0])
 v1 = np.array([1.0, 0.0, 1.0])
 v2 = np.array([0.0, 1.0, 1.0])
 v3 = [v1, v2, v3] V &lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Implement Gram–Schmidt&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;def gram_schmidt(V):
= []
     U for v in V:
     = v.copy()
         u for uj in U:
         -= np.dot(v, uj) / np.dot(uj, uj) * uj
             u 
         U.append(u)# Normalize
     = [u/np.linalg.norm(u) for u in U]
     E return np.array(E)
     
= gram_schmidt(V)
 E print("Orthonormal basis:\n", E)
print("Check orthonormality:\n", np.round(E @ E.T, 6))&lt;/code&gt;
    &lt;code&gt;Orthonormal basis:
 [[ 0.70710678  0.70710678  0.        ]
 [ 0.40824829 -0.40824829  0.81649658]
 [-0.57735027  0.57735027  0.57735027]]
Check orthonormality:
 [[1. 0. 0.]
 [0. 1. 0.]
 [0. 0. 1.]]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compare with NumPy QR&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.linalg.qr(np.column_stack(V))
 Q, R print("QR-based orthonormal basis:\n", Q)
print("Check Q^T Q = I:\n", np.round(Q.T @ Q, 6))&lt;/code&gt;
    &lt;code&gt;QR-based orthonormal basis:
 [[-0.70710678  0.40824829 -0.57735027]
 [-0.70710678 -0.40824829  0.57735027]
 [-0.          0.81649658  0.57735027]]
Check Q^T Q = I:
 [[ 1.  0. -0.]
 [ 0.  1. -0.]
 [-0. -0.  1.]]&lt;/code&gt;
    &lt;p&gt;Both methods give orthonormal bases.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Application: projection&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To project a vector \(x\) onto the span of \(V\):&lt;/p&gt;
    &lt;code&gt;= np.array([2.0, 2.0, 2.0])
 x = sum((x @ e) * e for e in E)
 proj print("Projection of x onto span(V):", proj)&lt;/code&gt;
    &lt;code&gt;Projection of x onto span(V): [2. 2. 2.]&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Run Gram–Schmidt on two vectors in 2D. Compare with just normalizing and checking orthogonality.&lt;/item&gt;
      &lt;item&gt;Replace one vector with a linear combination of others. What happens?&lt;/item&gt;
      &lt;item&gt;Use QR decomposition on a 4×3 random matrix and compare with Gram–Schmidt.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gram–Schmidt converts arbitrary independent vectors into an orthonormal basis.&lt;/item&gt;
      &lt;item&gt;Orthonormal bases simplify projections, decompositions, and computations.&lt;/item&gt;
      &lt;item&gt;In practice, QR decomposition is often used as a numerically stable implementation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;74. Orthogonal Projections onto Subspaces (Closest Point Principle)&lt;/head&gt;
    &lt;p&gt;Given a subspace spanned by vectors, the orthogonal projection of a vector \(x\) onto the subspace is the point in the subspace that is closest to \(x\). This is a cornerstone idea in least squares, data fitting, and signal processing.&lt;/p&gt;
    &lt;head rend="h4"&gt;Formula Recap&lt;/head&gt;
    &lt;p&gt;If \(Q\) is a matrix with orthonormal columns spanning the subspace, the projection of \(x\) is:&lt;/p&gt;
    &lt;p&gt;\[ \text{proj}(x) = Q Q^T x \]&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Projection onto a line (1D subspace)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Suppose the subspace is spanned by \(u = [1,2]\).&lt;/p&gt;
    &lt;code&gt;= np.array([1.0,2.0])
 u = np.array([3.0,1.0])
 x 
= u / np.linalg.norm(u)
 u_norm = np.dot(x, u_norm) * u_norm
 proj print("Projection of x onto span(u):", proj)&lt;/code&gt;
    &lt;code&gt;Projection of x onto span(u): [1. 2.]&lt;/code&gt;
    &lt;p&gt;This gives the closest point to \(x\) along the line spanned by \(u\).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Projection onto a plane (2D subspace in 3D)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([1.0,0.0,0.0])
 u1 = np.array([0.0,1.0,0.0])
 u2 = np.column_stack([u1,u2])   # Orthonormal basis for xy-plane
 Q 
= np.array([2.0,3.0,5.0])
 x = Q @ Q.T @ x
 proj print("Projection of x onto xy-plane:", proj)&lt;/code&gt;
    &lt;code&gt;Projection of x onto xy-plane: [2. 3. 0.]&lt;/code&gt;
    &lt;p&gt;Result drops the z-component → projection onto the plane.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;General projection using QR&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[1,1,0],
 A 0,1,1],
               [1,0,1]], dtype=float)
               [
= np.linalg.qr(A)
 Q, R = Q[:, :2]   # take first 2 independent columns
 Q = np.array([2,2,2], dtype=float)
 x 
= Q @ Q.T @ x
 proj print("Projection of x onto span(A):", proj)&lt;/code&gt;
    &lt;code&gt;Projection of x onto span(A): [2.66666667 1.33333333 1.33333333]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visualization (2D case)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;import matplotlib.pyplot as plt

0,0,x[0],x[1],angles='xy',scale_units='xy',scale=1,color='red',label="x")
 plt.quiver(0,0,proj[0],proj[1],angles='xy',scale_units='xy',scale=1,color='blue',label="Projection")
 plt.quiver(0,0,u[0],u[1],angles='xy',scale_units='xy',scale=1,color='green',label="Subspace")
 plt.quiver('equal'); plt.grid(); plt.legend(); plt.show() plt.axis(&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Project a vector onto the line spanned by \([2,1]\).&lt;/item&gt;
      &lt;item&gt;Project \([1,2,3]\) onto the plane spanned by \([1,0,1]\) and \([0,1,1]\).&lt;/item&gt;
      &lt;item&gt;Compare projection via formula \(Q Q^T x\) with manually solving least squares.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Orthogonal projection finds the closest point in a subspace.&lt;/item&gt;
      &lt;item&gt;Formula \(Q Q^T x\) works perfectly when \(Q\) has orthonormal columns.&lt;/item&gt;
      &lt;item&gt;Projections are the foundation of least squares, PCA, and many geometric algorithms.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;75. Least-Squares Problems (Fit When Exact Solve Is Impossible)&lt;/head&gt;
    &lt;p&gt;Sometimes a system of equations \(Ax = b\) has no exact solution - usually because it’s overdetermined (more equations than unknowns). In this case, we look for an approximate solution \(x^*\) that minimizes the error:&lt;/p&gt;
    &lt;p&gt;\[ x^* = \arg\min_x \|Ax - b\|^2 \]&lt;/p&gt;
    &lt;p&gt;This is the least-squares solution, which geometrically is the projection of \(b\) onto the column space of \(A\).&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Overdetermined system&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;3 equations, 2 unknowns:&lt;/p&gt;
    &lt;code&gt;= np.array([[1,1],
 A 1,2],
               [1,3]], dtype=float)
               [= np.array([6, 0, 0], dtype=float) b &lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Solve least squares with NumPy&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.linalg.lstsq(A, b, rcond=None)
 x_star, residuals, rank, s print("Least squares solution:", x_star)
print("Residual norm squared:", residuals)&lt;/code&gt;
    &lt;code&gt;Least squares solution: [ 8. -3.]
Residual norm squared: [6.]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compare with normal equations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ A^T A x = A^T b \]&lt;/p&gt;
    &lt;code&gt;= np.linalg.inv(A.T @ A) @ (A.T @ b)
 x_normal print("Solution via normal equations:", x_normal)&lt;/code&gt;
    &lt;code&gt;Solution via normal equations: [ 8. -3.]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Geometric picture&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The least-squares solution projects \(b\) onto the column space of \(A\):&lt;/p&gt;
    &lt;code&gt;= A @ x_star
 proj print("Projection of b onto Col(A):", proj)
print("Original b:", b)
print("Error vector (b - proj):", b - proj)&lt;/code&gt;
    &lt;code&gt;Projection of b onto Col(A): [ 5.  2. -1.]
Original b: [6. 0. 0.]
Error vector (b - proj): [ 1. -2.  1.]&lt;/code&gt;
    &lt;p&gt;The error vector is orthogonal to the column space.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verify orthogonality condition&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ A^T (b - Ax^*) = 0 \]&lt;/p&gt;
    &lt;code&gt;print("Check orthogonality:", A.T @ (b - A @ x_star))&lt;/code&gt;
    &lt;code&gt;Check orthogonality: [0. 0.]&lt;/code&gt;
    &lt;p&gt;The result should be (close to) zero.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Create a taller \(A\) (say 5×2) with random numbers and solve least squares for a random \(b\).&lt;/item&gt;
      &lt;item&gt;Compare the residual from &lt;code&gt;np.linalg.lstsq&lt;/code&gt;with geometric intuition (projection).&lt;/item&gt;
      &lt;item&gt;Modify \(b\) so that the system has an exact solution. Check if least squares gives it exactly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Least-squares finds the best-fit solution when no exact solution exists.&lt;/item&gt;
      &lt;item&gt;It works by projecting \(b\) onto the column space of \(A\).&lt;/item&gt;
      &lt;item&gt;This principle underlies regression, curve fitting, and countless applications in data science.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;76. Normal Equations and Geometry of Residuals (Why It Works)&lt;/head&gt;
    &lt;p&gt;The least-squares solution can be found by solving the normal equations:&lt;/p&gt;
    &lt;p&gt;\[ A^T A x = A^T b \]&lt;/p&gt;
    &lt;p&gt;This comes from the condition that the residual vector&lt;/p&gt;
    &lt;p&gt;\[ r = b - Ax \]&lt;/p&gt;
    &lt;p&gt;is orthogonal to the column space of \(A\).&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build an overdetermined system&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[1,1],
 A 1,2],
               [1,3]], dtype=float)
               [= np.array([6, 0, 0], dtype=float) b &lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Solve least squares via normal equations&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= A.T @ A
 ATA = A.T @ b
 ATb = np.linalg.solve(ATA, ATb)
 x_star 
print("Least-squares solution x*:", x_star)&lt;/code&gt;
    &lt;code&gt;Least-squares solution x*: [ 8. -3.]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute residual and check orthogonality&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= b - A @ x_star
 residual print("Residual vector:", residual)
print("Check A^T r ≈ 0:", A.T @ residual)&lt;/code&gt;
    &lt;code&gt;Residual vector: [ 1. -2.  1.]
Check A^T r ≈ 0: [0. 0.]&lt;/code&gt;
    &lt;p&gt;This verifies the residual is perpendicular to the column space of \(A\).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compare with NumPy’s least squares solver&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;*_ = np.linalg.lstsq(A, b, rcond=None)
 x_lstsq, print("NumPy lstsq solution:", x_lstsq)&lt;/code&gt;
    &lt;code&gt;NumPy lstsq solution: [ 8. -3.]&lt;/code&gt;
    &lt;p&gt;The solutions should match (within numerical precision).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Geometric picture&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(b\) is a point in \(\mathbb{R}^3\).&lt;/item&gt;
      &lt;item&gt;\(Ax\) is restricted to lie in the 2D column space of \(A\).&lt;/item&gt;
      &lt;item&gt;The least-squares solution picks the \(Ax\) closest to \(b\).&lt;/item&gt;
      &lt;item&gt;The error vector \(r = b - Ax^*\) is orthogonal to the subspace.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= A @ x_star
 proj print("Projection of b onto Col(A):", proj)&lt;/code&gt;
    &lt;code&gt;Projection of b onto Col(A): [ 5.  2. -1.]&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Change \(b\) to \([1,1,1]\). Solve again and check the residual.&lt;/item&gt;
      &lt;item&gt;Use a random tall \(A\) (say 6×2) and verify that the residual is always orthogonal.&lt;/item&gt;
      &lt;item&gt;Compute \(\|r\|\) and see how it changes when you change \(b\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Least squares works by making the residual orthogonal to the column space.&lt;/item&gt;
      &lt;item&gt;Normal equations are the algebraic way to encode this condition.&lt;/item&gt;
      &lt;item&gt;This orthogonality principle is the geometric heart of least-squares fitting.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;77. QR Factorization (Stable Least Squares via Orthogonality)&lt;/head&gt;
    &lt;p&gt;While normal equations solve least squares, they can be numerically unstable if \(A^T A\) is ill-conditioned. A more stable method uses QR factorization:&lt;/p&gt;
    &lt;p&gt;\[ A = Q R \]&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(Q\): matrix with orthonormal columns&lt;/item&gt;
      &lt;item&gt;\(R\): upper triangular matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Then the least-squares problem reduces to solving:&lt;/p&gt;
    &lt;p&gt;\[ Rx = Q^T b \]&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Overdetermined system&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[1,1],
 A 1,2],
               [1,3]], dtype=float)
               [= np.array([6, 0, 0], dtype=float) b &lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;QR factorization&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.linalg.qr(A)
 Q, R print("Q (orthonormal basis):\n", Q)
print("R (upper triangular):\n", R)&lt;/code&gt;
    &lt;code&gt;Q (orthonormal basis):
 [[-5.77350269e-01  7.07106781e-01]
 [-5.77350269e-01 -1.73054947e-16]
 [-5.77350269e-01 -7.07106781e-01]]
R (upper triangular):
 [[-1.73205081 -3.46410162]
 [ 0.         -1.41421356]]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Solve least squares using QR&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= Q.T @ b
 y = np.linalg.solve(R[:2,:], y[:2])  # only top rows matter
 x_star print("Least squares solution via QR:", x_star)&lt;/code&gt;
    &lt;code&gt;Least squares solution via QR: [ 8. -3.]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compare with NumPy’s &lt;code&gt;lstsq&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;*_ = np.linalg.lstsq(A, b, rcond=None)
 x_lstsq, print("NumPy lstsq:", x_lstsq)&lt;/code&gt;
    &lt;code&gt;NumPy lstsq: [ 8. -3.]&lt;/code&gt;
    &lt;p&gt;The answers should match closely.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Residual check&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= b - A @ x_star
 residual print("Residual vector:", residual)
print("Check orthogonality (Q^T r):", Q.T @ residual)&lt;/code&gt;
    &lt;code&gt;Residual vector: [ 1. -2.  1.]
Check orthogonality (Q^T r): [0.00000000e+00 3.46109895e-16]&lt;/code&gt;
    &lt;p&gt;Residual is orthogonal to the column space, confirming correctness.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Solve least squares for a 5×2 random matrix using both normal equations and QR. Compare results.&lt;/item&gt;
      &lt;item&gt;Check stability by making columns of \(A\) nearly dependent - see if QR behaves better than normal equations.&lt;/item&gt;
      &lt;item&gt;Compute projection of \(b\) using \(Q Q^T b\) and confirm it equals \(A x^*\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;QR factorization provides a numerically stable way to solve least squares.&lt;/item&gt;
      &lt;item&gt;It avoids the instability of normal equations.&lt;/item&gt;
      &lt;item&gt;In practice, modern solvers (like NumPy’s &lt;code&gt;lstsq&lt;/code&gt;) rely on QR or SVD under the hood.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;78. Orthogonal Matrices (Length-Preserving Transforms)&lt;/head&gt;
    &lt;p&gt;An orthogonal matrix \(Q\) is a square matrix whose columns (and rows) are orthonormal vectors. Formally:&lt;/p&gt;
    &lt;p&gt;\[ Q^T Q = Q Q^T = I \]&lt;/p&gt;
    &lt;p&gt;Key properties:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Preserves lengths: \(\|Qx\| = \|x\|\)&lt;/item&gt;
      &lt;item&gt;Preserves dot products: \(\langle Qx, Qy \rangle = \langle x, y \rangle\)&lt;/item&gt;
      &lt;item&gt;Determinant is either \(+1\) (rotation) or \(-1\) (reflection)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Construct a simple orthogonal matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;90° rotation in 2D:&lt;/p&gt;
    &lt;code&gt;= np.array([[0, -1],
 Q 1,  0]])
               [
print("Q^T Q =\n", Q.T @ Q)&lt;/code&gt;
    &lt;code&gt;Q^T Q =
 [[1 0]
 [0 1]]&lt;/code&gt;
    &lt;p&gt;Result = identity → confirms orthogonality.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Check length preservation&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([3,4])
 x print("Original length:", np.linalg.norm(x))
print("Transformed length:", np.linalg.norm(Q @ x))&lt;/code&gt;
    &lt;code&gt;Original length: 5.0
Transformed length: 5.0&lt;/code&gt;
    &lt;p&gt;Both lengths match.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Check dot product preservation&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([1,0])
 u = np.array([0,1])
 v 
print("Dot(u,v):", np.dot(u,v))
print("Dot(Q u, Q v):", np.dot(Q @ u, Q @ v))&lt;/code&gt;
    &lt;code&gt;Dot(u,v): 0
Dot(Q u, Q v): 0&lt;/code&gt;
    &lt;p&gt;Dot product is preserved.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Reflection matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Reflection about the x-axis:&lt;/p&gt;
    &lt;code&gt;= np.array([[1,0],
 R 0,-1]])
               [
print("R^T R =\n", R.T @ R)
print("Determinant of R:", np.linalg.det(R))&lt;/code&gt;
    &lt;code&gt;R^T R =
 [[1 0]
 [0 1]]
Determinant of R: -1.0&lt;/code&gt;
    &lt;p&gt;Determinant = -1 → reflection.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Random orthogonal matrix via QR&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.random.rand(3,3)
 M = np.linalg.qr(M)
 Q, _ print("Q (random orthogonal):\n", Q)
print("Check Q^T Q ≈ I:\n", np.round(Q.T @ Q, 6))&lt;/code&gt;
    &lt;code&gt;Q (random orthogonal):
 [[-0.59472353  0.03725157 -0.80306677]
 [-0.61109913 -0.67000966  0.42147943]
 [-0.52236172  0.74141714  0.42123492]]
Check Q^T Q ≈ I:
 [[ 1.  0. -0.]
 [ 0.  1. -0.]
 [-0. -0.  1.]]&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build a 2D rotation matrix for 45°. Verify it’s orthogonal.&lt;/item&gt;
      &lt;item&gt;Check whether scaling matrices (e.g., \(\text{diag}(2,1)\)) are orthogonal. Why or why not?&lt;/item&gt;
      &lt;item&gt;Generate a random orthogonal matrix with &lt;code&gt;np.linalg.qr&lt;/code&gt;and test its determinant.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Orthogonal matrices are rigid motions: they rotate or reflect without distorting lengths or angles.&lt;/item&gt;
      &lt;item&gt;They play a key role in numerical stability, geometry, and physics.&lt;/item&gt;
      &lt;item&gt;Every orthonormal basis corresponds to an orthogonal matrix.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;79. Fourier Viewpoint (Expanding in Orthogonal Waves)&lt;/head&gt;
    &lt;p&gt;The Fourier viewpoint treats functions or signals as combinations of orthogonal waves (sines and cosines). This is just linear algebra: sine and cosine functions form an orthogonal basis, and any signal can be expressed as a linear combination of them.&lt;/p&gt;
    &lt;head rend="h4"&gt;Formula Recap&lt;/head&gt;
    &lt;p&gt;For a discrete signal \(x\), the Discrete Fourier Transform (DFT) is:&lt;/p&gt;
    &lt;p&gt;\[ X_k = \sum_{n=0}^{N-1} x_n e^{-2\pi i kn / N}, \quad k=0,\dots,N-1 \]&lt;/p&gt;
    &lt;p&gt;The inverse DFT reconstructs the signal. Orthogonality of complex exponentials makes this work.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
import matplotlib.pyplot as plt&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build a simple signal&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.linspace(0, 1, 100, endpoint=False)
 t = np.sin(2*np.pi*3*t) + 0.5*np.sin(2*np.pi*5*t)
 signal 
 plt.plot(t, signal)"Signal = sin(3Hz) + 0.5 sin(5Hz)")
 plt.title("Time")
 plt.xlabel("Amplitude")
 plt.ylabel( plt.show()&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute Fourier transform (DFT)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.fft.fft(signal)
 X = np.fft.fftfreq(len(t), d=1/100)  # sampling rate = 100Hz
 freqs 
50], np.abs(X[:50]), basefmt=" ")
 plt.stem(freqs[:"Fourier spectrum")
 plt.title("Frequency (Hz)")
 plt.xlabel("Magnitude")
 plt.ylabel( plt.show()&lt;/code&gt;
    &lt;p&gt;Peaks appear at 3Hz and 5Hz → the frequencies of the original signal.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Reconstruct signal using inverse FFT&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.fft.ifft(X).real
 signal_reconstructed print("Reconstruction error:", np.linalg.norm(signal - signal_reconstructed))&lt;/code&gt;
    &lt;code&gt;Reconstruction error: 1.4664679821708477e-15&lt;/code&gt;
    &lt;p&gt;Error is near zero → perfect reconstruction.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Orthogonality check of sinusoids&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.sin(2*np.pi*3*t)
 u = np.sin(2*np.pi*5*t)
 v 
= np.dot(u, v)
 inner print("Inner product of 3Hz and 5Hz sinusoids:", inner)&lt;/code&gt;
    &lt;code&gt;Inner product of 3Hz and 5Hz sinusoids: 1.2961853812498703e-14&lt;/code&gt;
    &lt;p&gt;The result is ≈ 0 → confirms orthogonality.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Change the frequencies to 7Hz and 9Hz. Do the Fourier peaks move accordingly?&lt;/item&gt;
      &lt;item&gt;Mix in some noise and check how the spectrum looks.&lt;/item&gt;
      &lt;item&gt;Try cosine signals instead of sine. Do you still see orthogonality?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fourier analysis = linear algebra with orthogonal sinusoidal basis functions.&lt;/item&gt;
      &lt;item&gt;Any signal can be decomposed into orthogonal waves.&lt;/item&gt;
      &lt;item&gt;This orthogonal viewpoint powers audio, image compression, and signal processing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;80. Polynomial and Multifeature Least Squares (Fitting More Flexibly)&lt;/head&gt;
    &lt;p&gt;Least squares isn’t limited to straight lines. By adding polynomial or multiple features, we can fit curves and capture more complex relationships. This is the foundation of regression models in data science.&lt;/p&gt;
    &lt;head rend="h4"&gt;Formula Recap&lt;/head&gt;
    &lt;p&gt;Given data \((x_i, y_i)\), we build a design matrix \(A\):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For polynomial fit of degree \(d\):&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ A = \begin{bmatrix} 1 &amp;amp; x_1 &amp;amp; x_1^2 &amp;amp; \dots &amp;amp; x_1^d \\ 1 &amp;amp; x_2 &amp;amp; x_2^2 &amp;amp; \dots &amp;amp; x_2^d \\ \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ 1 &amp;amp; x_n &amp;amp; x_n^2 &amp;amp; \dots &amp;amp; x_n^d \end{bmatrix} \]&lt;/p&gt;
    &lt;p&gt;Then solve least squares:&lt;/p&gt;
    &lt;p&gt;\[ \hat{c} = \arg\min_c \|Ac - y\|^2 \]&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
import matplotlib.pyplot as plt&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Generate noisy quadratic data&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;0)
 np.random.seed(= np.linspace(-3, 3, 30)
 x = 1 - 2*x + 0.5*x**2
 y_true = y_true + np.random.normal(scale=2.0, size=x.shape)
 y_noisy 
="Noisy data")
 plt.scatter(x, y_noisy, label"g--", label="True curve")
 plt.plot(x, y_true, 
 plt.legend() plt.show()&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build polynomial design matrix (degree 2)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.column_stack([np.ones_like(x), x, x**2])
 A *_ = np.linalg.lstsq(A, y_noisy, rcond=None)
 coeffs, print("Fitted coefficients:", coeffs)&lt;/code&gt;
    &lt;code&gt;Fitted coefficients: [ 1.15666306 -2.25753954  0.72733812]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Plot fitted polynomial&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= A @ coeffs
 y_fit ="Noisy data")
 plt.scatter(x, y_noisy, label"r-", label="Fitted quadratic")
 plt.plot(x, y_fit, 
 plt.legend() plt.show()&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Higher-degree fit (overfitting demonstration)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.column_stack([x**i for i in range(6)])  # degree 5
 A_high *_ = np.linalg.lstsq(A_high, y_noisy, rcond=None)
 coeffs_high, 
= A_high @ coeffs_high
 y_fit_high ="Noisy data")
 plt.scatter(x, y_noisy, label"r-", label="Degree 5 polynomial")
 plt.plot(x, y_fit_high, "g--", label="True curve")
 plt.plot(x, y_true, 
 plt.legend() plt.show()&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Multifeature regression example&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Suppose we predict \(y\) from features \([x, x^2, \sin(x)]\):&lt;/p&gt;
    &lt;code&gt;= np.column_stack([np.ones_like(x), x, x**2, np.sin(x)])
 A_multi *_ = np.linalg.lstsq(A_multi, y_noisy, rcond=None)
 coeffs_multi, print("Multi-feature coefficients:", coeffs_multi)&lt;/code&gt;
    &lt;code&gt;Multi-feature coefficients: [ 1.15666306 -2.0492999   0.72733812 -0.65902274]&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fit degree 3, 4, 5 polynomials to the same data. Watch how the curve changes.&lt;/item&gt;
      &lt;item&gt;Add features like \(\cos(x)\) or \(\exp(x)\) - does the fit improve?&lt;/item&gt;
      &lt;item&gt;Compare training error (fit to noisy data) vs error on new test points.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Least squares can fit polynomials and arbitrary feature combinations.&lt;/item&gt;
      &lt;item&gt;The design matrix encodes how input variables transform into features.&lt;/item&gt;
      &lt;item&gt;This is the basis of regression, curve fitting, and many machine learning models.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Chapter 9. SVD, PCA, and Conditioning&lt;/head&gt;
    &lt;head rend="h3"&gt;81. Singular Values and SVD (Universal Factorization)&lt;/head&gt;
    &lt;p&gt;The Singular Value Decomposition (SVD) is one of the most powerful results in linear algebra. It says any \(m \times n\) matrix \(A\) can be factored as:&lt;/p&gt;
    &lt;p&gt;\[ A = U \Sigma V^T \]&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(U\): orthogonal \(m \times m\) matrix (left singular vectors)&lt;/item&gt;
      &lt;item&gt;\(\Sigma\): diagonal \(m \times n\) matrix with nonnegative numbers (singular values)&lt;/item&gt;
      &lt;item&gt;\(V\): orthogonal \(n \times n\) matrix (right singular vectors)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Singular values are always nonnegative and sorted \(\sigma_1 \geq \sigma_2 \geq \dots\).&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute SVD of a matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[3,1,1],
 A -1,3,1]])
               [
= np.linalg.svd(A, full_matrices=True)
 U, S, Vt 
print("U:\n", U)
print("Singular values:", S)
print("V^T:\n", Vt)&lt;/code&gt;
    &lt;code&gt;U:
 [[-0.70710678 -0.70710678]
 [-0.70710678  0.70710678]]
Singular values: [3.46410162 3.16227766]
V^T:
 [[-4.08248290e-01 -8.16496581e-01 -4.08248290e-01]
 [-8.94427191e-01  4.47213595e-01  5.27355937e-16]
 [-1.82574186e-01 -3.65148372e-01  9.12870929e-01]]&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;U&lt;/code&gt;: orthogonal basis in input space.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;S&lt;/code&gt;: singular values (as a 1D array).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;V^T&lt;/code&gt;: orthogonal basis in output space.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Reconstruct \(A\) from decomposition&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.zeros((U.shape[1], Vt.shape[0]))
 Sigma len(S), :len(S)] = np.diag(S)
 Sigma[:
= U @ Sigma @ Vt
 A_reconstructed print("Reconstruction error:", np.linalg.norm(A - A_reconstructed))&lt;/code&gt;
    &lt;code&gt;Reconstruction error: 1.5895974606912448e-15&lt;/code&gt;
    &lt;p&gt;The error should be near zero.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Rank from SVD&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Number of nonzero singular values = rank of \(A\).&lt;/p&gt;
    &lt;code&gt;= np.sum(S &amp;gt; 1e-10)
 rank print("Rank of A:", rank)&lt;/code&gt;
    &lt;code&gt;Rank of A: 2&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Geometry: effect of \(A\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SVD says:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;\(V\) rotates input space.&lt;/item&gt;
      &lt;item&gt;\(\Sigma\) scales along orthogonal directions (by singular values).&lt;/item&gt;
      &lt;item&gt;\(U\) rotates to output space.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This explains why SVD works for any matrix (not just square ones).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Low-rank approximation preview&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Keep only the top singular value(s) → best approximation of \(A\).&lt;/p&gt;
    &lt;code&gt;= 1
 k = np.outer(U[:,0], Vt[0]) * S[0]
 A_approx print("Rank-1 approximation:\n", A_approx)&lt;/code&gt;
    &lt;code&gt;Rank-1 approximation:
 [[1. 2. 1.]
 [1. 2. 1.]]&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute SVD for a random 5×3 matrix. Check if \(U\) and \(V\) are orthogonal.&lt;/item&gt;
      &lt;item&gt;Compare singular values of a diagonal matrix vs a rotation matrix.&lt;/item&gt;
      &lt;item&gt;Zero out small singular values and see how much of \(A\) is preserved.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SVD factorizes any matrix into rotations and scalings.&lt;/item&gt;
      &lt;item&gt;Singular values reveal rank and strength of directions.&lt;/item&gt;
      &lt;item&gt;It’s the universal tool of numerical linear algebra: the backbone of PCA, compression, and stability analysis.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;82. Geometry of SVD (Rotations + Stretching)&lt;/head&gt;
    &lt;p&gt;The Singular Value Decomposition (SVD) has a beautiful geometric interpretation: every matrix is just a combination of two rotations (or reflections) and a stretching.&lt;/p&gt;
    &lt;p&gt;For \(A = U \Sigma V^T\):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;\(V^T\): rotates (or reflects) the input space.&lt;/item&gt;
      &lt;item&gt;\(\Sigma\): stretches space along orthogonal axes by singular values \(\sigma_i\).&lt;/item&gt;
      &lt;item&gt;\(U\): rotates (or reflects) the result into the output space.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This turns any linear transformation into a rotation → stretching → rotation pipeline.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
import matplotlib.pyplot as plt&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Make a 2D matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[2, 1],
 A 1, 3]])               [&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Apply SVD&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.linalg.svd(A)
 U, S, Vt 
print("U:\n", U)
print("Singular values:", S)
print("V^T:\n", Vt)&lt;/code&gt;
    &lt;code&gt;U:
 [[-0.52573111 -0.85065081]
 [-0.85065081  0.52573111]]
Singular values: [3.61803399 1.38196601]
V^T:
 [[-0.52573111 -0.85065081]
 [-0.85065081  0.52573111]]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visualize effect on the unit circle&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The unit circle is often used to visualize linear transformations.&lt;/p&gt;
    &lt;code&gt;= np.linspace(0, 2*np.pi, 200)
 theta = np.vstack((np.cos(theta), np.sin(theta)))
 circle 
= A @ circle
 transformed 
0], circle[1], 'b--', label="Unit circle")
 plt.plot(circle[0], transformed[1], 'r-', label="Transformed")
 plt.plot(transformed["equal")
 plt.axis(
 plt.legend()"Action of A on the unit circle")
 plt.title( plt.show()&lt;/code&gt;
    &lt;p&gt;The circle becomes an ellipse. Its axes align with the singular vectors, and its radii are the singular values.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compare with decomposition steps&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Apply V^T
= Vt @ circle
 step1 # Apply Σ
= np.diag(S)
 Sigma = Sigma @ step1
 step2 # Apply U
= U @ step2
 step3 
0], circle[1], 'b--', label="Unit circle")
 plt.plot(circle[0], step3[1], 'g-', label="U Σ V^T circle")
 plt.plot(step3["equal")
 plt.axis(
 plt.legend()"SVD decomposition of transformation")
 plt.title( plt.show()&lt;/code&gt;
    &lt;p&gt;Both transformed shapes match → confirms SVD’s geometric picture.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Change \(A\) to a pure shear, like &lt;code&gt;[[1,2],[0,1]]&lt;/code&gt;. How does the ellipse look?&lt;/item&gt;
      &lt;item&gt;Try a diagonal matrix, like &lt;code&gt;[[3,0],[0,1]]&lt;/code&gt;. Do the singular vectors match the coordinate axes?&lt;/item&gt;
      &lt;item&gt;Scale the input circle to a square and see if geometry still works.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SVD = rotate → stretch → rotate.&lt;/item&gt;
      &lt;item&gt;The unit circle becomes an ellipse: axes = singular vectors, radii = singular values.&lt;/item&gt;
      &lt;item&gt;This geometric lens makes SVD intuitive and explains why it’s so widely used in data, graphics, and signal processing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;83. Relation to Eigen-Decompositions (ATA and AAT)&lt;/head&gt;
    &lt;p&gt;Singular values and eigenvalues are closely connected. While eigen-decomposition applies only to square matrices, SVD works for any rectangular matrix. The bridge between them is:&lt;/p&gt;
    &lt;p&gt;\[ A^T A v = \sigma^2 v \quad \text{and} \quad A A^T u = \sigma^2 u \]&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(v\): right singular vector (from eigenvectors of \(A^T A\))&lt;/item&gt;
      &lt;item&gt;\(u\): left singular vector (from eigenvectors of \(A A^T\))&lt;/item&gt;
      &lt;item&gt;\(\sigma\): singular values (square roots of eigenvalues of \(A^T A\) or \(A A^T\))&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Define a rectangular matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[2, 0],
 A 1, 1],
               [0, 1]])  # shape 3x2               [&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute SVD directly&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.linalg.svd(A)
 U, S, Vt print("Singular values:", S)&lt;/code&gt;
    &lt;code&gt;Singular values: [2.30277564 1.30277564]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compare with eigenvalues of \(A^T A\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= A.T @ A
 ATA = np.linalg.eig(ATA)
 eigvals, eigvecs 
print("Eigenvalues of A^T A:", eigvals)
print("Square roots (sorted):", np.sqrt(np.sort(eigvals)[::-1]))&lt;/code&gt;
    &lt;code&gt;Eigenvalues of A^T A: [5.30277564 1.69722436]
Square roots (sorted): [2.30277564 1.30277564]&lt;/code&gt;
    &lt;p&gt;Notice: singular values from SVD = square roots of eigenvalues of \(A^T A\).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compare with eigenvalues of \(A A^T\)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= A @ A.T
 AAT = np.linalg.eig(AAT)
 eigvals2, eigvecs2 
print("Eigenvalues of A A^T:", eigvals2)
print("Square roots:", np.sqrt(np.sort(eigvals2)[::-1]))&lt;/code&gt;
    &lt;code&gt;Eigenvalues of A A^T: [ 5.30277564e+00  1.69722436e+00 -2.01266546e-17]
Square roots: [2.30277564 1.30277564        nan]&lt;/code&gt;
    &lt;code&gt;/var/folders/_g/lq_pglm508df70x751kkxrl80000gp/T/ipykernel_31637/436251338.py:5: RuntimeWarning: invalid value encountered in sqrt
  print("Square roots:", np.sqrt(np.sort(eigvals2)[::-1]))&lt;/code&gt;
    &lt;p&gt;They match too → confirming the relationship.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verify singular vectors&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Right singular vectors (\(V\)) = eigenvectors of \(A^T A\).&lt;/item&gt;
      &lt;item&gt;Left singular vectors (\(U\)) = eigenvectors of \(A A^T\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;print("Right singular vectors (V):\n", Vt.T)
print("Eigenvectors of A^T A:\n", eigvecs)

print("Left singular vectors (U):\n", U)
print("Eigenvectors of A A^T:\n", eigvecs2)&lt;/code&gt;
    &lt;code&gt;Right singular vectors (V):
 [[-0.95709203  0.28978415]
 [-0.28978415 -0.95709203]]
Eigenvectors of A^T A:
 [[ 0.95709203 -0.28978415]
 [ 0.28978415  0.95709203]]
Left singular vectors (U):
 [[-0.83125078  0.44487192  0.33333333]
 [-0.54146663 -0.51222011 -0.66666667]
 [-0.12584124 -0.73465607  0.66666667]]
Eigenvectors of A A^T:
 [[-0.83125078  0.44487192  0.33333333]
 [-0.54146663 -0.51222011 -0.66666667]
 [-0.12584124 -0.73465607  0.66666667]]&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Try a square symmetric matrix and compare SVD with eigen-decomposition. Do they match?&lt;/item&gt;
      &lt;item&gt;For a tall vs wide rectangular matrix, check whether \(U\) and \(V\) differ.&lt;/item&gt;
      &lt;item&gt;Compute eigenvalues manually with &lt;code&gt;np.linalg.eig&lt;/code&gt;for a random \(A\) and confirm singular values.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Singular values = square roots of eigenvalues of \(A^T A\) (or \(A A^T\)).&lt;/item&gt;
      &lt;item&gt;Right singular vectors = eigenvectors of \(A^T A\).&lt;/item&gt;
      &lt;item&gt;Left singular vectors = eigenvectors of \(A A^T\).&lt;/item&gt;
      &lt;item&gt;SVD generalizes eigen-decomposition to all matrices, rectangular or square.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;84. Low-Rank Approximation (Best Small Models)&lt;/head&gt;
    &lt;p&gt;One of the most useful applications of SVD is low-rank approximation: compressing a large matrix into a smaller one while keeping most of the important information.&lt;/p&gt;
    &lt;p&gt;The Eckart–Young theorem says: If \(A = U \Sigma V^T\), then the best rank-\(k\) approximation (in least-squares sense) is:&lt;/p&gt;
    &lt;p&gt;\[ A_k = U_k \Sigma_k V_k^T \]&lt;/p&gt;
    &lt;p&gt;where we keep only the top \(k\) singular values (and corresponding vectors).&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
import matplotlib.pyplot as plt&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Create a matrix with hidden low-rank structure&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;0)
 np.random.seed(= np.random.randn(50, 5)   # 50 x 5
 U = np.random.randn(5, 30)   # 5 x 30
 V = U @ V  # true rank ≤ 5 A &lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Full SVD&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.linalg.svd(A, full_matrices=False)
 U, S, Vt print("Singular values:", S[:10])&lt;/code&gt;
    &lt;code&gt;Singular values: [4.90672194e+01 4.05935057e+01 3.39228766e+01 3.07883338e+01
 2.29261740e+01 3.97150036e-15 3.97150036e-15 3.97150036e-15
 3.97150036e-15 3.97150036e-15]&lt;/code&gt;
    &lt;p&gt;Only the first ~5 should be large; the rest close to zero.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build rank-1 approximation&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= 1
 k = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]
 A1 = np.linalg.norm(A - A1)
 error1 print("Rank-1 approximation error:", error1)&lt;/code&gt;
    &lt;code&gt;Rank-1 approximation error: 65.36149641872869&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Rank-5 approximation (should be almost exact)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= 5
 k = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]
 A5 = np.linalg.norm(A - A5)
 error5 print("Rank-5 approximation error:", error5)&lt;/code&gt;
    &lt;code&gt;Rank-5 approximation error: 5.756573247253659e-14&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visual comparison (image compression demo)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s see it on an image.&lt;/p&gt;
    &lt;code&gt;from sklearn.datasets import load_digits
= load_digits()
 digits = digits.images[0]  # 8x8 grayscale digit
 img 
= np.linalg.svd(img, full_matrices=False)
 U, S, Vt 
# Keep only top 2 singular values
= 2
 k = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]
 img2 
1,2,1)
 plt.subplot(="gray")
 plt.imshow(img, cmap"Original")
 plt.title(
1,2,2)
 plt.subplot(="gray")
 plt.imshow(img2, cmap"Rank-2 Approximation")
 plt.title( plt.show()&lt;/code&gt;
    &lt;p&gt;Even with just 2 singular values, the digit shape is recognizable.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Vary \(k\) in the image example (1, 2, 5, 10). How much detail do you keep?&lt;/item&gt;
      &lt;item&gt;Compare the approximation error \(\|A - A_k\|\) as \(k\) increases.&lt;/item&gt;
      &lt;item&gt;Apply low-rank approximation to random noisy data. Does it denoise?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SVD gives the best possible low-rank approximation in terms of error.&lt;/item&gt;
      &lt;item&gt;By truncating singular values, you compress data while keeping its essential structure.&lt;/item&gt;
      &lt;item&gt;This is the backbone of image compression, recommender systems, and dimensionality reduction.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;85. Principal Component Analysis (Variance and Directions)&lt;/head&gt;
    &lt;p&gt;Principal Component Analysis (PCA) is one of the most important applications of SVD. It finds the directions (principal components) where data varies the most, and projects the data onto them to reduce dimensionality while preserving as much information as possible.&lt;/p&gt;
    &lt;p&gt;Mathematically:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Center the data (subtract the mean).&lt;/item&gt;
      &lt;item&gt;Compute covariance matrix \(C = \frac{1}{n} X^T X\).&lt;/item&gt;
      &lt;item&gt;Eigenvectors of \(C\) = principal directions.&lt;/item&gt;
      &lt;item&gt;Eigenvalues = variance explained.&lt;/item&gt;
      &lt;item&gt;Equivalently: PCA = SVD of centered data matrix.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_digits&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Generate synthetic 2D data&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;0)
 np.random.seed(= np.random.randn(200, 2) @ np.array([[3,1],[1,0.5]])  # stretched cloud
 X 
0], X[:,1], alpha=0.3)
 plt.scatter(X[:,"Original data")
 plt.title("equal")
 plt.axis( plt.show()&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Center the data&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= X - X.mean(axis=0) X_centered &lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute SVD&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.linalg.svd(X_centered, full_matrices=False)
 U, S, Vt print("Principal directions (V):\n", Vt)&lt;/code&gt;
    &lt;code&gt;Principal directions (V):
 [[-0.94430098 -0.32908307]
 [ 0.32908307 -0.94430098]]&lt;/code&gt;
    &lt;p&gt;Rows of &lt;code&gt;Vt&lt;/code&gt; are the principal components.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Project data onto first component&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= X_centered @ Vt.T[:,0]
 X_pca1 
=0.3)
 plt.scatter(X_pca1, np.zeros_like(X_pca1), alpha"Data projected on first principal component")
 plt.title( plt.show()&lt;/code&gt;
    &lt;p&gt;This collapses data into 1D, keeping the most variance.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visualize principal axes&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;0], X_centered[:,1], alpha=0.3)
 plt.scatter(X_centered[:,for length, vector in zip(S, Vt):
0, vector[0]*length], [0, vector[1]*length], 'r-', linewidth=3)
     plt.plot(["Principal components (directions of max variance)")
 plt.title("equal")
 plt.axis( plt.show()&lt;/code&gt;
    &lt;p&gt;The red arrows show where the data spreads most.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;PCA on real data (digits)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= load_digits()
 digits = digits.data  # 1797 samples, 64 features
 X = X - X.mean(axis=0)
 X_centered 
= np.linalg.svd(X_centered, full_matrices=False)
 U, S, Vt 
= (S**2) / np.sum(S**2)
 explained_variance print("Explained variance ratio (first 5):", explained_variance[:5])&lt;/code&gt;
    &lt;code&gt;Explained variance ratio (first 5): [0.14890594 0.13618771 0.11794594 0.08409979 0.05782415]&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Reduce digits dataset to 2D using the top 2 components and plot. Do digit clusters separate?&lt;/item&gt;
      &lt;item&gt;Compare explained variance ratio for top 10 components.&lt;/item&gt;
      &lt;item&gt;Add noise to data and check if PCA filters it out when projecting to fewer dimensions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PCA finds directions of maximum variance using SVD.&lt;/item&gt;
      &lt;item&gt;By projecting onto top components, you compress data with minimal information loss.&lt;/item&gt;
      &lt;item&gt;PCA is the backbone of dimensionality reduction, visualization, and preprocessing in machine learning.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;86. Pseudoinverse (Moore–Penrose) and Solving Ill-Posed Systems&lt;/head&gt;
    &lt;p&gt;The Moore–Penrose pseudoinverse \(A^+\) generalizes the inverse of a matrix. It allows solving systems \(Ax = b\) even when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(A\) is not square, or&lt;/item&gt;
      &lt;item&gt;\(A\) is singular (non-invertible).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The solution given by the pseudoinverse is the least-squares solution with minimum norm:&lt;/p&gt;
    &lt;p&gt;\[ x = A^+ b \]&lt;/p&gt;
    &lt;p&gt;If \(A = U \Sigma V^T\), then:&lt;/p&gt;
    &lt;p&gt;\[ A^+ = V \Sigma^+ U^T \]&lt;/p&gt;
    &lt;p&gt;where \(\Sigma^+\) is obtained by taking reciprocals of nonzero singular values.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Solve an overdetermined system (more equations than unknowns)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[1,1],
 A 1,2],
               [1,3]])  # 3x2 system
               [= np.array([1,2,2])
 b 
*_ = np.linalg.lstsq(A, b, rcond=None)
 x_ls, print("Least-squares solution:", x_ls)&lt;/code&gt;
    &lt;code&gt;Least-squares solution: [0.66666667 0.5       ]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute with pseudoinverse&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.linalg.pinv(A)
 A_pinv = A_pinv @ b
 x_pinv print("Pseudoinverse solution:", x_pinv)&lt;/code&gt;
    &lt;code&gt;Pseudoinverse solution: [0.66666667 0.5       ]&lt;/code&gt;
    &lt;p&gt;Both match → pseudoinverse gives least-squares solution.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Solve an underdetermined system (fewer equations than unknowns)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[1,2,3]])  # 1x3
 A = np.array([1])
 b 
= np.linalg.pinv(A) @ b
 x_pinv print("Minimum norm solution:", x_pinv)&lt;/code&gt;
    &lt;code&gt;Minimum norm solution: [0.07142857 0.14285714 0.21428571]&lt;/code&gt;
    &lt;p&gt;Here, infinitely many solutions exist. The pseudoinverse picks the one with smallest norm.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compare with singular matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[1,2],
 A 2,4]])  # rank deficient
               [= np.array([1,2])
 b 
= np.linalg.pinv(A) @ b
 x_pinv print("Solution with pseudoinverse:", x_pinv)&lt;/code&gt;
    &lt;code&gt;Solution with pseudoinverse: [0.2 0.4]&lt;/code&gt;
    &lt;p&gt;Even when \(A\) is singular, pseudoinverse provides a solution.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Manual pseudoinverse via SVD&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[1,2],
 A 3,4]])
               [= np.linalg.svd(A)
 U, S, Vt = np.zeros((Vt.shape[0], U.shape[0]))
 S_inv for i in range(len(S)):
if S[i] &amp;gt; 1e-10:
     = 1/S[i]
         S_inv[i,i] 
= Vt.T @ S_inv @ U.T
 A_pinv_manual print("Manual pseudoinverse:\n", A_pinv_manual)
print("NumPy pseudoinverse:\n", np.linalg.pinv(A))&lt;/code&gt;
    &lt;code&gt;Manual pseudoinverse:
 [[-2.   1. ]
 [ 1.5 -0.5]]
NumPy pseudoinverse:
 [[-2.   1. ]
 [ 1.5 -0.5]]&lt;/code&gt;
    &lt;p&gt;They match.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Create an overdetermined system with noise and see how pseudoinverse smooths the solution.&lt;/item&gt;
      &lt;item&gt;Compare pseudoinverse with direct inverse (&lt;code&gt;np.linalg.inv&lt;/code&gt;) on a square nonsingular matrix.&lt;/item&gt;
      &lt;item&gt;Zero out small singular values manually and see how solution changes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The pseudoinverse solves any linear system, square or not.&lt;/item&gt;
      &lt;item&gt;It provides the least-squares solution in overdetermined cases and the minimum-norm solution in underdetermined cases.&lt;/item&gt;
      &lt;item&gt;Built on SVD, it is a cornerstone of regression, optimization, and numerical methods.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;87. Conditioning and Sensitivity (How Errors Amplify)&lt;/head&gt;
    &lt;p&gt;Conditioning tells us how sensitive a system is to small changes. For a linear system \(Ax = b\):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If \(A\) is well-conditioned, small changes in \(b\) or \(A\) → small changes in \(x\).&lt;/item&gt;
      &lt;item&gt;If \(A\) is ill-conditioned, tiny changes can cause huge swings in \(x\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The condition number is defined as:&lt;/p&gt;
    &lt;p&gt;\[ \kappa(A) = \|A\| \cdot \|A^{-1}\| \]&lt;/p&gt;
    &lt;p&gt;For SVD:&lt;/p&gt;
    &lt;p&gt;\[ \kappa(A) = \frac{\sigma_{\max}}{\sigma_{\min}} \]&lt;/p&gt;
    &lt;p&gt;where \(\sigma_{\max}\) and \(\sigma_{\min}\) are the largest and smallest singular values.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Large \(\kappa(A)\) → unstable system.&lt;/item&gt;
      &lt;item&gt;Small \(\kappa(A)\) → stable system.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Well-conditioned system&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[2,0],
 A 0,1]])
               [= np.array([1,1])
 b 
= np.linalg.solve(A, b)
 x = np.linalg.cond(A)
 cond print("Solution:", x)
print("Condition number:", cond)&lt;/code&gt;
    &lt;code&gt;Solution: [0.5 1. ]
Condition number: 2.0&lt;/code&gt;
    &lt;p&gt;Condition number = ratio of singular values → moderate size.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Ill-conditioned system&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[1, 1.0001],
 A 1, 1.0000]])
               [= np.array([2,2])
 b 
= np.linalg.lstsq(A, b, rcond=None)[0]
 x = np.linalg.cond(A)
 cond print("Solution:", x)
print("Condition number:", cond)&lt;/code&gt;
    &lt;code&gt;Solution: [ 2.00000000e+00 -5.73526099e-13]
Condition number: 40002.000075017124&lt;/code&gt;
    &lt;p&gt;Condition number is very large → instability.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Perturb the right-hand side&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([2, 2.001])  # tiny change
 b2 = np.linalg.lstsq(A, b2, rcond=None)[0]
 x2 print("Solution after tiny change:", x2)&lt;/code&gt;
    &lt;code&gt;Solution after tiny change: [ 12.001 -10.   ]&lt;/code&gt;
    &lt;p&gt;The solution changes drastically → shows sensitivity.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Relation to singular values&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.linalg.svd(A)
 U, S, Vt print("Singular values:", S)
print("Condition number (SVD):", S[0]/S[-1])&lt;/code&gt;
    &lt;code&gt;Singular values: [2.000050e+00 4.999875e-05]
Condition number (SVD): 40002.00007501713&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Scaling experiment&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;for scale in [1,1e-2,1e-4,1e-6]:
= np.array([[1,0],[0,scale]])
     A print(f"Scale={scale}, condition number={np.linalg.cond(A)}")     &lt;/code&gt;
    &lt;code&gt;Scale=1, condition number=1.0
Scale=0.01, condition number=100.0
Scale=0.0001, condition number=10000.0
Scale=1e-06, condition number=1000000.0&lt;/code&gt;
    &lt;p&gt;As scale shrinks, condition number explodes.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Generate random matrices and compute their condition numbers. Which are stable?&lt;/item&gt;
      &lt;item&gt;Compare condition numbers of Hilbert matrices (notoriously ill-conditioned).&lt;/item&gt;
      &lt;item&gt;Explore how rounding errors grow with high condition numbers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Condition number = measure of problem sensitivity.&lt;/item&gt;
      &lt;item&gt;\(\kappa(A) = \sigma_{\max}/\sigma_{\min}\).&lt;/item&gt;
      &lt;item&gt;Ill-conditioned problems amplify errors and are numerically unstable → why scaling, regularization, and good formulations matter.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;88. Matrix Norms and Singular Values (Measuring Size Properly)&lt;/head&gt;
    &lt;p&gt;Matrix norms measure the size or strength of a matrix. They extend the idea of vector length to matrices. Norms are crucial for analyzing stability, error growth, and performance of algorithms.&lt;/p&gt;
    &lt;p&gt;Some important norms:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frobenius norm:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ \|A\|_F = \sqrt{\sum_{i,j} |a_{ij}|^2} \]&lt;/p&gt;
    &lt;p&gt;Equivalent to treating the matrix as a big vector.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Spectral norm (operator 2-norm):&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ \|A\|_2 = \sigma_{\max} \]&lt;/p&gt;
    &lt;p&gt;The largest singular value - tells how much \(A\) can stretch a vector.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1-norm: maximum absolute column sum.&lt;/item&gt;
      &lt;item&gt;∞-norm: maximum absolute row sum.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build a test matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[1, -2, 3],
 A 0,  4, 5],
               [-1, 2, 1]])               [&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute different norms&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.linalg.norm(A, 'fro')
 fro = np.linalg.norm(A, 2)
 spec = np.linalg.norm(A, 1)
 one_norm = np.linalg.norm(A, np.inf)
 inf_norm 
print("Frobenius norm:", fro)
print("Spectral norm:", spec)
print("1-norm:", one_norm)
print("Infinity norm:", inf_norm)&lt;/code&gt;
    &lt;code&gt;Frobenius norm: 7.810249675906654
Spectral norm: 6.813953458914004
1-norm: 9.0
Infinity norm: 9.0&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compare spectral norm with largest singular value&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.linalg.svd(A)
 U, S, Vt print("Largest singular value:", S[0])
print("Spectral norm:", spec)&lt;/code&gt;
    &lt;code&gt;Largest singular value: 6.813953458914004
Spectral norm: 6.813953458914004&lt;/code&gt;
    &lt;p&gt;They match → spectral norm = largest singular value.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Frobenius norm from singular values&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ \|A\|_F = \sqrt{\sigma_1^2 + \sigma_2^2 + \dots} \]&lt;/p&gt;
    &lt;code&gt;= np.sqrt(np.sum(S**2))
 fro_from_svd print("Frobenius norm (from SVD):", fro_from_svd)&lt;/code&gt;
    &lt;code&gt;Frobenius norm (from SVD): 7.810249675906654&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Stretching effect demonstration&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Pick a random vector and see how much it grows:&lt;/p&gt;
    &lt;code&gt;= np.random.randn(3)
 x = np.linalg.norm(A @ x) / np.linalg.norm(x)
 stretch print("Stretch factor:", stretch)
print("Spectral norm (max possible stretch):", spec)&lt;/code&gt;
    &lt;code&gt;Stretch factor: 2.7537463268177698
Spectral norm (max possible stretch): 6.813953458914004&lt;/code&gt;
    &lt;p&gt;The stretch ≤ spectral norm, always.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compare norms for diagonal matrices - do they match the largest diagonal entry?&lt;/item&gt;
      &lt;item&gt;Generate random matrices and see how norms differ.&lt;/item&gt;
      &lt;item&gt;Compute Frobenius vs spectral norm for a rank-1 matrix.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frobenius norm = overall energy of the matrix.&lt;/item&gt;
      &lt;item&gt;Spectral norm = maximum stretching power (largest singular value).&lt;/item&gt;
      &lt;item&gt;Other norms (1-norm, ∞-norm) capture row/column dominance.&lt;/item&gt;
      &lt;item&gt;Singular values unify all these views of “matrix size.”&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;89. Regularization (Ridge/Tikhonov to Tame Instability)&lt;/head&gt;
    &lt;p&gt;When solving \(Ax = b\), if \(A\) is ill-conditioned (large condition number), small errors in data can cause huge errors in the solution. Regularization stabilizes the problem by adding a penalty term that discourages extreme solutions.&lt;/p&gt;
    &lt;p&gt;The most common form: ridge regression (a.k.a. Tikhonov regularization):&lt;/p&gt;
    &lt;p&gt;\[ x_\lambda = \arg\min_x \|Ax - b\|^2 + \lambda \|x\|^2 \]&lt;/p&gt;
    &lt;p&gt;Closed form:&lt;/p&gt;
    &lt;p&gt;\[ x_\lambda = (A^T A + \lambda I)^{-1} A^T b \]&lt;/p&gt;
    &lt;p&gt;Here \(\lambda &amp;gt; 0\) controls the amount of regularization:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Small \(\lambda\): solution close to least-squares.&lt;/item&gt;
      &lt;item&gt;Large \(\lambda\): smaller coefficients, more stability.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
import matplotlib.pyplot as plt&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build an ill-conditioned system&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[1, 1.001],
 A 1, 0.999]])
               [= np.array([2, 2]) b &lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Solve without regularization&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;*_ = np.linalg.lstsq(A, b, rcond=None)
 x_ls, print("Least squares solution:", x_ls)&lt;/code&gt;
    &lt;code&gt;Least squares solution: [ 2.00000000e+00 -2.84186735e-14]&lt;/code&gt;
    &lt;p&gt;The result may be unstable.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Apply ridge regularization&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= 0.1
 lam = np.linalg.inv(A.T @ A + lam*np.eye(2)) @ A.T @ b
 x_ridge print("Ridge solution (λ=0.1):", x_ridge)&lt;/code&gt;
    &lt;code&gt;Ridge solution (λ=0.1): [0.97561927 0.97559976]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compare effect of different λ&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.logspace(-4, 2, 20)
 lambdas = []
 solutions for lam in lambdas:
= np.linalg.inv(A.T @ A + lam*np.eye(2)) @ A.T @ b
     x_reg 
     solutions.append(np.linalg.norm(x_reg))
'o-')
 plt.semilogx(lambdas, solutions, "λ (regularization strength)")
 plt.xlabel("Solution norm")
 plt.ylabel("Effect of ridge regularization")
 plt.title( plt.show()&lt;/code&gt;
    &lt;p&gt;As \(\lambda\) increases, the solution becomes smaller and more stable.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Connection to SVD&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If \(A = U \Sigma V^T\):&lt;/p&gt;
    &lt;p&gt;\[ x_\lambda = \sum_i \frac{\sigma_i}{\sigma_i^2 + \lambda} (u_i^T b) v_i \]&lt;/p&gt;
    &lt;p&gt;Small singular values (causing instability) get damped by \(\frac{\sigma_i}{\sigma_i^2 + \lambda}\).&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Experiment with larger and smaller \(\lambda\). What happens to the solution?&lt;/item&gt;
      &lt;item&gt;Add random noise to \(b\). Compare least-squares vs ridge stability.&lt;/item&gt;
      &lt;item&gt;Plot how each coefficient changes with λ.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Regularization controls instability in ill-conditioned problems.&lt;/item&gt;
      &lt;item&gt;Ridge regression balances fit vs. stability using λ.&lt;/item&gt;
      &lt;item&gt;In SVD terms, regularization damps small singular values that cause wild solutions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;90. Rank-Revealing QR and Practical Diagnostics (What Rank Really Is)&lt;/head&gt;
    &lt;p&gt;In practice, we often need to determine the numerical rank of a matrix - not just the theoretical rank, but how many directions carry meaningful information beyond round-off errors or noise. A useful tool for this is the Rank-Revealing QR (RRQR) factorization.&lt;/p&gt;
    &lt;p&gt;For a matrix \(A\):&lt;/p&gt;
    &lt;p&gt;\[ A P = Q R \]&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(Q\): orthogonal matrix&lt;/item&gt;
      &lt;item&gt;\(R\): upper triangular matrix&lt;/item&gt;
      &lt;item&gt;\(P\): column permutation matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By reordering columns smartly, the diagonal of \(R\) reveals which directions are significant.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
from scipy.linalg import qr&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build a nearly rank-deficient matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[1, 2, 3],
 A 2, 4.001, 6],
               [3, 6, 9.001]])
               [print("Rank (theoretical):", np.linalg.matrix_rank(A))&lt;/code&gt;
    &lt;code&gt;Rank (theoretical): 3&lt;/code&gt;
    &lt;p&gt;This matrix is almost rank 2 but with small perturbations.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;QR with column pivoting&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= qr(A, pivoting=True)
 Q, R, P print("R:\n", R)
print("Column permutation:", P)&lt;/code&gt;
    &lt;code&gt;R:
 [[-1.12257740e+01 -7.48384925e+00 -3.74165738e+00]
 [ 0.00000000e+00 -1.20185042e-03 -1.84886859e-04]
 [ 0.00000000e+00  0.00000000e+00 -7.41196374e-05]]
Column permutation: [2 1 0]&lt;/code&gt;
    &lt;p&gt;The diagonal entries of \(R\) decrease rapidly → numerical rank is determined where they become tiny.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compare with SVD&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.linalg.svd(A)
 U, S, Vt print("Singular values:", S)&lt;/code&gt;
    &lt;code&gt;Singular values: [1.40009286e+01 1.00000000e-03 7.14238341e-05]&lt;/code&gt;
    &lt;p&gt;The singular values tell the same story: one is very small → effective rank ≈ 2.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Thresholding for rank&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= 1e-3
 tol = np.sum(S &amp;gt; tol)
 rank_est print("Estimated rank:", rank_est)&lt;/code&gt;
    &lt;code&gt;Estimated rank: 2&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Diagnostics on a noisy matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;0)
 np.random.seed(= np.random.randn(50, 10) @ np.random.randn(10, 10)  # rank ≤ 10
 B -1] += 1e-6 * np.random.randn(50)  # tiny noise
 B[:, 
= np.linalg.svd(B)
 U, S, Vt 'o-')
 plt.semilogy(S, "Singular values (log scale)")
 plt.title("Index")
 plt.xlabel("Value")
 plt.ylabel( plt.show()&lt;/code&gt;
    &lt;p&gt;The drop in singular values shows effective rank.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Change the perturbation in \(A\) from 0.001 to 0.000001. Does the numerical rank change?&lt;/item&gt;
      &lt;item&gt;Test QR with pivoting on random rectangular matrices.&lt;/item&gt;
      &lt;item&gt;Compare rank estimates from QR vs SVD for large noisy matrices.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rank-revealing QR is a practical tool to detect effective rank in real-world data.&lt;/item&gt;
      &lt;item&gt;SVD gives the most precise picture (singular values), but QR with pivoting is faster.&lt;/item&gt;
      &lt;item&gt;Understanding numerical rank is crucial for diagnostics, stability, and model complexity control.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Chapter 10. Applications and computation&lt;/head&gt;
    &lt;head rend="h3"&gt;91. 2D/3D Geometry Pipelines (Cameras, Rotations, and Transforms)&lt;/head&gt;
    &lt;p&gt;Linear algebra powers the geometry pipelines in computer graphics and robotics.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2D transforms: rotation, scaling, translation.&lt;/item&gt;
      &lt;item&gt;3D transforms: same ideas, but with an extra dimension.&lt;/item&gt;
      &lt;item&gt;Homogeneous coordinates let us unify all transforms (even translations) into matrix multiplications.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
import matplotlib.pyplot as plt&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Rotation in 2D&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ R(\theta) = \begin{bmatrix} \cos\theta &amp;amp; -\sin\theta \\ \sin\theta &amp;amp; \cos\theta \end{bmatrix} \]&lt;/p&gt;
    &lt;code&gt;= np.pi/4  # 45 degrees
 theta = np.array([[np.cos(theta), -np.sin(theta)],
 R 
               [np.sin(theta),  np.cos(theta)]])
= np.array([1, 0])
 point = R @ point
 rotated 
print("Original:", point)
print("Rotated:", rotated)&lt;/code&gt;
    &lt;code&gt;Original: [1 0]
Rotated: [0.70710678 0.70710678]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Translation using homogeneous coordinates&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In 2D:&lt;/p&gt;
    &lt;p&gt;\[ T(dx, dy) = \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; dx \\ 0 &amp;amp; 1 &amp;amp; dy \\ 0 &amp;amp; 0 &amp;amp; 1 \end{bmatrix} \]&lt;/p&gt;
    &lt;code&gt;= np.array([[1,0,2],
 T 0,1,1],
               [0,0,1]])
               [
= np.array([1,1,1])  # homogeneous (x=1,y=1)
 p_h = T @ p_h
 translated print("Translated point:", translated)&lt;/code&gt;
    &lt;code&gt;Translated point: [3 2 1]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Combine rotation + translation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Transformations compose by multiplying matrices.&lt;/p&gt;
    &lt;code&gt;= T @ np.block([[R, np.zeros((2,1))],
 M 1,2)), 1]])
                   [np.zeros((= M @ p_h
 combined print("Combined transform (rotation+translation):", combined)&lt;/code&gt;
    &lt;code&gt;Combined transform (rotation+translation): [2.         2.41421356 1.        ]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;3D rotation (around z-axis)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ R_z(\theta) = \begin{bmatrix} \cos\theta &amp;amp; -\sin\theta &amp;amp; 0 \\ \sin\theta &amp;amp; \cos\theta &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; 1 \end{bmatrix} \]&lt;/p&gt;
    &lt;code&gt;= np.pi/3
 theta = np.array([[np.cos(theta), -np.sin(theta), 0],
 Rz 0],
                [np.sin(theta),  np.cos(theta), 0,              0,             1]])
                [
= np.array([1,0,0])
 point3d = Rz @ point3d
 rotated3d print("3D rotated point:", rotated3d)&lt;/code&gt;
    &lt;code&gt;3D rotated point: [0.5       0.8660254 0.       ]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Camera projection (3D → 2D)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Simple pinhole model:&lt;/p&gt;
    &lt;p&gt;\[ \begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix} f \cdot x / z \\ f \cdot y / z \end{bmatrix} \]&lt;/p&gt;
    &lt;code&gt;= 1.0  # focal length
 f = np.array([[f,0,0],
 P 0,f,0],
               [0,0,1]])  # projection matrix
               [
= np.array([2,3,5])
 point3d = P @ point3d
 p_proj = p_proj[:2] / p_proj[2]  # divide by z
 p_proj print("Projected 2D point:", p_proj)&lt;/code&gt;
    &lt;code&gt;Projected 2D point: [0.4 0.6]&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Rotate a square in 2D, then translate it. Plot before/after.&lt;/item&gt;
      &lt;item&gt;Rotate a 3D point cloud around x, y, and z axes.&lt;/item&gt;
      &lt;item&gt;Project a cube into 2D using the pinhole camera model.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Geometry pipelines = sequences of linear transforms.&lt;/item&gt;
      &lt;item&gt;Homogeneous coordinates unify rotation, scaling, and translation.&lt;/item&gt;
      &lt;item&gt;Camera projection links 3D world to 2D images - a cornerstone of graphics and vision.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;92. Computer Graphics and Robotics (Homogeneous Tricks in Action)&lt;/head&gt;
    &lt;p&gt;Computer graphics and robotics both rely on homogeneous coordinates to unify rotations, translations, scalings, and projections into a single framework. With \(4 \times 4\) matrices in 3D, entire transformation pipelines can be built as matrix products.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
import matplotlib.pyplot as plt&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Homogeneous representation of a point&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In 3D:&lt;/p&gt;
    &lt;p&gt;\[ (x, y, z) \mapsto (x, y, z, 1) \]&lt;/p&gt;
    &lt;code&gt;= np.array([1,2,3,1])  # homogeneous point p &lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Define translation, rotation, and scaling matrices&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Translation by \((dx,dy,dz)\):&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[1,0,0,2],
 T 0,1,0,1],
               [0,0,1,3],
               [0,0,0,1]])               [&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Scaling by factors \((sx, sy, sz)\):&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.diag([2, 0.5, 1.5, 1]) S &lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rotation about z-axis (\(\theta = 90^\circ\)):&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.pi/2
 theta = np.array([[np.cos(theta), -np.sin(theta), 0, 0],
 Rz 0, 0],
                [np.sin(theta),  np.cos(theta), 0,              0,             1, 0],
                [0,              0,             0, 1]])                [&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Combine transforms into a pipeline&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= T @ Rz @ S  # first scale, then rotate, then translate
 M = M @ p
 p_transformed print("Transformed point:", p_transformed)&lt;/code&gt;
    &lt;code&gt;Transformed point: [1.  3.  7.5 1. ]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Robotics: forward kinematics of a 2-link arm&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each joint is a rotation + translation.&lt;/p&gt;
    &lt;code&gt;def link(theta, length):
return np.array([[np.cos(theta), -np.sin(theta), 0, length*np.cos(theta)],
     0, length*np.sin(theta)],
                      [np.sin(theta),  np.cos(theta), 0,              0,             1, 0],
                      [0,              0,             0, 1]])
                      [
= np.pi/4, np.pi/6
 theta1, theta2 = 2, 1.5
 L1, L2 
= link(theta1, L1)
 M1 = link(theta2, L2)
 M2 
= M1 @ M2 @ np.array([0,0,0,1])
 end_effector print("End effector position:", end_effector[:3])&lt;/code&gt;
    &lt;code&gt;End effector position: [1.80244213 2.8631023  0.        ]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Graphics: simple 3D camera projection&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= 2.0
 f = np.array([[f,0,0,0],
 P 0,f,0,0],
               [0,0,1,0]])
               [
= np.array([[x,y,z,1] for x in [0,1] for y in [0,1] for z in [0,1]])
 cube = (P @ cube.T).T
 proj = proj[:,:2] / proj[:,2:3]
 proj2d 
0], proj2d[:,1])
 plt.scatter(proj2d[:,"Projected cube")
 plt.title( plt.show()&lt;/code&gt;
    &lt;code&gt;/var/folders/_g/lq_pglm508df70x751kkxrl80000gp/T/ipykernel_31637/2038614107.py:8: RuntimeWarning: divide by zero encountered in divide
  proj2d = proj[:,:2] / proj[:,2:3]
/var/folders/_g/lq_pglm508df70x751kkxrl80000gp/T/ipykernel_31637/2038614107.py:8: RuntimeWarning: invalid value encountered in divide
  proj2d = proj[:,:2] / proj[:,2:3]&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Change order of transforms (&lt;code&gt;Rz @ S @ T&lt;/code&gt;). How does the result differ?&lt;/item&gt;
      &lt;item&gt;Add a third joint to the robotic arm and compute new end-effector position.&lt;/item&gt;
      &lt;item&gt;Project the cube with different focal lengths \(f\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Homogeneous coordinates unify all transformations.&lt;/item&gt;
      &lt;item&gt;Robotics uses this framework for forward kinematics.&lt;/item&gt;
      &lt;item&gt;Graphics uses it for camera and projection pipelines.&lt;/item&gt;
      &lt;item&gt;Both fields rely on the same linear algebra tricks - just applied differently.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;93. Graphs, Adjacency, and Laplacians (Networks via Matrices)&lt;/head&gt;
    &lt;p&gt;Graphs can be studied with linear algebra by encoding them into matrices. Two of the most important:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Adjacency matrix \(A\):&lt;/p&gt;
        &lt;p&gt;\[ A_{ij} = \begin{cases} 1 &amp;amp; \text{if edge between i and j exists} \\ 0 &amp;amp; \text{otherwise} \end{cases} \]&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Graph Laplacian \(L\):&lt;/p&gt;
        &lt;p&gt;\[ L = D - A \]&lt;/p&gt;
        &lt;p&gt;where \(D\) is the degree matrix ($D_{ii} = $ number of neighbors of node \(i\)).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These matrices let us analyze connectivity, diffusion, and clustering.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
import networkx as nx
import matplotlib.pyplot as plt&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build a simple graph&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= nx.Graph()
 G 0,1), (1,2), (2,3), (3,0), (0,2)])  # square with diagonal
 G.add_edges_from([(
=True, node_color="lightblue", node_size=800)
 nx.draw(G, with_labels plt.show()&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Adjacency matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= nx.to_numpy_array(G)
 A print("Adjacency matrix:\n", A)&lt;/code&gt;
    &lt;code&gt;Adjacency matrix:
 [[0. 1. 1. 1.]
 [1. 0. 1. 0.]
 [1. 1. 0. 1.]
 [1. 0. 1. 0.]]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Degree and Laplacian matrices&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.diag(A.sum(axis=1))
 D = D - A
 L print("Degree matrix:\n", D)
print("Graph Laplacian:\n", L)&lt;/code&gt;
    &lt;code&gt;Degree matrix:
 [[3. 0. 0. 0.]
 [0. 2. 0. 0.]
 [0. 0. 3. 0.]
 [0. 0. 0. 2.]]
Graph Laplacian:
 [[ 3. -1. -1. -1.]
 [-1.  2. -1.  0.]
 [-1. -1.  3. -1.]
 [-1.  0. -1.  2.]]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Eigenvalues of Laplacian (connectivity check)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.linalg.eigh(L)
 eigvals, eigvecs print("Laplacian eigenvalues:", eigvals)&lt;/code&gt;
    &lt;code&gt;Laplacian eigenvalues: [1.11022302e-16 2.00000000e+00 4.00000000e+00 4.00000000e+00]&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The number of zero eigenvalues = number of connected components.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Spectral embedding (clustering)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Use Laplacian eigenvectors to embed nodes in low dimensions.&lt;/p&gt;
    &lt;code&gt;= eigvecs[:,1:3]  # skip the trivial first eigenvector
 coords 0], coords[:,1], c=range(len(coords)), cmap="tab10", s=200)
 plt.scatter(coords[:,for i, (x,y) in enumerate(coords):
str(i), fontsize=12, ha="center", va="center", color="white")
     plt.text(x, y, "Spectral embedding of graph")
 plt.title( plt.show()&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Remove one edge from the graph and see how Laplacian eigenvalues change.&lt;/item&gt;
      &lt;item&gt;Add a disconnected node - does an extra zero eigenvalue appear?&lt;/item&gt;
      &lt;item&gt;Try a random graph and compare adjacency vs Laplacian spectra.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Adjacency matrices describe direct graph structure.&lt;/item&gt;
      &lt;item&gt;Laplacians capture connectivity and diffusion.&lt;/item&gt;
      &lt;item&gt;Eigenvalues of \(L\) reveal graph properties like connectedness and clustering - bridging networks with linear algebra.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;94. Data Preprocessing as Linear Ops (Centering, Whitening, Scaling)&lt;/head&gt;
    &lt;p&gt;Many machine learning and data analysis workflows begin with preprocessing, and linear algebra provides the tools.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Centering: subtract the mean → move data to origin.&lt;/item&gt;
      &lt;item&gt;Scaling: divide by standard deviation → normalize feature ranges.&lt;/item&gt;
      &lt;item&gt;Whitening: decorrelate features → make covariance matrix the identity.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each step can be written as a matrix operation.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
import matplotlib.pyplot as plt&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Generate correlated data&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;0)
 np.random.seed(= np.random.randn(200, 2) @ np.array([[3,1],[1,0.5]])
 X 0], X[:,1], alpha=0.4)
 plt.scatter(X[:,"Original correlated data")
 plt.title("equal")
 plt.axis( plt.show()&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Centering (subtract mean)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= X - X.mean(axis=0)
 X_centered print("Mean after centering:", X_centered.mean(axis=0))&lt;/code&gt;
    &lt;code&gt;Mean after centering: [ 8.88178420e-18 -1.22124533e-17]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Scaling (normalize features)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= X_centered / X_centered.std(axis=0)
 X_scaled print("Std after scaling:", X_scaled.std(axis=0))&lt;/code&gt;
    &lt;code&gt;Std after scaling: [1. 1.]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Whitening via eigen-decomposition&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Covariance of centered data:&lt;/p&gt;
    &lt;code&gt;= np.cov(X_centered.T)
 C = np.linalg.eigh(C)
 eigvals, eigvecs 
= eigvecs @ np.diag(1/np.sqrt(eigvals)) @ eigvecs.T
 W = X_centered @ W X_white &lt;/code&gt;
    &lt;p&gt;Check covariance:&lt;/p&gt;
    &lt;code&gt;print("Whitened covariance:\n", np.cov(X_white.T))&lt;/code&gt;
    &lt;code&gt;Whitened covariance:
 [[1.00000000e+00 2.54402864e-15]
 [2.54402864e-15 1.00000000e+00]]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compare scatter plots&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;1,3,1)
 plt.subplot(0], X[:,1], alpha=0.4)
 plt.scatter(X[:,"Original")
 plt.title(
1,3,2)
 plt.subplot(0], X_scaled[:,1], alpha=0.4)
 plt.scatter(X_scaled[:,"Scaled")
 plt.title(
1,3,3)
 plt.subplot(0], X_white[:,1], alpha=0.4)
 plt.scatter(X_white[:,"Whitened")
 plt.title(

 plt.tight_layout() plt.show()&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Original: elongated ellipse.&lt;/item&gt;
      &lt;item&gt;Scaled: axis-aligned ellipse.&lt;/item&gt;
      &lt;item&gt;Whitened: circular cloud (uncorrelated, unit variance).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Add a third feature and apply centering, scaling, whitening.&lt;/item&gt;
      &lt;item&gt;Compare whitening with PCA - they use the same eigen-decomposition.&lt;/item&gt;
      &lt;item&gt;Test what happens if you skip centering before whitening.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Centering → mean zero.&lt;/item&gt;
      &lt;item&gt;Scaling → unit variance.&lt;/item&gt;
      &lt;item&gt;Whitening → features uncorrelated, variance = 1. Linear algebra provides the exact matrix operations to make preprocessing systematic and reliable.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;95. Linear Regression and Classification (From Model to Matrix)&lt;/head&gt;
    &lt;p&gt;Linear regression and classification problems can be written neatly in matrix form. This unifies data, models, and solutions under the framework of least squares and linear decision boundaries.&lt;/p&gt;
    &lt;head rend="h4"&gt;Linear Regression Model&lt;/head&gt;
    &lt;p&gt;For data \((x_i, y_i)\):&lt;/p&gt;
    &lt;p&gt;\[ y \approx X \beta \]&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(X\): design matrix (rows = samples, columns = features).&lt;/item&gt;
      &lt;item&gt;\(\beta\): coefficients to solve for.&lt;/item&gt;
      &lt;item&gt;Solution (least squares):&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;\[ \hat{\beta} = (X^T X)^{-1} X^T y \]&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Linear regression example&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;0)
 np.random.seed(= np.linspace(0, 10, 30).reshape(-1,1)
 X = 3*X.squeeze() + 5 + np.random.randn(30)*2 y &lt;/code&gt;
    &lt;p&gt;Construct design matrix with bias term:&lt;/p&gt;
    &lt;code&gt;= np.column_stack([np.ones_like(X), X])
 X_design *_ = np.linalg.lstsq(X_design, y, rcond=None)
 beta_hat, print("Fitted coefficients:", beta_hat)&lt;/code&gt;
    &lt;code&gt;Fitted coefficients: [6.65833151 2.84547628]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visualize regression line&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= X_design @ beta_hat
 y_pred 
="Data")
 plt.scatter(X, y, label'r-', label="Fitted line")
 plt.plot(X, y_pred, 
 plt.legend() plt.show()&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Logistic classification with linear decision boundary&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= make_classification(n_features=2, n_redundant=0, n_informative=2,
 Xc, yc =1, n_samples=100, random_state=0)
                              n_clusters_per_class
0], Xc[:,1], c=yc, cmap="bwr", alpha=0.7)
 plt.scatter(Xc[:,"Classification data")
 plt.title( plt.show()&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Logistic regression via gradient descent&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;def sigmoid(z):
return 1/(1+np.exp(-z))
     
= np.column_stack([np.ones(len(Xc)), Xc])
 X_design = yc
 y 
= np.zeros(X_design.shape[1])
 w = 0.1
 lr 
for _ in range(2000):
= sigmoid(X_design @ w)
     preds = X_design.T @ (preds - y) / len(y)
     grad -= lr * grad
     w 
print("Learned weights:", w)&lt;/code&gt;
    &lt;code&gt;Learned weights: [-2.10451116  0.70752542  4.13295129]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Plot decision boundary&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.meshgrid(np.linspace(Xc[:,0].min()-1, Xc[:,0].max()+1, 200),
 xx, yy 1].min()-1, Xc[:,1].max()+1, 200))
                      np.linspace(Xc[:,
= np.c_[np.ones(xx.size), xx.ravel(), yy.ravel()]
 grid = sigmoid(grid @ w).reshape(xx.shape)
 probs 
=[0,0.5,1], alpha=0.3, cmap="bwr")
 plt.contourf(xx, yy, probs, levels0], Xc[:,1], c=yc, cmap="bwr", edgecolor="k")
 plt.scatter(Xc[:,"Linear decision boundary")
 plt.title( plt.show()&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Add polynomial features to regression and refit. Does the line bend into a curve?&lt;/item&gt;
      &lt;item&gt;Change learning rate in logistic regression - what happens?&lt;/item&gt;
      &lt;item&gt;Generate data that is not linearly separable. Can a linear model still classify well?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Regression and classification fit naturally into linear algebra with matrix formulations.&lt;/item&gt;
      &lt;item&gt;Least squares solves regression directly; logistic regression requires optimization.&lt;/item&gt;
      &lt;item&gt;Linear models are simple, interpretable, and still form the foundation of modern machine learning.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;96. PCA in Practice (Dimensionality Reduction Workflow)&lt;/head&gt;
    &lt;p&gt;Principal Component Analysis (PCA) is widely used to reduce dimensions, compress data, and visualize high-dimensional datasets. Here, we’ll walk through a full PCA workflow: centering, computing components, projecting, and visualizing.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_digits&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Load dataset (digits)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= load_digits()
 digits = digits.data  # shape (1797, 64)
 X = digits.target
 y print("Data shape:", X.shape)&lt;/code&gt;
    &lt;code&gt;Data shape: (1797, 64)&lt;/code&gt;
    &lt;p&gt;Each sample is an 8×8 grayscale image flattened into 64 features.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Center the data&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= X - X.mean(axis=0) X_centered &lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute PCA via SVD&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.linalg.svd(X_centered, full_matrices=False)
 U, S, Vt = (S**2) / (len(X) - 1)
 explained_variance = explained_variance / explained_variance.sum() explained_ratio &lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Plot explained variance ratio&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;30]), 'o-')
 plt.plot(np.cumsum(explained_ratio[:"Number of components")
 plt.xlabel("Cumulative explained variance")
 plt.ylabel("PCA explained variance")
 plt.title(True)
 plt.grid( plt.show()&lt;/code&gt;
    &lt;p&gt;This shows how many components are needed to capture most variance.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Project onto top 2 components for visualization&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= X_centered @ Vt[:2].T
 X_pca2 0], X_pca2[:,1], c=y, cmap="tab10", alpha=0.6, s=15)
 plt.scatter(X_pca2[:,
 plt.colorbar()"Digits dataset (PCA 2D projection)")
 plt.title( plt.show()&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Reconstruct images from reduced dimensions&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= 20
 k = X_centered @ Vt[:k].T
 X_pca20 = X_pca20 @ Vt[:k]
 X_reconstructed 
= plt.subplots(2, 10, figsize=(10,2))
 fig, axes for i in range(10):
0,i].imshow(X[i].reshape(8,8), cmap="gray")
     axes[0,i].axis("off")
     axes[1,i].imshow(X_reconstructed[i].reshape(8,8), cmap="gray")
     axes[1,i].axis("off")
     axes["Original (top) vs PCA reconstruction (bottom, 20 comps)")
 plt.suptitle( plt.show()&lt;/code&gt;
    &lt;p&gt;Even with only 20/64 components, the digits remain recognizable.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Change \(k\) to 5, 10, 30 - how do reconstructions change?&lt;/item&gt;
      &lt;item&gt;Use top 2 PCA components to classify digits with k-NN. How does accuracy compare to full 64 features?&lt;/item&gt;
      &lt;item&gt;Try PCA on your own dataset (images, tabular data).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PCA reduces dimensions while keeping maximum variance.&lt;/item&gt;
      &lt;item&gt;In practice: center → decompose → select top components → project/reconstruct.&lt;/item&gt;
      &lt;item&gt;PCA enables visualization, compression, and denoising in real-world workflows.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;97. Recommender Systems and Low-Rank Models (Fill the Missing Entries)&lt;/head&gt;
    &lt;p&gt;Recommender systems often deal with incomplete matrices - rows are users, columns are items, entries are ratings. Most entries are missing, but the matrix is usually close to low-rank (because user preferences depend on only a few hidden factors). SVD and low-rank approximations are powerful tools to fill in these missing values.&lt;/p&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
import matplotlib.pyplot as plt&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Simulate a user–item rating matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;0)
 np.random.seed(= np.random.randn(10, 3)   # 10 users, 3 latent features
 true_users = np.random.randn(3, 8)    # 8 items
 true_items = true_users @ true_items      # true low-rank ratings R_full &lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Hide some ratings (simulate missing data)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.random.rand(*R_full.shape) &amp;gt; 0.3  # keep 70% of entries
 mask = np.where(mask, R_full, np.nan)
 R_obs 
print("Observed ratings:\n", R_obs)&lt;/code&gt;
    &lt;code&gt;Observed ratings:
 [[-1.10781465         nan -3.56526968         nan -2.1729387   1.43510077
   1.46641178  0.79023284]
 [ 0.84819453         nan         nan         nan         nan         nan
   2.30434358  3.03008138]
 [        nan  0.32479187 -0.51818422         nan  0.02013802         nan
   1.29874918  1.33053637]
 [-1.81407786  1.24241182         nan -1.32723907         nan         nan
  -0.31110699         nan]
 [-0.48527696         nan -1.51957106         nan -0.86984941  0.52807989
          nan  0.33771451]
 [-0.26997359 -0.48498966         nan -2.73891459 -2.48167957  2.88740609
  -0.24614835         nan]
 [ 3.57769701 -1.608339    4.73789234  1.13583164  3.63451505 -2.60495928
   2.12453635  3.76472563]
 [ 0.69623809 -0.59117353 -0.28890188 -2.36431192         nan  1.50136796
   0.74268078         nan]
 [ 0.85768141  1.33357168         nan         nan  1.65089037 -2.46456289
   3.51030491  3.31220347]
 [-2.463496    0.60826298 -3.81241599 -2.11839267 -3.86597359  3.52934055
  -1.76203083 -2.63130953]]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Simple mean imputation (baseline)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.where(np.isnan(R_obs), np.nanmean(R_obs), R_obs) R_mean &lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Apply SVD for low-rank approximation&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Replace NaNs with zeros for SVD step
= np.nan_to_num(R_obs, nan=0.0)
 R_filled 
= np.linalg.svd(R_filled, full_matrices=False)
 U, S, Vt 
= 3  # latent dimension
 k = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :] R_approx &lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compare filled matrix with ground truth&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.nanmean((R_full - R_approx)**2)
 error print("Approximation error (MSE):", error)&lt;/code&gt;
    &lt;code&gt;Approximation error (MSE): 1.4862378490976202&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visualize original vs reconstructed&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= plt.subplots(1, 2, figsize=(8,4))
 fig, axes 0].imshow(R_full, cmap="viridis")
 axes[0].set_title("True ratings")
 axes[1].imshow(R_approx, cmap="viridis")
 axes[1].set_title("Low-rank approximation")
 axes[ plt.show()&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Vary \(k\) (2, 3, 5). Does error go down?&lt;/item&gt;
      &lt;item&gt;Mask more entries (50%, 80%) - how does SVD reconstruction perform?&lt;/item&gt;
      &lt;item&gt;Use iterative imputation: alternate filling missing entries with low-rank approximations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Recommender systems rely on low-rank structure of user–item matrices.&lt;/item&gt;
      &lt;item&gt;SVD provides a natural way to approximate and fill missing ratings.&lt;/item&gt;
      &lt;item&gt;This low-rank modeling idea underpins modern collaborative filtering systems like Netflix and Spotify recommenders.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;98. PageRank and Random Walks (Ranking with Eigenvectors)&lt;/head&gt;
    &lt;p&gt;The PageRank algorithm, made famous by Google, uses linear algebra and random walks on graphs to rank nodes (webpages, people, items). The idea: importance flows through links - being linked by important nodes makes you important.&lt;/p&gt;
    &lt;head rend="h4"&gt;The PageRank Idea&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Start a random walk on a graph: at each step, move to a random neighbor.&lt;/item&gt;
      &lt;item&gt;Add a “teleportation” step with probability \(1 - \alpha\) to avoid dead ends.&lt;/item&gt;
      &lt;item&gt;The steady-state distribution of this walk is the PageRank vector, found as the principal eigenvector of the transition matrix.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np
import networkx as nx
import matplotlib.pyplot as plt&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build a small directed graph&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= nx.DiGraph()
 G 
 G.add_edges_from([0,1), (1,2), (2,0),  # cycle among 0–1–2
     (2,3), (3,2),         # back-and-forth 2–3
     (1,3), (3,4), (4,1)   # small loop with 1–3–4
     (
 ])=True, node_color="lightblue", node_size=800, arrowsize=15)
 nx.draw_circular(G, with_labels plt.show()&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build adjacency and transition matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= G.number_of_nodes()
 n = nx.to_numpy_array(G, nodelist=range(n))
 A = A / A.sum(axis=1, keepdims=True)  # row-stochastic transition matrix P &lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Add teleportation (Google matrix)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= 0.85  # damping factor
 alpha = alpha * P + (1 - alpha) * np.ones((n,n)) / n G_matrix &lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Power iteration to compute PageRank&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.ones(n) / n  # start uniform
 r for _ in range(100):
= r @ G_matrix
     r /= r.sum()
 r print("PageRank vector:", r)&lt;/code&gt;
    &lt;code&gt;PageRank vector: [0.13219034 0.25472358 0.24044787 0.24044787 0.13219034]&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compare with NetworkX built-in&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= nx.pagerank(G, alpha=alpha)
 pr print("NetworkX PageRank:", pr)&lt;/code&gt;
    &lt;code&gt;NetworkX PageRank: {0: 0.13219008157546333, 1: 0.2547244023837789, 2: 0.24044771723264727, 3: 0.24044771723264727, 4: 0.13219008157546333}&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visualize node importance&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= [5000 * r_i for r_i in r]
 sizes =True, node_size=sizes, node_color="lightblue", arrowsize=15)
 nx.draw_circular(G, with_labels"PageRank visualization (node size ~ importance)")
 plt.title( plt.show()&lt;/code&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Change \(\alpha\) (e.g., 0.6 vs 0.95). Does ranking change?&lt;/item&gt;
      &lt;item&gt;Add a “dangling node” with no outlinks. How does teleportation handle it?&lt;/item&gt;
      &lt;item&gt;Try PageRank on a larger graph (like a random graph with 50 nodes).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PageRank is a random-walk steady state problem.&lt;/item&gt;
      &lt;item&gt;It reduces to finding the dominant eigenvector of the Google matrix.&lt;/item&gt;
      &lt;item&gt;This method generalizes beyond webpages - to influence ranking, recommendation, and network analysis.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;99. Numerical Linear Algebra Essentials (Floating Point, BLAS/LAPACK)&lt;/head&gt;
    &lt;p&gt;When working with linear algebra on computers, numbers are not exact. They live in floating-point arithmetic, and computations rely on highly optimized libraries like BLAS and LAPACK. Understanding these essentials is crucial to doing linear algebra at scale.&lt;/p&gt;
    &lt;head rend="h4"&gt;Floating Point Basics&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Numbers are stored in base-2 scientific notation:&lt;/p&gt;
        &lt;p&gt;\[ x = \pm (1.b_1b_2b_3\ldots) \times 2^e \]&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Limited precision means rounding errors.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Two key constants:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Machine epsilon ($\(): smallest difference detectable (\)^{-16}$ for double).&lt;/item&gt;
          &lt;item&gt;Overflow/underflow: too large or too small to represent.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Set Up Your Lab&lt;/head&gt;
    &lt;code&gt;import numpy as np&lt;/code&gt;
    &lt;head rend="h4"&gt;Step-by-Step Code Walkthrough&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Machine epsilon&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.finfo(float).eps
 eps print("Machine epsilon:", eps)&lt;/code&gt;
    &lt;code&gt;Machine epsilon: 2.220446049250313e-16&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Round-off error demo&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= 1e16
 a = 1.0
 b print("a + b - a:", (a + b) - a)  # may lose b due to precision limits&lt;/code&gt;
    &lt;code&gt;a + b - a: 0.0&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Stability of matrix inversion&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.array([[1, 1.0001], [1.0001, 1]])
 A = np.array([2, 2.0001])
 b 
= np.linalg.solve(A, b)
 x_direct = np.linalg.inv(A) @ b
 x_via_inv 
print("Solve:", x_direct)
print("Inverse method:", x_via_inv)&lt;/code&gt;
    &lt;code&gt;Solve: [1.499975 0.499975]
Inverse method: [1.499975 0.499975]&lt;/code&gt;
    &lt;p&gt;Notice: using &lt;code&gt;np.linalg.inv&lt;/code&gt; can be less stable - better to solve directly.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Conditioning of a matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.linalg.cond(A)
 cond print("Condition number:", cond)&lt;/code&gt;
    &lt;code&gt;Condition number: 20001.00000000417&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Large condition number → small input changes cause big output changes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;BLAS/LAPACK under the hood&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;= np.random.randn(500, 500)
 A = np.random.randn(500, 500)
 B 
# Matrix multiplication (calls optimized BLAS under the hood)
= A @ B C &lt;/code&gt;
    &lt;p&gt;This &lt;code&gt;@&lt;/code&gt; operator is not a naive loop - it calls a highly optimized C/Fortran routine.&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compare solving &lt;code&gt;Ax = b&lt;/code&gt;with&lt;code&gt;np.linalg.solve&lt;/code&gt;vs&lt;code&gt;np.linalg.inv(A) @ b&lt;/code&gt;for larger, ill-conditioned systems.&lt;/item&gt;
      &lt;item&gt;Use &lt;code&gt;np.linalg.svd&lt;/code&gt;on a nearly singular matrix. How stable are the singular values?&lt;/item&gt;
      &lt;item&gt;Check performance: time &lt;code&gt;A @ B&lt;/code&gt;for sizes 100, 500, 1000.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Numerical linear algebra = math + floating-point reality.&lt;/item&gt;
      &lt;item&gt;Always prefer stable algorithms (&lt;code&gt;solve&lt;/code&gt;,&lt;code&gt;qr&lt;/code&gt;,&lt;code&gt;svd&lt;/code&gt;) over naive inversion.&lt;/item&gt;
      &lt;item&gt;Libraries like BLAS/LAPACK make large computations fast, but understanding precision and conditioning prevents nasty surprises.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;100. Capstone Problem Sets and Next Steps (A Roadmap to Mastery)&lt;/head&gt;
    &lt;p&gt;This final section ties everything together. Instead of introducing a new topic, it provides capstone labs that combine multiple ideas from the book. Working through them will give you confidence that you can apply linear algebra to real problems.&lt;/p&gt;
    &lt;head rend="h4"&gt;Problem Set 1 - Image Compression with SVD&lt;/head&gt;
    &lt;p&gt;Take an image, treat it as a matrix, and approximate it with low-rank SVD.&lt;/p&gt;
    &lt;code&gt;import numpy as np
import matplotlib.pyplot as plt
from skimage import data, color

# Load grayscale image
= color.rgb2gray(data.astronaut())
 img = np.linalg.svd(img, full_matrices=False)
 U, S, Vt 
# Approximate with rank-k
= 50
 k = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]
 img_approx 
1,2,1)
 plt.subplot(="gray")
 plt.imshow(img, cmap"Original")
 plt.title("off")
 plt.axis(
1,2,2)
 plt.subplot(="gray")
 plt.imshow(img_approx, cmapf"Rank-{k} Approximation")
 plt.title("off")
 plt.axis(
 plt.show()&lt;/code&gt;
    &lt;p&gt;Try different \(k\) values (5, 20, 100). How does quality vs. compression trade off?&lt;/p&gt;
    &lt;head rend="h4"&gt;Problem Set 2 - Predictive Modeling with PCA + Regression&lt;/head&gt;
    &lt;p&gt;Combine PCA for dimensionality reduction with linear regression for prediction.&lt;/p&gt;
    &lt;code&gt;from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.decomposition import PCA

# Load dataset
= load_diabetes(return_X_y=True)
 X, y = train_test_split(X, y, random_state=0)
 X_train, X_test, y_train, y_test 
# PCA reduce features
= PCA(n_components=5)
 pca = pca.fit_transform(X_train)
 X_train_pca = pca.transform(X_test)
 X_test_pca 
# Regression on reduced space
= LinearRegression().fit(X_train_pca, y_train)
 model print("R^2 on test set:", model.score(X_test_pca, y_test))&lt;/code&gt;
    &lt;code&gt;R^2 on test set: 0.3691398497153573&lt;/code&gt;
    &lt;p&gt;Does reducing dimensions improve or hurt accuracy?&lt;/p&gt;
    &lt;head rend="h4"&gt;Problem Set 3 - Graph Analysis with PageRank&lt;/head&gt;
    &lt;p&gt;Apply PageRank to a custom-built network.&lt;/p&gt;
    &lt;code&gt;import networkx as nx

= nx.barabasi_albert_graph(20, 2)  # 20 nodes, scale-free graph
 G = nx.pagerank(G, alpha=0.85)
 pr 
=True, node_size=[5000*pr[n] for n in G], node_color="lightblue")
 nx.draw(G, with_labels"PageRank on a scale-free graph")
 plt.title( plt.show()&lt;/code&gt;
    &lt;p&gt;Which nodes dominate? How does structure affect ranking?&lt;/p&gt;
    &lt;head rend="h4"&gt;Problem Set 4 - Solving Differential Equations with Eigen Decomposition&lt;/head&gt;
    &lt;p&gt;Use eigenvalues/eigenvectors to solve a linear dynamical system.&lt;/p&gt;
    &lt;code&gt;= np.array([[0,1],[-2,-3]])
 A = np.linalg.eig(A)
 eigvals, eigvecs 
print("Eigenvalues:", eigvals)
print("Eigenvectors:\n", eigvecs)&lt;/code&gt;
    &lt;code&gt;Eigenvalues: [-1. -2.]
Eigenvectors:
 [[ 0.70710678 -0.4472136 ]
 [-0.70710678  0.89442719]]&lt;/code&gt;
    &lt;p&gt;Predict long-term behavior: will the system decay, oscillate, or grow?&lt;/p&gt;
    &lt;head rend="h4"&gt;Problem Set 5 - Least Squares for Overdetermined Systems&lt;/head&gt;
    &lt;code&gt;0)
 np.random.seed(= np.random.randn(100, 3)
 X = np.array([2, -1, 0.5])
 beta_true = X @ beta_true + np.random.randn(100)*0.1
 y 
*_ = np.linalg.lstsq(X, y, rcond=None)
 beta_hat, print("Estimated coefficients:", beta_hat)&lt;/code&gt;
    &lt;code&gt;Estimated coefficients: [ 1.99371939 -1.00708947  0.50661857]&lt;/code&gt;
    &lt;p&gt;Compare estimated vs. true coefficients. How close are they?&lt;/p&gt;
    &lt;head rend="h4"&gt;Try It Yourself&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Combine SVD and recommender systems - build a movie recommender with synthetic data.&lt;/item&gt;
      &lt;item&gt;Implement Gram–Schmidt by hand and test it against &lt;code&gt;np.linalg.qr&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Write a mini “linear algebra toolkit” with your favorite helper functions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Takeaway&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You’ve practiced vectors, matrices, systems, eigenvalues, SVD, PCA, PageRank, and more.&lt;/item&gt;
      &lt;item&gt;Real problems often combine multiple concepts - the labs show how everything fits together.&lt;/item&gt;
      &lt;item&gt;Next steps: dive deeper into numerical linear algebra, explore machine learning applications, or study advanced matrix factorizations (Jordan form, tensor decompositions).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This concludes the hands-on journey. By now, you don’t just know the theory - you can use linear algebra as a working tool in Python for data, science, and engineering.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://little-book-of.github.io/linear-algebra/books/en-US/lab.html"/><published>2025-09-26T09:46:13+00:00</published></entry></feed>