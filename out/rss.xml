<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 30 Jan 2026 21:45:57 +0000</lastBuildDate><item><title>How AI assistance impacts the formation of coding skills</title><link>https://www.anthropic.com/research/AI-assistance-coding-skills</link><description>&lt;doc fingerprint="b720e598aba307c3"&gt;
  &lt;main&gt;
    &lt;p&gt;Research shows AI helps people do parts of their job faster. In an observational study of Claude.ai data, we found AI can speed up some tasks by 80%. But does this increased productivity come with trade-offs? Other research shows that when people use AI assistance, they become less engaged with their work and reduce the effort they put into doing it—in other words, they offload their thinking to AI.&lt;/p&gt;
    &lt;p&gt;It’s unclear whether this cognitive offloading can prevent people from growing their skills on the job, or—in the case of coding—understanding the systems they’re building. Our latest study, a randomized controlled trial with software developers as participants, investigates this potential downside of using AI at work.&lt;/p&gt;
    &lt;p&gt;This question has broad implications—for how to design AI products that facilitate learning, for how workplaces should approach AI policies, and for broader societal resilience, among others. We focused on coding, a field where AI tools have rapidly become standard. Here, AI creates a potential tension: as coding grows more automated and speeds up work, humans will still need the skills to catch errors, guide output, and ultimately provide oversight for AI deployed in high-stakes environments. Does AI provide a shortcut to both skill development and increased efficiency? Or do productivity increases from AI assistance undermine skill development?&lt;/p&gt;
    &lt;p&gt;In a randomized controlled trial, we examined 1) how quickly software developers picked up a new skill (in this case, a Python library) with and without AI assistance; and 2) whether using AI made them less likely to understand the code they’d just written.&lt;/p&gt;
    &lt;p&gt;We found that using AI assistance led to a statistically significant decrease in mastery. On a quiz that covered concepts they’d used just a few minutes before, participants in the AI group scored 17% lower than those who coded by hand, or the equivalent of nearly two letter grades. Using AI sped up the task slightly, but this didn’t reach the threshold of statistical significance.&lt;/p&gt;
    &lt;p&gt;Importantly, using AI assistance didn’t guarantee a lower score. How someone used AI influenced how much information they retained. The participants who showed stronger mastery used AI assistance not just to produce code but to build comprehension while doing so—whether by asking follow-up questions, requesting explanations, or posing conceptual questions while coding independently.&lt;/p&gt;
    &lt;head rend="h2"&gt;Study design&lt;/head&gt;
    &lt;p&gt;We recruited 52 (mostly junior) software engineers, each of whom had been using Python at least once a week for over a year. We also made sure they were at least somewhat familiar with AI coding assistance, and were unfamiliar with Trio, the Python library on which our tasks were based.&lt;/p&gt;
    &lt;p&gt;We split the study into three parts: a warm-up; the main task consisting of coding two different features using Trio (which requires understanding concepts related to asynchronous programming, a skill often learned in a professional setting); and a quiz. We told participants that a quiz would follow the task, but encouraged them to work as quickly as possible.&lt;/p&gt;
    &lt;p&gt;We designed the coding task to mimic how someone might learn a new tool through a self-guided tutorial. Each participant was given a problem description, starter code, and a brief explanation of the Trio concepts needed to solve it. We used an online coding platform with an AI assistant in the sidebar which had access to participants’ code and could at any time produce the correct code if asked.1&lt;/p&gt;
    &lt;head rend="h3"&gt;Evaluation design&lt;/head&gt;
    &lt;p&gt;In our evaluation design, we drew on research in computer science education to identify four types of questions commonly used to assess mastery of coding skills:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Debugging: The ability to identify and diagnose errors in code. This skill is crucial for detecting when AI-generated code is incorrect and understanding why it fails.&lt;/item&gt;
      &lt;item&gt;Code reading: The ability to read and comprehend what code does. This skill enables humans to understand and verify AI-written code before deployment.&lt;/item&gt;
      &lt;item&gt;Code writing: The ability to write or select the correct approach to writing code. Low-level code writing, like remembering the syntax of functions, will be less important with the further integration of AI coding tools than high-level system design.&lt;/item&gt;
      &lt;item&gt;Conceptual: The ability to understand the core principles behind tools and libraries. Conceptual understanding is critical for assessing whether AI-generated code uses appropriate software design patterns that adhere to how the library is intended to be used.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our assessment focused most heavily on debugging, code reading, and conceptual problems, as we considered these the most important for providing oversight of what is increasingly likely to be AI-generated code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Results&lt;/head&gt;
    &lt;p&gt;On average, participants in the AI group finished about two minutes faster, although the difference was not statistically significant. There was, however, a significant difference in test scores: the AI group averaged 50% on the quiz, compared to 67% in the hand-coding group—or the equivalent of nearly two letter grades (Cohen's d=0.738, p=0.01). The largest gap in scores between the two groups was on debugging questions, suggesting that the ability to understand when code is incorrect and why it fails may be a particular area of concern if AI impedes coding development.&lt;/p&gt;
    &lt;head rend="h3"&gt;Qualitative analysis: AI interaction modes&lt;/head&gt;
    &lt;p&gt;We were particularly interested in understanding how participants went about completing the tasks we designed. In our qualitative analysis, we manually annotated screen recordings to identify how much time participants spent composing queries, what types of questions they asked, the types of errors they made, and how much time they spent actively coding.&lt;/p&gt;
    &lt;p&gt;One surprising result was how much time participants spent interacting with the AI assistant. Several took up to 11 minutes (30% of the total time allotted) composing up to 15 queries. This helped to explain why, on average, participants using AI finished faster although the productivity improvement was not statistically significant. We expect AI would be more likely to significantly increase productivity when used on repetitive or familiar tasks.&lt;/p&gt;
    &lt;p&gt;Unsurprisingly, participants in the No AI group encountered more errors. These included errors in syntax and in Trio concepts, the latter of which mapped directly to topics tested on the evaluation. Our hypothesis is that the participants who encountered more Trio errors (namely, the control group) likely improved their debugging skills through resolving these errors independently.&lt;/p&gt;
    &lt;p&gt;We then grouped participants by how they interacted with AI, identifying distinct patterns that led to different outcomes in completion time and learning.&lt;/p&gt;
    &lt;p&gt;Low-scoring interaction patterns: The low-scoring patterns generally involved a heavy reliance on AI, either through code generation or debugging. The average quiz scores in this group were less than 40%. They showed less independent thinking and more cognitive offloading. We further separated them into:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AI delegation (n=4): Participants in this group wholly relied on AI to write code and complete the task. They completed the task the fastest and encountered few or no errors in the process.&lt;/item&gt;
      &lt;item&gt;Progressive AI reliance (n=4): Participants in this group started by asking one or two questions but eventually delegated all code writing to the AI assistant. They scored poorly on the quiz largely due to not mastering any of the concepts on the second task.&lt;/item&gt;
      &lt;item&gt;Iterative AI debugging (n=4): Participants in this group relied on AI to debug or verify their code. They asked more questions, but relied on the assistant to solve problems, rather than to clarify their own understanding. They scored poorly as a result, and were also slower at completing the two tasks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;High-scoring interaction patterns: We considered high-scoring quiz patterns to be behaviors where the average quiz score was 65% or higher. Participants in these clusters used AI both for code generation and conceptual queries.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Generation-then-comprehension (n=2): Participants in this group first generated code and then manually copied or pasted the code into their work. After their code was generated, they asked the AI assistant follow-up questions to improve understanding. These participants were not particularly fast when using AI, but showed a higher level of understanding on the quiz. Interestingly, this approach looked nearly the same as that of the AI delegation group, except for the fact that they used AI to check their own understanding.&lt;/item&gt;
      &lt;item&gt;Hybrid code-explanation (n=3): Participants in this group composed hybrid queries in which they asked for code generation along with explanations of the generated code. Reading and understanding the explanations they asked for took more time, but helped in their comprehension.&lt;/item&gt;
      &lt;item&gt;Conceptual inquiry (n=7): Participants in this group only asked conceptual questions and relied on their improved understanding to complete the task. Although this group encountered many errors, they also independently resolved them. On average, this mode was the fastest among high-scoring patterns and second fastest overall, after AI delegation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our qualitative analysis does not draw a causal link between interaction patterns and learning outcomes, but it does point to behaviors associated with different learning outcomes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Our results suggest that incorporating AI aggressively into the workplace, particularly with respect to software engineering, comes with trade-offs. The findings highlight that not all AI-reliance is the same: the way we interact with AI while trying to be efficient affects how much we learn. Given time constraints and organizational pressures, junior developers or other professionals may rely on AI to complete tasks as fast as possible at the cost of skill development—and notably the ability to debug issues when something goes wrong.&lt;/p&gt;
    &lt;p&gt;Though preliminary, these results suggest important considerations as companies transition to a greater ratio of AI-written to human-written code. Productivity benefits may come at the cost of skills necessary to validate AI-written code if junior engineers’ skill development has been stunted by using AI in the first place. Managers should think intentionally about how to deploy AI tools at scale, and consider systems or intentional design choices that ensure engineers continue to learn as they work—and are thus able to exercise meaningful oversight over the systems they build.&lt;/p&gt;
    &lt;p&gt;For novice workers in software engineering or any other industry, our study can be viewed as a small piece of evidence toward the value of intentional skill development with AI tools. Cognitive effort—and even getting painfully stuck—is likely important for fostering mastery. This is also a lesson that applies to how individuals choose to work with AI, and which tools they use. Major LLM services also provide learning modes (e.g., Claude Code Learning and Explanatory mode or ChatGPT Study Mode) designed to foster understanding. Knowing how people learn when using AI can also help guide how we design it; AI assistance should enable humans to work more efficiently and develop new skills at the same time.&lt;/p&gt;
    &lt;p&gt;Prior studies have found mixed results on whether AI helps or hinders coding productivity. Our own research found that AI can reduce the time it takes to complete some work tasks by 80%—a result that may seem in tension with the findings presented here. But the two studies ask different questions and use different methods: our earlier observational work measured productivity on tasks where participants already had the relevant skills, while this study examines what happens when people are learning something new. It is possible that AI both accelerates productivity on well-developed skills and hinders the acquisition of new ones, though more research is needed to understand this relationship.&lt;/p&gt;
    &lt;p&gt;This study is only a first step towards uncovering how human-AI collaboration affects the experience of workers. Our sample was relatively small, and our assessment measured comprehension shortly after the coding task. Whether immediate quiz performance predicts longer-term skill development is an important question this study does not resolve. There remain many unanswered questions we hope future studies will investigate, for example: the effects of AI on tasks beyond coding, whether this effect dissipates longitudinally as engineers develop greater fluency, and whether AI assistance differs from human assistance while learning.&lt;/p&gt;
    &lt;p&gt;Ultimately, to accommodate skill development in the presence of AI, we need a more expansive view of the impacts of AI on workers. In an AI-augmented workplace, productivity gains matter, but so does the long-term development of the expertise those gains depend on.&lt;/p&gt;
    &lt;p&gt;Read the full paper for details.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Acknowledgments&lt;/head&gt;
    &lt;p&gt;This project was led by Judy Hanwen Shen and Alex Tamkin. Editorial support for this blog post was provided by Jake Eaton, Stuart Ritchie, and Sarah Pollack.&lt;/p&gt;
    &lt;p&gt;We would like to thank Ethan Perez, Miranda Zhang, and Henry Sleight for making this project possible through the Anthropic Safety Fellows Program. We would also like to thank Matthew Jörke, Juliette Woodrow, Sarah Wu, Elizabeth Childs, Roshni Sahoo, Nate Rush, Julian Michael, and Rose Wang for experimental design feedback.&lt;/p&gt;
    &lt;code&gt;@misc{aiskillformation2026,
  author = {Shen, Judy Hanwen and Tamkin, Alex},
  title = {How AI Impacts Skill Formation},
  year = {2026},
  eprint = {2601.20245},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  eprinttype = {arxiv}
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Importantly, this setup is different from agentic coding products like Claude Code; we expect that the impacts of such programs on skill development are likely to be more pronounced than the results here.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46820924</guid><pubDate>Fri, 30 Jan 2026 05:41:23 +0000</pubDate></item><item><title>Netflix Animation Studios Joins the Blender Development Fund as Corporate Patron</title><link>https://www.blender.org/press/netflix-animation-studios-joins-the-blender-development-fund-as-corporate-patron/</link><description>&lt;doc fingerprint="f8491175711529fd"&gt;
  &lt;main&gt;
    &lt;p&gt;Blender Foundation is thrilled to announce that Netflix Animation Studios is joining the Blender Development Fund as Corporate Patron.&lt;/p&gt;
    &lt;p&gt;This support will be dedicated towards general Blender core development, to continuously improve content creation tools for individuals and teams working in media and entertainment-related workflows.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This membership is a significant acknowledgement of Blender becoming more embedded in high-end animation studios’ workflows. I deeply appreciate this strategic initiative from Netflix Animation Studios as an investment in a diverse, public, and open-source friendly ecosystem of creative tools that will benefit the global community of content creators.&lt;/p&gt;
      &lt;p&gt;Francesco Siddi, CEO at Blender&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Netflix Animation Studios’ corporate membership with Blender reflects our ongoing support for open-source software in the animation community. We are proud to be the first major animation studio to support Blender’s continued development and growing adoption by current and future generations of animation professionals.&lt;/p&gt;
      &lt;p&gt;Darin Grant, SVP Global Technology at Netflix Animation Studios&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;About Netflix&lt;/head&gt;
    &lt;p&gt;Netflix is one of the world’s leading entertainment services, with over 300 million paid memberships in over 190 countries enjoying TV series, films and games across a wide variety of genres and languages. Members can play, pause and resume watching as much as they want, anytime, anywhere, and can change their plans at any time. Discover more about Netflix Animation Studios at https://www.netflixanimation.com/&lt;/p&gt;
    &lt;head rend="h2"&gt;About Blender&lt;/head&gt;
    &lt;p&gt;Blender, the world’s most popular free and open-source 3D creation software, offers a comprehensive solution for modelling, animation, VFX, and more. Maintained by the Blender Foundation, it’s the tool of choice for a vast global community of professional artists and enthusiasts, committed to open collaboration and 3D technology innovation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46821134</guid><pubDate>Fri, 30 Jan 2026 06:19:36 +0000</pubDate></item><item><title>Photoroom (YC S20) Is Hiring a Head of Cross-Platform (Rust) in Paris</title><link>https://jobs.ashbyhq.com/photoroom/dc994a7c-e104-46e1-81c3-b88d635398b9</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46821326</guid><pubDate>Fri, 30 Jan 2026 07:00:08 +0000</pubDate></item><item><title>BoldVoice (YC S21) Is Hiring Fullstack and Machine Learning Engineers</title><link>https://boldvoice.notion.site/careers-page?p=2e871a9bf729806c81f6e47f32e32622&amp;pm=s</link><description>&lt;doc fingerprint="10f452a104a33a8"&gt;
  &lt;main&gt;
    &lt;p&gt;JavaScript must be enabled in order to use Notion. Please enable JavaScript to continue.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46823430</guid><pubDate>Fri, 30 Jan 2026 12:00:12 +0000</pubDate></item><item><title>Email experiments: filtering out external images</title><link>https://www.terracrypt.net/posts/email-experiments-image-filtering.html</link><description>&lt;doc fingerprint="679d9d1a3d50dfad"&gt;
  &lt;main&gt;
    &lt;p&gt;I had a realization the other day that, on almost every email in my inbox, my mail client has a "show external images" option. Most email I receive references externally loaded images via HTML. There are good reasons for this (not needing to send a copy of an image in every email on a newsletter) as well as nefarious reasons (the remote server can track where/when you load the image). So most mail clients I've used don't load them by default for obvious privacy reasons.&lt;/p&gt;
    &lt;p&gt;This got me thinking about the reverse, though: what email do I receive that does not include external images? And the answer is, mostly, email sent manually by a real human! I'm fairly certain I've never sent an email to another person in my personal life with an externally loaded image in it. When I have, it's been work email with a standard corporate email signature (that I'm sure was being tracked, natch). Mostly, when sending images to someone, they're sent as attachments to the email.&lt;/p&gt;
    &lt;p&gt;So I had a realization that, if I wanted to naturally filter email that was sent by hand from email sent from an automated system, this might be a decent proxy for that. Here's the sieve rule I landed on for now:&lt;/p&gt;
    &lt;code&gt;if body :regex "&amp;lt;img[^&amp;gt;]*src=\"https" {
  fileinto "Inbox.Automated";
}&lt;/code&gt;
    &lt;p&gt;(Yes, it's typically folly to regex on HTML. This is a simple enough match though that I hope it's fine!)&lt;/p&gt;
    &lt;p&gt;So far, after a day or so of usage, it's been shockingly effective. There's enough potentially important stuff in my Automated folder that I will need to check that fairly frequently, but the one email that's made it through this filter into my normal inbox is one sent by hand to a mailman list. Honestly, I'll take it! This makes the volume of email in my normal inbox much much more manageable, and I expect most mail that lands there will be things I actually do want to pay attention to.&lt;/p&gt;
    &lt;p&gt;Probably you'll want to have another rule that prevents you from running this for email from people in your contacts. I haven't done that yet, but that'll be a natural next step.&lt;/p&gt;
    &lt;p&gt;This is part of my December Adventure 2025 series.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46823445</guid><pubDate>Fri, 30 Jan 2026 12:01:36 +0000</pubDate></item><item><title>Code is cheap. Show me the talk</title><link>https://nadh.in/blog/code-is-cheap/</link><description>&lt;doc fingerprint="cf52105ea8a337bd"&gt;
  &lt;main&gt;
    &lt;p&gt;30 January 2026&lt;/p&gt;
    &lt;head rend="h1"&gt;Code is cheap. Show me the talk.&lt;/head&gt;
    &lt;p&gt;TLDR; Software development, as it has been done for decades, is over. LLM coding tools have changed it fundamentally for the better or worse.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Talk is cheap. Show me the code.” — Linus Torvalds, August 2000&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When Linus Torvalds, the creator of Linux, made this quip in response to a claim about a complex piece of programming in the Linux kernel, [1] I was an oblivious, gangly, fledgling teenage n00b coder learning by copy-pasting open source Perl and VB snippets over dialup internet.&lt;/p&gt;
    &lt;p&gt;The quip has since become an adage in the software world. The gist of it back then was that, it was easy to talk about all the software stuff one would like to do, or could be hypothetically done, but unless one actually put in the effort and proved it, talk wasn’t of much value. Writing and proving good software was a high-effort, high-cost, high-skill endeavour.&lt;/p&gt;
    &lt;p&gt;Even when armed with a crystal clear software development plan and the exact know-how to implement it, any sufficiently complex piece of programming is high-effort, tedious, and time consuming to actually write and get to a form where it is functional, reliable, and at least reasonably future-ready. In the process of developing software, any number of unforeseen complexities and gotchas can arise with many unresolvable trade-offs,[2] both technical and external. It is not uncommon for software architectures to change mid-way multiple times. The cost of just trying things out is so exponentially high that the significant majority of ideas are simply never tried out.&lt;/p&gt;
    &lt;p&gt;After all, the real bottleneck is good old physical and biological human constraints—cognitive bandwidth, personal time and resources, and most importantly, the biological cost and constraints of having to sit for indefinite periods, writing code with one’s own hands line by line even if it is all in one’s head, while juggling and context-switching through the mental map of large systems. And if it is more than one individual, a whole host of interpersonal coordination and communication dynamics come into play. It is thus very difficult to prototype and try out not just grand ideas, but even reasonably simple ones. As many of us have done, most ideas are generally appended to a bottomless wishlist where they very likely stay forever. That’s how I have programmed and written software on a regular basis and enjoyed it—from hobby stuff to critical systems that millions of people depend on—for about 25 years.&lt;/p&gt;
    &lt;p&gt;All that has now been thrown out of the window, of course, for better or worse.&lt;/p&gt;
    &lt;p&gt;Coming back to Linus, fast-forward 25 years, when he merges a chunk of AI-generated code into his toy project and comments “Is this much better than I could do by hand? Sure is.”, [3] I, no longer the fledgling n00b, but someone with decades of software development scars and calluses (both physical and metaphorical), am able to grasp its implications. Not only that, now with a sizeable amount of first-hand experience with LLM-assisted coding, I am compelled to say, software development, as it has been done for decades, is over. Along with that, many other things are too.&lt;/p&gt;
    &lt;p&gt;I say that with the full awareness that it smacks of Fukuyama’s The End of History, [4] but I will reiterate:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Software development, as it has been done for decades, is over.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;I&lt;/head&gt;
    &lt;p&gt;I was lucky to be in the transitionary Goldilocks era to witness and to partake in the breakneck evolution of the internet and software landscape—dialup to DSL to gigabit; Basic, Visual Basic 4/5/6 and Delphi; rise and fall of cgi-bin; Altavista to Google; XMLHttpRequest kicking off Web 2.0; rise and fall of Flash; death of IE and the rise of Chrome; WAP to Symbian to Android and smartphone apps; the demise of SourceForge and the massive proliferation and success of FOSS (Free and Open-source Software); git and GitHub; rise of SaaS; ExpertsExchange to StackOverflow; the growth of the Linux world; sysadmin to devops to whateverOps; the ominous birthing of Node.js and MongoDB in the same year; microservices; the explosion of VC-funded software “unicorns”; crypto and web3 shams; the rapid darkening of patterns; widespread enshittification and monetisation of privacy, attention, and dignity; and the monumental bloating of software that has since become the norm.&lt;/p&gt;
    &lt;p&gt;All throughout this, I have been writing, maintaining, and deploying software both as a professional developer and as a FOSS hobbyist dabbling in a gazillion languages, frameworks, tools, and methodologies. From thinking that “indenting code is lame” (cringe) as a teen, from copy-pasting to CVS to svn to git, fighting space vs. tab battles, to maturing to “whatever floats your boat” and still regularly compressing PNGs to shave off a few KBs, I have been a dabbler, dilettante, and an addict, someone who has unconditionally enjoyed writing code and developing software.&lt;/p&gt;
    &lt;p&gt;But now? How I develop software now is not how I have done it all these years, all the right, wrong, good, bad, easy and hard bits combined. With the advent of code-assisting LLMs, it has been completely flipped on its head, and I don’t think there is any going back.&lt;/p&gt;
    &lt;p&gt;Now, that is some “Tears in rain”-esque [5] monologue right there.&lt;/p&gt;
    &lt;head rend="h2"&gt;Code&lt;/head&gt;
    &lt;p&gt;Barring a bunch of obvious objective 101s, there is no universal measure of what makes a codebase good or great. Styles, idioms, patterns, architectures all vary greatly. Even objectively provable technical choices are subject to trade-offs that defy consensus. For a software developer like me, historically, there have been a few rule-of-thumb indicators for quick evaluation of software. When I evaluate a FOSS project, I look at a bunch of factors, all a mix of objective and subjective, weighted differently under different contexts—the project’s age; is the commit activity overly sparse or frantic; frameworks and dependencies; is code consistently organised and commented without being over-abstracted; is there a community around it; are maintainers responsive; can I actually get it up and running quickly from a clear README; the quality and depth of its documentation …&lt;/p&gt;
    &lt;p&gt;Many of these rule-of-thumb signals give a reasonable glimpse of the mental model and the style of working of the maintainers and the likely future trajectory of the project. For example, concise comments, README, and documentation indicate thoughtfulness, extra effort, and empathy for other developers (and self). Mainly because, for mortal developers like me, documentation and tests are a necessity, but unpleasant, boring, and tedious things to write and maintain.&lt;/p&gt;
    &lt;p&gt;Well, those notions have now been abruptly and violently defenestrated by LLMs. They can now one-shot generate stunning looking documentation pages, dense (ironically, pedantically detailed) READMEs, build great looking user interfaces, neatly organise code with proper idioms, patterns, and comments. One can no longer know whether such a repository was “vibe” coded by a non-technical person who has never written a single line of code, or an experienced developer, who may or may not have used LLM assistance. These no longer indicate the quality of a codebase. On the contrary, the more stunning or perfect looking something is, the more suspicious it is now—was it low-effort, one-shot vibe coded?&lt;/p&gt;
    &lt;p&gt;With the tell-tale, rule-of-thumb measures of code and software quality being outright dead, without a much closer inspection and a bit of expert forensic analysis, it is now difficult to tell the wheat from the “slop”. One is now slowly being compelled to also look much more closely at the provenance of software—the who, why, their track record, and plans of governance.&lt;/p&gt;
    &lt;head rend="h2"&gt;Effort&lt;/head&gt;
    &lt;p&gt;Historically, it would take a reasonably long period of consistent effort and many iterations of refinement for a good developer to produce 10,000 lines of quality code that not only delivered meaningful results, but was easily readable and maintainable. While the number of lines of code is not a measure of code quality—it is often the inverse—a codebase with good quality 10,000 lines of code indicated significant time, effort, focus, patience, expertise, and often, skills like project management that went into it. Human traits.&lt;/p&gt;
    &lt;p&gt;Now, LLMs can not only one-shot generate that in seconds, they can handle many technical aspects of the software development workflow, from testing to sysadmin to publishing. Unlike the unpredictable outcomes of frenzied vibe coding, when steered with human expertise, the output can be high quality and highly effective.[6] This has been my personal experience as well. On a regular basis, I have been compressing work that would have taken me weeks and months to mere days and even hours. That too, without vibe coding, an AGENT.md file, or any fancy multi-agent workflows or orchestration. Just an LLM agent CLI at arm’s length.&lt;/p&gt;
    &lt;p&gt;As a developer with a bottomless wishlist of things I wish I could have done or tried out, I have been able to use LLM tools to not just rapidly prototype and validate complex ideas, but actually write good quality production-grade software (my own subjective metric, of course) with better code than I could have written manually—things where I knew exactly what I had to do, but was constrained by physical limits, and also things that were unclear to me and needed novel ideas, approaches, and leaps. All the while, learning and bettering my own understanding of things.&lt;/p&gt;
    &lt;p&gt;The physiological, cognitive, and emotional cost I generally incur to achieve the software outcomes I want or am capable of engineering, has undoubtedly reduced by several orders of magnitude. The time and bandwidth this has freed up, I now spend on engineering, architecting, debating, tinkering, trying to expand my imagination, and writing much more concise and meaningful code that I actually want to write.&lt;/p&gt;
    &lt;p&gt;Remember the old adage, “programming is 90% thinking and 10% typing”? It is now, for real.&lt;/p&gt;
    &lt;head rend="h2"&gt;Slop&lt;/head&gt;
    &lt;p&gt;Given all that, what is the value of code as an artefact, when it can be generated at an industrial scale within seconds by someone who has never written any code? Barring obviously bad LLM-generated code, when code is neatly structured and functional (yes, LLMs can write good code when steered competently), what makes it valuable or not? We wouldn’t want LLM-generated code in systems out there in the real world, but would instead prefer pure unadulterated human code, yes? Well, that would be a wonderful joke.[7] [8] [9] [10] [11]&lt;/p&gt;
    &lt;p&gt;The reality is that the significant majority of the code written by humans globally on a daily basis, is likely borderline junk.[12] Software development is not even a discipline that has reached any objective level of maturity. Medical doctors and civil engineers go through rigorous training to be issued licenses that are contingent on real world ramifications of their work. How about software developers and engineers? The world runs on shoddily engineered, poorly cobbled together, bloated systems with garbage code that humans have written, mostly directed by people in positions of power with perverse incentives who have absolutely no technical know-how or have any grounding in the humanities—the tyranny of non-tech “tech leaders”.[13]&lt;/p&gt;
    &lt;p&gt;One could, to trigger emotions, argue that AI slop is at least neatly formatted, well documented, and more syntactically consistent than the vast majority of human-written code. ( ͡° ͜ʖ ͡°)&lt;/p&gt;
    &lt;p&gt;Kidding aside, I am no fan of AI slop. Reading those obvious soulless LLM-generated messages and articles on the (dead) internet[14] is a waste of neuronal activation in the amygdala, if there is any activation at all. That so many people across the world LLM-speak and emote in the exact same manner on the internet, is creepy self-Pluribus-ification.[15] Without human creation, perfection and flaws, language, literature, art, music etc. are unenjoyable (to most). Infinite, instantly-generatable stuff without human constraints and limits, is actually very difficult to value.&lt;/p&gt;
    &lt;p&gt;As is code, then? Well, code is a bit different from art, literature, or any form of direct communication and evocation. Code was always a means to an end. Unlike poetry or prose, end users don’t read or care about code. They don’t care what language or framework or the architecture the hundred systems running behind a portal are made of. Code is hidden. They interact with the effect and outcomes of code through various forms of UX. I say that, slightly begrudgingly, as someone who enjoys writing, organising, and even nurturing code. For those who are immersed in it, there is an element of creativity and art in it, and many like me, are borderline curmudgeons on all things software.[16]&lt;/p&gt;
    &lt;p&gt;Ignoring outright bad code, in a world where functional code is so abundant that “good” and “bad” are indistinguishable, ultimately, what makes functional AI code slop or non-slop? I am strongly inclined to think that it is the framework of accountability, and ironically, the element of humanness. That is, all things (code) being equal, the ability to hold someone accountable at least emotionally and morally (and sometimes legally), for an artefact, instills value.&lt;/p&gt;
    &lt;p&gt;When one gets that big pull request (PR) on an open source repository, irrespective of its quality, if it is handwritten by a human, there is an intrinsic value and empathy for the human time and effort that is likely ascribed to it. It is known that there is a physical and cognitive cost that has been paid writing a lot of code before raising a PR. That is what makes that code “expensive” and not cheap.&lt;/p&gt;
    &lt;p&gt;When a PR is obviously LLM-generated, irrespective of how good it is, the first reaction is likely to be “slop!”, because it is no longer possible to instantly ascertain the human effort behind it. On the other hand, the effort required to read and validate it is disproportionately and exponentially high—setting aside people who have also offloaded reading of code to LLMs. It may very well be the best possible functional code, but it is one out of an infinite possible variation that could have been generated with no human cost or effort. Emotionally, it feels wrong and unfair to be burdened by such code dumps.&lt;/p&gt;
    &lt;p&gt;And, at that point, our reality has become a version of Borges’ Library of Babel.[17]&lt;/p&gt;
    &lt;head rend="h2"&gt;FOSS&lt;/head&gt;
    &lt;p&gt;Speaking of libraries, FOSS is perhaps the greatest public commons that humanity has created. The genesis of FOSS and its predecessors, various schemes for sharing code, can be traced to the fundamental premise that software was prohibitively expensive and required immense specialist skills to create. Only a tiny handful of people in the world had the ability to do that, and everyone else was naturally forced to use the creations of the few (proprietary or not). While the global developer ecosystem has exploded since then, the ratio of makers to users has largely remained the same. Largescale FOSS collaboration and community dynamics all stem from that—codebases as valuable shared artefacts.&lt;/p&gt;
    &lt;p&gt;What happens in a world where code is cheap and small to medium-sized software libraries and modules can be quickly created by an expert, perfectly customised and attuned to their needs, no matter how niche? Forget expertise, a world where anyone reasonably savvy can vibe code the small things they need for their private use, however they please. I see this happening everywhere. What is happening to StackOverflow[18] is also happening to software, although not as dramatically. This seems to strike at the very heart of the human dynamics, societal conditions, and incentives that drive FOSS collaboration and sharing. Add to that, if one considers the impending Cambrian explosion of FOSS projects manufactured at an unprecedented scale, the high-quality FOSS projects that remain and thrive, expert governance, curation, and trust are likely to become more valuable than the code itself.&lt;/p&gt;
    &lt;head rend="h2"&gt;Missing the forest for the trees&lt;/head&gt;
    &lt;p&gt;Humans have produced amazing software when there was no syntax highlighting, IDEs, or any kind of tooling. And humans also produce trash despite all the tooling and resources in the world. A good competent developer with good articulation skills and care for quality will use LLMs, or any other tools, in their own ways to produce quality outcomes. An incompetent developer with poor articulation skills or one with a lack of care for quality, will produce bad stuff, LLMs or not.&lt;/p&gt;
    &lt;p&gt;Thus, the extreme proponents of manic “agentic” vibe coding,[19] and the outright denouncers of LLMs, are both missing the forest for the trees. That there is a pragmatic middle path, where people who have the experience, expertise, competence, and the ability to articulate can use these tools to get the outcomes they desire with the right sets of trade-offs.&lt;/p&gt;
    &lt;p&gt;Vibe coding has its place, especially for non-technical people, who, for the first time, can tinker, explore, have fun, and empower themselves with software. I see this happening all around me. However, the fanatical acolytes of vibe coding are missing a very important thing that makes humans take artefacts seriously—finitude. They’re generating a vast Borgesian library where they themselves are likely to be lost in an ocean of slop generated by sycophantic agents. Slop, not because the code is of poor quality, but because anything that can be generated infinitely without effort and has no meaningful provenance, is very hard to value or take seriously. Humans fundamentally do not deal well with an infinite supply of anything, especially choices. Completely unsurprising because we are heavily constrained biological beings that have evolved on a finite planet with finite resources to live out finite lifetimes.&lt;/p&gt;
    &lt;p&gt;And then, the denouncers, they can’t seem to get past the argument from incredulity.[20] They denounce LLMs because they don’t personally like them for whatever reason, or have been unable to get desirable outcomes, or had the wrong expectations about them, or have simply gotten sick of them. But that is immaterial because there is a sizeable population who are using the exact same tools fruitfully and have the opposite experience. I am one of them.&lt;/p&gt;
    &lt;p&gt;All that said, the widespread braindead and outright stupid and harmful implementations of these technologies fuelled by hype, frenzy, and greed are an unfortunate reality and a massive cause of concern. The AI-business bubble is perhaps one of the biggest in history. The rise of FOSS AI technologies makes one hopeful. However, to incorrectly conflate bad actors, bad actions, bean-counting, and nonsensical implementations with fundamental, physical capabilities of these technologies—not theoretical, but the regular, proven, and practical—is irrational. It is missing the forest for the trees.&lt;/p&gt;
    &lt;head rend="h2"&gt;The human cost&lt;/head&gt;
    &lt;p&gt;All of this has been from the perspective of an experienced developer and engineer. For someone who has been weathered and bruised enough, these AI technologies provide extremely effective and powerful assistance.&lt;/p&gt;
    &lt;p&gt;But what about the young folks who are just starting out? If one does not have their fundamentals in place, if one has not developed an innate and nuanced understanding of systems and the process of software development, then these technologies are unreliable, dangerous genies. One asks for code, it gives code. One asks for changes, it gives changes. Soon, one is stuck with a codebase whose workings one doesn’t understand, and one is forced to go back to the genie and depend on it helplessly. And because one is hooked on and dependent on the genie, the natural circumstances that otherwise would allow for foundational and fundamental skills and understanding to develop, never arise, to the point of cognitive decline.[21] What then happens to an entire generation of juniors, who never get an opportunity to become seniors meaningfully?&lt;/p&gt;
    &lt;p&gt;Personally, I don’t care about the extreme vibe coders or denouncers or even slop. We are all going to drown in a deluge of slop, from which, many islands of sanity, recovery, and a new order of software will emerge. The real concern is for generations of learners who are being robbed of the opportunity to acquire the expertise to objectively discern what is slop and what is not. Even worse, the possibility that experienced folks who use these tools effectively, will feel disincentivised from mentoring and training junior folks in foundational ways, something that was a natural part of societal evolution. And not just with software development, but the wholesale offloading of agency and decision-making to black boxes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Talk&lt;/head&gt;
    &lt;p&gt;At this point, for a hands-on developer, reading and critically evaluating code have become more important than learning syntax and typing it out line by line. Of course, that is still an important skill, because the ability to read code effectively comes from that in the first place. But, the daily software development workflows have flipped over completely.&lt;/p&gt;
    &lt;p&gt;An experienced developer who can talk well, that is, imagine, articulate, define problem statements, architect and engineer, has a massive advantage over someone who cannot, more disproportionately than ever. Knowledge of specific language, syntax, and frameworks—code—is no longer a bottleneck. The physiological constraints of yore are no longer impediments. The machinery for instantly creating code at scale is now a commodity and available to everyone, just a &lt;code&gt;pip install&lt;/code&gt; equivalent away. It requires no special training, no new language or framework to learn, and has practically no entry barriers—just good old critical thinking and foundational human skills, and competence to run the machinery.&lt;/p&gt;
    &lt;p&gt;Conventional software development methodologies and roles—Waterfall[22] to Agile,[23] developer to tester, senior to junior—have fundamentally changed with traditional boundaries consolidating into unimaginably fast, compressed, blurry, iterative “agentic” loops. The dynamics of people, organisations, and public communities in software development, the very human incentives for sharing and collaboration,[24] [25] [26] are all changing.&lt;/p&gt;
    &lt;p&gt;For the first time ever, good talk is exponentially more valuable than good code. The ramifications of this are significant and disruptive. This time, it is different.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46823485</guid><pubDate>Fri, 30 Jan 2026 12:05:50 +0000</pubDate></item><item><title>Pangolin (YC S25) is hiring software engineers (open-source, Go, networking)</title><link>https://docs.pangolin.net/careers/join-us</link><description>&lt;doc fingerprint="578d4934f0eed0f"&gt;
  &lt;main&gt;
    &lt;div&gt;Skip to main content&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;We are looking for talented engineers to join our team and help build secure remote access. If you’re passionate about open-source software, networking, and security, we’d love to hear from you.&lt;head rend="h2"&gt;About Pangolin&lt;/head&gt; Pangolin delivers identity-aware remote access to internal apps and services. Our platform replaces legacy VPNs and simplifies secure access to infrastructure, applications, and developer environments. We build in the open and are self‑hosted by default so teams retain control over data and infrastructure. The system is policy‑driven, integrates with standard IdPs, exposes clear observability and health, and provides an API for automation. If you’re interested in open-source auth and networking infrastructure, we’d love to chat. &lt;head rend="h2"&gt;Open Roles&lt;/head&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46823544</guid><pubDate>Fri, 30 Jan 2026 12:11:49 +0000</pubDate></item><item><title>Wisconsin communities signed secrecy deals for billion-dollar data centers</title><link>https://www.wpr.org/news/4-wisconsin-communities-signed-secrecy-deals-billion-dollar-data-centers</link><description>&lt;doc fingerprint="e3b3b41930a616ae"&gt;
  &lt;main&gt;
    &lt;p&gt;This story was produced and originally published by Wisconsin Watch, a nonprofit, nonpartisan newsroom. It was made possible by donors like you.&lt;/p&gt;
    &lt;p&gt;How did a $1 billion, 520-acre data center proposed by one of the world’s richest companies go unnoticed in tiny Beaver Dam, Wisconsin?&lt;/p&gt;
    &lt;p&gt;A key reason: In a city that lists “communication matters” atop its core values, officials took steps to keep the project hidden for more than a year.&lt;/p&gt;
    &lt;head rend="h2"&gt;News with a little more humanity&lt;/head&gt;
    &lt;p&gt;WPR’s “Wisconsin Today” newsletter keeps you connected to the state you love without feeling overwhelmed. No paywall. No agenda. No corporate filter.&lt;/p&gt;
    &lt;p&gt;Now Meta, the trillion-dollar company that owns Facebook and Instagram, is building a complex as big as 12 football fields in a city with a population of 16,000, enough to fill only a fifth of Lambeau Field.&lt;/p&gt;
    &lt;p&gt;It’s one of seven major data center projects pending in Wisconsin that combined are worth more than $57 billion.&lt;/p&gt;
    &lt;p&gt;In four of them, including Beaver Dam, local government officials kept the massive projects under wraps through confidential nondisclosure agreements, a Wisconsin Watch investigation has found.&lt;/p&gt;
    &lt;p&gt;Secrecy also occurred in the three communities without NDAs.&lt;/p&gt;
    &lt;p&gt;In one, the Madison suburb of DeForest, officials worked behind the scenes for months before publicly announcing a proposed $12 billion data center, which residents are fighting.&lt;/p&gt;
    &lt;p&gt;The lack of public disclosure, while relatively common for typical development proposals in the planning stages, raises questions about how much time the public should have to digest projects that dramatically affect the economy, land use, energy, taxes, the environment and more.&lt;/p&gt;
    &lt;p&gt;“As soon as community leadership is contemplating, even entertaining it, I think they need to make the public aware,” said retired tech executive Prescott Balch, who is advising residents around Wisconsin where data centers are proposed. “Even if it makes it harder, that’s the right way to do it. And nobody is doing it that way.”&lt;/p&gt;
    &lt;p&gt;Blowback from residents who have been kept in the dark has spurred a new legislative proposal that would ban data center NDAs statewide.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Beaver Dam did it&lt;/head&gt;
    &lt;p&gt;Wisconsin has some 40 data centers, stretching from Kenosha to Eau Claire. But most are tiny compared with the big seven: three under construction in Beaver Dam, Mount Pleasant and Port Washington; and four proposed in DeForest, Janesville, Kenosha and Menomonie.&lt;/p&gt;
    &lt;p&gt;Besides storing and processing data, data centers are vital to advancing the use of artificial intelligence.&lt;/p&gt;
    &lt;p&gt;A case study in how projects each worth $1 billion or more are kept quiet is Beaver Dam, the Dodge County burg an hour northeast of Madison, where Meta’s data center is expected to open in 2027.&lt;/p&gt;
    &lt;p&gt;The Beaver Dam Area Development Corp., a quasi-government nonprofit that functions as the city’s economic development arm, signed an NDA on Dec. 1, 2023, not with Meta, but with a shell company no one had ever heard of, Balloonist LLC.&lt;/p&gt;
    &lt;p&gt;The agreement referred only to a “project,” making no mention of a data center or Meta.&lt;/p&gt;
    &lt;p&gt;The NDA was signed “very early, almost in the introductory period of that project,” the development corporation’s leader, Trent Campbell, told Wisconsin Watch. All major development projects have “different levels of confidentiality for different purposes. And this entity believed it to be necessary at the onset of the conversations.”&lt;/p&gt;
    &lt;p&gt;The NDA meant that the Beaver Dam Area Development Corp. could not reveal its discussions with Balloonist, or even disclose “the existence of the project.”&lt;/p&gt;
    &lt;p&gt;The NDA also put the wheels in motion.&lt;/p&gt;
    &lt;p&gt;For more than a year, the city quietly took official actions to make the data center a reality, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;July 2024: The city council voted 12-0 to approve a predevelopment agreement with another shell company, Degas LLC, that only later was identified with the data center. The agenda and the minutes of the meeting don’t mention a data center.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;November 2024: The city council created a tax incremental finance, or TIF, district for the data center to help fund development. The agenda and the minutes for that meeting do not mention a data center, though the agreement itself does.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Not until February 2025 — 14 months after the NDA was signed — did the Beaver Dam Area Development Corp. announce that it and the city were working with a company — then still unidentified — on a “potential data center project.”&lt;/p&gt;
    &lt;p&gt;Campbell noted to Wisconsin Watch that Gov. Tony Evers and other officials had identified the site for a major development as far back as 2019. For months after the NDA was signed, it wasn’t known whether the data center would come to fruition, he added.&lt;/p&gt;
    &lt;p&gt;“I know the opponents currently disagree, but I think the city acted in as transparent a way as they could,” Campbell said.&lt;/p&gt;
    &lt;p&gt;Eventually, a news report in April 2025 identified Meta, which declined comment for this story, as the company likely behind the data center.&lt;/p&gt;
    &lt;p&gt;Meta confirmed its involvement eight months later, saying on Facebook: “We’re proud to call Beaver Dam home. We are honored to have joined such an incredible community in 2025.”&lt;/p&gt;
    &lt;p&gt;The first reply to that post was from a Beaver Dam resident, who wrote: “We would have been honored to have the opportunity to decline this.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Secrecy without an NDA&lt;/head&gt;
    &lt;p&gt;NDAs also helped keep the public in the dark about data centers under consideration in the three other cities that used them.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Menomonie signed its NDA with Balloonist LLC in February 2024 — more than a year before the city in northwest Wisconsin announced a $1.6 billion data center proposal in July 2025. Two months after the NDA, the city council unanimously helped pave the way for a data center by changing a land use ordinance. The change gave, for the first time, a definition of the ordinance’s reference to “warehousing,” saying warehousing includes data centers. The city’s mayor put the proposed data center on hold in September 2025. In January 2026, the city council adopted a zoning ordinance for data centers that reversed the warehousing definition. “Based upon feedback from the community and elected officials, it is clear that additional discussion should occur regarding the appropriate level of regulation of data centers,” the city’s public works director told the council and the mayor.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kenosha signed its NDA, with Microsoft, in May 2024, six months before news reports surfaced saying the NDA kept the proposed data center operator’s name confidential. It was later announced that Microsoft had purchased 240 acres in the neighboring town of Paris, which the city annexed in December 2024. No dollar amount for the proposal has been announced.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Janesville announced in July 2025 it was approached by developers about a data center and put out a request for proposal. The city signed its NDA two months later and is now in negotiations with Viridian Acquisitions, a Colorado developer, for an $8 billion data center.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Port Washington in Ozaukee County and Mount Pleasant in Racine County responded to records requests from Wisconsin Watch saying they had not signed NDAs for their data centers.&lt;/p&gt;
    &lt;p&gt;In Port Washington, where three people were arrested during a city council meeting on the data center in December, residents are trying to recall Mayor Ted Neitzke, saying he has been secretive about the $15 billion data center from OpenAI, Oracle and Vantage Data Centers.&lt;/p&gt;
    &lt;p&gt;In Mount Pleasant, Microsoft this month announced plans to add 15 data centers, worth $13 billion, to the $7 billion complex under construction there.&lt;/p&gt;
    &lt;p&gt;NDAs are described by economic development officials as necessary and criticized by data center opponents as against the public interest.&lt;/p&gt;
    &lt;p&gt;NDAs and other steps to protect confidentiality are crucial at the early stages of a development proposal, said Tricia Braun, executive director of the Wisconsin Data Center Coalition.&lt;/p&gt;
    &lt;p&gt;“If I’m a company considering making strategic investments, regardless of industry, I don’t want my competition to know where I’m going, what I’m doing, what pace I’m doing it at,” said Braun, a former executive at the Wisconsin Economic Development Corp. “You want to make sure everything is buttoned up and bow tied before that type of information is put into the public realm.”&lt;/p&gt;
    &lt;p&gt;Questions have swirled around transparency even in communities where local government officials did not sign NDAs.&lt;/p&gt;
    &lt;p&gt;That includes DeForest, which lists “communicate clearly” among its core values.&lt;/p&gt;
    &lt;p&gt;The DeForest data center, proposed by Virginia-based QTS Data Centers, is controversial, in part, because the village board would have to annex 1,600 acres in the neighboring town of Vienna.&lt;/p&gt;
    &lt;p&gt;At one DeForest Village Board meeting about the project, Village President Jane Cahill Wolfgram said that based on emails she had been receiving from residents, there was “just one thing I think we need to clear up.”&lt;/p&gt;
    &lt;p&gt;“And you can ask any one of these board members. They will tell you, they just learned about this project in the last couple of weeks.”&lt;/p&gt;
    &lt;p&gt;That was Nov. 18, 2025.&lt;/p&gt;
    &lt;p&gt;But Village Board trustees had been offered one-on-one meetings with the developer some 10 weeks earlier, trustee Jan Steffenhagen-Hahn said in an email to Vienna resident Shawn Haney.&lt;/p&gt;
    &lt;p&gt;“Because of the scale of this project,” that’s when residents should have been notified, said Haney, a leader of a group that opposes the data center.&lt;/p&gt;
    &lt;p&gt;Other emails obtained by the group show that DeForest staff were strategizing with QTS representatives and Alliant Energy as early as March 2025 — seven months before announcing the proposal last October.&lt;/p&gt;
    &lt;p&gt;In one email, the village planner discussed with QTS representatives when to seek various village approvals, including annexation, while acknowledging that doing so without disclosing “any details of the project or operations will be difficult.”&lt;/p&gt;
    &lt;p&gt;Cahill Wolfgram told Wisconsin Watch she in fact had met with QTS on Oct. 1, three weeks before the public announcement. She expressed frustration that many residents are urging trustees to stop the data center.&lt;/p&gt;
    &lt;p&gt;“They’ve been brought in from the very early moments of this discussion and they have continued to be front and center of everything we’ve done,” Cahill Wolfgram said. “As village president, I know of nothing that has been done behind the scenes.”&lt;/p&gt;
    &lt;p&gt;A public hearing on the annexation is scheduled for Feb. 9.&lt;/p&gt;
    &lt;p&gt;The state Department of Administration, which reviews annexation proposals and issues advisory opinions, concluded the DeForest annexation is not in the public interest because of concerns over how the village would provide water and sewer services for the annexed area.&lt;/p&gt;
    &lt;p&gt;The Clean Economy Coalition of Wisconsin has called for state leaders to pause consideration of any data centers until a comprehensive strategy on them is adopted. In part, the coalition said comprehensive planning is needed to avoid more “stranded assets.”&lt;/p&gt;
    &lt;p&gt;Wisconsin Watch reported in December that Wisconsin utility ratepayers owe nearly $1 billion for stranded assets — coal power plants that have been or soon will be shut down. A push to provide new energy capacity for data centers poses the risk of creating more stranded assets.&lt;/p&gt;
    &lt;head rend="h2"&gt;Some states targeting NDAs&lt;/head&gt;
    &lt;p&gt;Microsoft on Jan. 13 announced new standards aimed at being a “good neighbor in the communities where we build, own and operate our data centers.” It mentioned transparency five times.&lt;/p&gt;
    &lt;p&gt;But University of Wisconsin-Milwaukee researchers called Microsoft’s initial Mount Pleasant data center a “microcosm of a larger problem with secrecy and lack of transparency about water and electricity demands” of data centers throughout the country. That, they wrote, “harms the public’s ability to determine whether hosting a data center is in their best interest.”&lt;/p&gt;
    &lt;p&gt;Mount Pleasant has wanted a major development where the data center is now under construction because a massive development signed with Foxconn in 2017 largely fell through.&lt;/p&gt;
    &lt;p&gt;Local government use of NDAs and other methods to keep data center development secret is widespread across the U.S.&lt;/p&gt;
    &lt;p&gt;In Minnesota, local elected officials were aware of data center proposals for months or even years before disclosing them. In Virginia, 25 out of 31 data center projects had NDAs. In one New Mexico county, county staff negotiated for a $165 billion data center with an NDA that kept elected officials in the dark.&lt;/p&gt;
    &lt;p&gt;Several states are targeting NDAs.&lt;/p&gt;
    &lt;p&gt;At least three — Florida, Michigan and New Jersey — are considering legislation to prohibit governments from signing data center NDAs. A Georgia bill would prohibit NDAs that hide information about data center electricity or water usage. New York is considering a bill to limit NDAs for economic development proposals generally.&lt;/p&gt;
    &lt;p&gt;Now, similar legislation is pending in Wisconsin.&lt;/p&gt;
    &lt;p&gt;Last week, state Rep. Clint Moses, R-Menomonie, citing questions about transparency over the Menomonie proposal, introduced a bill to prohibit NDAs for data center proposals in Wisconsin.&lt;/p&gt;
    &lt;p&gt;“I’ve never seen such overwhelming opposition from all sides of the aisle,” he told Wisconsin Watch, describing constituents’ feelings about data centers and secrecy surrounding them.&lt;/p&gt;
    &lt;p&gt;Moses said he understands the need for confidentiality in economic development generally, but that because data centers have such widespread impact, public notice is paramount.&lt;/p&gt;
    &lt;p&gt;“The earlier the better,” he said.&lt;/p&gt;
    &lt;p&gt;Braun, the data centers coalition leader, said the public should be notified when a data center proposal is ready to be considered for approvals by elected officials — after municipal staff do due diligence to determine whether things such as zoning, utility capacity, water and sewer would make a proposal potentially viable.&lt;/p&gt;
    &lt;p&gt;Balch, who helped defeat a proposed data center in the Racine County village of Caledonia, where he lives, said the public should be alerted well before local elected officials consider such votes.&lt;/p&gt;
    &lt;p&gt;“You have to use your judgment,” he said. “But at some point, you need to realize this is not a normal thing and we need to look out for the residents.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46824098</guid><pubDate>Fri, 30 Jan 2026 13:23:53 +0000</pubDate></item><item><title>HTTP Cats</title><link>https://http.cat/</link><description>&lt;doc fingerprint="edf6e74fda190558"&gt;
  &lt;main&gt;
    &lt;p&gt;https://http.cat/[status_code]&lt;/p&gt;
    &lt;p&gt;Note: If you need an extension at the end of the URL just add .jpg.&lt;/p&gt;
    &lt;p&gt;.jpg&lt;/p&gt;
    &lt;p&gt;Continue&lt;/p&gt;
    &lt;p&gt;Switching Protocols&lt;/p&gt;
    &lt;p&gt;Processing&lt;/p&gt;
    &lt;p&gt;Early Hints&lt;/p&gt;
    &lt;p&gt;OK&lt;/p&gt;
    &lt;p&gt;Created&lt;/p&gt;
    &lt;p&gt;Accepted&lt;/p&gt;
    &lt;p&gt;Non-Authoritative Information&lt;/p&gt;
    &lt;p&gt;No Content&lt;/p&gt;
    &lt;p&gt;Reset Content&lt;/p&gt;
    &lt;p&gt;Partial Content&lt;/p&gt;
    &lt;p&gt;Multi-Status&lt;/p&gt;
    &lt;p&gt;Already Reported&lt;/p&gt;
    &lt;p&gt;Transformation Applied&lt;/p&gt;
    &lt;p&gt;IM Used&lt;/p&gt;
    &lt;p&gt;Multiple Choices&lt;/p&gt;
    &lt;p&gt;Moved Permanently&lt;/p&gt;
    &lt;p&gt;Found&lt;/p&gt;
    &lt;p&gt;See Other&lt;/p&gt;
    &lt;p&gt;Not Modified&lt;/p&gt;
    &lt;p&gt;Use Proxy&lt;/p&gt;
    &lt;p&gt;Temporary Redirect&lt;/p&gt;
    &lt;p&gt;Permanent Redirect&lt;/p&gt;
    &lt;p&gt;Bad Request&lt;/p&gt;
    &lt;p&gt;Unauthorized&lt;/p&gt;
    &lt;p&gt;Payment Required&lt;/p&gt;
    &lt;p&gt;Forbidden&lt;/p&gt;
    &lt;p&gt;Not Found&lt;/p&gt;
    &lt;p&gt;Method Not Allowed&lt;/p&gt;
    &lt;p&gt;Not Acceptable&lt;/p&gt;
    &lt;p&gt;Proxy Authentication Required&lt;/p&gt;
    &lt;p&gt;Request Timeout&lt;/p&gt;
    &lt;p&gt;Conflict&lt;/p&gt;
    &lt;p&gt;Gone&lt;/p&gt;
    &lt;p&gt;Length Required&lt;/p&gt;
    &lt;p&gt;Precondition Failed&lt;/p&gt;
    &lt;p&gt;Payload Too Large&lt;/p&gt;
    &lt;p&gt;Request-URI Too Long&lt;/p&gt;
    &lt;p&gt;Unsupported Media Type&lt;/p&gt;
    &lt;p&gt;Request Range Not Satisfiable&lt;/p&gt;
    &lt;p&gt;Expectation Failed&lt;/p&gt;
    &lt;p&gt;Iâm a teapot&lt;/p&gt;
    &lt;p&gt;Page Expired&lt;/p&gt;
    &lt;p&gt;Enhance Your Calm&lt;/p&gt;
    &lt;p&gt;Misdirected Request&lt;/p&gt;
    &lt;p&gt;Unprocessable Entity&lt;/p&gt;
    &lt;p&gt;Locked&lt;/p&gt;
    &lt;p&gt;Failed Dependency&lt;/p&gt;
    &lt;p&gt;Too Early&lt;/p&gt;
    &lt;p&gt;Upgrade Required&lt;/p&gt;
    &lt;p&gt;Precondition Required&lt;/p&gt;
    &lt;p&gt;Too Many Requests&lt;/p&gt;
    &lt;p&gt;Request Header Fields Too Large&lt;/p&gt;
    &lt;p&gt;No Response&lt;/p&gt;
    &lt;p&gt;Blocked by Windows Parental Controls&lt;/p&gt;
    &lt;p&gt;Unavailable For Legal Reasons&lt;/p&gt;
    &lt;p&gt;SSL Certificate Error&lt;/p&gt;
    &lt;p&gt;SSL Certificate Required&lt;/p&gt;
    &lt;p&gt;HTTP Request Sent to HTTPS Port&lt;/p&gt;
    &lt;p&gt;Token expired/invalid&lt;/p&gt;
    &lt;p&gt;Client Closed Request&lt;/p&gt;
    &lt;p&gt;Internal Server Error&lt;/p&gt;
    &lt;p&gt;Not Implemented&lt;/p&gt;
    &lt;p&gt;Bad Gateway&lt;/p&gt;
    &lt;p&gt;Service Unavailable&lt;/p&gt;
    &lt;p&gt;Gateway Timeout&lt;/p&gt;
    &lt;p&gt;Variant Also Negotiates&lt;/p&gt;
    &lt;p&gt;Insufficient Storage&lt;/p&gt;
    &lt;p&gt;Loop Detected&lt;/p&gt;
    &lt;p&gt;Bandwidth Limit Exceeded&lt;/p&gt;
    &lt;p&gt;Not Extended&lt;/p&gt;
    &lt;p&gt;Network Authentication Required&lt;/p&gt;
    &lt;p&gt;Web Server Is Down&lt;/p&gt;
    &lt;p&gt;Connection Timed Out&lt;/p&gt;
    &lt;p&gt;Origin Is Unreachable&lt;/p&gt;
    &lt;p&gt;SSL Handshake Failed&lt;/p&gt;
    &lt;p&gt;Site Frozen&lt;/p&gt;
    &lt;p&gt;Network Connect Timeout Error&lt;/p&gt;
    &lt;p&gt;Developed by @rogeriopvl&lt;/p&gt;
    &lt;p&gt;Original Images by Tomomi Imura (@girlie_mac)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46824422</guid><pubDate>Fri, 30 Jan 2026 13:56:51 +0000</pubDate></item><item><title>Show HN: Amla Sandbox – WASM bash shell sandbox for AI agents</title><link>https://github.com/amlalabs/amla-sandbox</link><description>&lt;doc fingerprint="a5369d909929b8a8"&gt;
  &lt;main&gt;
    &lt;p&gt;Every popular agent framework runs LLM-generated code via &lt;code&gt;subprocess&lt;/code&gt; or &lt;code&gt;exec()&lt;/code&gt;. That's arbitrary code execution on your host. One prompt injection and you're done.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Framework&lt;/cell&gt;
        &lt;cell role="head"&gt;Execution Method&lt;/cell&gt;
        &lt;cell role="head"&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LangChain&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;exec(command, globals, locals)&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;CVE-2025-68664, GitHub #5294&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;AutoGen&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;subprocess.run()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Code Executors docs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;SWE-Agent&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;subprocess.run(["bash", ...])&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;SWE-ReX&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Some frameworks offer Docker isolation (OpenHands, AutoGen), but that requires running a Docker daemon and managing container infrastructure.&lt;/p&gt;
    &lt;p&gt;amla-sandbox is a WASM sandbox with capability enforcement. Agents can only call tools you explicitly provide, with constraints you define. Sandboxed virtual filesystem. No network. No shell escape.&lt;/p&gt;
    &lt;code&gt;uv pip install "git+https://github.com/amlalabs/amla-sandbox"&lt;/code&gt;
    &lt;p&gt;No Docker. No VM. One binary, works everywhere.&lt;/p&gt;
    &lt;code&gt;from amla_sandbox import create_sandbox_tool

sandbox = create_sandbox_tool(tools=[stripe_api, database])

# Agent writes one script instead of 10 tool calls (JavaScript)
result = sandbox.run('''
    const txns = await stripe.listTransactions({customer: "cus_123"});
    const disputed = txns.filter(t =&amp;gt; t.disputed);
    console.log(disputed[0]);
''', language="javascript")

# Or with shell pipelines
result = sandbox.run('''
    tool stripe.listTransactions --customer cus_123 | jq '[.[] | select(.disputed)] | .[0]'
''', language="shell")&lt;/code&gt;
    &lt;p&gt;Tool-calling is expensive. Every MCP call is a round trip through the model:&lt;/p&gt;
    &lt;code&gt;LLM → tool → LLM → tool → LLM → tool → ...
&lt;/code&gt;
    &lt;p&gt;Ten tool calls = ten LLM invocations. Code mode collapses this:&lt;/p&gt;
    &lt;code&gt;LLM → script that does all 10 things → result
&lt;/code&gt;
    &lt;p&gt;But you can't just eval whatever the model spits out. So people either pay the token tax or run unsafe code. This gives you both: code-mode efficiency with actual isolation.&lt;/p&gt;
    &lt;p&gt;The sandbox runs inside WebAssembly with WASI for a minimal syscall interface. WASM provides memory isolation by design—linear memory is bounds-checked, and there's no way to escape to the host address space. The wasmtime runtime we use is built with defense-in-depth and has been formally verified for memory safety.&lt;/p&gt;
    &lt;p&gt;On top of WASM isolation, every tool call goes through capability validation:&lt;/p&gt;
    &lt;code&gt;from amla_sandbox import Sandbox, MethodCapability, ConstraintSet, Param

sandbox = Sandbox(
    capabilities=[
        MethodCapability(
            method_pattern="stripe/charges/*",
            constraints=ConstraintSet([
                Param("amount") &amp;lt;= 10000,
                Param("currency").is_in(["USD", "EUR"]),
            ]),
            max_calls=100,
        ),
    ],
    tool_handler=my_handler,
)

# This works
sandbox.execute('await stripe.charges.create({amount: 500, currency: "USD"})')

# This fails - amount exceeds capability
sandbox.execute('await stripe.charges.create({amount: 50000, currency: "USD"})')&lt;/code&gt;
    &lt;p&gt;The design draws from capability-based security as implemented in systems like seL4—access is explicitly granted, not implicitly available. Agents don't get ambient authority just because they're running in your process. This matters because prompt injection is a fundamental unsolved problem; defense in depth through capability restriction limits the blast radius.&lt;/p&gt;
    &lt;code&gt;from amla_sandbox import create_sandbox_tool

sandbox = create_sandbox_tool()

# JavaScript
sandbox.run("console.log('hello'.toUpperCase())", language="javascript")  # -&amp;gt; "HELLO"

# Shell
sandbox.run("echo 'hello' | tr 'a-z' 'A-Z'", language="shell")  # -&amp;gt; "HELLO"

# With tools
def get_weather(city: str) -&amp;gt; dict:
    return {"city": city, "temp": 72}

sandbox = create_sandbox_tool(tools=[get_weather])
sandbox.run("const w = await get_weather({city: 'SF'}); console.log(w);", language="javascript")&lt;/code&gt;
    &lt;p&gt;With constraints:&lt;/p&gt;
    &lt;code&gt;sandbox = create_sandbox_tool(
    tools=[transfer_money],
    constraints={
        "transfer_money": {
            "amount": "&amp;lt;=1000",
            "currency": ["USD", "EUR"],
        },
    },
    max_calls={"transfer_money": 10},
)&lt;/code&gt;
    &lt;p&gt;Tools require object syntax:&lt;/p&gt;
    &lt;code&gt;// WORKS - tools always take an object argument
await get_weather({city: "SF"});
await transfer({to: "alice", amount: 500});

// FAILS - positional arguments don't work
await get_weather("SF");  // Error: argument after ** must be a mapping&lt;/code&gt;
    &lt;p&gt;Use &lt;code&gt;return&lt;/code&gt; or &lt;code&gt;console.log()&lt;/code&gt; for output:&lt;/p&gt;
    &lt;code&gt;// Return value is captured and output
return await get_weather({city: "SF"});  // -&amp;gt; {"city":"SF","temp":72}
return {a: 1, b: 2};  // -&amp;gt; {"a":1,"b":2}
return "hello";  // -&amp;gt; hello (strings not double-quoted)

// console.log also works
console.log(JSON.stringify({a: 1}));  // -&amp;gt; {"a":1}

// No return = no output
const x = 42;  // -&amp;gt; (no output)&lt;/code&gt;
    &lt;p&gt;VFS is writable only under /workspace and /tmp:&lt;/p&gt;
    &lt;code&gt;// WORKS - /workspace and /tmp are ReadWrite
await fs.writeFile('/workspace/data.json', '{}');
await fs.mkdir('/tmp/cache');

// FAILS - root is read-only
await fs.mkdir('/mydir');  // EACCES: Permission denied&lt;/code&gt;
    &lt;p&gt;For LangGraph integration:&lt;/p&gt;
    &lt;code&gt;from langgraph.prebuilt import create_react_agent
from langchain_anthropic import ChatAnthropic
from amla_sandbox import create_sandbox_tool

sandbox = create_sandbox_tool(tools=[get_weather, search_db])
agent = create_react_agent(
    ChatAnthropic(model="claude-sonnet-4-20250514"),
    [sandbox.as_langchain_tool()]  # LLM writes JS/shell that calls your tools
)&lt;/code&gt;
    &lt;p&gt;For fine-grained capability control:&lt;/p&gt;
    &lt;code&gt;from amla_sandbox import SandboxTool, MethodCapability, ConstraintSet, Param

caps = [
    MethodCapability(
        method_pattern="mcp:search_db",
        constraints=ConstraintSet([Param("query").starts_with("SELECT")]),
        max_calls=5,
    )
]

sandbox_tool = SandboxTool.from_functions([search_db], capabilities=caps)
agent = create_react_agent(model, [sandbox_tool.as_langchain_tool()])&lt;/code&gt;
    &lt;code&gt;┌────────────────────────────────────────────────┐
│              WASM Sandbox                      │
│  ┌──────────────────────────────────────────┐  │
│  │         Async Scheduler                  │  │
│  │   tasks waiting/running/ready            │  │
│  └──────────────────────────────────────────┘  │
│  ┌────────────┐ ┌──────────┐ ┌──────────────┐  │
│  │  VFS       │ │ Shell    │ │ Capabilities │  │
│  │ /workspace │ │ builtins │ │ validation   │  │
│  └────────────┘ └──────────┘ └──────────────┘  │
│                    ↓ yield                     │
└════════════════════════════════════════════════┘
                     │
                     ▼
┌─────────────────────────────────────────────┐
│              Python Host                    │
│                                             │
│   while sandbox.has_work():                 │
│       req = sandbox.step()  # tool call     │
│       sandbox.resume(execute(req))          │
│                                             │
└─────────────────────────────────────────────┘
&lt;/code&gt;
    &lt;p&gt;The sandbox yields on tool calls. Host executes them (after capability checks) and resumes. QuickJS runs inside WASM for the JS runtime.&lt;/p&gt;
    &lt;p&gt;First run compiles the WASM module (~300ms). Cache it:&lt;/p&gt;
    &lt;code&gt;amla-precompile&lt;/code&gt;
    &lt;p&gt;Subsequent loads: ~0.5ms.&lt;/p&gt;
    &lt;code&gt;from amla_sandbox import Param, ConstraintSet

constraints = ConstraintSet([
    Param("amount") &amp;gt;= 100,
    Param("amount") &amp;lt;= 10000,
    Param("currency").is_in(["USD", "EUR"]),
    Param("path").starts_with("/api/"),
])&lt;/code&gt;
    &lt;p&gt;Pattern matching for method names:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;stripe/charges/create&lt;/code&gt;— exact match&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;stripe/charges/*&lt;/code&gt;— single path segment&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;stripe/**&lt;/code&gt;— zero or more segments&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What you get: Isolation without infrastructure. Capability enforcement. Token efficiency.&lt;/p&gt;
    &lt;p&gt;What you don't get: Full Linux environment. Native module support. GPU access. Infinite loop protection (a &lt;code&gt;while(true){}&lt;/code&gt; will hang - the step limit only counts WASM yields, not JS instructions).&lt;/p&gt;
    &lt;p&gt;If you need a real VM with persistent state and arbitrary dependencies, use e2b or Modal. amla-sandbox is for the common case: agents running generated code with controlled tool access.&lt;/p&gt;
    &lt;p&gt;Python code is MIT. The WASM binary is currently proprietary—you can use it freely with this package, but you can't extract or redistribute it separately. We're working on open sourcing the WASM runtime.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46824877</guid><pubDate>Fri, 30 Jan 2026 14:34:32 +0000</pubDate></item><item><title>Buttered Crumpet, a custom typeface for Wallace and Gromit</title><link>https://jamieclarketype.com/case-study/wallace-and-gromit-font/</link><description>&lt;doc fingerprint="4dde0173bbd23507"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Wallace and Gromit Font&lt;/head&gt;
    &lt;head rend="h2"&gt;Categories:&lt;/head&gt;
    &lt;p&gt;A new typeface for Aardman’s iconic duo – meet Buttered Crumpet.&lt;/p&gt;
    &lt;head rend="h2"&gt;Overview&lt;/head&gt;
    &lt;p&gt;I was thrilled to be selected to design a custom typeface for Wallace &amp;amp; Gromit – Aardman’s most beloved and recognisable characters.&lt;/p&gt;
    &lt;p&gt;The brief called for a font with a distinct tone of voice that could work seamlessly across film, print and digital, while bringing warmth and continuity to their next chapter.&lt;/p&gt;
    &lt;head rend="h2"&gt;Method&lt;/head&gt;
    &lt;p&gt;We began by exploring warm, characterful styles, taking inspiration from Oswald Cooper’s original drawings for Cooper Black. We then took a creative turn, developing a softer, low-contrast design with a distinctly hand-crafted feel.&lt;/p&gt;
    &lt;p&gt;Each letterform was carefully shaped to feel expressive yet balanced, with serifs that resemble loaves of bread – a nod to Aardman’s tactile, playful world.&lt;/p&gt;
    &lt;head rend="h2"&gt;Outcomes&lt;/head&gt;
    &lt;p&gt;The finished typeface – Buttered Crumpet – gives Aardman a timeless, familiar tone of voice with bundles of charm. It includes over 200 characters, covering all Western European languages, and was designed in a single, carefully crafted weight with room for future expansion.&lt;/p&gt;
    &lt;p&gt;As a Bristol-based designer, it was a joy to create a lasting connection with my home city and one of its most renowned creative studios.&lt;/p&gt;
    &lt;quote&gt;I’ve loved rolling out this typeface and we’re starting to see it in action now. There have been lots of compliments.&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46825415</guid><pubDate>Fri, 30 Jan 2026 15:19:28 +0000</pubDate></item><item><title>Self Driving Car Insurance</title><link>https://www.lemonade.com/car/explained/self-driving-car-insurance/</link><description>&lt;doc fingerprint="4400d9e790b4d31"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How does Lemonade Autonomous Car insurance work?&lt;/head&gt;
    &lt;p&gt;Lemonade is the first to reward safer self-driving miles with real discounts.&lt;/p&gt;
    &lt;p&gt;Lemonade is the first to reward safer self-driving miles with real discounts.&lt;/p&gt;
    &lt;p&gt;Lemonade Autonomous Car insurance is the first car insurance designed specifically for self-driving cars. It offers Tesla owners 50% off every mile driven using Full Self-Driving technology by automatically tracking FSD miles versus manual miles through direct Tesla integration.&lt;/p&gt;
    &lt;p&gt;Lemonade Autonomous Car insurance is built on a simple principle: Tesla’s data shows that Full Self-Driving miles are twice as safe as manual driving, so they should cost 50% less on your insurance premiums.&lt;/p&gt;
    &lt;p&gt;When you drive using Tesla’s FSD technology, our system automatically tracks those miles and applies the discount. Manual miles are priced normally, while your FSD miles get the full 50% reduction, creating a pricing model that actually reflects the safety benefits of autonomous driving.&lt;/p&gt;
    &lt;p&gt;Unlike traditional car insurance that relies on estimates, Lemonade connects directly to your Tesla using Tesla’s Fleet API, which gives Lemonade access to vehicle data with a customer’s permission. This means:&lt;/p&gt;
    &lt;p&gt;The technology handles everything behind the scenes, so you just drive and save.&lt;/p&gt;
    &lt;p&gt;Lemonade Autonomous Car insurance is currently available for Tesla drivers in Arizona and launching in Oregon on February 26, 2026, with more states coming soon. To qualify, your Tesla needs:&lt;/p&gt;
    &lt;p&gt;Traditional insurers might offer small discounts for safety features like adaptive cruise control or lane-keeping assistance, but they don’t distinguish between these basic driver assistance systems and sophisticated autonomous driving technology.&lt;/p&gt;
    &lt;p&gt;Lemonade recognizes that Tesla’s Full Self-Driving represents a fundamentally different level of automation, one that deserves a fundamentally different insurance approach.&lt;/p&gt;
    &lt;p&gt;Tesla reports that FSD technology leads to:&lt;/p&gt;
    &lt;p&gt;These aren’t theoretical benefits, they’re real safety improvements that Lemonade’s pricing reflects. When autonomous driving technology demonstrably reduces risk, insurance costs should follow.&lt;/p&gt;
    &lt;p&gt;Instead of estimating your annual mileage and hoping for the best, Lemonade’s system knows exactly how many miles you drive in FSD mode versus manual mode. This usage-based insurance model means you pay precisely for the risk you represent.&lt;/p&gt;
    &lt;p&gt;As car manufacturers like Ford, GM, and others advance their autonomous driving technology, Lemonade is building the infrastructure to support them. Our approach with Tesla creates the foundation for insuring all types of self-driving vehicles as they become available.&lt;/p&gt;
    &lt;p&gt;If you own a Tesla with Hardware 4.0 or higher in an eligible state, you can start saving immediately with Lemonade Autonomous Car insurance. The 50% discount applies to every mile you drive using Full Self-Driving, potentially reducing your overall insurance costs significantly depending on how often you use the technology.&lt;/p&gt;
    &lt;p&gt;Lemonade Autonomous Car insurance works alongside our other products, so Tesla owners can bundle their autonomous coverage with homeowners, renters, pet, or term life insurance for additional discounts.&lt;/p&gt;
    &lt;p&gt;Getting started with Lemonade Autonomous Car insurance is straightforward:&lt;/p&gt;
    &lt;p&gt;Everything is managed through the Lemonade app, with transparent tracking of your FSD miles and savings.&lt;/p&gt;
    &lt;p&gt;Whether you’re a current Tesla owner interested in Lemonade Autonomous Car insurance or considering buying a Tesla to take advantage of this innovative coverage, Lemonade makes the process simple.&lt;/p&gt;
    &lt;p&gt;Our autonomous car insurance represents the future of auto insurance, pricing that actually reflects the safety benefits of new technology, seamless integration with cutting-edge vehicles, and transparent, fair coverage that evolves with your driving.&lt;/p&gt;
    &lt;p&gt;Ready to see how much you could save with the first insurance designed specifically for autonomous vehicles? Get a quote today and discover why safer miles should cost less.&lt;/p&gt;
    &lt;p&gt;You’ll save 50% on every mile driven using Tesla’s Full Self-Driving technology. Your total savings depend on how often you use FSD versus manual driving, but many drivers see significant reductions in their overall insurance premiums.&lt;/p&gt;
    &lt;p&gt;No special policies are required: Lemonade Autonomous Car insurance integrates with regular car insurance requirements. We simply price your FSD miles at 50% less than manual miles because they’re demonstrably safer.&lt;/p&gt;
    &lt;p&gt;Your Tesla needs Hardware 4.0 or higher and firmware version 2025.44.25.5 or newer. The firmware update is free and easy to install through your Tesla app or directly in your car.&lt;/p&gt;
    &lt;p&gt;Existing customers can add autonomous coverage at their next renewal. We don’t recommend canceling your current policy early, as you’d pay unnecessary fees for a new policy setup.&lt;/p&gt;
    &lt;p&gt;We’re working to expand to more states as quickly as possible while ensuring compliance with local regulations. Follow our updates for announcements about new state availability.&lt;/p&gt;
    &lt;p&gt;Please note: Lemonade articles and other editorial content are meant for educational purposes only, and should not be relied upon instead of professional legal, insurance or financial advice. The content of these educational articles does not alter the terms, conditions, exclusions, or limitations of policies issued by Lemonade, which differ according to your state of residence. While we regularly review previously published content to ensure it is accurate and up-to-date, there may be instances in which legal conditions or policy details have changed since publication. Any hypothetical examples used in Lemonade editorial content are purely expositional. Hypothetical examples do not alter or bind Lemonade to any application of your insurance policy to the particular facts and circumstances of any actual claim.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46825828</guid><pubDate>Fri, 30 Jan 2026 15:50:15 +0000</pubDate></item><item><title>Kimi K2.5 Technical Report [pdf]</title><link>https://github.com/MoonshotAI/Kimi-K2.5/blob/master/tech_report.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46826597</guid><pubDate>Fri, 30 Jan 2026 16:43:50 +0000</pubDate></item><item><title>Mamdani to kill the NYC AI chatbot caught telling businesses to break the law</title><link>https://themarkup.org/artificial-intelligence/2026/01/30/mamdani-to-kill-the-nyc-ai-chatbot-we-caught-telling-businesses-to-break-the-law</link><description>&lt;doc fingerprint="bb9f88c987e30fad"&gt;
  &lt;main&gt;
    &lt;p&gt;The Markup, now a part of CalMatters, uses investigative reporting, data analysis, and software engineering to challenge technology to serve the public good. Sign up for Klaxon, a newsletter that delivers our stories and tools directly to your inbox.&lt;/p&gt;
    &lt;p&gt;This article is co-reported with THE CITY, a non-profit newsroom that serves the people of New York. Sign up for its newsletter, The Scoop.&lt;/p&gt;
    &lt;p&gt;In a press conference this week on New York City’s $12 billion budget gap, Mayor Zohran Mamdani zeroed in on the previous administration’s artificial intelligence chatbot as one of “a number of different things we’re going to pursue for savings.”&lt;/p&gt;
    &lt;p&gt;The chatbot, which was released by the Eric Adams administration in fall of 2023, was meant to provide business owners with an accessible way to check city rules and regulations. But as first documented by The Markup and THE CITY, the bot provided answers that, if followed, would lead to illegal behavior by businesses, like taking a cut of employees’ tips.&lt;/p&gt;
    &lt;p&gt;A spokesperson for the mayor, Dora Pekec, confirmed in a text message that the new administration plans to take down the chatbot. She said a member of the Mamdani transition team had seen reporting on the bot from The Markup and THE CITY and presented it to the mayor as a possible place to save funds.&lt;/p&gt;
    &lt;p&gt;Artificial Intelligence&lt;/p&gt;
    &lt;head rend="h3"&gt;NYC’s AI Chatbot Tells Businesses to Break the Law&lt;/head&gt;
    &lt;p&gt;The Microsoft-powered bot says bosses can take workers’ tips and that landlords can discriminate based on source of income&lt;/p&gt;
    &lt;p&gt;At the press conference, Mamdani blamed Adams for the budget shortfall, saying he had been handed “a poisoned chalice.” To close the deficit, he said he would raise taxes on the wealthy and corporations and look “under the hood” of the city’s budget for potential savings.&lt;/p&gt;
    &lt;p&gt;When pressed by reporters on what he might cut, he singled out the chatbot.&lt;/p&gt;
    &lt;p&gt;“The previous administration had an AI chatbot that was functionally unusable,” Mamdani said. “It was costing the administration around half a million dollars. That, in and of itself, is not something that can bridge this kind of a gap, but it’s an indication of the ways in which money has been spent while refusing to account for the actual costs of what these programs are.”&lt;/p&gt;
    &lt;p&gt;The bot, built using Microsoft’s cloud computing platform, was part of an ambitious overhaul of digital services in New York called MyCity. The project was meant to streamline access to government but was criticized for relying on outside contractors.&lt;/p&gt;
    &lt;p&gt;It wasn’t clear how much it cost to maintain the chatbot. Just building the bot’s foundations reportedly cost nearly $600,000, close to the figure Mamdani provided. Pekec said they didn’t yet have a date for taking down the bot.&lt;/p&gt;
    &lt;head rend="h2"&gt;A broken bot&lt;/head&gt;
    &lt;p&gt;Testing by The Markup and THE CITY in 2024 showed that, despite promises from the Adams administration, the chatbot would confidently provide incorrect and potentially harmful information to visitors, even on high-stakes topics.&lt;/p&gt;
    &lt;p&gt;When asked about housing policy, for example, the bot suggested landlords could discriminate against tenants with Section 8 vouchers. Despite being an intended resource for business owners, the bot didn’t know the minimum wage, and told users it was fine to refuse to accept cash for payment despite a city law to the contrary, enacted in 2020.&lt;/p&gt;
    &lt;p&gt;After The Markup and THE CITY’s initial report was published, readers continued peppering the bot with sometimes farcical questions, which it continued failing to answer properly. The Adams administration defended the bot, saying it would improve over time.&lt;/p&gt;
    &lt;p&gt;“We’re identifying what the problems are, we’re gonna fix them, and we’re going to have the best chatbot system on the globe,” Adams said at a press conference. “People are going to come and watch what we’re doing in New York City.”&lt;/p&gt;
    &lt;p&gt;City administrators soon added disclaimers to the bot advising users to “not use its responses as legal or professional advice.” They also improved some of the bot’s answers, but also appeared to limit the kinds of questions the tool was willing to answer.&lt;/p&gt;
    &lt;p&gt;Today, the bot advises visitors to “ask an NYC government question only” and cautions that “responses may occasionally produce inaccurate or incomplete content.” Visitors must agree to accept the bot’s limitations before using it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46827665</guid><pubDate>Fri, 30 Jan 2026 18:02:23 +0000</pubDate></item><item><title>A judge gave the FBI permission to attempt to bypass biometrics</title><link>https://theintercept.com/2026/01/30/washington-post-hannah-natanson-fbi-biometrics-unlock-phone/</link><description>&lt;doc fingerprint="de55931016f73a98"&gt;
  &lt;main&gt;
    &lt;p&gt;The recent federal raid on the home of Washington Post reporter Hannah Natanson isn’t merely an attack by the Trump administration on the free press. It’s also a warning to anyone with a smartphone.&lt;/p&gt;
    &lt;p&gt;Included in the search and seizure warrant for the raid on Natanson’s home is a section titled “Biometric Unlock,” which explicitly authorized law enforcement personnel to obtain Natanson’s phone and both hold the device in front of her face and to forcibly use her fingers to unlock it. In other words, a judge gave the FBI permission to attempt to bypass biometrics: the convenient shortcuts that let you unlock your phone by scanning your fingerprint or face.&lt;/p&gt;
    &lt;p&gt;It is not clear if Natanson used biometric authentication on her devices, or if the law enforcement personnel attempted to use her face or fingers to unlock her devices. Natanson and the Washington Post did not respond to multiple requests for comment. The FBI declined to comment.&lt;/p&gt;
    &lt;p&gt;Natanson has not been charged with a crime. Investigators searched her home in connection with alleged communication between her and government contractor Aurelio Luis Perez-Lugones, who was initially charged with unlawfully retaining national defense information. Prosecutors recently added new charges including multiple counts of transmission of defense information to an unauthorized person. Attorneys for Perez-Lugones did not comment.&lt;/p&gt;
    &lt;p&gt;The warrant included a few stipulations limiting law enforcement personnel. Investigators were not authorized to ask Natanson details about what kind of biometric authentication she may have used on her devices. For instance, the warrant explicitly stated they could not ask Natanson which specific finger she uses for biometrics, if any. Although if Natanson were to voluntarily provide any such information, that would be allowed, according to the warrant.&lt;/p&gt;
    &lt;p&gt;Andrew Crocker, surveillance litigation director at the Electronic Frontier Foundation, told The Intercept that while the EFF has “seen warrants that authorize police to compel individuals to unlock their devices using biometrics in the past,” the caveat mandating that the subject of the search cannot be asked for specifics about their biometric setup is likely influenced by recent case law. “Last year the D.C. Circuit held that biometric unlocking can be a form of ‘testimony’ that is protected by the 5th Amendment,” Crocker said. This is especially the case when a person is “forced to demonstrate which finger unlocks the device.”&lt;/p&gt;
    &lt;p&gt;Crocker said that he “would like to see courts treat biometric locks as equivalent to password protection from a constitutional standpoint. Your constitutional right against self-incrimination should not be dependent on technical convenience or lack thereof.”&lt;/p&gt;
    &lt;p&gt;Activists and journalists have long been cautioned to disable biometrics in specific situations where they might face heightened risk of losing control of their phones, say when attending a protest or crossing a border. Martin Shelton, deputy director of digital security at Freedom of the Press Foundation, advised “journalists to disable biometrics when they expect to be in a situation where they expect a possible search.”&lt;/p&gt;
    &lt;p&gt;Instead of using biometrics, it’s safest to unlock your devices using an alphanumeric passphrase (a device protected solely by a passcode consisting of numbers is generally easier to access). There are numerous other safeguards to take if there’s a possibility your home may be raided, such as turning off your phone before going to bed, which puts it into an encrypted state until the next time it’s unlocked.&lt;/p&gt;
    &lt;p&gt;That said, there are a few specific circumstances when biometric-based authentication methods might make sense from a privacy perspective — such as in a public place where someone might spy on your passphrase over your shoulder.&lt;/p&gt;
    &lt;p&gt;IT’S EVEN WORSE THAN WE THOUGHT.&lt;/p&gt;
    &lt;p&gt;What we’re seeing right now from Donald Trump is a full-on authoritarian takeover of the U.S. government.&lt;/p&gt;
    &lt;p&gt;This is not hyperbole.&lt;/p&gt;
    &lt;p&gt;Court orders are being ignored. MAGA loyalists have been put in charge of the military and federal law enforcement agencies. The Department of Government Efficiency has stripped Congress of its power of the purse. News outlets that challenge Trump have been banished or put under investigation.&lt;/p&gt;
    &lt;p&gt;Yet far too many are still covering Trump’s assault on democracy like politics as usual, with flattering headlines describing Trump as “unconventional,” “testing the boundaries,” and “aggressively flexing power.”&lt;/p&gt;
    &lt;p&gt;The Intercept has long covered authoritarian governments, billionaire oligarchs, and backsliding democracies around the world. We understand the challenge we face in Trump and the vital importance of press freedom in defending democracy.&lt;/p&gt;
    &lt;head rend="h2"&gt;We’re independent of corporate interests. Will you help us?&lt;/head&gt;
    &lt;p&gt;IT’S BEEN A DEVASTATING year for journalism — the worst in modern U.S. history.&lt;/p&gt;
    &lt;p&gt;We have a president with utter contempt for truth aggressively using the government’s full powers to dismantle the free press. Corporate news outlets have cowered, becoming accessories in Trump’s project to create a post-truth America. Right-wing billionaires have pounced, buying up media organizations and rebuilding the information environment to their liking.&lt;/p&gt;
    &lt;p&gt;In this most perilous moment for democracy, The Intercept is fighting back. But to do so effectively, we need to grow.&lt;/p&gt;
    &lt;p&gt;That’s where you come in. Will you help us expand our reporting capacity in time to hit the ground running in 2026?&lt;/p&gt;
    &lt;head rend="h2"&gt;We’re independent of corporate interests. Will you help us?&lt;/head&gt;
    &lt;p&gt;I’M BEN MUESSIG, The Intercept’s editor-in-chief. It’s been a devastating year for journalism — the worst in modern U.S. history.&lt;/p&gt;
    &lt;p&gt;We have a president with utter contempt for truth aggressively using the government’s full powers to dismantle the free press. Corporate news outlets have cowered, becoming accessories in Trump’s project to create a post-truth America. Right-wing billionaires have pounced, buying up media organizations and rebuilding the information environment to their liking.&lt;/p&gt;
    &lt;p&gt;In this most perilous moment for democracy, The Intercept is fighting back. But to do so effectively, we need to grow.&lt;/p&gt;
    &lt;p&gt;That’s where you come in. Will you help us expand our reporting capacity in time to hit the ground running in 2026?&lt;/p&gt;
    &lt;head rend="h2"&gt;We’re independent of corporate interests. Will you help us?&lt;/head&gt;
    &lt;head rend="h2"&gt;Latest Stories&lt;/head&gt;
    &lt;p&gt;Israel’s War on Gaza&lt;/p&gt;
    &lt;head rend="h3"&gt;Israeli Military Found Gaza Health Ministry Death Toll Was Accurate. Will These Deniers Admit It?&lt;/head&gt;
    &lt;p&gt;Denial of the Gaza Health Ministry’s death toll helped buoy American support for Israel’s genocide.&lt;/p&gt;
    &lt;p&gt;Israel’s War on Gaza&lt;/p&gt;
    &lt;head rend="h3"&gt;Zohran Mamdani Wants NYC to Divest From Israel — But New Comptroller Pledges to Buy War Bonds&lt;/head&gt;
    &lt;p&gt;A human rights group fanned the flames of conflict by threatening legal action if the city invested in war crimes.&lt;/p&gt;
    &lt;p&gt;The Intercept Briefing&lt;/p&gt;
    &lt;head rend="h3"&gt;Even the Top Prosecutor in Minneapolis Doesn’t Know the Identity of the Agents Who Killed Alex Pretti&lt;/head&gt;
    &lt;p&gt;Mary Moriarty on steps local officials are taking to collect and preserve evidence despite federal obstruction, and Jill Garvey on how to document ICE safely.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46828881</guid><pubDate>Fri, 30 Jan 2026 19:38:44 +0000</pubDate></item><item><title>Antirender: remove the glossy shine on architectural renderings</title><link>https://antirender.com/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46829147</guid><pubDate>Fri, 30 Jan 2026 20:05:24 +0000</pubDate></item><item><title>Silver plunges 30% in worst day since 1980, gold tumbles</title><link>https://www.cnbc.com/2026/01/30/silver-gold-fall-price-usd-dollar-fed-warsh-chair-trump-metals.html</link><description>&lt;doc fingerprint="5a669b173db667ae"&gt;
  &lt;main&gt;
    &lt;p&gt;Gold and silver prices plunged Friday, as President Donald Trump's nomination for the next chair of the Federal Reserve, Kevin Warsh, appeared to relieve concerns about the central bank's independence and sent the dollar soaring.&lt;/p&gt;
    &lt;p&gt;Spot silver was down 28% at $83.45 an ounce, trading near its lows of the day. Silver futures plummeted 31.4% to settle at $78.53, marking its worst day since March 1980.&lt;/p&gt;
    &lt;p&gt;Meanwhile, spot gold shed around 9% to trade at $4,895.22 an ounce. Gold futures dropped 11.4% to settle at $4,745.10.&lt;/p&gt;
    &lt;p&gt;The sharp moves down were initially triggered by reports of Warsh's nomination. However, they gained steam in afternoon U.S. trading as investors who piled into the metals raced to book profits. Metals were also under pressure as the dollar spiked higher, making it more expensive for foreign investors to buy gold and silver and spoiling the theory that metals would replace the greenback as the globe's reserve currency.&lt;/p&gt;
    &lt;p&gt;The dollar index last traded around 0.8% higher.&lt;/p&gt;
    &lt;p&gt;"This is getting crazy," said Matt Maley, equity strategist at Miller Tabak. "Most of this is probably 'forced selling.' This has been the hottest asset for day traders and other short-term traders recently. So, there has been some leverage built up in silver. With the huge decline today, the margin calls went out."&lt;/p&gt;
    &lt;head rend="h2"&gt;Trump picks Warsh&lt;/head&gt;
    &lt;p&gt;National Economic Council Director Kevin Hassett had been the favorite to replace Powell for some time, but Warsh became the front-runner in prediction markets in recent days.&lt;/p&gt;
    &lt;p&gt;In a note on Friday morning, Evercore ISI's Krishna Guha said the market was "trading Warsh hawkish."&lt;/p&gt;
    &lt;p&gt;"The Warsh pick should help stabilize the dollar some and reduce (though not eliminate) the asymmetric risk of deep extended dollar weakness by challenging debasement trades – which is also why gold and silver are sharply lower," the firm's vice chairman said.&lt;/p&gt;
    &lt;p&gt;"But, we advise against overdoing the Warsh hawkish trade across asset markets – and even see some risk of a whipsaw. We see Warsh as a pragmatist not an ideological hawk in the tradition of the independent conservative central banker."&lt;/p&gt;
    &lt;p&gt;Claudio Wewel, FX strategist at J. Safra Sarasin Sustainable Asset Management, told CNBC's "Squawk Box Europe" on Friday that a "perfect storm" of geopolitical tensions had helped precious metals move higher this year, pointing to the U.S. capture of Venezuelan President Nicolás Maduro and Washington's threats to use military force in Greenland and Iran.&lt;/p&gt;
    &lt;p&gt;More recently, he said, speculation over who would be nominated as the next Fed chair had been influencing metals markets.&lt;/p&gt;
    &lt;p&gt;"The market has clearly been pricing the risk of a much more dovish contender, that's been largely helping the gold price along with other precious metal prices. Over the last 24 hours, the news flow has changed a little bit," Wewel said, prior to Trump's announcement.&lt;/p&gt;
    &lt;head rend="h2"&gt;'Even good assets can sell-off'&lt;/head&gt;
    &lt;p&gt;Gold and silver both enjoyed record-smashing rallies in 2025, surging 66% and 135%, respectively, over the course of the year.&lt;/p&gt;
    &lt;p&gt;Coeur Mining lost 17%. Silver ETFs were dragged into the action, with the ProShares Ultra Silver fund last seen more than 62% lower. The iShares Silver Trust ETF lost 31%. Both funds were headed for their worst days on record.&lt;/p&gt;
    &lt;p&gt;Precious metals have been on a stellar rally over the past 12 months, amid broader market volatility, the decline of the U.S. dollar, bubbling geopolitical tensions and concerns about the independence of the Federal Reserve.&lt;/p&gt;
    &lt;p&gt;Katy Stoves, investment manager at British wealth management firm Mattioli Woods, told CNBC on Friday morning that the moves were likely "a market-wide reassessment of concentration risk."&lt;/p&gt;
    &lt;p&gt;"Just as tech stocks — particularly AI-related names — have dominated market attention and capital flows, gold has similarly seen intense positioning and crowding," she said. "When everyone is leaning the same way, even good assets can sell off as positions get unwound. The parallel isn't accidental: both represent areas where capital has flooded in based on powerful narratives, and concentrated positions eventually face their day of reckoning."&lt;/p&gt;
    &lt;p&gt;Meanwhile, Toni Meadows, head of investment at BRI Wealth Management, contended that gold's run to the $5,000 mark had happened "too easily." He noted that the unwinding of the greenback had supported gold prices, but that the dollar had appeared to stabilize.&lt;/p&gt;
    &lt;p&gt;"Central bank buying has driven the longer-term rally but this has tailed off in recent months," he said. "The case for further reserve diversification is still there though as Trump's trade policies and intervention in foreign affairs will make a lot of countries nervous about holding U.S. assets, especially those countries in the emerging markets or aligned to China or Russia. Silver will mirror the direction of gold, so it is not surprising to see falls there."&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46829548</guid><pubDate>Fri, 30 Jan 2026 20:37:48 +0000</pubDate></item><item><title>Peerweb: Decentralized website hosting via WebTorrent</title><link>https://peerweb.lol/</link><description>&lt;doc fingerprint="868e3ff18d2cd634"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;🪐 PeerWeb&lt;/head&gt;
    &lt;head rend="h2"&gt;Decentralized Website Hosting via WebTorrent&lt;/head&gt;
    &lt;head rend="h3"&gt;🤔 What is PeerWeb?&lt;/head&gt;
    &lt;p&gt;PeerWeb is a revolutionary way to host and share websites using WebTorrent technology. Instead of relying on centralized servers, websites are distributed across a peer-to-peer network, making them censorship-resistant and always available. 🌍✨&lt;/p&gt;
    &lt;head rend="h3"&gt;📤 Quick Upload&lt;/head&gt;
    &lt;head rend="h4"&gt;Drag &amp;amp; Drop Your Website&lt;/head&gt;
    &lt;p&gt;Drop a folder with your website files&lt;/p&gt;
    &lt;head rend="h3"&gt;📚 How to Use PeerWeb&lt;/head&gt;
    &lt;head rend="h3"&gt;💡 Load Existing Site&lt;/head&gt;
    &lt;p&gt;To load a website from a torrent hash, enter it below:&lt;/p&gt;
    &lt;p&gt;🎯 Just the hash! PeerWeb automatically adds the magnet link prefix and trackers.&lt;/p&gt;
    &lt;head rend="h3"&gt;🧪 Demos&lt;/head&gt;
    &lt;p&gt; Functionality test page: &lt;lb/&gt;https://peerweb.lol/?orc=90c020bd252639622a14895a0fad713b91e0130c &lt;/p&gt;
    &lt;p&gt; SomaFM on PeerWeb:&lt;lb/&gt;https://peerweb.lol/?orc=908d19242ae1461f333a516d1f8b89c13ef2d259 &lt;/p&gt;
    &lt;p&gt; Chess on PeerWeb:&lt;lb/&gt;https://peerweb.lol/?orc=1e14b1ba7fcd03e5f165d53ed8223a333349db04 &lt;/p&gt;
    &lt;p&gt; Text Editor app on PeerWeb:&lt;lb/&gt;https://peerweb.lol/?orc=4e5f1204dcec68195bfcc89f9410a0b70a0ddfac &lt;/p&gt;
    &lt;head rend="h3"&gt;🐛 Debug Mode&lt;/head&gt;
    &lt;p&gt;For developers and troubleshooting, add &amp;amp;debug=true to see detailed progress:&lt;/p&gt;
    &lt;code&gt;https://peerweb.lol?orc=ABC123DEF456...&amp;amp;debug=true&lt;/code&gt;
    &lt;head rend="h3"&gt;🚀 Advanced Options&lt;/head&gt;
    &lt;head rend="h3"&gt;💾 Smart Caching&lt;/head&gt;
    &lt;p&gt;PeerWeb caches visited sites for lightning-fast loading! 🚀&lt;/p&gt;
    &lt;head rend="h3"&gt;🛡️ Security Features&lt;/head&gt;
    &lt;p&gt;Enhanced security with DOMPurify integration! 🔒&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46829582</guid><pubDate>Fri, 30 Jan 2026 20:40:00 +0000</pubDate></item><item><title>DHS ramps up surveillance in immigration raids, sweeping in citizens</title><link>https://apnews.com/article/digital-crackdown-immigration-minneapolis-trump-52662450a15a7be8d9df69036f9b5f12</link><description>&lt;doc fingerprint="3c219ed009a338d9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;DHS ramps up surveillance in immigration raids, sweeping in citizens&lt;/head&gt;
    &lt;head rend="h2"&gt;DHS ramps up surveillance in immigration raids, sweeping in citizens&lt;/head&gt;
    &lt;p&gt;Luis Martinez was on his way to work on a frigid Minneapolis morning when federal agents suddenly boxed him in, forcing the SUV he was driving to a dead stop in the middle of the street.&lt;/p&gt;
    &lt;p&gt;Masked agents rapped on the window, demanding Martinez produce his ID. Then one held his cellphone inches from Martinez’s face and scanned his features, capturing the shape of his eyes, the curves of his lips, the exact quadrants of his cheeks.&lt;/p&gt;
    &lt;p&gt;All the while, the agent kept asking: Are you a U.S. citizen?&lt;/p&gt;
    &lt;p&gt;The encounter in a Minneapolis suburb this week captures the tactics on display in the Trump administration’s immigration crackdown in Minnesota, which it describes as the largest of its kind and one that has drawn national scrutiny after federal agents shot and killed two U.S. citizens this month.&lt;/p&gt;
    &lt;p&gt;Across Minnesota and other states where the Department of Homeland Security has surged personnel, officials say enforcement efforts are targeted and focused on serious offenders. But photographs, videos and internal documents paint a different picture, showing agents leaning heavily on biometric surveillance and vast, interconnected databases — highlighting how a sprawling digital surveillance apparatus has become central to the Trump administration’s immigration crackdown.&lt;/p&gt;
    &lt;p&gt;Civil liberties experts warn the expanding use of those systems risks sweeping up citizens and noncitizens alike, often with little transparency or meaningful oversight.&lt;/p&gt;
    &lt;p&gt;Over the past year, Homeland Security and other federal agencies have dramatically expanded their ability to collect, share and analyze people’s personal data, thanks to a web of agreements with local, state, federal and international agencies, plus contracts with technology companies and data brokers. The databases include immigration and travel records, facial images and information drawn from vehicle databases.&lt;/p&gt;
    &lt;p&gt;In Martinez’s case, the face scan didn’t find a match and it wasn’t until he produced his U.S. passport, which he said he carried for fear of such an encounter, that federal agents let him go.&lt;/p&gt;
    &lt;p&gt;“I had been telling people that here in Minnesota it’s like a paradise for everybody, all the cultures are free here,” he said. “But now people are running out of the state because of everything that is happening. It’s terrifying. It’s not safe anymore.”&lt;/p&gt;
    &lt;p&gt;Together with other government surveillance data and systems, federal authorities can now monitor American cities at a scale that would have been difficult to imagine just a few years ago, advocates say. Agents can identify people on the street through facial recognition, trace their movements through license-plate readers and, in some cases, use commercially available phone-location data to reconstruct daily routines and associations.&lt;/p&gt;
    &lt;p&gt;When asked by The Associated Press about its expanding use of surveillance tools, the Department of Homeland Security said it would not disclose law enforcement sensitive methods.&lt;/p&gt;
    &lt;p&gt;“Employing various forms of technology in support of investigations and law enforcement activities aids in the arrest of criminal gang members, child sex offenders, murderers, drug dealers, identity thieves and more, all while respecting civil liberties and privacy interests,” it said.&lt;/p&gt;
    &lt;p&gt;Dan Herman, a former Customs and Border Protection senior adviser in the Biden administration who now works at the Center for American Progress, said the government’s access to facial recognition, other personal data and surveillance systems poses a threat to people’s privacy rights and civil liberties without adequate checks.&lt;/p&gt;
    &lt;p&gt;Have a news tip?&lt;/p&gt;
    &lt;p&gt;Contact AP’s global investigative team at [email protected]. For secure and confidential communications, use the free Signal app +1 (202) 281-8604.&lt;/p&gt;
    &lt;p&gt;“They have access to a tremendous amount of trade, travel, immigration and screening data. That’s a significant and valuable national security asset, but there’s a concern about the potential for abuse,” Herman said. “Everyone should be very concerned about the potential that this data could be weaponized for improper purposes.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Facial recognition&lt;/head&gt;
    &lt;p&gt;On Wednesday, DHS disclosed online that it has been using a facial recognition app, Mobile Fortify, that it said uses “trusted source photos” to compare scans of people’s faces that agents take to verify their identity. The app, which Customs and Border Protection said is made by the vendor NEC, uses facial comparison or fingerprint-matching systems.&lt;/p&gt;
    &lt;p&gt;The app was in operation for CBP and ICE before the immigration crackdown in the Los Angeles area in June, when website 404Media first reported its existence.&lt;/p&gt;
    &lt;p&gt;In interactions observed by reporters and videos posted online, federal agents are rarely seen asking for consent before holding their cellphones to people’s faces, and in some clips they continue scanning even after someone objects.&lt;/p&gt;
    &lt;p&gt;In two instances seen by an AP journalist near Columbia Heights, Minnesota, where immigration officials recently detained a 5-year-old boy and his father, masked agents held their phones a foot away from people’s faces to capture their biometric details.&lt;/p&gt;
    &lt;p&gt;The technology resembles facial recognition systems used at airports, but unlike airport screenings, where travelers are typically notified and can sometimes opt out, Martinez said he was given no choice.&lt;/p&gt;
    &lt;p&gt;According to a lawsuit filed against DHS by the state of Illinois and the city of Chicago this month, DHS has used Mobile Fortify in the field more than 100,000 times. The Department of Homeland Security told AP that Mobile Fortify supports “accurate identity and immigration-status verification during enforcement operations. It operates with a deliberately high-matching threshold,” and uses only some immigration data.&lt;/p&gt;
    &lt;p&gt;Without federal guidelines for the use of facial recognition tools, the U.S. Commission on Civil Rights warned in a September 2024 report their deployment raises concerns about accuracy, oversight, transparency, discrimination and access to justice.&lt;/p&gt;
    &lt;head rend="h2"&gt;Body-camera footage&lt;/head&gt;
    &lt;p&gt;Last year, the Trump administration scaled back a program to give Immigration and Customs Enforcement officials body cameras, but administration officials said some agents tied to the fatal shooting of Minneapolis ICU nurse Alex Pretti were wearing them and that footage is now being reviewed.&lt;/p&gt;
    &lt;p&gt;Gregory Bovino, who was the administration’s top Border Patrol official charged with the immigration crackdown in Minneapolis until Monday, began wearing a bodycam in response to a judge’s order late last year.&lt;/p&gt;
    &lt;p&gt;Body-camera video could help clarify events surrounding federal agents’ killing of Pretti, who was filming immigration agents with his cellphone when they shot him in the back.&lt;/p&gt;
    &lt;p&gt;Administration officials shifted their tone after independent video footage emerged raising serious questions about some Trump officials’ accusations that Pretti intended to harm agents.&lt;/p&gt;
    &lt;head rend="h2"&gt;Emerging technologies&lt;/head&gt;
    &lt;p&gt;Homeland Security and affiliated agencies are piloting and deploying more than 100 artificial intelligence systems, including some used in law enforcement activities, according to the department’s disclosure Wednesday.&lt;/p&gt;
    &lt;p&gt;Congress last year authorized U.S. Customs and Border Protection to get more than $2.7 billion to build out border surveillance systems and add in AI and other emerging technologies.&lt;/p&gt;
    &lt;p&gt;In recent weeks, DHS requested more information from private industry on how technology companies and data providers can support their investigations and help identify people.&lt;/p&gt;
    &lt;p&gt;Meanwhile, longtime government contractor Palantir was paid $30 million to extend a contract to build a system designed to locate people flagged for deportation. On Wednesday, the Trump administration disclosed it’s using Palantir’s AI models to sift through immigration enforcement tips submitted to its tip line.&lt;/p&gt;
    &lt;p&gt;DHS has also been exploring partnerships with license-plate reader companies like Flock Safety to expand their tracking capabilities.&lt;/p&gt;
    &lt;p&gt;Rachel Levinson-Waldman, who directs the Brennan Center for Justice’s Liberty and National Security Program, said more funding for government surveillance tools changes the landscape.&lt;/p&gt;
    &lt;p&gt;“We are developing these technologies for immigrant enforcement,” she said. “Are we also going to expand it or wield it against U.S. citizens who are engaging in entirely lawful or protest activity?”&lt;/p&gt;
    &lt;p&gt;___&lt;/p&gt;
    &lt;p&gt;AP freelance photojournalist Adam Gray contributed to this report from Minneapolis.&lt;/p&gt;
    &lt;p&gt;___&lt;/p&gt;
    &lt;p&gt;Contact AP’s global investigative team at [email protected] or https://www.ap.org/tips/&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46829887</guid><pubDate>Fri, 30 Jan 2026 21:06:28 +0000</pubDate></item><item><title>How I built my own secure version of Clawdbot</title><link>https://medium.com/ai-native-enterprise/how-i-built-my-own-enterprise-grade-clawdbot-without-the-security-nightmares-00ae01193280</link><description>&lt;doc fingerprint="2752ecdb688be0aa"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How I Built My Own Enterprise-Grade Clawdbot Without the Security Nightmares&lt;/head&gt;
    &lt;p&gt;When Clawdbot started gaining traction in mid December, I wanted it immediately.&lt;/p&gt;
    &lt;p&gt;An AI assistant that books flights, manages calendars, and fixes production bugs? All from a WhatsApp message? I was ready to install it that afternoon.&lt;/p&gt;
    &lt;p&gt;Then I read the GitGuardian report. 181 leaked secrets. A healthcare company’s Notion credentials exposed. A fintech startup’s Kubernetes cluster wide open. Telegram bot tokens everywhere.&lt;/p&gt;
    &lt;p&gt;I closed the browser tab.&lt;/p&gt;
    &lt;p&gt;But I couldn’t stop thinking about it. The capabilities were real. The demand was real (Best Buy sold out of Mac Minis in San Francisco). The security was the problem.&lt;/p&gt;
    &lt;p&gt;So I built my own enterprise-grade AI agent with the same capabilities as Clawdbot.&lt;/p&gt;
    &lt;head rend="h2"&gt;What made Clawdbot so compelling&lt;/head&gt;
    &lt;p&gt;Peter Steinberger’s project hit 9,000 GitHub stars in 24 hours. Within a week, it crossed 80,000 stars and attracted 2 million visitors. The name changed twice in three days (first to Moltbot after Anthropic’s trademark request, then to OpenClaw), but the signal was unmistakable.&lt;/p&gt;
    &lt;p&gt;Developers don’t want another chatbot. They want an AI that operates.&lt;/p&gt;
    &lt;p&gt;Clawdbot delivered: browser automation, calendar management, scheduled tasks, persistent memory across sessions. One user saved $4,200 negotiating a car price through it. Another cataloged their entire wine cellar.&lt;/p&gt;
    &lt;p&gt;I wanted that for my work. I run a B2B agency, and my days are split between client calls, competitive research, content workflows, and deliverable tracking. The idea of an AI assistant that could handle the tedious parts while I focused on strategy was too good to ignore.&lt;/p&gt;
    &lt;p&gt;But I also handle client credentials, access sensitive platforms, and can’t afford to be the next case study in a security blog.&lt;/p&gt;
    &lt;head rend="h2"&gt;The problem with Clawdbot (and why I couldn’t use it)&lt;/head&gt;
    &lt;p&gt;Casey Newton of Platformer captured it perfectly after her own experiment: “the specific ways in which it broke persuaded me to uninstall Clawdbot from my computer.” The initial setup was “the high-water mark.”&lt;/p&gt;
    &lt;p&gt;Another reviewer found that calendar management produced chaos. Appointments consistently appeared one day off. The tool couldn’t create recurring events, resulting in dozens of incorrect individual entries that the bot kept trying to re-add.&lt;/p&gt;
    &lt;p&gt;But the real issue wasn’t the bugs. It was the architecture.&lt;/p&gt;
    &lt;p&gt;Clawdbot creates files, captures tool outputs, and logs session transcripts that may include credentials pasted by users or fetched from external sources. The documentation warns about this. The mitigation? A &lt;code&gt;.gitignore&lt;/code&gt; rule.&lt;/p&gt;
    &lt;p&gt;That’s not enterprise security.&lt;/p&gt;
    &lt;p&gt;I needed the capabilities without the liability. So I broke down what Clawdbot actually does and rebuilt each piece properly.&lt;/p&gt;
    &lt;head rend="h2"&gt;What I actually built&lt;/head&gt;
    &lt;p&gt;Clawdbot’s 700+ plugins across 28 categories sound impressive, but the capabilities that matter cluster into four areas. These are the ones I use daily.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Messaging interface: I send commands from Slack and get results without context-switching.&lt;/item&gt;
      &lt;item&gt;Browser automation: My agent scrapes competitor pricing, extracts content from web apps, and fills forms when APIs don’t exist.&lt;/item&gt;
      &lt;item&gt;Scheduled tasks: Every morning at 7am, I get a briefing. Every Monday, I get a competitive landscape update. No manual triggering.&lt;/item&gt;
      &lt;item&gt;Persistent memory: My agent remembers client contexts, project details, and my preferences across sessions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s how I built each one.&lt;/p&gt;
    &lt;head rend="h2"&gt;The foundation: Claude Agent SDK&lt;/head&gt;
    &lt;p&gt;Everything runs on the Claude Agent SDK. This isn’t a wrapper around Claude. It’s the same tools, agent loop, and context management that power Claude Code, now programmable in Python.&lt;/p&gt;
    &lt;p&gt;The core loop is simple: gather context → take action → verify work → repeat.&lt;/p&gt;
    &lt;p&gt;The SDK provides built-in tools out of the box: Read, Write, Edit, Glob, and Grep for file operations; Bash for terminal commands and scripts; WebSearch and WebFetch for current information.&lt;/p&gt;
    &lt;p&gt;The key parameter is &lt;code&gt;allowed_tools&lt;/code&gt;. Unlike Clawdbot, which requests excessive permissions by default, I specify exactly what each agent can do. My research agent gets &lt;code&gt;["Read", "Glob", "Grep", "WebSearch", "WebFetch"]&lt;/code&gt;. My file operations agent gets &lt;code&gt;["Read", "Edit", "Bash"]&lt;/code&gt;. Nothing more.&lt;/p&gt;
    &lt;code&gt;from claude_agent_sdk import query, ClaudeAgentOptions&lt;lb/&gt;&lt;lb/&gt;async for message in query(&lt;lb/&gt;    prompt="Find all client mentions in yesterday's Slack threads and summarize",&lt;lb/&gt;    options=ClaudeAgentOptions(&lt;lb/&gt;        allowed_tools=["Read", "Grep", "WebFetch"],&lt;lb/&gt;        mcp_servers={"slack": slack_config}&lt;lb/&gt;    )&lt;lb/&gt;):&lt;lb/&gt;    if hasattr(message, "result"):&lt;lb/&gt;        print(message.result)&lt;/code&gt;
    &lt;p&gt;This is the foundation everything else builds on.&lt;/p&gt;
    &lt;head rend="h2"&gt;Layer 1: Slack as my interface&lt;/head&gt;
    &lt;p&gt;I chose Slack because my team already lives there. The official Slack MCP server handles OAuth authentication that respects existing permissions, and workspace admins can see exactly what the integration accesses.&lt;/p&gt;
    &lt;p&gt;My setup connects Slack to the Agent SDK through the MCP server. When I message my agent channel, it processes the request, runs the appropriate tools, and responds in the thread.&lt;/p&gt;
    &lt;code&gt;from claude_agent_sdk import query, ClaudeAgentOptions&lt;lb/&gt;&lt;lb/&gt;slack_config = {&lt;lb/&gt;    "command": "npx",&lt;lb/&gt;    "args": ["@anthropic-ai/slack-mcp-server"]&lt;lb/&gt;}&lt;lb/&gt;&lt;lb/&gt;async def handle_slack_message(message_text: str, channel: str):&lt;lb/&gt;    async for message in query(&lt;lb/&gt;        prompt=message_text,&lt;lb/&gt;        options=ClaudeAgentOptions(&lt;lb/&gt;            mcp_servers={"slack": slack_config},&lt;lb/&gt;            allowed_tools=["mcp__slack__*", "Read", "WebSearch"]&lt;lb/&gt;        )&lt;lb/&gt;    ):&lt;lb/&gt;        if hasattr(message, "result"):&lt;lb/&gt;            await post_to_slack(channel, message.result)&lt;/code&gt;
    &lt;p&gt;The key difference from Clawdbot: every action is logged, permissioned, and auditable. My IT setup can see exactly what the agent accessed. When a client asks “what data does your AI touch?”, I have answers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Layer 2: Browser automation that actually works&lt;/head&gt;
    &lt;p&gt;This is where Clawdbot earned its reputation. Booking flights. Filling forms. Scraping competitor data.&lt;/p&gt;
    &lt;p&gt;I use Microsoft’s Playwright MCP server, and the difference in approach matters.&lt;/p&gt;
    &lt;p&gt;Clawdbot uses screenshots. The model looks at pixels and decides where to click. This is slow, expensive (vision models), and ambiguous.&lt;/p&gt;
    &lt;p&gt;Playwright MCP uses the accessibility tree. Instead of pixels, the agent sees structured data about every element on the page. No vision models needed. Deterministic actions. Fast.&lt;/p&gt;
    &lt;p&gt;Here’s what my competitor research workflow looks like:&lt;/p&gt;
    &lt;code&gt;playwright_config = {&lt;lb/&gt;    "command": "npx",&lt;lb/&gt;    "args": ["@playwright/mcp@latest"]&lt;lb/&gt;}&lt;lb/&gt;&lt;lb/&gt;async def research_competitor(competitor_url: str):&lt;lb/&gt;    async for message in query(&lt;lb/&gt;        prompt=f"Go to {competitor_url}/pricing and extract their plan tiers, "&lt;lb/&gt;               f"features, and prices. Format as markdown.",&lt;lb/&gt;        options=ClaudeAgentOptions(&lt;lb/&gt;            mcp_servers={"playwright": playwright_config}&lt;lb/&gt;        )&lt;lb/&gt;    ):&lt;lb/&gt;        if hasattr(message, "result"):&lt;lb/&gt;            return message.result&lt;/code&gt;
    &lt;p&gt;One insight from months of use: browser automation works best for async, research-style tasks. Scraping pricing pages. Gathering reviews from Reddit. Extracting data from web apps without APIs.&lt;/p&gt;
    &lt;p&gt;It struggles with tight feedback loops. I tried using it for real-time coding assistance and the latency killed the experience. Design your workflows accordingly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Layer 3: Scheduled tasks (the killer feature)&lt;/head&gt;
    &lt;p&gt;This is what I use most. Clawdbot can send daily briefings and run recurring jobs, but with zero audit trail and full machine access.&lt;/p&gt;
    &lt;p&gt;I use APScheduler for simple recurring tasks and have Temporal ready for complex multi-step workflows.&lt;/p&gt;
    &lt;p&gt;My morning briefing runs at 7am every weekday:&lt;/p&gt;
    &lt;code&gt;from apscheduler.schedulers.asyncio import AsyncIOScheduler&lt;lb/&gt;from apscheduler.triggers.cron import CronTrigger&lt;lb/&gt;from claude_agent_sdk import query, ClaudeAgentOptions&lt;lb/&gt;&lt;lb/&gt;async def morning_briefing():&lt;lb/&gt;    async for message in query(&lt;lb/&gt;        prompt="""Generate my morning briefing:&lt;lb/&gt;        1. Check calendar for today's meetings and prep needed&lt;lb/&gt;        2. Summarize any urgent Slack threads from #clients and #team&lt;lb/&gt;        3. Flag any platform alerts or monitoring notifications&lt;lb/&gt;        4. List any deliverables due this week&lt;lb/&gt;&lt;lb/&gt;        Format as a concise bullet list I can scan in 2 minutes.""",&lt;lb/&gt;        options=ClaudeAgentOptions(&lt;lb/&gt;            mcp_servers={&lt;lb/&gt;                "slack": slack_config,&lt;lb/&gt;                "google-calendar": calendar_config,&lt;lb/&gt;                "memory": memory_config&lt;lb/&gt;            }&lt;lb/&gt;        )&lt;lb/&gt;    ):&lt;lb/&gt;        if hasattr(message, "result"):&lt;lb/&gt;            await post_to_slack("#gavriel-briefing", message.result)&lt;lb/&gt;&lt;lb/&gt;scheduler = AsyncIOScheduler()&lt;lb/&gt;scheduler.add_job(&lt;lb/&gt;    morning_briefing,&lt;lb/&gt;    CronTrigger(hour=7, minute=0, day_of_week="mon-fri")&lt;lb/&gt;)&lt;lb/&gt;scheduler.start()&lt;/code&gt;
    &lt;p&gt;I also run a Monday morning competitive landscape check:&lt;/p&gt;
    &lt;code&gt;async def weekly_competitor_scan():&lt;lb/&gt;    competitors = ["competitor1.com", "competitor2.com", "competitor3.com"]&lt;lb/&gt;&lt;lb/&gt;    async for message in query(&lt;lb/&gt;        prompt=f"""Research these competitors for any changes this week:&lt;lb/&gt;        {', '.join(competitors)}&lt;lb/&gt;&lt;lb/&gt;        Check for:&lt;lb/&gt;        - Pricing changes&lt;lb/&gt;        - New feature announcements&lt;lb/&gt;        - Blog posts or content&lt;lb/&gt;        - Social media activity&lt;lb/&gt;&lt;lb/&gt;        Compare to what you remember from last week.""",&lt;lb/&gt;        options=ClaudeAgentOptions(&lt;lb/&gt;            mcp_servers={&lt;lb/&gt;                "playwright": playwright_config,&lt;lb/&gt;                "memory": memory_config&lt;lb/&gt;            }&lt;lb/&gt;        )&lt;lb/&gt;    ):&lt;lb/&gt;        if hasattr(message, "result"):&lt;lb/&gt;            await post_to_slack("#competitive-intel", message.result)&lt;lb/&gt;&lt;lb/&gt;scheduler.add_job(&lt;lb/&gt;    weekly_competitor_scan,&lt;lb/&gt;    CronTrigger(hour=8, minute=0, day_of_week="mon")&lt;lb/&gt;)&lt;/code&gt;
    &lt;p&gt;The memory integration is key here. The agent remembers last week’s findings and can flag what actually changed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Layer 4: Persistent memory&lt;/head&gt;
    &lt;p&gt;Standard AI starts every conversation from zero. Clawdbot maintains context across sessions using local files. I use the MCP Memory Server, which stores knowledge as a graph of entities, observations, and relations.&lt;/p&gt;
    &lt;code&gt;memory_config = {&lt;lb/&gt;    "command": "npx",&lt;lb/&gt;    "args": ["@modelcontextprotocol/server-memory"],&lt;lb/&gt;    "env": {"MEMORY_FILE_PATH": "./memory-work.jsonl"}&lt;lb/&gt;}&lt;lb/&gt;&lt;lb/&gt;# Agent automatically stores and retrieves context&lt;lb/&gt;async for message in query(&lt;lb/&gt;    prompt="Remember that the Acme Corp deadline moved to March 15 "&lt;lb/&gt;           "and they prefer weekly updates on Fridays",&lt;lb/&gt;    options=ClaudeAgentOptions(&lt;lb/&gt;        mcp_servers={"memory": memory_config}&lt;lb/&gt;    )&lt;lb/&gt;):&lt;lb/&gt;    pass  # Memory stored&lt;lb/&gt;&lt;lb/&gt;# Later, in a different session...&lt;lb/&gt;async for message in query(&lt;lb/&gt;    prompt="What's the status on the Acme Corp project?",&lt;lb/&gt;    options=ClaudeAgentOptions(&lt;lb/&gt;        mcp_servers={"memory": memory_config}&lt;lb/&gt;    )&lt;lb/&gt;):&lt;lb/&gt;    # Agent knows about the March 15 deadline and Friday updates&lt;lb/&gt;    print(message.result)&lt;/code&gt;
    &lt;p&gt;A feature Clawdbot lacks: context isolation. I run separate memory files for work and personal contexts (&lt;code&gt;memory-work.jsonl&lt;/code&gt;, &lt;code&gt;memory-personal.jsonl&lt;/code&gt;). An agent running in my work context can't accidentally access personal memory.&lt;/p&gt;
    &lt;p&gt;For client work, each client gets their own memory namespace. When I’m working on Acme Corp, the agent has full context on that engagement without bleeding into other clients.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where it runs: xpander.ai&lt;/head&gt;
    &lt;p&gt;One thing Clawdbot users quickly discover: scheduled tasks don’t work when your laptop sleeps. You need infrastructure that stays on.&lt;/p&gt;
    &lt;p&gt;I deploy everything on xpander.ai. They have native Claude Code integration, so my Agent SDK code runs in their cloud with Slack integration, persistent state, and proper infrastructure out of the box.&lt;/p&gt;
    &lt;p&gt;The setup was straightforward. xpander has a Coding Agent template that uses their Claude Code connector. I started with that, customized the logic, and deployed. Now my agents run 24/7 without me managing servers.&lt;/p&gt;
    &lt;p&gt;What I get from xpander:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cloud infrastructure that doesn’t sleep when I do&lt;/item&gt;
      &lt;item&gt;Slack-native deployment (my agents live in channels, not a separate app)&lt;/item&gt;
      &lt;item&gt;Persistent state across sessions (handled automatically)&lt;/item&gt;
      &lt;item&gt;Centralized logs and observability&lt;/item&gt;
      &lt;item&gt;Team access without sharing credentials&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The alternative was spinning up a VPS, managing uptime, handling Slack OAuth myself, and building state management from scratch. xpander handles all of that so I can focus on the agent logic.&lt;/p&gt;
    &lt;head rend="h2"&gt;The security layer (what makes this enterprise-grade)&lt;/head&gt;
    &lt;p&gt;This is what separates my setup from a Clawdbot installation. And it’s why 181 secrets leaked from Clawdbot users.&lt;/p&gt;
    &lt;p&gt;I don’t store API keys in environment variables or config files. Akeyless provides dynamic secrets: credentials generated on-demand, automatically rotated, and revoked when no longer needed.&lt;/p&gt;
    &lt;p&gt;What drew me to Akeyless over self-hosted alternatives: they have an AI Agent Identity Provider specifically designed for this use case. My agents get verifiable, federated identities with short-lived credentials. When my agent needs database access, Akeyless issues a time-limited token with read-only permissions. When the task completes, the token expires. Nothing persists.&lt;/p&gt;
    &lt;code&gt;import akeyless&lt;lb/&gt;from claude_agent_sdk import query, ClaudeAgentOptions&lt;lb/&gt;&lt;lb/&gt;# Configure Akeyless client&lt;lb/&gt;configuration = akeyless.Configuration(host="https://api.akeyless.io")&lt;lb/&gt;with akeyless.ApiClient(configuration) as api_client:&lt;lb/&gt;    api = akeyless.V2Api(api_client)&lt;lb/&gt;&lt;lb/&gt;    # Get dynamic secret (credentials auto-expire after TTL)&lt;lb/&gt;    body = akeyless.GetDynamicSecretValue(&lt;lb/&gt;        name='/agents/readonly-db-access',&lt;lb/&gt;        token=access_token  &lt;lb/&gt;    )&lt;lb/&gt;    creds = api.get_dynamic_secret_value(body)&lt;lb/&gt;&lt;lb/&gt;    # Use the short-lived credentials&lt;lb/&gt;    async for message in query(&lt;lb/&gt;        prompt="Analyze client engagement trends for Q4",&lt;lb/&gt;        options=ClaudeAgentOptions(&lt;lb/&gt;            mcp_servers={&lt;lb/&gt;                "postgres": {&lt;lb/&gt;                    "command": "npx",&lt;lb/&gt;                    "args": ["postgres-mcp-server"],&lt;lb/&gt;                    "env": {&lt;lb/&gt;                        "POSTGRES_USER": creds['username'],&lt;lb/&gt;                        "POSTGRES_PASSWORD": creds['password']&lt;lb/&gt;                    }&lt;lb/&gt;                }&lt;lb/&gt;            }&lt;lb/&gt;        )&lt;lb/&gt;    ):&lt;lb/&gt;        pass&lt;/code&gt;
    &lt;p&gt;The SaaS model means I’m not managing vault infrastructure on top of everything else. One less thing to maintain.&lt;/p&gt;
    &lt;p&gt;Permission Scoping&lt;/p&gt;
    &lt;p&gt;Every agent runs with minimum necessary permissions. My research agent can read and search. My file operations agent can edit. Neither can do what the other does.&lt;/p&gt;
    &lt;p&gt;This is the opposite of Clawdbot’s approach, which requests excessive OAuth scopes by default. Users had to push back just to get read-only calendar access.&lt;/p&gt;
    &lt;p&gt;Audit Trail&lt;/p&gt;
    &lt;p&gt;Every action is logged. When I ask “what did the AI access yesterday?”, I can answer precisely. For client work, this isn’t optional. It’s required.&lt;/p&gt;
    &lt;head rend="h2"&gt;What I actually use it for&lt;/head&gt;
    &lt;p&gt;After a few weeks of iteration, these are my core workflows:&lt;/p&gt;
    &lt;p&gt;My morning briefing runs at 7am weekdays. It combines calendar, Slack, platform alerts, and deliverables into something I can scan in 2 minutes. Saves 20 minutes of context-switching.&lt;/p&gt;
    &lt;p&gt;Competitive research runs Monday mornings. Browser automation scrapes competitor sites, memory recalls last week’s state, and the agent flags what changed. Used to take half a day manually.&lt;/p&gt;
    &lt;p&gt;Before client kickoff calls, the agent pulls all context from the client files, generates a briefing doc, and identifies gaps I need to fill. Consistent prep without the manual assembly.&lt;/p&gt;
    &lt;p&gt;For content research, I send prompts like “Find discussions about [topic] on Reddit, HackerNews, and industry forums. Summarize the main pain points and what solutions people are using.” Returns a structured report I can work from.&lt;/p&gt;
    &lt;p&gt;Weekly client updates are the simplest. The agent drafts status updates based on deliverables completed, pulling from memory and Slack activity. I edit and send. Cuts the writing time by 70%.&lt;/p&gt;
    &lt;head rend="h2"&gt;What you gain&lt;/head&gt;
    &lt;p&gt;Building this took more work than running &lt;code&gt;npx clawdhub@latest install&lt;/code&gt;. But I gained:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Audit trails. Every action logged, traceable, compliant. When clients ask what data the AI touches, I have answers.&lt;/item&gt;
      &lt;item&gt;Access controls. Each agent has exactly the permissions it needs. A bug in my research agent can’t compromise my production systems.&lt;/item&gt;
      &lt;item&gt;Credential management. Dynamic secrets, automatic rotation, instant revocation. No tokens in &lt;code&gt;.env&lt;/code&gt;files. Nothing to leak.&lt;/item&gt;
      &lt;item&gt;Reliability. Scheduled tasks run on Temporal with automatic retries and state persistence. If something fails at 3am, it recovers gracefully.&lt;/item&gt;
      &lt;item&gt;Context isolation. Client A’s data never bleeds into Client B’s workflows. Work and personal stay separate.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Clawdbot explosion showed us what developers want: AI that operates, not just opines. The 181 leaked secrets showed us what happens when you build it without security.&lt;/p&gt;
    &lt;p&gt;I wanted both: the capabilities and the safety. Now I have it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The stack&lt;/head&gt;
    &lt;head rend="h2"&gt;What’s next&lt;/head&gt;
    &lt;p&gt;I’m still iterating. Current experiments:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multi-agent workflows using xpander’s Agent Graph System, where specialized agents hand off to each other (research → analysis → draft)&lt;/item&gt;
      &lt;item&gt;Voice interface via the Gemini Live API for hands-free briefings during my commute&lt;/item&gt;
      &lt;item&gt;Client-facing automation where the agent handles routine client requests directly in Slack&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The foundation is solid. Now I’m expanding what it can do.&lt;/p&gt;
    &lt;p&gt;If you’re building something similar, I’d love to hear about it. The tooling has finally caught up to the vision. The question is what we build with it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46830013</guid><pubDate>Fri, 30 Jan 2026 21:15:57 +0000</pubDate></item></channel></rss>