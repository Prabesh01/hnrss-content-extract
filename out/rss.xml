<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 29 Aug 2025 15:34:32 +0000</lastBuildDate><item><title>Some thoughts on LLMs and software development</title><link>https://martinfowler.com/articles/202508-ai-thoughts.html</link><description>&lt;doc fingerprint="192b3375fcf305ad"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Some thoughts on LLMs and Software Development&lt;/head&gt;
    &lt;p&gt;I√¢m about to head away from looking after this site for a few weeks (part vacation, part work stuff). As I contemplate some weeks away from the daily routine, I feel an urge to share some scattered thoughts about the state of LLMs and AI.&lt;/p&gt;
    &lt;p&gt;√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢&lt;/p&gt;
    &lt;p&gt;I√¢ve seen a few early surveys on the effect AI is having on software development, is it really speeding folks up, does it improve or wreck code quality? One of the big problems with these surveys is that they aren√¢t taking into account how people are using the LLMs. From what I can tell the vast majority of LLM usage is fancy auto-complete, often using co-pilot. But those I know who get the most value from LLMs reckon that auto-complete isn√¢t very useful, preferring approaches that allow the LLM to directly read and edit source code files to carry out tasks. My concern is that surveys that ignore the different work-flows of using LLMs will produce data that√¢s going to send people down the wrong paths.&lt;/p&gt;
    &lt;p&gt;(Another complication is the varying capabilities of different models.)&lt;/p&gt;
    &lt;p&gt;√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢&lt;/p&gt;
    &lt;p&gt;I√¢m often asked, √¢what is the future of programming?√¢ Should people consider entering software development now? Will LLMs eliminate the need for junior engineers? Should senior engineers get out of the profession before it√¢s too late? My answer to all these questions is √¢I haven√¢t the foggiest√¢. Furthermore I think anyone who says they know what this future will be is talking from an inappropriate orifice. We are still figuring out how to use LLMs, and it will be some time before we have a decent idea of how to use them well, especially if they gain significant improvements.&lt;/p&gt;
    &lt;p&gt;What I suggest, is that people experiment with them. At the least, read about what others are doing, but pay attention to the details of their workflows. Preferably experiment yourself, and do share your experiences.&lt;/p&gt;
    &lt;p&gt;√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢&lt;/p&gt;
    &lt;p&gt;I√¢m also asked: √¢is AI a bubble√¢? To which my answer is √¢OF COURSE IT√¢S A BUBBLE√¢. All major technological advances have come with economic bubbles, from canals and railroads to the internet. We know with near 100% certainty that this bubble will pop, causing lots of investments to fizzle to nothing. However what we don√¢t know is when it will pop, and thus how big the bubble will have grown, generating some real value in the process, before that happens. It could pop next month, or not for a couple of years.&lt;/p&gt;
    &lt;p&gt;We also know that when the bubble pops, many firms will go bust, but not all. When the dot-com bubble burst, it killed pets.com, it killed Webvan√¢¬¶ but it did not kill Amazon.&lt;/p&gt;
    &lt;p&gt;√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢&lt;/p&gt;
    &lt;p&gt;I retired from public speaking a couple of years ago. But while I don√¢t miss the stress of giving talks, I do miss hanging out with my friends in the industry. So I√¢m looking forward to catching up with many of them at GOTO Copenhagen. I√¢ve been involved with the GOTO conference series since the 1990s (when it was called JAOO), and continue to be impressed with how they put together a fascinating program.&lt;/p&gt;
    &lt;p&gt;√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢¬¢√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢&lt;/p&gt;
    &lt;p&gt;My former colleague Rebecca Parsons, has been saying for a long time that hallucinations aren√¢t a bug of LLMs, they are a feature. Indeed they are the feature. All an LLM does is produce hallucinations, it√¢s just that we find some of them useful.&lt;/p&gt;
    &lt;p&gt;One of the consequences of this is that we should always consider asking the LLM the same question more than once, perhaps with some variation in the wording. Then we can compare answers, indeed perhaps ask the LLM to compare answers for us. The difference in the answers can be as useful as the answers themselves.&lt;/p&gt;
    &lt;p&gt;Certainly if we ever ask a hallucination engine for a numeric answer, we should ask it at least three times, so we get some sense of the variation. Furthermore we shouldn√¢t ask an LLM to calculate an answer than we can calculate deterministically (yes, I√¢ve seen this). It is OK to ask an LLM to generate code to calculate an answer (but still do it more than once).&lt;/p&gt;
    &lt;p&gt;√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢&lt;/p&gt;
    &lt;p&gt;Other forms of engineering have to take into account the variability of the world. A structural engineer builds in tolerance for all the factors she can√¢t measure. (I remember being told early in my career that the unique characteristic of digital electronics was that there was no concept of tolerances.) Process engineers consider that humans are executing tasks, and will sometimes be forgetful or careless. Software Engineering is unusual in that it works with deterministic machines. Maybe LLMs mark the point where we join our engineering peers in a world on non-determinism.&lt;/p&gt;
    &lt;p&gt;√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢&lt;/p&gt;
    &lt;p&gt;I√¢ve often heard, with decent reason, an LLM compared to a junior colleague. But I find LLMs are quite happy to say √¢all tests green√¢, yet when I run them, there are failures. If that was a junior engineer√¢s behavior, how long would it be before H.R. was involved?&lt;/p&gt;
    &lt;p&gt;√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç √¢&lt;/p&gt;
    &lt;p&gt;LLMs create a huge increase in the attack surface of software systems. Simon Willison described the The Lethal Trifecta for AI agents: an agent that combines access to your private data, exposure to untrusted content, and a way to externally communicate (√¢exfiltration√¢). That √¢untrusted content√¢ can come in all sorts of ways, ask it to read a web page, and an attacker can easily put instructions on the website in 1pt white-on-white font to trick the gullible LLM to obtain that private data.&lt;/p&gt;
    &lt;p&gt;This is particularly serious when it comes to agents acting in a browser. Read an attacker√¢s web page, and it could trick the agent to go to your bank account in another tab and √¢buy you a present√¢ by transferring your balance to the kind attacker. Willison√¢s view is that √¢the entire concept of an agentic browser extension is fatally flawed and cannot be built safely√¢.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45055641</guid></item><item><title>Fuck up my site ‚Äì Turn any website into beautiful chaos</title><link>https://www.fuckupmysite.com/?url=https%3A%2F%2Fnews.ycombinator.com&amp;torchCursor=true&amp;comicSans=true&amp;fakeCursors=true&amp;peskyFly=true</link><description>&lt;doc fingerprint="a9a77b49597bf6f1"&gt;
  &lt;main&gt;
    &lt;p&gt;DESTRUCTION LEVEL0%&lt;/p&gt;
    &lt;p&gt;‚ñì&lt;/p&gt;
    &lt;head rend="h1"&gt;fuckupmysite&lt;/head&gt;
    &lt;p&gt;Some people just want to watch the web burn&lt;/p&gt;
    &lt;head rend="h2"&gt;üòàChaos Settings&lt;/head&gt;
    &lt;p&gt;3 of 6 agents of chaos enabled&lt;/p&gt;
    &lt;p&gt;‚ö†Ô∏è&lt;/p&gt;
    &lt;head rend="h3"&gt;IMPORTANT DISCLAIMER - FOR PARODY &amp;amp; ENTERTAINMENT ONLY&lt;/head&gt;
    &lt;p&gt;This tool is for parody and entertainment purposes only. It temporarily applies visual chaos effects to websites for comedic effect. We do not store, collect, or transmit any personal information.&lt;/p&gt;
    &lt;p&gt;NEVER enter passwords, credit card details, or any sensitive information while using this tool. The proxied sites are not secure and should not be used for any real transactions or logins.&lt;/p&gt;
    &lt;p&gt;By using this tool, you acknowledge that it's purely for entertainment and you will not enter any sensitive data. Banking, financial, healthcare, and government sites are blocked for safety.&lt;/p&gt;
    &lt;p&gt;Heads up: Not every site plays nice with the chaos. Got feedback or discovered something broken? Let me know on Twitter&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45057020</guid></item><item><title>Expert: LSP for Elixir</title><link>https://github.com/elixir-lang/expert</link><description>&lt;doc fingerprint="24853a795d5f6fa5"&gt;
  &lt;main&gt;
    &lt;p&gt;Expert is the official language server implementation for the Elixir programming language.&lt;/p&gt;
    &lt;p&gt;You can download Expert from the releases page for your operating system and architecture. Put the executable somewhere on your &lt;code&gt;$PATH&lt;/code&gt;, like &lt;code&gt;~/.local/bin/expert&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;For editor specific installation instructions, please refer to the Installation Instructions&lt;/p&gt;
    &lt;p&gt;If you want to try out the latest features, you can download a nightly build.&lt;/p&gt;
    &lt;p&gt;Using the GH CLI, you can run the following command to download the latest nightly build:&lt;/p&gt;
    &lt;code&gt;gh release download nightly --pattern 'expert_linux_amd64' --repo elixir-lang/expert&lt;/code&gt;
    &lt;p&gt;Then point your editor to the downloaded binary.&lt;/p&gt;
    &lt;p&gt;To build Expert from source, you need Zig &lt;code&gt;0.14.1&lt;/code&gt; installed on your system.&lt;/p&gt;
    &lt;p&gt;Then you can run the following command or follow the instructions in the Installation Instructions:&lt;/p&gt;
    &lt;code&gt;just release-local&lt;/code&gt;
    &lt;p&gt;This will build the Expert binary and place it in the &lt;code&gt;apps/expert/burrito_out&lt;/code&gt; directory. You can then point your
editor to this binary.&lt;/p&gt;
    &lt;p&gt;Thank you to our corporate sponsors! If you'd like to start sponsoring the project, please read more below.&lt;/p&gt;
    &lt;p&gt;For companies wanting to directly sponsor full time work on Expert, please reach out to Dan Janowski: EEF Chair of Sponsorship WG at danj@erlef.org.&lt;/p&gt;
    &lt;p&gt;Individuals can donate using GitHub sponsors. Team members are listed in the sidebar.&lt;/p&gt;
    &lt;p&gt;Expert source code is released under Apache License 2.0.&lt;/p&gt;
    &lt;p&gt;Check LICENSE file for more information.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45057322</guid></item><item><title>Claude Sonnet will ship in Xcode</title><link>https://developer.apple.com/documentation/xcode-release-notes/xcode-26-release-notes</link><description>&lt;doc fingerprint="33a9437a9587dba1"&gt;
  &lt;main&gt;
    &lt;p&gt;This page requires JavaScript. Please turn on JavaScript in your browser and refresh the page to view its content.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45058688</guid></item><item><title>Lucky 13: a look at Debian trixie</title><link>https://lwn.net/Articles/1033474/</link><description>&lt;doc fingerprint="8cefd81d9907a69e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Lucky 13: a look at Debian trixie&lt;/head&gt;
    &lt;quote&gt;Please consider subscribing to LWN&lt;p&gt;Subscriptions are the lifeblood of LWN.net. If you appreciate this content and would like to see more of it, your subscription will help to ensure that LWN continues to thrive. Please visit this page to join up and keep LWN on the net.&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;After more than two years of development, the Debian Project has released its new stable version, Debian 13 ("trixie"). The release comes with the usual bounty of upgraded packages and more than 14,000 new packages; it also debuts Advanced Package Tool (APT) 3.0 as the default package manager and makes 64-bit RISC-V a supported architecture. There are few surprises with trixie, which is exactly what many Linux users are hoping for‚Äîa free operating system that just works as expected.&lt;/p&gt;
    &lt;p&gt;Debian's stable releases are aptly named; the project prioritizes stability over shipping the latest software. The freeze schedule for trixie called for a soft freeze in April, which meant that (for example) the KDE Plasma 6.4 release in June was too late to make the cut‚Äîeven though trixie was not released until August. Users who prefer to live on the edge will want to run another distribution or follow Debian development by running the testing release that previews the next stable version‚ÄîDebian 14 ("forky"). Truly adventurous users may take their chances with the unstable ("sid") release.&lt;/p&gt;
    &lt;p&gt;That said, trixie is up-to-date enough for many folks; it includes GNOME 48, KDE Plasma 6.3, Xfce 4.20, GNU Emacs 30.1, GnuPG 2.4.7, LibreOffice 25.2, and more. Under the hood, it includes the most recent Linux LTS kernel (6.12.41), GNU Compiler Collection (GCC) 14.2, GNU C Library (glibc) 2.41, LLVM/Clang 19, Python 3.13, Rust 1.85, and systemd 257. The release notes have a section for well-known software that compares the version in Debian 12 against Debian 13. While some of the versions lag a bit behind the upstream, they are not woefully outdated.&lt;/p&gt;
    &lt;p&gt;The project now supports six major hardware architectures: x86-64/amd64, 32-bit Arm with a hardware FPU (armhf), 64-bit Arm (arm64), IBM POWER8 or newer (ppc64el), IBM S/390 (s390x), and 64-bit RISC-V. The i386 architecture is not supported for trixie, though the project continues to build some i386 packages to run on 64-bit systems; users with i386 systems cannot upgrade to trixie. The MIPS architectures (mipsel and mis64el) have also been removed in trixie.&lt;/p&gt;
    &lt;p&gt;The Arm EABI (armel) port that targets older 32-bit Arm devices prior to Arm v7 is still supported with trixie, but this release is the end of the line. There is no installation media for armel systems, but users who have bookworm installed can upgrade to trixie if they have supported hardware: the Raspberry Pi 1, Zero, and Zero W are the only devices mentioned in the release notes.&lt;/p&gt;
    &lt;p&gt;Upgrades from bookworm are supported, of course. The release notes suggest that users convert APT source files to the DEB822 format before the upgrade. APT 3.0 includes an "apt modernize-sources" command to convert APT data source files to DEB822, but that is not available in bookworm. Users are also expected to remove all third-party packages prior to running the upgrade. I tested the upgrade on one of my servers, after taking a snapshot to roll back to if needed, and all went smoothly. Users who are considering an upgrade should read the release notes carefully before forging ahead; in particular, users should be aware that it's possible (but not certain) for network interface names to change on upgrade.&lt;/p&gt;
    &lt;head rend="h4"&gt;Installation&lt;/head&gt;
    &lt;p&gt;For users who want to start fresh, Debian offers a variety of installer images and download methods; users can choose a 64MB minimal ISO image with the netboot installer, all the way up to a set of Blu-ray images. The project recommends using BitTorrent or Jigsaw Download (jigdo) for the largest images. BitTorrent probably needs no introduction, but jigdo is not as well-known. Jigdo is a method of downloading all of the individual packages for an image from multiple mirrors and then assembling them into an ISO image on the user's machine. It was a bit fiddly to use jigdo to download an image, but not overly so‚Äîand the speed of the whole process was comparable to simply downloading an ISO of the same size.&lt;/p&gt;
    &lt;p&gt;Debian's network install ("netinst") image is probably the best option for server installations and for experienced Linux users; it includes the packages required for a base install and then fetches the remaining software from Debian mirrors. Unlike the tiny netboot image, it includes the option of using either the graphical installer or the text-based installer.&lt;/p&gt;
    &lt;p&gt;The installer is a bit of a throwback to an earlier era when users were expected to know a lot more about the workings of a Linux system. Users who have only worked with distributions like Fedora and Ubuntu will notice that installing Debian requires many more steps than other popular distributions. For example, many desktop distributions have eliminated the step of setting a password for the root user‚Äîinstead, it is generally assumed that the primary user will also be the system administrator, so the default is to give the primary user sudo privileges instead. Debian does not take that approach; &lt;del&gt;in fact, there is no way to give a user sudo privileges during installation. Setting up sudo has to be done manually after the installation is completed&lt;/del&gt; Update: Users can skip creation of a root account and the installer will then set up the regular user as an administrator with sudo permissions. Apologies for the error.&lt;/p&gt;
    &lt;p&gt;For some folks, installing Debian will be a bit of a chore and may even be confusing for users who are new to Linux. For example, the text-mode installer requires users to specify the device for GRUB boot loader installation, without providing a default. If one chooses an invalid partition, the installer tells the user that the operation has failed and drops back to a menu listing all the installation steps. Presumably if one picks the wrong partition it will happily install GRUB to that and render the system unbootable. This is not insurmountable for experienced Linux users, but it would no doubt be a hurdle for many users.&lt;/p&gt;
    &lt;p&gt;More experienced Linux users are likely to appreciate the amount of control offered by the installer. For example, Fedora's recent web-based installer makes it difficult to even find the option to perform custom partitioning. Debian has a guided partitioning option for those who do not want to fuss with it, but the option to create custom partitions is not hidden from the user.&lt;/p&gt;
    &lt;p&gt;Debian has a better installation option for newer Linux users, though it is easy to miss: the live install images, which use the Calamares installer. Its workflow is more akin to the installation process one finds with Fedora and Ubuntu; it also sets up the primary user with sudo privileges rather than creating a root password. Unfortunately, the live images are not listed on the main page for installer images‚Äîthough they are mentioned, briefly, in the release notes.&lt;/p&gt;
    &lt;p&gt;The Debian installer also has the option of using a Braille display and/or speech synthesizer voice for the installation. I have not tried these options, but they are available for users who need them.&lt;/p&gt;
    &lt;head rend="h4"&gt;X.org&lt;/head&gt;
    &lt;p&gt;Many distributions are in the process of phasing out X.org support for GNOME and KDE as the upstream projects have started doing so. For example, Fedora will remove X.org session support for GNOME in Fedora 43, and the plan is for Ubuntu to do the same in its upcoming 25.10 release. GNOME will be completely removing X.org support in GNOME 49, which is planned for September.&lt;/p&gt;
    &lt;p&gt;Much has already been said about this, of course, and there is likely little new left to be said or that needs to be said. However, for users who still need or want X.org support, Debian 13 includes X.org sessions for GNOME and KDE. In testing trixie, I've spent some time in the GNOME and KDE X.org sessions as well as the Wayland sessions; if there are any gotchas or horrible bugs, I haven't encountered them (yet). This might be a compelling reason for some folks to switch to (or stick with) Debian.&lt;/p&gt;
    &lt;head rend="h4"&gt;Trying trixie&lt;/head&gt;
    &lt;p&gt;I use Debian for my personal web site and blogs, but it has been quite some time since I used it as my primary desktop operating system. Debian (and Ubuntu) derivatives, such as Linux Mint and Pop!_OS, yes‚Äîbut it's been several years since I've used vanilla Debian on the desktop for more than casual tinkering.&lt;/p&gt;
    &lt;p&gt;The Debian release announcement boasts about the number of packages included in trixie: 64,419 packages total, with 14,100 added and more than 6,000 removed as obsolete since bookworm. That is quite a few packages, but falls short of some other distributions. For example, "dnf repoquery --repo=fedora --available" shows more than 76,000 packages available for Fedora 42.&lt;/p&gt;
    &lt;p&gt;After installing Debian, I went to install some of my preferred software, such as aerc, Ghostty, niri, and Speech Note. The aerc packages in trixie are current, but Ghostty and niri are not packaged for Debian at all. Ghostty is written in Zig, which is also not available, so users who want to build it from source will need to install Zig separately and then build Ghostty. Speech Note is packaged as a Flatpak, but Debian does not enable Flatpaks or Flathub in the GNOME Software Store by default. Users who want Flatpaks on Debian via Flathub will need to install the flatpak package and manually add the Flathub repo:&lt;/p&gt;
    &lt;quote&gt;flatpak remote-add --if-not-exists flathub \ https://dl.flathub.org/repo/flathub.flatpakrepo&lt;/quote&gt;
    &lt;p&gt;Users will need to add the gnome-software-plugin-flatpak package for Flatpak support in GNOME Software, and plasma-discover-backend-flatpak to add it to KDE Discover.&lt;/p&gt;
    &lt;p&gt;Trixie ships with the Firefox extended-support release (ESR) by default: Firefox 128, which was released in July 2024. Happily, Mozilla offers a Debian repository for those who want to run more current versions. Even better, there is a little-advertised utility called extrepo that has a curated list of external repositories users might want to enable for Debian. To enable the Mozilla repository, for example, a user only needs to install extrepo, run "extrepo enable mozilla" as root (or with sudo), update the package cache, and look for the regular Firefox package. In all, extrepo includes more than 160 external repositories for applications like Docker CE, Signal, and Syncthing. Unfortunately, the extrepo utility does not have a separate "list" command to show the available repositories, though running "extrepo search" with no search parameter will return all of its DEB822-formatted repository entries. Some of the software is in an external repository due to a non-free license, other software (like Firefox) just has a development cycle that outpaces Debian's.&lt;/p&gt;
    &lt;p&gt;As one might expect, the Debian desktop experience is not dramatically different from other distributions; GNOME 48 on Debian is little different than GNOME 48 on Fedora, and the same is true for KDE, Xfce, etc. The primary difference is that users can expect more or less the same desktop experience running Debian stable in two years that they have today, which is not necessarily true for other distributions.&lt;/p&gt;
    &lt;head rend="h4"&gt;Miscellaneous&lt;/head&gt;
    &lt;p&gt;One of the features in Debian 13 is something that most users won't notice or appreciate at all: a transition to 64-bit time_t on 32-bit architectures, to avoid the Year 2038 problem. The short version is that 32-bit integers cannot hold a Unix epoch timestamp for dates after January 19, 2038. That may seem like a distant concern, even irrelevant for Debian trixie; after all, Debian 13 is only supported by the project until 2030. However, the project expects that some 32-bit embedded systems will still be running trixie in 2038, so Debian developers did the heavy lifting to complete the transition to 64-bit time_t now. LWN covered the early planning for this in 2023.&lt;/p&gt;
    &lt;p&gt;By now, most users have retired their DSA SSH keys; if not, now is the time to do so. DSA keys were disabled by default with OpenSSH in 2015, and they are entirely disabled now with the openssh-client and openssh-server packages in trixie. If there is a device that can, for some reason, only be connected to with DSA, users can install the openssh-client-ssh1 package and use ssh1 to make the connection.&lt;/p&gt;
    &lt;p&gt;As we covered in June 2024, Debian 13 has switched to using a tmpfs filesystem for the /tmp directory. By default, Debian allocates up to 50% of memory to /tmp, but this can be changed by following the instructions in the release notes. Note that this also applies to systems that are upgraded to trixie from bookworm.&lt;/p&gt;
    &lt;head rend="h4"&gt;Forward to forky&lt;/head&gt;
    &lt;p&gt;Debian Project Leader (DPL) Andreas Tille recently announced "&lt;quote&gt;Debian's 100000th birthday&lt;/quote&gt;", so clearly the project has a bit of experience with putting out solid releases. Granted, he was reporting the number in binary, but even when converted to decimal numbers (32 years), it's an impressive track record.&lt;/p&gt;
    &lt;p&gt;While testing, I installed trixie on a couple of systems, including a new Framework 12-inch laptop. My original intent was to just see whether Debian had any problems with the new hardware (it didn't), but now I'm leaning toward sticking with Debian on this system for a while to see if stability suits me.&lt;/p&gt;
    &lt;p&gt;With trixie out the door, the Debian Project has already turned its attention to working on forky, which has no release date set. Debian has stuck to a loose schedule of a new stable release roughly every two years. Most likely we will see Debian 14 sometime in 2027. After the forky release, trixie will still receive updates from Debian's security team through 2028, and then from its LTS team through 2030.&lt;/p&gt;
    &lt;p&gt;As of yet, there are no major new features or changes announced for forky; it seems likely that those will be coming to light in the coming months now that the project has trixie out the door. LWN will, of course, be reporting on those developments as they happen.&lt;/p&gt;
    &lt;p&gt; Posted Aug 20, 2025 13:54 UTC (Wed) by bluca (subscriber, #118303) [Link] (12 responses) It's been a while, but IIRC if you skip setting a root password in the installer, then the created user will be added automatically to the sudo group Posted Aug 20, 2025 14:07 UTC (Wed) by rschroev (subscriber, #4164) [Link] (4 responses) &amp;gt; Alternatively, you can lock the root account's password by leaving this setting empty, and instead use the system's initial user account (which will be set up in the next step) to gain administrative privileges. This will be enabled for you by adding that initial user to the 'sudo' group. Admittedly it's quite a wall of text. If you do leave the root password empty (and only then) is sudo installed automatically, with a config file that grants sudo access to users in the sudo group, in addition to putting the initial user in that sudo group. Posted Aug 20, 2025 14:11 UTC (Wed) by jzb (editor, #7867) [Link] (2 responses) Posted Aug 20, 2025 14:17 UTC (Wed) by rschroev (subscriber, #4164) [Link] (1 responses) Posted Aug 21, 2025 11:40 UTC (Thu) by Karellen (subscriber, #67644) [Link] Posted Aug 20, 2025 14:14 UTC (Wed) by jzb (editor, #7867) [Link] Posted Aug 20, 2025 14:09 UTC (Wed) by jzb (editor, #7867) [Link] (4 responses) Posted Aug 20, 2025 17:48 UTC (Wed) by josh (subscriber, #17465) [Link] (3 responses) Posted Aug 21, 2025 10:55 UTC (Thu) by alx.manpages (subscriber, #145117) [Link] (2 responses) I want a root password for login as root, Which means that with the current installer I currently am forced to set up sudo(8) after installation. Posted Aug 21, 2025 12:43 UTC (Thu) by rschroev (subscriber, #4164) [Link] (1 responses) Posted Aug 21, 2025 19:07 UTC (Thu) by alx.manpages (subscriber, #145117) [Link] Yup, that's an alternative I always thought should be possible. I never tried it, though. Since I know my approach works, it always felt risky to try it in the other way. :) Also, I have a sudoers file that I just cp(1) into /etc/sudoers.d and it works, which is easy. (Although it is painful to install and configure sudo(8) until I actually have sudo(8).) Posted Aug 20, 2025 14:11 UTC (Wed) by smcv (subscriber, #53363) [Link] (1 responses) &amp;gt; To allow direct password-based access via the 'root' account, Posted Aug 29, 2025 9:26 UTC (Fri) by emorrp1 (guest, #99512) [Link] Posted Aug 20, 2025 16:06 UTC (Wed) by cjwatson (subscriber, #7322) [Link] Posted Aug 20, 2025 17:48 UTC (Wed) by josh (subscriber, #17465) [Link] I see what you did there. Posted Aug 21, 2025 4:17 UTC (Thu) by alison (subscriber, #63752) [Link] (5 responses) Posted Aug 21, 2025 13:59 UTC (Thu) by jzb (editor, #7867) [Link] (2 responses) If you're getting Firefox ESR from the Debian repositories, then it is updated by the Debian packagers with a number of patches applied. You can examine the patches applied to various versions here: https://sources.debian.org/patches/firefox-esr/. The new ML features postdate Firefox 128, I believe, so it's unclear right now if they'll turn those off or not. I wouldn't be surprised if they do... If you want current-ish Firefox without some of the AI-type stuff, you might check out LibreWolf or other forks. Posted Aug 22, 2025 4:52 UTC (Fri) by alison (subscriber, #63752) [Link] (1 responses) Posted Aug 29, 2025 13:17 UTC (Fri) by MKesper (subscriber, #38539) [Link] Posted Aug 22, 2025 3:04 UTC (Fri) by pabs (subscriber, #43278) [Link] Posted Aug 29, 2025 9:21 UTC (Fri) by emorrp1 (guest, #99512) [Link] https://tracker.debian.org/pkg/firefox-esr/news/?page=3 (for the exact dates we got 128.3) If you install from outside of debian (e.g. extrepo) then yes obviously it's cycle will be outside of distro control. Posted Aug 21, 2025 11:25 UTC (Thu) by alx.manpages (subscriber, #145117) [Link] (2 responses) This is not recommended. testing is the least secure flavour of Debian, as bug fixes are applied to stable (if appropriate), and also arrive at unstable (Sid) as normal patches, but due to migration policies, they can take months to arrive at testing. See &amp;lt;https://www.debian.org/doc/manuals/debian-faq/choosing.en...&amp;gt;. Posted Aug 22, 2025 3:14 UTC (Fri) by pabs (subscriber, #43278) [Link] (1 responses) https://wiki.debian.org/DebianTesting#Best_practices_for_... I have been using this setup for years, it works great. Posted Aug 22, 2025 7:19 UTC (Fri) by alx.manpages (subscriber, #145117) [Link] Recommending testing over unstable, saying that testing is for the bleeding edge and unstable is for the adventurous, that's at least a dangerous recommendation. Such a recommendation would need to come with a disclosure that unstable is safer (even if it might crash more often) and explains how to deal with the security issues in testing. Posted Aug 21, 2025 14:22 UTC (Thu) by tcabot (subscriber, #6656) [Link] (1 responses) Posted Aug 29, 2025 12:56 UTC (Fri) by kkremitzki (subscriber, #115703) [Link] https://packages.debian.org/trixie/all/extrepo-offline-da... &lt;head&gt;sudo user&lt;/head&gt;&lt;head&gt;sudo user&lt;/head&gt;&lt;head&gt;sudo user&lt;/head&gt;&lt;head&gt;sudo user&lt;/head&gt;&lt;head&gt;sudo user&lt;/head&gt;&lt;head&gt;sudo user&lt;/head&gt;&lt;head&gt;sudo user&lt;/head&gt;&lt;head&gt;sudo user&lt;/head&gt;&lt;head&gt;sudo user&lt;/head&gt;&lt;lb/&gt; and I also want sudo(8) for my primary account.&lt;head&gt;sudo user&lt;/head&gt;&lt;head&gt;sudo user&lt;/head&gt;&lt;head&gt;sudo user&lt;/head&gt;&lt;lb/&gt; &amp;gt; you can set the password for that account here.&lt;lb/&gt; &amp;gt; .&lt;lb/&gt; &amp;gt; Alternatively, you can lock the root account's password&lt;lb/&gt; &amp;gt; by leaving this setting empty, and&lt;lb/&gt; &amp;gt; instead use the system's initial user account&lt;lb/&gt; &amp;gt; (which will be set up in the next step)&lt;lb/&gt; &amp;gt; to gain administrative privileges.&lt;lb/&gt; &amp;gt; This will be enabled for you&lt;lb/&gt; &amp;gt; by adding that initial user to the 'sudo' group.&lt;head&gt;sudo user&lt;/head&gt;&lt;head&gt;64-bit time_t&lt;/head&gt;&lt;head&gt;aptly named&lt;/head&gt;&lt;head&gt;thanks for the informative article&lt;/head&gt;&lt;head&gt;thanks for the informative article&lt;/head&gt;&lt;head&gt;thanks for the informative article&lt;/head&gt;&lt;head&gt;thanks for the informative article&lt;/head&gt;&lt;lb/&gt; It's configurable and can also be disabled, if you decide to do so.&lt;lb/&gt; I never saw Firefox doing a restart itself, though. It displays a message to restart it myself to be able to continue browsing.&lt;head&gt;thanks for the informative article&lt;/head&gt;&lt;head&gt;thanks for the informative article&lt;/head&gt;&lt;head&gt;testing is for actually testing. unstable is for users who want the bleeding edge&lt;/head&gt;&lt;head&gt;testing is for actually testing. unstable is for users who want the bleeding edge&lt;/head&gt;&lt;head&gt;testing is for actually testing. unstable is for users who want the bleeding edge&lt;/head&gt;&lt;head&gt;TIL about extrepo&lt;/head&gt;&lt;head&gt;TIL about extrepo&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45059160</guid></item><item><title>PSA: Libxslt is unmaintained and has 5 unpatched security bugs</title><link>https://vuxml.freebsd.org/freebsd/b0a3466f-5efc-11f0-ae84-99047d0a6bcc.html</link><description>&lt;doc fingerprint="1599f515f4b7b7f2"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;On 6/16/25 15:12, Alan Coopersmith wrote:&lt;/p&gt;
      &lt;p&gt; BTW, users of libxml2 may also be using its sibling project, libxslt, which currently has no active maintainer, but has three unfixed security issues reported against it according to https://gitlab.gnome.org/Teams/Releng/security/-/wikis/2025#libxml2-and-libxslt &lt;/p&gt;
      &lt;p&gt;2 of the 3 have now been disclosed:&lt;/p&gt;
      &lt;p&gt;(CVE-2025-7424) libxslt: Type confusion in xmlNode.psvi between stylesheet and source nodes&lt;lb/&gt; https://gitlab.gnome.org/GNOME/libxslt/-/issues/139 https://project-zero.issues.chromium.org/issues/409761909&lt;/p&gt;
      &lt;p&gt;(CVE-2025-7425) libxslt: heap-use-after-free in xmlFreeID caused by `atype` corruption&lt;lb/&gt; https://gitlab.gnome.org/GNOME/libxslt/-/issues/140&lt;lb/&gt;https://project-zero.issues.chromium.org/issues/410569369&lt;/p&gt;
      &lt;p&gt;Engineers from Apple &amp;amp; Google have proposed patches in the GNOME gitlab issues, but neither has had a fix applied to the git repo since there is currently no maintainer for libxslt.&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45060004</guid></item><item><title>Strange CW Keys</title><link>https://sites.google.com/site/oh6dccw/strangecwkeys</link><description>&lt;doc fingerprint="7ff4f9d4ca9ca5f5"&gt;
  &lt;main&gt;
    &lt;p&gt;Made by OH6DC&lt;/p&gt;
    &lt;p&gt;You can also use the Text-only index page (divided into useful categories).&lt;/p&gt;
    &lt;p&gt;Lever arch file CW key&lt;/p&gt;
    &lt;p&gt;Lambic pedals&lt;/p&gt;
    &lt;p&gt;Valentine's day lollipop CW paddle&lt;/p&gt;
    &lt;p&gt;Rubber stamp CW key&lt;/p&gt;
    &lt;p&gt;Letter scale CW key&lt;/p&gt;
    &lt;p&gt;Clamp cootie&lt;/p&gt;
    &lt;p&gt;Code book&lt;/p&gt;
    &lt;p&gt;Pepper mill CW key&lt;/p&gt;
    &lt;p&gt;Lightsaber CW key&lt;/p&gt;
    &lt;p&gt;Nutcracker CW key&lt;/p&gt;
    &lt;p&gt;Straight(ener) key&lt;/p&gt;
    &lt;p&gt;Smoke alarm CW key&lt;/p&gt;
    &lt;p&gt;Teletubbygraph key&lt;/p&gt;
    &lt;p&gt;Soap dispenser CW key&lt;/p&gt;
    &lt;p&gt;Vinyl record player CW key&lt;/p&gt;
    &lt;p&gt;Moomin triangle CW key&lt;/p&gt;
    &lt;p&gt;Antiperspirant roll-on CW key&lt;/p&gt;
    &lt;p&gt;Dual banana CW paddle&lt;/p&gt;
    &lt;p&gt;Power twister CW key&lt;/p&gt;
    &lt;p&gt;Handsaw CW key&lt;/p&gt;
    &lt;p&gt;Hole punch CW key&lt;/p&gt;
    &lt;p&gt;Watering can CW key&lt;/p&gt;
    &lt;p&gt;Toilet brush CW key&lt;/p&gt;
    &lt;p&gt;CW glove&lt;/p&gt;
    &lt;p&gt;Remote control CW key&lt;/p&gt;
    &lt;p&gt;Tea bag CW key&lt;/p&gt;
    &lt;p&gt;Eyebrow-raising CW key with optical transmitter&lt;/p&gt;
    &lt;p&gt;Back scratcher CW key&lt;/p&gt;
    &lt;p&gt;Whisk CW key&lt;/p&gt;
    &lt;p&gt;Pliers CW key&lt;/p&gt;
    &lt;p&gt;Liver casserole CW key&lt;/p&gt;
    &lt;p&gt;Licorice pipe CW key&lt;/p&gt;
    &lt;p&gt;Chocolate CW key&lt;/p&gt;
    &lt;p&gt;Ski-W key&lt;/p&gt;
    &lt;p&gt;Power drill CW keyer&lt;/p&gt;
    &lt;p&gt;Six megapixel CW key&lt;/p&gt;
    &lt;p&gt;Suspenders CW key&lt;/p&gt;
    &lt;p&gt;Spirit bottle cap CW key&lt;/p&gt;
    &lt;p&gt;Speed skate CW key&lt;/p&gt;
    &lt;p&gt;Flower CW key&lt;/p&gt;
    &lt;p&gt;Knee pad sideswiper CW key for portable operation&lt;/p&gt;
    &lt;p&gt;QRP transmitter powered by a CW key&lt;/p&gt;
    &lt;p&gt;Alarm clock CW key&lt;/p&gt;
    &lt;p&gt;Hammer CW key&lt;/p&gt;
    &lt;p&gt;CW gun&lt;/p&gt;
    &lt;p&gt;Nail clipper CW key&lt;/p&gt;
    &lt;p&gt;Ballpoint pen CW key&lt;/p&gt;
    &lt;p&gt;Rotary dial CW key&lt;/p&gt;
    &lt;p&gt;Hammock CW key&lt;/p&gt;
    &lt;p&gt;Joystick CW key&lt;/p&gt;
    &lt;p&gt;Rowing boat CW key&lt;/p&gt;
    &lt;p&gt;Guitar CW key&lt;/p&gt;
    &lt;p&gt;Wallet CW key&lt;/p&gt;
    &lt;p&gt;Radio controlled CW key&lt;/p&gt;
    &lt;p&gt;Amaryllis telegraphiensis&lt;/p&gt;
    &lt;p&gt;Multi-function knife with CW key&lt;/p&gt;
    &lt;p&gt;Toilet paper roll CW key&lt;/p&gt;
    &lt;p&gt;Table ice hockey CW key&lt;/p&gt;
    &lt;p&gt;Big toe CW key&lt;/p&gt;
    &lt;p&gt;Waffle iron CW key&lt;/p&gt;
    &lt;p&gt;Lego straight key&lt;/p&gt;
    &lt;p&gt;Lego bug&lt;/p&gt;
    &lt;p&gt;Pogo stick CW key&lt;/p&gt;
    &lt;p&gt;Crutch CW key&lt;/p&gt;
    &lt;p&gt;Smoke signal CW key&lt;/p&gt;
    &lt;p&gt;CCW key&lt;/p&gt;
    &lt;p&gt;Necktie CW key&lt;/p&gt;
    &lt;p&gt;Toothbrush CW key&lt;/p&gt;
    &lt;p&gt;Bench press CW key&lt;/p&gt;
    &lt;p&gt;Handshake CW key&lt;/p&gt;
    &lt;p&gt;Chopsticks CW key&lt;/p&gt;
    &lt;p&gt;Trailer hitch CW key&lt;/p&gt;
    &lt;p&gt;Typewriter CW keyboard&lt;/p&gt;
    &lt;p&gt;Refrigerator CW key&lt;/p&gt;
    &lt;p&gt;Mobile phone CW key&lt;/p&gt;
    &lt;p&gt;Paper cup iambic paddles&lt;/p&gt;
    &lt;p&gt;Morsetrap CW key&lt;/p&gt;
    &lt;p&gt;Fingertips CW key&lt;/p&gt;
    &lt;p&gt;Vacuum cleaner semi-automatic CW key&lt;/p&gt;
    &lt;p&gt;Banana CW key&lt;/p&gt;
    &lt;p&gt;Rolling pin CW key&lt;/p&gt;
    &lt;p&gt;Toaster CW key&lt;/p&gt;
    &lt;p&gt;Cheese slicer CW key&lt;/p&gt;
    &lt;p&gt;Rocking chair CW key&lt;/p&gt;
    &lt;p&gt;QLF pedal for left foot CW&lt;/p&gt;
    &lt;p&gt;Cross-country ski shoe CW key&lt;/p&gt;
    &lt;p&gt;CW insoles&lt;/p&gt;
    &lt;p&gt;QRQ paddles&lt;/p&gt;
    &lt;p&gt;Onion chopper CW key&lt;/p&gt;
    &lt;p&gt;Beer can CW key&lt;/p&gt;
    &lt;p&gt;Egg slicer CW key&lt;/p&gt;
    &lt;p&gt;Stapler CW key&lt;/p&gt;
    &lt;p&gt;Bicycle pump CW key&lt;/p&gt;
    &lt;p&gt;Iron bar CW key&lt;/p&gt;
    &lt;p&gt;Homebrew semi-automatic bug&lt;/p&gt;
    &lt;p&gt;Hacksaw blade sideswiper CW key&lt;/p&gt;
    &lt;p&gt;Plywood CW key&lt;/p&gt;
    &lt;p&gt;Home | Homebrew QRP&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45060161</guid></item><item><title>Contrastive Representations for Temporal Reasoning</title><link>https://princeton-rl.github.io/CRTR/</link><description>&lt;doc fingerprint="2bf0677682200a82"&gt;
  &lt;main&gt;
    &lt;p&gt;Key Question: Can we improve temporal reasoning by learning better representations?&lt;/p&gt;
    &lt;p&gt;Method: A novel negative sampling scheme provably enables learning temporal representations.&lt;/p&gt;
    &lt;p&gt;Result: CRTR learns representations that enable solving Rubik's Cube without hand-crafted heuristics and improves the success rates overall.&lt;/p&gt;
    &lt;p&gt;In classical AI, perception relies on learning spatial representations, while planning‚Äîtemporal reasoning over action sequences‚Äîis typically achieved through search. We study whether such reasoning can instead emerge from representations that capture both spatial and temporal structure. We show that standard temporal contrastive learning, despite its popularity, often fails to capture temporal structure, due to reliance on spurious features. To address this, we introduce Contrastive Representations for Temporal Reasoning (CRTR), a method that uses a negative sampling scheme to provably remove these spurious features and facilitate temporal reasoning. CRTR achieves strong results on domains with complex temporal structure, such as Sokoban and Rubik‚Äôs Cube. In particular, for the Rubik‚Äôs Cube, CRTR learns representations that generalize across all initial states and allow solving the puzzle much faster than BestFS‚Äîthough with longer solutions. To our knowledge, this is the first demonstration of efficiently solving arbitrary Cube states using only learned representations, without hand-crafted search heuristics.&lt;/p&gt;
    &lt;p&gt;CRTR performs well in all the evaluated domains. Success rate as a function of search budget across five domains. CRTR compared to baselines: CRL, Supervised and DeepCubeA. Results are averaged over 5 seeds; shaded regions indicate standard error. For the Rubik's Cube, both the supervised and random baselines achieve a success rate of zero for all search budgets.&lt;/p&gt;
    &lt;p&gt;Distances given by CRTR representations reflect the temporal structure well. Correlation (Spearman's œÅ) between the distance induced by learned embeddings and actual distance across the training, CRTR compared with CRL.&lt;/p&gt;
    &lt;p&gt;CRTR solves most tasks without requiring any search. Adding BestFS results in shorter solutions. We plot the fraction of configurations solved with a solution length of at most x, while limiting the number of nodes created to 6000. Surprisingly, on the Rubik's cube CRTR achieves a higher success rate without search, solving all board configurations within the budget.&lt;/p&gt;
    &lt;p&gt;CRTR without search exhibits a block-building-like behavior. Intermediate states from solving a randomly scrambled cube, illustrating how the algorithm gradually builds partial structure. The average solve is about 400 moves, and we see similar block building behavior across solves.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45061689</guid></item><item><title>Probability of typing a wrong Bitcoin address</title><link>https://www.johndcook.com/blog/2025/08/28/wrong-address/</link><description>&lt;doc fingerprint="6bf701d5a06fb3fd"&gt;
  &lt;main&gt;
    &lt;p&gt;I heard someone say that Bitcoin is dangerous because you could easily make a typo when entering an address, sending money to the wrong person, and have no recourse. There are dangers associated with Bitcoin, such as losing a private key, but address typos are not a major concern.&lt;/p&gt;
    &lt;head rend="h2"&gt;Checksums&lt;/head&gt;
    &lt;p&gt;There are several kinds of Bitcoin addresses. Each is at least 20 bytes (160 bits) long, with at least 4 bytes (32 bits) of checksum. The chances of a typo resulting in a valid checksum are about 1 in 232.&lt;/p&gt;
    &lt;head rend="h2"&gt;Used addresses&lt;/head&gt;
    &lt;p&gt;Let‚Äôs ignore the checksum for this section.&lt;/p&gt;
    &lt;p&gt;Because addresses are formed by cryptographic hash functions, we can assume the values are essentially randomly distributed in the space of possible addresses. The addresses are deterministic, but for modeling purposes, random is as random does.&lt;/p&gt;
    &lt;p&gt;This means a typo of an actual address is no more or less likely to be another actual address than an address typed at random. This is unlike, say, English words: a mistyped English word is more likely to be another English word than random keystrokes would be.&lt;/p&gt;
    &lt;p&gt;There have been on the order of a billion Bitcoin addresses used, in a space of 2160 possibilities. (Actually more since some addresses have more than 160 bits.) There‚Äôs about a 1 in 1039 chance that a random 160-bit sequence corresponds to an address somewhere on the Bitcoin blockchain.&lt;/p&gt;
    &lt;head rend="h2"&gt;Addresses close in edit distance&lt;/head&gt;
    &lt;p&gt;Someone with the Caesarean handle Veni Vidi Vici on X asked&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;What about the odds that out of those 1B addresses, two of them are one character swap away from each other?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That‚Äôs an interesting question. Let‚Äôs assume the addresses are Base58-encoded strings of length 26. Addresses could be longer, but assuming the minimum length increases the probability of addresses being close.&lt;/p&gt;
    &lt;p&gt;How many addresses are within one or two character swaps of another? I addressed a similar question here a couple weeks ago. If all the characters were unique, the number of strings within k swaps of each other would be&lt;/p&gt;
    &lt;p&gt;|S1(26, 26 ‚àí k)|&lt;/p&gt;
    &lt;p&gt;where S1 denotes Stirling numbers of the first kind. For k = 1 this would be 325 and for k = 2 this would be 50,050. This assumes all the characters are unique; I haven‚Äôt thought through the case where characters are repeated.&lt;/p&gt;
    &lt;p&gt;For round numbers, let‚Äôs say there are a billion addresses, and for each address there are a million other addresses that are close in some sense, plausible typos of the address. That would be 1012 addresses and typos, spread out in a space of ‚âà1045 (i.e. 5826) possible addresses.&lt;/p&gt;
    &lt;p&gt;Now there‚Äôs an implicit Birthday Problem here. No particular address is likely to collide with another, even when you allow typos, but what about the likelihood that some address collides?&lt;/p&gt;
    &lt;p&gt;Say we partition our space of 1045 addresses into N = 1029 addresses with a million possible typos for each address. Then as a rule of thumb, you‚Äôd need around ‚àöN random draws before you have a 50-50 chance of seeing a collision. Since 109 is a lot less than 1014.5, it‚Äôs unlikely that any two addresses collide, even when you consider each address along with a million associated typos.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45061980</guid></item><item><title>Tesla said it didn't have key data in a fatal crash, then a hacker found it</title><link>https://www.washingtonpost.com/technology/2025/08/29/tesla-autopilot-crashes-evidence-testimony-wrongful-death/</link><description>&lt;doc fingerprint="dd3fc976799bc8be"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Updates to Consumer Terms and Privacy Policy&lt;/head&gt;
    &lt;p&gt;Today, we're rolling out updates to our Consumer Terms and Privacy Policy that will help us deliver even more capable, useful AI models. We're now giving users the choice to allow their data to be used to improve Claude and strengthen our safeguards against harmful usage like scams and abuse. Adjusting your preferences is easy and can be done at any time.&lt;/p&gt;
    &lt;p&gt;These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude Gov, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud‚Äôs Vertex AI.&lt;/p&gt;
    &lt;p&gt;By participating, you‚Äôll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You‚Äôll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.&lt;/p&gt;
    &lt;p&gt;You‚Äôre always in control of this setting and whether we use your data in this way. If you‚Äôre a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.&lt;/p&gt;
    &lt;p&gt;Starting today, we‚Äôre rolling out notifications so you can review these updates and manage your settings. If you‚Äôre an existing user, you have until September 28, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After September 28, you‚Äôll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Extended data retention&lt;/head&gt;
    &lt;p&gt;We are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you‚Äôll continue with our existing 30-day data retention period.&lt;/p&gt;
    &lt;p&gt;The new five-year retention period will also apply to feedback you submit to us about Claude‚Äôs responses to prompts.&lt;/p&gt;
    &lt;p&gt;To protect users‚Äô privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users‚Äô data to third parties.&lt;/p&gt;
    &lt;p&gt;You can find more details about the Consumer Terms and Privacy Policy updates in our FAQ section below.&lt;/p&gt;
    &lt;head rend="h4"&gt;FAQ&lt;/head&gt;
    &lt;head rend="h4"&gt;What‚Äôs changing?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).&lt;list rend="ul"&gt;&lt;item&gt;If you‚Äôre a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until September 28, 2025 to make your selection.&lt;/item&gt;&lt;item&gt;If you‚Äôre a new user, you can pick your setting for model training during the signup process.&lt;/item&gt;&lt;item&gt;You can change your selection at any time in your Privacy Settings.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These updates do not apply to services under our Commercial Terms, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude for Work, which includes our Team and Enterprise plans&lt;/item&gt;
      &lt;item&gt;Our API, Amazon Bedrock, or Google Cloud‚Äôs Vertex API&lt;/item&gt;
      &lt;item&gt;Claude Gov and Claude for Education&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Why are you making this change?&lt;/head&gt;
    &lt;p&gt;All large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.&lt;/p&gt;
    &lt;p&gt;It‚Äôs up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why are you extending the data retention period?&lt;/head&gt;
    &lt;p&gt;AI development cycles span years‚Äîmodels released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.&lt;/p&gt;
    &lt;p&gt;The extended retention period also helps us improve our classifiers‚Äîsystems that help us identify misuse‚Äîto detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.&lt;/p&gt;
    &lt;p&gt;If you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.&lt;/p&gt;
    &lt;head rend="h4"&gt;What action do I need to take?&lt;/head&gt;
    &lt;p&gt;Current users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select "not now" and decide later. You have until September 28, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once September 28 arrives, you'll need to select your preference to continue using Claude.&lt;/p&gt;
    &lt;p&gt;If you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember‚Äîyou can always update your preference in Privacy Settings.&lt;/p&gt;
    &lt;head rend="h4"&gt;What happens if I allow my data to be used for model training and then change my mind?&lt;/head&gt;
    &lt;p&gt;You can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45062614</guid></item><item><title>Updates to Consumer Terms and Privacy Policy</title><link>https://www.anthropic.com/news/updates-to-our-consumer-terms</link><description>&lt;doc fingerprint="dd3fc976799bc8be"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Updates to Consumer Terms and Privacy Policy&lt;/head&gt;
    &lt;p&gt;Today, we're rolling out updates to our Consumer Terms and Privacy Policy that will help us deliver even more capable, useful AI models. We're now giving users the choice to allow their data to be used to improve Claude and strengthen our safeguards against harmful usage like scams and abuse. Adjusting your preferences is easy and can be done at any time.&lt;/p&gt;
    &lt;p&gt;These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude Gov, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud‚Äôs Vertex AI.&lt;/p&gt;
    &lt;p&gt;By participating, you‚Äôll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You‚Äôll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.&lt;/p&gt;
    &lt;p&gt;You‚Äôre always in control of this setting and whether we use your data in this way. If you‚Äôre a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.&lt;/p&gt;
    &lt;p&gt;Starting today, we‚Äôre rolling out notifications so you can review these updates and manage your settings. If you‚Äôre an existing user, you have until September 28, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After September 28, you‚Äôll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Extended data retention&lt;/head&gt;
    &lt;p&gt;We are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you‚Äôll continue with our existing 30-day data retention period.&lt;/p&gt;
    &lt;p&gt;The new five-year retention period will also apply to feedback you submit to us about Claude‚Äôs responses to prompts.&lt;/p&gt;
    &lt;p&gt;To protect users‚Äô privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users‚Äô data to third parties.&lt;/p&gt;
    &lt;p&gt;You can find more details about the Consumer Terms and Privacy Policy updates in our FAQ section below.&lt;/p&gt;
    &lt;head rend="h4"&gt;FAQ&lt;/head&gt;
    &lt;head rend="h4"&gt;What‚Äôs changing?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).&lt;list rend="ul"&gt;&lt;item&gt;If you‚Äôre a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until September 28, 2025 to make your selection.&lt;/item&gt;&lt;item&gt;If you‚Äôre a new user, you can pick your setting for model training during the signup process.&lt;/item&gt;&lt;item&gt;You can change your selection at any time in your Privacy Settings.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These updates do not apply to services under our Commercial Terms, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude for Work, which includes our Team and Enterprise plans&lt;/item&gt;
      &lt;item&gt;Our API, Amazon Bedrock, or Google Cloud‚Äôs Vertex API&lt;/item&gt;
      &lt;item&gt;Claude Gov and Claude for Education&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Why are you making this change?&lt;/head&gt;
    &lt;p&gt;All large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.&lt;/p&gt;
    &lt;p&gt;It‚Äôs up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why are you extending the data retention period?&lt;/head&gt;
    &lt;p&gt;AI development cycles span years‚Äîmodels released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.&lt;/p&gt;
    &lt;p&gt;The extended retention period also helps us improve our classifiers‚Äîsystems that help us identify misuse‚Äîto detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.&lt;/p&gt;
    &lt;p&gt;If you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.&lt;/p&gt;
    &lt;head rend="h4"&gt;What action do I need to take?&lt;/head&gt;
    &lt;p&gt;Current users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select "not now" and decide later. You have until September 28, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once September 28 arrives, you'll need to select your preference to continue using Claude.&lt;/p&gt;
    &lt;p&gt;If you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember‚Äîyou can always update your preference in Privacy Settings.&lt;/p&gt;
    &lt;head rend="h4"&gt;What happens if I allow my data to be used for model training and then change my mind?&lt;/head&gt;
    &lt;p&gt;You can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45062683</guid></item><item><title>If you have a Claude account, they're going to train on your data moving forward</title><link>https://old.reddit.com/r/LocalLLaMA/comments/1n2ubjx/if_you_have_a_claude_personal_account_they_are/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45062738</guid></item><item><title>Meta might be secretly scanning your phone's camera roll</title><link>https://www.zdnet.com/article/meta-might-be-secretly-scanning-your-phones-camera-roll-how-to-check-and-turn-it-off/</link><description>&lt;doc fingerprint="b4b2db1208265aff"&gt;
  &lt;main&gt;
    &lt;p&gt;'ZDNET Recommends': What exactly does it mean?&lt;/p&gt;
    &lt;p&gt;ZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we‚Äôre assessing.&lt;/p&gt;
    &lt;p&gt;When you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.&lt;/p&gt;
    &lt;p&gt;ZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.&lt;/p&gt;
    &lt;head rend="h1"&gt;Meta might be secretly scanning your phone's camera roll - how to check and turn it off&lt;/head&gt;
    &lt;p&gt;Follow ZDNET: Add us as a preferred source on Google.&lt;/p&gt;
    &lt;head rend="h3"&gt;ZDNET's key takeaways&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Meta could be scanning your camera roll right now.&lt;/item&gt;
      &lt;item&gt;It's using your photos to provide AI-powered suggestions.&lt;/item&gt;
      &lt;item&gt;Check Facebook settings to turn off the features.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Meta could be analyzing and retaining your phone's photos without your explicit consent.&lt;/p&gt;
    &lt;p&gt;Some Facebook users have noticed that, deep within their app settings, Meta has switched on two toggles that allow it to access their camera roll to offer AI-powered suggestions, including "personalized creative ideas, like travel highlights and collages."&lt;/p&gt;
    &lt;p&gt;Also: How to delete Facebook, Messenger, or Instagram - if you want Meta out of your life&lt;/p&gt;
    &lt;p&gt;The problem? The toggles for the AI suggestion features, called "camera roll sharing suggestions," appear to be turned on for users who claim they haven't seen a pop-up from Facebook asking for permission to enable them. If you get that "cloud processing" pop-up and tap "Allow" on it, you'll agree to Meta's AI Terms of Service and permit your "media and facial features" to be analyzed by AI.&lt;/p&gt;
    &lt;p&gt;Facebook will then use your camera roll images -- including the dates on them and the presence of people or objects -- to suggest collages, themed albums, recap posts, or AI restyled versions of your pictures. These AI suggestions are only visible to you, unless you choose to share them, and Meta says the media won't be used for ad targeting.&lt;/p&gt;
    &lt;p&gt;Also: How to protect your privacy from Facebook - and what doesn't work&lt;/p&gt;
    &lt;p&gt;But, to be clear, you're still giving Meta the right to access and retain your camera roll images, and that could raise serious privacy concerns, especially for users who never knowingly opted in.&lt;/p&gt;
    &lt;p&gt;ZDNET's editorial director found Meta's camera roll sharing suggestions enabled in her Facebook app without her knowledge. I also noticed they were enabled for me, although I vaguely recall seeing a pop-up from Facebook about the new features a few weeks ago. I think I dismissed it quickly, and I can't remember whether I tapped Allow or Don't allow on it.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to stop Facebook from scanning your camera roll&lt;/head&gt;
    &lt;p&gt;Meta said its camera roll sharing suggestions are not enabled by default. If you're worried you dismissed Facebook's pop-up, unknowingly opted-in, and gave access to your camera roll, here's how to check and turn it off.&lt;/p&gt;
    &lt;head rend="h2"&gt;1. Open the Facebook app&lt;/head&gt;
    &lt;p&gt;The settings you'll want to check can be found in the Facebook mobile app.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Grab your iPhone or Android phone.&lt;/item&gt;
      &lt;item&gt;Open the Facebook app. You'll need to be signed into your account.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;2. Go to the Menu &amp;gt; Settings and Privacy&lt;/head&gt;
    &lt;p&gt;Facebook hides most of its settings in the menu -- the three-line hamburger icon in the bottom corner of the app.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Tap the Menu icon in the bottom right corner of the screen.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Once the menu opens, look for Settings and Privacy with a gear icon. This will take you directly to Settings.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;3. Select Settings&lt;/head&gt;
    &lt;p&gt;Once you find and tap Privacy and Settings to expand the dropdown options, tap Settings again.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Under Settings and Privacy, tap Settings.&lt;/item&gt;
      &lt;item&gt;Now, scroll down and look for "Camera roll sharing suggestions."&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;4. Go to Camera roll sharing suggestions&lt;/head&gt;
    &lt;p&gt;Meta placed the toggles that grant it access to your camera roll under the "Camera roll sharing suggestions" setting. You'll need to go there to see if they're on and, if so, switch them off.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Look for the option labeled "Camera roll sharing suggestions" and tap it.&lt;/item&gt;
      &lt;item&gt;This will open a preference page with a couple of toggles.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;5. Turn off both toggles&lt;/head&gt;
    &lt;p&gt;Once you're inside the camera roll sharing suggestions page, notice the two separate switches. If they're blue and the toggle circle is pushed to the right, they're on -- meaning Meta is already processing and retaining your phone's photos. Turn them off so the app can't automatically upload and analyze your camera roll.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find the option labeled "Get camera roll suggestions when you're browsing Facebook." If the switch is on (blue), tap it once to turn it off (gray). This will stop Facebook from using basic camera roll data, such as which videos you've favorited and when photos were taken, to suggest sharing media you haven't yet uploaded.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Find the option labeled "Get creative ideas made for you by allowing camera roll cloud processing." If the switch is on (blue), tap it once to turn it off (gray). This will stop Facebook from continuously uploading media from your camera roll -- and using details like time, location, themes, and the presence of people or objects -- to generate personalized creative ideas such as recaps and AI restylings.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;FAQs&lt;/head&gt;
    &lt;head rend="h3"&gt;Why is Facebook cloud-processing my device's camera roll?&lt;/head&gt;
    &lt;p&gt;Meta is uploading and analyzing your camera roll photos and videos, even ones you haven't posted, in its cloud in order to generate AI-powered suggestions like collages, monthly recaps, themed albums, or AI-restyled versions of your images.&lt;/p&gt;
    &lt;head rend="h3"&gt;Where is this feature being tested?&lt;/head&gt;
    &lt;p&gt;Meta has confirmed the feature is a test, saying, "We're exploring ways to make content sharing easier for people on Facebook by testing suggestions of ready-to-share and curated content from a person's camera roll."&lt;/p&gt;
    &lt;p&gt;The test is currently available in the US and Canada, but it's not available in Illinois or Texas due to those states' privacy laws.&lt;/p&gt;
    &lt;head rend="h3"&gt;Did Facebook ask for my consent before turning this on?&lt;/head&gt;
    &lt;p&gt;Meta is showing a pop-up asking users if they want to enable cloud processing, but some users claim they haven't seen it. Instead, they found the toggles in their settings already switched on by default, raising questions about whether clear consent was given.&lt;/p&gt;
    &lt;head rend="h3"&gt;Can I remove my photos once they've been uploaded?&lt;/head&gt;
    &lt;p&gt;ZDNET's sister site, CNET, reports that Meta pulls from your newer pictures (roughly the last 30 days) and if you disable the feature, your uploaded photos will be deleted after 30 days. The only way to confirm is by downloading your Facebook account data.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why is this a potential privacy issue?&lt;/head&gt;
    &lt;p&gt;It expands Meta's reach beyond the content you've chosen to upload and share online -- into your private, unposted photos and videos. For many, that's a major red flag and a line they're not comfortable crossing, understandably so.&lt;/p&gt;
    &lt;p&gt;Also: What Zuckerberg's 'personal superintelligence' sales pitch leaves out&lt;/p&gt;
    &lt;p&gt;Even if Meta is asking for consent to access your camera roll in order to analyze your phone's photos and provide AI-powered suggestions, the company could have done a better job of being clear and explicit about what it's trying to do.&lt;/p&gt;
    &lt;p&gt;How many users, like me, simply dismissed the consent pop-up without fully realizing what they'd just agreed to?&lt;/p&gt;
    &lt;p&gt;Editor's note: This article was updated on Aug. 24, 2025 to clarify that Meta's camera roll sharing suggestions are not turned on by default and are entirely opt-in. Still, some users say they never knowingly agreed and are finding the features enabled in their settings.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45062910</guid></item><item><title>Deepnote (YC S19) is hiring engineers to build a better Jupyter notebook</title><link>https://deepnote.com/join-us</link><description>&lt;doc fingerprint="b9d2255309e5a6a1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;We build tools for explorers&lt;/head&gt;
    &lt;p&gt;We are here to revolutionize how data teams work together.&lt;/p&gt;
    &lt;p&gt;We started Deepnote to help data teams solve the hardest problems. We don‚Äôt just need better algorithms, bigger data sets, and more computing power. We need tools that help us explore, collaborate, and share. These tools don‚Äôt exist yet. We need to invent them first.&lt;/p&gt;
    &lt;p&gt;Data work is as much a scientific and creative process as it is an engineering one. It involves working together, failing, learning, and going back to the drawing board. Data professionals are explorers. To make projects successful, we need tools that are both powerful and easy to use. Tools that help us collaborate and share our work in an engaging way. Tools that make working with data fun again.&lt;/p&gt;
    &lt;p&gt;That‚Äôs why we‚Äôre building the new standard in data tooling: a notebook that brings teams together to code, query, visualize, organize, and share ‚Äî all in one place.&lt;/p&gt;
    &lt;p&gt;We are building tools for explorers. Join us.&lt;/p&gt;
    &lt;p&gt;Read more about us on TechCrunch&lt;/p&gt;
    &lt;p&gt;Read more about us on Nature&lt;/p&gt;
    &lt;head rend="h2"&gt;Build the future with us&lt;/head&gt;
    &lt;p&gt;We‚Äôre building a collaborative notebook that beautifully integrates analytics and data science into every workflow and decision. But it‚Äôs not just about designing, shipping, and selling. It‚Äôs about the people who power it ‚Äî and that means you.&lt;/p&gt;
    &lt;head rend="h2"&gt;Get ready to do your best work&lt;/head&gt;
    &lt;p&gt;Transforming how people work with data isn't easy. But we built a culture that allows us to do precisely that.&lt;/p&gt;
    &lt;head rend="h3"&gt;We move with urgency&lt;/head&gt;
    &lt;p&gt;We are a small, passionate team revolutionizing how data teams work. We give everyone the tools they need and enable them to take action.&lt;/p&gt;
    &lt;head rend="h3"&gt;We keep learning&lt;/head&gt;
    &lt;p&gt;We are knowledge-seekers. We invest in continuous learning across every role and encourage a culture of proactive feedback.&lt;/p&gt;
    &lt;head rend="h3"&gt;We take ownership&lt;/head&gt;
    &lt;p&gt;We are makers. We expect everyone to be a decision-maker ‚Äî no politics or walls to get in the way.&lt;/p&gt;
    &lt;head rend="h3"&gt;We collaborate&lt;/head&gt;
    &lt;p&gt;We are partners. We work in a fully transparent environment and put open, effective communication above all else.&lt;/p&gt;
    &lt;head rend="h2"&gt;Backed by the best in the business&lt;/head&gt;
    &lt;p&gt;We‚Äôre backed by industry leaders ‚Äî and they‚Äôre as excited about reimagining the future as we are.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Y Combinator&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Index Ventures&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Accel&lt;/head&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Greg Brockman&lt;p&gt;CTO at OpenAI&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;Elad Gil&lt;p&gt;Angel Investor&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;Naval Ravikant&lt;p&gt;Angel Investor&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;Elena Verna&lt;p&gt;Angel Investor&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Explore open positions&lt;/head&gt;
    &lt;p&gt;Thousands of data professionals already use Deepnote ‚Äî but we‚Äôre only scratching the surface of what‚Äôs possible. We‚Äôre building out our core team, and we want kind, curious explorers to join and grow with us.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45062914</guid></item><item><title>Gun Maker Sig Sauer Citing National Security to Keep Documents from Public</title><link>https://practicalshootinginsights.com/eighth-circuit-fmeca-update/</link><description>&lt;doc fingerprint="df1fc178385f5482"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;In the Eighth Circuit, the Fight Over Sig Sauer‚Äôs P320 FMECA Goes Public&lt;/head&gt;
    &lt;p&gt;The secrecy battle over the Army‚Äôs Failure Modes, Effects, and Criticality Analysis (FMECA) for Sig Sauer‚Äôs P320 has followed Glasscock v. Sig Sauer to the Eighth Circuit. A media intervenor is now asking the appellate court to keep key records open‚Äîand their brief places Practical Shooting Insights (this site) squarely in the middle of the story.&lt;/p&gt;
    &lt;head rend="h2"&gt;What‚Äôs new&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Trace intervenes in the appeal. The newsroom moved to intervene for the limited purpose of opposing sealed filings tied to class certification and the FMECA, arguing the public‚Äôs right of access and noting the district court cited the FMECA nine times when it certified the class.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sig Sauer says ‚Äúnational security‚Äù and asks for deference to the Army. In opposing intervention, Sig Sauer urges the court to leave FMECA-related material sealed and to give the Army time to weigh in, framing the dispute in terms of protecting ‚Äúmilitary secrets.‚Äù&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A second FMECA document emerges. Sig Sauer‚Äôs opposition confirms there are two FMECA records in the class-certification exhibits: a FMECA Spreadsheet and a FMECA Memorandum‚Äîthe latter not previously described in public filings‚Äîraising fresh questions about what the memo contains and who authored it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PSI‚Äôs reporting is part of the record. The Trace‚Äôs filing tells the court the unredacted FMECA was found on CourtListener, de‚Äëobscured, and published on Practical Shooting Insights, where it ‚Äúremains available‚Äù‚Äîand it recounts Sig Sauer‚Äôs own executive discussing it on a podcast while pointing viewers to this website.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The FMECA document was previously published here.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Trace‚Äôs pitch: This isn‚Äôt secret anymore&lt;/head&gt;
    &lt;p&gt;The Trace walks the appellate court through how the FMECA left the bottle: it was posted on this website and then widely republished; a YouTube explainer discussing it surpassed 100,000 views. The filing quotes Sig Sauer‚Äôs VP of Consumer Affairs Phil Strader being asked on the Behind the Lens podcast why the FMECA shouldn‚Äôt be public and responding, ‚ÄúNo, there‚Äôs not‚Äù (nothing to hide), while directing viewers to this website to see the document and describing its contents.&lt;/p&gt;
    &lt;p&gt;The reporting regarding Phil Strader‚Äôs interview was previously published here&lt;/p&gt;
    &lt;p&gt;How many times has the unredacted FMECA been ‚Äúshared‚Äù? The filings don‚Äôt give a precise share count. What they do document is widespread republication and discussion, including the 100k‚Äëplus video and multiple re‚Äëposts mirroring the PSI copy. In other words, the genie is out of the bottle.&lt;/p&gt;
    &lt;p&gt;The Trace also points to DoD Instruction 5230.24, the policy Sig Sauer invokes, noting it does not authorize withholding unclassified information about evaluations of performance and reliability of military equipment‚Äîand that the PSI‚Äëhosted FMECA bears no DoD distribution markings.&lt;/p&gt;
    &lt;head rend="h2"&gt;Sig Sauer‚Äôs response: Let the Army decide‚Äîand keep the lid on&lt;/head&gt;
    &lt;p&gt;Sig Sauer tells the Eighth Circuit The Trace lacks standing and that parallel briefing is already underway in the district court. Substantively, Sig Sauer leans on military‚Äësecrets concerns, requests time for the Army to opine on release, and characterizes the FMECA as controlled technical information created under the MHS contract. (The company also recounts how the spreadsheet briefly became public in another case before being pulled down.)&lt;/p&gt;
    &lt;p&gt;Two details in Sig Sauer‚Äôs papers matter going forward:&lt;/p&gt;
    &lt;p&gt;1) The ‚ÄúFMECA Memorandum.‚Äù Sig Sauer identifies the memo alongside the previously published spreadsheet. If the memo is Sig Sauer‚Äëauthored, it could reveal how the company framed the Army analysis internally‚Äîan issue directly relevant to notice, risk mitigation, and marketing claims.&lt;/p&gt;
    &lt;p&gt;2) Ongoing Army communications. Sig Sauer‚Äôs litigation counsel filed a declaration stating he asked the Army about the FMECA‚Äôs distribution status and that key Army decision‚Äëmakers were unavailable the week of the deadline; Sig Sauer says the Army may submit information and seeks additional time.&lt;/p&gt;
    &lt;head rend="h2"&gt;The transparency question, distilled&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Is the FMECA ‚Äúnational‚Äësecurity‚Äù material? The Trace says no‚Äîand points to DoDI 5230.24‚Äôs carve‚Äëout: it does not provide authority to withhold unclassified evaluations of performance and reliability‚Äîexactly what a FMECA is. It also underscores the lack of any DoD marking on the PSI copy.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Is secrecy even possible at this point? The record shows the unredacted spreadsheet is online on this website, has been reposted broadly, and has been discussed by Sig Sauer‚Äôs own executive on air‚Äîwho told listeners where to find it. One video discussing it has 100,000+ views.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Why this matters to the class action‚Äîand to owners&lt;/head&gt;
    &lt;p&gt;The district court relied on the FMECA repeatedly when certifying the Missouri class, including on notice and risk‚Äëmitigation questions‚Äîthe very issues consumers care about. Keeping the heart of that analysis under seal on appeal would blunt the public‚Äôs ability to scrutinize a product‚Äësafety fight with real‚Äëworld consequences.&lt;/p&gt;
    &lt;head rend="h2"&gt;My role, plainly&lt;/head&gt;
    &lt;p&gt;Practical Shooting Insights is an independent site covering the shooting‚Äësports and firearms industry. The Trace‚Äôs filing names PSI as the first publisher of the unredacted spreadsheet and quotes Strader pointing viewers here. I will continue to publish filings and analysis so readers can compare the arguments to the documents themselves.&lt;/p&gt;
    &lt;head rend="h2"&gt;What to watch next&lt;/head&gt;
    &lt;p&gt;1) Whether the Eighth Circuit permits intervention and applies the strong presumption of public access to class‚Äëcertification records.&lt;lb/&gt; 2) If the Army weighs in‚Äîand on what basis‚Äîregarding the FMECA‚Äôs distribution status.&lt;lb/&gt; 3) Disclosure of the FMECA Memorandum. If it‚Äôs Sig Sauer-authored, it could illuminate internal framing of hazards and fixes‚Äîmaterial at the core of consumer‚Äëprotection claims.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45063431</guid></item><item><title>Show HN: A minimal TS library that generates prompt injection attacks</title><link>https://prompt-injector.blueprintlab.io/</link><description>&lt;doc fingerprint="8604b754f92e9658"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Lightweight AI Security Testing Library&lt;/head&gt;
    &lt;p&gt;A minimal TypeScript library with 25+ curated prompt injection patterns from leading security research. Easy to integrate, comprehensive coverage, production-ready.&lt;/p&gt;
    &lt;code&gt;npm install prompt-injector&lt;lb/&gt; import { PromptInjector } from 'prompt-injector'&lt;/code&gt;
    &lt;p&gt;√∞¬Ø&lt;/p&gt;
    &lt;p&gt;0&lt;/p&gt;
    &lt;p&gt;Attack Patterns&lt;/p&gt;
    &lt;p&gt;√∞¬¨&lt;/p&gt;
    &lt;p&gt;4&lt;/p&gt;
    &lt;p&gt;Attack Categories&lt;/p&gt;
    &lt;p&gt;√∞&lt;/p&gt;
    &lt;p&gt;SOTA&lt;/p&gt;
    &lt;p&gt;Research Based&lt;/p&gt;
    &lt;p&gt;√∞&lt;/p&gt;
    &lt;p&gt;0&lt;/p&gt;
    &lt;p&gt;Generated Prompts&lt;/p&gt;
    &lt;head rend="h3"&gt;Attack Categories&lt;/head&gt;
    &lt;p&gt;√∞&lt;/p&gt;
    &lt;head rend="h4"&gt;Jailbreaking (5 patterns)&lt;/head&gt;
    &lt;p&gt;Role-play and persona-based attacks that attempt to bypass AI safety guidelines through character roleplay and fictional scenarios.&lt;/p&gt;
    &lt;p&gt;√∞&lt;/p&gt;
    &lt;head rend="h4"&gt;Instruction Hijacking (6 patterns)&lt;/head&gt;
    &lt;p&gt;Direct attempts to override system prompts and inject new instructions that change AI behavior and responses.&lt;/p&gt;
    &lt;p&gt;√∞&lt;/p&gt;
    &lt;head rend="h4"&gt;Encoding Attacks (7 patterns)&lt;/head&gt;
    &lt;p&gt;Obfuscation techniques using Base64, ROT13, Unicode, and other encodings to bypass content filters and detection systems.&lt;/p&gt;
    &lt;p&gt;√∞¬ß &lt;/p&gt;
    &lt;head rend="h4"&gt;Logic Traps (6 patterns)&lt;/head&gt;
    &lt;p&gt;Sophisticated reasoning exploits using hypothetical scenarios, false urgency, and academic authority to manipulate responses.&lt;/p&gt;
    &lt;head rend="h3"&gt;Quick Start Example&lt;/head&gt;
    &lt;code&gt;import { PromptInjector } from 'prompt-injector';

// Initialize with your preferred configuration
const injector = new PromptInjector({
  severity: 'intermediate',
  categories: ['jailbreak', 'instruction-hijack'],
  maxAttempts: 50
});

// Generate test cases
const testCases = injector.generateTests('customer-service-bot');

// Test your AI system
const results = await injector.runTests(yourAISystem);
const report = injector.generateReport(results);

console.log(`Risk Score: ${report.summary.riskScore}`);&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45063547</guid></item><item><title>Show HN: Sosumi.ai ‚Äì Convert Apple Developer docs to AI-readable Markdown</title><link>https://sosumi.ai/</link><description>&lt;doc fingerprint="b0cfd92d22047de1"&gt;
  &lt;main&gt;
    &lt;p&gt;Ever notice Claude struggling to write Swift code? It might not be their fault!&lt;/p&gt;
    &lt;p&gt; Apple Developer docs are locked behind JavaScript, making them invisible to most LLMs. If they try to fetch it, all they see is &lt;quote&gt;This page requires JavaScript. Please turn on JavaScript in your browser and refresh the page to view its content.&lt;/quote&gt;&lt;/p&gt;
    &lt;p&gt;This service translates Apple Developer documentation pages into AI-friendly Markdown.&lt;/p&gt;
    &lt;head rend="h2"&gt;HTTP Usage&lt;/head&gt;
    &lt;p&gt;Replace &lt;code&gt;developer.apple.com&lt;/code&gt; with &lt;code&gt;sosumi.ai&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Original&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;code&gt;https://developer.apple.com/documentation/swift/array&lt;/code&gt;
      &lt;/item&gt;
      &lt;item rend="dt-2"&gt;AI-readable&lt;/item&gt;
      &lt;item rend="dd-2"&gt;
        &lt;code&gt;https://sosumi.ai/documentation/swift/array&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Examples&lt;/head&gt;
    &lt;head rend="h2"&gt;MCP Usage&lt;/head&gt;
    &lt;p&gt; Connect your MCP client to &lt;code&gt;https://sosumi.ai/mcp&lt;/code&gt;:
                &lt;/p&gt;
    &lt;code&gt;{
  "mcpServers": {
    "sosumi": {
      "command": "npx",
      "args": [
        "-y",
        "mcp-remote",
        "https://sosumi.ai/mcp"
      ]
    }  }
}&lt;/code&gt;
    &lt;head rend="h3"&gt;Available Resources&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;list rend="dl"&gt;
          &lt;item rend="dt-1"&gt;
            &lt;code&gt;doc://{path}&lt;/code&gt;
          &lt;/item&gt;
          &lt;item rend="dd-1"&gt;Get Apple Developer documentation as markdown &lt;lb/&gt;Example:&lt;code&gt;doc://swift/array&lt;/code&gt;returns Swift Array documentation&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Available Tools&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;list rend="dl"&gt;
          &lt;item rend="dt-1"&gt;
            &lt;code&gt;search&lt;/code&gt;
          &lt;/item&gt;
          &lt;item rend="dd-1"&gt;Search Apple Developer documentation &lt;lb/&gt;Parameters:&lt;code&gt;query&lt;/code&gt;(string)&lt;lb/&gt;Returns structured results with titles, URLs, descriptions, breadcrumbs, and tags&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45063874</guid></item><item><title>Private Equity Snaps Up Disability Services, Challenging Regulators</title><link>https://www.governing.com/management-and-administration/private-equity-snaps-up-disability-services-challenging-regulators</link><description>&lt;doc fingerprint="2e5009511c922534"&gt;
  &lt;main&gt;
    &lt;p&gt;People with intellectual or developmental disabilities have suffered abuse, neglect and even death while under the care of private equity-owned providers, according to a recent report from watchdog group Private Equity Stakeholder Project.&lt;/p&gt;
    &lt;p&gt;‚ÄúPrivate equity firms are, more than many other types of investors, laser-focused on maximizing their cash flow, often trying to double or triple their investment over a relatively short period of time, usually just a handful of years,‚Äù said Eileen O‚ÄôGrady, the report‚Äôs author. ‚ÄúThe way that private equity firms will often do that is to cut costs.‚Äù&lt;/p&gt;
    &lt;p&gt;For companies that provide essential services for people with disabilities, she said, ‚Äúthose cuts can have really harmful impacts on people‚Äôs lives.‚Äù&lt;/p&gt;
    &lt;p&gt;In late 2023, Florida moved to revoke the license of NeuroRestorative, one branch of the private equity-owned health services company Sevita, which provides services for people with disabilities. State regulators cited repeat violations by NeuroRestorative and a failure to ‚Äúprotect the rights of its clients to be free from physical abuse.‚Äù Ultimately the state opted not to revoke the license and fined the company $13,000 in a settlement.&lt;/p&gt;
    &lt;p&gt;But in recent years regulators have documented instances of patient harm at Sevita‚Äôs affiliates in multiple other states, including Colorado, Indiana, Iowa, Massachusetts and Utah. In 2019, a U.S. Senate committee conducted a probe into the company‚Äôs operations in Iowa and Oregon following multiple reports of patient abuse and neglect.&lt;/p&gt;
    &lt;p&gt;‚ÄúAny entity that receives taxpayer dollars, but especially those charged with caring for our fellow Americans who may have an intellectual disability, ought to be doing everything under the sun to ensure quality care and continually improve,‚Äù U.S. Sen. Chuck Grassley, an Iowa Republican, said in a statement in 2020 following his investigation.&lt;/p&gt;
    &lt;p&gt;In a statement to Stateline, Sevita did not address the sanctions directly, but avowed its commitment to providing services and supports to give people greater independence, regardless of their intellectual or physical challenges.&lt;/p&gt;
    &lt;p&gt;‚ÄúSince 2019, when new ownership acquired the company, there has been significant capital investment to improve and expand our services, enhance facilities, implement robust training and new technologies, and strengthen our workforce ‚Äî all with the goal of better serving our individuals and communities,‚Äù the statement said.&lt;/p&gt;
    &lt;p&gt;The disability care industry has proven increasingly attractive to private equity.&lt;/p&gt;
    &lt;p&gt;In recent years, a handful of large private equity-owned companies such as Sevita have snapped up hundreds of smaller providers of disability services ‚Äî often community nonprofits, mom-and-pop businesses and religious organizations ‚Äî and rolled them into larger corporations.&lt;/p&gt;
    &lt;p&gt;From 2013 to 2023, private equity firms acquired more than 1,000 disability and elder care providers, according to the report by the Private Equity Stakeholder Project. That‚Äôs likely an undercount because they‚Äôre generally not required to disclose acquisitions, the report said.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cash Cow&lt;/head&gt;
    &lt;p&gt;Private equity firms use pooled investments from pension funds, sovereign wealth funds, endowments and wealthy individuals to buy a controlling stake in a company. They seek to maximize its value ‚Äî often by cutting costs ‚Äî and then sell it at a profit.&lt;/p&gt;
    &lt;p&gt;Most of Sevita‚Äôs revenue comes from providing disability services. It operates companies in 40 states under various brands, including Mentor Network, NeuroRestorative and REM.&lt;/p&gt;
    &lt;p&gt;Sevita is currently owned by private equity firms Centerbridge Partners and Vistria Group, which also own Help at Home, a home health company with more than 200 locations across about a dozen states.&lt;/p&gt;
    &lt;p&gt;Nearly all of Sevita‚Äôs revenue comes from Medicaid, according to a February 2025 report from S&amp;amp;P Global.&lt;/p&gt;
    &lt;p&gt;Through Medicaid and Medicare, the government pays for most services for people with intellectual or developmental disabilities. The two programs cover services such as group homes, adult day programs, in-home care, and physical and occupational therapy.&lt;/p&gt;
    &lt;p&gt;‚ÄúSevita has been owned by private equity firms for over a decade now, and has been under investigation and scrutiny at the federal and state level for basically that entire time,‚Äù O‚ÄôGrady said.&lt;/p&gt;
    &lt;p&gt;In 2022, Iowa fined a NeuroRestorative group home $10,500 after a resident was left unattended in a liquor store and drank three-quarters of a bottle of vodka. The same year, Massachusetts temporarily removed Sevita‚Äôs license to operate group homes after regulators reported inadequate staff training and supervision, and a ‚Äúmyriad of issues that were uncovered onsite,‚Äù according to a Massachusetts Department of Developmental Services report.&lt;/p&gt;
    &lt;p&gt;The federal Centers for Medicare &amp;amp; Medicaid Services has fined a NeuroRestorative facility in Utah four times since 2022. A February 2024 inspection report by the agency found the facility ‚Äúfailed to prevent abuse, neglect ‚Ä¶ and exploitation‚Äù of residents.&lt;/p&gt;
    &lt;p&gt;Last year, Florida fined another Sevita brand, Florida Mentor, for improper use of restraints. More issues have been documented in Sevita-owned locations in Arkansas, California, Colorado, Illinois, Indiana, New Hampshire and Nevada.&lt;/p&gt;
    &lt;p&gt;Meanwhile, Sevita‚Äôs owners, Centerbridge and Vistria, have collected nearly half a billion dollars since 2019 by loading Sevita and Help at Home with debt in order to pay dividends to investors, according to Moody‚Äôs, a financial services company.&lt;/p&gt;
    &lt;p&gt;Similar financial maneuvering contributed to the recent collapse of Steward Health Care, a private equity-owned hospital system that once had more than 30 hospitals nationwide. Steward has become a cautionary tale about the harm that profit-driven private equity firms can do to a state‚Äôs health system.&lt;/p&gt;
    &lt;p&gt;‚ÄúBefore Steward Health Care ultimately collapsed, executives spent years hiding their financial information from state regulators, putting patients and our health care system at risk,‚Äù Massachusetts Democratic House Speaker Ron Mariano said in a statement earlier this year announcing a new state law that beefs up reporting and financial requirements for private investors.&lt;/p&gt;
    &lt;p&gt;‚ÄúThat‚Äôs why ensuring that our institutions are equipped to monitor the health care landscape, and to guard against trends and transactions that drive up costs without improving patient outcomes, is so important.‚Äù&lt;/p&gt;
    &lt;head rend="h3"&gt;David vs. Goliath&lt;/head&gt;
    &lt;p&gt;After two residents of a New Jersey group home died from choking on food in 2017, attorney Cory Bernstein became interested in private equity‚Äôs involvement in disability services. The residents had been living in homes operated by AdvoServ, a company then owned by the private equity firm Wellspring Capital Management. The state had cited AdvoServ more times than any other operator in New Jersey for abuse, neglect and unsafe conditions.&lt;/p&gt;
    &lt;p&gt;AdvoServ later ceased operations in 2019 after multiple state agencies, including in New Jersey, Florida and Maryland, launched investigations.&lt;/p&gt;
    &lt;p&gt;But even when state regulators are doing all they can to protect people with disabilities from substandard care, they‚Äôre limited in how much they can hold a company accountable, Bernstein told Stateline.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt‚Äôs state-level oversight on a national entity with not much [help] coming from the federal side,‚Äù said Bernstein, who is now a staff attorney at the National Disability Rights Network, a membership organization of federally mandated state disability advocacy programs.&lt;/p&gt;
    &lt;p&gt;‚ÄúStates just don‚Äôt really have the resources or tools to do what needs to be done.‚Äù&lt;/p&gt;
    &lt;p&gt;A regulatory agency in Georgia might shut down all the group homes owned by a certain company, for example, but those regulators can‚Äôt do anything about the company‚Äôs abuses in, say, Montana. With branches in multiple states, a company is better able to withstand sanctions or even a loss of license in one state, he said.&lt;/p&gt;
    &lt;p&gt;‚Äú[States] are not set up to go up against a national operator with billions of dollars in resources in a regulatory or oversight battle,‚Äù Bernstein said.&lt;/p&gt;
    &lt;p&gt;Further complicating things for state regulators and for consumers is that a large services company such as Sevita might operate under multiple brand names, even in one state. It can be hard to parse out who owns a sanctioned business. Multiple brand names can also obscure a company‚Äôs monopoly on a particular regional market.&lt;/p&gt;
    &lt;p&gt;When Florida regulators reached a settlement agreement with Sevita‚Äôs NeuroRestorative last year, the state dismissed its proposed license revocation. O‚ÄôGrady believes one reason the state chose to settle is the difficulty of finding alternative facilities to relocate the residents who would have been displaced from the 13 locations the company operated around the state.&lt;/p&gt;
    &lt;p&gt;‚ÄúBecause of that dearth of alternatives and the impotence of the state to act more fully, this company will continue to be allowed to operate,‚Äù she said.&lt;/p&gt;
    &lt;p&gt;Further complicating oversight: Large companies often operate various services that are overseen by different agencies. Group homes might be regulated under the state‚Äôs Medicaid program, while facilities that provide more intensive care might come under federal Medicare oversight.&lt;/p&gt;
    &lt;p&gt;There could be ‚Äútwo completely different oversight systems for facilities serving the same population in the same state with the same name,‚Äù Bernstein said.&lt;/p&gt;
    &lt;head rend="h3"&gt;State Solutions&lt;/head&gt;
    &lt;p&gt;Some states have moved to address problems with private equity involvement in health care by passing tighter restrictions on mergers and acquisitions of health care companies.&lt;/p&gt;
    &lt;p&gt;In Rhode Island, where private equity companies‚Äô mismanagement of health care providers threatened the future of local hospitals, a robust oversight law allowed the state attorney general to impose conditions to protect the hospitals‚Äô finances.&lt;/p&gt;
    &lt;p&gt;More states are following suit. In 2023 alone, 24 states enacted laws related to health system consolidation and competition, while this year at least half a dozen have considered legislation to check private equity-fueled health care mergers.&lt;/p&gt;
    &lt;p&gt;This article was published by Stateline. Read the original here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45063972</guid></item><item><title>Show HN: Find Hidden Gems on HN</title><link>https://pj4533.com/hn-overlooked/</link><description>&lt;doc fingerprint="b0e9d773c204af23"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;This tool helps you discover recent hidden gems on Hacker News ‚Äì high-effort posts that haven't gotten much attention.&lt;/p&gt;
      &lt;head rend="h3"&gt;Why "Recent"?&lt;/head&gt;
      &lt;p&gt;We search the HN API's Ask, Show, and New story feeds, which typically contain posts from the last 3-7 days. This ensures fresh content while keeping the search fast.&lt;/p&gt;
      &lt;head rend="h3"&gt;Passion Score&lt;/head&gt;
      &lt;p&gt;Posts are ranked by their Passion Score, which identifies high-effort, low-engagement content:&lt;/p&gt;
      &lt;p&gt; Passion Score = (Text Length Score) / (Engagement + 1) &lt;lb/&gt; Where: &lt;lb/&gt;‚Ä¢ Text Length Score = min(text_length / 500, 10) &lt;lb/&gt;‚Ä¢ Engagement = votes + (comments √ó 2) &lt;/p&gt;
      &lt;p&gt;Higher scores indicate more "overlooked" posts ‚Äì substantial writing with minimal recognition. Perfect for finding thoughtful contributions that the community may have missed.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45064210</guid></item><item><title>Deploying DeepSeek on 96 H100 GPUs</title><link>https://lmsys.org/blog/2025-05-05-large-scale-ep/</link><description>&lt;doc fingerprint="ec5d845b04b8c994"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Deploying DeepSeek with PD Disaggregation and Large-Scale Expert Parallelism on 96 H100 GPUs&lt;/head&gt;
    &lt;p&gt;by: The SGLang Team, May 05, 2025&lt;/p&gt;
    &lt;p&gt;DeepSeek is a popular open-source large language model (LLM) praised for its strong performance. However, its large size and unique architecture, which uses Multi-head Latent Attention (MLA) and Mixture of Experts (MoE), require an advanced system for efficient serving at scale. In this blog, we explain how we match DeepSeek's inference system performance with SGLang.&lt;/p&gt;
    &lt;p&gt;Our implementation, shown in the figure above, runs on 12 nodes in the Atlas Cloud, each equipped with 8 H100 GPUs. It uses prefill-decode disaggregation and large-scale expert parallelism (EP), achieving a speed of 52.3k input tokens per second and 22.3k output tokens per second per node for 2000-token input sequences. To the best of our knowledge, this represents the first open-source implementation to nearly match the throughput reported in the official DeepSeek blog at large scale. By deploying this implementation locally, it translates to a cost of $0.20/1M output tokens, which is about one-fifth the cost of the official DeepSeek Chat API. Compared to vanilla tensor parallelism using the same resources, this optimized strategy improves the output throuhgput by up to 5x. This blog dives into our parallelism design, optimization methods, and results. All components of our work are fully open-source, allowing others to explore and build on our efforts. The instructions for reproducing our experiments are fully available here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Highlight&lt;/head&gt;
    &lt;p&gt;‚úÖ SGLang now supports prefill-decode (PD) disaggregation and large-scale EP, including the full functionality of DeepEP, DeepGEMM, and EPLB.&lt;/p&gt;
    &lt;p&gt;‚úÖ Leveraging these new features, our team successfully replicated DeepSeek's inference system using 12 nodes, each with 8 H100 GPUs. In total, SGLang achieves a throughput of 52.3k input tokens per second and 22.3k output tokens per second per node for input sequences of 2000 tokens.&lt;/p&gt;
    &lt;p&gt;‚úÖ This blog explains technical details of our approach, focusing on optimizations for efficiency, peak memory usage reduction, and workload balancing. The profile results show that our implementation achieves nearly on-par performance with the official DeepSeek‚Äôs report.&lt;/p&gt;
    &lt;p&gt;‚úÖ All experiments and code are fully open-sourced for community access and further development.&lt;/p&gt;
    &lt;head rend="h2"&gt;Outline&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Parallelism Design&lt;/item&gt;
      &lt;item&gt;Prefill and Decode Disaggregation&lt;/item&gt;
      &lt;item&gt;Large-scale Expert Parallelism&lt;/item&gt;
      &lt;item&gt;Evaluation&lt;/item&gt;
      &lt;item&gt;Toolkits&lt;/item&gt;
      &lt;item&gt;Limitations and Future Work&lt;/item&gt;
      &lt;item&gt;Conclusion&lt;/item&gt;
      &lt;item&gt;Acknowledgment&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Parallelism Design&lt;/head&gt;
    &lt;p&gt;Efficient parallelism is essential to manage the computational complexity and memory demands of DeepSeek's architecture. This section outlines our approach to optimizing key components: attention layers, dense feed-forward networks (FFNs), sparse FFNs, and the language model (LM) head. Each component leverages tailored parallelism strategies to enhance scalability, memory efficiency, and performance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Attention Layers&lt;/head&gt;
    &lt;p&gt;DeepSeek employs Multi-head Latent Attention (MLA) to effectively model complex dependencies within input sequences. To optimize this mechanism, we implement DP Attention, a data parallelism strategy that eliminates KV cache duplication across devices, significantly reducing memory overhead. Introduced in SGLang v0.4, this approach has been extended to support hybrid data and tensor parallelism, offering flexibility for processing small batch sizes efficiently.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dense FFNs&lt;/head&gt;
    &lt;p&gt;Despite using only three dense FFN layers, DeepSeek-V3's computation can significantly increase peak memory usage, potentially leading to system crashes if not carefully managed. To address this, we adopt Data Parallelism (DP) over tensor parallelism (TP), leveraging the following advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enhanced Scalability: With an intermediate dimension of 18,432, high TP degrees (e.g., TP32) result in inefficient fragmentation into small-unit segments (e.g., 576 units), which are not divisible by 128‚Äîa common alignment boundary for modern GPUs such as H100. This misalignment hampers computational efficiency and memory utilization. DP provides a more scalable solution by avoiding fragmentation, ensuring balanced workload distribution across devices.&lt;/item&gt;
      &lt;item&gt;Optimized Memory Efficiency: Traditionally, TP reduces memory usage as worker size increases, but this advantage diminishes under DP attention. In a pure TP setup, memory demand for a single-layer Transformer model scales with DP size as: $$\text{Memory}=\frac{N_{\text{param}}}{\text{TP}}+(1+k)N_{\text{hidden_state}}\cdot \text{DP}\notag$$ Here, $N_{\text{hidden_state}}=n_\text{token}\times n_\text{hidden_size}$ is the size of the hidden state on each device (DP rank), $N_{\text{param}}=n_\text{intermediate_size}\times n_\text{hidden_size}$ is the number of model parameters, and $k$ is a coefficient representing extra memory overhead from CUDA Graph duplication. By assuming $\text{DP}=\text{TP}$, this memory usage function is minimized when $\text{TP}=\sqrt{\frac{N_{\text{param}}}{(1+k)N_{\text{hidden_state}}}}$. DeepSeek-V3 uses an intermediate size of 18,432. During the prefill phase, CUDA Graph is typically disabled, so $k = 0$. However, the token size per device can easily exceed 2,048, resulting in an optimal TP size of 3 or less. In the decode phase, a practical configuration might use 128 tokens per device and set $k = 3$. In this case, the memory-optimal TP size is 6. In both phases, a lower TP degree minimizes memory usage per device. As a result, DP may offer a more memory-efficient approach for scaling compared to relying solely on TP.&lt;/item&gt;
      &lt;item&gt;Minimized Communication Overhead: In pure TP, each FFN necessitates two all-reduce operations, resulting in substantial communication overhead. By leveraging DP, we optimize this process to a single reduce-scatter following the prior attention layer and an all-gather before the next, reducing communication costs by 50%. Furthermore, when attention is also computed under pure DP, inter-device communication is entirely eliminated, significantly enhancing overall efficiency.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The integration of DP dense FFN with DP attention is illustrated in the left figure below. Users can enable this feature by setting &lt;code&gt;--moe-dense-tp-size=1&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sparse FFNs&lt;/head&gt;
    &lt;p&gt;In DeepSeek-V3's Mixture of Experts (MoE) architecture, sparse FFNs require substantial expert weights, creating a significant memory bottleneck. To address this, we implement Expert Parallelism (EP), which distributes expert weights across multiple devices. This approach effectively scales memory capacity while maintaining high performance, though it does introduce challenges like irregular all-to-all communication and workload imbalance.&lt;/p&gt;
    &lt;p&gt;The figure in the right figure above illustrates our EP implementation using the DeepEP framework, with further details on our EP design and optimizations provided in the following sections.&lt;/p&gt;
    &lt;head rend="h3"&gt;LM Head&lt;/head&gt;
    &lt;p&gt;The LM head computes output probabilities over a large vocabulary, a resource-intensive operation traditionally handled with vocabulary parallelism to aggregate token logits from TP groups. To enhance scalability and efficiency, we adopt Data Parallelism (DP), mirroring our dense FFN strategy. This reduces memory overhead and simplifies communication across devices, delivering a more streamlined solution.&lt;/p&gt;
    &lt;head rend="h2"&gt;Prefill and Decode Disaggregation&lt;/head&gt;
    &lt;p&gt;LLM inference comprises two distinct phases: Prefill and Decode. The Prefill phase is computation-intensive, processing the entire input sequence, while the Decode phase is memory-intensive, managing the Key-Value (KV) cache for token generation. Traditionally, these phases are handled within a unified engine, where combined scheduling of prefill and decode batches introduces inefficiencies. To address these challenges, we introduce Prefill and Decode (PD) Disaggregation in SGLang.&lt;/p&gt;
    &lt;head rend="h3"&gt;Issues with Unified Scheduling&lt;/head&gt;
    &lt;p&gt;The conventional unified engine, which processes prefill and decode batches together, results in three significant problems:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Prefill Interruption: Incoming prefill batches frequently interrupt ongoing decode batches, causing substantial delays in token generation.&lt;/item&gt;
      &lt;item&gt;DP Attention Imbalance: In DP attention, one DP worker may process a prefill batch while another handles a decode batch simultaneously, leading to increased decode latency.&lt;/item&gt;
      &lt;item&gt;Incompatible with DeepEP: As we will discuss in a later section, DeepEP executes different dispatch modes for prefill and decode, making unified scheduling imcompatible with DeepEP.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;PD Disaggregation resolves these by separating the two stages, enabling tailored optimizations for each.&lt;/p&gt;
    &lt;head rend="h3"&gt;Implementation Details&lt;/head&gt;
    &lt;p&gt;The PD Disaggregation design in SGLang, depicted in the diagram below, interleaves execution between a Prefill Server and a Decode Server:&lt;/p&gt;
    &lt;p&gt;Upon receiving an input request, the workflow proceeds as follows:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A Prefill Server and a Decode Server pair via a handshake, establishing a local sender and receiver, respectively.&lt;/item&gt;
      &lt;item&gt;The Decode Server pre-allocates the KV cache, signaling the Prefill Server to begin the model forward pass and compute the KV caches.&lt;/item&gt;
      &lt;item&gt;Once computed, the data transfers to the Decode Server, which handles iterative token generation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This separation ensures each phase operates under optimal conditions, maximizing GPU resource utilization. To further enhance performance, our implementation incorporates:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Non-blocking Transfer: Data send and receive operations run in a background thread, keeping the scheduler‚Äôs event loop uninterrupted.&lt;/item&gt;
      &lt;item&gt;RDMA-Based Transfer: Remote Direct Memory Access (RDMA) leverages queue pairs for connections and scatter-gather elements (SGE) for efficient transfer of non-contiguous memory chunks.&lt;/item&gt;
      &lt;item&gt;Flexible API Integration: SGLang offers adaptable APIs that integrate high-performance RDMA libraries like Mooncake and NIXL, streamlining data transfers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More details can be found in our design document.&lt;/p&gt;
    &lt;head rend="h2"&gt;Large-scale Expert Parallelism&lt;/head&gt;
    &lt;head rend="h3"&gt;Expert Parallelism with DeepEP&lt;/head&gt;
    &lt;p&gt;DeepEP, implemented by the DeepSeek team, is a communication library designed to streamline EP in MoE models. It tackles the challenge of efficiently routing tokens to specific experts across multiple GPUs. By providing optimized communication kernels, DeepEP reduces latency and boosts throughput, making it ideal for large-scale inference tasks.&lt;/p&gt;
    &lt;p&gt;DeepEP provides two specialized dispatch modes to address varying workload demands:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Normal Dispatch: Optimized for handling long input sequences, such as during the prefill phase, this mode prioritizes maximum computational throughput. However, it generates symbolic shapes that are incompatible with CUDA Graph, rendering it less effective for the decode phase, where kernel launch overhead becomes a significant bottleneck.&lt;/item&gt;
      &lt;item&gt;Low-Latency Dispatch: Tailored for generating output tokens during the decode phase, this mode prioritizes minimal delay to ensure real-time performance. It supports CUDA Graph but requires preallocating a fixed memory size. If the memory demand exceeds this preallocation, a runtime error occurs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In SGLang, the integration of DeepEP provides auto mode that dynamically selects between these two dispatch modes based on the workload. However, without PD disaggregation, the auto mode faces a limitation: it cannot simultaneously support both normal dispatch (for prefill) and low-latency dispatch (for decode) within the same communication group. This restriction hinders its compatibility with DP attention, which is crucial for memory-efficient inference. The compatibility of each mode is outlined in the table below:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Long Input&lt;/cell&gt;
        &lt;cell role="head"&gt;Long Output&lt;/cell&gt;
        &lt;cell role="head"&gt;DP Attention&lt;/cell&gt;
        &lt;cell role="head"&gt;CUDA Graph&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Normal&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Low-Latency&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Auto&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;PD disaggregation addresses this by separating prefill and decode phases, allowing normal dispatch for the prefill phase and low-latency dispatch for the decode phase, both under DP attention. This integration optimizes resource utilization and enhances overall performance by aligning the dispatch mode with the specific needs of each phase.&lt;/p&gt;
    &lt;head rend="h3"&gt;DeepGEMM Integration&lt;/head&gt;
    &lt;p&gt;DeepGEMM is another high-efficient library developed by the DeepSeek team, specifically designed to optimize computations in MoE models. It provides two specialized functions for handling MoE-related matrix multiplications (Grouped GEMMs), each tailored to different phases of the inference process.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Grouped GEMMs (contiguous layout): This kernel is designed for dynamic input shapes, making it ideal for the prefill phase of MoE inference. It processes inputs where the data for different experts is concatenated contiguously, allowing for flexible handling of varying input sizes.&lt;/item&gt;
      &lt;item&gt;Grouped GEMMs (masked layout): This kernel assumes a fixed input shape and uses a mask tensor to compute only the valid portions of the input. It is compatible with CUDA Graph, which optimizes kernel launches, making it well-suited for the decode phase where reducing overhead is critical.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;DeepGEMM integrates smoothly with the dispatch modes of DeepEP:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For the contiguous layout kernel, which is used with normal dispatch in the prefill phase, an additional step is required. Since normal dispatch outputs a symbolic shape, a permutation is needed to transform the output into the contiguous format expected by the kernel. We referred to the LightLLM project and implemented a custom Triton kernel for efficient permutation. This kernel ensures that the output from normal dispatch is correctly rearranged, enabling smooth integration with the contiguous GEMM kernel.&lt;/item&gt;
      &lt;item&gt;The masked layout kernel pairs seamlessly with DeepEP‚Äôs low-latency dispatch, as both are optimized for the decode phase and support CUDA Graph.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SGLang also integrates DeepGEMM for MoE computation under tensor parallelism. Additionally, DeepGEMM provides a highly efficient general GeMM kernel, which can be activated in SGLang by setting the environment variable &lt;code&gt;SGL_ENABLE_JIT_DEEPGEMM&lt;/code&gt; to 1, offering even greater computational efficiency for non-MoE operations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Two-batch Overlap&lt;/head&gt;
    &lt;p&gt;In multi-node environments, limited communication bandwidth can significantly increase overall latency. To tackle this challenge, we implemented Two-batch Overlap (TBO) following DeepSeek's system design. TBO splits a single batch into two micro-batches, allowing computation and communication to overlap, which also lowers peak memory usage by halving the effective batch size. However, putting TBO into practice introduces specific implementation difficulties.&lt;/p&gt;
    &lt;head rend="h5"&gt;Implementation Challenges&lt;/head&gt;
    &lt;p&gt;Although DeepSeek released the design framework of TBO, there are two slight implementation challenges.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Code Complexity: Directly coding TBO can lead to duplicated logic for managing multiple micro-batches. This increases the complexity of the codebase, making it harder to maintain and prone to errors, especially as the number of micro-batches or overlapping scenarios grows.&lt;/item&gt;
      &lt;item&gt;Synchronization Issues in the Prefill Phase: Achieving effective overlap between computation and communication needs consideration when the normal dispatch in DeepEP block the CPU. This blocking behavior can stall the pipeline, leaving the GPU idle and undermining the performance benefits of TBO.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Abstraction for Clean Implementation&lt;/head&gt;
    &lt;p&gt;To create a more maintainable and reusable codebase, we use an abstraction layer consisting of operations and yield points. This method simplifies development by allowing us to write code as if handling a single micro-batch, while strategically pausing execution by inserting yield points to let other micro-batches proceed. It eliminates code duplication, reduces the potential need for variable postfixes, and efficiently manages cases where some executions complete at a layer's end while others have not. Additionally, it supports easy adaptation to different overlapping region choices or future enhancements, like a three-batch overlap, with minimal code changes. Below is a concise demonstration of this approach:&lt;/p&gt;
    &lt;code&gt;operations = [
    self._forward_attn,
    YieldOperation(),  # Pause execution for other micro-batches
    self._forward_dispatch,
    self._forward_mlp,
    YieldOperation(),  # Another pause point
    self._forward_combine,
]

# Process a single micro-batch without duplicating code
def _forward_attn(self, state):
    state.hidden_states = self.self_attn(state.hidden_states, ...)
&lt;/code&gt;
    &lt;head rend="h5"&gt;Prefill Overlapping Implementation&lt;/head&gt;
    &lt;p&gt;We refine the launch order during the prefill phase to avoid CPU-blocking via the dispatch operation in DeepEP, even though we are using its asynchronous mode. Specifically:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The dispatch operation blocks the CPU until the GPU receives metadata from other ranks to allocate correctly sized tensors.&lt;/item&gt;
      &lt;item&gt;An improper implementation would leave the computation stream idle during this period, as no computation tasks are submitted to the GPU.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To optimize, we prioritize submitting computation tasks to the GPU before launching CPU-blocking communication. This ensures the GPU remains active during communication. As illustrated in the figure below, TBO with a proper launch order, indicated by bolded borders, avoids bubble caused by a CPU-blocking operation (i.e., normal dispatch).&lt;/p&gt;
    &lt;head rend="h3"&gt;Expert Parallelism Load Balancer&lt;/head&gt;
    &lt;p&gt;In MoE models, EP often leads to uneven workload distribution across GPUs. This imbalance forces the system to wait for the slowest GPU computation or communication, wasting compute cycles and increasing memory usage due to expert activations. As the number of GPUs (EP size) increases, the imbalance issue gets more severe.&lt;/p&gt;
    &lt;p&gt;To address this, DeepSeek developed the Expert Parallelism Load Balancer (EPLB). EPLB takes expert distribution statistics as input and computes an optimal arrangement of experts to minimize imbalance. Users can allocate redundant experts (e.g., 32 additional experts), which, when combined with the original 256, create a pool of 288 experts. This pool allows EPLB to strategically place or replicate experts‚Äîfor instance, duplicating the most frequently used expert multiple times or grouping a moderately used expert with rarely used ones on a single GPU.&lt;/p&gt;
    &lt;p&gt;Beyond balancing workloads, EPLB offers greater flexibility in parallelism design. With the original 256 experts, parallelism sizes are restricted to powers of two. EPLB‚Äôs use of 288 experts enables more diverse configurations, such as parallelism sizes of 12 or 72.&lt;/p&gt;
    &lt;p&gt;In the figure below, we demonstrate the effects of scale and EPLB algorithm to the imbalance issue via simulation. We compute GPU balancedness as the ratio between mean computation time and maximum computation time for a MoE layer among GPUs, and we use the number of tokens for a GPU to estimate the computation time for it. As can be seen, utilization rate decreases when the system scales with the number of nodes, and enabling EPLB significantly improves the utilization.&lt;/p&gt;
    &lt;head rend="h5"&gt;EPLB for Real-World Serving&lt;/head&gt;
    &lt;p&gt;For EPLB to be effective, the input distribution must closely match the actual serving workload. Two strategies enhance this alignment:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Increasing Batch Size: Larger batches reduce random fluctuations in expert usage, which improves balance, which can be achieved by scaling the cluster or using techniques like Multi-Token Prediction (MTP).&lt;/item&gt;
      &lt;item&gt;Periodic Rebalancing: Regularly updating the expert arrangement leverages temporal locality but requires efficient reloading of experts. This necessitates minimizing the cost of expert reloading operations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Even with EPLB, some imbalance is inevitable, making further optimization a valuable future direction.&lt;/p&gt;
    &lt;head rend="h5"&gt;Implementation of Rebalancing&lt;/head&gt;
    &lt;p&gt;SGLang implements expert rebalancing in three stages to ensure efficiency and minimal disruption:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;System Loading Stage: Weights are optionally preloaded from disk to main memory for faster rebalancing or kept on disk with memory mapping (mmap) for reduced memory usage.&lt;/item&gt;
      &lt;item&gt;Rebalance Preparation Stage: Required weights are asynchronously transferred to device memory in the background, utilizing free DMA hardware engines without interrupting ongoing GPU operations.&lt;/item&gt;
      &lt;item&gt;Rebalance Execution Stage: A device-to-device copy updates the weights. This step can be further optimized through physical memory rebinding techniques.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This staged approach ensures that rebalancing is both efficient and non-disruptive, maintaining system performance during updates.&lt;/p&gt;
    &lt;head rend="h2"&gt;Evaluation&lt;/head&gt;
    &lt;head rend="h3"&gt;End-to-end Performance&lt;/head&gt;
    &lt;head rend="h5"&gt;Experimental Setup&lt;/head&gt;
    &lt;p&gt;We evaluated the end-to-end performance of different configurations of SGLang using DeepSeek-V3 on a cluster of 12 nodes, connected via InfiniBand and each equipped with 8 H100 GPUs. This evaluation highlights the throughput improvements enabled by our advanced optimization techniques. We compared the following four settings:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SGLang with TP16 x 6: Every two nodes are paired with an independent group, running DeepSeek-V3 inference with a TP size of 16 and DP attention.&lt;/item&gt;
      &lt;item&gt;SGLang with PD Disaggregation: This version incorporates PD disaggregation and full EP optimization. For the EPLB, we adopt a distribution matching the input/output data, as real-time serving statistics are unavailable.&lt;/item&gt;
      &lt;item&gt;SGLang with PD Disaggregation and simulated MTP: To simulate MTP‚Äôs effects, we firstly double the batch size and halve the Key-Value KV cache length to maintain the same workload for GroupedGeMM computation and memory access. Moreover, we insert dummy kernels after the real attention computation to ensure the attention phase takes the same time as in DeepSeek‚Äôs profile, accurately reflecting the slowdown caused by MTP‚Äôs attention mechanism. We conservatively assume a 70% acceptance rate under MTP.&lt;/item&gt;
      &lt;item&gt;DeepSeek Profile Results: Throughput estimates are derived from DeepSeek‚Äôs official profiling data.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Performance Analysis of Prefill and Decode Phases&lt;/head&gt;
    &lt;p&gt;To accommodate varying workload demands, we independently evaluated the prefill (P) and decode (D) phases, assuming unlimited resources for the non-tested phase to isolate and maximize the load on the tested nodes‚Äîmirroring the setup used by DeepSeek. The results are summarized below:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Prefill Phase: On 4 nodes (4√ó8√óH100, EP32), the system achieved per-node throughputs of 57,674, 54,543, and 50,302 tokens per second for prompt lengths of 1K, 2K, and 4K, respectively. As shown in the bar chart below, this represents up to a 3.3√ó improvement over the TP16 baseline, largely attributable to the optimized GroupedGeMM kernel (DeepGEMM) and two-batch overlap. Assuming a perfectly balanced workload, our system‚Äôs throughput is within 5.6% of DeepSeek's official profile.&lt;/item&gt;
      &lt;item&gt;Decode Phase: Evaluated on 9 nodes (9√ó8√óH100, EP72; half the scale of DeepSeek), the system achieved 22,282 tokens/sec per node for 2K inputs‚Äîrepresenting a 5.2√ó speedup over the TP16 baseline. Under simulated MTP conditions‚Äîwith attention kernels intentionally slowed to reflect real-world latency‚Äîthe system sustained a high throughput of 17,373 tokens/sec per node for 4K inputs, just 6.6% below DeepSeek‚Äôs official profile. As shown in the figure on the right, these performance gains are largely attributed to 4√ó larger batch sizes enabled by EP, which enhances scalability by significantly reducing per-GPU memory consumption of model weights.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Profile Results&lt;/head&gt;
    &lt;p&gt;This section compares SGLang‚Äôs performance with DeepSeek‚Äôs inference system, aligning our experimental setup as closely as possible to DeepSeek‚Äôs production environment. We analyze overall throughput and detailed kernel breakdowns, benchmarking against DeepSeek‚Äôs blog and public profile data.&lt;/p&gt;
    &lt;head rend="h5"&gt;Overall Throughput&lt;/head&gt;
    &lt;p&gt;For prefill, we tested a scenario with 16,384 tokens per device and an input length of 4,096. Due to uncertainty in DeepSeek‚Äôs expert distribution, we evaluated two cases: one with default expert distribution and another with simulated perfect EPLB (random expert selection following group-limited routing semantics) as a performance upper bound.&lt;/p&gt;
    &lt;p&gt;The results are presented below:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;DeepSeek Blog (excl. cache hit)&lt;/cell&gt;
        &lt;cell role="head"&gt;DeepSeek Profile&lt;/cell&gt;
        &lt;cell role="head"&gt;SGLang (Default)&lt;/cell&gt;
        &lt;cell role="head"&gt;SGLang + Simulated Perfect EPLB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Batch Size&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;16,384&lt;/cell&gt;
        &lt;cell&gt;16,384&lt;/cell&gt;
        &lt;cell&gt;16,384&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Input Length&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;4,096&lt;/cell&gt;
        &lt;cell&gt;4,096&lt;/cell&gt;
        &lt;cell&gt;4,096&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Throughput (per node)&lt;/cell&gt;
        &lt;cell&gt;32,206&lt;/cell&gt;
        &lt;cell&gt;62,713&lt;/cell&gt;
        &lt;cell&gt;50,302&lt;/cell&gt;
        &lt;cell&gt;59,337&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;DeepSeek‚Äôs profile reports a throughput roughly twice that of its production environment. SGLang with default expert imbalance is 20% slower than DeepSeek‚Äôs profile, while the simulated perfect EPLB case narrows the gap to 6%.&lt;/p&gt;
    &lt;p&gt;For decode, the results are shown below:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;DeepSeek Blog&lt;/cell&gt;
        &lt;cell role="head"&gt;DeepSeek Profile&lt;/cell&gt;
        &lt;cell role="head"&gt;SGLang (Default)&lt;/cell&gt;
        &lt;cell role="head"&gt;SGLang + Simulated MTP (Slow Attention)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Batch Size&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;128&lt;/cell&gt;
        &lt;cell&gt;256&lt;/cell&gt;
        &lt;cell&gt;128&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;KV Cache Length&lt;/cell&gt;
        &lt;cell&gt;4,989&lt;/cell&gt;
        &lt;cell&gt;4,096&lt;/cell&gt;
        &lt;cell&gt;2,000&lt;/cell&gt;
        &lt;cell&gt;4,000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Number of Nodes&lt;/cell&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Throughput (per node)&lt;/cell&gt;
        &lt;cell&gt;14,800&lt;/cell&gt;
        &lt;cell&gt;18,598&lt;/cell&gt;
        &lt;cell&gt;22,282&lt;/cell&gt;
        &lt;cell&gt;17,373&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Using half the nodes of DeepSeek, SGLang with simulated MTP is only slightly slower than DeepSeek‚Äôs profile. In a higher batch size setting (256 sequences, 2,000 input length), SGLang achieves 22,282 tokens per second per node, demonstrating strong scalability.&lt;/p&gt;
    &lt;head rend="h5"&gt;Detail Breakdown&lt;/head&gt;
    &lt;p&gt;The figure below breaks down kernel execution times for prefill, including unit test results as a theoretical upper bound:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Default EPLB: Communication kernels exhibit longer execution times and higher variance compared to DeepSeek‚Äôs profile, likely due to greater expert imbalance. This leads to extended computation stream bubbles, slowing down overall performance.&lt;/item&gt;
      &lt;item&gt;Simulated Perfect EPLB: This setup aligns more closely with DeepSeek‚Äôs profile, though discrepancies remain, indicating potential areas for optimization.&lt;/item&gt;
      &lt;item&gt;Comparison with Unit Tests: Both DeepSeek and SGLang have a communication time slower than unit test results, while the latter is achievable when disabling TBO, revealing a potential optimization direction if communication is the bottleneck.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SGLang‚Äôs decode kernel breakdown aligns closely with DeepSeek‚Äôs, as shown below:&lt;/p&gt;
    &lt;p&gt;Key observations include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Combine Time Discrepancy: SGLang‚Äôs combine operation appears 2x slower than DeepSeek‚Äôs due to shorter attention computation, causing communication kernels to busy-wait. In the simulated slow attention experiment, combine time matches DeepSeek‚Äôs, confirming this hypothesis.&lt;/item&gt;
      &lt;item&gt;MoE Performance: SGLang‚Äôs MoE kernels are 25% slower, possibly because DeepSeek‚Äôs 18 nodes (versus our 9) distribute experts more efficiently, reducing memory access overhead for GEMM operations.&lt;/item&gt;
      &lt;item&gt;Dispatch Optimization Potential: Both DeepSeek and SGLang show dispatch times of ~0.17ms per layer, but unit tests with DeepEP reveal a potential of 0.06ms occupying SMs. Currently, dispatch spends significant time busy-waiting for data. Inserting slow dummy kernels between send/receive operations reduces dispatch time to 0.09ms, and in-flight duration analysis using unit test data suggests further improvements are possible.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While minor enhancements remain‚Äîprimarily in kernel fusion under "Other Kernels"‚ÄîSGLang‚Äôs decode performance is largely aligned with DeepSeek‚Äôs, with prefill optimization as the next focus.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ablation Study: Two-batch Overlap&lt;/head&gt;
    &lt;head rend="h5"&gt;Impact of Batch Size and Attention Time&lt;/head&gt;
    &lt;p&gt;This section investigates TBO performance across varying batch sizes and simulated MTP scenarios.&lt;/p&gt;
    &lt;p&gt;TBO delivers two significant benefits in the prefill phase, as evidenced by throughput comparisons and memory usage optimizations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Support for Larger Batch Sizes: In the vanilla configuration, each device processes up to 8,192 tokens before encountering out-of-memory (OOM) errors at 16,384 tokens. TBO mitigates this by optimizing memory usage for input tokens, enabling inference with batches as large as 16,384 tokens per device. This further boosts performance to 40.5% increase when comparing the TBO flag with all other configurations made optimal.&lt;/item&gt;
      &lt;item&gt;Enhanced Throughput: By overlapping computation (e.g., attention and MLP phases) with communication (e.g., DeepEP Combine and Dispatch), TBO achieves a 27% to 35% throughput increase compared to the vanilla setup, even when processing the same token count per device.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;TBO‚Äôs impact in the decode phase varies by scenario, with performance tied to batch size and attention processing time:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Real Test Cases: Speedup in practical scenarios is contingent on batch size exceeding a threshold between 64 and 128 tokens. Below this, TBO yields minimal or negative gains (e.g., -27% at 32 tokens/device), as small decode batch sizes hinder kernel efficiency. The speedup reaches 25.5% at 256 tokens with a performance of 22,310 tokens per second.&lt;/item&gt;
      &lt;item&gt;Simulated MTP Scenario: TBO provides the most substantial speedup in simulated MTP cases when processing 128 requests to generate 256 tokens per decode step. This is due to prolonged attention processing time, which aligns computation (e.g., DP Attention layers) with DeepEP communication overhead (e.g., combine and dispatch steps). The evaluation shows a 35% speedup at 128 sequences/device, with throughput 17,552 tokens per second compared to 12,929 without TBO.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Detail Breakdown&lt;/head&gt;
    &lt;p&gt;We evaluated three prefill scenarios: TBO with 16k tokens per batch, TBO with 8k tokens, and no-TBO with 8k tokens. The figure below reveals key insights:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;TBO Efficiency: Comparing the 8k cases, TBO improves overall efficiency by overlapping computation and communication, as expected.&lt;/item&gt;
      &lt;item&gt;Batch Size Impact: Reducing the batch size from 16k to 8k with TBO results in a slight slowdown, reflecting diminished kernel efficiency with smaller batches.&lt;/item&gt;
      &lt;item&gt;Kernel Performance: Interestingly, the no-TBO 8k case outperforms the TBO 16k case in per-kernel speed, despite both having an effective batch size of 8k for kernels. This may stem from reduced streaming multiprocessors (SMs) with TBO, potential noisy neighbor effects during overlap, or kernel incompatibility between computation and communication. These findings suggest future optimization directions for SGLang.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For the decode phase, we analyzed three configurations: TBO with a batch size of 256, no-TBO with 256, and no-TBO with 128. The time breakdown is shown below:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;TBO vs. No-TBO (Batch Size 256): Without TBO, communication time increases significantly due to the lack of overlap. However, computation kernels, particularly GEMM, benefit from a larger effective batch size, resulting in faster execution.&lt;/item&gt;
      &lt;item&gt;TBO (256) vs. No-TBO (128): Comparing cases with the same kernel batch size, only non-overlapped communication slows down in the no-TBO setup, while computation remains consistent. Unlike prefill, decode communication kernels either fully utilize SMs (during send/receive) or none (during inflight waiting), avoiding resource contention with computation kernels.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Ablation Study: EPLB&lt;/head&gt;
    &lt;p&gt;This section evaluates the impact of the EPLB on system performance through overall throughput analysis and detailed case studies. Given EPLB's sensitivity to workload distribution and distribution shifts in production environments, we focus on qualitative and generalizable insights rather than real-world performance, which requires production data.&lt;/p&gt;
    &lt;head rend="h5"&gt;Overall Results&lt;/head&gt;
    &lt;p&gt;The figure below illustrates EPLB's effect on throughput in large-scale settings. EPLB delivers a significant speedup of 1.49x (prefill) and 2.54x (decode), as expected, due to its ability to mitigate workload imbalances across GPUs. As the number of ranks scales, imbalances grow, and EPLB effectively addresses this in our large-scale experiments, leading to notable throughput improvements.&lt;/p&gt;
    &lt;head rend="h5"&gt;Case Study: Workload Imbalance Versus Overall Throughput&lt;/head&gt;
    &lt;p&gt;To explore the relationship between workload imbalance and throughput, we conducted a case study using a decode experiment with 1800 input tokens, 100 output tokens, and a batch size of 256. Throughput and balancedness (average token count divided by maximum token count across experts) were plotted against decoding steps:&lt;/p&gt;
    &lt;p&gt;The results reveal a strong correlation between balancedness and throughput, emphasizing the importance of maintaining high balancedness for optimal performance.&lt;/p&gt;
    &lt;head rend="h5"&gt;Case Study: Expert Distribution Statistics&lt;/head&gt;
    &lt;p&gt;The following figure presents expert distribution statistics for prefill and decode sample data:&lt;/p&gt;
    &lt;p&gt;Key observations include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Imbalance in Expert Usage: Most experts are infrequently used, while a small subset is heavily utilized, underscoring the inherent imbalance in MoE models.&lt;/item&gt;
      &lt;item&gt;Prefill vs. Decode Differences: Although prefill and decode distributions share similarities, notable differences exist. This supports the use of PD disaggregation, which enables distinct expert placements for each phase, optimizing performance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These findings highlight EPLB's role in addressing workload imbalances and the value of tailoring expert placement to phase-specific demands.&lt;/p&gt;
    &lt;head rend="h2"&gt;Toolkits&lt;/head&gt;
    &lt;head rend="h3"&gt;Disposable Tensor&lt;/head&gt;
    &lt;p&gt;Memory management in PyTorch can be challenging due to persistent object references, especially in GPU-intensive workflows where CUDA memory is a scarce resource. Consider the following example:&lt;/p&gt;
    &lt;code&gt;def ffn(hidden_state: torch.Tensor, linear1: nn.Linear, linear2: nn.Linear):
    intermediate_state = linear1(hidden_state)
    del hidden_state  # Attempt to free memory, but no effect due to external reference
    return linear2(nn.ReLU(intermediate_state))

hidden_state = ffn(hidden_state, linear1, linear2)
&lt;/code&gt;
    &lt;p&gt;In this code, &lt;code&gt;del hidden_state&lt;/code&gt; is intended to release the memory occupied by &lt;code&gt;hidden_state&lt;/code&gt; after &lt;code&gt;intermediate_state&lt;/code&gt; is computed. However, as &lt;code&gt;hidden_state&lt;/code&gt; is still referenced outside the function, the &lt;code&gt;del&lt;/code&gt; operation has no effect. This increases peak memory usage, risking performance slowdowns or out-of-memory errors.&lt;/p&gt;
    &lt;p&gt;SGLang addresses this with the DisposableTensor class, a subclass of &lt;code&gt;torch.Tensor&lt;/code&gt; which introduces a dispose() method to explicitly and immediately release a tensor‚Äôs memory, circumventing Python‚Äôs reference counting limitations. Here‚Äôs how it works:&lt;/p&gt;
    &lt;code&gt;def ffn(hidden_state: torch.Tensor, linear1: nn.Linear, linear2: nn.Linear):
    intermediate_state = linear1(hidden_state)
    hidden_state.dispose()  # Immediately releases CUDA memory
    return linear2(nn.ReLU(intermediate_state))

# Wrap the tensor in DisposableTensor
hidden_state = DisposableTensor(hidden_state)
hidden_state = ffn(hidden_state, linear1, linear2)
&lt;/code&gt;
    &lt;p&gt;By wrapping &lt;code&gt;hidden_state&lt;/code&gt; in a &lt;code&gt;DisposableTensor&lt;/code&gt; and calling &lt;code&gt;dispose()&lt;/code&gt; when it‚Äôs no longer needed, the CUDA memory is freed right away. This ensures that memory is released as soon as the tensor‚Äôs role in the computation is complete, reducing peak memory usage and improving overall efficiency.&lt;/p&gt;
    &lt;head rend="h3"&gt;Expert Workload Extraction and Simulation&lt;/head&gt;
    &lt;p&gt;SGLang also includes a toolset for analyzing and simulating expert workload distribution in MoE models. This feature enables users to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dump Expert Workload Statistics: Extract either accumulated statistics or per-batch workload data. Accumulated stats support the EPLB manager for real-time optimization, while per-batch data provides granular insights for analysis and simulation.&lt;/item&gt;
      &lt;item&gt;Simulate Expert Utilization: Model expert balance across various configurations without requiring costly hardware or repeated trials. For instance, users can gather workload data from a modest setup (e.g., 2x8xH100 or 8xH200) and simulate the performance for a large-scale 22-node deployment.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This simulation capability allows users to evaluate how factors like rebalancing frequency, node count, or batch size impact system performance. It‚Äôs a cost-effective way to fine-tune configurations before scaling up.&lt;/p&gt;
    &lt;head rend="h2"&gt;Limitations and Future Work&lt;/head&gt;
    &lt;p&gt;While our implementation of SGLang for DeepSeek-V3 inference demonstrates significant throughput improvements, several limitations and areas for future enhancement remain:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Latency Optimization: The current focus on throughput leaves Time to First Token (TTFT) at 2‚Äì5 seconds and Inter-Token Latency (ITL) at approximately 100ms, requiring further optimizations for real-time use cases.&lt;/item&gt;
      &lt;item&gt;Sequence Length Constraints: Limited to shorter sequences due to the use of 96 GPUs. Expanding GPU resources would support longer sequences, essential for specific applications.&lt;/item&gt;
      &lt;item&gt;Multi-Token Prediction (MTP) Integration: SGLang supports MTP but lacks full integration with DP attention, reducing efficiency in mixed parallelism configurations.&lt;/item&gt;
      &lt;item&gt;EPLB Distribution: The experiments in this blog utilizes in-distribution data for Expert Parallelism Load Balancer (EPLB), which may not reflect real-world variability. Future work should experiment performances when having distribution shifts.&lt;/item&gt;
      &lt;item&gt;Flexible Tensor Parallelism (TP) Sizes: For DeepSeek-V3, memory-optimal TP sizes for dense FFNs are small but larger than 1. Currently, SGLang only supports pure TP or DP, leading to suboptimal memory use. Flexible TP options are needed.&lt;/item&gt;
      &lt;item&gt;Blackwell Support: Currently, our implementation supports only the NVIDIA Hopper architecture. We are actively working to extend compatibility to the next-generation Blackwell architecture. If you are interested in supporting or sponsoring this development, welcome to contact lmsys.org@gmail.com.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;By leveraging PD disaggregation, EP, and a carefully crafted parallelism design, we‚Äôve reproduced DeepSeek‚Äôs inference framework in SGLang with exceptional performance. Our open-source efforts‚Äîachieving 52.3k input tokens per second and 22.3k output tokens per second‚Äîdemonstrate SGLang‚Äôs power for large-scale LLM inference. We invite the community to explore, replicate, and extend this work to push the boundaries of efficient AI deployment.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgment&lt;/head&gt;
    &lt;p&gt;We would like to express our heartfelt gratitude to the following teams and collaborators:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SGLang Core Team and Community Contributors ‚Äî Jingyi Chen, Cheng Wan, Liangsheng Yin, Baizhou Zhang, Ke Bao, Jiexin Liang, Xiaoyu Zhang, Yanbo Yang, Fan Yin, Chao Wang, Laixin Xie, Runkai Tao, Yuhong Guo, Kaihong Zhang, Lei Yu, Yu-Hsuan Tseng, Qilin Tian, Peng Zhang, Yi Zhang, Yineng Zhang, Byron Hsu, and many others.&lt;/item&gt;
      &lt;item&gt;Atlas Cloud Team ‚Äî Jerry Tang, Wei Xu, Simon Xue, Harry He, Eva Ma, and colleagues ‚Äî for providing a 96-device NVIDIA H100 cluster and offering responsive engineering support.&lt;/item&gt;
      &lt;item&gt;NVIDIA Solution Architect Team ‚Äî Xuting Zhou, Jinyan Chen, and colleagues ‚Äî for their work on the seamless integration of expert parallelism.&lt;/item&gt;
      &lt;item&gt;NVIDIA Enterprise Product Team ‚Äî Trevor Morris, Elfie Guo, Kaixi Hou, Kushan Ahmadian, and colleagues ‚Äî for optimizing the DeepSeek R1 kernels.&lt;/item&gt;
      &lt;item&gt;LinkedIn Team ‚Äî Biao He, Qingquan Song, Chunan Zeng, Yun Dai, Yubo Wang, and colleagues ‚Äî for optimizing the Flash-Attention 3 backend.&lt;/item&gt;
      &lt;item&gt;Mooncake Team ‚Äî Shangming Cai, Teng Ma, Mingxing Zhang, and colleagues ‚Äî for their collaboration on PD disaggregation in SGLang.&lt;/item&gt;
      &lt;item&gt;FlashInfer Team ‚Äî Zihao Ye, Yong Wu, Yaxing Cai ‚Äî for additional DeepSeek R1 kernel optimizations.&lt;/item&gt;
      &lt;item&gt;Dynamo Team - Kyle Kranen, Vikram Sharma Mailthody, and colleagues - for extra support on PD disaggregation in SGLang.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thank you all for your invaluable support and collaboration.&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix&lt;/head&gt;
    &lt;p&gt;Related PRs: #1970 #2925 #4068 #4165 #4232 #4390 #4435 #4521 #4654 #4767 #4770 #4836 #4880 #4957 #5068 #5085 #5295 #5415 #5432 #5435 #5530 #5558 #5561 #5626 #5657 #5805 #5819 #5890 DeepEP#142&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45064329</guid></item></channel></rss>