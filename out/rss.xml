<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 31 Dec 2025 00:55:01 +0000</lastBuildDate><item><title>Hive (YC S14) Is Hiring a Staff Software Engineer (Data Systems)</title><link>https://jobs.ashbyhq.com/hive.co/cb0dc490-0e32-4734-8d91-8b56a31ed497</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46433661</guid><pubDate>Tue, 30 Dec 2025 14:31:34 +0000</pubDate></item><item><title>What Happened to Abit Motherboards</title><link>https://dfarq.homeip.net/what-happened-to-abit-motherboards/</link><description>&lt;doc fingerprint="39579387f502b633"&gt;
  &lt;main&gt;
    &lt;p&gt;At the end of the year in 2008, one of the most legendary motherboard manufacturers of all time sadly went out of business. I am talking about Abit. What happened to Abit motherboards? A combination of factors took it down, including declining quality, loss of a key engineer, and a good old-fashioned scandal.&lt;/p&gt;
    &lt;head rend="h2"&gt;Abit took 7 years to become an overnight sensation&lt;/head&gt;
    &lt;p&gt;Abit wasn’t exactly a newcomer when I first learned about them in 1996. The company was founded in 1989 and made a number of 386SX, 386DX, and 486 motherboards. But it was the early hardware sites like Tom’s Hardware Guide and Anandtech that really helped to put Abit on the map during the Socket 7 era and distinguish them from the rest of the Taiwainese motherboard makers. The Abit IT5H was a Socket 7 board based on the Intel 430HX chipset that performed extremely well.&lt;/p&gt;
    &lt;head rend="h3"&gt;The jumperless Abit IT5H&lt;/head&gt;
    &lt;p&gt;Thing is, we already had an HX-based board that performed really well. Asus had those bases covered with its P55T2P4. What made Abit special was its board was jumperless. When you installed a processor, it initialized it using safe settings, and then you could go in and configure it to run at the speed you wanted using a feature called the CPU Softmenu. The Softmenu allowed you to change voltages and front side bus speed, not just the multiplier. You could even run your CPU at non-standard bus speeds like 75 or 83 MHz. Running a 166 MHz CPU at 83 MHz with a 2X multiplier actually ran faster than a 200 MHz CPU on a 66 MHz bus with a 3X multiplier. If you actually owned a 200 MHz processor, or a processor that overclocked well, you could run it at 83 MHz with a multiplier of 2.5, reach 208 MHz, and run rings around a CPU running on a 66 MHz bus with a 3x multiplier. It was the ultimate Socket 7 system at the time.&lt;/p&gt;
    &lt;p&gt;Overclockers loved the IT5H because they could easily test settings without looking up jumper settings and changing clumsy jumper blocks.&lt;/p&gt;
    &lt;head rend="h3"&gt;Abit BP6: Dual CPUs on a budget&lt;/head&gt;
    &lt;p&gt;And then there was the legendary Abit BP6. Socket 370 era Celeron processors had a Pentium II core, but Intel disabled the ability to change the multiplier to discourage overclocking and they also disabled the ability to run them in multi-processor configurations. Enthusiasts figured out that if they wired the processors up a bit differently, they could restore the multiprocessor capability. With the BP6, Abit made that unnecessary. They just wired the board up so that you could drop a pair of cheap Celeron processors into it and have a very inexpensive dual CPU setup.&lt;/p&gt;
    &lt;head rend="h2"&gt;What happened to Abit to cause its demise?&lt;/head&gt;
    &lt;p&gt;One major problem for Abit was the quality of the capacitors they used was not as high as Asus. That meant Abit motherboards didn’t age as well as Asus boards did. Arguably, in the ’90s, that wasn’t as huge of a problem because enthusiasts would upgrade every 2 or 3 years. As long as the board lasted 3 years, nobody noticed. But as the century turned, people started expecting to be able to keep their computers a little bit longer. Abit’s propensity to go cheap on the capacitors left it extremely vulnerable when capacitor plague kicked in, and indeed, Abit was one of the hardest hit.&lt;/p&gt;
    &lt;p&gt;Starting in 2002, Abit started outsourcing production of some low end boards to Elite Computer Systems, a notorious cost-cutting manufacturer. You bought from companies like Abit to avoid accidentally buying a no-name board actually made by ECS. So this was problematic.&lt;/p&gt;
    &lt;p&gt;Abit suffered a major blow in March 2003 when Oscar Wu, the mastermind behind the CPU Softmenu and much of the hardware design, departed Abit for rival motherboard maker DFI.&lt;/p&gt;
    &lt;p&gt;But perhaps the biggest problem came in December 2004, when questionable accounting practices caused its stock to be delisted. Abit had been inflating its counts and potentially embezzling funds. It wasn’t quite Miniscribe or Media Vision, let alone Worldcom. But adding fraud and dishonesty to a reputation for declining quality isn’t a recipe for longevity.&lt;/p&gt;
    &lt;p&gt;On 25 January 2006, Abit sold itself to Universal Scientific Industrial. USI sold motherboards under the new brand name Universal Abit. But the venture wasn’t successful, and Universal Abit announced that it would close December 31, 2008, and officially cease to exist on January 1, 2009.&lt;/p&gt;
    &lt;head rend="h3"&gt;Abit’s legacy&lt;/head&gt;
    &lt;p&gt;Today, Abit motherboards are prized by collectors, but if you want to actually use them, you will need to replace the capacitors. That said, if you use high quality, brand name capacitors, the boards will perform. Likely they’ll do better than they did when they were new, since the classic-era Abit boards tended to be really well built. It’s unfortunate that Abit cheaped out on the capacitors.&lt;/p&gt;
    &lt;p&gt;David Farquhar is a computer security professional, entrepreneur, and author. He has written professionally about computers since 1991, so he was writing about retro computers when they were still new. He has been working in IT professionally since 1994 and has specialized in vulnerability management since 2013. He holds Security+ and CISSP certifications. Today he blogs five times a week, mostly about retro computers and retro gaming covering the time period from 1975 to 2000.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46433915</guid><pubDate>Tue, 30 Dec 2025 14:58:18 +0000</pubDate></item><item><title>Show HN: 22 GB of Hacker News in SQLite</title><link>https://hackerbook.dosaygo.com</link><description>&lt;doc fingerprint="60e736960d5d7ecf"&gt;
  &lt;main&gt;
    &lt;p&gt;Hacker Book new | front | start | ask | show | jobs | query Someday, Month 00, 0000 &amp;lt; &amp;gt; ARCHIVE Loading… Y Combinator | Apply | Companies | Blog | Live HN | Contact &amp;lt; &amp;gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46435308</guid><pubDate>Tue, 30 Dec 2025 17:01:59 +0000</pubDate></item><item><title>Toro: Deploy Applications as Unikernels</title><link>https://github.com/torokernel/torokernel</link><description>&lt;doc fingerprint="3069f0a7771d385"&gt;
  &lt;main&gt;
    &lt;p&gt;Toro is a unikernel dedicated to deploy applications as microVMs. Toro leverages on virtio-fs and virtio-vsocket to provide a minimalistic architecture.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Support x86-64 architecture&lt;/item&gt;
      &lt;item&gt;Support up to 512GB of RAM&lt;/item&gt;
      &lt;item&gt;Support QEMU-KVM microvm and Firecracker&lt;/item&gt;
      &lt;item&gt;Cooperative and I/O bound threading scheduler&lt;/item&gt;
      &lt;item&gt;Support virtio-vsocket for networking&lt;/item&gt;
      &lt;item&gt;Support virtio-fs for filesystem&lt;/item&gt;
      &lt;item&gt;Fast boot up&lt;/item&gt;
      &lt;item&gt;Tiny image&lt;/item&gt;
      &lt;item&gt;Built-in gdbstub&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can try Toro by running the HelloWorld example using a Docker image that includes all the required tools. To do so, execute the following commands in a console (these steps require you to install before KVM and Docker):&lt;/p&gt;
    &lt;code&gt;wget https://raw.githubusercontent.com/torokernel/torokernel/master/ci/Dockerfile
sudo docker build -t torokernel-dev .
sudo docker run --privileged --rm -it torokernel-dev
cd examples/HelloWorld
python3 ../CloudIt.py -a HelloWorld&lt;/code&gt;
    &lt;p&gt;If these commands execute successfully, you will get the output of the HelloWorld example. You can also pull the image from dockerhub instead of building it:&lt;/p&gt;
    &lt;code&gt;sudo docker pull torokernel/torokernel-dev:latest
sudo docker run --privileged --rm -it torokernel/torokernel-dev:latest&lt;/code&gt;
    &lt;p&gt;You can share a directory from the host by running:&lt;/p&gt;
    &lt;code&gt;sudo docker run --privileged --rm --mount type=bind,source="$(pwd)",target=/root/torokernel -it torokernel/torokernel-dev:latest&lt;/code&gt;
    &lt;p&gt;You will find $pwd from host at &lt;code&gt;/root/torokernel&lt;/code&gt; in the container.&lt;/p&gt;
    &lt;p&gt;Execute the commands in &lt;code&gt;ci/Dockerfile&lt;/code&gt; to install the required components locally. Then, Go to &lt;code&gt;torokernel/examples&lt;/code&gt; and edit &lt;code&gt;CloudIt.py&lt;/code&gt; to set the correct paths to Qemu and fpc. Optionally, you can install vsock-socat from here and virtio-fs from here. You need to set the correct path to virtiofsd and socat.&lt;/p&gt;
    &lt;p&gt;Go to &lt;code&gt;examples/HelloWorld/&lt;/code&gt; and execute:&lt;/p&gt;
    &lt;code&gt;python3 ../CloudIt.py -a HelloWorld&lt;/code&gt;
    &lt;p&gt;To run the StaticWebserver, you require virtiofsd and socat. To compile socat, execute the following commands:&lt;/p&gt;
    &lt;code&gt;git clone git@github.com:stefano-garzarella/socat-vsock.git
cd socat-vsock
autoreconf -fiv
./configure
make socat&lt;/code&gt;
    &lt;p&gt;Set the path to socat binary in CloudIt.py and then execute:&lt;/p&gt;
    &lt;code&gt;python3 ../CloudIt.py -a StaticWebServer -r -d /path-to-directory/ -f 4000:80&lt;/code&gt;
    &lt;p&gt;You have to replace the &lt;code&gt;/path-to-directory/&lt;/code&gt; to a directory that containing the files, e.g., index.html. To try it, you can execute:&lt;/p&gt;
    &lt;code&gt;wget http://127.0.0.1:4000/index.html
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;-f&lt;/code&gt; parameter indicates a forwarding of the 4000 port from the host to the 80 port in the guest using vsock.&lt;/p&gt;
    &lt;p&gt;This example shows how cores can communicate by using the VirtIOBus device. In this example, core #0 sends a packet to every core in the system with the ping string. Each core responds with a packet that contains the message pong. This example is configured to use three cores. To launch it, simply executes the following commands in the context of the container presented above:&lt;/p&gt;
    &lt;code&gt;python3 ../CloudIt.py -a InterCoreComm&lt;/code&gt;
    &lt;p&gt;You will get the following output:&lt;/p&gt;
    &lt;p&gt;You have many ways to contribute to Toro. One of them is by joining the Google Group here. In addition, you can find more information here.&lt;/p&gt;
    &lt;p&gt;GPLv3&lt;/p&gt;
    &lt;p&gt;[0] A Dedicated Kernel named Toro. Matias Vara. FOSDEM 2015.&lt;/p&gt;
    &lt;p&gt;[1] Reducing CPU usage of a Toro Appliance. Matias Vara. FOSDEM 2018.&lt;/p&gt;
    &lt;p&gt;[2] Toro, a Dedicated Kernel for Microservices. Matias Vara and Cesar Bernardini. Open Source Summit Europe 2018.&lt;/p&gt;
    &lt;p&gt;[3] Speeding Up the Booting Time of a Toro Appliance. Matias Vara. FOSDEM 2019.&lt;/p&gt;
    &lt;p&gt;[4] Developing and Deploying Microservices with Toro Unikernel. Matias Vara. Open Source Summit Europe 2019.&lt;/p&gt;
    &lt;p&gt;[5] Leveraging Virtio-fs and Virtio-vsocket in Toro Unikernel. Matias Vara. DevConfCZ 2020.&lt;/p&gt;
    &lt;p&gt;[6] Building a Cloud Infrastructure to Deploy Microservices as Microvm Guests. Matias Vara. KVM Forum 2020.&lt;/p&gt;
    &lt;p&gt;[7] Running MPI applications on Toro unikernel. Matias Vara. FOSDEM 2023.&lt;/p&gt;
    &lt;p&gt;[8] Is Toro unikernel faster for MPI?. Matias Vara. FOSDEM 2024.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46435418</guid><pubDate>Tue, 30 Dec 2025 17:09:57 +0000</pubDate></item><item><title>A Vulnerability in Libsodium</title><link>https://00f.net/2025/12/30/libsodium-vulnerability/</link><description>&lt;doc fingerprint="1c17b053a8636291"&gt;
  &lt;main&gt;
    &lt;p&gt;Libsodium is now 13 years old!&lt;/p&gt;
    &lt;p&gt;I started that project to pursue Dan Bernsteinâs desire to make cryptography simple to use. That meant exposing a limited set of high-level functions and parameters, providing a simple API, and writing documentation for users, not cryptographers. Libsodiumâs goal was to expose APIs to perform operations, not low-level functions. Users shouldnât even have to know or care about what algorithms are used internally. This is how Iâve always viewed libsodium.&lt;/p&gt;
    &lt;p&gt;Never breaking the APIs is also something Iâm obsessed with. APIs may not be great, and if I could start over from scratch, I would have made them very different, but as a developer, the best APIs are not the most beautifully designed ones, but the ones that you donât have to worry about because they donât change and upgrades donât require any changes in your application either. Libsodium started from the NaCl API, and still adheres to it.&lt;/p&gt;
    &lt;p&gt;These APIs exposed high-level functions, but also some lower-level functions that high-level functions wrap or depend on. Over the years, people started using these low-level functions directly. Libsodium started to be used as a toolkit of algorithms and low-level primitives.&lt;/p&gt;
    &lt;p&gt;That made me sad, especially since it is clearly documented that only APIs from builds with &lt;code&gt;--enable-minimal&lt;/code&gt; are guaranteed to be tested and stable. But after all, it makes sense. When building custom protocols, having a single portable library with a consistent interface for different functions is far better than importing multiple dependencies, each with their own APIs and sometimes incompatibilities between them.&lt;/p&gt;
    &lt;p&gt;Thatâs a lot of code to maintain. It includes features and target platforms I donât use but try to support for the community. I also maintain a large number of other open source projects.&lt;/p&gt;
    &lt;p&gt;Still, the security track record of libsodium is pretty good, with zero CVEs in 13 years even though it has gotten a lot of scrutiny.&lt;/p&gt;
    &lt;p&gt;However, while recently experimenting with adding support for batch signatures, I noticed inconsistent results with code originally written in Zig. The culprit was a check that was present in a function in Zig, but that I forgot to add in libsodium.&lt;/p&gt;
    &lt;head rend="h2"&gt;The bug&lt;/head&gt;
    &lt;p&gt;The function &lt;code&gt;crypto_core_ed25519_is_valid_point()&lt;/code&gt;, a low-level function used to check if a given elliptic curve point is valid, was supposed to reject points that arenât in the main cryptographic group, but some points were slipping through.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why does this matter?&lt;/head&gt;
    &lt;p&gt;Edwards25519 is like a special mathematical playground where cryptographic operations happen.&lt;/p&gt;
    &lt;p&gt;It is used internally for Ed25519 signatures, and includes multiple subgroups of different sizes (order):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Order 1: just the identity (0, 1)&lt;/item&gt;
      &lt;item&gt;Order 2: identity + point (0, -1)&lt;/item&gt;
      &lt;item&gt;Order 4: 4 points&lt;/item&gt;
      &lt;item&gt;Order 8: 8 points&lt;/item&gt;
      &lt;item&gt;Order L: the âmain subgroupâ (L = ~2^252 points) where all operations are expected to happen&lt;/item&gt;
      &lt;item&gt;Order 2L, 4L, 8L: very large, but not prime order subgroups&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The validation function was designed to reject points not in the main subgroup. It properly rejected points in the small-order subgroups, but not points in the mixed-order subgroups.&lt;/p&gt;
    &lt;head rend="h2"&gt;What went wrong technically?&lt;/head&gt;
    &lt;p&gt;To check if a point is in the main subgroup (the one of order L), the function multiplies it by L. If the order is L, multiplying any point by L gives the identity point (the mathematical equivalent of zero). So, the code does the multiplication and checks that we ended up with the identity point.&lt;/p&gt;
    &lt;p&gt;Points are represented by coordinates. In the internal representation used here, there are three coordinates: X, Y, and Z. The identity point is represented internally with coordinates where X = 0 and Y = Z. Z can be anything depending on previous operations; it doesnât have to be 1.&lt;/p&gt;
    &lt;p&gt;The old code only checked X = 0. It forgot to verify Y = Z. This meant some invalid points (where X = 0 but Y â Z after the multiplication) were incorrectly accepted as valid.&lt;/p&gt;
    &lt;p&gt;Concretely: take any main-subgroup point Q (for example, the output of &lt;code&gt;crypto_core_ed25519_random&lt;/code&gt;) and add the order-2 point (0, -1), or equivalently negate both coordinates. Every such Q + (0, -1) would have passed validation before the fix, even though itâs not in the main subgroup.&lt;/p&gt;
    &lt;head rend="h2"&gt;The fix&lt;/head&gt;
    &lt;p&gt;The fix is trivial and adds the missing check:&lt;/p&gt;
    &lt;code&gt;// OLD:
return fe25519_iszero(pl.X);
&lt;/code&gt;
    &lt;code&gt;// NEW:
fe25519_sub(t, pl.Y, pl.Z);
return fe25519_iszero(pl.X) &amp;amp; fe25519_iszero(t);
&lt;/code&gt;
    &lt;p&gt;Now it properly verifies both conditions: X must be zero and Y must equal Z.&lt;/p&gt;
    &lt;head rend="h2"&gt;Who is affected?&lt;/head&gt;
    &lt;p&gt;You may be affected if you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use a point release &amp;lt;= &lt;code&gt;1.0.20&lt;/code&gt;or a version of&lt;code&gt;libsodium&lt;/code&gt;released before December 30, 2025.&lt;/item&gt;
      &lt;item&gt;Use &lt;code&gt;crypto_core_ed25519_is_valid_point()&lt;/code&gt;to validate points from untrusted sources&lt;/item&gt;
      &lt;item&gt;Implement custom cryptography using arithmetic over the Edwards25519 curve&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But donât panic. Most users are not affected.&lt;/p&gt;
    &lt;p&gt;None of the high-level APIs (&lt;code&gt;crypto_sign_*&lt;/code&gt;) are affected; they donât even use or need that function. Scalar multiplication using &lt;code&gt;crypto_scalarmult_ed25519&lt;/code&gt; wonât leak anything even if the public key is not on the main subgroup. And public keys created with the regular &lt;code&gt;crypto_sign_keypair&lt;/code&gt; and &lt;code&gt;crypto_sign_seed_keypair&lt;/code&gt; functions are guaranteed to be on the correct subgroup.&lt;/p&gt;
    &lt;head rend="h2"&gt;Recommendation&lt;/head&gt;
    &lt;p&gt;Support for the Ristretto255 group was added to libsodium in 2019 specifically to solve cofactor-related issues. With Ristretto255, if a point decodes, itâs safe. No further validation is required.&lt;/p&gt;
    &lt;p&gt;If you implement custom cryptographic schemes doing arithmetic over a finite field group, using Ristretto255 is recommended. Itâs easier to use, and as a bonus, low-level operations will run faster than over Edwards25519.&lt;/p&gt;
    &lt;p&gt;If you canât update libsodium and need an application-level workaround, use the following function:&lt;/p&gt;
    &lt;code&gt;int is_on_main_subgroup(const unsigned char p[crypto_core_ed25519_BYTES])
{
    /* l - 1 (group order minus 1) */
    static const unsigned char L_1[crypto_core_ed25519_SCALARBYTES] = {
        0xec, 0xd3, 0xf5, 0x5c, 0x1a, 0x63, 0x12, 0x58,
        0xd6, 0x9c, 0xf7, 0xa2, 0xde, 0xf9, 0xde, 0x14,
        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x10
    };
    /* Identity point encoding: (x=0, y=1) */
    static const unsigned char ID[crypto_core_ed25519_BYTES] = {
        0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00
    };
    unsigned char t[crypto_core_ed25519_BYTES];
    unsigned char r[crypto_core_ed25519_BYTES];
    if (crypto_scalarmult_ed25519_noclamp(t, L_1, p) != 0 ||
        crypto_core_ed25519_add(r, t, p) != 0) {
        return 0;
    }
    return sodium_memcmp(r, ID, sizeof ID) == 0;
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Fixed packages&lt;/head&gt;
    &lt;p&gt;This issue was fixed immediately after discovery. All &lt;code&gt;stable&lt;/code&gt; packages released after December 30, 2025 include the fix:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;official tarballs&lt;/item&gt;
      &lt;item&gt;binaries for Visual Studio&lt;/item&gt;
      &lt;item&gt;binaries for MingW&lt;/item&gt;
      &lt;item&gt;NuGet packages for all architectures including Android&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;swift-sodium&lt;/code&gt;xcframework (but&lt;code&gt;swift-sodium&lt;/code&gt;doesnât expose low-level functions anyway)&lt;/item&gt;
      &lt;item&gt;rust &lt;code&gt;libsodium-sys-stable&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;libsodium.js&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A new point release is also going to be tagged.&lt;/p&gt;
    &lt;p&gt;If &lt;code&gt;libsodium&lt;/code&gt; is useful to you, please keep in mind that it is maintained by one person, for free, in time I could spend with my family or on other projects. The best way to help the project would be to consider sponsoring it, which helps me dedicate more time to improving it and making it great for everyone, for many more years to come.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46435614</guid><pubDate>Tue, 30 Dec 2025 17:24:57 +0000</pubDate></item><item><title>Electrolysis can solve one of our biggest contamination problems</title><link>https://ethz.ch/en/news-and-events/eth-news/news/2025/11/electrolysis-can-solve-one-of-our-biggest-contamination-problems.html</link><description>&lt;doc fingerprint="cb108501eca1c785"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Electrolysis can solve one of our biggest contamination problems&lt;/head&gt;
    &lt;p&gt;ETH Zurich researchers have developed a process that can be used on site to render environmental toxins such as DDT and lindane harmless and convert them into valuable chemicals – a breakthrough for the remediation of contaminated sites and a sustainable circular economy.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Read&lt;/item&gt;
      &lt;item&gt;Number of comments&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;In brief&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Persistent organic pollutants such as DDT and lindane still pollute the environment and affect humans decades after their use.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;ETH researchers have developed a new electrochemical process that completely dehalogenates these long-lived toxins and converts them into valuable industrial chemicals.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The method uses cheap equipment, prevents side reactions and could be used on contaminated landfills, soils or sludge.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mobile systems could be used on site in the future – an important step towards the remediation of contaminated sites and the creation of a sustainable circular economy.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They were once considered miracle workers – insecticides such as lindane or DDT were produced and used millions of times during the 20th century. But what was hailed as progress led to a global environmental catastrophe: persistent organic pollutants (POPs) are so chemically stable that they remain in soil, water and organisms for decades. They accumulate in the fatty tissue of animals and thus enter the human food chain. Many of these substances were banned long ago, but their traces can still be found today – even in human blood.&lt;/p&gt;
    &lt;p&gt;How to remediate such contaminated sites, be they soils, bodies of water or landfills, is one of the major unresolved questions of environmental protection. How can highly stable poisons be rendered harmless without creating new problems? Researchers at ETH Zurich, led by Bill Morandi, Professor of Synthetic Organic Chemistry, have now found a promising approach. Using an innovative electrochemical method, they are not only able to break down these long-lived pollutants but also to convert them into valuable raw materials for the chemical industry.&lt;/p&gt;
    &lt;head rend="h2"&gt;Converting pollutants into raw materials&lt;/head&gt;
    &lt;p&gt;A key distinction between this and previous work is that the carbon skeleton of the pollutants is recycled and made reusable, while the halide component is sequestered as a harmless inorganic salt. “The previous methods were also energetically inefficient,” says Patrick Domke, a doctoral student in Morandi’s group. He explains: “The processes were expensive and still led to outcomes that were harmful to the environment.”&lt;/p&gt;
    &lt;p&gt;Together with electrochemistry specialist Alberto Garrido-Castro, a former postdoc in this group, Domke developed a process that renders the pollutants in question completely harmless. During this project, the two researchers were able to draw on the many years of experience of ETH professor Morandi, who has been working on the transformation of such compounds for years. “The key advance of this new technology is the use of alternating current to sequester the problematic halogen atoms as innocuous salts such as NaCl (table salt), while still generating valuable hydrocarbons,” says Morandi.&lt;/p&gt;
    &lt;head rend="h2"&gt;Using electricity to break down toxins&lt;/head&gt;
    &lt;p&gt;Electrolysis enables almost complete dehalogenation of pollutants under mild, environmentally friendly and cost-effective conditions. It cleaves the stable carbon-halogen bonds, leaving behind only harmless salts such as table salt and useful hydrocarbons such as benzene, diphenylethane or cyclododecatriene. These are actually sought-after intermediates in the chemical industry, for example, for plastics, varnishes, coatings and pharmaceutical applications. In this way, the technology not only contributes to the remediation of contaminated sites but also to the sustainable circular economy.&lt;/p&gt;
    &lt;p&gt;“What makes our process so special from a technical point of view is that we supply electricity using alternating current, similar to the electrical waveform delivered to households. It is one of the most cost-effective resources in chemistry,” explains Garrido-Castro. “Alternating current protects the electrodes from wear, which is why we can reuse them for many subsequent electrolysis cycles. In addition, the alternating current suppresses unwanted side reactions and the formation of poisonous chlorine gas, allowing the pollutant’s halogen atoms to be fully converted to inorganic salts.” The reactor used by the researchers consists of an undivided electrolysis cell in which dimethyl sulfoxide (DMSO) is used as a solvent – itself a by-product of the pulp process in paper production.&lt;/p&gt;
    &lt;head rend="h2"&gt;A fully thought-out circular economy&lt;/head&gt;
    &lt;p&gt;The process can be applied not only to pure substances but also to mixtures from contaminated soils. Soil or sludge can therefore be treated without pre-treatment or further separation processes. A prototype of the reactor has already been successfully tested on classic environmental toxins such as lindane and DDT. “Our system is mobile and can be assembled on site. This eliminates the need to transport these hazardous substances,” explains Domke.&lt;/p&gt;
    &lt;quote&gt;“Our motivation was to solve one of the biggest environmental problems of the last century. We cannot simply leave the pollution to future generations.”Alberto Garrido-Castro&lt;/quote&gt;
    &lt;head rend="h2"&gt;Spark Award 2025 – these projects have made it to the finals&lt;/head&gt;
    &lt;p&gt;On 27 November 2025 at ETH Zurich @ Open-i, ETH Zurich will award the Spark Award for the best invention of the year for the 14th time. The criteria for this award are originality, patent strength and market potential.&lt;/p&gt;
    &lt;p&gt;Click here to find all the Spark Award nominees of 2025.&lt;/p&gt;
    &lt;p&gt;Spark Award ceremony, Industry Day @ Open-i, Thursday, 27 November 2025, 1.30 p.m., Zurich Convention Center. Registration is required.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46436127</guid><pubDate>Tue, 30 Dec 2025 18:08:32 +0000</pubDate></item><item><title>A faster heart for F-Droid. Our new server is here</title><link>https://f-droid.org/2025/12/30/a-faster-heart-for-f-droid.html</link><description>&lt;doc fingerprint="fdf7b599e0066ed7"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;A faster heart for F-Droid. Our new server is here!&lt;/head&gt;Posted on Dec 30, 2025 by F-Droid&lt;p&gt;Donations are a key part of what keeps F-Droid independent and reliable and our latest hardware update is a direct result of your support. Thanks to donations from our incredible community, F-Droid has replaced one of its most critical pieces of infrastructure, our core server hardware. It was overdue for a refresh, and now we are happy to give you an update on the new server and how it impacts the project.&lt;/p&gt;&lt;p&gt;This upgrade touches a core part of the infrastructure that builds and publishes apps for the main F-Droid repository. If the server is slow, everything downstream gets slower too. If it is healthy, the entire ecosystem benefits.&lt;/p&gt;&lt;head rend="h2"&gt;Why did we wait?&lt;/head&gt;&lt;p&gt;This server replacement took a bit longer than we would have liked. The biggest reason is that sourcing reliable parts right now is genuinely hard. Ongoing global trade tensions have made supply chains unpredictable, and that hit the specific components we needed. We had to wait for quotes, review, replan, and wait again when quotes turned out to have unexpected long waits, before we finally managed to receive hardware that met our requirements.&lt;/p&gt;&lt;p&gt;Even with the delays, the priority never changed. We were looking for the right server set up for F-Droid, built to last for the long haul.&lt;/p&gt;&lt;head rend="h2"&gt;A note about the host&lt;/head&gt;&lt;p&gt;Another important part of this story is where the server lives and how it is managed. F-Droid is not hosted in just any data center where commodity hardware is managed by some unknown staff. We worked out a special arrangement so that this server is physically held by a long time contributor with a proven track record of securely hosting services. We can control it remotely, we know exactly where it is, and we know who has access. That level of transparency and trust is not common in infrastructure, but it is central to how we think about resilience and stewardship.&lt;/p&gt;&lt;p&gt;This was not the easiest path, and it required careful coordination and negotiation. But we are glad we did it this way. It fits our values and our threat model, and it keeps the project grounded in real people rather than anonymous systems.&lt;/p&gt;&lt;head rend="h2"&gt;Old hardware, new momentum&lt;/head&gt;&lt;p&gt;The previous server was 12 year old hardware and had been running for about five years. In infrastructure terms, that is a lifetime. It served F-Droid well, but it was reaching the point where speed and maintenance overhead were becoming a daily burden.&lt;/p&gt;&lt;p&gt;The new system is already showing a huge improvement. Stats of the running cycles from the last two months suggest it can handle the full build and publish actions much faster than before. E.g. this year, between January and September, we published updates once every 3 or 4 days, that got down to once every 2 days in October, to every day in November and itâs reaching twice a day in December. (You can see this in the frequency of index publishing after October 18, 2025 in our f-droid.org transparency log). That extra capacity gives us more breathing room and helps shorten the gap between when apps are updated and when those updates reach users. We can now build all the auto-updated apps in the (UTC) morning in one cycle, and all the newly included apps, fixed apps and manually updated apps, through the day, in the evening cycle.&lt;/p&gt;&lt;p&gt;We are being careful here, because real world infrastructure always comes with surprises. But the performance gains are real, and they are exciting.&lt;/p&gt;&lt;head rend="h2"&gt;What donations make possible&lt;/head&gt;&lt;p&gt;This upgrade exists because of community support, pooled over time, turned into real infrastructure, benefiting everyone who relies on F-Droid.&lt;/p&gt;&lt;p&gt;A faster server does not just make our lives easier. It helps developers get timely builds. It reduces maintenance risk. It strengthens the health of the entire repository.&lt;/p&gt;&lt;p&gt;So thank you. Every donation, whether large or small, is part of how this project stays reliable, independent, and aligned with free software values.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46436409</guid><pubDate>Tue, 30 Dec 2025 18:36:37 +0000</pubDate></item><item><title>Escaping containment: A security analysis of FreeBSD jails [video]</title><link>https://media.ccc.de/v/39c3-escaping-containment-a-security-analysis-of-freebsd-jails</link><description>&lt;doc fingerprint="1512f8d2f0f5cfeb"&gt;
  &lt;main&gt;
    &lt;p&gt;ilja and Michael Smith&lt;/p&gt;
    &lt;p&gt;FreeBSD’s jail mechanism promises strong isolation—but how strong is it really? &lt;lb/&gt;In this talk, we explore what it takes to escape a compromised FreeBSD jail by auditing the kernel’s attack surface, identifying dozens of vulnerabilities across exposed subsystems, and developing practical proof-of-concept exploits. We’ll share our findings, demo some real escapes, and discuss what they reveal about the challenges of maintaining robust OS isolation.&lt;/p&gt;
    &lt;p&gt;FreeBSD’s jail feature is one of the oldest and most mature OS-level isolation mechanisms in use today, powering hosting environments, container frameworks, and security sandboxes. But as with any large and evolving kernel feature, complexity breeds opportunity. This research asks a simple but critical question: If an attacker compromises root inside a FreeBSD jail, what does it take to break out?&lt;/p&gt;
    &lt;p&gt;To answer that, we conducted a large-scale audit of FreeBSD kernel code paths accessible from within a jail. We systematically examined privileged operations, capabilities, and interfaces that a jailed process can still reach, hunting for memory safety issues, race conditions, and logic flaws. The result: roughly 50 distinct issues uncovered across multiple kernel subsystems, ranging from buffer overflows and information leaks to unbounded allocations and reference counting errors—many of which could crash the system or provide vectors for privilege escalation beyond the jail.&lt;/p&gt;
    &lt;p&gt;We’ve developed proof-of-concept exploits and tools to demonstrate some of these vulnerabilities in action. We’ve responsibly disclosed our findings to the FreeBSD security team and are collaborating with them on fixes. Our goal isn’t to break FreeBSD, but to highlight the systemic difficulty of maintaining strict isolation in a large, mature codebase.&lt;/p&gt;
    &lt;p&gt;This talk will present our methodology, tooling, and selected demos of real jail escapes. We’ll close with observations about kernel isolation boundaries, lessons learned for other OS container systems, and a call to action for hardening FreeBSD’s jail subsystem against the next generation of threats.&lt;/p&gt;
    &lt;p&gt;Licensed to the public under http://creativecommons.org/licenses/by/4.0&lt;/p&gt;
    &lt;head rend="h3"&gt;Download&lt;/head&gt;
    &lt;head rend="h4"&gt;Video&lt;/head&gt;
    &lt;head rend="h4"&gt;These files contain multiple languages.&lt;/head&gt;
    &lt;p&gt;This Talk was translated into multiple languages. The files available for download contain all languages as separate audio-tracks. Most desktop video players allow you to choose between them.&lt;/p&gt;
    &lt;p&gt;Please look for "audio tracks" in your desktop video player.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46436828</guid><pubDate>Tue, 30 Dec 2025 19:15:05 +0000</pubDate></item><item><title>FediMeteo: A €4 FreeBSD VPS Became a Global Weather Service</title><link>https://it-notes.dragas.net/2025/02/26/fedimeteo-how-a-tiny-freebsd-vps-became-a-global-weather-service-for-thousands/</link><description>&lt;doc fingerprint="b20d9871206db6ab"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Personal Introduction&lt;/head&gt;
    &lt;p&gt;Weather has always significantly influenced my life. When I was a young athlete, knowing the forecast in advance would have allowed me to better plan my training sessions. As I grew older, I could choose whether to go to school on my motorcycle or, for safety reasons, have my grandfather drive me. And it was him, my grandfather, who was my go-to meteorologist. He followed all weather patterns and forecasts, a remnant of his childhood in the countryside and his life on the move. It's to him that I dedicate FediMeteo.&lt;/p&gt;
    &lt;p&gt;The idea for FediMeteo started almost by chance while I was checking the holiday weather forecast to plan an outing. Suddenly, I thought how nice it would be to receive regular weather updates for my city directly in my timeline. After reflecting for a few minutes, I registered a domain and started planning.&lt;/p&gt;
    &lt;head rend="h2"&gt;Design Principles&lt;/head&gt;
    &lt;p&gt;The choice of operating system was almost automatic. The idea was to separate instances by country, and FreeBSD jails are one of the most useful tools for this purpose.&lt;/p&gt;
    &lt;p&gt;I initially thought the project would generate little interest. I was wrong. After all, weather affects many of our lives, directly or indirectly. So I decided to structure everything in this way:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;I would use a test VPS to see how things would go. The VPS was a small VM on a German provider with 4 shared cores, 4GB of RAM, 120GB of SSD disk space, and a 1Gbit/sec internet connection and now is a 4 euro per month VPS in Milano, Italy - 4 shared cores, 8 GB RAM and 75GB disk space.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I would separate various countries into different instances, for both management and security reasons, as well as to have the possibility of relocating just some of them if needed.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Weather data would come from a reliable and open-source friendly source. I narrowed it down to two options: wttr.in and Open-Meteo, two solutions I know and that have always given me reliable results.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I would pay close attention to accessibility: forecasts would be in local languages, consultable via text browsers, with emojis to give an idea even to those who don't speak local languages, and everything would be accessible without JavaScript or other requirements. One's mother tongue is always more "familiar" than a second language, even if you're fluent.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I would manage everything according to Unix philosophy: small pieces working together. The more years pass, the more I understand how valuable this approach is.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The software chosen to manage the instances is snac. Snac embodies my philosophy of minimal and effective software, perfect for this purpose. It provides clear web pages for those who want to consult via the web, "speaks" the ActivityPub protocol perfectly, produces RSS feeds for each user (i.e., city), has extremely low RAM and CPU consumption, compiles in seconds, and is stable. The developer is an extremely helpful and positive person, and in my opinion, this carries equal weight as everything else.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I would do it for myself. If there was no interest, I would have kept it running anyway, without expanding it. So no anxiety or fear of failure.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Technical Implementation&lt;/head&gt;
    &lt;p&gt;I started setting up the first "pieces" during the days around Christmas 2024. The scheme was clear: each jail would handle everything internally. A Python script would download data, city by city, and produce markdown. The city coordinates would be calculated via the geopy library and passed to wttr.in and Open-Meteo. No data would be stored locally. This approach gives the ability to process all cities together. Just pass the city and country to the script, and the markdown would be served. At that point, snac comes into play: without the need to use external utilities, the "snac note" command allows posting from stdin by specifying the instance directory and the user to post from. No need to make API calls with external utilities, having to manage API keys, permissions, etc.&lt;/p&gt;
    &lt;head rend="h3"&gt;Setting Up for Italy&lt;/head&gt;
    &lt;p&gt;To simplify things, I first structured the jail for Italy. I made a list of the main cities, normalizing them. For example, La Spezia became la_spezia. ForlÃ¬, with an accent, became forli - this for maximum compatibility since each city would be a snac user. I then created a script that takes this list and creates snac users via "snac adduser." At that point, after creating all the users, the script would modify the JSON of each user to convert the city name to uppercase, insert the bio (a standard text), activate the "bot" flag, and set the avatar, which was the same for all users at the time. This script is also able to add a new city: just run the script with the (normalized) name of the city, and it will add it - also adding it to the "cities.txt" file, so it will be updated in the next weather update cycle.&lt;/p&gt;
    &lt;head rend="h3"&gt;Core Application Development&lt;/head&gt;
    &lt;p&gt;I then created the heart of the service. A Python application (initially only in Italian, then multilingual, separating the operational part from the text) able to receive (via command line) the name of a city and a country code (corresponding to the file with texts in the local language). The script determines the coordinates and then, using API calls, requests the current weather conditions, those for the next 12 hours, and the next 7 days. I conducted experiments with both wttr.in and Open-Meteo, and both gave good results. However, I settled on Open-Meteo because, for my uses, it has always provided very reliable results. This application directly provides an output in Markdown since snac supports it, at least partially.&lt;/p&gt;
    &lt;p&gt;The cities.txt file is also crucial for updates. I created a script - post.sh, in pure sh, that scrolls through all cities, and for each one, launches the FediMeteo application and publishes its output using snac directly via command line. Once the job is finished, it makes a call to my instance of Uptime-Kuma, which keeps an eye on the situation. In case of failure, the monitoring will alert me that there have been no recent updates, and I can check.&lt;/p&gt;
    &lt;p&gt;At this point, the system cron takes care of launching post.sh every 6 hours. The requests are serialized, so the cities will update one at a time, and the posts will be sent to followers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Growth and Unexpected Success&lt;/head&gt;
    &lt;p&gt;After listing all Italian provincial capitals, I started testing everything. It worked perfectly. Of course, I had to make some adjustments at all levels. For example, one of the problems encountered was that snac did not set the language of the posts, and some users could have missed them. The developer was very quick and, as soon as I exposed the problem, immediately modified the program so that the post could keep the system language, set as an environment variable in the sh script.&lt;/p&gt;
    &lt;p&gt;After two days, I decided to start adding other countries and announce the project. And the announcement was unexpectedly well received: there were many boosts, and people started asking me to add their cities or countries. I tried to do what I could, within the limits of my physical condition, as in those days, I had the flu that kept me at home with a fever and illness for several days. I started adding many countries in the heart of Europe, translating the main indications into local languages but maintaining emojis so that everything would be understandable even to those who don't speak the local language. There were some small problems reported by some users. One of them: not all weather conditions had been translated, so sometimes they appeared in Italian - as well as errors. In bilingual countries, I tried to include all local languages. Sometimes, unfortunately, making mistakes as I encountered dynamics unknown to me or difficult to interpret. For example, in Ireland, forecasts were published in Irish, but it was pointed out to me that not everyone speaks it, so I modified and published in English.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Turning Point&lt;/head&gt;
    &lt;p&gt;The turning point was when FediFollows (@FediFollows@social.growyourown.services - who also manages the site Fedi Directory) started publishing the list of countries and cities, highlighting the project. Many people became aware of FediMeteo and started following the various accounts, the various cities. And from here came requests to add new countries and some new information, such as wind speed. Moreover, I was asked (rightly, to avoid flooding timelines) to publish posts as unlisted - this way, followers would see the posts, but they wouldn't fill local timelines. Snac didn't support this, but again, the snac dev came to my rescue in a few hours.&lt;/p&gt;
    &lt;head rend="h2"&gt;Scaling Challenges&lt;/head&gt;
    &lt;p&gt;But with new countries came new challenges. For example, in my original implementation, all units of measurement were in metric/decimal/Celsius - and this doesn't adapt well to realities like the USA. Moreover, focusing on Europe, almost all countries were located in a single timezone, while for larger countries (such as Australia, USA, Canada, etc.), this is totally different. So I started developing a more complete and global version and, in the meantime, added almost all of Europe. The new version would have to be backward compatible, would have to take into account timezone differences for each city, different measurements (e.g., degrees C and F), as well as, initially more difficult part, being able to separate cities with the same name based on states or provinces. I had already seen a similar problem with the implementation of support for Germany, so it had to be addressed properly.&lt;/p&gt;
    &lt;p&gt;The original goal was to have a VPS for each continent, but I soon realized that thanks to the quality of snac's code and FreeBSD's efficient management, even keeping countries in separate jails, the load didn't increase much. So I decided to challenge myself and the limits of the economical 4 euros per month VPS. That is, to insert as much as possible until seeing what the limits were. Limits that, to date, I have not yet reached. I would also soon exhaust the available API calls for Open-Meteo's free accounts, so I tried to contact the team and explain everything. I was positively surprised to read that they appreciated the project and provided me with a dedicated API key.&lt;/p&gt;
    &lt;p&gt;Compatible with my free time, I managed to complete the richer and more complete version of my Python program. I'm not a professional dev, I'm more oriented towards systems, so the code is probably quite poor in the eyes of an expert dev. But, in the end, it just needs to take an input and give me an output. It's not a daemon, it's not a service that responds on the network. For that, snac takes care of it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Expansion to North America&lt;/head&gt;
    &lt;p&gt;So I decided to start with a very important launch: the USA and Canada. A non-trivial part was identifying the main cities in order to cover, state by state, all the territory. In the end, I identified more than 1200 cities. A number that, by itself, exceeded the sum of all other countries (at that time). And the program, now, is able to take an input with a separator (two underscores: __) between city and state. In this way, it's possible to perfectly understand the differences between city and state: new_york__new_york is an example I like to make, but there are many.&lt;/p&gt;
    &lt;p&gt;The launch of the USA was interesting: despite having had many previous requests, the reception was initially quite lukewarm, to my extreme surprise. The number of followers in Canada, in a few hours, far exceeded that of the USA. On the contrary, the country with the most followers (in a few days, more than 1000) was Germany. Followed by the UK - which I expected would have been the first.&lt;/p&gt;
    &lt;head rend="h2"&gt;System Performance&lt;/head&gt;
    &lt;p&gt;The VPS held up well. Except for the moments when FediFollows launched (after fixing some FreeBSD tuning, the service slowed slightly but didn't crash), the load remained extremely low. So I continued to expand: Japan, Australia, New Zealand, etc.&lt;/p&gt;
    &lt;head rend="h2"&gt;Current Status&lt;/head&gt;
    &lt;p&gt;At the time of the last update of this article (30 December 2025), the supported countries are 38: Argentina, Australia, Austria, Belgium, Brazil, Bulgaria, Canada, Croatia, Czechia, Denmark, Estonia, Finland, France, Germany, Greece, Hungary, India, Ireland, Italy, Japan, Latvia, Lithuania, Malta, Mexico, Netherlands, New Zealand, Norway, Poland, Portugal, Romania, Slovakia, Slovenia, Spain, Sweden, Switzerland, Taiwan, the United Kingdom, and the United States of America (with more regions coming soon!).&lt;/p&gt;
    &lt;p&gt;Direct followers in the Fediverse are around 7,707 and growing daily, excluding those who follow hashtags or cities via RSS, whose number I can't estimate. However, a quick look at the logs suggests there are many more.&lt;/p&gt;
    &lt;p&gt;The cities currently covered are 2937 - growing based on new countries and requests.&lt;/p&gt;
    &lt;head rend="h2"&gt;Challenges Encountered&lt;/head&gt;
    &lt;p&gt;There have been some problems. The most serious, by my fault, was the API key leak: I had left a debug code active and, the first time Open-Meteo had problems, the error message also included the API call - including the API key. Some users reported it to me (others just mocked) and I fixed the code and immediately reported everything to the Open-Meteo team, who kindly gave me a new API Key and deactivated the old one.&lt;/p&gt;
    &lt;p&gt;A further problem was related to geopy. It makes a call to Nominatim to determine coordinates. One of the times Nominatim didn't respond, my program wasn't able to determine the position and went into error. I solved this by introducing coordinate caching: now the program, the first time it encounters a city, requests and saves the coordinates. If present, they will be used in the future without making a new request via geopy. This is both lighter on their servers and faster and safer for us.&lt;/p&gt;
    &lt;head rend="h2"&gt;Infrastructure Details&lt;/head&gt;
    &lt;p&gt;And the VPS? It has no problems and is surprisingly fast and effective. FreeBSD 14.3-RELEASE, BastilleBSD to manage the jails. Currently, there are 39 jails - one for haproxy, the FediMeteo website, so nginx, and the snac instance for FediMeteo announcements and support - the other 38 for the individual instances. Each of them, therefore, has its autonomous ZFS dataset. Every 15 minutes, there is a local snapshot of all datasets. Every hour, the homepage is regenerated: a small script calculates the number of followers (counting, instance by instance, the followers of individual cities, since I don't publish except in aggregate to avoid possible triangulations and privacy leaks of users). Every hour, moreover, an external backup is made via zfs-autobackup (on encrypted at rest dataset), and once a day, a further backup is made in my datacenter, on disks encrypted with geli. The occupied RAM is 501 MB (yes, exactly: 501 MB), which rises slightly when updates are in progress. Updates normally occur every 6 hours. I have tried, as much as possible, to space them out to avoid overloads in timelines (or on the server itself). Only for the USA, I added a sleep of 5 seconds between one city and another, to give snac the opportunity to better organize the sending of messages. It probably wouldn't be necessary, with the current numbers, but better safe than sorry. In this way, the USA is processed in about 2 and a half hours, but the other jails (thus countries) can work autonomously and send their updates.&lt;/p&gt;
    &lt;p&gt;The average load of the VPS (taking as reference both the last 24 hours and the last two weeks) is about 25%, as it rises to 70/75% when updates occur for larger instances (such as the USA), or when it is announced by FediFollows. Otherwise, it is on average less than 10%. So, the VPS still has huge margin, and new instances, with new nations, will still be inside it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;This article, although in some parts very conversational, aims to demonstrate how it's possible to build solid, valid, and efficient solutions without the need to use expensive and complex services. Moreover, this is the demonstration of how it's possible to have your online presence without the need to put your data in the hands of third parties or without necessarily having to resort to complex stacks. Sometimes, less is more.&lt;/p&gt;
    &lt;p&gt;The success of this project demonstrates, once again, that my grandfather was right: weather forecasts interest everyone. He worried about my health and, thanks to his concerns, we spent time together. In the same way, I see many followers and friends talking to me or among themselves about the weather, their experiences, what happens. Again, in my life, weather forecasts have helped sociality and socialization.&lt;/p&gt;
    &lt;p&gt;Thank you, Grandpa.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46436889</guid><pubDate>Tue, 30 Dec 2025 19:21:48 +0000</pubDate></item><item><title>Zpdf: PDF text extraction in Zig – 5x faster than MuPDF</title><link>https://github.com/Lulzx/zpdf</link><description>&lt;doc fingerprint="e6469261be5c004a"&gt;
  &lt;main&gt;
    &lt;p&gt;A PDF text extraction library written in Zig.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Memory-mapped file reading for efficient large file handling&lt;/item&gt;
      &lt;item&gt;Streaming text extraction with efficient arena allocation&lt;/item&gt;
      &lt;item&gt;Multiple decompression filters: FlateDecode, ASCII85, ASCIIHex, LZW, RunLength&lt;/item&gt;
      &lt;item&gt;Font encoding support: WinAnsi, MacRoman, ToUnicode CMap&lt;/item&gt;
      &lt;item&gt;XRef table and stream parsing (PDF 1.5+)&lt;/item&gt;
      &lt;item&gt;Configurable error handling (strict or permissive)&lt;/item&gt;
      &lt;item&gt;Multi-threaded parallel page extraction&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Text extraction performance vs MuPDF 1.26 (&lt;code&gt;mutool convert -F text&lt;/code&gt;) on Apple M4 Pro:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Document&lt;/cell&gt;
        &lt;cell role="head"&gt;Pages&lt;/cell&gt;
        &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;cell role="head"&gt;zpdf&lt;/cell&gt;
        &lt;cell role="head"&gt;MuPDF&lt;/cell&gt;
        &lt;cell role="head"&gt;Speedup&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;C++ Standard Draft&lt;/cell&gt;
        &lt;cell&gt;2,134&lt;/cell&gt;
        &lt;cell&gt;8 MB&lt;/cell&gt;
        &lt;cell&gt;250 ms&lt;/cell&gt;
        &lt;cell&gt;968 ms&lt;/cell&gt;
        &lt;cell&gt;3.9x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Pandas Documentation&lt;/cell&gt;
        &lt;cell&gt;3,743&lt;/cell&gt;
        &lt;cell&gt;15 MB&lt;/cell&gt;
        &lt;cell&gt;395 ms&lt;/cell&gt;
        &lt;cell&gt;1,112 ms&lt;/cell&gt;
        &lt;cell&gt;2.8x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Intel SDM&lt;/cell&gt;
        &lt;cell&gt;5,252&lt;/cell&gt;
        &lt;cell&gt;25 MB&lt;/cell&gt;
        &lt;cell&gt;451 ms&lt;/cell&gt;
        &lt;cell&gt;2,099 ms&lt;/cell&gt;
        &lt;cell&gt;4.7x&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Document&lt;/cell&gt;
        &lt;cell role="head"&gt;Pages&lt;/cell&gt;
        &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;cell role="head"&gt;zpdf&lt;/cell&gt;
        &lt;cell role="head"&gt;MuPDF&lt;/cell&gt;
        &lt;cell role="head"&gt;Speedup&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;C++ Standard Draft&lt;/cell&gt;
        &lt;cell&gt;2,134&lt;/cell&gt;
        &lt;cell&gt;8 MB&lt;/cell&gt;
        &lt;cell&gt;131 ms&lt;/cell&gt;
        &lt;cell&gt;966 ms&lt;/cell&gt;
        &lt;cell&gt;7.4x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Pandas Documentation&lt;/cell&gt;
        &lt;cell&gt;3,743&lt;/cell&gt;
        &lt;cell&gt;15 MB&lt;/cell&gt;
        &lt;cell&gt;218 ms&lt;/cell&gt;
        &lt;cell&gt;1,117 ms&lt;/cell&gt;
        &lt;cell&gt;5.1x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Intel SDM&lt;/cell&gt;
        &lt;cell&gt;5,252&lt;/cell&gt;
        &lt;cell&gt;25 MB&lt;/cell&gt;
        &lt;cell&gt;117 ms&lt;/cell&gt;
        &lt;cell&gt;2,098 ms&lt;/cell&gt;
        &lt;cell&gt;17.9x&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Peak throughput: 45,000 pages/sec (Intel SDM, parallel)&lt;/p&gt;
    &lt;p&gt;Build with &lt;code&gt;zig build -Doptimize=ReleaseFast&lt;/code&gt; for these results.&lt;/p&gt;
    &lt;p&gt;zpdf uses SIMD-accelerated routines for hot paths:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Whitespace skipping (content streams are whitespace-heavy)&lt;/item&gt;
      &lt;item&gt;Delimiter detection (tokenization)&lt;/item&gt;
      &lt;item&gt;Keyword search (&lt;code&gt;stream&lt;/code&gt;,&lt;code&gt;endstream&lt;/code&gt;,&lt;code&gt;startxref&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;String boundary scanning&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Auto-detects: NEON (ARM64), AVX2/SSE4.2 (x86_64), or scalar fallback.&lt;/p&gt;
    &lt;p&gt;Note: MuPDF's threading (&lt;code&gt;-T&lt;/code&gt; flag) is for rendering/rasterization only. Text extraction via &lt;code&gt;mutool convert -F text&lt;/code&gt; is single-threaded by design. zpdf parallelizes text extraction across pages.&lt;/p&gt;
    &lt;p&gt;Text extraction accuracy vs MuPDF (reference) on US Constitution (85 pages):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Tool&lt;/cell&gt;
        &lt;cell role="head"&gt;Char Accuracy&lt;/cell&gt;
        &lt;cell role="head"&gt;WER&lt;/cell&gt;
        &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;cell role="head"&gt;vs MuPDF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;zpdf&lt;/cell&gt;
        &lt;cell&gt;99.6%&lt;/cell&gt;
        &lt;cell&gt;2.1%&lt;/cell&gt;
        &lt;cell&gt;2 ms&lt;/cell&gt;
        &lt;cell&gt;24x faster&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MuPDF&lt;/cell&gt;
        &lt;cell&gt;100%&lt;/cell&gt;
        &lt;cell&gt;0%&lt;/cell&gt;
        &lt;cell&gt;54 ms&lt;/cell&gt;
        &lt;cell&gt;1x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Tika&lt;/cell&gt;
        &lt;cell&gt;97.4%&lt;/cell&gt;
        &lt;cell&gt;10.6%&lt;/cell&gt;
        &lt;cell&gt;1,307 ms&lt;/cell&gt;
        &lt;cell&gt;24x slower&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;pdftotext&lt;/cell&gt;
        &lt;cell&gt;57.0%&lt;/cell&gt;
        &lt;cell&gt;19.8%&lt;/cell&gt;
        &lt;cell&gt;90 ms&lt;/cell&gt;
        &lt;cell&gt;1.7x slower&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Char Accuracy: Sequence similarity vs MuPDF baseline (higher = better)&lt;/item&gt;
      &lt;item&gt;WER: Word Error Rate vs MuPDF baseline (lower = better)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MuPDF is the accuracy baseline (100%). zpdf is 650x faster than Tika with better accuracy.&lt;/p&gt;
    &lt;p&gt;Run &lt;code&gt;PYTHONPATH=python python benchmark/accuracy.py&lt;/code&gt; to reproduce.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zig 0.15.2 or later&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;zig build              # Build library and CLI
zig build test         # Run tests&lt;/code&gt;
    &lt;code&gt;const zpdf = @import("zpdf");

pub fn main() !void {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    const doc = try zpdf.Document.open(allocator, "file.pdf");
    defer doc.close();

    var buf: [4096]u8 = undefined;
    var writer = std.fs.File.stdout().writer(&amp;amp;buf);
    defer writer.interface.flush() catch {};

    for (0..doc.pages.items.len) |page_num| {
        try doc.extractText(page_num, &amp;amp;writer.interface);
    }
}&lt;/code&gt;
    &lt;code&gt;zpdf extract document.pdf           # Extract all pages to stdout
zpdf extract -p 1-10 document.pdf   # Extract pages 1-10
zpdf extract -o out.txt document.pdf # Output to file
zpdf info document.pdf              # Show document info
zpdf bench document.pdf             # Run benchmark&lt;/code&gt;
    &lt;code&gt;import zpdf

with zpdf.Document("file.pdf") as doc:
    print(doc.page_count)

    # Single page
    text = doc.extract_page(0)

    # All pages (parallel by default)
    all_text = doc.extract_all()

    # Page info
    info = doc.get_page_info(0)
    print(f"{info.width}x{info.height}")&lt;/code&gt;
    &lt;p&gt;Build the shared library first:&lt;/p&gt;
    &lt;code&gt;zig build -Doptimize=ReleaseFast
PYTHONPATH=python python3 examples/basic.py&lt;/code&gt;
    &lt;code&gt;src/
├── root.zig         # Document API and core types
├── capi.zig         # C ABI exports for FFI
├── parser.zig       # PDF object parser
├── xref.zig         # XRef table/stream parsing
├── pagetree.zig     # Page tree resolution
├── decompress.zig   # Stream decompression filters
├── encoding.zig     # Font encoding and CMap parsing
├── interpreter.zig  # Content stream interpreter
├── simd.zig         # SIMD string operations
└── main.zig         # CLI

python/zpdf/         # Python bindings (cffi)
examples/            # Usage examples
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Feature&lt;/cell&gt;
        &lt;cell role="head"&gt;zpdf&lt;/cell&gt;
        &lt;cell role="head"&gt;MuPDF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Text Extraction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Reading order / layout analysis&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Two-column detection&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Paragraph grouping&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Word/line bounding boxes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Font Support&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WinAnsi/MacRoman&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;ToUnicode CMap&lt;/cell&gt;
        &lt;cell&gt;Partial*&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CID fonts (Type0)&lt;/cell&gt;
        &lt;cell&gt;Partial*&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Embedded fonts&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Compression&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;FlateDecode, LZW, ASCII85/Hex&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;JBIG2, JPEG2000&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;PDF Features&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Incremental updates&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Encrypted PDFs&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Forms / Annotations&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Rendering&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;*ToUnicode/CID fonts: Works when CMap is embedded directly. References to compressed object streams not yet supported (affects some Greek, Chinese, Japanese, Korean PDFs).&lt;/p&gt;
    &lt;p&gt;Use zpdf when: Speed matters, simple layouts, batch processing raw text.&lt;/p&gt;
    &lt;p&gt;Use MuPDF when: Complex layouts, encrypted PDFs, non-Latin scripts.&lt;/p&gt;
    &lt;p&gt;MIT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46437288</guid><pubDate>Tue, 30 Dec 2025 19:57:10 +0000</pubDate></item><item><title>Everything as code: How we manage our company in one monorepo</title><link>https://www.kasava.dev/blog/everything-as-code-monorepo</link><description>&lt;doc fingerprint="b79cf4185d653cf8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Everything as Code: How We Manage Our Company In One Monorepo&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Last week, I updated our pricing limits. One JSON file. The backend started enforcing the new caps, the frontend displayed them correctly, the marketing site showed them on the pricing page, and our docs reflected the change—all from a single commit.&lt;/p&gt;
    &lt;p&gt;No sync issues. No "wait, which repo has the current pricing?" No deploy coordination across three teams. Just one change, everywhere, instantly.&lt;/p&gt;
    &lt;p&gt;At Kasava, our entire platform lives in a single repository. Not just the code—everything:&lt;/p&gt;
    &lt;code&gt;kasava/                              # 5,470+ files TypeScript files
├── frontend/                       # Next.js 16 + React 19 application
│   └── src/
│       ├── app/                   # 25+ route directories
│       └── components/            # 45+ component directories
├── backend/                        # Cloudflare Workers API
│   └── src/
│       ├── services/              # 55+ business logic services
│       └── workflows/             # Mastra AI workflows
├── website/                        # Marketing site (kasava.ai)
├── docs/                           # Public documentation (Mintlify)
├── docs-internal/                  # 12+ architecture docs &amp;amp; specs
├── marketing/
│   ├── blogs/                     # Blog pipeline (drafts → review → published)
│   ├── investor-deck/             # Next.js site showing investment proposal
│   └── email/                     # MJML templates for Loops.so campaigns
├── external/
│   ├── chrome-extension/          # WXT + React bug capture tool
│   ├── google-docs-addon/         # @helper AI assistant (Apps Script)
│   └── google-cloud-functions/
│       ├── tree-sitter-service/   # AST parsing for 10+ languages
│       └── mobbin-research-service/
├── scripts/                        # Deployment &amp;amp; integration testing
├── infra-tester/                   # Integration test harness
└── github-simulator/               # Mock GitHub API for local dev

&lt;/code&gt;
    &lt;head rend="h2"&gt;Why This Matters: AI-Native Development&lt;/head&gt;
    &lt;p&gt;This isn't about abstract philosophies on design patterns for 'how we should work.' It's about velocity in an era where products change fast and context matters.&lt;/p&gt;
    &lt;p&gt;AI is all about context. And this monorepo is our company—not just the product.&lt;/p&gt;
    &lt;p&gt;When our AI tools help us write documentation, they have immediate access to the actual code being documented. When we update our marketing website, the AI can verify claims against the real implementation. When we write blog posts like this one, the AI can fact-check every code example, every number, every architectural claim against the source of truth.&lt;/p&gt;
    &lt;p&gt;This means we move faster:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Documentation updates faster because the AI sees code changes and suggests doc updates in the same context&lt;/item&gt;
      &lt;item&gt;Website updates faster because pricing, features, and capabilities are pulled from the same config files that power the app&lt;/item&gt;
      &lt;item&gt;Blog posts ship faster because the AI can run self-referential checks—validating that our "5,470+ TypeScript files" claim is accurate by actually counting them&lt;/item&gt;
      &lt;item&gt;Nothing goes out of sync because there's only one source of truth, and AI has access to all of it&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When you ask Claude to "update the pricing page to reflect the new limits," it can:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Read the backend service that enforces limits&lt;/item&gt;
      &lt;item&gt;Check the frontend that displays them&lt;/item&gt;
      &lt;item&gt;Update the marketing site&lt;/item&gt;
      &lt;item&gt;Verify the docs are consistent&lt;/item&gt;
      &lt;item&gt;Flag any blog posts that might mention outdated numbers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All in one conversation. All in one repository.&lt;/p&gt;
    &lt;p&gt;This is what "AI-native development" actually means: structuring your work so AI can be maximally helpful, not fighting against fragmentation.&lt;/p&gt;
    &lt;p&gt;And it reinforces a shipping culture.&lt;/p&gt;
    &lt;p&gt;Everything-as-code means everything ships the same way: &lt;code&gt;git push&lt;/code&gt;. Want to update the website pricing page? &lt;code&gt;git push&lt;/code&gt;. New blog post ready to go live? &lt;code&gt;git push&lt;/code&gt;. Fix a typo in the docs? &lt;code&gt;git push&lt;/code&gt;. Deploy a backend feature? &lt;code&gt;git push&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;No separate CMSs to log into. No WordPress admin panels. No waiting for marketing tools to sync. No "can someone with Contentful access update this?" The same Git workflow that ships code also ships content, documentation, and marketing. Everyone on the team can ship anything, and it all goes through the same review process, the same CI/CD, the same audit trail.&lt;/p&gt;
    &lt;p&gt;This uniformity removes friction and removes excuses. Shipping becomes muscle memory.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Everything in One Repo?&lt;/head&gt;
    &lt;head rend="h3"&gt;1. Atomic Changes Across Boundaries (That AI Can Understand)&lt;/head&gt;
    &lt;p&gt;When a backend API changes, the frontend type definitions update in the same commit. When we add a new feature, the documentation can ship alongside it. No version mismatches. No "which version of the API does this frontend need?"&lt;/p&gt;
    &lt;p&gt;AI can see and validate the entire change in context.&lt;/p&gt;
    &lt;p&gt;When we ask Claude to add a feature, it doesn't just write backend code. It sees the frontend that will consume it, the docs that need updating, and the marketing site that might reference it. All in one view. All in one conversation.&lt;/p&gt;
    &lt;p&gt;Real example from our codebase—adding Asana integration:&lt;/p&gt;
    &lt;code&gt;commit: "feat: add Asana integration"
├── backend/src/services/AsanaService.ts
├── backend/src/routes/api/integrations/asana.ts
├── frontend/src/components/integrations/asana/
├── frontend/src/app/integrations/asana/
├── docs/integrations/asana.mdx
└── website/src/app/integrations/page.tsx
&lt;/code&gt;
    &lt;p&gt;One PR. One review. One merge. Everything ships together.&lt;/p&gt;
    &lt;p&gt;Another example—keeping pricing in sync:&lt;/p&gt;
    &lt;p&gt;We have a single &lt;code&gt;billing-plans.json&lt;/code&gt; that defines all plan limits and features:&lt;/p&gt;
    &lt;code&gt;// frontend/src/config/billing-plans.json (also copied to website/src/config/)
{
  "plans": {
    "free": { "limits": { "repositories": 1, "aiChatMessagesPerDay": 10 } },
    "starter": {
      "limits": { "repositories": 10, "aiChatMessagesPerDay": 100 }
    },
    "professional": {
      "limits": { "repositories": 50, "aiChatMessagesPerDay": 1000 }
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;The backend enforces these limits. The frontend displays them in settings. The marketing website shows them on the pricing page. When we change a limit, one JSON update propagates everywhere—no "the website says 50 repos but the app shows 25" bugs.&lt;/p&gt;
    &lt;p&gt;And AI validates all of it. When we update &lt;code&gt;billing-plans.json&lt;/code&gt;, we can ask Claude to verify that the backend, frontend, and website are all consistent. It reads all three implementations and confirms they match—or tells us what needs fixing.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Cross-Project Refactoring&lt;/head&gt;
    &lt;p&gt;Renaming a function? Your IDE finds all usages across frontend, backend, docs examples, and blog code snippets. One find-and-replace. One commit.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. Single Source of Truth&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dependencies: Shared tooling configured once&lt;/item&gt;
      &lt;item&gt;CI/CD: One pipeline to understand&lt;/item&gt;
      &lt;item&gt;Search: Find anything with one &lt;code&gt;grep&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The Structure: What Lives Where&lt;/head&gt;
    &lt;head rend="h3"&gt;Core Application&lt;/head&gt;
    &lt;code&gt;frontend/                        # Customer-facing Next.js app
├── src/
│   ├── app/                    # Next.js 15 App Router
│   │   ├── analytics/         # Semantic commit analysis
│   │   ├── bug-reports/       # AI-powered bug tracking
│   │   ├── chat/              # AI assistant interface
│   │   ├── code-search/       # Semantic code search
│   │   ├── dashboard/         # Main dashboard
│   │   ├── google-docs-assistant/
│   │   ├── integrations/      # GitHub, Linear, Jira, Asana
│   │   ├── prd/               # PRD management
│   │   └── ...                # 25+ route directories total
│   ├── components/            # 45+ component directories
│   │   ├── ai-elements/      # AI-specific UI
│   │   ├── bug-reports/      # Bug tracking UI
│   │   ├── dashboard/        # Dashboard widgets
│   │   ├── google-docs/      # Google Docs integration
│   │   ├── onboarding/       # User onboarding flow
│   │   └── ui/               # shadcn/ui base components
│   ├── mastra/               # Frontend Mastra integration
│   └── lib/                  # SDK, utilities, hooks

backend/                        # Cloudflare Workers API
├── src/
│   ├── routes/               # Hono API endpoints
│   ├── services/             # 55+ business logic services
│   ├── workflows/            # Mastra AI workflows
│   │   ├── steps/           # Reusable workflow steps
│   │   └── RepositoryIndexingWorkflow.ts
│   ├── db/                   # Drizzle ORM schema
│   ├── durable-objects/      # Stateful edge computing
│   ├── workers/              # Queue consumers
│   └── mastra/               # AI agents and tools
&lt;/code&gt;
    &lt;p&gt;These two talk to each other constantly. Having them in the same repo means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;API changes include frontend updates&lt;/item&gt;
      &lt;item&gt;Type safety across the boundary&lt;/item&gt;
      &lt;item&gt;Shared testing utilities&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Marketing Properties&lt;/head&gt;
    &lt;code&gt;website/                        # kasava.ai marketing site
├── src/
│   ├── app/                   # Landing pages, blog
│   ├── components/            # Shared marketing components
│   └── lib/                   # Utilities

marketing/
├── blogs/
│   ├── queue/
│   │   └── drafts/           # Ideas and drafts
│   ├── review/               # Ready for editing
│   └── published/            # Live on the site
├── investor-deck/            # Next.js presentation (not PowerPoint!)
└── email/
    ├── CLAUDE.md             # Email writing guidelines
    └── mjml/                 # 7+ email campaign loops
        ├── loop-1-welcome/
        ├── loop-2-github-connected/
        ├── loop-3-trial-conversion/
        └── ...
&lt;/code&gt;
    &lt;p&gt;Yes, even blog posts are code. They're Markdown files with frontmatter, versioned in Git, reviewed in PRs. Email templates are MJML that version controls our entire customer communication system.&lt;/p&gt;
    &lt;p&gt;Even our investor deck is code — a Next.js 16 static site with 17 React slide components, keyboard navigation, and PDF export. No PowerPoint, no Google Slides. When we update metrics or messaging, it's a code change with full Git history, reviewed in a PR, and deployed with &lt;code&gt;git push&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Why this matters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Marketing can update copy without engineering&lt;/item&gt;
      &lt;item&gt;Changes are reviewed and tracked&lt;/item&gt;
      &lt;item&gt;Rollback is one &lt;code&gt;git revert&lt;/code&gt;away&lt;/item&gt;
      &lt;item&gt;Email campaigns are testable and diffable&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Documentation&lt;/head&gt;
    &lt;code&gt;docs/                           # Public docs (Mintlify)
├── index.mdx                  # Landing page
├── quickstart.mdx             # Getting started
├── demo-mode.mdx              # Demo mode guide
├── features/                  # Product features
│   ├── ai-chat.mdx
│   ├── code-intelligence.mdx
│   ├── code-search.mdx
│   └── prds.mdx
├── integrations/              # Integration guides
│   ├── github.mdx
│   ├── linear.mdx
│   ├── jira.mdx
│   └── asana.mdx
└── bug-tracking/              # Bug tracking docs

docs-internal/                  # Engineering knowledge base
├── GITHUB_CHAT_ARCHITECTURE.md
├── QUEUE_ARCHITECTURE_SUMMARY.md
├── UNIFIED_TASK_ANALYTICS_QUEUE.md
├── features/                  # Feature specs
├── migrations/                # Migration guides
├── plans/                     # Implementation plans
└── research/                  # Research notes
&lt;/code&gt;
    &lt;p&gt;Public docs deploy automatically when we push. Internal docs are searchable alongside code—when someone asks "how does the queue work?", they find the actual architecture document, not a stale wiki page.&lt;/p&gt;
    &lt;head rend="h3"&gt;External Services&lt;/head&gt;
    &lt;code&gt;external/
├── chrome-extension/          # WXT-based bug capture tool
│   ├── entrypoints/          # popup, content scripts, background
│   ├── lib/                  # Screen capture, console logging
│   ├── components/           # React UI components
│   └── wxt.config.ts         # WXT configuration
│
├── google-docs-addon/        # @helper mentions in Docs
│   ├── Code.gs              # Main Apps Script (18KB)
│   ├── Sidebar.html         # React-like UI (26KB)
│   ├── Settings.html        # Configuration UI
│   └── appsscript.json      # Manifest
│
└── google-cloud-functions/
    ├── tree-sitter-service/  # AST parsing
    │   └── Supports: JS, TS, Python, Go, Rust,
    │       Java, C, C++, Ruby, PHP, C#
    └── mobbin-research-service/  # UX research
&lt;/code&gt;
    &lt;p&gt;These deploy to completely different platforms (Chrome Web Store, Google Apps Script, GCP) but live together because:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;They share API contracts with the main app&lt;/item&gt;
      &lt;item&gt;Changes often span boundaries&lt;/item&gt;
      &lt;item&gt;One team maintains everything&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Development Infrastructure&lt;/head&gt;
    &lt;code&gt;github-simulator/              # Mock GitHub API for local dev
infra-tester/                  # Integration test harness
scripts/
├── google-cloud/             # GCP deployment scripts
├── test-credentials.ts       # Credential testing
└── test-webhook-integration.ts
&lt;/code&gt;
    &lt;p&gt;Local development shouldn't require external services. Mock servers live with the code they simulate.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Deploys Where&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Component&lt;/cell&gt;
        &lt;cell role="head"&gt;Tech Stack&lt;/cell&gt;
        &lt;cell role="head"&gt;Deploys To&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Frontend&lt;/cell&gt;
        &lt;cell&gt;Next.js 15, React 19, Tailwind v4&lt;/cell&gt;
        &lt;cell&gt;Vercel&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Backend&lt;/cell&gt;
        &lt;cell&gt;Cloudflare Workers, Hono, Mastra&lt;/cell&gt;
        &lt;cell&gt;Cloudflare&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Website&lt;/cell&gt;
        &lt;cell&gt;Next.js, custom components&lt;/cell&gt;
        &lt;cell&gt;Vercel&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Investor Deck&lt;/cell&gt;
        &lt;cell&gt;Next.js, custom components&lt;/cell&gt;
        &lt;cell&gt;Vercel&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Docs&lt;/cell&gt;
        &lt;cell&gt;Mintlify MDX&lt;/cell&gt;
        &lt;cell&gt;Mintlify&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Chrome Extension&lt;/cell&gt;
        &lt;cell&gt;WXT, React, Tailwind&lt;/cell&gt;
        &lt;cell&gt;Chrome Web Store&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Google Docs Add-on&lt;/cell&gt;
        &lt;cell&gt;Apps Script, HTML&lt;/cell&gt;
        &lt;cell&gt;Google Workspace Marketplace&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Tree-sitter Service&lt;/cell&gt;
        &lt;cell&gt;Node.js, GCP Functions&lt;/cell&gt;
        &lt;cell&gt;Google Cloud&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Email Templates&lt;/cell&gt;
        &lt;cell&gt;MJML&lt;/cell&gt;
        &lt;cell&gt;Loops.so&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;How We Make It Work&lt;/head&gt;
    &lt;head rend="h3"&gt;No Workspaces (And That's Fine)&lt;/head&gt;
    &lt;p&gt;We deliberately don't use npm/yarn workspaces. (Well, we do in one specific use case but that's for another post.) Each directory is its own independent npm project:&lt;/p&gt;
    &lt;code&gt;cd frontend &amp;amp;&amp;amp; npm install    # Frontend dependencies
cd backend &amp;amp;&amp;amp; npm install     # Backend dependencies
cd external/chrome-extension &amp;amp;&amp;amp; npm install  # Extension dependencies
&lt;/code&gt;
    &lt;p&gt;Why? Simplicity. No hoisting confusion. No "which version of React am I actually getting?" Each project is isolated and predictable.&lt;/p&gt;
    &lt;head rend="h3"&gt;Selective CI/CD&lt;/head&gt;
    &lt;p&gt;We run 5 GitHub Actions workflows, each triggered by specific paths:&lt;/p&gt;
    &lt;code&gt;# .github/workflows/frontend-tests.yml
name: Frontend Tests
on:
  push:
    paths:
      - "frontend/**"
      - ".github/workflows/frontend-tests.yml"
# Runs: type-check, lint, demo data validation, tests with coverage
&lt;/code&gt;
    &lt;code&gt;# .github/workflows/backend-tests.yml
name: Backend Tests
on:
  push:
    paths:
      - "backend/**"
      - ".github/workflows/backend-tests.yml"
# Runs: unit tests, integration tests, e2e tests
&lt;/code&gt;
    &lt;code&gt;# .github/workflows/tree-sitter-tests.yml
name: Tree-sitter Tests
on:
  push:
    paths:
      - "external/google-cloud-functions/tree-sitter-service/**"
# Runs: parsing tests for all 10+ supported languages
&lt;/code&gt;
    &lt;p&gt;Change the Chrome extension? Only relevant tests run. Update the backend? Backend tests plus any integration tests that depend on it.&lt;/p&gt;
    &lt;head rend="h3"&gt;The CLAUDE.md Convention&lt;/head&gt;
    &lt;p&gt;Every major directory has a CLAUDE.md file that documents:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What this code does&lt;/item&gt;
      &lt;item&gt;Tech stack and versions&lt;/item&gt;
      &lt;item&gt;Quick start commands&lt;/item&gt;
      &lt;item&gt;Architecture decisions&lt;/item&gt;
      &lt;item&gt;Common patterns&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;CLAUDE.md                          # Root-level overview
├── frontend/CLAUDE.md            # Next.js 15, React 19, Tailwind v4
├── backend/CLAUDE.md             # Cloudflare Workers, Hono, Mastra
├── external/chrome-extension/CLAUDE.md
├── external/google-cloud-functions/CLAUDE.md
└── marketing/email/CLAUDE.md     # MJML email guidelines
&lt;/code&gt;
    &lt;p&gt;This isn't just for humans—AI coding assistants read these files. When Claude Code works on our frontend, it reads &lt;code&gt;frontend/CLAUDE.md&lt;/code&gt; and knows we're using Next.js 15 with React 19, npm (not pnpm), and specific patterns.&lt;/p&gt;
    &lt;head rend="h3"&gt;Consistent Tooling&lt;/head&gt;
    &lt;p&gt;One configuration, everywhere:&lt;/p&gt;
    &lt;code&gt;.prettierrc              # Formatting (all JS/TS)
.eslintrc               # Linting (shared rules)
tsconfig.json           # TypeScript base config
&lt;/code&gt;
    &lt;p&gt;New developer? &lt;code&gt;npm install&lt;/code&gt; in the directory you're working on. Everything works.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Challenges (And How We Handle Them)&lt;/head&gt;
    &lt;head rend="h3"&gt;Challenge: Repository Size&lt;/head&gt;
    &lt;p&gt;Why it's not a problem (yet):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Clone time: ~20 seconds&lt;/item&gt;
      &lt;item&gt;Git operations: still snappy&lt;/item&gt;
      &lt;item&gt;We haven't needed sparse checkout, LFS, or shallow clones&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When we might need to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Large binary assets would go to R2/S3, not git&lt;/item&gt;
      &lt;item&gt;If we hit 1GB+, we'd look at shallow clones for CI&lt;/item&gt;
      &lt;item&gt;Truly independent services could be extracted&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Challenge: Build Times&lt;/head&gt;
    &lt;p&gt;Problem: If everything is connected, does everything rebuild?&lt;/p&gt;
    &lt;p&gt;Reality: No. Each project builds independently:&lt;/p&gt;
    &lt;code&gt;# Frontend build (only rebuilds frontend)
cd frontend &amp;amp;&amp;amp; npm run build

# Backend build (only rebuilds backend)
cd backend &amp;amp;&amp;amp; npm run build

# Extension build (only rebuilds extension)
cd external/chrome-extension &amp;amp;&amp;amp; npm run build
&lt;/code&gt;
    &lt;p&gt;We use Turbopack for frontend dev (fast HMR), Wrangler for backend dev (fast reload), and WXT for extension dev (fast rebuild).&lt;/p&gt;
    &lt;head rend="h3"&gt;Challenge: Permission Boundaries&lt;/head&gt;
    &lt;p&gt;Problem: Not everyone should see everything.&lt;/p&gt;
    &lt;p&gt;Our situation: We're a small team. Everyone can see everything. That's a feature, not a bug—it enables cross-pollination.&lt;/p&gt;
    &lt;p&gt;If we grew and needed boundaries:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GitHub CODEOWNERS for review requirements&lt;/item&gt;
      &lt;item&gt;Branch protection rules&lt;/item&gt;
      &lt;item&gt;Potentially split truly sensitive codebases (but we'd resist this)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Challenge: Context Switching&lt;/head&gt;
    &lt;p&gt;Problem: Jumping between TypeScript (frontend), TypeScript (backend), Apps Script (Google add-on), and MJML (emails) feels disorienting.&lt;/p&gt;
    &lt;p&gt;Solutions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Consistent patterns across projects (same linting, same formatting)&lt;/item&gt;
      &lt;item&gt;CLAUDE.md files explain context immediately&lt;/item&gt;
      &lt;item&gt;IDE workspace configurations&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Our monorepo isn't about following a trend. It's about removing friction between things that naturally belong together, something that is critical when related context is everything.&lt;/p&gt;
    &lt;p&gt;When a feature touches the backend API, the frontend component, the documentation, and the marketing site—why should that be four repositories, four PRs, four merge coordination meetings?&lt;/p&gt;
    &lt;p&gt;The monorepo isn't a constraint. It's a force multiplier.&lt;/p&gt;
    &lt;p&gt;Kasava is built as a unified platform. See what we've built&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46437381</guid><pubDate>Tue, 30 Dec 2025 20:05:42 +0000</pubDate></item><item><title>Professional software developers don't vibe, they control</title><link>https://arxiv.org/abs/2512.14012</link><description>&lt;doc fingerprint="8ddd161bc8db90cf"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Software Engineering&lt;/head&gt;&lt;p&gt; [Submitted on 16 Dec 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Professional Software Developers Don't Vibe, They Control: AI Agent Use for Coding in 2025&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:The rise of AI agents is transforming how software can be built. The promise of agents is that developers might write code quicker, delegate multiple tasks to different agents, and even write a full piece of software purely out of natural language. In reality, what roles agents play in professional software development remains in question. This paper investigates how experienced developers use agents in building software, including their motivations, strategies, task suitability, and sentiments. Through field observations (N=13) and qualitative surveys (N=99), we find that while experienced developers value agents as a productivity boost, they retain their agency in software design and implementation out of insistence on fundamental software quality attributes, employing strategies for controlling agent behavior leveraging their expertise. In addition, experienced developers feel overall positive about incorporating agents into software development given their confidence in complementing the agents' limitations. Our results shed light on the value of software development best practices in effective use of agents, suggest the kinds of tasks for which agents may be suitable, and point towards future opportunities for better agentic interfaces and agentic use guidelines.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.SE&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46437391</guid><pubDate>Tue, 30 Dec 2025 20:06:46 +0000</pubDate></item><item><title>Sabotaging Bitcoin</title><link>https://blog.dshr.org/2025/12/sabotaging-bitcoin.html</link><description>&lt;doc fingerprint="37e79084a8231e31"&gt;
  &lt;main&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Source&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;In 2024 Soroush Farokhnia &amp;amp; Amir Kafshdar Goharshady published Options and Futures Imperil Bitcoin's Security and:&lt;/p&gt;&lt;quote&gt;showed that (i) a successful block-reverting attack does not necessarily require ... a majority of the hash power; (ii) obtaining a majority of the hash power ... costs roughly 6.77 billion ... and (iii) Bitcoin derivatives, i.e. options and futures, imperil Bitcoin’s security by creating an incentive for a block-reverting/majority attack.&lt;/quote&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Source&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;quote&gt;90% of transaction volume on the Bitcoin blockchain is not tied to economically meaningful activities but is the byproduct of the Bitcoin protocol design as well as the preference of many participants for anonymity ... exchanges play a central role in the Bitcoin system. They explain 75% of real Bitcoin volume.Of course, just because they aren't "economically meaningful" doesn't mean they aren't worth attacking! The average block has ~3.2K transactions, so ~$121.6M/block. As a check. $121.6M * 144 block/day = $17.5B. So to recover their cost for a 51% attack would require double-spending about 8 hours worth of transactions.&lt;/quote&gt;&lt;p&gt;I agree with their technical analysis of the attack, but I believe there would be significant difficulties in putting it into practice. Below the fold I try to set out these difficulties.&lt;/p&gt;&lt;p&gt; brevity is for the weak&lt;lb/&gt;Maciej Cegłowski&lt;/p&gt;&lt;p&gt;Maciej Cegłowski&lt;/p&gt;&lt;p&gt;First, I should point out that I wrote about using derivatives to profit from manipulating Bitcoin's price more than three years ago in Pump-and-Dump Schemes. These schemes have a long history in cryptocurrencies, but they are not the attack involved here. I don't claim expertise in derivatives trading, so it is possible my analysis is faulty. If so, please point out the problems in a comment.&lt;/p&gt;&lt;head rend="h3"&gt;The Attack&lt;/head&gt;Farokhnia &amp;amp; Goharshady build on the 2018 work of Ittay Eyal &amp;amp; Emin Gün Sirer in Majority is not enough: Bitcoin mining is vulnerable:&lt;quote&gt;The key idea behind this strategy, called Selfish Mining, is for a pool to keep its discovered blocks private, thereby intentionally forking the chain. The honest nodes continue to mine on the public chain, while the pool mines on its own private branch. If the pool discovers more blocks, it develops a longer lead on the public chain, and continues to keep these new blocks private. When the public branch approaches the pool's private branch in length, the selfish miners reveal blocks from their private chain to the public.In April 2024 Farokhnia &amp;amp; Goharshady observed that:&lt;lb/&gt;...&lt;lb/&gt;We further show that the Bitcoin mining protocol will never be safe against attacks by a selfish mining pool that commands more than 1/3 of the total mining power of the network. Such a pool will always be able to collect mining rewards that exceed its proportion of mining power, even if it loses every single block race in the network. The resulting bound of 2/3 for the fraction of Bitcoin mining power that needs to follow the honest protocol to ensure that the protocol remains resistant to being gamed is substantially lower than the 50% figure currently assumed, and difficult to achieve in practice.&lt;/quote&gt;&lt;quote&gt;Given that the rule of thumb followed by most practitioners is to wait for 6 confirmations, a fork that goes 6 levels deep can very likely diminish the public’s trust in Bitcoin and cause a crash in its market price. It is also widely accepted that a prolonged majority attack (if it happens) would be catastrophic to the cryptocurrency and can cause its downfall.But, as they lay out, this possibility is discounted:&lt;/quote&gt;&lt;quote&gt;The conventional wisdom in the blockchain community is to assume that such block-reverting attacks are highly unlikely to happen. The reasoning goes as follows:&lt;item&gt;Reverting multiple blocks and specifically double-spending a transaction that has 6 confirmations requires control of a majority of the mining power;&lt;/item&gt;&lt;item&gt;Having a majority of the mining power is prohibitively expensive and requires an outlandish investment in hardware;&lt;/item&gt;&lt;item&gt;Even if a miner, mining pool or group of pools does control a majority of the mining power, they have no incentive to act dishonestly and revert the blockchain, as that would crash the price of Bitcoin, which is ultimately not in their favor, since they rely on mining rewards denominated in BTC for their income.&lt;/item&gt;&lt;/quote&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Source&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;These huge futures markets enable Farokhnia &amp;amp; Goharshady's attack:&lt;/p&gt;&lt;quote&gt;In short, an attacker can first use the Bitcoin derivatives market to short Bitcoin by purchasing a sufficient amount of put options or other equivalent financial instruments. She can then invest any of the amounts calculated above, depending on the timeline of the attack, to obtain the necessary hardware and hash power to perform the attack. If the attacker chooses to obtain a majority of the hash power, her success is guaranteed and she can revert the blocks as deeply as she wishes. However, she also has the option of a smaller upfront investment in hardware in exchange for longer wait times to achieve a high probability of success. In any case, as long as her earnings from shorting Bitcoin and then causing an intentional price crash outweighs her investments in hardware, there is a clear financial incentive to perform such an attack. The numbers above show that the annual trade volume in Bitcoin derivatives is more than three orders of magnitude larger than the required investment in hardware. Thus, it is possible and profitable to perform such an attack.&lt;/quote&gt;&lt;head rend="h3"&gt;Assumptions&lt;/head&gt;Farokhnia &amp;amp; Goharshady make some simplifying assumptions:&lt;quote&gt;&lt;list&gt;The justification for the first assumption is that it keeps our analysis sound, i.e. we can only over-approximate the cost by making this assumption. As for the second assumption, we note that electricity costs are often negligible in comparison to hardware costs and that our main argument, i.e. the vulnerability of Bitcoin to majority attacks and block-reverting attacks, remains intact even if the estimates we obtain here are doubled. Indeed, as we will soon see, the trade volume of Bitcoin derivatives is more than three orders of magnitude larger than the numbers obtained here.&lt;/list&gt;&lt;item&gt;We only consider the cost of hardware at the time of writing. We assume the attacker is buying the hardware, rather than renting it and do not consider potential discounts on bulk orders.&lt;/item&gt;&lt;item&gt;We ignore electricity costs as they vary widely based on location.&lt;/item&gt;&lt;/quote&gt;&lt;head rend="h3"&gt;Goal&lt;/head&gt;As Farokhnia &amp;amp; Goharshady stress, the success of a block-reverting attack is probabilistic, so the attacker needs to have a high enough probability of making a large enough profit to make up for the risk of failure.&lt;p&gt;My analysis thus assumes that the goal of the attacker is to have a 95% probability of earning at least double the cost of the attack.&lt;/p&gt;&lt;head rend="h3"&gt;Attacker&lt;/head&gt;There are two different kinds of attackers with different sets of difficulties:&lt;list rend="ul"&gt;&lt;item&gt;Outsiders: someone who has to acquire or rent sufficient hash power.&lt;/item&gt;&lt;item&gt;Insiders: someone or some mining pool who already controls sufficient hash power.&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;Obtaining and maintaining for the duration of the attack sufficient hash power without detection.&lt;/item&gt;&lt;item&gt;Obtaining and maintaining for the duration of the attack a sufficient short position in Bitcoin without detection.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Hash Power&lt;/head&gt;The outsider's problems are more complex than the insider's.&lt;head rend="h4"&gt;Outsider Attack&lt;/head&gt;The outsider attacker requires three kinds of resource:&lt;list rend="ul"&gt;&lt;item&gt;Mining rigs.&lt;/item&gt;&lt;item&gt;Power to run the rigs.&lt;/item&gt;&lt;item&gt;Data center space to hold the rigs.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h5"&gt;Mining rigs&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Could they acquire mining rigs sufficient to provide 30% of the combined insider and outsider hash power, or ~43% of the pre-attack hash power?&lt;/item&gt;&lt;item&gt;How long would it take to acquire the rigs?&lt;/item&gt;&lt;item&gt;Would their acquisition of the rigs be detected?&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Because the economic life of mining rigs is less than two years, the first part of Bitmain's production goes into maintaining the hash rate by replacing obsolete rigs. The second part goes into increasing the hash rate. If we assume that the outsider attacker could absorb the second part of Bitmain's production, how long would it take to get the necessary 43% of the previous hash power?&lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Source&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;The lack of rigs to increase the hash rate over a period of much less than two years would clearly be detectable.&lt;/p&gt;&lt;head rend="h5"&gt;Power&lt;/head&gt;The Cambridge Bitcoin Energy Consumption Index's current estimate is that the network consumes 22GW. The outside attacker would need 43% of this, or about 9.5GW, for the duration of the attack. For context, Meta's extraordinarily aggressive AI data center plans claim to bring a single 1GW data center online in 2026, and the first 2GW phase of their planned $27B 5GW Louisiana data center in 2030. The constraint on the roll-out is largely that lack of access to sufficient power. The attacker would need double the power Meta's Louisiana data center plans to have in 2030.&lt;p&gt;Access to gigawatts of power is available only on long-term contracts and only after significant delays.&lt;/p&gt;&lt;head rend="h5"&gt;Data centers&lt;/head&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Hyperion&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Estimates for AI data centers are that 60% of the capital cost is the hardware and 40% everything else. Thus the "everything else" for Meta's $27B 5GW data center is $10.8B. "Everything else" for the attacker's two similar data centers would thus be $21.6B. Plus say 5 years of interest at 5% or $5.4B.&lt;/p&gt;&lt;head rend="h5"&gt;Operational cost&lt;/head&gt;Ignoring the evident impossibility of the outsider attacker amassing the necessary mining rigs, power and data center space, what would the operational costs of the attack be?&lt;p&gt;It is hard to estimate the costs for power, data center space, etc. But an estimate can be based upon the cost to rent hash power, noting that in practice renting 43% of the total would be impossible, and guessing that renters have a 30% margin. A typical rental fee would be $0.10/TH/day so the costs might be $0.07/TH/day. The attack would have a 95% probability of needing 482EH/s over 34 days or less, so $516M or less.&lt;/p&gt;&lt;p&gt;Thus the estimated total cost for the hash power used in the attack would have a 95% probability of being no more than $7.66B. Plus about $27B in data center cost, which could presumably be repurposed to AI after the attack.&lt;/p&gt;&lt;head rend="h4"&gt;Insider Attack&lt;/head&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Source&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;The insider's loss of income from the blocks they would otherwise have mined would have a 95% probability of being 4,590 BTC or less, or about $425M.&lt;/p&gt;&lt;head rend="h3"&gt;Short Position&lt;/head&gt;Both kinds of attackers need to ensure that, when the attack succeeds, they have a large enough short position in Bitcoin that would generate their expected return from the attack's decrease in the Bitcoin price. There are two possibilities:&lt;list rend="ul"&gt;&lt;item&gt;When the attacker's chain is within one block of being the longest, they have ten minutes to purchase the shorts. There is unlikely to be enough liquidity in the market to accommodate this sudden demand, which in any case would greatly increase the price of the shorts. I will ignore this possibility in what follows.&lt;/item&gt;&lt;item&gt;At the start of the attack the attacker gradually accumulates sufficient shorts. Even assuming there were enough liquidity, and that the purchases didn't increase the price, the attacker has to bear both the cost of maintaining the shorts for the duration of the attack, and the risk of the market moving up enough to cause the position to be liquidated.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Farokhnia &amp;amp; Goharshady note that:&lt;/p&gt;&lt;quote&gt;At the time of writing, the open interest of BTC options is a bit more than 20 billion USD. Thus, a malicious party performing the attack mentioned in this work would need to obtain a considerable amount of the available put contracts. This may lead to market disruptions whose analysis is beyond the scope of this work. This being said, if the derivatives market continues to grow and becomes much larger than it currently is, purchasing this amount of contracts might not even be detected.There are two different kinds of market in which Bitcoin shorts are available:&lt;/quote&gt;&lt;list rend="ul"&gt;&lt;item&gt;Regulated exchanges such as the CME offering options on Bitcoin and stock exchanges with Bitcoin ETFs and Bitcoin treasury companies such as Strategy.&lt;/item&gt;&lt;item&gt;Unregulated exchanges such as Binance offering "perpetual futures" (perps) on Bitcoin.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h4"&gt;Unregulated Exchanges&lt;/head&gt;Patrick McKenzie's Perpetual futures, explained is a clear and comprehensive description of the derivative common on unregulated exchanges:&lt;quote&gt;Instead of all of a particular futures vintage settling on the same day, perps settle multiple times a day for a particular market on a particular exchange. The mechanism for this is the funding rate. At a high level: winners get paid by losers every e.g. 4 hours and then the game continues, unless you’ve been blown out due to becoming overleveraged or for other reasons (discussed in a moment).So the exchange makes money from commissions, and from the spread against the actual spot price. The price of the perp is maintained close to the spot price by the "basis trade", traders providing liquidity by shorting the perp and buying the spot when the perp is above spot, and vice versa. Of course, the spot price itself may have been manipulated, for example by Pump-and-Dump Schemes.&lt;lb/&gt;Consider a toy example: a retail user buys 0.1 Bitcoin via a perp. The price on their screen, which they understand to be for Bitcoin, might be $86,000 each, and so they might pay $8,600 cash. Should the price rise to $90,000 before the next settlement, they will get +/- $400 of winnings credited to their account, and their account will continue to reflect exposure to 0.1 units of Bitcoin via the perp. They might choose to sell their future at this point (or any other). They’ll have paid one commission (and a spread) to buy, one (of each) to sell, and perhaps they’ll leave the casino with their winnings, or perhaps they’ll play another game.&lt;lb/&gt;Where did the money come from? Someone else was symmetrically short exposure to Bitcoin via a perp. It is, with some very important caveats incoming, a closed system: since no good or service is being produced except the speculation, winning money means someone else lost.&lt;/quote&gt;&lt;p&gt;How else does the exchange make money?&lt;/p&gt;&lt;quote&gt;Perp funding rates also embed an interest rate component. This might get quoted as 3 bps a day, or 1 bps every eight hours, or similar. However, because of the impact of leverage, gamblers are paying more than you might expect: at 10X leverage that’s 30 bps a day.A "basis point (bps)" is "one hundredth of 1 percentage point", so 30bps/day is 0.3%/day or around 120%/year. But the lure of leverage is the competitive advantage of unregulated exchanges:&lt;/quote&gt;&lt;quote&gt;In a standard U.S. brokerage account, Regulation T has, for almost 100 years now, set maximum leverage limits (by setting minimums for margins). These are 2X at position opening time and 4X “maintenance” (before one closes out the position). Your brokerage would be obligated to forcibly close your position if volatility causes you to exceed those limits.Unregulated markets are different:&lt;/quote&gt;&lt;quote&gt;Binance allows up to 125x leverage on BTC.Although these huge amounts of leverage greatly increase the reward from a small market movement in favor of the position, they greatly reduce the amount the market has to move against the position before something bad happens. The first bad thing is liquidation:&lt;/quote&gt;&lt;quote&gt;One reason perps are structurally better for exchanges and market makers is that they simplify the business of blowing out leveraged traders. The exact mechanics depend on the exchange, the amount, etc, but generally speaking you can either force the customer to enter a closing trade or you can assign their position to someone willing to bear the risk in return for a discount.The bigger and faster the market move, the more likely the loss exceeds your collateral:&lt;lb/&gt;Blowing out losing traders is lucrative for exchanges except when it catastrophically isn’t. It is a priced service in many places. The price is quoted to be low (“a nominal fee of 0.5%” is one way Binance describes it) but, since it is calculated from the amount at risk, it can be a large portion of the money lost. If the account’s negative balance is less than the liquidation fee, wonderful, thanks for playing and the exchange / “the insurance fund” keeps the rest, as a tip.&lt;/quote&gt;&lt;quote&gt;In the case where the amount an account is negative by is more than the fee, that “insurance fund” can choose to pay the winners on behalf of the liquidated user, at management’s discretion. Management will usually decide to do this, because a casino with a reputation for not paying winners will not long remain a casino.The second bad thing is automatic de-leveraging (ADL):&lt;lb/&gt;But tail risk is a real thing. The capital efficiency has a price: there physically does not exist enough money in the system to pay all winners given sufficiently dramatic price moves. Forced liquidations happen. Sophisticated participants withdraw liquidity (for reasons we’ll soon discuss) or the exchange becomes overwhelmed technically / operationally. The forced liquidations eat through the diminished / unreplenished liquidity in the book, and the magnitude of the move increases.&lt;/quote&gt;&lt;quote&gt;Risk in perps has to be symmetric: if (accounting for leverage) there are 100,000 units of Somecoin exposure long, then there are 100,000 units of Somecoin exposure short. This does not imply that the shorts or longs are sufficiently capitalized to actually pay for all the exposure in all instances.McKenzie illustrates ADL with an example:&lt;lb/&gt;In cases where management deems paying winners from the insurance fund would be too costly and/or impossible, they automatically deleverage some winners.&lt;/quote&gt;&lt;quote&gt;So perhaps you understood, prior to a 20% move, that you were 4X leveraged. You just earned 80%, right? Ah, except you were only 2X leveraged, so you earned 40%. Why were you retroactively only 2X? That’s what automatic deleveraging means. Why couldn’t you get the other 40% you feel entitled to? Because the collective group of losers doesn’t have enough to pay you your winnings and the insurance fund was insufficient or deemed insufficient by management.For our purposes, this is an important note:&lt;/quote&gt;&lt;quote&gt;In theory, this can happen to the upside or the downside. In practice in crypto, this seems to usually happen after sharp decreases in prices, not sharp increases. For example, October 2025 saw widespread ADLing as (more than) $19 billion of liquidations happened, across a variety of assets.How does this affect the outsider attacker? Lets assume that the attack has a 95% probability of costing no more than $7.5B and would reduce the Bitcoin price from $100K to $80K in a single 4-hour period. With 10X leverage this would generate $200K/BTC in gains.&lt;/quote&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Source&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Source&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;The way liquidation of a short works is that as the market moves up, the initial leverage increases. Each exchange will have a limit on the leverage it will allow so, allowing for the liquidation fee, if the leverage of the short position gets to this limit the exchange will liquidate it.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell&gt;Move %&lt;/cell&gt;&lt;cell&gt;Leverage&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;10&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;11.1&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;2&lt;/cell&gt;&lt;cell&gt;12.5&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;cell&gt;14.3&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;cell&gt;16.7&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;5&lt;/cell&gt;&lt;cell&gt;20&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;6&lt;/cell&gt;&lt;cell&gt;25&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;7&lt;/cell&gt;&lt;cell&gt;33.3&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;8&lt;/cell&gt;&lt;cell&gt;50&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;9&lt;/cell&gt;&lt;cell&gt;100&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;In the unlikely event that the attack succeeds early enough to avoid liquidation there would have been one of those "sharp decreases in prices" that cause ADL, so as a huge winner it would be essentially certain that the attacker would suffer ADL and most of the winnings needed to justify the attack would evaporate.&lt;/p&gt;&lt;head rend="h4"&gt;Regulated Exchanges&lt;/head&gt;The peak open interest in Bitcoin futures on the Chicago Mercantile Exchange over the past year was less than $20B, so even if we add together both kinds of exchange, the peak open interest over the last year isn't enough for the attacker.&lt;head rend="h3"&gt;Conclusions&lt;/head&gt;Neither an outsider nor an insider attack appears feasible.&lt;head rend="h4"&gt;Outsider Attack&lt;/head&gt;An outsider attack seems infeasible because in practice:&lt;list rend="ul"&gt;&lt;item&gt;They could not acquire 43% or more of the hash power.&lt;/item&gt;&lt;item&gt;Even if they could it would take so long as to make detection inevitable.&lt;/item&gt;&lt;item&gt;Even if they could and they were not detected, the high cost of the rigs makes the necessary shorts large relative to the open interest, and expensive to maintain.&lt;/item&gt;&lt;item&gt;These large shorts would need to be leveraged perpetual futures, bringing significant risks of loss of collateral through liquidation, and of the potential payoff being reduced through automatic de-leveraging.&lt;/item&gt;&lt;item&gt;The attacker would need more than the peak aggregate open interest in Bitcoin futures over the past year.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h4"&gt;Insider Attack&lt;/head&gt;The order-of-magnitude lower direct cost of an insider attack makes it appear less infeasible, but insiders have to consider the impact on their continuing mining business. If the assumed 20% drop in the Bitcoin price were sustained for a year, the cost to the miner controlling 30% of the hash rate would be about 15,750 BTC or nearly $1.5B making the total cost of the attack (excluding the cost of carrying the shorts) almost $2B.&lt;table&gt;&lt;row&gt;&lt;cell&gt;Source&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Source&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;quote&gt;mining-company stocks are still flying, even with cryptocurrency prices in retreat. That's because these firms have something in common with the hottest investment theme on the planet: the massive, electricity-hungry data centers expected to power the artificial-intelligence boom. Some companies are figuring out how to remake themselves as vital suppliers to Alphabet, Amazon, Meta, Microsoft and other "hyperscalers" bent on AI dominance.I wonder why the date is 2028! As profit-driven miners use their bouyant stock price to fund a pivot to AI the hash rate and the network difficuty will decrease, making an insider attack less infeasible. The drop in their customer's income will likely encourage Bitmain to similarly pivot to AI, devoting an increasing proportion of their wafers to AI chips, especially given the Chinese government's goal of localizing AI.&lt;lb/&gt;...&lt;lb/&gt;Miners often have to build new, specialized facilities, because running AI requires more-advanced cooling and network systems, as well as replacing bitcoin-mining computers with AI-focused graphics processing units. But signing deals with miners allows AI giants to expand faster and cheaper than starting new facilities from scratch.&lt;lb/&gt;...&lt;lb/&gt;Shares of Core Scientific quadrupled in 2024 after the company signed its first AI contract that February. The stock has gained 10% this year. The company now expects to exit bitcoin mining entirely by 2028.&lt;/quote&gt;&lt;p&gt;A 30% miner whose rigs were fully depreciated might consider an insider attack shortly before the halvening as a viable exit strategy, since their future earnings from mining would be greatly reduced. But they would still be detected.&lt;/p&gt;&lt;head rend="h3"&gt;Counter-measures&lt;/head&gt;Even if we assume the feasibility of both the hash rate and the short position aspects of the attack, it is still the case that for example, an attack with 30% of the hash power and a 95% probability of success will, on average, last 17 days. it seems very unlikely that the coincidence over an extended period of a large reduction in the expected hash rate and a huge increase in short interest would escape attention from Bitcoin HODl-ers, miners and exchanges, not to mention Bitmain. What counter-measures could they employ?&lt;table&gt;&lt;row&gt;&lt;cell&gt;Source&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;list rend="ul"&gt;&lt;item&gt;The 6-block rule is just a convention, there is no dial that can be turned.&lt;/item&gt;&lt;item&gt;Much of the access to the Bitcoin blockchain is via APIs that typically have the 6-block rule hard-codded in.&lt;/item&gt;&lt;item&gt;Many, typically low-value, transactions do not wait for even a single confirmation.&lt;/item&gt;&lt;item&gt;Even it were possible, changing from a one-hour to a four-hour confirmation would have significant negative impacts on the Bitcoin ecosystem.&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46437876</guid><pubDate>Tue, 30 Dec 2025 20:53:59 +0000</pubDate></item><item><title>Humans May Be Able to Grow New Teeth Within Just 4 Years</title><link>https://www.popularmechanics.com/science/health/a69878870/human-new-tooth-regrowth-trials-japan-timeline/</link><description>&lt;doc fingerprint="3d8aa37b74ab8fb7"&gt;
  &lt;main&gt;
    &lt;p&gt;Here’s what you’ll learn when you read this story:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;While bones can regrow themselves when they break, teeth aren’t so lucky, and that leads to millions of people worldwide suffering from some form of edentulism, a.k.a. toothlessness.&lt;/item&gt;
      &lt;item&gt;Now, Japanese researchers are moving a promising, tooth-regrowing medicine into human trials.&lt;/item&gt;
      &lt;item&gt;If the trial is successful, the researchers hope the drug will become available for all forms of toothlessness sometime around 2030.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The average adult human body contains 206 bones—the hardened mixtures of calcium, minerals, and collagen that provide the biological scaffolding that walks us through our day. While we may not think of them much, bones are incredibly resilient. But if they do break, they have this nifty trick of regrowing themselves.&lt;/p&gt;
    &lt;p&gt;Teeth, however, are not bones. Although they’re made of some of the same stuff and are the hardest material in the human body (thanks to its protective layer of enamel), they lack the crucial ability to heal and regrow themselves. But that may not always be the case. Japanese researchers are moving forward with an experimental drug that promises to regrow human teeth. Human trials began in September 2024.&lt;/p&gt;
    &lt;p&gt;“We want to do something to help those who are suffering from tooth loss or absence,” Katsu Takahashi, the head of dentistry at the medical research institute at Kitano Hospital in Osaka, told The Mainichi. “While there has been no treatment to date providing a permanent cure, we feel that people’s expectations for tooth growth are high.”&lt;/p&gt;
    &lt;p&gt;This development follows years of study around a particularly antibody named Uterine sensitization–associated gene-1 (USAG-1), which has been shown to inhibit the growth of teeth in ferrets and mice. Back in 2021, scientists from the Kyoto University—who will also be involved in future human trials—discovered a monoclonal antibody (a technique usually used in fighting cancer) that disrupted the interaction between USAG-1 and molecules known as bone morphogenetic protein, or BMP.&lt;/p&gt;
    &lt;p&gt;“We knew that suppressing USAG-1 benefits tooth growth. What we did not know was whether it would be enough,” Kyoto University’s Katsu Takahashi, a co-author of the study, said in a press statement at the time. “Ferrets are diphyodont animals with similar dental patterns to humans.”&lt;/p&gt;
    &lt;p&gt;Now, scientists will see just how similar, because humans are undergoing a similar trial. Lasting 11 months, this study focuses on 30 males between the ages of 30 and 64—each missing at least one tooth. The drug will be administered intravenously to prove its effectiveness and safety, and luckily, no side effects have been reported in previous animal studies.&lt;/p&gt;
    &lt;p&gt;If all goes well, Kitano Hospital will administer the treatment to patients between the ages of 2 to 7 who are missing at least four teeth, with the end goal of having a tooth-regrowing medicine available by the year 2030. While these treatments are currently focused on patients with congenital tooth deficiency, Takahashi hopes the treatment will be available for anyone who’s lost a tooth.&lt;/p&gt;
    &lt;p&gt;Darren lives in Portland, has a cat, and writes/edits about sci-fi and how our world works. You can find his previous stuff at Gizmodo and Paste if you look hard enough.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46438169</guid><pubDate>Tue, 30 Dec 2025 21:22:53 +0000</pubDate></item><item><title>U.S. cybersecurity experts plead guilty for ransomware attacks</title><link>https://www.tomshardware.com/tech-industry/cyber-security/u-s-cybersecurity-experts-plead-guilty-for-ransomware-attacks-face-20-years-in-prison-each-group-demanded-up-to-usd10-million-from-each-victim</link><description>&lt;doc fingerprint="176010dad603011c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;U.S. cybersecurity experts plead guilty for ransomware attacks, face 20 years in prison each — group demanded up to $10 million from each victim&lt;/head&gt;
    &lt;p&gt;These cybersecurity experts were hired to prevent the exact attacks they perpetrated.&lt;/p&gt;
    &lt;p&gt;Two former cybersecurity experts pled guilty to conspiracy to obstruct commerce by extortion and are set to be sentenced up to 20 years in prison each for attacking several U.S. companies with ransomware and holding their data hostage for up to $10 million. BleepingComputer reported that the two offenders were former employees of Sygnia and DigitalMint — cybersecurity incident response firms that help companies that have been affected by ransomware and other cyberattacks.&lt;/p&gt;
    &lt;p&gt;“These defendants used their sophisticated cybersecurity training and experience to commit ransomware attacks — the very type of crime that they should have been working to stop,” Assistant Attorney General A. Tysen Duva said in a statement. “Extortion via the internet victimizes innocent citizens every bit as much as taking money directly out of their pockets. The Department of Justice is committed to using all tools available to identify and arrest perpetrators of ransomware attacks wherever we have jurisdiction.”&lt;/p&gt;
    &lt;p&gt;Ryan Clifford Goldberg, 40, of Watkinsville, Georgia, was a Sygnia incident response manager, while Kevin Tyler Martin, 36, of Roanoke, Texas, was a ransomware threat negotiator for DigitalMint. Another unnamed co-conspirator had the same position as Martin at DigitalMint, but they haven't been identified yet. According to the Justice Department, the three people tapped the ALPHV BlackCat ransomware-as-a-service for their activities, paying its administrators a 20% cut of their proceeds.&lt;/p&gt;
    &lt;p&gt;The three conspirators attacked several U.S. companies across different states, including those based in Maryland, California, Florida, and Virginia. Of all the victims, court records show that only a Florida-based medical device maker paid a ransom of $1.27 million — a fraction of the $10 million the group demanded from the company. After paying BlackCat’s 20% cut, the group split the remainder three ways and laundered the Bitcoin through different channels.&lt;/p&gt;
    &lt;p&gt;Neither the DOJ nor the United States Southern District Court of Florida mentioned how the two were caught, so we don’t know what led to their arrest — nevertheless, Goldberg has been in federal custody since September 2023. The FBI Miami Field Office was the main agency behind this investigation, and has been assisted by the U.S. Secret Service. Aside from their arrest, indictment, and sentencing, the Southern District of Florida is also handling the asset forfeiture case, meaning the perpetrators will likely yield the proceeds of their crime to the victims or the state.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, &amp;amp; reviews in your feeds.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;Jowi Morales is a tech enthusiast with years of experience working in the industry. He’s been writing with several tech publications since 2021, where he’s been interested in tech hardware and consumer electronics.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;alrighty_then&lt;/header&gt;You can make a good living in tech without going black hat. Did they owe the mob or something? Glad they were caught.Reply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Sluggotg&lt;/header&gt;I hope they do get 20 years. We have all seen people commit horrid crimes and get 5 years. They need to make an example out of these guys.Reply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;inquisitor2&lt;/header&gt;So you steal from companies and it's 20 years....you crash the housing market and destroy millions of people's lives....you get... light regulations in place until the next corrupt administration removes them.....and start again with a brand new name for the same bs product. But let the outrage ring out for these horrid crimes.....horrid.....lol....1.2 million that was probably insured or will be a writeoffReply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;bill001g&lt;/header&gt;Reply&lt;quote/&gt;Seems to be federal charges so they likely will have to do more than many of the states that let everyone out after a fraction of the time. If you asked for volunteers to go to jail and you paid them a million dollars a year they would have a line miles long. People embezzle a couple hundred thousand and get probation. Can't even really order them to pay it back because with a felony conviction for theft nobody is going to hire them, they will be very lucky to get even a minimum wage job.Sluggotg said:I hope they do get 20 years. We have all seen people commit horrid crimes and get 5 years. They need to make an example out of these guys.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46438255</guid><pubDate>Tue, 30 Dec 2025 21:31:00 +0000</pubDate></item><item><title>OpenAI's cash burn will be one of the big bubble questions of 2026</title><link>https://www.economist.com/leaders/2025/12/30/openais-cash-burn-will-be-one-of-the-big-bubble-questions-of-2026</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46438390</guid><pubDate>Tue, 30 Dec 2025 21:44:07 +0000</pubDate></item><item><title>Honey's Dieselgate: Detecting and tricking testers</title><link>https://vptdigital.com/blog/honey-detecting-testers/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46438522</guid><pubDate>Tue, 30 Dec 2025 21:59:35 +0000</pubDate></item><item><title>NYC Mayoral Inauguration bans Raspberry Pi and Flipper Zero alongside explosives</title><link>https://blog.adafruit.com/2025/12/30/nyc-mayoral-inauguration-bans-raspberry-pi-and-flipper-zero-alongside-explosives/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46438828</guid><pubDate>Tue, 30 Dec 2025 22:28:16 +0000</pubDate></item><item><title>Project ideas to appreciate the art of programming</title><link>https://codecrafters.io/blog/programming-project-ideas</link><description>&lt;doc fingerprint="3d1a305b2d76c99f"&gt;
  &lt;main&gt;
    &lt;p&gt;Many developers want to start a side project but aren't sure what to build. The internet is full of ideas that are basic and dull.&lt;/p&gt;
    &lt;p&gt;Here's our list of 73 project ideas to inspire you. We have chosen projects that teach a lot and are fun to build.&lt;/p&gt;
    &lt;p&gt;Build a BitTorrent client that can download files using the BitTorrent protocol. You can start with single-file torrents. This is a great way to learn how P2P networking works.&lt;/p&gt;
    &lt;p&gt;Read the official BitTorrent specification here.&lt;/p&gt;
    &lt;p&gt;Build a program that solves Wordle. This can be a great lesson on information theory and entropy. You'll also get hands-on experience at optimizing computations.&lt;/p&gt;
    &lt;p&gt;This YouTube video will get you started.&lt;/p&gt;
    &lt;p&gt;Implement Optimal Transport from scratch to morph one face into another while preserving identity and structure. You'll apply linear programming to a real problem.&lt;/p&gt;
    &lt;p&gt;Here are some OT resources and a paper which proposes a solution.&lt;/p&gt;
    &lt;p&gt;Create a spreadsheet with support for cell references, simple formulas, and live updates. You'll learn about dependency graphs, parsing, and reactive UI design.&lt;/p&gt;
    &lt;p&gt;The founder of the GRID spreadsheet engine shares some insights here.&lt;/p&gt;
    &lt;p&gt;Build a lightweight container runtime from scratch without Docker. You'll learn about kernel namespaces, chroot, process isolation, and more.&lt;/p&gt;
    &lt;p&gt;Read this to understand how containers work.&lt;/p&gt;
    &lt;p&gt;Build a system that uses Euclid's postulates to derive geometric proofs and visualize the steps. You'll learn symbolic representation, rule systems, logic engines, and proof theory.&lt;/p&gt;
    &lt;p&gt;You can efficiently implement Gelernter's visionary 1959 paper today.&lt;/p&gt;
    &lt;p&gt;Google uses a crawler to navigate web pages and save their contents. By building one, you'll learn how web search works. It's also great practice for system design.&lt;/p&gt;
    &lt;p&gt;You can make your own list of sites and create a search engine on a topic of your interest.&lt;/p&gt;
    &lt;p&gt;This article proposes a design approach.&lt;/p&gt;
    &lt;p&gt;Build a DNS server that listens for queries, parses packets, resolves domains, and caches results. Learn more about low-level networking, UDP, TCP, and the internet.&lt;/p&gt;
    &lt;p&gt;Start with how DNS works and dive into the DNS packet format.&lt;/p&gt;
    &lt;p&gt;Build a game where players connect two actors through shared credits with other actors, and reveal the optimal path at the end. You'll learn how to deal with massive graphs.&lt;/p&gt;
    &lt;p&gt;Explore how to create fast graphs, and then try Landmark Labelling for supreme performance.&lt;/p&gt;
    &lt;p&gt;Implement the RAFT protocol from scratch to support distributed computing. Learn consensus, failure recovery, and how to build fault-tolerant distributed systems.&lt;/p&gt;
    &lt;p&gt;Visit this page for the RAFT paper and other resources.&lt;/p&gt;
    &lt;p&gt;Design a program from scratch that creates satisfying crosswords with adjustable difficulty. You'll learn procedural generation, constraint propagation, and difficulty modeling.&lt;/p&gt;
    &lt;p&gt;For example, you can implement the Wave Function Collapse algorithm explained here.&lt;/p&gt;
    &lt;p&gt;Bitcask is an efficient embedded key-value store designed to handle production-grade traffic. Building this will improve your understanding of databases and efficient storage.&lt;/p&gt;
    &lt;p&gt;You can implement this short paper.&lt;/p&gt;
    &lt;p&gt;Apps like Shazam extract unique features from audio. This fingerprint is then used to match and identify sounds. You'll need to learn hash-based lookups and a bit of signal processing.&lt;/p&gt;
    &lt;p&gt;Here's a detailed post with everything you need to know.&lt;/p&gt;
    &lt;p&gt;Recreate the industry-changing game using SDL, and add some story elements, NPC interactions, and levels. It'll be a perfect intro to game development.&lt;/p&gt;
    &lt;p&gt;This video will set you up.&lt;/p&gt;
    &lt;p&gt;Implement an algorithm from scratch to compare two text files or programs. This will involve dynamic programming and application of graph traversal.&lt;/p&gt;
    &lt;p&gt;Here's the classic paper behind Myers' diff, used in Git for years.&lt;/p&gt;
    &lt;p&gt;Generate UML class diagrams from source code with support for relationships like inheritance. You'll visualize object-oriented code and learn how to parse with ASTs.&lt;/p&gt;
    &lt;p&gt;This article explains parsing with ASTs.&lt;/p&gt;
    &lt;p&gt;Write your own encoder/decoder for the BMP image format and build a tiny viewer for it. You'll learn binary parsing, image encoding, and how to work with pixel buffers and headers.&lt;/p&gt;
    &lt;p&gt;The Wikipedia article is a good place to start.&lt;/p&gt;
    &lt;p&gt;Build a FUSE filesystem for Linux from scratch, with indexing, file metadata, and caching. You'll have to optimize data structures for storage and performance.&lt;/p&gt;
    &lt;p&gt;This article talks about the concepts used in filesystems.&lt;/p&gt;
    &lt;p&gt;Write the qubit and quantum gates from scratch. Use them to simulate a circuit for a quantum algorithm like Bernstein-Vazirani or Simon's algorithm.&lt;/p&gt;
    &lt;p&gt;Read this short paper for the essentials without any fluff.&lt;/p&gt;
    &lt;p&gt;Write a video player that decodes H.264/H.265 using ffmpeg, and supports casting local files to smart devices. Learn packet buffering, discovery protocols, and stream encoding.&lt;/p&gt;
    &lt;p&gt;Get started with this article.&lt;/p&gt;
    &lt;p&gt;Build a Redis clone from scratch that supports basic commands, RDB persistence, replica sync, streams, and transactions. You'll get to deep dive into systems programming.&lt;/p&gt;
    &lt;p&gt;You can use the official Redis docs as a guide.&lt;/p&gt;
    &lt;p&gt;Build a client-side video editor that runs in the browser without uploading files to a server. Learn how to work with WASM, and why people love using it for high performance tasks.&lt;/p&gt;
    &lt;p&gt;Visit the official WebAssembly site to get started.&lt;/p&gt;
    &lt;p&gt;This is a rite of passage. You'll get hands-on experience with encryption, token expiration, refresh flows, and how to manage user sessions securely.&lt;/p&gt;
    &lt;p&gt;Implement username and password auth. Then manage sessions with JWT or session IDs.&lt;/p&gt;
    &lt;p&gt;You have used it in searches and other places where you write text. Implement a solution that suggests the right words, and then optimize heavily for speed.&lt;/p&gt;
    &lt;p&gt;This YouTube video gives an idea of the implementation process.&lt;/p&gt;
    &lt;p&gt;Build a simple SQL engine that reads .db files, uses indexes and executes queries. It's a deep dive into how real-world databases are built and run efficiently.&lt;/p&gt;
    &lt;p&gt;You need to understand B-trees and how SQLite stores data on disk.&lt;/p&gt;
    &lt;p&gt;Remove background sounds from audio files. You'll learn signal processing and denoising techniques used in GPS, mouse input, sensors, object tracking, etc.&lt;/p&gt;
    &lt;p&gt;You can use a technique like Kalman Filtering to do this.&lt;/p&gt;
    &lt;p&gt;Design a file sharing app with sync, cloud storage, and basic p2p features that can scale to some extent. You'll get practice in cloud architecture and backend design.&lt;/p&gt;
    &lt;p&gt;This article dives into the system design.&lt;/p&gt;
    &lt;p&gt;Build a map engine to index roads, terrain (rivers, mountains), places (shops, landmarks), and areas (cities, states). Learn spatial indexing, range queries, and zoom-level abstractions.&lt;/p&gt;
    &lt;p&gt;Start by implementing an R-tree from scratch by following the original paper.&lt;/p&gt;
    &lt;p&gt;Use Natural Earth and GeoFabrik datasets to populate your map engine.&lt;/p&gt;
    &lt;p&gt;Recreate a city's road network, simulate traffic using real open data, and design an improved version. Tackle an NP-hard optimization problem with real constraints.&lt;/p&gt;
    &lt;p&gt;In some cases, nature has long solved what we call hard. Implement SMA or ACO here.&lt;/p&gt;
    &lt;p&gt;Develop a decentralized collaborative text editor. Similar to Google Docs, but without any central server. Use CRDTs to manage concurrent edits and ensure eventual consistency.&lt;/p&gt;
    &lt;p&gt;Use ropes, gap buffers, or piece tables to build a fast text buffer optimized for efficient editing.&lt;/p&gt;
    &lt;p&gt;Read this article on designing data structures for such apps.&lt;/p&gt;
    &lt;p&gt;Evolve working models of machinery using only primitive mechanical parts and constraints. You'll learn about genetic algorithms, fitness functions, and physics simulation.&lt;/p&gt;
    &lt;p&gt;You can design bridges, cars, clocks, calculators, catapults, and more. NASA used GAs to design an antenna for their space mission.&lt;/p&gt;
    &lt;p&gt;This YouTube video shows how interesting evolutionary design can get.&lt;/p&gt;
    &lt;p&gt;Create a server from scratch that supports HTTP requests, static files, routing, and reverse proxying. Learn socket programming and how web servers work.&lt;/p&gt;
    &lt;p&gt;This page will get you started.&lt;/p&gt;
    &lt;p&gt;Estimate a depth (disparity) map from a stereo image pair using Markov Random Fields. You'll learn about computer vision, graphical models, and inference techniques.&lt;/p&gt;
    &lt;p&gt;Start with the Middlebury Dataset and this article on belief propagation for stereo matching.&lt;/p&gt;
    &lt;p&gt;Build a minimal Git with core features like init, commit, diff, log, and branching. Learn how version control works using content-addressable storage, hashes, and trees.&lt;/p&gt;
    &lt;p&gt;Check out Write yourself a Git for an overview of git internals.&lt;/p&gt;
    &lt;p&gt;Build a Unix debugger with stepping, breakpoints, and memory inspection. You'll learn low-level systems programming and process control.&lt;/p&gt;
    &lt;p&gt;This article discusses the internal structure of GDB.&lt;/p&gt;
    &lt;p&gt;Build a deep learning framework from scratch with a tensor class, autograd, basic layers, and optimizers. Grasp the internals of backpropagation and gradient descent.&lt;/p&gt;
    &lt;p&gt;Start by building a simple 3-layer feedforward NN (multilayer perceptron) with your framework.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy explains the basic concepts in this YouTube Video.&lt;/p&gt;
    &lt;p&gt;Build a Chess app from scratch, where users can play against each other or your own UCI engine. This project offers a blend of algorithms, UI, game logic, and AI.&lt;/p&gt;
    &lt;p&gt;You can go one step further and make the engine play itself to improve like AlphaZero and Leela.&lt;/p&gt;
    &lt;p&gt;You can start with the rules and the chess programming wiki.&lt;/p&gt;
    &lt;p&gt;Build a fast search engine from scratch for the Wikipedia dump with typo tolerance and semantic ranking, and fuzzy queries. You'll learn indexing, tokenization, and ranking algorithms.&lt;/p&gt;
    &lt;p&gt;This article offers a good introduction to the basics of information retrieval.&lt;/p&gt;
    &lt;p&gt;Build a caching system to avoid redundant fetches for static assets. You'll learn web caching, log analysis, and how to use probabilistic data structures in a real setting.&lt;/p&gt;
    &lt;p&gt;You can use this dataset containing two month's worth of HTTP requests to the NASA server.&lt;/p&gt;
    &lt;p&gt;This article introduces some of the key concepts.&lt;/p&gt;
    &lt;p&gt;Build a short-video app with infinite scroll, social graphs of friends and subs, and a tailored feed. You'll learn efficient preloading, knowledge graphs, and behavioral signals.&lt;/p&gt;
    &lt;p&gt;Read this article on Monolith, Bytedance's recommendation system.&lt;/p&gt;
    &lt;p&gt;Implement NTP from scratch to build a background service that syncs system time with time servers. You'll learn daemon design and the internals of network time sync.&lt;/p&gt;
    &lt;p&gt;RFC 5905 is a great place to start.&lt;/p&gt;
    &lt;p&gt;Implement HyperLogLog from scratch to provide analytics on number of users engaging with hashtags in real time. You'll learn some key concepts around big data systems.&lt;/p&gt;
    &lt;p&gt;Start with Google's paper on HyperLogLog.&lt;/p&gt;
    &lt;p&gt;Write a query planner that rewrites SQL queries for better performance. You'll learn cost estimation, join reordering, and index selection.&lt;/p&gt;
    &lt;p&gt;This DuckDB post explains how optimizers work.&lt;/p&gt;
    &lt;p&gt;Implement an encrypted voting system for anonymity. Use zero-knowledge proofs to verify results.&lt;/p&gt;
    &lt;p&gt;Learn cryptographic primitives, smart contracts, and ZKPs.&lt;/p&gt;
    &lt;p&gt;For example, this paper attempts to define such a protocol.&lt;/p&gt;
    &lt;p&gt;Build a mesh VPN where nodes relay traffic without central servers. You'll learn NAT traversal, encrypted tunneling, and decentralized routing.&lt;/p&gt;
    &lt;p&gt;Tailscale's blog introduces some key concepts.&lt;/p&gt;
    &lt;p&gt;Build a file archiver that compresses, bundles, and encrypts your files. Implement compression and encryption algorithms from scratch. Benchmark your performance against zip.&lt;/p&gt;
    &lt;p&gt;You can refer to the official .zip specification.&lt;/p&gt;
    &lt;p&gt;Build a basic ray tracer to render 3D scenes with spheres, planes, and lights. This will be great practice in writing clean abstractions and optimizing performance-heavy code.&lt;/p&gt;
    &lt;p&gt;You can refer to the Ray Tracing in One Weekend ebook.&lt;/p&gt;
    &lt;p&gt;Create your own language. It is best to start with an interpreted language that does not need a complier. Design your own grammar, parser, and an evaluation engine.&lt;/p&gt;
    &lt;p&gt;Crafting Interpreters is by far the best resource you can refer to.&lt;/p&gt;
    &lt;p&gt;Recreate WhatsApp with chats, groups, history, encryption, notifications, and receipts. You'll get practice at building a production-grade app with an API, data store, and security.&lt;/p&gt;
    &lt;p&gt;You can draw inspiration from this system design approach.&lt;/p&gt;
    &lt;p&gt;Build a service to provide routes for a fleet of vehicles with limited capacity to deliver Amazon packages. You'll learn to optimize routing under constraints.&lt;/p&gt;
    &lt;p&gt;Here's a comparison of effective VRP algorithms.&lt;/p&gt;
    &lt;p&gt;Build a basic broker to handle topic creation, produce and consume requests. You'll implement concurrency, avoid race conditions, and learn how distributed logs work.&lt;/p&gt;
    &lt;p&gt;Here's a guide to the Kafka protocol.&lt;/p&gt;
    &lt;p&gt;Build an interactive knowledge graph that connects entities across media. Monitor the web for new content to keep it updated. You'll learn graph databases and data handling.&lt;/p&gt;
    &lt;p&gt;Explore this GitHub repo to learn about knowledge graphs and find some inspiration.&lt;/p&gt;
    &lt;p&gt;Create a malware and test it against simple firewalls on your VMs. Don't share the code with anyone, though. This project is a solid intro to cybersecurity.&lt;/p&gt;
    &lt;p&gt;This YouTube video discusses the core concepts.&lt;/p&gt;
    &lt;p&gt;Emulate the iconic GBA. You'll learn about CPU architecture, memory, graphics, and input.&lt;/p&gt;
    &lt;p&gt;You can use the GBATEK document to get started.&lt;/p&gt;
    &lt;p&gt;Implement a minimal userspace TCP/IP stack for Linux based on its core specification. You'll learn network and system programming at a deeper level.&lt;/p&gt;
    &lt;p&gt;You can get started with IETF's docs on TCP, IP, internet checksum, and more.&lt;/p&gt;
    &lt;p&gt;Write your own lock-free data structures using atomic primitives. You'll learn a lot about concurrency, memory management, and atomic operations.&lt;/p&gt;
    &lt;p&gt;Read about the concept here, and make sure to run a lot of tests like these.&lt;/p&gt;
    &lt;p&gt;Build a program to distribute requests across backend servers, check health, and support sessions. Learn about socket programming, concurrency, and scalable web infrastructure.&lt;/p&gt;
    &lt;p&gt;This article is an excellent intro to the topic.&lt;/p&gt;
    &lt;p&gt;Implement your own version of &lt;code&gt;malloc&lt;/code&gt;. Learn how memory allocation works under the hood by working with pointers, heaps, alignment, fragmentation, and system calls.&lt;/p&gt;
    &lt;p&gt;This tutorial by Marwan Burelle is the perfect place to start.&lt;/p&gt;
    &lt;p&gt;Build an app to host and stream 4k videos by writing a streaming protocol from scratch. You'll learn about file storage, video encoding, adaptive streaming, and scalable content delivery.&lt;/p&gt;
    &lt;p&gt;This YouTube video provides an overview of how it works.&lt;/p&gt;
    &lt;p&gt;Build an app that controls IR appliances and manual switches with device grouping, scheduling, and automation. This project is a mix of embedded systems, app dev, and UI design.&lt;/p&gt;
    &lt;p&gt;Watch this video for some inspiration.&lt;/p&gt;
    &lt;p&gt;Build a Continuous Integration system that watches a Git repo, runs tests in isolated environments, and reports results. You'll learn process orchestration and containerization.&lt;/p&gt;
    &lt;p&gt;This concise article will be helpful.&lt;/p&gt;
    &lt;p&gt;Implement a decision tree from scratch and use it to solve a binary classification task with random forest. You'll learn information gain, recursive partitioning, and overfitting.&lt;/p&gt;
    &lt;p&gt;This blog post covers the basics.&lt;/p&gt;
    &lt;p&gt;Build a shell with command parsing, process execution, piping, redirection, and job control. You'll learn a lot about system programming, memory, and operating systems.&lt;/p&gt;
    &lt;p&gt;This article discusses its architecture.&lt;/p&gt;
    &lt;p&gt;Build a node that can download and verify blocks from the Bitcoin network. You'll learn about Merkle trees, transactions, Bitcoin's P2P protocol, and more.&lt;/p&gt;
    &lt;p&gt;You can refer to the Mastering Bitcoin ebook here.&lt;/p&gt;
    &lt;p&gt;Write a simple &lt;code&gt;make&lt;/code&gt; tool that reads a Makefile and builds targets. You'll explore some neat concepts around caching, filesystem, version control, and automation.&lt;/p&gt;
    &lt;p&gt;This paper discusses how it works, and also suggests a non-recursive &lt;code&gt;make&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Build an extension to store passwords, OTPs, page state, scroll, forms, clipboard history, with auto-fill across sessions. You'll learn DOM inspection, secure storage, and sync protocols.&lt;/p&gt;
    &lt;p&gt;Start with the official Chrome or Mozilla docs on extensions.&lt;/p&gt;
    &lt;p&gt;Build a bot that uses trading algorithms to simulate or execute trades. Learn to structure event-driven systems, integrate APIs, and automate decisions under constraints.&lt;/p&gt;
    &lt;p&gt;For example, this paper describes one such algorithm you can implement.&lt;/p&gt;
    &lt;p&gt;Build a simple browser that parses HTML/CSS to render text and images. You'll learn tokenization, DOM trees, box models, style cascading, and layout algorithms.&lt;/p&gt;
    &lt;p&gt;This blog series explains the basic concepts.&lt;/p&gt;
    &lt;p&gt;Build a background app that uses your phone's GPS, motion, and mic to identify events and store a journal of your life. You'll learn signal processing, sensor APIs, and background tasks.&lt;/p&gt;
    &lt;p&gt;This article offers a glimpse into the experience of working with sensors.&lt;/p&gt;
    &lt;p&gt;Build a tiny renderer from scratch. This'll make you a better programmer across the board, and it's a must if you're serious about computer graphics.&lt;/p&gt;
    &lt;p&gt;Read this wiki to get started.&lt;/p&gt;
    &lt;p&gt;Build a laser tag system with real-time hit detection, wireless comms between guns, and live sync. You'll learn IR encoding, MQTT protocol, and lockstep/event-driven design.&lt;/p&gt;
    &lt;p&gt;This project attempts to do something similar.&lt;/p&gt;
    &lt;p&gt;Build an audio streaming system from scratch that plays music in sync across multiple devices or speakers. You'll learn low-latency transport, clock sync, buffering, and jitter handling.&lt;/p&gt;
    &lt;p&gt;Snapcast explains its sync approach here.&lt;/p&gt;
    &lt;p&gt;Build a p2p mesh network that routes traffic without ISPs or central servers. You'll learn distributed routing (like BGP for P2P), NAT traversal, and encrypted packet forwarding.&lt;/p&gt;
    &lt;p&gt;The Scuttlebutt Protocol and IPFS are great references.&lt;/p&gt;
    &lt;p&gt;That's a wrap.&lt;/p&gt;
    &lt;p&gt;If you do implement anything as a result of this post, or have any feedback on the ideas, let us know!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46439027</guid><pubDate>Tue, 30 Dec 2025 22:47:36 +0000</pubDate></item><item><title>Honey's Dieselgate: Detecting and Tricking Testers</title><link>https://www.benedelman.org/honey-detecting-testers/</link><description>&lt;doc fingerprint="e3c9ca5fa13427db"&gt;
  &lt;main&gt;
    &lt;p&gt;In a post on VPT’s blog, I present significant new research showing Honey not just violating affiliate network “stand-down” rules, but hiding those violations by behaving properly when there is reason to think a user could be a tester. Honey goes as far as checking a user’s cookies for other domains to see whether the user has logged into affiliate network management consoles — widely used by network compliance staff, merchants, and affiliates to manage campaigns. If a user has logged in there, he’s probably a tester — revealing Honey’s goal of breaking the rules as much as possible, without getting caught.&lt;/p&gt;
    &lt;p&gt;Details: Honey’s Dieselgate: Detecting and Tricking Testers.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46439369</guid><pubDate>Tue, 30 Dec 2025 23:21:50 +0000</pubDate></item></channel></rss>