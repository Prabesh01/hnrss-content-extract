<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 28 Oct 2025 11:32:47 +0000</lastBuildDate><item><title>Claude for Excel</title><link>https://www.claude.com/claude-for-excel</link><description>&lt;doc fingerprint="3d3f8e961dffc20a"&gt;
  &lt;main&gt;
    &lt;p&gt;Piloting Claude for Excel&lt;/p&gt;
    &lt;p&gt;Claude understands your entire workbookâfrom nested formulas to multiple tab dependencies. Get explanations with cell-level citations, and update assumptions while preserving formulas. Now in beta as a research preview.&lt;/p&gt;
    &lt;head rend="h2"&gt;How teams use Claude for Excel&lt;/head&gt;
    &lt;p&gt;Claude listens carefully, follows instructions precisely, â¨and thinks through complex problems.&lt;/p&gt;
    &lt;head rend="h3"&gt;Get answers about any cell in seconds&lt;/head&gt;
    &lt;p&gt;Navigate complex models instantly. Ask Claude about specific formulas, entire worksheets, or calculation flows across tabs. Every explanation includes cell-level citations so you can verify the logic.&lt;/p&gt;
    &lt;head rend="h3"&gt;Test scenarios without breaking formulas&lt;/head&gt;
    &lt;p&gt;Update assumptions across your entire model while preserving all dependencies. Test different scenarios quicklyâClaude highlights every change with explanations for full transparency.&lt;/p&gt;
    &lt;head rend="h3"&gt;Debug and fix errors&lt;/head&gt;
    &lt;p&gt;Trace #REF!, #VALUE!, and circular reference errors to their source in seconds. Claude explains what went wrong and how to fix it without disrupting the rest of your model.&lt;/p&gt;
    &lt;head rend="h3"&gt;Build models or fill existing templates&lt;/head&gt;
    &lt;p&gt;Create draft financial models from scratch based on your requirements. Or populate existing templates with fresh data while maintaining all formulas and structure.&lt;/p&gt;
    &lt;p&gt;The Claude you trust, right in Excel&lt;/p&gt;
    &lt;head rend="h3"&gt;Transparency and visibility&lt;/head&gt;
    &lt;p&gt;See Claudeâs changes in real time with explanations&lt;/p&gt;
    &lt;head rend="h3"&gt;Formula integrity&lt;/head&gt;
    &lt;p&gt;Maintain Excel model structure and formatting&lt;/p&gt;
    &lt;head rend="h3"&gt;Enterprise security&lt;/head&gt;
    &lt;p&gt;Works within your existing compliance framework&lt;/p&gt;
    &lt;p&gt;FAQ&lt;/p&gt;
    &lt;p&gt;Claude for Excel is available in beta as a research preview through a waitlist for 1,000 Max, Team and Enterprise plan customers. Weâll gradually expand access as we build confidence through this limited preview.&lt;/p&gt;
    &lt;p&gt;Claude for Excel works within your existing security framework. Claude can make mistakes, so you should always review changes before finalizing, especially for client-facing deliverables.&lt;/p&gt;
    &lt;p&gt;Claude for Excel is currently in beta as a research preview, so itâs best for model analysis, assumption updates, error debugging, template population, formula explanations, multi-tab navigation. Claude doesnât have advanced Excel capabilities including pivot tables, conditional formatting, data validation, data tables, macros, and VBA. Weâre actively working on these features.&lt;/p&gt;
    &lt;p&gt;Yes, Claude is trained to recognize common financial modeling patterns, formula structures, and industry-standard calculations. However, always verify outputs match your specific methodologies.&lt;/p&gt;
    &lt;p&gt;Currently .xlsx and .xlsm files are supported. File size limits apply based on your Claude plan.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45722639</guid><pubDate>Mon, 27 Oct 2025 16:09:22 +0000</pubDate></item><item><title>Show HN: JSON Query</title><link>https://jsonquerylang.org/</link><description>&lt;doc fingerprint="481baa954fb0882b"&gt;
  &lt;main&gt;
    &lt;code&gt;name(argument1, argument2, ...)&lt;/code&gt;
    &lt;p&gt;A function is defined as a function name followed by comma separated arguments wrapped in round brackets. it is important to understand functions like &lt;code&gt;filter&lt;/code&gt;, &lt;code&gt;sort&lt;/code&gt;, and &lt;code&gt;max&lt;/code&gt; are executed as a method in a chain: the operation is applied to the data input, and forwarded to the next method in the chain (if any).&lt;/p&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;sort(.address.city, "asc")&lt;/code&gt;
    &lt;code&gt;filter(.age &amp;gt;= 21) | sort(.age, "asc")&lt;/code&gt;
    &lt;p&gt;Documentation:&lt;/p&gt;
    &lt;p&gt;Function reference:&lt;/p&gt;
    &lt;code&gt;left operator right&lt;/code&gt;
    &lt;p&gt;JSON Query supports all basic operators. Operators must have both a left and right hand side. To override the default precedence, an operator can be wrapped in parentheses &lt;code&gt;(...)&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;.age &amp;gt;= 18&lt;/code&gt;
    &lt;code&gt;filter(.age &amp;gt;= 18 and .age &amp;lt;= 65)&lt;/code&gt;
    &lt;p&gt;Documentation:&lt;/p&gt;
    &lt;p&gt;Operator reference:&lt;/p&gt;
    &lt;code&gt;query2 | query2 | ...&lt;/code&gt;
    &lt;p&gt;A pipe is an array containing a series of queries. The queries in the pipeline are executed one by one, and the output of the first is the input for the next.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;filter(.age &amp;gt;= 18) | sort(.name)&lt;/code&gt;
    &lt;p&gt;Documentation:&lt;/p&gt;
    &lt;code&gt;{prop1: query1, prop2: query2, ...}&lt;/code&gt;
    &lt;p&gt;An object is defined as a regular JSON object with a property name as key, and a query as value. Objects can be used to transform data or to execute multiple query pipelines in parallel.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;{
  names: map(.name),
  numberOfNames: size()
}&lt;/code&gt;
    &lt;p&gt;Documentation:&lt;/p&gt;
    &lt;code&gt;[query1, query2, ...]&lt;/code&gt;
    &lt;p&gt;An array is defined as a regular JSON array: enclosed in square brackets, with items separated by a comma.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;filter(.city in ["New York", "Atlanta"])&lt;/code&gt;
    &lt;p&gt;Documentation:&lt;/p&gt;
    &lt;code&gt;.prop1.prop2&lt;/code&gt;
    &lt;p&gt;A property retrieves a property from an object. Multiple consecutive properties will retrieve a nested property.&lt;/p&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;.age&lt;/code&gt;
    &lt;code&gt;.address.city&lt;/code&gt;
    &lt;code&gt;"first name"&lt;/code&gt;
    &lt;code&gt;get()&lt;/code&gt;
    &lt;code&gt;get("address", "city")&lt;/code&gt;
    &lt;p&gt;Documentation:&lt;/p&gt;
    &lt;code&gt;"string", number, boolean, null&lt;/code&gt;
    &lt;p&gt;JSON Query supports the following primitive values, the same as in JSON: &lt;code&gt;"string"&lt;/code&gt;, &lt;code&gt;number&lt;/code&gt;, &lt;code&gt;boolean&lt;/code&gt;, &lt;code&gt;null&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;"Hello world"&lt;/code&gt;
    &lt;code&gt;"Multi line text\nwith \"quoted\" contents"&lt;/code&gt;
    &lt;code&gt;42&lt;/code&gt;
    &lt;code&gt;2.74&lt;/code&gt;
    &lt;code&gt;-1.2e3&lt;/code&gt;
    &lt;code&gt;true&lt;/code&gt;
    &lt;code&gt;false&lt;/code&gt;
    &lt;code&gt;null&lt;/code&gt;
    &lt;p&gt;Documentation:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45722826</guid><pubDate>Mon, 27 Oct 2025 16:22:52 +0000</pubDate></item><item><title>JetKVM – Control any computer remotely</title><link>https://jetkvm.com/</link><description>&lt;doc fingerprint="af5976106919f929"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Ultra-Low Latency&lt;/head&gt;
    &lt;p&gt;High-definition 1080p video at 60 FPS with 30-60 millisecond latency, using efficient H.264 encoding. Smooth mouse and keyboard action transfer for responsive remote interaction.&lt;/p&gt;
    &lt;head rend="h2"&gt;Free &amp;amp; Optional Cloud Access&lt;/head&gt;
    &lt;p&gt;Optional remote management via our open-source JetKVM Cloud using WebRTC. Privacy-first design with opt-in cloud access that provides secure and fast direct connections, even behind the most restrictive NAT environments, with our STUN and TURN servers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Open Source: Built for Collaboration&lt;/head&gt;
    &lt;p&gt;JetKVM is built on a robust Golang foundation and powered by Linux for adaptability and transparency. Whether you're a seasoned developer or an enthusiastic tinkerer, you can easily modify or fine-tune the software using familiar tooling and straightforward SSH uploads.&lt;/p&gt;
    &lt;head rend="h4"&gt;Available Source Code&lt;/head&gt;
    &lt;head rend="h5"&gt;KVM Runtime&lt;/head&gt;
    &lt;p&gt;Combining a Go-based backend with a React-powered WebRTC dashboard. Perfect for forking, submitting new features, fixing bugs, or customizing local streaming and control.&lt;/p&gt;
    &lt;head rend="h5"&gt;Cloud API &amp;amp; Dashboard&lt;/head&gt;
    &lt;p&gt;Our cloud-hosted management interface is fully open source. Delve into our secure remote connection orchestration or fork it to build specialized workflows and unique integrations.&lt;/p&gt;
    &lt;head rend="h5"&gt;Core System&lt;/head&gt;
    &lt;p&gt;Minimal Linux system built with BusyBox for core utilities. No bloat or unnecessary services - just the essential components needed for stable remote access.&lt;/p&gt;
    &lt;head rend="h2"&gt;Universally loved&lt;/head&gt;
    &lt;p&gt;Every single tech reviewer who's tested JetKVM has given it a glowing review. No exceptions. From professional data centers to home labs, the verdict is unanimous: this is the remote access solution the tech world has been waiting for.&lt;/p&gt;
    &lt;head rend="h2"&gt;Unlimited Hackability&lt;/head&gt;
    &lt;p&gt;The JetKVM hardware is fully customizable. Through the RJ12 extension port, extra hardware capabilities can easily be added by anyone. The JetKVM extension port is the way to fully customize your device.&lt;/p&gt;
    &lt;head rend="h2"&gt;Seamless Remote Control&lt;/head&gt;
    &lt;p&gt;Experience fluid control and crystal-clear video quality that makes remote access feel local. Perfect for IT professionals, developers, and power users who demand responsive remote management.&lt;/p&gt;
    &lt;head rend="h2"&gt;Stay updated on our latest projects&lt;/head&gt;
    &lt;p&gt;Join our newsletter to receive updates about new features, product launches, and early access opportunities.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45723159</guid><pubDate>Mon, 27 Oct 2025 16:44:17 +0000</pubDate></item><item><title>Why Busy Beaver hunters fear the Antihydra</title><link>https://benbrubaker.com/why-busy-beaver-hunters-fear-the-antihydra/</link><description>&lt;doc fingerprint="eeaad01055b00fc9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why Busy Beaver Hunters Fear the Antihydra&lt;/head&gt;
    &lt;p&gt;In the summer of 2024, I reported on an online community that nailed down the precise value of a number called BB(5) — the first big breakthrough in 50 years on an old problem in theoretical computer science known as the busy beaver game. BB(5), now known to be 47,176,870, is the fifth of the so-called busy beaver numbers, which measure the complexity of the craziest computations that simple computer programs can complete.1The team recently released a paper describing their results in detail.&lt;/p&gt;
    &lt;p&gt;The next step in this idiosyncratic research effort is to identify the sixth busy beaver number BB(6), and there has been some notable progress on that front — I wrote a follow-up story about it a few months ago. But busy beaver researchers don’t expect to nail down the true value of BB(6) any time soon. That’s because doing so would require them to understand the behavior of a program with the awesome name “Antihydra,” which resembles a longstanding open problem in mathematics called the Collatz conjecture.2Antihydra should not be confused with the false hydra, a very cool and very terrifying monster conceived by D&amp;amp;D blogger Arnold Kemp. A twitter user sharing my first busy beaver story summed up this state of affairs more succinctly:&lt;/p&gt;
    &lt;p&gt;Both of my stories alluded to the Antihydra barrier only very briefly. In this blog post I will explore it in more detail: What exactly is Antihydra, what is the Collatz conjecture, how are they connected, and what makes them so daunting?&lt;/p&gt;
    &lt;head rend="h2"&gt;Busy Beaver Basics&lt;/head&gt;
    &lt;p&gt;If you haven’t already read my two Quanta stories about the busy beaver game, I recommend doing so before reading further, mainly just because they’re both really fun! Here I’ll recap how the busy beaver game works so that we’re all on the same page.&lt;/p&gt;
    &lt;p&gt;I wrote above that the busy beaver numbers “measure the complexity of the craziest computations that simple computer programs can complete.” To define them more precisely, we first need a mathematical framework for gauging the complexity of computer programs themselves, to decide which ones are “simple.” Then we need a way to quantify the complexity of computations — what computer programs do — so that we can identify the craziest ones.&lt;/p&gt;
    &lt;p&gt;In the busy beaver game, computer programs are represented by hypothetical devices called Turing machines, which compute in discrete steps by reading and writing 0s and 1s on an infinite tape divided into cells. A unique list of rules governs the behavior of each Turing machine. Anything you can do with an ordinary computer program, you can in principle do with the right set of Turing machine rules.3In the busy beaver literature, these rules are called “states.” “In principle” is doing a lot of work in this sentence — even if you managed to acquire the requisite infinite tape, computing with a Turing machine would be horrendously inefficient. But Turing machines are easier to analyze theoretically than more practical programming languages.&lt;/p&gt;
    &lt;p&gt;Let’s unpack how Turing machines work in a bit more detail. At each step, a Turing machine consults one of its rules and edits one cell on the tape. Each rule has two cases: what to do if the current cell contains a 0, and what to do if it contains a 1. “What to do” here means what to write in the current cell, which direction to move next, and which rule to consult for the next step. One case of one rule breaks this pattern: It tells the Turing machine to “halt,” or stop running. But by itself, the existence of this instruction doesn’t guarantee that a Turing machine will halt — the machine might never get there. Quanta’s visual designer Kristina Armitage encapsulated all of this in a beautiful infographic.4In my first Busy Beaver story, you will also find animations of Turing machines in action.&lt;/p&gt;
    &lt;p&gt;The number of rules that a Turing machine has will be our measure of program complexity. This choice lets us replace our vague question about the craziest things that simple computer programs can do with a series of specific questions about different degrees of craziness, corresponding to different busy beaver numbers. You learn the value of BB(1) by answering the question “what’s the most complex computation that a one-rule Turing machine can complete?” Likewise, BB(2) measures the most complex computation that a two-rule machine can complete, and so on.&lt;/p&gt;
    &lt;p&gt;To answer these questions, we need a precise definition of what makes one computation more complex than another. A natural measure is how many steps the Turing machine needs to complete the computation. “Complete” is important — every Turing machine that never halts will run for infinitely many steps, but that’s not really a fair comparison. The number of steps that a Turing machine takes before halting (and indeed, whether it halts at all) can depend on the initial pattern of 0s and 1s on the tape. For the busy beaver game, we always start from the so-called “blank tape,” which has 0s in every cell.&lt;/p&gt;
    &lt;p&gt;We now have all the necessary pieces to formally define the busy beaver numbers. Let’s take BB(6) to be specific: It is the longest finite runtime among all six-rule Turing machines, when those machines start with a blank tape. Finding this number is straightforward in principle. First, list out all possible six-rule Turing machines. Next, sort them into two categories: those that will eventually halt when they start running on the blank tape, and those that will run forever. Toss out all the non-halting machines. Finally, measure how many steps each of the halting machines takes before stopping. The largest number is BB(6).&lt;/p&gt;
    &lt;p&gt;The problem with this plan lies in the second step, where you divide the Turing machines into two groups based on whether or not they halt. It turns out that deciding whether a Turing machine will halt can be an extremely hard problem, to put it mildly. And if you can’t tell whether a given machine will halt, then you don’t know whether your list of halting Turing machines is complete, so you can’t know whether you’ve found the longest runtime! As of this writing, researchers have classified the vast majority of six-rule machines as either halting or non-halting. But there are 1,618 “holdouts” whose fate remains unknown.&lt;/p&gt;
    &lt;p&gt;Antihydra is one of these holdout machines. To nail down the value of BB(6), researchers must first determine whether Antihydra halts, and that seems to be beyond the reach of any known mathematical technique. To understand why, we need to take a step back and ask, “what exactly are these Turing machines doing?”&lt;/p&gt;
    &lt;head rend="h2"&gt;Leveling Up&lt;/head&gt;
    &lt;p&gt;You may object at this point that we already know exactly what these Turing machines are doing: Each one is just following a specific sequence of rules, writing 0s and 1s on the tape as it goes. But this “low-level” description is a bit like saying “when I push these buttons, my pocket calculator toggles transistors on and off in this specific pattern.” That may very well be true, but “high-level” descriptions like “when I push these buttons, my pocket calculator multiplies 3 and 4” are usually more useful.&lt;/p&gt;
    &lt;p&gt;There’s no guarantee that any given Turing machine’s behavior admits such a simple high-level description.5Also, in many cases low-level descriptions are perfectly adequate. For example, the easiest way to prove that a Turing machine halts is just to simulate it step by step until it stops running. When that happens, you don’t need a deeper understanding of why it halted: Just note its runtime and move on. But remember that Turing machines can carry out all possible computations — that means that at least some Turing machines must be executing programs with high-level descriptions that humans can understand.&lt;/p&gt;
    &lt;p&gt;Actually, the most notable five- and six-rule Turing machines that busy beaver researchers have studied so far all have relatively simple high-level descriptions — that includes the longest-running five- and six-rule machines that eventually halt, the most complex non-halting five-rule machines, and holdouts like Antihydra.6This is an empirical observation, not a self-evident truth. In fact, some researchers expected that the longest-running Turing machines would be “spaghetti code” machines that lack any high-level description!&lt;/p&gt;
    &lt;p&gt;Let’s look at a specific example. The fifth busy beaver, which runs for 47,176,870 steps before halting, obeys the following low-level rules:&lt;/p&gt;
    &lt;p&gt;In 1993, the mathematician Pascal Michel proved that these rules are equivalent to a simple high-level program:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Set \(x = 0\).&lt;/item&gt;
      &lt;item&gt;Divide \(x\) by 3 and check the remainder. &lt;list rend="ul"&gt;&lt;item&gt;If the remainder is 0, calculate \((5x + 18)/3\). The result is your new value of \(x\).&lt;/item&gt;&lt;item&gt;If the remainder is 1, calculate \((5x + 22)/3\). The result is your new value of \(x\).&lt;/item&gt;&lt;item&gt;If the remainder is 2, halt.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;If you haven’t halted, go back to step 2 and plug in the new value of \(x\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once you have a high-level description like this, you can use it to determine whether the machine will halt — and if so, exactly how many steps it will take.7Each step in a high-level program like this one corresponds to many individual Turing machine steps. Whenever you prove an equivalence between high-level and low-level descriptions, you get formulas that you can use to compute how long each high-level step will take. I won’t say anything about how to actually prove these equivalences. In this case, the high-level program just repeatedly plugs in new values of \(x\) until it finds one that leaves a remainder of 2 when divided by 3. One third of numbers have this property, so you might guess that the program will take three tries to find one, give or take a few. If you start from a random value of \(x\), you’ll find that three iterations is indeed typical. But it turns out that if you start from \(x = 0\), this program will repeat the second step 15 times before it lands on a number with remainder 2! Busy beaver researchers often like to anthropomorphize the Turing machines they study, imagining that the machines are actively trying to run for as long as possible. Adopting that perspective, we might say that this Turing machine got very lucky.&lt;/p&gt;
    &lt;p&gt;The fifth busy beaver is just one member of a family of “Collatz-like” Turing machines whose high-level behavior has the following general form:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Set \(x\) equal to some starting value (which may or may not be 0).&lt;/item&gt;
      &lt;item&gt;Divide \(x\) by a fixed number \(N\). The remainder tells you what formula to use to get your new value of \(x\).&lt;/item&gt;
      &lt;item&gt;Check if you’ve met a specific halting condition. If not, go back to step 2 with the new value of \(x\).8As we saw in the above example, the halting condition can be as simple as “the remainder has a specific value.” Below we’ll see some examples with different halting conditions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The family of Collatz-like Turing machines includes both halting and non-halting machines. It gets its name from a procedure for generating number sequences devised in 1937 by the mathematician Lothar Collatz:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Choose a starting value for \(x\).&lt;/item&gt;
      &lt;item&gt;Check whether \(x\) is even or odd. &lt;list rend="ul"&gt;&lt;item&gt;If it’s even, calculate \(x/2\). The result is your new value of \(x\).&lt;/item&gt;&lt;item&gt;If it’s odd, calculate \(3x + 1\). The result is your new value of \(x\).&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Check whether \(x = 1\). If not, go back to step 2.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This looks very similar to our general description of high-level behavior for Collatz-like machines, with \(x = 1\) as the halting condition.9“Check whether \(x\) is even or odd” is just another way of saying “divide \(x\) by 2 and check the remainder.” Strictly speaking, we don’t have to specify that the sequence stops when \(x = 1\). But if we keep applying the rules after it hits 1, the sequence enters an infinite loop: 1 &amp;gt; 4 &amp;gt; 2 &amp;gt; 1 and so on. Try iterating these rules from any initial integer value of \(x\) — I’m willing to bet however much you like that you’ll eventually hit 1. The Collatz conjecture asserts that this happens for every positive integer, no matter how large. People have tested this empirically for all integers up to at least 2 billion trillion (!) without finding any counterexamples, which strongly suggests that the conjecture is true. But nobody knows how to rigorously prove it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cryptozoology&lt;/head&gt;
    &lt;p&gt;Let’s take a step back. At the beginning of this post I noted a link between the Collatz conjecture and Antihydra: Nobody knows how to prove the Collatz conjecture, and that’s why researchers don’t know how to conclusively determine whether Antihydra halts. But now I’ve instead linked the Collatz conjecture to the fifth busy beaver, a machine that has been proved to halt. What’s going on here?&lt;/p&gt;
    &lt;p&gt;The resolution to this apparent puzzle is that for the busy beaver game, we only care about whether a Turing machine halts when it starts running from a specific tape configuration, namely the blank tape. That means we only care about whether the corresponding Collatz-like sequence halts for a single input. The Collatz conjecture, meanwhile, asks whether you eventually hit \(x = 1\) for every input. It’s easy to show that the Collatz sequence ultimately hits \(x = 1\) for any one input, just as it’s easy to show that the fifth busy beaver halts (once you’ve established an equivalence between its low-level rules and the high-level Collatz-like program).10As it happens, the busy beaver hunters Heiner Marxen and Jürgen Buntrock first proved that the fifth busy beaver halted by direct simulation (albeit with some tricks to speed things up). Michel only identified its high-level behavior after the fact.&lt;/p&gt;
    &lt;p&gt;We can easily construct a variant of the Collatz problem that’s hard to solve even for a single input. All we need to do is change the \(3x + 1\) rule for odd numbers to \(5x + 1\). In that case, trajectories that start from certain inputs (such as \(x = 7\)) look like they will diverge, never hitting 1 or falling into a cycle. But researchers haven’t been able to prove that any of these trajectories diverges. There’s an inherent asymmetry here. If you want to prove that a sequence does eventually end up somewhere, you can always just use brute force, at least in principle. But if you want to prove that a sequence never terminates, even a single input can be hard.&lt;/p&gt;
    &lt;p&gt;We’re now finally ready to confront the terror that is Antihydra. It obeys the following high-level rules:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Set \(x = 8\).11This may seem like a weird starting point, given that we’re supposed to start with the blank tape in the busy beaver game. That’s still true here — it’s just that Antihydra spends a while futzing around on the tape before it starts iterating this sequence, and the high-level effect of all that futzing is to set the starting value to 8.&lt;/item&gt;
      &lt;item&gt;Check whether \(x\) is even or odd. &lt;list rend="ul"&gt;&lt;item&gt;If it’s even, calculate \(3x/2\). The result is your new value of \(x\). Add one to a running tally of how many times you’ve applied this even rule.&lt;/item&gt;&lt;item&gt;If it’s odd, calculate \((3x-1)/2\). The result is your new value of \(x\). Add one to a running tally of how many times you’ve applied this odd rule.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Check whether your “odd” count is more than twice as large as your “even” count. If so, halt. If not, go back to step 2.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is a very curious set of rules. The formulas \(3x/2\) and \((3x-1)/2\) don’t appear to systematically favor odd or even numbers, so you might expect that iterating them again and again will look like repeatedly flipping a coin and keeping track of how often you get heads versus tails. Early on in a sequence of coin flips, it’s distinctly possible that you’ll end up with more than twice as many heads as tails. But if this doesn’t happen right away, it becomes less and less likely the longer you keep going. Researchers have now simulated the behavior of Antihydra out to more than 270 billion steps, and as expected, the “even” and “odd” tallies are pretty close to equal — nowhere near the extreme imbalance demanded by the halting condition. So it seems overwhelmingly likely that Antihydra never halts. But nobody knows how to prove it! The mathematician John Conway coined the delightful term “probviously” for situations like this — ones where the specific problem of interest is very hard to solve, but probabilistic reasoning about the “typical” behavior of similar problems makes the answer seem obvious.&lt;/p&gt;
    &lt;p&gt;Antihydra’s behavior is qualitatively similar to the \(5x + 1\) version of the Collatz conjecture, where we don’t know how to prove that any single trajectory diverges. I want to stress that as far as researchers know, there isn’t a more precise mathematical link between these two problems: If you resolved one of them, it wouldn’t automatically resolve the other. But the problems seem hard for very similar reasons. If someone does manage to prove the Collatz conjecture, the mathematical techniques used in the proof would likely be promising for the Antihydra problem (and vice versa).&lt;/p&gt;
    &lt;p&gt;Actually, Antihydra is just one of many probviously non-halting Turing machines with Collatz-like behavior. Busy beaver hunter Shawn Ligocki dubbed these machines “cryptids” when they were first identified in variants of the standard busy beaver game.12These variants use extra tape symbols in addition to 0 and 1. For example, the BB(3,3) version of the busy beaver game studies the behavior of Turing machines with three rules that can read and write three symbols: 0, 1, and 2.&lt;/p&gt;
    &lt;p&gt;The first two cryptids to be discovered were named Bigfoot and Hydra;13Antihydra was named for a mathematical connection to Hydra. researchers have now identified so many cryptids that it no longer makes sense to give each one its own name. The existence of all these cryptids implies that busy beaver numbers beyond BB(5) will remain out of reach until researchers develop new mathematical tools for tackling Collatz-like problems. And the legendary mathematician Paul Erdős reportedly said “Mathematics may not be ready for such problems.”&lt;/p&gt;
    &lt;p&gt;But that doesn’t mean busy beaver hunters should give up. There’s still plenty of questions to explore in what might be called “cryptid ecology.” How many subspecies of cryptids are there? How are they related to each other, and to other unsolved problems in mathematics beyond the Collatz conjecture? Since the beginning of the busy beaver game, avid hunters have repeatedly encountered surprising new Turing machine behavior, and that pattern shows no sign of letting up.&lt;/p&gt;
    &lt;p&gt;This past August I visited Tahquamenon Falls in Michigan’s upper peninsula, a part of the state that’s apparently an epicenter of bigfoot sightings. Fortunately I didn’t encounter any cryptids, but I did learn some new things about a few friendlier critters. Surprising discoveries can come from anywhere!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45723359</guid><pubDate>Mon, 27 Oct 2025 16:56:04 +0000</pubDate></item><item><title>The new calculus of AI-based coding</title><link>https://blog.joemag.dev/2025/10/the-new-calculus-of-ai-based-coding.html</link><description>&lt;doc fingerprint="375020776f1115df"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Driving at 200mph&lt;/head&gt;
    &lt;p&gt;Here's where it gets interesting. A typical software team, even an experienced one, doesn't get things right all the time. Even with good testing and engineering practices, bugs occasionally make it through. We've all heard the phrase "testing in production." That reality is the main reason I've always believed that focusing on testing alone is not enough, and investing in blast radius and time to recovery is just as important.&lt;/p&gt;
    &lt;p&gt;AI assisted code is no different, it may contain bugs even when thoroughly reviewed by a human, and I suspect the probabilities are not significantly different. However, when teams ship commits at 10x the rate, the overall math changes. What used to be a production impacting bug once or twice a year, can become a weekly occurrence. Even if most bugs get caught in integration or testing environments, they will still impact the shared code base, requiring investigation and slowing the rest of the team down. Once again, this is not just hyperbole—our team sees signs that these are the challenges that pop up with a step function increase in throughput.&lt;/p&gt;
    &lt;p&gt;I am increasingly convinced that in order for agentic development to increase engineering velocity by an order of magnitude, we need to decrease the probability of problematic commits by an order of magnitude too. And likely by even more than that, since at high velocities individual commits can begin interacting with each other in unexpected ways too.&lt;/p&gt;
    &lt;p&gt;In other words, driving at 200mph, you need a lot of downforce to keep the car on the track!&lt;/p&gt;
    &lt;head rend="h2"&gt;The Cost-Benefit Rebalance&lt;/head&gt;
    &lt;p&gt;One of the best ways to reduce the chance of bugs is to improve testing. I'm an airplane geek, and have always admired the testing ideas used by the airplane manufacturers. From early simulations, to component testing, to wind tunnel testing, to testing to breaking point, and ultimately test flights of fully assembled aircraft. Even flight simulators play a role in improving the overall safety of the industry. Some of these ideas have been tried in the software industry, but they are far from ubiquitous.&lt;/p&gt;
    &lt;p&gt;As an example, I've always liked "wind tunnel" style tests, that test fully assembled system in a controlled environment. To achieve that, one pattern I've used is implementing high fidelity "fake" versions of external dependencies that can be run locally. If you do that, you can then write build-time tests that run locally and verify end-to-end behavior of the whole system. You can even inject unexpected behaviors and failures into fake dependencies, to test how the system handles them. Such tests are easy to write and execute because they run locally, and they are great at catching those sneaky bugs in the seams between components.&lt;/p&gt;
    &lt;p&gt;Unfortunately, faking all the external dependencies isn't always easy for a service with moderate level of complexity. And even if you do, you now have to own keeping up with the real dependencies as they evolve. For those reasons, in my experience most teams don't write such tests.&lt;/p&gt;
    &lt;p&gt;I think we are seeing early signs that agentic coding can change the calculus here. AI agents are great at spitting out large volumes of code, especially when the desired behavior is well known and there's little ambiguity. Ideas that were sound in principle, but too expensive to implement and maintain just had their costs decrease by an order of magnitude. I really love riding such shifts in the industry, because they open the doors to new approaches that weren't practical in the past.&lt;/p&gt;
    &lt;p&gt;Our project (with the help of an AI agent) maintains fake implementations of external dependencies like authentication, storage, chain replication, and inference engine to be used in tests. We then wrote a test harness that uses those fakes to spin up our entire distributed system, including all the micro-services, on developers' machines. Build-time tests then spin up our canaries against that fully assembled stack verifying the system as a whole works.&lt;/p&gt;
    &lt;p&gt;I'm really bullish on this approach catching a category of bugs that in the past could only be caught once the change was committed and made it to the test environment. A few years ago, ideas like these would receive resistance as nice, but too expensive. This time around, it took just a few days to implement for a relatively complex system.&lt;/p&gt;
    &lt;head rend="h2"&gt;Driving Fast Requires Tighter Feedback Loop&lt;/head&gt;
    &lt;p&gt;Agentic coding changes that dynamic. In the amount of time it takes to build, package, and test one set of commits, another dozen might be waiting to go out. By the time a change set is ready to deploy to production, it may contain 100 or more commits. And if one of those commits contains a problem, the deployment needs to be rolled back grinding the pipeline to a halt. In the meantime, even more changes accumulate, adding to the chaos and the risk.&lt;/p&gt;
    &lt;p&gt;I'm a Formula 1 fan, and this reminds me of how an accident on the track can cause a Yellow Flag to be raised. Normally, the cars zoom around the track at immense speeds and accelerations. But if an accident occurs, the race marshals raise a yellow flag, which requires all the cars to slow down behind the pace car. An exciting race turns into a leisurely drive around the track until the debris is cleaned up and the track is safe again. To minimize such slow downs, race organizers go to great lengths to prepare for all types of accidents, and make sure they can clean up the track and restart the race in minutes.&lt;/p&gt;
    &lt;p&gt;Just like whole-system local tests help tighten the feedback loop for catching certain bugs, we may need to think similarly about how we implement our CICD pipelines. When teams are moving at the speed of dozen of commits per hour, problematic issues will need to be identified, isolated, and reverted in minutes instead of hours or days. That means that a typical build and test infrastructure will need to become an order of magnitude faster than it is today. Just like online video games become unplayable when there is high lag between player's inputs and the game's reaction, it's really hard to move 10x faster if every commit still requires a lengthy delay before you see the feedback.&lt;/p&gt;
    &lt;head rend="h2"&gt;The communication bottleneck&lt;/head&gt;
    &lt;p&gt;I enjoy observing well-run operations. If you've ever peeked behind the curtain of a busy restaurant, then at first sight you may think it's chaos. But if you take a second to notice the details, you'll see that all members are constantly coordinating with each other. Chefs, cooks, wait staff, bussers, and managers pass information back and forth in a continuous stream. By staying in constant sync, a well run restaurant manages to serve its patrons even during peak times, without sacrificing on quality or latency.&lt;/p&gt;
    &lt;p&gt;I believe that achieving similar increase in velocity for a software team requires constraints on how teams communicate. When your throughput increases by an order of magnitude, you're not just writing more code - you're making more decisions. Should we use this caching strategy or that one? How should we handle this edge case? What's the right abstraction here? At normal velocity, a team might make one or two of these decisions per week. At 10x velocity, they are making multiple each day.&lt;/p&gt;
    &lt;p&gt;The challenge is that many of these decisions impact what others are working on. Engineer A decides to refactor the authentication flow, which affects the API that Engineer B is about to extend. These aren't just implementation details - they're architectural choices that ripple through the codebase.&lt;/p&gt;
    &lt;p&gt;I find that traditional coordination mechanisms introduce too much latency here. Waiting for a Slack response or scheduling a quick sync for later in the day means either creating a bottleneck - the decision blocks progress - or risking going down the wrong path before realizing the conflict. At high throughput, the cost of coordination can dominate!&lt;/p&gt;
    &lt;p&gt;One approach is to eliminate coordination - if everybody works on independent components, they are unlikely to need to coordinate. But I find that ideal impractical in most real-world systems. So another alternative is to significantly decrease the cost of coordination. Our team sits on the same floor, and I think that's been critical to our velocity. When someone needs to make a decision that might impact others, they can walk over and hash it out in minutes in front of a whiteboard. We align on the approach, discuss trade-offs in real time, and both engineers get back to work. The decision gets made quickly, correctly, and without creating a pile-up of blocked work.&lt;/p&gt;
    &lt;p&gt;I recognize this doesn't solve the problem for distributed teams—that remains an open challenge.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Path Forward&lt;/head&gt;
    &lt;p&gt;I'm really excited about the potential of agentic development. I think it has the capability to not only improve the efficiency of software development, but also allow us to tackle problems that were previously too niche or expensive to solve. The gains are real - our team's 10x throughput increase isn't theoretical, it's measurable.&lt;/p&gt;
    &lt;p&gt;But here's the critical part: these gains won't materialize if we simply bolt AI agents onto our existing development practices. Like adding a turbocharger to a car with narrow tires and old brakes, the result won't be faster lap times - it will be crashes. At 10x code velocity, our current approaches to testing, deployment, and team coordination become the limiting factors. The bottleneck just moves.&lt;/p&gt;
    &lt;p&gt;This means we need to fundamentally rethink how we approach building software. CICD pipelines designed for 10 commits per day will buckle under 100. Testing strategies that were "good enough" at normal velocity will let too many bugs through at high velocity. Communication patterns that worked fine before will create constant pile-ups of blocked work.&lt;/p&gt;
    &lt;p&gt;The good news is that we already have great ideas for comprehensive testing, rapid deployment, and efficient coordination - ideas that have shown promise but haven't seen wide adoption because they were too expensive to implement and maintain. What's changed is that agentic development itself can dramatically lower those costs. The same AI agents that are increasing our code throughput can also help us build the infrastructure needed to sustain that throughput.&lt;/p&gt;
    &lt;p&gt;This is the real opportunity: not just writing more code faster, but using AI to make previously impractical engineering practices practical. The teams that succeed with agentic development will be the ones who recognize that the entire software development lifecycle needs to evolve in concert.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45723686</guid><pubDate>Mon, 27 Oct 2025 17:17:38 +0000</pubDate></item><item><title>MCP-Scanner – Scan MCP Servers for vulnerabilities</title><link>https://github.com/cisco-ai-defense/mcp-scanner</link><description>&lt;doc fingerprint="1d5afd5c98027a90"&gt;
  &lt;main&gt;
    &lt;p&gt;A Python tool for scanning MCP (Model Context Protocol) servers and tools for potential security findings. The MCP Scanner combines Cisco AI Defense inspect API, YARA rules and LLM-as-a-judge to detect malicious MCP tools.&lt;/p&gt;
    &lt;p&gt;The MCP Scanner provides a comprehensive solution for scanning MCP servers and tools for security findings. It leverages three powerful scanning engines (Yara, LLM-as-judge, Cisco AI Defense) that can be used together or independently.&lt;/p&gt;
    &lt;p&gt;The SDK is designed to be easy to use while providing powerful scanning capabilities, flexible authentication options, and customization.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multiple Modes: Run scanner as a stand-alone CLI tool or REST API server&lt;/item&gt;
      &lt;item&gt;Multi-Engine Security Analysis: Use all three scanning engines together or independently based on your needs.&lt;/item&gt;
      &lt;item&gt;Comprehensive Scanning: Scan MCP tools, prompts, and resources for security findings&lt;/item&gt;
      &lt;item&gt;Explicit Authentication Control: Fine-grained control over authentication with explicit Auth parameters.&lt;/item&gt;
      &lt;item&gt;OAuth Support: Full OAuth authentication support for both SSE and streamable HTTP connections.&lt;/item&gt;
      &lt;item&gt;Custom Endpoints: Configure the API endpoint to support any Cisco AI Defense environments.&lt;/item&gt;
      &lt;item&gt;MCP Server Integration: Connect directly to MCP servers to scan tools, prompts, and resources with flexible authentication.&lt;/item&gt;
      &lt;item&gt;Customizable YARA Rules: Add your own YARA rules to detect specific patterns.&lt;/item&gt;
      &lt;item&gt;Comprehensive Reporting: Detailed reports on detected security findings.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Python 3.11+&lt;/item&gt;
      &lt;item&gt;uv (Python package manager)&lt;/item&gt;
      &lt;item&gt;A valid Cisco AI Defense API Key (optional)&lt;/item&gt;
      &lt;item&gt;LLM Provider API Key (optional)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;uv venv -p &amp;lt;Python version less than or equal to 3.13&amp;gt; /path/to/your/choice/of/venv/directory
source /path/to/your/choice/of/venv/directory/bin/activate
uv pip install cisco-ai-mcp-scanner&lt;/code&gt;
    &lt;code&gt;git clone https://github.com/cisco-ai-defense/mcp-scanner
cd mcp-scanner
# Install with uv (recommended)

uv venv -p &amp;lt;Python version less than or equal to 3.13&amp;gt; /path/to/your/choice/of/venv/directory

source /path/to/your/choice/of/venv/directory/bin/activate

uv pip install .
# Or install in development mode
uv pip install -e .&lt;/code&gt;
    &lt;code&gt;Cisco AI Defense API (only required for API analyzer)
export MCP_SCANNER_API_KEY="your_cisco_api_key"
export MCP_SCANNER_ENDPOINT="https://us.api.inspect.aidefense.security.cisco.com/api/v1"
# For other endpoints please visit https://developer.cisco.com/docs/ai-defense/getting-started/#base-url&lt;/code&gt;
    &lt;p&gt;Tested LLMs: OpenAI GPT-4o and GPT-4.1&lt;/p&gt;
    &lt;code&gt;# AWS Bedrock Claude with AWS credentials (profile)
export AWS_PROFILE="your-profile"
export AWS_REGION="us-east-1"
export MCP_SCANNER_LLM_MODEL="bedrock/anthropic.claude-sonnet-4-5-20250929-v2:0" # Any AWS Bedrock supported model

# AWS Bedrock Claude with API key (Bearer token)
export MCP_SCANNER_LLM_API_KEY="bedrock-api-key-..." # Generated via Amazon Bedrock -&amp;gt; API Keys
export AWS_REGION="us-east-1"
export MCP_SCANNER_LLM_MODEL="bedrock/us.anthropic.claude-sonnet-4-5-20250929-v2:0" # Any AWS Bedrock supported model

# LLM Provider API Key (required for LLM analyzer)
export MCP_SCANNER_LLM_API_KEY="your_llm_api_key"  # OpenAI

# LLM Model Configuration (optional - defaults provided)
export MCP_SCANNER_LLM_MODEL="gpt-4o"  # Any LiteLLM-supported model
export MCP_SCANNER_LLM_BASE_URL="https://api.openai.com/v1"  # Custom LLM endpoint
export MCP_SCANNER_LLM_API_VERSION="2024-02-01"  # API version (if required)

# For Azure OpenAI (example)
export MCP_SCANNER_LLM_BASE_URL="https://your-resource.openai.azure.com/"
export MCP_SCANNER_LLM_API_VERSION="2024-02-01"
export MCP_SCANNER_LLM_MODEL="azure/gpt-4"

# For Extended Thinking Models (longer timeout)
export MCP_SCANNER_LLM_TIMEOUT=300&lt;/code&gt;
    &lt;p&gt;If you are using a local LLM endpoint such as Ollama, vLLM, or LocalAI, the &lt;code&gt;MCP_SCANNER_LLM_API_KEY&lt;/code&gt; variable is still required but can be set to any value.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;export MCP_SCANNER_LLM_API_KEY=test
export MCP_SCANNER_LLM_ENDPOINT=http://localhost:11434&lt;/code&gt;
    &lt;p&gt;The fastest way to get started is using the &lt;code&gt;mcp-scanner&lt;/code&gt; CLI command. Global flags (like &lt;code&gt;--analyzers&lt;/code&gt;, &lt;code&gt;--format&lt;/code&gt;, etc.) must be placed before a subcommand.&lt;/p&gt;
    &lt;code&gt;# Scan well-known client configs on this machine
mcp-scanner --scan-known-configs --analyzers yara --format summary

# Stdio server (example using uvx mcp-server-fetch)
mcp-scanner --stdio-command uvx --stdio-arg=--from --stdio-arg=mcp-server-fetch --stdio-arg=mcp-server-fetch --analyzers yara --format summary

# Remote server (deepwiki example)
mcp-scanner --server-url https://mcp.deepwki.com/mcp --analyzers yara --format summary

# MCP Scanner as REST API
mcp-scanner-api --host 0.0.0.0 --port 8080
&lt;/code&gt;
    &lt;code&gt;import asyncio
from mcpscanner import Config, Scanner
from mcpscanner.core.models import AnalyzerEnum

async def main():
    # Create configuration with your API keys
    config = Config(
        api_key="your_cisco_api_key",
        llm_provider_api_key="your_llm_api_key"
    )

    # Create scanner
    scanner = Scanner(config)

    # Scan all tools on a remote server
    tool_results = await scanner.scan_remote_server_tools(
        "https://mcp.deepwki.com/mcp",
        analyzers=[AnalyzerEnum.API, AnalyzerEnum.YARA, AnalyzerEnum.LLM]
    )

    # Print tool results
    for result in tool_results:
        print(f"Tool: {result.tool_name}, Safe: {result.is_safe}")

    # Scan all prompts on a server
    prompt_results = await scanner.scan_remote_server_prompts(
        "http://127.0.0.1:8000/mcp",
        analyzers=[AnalyzerEnum.LLM]
    )

    # Print prompt results
    for result in prompt_results:
        print(f"Prompt: {result.prompt_name}, Safe: {result.is_safe}")

    # Scan all resources on a server
    resource_results = await scanner.scan_remote_server_resources(
        "http://127.0.0.1:8000/mcp",
        analyzers=[AnalyzerEnum.LLM],
        allowed_mime_types=["text/plain", "text/html"]
    )

    # Print resource results
    for result in resource_results:
        print(f"Resource: {result.resource_name}, Safe: {result.is_safe}, Status: {result.status}")

# Run the scanner
asyncio.run(main())&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;remote: scan a remote MCP server (SSE or streamable HTTP). Supports &lt;code&gt;--server-url&lt;/code&gt;, optional&lt;code&gt;--bearer-token&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;stdio: launch and scan a stdio MCP server. Requires &lt;code&gt;--stdio-command&lt;/code&gt;; accepts&lt;code&gt;--stdio-args&lt;/code&gt;,&lt;code&gt;--stdio-env&lt;/code&gt;, optional&lt;code&gt;--stdio-tool&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;config: scan servers from a specific MCP config file. Requires &lt;code&gt;--config-path&lt;/code&gt;; optional&lt;code&gt;--bearer-token&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;known-configs: scan servers from well-known client config locations on this machine; optional &lt;code&gt;--bearer-token&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;prompts: scan prompts on an MCP server. Requires &lt;code&gt;--server-url&lt;/code&gt;; optional&lt;code&gt;--prompt-name&lt;/code&gt;,&lt;code&gt;--bearer-token&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;resources: scan resources on an MCP server. Requires &lt;code&gt;--server-url&lt;/code&gt;; optional&lt;code&gt;--resource-uri&lt;/code&gt;,&lt;code&gt;--mime-types&lt;/code&gt;,&lt;code&gt;--bearer-token&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Top-level flags (e.g., &lt;code&gt;--server-url&lt;/code&gt;, &lt;code&gt;--stdio-*&lt;/code&gt;, &lt;code&gt;--config-path&lt;/code&gt;, &lt;code&gt;--scan-known-configs&lt;/code&gt;) remain supported when no subcommand is used, but subcommands are recommended.&lt;/p&gt;
    &lt;code&gt;# YARA-only scan of all servers defined in well-known config locations
mcp-scanner --scan-known-configs --analyzers yara --format summary

# Detailed output
mcp-scanner --scan-known-configs --analyzers yara --detailed&lt;/code&gt;
    &lt;code&gt;# Expand ~ yourself if needed by your shell
mcp-scanner --config-path "$HOME/.codeium/windsurf/mcp_config.json" \
 --analyzers yara --format by_tool&lt;/code&gt;
    &lt;code&gt;# Use repeated --stdio-arg for reliable argument passing
mcp-scanner --analyzers yara --format summary \
  stdio --stdio-command uvx \
  --stdio-arg=--from --stdio-arg=mcp-server-fetch --stdio-arg=mcp-server-fetch

# Or list-form (ensure it doesn't conflict with later flags)
mcp-scanner --analyzers yara --detailed \
  stdio --stdio-command uvx \
  --stdio-args --from mcp-server-fetch mcp-server-fetch

# Scan only a specific tool on the stdio server
mcp-scanner --analyzers yara --format summary \
  stdio --stdio-command uvx \
  --stdio-arg=--from --stdio-arg=mcp-server-fetch --stdio-arg=mcp-server-fetch \
  --stdio-tool fetch&lt;/code&gt;
    &lt;code&gt;# Direct remote server with Bearer token
mcp-scanner --analyzers yara --format summary \
  remote --server-url https://your-mcp-server/sse --bearer-token "$TOKEN"

# Apply Bearer token to all remote servers discovered from configs
mcp-scanner --analyzers yara --detailed known-configs --bearer-token "$TOKEN"
mcp-scanner --analyzers yara --format by_tool \
  config --config-path "$HOME/.codeium/windsurf/mcp_config.json" --bearer-token "$TOKEN"&lt;/code&gt;
    &lt;code&gt;# Scan all prompts on an MCP server
mcp-scanner --analyzers llm prompts --server-url http://127.0.0.1:8000/mcp

# Scan all prompts with detailed output
mcp-scanner --analyzers llm --detailed prompts --server-url http://127.0.0.1:8000/mcp

# Scan all prompts with table format
mcp-scanner --analyzers llm --format table prompts --server-url http://127.0.0.1:8000/mcp

# Scan a specific prompt by name
mcp-scanner --analyzers llm prompts --server-url http://127.0.0.1:8000/mcp --prompt-name "greet_user"

# Get raw JSON output
mcp-scanner --analyzers llm --raw prompts --server-url http://127.0.0.1:8000/mcp&lt;/code&gt;
    &lt;code&gt;# Scan all resources on an MCP server
mcp-scanner --analyzers llm resources --server-url http://127.0.0.1:8000/mcp

# Scan all resources with detailed output
mcp-scanner --analyzers llm --detailed resources --server-url http://127.0.0.1:8000/mcp

# Scan all resources with table format
mcp-scanner --analyzers llm --format table resources --server-url http://127.0.0.1:8000/mcp

# Scan a specific resource by URI
mcp-scanner --analyzers llm resources --server-url http://127.0.0.1:8000/mcp \
  --resource-uri "file://test/document.txt"

# Scan with custom MIME type filtering
mcp-scanner --analyzers llm resources --server-url http://127.0.0.1:8000/mcp \
  --mime-types "text/plain,text/html,application/json"&lt;/code&gt;
    &lt;p&gt;The API server provides a REST interface to the MCP scanner functionality, allowing you to integrate security scanning into web applications, CI/CD pipelines, or other services. It exposes the same scanning capabilities as the CLI tool but through HTTP endpoints.&lt;/p&gt;
    &lt;code&gt;# Start the API server (loads configuration from .env file)
mcp-scanner-api --port 8000

# Or with custom host and port
mcp-scanner-api --host 0.0.0.0 --port 8080

# Enable development mode with auto-reload
mcp-scanner-api --reload&lt;/code&gt;
    &lt;p&gt;Once running, the API server provides endpoints for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/scan-tool&lt;/code&gt;- Scan a specific tool on an MCP server&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/scan-all-tools&lt;/code&gt;- Scan all tools on an MCP server&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/scan-prompt&lt;/code&gt;- Scan a specific prompt on an MCP server&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/scan-all-prompts&lt;/code&gt;- Scan all prompts on an MCP server&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/scan-resource&lt;/code&gt;- Scan a specific resource on an MCP server&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/scan-all-resources&lt;/code&gt;- Scan all resources on an MCP server&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/health&lt;/code&gt;- Health check endpoint&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Documentation is available in docs/api-reference.md or as interactive documentation at &lt;code&gt;http://localhost:8000/docs&lt;/code&gt; when the server is running.&lt;/p&gt;
    &lt;p&gt;The scanner supports multiple output formats:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;summary&lt;/code&gt;: Concise overview with key findings&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;detailed&lt;/code&gt;: Comprehensive analysis with full findings breakdown&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;table&lt;/code&gt;: Clean tabular format&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;by_severity&lt;/code&gt;: Results grouped by severity level&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;raw&lt;/code&gt;: Raw JSON output&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;mcp-scanner --server-url http://127.0.0.1:8001/sse --format detailed&lt;/code&gt;
    &lt;code&gt;=== MCP Scanner Detailed Results ===

Scan Target: http://127.0.0.1:8001/sse

Tool: execute_system_command
Status: completed
Safe: No
Analyzer Results:
  • api_analyzer:
    - Severity: HIGH
    - Threat Summary: Detected 1 threat: security violation
    - Threat Names: SECURITY VIOLATION
    - Total Findings: 1
  • yara_analyzer:
    - Severity: HIGH
    - Threat Summary: Detected 2 threats: system access, command injection
    - Threat Names: SECURITY VIOLATION, SUSPICIOUS CODE EXECUTION
    - Total Findings: 2
  • llm_analyzer:
    - Severity: HIGH
    - Threat Summary: Detected 2 threats: prompt injection, tool poisoning
    - Threat Names: PROMPT INJECTION, SUSPICIOUS CODE EXECUTION
    - Total Findings: 2
&lt;/code&gt;
    &lt;code&gt;mcp-scanner --server-url http://127.0.0.1:8002/sse --format table&lt;/code&gt;
    &lt;code&gt;=== MCP Scanner Results Table ===

Scan Target: http://127.0.0.1:8002/sse

Scan Target                   Tool Name     Status     API      YARA     LLM      Severity
-----------------------------------------------------------------------------------------
http://127.0.0.1:8002/sse     exec_secrets  UNSAFE     HIGH     HIGH     HIGH     HIGH
http://127.0.0.1:8002/sse     safe_command  SAFE       SAFE     SAFE     SAFE     SAFE
&lt;/code&gt;
    &lt;p&gt;For detailed documentation, see the docs/ directory:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Architecture - System architecture and components&lt;/item&gt;
      &lt;item&gt;Authentication - OAuth and security configuration&lt;/item&gt;
      &lt;item&gt;Programmatic Usage - Programmatic usage examples and advanced usage&lt;/item&gt;
      &lt;item&gt;API Reference - Complete REST API documentation&lt;/item&gt;
      &lt;item&gt;Output Formats - Detailed output format options&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;https://www.cisco.com/site/us/en/products/security/ai-defense/index.html&lt;/p&gt;
    &lt;p&gt;Distributed under the &lt;code&gt;Apache 2.0&lt;/code&gt; License. See LICENSE for more information.&lt;/p&gt;
    &lt;p&gt;Project Link: https://github.com/cisco-ai-defense/mcp-scanner&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45723699</guid><pubDate>Mon, 27 Oct 2025 17:18:39 +0000</pubDate></item><item><title>Creating an all-weather driver</title><link>https://waymo.com/blog/2025/10/creating-an-all-weather-driver</link><description>&lt;doc fingerprint="39afdc53bfb06cf0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Creating an all-weather Driver&lt;/head&gt;
    &lt;p&gt;Life doesn't freeze when winter comes—if anything, that's when riders need reliable transportation most, when being exposed to the elements becomes less appealing. Today, the Waymo Driver successfully navigates rain, fog, sandstorms, and freezing temperatures. As we expand to more cities across the U.S. and globally, we're applying the same systematic, scientific approach that enabled us to validate the Waymo Driver for these conditions to advance our capabilities for snowier, winter weather.&lt;/p&gt;
    &lt;p&gt;Our proven, safety-guided methodology involves four key steps:&lt;/p&gt;
    &lt;p&gt;Understanding the Challenge&lt;/p&gt;
    &lt;p&gt;Snow isn't a single phenomenon—it's a spectrum of conditions that can affect a human or autonomous driver in multiple ways. Atmospheric conditions can range from a light dusting to a complete whiteout, while road surfaces may be snow-covered or have icy patches, and environmental factors like snow buildup along roadsides add further complexity. For years, we've been advancing our system in some of the snowiest conditions across the country —regularly driving in Upstate New York, Michigan's Upper Peninsula, and the Sierra. We've amassed tens of thousands of miles in diverse, snowy conditions. This has allowed the Waymo Driver’s AI to learn from real driving experience and train to navigate a wide range of winter weather.&lt;/p&gt;
    &lt;p&gt;Designing Generalizable Solutions&lt;/p&gt;
    &lt;p&gt;At Waymo, we're building one autonomous system that works across diverse conditions—the same Waymo Driver navigating foggy San Francisco can navigate snowy Denver. Our 6th-generation Driver is informed by over 100 million fully autonomous miles of driving experience, combining state-of-the-art hardware and AI to adapt to and sustain fully autonomous operations in cities with harsher weather.&lt;/p&gt;
    &lt;p&gt;The Waymo Driver uses cameras, radar, and lidar to perceive the world around it, with each sensor providing a complementary field of view that's especially helpful in inclement weather. Its automated cleaning system –using clever engineering and heating elements – keeps the sensors clear so the vehicle can continue serving riders without needing to pull over.&lt;/p&gt;
    &lt;p&gt;Our system provides context not only about where it's operating, but also about the conditions it’s operating under. We're creating state-of-the-art AI, building on top of our existing models with richer inputs and advanced capabilities designed to navigate winter conditions. For example, our AI can distinguish between where there's snow, slush, ice, and normal road surface. The Waymo Driver then uses this information to adjust its driving behavior to match the road conditions in real-time, allowing the Waymo Driver to navigate based on what it sees (and feels), also inferring insights from other road users—adapting to blocked roads, detours, and changing surface conditions. When the system detects lower traction, it automatically adjusts its speed, acceleration, and braking. Each vehicle essentially acts as a mobile weather station, gathering data to inform its own driving decisions and share with the rest of the fleet in the city. These responses are consistent and thoroughly tested, providing predictable and safe navigation in challenging conditions.&lt;/p&gt;
    &lt;p&gt;Rigorously Validating Our Capabilities&lt;/p&gt;
    &lt;p&gt;We validate our generalizable system through real-world driving, closed-course testing, and large-scale simulation. With our growing operations in snowy cities like Detroit, Denver, and Washington D.C., in addition to visits to other areas, we're deepening our understanding of winter weather conditions and validating our capabilities. At closed-course testing facilities, we push the system to its limits in controlled environments, teaching it to recognize and respond to extreme scenarios like losing traction on ice. Then, we expand our learning year-round through simulation, long after the last snowflake has melted, so the Waymo Driver is prepared for rare and unusual events, like once-in-100-year snow New Orleans experienced this past winter.&lt;/p&gt;
    &lt;p&gt;Scaling Responsibly&lt;lb/&gt;Once we've validated our technology and operations by our Safety Framework and high caliber for rider excellence, we expand our service with clear guidelines about when our vehicles will operate based on local conditions. As we scale, we're also refining our operations to support winter service—from keeping our fleet clean and charged in freezing temperatures to optimizing the rider experience. Winter weather is complex, but we're committed to providing reliable service when riders need it most. As we continue expanding to more cities around the world, our progress is guided by safety, and riders can trust that the Waymo Driver is ready when we open our doors.&lt;lb/&gt;Looking for an all-weather Driver instead of all-weather tires? Follow along on our progress to bring Waymo to more cities at waymo.com/updates.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45724913</guid><pubDate>Mon, 27 Oct 2025 18:57:57 +0000</pubDate></item><item><title>Study finds growing social circles may fuel polarization</title><link>https://phys.org/news/2025-10-friends-division-social-circles-fuel.html</link><description>&lt;doc fingerprint="b71dc3835465878d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;More friends, more division: Study finds growing social circles may fuel polarization&lt;/head&gt;
    &lt;head rend="h5"&gt;Sadie Harley&lt;/head&gt;
    &lt;p&gt;scientific editor&lt;/p&gt;
    &lt;head rend="h5"&gt;Robert Egan&lt;/head&gt;
    &lt;p&gt;associate editor&lt;/p&gt;
    &lt;p&gt;Between 2008 and 2010, polarization in society increased dramatically alongside a significant shift in social behavior: the number of close social contacts rose from an average of two to four or five people. The connection between these two developments could provide a fundamental explanation for why societies around the world are increasingly fragmenting into ideological bubbles.&lt;/p&gt;
    &lt;p&gt;"The big question that not only we, but many countries are currently grappling with, is why polarization has increased so dramatically in recent years," says Stefan Thurner from the Complexity Science Hub (CSH), explaining the study's motivation. The research was published in Proceedings of the National Academy of Sciences.&lt;/p&gt;
    &lt;p&gt;The researchers' findings confirm that increasing polarization is not merely perceived—it is measurable and objectively occurring. "And this increase happened suddenly, between 2008 and 2010," says Thurner. The question remained: what caused it?&lt;/p&gt;
    &lt;head rend="h2"&gt;The friendship shift: From two to five close contacts&lt;/head&gt;
    &lt;p&gt;To investigate, Thurner and his team examined whether social networks had changed—specifically, whether people's close friendships had shifted. "For decades, sociological studies showed that people maintained an average of about two close friends—people who could influence their opinions on important issues," explains Thurner.&lt;/p&gt;
    &lt;p&gt;Here too, the researchers identified a striking change: "Around 2008, there was a sharp increase from an average of two close friends to four or five," explains CSH scientist Jan Korbel.&lt;/p&gt;
    &lt;head rend="h2"&gt;The paradox: More connection, more division&lt;/head&gt;
    &lt;p&gt;Are these two developments related? Do more close friends—and thus denser social networks—lead to network fragmentation and ultimately societal polarization?&lt;/p&gt;
    &lt;p&gt;Using a model based on real data, the researchers discovered this could indeed be the case: "When network density increases with more connections, polarization within the collective inevitably rises sharply," says Markus Hofer from CSH.&lt;/p&gt;
    &lt;p&gt;"This finding impressed us greatly because it could provide a fundamental explanation for the peculiar form of polarization we're currently observing simultaneously across many parts of the world—one that definitely threatens democracy," Thurner continues.&lt;/p&gt;
    &lt;p&gt;"When people are more connected with each other, they encounter different opinions more frequently. This inevitably leads to more conflict and thus greater societal polarization," adds Korbel.&lt;/p&gt;
    &lt;p&gt;Polarization has always existed, but what is happening now goes far beyond historical patterns. Greater connectivity has led to the formation of fewer but more tightly-knit groups with strongly differing opinions, between which there is hardly any exchange.&lt;/p&gt;
    &lt;p&gt;"There are few bridges between these 'bubbles,' and when they exist, they are often negative or even hostile," says Korbel. "This is called fragmentation, and it represents a new social phenomenon," adds Thurner.&lt;/p&gt;
    &lt;head rend="h2"&gt;Behind the numbers: Tracking polarization through decades of data&lt;/head&gt;
    &lt;p&gt;For their study, the researchers analyzed extensive existing survey data on both polarization and social networks.&lt;/p&gt;
    &lt;p&gt;"To measure political polarization, we used over 27,000 surveys from the Pew Research Center, which regularly records political attitudes of people in the US," explains Hofer.&lt;/p&gt;
    &lt;p&gt;"The key advantage of this data is that the questions have remained virtually unchanged over time, enabling reliable long-term comparisons."&lt;/p&gt;
    &lt;p&gt;The researchers found that political attitudes became significantly more one-sided between 1999 and 2017. For example, only 14% of respondents consistently expressed liberal views in 1999, but by 2017, this had risen to 31%. Conversely, only 6% of respondents consistently held conservative views in 1999, compared to 16% in 2017.&lt;/p&gt;
    &lt;p&gt;"More and more people are clearly aligning themselves with one political camp rather than holding a mixture of liberal and conservative views," explains Hofer.&lt;/p&gt;
    &lt;p&gt;To analyze friendship networks, the researchers combined 30 different surveys totaling over 57,000 respondents from Europe and the US, including the General Social Survey (US) and the European Social Survey.&lt;/p&gt;
    &lt;p&gt;"Despite minor differences between individual surveys, the data consistently show that the average number of close friendships rose from 2.2 in 2000 to 4.1 in 2024," says Hofer.&lt;/p&gt;
    &lt;p&gt;"The decisive contribution of this study is that it reconciled both phenomena using a mathematical social model," explains Thurner.&lt;/p&gt;
    &lt;p&gt;"This enabled us to show that increasing connectivity must lead to sudden polarization once a critical connectivity density is exceeded—just like a phase transition in physics, such as water turning to ice," adds Hofer.&lt;/p&gt;
    &lt;p&gt;"It is fascinating that these phase transitions also exist in societies. The exact location of these critical thresholds still needs clarification. According to our results, for close relationships, it lies somewhere between three and four people," the researchers note.&lt;/p&gt;
    &lt;head rend="h2"&gt;The smartphone era: When connection may have become fragmentation&lt;/head&gt;
    &lt;p&gt;The sharp rise in both polarization and the number of close friends occurred between 2008 and 2010—precisely when social media platforms and smartphones first achieved widespread adoption. This technological shift may have fundamentally changed how people connect with each other, indirectly promoting polarization.&lt;/p&gt;
    &lt;p&gt;"Democracy depends on all parts of society being involved in decision-making, which requires that everyone be able to communicate with each other. But when groups can no longer talk to each other, this democratic process breaks down," emphasizes Stefan Thurner.&lt;/p&gt;
    &lt;p&gt;Tolerance plays a central role. "If I have two friends, I do everything I can to keep them—I am very tolerant towards them. But if I have five and things become difficult with one of them, it's easier to end that friendship because I still have 'backups.' I no longer need to be as tolerant," explains Thurner.&lt;/p&gt;
    &lt;p&gt;What disappears as a result is a societal baseline of tolerance—a development that could contribute to the long-term erosion of democratic structures. To prevent societies from increasingly fragmenting, Thurner emphasizes the importance of learning early how to engage with different opinions and actively cultivating tolerance.&lt;/p&gt;
    &lt;p&gt;More information: Thurner, Stefan, Why more social interactions lead to more polarization in societies, Proceedings of the National Academy of Sciences (2025). DOI: 10.1073/pnas.2517530122. doi.org/10.1073/pnas.2517530122&lt;/p&gt;
    &lt;p&gt;Journal information: Proceedings of the National Academy of Sciences&lt;/p&gt;
    &lt;p&gt;Provided by Complexity Science Hub Vienna&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45725009</guid><pubDate>Mon, 27 Oct 2025 19:06:34 +0000</pubDate></item><item><title>Easy RISC-V</title><link>https://dramforever.github.io/easyriscv/</link><description>&lt;doc fingerprint="bae7c0d12017aa49"&gt;
  &lt;main&gt;&lt;p&gt;(Last updated: 2025-10-28 08:04)&lt;/p&gt;&lt;p&gt;This page is not designed to be used on a narrow screen or without CSS. If you’re having issues using the emulator, try the emulators disabled version.&lt;/p&gt;&lt;p&gt;An interactive introduction to RISC-V assembly programming, by dramforever.&lt;/p&gt;&lt;p&gt;Interested in the code? Want to report an issue? Check out the GitHub page: https://github.com/dramforever/easyriscv&lt;/p&gt;&lt;p&gt;Inspired by Easy 6502 by Nick Morgan, this is a quick-ish introductory tutorial to RISC-V assembly programming. This tutorial is intended for those with a basic familiarity with low level computer science concepts, but unfamiliar with RISC-V. If you’re curious about RISC-V, I hope this will be a good start to your journey to learning about it.&lt;/p&gt;&lt;p&gt;RISC-V (pronounced “risk-five”), as its name suggests, is RISC (Reduced instruction set computer) architecture. Having started its life at UC Berkerley, RISC-V has bred a lively community of students, researchers, engineers and hobbyists working on software and hardware. Some highlights of RISC-V include:&lt;/p&gt;&lt;p&gt;RISC-V is less mature than more established architectures like x86 or Arm, but it is quickly gaining steam and has found great success in many areas of application, such as embedded systems, custom processors, education, and research.&lt;/p&gt;&lt;p&gt;This article will cover the 32-bit bare bones RV32I_Zicsr instruction set with a tiny subset of the privileged architecture. You’ll probably never find a “real” chip with such bare bones instruction support. Most of them will have more extensions for other features like floating point or compressed instructions. However, I would still consider what we have here a “complete” instruction set. For example, Rust has Tier 2 support for the target &lt;code&gt;riscv32i-unknown-none-elf&lt;/code&gt;
which works completely fine with only the instructions we’ll cover
here.&lt;/p&gt;&lt;p&gt;Speaking of instructions we will cover, why don’t we meet the 45 of them right here and now:&lt;/p&gt;&lt;code&gt;lui auipc
jal jalr
beq bne blt bge bltu bgeu
lb lh lw lbu lhu sb sh sw
addi slti sltiu xori ori andi slli srli srai
add sub slt sltu xor or and sll srl sra
ecall ebreak
csrrw csrrs csrrc csrrwi csrrsi csrrci&lt;/code&gt;&lt;p&gt;Some of these instruction names should ring a bell (&lt;code&gt;add&lt;/code&gt;,
&lt;code&gt;or&lt;/code&gt;, &lt;code&gt;xor&lt;/code&gt;). Others will look like they have some
pattern to it. A few weird ones like &lt;code&gt;auipc&lt;/code&gt; stand out. These
instructions form the foundation of RISC-V, performing the basic tasks a
processor would do.&lt;/p&gt;&lt;p&gt;You will also catch a glimpse of what creating an operating system on RISC-V is like, namely handling exceptions and privilege levels.&lt;/p&gt;&lt;p&gt;Let’s get started.&lt;/p&gt;&lt;p&gt;Throughout this article you will see emulator panes like these:&lt;/p&gt;&lt;p&gt;(If you just see a code block, there’s a JavaScript problem. Make sure you’ve enabled JavaScript, probably…)&lt;/p&gt;&lt;p&gt;You can use the buttons to control each emulator. Go ahead and click on ‘Start’. A register view should pop up showing the state of the emulator. Now click on ‘Run’. You’ll notice that:&lt;/p&gt;&lt;code&gt;a0 (x10) 0x00000000&lt;/code&gt;&lt;p&gt;Changed into:&lt;/p&gt;&lt;code&gt;a0 (x10) 0x00000123&lt;/code&gt;&lt;p&gt;And the emulator stopped. Congratulations, you’ve run your first RISC-V assembly program. First here, at least.&lt;/p&gt;&lt;p&gt;‘Start’ assembles your code and, well, starts the emulator. If there’s a problem with your code, it will tell you about it and the emulator will not start.&lt;/p&gt;&lt;p&gt;When the emulator is started, you can see the current state of the registers in the side pane. More controls also becomes available. ‘Run’ runs until the end or until you hit ‘Pause’. ‘Step’ runs a single step.&lt;/p&gt;&lt;p&gt;If you hit ‘Step’, you’ll notice that the above program takes two steps to run. You may have guessed correctly that the first step corresponds to &lt;code&gt;addi&lt;/code&gt;, and the second corresponds to
&lt;code&gt;ebreak&lt;/code&gt;. The top of the register panel shows
&lt;code&gt;pc&lt;/code&gt;, the current instruction address, and in parentheses the
current instruction.&lt;/p&gt;&lt;p&gt;‘Dump’ opens a new window containing some text. There are two sections: the first is the symbol table, which tells you about the labels in your code:&lt;/p&gt;&lt;code&gt;# Symbols
# 0x40000000 start&lt;/code&gt;&lt;p&gt;The second section is an annotated version of your code:&lt;/p&gt;&lt;code&gt;start:
{ 0x40000000: 12300513 } addi x10, x0, 0x123
{ 0x40000004: 00100073 } ebreak&lt;/code&gt;&lt;p&gt;This tells you that the &lt;code&gt;addi&lt;/code&gt; instruction encodes to hex
&lt;code&gt;12300513&lt;/code&gt;, and starts at address hex &lt;code&gt;40000000&lt;/code&gt;.
Similarly, &lt;code&gt;ebreak&lt;/code&gt; encodes as &lt;code&gt;00100073&lt;/code&gt; at
address hex &lt;code&gt;40000004&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;(Note: RISC-V instructions are little-endian, meaning that the four bytes of &lt;code&gt;addi&lt;/code&gt; are actually
&lt;code&gt;13 05 30 12&lt;/code&gt;.)&lt;/p&gt;&lt;p&gt;We’ll talk in detail about all of &lt;code&gt;pc&lt;/code&gt;, registers,
instructions, labels, and the two checkboxes later.&lt;/p&gt;&lt;p&gt;Now you may have also guessed that &lt;code&gt;addi x10, x0, 0x123&lt;/code&gt;
means &lt;code&gt;x10 = x0 + 0x123&lt;/code&gt;. As for &lt;code&gt;ebreak&lt;/code&gt;, for
now, just remember that &lt;code&gt;ebreak&lt;/code&gt; stops the emulator.&lt;/p&gt;&lt;p&gt;The program counter, or &lt;code&gt;pc&lt;/code&gt; is the address of
the current instruction. It points to the instruction to be
executed.&lt;/p&gt;&lt;p&gt;RV32I has 31 general purpose registers numbered &lt;code&gt;x1&lt;/code&gt; through
&lt;code&gt;x31&lt;/code&gt;. These can contain any 32-bit data.&lt;/p&gt;&lt;p&gt;(If you’re wondering, there are no flags for RV32I.)&lt;/p&gt;&lt;p&gt;The register &lt;code&gt;x0&lt;/code&gt; is a
special “zero register”. For computational instructions, you can use
&lt;code&gt;x0&lt;/code&gt; anywhere a register is expected. Reading it always gives
zero, and writing to it just gets ignored. The use of a special register
simplifies the design of the architecture, and this design is shared by
MIPS and Arm AArch64. We will make good use of &lt;code&gt;x0&lt;/code&gt; soon.&lt;/p&gt;&lt;p&gt;(Note: In the emulator, the instruction listed in parenthesis next to &lt;code&gt;pc&lt;/code&gt; in the register view is provided as a convenience and is
not part of the processor state.)&lt;/p&gt;&lt;p&gt;But before we can start talking about instructions themselves, we need a way to talk about the instruction syntax so I can, you know, write it down for you.&lt;/p&gt;&lt;p&gt;The syntax of an instruction is the instruction name and then several comma-separated operands. For example, for this instruction we’ve seen above:&lt;/p&gt;&lt;code&gt;addi x10, x0, 0x123&lt;/code&gt;&lt;p&gt;&lt;code&gt;x10&lt;/code&gt; is the destination register or
&lt;code&gt;rd&lt;/code&gt;. The next operand is
the first (and only) source
register or &lt;code&gt;rs1&lt;/code&gt;. The last operand is an
immediate value or &lt;code&gt;imm&lt;/code&gt;. Using these
abbreviations, we can summarize that the syntax for &lt;code&gt;addi&lt;/code&gt;
is:&lt;/p&gt;&lt;code&gt;addi rd, rs1, imm&lt;/code&gt;&lt;p&gt;Some other instructions have a second source register or &lt;code&gt;rs2&lt;/code&gt;. For example, the
non-immediate &lt;code&gt;add&lt;/code&gt; instruction has this syntax:&lt;/p&gt;&lt;code&gt;add rd, rs1, rs2&lt;/code&gt;&lt;p&gt;Some other instructions have no operands, like &lt;code&gt;ebreak&lt;/code&gt;.
Others have slightly more complex operands.&lt;/p&gt;&lt;p&gt;Using the registers as a playground of numbers, we can use computational instructions to work with them.&lt;/p&gt;&lt;p&gt;As we’ve seen above, you can get a RISC-V machine to add numbers together.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;addi&lt;/code&gt;
instruction adds the value in &lt;code&gt;rs1&lt;/code&gt; to the immediate value
&lt;code&gt;imm&lt;/code&gt;, and puts the result in &lt;code&gt;rd&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;addi rd, rs1, imm&lt;/code&gt;&lt;p&gt;The &lt;code&gt;add&lt;/code&gt; instruction
adds the value in &lt;code&gt;rs1&lt;/code&gt; to the value in &lt;code&gt;rs2&lt;/code&gt;, and
puts the result in &lt;code&gt;rd&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;add rd, rs1, rs2&lt;/code&gt;&lt;p&gt;The opposite of addition is subtraction. The &lt;code&gt;sub&lt;/code&gt; instruction subtracts the
value in &lt;code&gt;rs2&lt;/code&gt; from the value in &lt;code&gt;rs1&lt;/code&gt;
(i.e. &lt;code&gt;rs1 - rs2&lt;/code&gt;), and puts the result in &lt;code&gt;rd&lt;/code&gt;.
There’s no corresponding &lt;code&gt;subi&lt;/code&gt; instruction — Just use
&lt;code&gt;addi&lt;/code&gt; with a negative number.&lt;/p&gt;&lt;code&gt;sub rd, rs1, rs2&lt;/code&gt;&lt;p&gt;Step through this demo program and try writing your own additions and subtractions:&lt;/p&gt;&lt;p&gt;One thing you should note is that the immediate value has a limited range, namely &lt;code&gt;[-2048, 2047]&lt;/code&gt;, the range of a 12-bit two’s
complement signed integer. This limitation is because RV32I uses fixed
32-bit i.e. 4-byte instructions, and only the top 12 bits are available
to encode an immediate value. You can see the hexadecimal value encoded
in the instruction from the ‘Dump’. This article will not go into much
further detail about instruction encodings.&lt;/p&gt;&lt;code&gt;{ 0x40000000: 12300513 } addi x10, x0, 0x123
{ 0x40000004: 55500593 } addi x11, x0, 0x555&lt;/code&gt;&lt;p&gt;Even instructions as simple as addition and subtraction have other interesting uses. We have already used &lt;code&gt;addi x10, x0, 0x123&lt;/code&gt;
to put &lt;code&gt;0x123&lt;/code&gt; in the register &lt;code&gt;x10&lt;/code&gt;. When writing
in assembly, we can use a little shortcut called pseudoinstructions. The
&lt;code&gt;li&lt;/code&gt; (“load immediate”)
pseudoinstruction is a convenient way to put a small value in a
register. It expands to &lt;code&gt;addi rd, x0, imm&lt;/code&gt; when
&lt;code&gt;imm&lt;/code&gt; is in the range &lt;code&gt;[-2048, 2047]&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;li rd, imm&lt;/code&gt;&lt;p&gt;When &lt;code&gt;imm&lt;/code&gt; is &lt;code&gt;0&lt;/code&gt;, &lt;code&gt;addi&lt;/code&gt; copies the
value without changing it because adding zero is the same as doing
nothing. The &lt;code&gt;mv&lt;/code&gt; (“move”)
pseudoinstruction copies the value from &lt;code&gt;rs1&lt;/code&gt; to
&lt;code&gt;rd&lt;/code&gt;. It expands to &lt;code&gt;addi rd, rs1, 0&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;mv rd, rs1&lt;/code&gt;&lt;p&gt;Using the pseudoinstruction is exactly equivalent to using the “real” instruction. You can see in the dump that the two are assembled exactly the same way.&lt;/p&gt;&lt;p&gt;Subtracting from zero is negation. What’s the negative of &lt;code&gt;0x123&lt;/code&gt;?&lt;/p&gt;&lt;p&gt;Hmm, we get &lt;code&gt;0xfffffedd&lt;/code&gt;. That’s the 32-bit two’s complement
representation of &lt;code&gt;-291&lt;/code&gt;, or &lt;code&gt;-0x123&lt;/code&gt;. There’s
plenty of tutorials on this out there, so we’ll just note that whenever
something is “signed”, RISC-V uses two’s complement representation. The
benefit of this is that there are fewer instructions for separate signed
and unsigned instructions — both signed and unsigned numbers have the
same overflow wrap-around behavior.&lt;/p&gt;&lt;p&gt;Speaking of overflow wrap-around, what happens if we add something too much and it overflows? We’ll use &lt;code&gt;add&lt;/code&gt; to repeatedly
double &lt;code&gt;0x123&lt;/code&gt; and see what happens:&lt;/p&gt;&lt;p&gt;As &lt;code&gt;0x123&lt;/code&gt; crawls up to the upper bits and eventually we
get to &lt;code&gt;0x9180_0000&lt;/code&gt;, in the next iteration it turns into
&lt;code&gt;0x2300_0000&lt;/code&gt;. There was an overflow! Doubling of
&lt;code&gt;0x9180_0000&lt;/code&gt; gives &lt;code&gt;0x1_2300_0000&lt;/code&gt;, but that
needs 33 bits in binary, so the highest bit can’t be put in the result.
Since RISC-V doesn’t have flag bits for carry or overflow, it’s simply
gone. The programmer is expected to deal with this.&lt;/p&gt;&lt;p&gt;While we’re talking about bits, another thing we can do with bits is performing bitwise logical operations on them.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;and&lt;/code&gt; instruction
performs a bitwise-“and” between the bits of &lt;code&gt;rs1&lt;/code&gt; and
&lt;code&gt;rs2&lt;/code&gt; and puts the result in &lt;code&gt;rd&lt;/code&gt;. The &lt;code&gt;or&lt;/code&gt; and &lt;code&gt;xor&lt;/code&gt; instructions similarly
performs bitwise-“or” and bitwise-“xor”, respectively.&lt;/p&gt;&lt;code&gt;and rd, rs1, rs2
or rd, rs1, rs2
xor rd, rs1, rs2&lt;/code&gt;&lt;p&gt;Immediate operand versions of the three, namely &lt;code&gt;andi&lt;/code&gt;, &lt;code&gt;ori&lt;/code&gt;, &lt;code&gt;xori&lt;/code&gt; also exist.&lt;/p&gt;&lt;code&gt;andi rd, rs1, imm
ori rd, rs1, imm
xori rd, rs1, imm&lt;/code&gt;&lt;p&gt;Here are some random bit operation examples you can play with:&lt;/p&gt;&lt;p&gt;Remember that the immediate value is in the range &lt;code&gt;[-2048, 2047]&lt;/code&gt;. For negative values, the two’s complement
representation used means that the high bits are all ones. For example,
using &lt;code&gt;-1&lt;/code&gt; as &lt;code&gt;imm&lt;/code&gt; means the second operand is
binary all ones, or &lt;code&gt;0xffff_ffff&lt;/code&gt;. This allows us to use
&lt;code&gt;xori rd, rs1, -1&lt;/code&gt; as bitwise-“not”.&lt;/p&gt;&lt;p&gt;Another interesting operation you can do is to round/align something up or down to a multiple of a power of two. For example, if you want to find the closest multiple of 16 below &lt;code&gt;a&lt;/code&gt;, in binary that would be clearing the lowest
4 bits, or &lt;code&gt;a &amp;amp; ~0b1111&lt;/code&gt;. Conveniently, that’s
&lt;code&gt;a &amp;amp; -16&lt;/code&gt; in two’s complement.&lt;/p&gt;&lt;p&gt;Aligning up is less intuitive, but one idea would be adding 16 first. However that gives an incorrect result for multiples of 16. It’s easy enough to fix though: adding one less works exactly right: &lt;code&gt;(a + 15) &amp;amp; -16&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Usually when you write a comparison of some sort like &lt;code&gt;a == b&lt;/code&gt; or &lt;code&gt;a &amp;gt;= b&lt;/code&gt;, it’s used as a condition
for some &lt;code&gt;if&lt;/code&gt; or loop, but… those things are complicated!
We’ll get to it later.&lt;/p&gt;&lt;p&gt;Sometimes you just want a boolean value out of a comparison. The C convention uses 1 for true and 0 for false, and since the world runs on C now, that’s what RISC-V provides.&lt;/p&gt;&lt;p&gt;In C there are six comparison operators:&lt;/p&gt;&lt;code&gt;== != &amp;lt; &amp;gt; &amp;lt;= &amp;gt;=&lt;/code&gt;&lt;p&gt;The values being compared can also be both signed or both unsigned.&lt;/p&gt;&lt;p&gt;How many comparison instructions do we have at our disposal? Let’s see…&lt;/p&gt;&lt;p&gt;The &lt;code&gt;slt&lt;/code&gt; (“set less
than”) instruction compares &lt;code&gt;rs1&lt;/code&gt; and &lt;code&gt;rs2&lt;/code&gt; as
signed 32-bit integers, and sets &lt;code&gt;rd&lt;/code&gt; to &lt;code&gt;1&lt;/code&gt; if
&lt;code&gt;rs1 &amp;lt; rs2&lt;/code&gt;, and &lt;code&gt;0&lt;/code&gt; otherwise
(&lt;code&gt;rs1 &amp;gt;= rs2&lt;/code&gt;). The &lt;code&gt;sltu&lt;/code&gt; instruction is similar
but it treats the operands as unsigned values. &lt;code&gt;slti&lt;/code&gt; and &lt;code&gt;sltiu&lt;/code&gt; are similar but the
second operand is an immediate value.&lt;/p&gt;&lt;code&gt;slt rd, rs1, rs2
sltu rd, rs1, rs2
slti rd, rs1, imm
sltiu rd, rs1, imm&lt;/code&gt;&lt;p&gt;(Of particular note is &lt;code&gt;sltiu&lt;/code&gt;, where the immediate
operand still has the range &lt;code&gt;[-2048, 2047]&lt;/code&gt; but is sign
extended to 32 bits and then treated as an unsigned value, like what
would happen in C with &lt;code&gt;a &amp;lt; (unsigned)-1&lt;/code&gt;.)&lt;/p&gt;&lt;p&gt;That’s… one of the six comparisons settled. What about the others? As it turns out, we can synthesize any of the other five, using up to two instructions.&lt;/p&gt;&lt;p&gt;Making &lt;code&gt;&amp;gt;&lt;/code&gt; from &lt;code&gt;&amp;lt;&lt;/code&gt; is easy, as you can
just swap the operands. Using &lt;code&gt;xori&lt;/code&gt; with &lt;code&gt;1&lt;/code&gt; we
can invert the result of a comparison, giving as &lt;code&gt;&amp;lt;=&lt;/code&gt; and
&lt;code&gt;&amp;gt;=&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;That was signed comparison but unsigned comparison works the same using &lt;code&gt;sltu&lt;/code&gt; instead of &lt;code&gt;slt&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;As for &lt;code&gt;==&lt;/code&gt; and &lt;code&gt;!=&lt;/code&gt;, let’s tackle the easier
case of &lt;code&gt;a == 0&lt;/code&gt; and &lt;code&gt;a != 0&lt;/code&gt; first. We will use
the fact that for unsigned values, &lt;code&gt;a != 0&lt;/code&gt; is equivalent to
&lt;code&gt;a &amp;gt; 0&lt;/code&gt;. The negation of that is &lt;code&gt;a &amp;lt;= 0&lt;/code&gt;,
which is the same as &lt;code&gt;a &amp;lt; 1&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;As a bonus, this is also how we get logical not and converting integer to boolean.&lt;/p&gt;&lt;p&gt;Now that we have these, &lt;code&gt;a == b&lt;/code&gt; is just
&lt;code&gt;(a - b) == 0&lt;/code&gt;, and &lt;code&gt;a != b&lt;/code&gt; is just
&lt;code&gt;(a - b) != 0&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;In summary: (&lt;code&gt;[u]&lt;/code&gt; means use &lt;code&gt;u&lt;/code&gt; for unsigned
comparison and nothing for signed comparison)&lt;/p&gt;&lt;code&gt;a &amp;lt; b&lt;/code&gt;: &lt;code&gt;slt[u]&lt;/code&gt;&lt;code&gt;a &amp;gt; b&lt;/code&gt;: &lt;code&gt;slt[u] reversed&lt;/code&gt;&lt;code&gt;a &amp;lt;= b&lt;/code&gt;: &lt;code&gt;slt[u] reversed ; xori 1&lt;/code&gt;&lt;code&gt;a &amp;gt;= b&lt;/code&gt;: &lt;code&gt;slt[u] ; xori 1&lt;/code&gt;&lt;code&gt;a == 0&lt;/code&gt;: &lt;code&gt;sltu x0&lt;/code&gt;&lt;code&gt;a != 0&lt;/code&gt;: &lt;code&gt;sltiu 1&lt;/code&gt;&lt;code&gt;a == b&lt;/code&gt;: &lt;code&gt;sub ; sltu x0&lt;/code&gt;&lt;code&gt;a != b&lt;/code&gt;: &lt;code&gt;sub ; sltiu 1&lt;/code&gt;&lt;p&gt;There is no way I can do justice to the usage of bit shifts in the middle of a tutorial on RISC-V assembly. If you’re here, you’ve probably heard of them. There’s nothing really special to the way they appear in usage for RISC-V.&lt;/p&gt;&lt;p&gt;There are two variants for right shifting: &lt;code&gt;srl&lt;/code&gt; and &lt;code&gt;srli&lt;/code&gt; (“shift right logical
(immediate)”) performs “logical” or unsigned right shift where the
leftmost or most significant bits are filled with zeros.&lt;/p&gt;&lt;p&gt;&lt;code&gt;sra&lt;/code&gt; and &lt;code&gt;srai&lt;/code&gt; (“shift right
arithmetic (immediate)”) performs “arithmetic” or signed right shift
where the leftmost bits are filled with the same of what highest/sign
bit was. So if you shift a negative value, you get a negative result; if
you shift a non-negative value, you get a non-negative result.&lt;/p&gt;&lt;code&gt;srl rd, rs1, rs2
sra rd, rs1, rs2
srli rd, rs1, imm
srai rd, rs1, imm&lt;/code&gt;&lt;p&gt;As before, the ones with the &lt;code&gt;i&lt;/code&gt; suffix take an immediate
value as the second operand, and the ones without &lt;code&gt;i&lt;/code&gt; take a
register.&lt;/p&gt;&lt;p&gt;So &lt;code&gt;a&lt;/code&gt; means “arithmetic”, &lt;code&gt;l&lt;/code&gt; means “logical”.
Got it.&lt;/p&gt;&lt;p&gt;Left shifts have no such distinction. For consistency they are still “logical”: &lt;code&gt;sll&lt;/code&gt; is left
shift, and &lt;code&gt;slli&lt;/code&gt; is
left shift with immediate.&lt;/p&gt;&lt;code&gt;sll rd, rs1, rs2
slli rd, rs1, imm&lt;/code&gt;&lt;p&gt;Aha, now we can blow up &lt;code&gt;0x123&lt;/code&gt; without repeating myself
so much:&lt;/p&gt;&lt;p&gt;The immediate value for shift instructions are special: they can only be in the range of 0 to 31, inclusive, because it doesn’t make sense to shift by a negative amount, or by more than 31. When the shift amount is taken from a register, the value is considered modulo 32, or in other words only the last 5 bits are taken into account:&lt;/p&gt;&lt;p&gt;For some fun, let’s try multiplying a value by 10, something you would do when parsing decimal numbers: &lt;code&gt;a * 10&lt;/code&gt; can be
rewritten as &lt;code&gt;(a &amp;lt;&amp;lt; 1) + (a &amp;lt;&amp;lt; 3)&lt;/code&gt;:&lt;/p&gt;&lt;p&gt;That’s it?&lt;/p&gt;&lt;p&gt;You may have noticed some glaring omissions. What we’ve learned doesn’t even cover grade school math: multiplication and division are missing.&lt;/p&gt;&lt;p&gt;RISC-V is designed with extensions in mind. Remember that as said in the introduction, RV32I is the barest bones of the barest bones we’ve got. Forcing everyone to make their processors with multiplication and division even for tasks that don’t need them would waste silicon area and money on every chip. Instead those making RISC-V processors have great freedom to choose, and indeed some would say they have too much freedom.&lt;/p&gt;&lt;p&gt;For us… Honestly, I’m just glad we’ve been dealt a hand that we can tackle completely in full. There’s no way I’m finishing writing this tutorial if RV32I wasn’t so bare boned.&lt;/p&gt;&lt;p&gt;(Operand &lt;code&gt;a&lt;/code&gt; is &lt;code&gt;rs1&lt;/code&gt;, and &lt;code&gt;b&lt;/code&gt; is
&lt;code&gt;rs2&lt;/code&gt; or immediate. In the instruction name &lt;code&gt;[i]&lt;/code&gt;
means an immediate variant is available. Subscript &lt;code&gt;u&lt;/code&gt; means
unsigned and &lt;code&gt;s&lt;/code&gt; means two’s complement signed.)&lt;/p&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell role="head"&gt;Instruction&lt;/cell&gt;&lt;cell role="head"&gt;Operation&lt;/cell&gt;&lt;cell role="head"&gt;Immediate range&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;&lt;code&gt;add[i]&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;a + b&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;[-2048, 2047]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;&lt;code&gt;sub&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;a - b&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;(n/a)&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;&lt;code&gt;slt[i]&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;(a &amp;lt;s b) ? 1 : 0&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;[-2048, 2047]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;&lt;code&gt;slt[i]u&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;(a &amp;lt;u b) ? 1 : 0&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;[-2048, 2047]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;&lt;code&gt;xor[i]&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;a ^ b&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;[-2048, 2047]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;&lt;code&gt;or[i]&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;a | b&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;[-2048, 2047]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;&lt;code&gt;and[i]&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;a &amp;amp; b&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;[-2048, 2047]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;&lt;code&gt;sll[i]&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;a &amp;lt;&amp;lt; b&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;[0, 31]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;&lt;code&gt;srl[i]&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;a &amp;gt;&amp;gt;u b&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;[0, 31]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;&lt;code&gt;sra[i]&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;a &amp;gt;&amp;gt;s b&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;[0, 31]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;The &lt;code&gt;addi&lt;/code&gt; instruction has limit on the immediate value.
How do we make bigger values?&lt;/p&gt;&lt;p&gt;The &lt;code&gt;lui&lt;/code&gt; (“load upper
immediate”) instruction takes an immediate in the range
&lt;code&gt;[0, 1048575]&lt;/code&gt; (i.e. up to &lt;code&gt;220 - 1&lt;/code&gt;)
and sets &lt;code&gt;rd&lt;/code&gt; to that value left shifted 12 bits:&lt;/p&gt;&lt;code&gt;lui rd, imm20&lt;/code&gt;&lt;p&gt;That was… slightly confusing. Why don’t we give it a try:&lt;/p&gt;&lt;p&gt;Instead of &lt;code&gt;li&lt;/code&gt; loading a “low” immediate, we control the
upper 20 bits of what we put in the register. After that, we
can use another &lt;code&gt;addi&lt;/code&gt; instruction to fill in the lower bits.
For example, if we want &lt;code&gt;0x12345&lt;/code&gt;:&lt;/p&gt;&lt;p&gt;For convenience, in assembly you can use &lt;code&gt;%hi()&lt;/code&gt; and &lt;code&gt;%lo()&lt;/code&gt; to extract the, well,
high 20 and low 12 bits of a value. The previous example could also be
written:&lt;/p&gt;&lt;p&gt;Letting &lt;code&gt;lui&lt;/code&gt; handle the high 20 bits, and
&lt;code&gt;addi&lt;/code&gt; for the low 12 bits, you can make any 32-bit
value.&lt;/p&gt;&lt;p&gt;(A small complication arises if you want to use values with bit 11 set. In that case, the immediate operand to &lt;code&gt;addi&lt;/code&gt; will have
to be negative. However &lt;code&gt;%hi&lt;/code&gt; understands this and adds one
to compensate, so this &lt;code&gt;%hi&lt;/code&gt;/&lt;code&gt;%lo&lt;/code&gt; combination
does work for everything.)&lt;/p&gt;&lt;p&gt;So far, everything that we’ve had so far can be done on even the most basic programmer’s calculator. To truly make a computer… do computer stuff, we’d want loops and conditionals.&lt;/p&gt;&lt;p&gt;In RISC-V parlance, a branch is a conditional transfer of control flow, and a jump is an unconditional transfer of control flow.&lt;/p&gt;&lt;p&gt;I think the branch instructions are slightly simpler, so let’s start with those.&lt;/p&gt;&lt;p&gt;All the branch instruction follow the form “If some comparison, go to somewhere.” The conditions are:&lt;/p&gt;&lt;code&gt;beq&lt;/code&gt;:
&lt;code&gt;rs1 == rs2&lt;/code&gt; (“equal”)&lt;code&gt;bne&lt;/code&gt;:
&lt;code&gt;rs1 != rs2&lt;/code&gt; (“not equal”)&lt;code&gt;blt&lt;/code&gt;:
&lt;code&gt;rs1 &amp;lt; rs2&lt;/code&gt; signed (“less than”)&lt;code&gt;bge&lt;/code&gt;:
&lt;code&gt;rs1 &amp;gt;= rs2&lt;/code&gt; signed (“greater or equal”)&lt;code&gt;bltu&lt;/code&gt;:
&lt;code&gt;rs1 &amp;lt; rs2&lt;/code&gt; signed (“less than unsigned”)&lt;code&gt;bgeu&lt;/code&gt;:
&lt;code&gt;rs1 &amp;gt;= rs2&lt;/code&gt; signed (“greater or equal unsigned”)&lt;p&gt;(In case you’re wondering about the confusing choice of ordering operators here, it’s just that the negation of &lt;code&gt;&amp;lt;&lt;/code&gt; is
&lt;code&gt;&amp;gt;=&lt;/code&gt;.)&lt;/p&gt;&lt;code&gt;beq rs1, rs2, label
bne rs1, rs2, label
blt rs1, rs2, label
bge rs1, rs2, label
bltu rs1, rs2, label
bgeu rs1, rs2, label&lt;/code&gt;
&lt;p&gt;Oh, right, almost forgot to explain what labels are. Labels are convenience identifiers for addresses at some line of your code. They are some identifier followed by a colon (like &lt;code&gt;this:&lt;/code&gt;). They
can appear on a line of its own, or before any instruction on the line.
You can see which address they point to using the “Dump” button. The
third operand of a branch instruction is a label to jump to if the
condition holds.&lt;/p&gt;&lt;p&gt;Let’s add up all the numbers from 1 to 100:&lt;/p&gt;&lt;p&gt;You can try your hands on making your favorite loops, like fibonacci numbers or something. Speaking of trying your hands, just so we’re ready, here’s what an infinite loop looks like. Try pausing or stopping the loop, and single stepping through the instructions.&lt;/p&gt;&lt;p&gt;(If you know a thing or two about JavaScript in the browser, you’ll know that a real infinite loop in JavaScript makes the whole page becomes unresponsive, unless it’s in a worker or something. The “Run” button here just runs the emulator for a certain number of steps, pausing by giving back control to the event loop in between.)&lt;/p&gt;&lt;p&gt;(This isn’t the preferred way to write an unconditional jump. We’ll see what is later.)&lt;/p&gt;&lt;p&gt;By the way, there’s no &lt;code&gt;bgt[u]&lt;/code&gt; or &lt;code&gt;ble[u]&lt;/code&gt;
because you can just swap &lt;code&gt;rs1&lt;/code&gt; and &lt;code&gt;rs2&lt;/code&gt; to get
those.&lt;/p&gt;&lt;p&gt;There are two jump instructions in RISC-V. One of them is &lt;code&gt;jal&lt;/code&gt; “jump and link”, which
sets &lt;code&gt;rd&lt;/code&gt; to the address of the following instruction, and
then jumps to a label:&lt;/p&gt;&lt;code&gt;jal rd, label&lt;/code&gt;
&lt;p&gt;Another is &lt;code&gt;jalr&lt;/code&gt;
“jump and link register”, which sets &lt;code&gt;rd&lt;/code&gt; to the address of
the following instruction, and then jumps to the address at
&lt;code&gt;imm + rs1&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;jalr rd, imm(rs1)&lt;/code&gt;
&lt;p&gt;(Actually, the address jumped to is &lt;code&gt;(imm + rs1) &amp;amp; ~1&lt;/code&gt;, i.e. the least significant bit is
cleared. This distinction won’t come up in normal code, like, pretty
much ever.)&lt;/p&gt;&lt;p&gt;Eesh, that’s some funky looking syntax. When you see parentheses like this, it has something to do with an address. Parens means address.&lt;/p&gt;&lt;p&gt;That’s… still a lot going on. Let’s take on some simpler cases first: If &lt;code&gt;rd&lt;/code&gt; is &lt;code&gt;x0&lt;/code&gt; then the only thing these
instructions do is jumping. We can use it instead of the branch
instructions for an unconditional jump.&lt;/p&gt;&lt;p&gt;For convenience, a pseudoinstruction is available for you: &lt;code&gt;j&lt;/code&gt; (“jump”) is for
&lt;code&gt;jal&lt;/code&gt; with &lt;code&gt;rd&lt;/code&gt; being &lt;code&gt;x0&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;j label&lt;/code&gt;
&lt;p&gt;As for why you would want to do this… Well, we only have 32 bits per instruction, and since the &lt;code&gt;jal&lt;/code&gt; instruction only needs one
register number instead of the branch instructions’ two, and it doesn’t
need a condition, the instruction encoding permits jumping over a longer
range. So this is always preferred over something like
&lt;code&gt;beq x0, x0, label&lt;/code&gt; for a jump.&lt;/p&gt;&lt;p&gt;As for &lt;code&gt;jalr&lt;/code&gt;, you can jump to an address that’s stored in
a register. In C, that would be dealing with function pointers. You’d
need this any time dynamic dispatch is needed. For example, we load the
address of &lt;code&gt;foo&lt;/code&gt; into a register first before jumping to
it.&lt;/p&gt;&lt;p&gt;In case you forgot by now, the &lt;code&gt;lui&lt;/code&gt;/&lt;code&gt;addi&lt;/code&gt;
combo at the start puts the address of the label &lt;code&gt;foo&lt;/code&gt; in
register &lt;code&gt;x10&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Similar to &lt;code&gt;j&lt;/code&gt;, &lt;code&gt;jr&lt;/code&gt; (“jump register”) is a
psuedoinstruction for &lt;code&gt;jalr&lt;/code&gt; with &lt;code&gt;rd&lt;/code&gt; being
&lt;code&gt;x0&lt;/code&gt; and &lt;code&gt;imm&lt;/code&gt; being &lt;code&gt;0&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;jr rs1&lt;/code&gt;
&lt;p&gt;Hmmm… If I didn’t really need the address in &lt;code&gt;x10&lt;/code&gt;, that
&lt;code&gt;addi&lt;/code&gt; would be unnecessary, since &lt;code&gt;jalr&lt;/code&gt; has the
ability to add a low immediate on its own:&lt;/p&gt;&lt;p&gt;What’s the advantage of this over &lt;code&gt;jal x0&lt;/code&gt;? Since
&lt;code&gt;%hi&lt;/code&gt; and &lt;code&gt;%lo&lt;/code&gt; can represent any 32-bit value,
this two-instruction combo can jump to any address, free from range
restrictions. You do need a free scratch register for the high part of
the address though, but since RISC-V gives you 31 of them, this
shouldn’t be too much of a problem.&lt;/p&gt;&lt;p&gt;What’s the deal with the destination register then? What do you need the address of the next instruction for? For jumping back of course. We can use this functionality to call functions and return back.&lt;/p&gt;&lt;p&gt;Note that I used the register &lt;code&gt;x1&lt;/code&gt; for this, which is the
register for providing the return address by convention. For
convenience, if the destination register is omitted in &lt;code&gt;jal&lt;/code&gt;,
it defaults to &lt;code&gt;x1&lt;/code&gt;. Meanwhile, &lt;code&gt;ret&lt;/code&gt; (“return”) is a
pseudoinstruction that stands for &lt;code&gt;jr x1&lt;/code&gt;,
i.e. &lt;code&gt;jalr x0, 0(x1)&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;jal label
ret&lt;/code&gt;
&lt;p&gt;So the example above can be rewritten more conveniently as:&lt;/p&gt;&lt;p&gt;That’s a nice computer we have here. Now we have… all of 31 × 4 = 124 bytes of storage in the form of registers to work with. I want more…&lt;/p&gt;&lt;p&gt;The emulator has 1 MiB of memory starting at address &lt;code&gt;0x4000_0000&lt;/code&gt;. That’s &lt;code&gt;0x4000_0000&lt;/code&gt; to
&lt;code&gt;0x400f_ffff&lt;/code&gt;, inclusive. The assembler starts assembling at
the beginning of memory, as you can see in the dump, starting at address
&lt;code&gt;0x4000_0000&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;.word&lt;/code&gt; directive straight up puts a
4-byte/32-bit word into the current position. You can specify multiple
values separated by commas.&lt;/p&gt;&lt;code&gt;.word value [ , value [ , ...  ] ]&lt;/code&gt;
&lt;p&gt;The &lt;code&gt;lw&lt;/code&gt; (“load word”)
instruction loads a word from the address &lt;code&gt;rs1 + imm&lt;/code&gt; and
puts it in &lt;code&gt;rd&lt;/code&gt;, in other words it reads the word from
memory:&lt;/p&gt;&lt;code&gt;lw rd, imm(rs1)&lt;/code&gt;
&lt;p&gt;As with &lt;code&gt;jalr&lt;/code&gt;, you can combine it with &lt;code&gt;lui&lt;/code&gt;
to access any address.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;sw&lt;/code&gt; (“store word”)
instruction stores &lt;code&gt;rs2&lt;/code&gt; to a word in memory at address
&lt;code&gt;rs2 + imm&lt;/code&gt;, in other words it writes the word to memory:&lt;/p&gt;&lt;code&gt;sw rs2, imm(rs1)&lt;/code&gt;
&lt;p&gt;Just to make absolutely sure we’re clear on this, load means reading from memory, store means writing to memory. Both words can be nouns and verbs. Also, a word is 32-bit for RISC-V.&lt;/p&gt;&lt;p&gt;Let’s have some fun. Can we have the program read itself?&lt;/p&gt;&lt;p&gt;Ohh that’s fun. Does this mean I can also write programs with just &lt;code&gt;.word&lt;/code&gt;?&lt;/p&gt;&lt;p&gt;Oh that’s nice. Just a peek into the world of machine code and instruction encodings… which we will not be getting into.&lt;/p&gt;&lt;p&gt;With memory accesses under our belt, we can address a lot more data easily. Here’s an example where we find the sum of all the values in an array. Note how we can access different addresses of memory, whereas there is no way to address a register by a number in another register.&lt;/p&gt;&lt;p&gt;The equivalent in C would be something like&lt;/p&gt;&lt;code&gt;uint32_t array[], length;

uint32_t *current = array;
uint32_t *end = array + length;
uint32_t sum = 0;

for (; current != end; current ++) {
    sum += *current;
}&lt;/code&gt;
&lt;p&gt;Note how adding one to a pointer to word bumps the address by 4, because the addresses are all byte addresses, and one word is four bytes. In C, the compiler handles the multiplier for you, but in assembly you have to remember to do it manually.&lt;/p&gt;&lt;p&gt;Not everything in memory is word sized. You’ve already seen an array, which is multiple-word-sized. There are also stuff smaller than word-sized.&lt;/p&gt;&lt;p&gt;An obvious one is the byte, which is, well, 1-byte/8-bit and written &lt;code&gt;[u]int8_t&lt;/code&gt; in C. In
the middle is the halfword,
which is 2-byte/16-bit and written &lt;code&gt;[u]int16_t&lt;/code&gt; in C. You can
use the directives &lt;code&gt;.byte&lt;/code&gt; and &lt;code&gt;.half&lt;/code&gt; respectively for those
data types.&lt;/p&gt;&lt;code&gt;.byte value [ , value [ , ...  ] ]
.half value [ , value [ , ...  ] ]&lt;/code&gt;
&lt;p&gt;And just in case you don’t remember those, &lt;code&gt;.2byte&lt;/code&gt; means the same as
&lt;code&gt;.half&lt;/code&gt;, and &lt;code&gt;.4byte&lt;/code&gt; means the same as
&lt;code&gt;.word&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;.2byte value [ , value [ , ...  ] ] # Same as .half
.4byte value [ , value [ , ...  ] ] # Same as .word&lt;/code&gt;
&lt;p&gt;There’s a small problem with loading smaller-than-word sized values into word-sized registers: What do you do with the rest of the bits? Obviously the lowest of the bits gets the actual value loaded. There are two most useful ways to fill the upper bits:&lt;/p&gt;&lt;p&gt;Zero extension is easy enough. As the name suggests, sign extension has something to do with signed values. It’s what happens when you convert a narrower signed value into a wider one.&lt;/p&gt;&lt;p&gt;(Keeping the rest of the bits unchanged isn’t a good option. It complicates the implementation for processor, especially of modern high performance design, to just write parts of a register. It would be easiest if the new value didn’t depend on the old value.)&lt;/p&gt;&lt;p&gt;For example, the signed byte value &lt;code&gt;-100&lt;/code&gt; is
&lt;code&gt;0x9c&lt;/code&gt;. Since the highest bit i.e. the sign bit of it is
&lt;code&gt;1&lt;/code&gt;, when we expand it into 32 bits we fill the high 24 bits
with one so the new value, &lt;code&gt;0xffff_ff9c&lt;/code&gt; still represents
&lt;code&gt;-100&lt;/code&gt;. This is sign extension.&lt;/p&gt;&lt;p&gt;If we want to convert the unsigned byte value &lt;code&gt;156&lt;/code&gt;, still
&lt;code&gt;0x9c&lt;/code&gt;, into an unsigned word, it would have to be
&lt;code&gt;0x0000_009c&lt;/code&gt; to preserve its value.&lt;/p&gt;&lt;p&gt;For bytes, the &lt;code&gt;lb&lt;/code&gt;
(“load byte”) instruction loads a byte and sign extends the result, and
the &lt;code&gt;lbu&lt;/code&gt; (“load byte
unsigned”) instruction does the same but zero extends the result. As
with &lt;code&gt;lw&lt;/code&gt;, the address is &lt;code&gt;rs1 + imm&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;lb rd, imm(rs1)
lbu rd, imm(rs1)&lt;/code&gt;
&lt;p&gt;Similarly for &lt;code&gt;lh&lt;/code&gt;
(“load half”) and &lt;code&gt;lhu&lt;/code&gt;
(“load half unsigned”), just for unsigned halfwords (two bytes each,
remember):&lt;/p&gt;&lt;code&gt;lh rd, imm(rs1)
lhu rd, imm(rs1)&lt;/code&gt;
&lt;p&gt;We can try out the sign extension and zero extension example from earlier.&lt;/p&gt;&lt;p&gt;Correspondingly, the &lt;code&gt;sb&lt;/code&gt; (“store byte”) and &lt;code&gt;sh&lt;/code&gt; (“store half”) do the
opposite of &lt;code&gt;lb&lt;/code&gt; and &lt;code&gt;lh&lt;/code&gt;, storing bytes and
halfwords to memory. Instead of widening small values to register size,
these take the lowest order bits from &lt;code&gt;rs1&lt;/code&gt; and stores it to
memory. (There’s no &lt;code&gt;sbu&lt;/code&gt; and &lt;code&gt;shu&lt;/code&gt; because stores
are narrowing instead of widening operations.)&lt;/p&gt;&lt;code&gt;sb rs2, imm(rs1)
sh rs2, imm(rs1)&lt;/code&gt;
&lt;p&gt;While we’re at it, here’s two more minor details. Firstly, endianness. While theoretically big endian RISC-V machines can exist, I’ve never seen one… and this emulator is little endian, meaning that the four bytes in a word are laid out in memory lowest first. So, &lt;code&gt;.byte 0x1, 0x2, 0x3, 0x4&lt;/code&gt; would be
the same as &lt;code&gt;.word 0x04030201&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Secondly, memory accesses should be aligned for maximum efficiency. This means that the address for a halfword/2byte should be a multiple of two, and the address for a word/4byte should be a multiple of four. Misaligned accesses (meaning, well, when the address is not aligned) may not work as expected.&lt;/p&gt;&lt;p&gt;For user programs running on a rich operating systems, misaligned accesses are supported but may be slow. In embedded application running on microcontrollers and such, it might not work at all.&lt;/p&gt;&lt;p&gt;This emulator supports misaligned memory accesses.&lt;/p&gt;&lt;p&gt;Now you can try translating some basic C code into RISC-V assembly. Functions are… still out of the question for now. Variables have to be either global or put in registers. What else are we missing…&lt;/p&gt;&lt;p&gt;Is it Hello World time? I think it’s Hello World time…&lt;/p&gt;&lt;p&gt;For a computer to not just be a space heater, we need some way for it to at least generate output and take input. While other architectures may have dedicated I/O instructions, RISC-V uses memory mapped I/O. Essentially, this means that loads and stores to special addresses communicate with other devices. They do not work like normal memory, and you should only use the supported widths to access them.&lt;/p&gt;&lt;p&gt;One output device we have here is at address &lt;code&gt;0x1000_0000&lt;/code&gt;. Any 32-bit writes to it appends the lowest 8
bits as a byte to the text in the output pane. In other words, a
&lt;code&gt;sw&lt;/code&gt; to that address writes a byte of output.&lt;/p&gt;&lt;p&gt;(The output pane uses UTF-8 encoding.)&lt;/p&gt;&lt;p&gt;Eh, close enough to greeting the entire world. We could refactor it a bit to use a loop, or whatever… Now that we think about it, how about going one step further and organize our code into some functions?&lt;/p&gt;&lt;p&gt;We already know how to call a function and return back. Namely, &lt;code&gt;jal&lt;/code&gt; calls a function, and &lt;code&gt;ret&lt;/code&gt; returns. Usually
functions take arguments, uses local variables, and returns results.
Since there’s no real difference between the 31 general purpose
registers, on account of them being, well, general purpose, we could
just use any of them as we wish. Usually though, there are some standard
conventions to follow&lt;/p&gt;&lt;p&gt;This whole time you probably have noticed that registers are listed with two names each, and indeed both work identically in assembly.&lt;/p&gt;&lt;p&gt;These register aliases are named after their uses:&lt;/p&gt;&lt;code&gt;s0&lt;/code&gt; through
&lt;code&gt;s11&lt;/code&gt; are saved registers&lt;code&gt;t0&lt;/code&gt; through
&lt;code&gt;t6&lt;/code&gt; are temporary registers&lt;code&gt;a0&lt;/code&gt; through
&lt;code&gt;a7&lt;/code&gt; are argument registers&lt;code&gt;zero&lt;/code&gt; is the,
well, zero register&lt;code&gt;ra&lt;/code&gt; is for the
return address, by convention, as we’ve seen&lt;code&gt;sp&lt;/code&gt; … we’ll talk
about &lt;code&gt;sp&lt;/code&gt; later&lt;code&gt;tp&lt;/code&gt;
and &lt;code&gt;gp&lt;/code&gt; is out of the
scope of this document.)&lt;p&gt;(Yeah it’s… all placed in a weird order. The reason is out of the scope of this tutorial.)&lt;/p&gt;&lt;p&gt;When you call a function, you put up to eight arguments in the… well, argument registers, in the order &lt;code&gt;a0&lt;/code&gt;, &lt;code&gt;a1&lt;/code&gt;, …,
&lt;code&gt;a7&lt;/code&gt;. After that you use &lt;code&gt;jal&lt;/code&gt; or something, which
puts the return address in &lt;code&gt;ra&lt;/code&gt;, and jumps to the
function.&lt;/p&gt;&lt;p&gt;Inside, the function, if it wishes to use the call-saved registers &lt;code&gt;s0&lt;/code&gt; through &lt;code&gt;s11&lt;/code&gt;, it must save their values at
the start of the function, and restore them before returning. The non
call-saved registers &lt;code&gt;a0&lt;/code&gt; through &lt;code&gt;a7&lt;/code&gt;,
&lt;code&gt;t0&lt;/code&gt; through &lt;code&gt;t6&lt;/code&gt; and &lt;code&gt;ra&lt;/code&gt; may be
modified without restoring their values.&lt;/p&gt;&lt;p&gt;When the called function is done, it would, as mentioned, restore any used call-saved registers, and jump back to the return address, resuming the calling code.&lt;/p&gt;&lt;p&gt;Here’s a basic-ish example:&lt;/p&gt;&lt;code&gt;int memcmp(const void *a, const void *b, size_t n)&lt;/code&gt;
&lt;p&gt;The parameter &lt;code&gt;a&lt;/code&gt; is passed in &lt;code&gt;a0&lt;/code&gt;,
&lt;code&gt;b&lt;/code&gt; is passed in &lt;code&gt;a1&lt;/code&gt;, and &lt;code&gt;n&lt;/code&gt; is
passed in &lt;code&gt;a2&lt;/code&gt;. The return value will be in &lt;code&gt;a0&lt;/code&gt;.
Here’s an implementation and test run:&lt;/p&gt;&lt;p&gt;Here’s a slightly better-organized “Hello World”, using a &lt;code&gt;puts&lt;/code&gt; function:&lt;/p&gt;&lt;p&gt;Although we can write some very basic functions now, there are still a few problems:&lt;/p&gt;&lt;code&gt;ra&lt;/code&gt; would be overwritten, and then you can’t return back
from the outer function anymore.&lt;p&gt;Clearly, both would require using memory somehow. We can feed two birds with one scone by using memory in a structured way: The stack.&lt;/p&gt;&lt;p&gt;Unlike some other architectures, the &lt;code&gt;sp&lt;/code&gt; register is not
really special in any way. But just like how we can designate how
&lt;code&gt;a0&lt;/code&gt; is used, we can have some conventions about how
&lt;code&gt;sp&lt;/code&gt; is supposed to be used:&lt;/p&gt;&lt;code&gt;sp&lt;/code&gt; needs to have the same value as when the
function was entered&lt;code&gt;sp&lt;/code&gt; always points to somewhere in an area of
memory called the “stack”, and it is always 16-byte
aligned.&lt;p&gt;And, for the stack itself:&lt;/p&gt;&lt;code&gt;address &amp;gt;= sp&lt;/code&gt; are “in the stack”, and
&lt;code&gt;address &amp;lt; sp&lt;/code&gt; are free space that the stack can grow
into.&lt;code&gt;sp&lt;/code&gt;, and deallocate space by incrementing &lt;code&gt;sp&lt;/code&gt;.
Of course, allocations and deallocations must be balanced properly.&lt;p&gt;An example is in order. Let’s say you have a function &lt;code&gt;foo&lt;/code&gt; which just calls &lt;code&gt;bar&lt;/code&gt; twice.&lt;/p&gt;&lt;code&gt;void foo() {
    bar();
    bar();
}&lt;/code&gt;
&lt;p&gt;Inside &lt;code&gt;foo&lt;/code&gt;, it would need to save the initial
&lt;code&gt;ra&lt;/code&gt;, so it can return back later. Even though
&lt;code&gt;ra&lt;/code&gt; takes only 4 bytes, &lt;code&gt;sp&lt;/code&gt; needs to be 16-byte
aligned at all times, so we round that up to 16 bytes. Decrementing
&lt;code&gt;sp&lt;/code&gt; by 16 we allocate the space:&lt;/p&gt;&lt;code&gt;foo:
    addi sp, sp, -16&lt;/code&gt;
&lt;p&gt;Now, in addition to all of the non call-saved registers, we have 16 bytes of scratch space at &lt;code&gt;sp&lt;/code&gt; through &lt;code&gt;sp + 15&lt;/code&gt;.
We can backup the value of &lt;code&gt;ra&lt;/code&gt; here&lt;/p&gt;&lt;code&gt;    ...
    sw ra, 0(sp)&lt;/code&gt;
&lt;p&gt;Then we just call &lt;code&gt;bar&lt;/code&gt; twice, which overwrites
&lt;code&gt;ra&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;    ...
    jal bar
    jal bar&lt;/code&gt;
&lt;p&gt;At the end of the function, we just need to get back the return address, deallocate the stack space, and return. Although using any register would suffice for the return address, since it is the backed up value of &lt;code&gt;ra&lt;/code&gt; after all, we load it back to
&lt;code&gt;ra&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;    ...
    lw ra, 0(sp)
    addi sp, sp, 16
    ret&lt;/code&gt;
&lt;p&gt;In a similar way you can save and restore the &lt;code&gt;s&lt;/code&gt;
(remember, call-saved) registers. Usually, the most convenient way to
manage this is to put values that need to be preserved across inner
function calls in the &lt;code&gt;s&lt;/code&gt; registers, and then add code at the
beginning to save them, and add code at the end to restore them.&lt;/p&gt;&lt;p&gt;Obligatory recursive Fibonacci time!&lt;/p&gt;&lt;p&gt;The algorithm should be fairly straightforward:&lt;/p&gt;&lt;code&gt;fibonacci(n) {
    if (n &amp;lt; 2) { return n; }
    else { return fib(n - 1) + fib(n - 2); }
}&lt;/code&gt;
&lt;p&gt;What’s worth noting here is the fairly symmetric pattern of saving registers at the start:&lt;/p&gt;&lt;code&gt;    addi sp, sp, -16
    sw ra, 0(sp)
    sw s0, 4(sp)
    sw s1, 8(sp)&lt;/code&gt;
&lt;p&gt;And restoring them at the end:&lt;/p&gt;&lt;code&gt;    lw ra, 0(sp)
    lw s0, 4(sp)
    lw s1, 8(sp)
    addi sp, sp, 16
    ret&lt;/code&gt;
&lt;p&gt;A little thing to also note that the &lt;code&gt;s&lt;/code&gt; registers are
only saved in the more complex branch, where as the simpler branch just
returns directly. This is also acceptable from a calling convention
perspective.&lt;/p&gt;&lt;p&gt;(Note: In the emulator, the &lt;code&gt;sp&lt;/code&gt; register is initialized
to an address that would be convenient for you for use as a stack, as a,
well, convenience.)&lt;/p&gt;&lt;p&gt;Let’s go back to this example:&lt;/p&gt;&lt;code&gt;    # void puts(const char *);
puts:
    lui t1, %hi(0x10000000)
puts_loop:
    lb t0, 0(a0)
    beq t0, zero, puts_done
    sw t0, 0(t1)
    addi a0, a0, 1
    j puts_loop

puts_done:
    ret&lt;/code&gt;
&lt;p&gt;Having to name things like &lt;code&gt;puts_loop&lt;/code&gt;,
&lt;code&gt;puts_done&lt;/code&gt; is a bit annoying. There’s a shorter way: numeric labels.&lt;/p&gt;&lt;p&gt;A numeric label is one with a name of a decimal number. To refer to a numeric label, use the number and a &lt;code&gt;f&lt;/code&gt; suffix for “forward”,
and &lt;code&gt;b&lt;/code&gt; for “backward”, and it will correspond to the nearest
numeric label with that number, searching forwards or backwards,
respectively.&lt;/p&gt;&lt;p&gt;So, the &lt;code&gt;puts&lt;/code&gt; example from earlier can be rewritten:&lt;/p&gt;&lt;code&gt;    # void puts(const char *);
puts:
    lui t1, %hi(0x10000000)
1:
    lb t0, 0(a0)
    beq t0, zero, 2f
    sw t0, 0(t1)
    addi a0, a0, 1
    j 1b

2:
    ret&lt;/code&gt;
&lt;p&gt;Yeah I don’t really like this syntax either, but it is what we’ve got.&lt;/p&gt;&lt;p&gt;Remember that oddball instruction I mentioned way back, &lt;code&gt;auipc&lt;/code&gt;?&lt;/p&gt;&lt;p&gt;I don’t know about your experience, but the first time I saw RISC-V disassembly, this is the one instruction that caught my eye. And this memory has stuck with me ever since. It’s a rather common occurrence in real RISC-V programs, and somehow I’ve been hiding it from you this whole time. If you take a sneak peek at the next section’s title, you’ll see how far we’ve come without &lt;code&gt;auipc&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;So what does it do?&lt;/p&gt;&lt;p&gt;The &lt;code&gt;auipc&lt;/code&gt; (“add
upper immediate to pc”) instruction is very similar to &lt;code&gt;lui&lt;/code&gt;.
Instead of setting &lt;code&gt;rd&lt;/code&gt; to &lt;code&gt;imm20 &amp;lt;&amp;lt; 12&lt;/code&gt;, it
sets it to &lt;code&gt;pc + (imm20 &amp;lt;&amp;lt; 12)&lt;/code&gt;, where &lt;code&gt;pc&lt;/code&gt;
is the address of the &lt;code&gt;auipc&lt;/code&gt; instruction itself.&lt;/p&gt;&lt;code&gt;auipc rd, imm20&lt;/code&gt;
&lt;p&gt;It works very similarly to &lt;code&gt;lui&lt;/code&gt;. You can think of them as
a pair: the “base” of &lt;code&gt;lui&lt;/code&gt; is &lt;code&gt;0&lt;/code&gt;, whereas the
“base” of &lt;code&gt;auipc&lt;/code&gt; is the address of the &lt;code&gt;auipc&lt;/code&gt;
instruction. So this code:&lt;/p&gt;&lt;code&gt;start:
    lui a0, 3
    addi a0, a0, 4&lt;/code&gt;
&lt;p&gt;Gives you &lt;code&gt;0x3004&lt;/code&gt;, whereas this:&lt;/p&gt;&lt;code&gt;start:
    auipc a0, 3
    addi a0, a0, 4&lt;/code&gt;
&lt;p&gt;Gives you &lt;code&gt;start + 0x3004&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Why would you need this? On modern systems, it’s often desirable to have machine code that can be moved around in address space. For example, a shared library i.e. dynamically linked library can be loaded into any program, at any address. It would be helpful if the machine code does not need to be patched every time. This is called position independent code (PIC).&lt;/p&gt;&lt;p&gt;Some instructions already exhibit position independence. For example, as mentioned earlier when we talked about using &lt;code&gt;lui&lt;/code&gt; and
&lt;code&gt;jalr&lt;/code&gt; as a pair, the branch instructions and
&lt;code&gt;jal&lt;/code&gt; are encoded, as with all RV32I instructions, into
32-bit instruction words, so they can’t possibly be able to encode every
possible address. Instead, the jump destination is &lt;code&gt;pc&lt;/code&gt; plus
some offset (&lt;code&gt;pc&lt;/code&gt; being, as before, the jump/branch
instruction itself), and the offset itself is encoded.&lt;/p&gt;&lt;p&gt;You can see these are three different instructions that jump to itself. Since the offset is &lt;code&gt;0&lt;/code&gt; in each case, the encoding is
the same. Use the “Dump” button to see for yourself.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;auipc&lt;/code&gt; instruction allows for very flexible position
independence. You can make arbitrary calculations based on the address
at which code is located. The immediate-bit operand mirroring
&lt;code&gt;lui&lt;/code&gt; means that it is well suited for two-instruction pairs,
just like &lt;code&gt;lui&lt;/code&gt;. These kind of “&lt;code&gt;pc&lt;/code&gt; plus
something” calculations are known as pc-relative
addressing.&lt;/p&gt;&lt;p&gt;The syntax for getting the assembler to generate the immediate values for pc-relative addressing a bit arcane but hear me out:&lt;/p&gt;&lt;p&gt;Like &lt;code&gt;%hi()&lt;/code&gt; and &lt;code&gt;%lo()&lt;/code&gt;, &lt;code&gt;%pcrel_hi()&lt;/code&gt; and &lt;code&gt;%pcrel_lo()&lt;/code&gt; gives you
the immediate values needed for pc-relative addressing. You pass the
label you want to address to &lt;code&gt;%pcrel_hi()&lt;/code&gt;, but pass a label
to the &lt;code&gt;auipc&lt;/code&gt; instruction to
&lt;code&gt;%pcrel_lo()&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Unlike &lt;code&gt;%lo()&lt;/code&gt;, We need the address of the
&lt;code&gt;auipc&lt;/code&gt; instruction itself to calculate the immediate value,
and this is why you need to pass a label to it. You don’t need to write
&lt;code&gt;foo&lt;/code&gt; again, since the assembler will look at the
&lt;code&gt;auipc&lt;/code&gt; instruction and see it’s supposed to be for
&lt;code&gt;foo&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;If you hate writing that, you can also use the convenience pseudoinstruction &lt;code&gt;la&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;la rd, label&lt;/code&gt;
&lt;p&gt;Just like a &lt;code&gt;lui&lt;/code&gt; + &lt;code&gt;jalr&lt;/code&gt; pair, an
&lt;code&gt;auipc&lt;/code&gt; + &lt;code&gt;jalr&lt;/code&gt; can be used to jump to somewhere
farther away than one &lt;code&gt;jal&lt;/code&gt; can reach in position-independent
code.&lt;/p&gt;&lt;p&gt;One very common case is to call a function that might not be within reach of &lt;code&gt;jal&lt;/code&gt;. You can use the pseudoinstruction &lt;code&gt;call&lt;/code&gt; for that.&lt;/p&gt;&lt;code&gt;call label&lt;/code&gt;
&lt;p&gt;This expands to:&lt;/p&gt;&lt;code&gt;1:
    auipc ra, %pcrel_hi(label)
    jalr ra, %pcrel_lo(1b)(ra)&lt;/code&gt;
&lt;p&gt;Notice how &lt;code&gt;ra&lt;/code&gt; is used as a temporary register to store
the intermediate result, which is immediately overwritten by
&lt;code&gt;jalr&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;In fact, there really isn’t any reason to prefer &lt;code&gt;lui&lt;/code&gt;
over &lt;code&gt;auipc&lt;/code&gt; when using a label. This is why you if you
disassemble a real RISC-V program, you see it everywhere, even in
non-position-independent code.&lt;/p&gt;&lt;p&gt;Now would be a good time to take a break, since we’re ready to head into…&lt;/p&gt;&lt;p&gt;We’re going to write an extremely bare bones operating system.&lt;/p&gt;&lt;p&gt;One of the tasks an operating system performs is to control what programs can and cannot do. On RISC-V, the most basic of this control is implemented using privilege levels. RISC-V defines… let’s just say, several privilege levels, but we’re only going to use two here:&lt;/p&gt;&lt;p&gt;The lower the privilege level number goes, the less privileged that level is. Higher privilege levels treat lower privilege levels as generally completely unreliable and untrusted, and must isolate themselves from adversarial software and failures of lower privilege levels.&lt;/p&gt;&lt;p&gt;(However, we won’t be talking about all of the features that make this full isolation possible, and the emulator you’ve been seeing does not have enough features for that anyway. Therefore, the operating system we’ll be building will leave itself unprotected in various ways.)&lt;/p&gt;&lt;p&gt;The privilege levels are sometimes called “modes” for short. And, if that’s not short enough, we can shorten the level names themselves, ending up with M-mode and U-mode. All of the ways to refer to these privilege levels are interchangable.&lt;/p&gt;&lt;p&gt;When a RISC-V machine starts (This is known as “reset”), it begins execution in Machine mode. On a typical “embedded” system where only Machine mode and User mode are implemented, execution begins in the initialization code read from flash memory. This code can either perform what needs to be done itself, or it can be an operating system that manages some tasks, each executing in User mode.&lt;/p&gt;&lt;p&gt;The former design is used for simpler programs, and is analogous to the programs we’ve seen and run so far. The latter is more complicated. We’ll see the basics of how to achieve that soon.&lt;/p&gt;&lt;p&gt;The control and status registers (CSRs) deal with various features that are in some sense “special”. No I don’t have a better explanation of what “special” means.&lt;/p&gt;&lt;p&gt;Six instructions are available for manipulating CSRs.&lt;/p&gt;&lt;code&gt;csrrw rd, csr, rs1
csrrs rd, csr, rs1
csrrc rd, csr, rs1
csrrwi rd, csr, uimm5
csrrsi rd, csr, uimm5
csrrci rd, csr, uimm5&lt;/code&gt;
&lt;p&gt;To refer to a CSR in these instructions, use its name in assembly code. We’ll get to those in a bit.&lt;/p&gt;&lt;p&gt;The pattern works like this. Each of the instructions atomically reads the old value of the CSR, and writes the new value based on some operation performed on the old value and the last operand. The possible operations are:&lt;/p&gt;&lt;code&gt;csrrw&lt;/code&gt; (“CSR read
write”): &lt;code&gt;{ csr = rs1; rd = csr_old; }&lt;/code&gt;&lt;code&gt;csrrs&lt;/code&gt; (“CSR read
set”): &lt;code&gt;{ csr = csr | rs1; rd = csr_old; }&lt;/code&gt;&lt;code&gt;csrrc&lt;/code&gt; (“CSR read
clear”): &lt;code&gt;{ csr = csr &amp;amp; ~rs1; rd = csr_old; }&lt;/code&gt;&lt;p&gt;Where &lt;code&gt;&amp;amp;&lt;/code&gt;, &lt;code&gt;|&lt;/code&gt;, &lt;code&gt;~&lt;/code&gt; are bitwise
“and”, “or”, “not” respectively.&lt;/p&gt;&lt;p&gt;Specifically, note that &lt;code&gt;rd&lt;/code&gt; and &lt;code&gt;rs1&lt;/code&gt; can be
the same. For example, this instruction swaps the value in
&lt;code&gt;a0&lt;/code&gt; and &lt;code&gt;mscratch&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;csrrw a0, mscratch, a0&lt;/code&gt;
&lt;p&gt;For the “immediate” variants, instead of a register, they take an “unsigned”/zero-extended 5-bit immediate value, i.e. an immediate value 0 through 31, inclusive. This is represented using &lt;code&gt;uimm5&lt;/code&gt; in
the assembly syntax description. The operation is the same
otherwise.&lt;/p&gt;&lt;code&gt;csrrwi&lt;/code&gt; (“CSR
read write immediate”): &lt;code&gt;{ csr = uimm5; rd = csr_old; }&lt;/code&gt;&lt;code&gt;csrrsi&lt;/code&gt; (“CSR
read set immediate”):
&lt;code&gt;{ csr = csr | uimm5; rd = csr_old; }&lt;/code&gt;&lt;code&gt;csrrci&lt;/code&gt; (“CSR
read clear immediate”):
&lt;code&gt;{ csr = csr &amp;amp; ~uimm5; rd = csr_old; }&lt;/code&gt;&lt;p&gt;The full feature set of these instructions are designed for manipulating bit fields in CSRs, which we will not be doing that much of in this tutorial. Still, this orthogonal design should be fairly intuitive to remember.&lt;/p&gt;&lt;p&gt;CSRs and fields in CSRs do not behave like general purpose registers: Some of them are read/write, some are read-only. Also, invalid values have special behaviors. We will touch on more details as we introduce the individual CSRs themselves, but one thing you may have noticed is that we don’t seem to have read-only CSR instructions. Read-only access is achieved using special cases in the instruction encodings:&lt;/p&gt;&lt;code&gt;csrrs&lt;/code&gt; and &lt;code&gt;csrrc&lt;/code&gt; do not write to the CSR if
&lt;code&gt;rs1&lt;/code&gt; is &lt;code&gt;x0&lt;/code&gt; (a.k.a. &lt;code&gt;zero&lt;/code&gt;) (Note
that just the value of &lt;code&gt;rs1&lt;/code&gt; being 0 is not enough.)&lt;code&gt;csrrsi&lt;/code&gt; and &lt;code&gt;csrrci&lt;/code&gt; do not write to the CSR
if &lt;code&gt;uimm5&lt;/code&gt; is 0.&lt;p&gt;While we’re at it:&lt;/p&gt;&lt;code&gt;csrrw&lt;/code&gt; and &lt;code&gt;csrrwi&lt;/code&gt; do not read the CSR if
&lt;code&gt;rd&lt;/code&gt; is &lt;code&gt;x0&lt;/code&gt; (a.k.a. &lt;code&gt;zero&lt;/code&gt;). (Note
that writing to &lt;code&gt;x0&lt;/code&gt; has no effect anyway, since it’s
constant 0.)&lt;p&gt;(No standard RISC-V CSR is write-only, or has side effects on read.)&lt;/p&gt;&lt;p&gt;As a convenience, the pseudoinstructions &lt;code&gt;csrr&lt;/code&gt; (“CSR read”) and &lt;code&gt;csrw&lt;/code&gt; (“CSR write”) are
available. &lt;code&gt;csrw csr, rs1&lt;/code&gt; expands to
&lt;code&gt;csrrw x0, csr, rs1&lt;/code&gt;. Meanwhile, &lt;code&gt;csrr rd, csr&lt;/code&gt;
expands specifically to &lt;code&gt;csrrs rd, csr, x0&lt;/code&gt;, just so we can
agree on an encoding.&lt;/p&gt;&lt;code&gt;csrw csr, rs1
csrr rd, csr&lt;/code&gt;
&lt;p&gt;You may have seen these CSR things if you’ve scrolled down on the register view. Yes, we’re finally getting into those.&lt;/p&gt;&lt;p&gt;An example of CSRs is counters. Two basic read-only counters are &lt;code&gt;cycle&lt;/code&gt; and
&lt;code&gt;instret&lt;/code&gt;. These
counters, well, count the number of “cycles” and “instructions
retired”. “Retired” is a technical term basically meaning “successfully
completed”.&lt;/p&gt;&lt;p&gt;Since a 32-bit counter will overflow quite fast, on RV32, the counters have “high” counterparts: &lt;code&gt;cycleh&lt;/code&gt; and &lt;code&gt;instreth&lt;/code&gt;. So, for
example, the full cycle counter has 64 bits, with the lower 32 bits in
the CSR &lt;code&gt;cycle&lt;/code&gt; and higher 32 bits in the CSR
&lt;code&gt;cycleh&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;While the emulator is running, scroll down on the register view panel, and on the bottom you’ll see the values of these counters. For convenience, they’re shown combined, so, &lt;code&gt;cycle = 0x11223344_55667788&lt;/code&gt; means &lt;code&gt;cycleh&lt;/code&gt; is
&lt;code&gt;0x11223344&lt;/code&gt;, and &lt;code&gt;cycle&lt;/code&gt; is
&lt;code&gt;0x55667788&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;On real hardware &lt;code&gt;cycle&lt;/code&gt; is coupled to the clock cycle. In
this emulator, every time you press “Step”, it counts as a cycle. When
you press “Run” and it starts, well, running, a certain number of cycles
happen periodically.&lt;/p&gt;&lt;p&gt;Let’s look at a really simple example:&lt;/p&gt;&lt;p&gt;It takes 4 cycles for this program to stop, but &lt;code&gt;instret&lt;/code&gt;
ends up at only 3 because the final &lt;code&gt;ebreak&lt;/code&gt; instruction
never actually completes.&lt;/p&gt;&lt;p&gt;(Do not confuse “retired” with “retried”.)&lt;/p&gt;&lt;p&gt;A program can read its own counters. For example, this fun little program loops until the cycle count is over 1000, assuming the low 32 bits doesn’t overflow before it has time to react:&lt;/p&gt;&lt;p&gt;Technically &lt;code&gt;cycle&lt;/code&gt; and &lt;code&gt;instret&lt;/code&gt; are not part
of the privileged architecture. The real fun begins now.&lt;/p&gt;&lt;p&gt;The emulator shows the current privilege level as &lt;code&gt;(priv)&lt;/code&gt;. It is in parentheses to remind you of a very
important fact:&lt;/p&gt;&lt;p&gt;There is no CSR for the current privilege level.&lt;/p&gt;&lt;p&gt;In general, it is not possible for a RISC-V program to learn what privilege level it’s in. This is required for the Popek and Goldberg conditions of virtualization to work, specifically because being able to read the current privilege level at a lower-than-maximum privilege level would be a “sensitive” but “unprivileged” instruction.&lt;/p&gt;&lt;p&gt;If you’re writing a program for a certain privilege level, you should simply assume that it is correctly being run at that privilege level.&lt;/p&gt;&lt;p&gt;A fundamental way an operating system does its job is through handling exceptions. In general, exceptions occur when there’s a problem with a specific instruction, and execution cannot continue. For example, since &lt;code&gt;cycle&lt;/code&gt; is a read-only CSR, writing to it is
an illegal instruction:&lt;/p&gt;&lt;p&gt;Since we have no exception handling in the program, we’ll have to inspect what happened manually in the emulator. Indeed, a lot has happened:&lt;/p&gt;&lt;p&gt;Firstly, this message tells you that an exception happened:&lt;/p&gt;&lt;code&gt;[ Exception: Illegal instruction (2) | tval = 0xc0001073, epc = 0x4000000c ]&lt;/code&gt;
&lt;p&gt;The same information is now also available in the CSRs, as follows:&lt;/p&gt;&lt;code&gt;mcause&lt;/code&gt; (“M-mode
trap cause”): The kind of exception.&lt;code&gt;mepc&lt;/code&gt; (“M-mode
exception pc”): The address of the instruction that caused the
exception.&lt;code&gt;mtval&lt;/code&gt; (“M-mode
trap value”): Extra information about the exception.&lt;code&gt;mstatus&lt;/code&gt; (“M-mode
status”): It is set to &lt;code&gt;0x00001800&lt;/code&gt;. The two bits in the
middle, &lt;code&gt;mstatus[12:11]&lt;/code&gt; (In C syntax,
&lt;code&gt;(mstatus &amp;gt;&amp;gt; 11) &amp;amp; 0x3&lt;/code&gt;) is the
&lt;code&gt;mstatus.MPP&lt;/code&gt; (“M-mode previous privilege level”) field,
which contains 3, meaning that the exception occurred while running in
Machine mode.&lt;p&gt;When an exception happens, in addition to recording the exception information in these CSR fields, &lt;code&gt;pc&lt;/code&gt; is set to
&lt;code&gt;mtvec&lt;/code&gt;, which is supposed to be the handler address. Let’s
write ourselves an exception handler that simply prints a message and
stops the emulator, and see the handling in action:&lt;/p&gt;&lt;p&gt;Yeah it just prints &lt;code&gt;Oh no!&lt;/code&gt; on error. Baby steps…&lt;/p&gt;&lt;p&gt;The checkboxes “Pause on exc.” and “Print on exc.” control whether the emulator should pause or print a message, respectively, when an exception occurs. You can uncheck those if you want the exception handler set in the program to run without interference.&lt;/p&gt;&lt;p&gt;(Another case that will cause a jump to &lt;code&gt;mtvec&lt;/code&gt; is interrupts. However, this feature
does not exist in the emulator. The two cases are collectively called
traps.)&lt;/p&gt;&lt;p&gt;These are the exceptions possible in this emulator, and their respective numeric codes:&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Description&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;Instruction address misaligned&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;Instruction access fault&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;2&lt;/cell&gt;&lt;cell&gt;Illegal instruction&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;cell&gt;Breakpoint&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;5&lt;/cell&gt;&lt;cell&gt;Load access fault&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;7&lt;/cell&gt;&lt;cell&gt;Store/AMO access fault&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;8&lt;/cell&gt;&lt;cell&gt;Environment call from User mode&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;11&lt;/cell&gt;&lt;cell&gt;Environment call from Machine mode&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;“Instruction address misaligned” happens when attempting to jump to an instruction that is not 4-byte aligned. The exception happens on the jump or branch instruction, not the target.&lt;/p&gt;&lt;p&gt;“Load access fault” and “Store/AMO access fault” happens when accessing an invalid memory address, or accessing a memory address in an invalid way.&lt;/p&gt;&lt;p&gt;(“AMO” stands for “atomic memory operation”, which we will not talk about and is not featured in the emulator.)&lt;/p&gt;&lt;p&gt;“Illegal instruction” happens not only in the self explanatory way when an invalid instruction is executed, but also when accessing a CSR in an invalid way, or from too low a privilege level.&lt;/p&gt;&lt;p&gt;“Breakpoint”, “Environment call from User mode” and “Environment call from Machine mode” will be explained in a future section.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;mret&lt;/code&gt; (“M-mode
return”) instruction performs the reverse of part of what happens when
an exception occurs. To be precise, what happens is:&lt;/p&gt;&lt;code&gt;mstatus.MPP&lt;/code&gt;&lt;code&gt;mstatus.MPP&lt;/code&gt; is set to 0&lt;code&gt;pc&lt;/code&gt; is set to &lt;code&gt;mepc&lt;/code&gt;&lt;p&gt;(You can think of the privilege mode bits as shifting in a chain &lt;code&gt;0 → MPP → priv&lt;/code&gt;. And, to be even more precise,
&lt;code&gt;mstatus.MPP&lt;/code&gt; is set to the lowest supported privilege mode
since it’s not supposed to contain unsupported modes.)&lt;/p&gt;&lt;p&gt;&lt;code&gt;mret&lt;/code&gt; takes no operands, so the assembly syntax is
simply:&lt;/p&gt;&lt;code&gt;mret&lt;/code&gt;
&lt;p&gt;If we do &lt;code&gt;mret&lt;/code&gt; after getting an exception, then we simply
go back to retrying the same instruction again. This is useful for more
featureful implementations, where for example, after handling a page
fault the correct course of action is to retry the faulting
instruction.&lt;/p&gt;&lt;p&gt;However, &lt;code&gt;mstatus&lt;/code&gt; and &lt;code&gt;mepc&lt;/code&gt; are also
writable. This gives us more flexibility in the use of
&lt;code&gt;mret&lt;/code&gt;. As an analogy, the same &lt;code&gt;jr&lt;/code&gt; instruction
(really &lt;code&gt;jalr&lt;/code&gt; instruction) can be used to return from a
call, and also can be used to jump to any address. Similarly,
&lt;code&gt;mret&lt;/code&gt; not only lets us return from an exception, but also
lets us jump to any address and switch to any privilege
level.&lt;/p&gt;&lt;p&gt;Even though &lt;code&gt;mret&lt;/code&gt; is named “return”, it is in fact the
only way to lower the privilege level to enter User mode.
Here’s an example of entering User mode, with a User mode program that
does something bad:&lt;/p&gt;&lt;p&gt;As you can see, after we enter User mode, all of the CSRs used for exception handling become completely inaccessible, not even readable. As with writing a read-only CSR, accessing an CSR without permission also causes an illegal instruction exception.&lt;/p&gt;&lt;p&gt;Moreover, when an exception happens, we go back to Machine mode, so the exception handler runs in Machine mode. Here the handler does nothing except stopping the emulator.&lt;/p&gt;&lt;p&gt;Sometimes, a program may wish to intentionally cause an exception. There are several well-defined way to do that:&lt;/p&gt;&lt;code&gt;unimp&lt;/code&gt; has the same encoding
as &lt;code&gt;csrrw zero, cycle, zero&lt;/code&gt;, and it is the canonical RV32I
illegal instruction. It causes causes an “Illegal instruction”
exception.&lt;code&gt;ebreak&lt;/code&gt; causes a
“Breakpoint” exception&lt;code&gt;ecall&lt;/code&gt; causes an
“Environment call from User mode” exception when executed in User mode,
and “Environment call from Machine mode” exception when executed in
Machine mode.&lt;p&gt;Give those exceptions a try here:&lt;/p&gt;&lt;p&gt;As the names suggest, &lt;code&gt;ebreak&lt;/code&gt; is used for debugging
breakpoints. As a special case, in this emulator &lt;code&gt;ebreak&lt;/code&gt; in
Machine mode stops the emulator. You can think of it as the emulator
being a debugger, and the debugger catching the breakpoint.&lt;/p&gt;&lt;p&gt;&lt;code&gt;unimp&lt;/code&gt; can be used to intentionally crash a program upon
detection of some unrecoverable error.&lt;/p&gt;&lt;p&gt;Meanwhile, &lt;code&gt;ecall&lt;/code&gt; is used for things like system calls.
“Environment call from User mode” is a distinct exception cause code to
make it easy to check specifically for this case.&lt;/p&gt;&lt;p&gt;One thing that you would want in your trap handler is to not trust or disturb any general purpose registers in the code that the trap occurred in, unless you intentionally want to do so, for example to return a value from a system call. So you’d want to save all the registers to memory, before doing anything else. However, accessing memory requires a general purpose register.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;mscratch&lt;/code&gt;
(“M-mode scratch”) CSR can help with this. This register, unlike all the
others, have no special functionality. It can hold any 32-bit value.
However, like all the other M-mode CSRs, it can only be accessed in
Machine mode. User mode code cannot change the value of it.&lt;/p&gt;&lt;p&gt;So for example, you can stash the operating system stack pointer in &lt;code&gt;mscratch&lt;/code&gt; before switching to User mode, and it will stay in
&lt;code&gt;mscratch&lt;/code&gt; untouched in User mode. At the top of the handler,
&lt;code&gt;csrrw sp, mscratch, sp&lt;/code&gt; to swap from the user stack pointer
to the operating system stack pointer.&lt;/p&gt;&lt;code&gt;handler:
    csrrw sp, mscratch, sp
    # Save registers except sp
    csrr t0, mscratch
    # t0 = user sp, save it
    # Save user pc
    ...&lt;/code&gt;
&lt;p&gt;And, to restore:&lt;/p&gt;&lt;code&gt;    lw t0, ... # Load user pc
    csrw mepc, t0
    lw t0, ... # Load user sp
    csrw mscratch, t0
    # Restore registers except sp
    csrrw sp, mscratch, sp
    mret&lt;/code&gt;
&lt;p&gt;We’ll see the full code for this in the following section.&lt;/p&gt;&lt;p&gt;We have enough of to write a very very bare bones operating system. It will support these features:&lt;/p&gt;&lt;code&gt;a7 = 1&lt;/code&gt;: putchar, &lt;code&gt;a0&lt;/code&gt; is the byte to
write&lt;code&gt;a7 = 2&lt;/code&gt;: exit&lt;p&gt;We design the exception handling as follows:&lt;/p&gt;&lt;code&gt;mscratch&lt;/code&gt; is 0.&lt;code&gt;mscratch&lt;/code&gt; points to the operating
system stack pointer&lt;code&gt;mscratch&lt;/code&gt; is 0, the exception came
from M-mode, which we cannot handle, so we report a fatal
exception.&lt;code&gt;trap_main&lt;/code&gt;, which manipulates
U-mode registers in memory&lt;code&gt;trap_main&lt;/code&gt;, we restore registers from memory,
deallocate the space from the stack, and go back to U-mode, as outlined
in the previous section.&lt;p&gt;The structure to save registers in is fairly simple:&lt;/p&gt;&lt;code&gt;struct regs {
  unsigned long pc;
  unsigned long ra; // x1
  unsigned long sp; // x2
  ...
  unsigned long t6; // x31
};&lt;/code&gt;
&lt;p&gt;Basically you can think of it as an array where element 0 is &lt;code&gt;pc&lt;/code&gt;, and elements 1 through 31 are registers x1 through
x31.&lt;/p&gt;&lt;p&gt;Inside &lt;code&gt;trap_main&lt;/code&gt;, we check &lt;code&gt;mcause&lt;/code&gt; to see if
it’s a system call. If it is, we dispatch based on &lt;code&gt;a7&lt;/code&gt;. If
it’s not, we report an exception from U-mode.&lt;/p&gt;&lt;p&gt;At the beginning, we simply initialize the &lt;code&gt;struct regs&lt;/code&gt;
structure on stack, initialize user &lt;code&gt;sp&lt;/code&gt; and &lt;code&gt;pc&lt;/code&gt;
in it, and jump to the same code that handles returning to U-mode.&lt;/p&gt;&lt;p&gt;Here’s the assembly code with User mode code at the bottom. You may want to uncheck “Pause on exc.” and “Print on exc.” for convenience.&lt;/p&gt;&lt;p&gt;Do not be too hard on yourself if you have trouble understanding the code fully. This is, after all, a fairly complete OS kernel entry and exit implementation. Really, the most important part I’m showing you here is that it is possible.&lt;/p&gt;&lt;p&gt;For reference, here’s some of the OS code in pseudo-C.&lt;/p&gt;&lt;code&gt;void trap_main(struct regs *regs) {
    unsigned long cause = csr_read(mcause);
    if (cause != 8)
        do_bad_exception(regs, cause);

    # Call do_syscall with args from ecall
    unsigned long ret = do_syscall(regs-&amp;gt;a0, ..., regs-&amp;gt;a7);
    regs-&amp;gt;a0 = ret;

    // Bump user pc by 4, skip over ecall instruction
    regs-&amp;gt;pc += 4;
}

unsigned long do_syscall(
    unsigned long a0,
    ...,
    unsigned long a7
) {
    if (a7 == 1)
        sys_putchar(a0);
    else if (a7 == 8)
        sys_exit();
    else
        return -1;
}

unsigned long sys_putchar(char a) {
    kputchar(a);
    return 0;
}

[[noreturn]]
unsigned long sys_exit(char a) {
    ebreak();
}

[[noreturn]]
void do_bad_exception(struct regs *regs, unsigned long cause) {
    kputs("Exception 0x");
    kputchar(hex_chars[cause]);
    kputchar('\n');
    ebreak();
}

[[noreturn]]
void fatal() {
    kputs("Fatal exception\n");
    ebreak();
}

void kputs(const char *str) {
    while (*str) {
        u32 val = (u32)*str;
        writel(0x10000000, val); // MMIO write
        str ++;
    }
}

void kputchar(char c) {
    u32 val = (u32)c;
    writel(0x10000000, val); // MMIO write
}&lt;/code&gt;
&lt;p&gt;And here’s the user code, again in pseudo C:&lt;/p&gt;&lt;code&gt;[[noreturn]]
void user_entry() {
    puts(...);
    exit();
}

void puts(const char *str) {
    while (*str) {
        putchar(*str);
        str ++;
    }
}

void putchar(char c) {
    ecall(a0 = c, a7 = 1);
}

void exit() {
    ecall(a7 = 2);
}&lt;/code&gt;
&lt;p&gt;As long as this tutorial is, some simplifications have been made. Here are some of the most egregious lies and omissions, compared to the “real” RISC-V architecture and “real” RISC-V assembly code found in the world:&lt;/p&gt;&lt;code&gt;li&lt;/code&gt; pseudoinstruction should support a wider range
of constants.&lt;code&gt;mstatus&lt;/code&gt; is a lot more complicated than what I have
described.&lt;code&gt;%hi&lt;/code&gt;, &lt;code&gt;%lo&lt;/code&gt;, &lt;code&gt;%pcrel_hi&lt;/code&gt;,
&lt;code&gt;%pcrel_lo&lt;/code&gt; are more complicated than what I have
described.&lt;p&gt;There are also very important topics that are common or even ubiquitous in the RISC-V world, but I chose not to cover:&lt;/p&gt;&lt;p&gt;However, what I’ve taught you should be more than enough to get you started into learning more on your own, or with further materials.&lt;/p&gt;&lt;p&gt;Here are some references and tutorials I would personally recommend, if you’re looking to get further into RISC-V low-level development&lt;/p&gt;&lt;p&gt;Other useful resources that I have used while writing this tutorial:&lt;/p&gt;&lt;code&gt;arch/riscv/kernel/entry.S&lt;/code&gt; from Linux https://elixir.bootlin.com/linux/latest/source/arch/riscv/kernel/entry.S&lt;p&gt;Thanks to these folks for UI design help and content suggestions:&lt;/p&gt;&lt;p&gt;And thanks to you for coming along with me on this journey. Come on over to https://github.com/dramforever/easyriscv if you have suggestions, grievances, or just want to share some thoughts.&lt;/p&gt;&lt;p&gt;This tutorial is provided under the CC0 license. To the maximum extent permitted by law, this tutorial is dedicated to the public domain.&lt;/p&gt;&lt;p&gt;The associated code in the repository is provided under, of your choosing, either the CC0 license or the 0-clause “BSD” license.&lt;/p&gt;&lt;code&gt;add&lt;/code&gt;&lt;code&gt;addi&lt;/code&gt;&lt;code&gt;and&lt;/code&gt;&lt;code&gt;andi&lt;/code&gt;&lt;code&gt;auipc&lt;/code&gt;&lt;code&gt;beq&lt;/code&gt;&lt;code&gt;bge&lt;/code&gt;&lt;code&gt;bgeu&lt;/code&gt;&lt;code&gt;blt&lt;/code&gt;&lt;code&gt;bltu&lt;/code&gt;&lt;code&gt;bne&lt;/code&gt;&lt;code&gt;call&lt;/code&gt;&lt;code&gt;csrr&lt;/code&gt;&lt;code&gt;csrrc&lt;/code&gt;&lt;code&gt;csrrci&lt;/code&gt;&lt;code&gt;csrrs&lt;/code&gt;&lt;code&gt;csrrsi&lt;/code&gt;&lt;code&gt;csrrw&lt;/code&gt;&lt;code&gt;csrrwi&lt;/code&gt;&lt;code&gt;csrw&lt;/code&gt;&lt;code&gt;ebreak&lt;/code&gt;&lt;code&gt;ecall&lt;/code&gt;&lt;code&gt;j&lt;/code&gt;&lt;code&gt;jal&lt;/code&gt;&lt;code&gt;jalr&lt;/code&gt;&lt;code&gt;jr&lt;/code&gt;&lt;code&gt;la&lt;/code&gt;&lt;code&gt;lb&lt;/code&gt;&lt;code&gt;lbu&lt;/code&gt;&lt;code&gt;lh&lt;/code&gt;&lt;code&gt;lhu&lt;/code&gt;&lt;code&gt;li&lt;/code&gt;&lt;code&gt;lui&lt;/code&gt;&lt;code&gt;lw&lt;/code&gt;&lt;code&gt;mret&lt;/code&gt;&lt;code&gt;mv&lt;/code&gt;&lt;code&gt;or&lt;/code&gt;&lt;code&gt;ori&lt;/code&gt;&lt;code&gt;ret&lt;/code&gt;&lt;code&gt;sb&lt;/code&gt;&lt;code&gt;sh&lt;/code&gt;&lt;code&gt;sll&lt;/code&gt;&lt;code&gt;slli&lt;/code&gt;&lt;code&gt;slt&lt;/code&gt;&lt;code&gt;slti&lt;/code&gt;&lt;code&gt;sltiu&lt;/code&gt;&lt;code&gt;sltu&lt;/code&gt;&lt;code&gt;sra&lt;/code&gt;&lt;code&gt;srai&lt;/code&gt;&lt;code&gt;srl&lt;/code&gt;&lt;code&gt;srli&lt;/code&gt;&lt;code&gt;sub&lt;/code&gt;&lt;code&gt;sw&lt;/code&gt;&lt;code&gt;unimp&lt;/code&gt;&lt;code&gt;xor&lt;/code&gt;&lt;code&gt;xori&lt;/code&gt;&lt;code&gt;imm&lt;/code&gt;&lt;code&gt;pc&lt;/code&gt;&lt;code&gt;rd&lt;/code&gt;&lt;code&gt;rs1&lt;/code&gt;&lt;code&gt;rs2&lt;/code&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45726192</guid><pubDate>Mon, 27 Oct 2025 20:57:12 +0000</pubDate></item><item><title>OpenAI says over a million people talk to ChatGPT about suicide weekly</title><link>https://techcrunch.com/2025/10/27/openai-says-over-a-million-people-talk-to-chatgpt-about-suicide-weekly/</link><description>&lt;doc fingerprint="340b5d718824effe"&gt;
  &lt;main&gt;
    &lt;p&gt;OpenAI released new data on Monday illustrating how many of ChatGPT’s users are struggling with mental health issues and talking to the AI chatbot about it. The company says that 0.15% of ChatGPT’s active users in a given week have “conversations that include explicit indicators of potential suicidal planning or intent.” Given that ChatGPT has more than 800 million weekly active users, that translates to more than a million people a week.&lt;/p&gt;
    &lt;p&gt;The company says a similar percentage of users show “heightened levels of emotional attachment to ChatGPT,” and that hundreds of thousands of people show signs of psychosis or mania in their weekly conversations with the AI chatbot.&lt;/p&gt;
    &lt;p&gt;OpenAI says these types of conversations in ChatGPT are “extremely rare,” and thus difficult to measure. That said, the company estimates these issues affect hundreds of thousands of people every week.&lt;/p&gt;
    &lt;p&gt;OpenAI shared the information as part of a broader announcement about its recent efforts to improve how models respond to users with mental health issues. The company claims its latest work on ChatGPT involved consulting with more than 170 mental health experts. OpenAI says these clinicians observed that the latest version of ChatGPT “responds more appropriately and consistently than earlier versions.”&lt;/p&gt;
    &lt;p&gt;In recent months, several stories have shed light on how AI chatbots can adversely affect users struggling with mental health challenges. Researchers have previously found that AI chatbots can lead some users down delusional rabbit holes, largely by reinforcing dangerous beliefs through sycophantic behavior.&lt;/p&gt;
    &lt;p&gt;Addressing mental health concerns in ChatGPT is quickly becoming an existential issue for OpenAI. The company is currently being sued by the parents of a 16-year-old boy who confided his suicidal thoughts to ChatGPT in the weeks leading up to his suicide. State attorneys general from California and Delaware — which could block the company’s planned restructuring — have also warned OpenAI that it needs to protect young people who use their products.&lt;/p&gt;
    &lt;p&gt;Earlier this month, OpenAI CEO Sam Altman claimed in a post on X that the company has “been able to mitigate the serious mental health issues” in ChatGPT, though he did not provide specifics. The data shared on Monday appears to be evidence for that claim, though it raises broader issues about how widespread the problem is. Nevertheless, Altman said OpenAI would be relaxing some restrictions, even allowing adult users to start having erotic conversations with the AI chatbot.&lt;/p&gt;
    &lt;head rend="h3"&gt;2-FOR-1 DISCOUNT: Bring a +1 and save 60%&lt;/head&gt;
    &lt;head rend="h4"&gt;Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, Vinod Khosla — some of the 250+ heavy hitters leading 200+ sessions designed to deliver the insights that fuel startup growth and sharpen your edge. And don’t miss 300+ showcasing startups in all sectors.&lt;lb/&gt;Bring a +1 and save 60% on their pass, or get your pass by Oct 27 to save up to $444.&lt;/head&gt;
    &lt;head rend="h3"&gt;2-FOR-1 DISCOUNT: Bring a +1 and save 60%&lt;/head&gt;
    &lt;head rend="h4"&gt;Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, Vinod Khosla — some of the 250+ heavy hitters leading 200+ sessions designed to deliver the insights that fuel startup growth and sharpen your edge. And don’t miss 300+ showcasing startups in all sectors. Bring a +1 and save 60% on their pass, or get your pass by Oct 27 to save up to $444.&lt;/head&gt;
    &lt;p&gt;In the Monday announcement, OpenAI claims the recently updated version of GPT-5 responds with “desirable responses” to mental health issues roughly 65% more than the previous version. On an evaluation testing AI responses around suicidal conversations, OpenAI says its new GPT-5 model is 91% compliant with the company’s desired behaviors, compared to 77% for the previous GPT‑5 model.&lt;/p&gt;
    &lt;p&gt;The company also says its latest version of GPT-5 also holds up to OpenAI’s safeguards better in long conversations. OpenAI has previously flagged that its safeguards were less effective in long conversations.&lt;/p&gt;
    &lt;p&gt;On top of these efforts, OpenAI says it’s adding new evaluations to measure some of the most serious mental health challenges facing ChatGPT users. The company says its baseline safety testing for AI models will now include benchmarks for emotional reliance and non-suicidal mental health emergencies.&lt;/p&gt;
    &lt;p&gt;OpenAI has also recently rolled out more controls for parents of children who use ChatGPT. The company says it’s building an age prediction system to automatically detect children using ChatGPT, and impose a stricter set of safeguards.&lt;/p&gt;
    &lt;p&gt;Still, it’s unclear how persistent the mental health challenges around ChatGPT will be. While GPT-5 seems to be an improvement over previous AI models in terms of safety, there still seems to be a slice of ChatGPT’s responses that OpenAI deems “undesirable.” OpenAI also still makes its older and less-safe AI models, including GPT-4o, available for millions of its paying subscribers.&lt;/p&gt;
    &lt;p&gt;If you or someone you know needs help, call 1-800-273-8255 for the National Suicide Prevention Lifeline. You can also text HOME to 741-741 for free; text 988; or get 24-hour support from the Crisis Text Line. Outside of the U.S., please visit the International Association for Suicide Prevention for a database of resources.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45727060</guid><pubDate>Mon, 27 Oct 2025 22:26:30 +0000</pubDate></item><item><title>Linux VM without VM software – User Mode Linux</title><link>https://popovicu.com/posts/linux-vm-without-vm-software-user-mode/</link><description>&lt;doc fingerprint="751b991d55a32fe1"&gt;
  &lt;main&gt;
    &lt;p&gt;If you carefully read the Linux kernel docs, you will find an interesting statement:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Linux has also been ported to itself. You can now run the kernel as a userspace application - this is called UserMode Linux (UML).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Today, we’ll explore how you can start an unconventional VM by running a Linux kernel as a process within the Linux kernel itself. This approach doesn’t require installing virtualization software like QEMU, nor does it need root privileges, which opens up some intriguing possibilities.&lt;/p&gt;
    &lt;head rend="h2"&gt;Table of contents&lt;/head&gt;
    &lt;head&gt;Open Table of contents&lt;/head&gt;
    &lt;head rend="h2"&gt;Kernel’s Hardware Abstraction&lt;/head&gt;
    &lt;p&gt;A fundamental responsibility of the kernel is to abstract hardware and offer a consistent interface to userspace. This includes managing shared resources like the CPU and memory for multiple tasks. The kernel determines the underlying hardware (e.g., through a device tree on some platforms, which lists system components) and connects the appropriate drivers.&lt;/p&gt;
    &lt;p&gt;This hardware can also be entirely virtual. In a QEMU virtual machine, for instance, resources like memory and attached disks are virtualized by the QEMU userspace application, incurring a certain performance overhead. The CPU presents an interesting case, as it too can be virtualized in userspace, particularly when emulating a different architecture.&lt;/p&gt;
    &lt;p&gt;A fascinating aspect of drivers for virtualized hardware is that they can be enlightened — or, more formally, paravirtualized. This means the drivers are aware they’re running on virtualized hardware and can leverage this by communicating with the hardware in specialized ways. While the specifics are complex, one can imagine drivers interacting with virtual hardware in ways not feasible with physical counterparts. Online sources suggest that paravirtualization can achieve performance levels close to those of physical devices using traditional drivers.&lt;/p&gt;
    &lt;head rend="h2"&gt;UML - Kernel in a Userspace Process&lt;/head&gt;
    &lt;p&gt;Personally, I view UML as a paravirtualized kernel configuration. Instead of running directly on bare metal, the UML kernel operates atop an existing kernel instance, leveraging some of its userspace functionalities. For instance, rather than linking the console driver to a physical UART, it can utilize standard userspace input/output. Similarly, a block device driver can target a file on the host’s filesystem instead of a physical disk.&lt;/p&gt;
    &lt;p&gt;In this setup, UML is essentially a userspace process that cleverly employs concepts like files and sockets to launch a new Linux kernel instance capable of running its own processes. The exact mapping of these processes to the host — specifically, how the CPU is virtualized — is something I’m not entirely clear on, and I’d welcome insights in the comments. One could envision an implementation where guest threads and processes map to host counterparts but with restricted system visibility, akin to containers, yet still operating within a nested Linux kernel.&lt;/p&gt;
    &lt;p&gt;This page from the kernel’s documentation has a pretty good illustration of what this looks like:&lt;/p&gt;
    &lt;code&gt;            +----------------+
            | Process 2 | ...|
+-----------+----------------+
| Process 1 | User-Mode Linux|
+----------------------------+
|       Linux Kernel         |
+----------------------------+
|         Hardware           |
+----------------------------+&lt;/code&gt;
    &lt;p&gt;I highly recommend checking out that page for more detailed documentation, particularly for the compelling reasons listed for its usefulness. The final point is especially appealing:&lt;/p&gt;
    &lt;quote&gt;
      &lt;item&gt;It’s extremely fun.&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;And that’s precisely why we’re diving into it today!&lt;/p&gt;
    &lt;head rend="h2"&gt;Building a UML Kernel&lt;/head&gt;
    &lt;p&gt;First things first: it’s crucial to understand that a UML kernel can run only on x86 platforms. You can layer an x86 UML kernel on top of an existing x86 kernel; as far as I know, no other configurations are supported.&lt;/p&gt;
    &lt;p&gt;Next, we’ll build the UML binary. The configuration process starts with:&lt;/p&gt;
    &lt;code&gt;ARCH=um make menuconfig&lt;/code&gt;
    &lt;p&gt;You can configure the kernel much like you normally would. You’ll immediately notice several UML-specific options on the initial configuration page. I tend to think of these as “enlightened” drivers, designed to use the host’s userspace facilities as virtual hardware.&lt;/p&gt;
    &lt;p&gt;For this demonstration, I specifically enabled the &lt;code&gt;BLK_DEV_UBD&lt;/code&gt; option. The documentation explains:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The User-Mode Linux port includes a driver called UBD which will let you access arbitrary files on the host computer as block devices. Unless you know that you do not need such virtual block devices, say Y here.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This option wasn’t enabled by default (which surprised me a bit), so I recommend setting it to &lt;code&gt;Y&lt;/code&gt;. Once you’ve finalized your configuration, building is straightforward:&lt;/p&gt;
    &lt;code&gt;ARCH=um make -j16&lt;/code&gt;
    &lt;p&gt;And this produces a &lt;code&gt;linux&lt;/code&gt; binary right there!&lt;/p&gt;
    &lt;code&gt;$ file linux
linux: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, for GNU/Linux 3.2.0, BuildID[sha1]=742d088d46f7c762b29257e4c44042f321dc4ad5, with debug_info, not stripped&lt;/code&gt;
    &lt;p&gt;Interestingly, it’s dynamically linked to the C standard library:&lt;/p&gt;
    &lt;code&gt;$ ldd linux
        linux-vdso.so.1 (0x00007ffc0a3ce000)
        libc.so.6 =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f3490409000)
        /lib64/ld-linux-x86-64.so.2 (0x00007f3490601000)&lt;/code&gt;
    &lt;head rend="h2"&gt;Building Userspace&lt;/head&gt;
    &lt;p&gt;To do anything meaningful within our nested kernel, we need a userspace. For simplicity, I chose to download the latest Buildroot and build it for x86/64.&lt;/p&gt;
    &lt;p&gt;If you’re feeling adventurous and want to try building a minimal userspace from scratch but aren’t sure where to begin, pairing this with the micro Linux distro exercise could be a lot of fun.&lt;/p&gt;
    &lt;head rend="h2"&gt;Running the Nested Kernel&lt;/head&gt;
    &lt;p&gt;To make things interesting, I decided to provide a block device to the nested kernel, write some data to it, and then verify that data from the host system.&lt;/p&gt;
    &lt;p&gt;First, let’s create the disk image:&lt;/p&gt;
    &lt;code&gt;$ dd if=/dev/urandom of=./disk.ext4 bs=1M count=100&lt;/code&gt;
    &lt;p&gt;Next, we’ll format it with ext4:&lt;/p&gt;
    &lt;code&gt;$ sudo mkfs.ext4 ./disk.ext4&lt;/code&gt;
    &lt;p&gt;Now, it’s time to fire up the kernel in userspace. I’ll use the Buildroot image (an &lt;code&gt;ext2&lt;/code&gt; file provided by Buildroot) as the root filesystem:&lt;/p&gt;
    &lt;code&gt;./linux ubd0=/tmp/uml/rootfs.ext2 ubd1=/tmp/uml/disk.ext4 root=/dev/ubda&lt;/code&gt;
    &lt;p&gt;And just like that, we’re greeted by a very familiar kernel boot sequence!&lt;/p&gt;
    &lt;code&gt;Core dump limits :
        soft - 0
        hard - NONE
Checking that ptrace can change system call numbers...OK
Checking syscall emulation for ptrace...OK
Checking environment variables for a tempdir...none found
Checking if /dev/shm is on tmpfs...OK
Checking PROT_EXEC mmap in /dev/shm...OK
Linux version 6.14.7 (uros@debian-home) (gcc (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #6 Mon May 19 16:27:13 PDT 2025
Zone ranges:
  Normal   [mem 0x0000000000000000-0x0000000063ffffff]
Movable zone start for each node
Early memory node ranges
  node   0: [mem 0x0000000000000000-0x0000000003ffffff]
Initmem setup node 0 [mem 0x0000000000000000-0x0000000003ffffff]
random: crng init done
Kernel command line: ubd0=/tmp/uml/rootfs.ext2 ubd1=/tmp/uml/disk.ext4 root=/dev/ubda console=tty0
printk: log buffer data + meta data: 16384 + 57344 = 73728 bytes
Dentry cache hash table entries: 8192 (order: 4, 65536 bytes, linear)
Inode-cache hash table entries: 4096 (order: 3, 32768 bytes, linear)
Sorting __ex_table...
Built 1 zonelists, mobility grouping on.  Total pages: 16384
mem auto-init: stack:all(zero), heap alloc:off, heap free:off
SLUB: HWalign=64, Order=0-3, MinObjects=0, CPUs=1, Nodes=1
NR_IRQS: 64
clocksource: timer: mask: 0xffffffffffffffff max_cycles: 0x1cd42e205, max_idle_ns: 881590404426 ns
Calibrating delay loop... 8931.73 BogoMIPS (lpj=44658688)
Checking that host ptys support output SIGIO...Yes
pid_max: default: 32768 minimum: 301
Mount-cache hash table entries: 512 (order: 0, 4096 bytes, linear)
Mountpoint-cache hash table entries: 512 (order: 0, 4096 bytes, linear)
Memory: 57488K/65536K available (3562K kernel code, 944K rwdata, 1244K rodata, 165K init, 246K bss, 7348K reserved, 0K cma-reserved)
...&lt;/code&gt;
    &lt;p&gt;and at the end, we have the Buildroot login:&lt;/p&gt;
    &lt;code&gt;Run /sbin/init as init process
EXT4-fs (ubda): warning: mounting unchecked fs, running e2fsck is recommended
EXT4-fs (ubda): re-mounted 23cafb4d-e18f-4af4-829d-f0dc7303e6c4 r/w. Quota mode: none.
EXT4-fs error (device ubda): ext4_mb_generate_buddy:1217: group 1, block bitmap and bg descriptor inconsistent: 7466 vs 7467 free clusters
Seeding 256 bits and crediting
Saving 256 bits of creditable seed for next boot
Starting syslogd: OK
Starting klogd: OK
Running sysctl: OK
Starting network: OK
Starting crond: OK

Welcome to Buildroot
buildroot login:&lt;/code&gt;
    &lt;p&gt;The boot process was surprisingly quick.&lt;/p&gt;
    &lt;p&gt;Now, let’s create a mountpoint for our disk within the UML instance:&lt;/p&gt;
    &lt;code&gt;# mkdir /mnt/disk&lt;/code&gt;
    &lt;p&gt;Then, we mount the second UBD device (&lt;code&gt;ubdb&lt;/code&gt;) to this mountpoint:&lt;/p&gt;
    &lt;code&gt;# mount /dev/ubdb /mnt/disk/&lt;/code&gt;
    &lt;p&gt;With the disk mounted, we can write a test file:&lt;/p&gt;
    &lt;code&gt;# echo "This is a UML test!" &amp;gt; /mnt/disk/foo.txt
# cat /mnt/disk/foo.txt
This is a UML test!&lt;/code&gt;
    &lt;p&gt;I can now shut down the UML VM:&lt;/p&gt;
    &lt;code&gt;# poweroff&lt;/code&gt;
    &lt;p&gt;which gives&lt;/p&gt;
    &lt;code&gt;# Stopping crond: stopped /usr/sbin/crond (pid 64)
OK
Stopping network: OK
Stopping klogd: OK
Stopping syslogd: stopped /sbin/syslogd (pid 40)
OK
Seeding 256 bits and crediting
Saving 256 bits of creditable seed for next boot
EXT4-fs (ubdb): unmounting filesystem e950822b-09f7-49c2-bb25-9755a249cfa1.
umount: devtmpfs busy - remounted read-only
EXT4-fs (ubda): re-mounted 23cafb4d-e18f-4af4-829d-f0dc7303e6c4 ro. Quota mode: none.
The system is going down NOW!
Sent SIGTERM to all processes
Sent SIGKILL to all processes
Requesting system poweroff
reboot: Power down&lt;/code&gt;
    &lt;p&gt;On my host system:&lt;/p&gt;
    &lt;code&gt;$ sudo mount ./disk.ext4 ./img&lt;/code&gt;
    &lt;code&gt;$ cat ./img/foo.txt
This is a UML test!&lt;/code&gt;
    &lt;p&gt;This little experiment confirms that we successfully ran a VM using UML, wrote data to a block device within it, and those changes persisted, accessible from the host system.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Throughout this article, I’ve referred to UML as a VM, and you’d be right to raise an eyebrow. On one hand, it embodies the idea of hardware virtualization via host userspace facilities, and the environment gets its own distinct kernel. On the other hand, this guest kernel is intrinsically linked to the host’s kernel. While it aims for isolation, it doesn’t achieve the same level you’d expect from a QEMU VM powered by KVM.&lt;/p&gt;
    &lt;p&gt;What’s the real-world utility here? Is UML suitable for running isolated workloads? My educated guess is: probably not for most production scenarios. I believe UML’s primary strength lies in kernel debugging, rather than serving as a full-fledged, production-ready virtualization stack. For robust VM needs, KVM virtualization (operating at a different architectural layer) is far more battle-tested. Of course, containers offer another alternative if sharing the host kernel is acceptable for your workloads. UML carves out an interesting niche between these two: offering a separate kernel instance while still maintaining a unique connection to the host kernel. It’s a fascinating concept.&lt;/p&gt;
    &lt;p&gt;Perhaps in the future, this intriguing technology will garner more attention and see wider adoption. For now, though, it’s a fantastic tool for experimentation and, at the very least, a lot of fun to play with!&lt;/p&gt;
    &lt;p&gt;Happy hacking!&lt;/p&gt;
    &lt;p&gt;For updates, please consider following me on Twitter/X and LinkedIn.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45727097</guid><pubDate>Mon, 27 Oct 2025 22:30:59 +0000</pubDate></item><item><title>Iroh-blobs</title><link>https://www.iroh.computer/blog/iroh-blobs-0-95-new-features</link><description>&lt;doc fingerprint="1e5ea82579a148e8"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;iroh-blobs 0.95 - New features&lt;/head&gt;by rklaehn&lt;p&gt;Iroh-blobs 0.95 contains a number of significant new features that are worth explaining in detail. There are several new features that are useful for blobs users and also for iroh users in general.&lt;/p&gt;&lt;p&gt;Let's start with a feature that is essential for blobs itself, but can also be useful for many other protocols.&lt;/p&gt;&lt;head rend="h1"&gt;Connection pool&lt;/head&gt;&lt;p&gt;There is a new connection pool in &lt;code&gt;util::connection_pool&lt;/code&gt;. This is useful whenever you have a protocol that has to talk to a large number of endpoints while keeping an upper bound of concurrent open connections. In blobs, this is used whenever you use the downloader to orchestrate blobs downloads from multiple providers.&lt;/p&gt;&lt;p&gt;Iroh connections are relatively lightweight, but even so you don't want to keep thousands of them open at the same time. But opening a new connection every time you do a small exchange with a peer is very wasteful. The &lt;code&gt;ConnectionPool&lt;/code&gt; gives you an API to deal with these tradeoffs.&lt;/p&gt;&lt;head rend="h2"&gt;Basic usage&lt;/head&gt;&lt;p&gt;Let's first look at basic usage:&lt;/p&gt;&lt;code&gt;let pool = ConnectionPool::new(ep, iroh_blobs::ALPN, Options::default());
let conn = pool.get_or_connect(remote_id)?;
/// use the connection as usual.
&lt;/code&gt;&lt;p&gt;&lt;code&gt;get_or_connect&lt;/code&gt; will try to get an existing connection from the pool. If there is none, it will create one and store it. The connection will be kept in the pool for a configurable time. Idle connections will be closed as needed. So you can just use this as a drop-in replacement for endpoint.connect and be sure that you won't ever create an unbounded number of connections.&lt;/p&gt;&lt;head rend="h2"&gt;Advanced features&lt;/head&gt;&lt;p&gt;There are some advanced features that can be configued using non-default options.&lt;/p&gt;&lt;code&gt;pub struct Options {
    pub idle_timeout: Duration,
    pub connect_timeout: Duration,
    pub max_connections: usize,
    pub on_connected: Option&amp;lt;OnConnected&amp;gt;,
}
&lt;/code&gt;&lt;p&gt;You can configure the max number of connections to be retained, the maximum tolerable duration for connection establishment, and the max duration connections are kept when idle.&lt;/p&gt;&lt;p&gt;So far, pretty straightforward. There is an additional option to perform some setup before the connection is handed out to the user. For example, you can reject connections based on the data available at this time from the endpoint and the connection, or wait for the connection to reach a certain state before handing it out.&lt;/p&gt;&lt;p&gt;As an example, you might want to do iroh-blobs transfers only on direct connections in order to get good performance or reduce bandwidth use on the relay. If establishing direct connections is not possible, the connection establishment would time out, and you would never even attempt a transfer from such a node.&lt;/p&gt;&lt;code&gt;async fn on_connected(ep: Endpoint, conn: Connection) -&amp;gt; io::Result&amp;lt;()&amp;gt; {
    let Ok(id) = conn.remote_node_id() else {
        return Err(io::Error::other("unable to get node id"));
    };
    let Some(watcher) = ep.conn_type(id) else {
        return Err(io::Error::other("unable to get conn_type watcher"));
    };
    let mut stream = watcher.stream();
    while let Some(status) = stream.next().await {
        if let ConnectionType::Direct { .. } = status {
            return Ok(());
        }
    }
    Err(io::Error::other("connection closed before becoming direct"))
};
let options = Options::default().with_on_connected(on_connected);
let pool = ConnectionPool::new(ep, iroh_blobs::ALPN, options);

let conn = pool.get_or_connect(remote_id)?;
/// use the connection as usual.
&lt;/code&gt;&lt;p&gt;The code to await a direct connection will change quite a bit once we have QUIC multipath. But the capability will remain, and we will update the test code to reflect the new API.&lt;/p&gt;&lt;p&gt;The connection pool is generic enough that it will move to its own crate together with some other iroh utilities. It lives in blobs only until iroh 1.0 is released.&lt;/p&gt;&lt;p&gt;Until then, just depend on iroh-blobs. Iroh-blobs without persistent storage is a very lightweight dependency.&lt;/p&gt;&lt;p&gt;One thing to keep in mind when using the connection pool: the connection pool needs the ability to track which connections are currently being used. To do this, the connection pool does not return &lt;code&gt;Connection&lt;/code&gt; but &lt;code&gt;ConnectionRef&lt;/code&gt;, a struct that derefs to &lt;code&gt;Connection&lt;/code&gt; but contains some additional lifetime tracking.&lt;/p&gt;&lt;p&gt;But &lt;code&gt;Connection&lt;/code&gt; is &lt;code&gt;Clone&lt;/code&gt;, so in principle there is nothing stopping you from cloning the wrapped connection and losing the lifetime tracking. Don't do this. If you work with connections from the pool, you should pass around either a &lt;code&gt;ConnectionRef&lt;/code&gt; or a &lt;code&gt;&amp;amp;Connection&lt;/code&gt; to make sure the underlying &lt;code&gt;ConnectionRef&lt;/code&gt; stays alive.&lt;/p&gt;&lt;p&gt;Incorrect usage of &lt;code&gt;ConnectionRef&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;fn handle_connection(connection: Connection) { tokio::spawn(...) }

let conn = pool.get_or_connect(remote_id)?;
handle_connection(conn.clone()); // clones the Connection out of the ConnectionRef.
/// The ConnectionRef will be dropped here, and the pool will consider the connection idle!
&lt;/code&gt;&lt;p&gt;Correct usage of &lt;code&gt;ConnectionRef&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;fn handle_connection(connection: ConnectionRef) { tokio::spawn(...) }

let conn = pool.get_or_connect(remote_id)?;
handle_connection(conn.clone());
/// The ConnectionRef will be moved into the task, and its lifetime will be properly tracked!
&lt;/code&gt;&lt;p&gt;We experimented with a safer callback-based API, but it turned out to be just too inconvenient to use.&lt;/p&gt;&lt;head rend="h1"&gt;Abstract request and response streams&lt;/head&gt;&lt;p&gt;Iroh-blobs is a protocol that tries to avoid overabstraction. For example as of now you can only use the BLAKE3 hash function, and we hardcode the chunk group size to a value that should work well for all users.&lt;/p&gt;&lt;p&gt;But sometimes there are cases where a bit of abstraction is needed. There was a user request to be able to use compression with iroh-blobs in sendme. One way to do this is to compress files before adding them to the blob store. But this has various downsides. It requires you to create a copy of all data before adding it to the blob store, and will also not lead to very good compression rates when dealing with a large number of small files, since each file will have to be compressed in isolation.&lt;/p&gt;&lt;p&gt;It would be better to compress requests and response streams of the entire protocol and expose the resulting protocol under a different ALPN. With this approach the compression algorithm would be able to find redundancies between multiple files when handling a request for multiple blobs.&lt;/p&gt;&lt;p&gt;This was previously impossible since iroh-blobs worked directly with &lt;code&gt;iroh::endpoint::SendStream&lt;/code&gt; and &lt;code&gt;iroh::endpoint::RecvStream&lt;/code&gt;. So we added traits to allow wrapping send and receive stream in a transform such as compression/decompression.&lt;/p&gt;&lt;p&gt;By default, iroh-blobs still works directly with &lt;code&gt;iroh::endpoint::SendStream&lt;/code&gt; and &lt;code&gt;iroh::endpoint::RecvStream&lt;/code&gt;, so for normal use nothing changes.&lt;/p&gt;&lt;p&gt;The traits are a bit similar to Stream and Sink, but with two important additions.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;We allow sending and receiving Bytes, since iroh streams work with bytes internally. That way we avoid a copy in the default case.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;We have methods stop and reset to close the stream, and on the send stream a method stopped that returns a future that resolves when the remote side has closed the stream.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Wrapping the entire iroh-blobs protocol into compression is pretty straightforward except for some boilerplate. We have an example compression.rs that shows how to do this.&lt;/p&gt;&lt;p&gt;We will have this as an optional feature of sendme in one of the next releases.&lt;/p&gt;&lt;p&gt;Just like the connection pool, these traits are generally useful whenever you want to derive iroh protocols by wrapping existing protocols, so they will move to a separate crate once iroh 1.0 is released.&lt;/p&gt;&lt;head rend="h1"&gt;Enhanced provider events&lt;/head&gt;&lt;p&gt;This change is from iroh-blobs 0.93&lt;/p&gt;&lt;p&gt;On the provider side, it is now possible to have very detailed events about what the provider is doing. The provider events are now implemented as an irpc protocol. For each request type you can use an event mask to configure if you want to be notified at all, and if you need the ability to intercept the request, e.g. if you only want to serve certain hashes.&lt;/p&gt;&lt;p&gt;There is an example how to use the new provider events to limit by provider node id or hash.&lt;/p&gt;&lt;p&gt;Here is a provider event handler that serves only blobs requests for hashes in a fixed set of allowed hashes:&lt;/p&gt;&lt;code&gt;fn limit_by_hash(allowed_hashes: HashSet&amp;lt;Hash&amp;gt;) -&amp;gt; EventSender {
    let mask = EventMask {
        // We want to get a request for each get request that we can answer
        // with OK or not OK depending on the hash. We do not want detailed
        // events once it has been decided to handle a request.
        get: RequestMode::Intercept,
        ..EventMask::DEFAULT
    };
    let (tx, mut rx) = EventSender::channel(32, mask);
    n0_future::task::spawn(async move {
        while let Some(msg) = rx.recv().await {
            if let ProviderMessage::GetRequestReceived(msg) = msg {
                let res = if !msg.request.ranges.is_blob() {
                    Err(AbortReason::Permission)
                } else if !allowed_hashes.contains(&amp;amp;msg.request.hash) {
                    Err(AbortReason::Permission)
                } else {
                    Ok(())
                };
                msg.tx.send(res).await.ok();
            }
        }
    });
    tx
}
&lt;/code&gt;&lt;head rend="h1"&gt;What's next&lt;/head&gt;&lt;p&gt;The next major feature in iroh-blobs will be a minimal version of multiprovider downloads for individual blobs.&lt;/p&gt;&lt;p&gt;As soon as iroh 1.0 is released, several generic parts of iroh-blobs will move to a separate iroh utilities crate.&lt;/p&gt;&lt;p&gt;To get started, take a look at our docs, dive directly into the code, or chat with us in our discord channel.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45727557</guid><pubDate>Mon, 27 Oct 2025 23:28:13 +0000</pubDate></item><item><title>Complete Digitization of Leonardo da Vinci's Codex Atlanticus</title><link>https://www.openculture.com/2025/10/digitization-of-leonardo-da-vincis-codex-atlanticus.html</link><description>&lt;doc fingerprint="3a053512a6ea9c9c"&gt;
  &lt;main&gt;
    &lt;p&gt;No historical figure better fits the definition of “Renaissance man” than Leonardo da Vinci, but that term has become so overused as to become misleading. We use it to express mild surprise that one person could use both their left and right hemispheres equally well. But in Leonardo’s day, people did not think of themselves as having two brains, and the worlds of art and science were not so far apart as they are now.&lt;/p&gt;
    &lt;p&gt;That Leonardo was able to combine fine arts and fine engineering may not have been overly surprising to his contemporaries, though he was an extraordinarily brilliant example of the phenomenon. The more we learn about him, the more we see how closely related the two pursuits were in his mind.&lt;/p&gt;
    &lt;p&gt;He approached everything he did as a technician. The uncanny effects he achieved in painting were the result, as in so much Renaissance art, of mathematical precision, careful study, and firsthand observation.&lt;/p&gt;
    &lt;p&gt;His artistic projects were also experiments. Some of them failed, as most experiments do, and some he abandoned, as he did so many scientific projects. No matter what, he never undertook anything, whether mechanical, anatomical, or artistic, without careful planning and design, as his copious notebooks testify. As more and more of those notebooks have become available online, both Renaissance scholars and laypeople alike have learned considerably more about how Leonardo’s mind worked.&lt;/p&gt;
    &lt;p&gt;First, there was the Codex Arundel. It is, writes Jonathan Jones at The Guardian, “the living record of a universal mind”—but also, specifically, the mind of a “technophile.” Then, the Victoria and Albert National Art Library announced the digitization of Codex Forster, which contains some of Leonardo’s earliest notebooks. Now The Visual Agency has released a complete digitization of Leonardo’s Codex Atlanticus, a huge collection of the artist, engineer, and inventor’s finely-illustrated notes.&lt;/p&gt;
    &lt;p&gt;“No other collection counts more original papers written by Leonardo,” notes Google. The Codex Atlanticus “consists of 1119 papers, most of them drawn or written on both sides.” Its name has “nothing to do with the Atlantic Ocean, or with some esoteric, mysterious content hidden in its pages.” The 12-volume collection acquired its title because the drawings and writings were bound with the same size paper that was used for making atlases. Gathered in the 16th century by sculptor Pompeo Leoni, the papers descended from Leonardo’s close student Giovan Francesco Melzi, who was entrusted with them after his teacher’s death.&lt;/p&gt;
    &lt;p&gt;The history of the Codex itself makes for a fascinating narrative, much of which you can learn at Google’s Ten Key Facts slideshow. The notebooks span Leonardo’s career, from 1478, when he was “still working in his native Tuscany, to 1519, when he died in France.” The collection was taken from Milan by Napoleon and brought to France, where it remained in the Louvre until 1815, when the Congress of Vienna ruled that all artworks stolen by the former Emperor be returned. (The emissary tasked with returning the Codex could not decipher Leonardo’s mirror writing and took it for Chinese.)&lt;/p&gt;
    &lt;p&gt;The Codex contains not only engineering diagrams, anatomy studies, and artistic sketches, but also fables written by Leonardo, inspired by Florentine literature. And it features Leonardo’s famed “CV,” a letter he wrote to the Duke of Milan describing in nine points his qualifications for the post of military engineer. In point four, he writes, “I still have very convenient bombing methods that are easy to transport; they launch stones and similar such in a tempest full of smoke to frighten the enemy, causing great damage and confusion.”&lt;/p&gt;
    &lt;p&gt;As if in illustration, elsewhere in the Codex, the drawing above appears, “one of the most celebrated” of the collection.” It was “shown to traveling foreigners visiting the Ambrosiana [the Biblioteca Ambrosiana in Milan, where the Codex resides] since the 18th century, usually arousing much amazement.” It is still amazing, especially if we consider the possibility that its artistry might have been something of a byproduct for its creator, whose primary motivation seems to have been solving technical problems—in the most elegant ways imaginable.&lt;/p&gt;
    &lt;p&gt;See the complete digitization of Leonardo’s Codex Atlanticus here.&lt;/p&gt;
    &lt;p&gt;Note: An earlier version of this post appeared on our site in 2019.&lt;/p&gt;
    &lt;p&gt;Related Content:&lt;/p&gt;
    &lt;p&gt;How Leonardo da Vinci Drew an Accurate Satellite Map of an Italian City (1502)&lt;/p&gt;
    &lt;p&gt;Leonardo da Vinci’s Handwritten Resume (Circa 1482)&lt;/p&gt;
    &lt;p&gt;Leonardo Da Vinci’s To-Do List from 1490: The Plan of a Renaissance Man&lt;/p&gt;
    &lt;p&gt;Josh Jones is a writer and musician based in Durham, NC.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45728975</guid><pubDate>Tue, 28 Oct 2025 03:32:09 +0000</pubDate></item><item><title>Show HN: Ordered – A sorted collection library for Zig</title><link>https://news.ycombinator.com/item?id=45729457</link><description>&lt;doc fingerprint="2fe8a8a3ce484c88"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;I made an early version of a sorted collection library for Zig. Sorted collections are data structures that maintain the data in sorted order. Examples of these data structures are `java.util.TreeMap` in Java and `std::map` in C++. These data structures are mainly used for fast lookups (point search) and fast range searches.&lt;/p&gt;
      &lt;p&gt;The library is available on GitHub: https://github.com/CogitatorTech/ordered&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45729457</guid><pubDate>Tue, 28 Oct 2025 05:26:38 +0000</pubDate></item><item><title>Picture gallery: Amiga prototype "Lorraine" at the Amiga 40 event</title><link>https://www.amiga-news.de/en/news/AN-2025-10-00110-EN.html</link><description>&lt;doc fingerprint="76b8ee2d4cb73196"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;DEUTSCHE VERSION&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Links&lt;/cell&gt;
        &lt;cell&gt;|&lt;/cell&gt;
        &lt;cell&gt;Forums&lt;/cell&gt;
        &lt;cell&gt;|&lt;/cell&gt;
        &lt;cell&gt;Comments&lt;/cell&gt;
        &lt;cell&gt;|&lt;/cell&gt;
        &lt;cell&gt;Report news&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Chat&lt;/cell&gt;
        &lt;cell&gt;|&lt;/cell&gt;
        &lt;cell&gt;Polls&lt;/cell&gt;
        &lt;cell&gt;|&lt;/cell&gt;
        &lt;cell&gt;Newsticker&lt;/cell&gt;
        &lt;cell&gt;|&lt;/cell&gt;
        &lt;cell&gt;Archive&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[Login] [Register] [Forgot your password?]&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="20"/&gt;
      &lt;row span="20"&gt;
        &lt;cell&gt;23.Oct.2025&lt;p&gt;a1k.org (Webseite)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; Picture gallery: Amiga prototype "Lorraine" at the Amiga 40&lt;p&gt;Dale Luck from the original Amiga development team has preserved the very first Amiga prototype, which used three huge stacks of well-equipped breadboards instead of custom chips. The computer was on display in Germany for the first time last weekend at the Amiga 40 event. Amiga user ‘Pittrock’ took pictures of the exhibit and kindly gave us permission to publish them here:&lt;/p&gt;&lt;p&gt;(cg)&lt;/p&gt;&lt;p&gt;[News message: 23. Oct. 2025, 22:53] [Comments: 0]&lt;/p&gt;&lt;p&gt;[Send via e-mail] [Print version] [ASCII version]&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt; Masthead | Privacy policy | Netiquette | Advertising | Contact &lt;p&gt;Copyright © 1998-2025 by amiga-news.de - all rights reserved.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45729467</guid><pubDate>Tue, 28 Oct 2025 05:28:55 +0000</pubDate></item><item><title>Poker Tournament for LLMs</title><link>https://pokerbattle.ai/event</link><description>&lt;doc fingerprint="7b6effef6ef87f8c"&gt;
  &lt;main&gt;
    &lt;p&gt;Loading event data...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45730094</guid><pubDate>Tue, 28 Oct 2025 07:42:18 +0000</pubDate></item><item><title>Criminal complaint against facial recognition company Clearview AI</title><link>https://noyb.eu/en/criminal-complaint-against-facial-recognition-company-clearview-ai</link><description>&lt;doc fingerprint="fab418b895b1f840"&gt;
  &lt;main&gt;
    &lt;p&gt;Today, noyb has filed a criminal complaint against Clearview AI and its managers. The facial recognition company is known for scraping billions of photos of Europeans and people around the world on the internet – and selling its facial recognition system to law enforcement and state actors. Several EU data protection authorities have already imposed fines and bans on Clearview AI. But the US company simply ignores these actions – given the lack of enforcement.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Original Complaints against Clearview filed in 2021&lt;/item&gt;
      &lt;item&gt;Decision of Austrian DPA deeming Clearview illegal&lt;/item&gt;
      &lt;item&gt;Several Clearview fines:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Background. Clearview AI is a US company that scrapes the internet and adds all of the faces it can find in photos and videos to its database. It claims to have collected more than 60 billion photos. This allows customers of Clearview AI to identify people by uploading a photo and obtaining other pictures of the same person, including links, the name of a subpage of a website and other meta data. The company originally tried to operate largely under the radar but the New York Times revealed its practices in 2020. While Clearview AI primarily promotes its facial recognition software as a tool for law enforcement, it was also used by companies such as Walmart or Bank of America.&lt;/p&gt;
    &lt;p&gt;Max Schrems: “Facial recognition technology is extremely invasive. It allows for mass surveillance and immediate identification of millions of people. Clearview AI amassed a global database of photos and biometric data, which makes it possible to identify people within seconds. Such power is extremely concerning and undermines the idea of a free society, where surveillance is the exception instead of the rule.”&lt;/p&gt;
    &lt;p&gt;Clearly illegal and intrusive. EU data protection authorities have already repeatedly held that Clearview AI, which processed the data of millions of Europeans, clearly violated the GDPR. The French, the Greek, the Italian and the Dutch authority imposed fines of roughly 100 million euros on Clearview for its intrusive practices. The Austrian data protection authority considered that Clearview AI has acted illegally. Several bans were issued. These decisions were not challenged by the US company.&lt;/p&gt;
    &lt;p&gt;Ignoring the law. Instead, Clearview AI is simply ignoring the EU authorities. Only in the UK did the company appeal the decision and fine imposed by the British ICO, with a final court decision yet to be issued. EU data protection authorities did not come up with a way to enforce its fines and bans against the US company, allowing Clearview AI to effectively dodge the law.&lt;/p&gt;
    &lt;p&gt;Max Schrems: “Clearview AI seems to simply ignore EU fundamental rights and just spits in the face of EU authorities.”&lt;/p&gt;
    &lt;p&gt;Criminal Complaint. However, EU law is not limited to administrative fines under the GDPR. Article 84 GDPR also allows EU Member States to foresee criminal sanctions for GDPR breaches. Austria has implemented such a criminal provision for certain GDPR violations in § 63 of its national Data Protection Act. In contrast to GDPR violations, criminal violations also allow actions to be taken against managers and to use the full range of criminal procedures, including EU-wide actions. For that reason, noyb now filed a criminal complaint with the public prosecutors in Austria. If successful, Clearview AI and its executives could face jail time and be held personally liable, in particular if traveling to Europe.&lt;/p&gt;
    &lt;p&gt;Max Schrems: “We even run cross-border criminal procedures for stolen bikes, so we hope that the public prosecutor also takes action when the personal data of billions of people was stolen – as has been confirmed by multiple authorities.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45730411</guid><pubDate>Tue, 28 Oct 2025 08:34:57 +0000</pubDate></item><item><title>Geometry and Physics of Wrinkling (2003) [pdf]</title><link>https://softmath.seas.harvard.edu/wp-content/uploads/2019/10/2003-03.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45730607</guid><pubDate>Tue, 28 Oct 2025 09:02:47 +0000</pubDate></item><item><title>Situated Software – Clay Shirky (2004)</title><link>http://shirky.com/essays/situated-software/</link><description>&lt;doc fingerprint="ed181d52686f258d"&gt;
  &lt;main&gt;
    &lt;p&gt;First published March 30, 2004 on the “Networks, Economics, and Culture” mailing list.&lt;/p&gt;
    &lt;p&gt;I teach at NYU’s Interactive Telecommunications Program (ITP), where the student population is about evenly divided between technologists who care about aesthetics and artists who aren’t afraid of machines, which makes it a pretty good place to see the future.&lt;/p&gt;
    &lt;p&gt;Part of the future I believe I’m seeing is a change in the software ecosystem which, for the moment, I’m calling situated software. This is software designed in and for a particular social situation or context. This way of making software is in contrast with what I’ll call the Web School (the paradigm I learned to program in), where scalability, generality, and completeness were the key virtues.&lt;/p&gt;
    &lt;p&gt;I see my students cheerfully ignoring Web School practices and yet making interesting work, a fact that has given me persistent cognitive dissonance for a year, so I want to describe the pattern here, even in its nascent stages, to see if other people are seeing the same thing elsewhere.&lt;/p&gt;
    &lt;p&gt;Users By The Dozens&lt;/p&gt;
    &lt;p&gt;We’ve always had a tension between enterprise design practices and a “small pieces, loosely joined” way of making software, to use David Weinberger’s felicitous phrase. The advantages to the latter are in part described in Worse is Better and The Cathedral and the Bazaar. Situated software is in the small pieces category, with the following additional characteristic — it is designed for use by a specific social group, rather than for a generic set of “users”.&lt;/p&gt;
    &lt;p&gt;The biggest difference this creates relative to classic web applications is that it becomes easy to build applications to be used by dozens of users, an absurd target population in current design practice. Making form-fit software for a small group of users has typically been the province of banks and research labs — because of the costs involved, Web School applications have concentrated on getting large-scale audiences. And by privileging the value that comes with scale, Web School applications put other kinds of value, particularly social value, out of reach.&lt;/p&gt;
    &lt;p&gt;We’ve been killing conversations about software with “That won’t scale” for so long we’ve forgotten that scaling problems aren’t inherently fatal. The N-squared problem is only a problem if N is large, and in social situations, N is usually not large. A reading group works better with 5 members than 15; a seminar works better with 15 than 25, much less 50, and so on.&lt;/p&gt;
    &lt;p&gt;This in turn gives software form-fit to a particular group a number of desirable characteristics — it’s cheaper and faster to build, has fewer issues of scalability, and likelier uptake by its target users. It also has several obvious downsides, including less likelihood of use outside its original environment, greater brittleness if it is later called on to handle larger groups, and a potentially shorter lifespan.&lt;/p&gt;
    &lt;p&gt;I see my students making some of these tradeoffs, though, because the kinds of scarcities the Web School was meant to address — the expense of adequate hardware, the rarity of programming talent, and the sparse distribution of potential users — are no longer the constraints they once were.&lt;/p&gt;
    &lt;p&gt;Teachers on the Run&lt;/p&gt;
    &lt;p&gt;The first inkling I got that the Web School rationale might be weakening was an application written by two of my former students, Paul Berry and Keren Merimeh. In November of 2002, as a project for a class on the feeling of online spaces called Social Weather, they created an application called (alarmingly) Teachers on the Run.&lt;/p&gt;
    &lt;p&gt;Teachers on the Run was essentially HotorNot for ITP professors, to allow students to describe and rate us in advance of spring course registration. Every professor was listed in a database; students could come by anonymously and either enter a comment about a professor or cast a vote agreeing or disagreeing with an earlier comment. The descriptions were sorted in vote total order, so that a +5 description (5 more students had agreed than disagreed) was displayed higher than a +2 or a -3. And that was it — a list of names, a list of comments, click to vote, and a simple sorting algorithm.&lt;/p&gt;
    &lt;p&gt;They launched it on a Friday. By Saturday night, another student called me at home to tell me I’d better take a look at it. There are only 200 or so students at ITP, but Teachers on the Run had already accumulated hundreds of comments, most positive, some negative, a few potentially libelous. More importantly, though, there had been over a thousand votes in 24 hours. By Monday morning, I had students telling me they knew what was on the site, not because they’d seen it, but because it had been the only topic of conversation over the weekend.&lt;/p&gt;
    &lt;p&gt;The curious thing to me about Teachers on the Run was that it worked where the Web School version failed. RateMyProfessors.com has been available for years, with a feature set that put the simplistic write/read/vote capabilities of Teachers on the Run to shame. Yet no one at ITP had ever bothered to use RateMyProfessors.com, though the weekend’s orgy of rating and voting demonstrated untapped demand.&lt;/p&gt;
    &lt;p&gt;Despite the social energy it unleashed, I missed the importance of Teachers on the Run. I told myself that it had succeeded for a number of reasons that were vaguely unfair: The users knew the programmers; the names database had been populated in advance; the programmers could use the in-house mailing list to launch the application rather than trying to get attention through press releases and banner ads. Most damning of all, it wouldn’t scale, the sine qua non of successful Web applications. DOA, QED.&lt;/p&gt;
    &lt;p&gt;Then I saw the design process my most recent class went through.&lt;/p&gt;
    &lt;p&gt;The Class&lt;/p&gt;
    &lt;p&gt;In a class called Social Software, which I taught last fall, the students worked in small groups to design and launch software to support some form of group interaction. To anchor the class, I required that whatever project they came up with be used by other ITP students. This first order benefits of this strategy were simple: the designers came from the same population as the users, and could thus treat their own instincts as valid; beta-testers could be recruited by walking down the hall; and it kept people from grandiose “boil the ocean” attempts.&lt;/p&gt;
    &lt;p&gt;What I hadn’t anticipated was the second-order benefits. Time and again the groups came up against problems that they solved in part by taking advantage of social infrastructure or context-sensitive information that wouldn’t be available to adherents of the Web School. Two strategies in particular stand out.&lt;/p&gt;
    &lt;p&gt;The first had to do with reputation systems. One project, The Orderer (designed by Vena Chitturi, Fa-yi Chou, Rachel Fishman, and Cindy Yang) was for coordinating group restaurant orders, common in late-night work sessions. The other, WeBe (Brandon Brown, Yoonjung Kim, Olivier Massot, Megan Phalines) was a tool for coordinating group purchases of things like chips or motors. Because money was involved, a Web School approach would require some way of dealing with the threat of non-payment, using things like pre-pay or escrow accounts, or formal reputation systems.&lt;/p&gt;
    &lt;p&gt;Instead, in both projects the students decided that since all the users were part of the ITP community, they would simply make it easy to track the deadbeats, with the threat of public broadcast of their names. The possibility of being shamed in front of the community became part of the application design, even though the community and the putative shame were outside the framework of the application itself.&lt;/p&gt;
    &lt;p&gt;Communal Attention&lt;/p&gt;
    &lt;p&gt;The other strategy had to do with communal attention. Two other projects, Scout (Karen Bonna, Christine Brumback, Dennis Crowley, Alex Rainert) and CoDeck (Mark Argo, Dan Melinger, Shawn Van Every, Ahmi Wolf) ended up being situated in the community in a more literal fashion. Scout indicates physical presence, by allowing students to register themselves as being present somewhere on the ITP floor, and displaying that information. CoDeck is a community-based video server, designed to allow video artists to share and comment on each other’s work.&lt;/p&gt;
    &lt;p&gt;Both groups had the classic problem of notification — getting a user to tune in requires interrupting their current activity, not something users have been known to relish. Billions were spent on Web School applications that assumed users would bookmark for a return visit, or would happily accept email alerts, but despite a few well-publicized successes like Schwab.com and eBay, users have mostly refused to “check back often.”&lt;/p&gt;
    &lt;p&gt;Both Scout and CoDeck hit on the same solution: take most of the interface off the PC’s dislocated screen, and move it into a physical object in the lounge, the meeting place/dining room/foosball emporium in the center of the ITP floor. Scout and CoDeck each built kiosks in the lounge with physical interfaces in lieu of keyboard/mouse interaction. Scout used a bar code reader to swipe in; CoDeck gutted a mid-70’s BetaMax chassis and put a Linux machine inside, then used the BetaMax buttons to let the user control the video stream. Both Scout and CoDeck have web sites where users can enter or retrieve data, but the core piece of each is location in physical space that puts the application in a social context.&lt;/p&gt;
    &lt;p&gt;These projects all took the course’s original dictum — the application must be useful to the community — and began to work with its corollary as well — the community must be useful to the application.&lt;/p&gt;
    &lt;p&gt;Group Capabilities&lt;/p&gt;
    &lt;p&gt;We constantly rely on the cognitive capabilities of individuals in software design — we assume a user can associate the mouse with the cursor, or that icons will be informative. We rarely rely on the cognitive capabilities of groups, however, though we rely on those capabilities in the real world all the time.&lt;/p&gt;
    &lt;p&gt;In brainstorming sessions, a group can generate not just more ideas but more kinds of ideas than the same individuals working in isolation, and a group consensus is often more accurate than the guess of the group’s most knowledgeable individual. Groups also know a lot about themselves. People in work groups know who to go to for design advice, or who is unreliable in a pinch, without any formal designation of those roles. Members of social groups know who it’s fun to go drinking with or who you shouldn’t lend money to (often the same person) without needing that knowledge to be spelled out in a FAQ.&lt;/p&gt;
    &lt;p&gt;Web School software ignores this kind of knowledge, because it is hard to make explicit. On most large mailing lists, for example, only a handful of posters start discussions, while most posters simply follow-up; and, at a higher level, only a handful of the members post at all, while a most simply lurk. We’ve known about these patterns for decades, but mailing list software still does not offer any features specific to starting vs. continuing threads, nor does it treat high-volume posters and lurkers differently.&lt;/p&gt;
    &lt;p&gt;There is another strategy, however, analogous to asking the user to recognizing icons; the designer can simply assume the group has a certain capability, without needing to recapitulate it in code. If you have an uncollected payment in a communal buying pool, the software can kick out a message that says “Deadbeat alert. Deal with it.” A real world group will have some way of handling the problem, usually through moral suasion or the threat of lost reputational capital, or even, in extreme cases, ostracism.&lt;/p&gt;
    &lt;p&gt;This is no different than what happens in offline groups every day, but the solution feels wrong, in Web School terms, because those web applications can’t assume there is a tacit reputation system. By relying on existing social fabric, situated software is guaranteed not to work at the scale Web School apps do, but for the same reason, it can work in ways Web School software can’t.&lt;/p&gt;
    &lt;p&gt;Outside Eyes&lt;/p&gt;
    &lt;p&gt;I finally started regarding situated software as a practical development strategy, rather than as a degenerate case of “real” application development, when I invited outside reviewers into the Social Software class for a mid-term critique. These were all people who work with social software for a living, and the critique session was enormously valuable. Two of the recommendations made by the reviewers, however, struck me funny.&lt;/p&gt;
    &lt;p&gt;The first was the suggestion, made to the CoDeck group, that they should make all the features of their video tool available over the web — upload, download, comment, and so on. The second recommendation was an exhortation to the WeBe group that they should look at Web School group-buying sites like Mercata and MobShop as guides to their own work.&lt;/p&gt;
    &lt;p&gt;This was the moment for me when cognitive dissonance finally became unsupportable. Each of those comments was a) exactly what I would have said, had I been an outside reviewer in someone else’s class, and b) obviously wrong, given the problem the respective groups were attacking.&lt;/p&gt;
    &lt;p&gt;The suggestion about general web accessibility for the CoDeck interface came in the form of a rhetorical question — “Why not make it as broadly accessible as possible?” In the Web School, of course, the answer is “No reason”, since more users are always A Good Thing, but for CoDeck there were several good reasons for not simply turning their project into a Web video app.&lt;/p&gt;
    &lt;p&gt;First, the physicalization of the interface, using the gutted BetaMax deck, provides a communal affordance that it is impossible to replicate over the web. Second, since CoDeck serves a tight community, the density of communication among ITP video makers would be diluted by general accessibility. Third, having the video deck in the lounge makes it self-policing; the cohesion of the community keeps it largely free from abuse, whereas a generally accessible and password-free “upload and critique” video site would become a cesspool of porn within hours. Finally, serving a local community maximizes use of free bandwidth on the local network, enabling features that would saddle a public system with crippling costs.&lt;/p&gt;
    &lt;p&gt;WeBe Small&lt;/p&gt;
    &lt;p&gt;Similarly, the recommendation that WeBe should look at Mercata and MobShop carried with it the assumption that the goal should eventually be to operate at large scale. However, Mercata and MobShop failed because they were built to scale.&lt;/p&gt;
    &lt;p&gt;Those sites required a virtuous circle, where more users meant more savings meant more users. Alas, the thought that somewhere, someone else was saving a bundle on Tupperware was never enough to attract users, and without critical mass, the virtuous circle turned vicious. Like RateMyProfessors.com, the mere existence of a Web School app wasn’t enough, and having been built for tens of thousands of users, it couldn’t operate for dozens or even hundreds.&lt;/p&gt;
    &lt;p&gt;WeBe, on the other hand, was copying a small-scale pattern they first observed when a fellow student, Scott Fitzgerald, orchestrated a 30-license discount purchase of Max, the multi-media editing software. He used the ITP mailing list to recruit buyers, and then walked around the floor twisting arms and collecting checks. This required real social fabric to work — everyone knew and trusted Scott.&lt;/p&gt;
    &lt;p&gt;As the instigator, Scott also benefited from the good karma — everyone who participated saved quite a bit of money, enhancing his reputation. Unlike actual capital, reputational capital is easier to accumulate in smaller and more closed social systems. The idea for WeBe came about in part because Scott said the purchase, though successful, had required too much work. Whatever the WeBe group could do to make ITP group purchases easier, they didn’t need to build identity or reputation systems. Because the software was situated in a particular (and particularly tight) community, they got those things for free.&lt;/p&gt;
    &lt;p&gt;Old Scarcities Fade Away&lt;/p&gt;
    &lt;p&gt;Where the Web School works well, it works because it is the right kind of response to some sort of scarcity. There’s scarcity of funds: Servers are expensive, not to mention load-balancing routers, tape backups, and the other accouterments of serious uptime. There’s scarcity of talent: Good programmers are hard to find; great programmers are scarce as hen’s teeth. And there’s scarcity of use: Users are busy, they are creatures of habit, and there is significant competition for their attention.&lt;/p&gt;
    &lt;p&gt;However, addressing these scarcities can give Web School design a kind of merry-go-round quality. You need to scale because building a useful web application is so expensive, but much of the expense comes from the requirements of scale. Furthermore, these scarcities amplify one another: You need a big hardware budget to build an application that can scale, but you need good programmers and system administrators to handle the load, whose salaries require an increased marketing budget, to attract enough users to pay for it all.&lt;/p&gt;
    &lt;p&gt;What I think I’m seeing my students do is get off that ride. They can do this because none of the scarcities the Web School addresses are as significant as they used to be. First of all, Moore’s Law and its equivalent for storage, plus the gradual improvement in operating systems, means that an $800 desktop machine can also be a pretty good server right out of the box.&lt;/p&gt;
    &lt;p&gt;Second, user attention was scarce in part because there were so few users at all. In the 90’s, launching an application on the Web meant forgoing any direct connection with a particular real world community, because internet users were spread thin, and outside the IT industry, most real world groups had only a sub-set of members who were online.&lt;/p&gt;
    &lt;p&gt;Those days are ending, and in some places they are over already. In the US today, if you are under 35 or make over 35,000 dollars a year, you are probably online, and if both of those things are true, then most of the people you know are probably online as well. You can now launch an application for a real world group, confident that all of them will have access to the internet.&lt;/p&gt;
    &lt;p&gt;The Nature of Programming, and the Curious Case of MySQL&lt;/p&gt;
    &lt;p&gt;Finally, the practice of programming is changing. Gartner recently caused a stir by saying there would be 235,000 fewer programmers in the US ten years from now. This would have been like predicting in the 80s, that there would be fewer typists in the US by 2004. Such a prediction would be true in one sense — the office typing pool has disappeared, and much data entry work has moved overseas. But actual typing, fingers hitting the keyboard, has not disappeared, it has spread everywhere.&lt;/p&gt;
    &lt;p&gt;So with programming; though all the attention is going to outsourcing, there’s also a lot of downsourcing going on, the movement of programming from a job description to a more widely practiced skill. If by programmer we mean “people who write code” instead of “people who are paid to write code”, the number of programmers is going to go up, way up, by 2015, even though many of the people using perl and JavaScript and Flash don’t think of themselves as programmers.&lt;/p&gt;
    &lt;p&gt;A variety of technologies are driving this — perl, PHP, ActionScript, DHTML — with a lot of mixing and matching and no one core tool, with one curious exception. Every application of this pattern I’ve seen has used a MySQL database.&lt;/p&gt;
    &lt;p&gt;There’s an analogy here with web server software. In the mid-90s, getting a web server running was such a messy endeavor that it was a project goal in and of itself. Then Apache came along, and so simplified the process that the web server became a simple building block for larger things.&lt;/p&gt;
    &lt;p&gt;MySQL does this for databases. This matters for the development of group applications, because the ability to sort is a public good. If Teachers on the Run had simply been a list of professors with attached comments, it would have been a write-only application, like those worthless “Tell us what you think!” comment forms on the bottom of news articles. One of the critical things any group wants to know is “What does everyone else think?”, especially if there is reason to believe that the group in aggregate knows more than any individual. Adding the ‘users rate comments’ system, and then pulling the data out by rating instead of time, made the system valuable.&lt;/p&gt;
    &lt;p&gt;You can of course build these kind of features in other ways, but MySQL makes the job much easier, so much easier in fact that after MySQL, it becomes a different kind of job. There are complicated technical arguments for and against using MySQL vs. other databases, but none of those arguments matter anymore. For whatever reason, MySQL seems to be a core tool for this particular crop of new applications.&lt;/p&gt;
    &lt;p&gt;Software for Your Mom&lt;/p&gt;
    &lt;p&gt;Situated software isn’t a technological strategy so much as an attitude about closeness of fit between software and its group of users, and a refusal to embrace scale, generality or completeness as unqualified virtues. Seen in this light, the obsession with personalization of Web School software is an apology for the obvious truth — most web applications are impersonal by design, as they are built for a generic user. Allowing the user to customize the interface of a Web site might make it more useful, but it doesn’t make it any more personal than the ATM putting your name on the screen while it spits out your money.&lt;/p&gt;
    &lt;p&gt;Situated software, by contrast, doesn’t need to be personalized — it is personal from its inception. Teachers on the Run worked this way. Everyone knew that Paul and Keren built it. You could only rate Clay and Marianne and Tom and the other ITP professors. You didn’t even know it even existed unless you were on the ITP mailing list. The application’s lack of generality or completeness, in other words, communicated something — “We built this for you” — that the impersonal facade of RateMyProfessors.com doesn’t have and can’t fake.&lt;/p&gt;
    &lt;p&gt;One of my students mentioned building a web application for his mother, a schoolteacher, to keep track of her class. If you were working alone, unpaid, and in your spare time, there’s no way you could make an application that would satisfy the general and complete needs of schoolteachers everywhere. You could make one for your mom, though.&lt;/p&gt;
    &lt;p&gt;Small, purpose-built apps have always existed, of course — learning BASIC used to be a rite of passage for PC owners, and data intensive institutions like investment banks and research labs write software for small groups of users. Now, though, the combination of good tools, talented users and the internet as a social stage makes the construction of such software simpler, the quality of the result better, and the delivery to the users as simple as clicking a link. The design center of a dozen users, so hard to serve in the past, may become normal practice.&lt;/p&gt;
    &lt;p&gt;What Next?&lt;/p&gt;
    &lt;p&gt;So what happens next? If what I’m seeing is not transitory or limited to a narrow set of situations, then we’ll see a rise in these small form-fit applications. This will carry some obvious downsides, including tying the developers of such applications to community support roles, and shortening the useful lifespan of the software made in this way.&lt;/p&gt;
    &lt;p&gt;Expectations of longevity, though, are the temporal version of scale — we assume applications should work for long periods in part because it costs so much to create them. Once it’s cheap and easy to throw together an application, though, that rationale weakens. Businesses routinely ask teams of well-paid people to put hundreds of hours of work creating a single PowerPoint deck that will be looked at in a single meeting. The idea that software should be built for many users, or last for many years, are cultural assumptions not required by the software itself.&lt;/p&gt;
    &lt;p&gt;Indeed, as a matter of effect, most software built for large numbers of users or designed to last indefinitely fails at both goals anyway. Situated software is a way of saying “Most software gets only a few users for a short period; why not take advantage of designing with that in mind?”&lt;/p&gt;
    &lt;p&gt;This, strangely, is a kind of progress, not because situated software will replace other kinds of applications, but because it mostly won’t. For all the value we get out of the current software ecosystem, it doesn’t include getting an application built for a handful of users to use for a few months. Now, though, I think we’re starting to see a new software niche, where communities get form-fit tools for very particular needs, tools that fail most previous test of design quality or success, but which nevertheless function well, because they are so well situated in the community that uses them.&lt;/p&gt;
    &lt;p&gt;Thanks to Shawn Van Every for invaluable comments on an earlier draft of this piece.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45731069</guid><pubDate>Tue, 28 Oct 2025 10:20:16 +0000</pubDate></item><item><title>Understanding the Worst .NET Vulnerability</title><link>https://andrewlock.net/understanding-the-worst-dotnet-vulnerability-request-smuggling-and-cve-2025-55315/</link><description>&lt;doc fingerprint="b71699087fe1929d"&gt;
  &lt;main&gt;
    &lt;p&gt;I admit, that's a very click-baity headline, but Microsoft have given the vulnerability a CVSS score of 9.9, their highest ever. Time to panic, right?&lt;/p&gt;
    &lt;p&gt;In this post I try to provide a bit more context. I explain how request smuggling vulnerabilities work in general, how it works in this case, what attackers could use it for, how the vulnerability was fixed, what you can do to protect yourself.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;WARNING: I am not a security professional, so do not take anything in this post as gospel or advice. I'm just a developer trying to make sense of things. 😄 All of the details in this post are based on information that was provided or referenced in the original announcement.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;What is the CVE-2025-55315 vulnerability?&lt;/head&gt;
    &lt;p&gt;On October 14th 2025, on a standard Microsoft "patch Tuesday", Microsoft released new versions of all their supported versions of .NET, and also published a security advisory: Microsoft Security Advisory CVE-2025-55315: .NET Security Feature Bypass Vulnerability. The high level summary from that announcement said:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Inconsistent interpretation of http requests ('http request/response smuggling') in ASP.NET Core allows an authorized attacker to bypass a security feature over a network.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The advice was "patch all of your things", but the real headline was that this vulnerability was given a CVSS score of 9.9 our of 10, which you know, sounds pretty bad! Barry Dorrans AKA blowdart, .NET security head honcho, gave an explanation of the reasoning behind the score in a comment on the original issue:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The bug enables HTTP Request Smuggling, which on its own for ASP.NET Core would be nowhere near that high, but that's not how we rate things...&lt;/p&gt;
      &lt;p&gt;Instead, we score based ~~~~h~~ow~~ the bug might affect applications built on top of ASP.NET.&lt;/p&gt;
      &lt;p&gt;Request Smuggling allows an attacker to hide an extra request inside an another, and what that hidden request can do is very application specific.&lt;/p&gt;
      &lt;p&gt;The smuggled request could cause your application code to&lt;/p&gt;
      &lt;item&gt;Login as a different user (EOP)&lt;/item&gt;
      &lt;item&gt;Make an internal request (SSRF)&lt;/item&gt;
      &lt;item&gt;Bypass CSRF checks&lt;/item&gt;
      &lt;item&gt;Perform an injection attack&lt;/item&gt;
      &lt;p&gt;But we don't know what's possible because it's dependent on how you've written your app.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That does all sound pretty scary! 😱 So you can understand the consternation that the issue has caused, especially given the hesitation to explain exactly what "how you've written your app" means.&lt;/p&gt;
    &lt;p&gt;Out of curiosity, I decided to dig in further to really understand this vulnerability, how it could impact you, and what "how you've written your app" could mean.&lt;/p&gt;
    &lt;head rend="h2"&gt;How does request smuggling work?&lt;/head&gt;
    &lt;p&gt;Before we get to the actual patched vulnerability in ASP.NET Core and how the vulnerability works, I think it's important to have some background about the general class of exploits known as HTTP request smuggling.&lt;/p&gt;
    &lt;p&gt;HTTP request smuggling is a security exploit that has been known about for a long time (according to Wikipedia, it was first documented in 2005). It fundamentally arises when you have two different servers processing an HTTP request (e.g. a server and a proxy server), and where those two servers differ in how they handle "invalid" HTTP requests.&lt;/p&gt;
    &lt;p&gt;In all cases of HTTP request smuggling, the exploit works by creating an invalid HTTP request (or sometimes just an ambiguous request), that looks a bit like two HTTP requests glued together. In summary, the exploit then works a bit like this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The proxy server receives the ambiguous HTTP request&lt;/item&gt;
      &lt;item&gt;The proxy server forwards the request (unmodified) to the destination server&lt;/item&gt;
      &lt;item&gt;The server interprets the ambiguous request as two pipelined HTTP requests sent to the server, and processes them separately.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I think it's easiest to understand the problem with an example, so the request below shows an example from the original 2005 paper.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note that this is not an example of the request smuggling vulnerability in CVE-2025-55315, it's just a representative example of request smuggling in general.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Let's imagine the attacker sends an HTTP request that looks like this:&lt;/p&gt;
    &lt;code&gt;POST /some_script.jsp HTTP/1.0
Connection: Keep-Alive
Content-Type: application/x-www-form-urlencoded
Content-Length: 9
Content-Length: 204

this=thatPOST /vuln_page.jsp HTTP/1.0
Content-Type: application/x-www-form-urlencoded
Content-Length: 95

param1=value1&amp;amp;data=&amp;lt;script&amp;gt;alert("stealing%20your%20data:"%2bdocument.cookie)&amp;lt;/script&amp;gt;&amp;amp;foobar
&lt;/code&gt;
    &lt;p&gt;The important feature of this request is that there are two &lt;code&gt;Content-Length&lt;/code&gt; headers, with different values: &lt;code&gt;9&lt;/code&gt; or &lt;code&gt;204&lt;/code&gt;. This is the core of the exploit; the difference between which of the these two headers the HTTP proxy and HTTP server honour is what causes the vulnerability.&lt;/p&gt;
    &lt;p&gt;Let's walk through how the exploit works, step-by-step:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The attacker sends the above HTTP request.&lt;/item&gt;
      &lt;item&gt;The HTTP proxy receives the request, notes the duplicate &lt;code&gt;Content-Length&lt;/code&gt;headers, and accepts the second header, the&lt;code&gt;204&lt;/code&gt;length. That means the whole rest of the request is treated as the message body, and seems fine as far as the proxy is concerned.&lt;/item&gt;
      &lt;item&gt;The HTTP proxy forwards the request on to the destination server.&lt;/item&gt;
      &lt;item&gt;This server also notes the duplicate &lt;code&gt;Content-Length&lt;/code&gt;header, but it takes the first of the headers, with the length of&lt;code&gt;9&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The server reads &lt;code&gt;9&lt;/code&gt;bytes of the body (i.e.&lt;code&gt;this=that&lt;/code&gt;) and treats that as the whole request. As far as the server is concerned, the whole (valid) request has been received, and it sees the rest of the data as a whole new request.&lt;/item&gt;
      &lt;item&gt;That means that the destination server sees an entirely new HTTP request to process, &lt;code&gt;POST /vuln_page.jsp&lt;/code&gt;, and treats it as a new request.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That's the core of the issue; the proxy saw one request, while the destination server saw two—the second request has been "smuggled" past the proxy to the server.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;The request smuggling technique shown here, where you have multiple&lt;/p&gt;&lt;code&gt;Content-Length&lt;/code&gt;headers isn't the "canonical" example you'll generally see referenced, but I used it here because it's simpler to understand in a lot of ways.&lt;p&gt;The canonical request smuggling attack is where you send both a&lt;/p&gt;&lt;code&gt;Content-Length&lt;/code&gt;header and a&lt;code&gt;Transfer-Encoding: chunked&lt;/code&gt;header (which specifies the length of the body as part of the body itself). As before, the request smuggling exploit relies on differences in how proxy and destination servers interpret these conflicting headers.&lt;/quote&gt;
    &lt;p&gt;So as you've seen, request smuggling enables sending a secret request to a destination server that an intermediate proxy server hasn't seen. In the next section we'll look at why that's a bad thing, and how it can be exploited.&lt;/p&gt;
    &lt;head rend="h2"&gt;How can an attacker exploit request smuggling?&lt;/head&gt;
    &lt;p&gt;On the face of it, request smuggling might not seem like a big deal. So the server sees two requests, so what? You could always send two requests to the server anyway, right? Well, yes and no.&lt;/p&gt;
    &lt;p&gt;The issue with request smuggling is really all about the mismatch between the proxy and destination servers. Thanks to this mismatch, and depending on what behaviours and expectations the target application has, attackers can use request smuggling to&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reflect malicious data to other users on sites that are vulnerable to cross-site scripting.&lt;/item&gt;
      &lt;item&gt;Poison caches with bad data.&lt;/item&gt;
      &lt;item&gt;Exfiltrate authentication credentials or other data from client requests.&lt;/item&gt;
      &lt;item&gt;Invoke endpoints that shouldn't be publicly accessible (because the proxy would block external access to them).&lt;/item&gt;
      &lt;item&gt;Replace/override authentication controls handled by the proxy.&lt;/item&gt;
      &lt;item&gt;Redirect users to malicious sites on sites vulnerable to open-redirect attacks.&lt;/item&gt;
      &lt;item&gt;And more…&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As you can see, these are all Bad™️, so you can kind of understand why the 9.9 rating was given! 😱&lt;/p&gt;
    &lt;p&gt;That said, it's worth mentioning that not all of these attacks will be fruitful against all applications. Some of the easiest to understand versions of these exploits are where the proxy is not just doing "dumb" forwarding of requests, but rather it's validating or enhancing the request in some way.&lt;/p&gt;
    &lt;p&gt;For example, if you have a proxy sat in front of your server which is responsible for handling TLS termination and client-authentication and identification using certificates, then request smuggling could be used to bypass these checks and insert your own identification.&lt;/p&gt;
    &lt;p&gt;As an example of that attack, the HTTP request below demonstrates using a &lt;code&gt;Content-Length&lt;/code&gt; and &lt;code&gt;Transfer-Encoding&lt;/code&gt; request smuggling attack to "hide" the request to &lt;code&gt;/admin&lt;/code&gt; from the front-end proxy, and insert a malicious &lt;code&gt;X-SSL-CLIENT-CN&lt;/code&gt; header, which would normally be added by the front-end proxy:&lt;/p&gt;
    &lt;code&gt;POST /example HTTP/1.1
Host: some-website.com
Content-Type: x-www-form-urlencoded
Content-Length: 64
Transfer-Encoding: chunked

0

GET /admin HTTP/1.1
X-SSL-CLIENT-CN: administrator
Foo: x
&lt;/code&gt;
    &lt;p&gt;In this example, the server assumes that the &lt;code&gt;X-SSL-CLIENT-CN: administrator&lt;/code&gt; header was added by the proxy, and so the server assumes that the proxy already did all the necessary authentication and authorization. The attacker is able to perform a request as an entirely different user.&lt;/p&gt;
    &lt;p&gt;Request smuggling is clearly a big problem whenever you have a front-end proxy that does some functionality, but even when it's essentially a dumb proxy, request smuggling can still be used to steal and exfiltrate data from other user's requests, even if the attacked site is not vulnerable to cross-site scripting or other vulnerabilities.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In these attacks, simply having functionality that displays data provided by a user (even sanitised) can be sufficient to steal the credentials of other users. So something as simple as displaying a user name or a comment could be sufficient.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This post is long enough, and there are so many different attacks, that I'm going to leave it there for looking at exploits. If you'd like to learn more about what's possible, along with simple explanations and examples of exploits, I recommend the PortSwigger documentation on exploiting request smuggling.&lt;/p&gt;
    &lt;head rend="h2"&gt;Does request smuggling only apply if I have a proxy?&lt;/head&gt;
    &lt;p&gt;In general, whenever people talk about request smuggling, they normally talk about the case where you have multiple servers: the canonical example is a proxy server and a destination server, as I've discussed so far. But don't be fooled, these issues and vulnerabilities can apply even if you aren't strictly using a proxy.&lt;/p&gt;
    &lt;p&gt;The key feature of the vulnerability is that there's an opportunity for confusion between two "systems", whether they're full "servers" or not. This obviously applies to proxy servers, but could also apply to your application if you're doing anything where you're reading/manipulating/forwarding request streams, or where there's the possibility for confusion inside the same application.&lt;/p&gt;
    &lt;p&gt;For ASP.NET Core applications, if you're working with &lt;code&gt;HttpRequest.Body&lt;/code&gt; or &lt;code&gt;HttpRequest.BodyReader&lt;/code&gt;, or other similar methods then you may be vulnerable to attacks even if you're not explicitly using a proxy server. Even if you don't think of your application as a proxy or as using a proxy, if you're doing "proxy-like" things, then you could be vulnerable.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Put in other words, if you're reading, manipulating, or forwarding request streams directly in ASP.NET Core, as opposed to just relying on the built-in model binding, then you could be at risk to request smuggling attacks. It's very hard to enumerate all the attack vectors, so you should consider any code that does so as a potential avenue of exploitation.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;We've now covered how request smuggling works and can be exploited in general, so it's time to look at the specific version of request smuggling that is targeted in the .NET CVE-2025-55315 vulnerability.&lt;/p&gt;
    &lt;head rend="h2"&gt;How does the request smuggling in CVE-2025-55315 work?&lt;/head&gt;
    &lt;p&gt;As we've seen, HTTP request smuggling is a general technique that relies on differences between proxies and servers in how they parse HTTP requests. I've shown two specific versions of this so far: duplicate &lt;code&gt;Content-Length&lt;/code&gt; headers, and &lt;code&gt;Content-Length&lt;/code&gt;/&lt;code&gt;Transfer-Encoding&lt;/code&gt; confusion, but these are not exhaustive. There are variations on these approaches which also lead to request smuggling.&lt;/p&gt;
    &lt;p&gt;The request smuggling vulnerability in CVE-2025-55315 relies on a variation which (as far as I can tell) was first reported in June 2025 by Jeppe Bonde Weikop on their blog. This variation relies on &lt;code&gt;Transfer-Encoding&lt;/code&gt; and the Chunk Extensions feature.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;All the details and images in this section are based on the descriptions and examples in the original post. That post is excellent, so if you want even more detail and explanation than here, you should definitely read it, and then you can skip the abbreviated version I provide here.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;To understand the vulnerability, we'll first look at how chunked transfer encoding works and what chunk extensions are. We'll then look at how invalid line-endings can lead to differences in interpretation of a request. Finally, we'll look at how this difference in interpretation can open the way for request smuggling, and how ASP.NET Core fixed the problem.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;code&gt;Transfer-Encoding: chunked&lt;/code&gt; and chunk extensions&lt;/head&gt;
    &lt;p&gt;To understand the vulnerability, we first need to understand how &lt;code&gt;Transfer-Encoding: chunked&lt;/code&gt; works, and how chunk extensions complicate things.&lt;/p&gt;
    &lt;p&gt;When you're sending a request, you might not always know up-front how big the request is that you're sending. Let's take a practical example of serializing a .NET object to JSON into a request body. The only way to know for sure how big the serialized data is going to be is to actually serialize it. So you could serialize the data to memory before writing the request, but if the data is very big, then that could cause issues with allocating big arrays.&lt;/p&gt;
    &lt;p&gt;Instead, &lt;code&gt;Transfer-Encoding: chunked&lt;/code&gt; allows sending the request data in multiple "chunks". You need to know the size of each individual chunk, but not the overall size of the data, or how many chunks there are. This works well for serializing to a small buffer, sending that small buffer as a chunk, and then re-using the buffer to serialize the next part, until you have serialized the whole object.&lt;/p&gt;
    &lt;p&gt;In terms of the HTTP request itself, each chunk consists of a header and a body. The header consists of a hexadecimal-formatted number of bytes, followed by a &lt;code&gt;\r\n&lt;/code&gt; (&lt;code&gt;CRLF&lt;/code&gt;) line ending. The chunk body is then the specified number of bytes, followed by another &lt;code&gt;\r\n&lt;/code&gt;. You can have as many chunks as you need, and the request will keep being passed until you send a &lt;code&gt;0&lt;/code&gt; length chunk, which indicates the end of the request.&lt;/p&gt;
    &lt;p&gt;As an example, the following HTTP &lt;code&gt;POST&lt;/code&gt; shows posting some JSON to an endpoint, but the JSON is sent as three distinct chunks:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Chunk 1: The header is &lt;code&gt;9&lt;/code&gt;indicating 9 bytes will be sent (followed by&lt;code&gt;\r\n&lt;/code&gt;), and then the 9 bytes of the start of the JSON document in the chunk body, again followed by&lt;code&gt;\r\n&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Chunk 1: The header is &lt;code&gt;e&lt;/code&gt;indicating 14 bytes (14 in hexadecimal is&lt;code&gt;e&lt;/code&gt;) will be sent (followed by&lt;code&gt;\r\n&lt;/code&gt;), and then the remaining 14 bytes of the end of the JSON document, followed by&lt;code&gt;\r\n&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The final chunk is an "empty" chunk, &lt;code&gt;0\r\n\r\n&lt;/code&gt;, indicating the end of the request.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We're going to see shortly that line endings are very important, so the following diagram shows the same as the above HTTP request, but with the line endings included:&lt;/p&gt;
    &lt;p&gt;That's "normal" chunked transfer encoding, so now we come to chunk extensions. Chunk extensions are part of the HTTP 1.1 protocol which allows for adding key-value pairs of metadata to individual chunks. The following example shows the same request as before, but with a chunk extension, &lt;code&gt;;foo=bar&lt;/code&gt; in the second chunk:&lt;/p&gt;
    &lt;p&gt;A chunk extension is indicated by a &lt;code&gt;;&lt;/code&gt; after the chunk header length, followed by one or more key-value pairs in the form &lt;code&gt;key=value&lt;/code&gt;. It's important to understand that chunk extensions are not part of the data that's seen by a request handler; chunk extensions are just metadata about the individual chunk. And tl;dr; they're completely useless 😅&lt;/p&gt;
    &lt;p&gt;To the closest approximation, no-one cares about chunk extensions; client implementations don't send them, and servers just ignore them. If that's the case, how can they be the cause of such a problematic bug in .NET?&lt;/p&gt;
    &lt;p&gt;The problem is how the implementation ignores them…&lt;/p&gt;
    &lt;head rend="h3"&gt;Invalid chunk extensions with incorrect line endings&lt;/head&gt;
    &lt;p&gt;In general with HTTP, clients and server implementations often try to follow the robustness principle of "be conservative in what you send, and lenient with what you accept". Unfortunately, it's this very leniency which can sometimes leave us in hot water. After all, it was leniency around requests containing both a &lt;code&gt;Content-Length&lt;/code&gt; and &lt;code&gt;Transfer-Encoding&lt;/code&gt; header that was the root cause of the original request smuggling exploit.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note that the HTTP 1.1 RFC now forbids forwarding both these headers, precisely to avoid request smuggling attacks.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;For chunk extensions though, leniency is often accidentally built in to the server implementations. Given that no implementations actually do anything with the chunk extensions, the canonical approach to handling them when parsing a chunk header is just to ignore them. When a &lt;code&gt;;&lt;/code&gt; is parsed, it's common to just look for the end of the line, and ignore everything in between.&lt;/p&gt;
    &lt;p&gt;For ASP.NET Core (prior to the fix), on finding a &lt;code&gt;;&lt;/code&gt; in the chunk header, Kestrel would "parse" the extension, but in practice, it would search for the carriage return &lt;code&gt;\r&lt;/code&gt; and then check for the following &lt;code&gt;\n&lt;/code&gt;, skipping everything in between, a little bit like this (very simplified compared to original code):&lt;/p&gt;
    &lt;code&gt;private void ParseExtension(ReadOnlySequence&amp;lt;byte&amp;gt; buffer)
{
    while(true)
    {
        // Chunk-extensions not currently parsed
        // Just drain the data
        var extensionCursor = buffer.PositionOf(ByteCR);
        var suffixBuffer = buffer.Slice(extensionCursor); // skips over extensionCursor bytes

        var suffixSpan = suffixBuffer.Slice(0, 2).ToSpan();

        if (suffixSpan[1] == '\n')
        {
            // We consumed the \r\n at the end of the extension, so switch modes.
            return;
        }

        // Otherwise, keep reading data until we do find \r\n
        buffer = ReadMoreData();
    }
}
&lt;/code&gt;
    &lt;p&gt;The implementation in ASP.NET Core wasn't particularly special; most servers simply skip over the bytes until they find a &lt;code&gt;\r\n&lt;/code&gt;. The big question is exactly how the servers search for &lt;code&gt;\r\n&lt;/code&gt;. What happens if they see a lone &lt;code&gt;\r&lt;/code&gt;, or a lone &lt;code&gt;\n&lt;/code&gt;? Do they treat that the same as a &lt;code&gt;\r\n&lt;/code&gt;? Do they throw an error if they find an un-paired &lt;code&gt;\r&lt;/code&gt; or &lt;code&gt;\n&lt;/code&gt;? Or do they ignore it and keep looking for a &lt;code&gt;\r\n&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;That ambiguity is at the heart of the CVE-2025-55315 request smuggling vulnerability. Differences in how proxy and server implementations treat standalone &lt;code&gt;\r&lt;/code&gt; or &lt;code&gt;\n&lt;/code&gt; in a chunk header allow for request smuggling exploits that use this ambiguity.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Note that according to the RFC, implementers must not treat&lt;/p&gt;&lt;code&gt;\r&lt;/code&gt;or&lt;code&gt;\n&lt;/code&gt;as "valid" line terminators for a chunk header, and neither&lt;code&gt;\r&lt;/code&gt;or&lt;code&gt;\n&lt;/code&gt;are allowed elsewhere in chunk headers, so correct implementations must reject requests that include these standalone line endings in chunk headers.&lt;/quote&gt;
    &lt;p&gt;For complete clarity, the following example is the same as the previous implementation but with an invalid chunk header in the chunk extension of the second chunk. Instead of ending with &lt;code&gt;\r\n&lt;/code&gt;, the chunk extension ends with a single &lt;code&gt;\n&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;That's the root cause of the request smuggling vulnerability, so in the next section we'll look at how this could be used to craft a malicious HTTP request.&lt;/p&gt;
    &lt;head rend="h3"&gt;Exploiting invalid chunk extensions for request smuggling&lt;/head&gt;
    &lt;p&gt;Just as with other examples of request smuggling, the chunk extensions approach relies on differences in how a proxy parses a request compared to a subsequent server. This difference means the proxy sees one request, while the destination request sees two requests, and allows for all the same exploits I discussed earlier.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;As discussed, these examples come from this excellent blog post, so see that post for more details, variations on the attack, and further ways to exploit the vulnerability.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The following example shows a malicious HTTP request that exploits a difference in line-ending handling between a proxy and the destination server to smuggle a request to the &lt;code&gt;/admin&lt;/code&gt; endpoint. We can imagine that the proxy is configured to automatically reject requests to &lt;code&gt;/admin&lt;/code&gt; normally, and the server assumes that the proxy handles that for us.&lt;/p&gt;
    &lt;p&gt;In this example the attacker creates a malformed chunk header with a chunk extension by sending &lt;code&gt;2;\n&lt;/code&gt;. The &lt;code&gt;;&lt;/code&gt; ensures that both the proxy and and server treat the header as a chunk extension, but using &lt;code&gt;\n&lt;/code&gt; instead of &lt;code&gt;\r\n&lt;/code&gt; results in differential parsing:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The proxy only sees a single request: &lt;list rend="ul"&gt;&lt;item&gt;It treats the &lt;code&gt;\n&lt;/code&gt;as a "valid" line-ending for the chunk header&lt;/item&gt;&lt;item&gt;It then treats the &lt;code&gt;xx&lt;/code&gt;as the chunk body&lt;/item&gt;&lt;item&gt;&lt;code&gt;47&lt;/code&gt;is the next chunk header&lt;/item&gt;&lt;item&gt;The next 71 bytes (&lt;code&gt;47&lt;/code&gt;is hex, which is 71 in decimal) are treated as the chunk body.&lt;/item&gt;&lt;item&gt;Finally there's the empty chunk block&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;It treats the &lt;/item&gt;
      &lt;item&gt;The server sees two requests: &lt;list rend="ul"&gt;&lt;item&gt;The server ignores the lone &lt;code&gt;\n&lt;/code&gt;, and skips all the way to&lt;code&gt;xx\r\n&lt;/code&gt;&lt;/item&gt;&lt;item&gt;It then treats the &lt;code&gt;47&lt;/code&gt;as the chunk body&lt;/item&gt;&lt;item&gt;It sees an ending chunk,&lt;code&gt;0\r\n\r\n&lt;/code&gt;and thinks the request is over&lt;/item&gt;&lt;item&gt;The remaining data is treated as a completely separate request, which contains only an empty chunk in the body.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;The server ignores the lone &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is pretty much the simplest example, but you can essentially exploit this difference in all the ways I described previously. Exactly what the implications are for your application are hard to say, but given that all sorts of security bypass, credential stealing, and injection attacks are possible, it's easy to understand why the vulnerability received a CVSS rating of 9.9.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;One very interesting thing I found was looking at the security advisories for the same flaw in other HTTP implementations from other languages. In the python aiohttp and ruby puma servers, for example, give the vulnerability only a moderate severity rating in both cases. In netty it's even given a low severity.&lt;/p&gt;
      &lt;p&gt;As far as I can tell, these servers are essentially vulnerable in the same way as ASP.NET Core is, so it's just an interesting data point, and I think reflects how Microsoft really want to make sure this gets the visibility it deserves and that customers patch their apps!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;How was the vulnerability fixed?&lt;/head&gt;
    &lt;p&gt;As with most fixes for request-smuggling, the solution is to stop being lenient and/or ambiguous about how standalone line-endings are handled in chunk headers.&lt;/p&gt;
    &lt;p&gt;In ASP.NET Core, the PR that fixes the issue does so by explicitly checking for any line-endings, instead of just looking for &lt;code&gt;\r&lt;/code&gt;. If it finds a line ending and it's not strictly &lt;code&gt;\r\n&lt;/code&gt;, then Kestrel now throws a &lt;code&gt;KestrelBadHttpRequestException&lt;/code&gt; and returns a &lt;code&gt;400&lt;/code&gt; response.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;I'll mention here there is an&lt;/p&gt;&lt;code&gt;AppContext&lt;/code&gt;switch for opting-in to the dangerous/vulnerable parsing behaviour after you have patched your application, but please don't use it, I can't believe there's really a good (or safe) reason to.😅&lt;/quote&gt;
    &lt;p&gt;The vulnerability has been patched in ASP.NET Core, so what should you do?&lt;/p&gt;
    &lt;head rend="h2"&gt;What should you do?&lt;/head&gt;
    &lt;p&gt;Obviously the good news here is that there is a fix for ASP.NET Core. As described in the original issue, the important thing is to update to the latest supported version of ASP.NET Core as soon as possible.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;There's no announced evidence of the request smuggling vulnerability being exploited in the wild, but given the vast number of ways that request smuggling could be used, would we even know? 🤔&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That means you should update your version of .NET 8, .NET 9, or .NET 10:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Vulnerable versions&lt;/cell&gt;
        &lt;cell role="head"&gt;Lowest patched version&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;.NET 10&lt;/cell&gt;
        &lt;cell&gt;10.0.0-rc2&lt;/cell&gt;
        &lt;cell&gt;10.0.0-rc2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;.NET 9&lt;/cell&gt;
        &lt;cell&gt;9.0.0 - 9.0.9&lt;/cell&gt;
        &lt;cell&gt;9.0.10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;.NET 8&lt;/cell&gt;
        &lt;cell&gt;8.0.0 - 8.0.20&lt;/cell&gt;
        &lt;cell&gt;8.0.21&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If you're using ASP.NET Core 2.3 on .NET Framework, then you'll need to update your version of Microsoft.AspNetCore.Server.Kestrel.Core:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Vulnerable versions&lt;/cell&gt;
        &lt;cell role="head"&gt;Lowest patched version&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Microsoft.AspNetCore.Server.Kestrel.Core&lt;/cell&gt;
        &lt;cell&gt;2.0.0-2.3.0&lt;/cell&gt;
        &lt;cell&gt;2.3.6&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If you are doing self-contained deployments of your applications, you'll need to update to the patched versions and then redeploy your applications.&lt;/p&gt;
    &lt;p&gt;And if you're using older versions of .NET Core? Well, then you can't patch… HeroDevs provide additional support for out-of-support versions of .NET (and have confirmed they'll be patching it in .NET 6), but this vulnerability is present in basically all versions of .NET Core as far as I can tell. I've personally tested down to .NET Core 3.0 and I can confirm that the vulnerability is there and there are no patches coming for you. The best thing to do is to update to a supported version of .NET.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;⚠️ If you are running ASP.NET Core using &amp;lt;=.NET Core 3.0, .NET Core 3.1, .NET 5, .NET 6 (unless supported by HeroDevs), or .NET 7, then you are vulnerable, and there are no patches. You should update to a supported version of .NET as soon as possible. Ironically, if you're stuck on old .NET Framework Web Forms or MVC applications you are apparently not vulnerable.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It's worth noting that if you are stuck on one of these old framework versions and can't upgrade, then probably the best way to protect yourself is to ensure that you have a proxy in front of your application which is confirmed to not be vulnerable (though obviously you are likely vulnerable to other exploits 😅).&lt;/p&gt;
    &lt;p&gt;For example, Azure App Services (AAS) confirmed that applications running in AAS are no longer vulnerable, even if you haven't updated, because the proxy that AAS uses (itself a YARP based ASP.NET Core proxy) has been patched. By blocking the requests at the proxy level, ambiguous requests will never make it to your application, so you are protected.&lt;/p&gt;
    &lt;p&gt;Unfortunately, right now, it's not clear exactly where you stand if you're using a service other than AAS for hosting your applications. Even IIS hasn't been confirmed to be safe or vulnerable at this point, but I did some unofficial testing on my Windows 11 box, and as fat as I can tell, it is vulnerable.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Note that various people in the original issue are attempting to test IIS by using the&lt;/p&gt;&lt;code&gt;Content-Length&lt;/code&gt;/&lt;code&gt;Transfer-Encoding&lt;/code&gt;version of request smuggling, which is not applicable here; we're interested in the chunk-extensions based version.&lt;/quote&gt;
    &lt;p&gt;Another interesting point is that this is vulnerability in HTTP/1.0 and HTTP/1.1 only; it is not a vulnerability in HTTP/2 or HTTP/3. HTTP/2 and HTTP/3 do not support chunked transfer encoding, and instead uses a different, more efficient, binary framing layer for data streaming. So another way to protect those applications which you can't upgrade may be to enforce that client's can only use HTTP/2 or HTTP/3. Be aware that's liable to break a lot of clients that are still using HTTP/1.1 though!&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;You can configure the HTTP protocols allowed by Kestrel by configuring your Kestrel endpoints. The documentation shows various ways to do this.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;How to know if you're affected?&lt;/head&gt;
    &lt;p&gt;The "simplest" way to know if you're affected is to check the version of .NET you're using to run your applications, using &lt;code&gt;dotnet --info&lt;/code&gt; and verify that you're using one of the patched versions. If you are, you're safe. That's the only "supported" way to know that you're safe, and it's the one way I would recommend. As far as I can tell, there isn't currently a generalised tool to point at an application to find out if it's vulnerable, though it would likely be possible to write one.&lt;/p&gt;
    &lt;p&gt;The folks at HeroDevs re-implemented the functional tests from the original ASP.NET Core fix as a console application compiled against multiple versions of ASP.NET Core. They used this to confirm that unpatched versions of .NET 8-.NET 10 are vulnerable, while patched versions are not. They also used this to verify .NET 6 is vulnerable, and I tweaked it to confirm everything down to at least .NET Core 3.0 is vulnerable.&lt;/p&gt;
    &lt;p&gt;The test in the repro works by sending a chunked transfer encoding request to ASP.NET Core, with an invalid line ending in a chunk extension header. The vulnerability is identified by ASP.NET Core "hanging", waiting for more data, until it eventually times out. The "fixed" version immediately throws the &lt;code&gt;BadRequest&lt;/code&gt; exception included in the fix.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I saw some confusion about this test online; the argument was "if both the fixed and broken versions throw an exception, why does it matter"? However, that's not the point of the test. The fact that Kestrel is paused waiting for more data indicates that a smuggled HTTP request would have been executed. You can see how this can be leveraged to exfiltrate data or attack other users both in the chunk extensions blog or on PortSwigger's site.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I used a similar approach to try to understand whether IIS might be vulnerable by sending the same crafted HTTP request to IIS and seeing if it hung until timing out: it did on my version of IIS (&lt;code&gt;10.0.26100.1882&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;# Send an HTTP request with an invalid chunk extension, and see
# if it times out or if it's rejected with a 400... It times out 🙁
echo -e "GET / HTTP/1.1\r\nHost:\r\nTransfer-Encoding: chunked\r\n\r\n1;\n" \
  | nc localhost 80
&lt;/code&gt;
    &lt;p&gt;So does that definitely mean IIS is vulnerable? No, don't trust me, I'm not a security researcher 😅 But until you hear otherwise, I would play it safe and assume that IIS won't protect you from chunk extension request smuggling attacks. And in general, I would apply the same rules to any other proxies you are relying on in your infrastructure.&lt;/p&gt;
    &lt;p&gt;And as a final reminder, even though request smuggling is typically described and demonstrated using a proxy in front of your server, just not using a proxy does not mean you're automatically safe. If you're reading, manipulating, or forwarding request streams directly in ASP.NET Core, as opposed to just relying on the built-in model binding, then you might be at risk to request smuggling attacks. It's best to play it safe, patch your apps, and wherever possible leave the complexity of manipulating requests to ASP.NET Core.&lt;/p&gt;
    &lt;p&gt;In general, I would make sure to subscribe to the ASP.NET Core issue on GitHub, as it's likely that any more announcements around the issue will also be reported there.&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;p&gt;In this post I discuss the recent ASP.NET Core vulnerability: Microsoft Security Advisory CVE-2025-55315: .NET Security Feature Bypass Vulnerability. This advisory warns of a request smuggling vulnerability that affects basically all versions of ASP.NET Core.&lt;/p&gt;
    &lt;p&gt;I described how request smuggling works in general, using a simple example of request smuggling to show how ambiguity in how HTTP is parsed can lead to HTTP proxies and HTTP servers in handling the same HTTP request in different ways. This can lead to the server seeing two requests where the proxy only sees a single request.&lt;/p&gt;
    &lt;p&gt;After walking through a request smuggling example, I discussed some of the ways attackers could exploit a request smuggling vulnerability. That includes reflecting malicious data to other users of your app, exfiltrating authentication credentials or other data from client requests, invoking endpoints that shouldn't be publicly accessible, and various other attacks.&lt;/p&gt;
    &lt;p&gt;Next I walked through the specific request smuggling vulnerability identified in CVE-2025-55315. This uses ambiguities in the parsing of chunk extensions when sending requests that use chunked transfer encoding. Chunk extensions are generally ignored by all servers, but lenient handling can lead to differential handling between proxy and server, providing an avenue for request smuggling.&lt;/p&gt;
    &lt;p&gt;Finally, I walked through the mitigation steps you should take: patching your applications. I described the information we currently have about vulnerable or patched proxy servers, and how old versions of ASP.NET Core are not going to be getting patches, so will remain vulnerable (shout out again to HeroDevs for supporting .NET 6). If you're running in AAS, then you're ok, but otherwise, you need to check with your proxy provider to establish whether you are vulnerable or not.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45731315</guid><pubDate>Tue, 28 Oct 2025 11:03:38 +0000</pubDate></item></channel></rss>