<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 01 Sep 2025 16:11:49 +0000</lastBuildDate><item><title>Eternal Struggle</title><link>https://yoavg.github.io/eternal/</link><description>&lt;doc fingerprint="57c9d1e55408cc08"&gt;
  &lt;main&gt;
    &lt;p&gt;change background&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45086020</guid></item><item><title>We should have the ability to run any code we want on hardware we own</title><link>https://hugotunius.se/2025/08/31/what-every-argument-about-sideloading-gets-wrong.html</link><description>&lt;doc fingerprint="f58638d75127bfe5"&gt;
  &lt;main&gt;
    &lt;p&gt;Sideloading has been a hot topic for the last decade. Most recently, Google has announced further restrictions on the practice in Android. Many hundreds of comment threads have discussed these changes over the years. One point in particular is always made: √¢I should be able to run whatever code I want on hardware I own√¢. I agree entirely with this point, but within the context of this discussion it√¢s moot.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;√¢I should be able to run whatever code I want on hardware I own√¢&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When Google restricts your ability to install certain applications they aren√¢t constraining what you can do with the hardware you own, they are constraining what you can do using the software they provide with said hardware. It√¢s through this control of the operating system that Google is exerting control, not at the hardware layer. You often don√¢t have full access to the hardware either and building new operating systems to run on mobile hardware is impossible, or at least much harder than it should be. This is a separate, and I think more fruitful, point to make. Apple is a better case study than Google here. Apple√¢s success with iOS partially derives from the tight integration of hardware and software. An iPhone without iOS is a very different product to what we understand an iPhone to be. Forcing Apple to change core tenets of iOS by legislative means would undermine what made the iPhone successful.&lt;/p&gt;
    &lt;p&gt;You shouldn√¢t take away from this that I am some stalwart defender of the two behemoths Apple and Google, far from it. However, our critique shouldn√¢t be of the restrictions in place in the operating systems they provide √¢ rather, it should focus on the ability to truly run any code we want on hardware we own. In this context this would mean having the ability and documentation to build or install alternative operating systems on this hardware. It should be possible to run Android on an iPhone and manufacturers should be required by law to provide enough technical support and documentation to make the development of new operating systems possible. If you want to play Playstation games on your PS5 you must suffer Sony√¢s restrictions, but if you want to convert your PS5 into an emulator running Linux that should be possible.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45087396</guid></item><item><title>A Linux version of the Procmon Sysinternals tool</title><link>https://github.com/microsoft/ProcMon-for-Linux</link><description>&lt;doc fingerprint="a010c81b3ca44182"&gt;
  &lt;main&gt;
    &lt;p&gt;Process Monitor (Procmon) is a Linux reimagining of the classic Procmon tool from the Sysinternals suite of tools for Windows. Procmon provides a convenient and efficient way for Linux developers to trace the syscall activity on the system.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OS: Ubuntu 18.04 lts&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;cmake&lt;/code&gt;&amp;gt;= 3.14 (build-time only)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;libsqlite3-dev&lt;/code&gt;&amp;gt;= 3.22 (build-time only)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please see installation instructions here.&lt;/p&gt;
    &lt;p&gt;Please see build instructions here.&lt;/p&gt;
    &lt;code&gt;Usage: procmon [OPTIONS]
   OPTIONS
      -h/--help                Prints this help screen
      -p/--pids                Comma separated list of process IDs to monitor
      -e/--events              Comma separated list of system calls to monitor
      -c/--collect [FILEPATH]  Option to start Procmon in a headless mode
      -f/--file FILEPATH       Open a Procmon trace file
      -l/--log FILEPATH        Log debug traces to file&lt;/code&gt;
    &lt;p&gt;The following traces all processes and syscalls on the system:&lt;/p&gt;
    &lt;code&gt;sudo procmon&lt;/code&gt;
    &lt;p&gt;The following traces processes with process id 10 and 20:&lt;/p&gt;
    &lt;code&gt;sudo procmon -p 10,20&lt;/code&gt;
    &lt;p&gt;The following traces process 20 only syscalls read, write and open at:&lt;/p&gt;
    &lt;code&gt;sudo procmon -p 20 -e read,write,openat&lt;/code&gt;
    &lt;p&gt;The following traces process 35 and opens Procmon in headless mode to output all captured events to file &lt;code&gt;procmon.db&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;sudo procmon -p 35 -c procmon.db&lt;/code&gt;
    &lt;p&gt;The following opens a Procmon &lt;code&gt;tracefile&lt;/code&gt;, &lt;code&gt;procmon.db&lt;/code&gt;, within the Procmon TUI:&lt;/p&gt;
    &lt;code&gt;sudo procmon -f procmon.db&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ask a question on Stack Overflow (tag with ProcmonForLinux)&lt;/item&gt;
      &lt;item&gt;Request a new feature on GitHub&lt;/item&gt;
      &lt;item&gt;Vote for popular feature requests&lt;/item&gt;
      &lt;item&gt;File a bug in GitHub Issues&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you are interested in fixing issues and contributing directly to the code base, please see the document How to Contribute, which covers the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How to build and run from the source&lt;/item&gt;
      &lt;item&gt;The development workflow, including debugging and running tests&lt;/item&gt;
      &lt;item&gt;Coding Guidelines&lt;/item&gt;
      &lt;item&gt;Submitting pull requests&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please see also our Code of Conduct.&lt;/p&gt;
    &lt;p&gt;Copyright (c) Microsoft Corporation. All rights reserved.&lt;/p&gt;
    &lt;p&gt;Licensed under the MIT License.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45087748</guid></item><item><title>Lewis and Clark marked their trail with laxatives</title><link>https://offbeatoregon.com/2501d1006d_biliousPills-686.077.html</link><description>&lt;doc fingerprint="b4199d0d24278264"&gt;
  &lt;main&gt;
    &lt;head rend="h4"&gt;ASTORIA, CLATSOP COUNTY; 1800s:&lt;/head&gt;
    &lt;head rend="h1"&gt;Lewis and Clark marked their trail with laxatives&lt;/head&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;Audio version is not yet available&lt;/head&gt;
          &lt;head&gt;By Finn J.D. John&lt;/head&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;‚ÄúSome people have stated that the Lewis and Clark Expedition would have been better off if they had taken a trained physician along to care for the numerous problems that they encountered. I totally disagree,‚Äù says physician and historian David Peck. ‚ÄúI think a trained physician would have been overly confident and possibly would have been much more aggressive in their treatment of illnesses, often times to the detriment of the patient.‚Äù&lt;/p&gt;
          &lt;p&gt;In lieu of a trained physician, the Corps of Discovery‚Äôs leaders got some basic medical training, along with a bag full of the tools of allopathic intervention: lancets for bleeding patients, blister powder for inducing ‚Äúheat,‚Äù opium products for relieving pain and inducing sleep √¢ and purgatives.&lt;/p&gt;
          &lt;p&gt;Those purgatives are the heroes of our story today. They came in the form of beefy pills, about four times the size of a standard aspirin tablet, which Rush called ‚ÄúDr. Rush‚Äôs Bilious Pills.‚Äù They contained about 10 grains of calomel and 10 to 15 grains of jalap.&lt;/p&gt;
          &lt;p&gt;Jalap, the powdered root of a Mexican variety of morning glory, is a natural laxative of considerable power.&lt;/p&gt;
          &lt;p&gt;And calomel ... ah, calomel. Calomel was the wonder drug of the age. Its chemical name is mercury chloride. In large doses (and they don‚Äôt get much larger than 10 grains, or 20 if a fellow takes two of them, as Dr. Rush recommended!) it functions as a savage purgative, causing lengthy and productive sessions in the outhouse and leaving a patient thoroughly depleted and hopefully in full restoration of his bile balance.&lt;/p&gt;
          &lt;p&gt;Calomel also was the only thing known to be effective against syphilis, which was always an issue with military outfits. Whether picked up from a friendly lady in a waterfront St. Louis ‚Äúsporting house‚Äù before the journey, or from an equally friendly Native lady met along the way, syphilis went with soldiers like ice cold milk with an Oreo cookie.&lt;/p&gt;
          &lt;p&gt;When symptoms broke out, the patient would be dosed with ‚Äúthunder clappers‚Äù and slathered with topical mercury ointments until he started salivating ferociously, which was a symptom of mild mercury poisoning but at the time was considered a sure sign that the body was purging the sickness out of itself.&lt;/p&gt;
          &lt;p&gt;And yes, a few of the men did end up needing treatment for syphilis. But everyone in the party needed a good laxative √¢on the regular√¢ (sorry about that). Week after week, hunting parties went out and brought back animals to eat. The explorers lived on almost nothing but meat.&lt;/p&gt;
          &lt;p&gt;And this low-fiber diet had predictable results.&lt;/p&gt;
          &lt;p&gt;It had another result, too, which was less predictable √¢ although highly convenient for later historians. The fact is, mercury chloride is only slightly soluble in human digestion. Plus, the reason it works is, it irritates the tissues of the digestive tract severely, causing the body to expel it just as fast as it possibly can before more damage can be done. So, most of the calomel in any given ‚Äúbilious pill‚Äù gets blown out post-haste in the ensuing ‚Äúpurge.‚Äù&lt;/p&gt;
          &lt;p&gt;Then, once out of the body and in the earth, it lasts literally for centuries without breaking down or dissolving away.&lt;/p&gt;
          &lt;p&gt;So as Lewis and Clark and their crew made their way across the continent, and across Oregon, they were unknowingly depositing a trail of heavy-metal laxatives along the way √¢ a trail that historians and scientists have been able to detect and use to document almost their every, uh, movement.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45087815</guid></item><item><title>Preserving Order in Concurrent Go Apps: Three Approaches Compared</title><link>https://destel.dev/blog/preserving-order-in-concurrent-go</link><description>&lt;doc fingerprint="763a97941ef85e95"&gt;
  &lt;main&gt;
    &lt;p&gt;Concurrency is one of Go‚Äôs greatest strengths, but it comes with a fundamental trade-off: when multiple goroutines process data simultaneously, the natural ordering gets scrambled. Most of the time, this is fine ‚Äì unordered processing is enough, it‚Äôs faster and simpler.&lt;/p&gt;
    &lt;p&gt;But sometimes, order matters.&lt;/p&gt;
    &lt;head rend="h2"&gt;When Order Matters&lt;/head&gt;
    &lt;p&gt;Here are three real-world scenarios where preserving order becomes critical:&lt;/p&gt;
    &lt;p&gt;Real-time Log Enrichment: You‚Äôre processing a high-volume log stream, enriching each entry with user metadata from a database or external API. Sequential processing can‚Äôt keep up with the incoming rate, but concurrent processing breaks the sequence, making the enriched logs unusable for downstream consumers that depend on chronological order.&lt;/p&gt;
    &lt;p&gt;Finding the First Match in a File List: You need to download a list of files from cloud storage and find the first one containing a specific string. Concurrent downloads are much faster, but they complete out of order ‚Äì the 50th file might finish before the 5th file, so you can‚Äôt simply return the first match you find without knowing if an earlier file also contains the string.&lt;/p&gt;
    &lt;p&gt;Time Series Data Processing: This scenario inspired my original implementation. I needed to download 90 days of transaction logs (~600MB each), extract some data, then compare consecutive days for trend analysis. Sequential downloads took hours; concurrent downloads could give an order of magnitude speedup, but would destroy the temporal relationships I needed for comparison.&lt;/p&gt;
    &lt;p&gt;The challenge is clear: we need the speed benefits of concurrent processing without sacrificing the predictability of ordered results. This isn‚Äôt just a theoretical problem ‚Äì it‚Äôs a practical constraint that affects real systems at scale.&lt;/p&gt;
    &lt;p&gt;In this article, we‚Äôll explore three approaches I‚Äôve developed and used in production Go applications. We‚Äôll build a concurrent &lt;code&gt;OrderedMap&lt;/code&gt; function that transforms a channel of inputs into a channel of outputs while preserving order. Through benchmarks of each approach, we‚Äôll understand their trade-offs and discover surprising performance insights along the way.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Problem: Why Concurrency Breaks Order&lt;/head&gt;
    &lt;p&gt;Let‚Äôs quickly recall why concurrency messes up ordering. One of the reasons is that goroutines process tasks at different speeds. Another common reason ‚Äì we can‚Äôt predict how exactly goroutines will be scheduled by the Go runtime.&lt;/p&gt;
    &lt;p&gt;For example, goroutine #2 might finish processing item #50 before goroutine #1 finishes item #10, causing results to arrive out of order. This is the natural behavior of concurrent processing.&lt;/p&gt;
    &lt;p&gt;If you want to see this in action, here‚Äôs a quick demo the Go playground.&lt;/p&gt;
    &lt;head rend="h2"&gt;Design Philosophy: Backpressure vs Buffering&lt;/head&gt;
    &lt;p&gt;The classic approach to ordered concurrency uses some sort of reorder buffer or queue. When a worker calculates a result but it‚Äôs too early to write it to the output, the result gets stored in that buffer until it can be written in the correct order.&lt;/p&gt;
    &lt;p&gt;In such designs buffers can typically grow without bound. This happens when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The input is skewed ‚Äì early items take longer to process than later items&lt;/item&gt;
      &lt;item&gt;Downstream consumers are slow&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The algorithms presented below are backpressure-first. If a worker can‚Äôt yet write its result to the output channel, it blocks. This design is memory-bound and preserves the behavior developers expect from Go channels.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Technically speaking, such algorithms also do buffering, but here out-of-order items are held on the stacks of running goroutines. So, to get a larger ‚Äúbuffer‚Äù in these algorithms, you can simply increase the concurrency level. This works well in practice since typically when applications need larger buffers they also need higher concurrency levels.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Establishing a Performance Baseline&lt;/head&gt;
    &lt;p&gt;To understand the true cost of ordering, we first need a baseline to measure against. Let‚Äôs implement and benchmark a basic concurrent &lt;code&gt;Map&lt;/code&gt; function that doesn‚Äôt preserve order ‚Äì this will show us exactly what overhead the ordering approaches add.&lt;/p&gt;
    &lt;p&gt;Our &lt;code&gt;Map&lt;/code&gt; function transforms an input channel into an output channel using a user-supplied function &lt;code&gt;f&lt;/code&gt;. It‚Äôs built on top of a simple worker pool, which spawns multiple goroutines to process input items concurrently.&lt;/p&gt;
    &lt;code&gt;// Map transforms items from the input channel using n goroutines, and the
// provided function f. Returns a new channel with transformed items.
func Map[A, B any](in &amp;lt;-chan A, n int, f func(A) B) &amp;lt;-chan B {
	out := make(chan B)
	Loop(in, n, out, func(a A) {
		out &amp;lt;- f(a)
	})
	return out
}

// Loop is a worker pool implementation. It calls function f for each 
// item from the input channel using n goroutines. This is a non-blocking function 
// that signals completion by closing the done channel when all work is finished.
func Loop[A, B any](in &amp;lt;-chan A, n int, done chan&amp;lt;- B, f func(A)) {
	var wg sync.WaitGroup

	for i := 0; i &amp;lt; n; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for a := range in {
				f(a)
			}
		}()
	}

	go func() {
		wg.Wait()
		if done != nil {
			close(done)
		}
	}()
}

// Discard is a non-blocking function that consumes and discards
// all items from the input channel
func Discard[A any](in &amp;lt;-chan A) {
	go func() {
		for range in {
			// Discard the value
		}
	}()
}

func BenchmarkMap(b *testing.B) {
	for _, n := range []int{1, 2, 4, 8, 12, 50} {
		b.Run(fmt.Sprint("n=", n), func(b *testing.B) {
			in := make(chan int)
			defer close(in)
			out := Map(in, n, func(a int) int {
				//time.Sleep(50 * time.Microsecond)
				return a // no-op: just return the original value
			})
			Discard(out)

			b.ReportAllocs()
			b.ResetTimer()

			for i := 0; i &amp;lt; b.N; i++ {
				in &amp;lt;- 10 // write something to the in chan
			}
		})
	}
}&lt;/code&gt;
    &lt;p&gt;As you can see, &lt;code&gt;Map&lt;/code&gt; uses &lt;code&gt;Loop&lt;/code&gt; to create a worker pool that processes items concurrently, while &lt;code&gt;Loop&lt;/code&gt; itself handles the low-level goroutine management and synchronization. This separation of concerns will become important later when we build our ordered variants.&lt;/p&gt;
    &lt;p&gt;What exactly are we measuring here? We‚Äôre measuring throughput ‚Äì how fast we can push items through the entire pipeline. Since the &lt;code&gt;Map&lt;/code&gt; function creates backpressure (blocking when the pipeline is full), the rate at which we can feed items into the input channel acts as an accurate proxy for overall processing speed.
Let‚Äôs run the benchmark (I used Apple M2 Max laptop to run it):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Goroutines&lt;/cell&gt;
        &lt;cell role="head"&gt;Time /op&lt;/cell&gt;
        &lt;cell role="head"&gt;Allocs/op&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;408.6ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;445.1ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;546.4ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;600.2ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;1053ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;You might wonder: ‚ÄúShouldn‚Äôt higher concurrency increase throughput?‚Äù In real applications, absolutely ‚Äì but only when there‚Äôs actual work to parallelize. Here I used a trivial no-op transformation to isolate and benchmark the pure overhead of goroutines, channels, and coordination. As expected, this overhead grows with the number of goroutines.&lt;/p&gt;
    &lt;p&gt;We‚Äôll use this overhead-focused benchmark for comparisons later in the article, but to demonstrate that concurrency improves performance, let‚Äôs run one more benchmark with some work simulated (50Œºs sleep):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Goroutines&lt;/cell&gt;
        &lt;cell role="head"&gt;Time /op&lt;/cell&gt;
        &lt;cell role="head"&gt;Speedup&lt;/cell&gt;
        &lt;cell role="head"&gt;Allocs/op&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;61656ns&lt;/cell&gt;
        &lt;cell&gt;1.0x&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;30429ns&lt;/cell&gt;
        &lt;cell&gt;2.0x&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;15207ns&lt;/cell&gt;
        &lt;cell&gt;4.1x&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;7524ns&lt;/cell&gt;
        &lt;cell&gt;8.2x&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;5034ns&lt;/cell&gt;
        &lt;cell&gt;12.2x&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;1277ns&lt;/cell&gt;
        &lt;cell&gt;48.3x&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Perfect! Here we see the dramatic benefits of concurrency when there‚Äôs real work to be done. With 50Œºs of work per item, increasing concurrency from 1 to 50 goroutines improves performance by nearly 50x. This demonstrates why concurrent processing is so valuable in real applications.&lt;/p&gt;
    &lt;p&gt;We‚Äôre now ready to compare the 3 approaches and measure exactly what price we pay for adding order preservation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Approach 1: ReplyTo Channels&lt;/head&gt;
    &lt;p&gt;This is probably the most Go-native way to implement ordered concurrency. The ReplyTo pattern is well-known in Go (I also used it in my batching article), but somehow this was the hardest approach for me to explain clearly.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs how it works:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A packer goroutine creates jobs by attaching a unique &lt;code&gt;replyTo&lt;/code&gt;channel to every input item.&lt;/item&gt;
      &lt;item&gt;Workers process jobs concurrently, and send results through those &lt;code&gt;replyTo&lt;/code&gt;channels.&lt;/item&gt;
      &lt;item&gt;An unpacker goroutine unpacks the values sent via &lt;code&gt;replyTo&lt;/code&gt;channels and writes them to the output.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The following diagram illustrates how this pattern in more detail:&lt;/p&gt;
    &lt;p&gt;The left part of this diagram is sequential (packer and unpacker) while the worker pool on the right operates concurrently. Notice that workers can only send results when the unpacker is ready to receive them, because the &lt;code&gt;replyTo&lt;/code&gt; channels are unbuffered. This creates natural backpressure and prevents unnecessary buffering.&lt;/p&gt;
    &lt;code&gt;func OrderedMap1[A, B any](in &amp;lt;-chan A, n int, f func(A) B) &amp;lt;-chan B {
	type Job struct {
		Item    A
		ReplyTo chan B
	}

	// Packer goroutine.
	// `jobs` chan will be processed by the pool
	// `replies` chan will be consumed by unpacker goroutine
	jobs := make(chan Job)
	replies := make(chan chan B, n)
	go func() {
		for item := range in {
			replyTo := make(chan B)
			jobs &amp;lt;- Job{Item: item, ReplyTo: replyTo}
			replies &amp;lt;- replyTo
		}
		close(jobs)
		close(replies)
	}()

	// Worker pool of n goroutines.
	// Sends results back via replyTo channels
	Loop[Job, any](jobs, n, nil, func(job Job) {
		job.ReplyTo &amp;lt;- f(job.Item) // Calculate the result and send it back
		close(job.ReplyTo)
	})

	// Unpacker goroutine.
	// Unpacks replyTo channels in order and sends results to the `out` channel
	out := make(chan B)
	go func() {
		defer close(out)
		for replyTo := range replies {
			result := &amp;lt;-replyTo
			out &amp;lt;- result
		}
	}()
	return out
}&lt;/code&gt;
    &lt;p&gt;Performance Results:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Goroutines&lt;/cell&gt;
        &lt;cell role="head"&gt;Time /op&lt;/cell&gt;
        &lt;cell role="head"&gt;vs Baseline&lt;/cell&gt;
        &lt;cell role="head"&gt;Allocs/op&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;818.7ns&lt;/cell&gt;
        &lt;cell&gt;+410ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;808.9ns&lt;/cell&gt;
        &lt;cell&gt;+364ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;826.8ns&lt;/cell&gt;
        &lt;cell&gt;+280ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;825.6ns&lt;/cell&gt;
        &lt;cell&gt;+225ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;772.3ns&lt;/cell&gt;
        &lt;cell&gt;-281ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This approach introduces up to 410ns of overhead per input item compared to our baseline. Part of this cost comes from allocating a new &lt;code&gt;replyTo&lt;/code&gt; channel for every item. Unfortunately, we can‚Äôt use a package level &lt;code&gt;sync.Pool&lt;/code&gt; to mitigate this because our function is generic ‚Äì channels for different types can‚Äôt share the same pool.&lt;/p&gt;
    &lt;p&gt;What‚Äôs also interesting about this result is that the overhead brought by ordering becomes smaller as the number of goroutines grows. At some point even an inversion happens ‚Äì &lt;code&gt;OrderedMap1&lt;/code&gt; becomes faster than &lt;code&gt;Map&lt;/code&gt; (-281ns at 50 goroutines).&lt;/p&gt;
    &lt;p&gt;I haven‚Äôt investigated this phenomenon deeply. I believe it can‚Äôt be caused by inefficiencies inside &lt;code&gt;Map&lt;/code&gt; since it‚Äôs already based on the simplest possible channel-based worker pool. One guess that I have is that in &lt;code&gt;Map&lt;/code&gt; we have 50 goroutines competing to write into a single output channel. On the contrary, in &lt;code&gt;OrderedMap&lt;/code&gt;, despite additional moving parts, only one goroutine is writing to the output.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs now move on to the next approach.&lt;/p&gt;
    &lt;head rend="h2"&gt;Approach 2: sync.Cond for Turn-Taking&lt;/head&gt;
    &lt;p&gt;This was the first algorithm I implemented when I needed ordered concurrency, and it‚Äôs much easier to explain than the ReplyTo approach.&lt;/p&gt;
    &lt;p&gt;Here we attach an incremental index to each item and send it to the worker pool. Each worker performs the calculation, then waits its turn to write the result to the output channel.&lt;/p&gt;
    &lt;p&gt;This conditional waiting is implemented using a shared &lt;code&gt;currentIndex&lt;/code&gt; variable protected by &lt;code&gt;sync.Cond&lt;/code&gt;, a powerful but underused concurrency primitive from the standard library that allows goroutines to wait for specific conditions and be woken up when those conditions change.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs how the turn-taking mechanism works:&lt;/p&gt;
    &lt;p&gt;Here, after each write, all workers wake up (using broadcast) and recheck ‚Äúis it my turn?‚Äù condition&lt;/p&gt;
    &lt;code&gt;func OrderedMap2[A, B any](in &amp;lt;-chan A, n int, f func(A) B) &amp;lt;-chan B {
	type Job struct {
		Item  A
		Index int
	}

	// Indexer goroutine.
	// Assign an index to each item from the input channel
	jobs := make(chan Job)
	go func() {
		i := 0
		for item := range in {
			jobs &amp;lt;- Job{Item: item, Index: i}
			i++
		}
		close(jobs)
	}()

	// Shared state.
	// Index of the next result that must be written to the output channel.
	nextIndex := 0
	cond := sync.NewCond(new(sync.Mutex))

	// Worker pool of n goroutines.
	out := make(chan B)
	Loop(jobs, n, out, func(job Job) {
		result := f(job.Item) // Calculate the result

		// Cond must be used with a locked mutex (see stdlib docs)
		cond.L.Lock()

		// wait until it's our turn to write the result
		for job.Index != nextIndex {
			cond.Wait()
		}

		// Write the result
		out &amp;lt;- result

		// Increment the index and notify all other workers
		nextIndex++
		cond.Broadcast()

		cond.L.Unlock()
	})

	return out
}&lt;/code&gt;
    &lt;p&gt;Performance Results:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Goroutines&lt;/cell&gt;
        &lt;cell role="head"&gt;Time /op&lt;/cell&gt;
        &lt;cell role="head"&gt;vs Baseline&lt;/cell&gt;
        &lt;cell role="head"&gt;Allocs/op&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;867.7ns&lt;/cell&gt;
        &lt;cell&gt;+459ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;1094ns&lt;/cell&gt;
        &lt;cell&gt;+649ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;1801ns&lt;/cell&gt;
        &lt;cell&gt;+1255ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;2987ns&lt;/cell&gt;
        &lt;cell&gt;+2387ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;16074ns&lt;/cell&gt;
        &lt;cell&gt;+15021ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The results are telling ‚Äì no more per-item allocations, which is excellent for memory efficiency. But there‚Äôs a critical flaw: significant performance degradation as goroutine count increases. This happens because of the shared state and the ‚Äúthundering herd‚Äù problem: after each write, all goroutines wake up via &lt;code&gt;cond.Broadcast()&lt;/code&gt;, but only one will do useful work.&lt;/p&gt;
    &lt;p&gt;This inefficiency led me to think: ‚ÄúHow can I wake only the goroutine that should write next?‚Äù And this is how the 3rd approach was born.&lt;/p&gt;
    &lt;head rend="h2"&gt;Approach 3: Permission Passing Chain&lt;/head&gt;
    &lt;p&gt;Here‚Äôs the key insight: when is it safe to write output #5? After output #4 was written. Who knows when output #4 was written? The goroutine that wrote it.&lt;/p&gt;
    &lt;p&gt;In this algorithm, any job must hold the write permission before its worker can send results to the output channel. We chain jobs together so each one knows exactly which job comes next and can pass the permission to it. This is done by attaching two channels to each job: &lt;code&gt;canWrite&lt;/code&gt; channel to receive the permission, and &lt;code&gt;nextCanWrite&lt;/code&gt; channel to pass the permission to the next job.&lt;/p&gt;
    &lt;p&gt;This chain structure makes the worker logic remarkably simple:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Calculate: Process the job using the provided function&lt;/item&gt;
      &lt;item&gt;Wait: Receive the permission from &lt;code&gt;canWrite&lt;/code&gt;channel&lt;/item&gt;
      &lt;item&gt;Write: Send the result to the output channel&lt;/item&gt;
      &lt;item&gt;Pass: Send the permission to the next job via &lt;code&gt;nextCanWrite&lt;/code&gt;channel&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here‚Äôs the diagram that illustrates the whole flow:&lt;/p&gt;
    &lt;p&gt;The green arrows show how the permission to write is passed from one job to another along the chain. Essentially this is a token-passing algorithm that eliminates the ‚Äúthundering herd‚Äù problem entirely ‚Äì each goroutine wakes exactly one other goroutine, creating efficient point-to-point signaling rather than expensive broadcasts.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs see how this translates to code. The implementation has two parts: a ‚Äúlinker‚Äù goroutine that builds the chain, and workers that follow the calculate-wait-write-pass pattern:&lt;/p&gt;
    &lt;code&gt;func OrderedMap3[A, B any](in &amp;lt;-chan A, n int, f func(A) B) &amp;lt;-chan B {
	type Job[A any] struct {
		Item         A
		CanWrite     chan struct{}
		NextCanWrite chan struct{} // canWrite channel of the next job
	}

	// Linker goroutine:
	// Builds a chain of jobs where each has a CanWrite channel attached.
	// Additionally, each job knows about the CanWrite channel of the next job in the chain.
	jobs := make(chan Job[A])
	go func() {
		defer close(jobs)

		var canWrite, nextCanWrite chan struct{}
		nextCanWrite = make(chan struct{}, 1)
		close(nextCanWrite) // the first job can write immediately

		for item := range in {
			canWrite, nextCanWrite = nextCanWrite, make(chan struct{}, 1)
			jobs &amp;lt;- Job[A]{item, canWrite, nextCanWrite}
		}
	}()

	// Worker pool of n goroutines.
	// Jobs pass the write permission along the chain.
	out := make(chan B)
	Loop(jobs, n, out, func(job Job[A]) {
		result := f(job.Item) // Calculate the result

		&amp;lt;-job.CanWrite          // Wait for the write permission
		out &amp;lt;- result           // Write to the output channel
		close(job.NextCanWrite) // Pass the permission to the next job
	})

	return out
}&lt;/code&gt;
    &lt;p&gt;Performance Results:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Goroutines&lt;/cell&gt;
        &lt;cell role="head"&gt;Time /op&lt;/cell&gt;
        &lt;cell role="head"&gt;vs Baseline&lt;/cell&gt;
        &lt;cell role="head"&gt;Allocs/op&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;927.2ns&lt;/cell&gt;
        &lt;cell&gt;+519ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;939.8ns&lt;/cell&gt;
        &lt;cell&gt;+495ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;860.7ns&lt;/cell&gt;
        &lt;cell&gt;+314ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;823.8ns&lt;/cell&gt;
        &lt;cell&gt;+224ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;609.8ns&lt;/cell&gt;
        &lt;cell&gt;-443ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Here the result is very similar to what we‚Äôve seen in the ReplyTo approach. Almost the same overhead, the same inversion at higher levels of concurrency, and the same extra allocation per item. But there‚Äôs one difference‚Ä¶&lt;/p&gt;
    &lt;p&gt;Unlike approach 1, here we‚Äôre allocating a non-generic &lt;code&gt;chan struct{}&lt;/code&gt;. This means we can use a package level &lt;code&gt;sync.Pool&lt;/code&gt; to eliminate those allocations ‚Äì let‚Äôs explore that next.&lt;/p&gt;
    &lt;head rend="h2"&gt;Approach 3a: Zero-Allocation Permission Passing Chain&lt;/head&gt;
    &lt;p&gt;Let‚Äôs create a pool for &lt;code&gt;canWrite&lt;/code&gt; channels. Implementation is straightforward ‚Äì the pool itself and make/release functions.&lt;/p&gt;
    &lt;code&gt;// Package-level pool for canWrite channels
type chainedItem[A any] struct {
	Value        A
	CanWrite     chan struct{}
	NextCanWrite chan struct{} // canWrite channel for the next item
}

var canWritePool sync.Pool

func makeCanWriteChan() chan struct{} {
	ch := canWritePool.Get()
	if ch == nil {
		return make(chan struct{}, 1)
	}
	return ch.(chan struct{})
}

func releaseCanWriteChan(ch chan struct{}) {
	canWritePool.Put(ch)
}&lt;/code&gt;
    &lt;p&gt;Now let‚Äôs use the pool in the permission passing algorithm. Since channels are reused, we can no longer signal by closing them. Instead workers must read and write empty structs form/to these channels.&lt;/p&gt;
    &lt;code&gt;func OrderedMap3a[A, B any](in &amp;lt;-chan A, n int, f func(A) B) &amp;lt;-chan B {
	type Job[A any] struct {
		Item         A
		CanWrite     chan struct{}
		NextCanWrite chan struct{} // canWrite channel of the next job
	}

	// Linker goroutine:
	// Builds a chain of jobs where each has a CanWrite channel attached.
	// Additionally, each job knows about the CanWrite channel of the next job in the chain.
	jobs := make(chan Job[A])
	go func() {
		defer close(jobs)

		var canWrite, nextCanWrite chan struct{}
		nextCanWrite = makeCanWriteChan()
		nextCanWrite &amp;lt;- struct{}{} // the first job can write immediately

		for item := range in {
			canWrite, nextCanWrite = nextCanWrite, makeCanWriteChan()
			jobs &amp;lt;- Job[A]{item, canWrite, nextCanWrite}
		}
	}()

	// Worker pool of n goroutines.
	// Jobs pass the write permission along the chain.
	out := make(chan B)
	Loop(jobs, n, out, func(job Job[A]) {
		result := f(job.Item) // Calculate the result

		&amp;lt;-job.CanWrite                    // Wait for the write permission
		out &amp;lt;- result                     // Write to the output channel
		releaseCanWriteChan(job.CanWrite) // Release our canWrite channel to the pool
		job.NextCanWrite &amp;lt;- struct{}{}    // Pass the permission to the next job
	})

	return out
}&lt;/code&gt;
    &lt;p&gt;Performance Results with Pooling:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Goroutines&lt;/cell&gt;
        &lt;cell role="head"&gt;Time /op&lt;/cell&gt;
        &lt;cell role="head"&gt;vs Baseline&lt;/cell&gt;
        &lt;cell role="head"&gt;Allocs/op&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;891.0ns&lt;/cell&gt;
        &lt;cell&gt;+482ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;916.5ns&lt;/cell&gt;
        &lt;cell&gt;+471ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;879.5ns&lt;/cell&gt;
        &lt;cell&gt;+333ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;872.6ns&lt;/cell&gt;
        &lt;cell&gt;+272ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;657.6ns&lt;/cell&gt;
        &lt;cell&gt;-395ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Perfect! Zero allocations and good performance, meaning less GC pressure for long running jobs. But this approach has one more trick up its sleeve‚Ä¶&lt;/p&gt;
    &lt;head rend="h2"&gt;One more thing: Building Reusable Abstractions&lt;/head&gt;
    &lt;p&gt;The permission passing approach has another significant advantage over the ReplyTo method: it controls when to write rather than where to write.&lt;/p&gt;
    &lt;p&gt;I‚Äôll admit it ‚Äì sometimes I get a bit obsessed with building clean abstractions. When working on rill, I really wanted to extract this ordering logic into something reusable and testable. This ‚Äúwhen vs where‚Äù distinction was an AHA moment for me.&lt;/p&gt;
    &lt;p&gt;Since the algorithm doesn‚Äôt care where the outputs are written, it‚Äôs easy to abstract it into a separate function ‚Äì &lt;code&gt;OrderedLoop&lt;/code&gt;. The API is very similar to the &lt;code&gt;Loop&lt;/code&gt; function we used before, but here the user function receives two arguments ‚Äì an &lt;code&gt;item&lt;/code&gt; and a &lt;code&gt;canWrite&lt;/code&gt; channel. It‚Äôs important that the user function must read from the &lt;code&gt;canWrite&lt;/code&gt; channel exactly once to avoid deadlocks or undefined behavior.&lt;/p&gt;
    &lt;code&gt;func OrderedLoop[A, B any](in &amp;lt;-chan A, done chan&amp;lt;- B, n int, f func(a A, canWrite &amp;lt;-chan struct{})) {
	type Job[A any] struct {
		Item         A
		CanWrite     chan struct{}
		NextCanWrite chan struct{} // canWrite channel of the next job
	}

	// Linker goroutine:
	// Builds a chain of jobs where each has a CanWrite channel attached.
	// Additionally, each job knows about the CanWrite channel of the next job in the chain.
	jobs := make(chan Job[A])
	go func() {
		defer close(jobs)

		var canWrite, nextCanWrite chan struct{}
		nextCanWrite = makeCanWriteChan()
		nextCanWrite &amp;lt;- struct{}{} // the first job can write immediately

		for item := range in {
			canWrite, nextCanWrite = nextCanWrite, makeCanWriteChan()
			jobs &amp;lt;- Job[A]{item, canWrite, nextCanWrite}
		}
	}()

	// Worker pool of n goroutines.
	// Jobs pass the write permission along the chain.
	Loop(jobs, n, done, func(job Job[A]) {
		f(job.Item, job.CanWrite) // Do the work

		releaseCanWriteChan(job.CanWrite) // Release item's canWrite channel to the pool
		job.NextCanWrite &amp;lt;- struct{}{}    // Pass the permission to the next job
	})
}&lt;/code&gt;
    &lt;p&gt;The typical usage looks like:&lt;/p&gt;
    &lt;code&gt;OrderedLoop(in, out, n, func(a A, canWrite &amp;lt;-chan struct{}) {
	// [Do processing here]
	
	// Everything above this line is executed concurrently,
	// everything below it is executed sequentially and in order
	&amp;lt;-canWrite
	
	// [Write results somewhere]
})
&lt;/code&gt;
    &lt;p&gt;With this abstraction in hand it‚Äôs remarkably simple to build any ordered operations. For example &lt;code&gt;OrderedMap&lt;/code&gt; becomes just 7 lines of code:&lt;/p&gt;
    &lt;code&gt;func OrderedMap3b[A, B any](in &amp;lt;-chan A, n int, f func(A) B) &amp;lt;-chan B {
	out := make(chan B)
	OrderedLoop(in, out, n, func(a A, canWrite &amp;lt;-chan struct{}) {
		result := f(a)
		&amp;lt;-canWrite
		out &amp;lt;- result
	})
	return out
}&lt;/code&gt;
    &lt;p&gt;We can also easily build an &lt;code&gt;OrderedFilter&lt;/code&gt; that conditionally writes outputs:&lt;/p&gt;
    &lt;code&gt;func OrderedFilter[A any](in &amp;lt;-chan A, n int, predicate func(A) bool) &amp;lt;-chan A {
	out := make(chan A)
	OrderedLoop(in, out, n, func(a A, canWrite &amp;lt;-chan struct{}) {
		keep := predicate(a)
		&amp;lt;-canWrite
		if keep {
			out &amp;lt;- a
		}
	})
	return out
}&lt;/code&gt;
    &lt;p&gt;Or even an &lt;code&gt;OrderedSplit&lt;/code&gt; that distributes items to two channels based on a predicate:&lt;/p&gt;
    &lt;code&gt;func OrderedSplit[A any](in &amp;lt;-chan A, n int, predicate func(A) bool) (&amp;lt;-chan A, &amp;lt;-chan A) {
	outTrue := make(chan A)
	outFalse := make(chan A)
	done := make(chan struct{})
	
	OrderedLoop(in, done, n, func(a A, canWrite &amp;lt;-chan struct{}) {
		shouldGoToTrue := predicate(a)
		&amp;lt;-canWrite
		if shouldGoToTrue {
			outTrue &amp;lt;- a
		} else {
			outFalse &amp;lt;- a
		}
	})
	
	go func() {
		&amp;lt;-done
		close(outTrue)
		close(outFalse)
	}()
	
	return outTrue, outFalse
}&lt;/code&gt;
    &lt;p&gt;Simply put, this abstraction makes building ordered operations trivial.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance Comparison&lt;/head&gt;
    &lt;p&gt;Here‚Äôs how all approaches perform across different concurrency levels:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Concurrency&lt;/cell&gt;
        &lt;cell role="head"&gt;Baseline&lt;/cell&gt;
        &lt;cell role="head"&gt;Approach 1&lt;p&gt;(ReplyTo)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Approach 2&lt;p&gt;(sync.Cond)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Approach 3&lt;p&gt;(Permission)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Approach 3a&lt;p&gt;(+ Pool)&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;408.6ns&lt;/cell&gt;
        &lt;cell&gt;818.7ns&lt;/cell&gt;
        &lt;cell&gt;867.7ns&lt;/cell&gt;
        &lt;cell&gt;927.2ns&lt;/cell&gt;
        &lt;cell&gt;891.0ns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;445.1ns&lt;/cell&gt;
        &lt;cell&gt;808.9ns&lt;/cell&gt;
        &lt;cell&gt;1094ns&lt;/cell&gt;
        &lt;cell&gt;939.8ns&lt;/cell&gt;
        &lt;cell&gt;916.5ns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;546.4ns&lt;/cell&gt;
        &lt;cell&gt;826.8ns&lt;/cell&gt;
        &lt;cell&gt;1801ns&lt;/cell&gt;
        &lt;cell&gt;860.7ns&lt;/cell&gt;
        &lt;cell&gt;879.5ns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;600.2ns&lt;/cell&gt;
        &lt;cell&gt;825.6ns&lt;/cell&gt;
        &lt;cell&gt;2987ns&lt;/cell&gt;
        &lt;cell&gt;823.8ns&lt;/cell&gt;
        &lt;cell&gt;872.6ns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;1053ns&lt;/cell&gt;
        &lt;cell&gt;772.3ns&lt;/cell&gt;
        &lt;cell&gt;16074ns&lt;/cell&gt;
        &lt;cell&gt;609.8ns&lt;/cell&gt;
        &lt;cell&gt;657.6ns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Zero allocs&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Key Takeaways&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;sync.Cond is a no-go for ordered concurrency ‚Äì While it starts with decent performance at low concurrency, it completely falls apart as goroutine count increases, due to the thundering herd problem.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;ReplyTo is a strong contender ‚Äì it adds at most ~500ns of overhead compared to the baseline, but requires one additional allocation per input item, increasing GC pressure.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Permission Passing emerges as the clear winner ‚Äì It has it all:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Good performance: at most ~500ns of overhead compared to the baseline&lt;/item&gt;
          &lt;item&gt;Zero allocations: Less GC pressure for long running tasks&lt;/item&gt;
          &lt;item&gt;Clean abstraction: Core synchronization logic can be abstracted away and used to build various concurrent operations.&lt;/item&gt;
          &lt;item&gt;Maintainability: Separation of concerns and the intuitive ‚Äúcalculate ‚Üí wait ‚Üí write ‚Üí pass‚Äù pattern make code easy to support and reason about&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This exploration shows that ordered concurrency doesn‚Äôt have to be expensive. With the right approach, you can have concurrency, ordering and backpressure at the same time. The permission passing pattern, in particular, demonstrates how Go‚Äôs channels can be used creatively to solve complex coordination problems.&lt;/p&gt;
    &lt;p&gt;Finally, these patterns have been battle-tested in production through rill concurrency toolkit (1.7k üåü on GitHub). It implements &lt;code&gt;Map&lt;/code&gt;, &lt;code&gt;OrderedMap&lt;/code&gt;, and many other concurrent operations. Rill focuses on composability ‚Äì operations chain together into larger pipelines ‚Äì while adding comprehensive error handling, context-friendly design, and maintaining over 95% test coverage.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45089938</guid></item><item><title>Telli (YC F24) is hiring engineers, designers, and interns (on-site in Berlin)</title><link>https://hi.telli.com/join-us</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45090216</guid></item><item><title>UK's largest battery storage facility at Tilbury substation</title><link>https://www.nationalgrid.com/national-grid-connects-uks-largest-battery-storage-facility-tilbury-substation</link><description>&lt;doc fingerprint="95c5807b1a9ef1c2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;National Grid connects UK‚Äôs largest battery storage facility at Tilbury substation&lt;/head&gt;
    &lt;p&gt;National Grid has connected the UK‚Äôs largest battery energy storage system (BESS) to its transmission network at Tilbury substation in Essex.&lt;/p&gt;
    &lt;p&gt;The 300MW Thurrock Storage project, developed by Statera Energy, is now energised and delivering electricity flexibly to the network across London and the south east.&lt;/p&gt;
    &lt;p&gt;With a total capacity of 600MWh, Thurrock Storage is capable of powering up to 680,000 homes, and can help to balance supply and demand by soaking up surplus clean electricity and discharging it instantaneously when the grid needs it.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Our Tilbury substation once served a coal plant, and with battery connections like this, it‚Äôs today helping to power a more sustainable future for the region and the country.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;National Grid reinforced its Tilbury substation to ensure the network in the region could safely carry the battery‚Äôs significant additional load, with new protection and control systems installed to ensure a robust connection.&lt;/p&gt;
    &lt;p&gt;The substation previously served the coal-fired Tilbury A and B power stations on adjacent land prior to their demolition, so the connection of the Thurrock Storage facility marks a symbolic transition from coal to clean electricity at the site.&lt;/p&gt;
    &lt;p&gt;John Twomey, director of customer and network development at National Grid Electricity Transmission, said:&lt;/p&gt;
    &lt;p&gt;‚ÄúBattery storage plays a vital role in Britain‚Äôs clean energy transition. Connecting Thurrock Storage, the UK‚Äôs biggest battery, to our transmission network marks a significant step on that journey.&lt;/p&gt;
    &lt;p&gt;‚ÄúOur Tilbury substation once served a coal plant, and with battery connections like this, it‚Äôs today helping to power a more sustainable future for the region and the country.‚Äù&lt;/p&gt;
    &lt;p&gt;Tom Vernon, Statera Energy CEO and founder, said:&lt;/p&gt;
    &lt;p&gt;‚ÄúWe are delighted that Thurrock Storage is now energised, following its successful connection to the grid by National Grid Electricity Transmission. Increasing BESS capacity is essential for supporting the grid when renewable generation, such as solar and wind, is low or changes quickly. It ensures that energy can be stored efficiently and returned to the grid whenever it‚Äôs needed.‚Äù&lt;/p&gt;
    &lt;p&gt;National Grid is continuing work at Tilbury substation to connect the 450MW Thurrock Flexible Generation facility, another Statera project that is set to support the energy needs of the region.&lt;/p&gt;
    &lt;p&gt;The connection of the UK‚Äôs biggest battery follows energisation in July of the 373MW Cleve Hill Solar Park in Kent ‚Äì the largest solar plant in the country ‚Äì which National Grid connected to its adjacent Cleve Hill substation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45091119</guid></item><item><title>De-Googling TOTP Authenticator Codes</title><link>https://imrannazar.com/articles/degoogle-otp</link><description>&lt;doc fingerprint="8ce975dd25b7e8d3"&gt;
  &lt;main&gt;
    &lt;p&gt;In the ongoing effort to extricate myself from Google's services, I've been paring down my usage of their apps on my (admittedly Android) phone. I'm now down to two Google apps I use regularly: Maps (for traffic data) and Authenticator (for TOTP[A]Time-based One Time Password codes).&lt;/p&gt;
    &lt;p&gt;Now, I spend most of my time in a terminal window on MacOS or connected to a Linux machine; it'd be nice if I could get TOTPs on the command-line, and it turns out there's a utility called &lt;code&gt;oathtool&lt;/code&gt; that allows for TOTP generation on the CLI. However, that would mean switching my OTP provider, which usually involves:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Logging into each service that has an OTP registered in the app;&lt;/item&gt;
      &lt;item&gt;Disabling two-factor authentication (2FA);&lt;/item&gt;
      &lt;item&gt;Re-enabling 2FA and using the "manual entry" code as input to &lt;code&gt;oathtool&lt;/code&gt;;&lt;/item&gt;
      &lt;item&gt;Doing it all again for the next website or service.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Fortunately, Google's Authenticator provides a way to migrate codes between instances of the app based on scanning QR codes, and we can use this to migrate them away from Google into a TOTP handler of our choosing. It's another four-step process:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Generating a QR code in Google Authenticator for the codes you want to export;&lt;/item&gt;
      &lt;item&gt;Decoding the QR somewhere off-device, into a URL;&lt;/item&gt;
      &lt;item&gt;Decoding the URL into its constituent services and secret values;&lt;/item&gt;
      &lt;item&gt;Setting up &lt;code&gt;oathtool&lt;/code&gt;to use the secrets.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note that the below steps are presented just as I went through them, you may be able to find efficiencies or you may run into troubles that I didn't (especially if you're trying this exclusively on Windows); "your mileage may vary" is apt here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Going from Authenticator to a migration URL&lt;/head&gt;
    &lt;p&gt;The first step is getting the code out of Authenticator, through the Transfer Codes menu option in the app. Picking the services you'd like to extract leads you to a code like this:&lt;/p&gt;
    &lt;p&gt;You may have an app on your phone that decodes QRs, but I don't; instead, I transferred the file to my MacOS machine over Tailscale, and used a command-line tool called &lt;code&gt;qrtool&lt;/code&gt; to get the QR content:&lt;/p&gt;
    &lt;head rend="h3"&gt;Decoding the migration QR&lt;/head&gt;
    &lt;quote&gt;$ brew install qrtool $ qrtool decode Screenshot_20250901_062719_Authenticator.jpg otpauth-migration://offline?data=CjwKC2kqSJnNaAyKkw6jEhJUaGUgUmlja3JvbGwgU3RvcmUgASgBMAJCEzg4Yzg5ZTE3NTY3MDQzOTE0MzkQAhgBIAA%3D&lt;/quote&gt;
    &lt;head rend="h2"&gt;Decoding the URL into secrets&lt;/head&gt;
    &lt;p&gt;So we have our migration URL, with a Base64-encoded data block. Unfortunately, if we were to simply decode the data, we'd end up with some binary gibberish:&lt;/p&gt;
    &lt;head rend="h3"&gt;Trying to decode the URL directly&lt;/head&gt;
    &lt;quote&gt;$ php -r 'var_dump(base64_decode("CjwKC2kqSJnNaAyKkw6jEhJUaGUgUmlja3JvbGwgU3RvcmUgASgBMAJCEzg4Yzg5ZTE3NTY3MDQzOTE0MzkQAhgBIAA%3D"));' string(69) " &amp;lt; i*H??h??√Ω&lt;/quote&gt;
    &lt;p&gt;It turns out that this is a Protobuf-encoded data string, and we need to use Google's Protobuf library to get the data out. It turns out Tim Brooks has already done this with a short piece of Python at: https://github.com/brookst/otpauth_migrate&lt;/p&gt;
    &lt;p&gt;I decided to install this on a Linux machine I tend to be connected to (entirely unrelated to my Python installation being broken on Mac...):&lt;/p&gt;
    &lt;head rend="h3"&gt;Extracting the data via &lt;code&gt;otpauth_migrate&lt;/code&gt;&lt;/head&gt;
    &lt;quote&gt;$ git clone https://github.com/brookst/otpauth_migrate $ cd otpauth_migrate $ ./otpauth_migrate.py otpauth-migration://offline?data=CjwKC2kqSJnNaAyKkw6jEhJUaGUgUmlja3JvbGwgU3RvcmUgASgBMAJCEzg4Yzg5ZTE3NTY3MDQzOTE0MzkQAhgBIAA%3D secret: "i*H\231\315h\014\212\223\016\243" name: "The Rickroll Store" algorithm: ALGORITHM_SHA1 digits: DIGIT_COUNT_SIX type: OTP_TYPE_TOTP Secret code = NEVERGONNAGIVEYOUM======&lt;/quote&gt;
    &lt;p&gt;This tool is intelligent enough to extract any number of names and secrets from a migration URL, so you can export all your codes from Authenticator into one giant QR without needing to do each separately.&lt;/p&gt;
    &lt;head rend="h2"&gt;Using oathtool to generate OTPs&lt;/head&gt;
    &lt;p&gt;The final step is to use this secret code with &lt;code&gt;oathtool&lt;/code&gt;, which takes the secret directly as a parameter. If you instead want to refer to the service by name, Michael Bushey[1]"CLI 2-Factor Authentication", Michael Bushey, 2023 has a quick wrapper script which extracts the secrets from a locally-stored file:&lt;/p&gt;
    &lt;head rend="h3"&gt;Wrapper script to generate OTPs: &lt;code&gt;/usr/local/bin/otp&lt;/code&gt;&lt;/head&gt;
    &lt;quote&gt;&lt;del&gt;#!/bin/bash&lt;/del&gt;OTPKEY="$(sed -n "s/${1}=//p" ~/.otpkeys)" if [ -z "$OTPKEY" ]; then echo "$(basename $0): Bad Service Name '$1'" exit fi date oathtool --totp -b "$OTPKEY"&lt;/quote&gt;
    &lt;head rend="h3"&gt;OTP key store: &lt;code&gt;~/.otpkeys&lt;/code&gt;&lt;/head&gt;
    &lt;quote&gt;rickroll=NEVERGONNAGIVEYOUM======&lt;/quote&gt;
    &lt;p&gt;With this in place, you won't need to use your Authenticator app again. The tool outputs the current date and time, so you can double-check that your code won't expire (at :00 seconds) before you get a chance to type it in:&lt;/p&gt;
    &lt;quote&gt;$ otp rickroll Mon Sep 1 07:10:42 AM UTC 2025 200213&lt;/quote&gt;
    &lt;head rend="h2"&gt;Future expansion&lt;/head&gt;
    &lt;p&gt;There's a security issue here, of course, which is the exposed secret key sitting in a file on-disk. I'm happy to sit with that and not require a password to generate OTPs every time, but if you're interested in adapting the wrapper script to use symmetric encryption to secure the keys, Vivek Gite[2]"Use oathtool Linux command line for 2 step verification (2FA)", Vivek Gite, updated Feb 2025 has a set of scripts which employ &lt;code&gt;gpg&lt;/code&gt; for the job.&lt;/p&gt;
    &lt;p&gt;Now I just need to find a way to get traffic data into a maps App that doesn't involve Google's servers... Thoughts welcome.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45091202</guid></item><item><title>CocoaPods Is Deprecated</title><link>https://blog.cocoapods.org/CocoaPods-Specs-Repo/</link><description>&lt;doc fingerprint="42f18d113d47e29d"&gt;
  &lt;main&gt;&lt;p&gt;30 November 2024&lt;/p&gt;Follow @orta&lt;p&gt;TLDR: In two years we plan to turn CocoaPods trunk to be read-only. At that point, no new versions or pods will be added to trunk. - Note, this post has been updated in May 2025.&lt;/p&gt;&lt;p&gt;Last month I wrote about how CocoaPods is currently being maintained, I also noted that we were discussing converting the main CocoaPods spec repo "trunk" to be read-only:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;We are discussing that on a very long, multi-year, basis we can drastically simplify the security of CocoaPods trunk by converting the Specs Repo to be read-only. Infrastructure like the Specs repo and the CDN would still operate as long as GitHub and jsDelivr continue to exist, which is pretty likely to be a very long time. This will keep all existing builds working.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I plan to implement the read-only mode so that when someone submits a new Podspec to CocoaPods, it will always be denied at the server level. I would then convert the "CocoaPods/Specs" repo to be marked as "Archived" on GitHub which should cover all of our bases.&lt;/p&gt;&lt;p&gt;Making the switch will not break builds for people using CocoaPods in 2026 onwards, but at that point, you're not getting any more updates to dependencies which come though CocoaPods trunk. This shouldn't affect people who use CocoaPods with their own specs repos, or have all of their dependencies vendored (e.g. they all come from npm.)&lt;/p&gt;&lt;p&gt;May 2025 Update: Since this post was originally written, we've had enough security researchers abusing scripting capabilities in CocoaPods that we are now introducing a block on allowing new CocoaPods to use the &lt;code&gt;prepare_command&lt;/code&gt; field in a Podspec. Any existing Pods using &lt;code&gt;prepare_command&lt;/code&gt; are hard-coded to bypass this check.&lt;/p&gt;&lt;head rend="h2"&gt;Timeline&lt;/head&gt;&lt;p&gt;My goal is to send 2 very hard-to-miss notifications en-masse, and then do a test run a month before the final shutdown.&lt;/p&gt;&lt;head rend="h3"&gt;May 2025&lt;/head&gt;&lt;p&gt;We are stopping new CocoaPods from being added which use the &lt;code&gt;prepare_command&lt;/code&gt; field&lt;/p&gt;&lt;head rend="h3"&gt;Mid-late 2025&lt;/head&gt;&lt;p&gt;I will email all email addresses for people who have contributed a Podspec, informing them of the impending switch to read-only, and linking them to this blog post.&lt;/p&gt;&lt;head rend="h3"&gt;September-October 2026&lt;/head&gt;&lt;p&gt;I will, again, email all email addresses for people who have contributed a Podspec, informing them of the impending switch to read-only, and linking them to this blog post, noting that they have roughly a month before we do a test run of going read-only.&lt;/p&gt;&lt;head rend="h3"&gt;November 1-7th 2026&lt;/head&gt;&lt;p&gt;I will trigger a test run, giving automation a chance to break early&lt;/p&gt;&lt;head rend="h3"&gt;December 2nd 2026&lt;/head&gt;&lt;p&gt;I will switch trunk to not accept new Podspecs permanently. This is a Wednesday after American Thanksgiving, so I think folks won't be in rush mode.&lt;/p&gt;&lt;head rend="h2"&gt;Contact&lt;/head&gt;&lt;p&gt;These dates are not set in stone, and maybe someone out there has a good reason for us to amend the timeline. I don't think I'm amenable to moving it forwards, but within reason there's space for backwards.&lt;/p&gt;&lt;p&gt;If you have questions, you can contact the team via [email protected], me personally at [email protected] or reach out to me via Bluesky: @orta.io.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45091493</guid></item><item><title>Ask HN: Do custom ROMs exist for electric cars, for example Teslas?</title><link>https://news.ycombinator.com/item?id=45092204</link><description>&lt;doc fingerprint="a5f81112ae47e7d9"&gt;
  &lt;main&gt;
    &lt;p&gt;I think I know what you are asking but it is complicated.&lt;/p&gt;
    &lt;p&gt;For safety, regulator, historical and frankly common sense reasons, a car is not one system. It is a system of system that communicate via a CAN BUS, https://en.wikipedia.org/wiki/CAN_bus. This is still true for electric cars. Can this be hacked? Like everything else, yes.&lt;/p&gt;
    &lt;p&gt;Can you side load a new ROM like an android device? Not that know of and hope that never becomes a reality because your phone crashing is different than you car crashing (figuratively and literally). Can you enable/disable features? Yes, usually through ECU hacking. On my P3 Volvo, I bought a cheap stripped down Chinese clone of Volvo's diagnostic tool called DiCE. Once the ECU is decrypted, which is done through brute force, you can use something like https://d5t5.com/article/vdash-volvo-diagnostic or P3Tool to change level settings like the theme of LED dash or engine tuning.&lt;/p&gt;
    &lt;p&gt;I don't know about electric cars, but for gas powered cars there are open source ECUs [0][1]. There are also tuners that directly modify the car's firmware to improve performance. Finally, you can connect a computer to the CAN bus [2], which allows you to capture and replay commands, as well as craft your own commands. This is how Comma's openpilot [3] works: it connects to the CAN bus and sends commands for all supported functionality.&lt;/p&gt;
    &lt;p&gt;There are of course after market ECU tweaks and parts that, for example, will change your throttle response with a physical piece of hardware‚ÄîPedal Commander is a simple example.&lt;/p&gt;
    &lt;p&gt;Not really. You might want to look at what Rivian has been sharing about their vehicle hardware and software architecture. Sandy Munro did a few on site visits with their team.&lt;/p&gt;
    &lt;p&gt;I think you are underestimating how complex EVs are, how much software goes into them, and what goes into coming up with an alternative software stack. Also, I doubt that the likes of Rivian, Tesla, etc. are going to just let people boot whatever on their cars. Why would they?&lt;/p&gt;
    &lt;p&gt;But at the lower levels, hacking things like battery management systems is definitely a thing that is done and somewhat supported. A lot of retrofits where ICE engines are swapped out for an electrical drive train end up repurposing drive trains from EVs.&lt;/p&gt;
    &lt;p&gt;No. Aftermarket ECUs absolutely exist for almost all internal combustion engines. Other aftermarket modules are rare. Integration of them into a complete system even more so.&lt;/p&gt;
    &lt;p&gt;Does "infotainment system" include the cluster displays (speedometer etc) in vehicles where that is entirely a digital display (must be most at this point). I really hate the (very non-traditional) way my current vehicle displays this info. Whats funny is that both the highest trim level of this model and the one-step-cheaper model both do it in a more traditional, far superior way.&lt;/p&gt;
    &lt;p&gt;The only parts where that's true are for things like FCC certification. The US does not have an affirmative certification process for automotive software, including safety critical systems. NHTSA instead puts out a set of rules called FMVSS that manufacturers and aftermarket parts must comply with. Manufacturers then self-certify that they meet FMVSS and produce a bunch of documentation demonstrating that if NHTSA asks.&lt;/p&gt;
    &lt;p&gt;Note that FMVSS has almost nothing to say on the topic of software. The industry broadly follows industry standards like ISO 26262 and the less universal 21448, but these don't have firm legal weight outside their status as standards of practice, nor do they preclude installing your own software.&lt;/p&gt;
    &lt;p&gt;The situation in Europe is different and an affirmative certification process does exist there.&lt;/p&gt;
    &lt;p&gt;I believe these systems are quite coupled with the hardware itself, making it quite difficult to port any custom ROM or such on them. I am not aware of any projects with the goals of creating an open-source Android ROM for a car. Even Phone ROMs are slowly dying off, with the exceptions of Lineage and GrapheneOS.&lt;/p&gt;
    &lt;p&gt;I believe law environment need to change to make possible digital custom car's ROM. Now everything can be closed in same of safety, security, user convience...&lt;/p&gt;
    &lt;p&gt;Security (for vendor) from obscurity. AFAIK most of car owners cannot just buy the replace electronics for his car on used market so most of owners afraid of messing with proprietary computers in the car.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45092204</guid></item><item><title>Tetris is NP-hard even with O(1) rows or columns [pdf]</title><link>https://martindemaine.org/papers/ThinTetris_JIP/paper.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45092324</guid></item><item><title>Zfsbackrest: Pgbackrest style encrypted backups for ZFS filesystems</title><link>https://github.com/gargakshit/zfsbackrest</link><description>&lt;doc fingerprint="bf55c3c41a079343"&gt;
  &lt;main&gt;
    &lt;quote&gt;&lt;g-emoji&gt;‚ö†Ô∏è&lt;/g-emoji&gt;Experimental:&lt;lb/&gt;Do not use it as your only way for backups. This is something I wrote over a weekend. There's a lot of things that need work here.&lt;/quote&gt;
    &lt;p&gt;pgbackrest style encrypted backups for ZFS filesystems.&lt;/p&gt;
    &lt;p&gt;You need age installed to generate encryption keys. Encryption is NOT optional.&lt;/p&gt;
    &lt;code&gt;$ go install github.com/gargakshit/zfsbackrest/cmd/zfsbackrest@latest&lt;/code&gt;
    &lt;p&gt;Create &lt;code&gt;/etc/zfsbackrest.toml&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;debug = true # warning, may log sensitive data

[repository]
# zfsbackrest does not support changing the list of datasets after a repository
# is initialized YET. That's one feature I need.
included_datasets = ["storage/*"] # Glob is supported

[repository.s3]
# zfsbackrest does NOT support non-secure S3 endpoints.
endpoint = "todo"
bucket = "todo"
key = "todo"
secret = "todo"
region = "todo"

[repository.expiry]
# Child backups expire if the parent expires. See the model below for a better
# explanation.
full = "336h" # 14 days
diff = "120h" # 5 days
incr = "24h" # 1 day

[upload_concurrency]
full = 2
diff = 4
incr = 4&lt;/code&gt;
    &lt;code&gt;$ zfsbackrest init --age-recipient-public-key="&amp;lt;your age public key&amp;gt;"&lt;/code&gt;
    &lt;code&gt;$ zfsbackrest backup --type &amp;lt;full | diff | incr&amp;gt;&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;full&lt;/code&gt; backups are standalone. They do not depend on any other backups. They are
also huge in size because of that.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;diff&lt;/code&gt; backups are sent incrementally from the latest &lt;code&gt;full&lt;/code&gt; backup. They depend
on the parent &lt;code&gt;full&lt;/code&gt; backup to be present in the repository to restore.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;incr&lt;/code&gt; backups are send incrementally from the latest &lt;code&gt;diff&lt;/code&gt; backup. They depend
on the parent &lt;code&gt;diff&lt;/code&gt; backup to restore.&lt;/p&gt;
    &lt;code&gt;$ zfsbackrest detail&lt;/code&gt;
    &lt;p&gt;It shows a list of backups, orphans and all.&lt;/p&gt;
    &lt;p&gt;Sometimes, orphaned backups are left as an artefact of incomplete or cancelled backups. You can clean those by running&lt;/p&gt;
    &lt;code&gt;$ zfsbackrest cleanup --orphans --dry-run=false&lt;/code&gt;
    &lt;p&gt;You can clean up expired backups by running&lt;/p&gt;
    &lt;code&gt;$ zfsbackrest cleanup --expired --dru-run=false&lt;/code&gt;
    &lt;p&gt;To restore the backups, you'll need your age identity file (private key).&lt;/p&gt;
    &lt;code&gt;zfsbackrest restore -i &amp;lt;path-to-age-identity-file&amp;gt; \
  -s &amp;lt;name of the dataset to restore from&amp;gt; \
  -b &amp;lt;optionally, the backup ID to restore from, leave empty to restore the latest&amp;gt; \
  -d &amp;lt;name of the dataset to restore to&amp;gt; # Restoring to a dataset that already exists on your local FS will fail.&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;zfsbackrest&lt;/code&gt; doesn't write or modify actual &lt;code&gt;zfs&lt;/code&gt; datasets. It makes extensive
use of snapshots. List of &lt;code&gt;zfs&lt;/code&gt; operations used by &lt;code&gt;zfsbackrest&lt;/code&gt; are&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;backup&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;&lt;code&gt;zfs snapshot&lt;/code&gt;- Creating a&lt;code&gt;zfs&lt;/code&gt;snapshot for&lt;code&gt;zfsbackrest&lt;/code&gt;&lt;/item&gt;
          &lt;item&gt;&lt;code&gt;zfs hold&lt;/code&gt;- Creating a reference to that snapshot to prevent removal&lt;/item&gt;
          &lt;item&gt;&lt;code&gt;zfs send&lt;/code&gt;- Sending the snapshot incrementally&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;cleanup&lt;/code&gt;/&lt;code&gt;force-destroy&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;zfs release&lt;/code&gt;- Release the held snapshot&lt;/item&gt;&lt;item&gt;&lt;code&gt;zfs destroy&lt;/code&gt;- Destroy the snapshot&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;restore&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;&lt;code&gt;zfs recv&lt;/code&gt;- Receiving the remote snapshot&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;TODO&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45092605</guid></item><item><title>Show HN: Simple modenized .NET NuGet server reached RC</title><link>https://github.com/kekyo/nuget-server</link><description>&lt;doc fingerprint="8d11fb69a8142539"&gt;
  &lt;main&gt;
    &lt;p&gt;Simple modenized NuGet server implementation.&lt;/p&gt;
    &lt;p&gt;A simple NuGet server implementation built on Node.js that provides essential NuGet v3 API endpoints.&lt;/p&gt;
    &lt;p&gt;Compatible with &lt;code&gt;dotnet restore&lt;/code&gt; and standard NuGet clients for package publishing, querying, and manually downloading.&lt;/p&gt;
    &lt;p&gt;A modern browser-based UI is also provided:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You can refer to registered packages. You can check various package attributes.&lt;/item&gt;
      &lt;item&gt;You can download packages by version.&lt;/item&gt;
      &lt;item&gt;You can also publish (upload) packages.&lt;/item&gt;
      &lt;item&gt;You can manage user accounts.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Browse package list:&lt;/p&gt;
    &lt;p&gt;Publishing packages:&lt;/p&gt;
    &lt;p&gt;User account managements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Easy setup, run NuGet server in 10 seconds!&lt;/item&gt;
      &lt;item&gt;NuGet V3 API compatibility: Support for modern NuGet client operations&lt;/item&gt;
      &lt;item&gt;No need database management: Store package file and nuspecs into filesystem directly, feel free any database managements&lt;/item&gt;
      &lt;item&gt;Package publish: Flexible client to upload &lt;code&gt;.nupkg&lt;/code&gt;files via&lt;code&gt;HTTP POST&lt;/code&gt;using cURL and others&lt;/item&gt;
      &lt;item&gt;Basic authentication: Setup authentication for publish and general access when you want it&lt;/item&gt;
      &lt;item&gt;Reverse proxy support: Configurable trusted reverse proxy handling for proper URL resolution&lt;/item&gt;
      &lt;item&gt;Modern Web UI with enhanced features: &lt;list rend="ul"&gt;&lt;item&gt;Multiple package upload: Drag &amp;amp; drop multiple .nupkg files at once&lt;/item&gt;&lt;item&gt;User account management: Add/delete users, reset passwords (admin only)&lt;/item&gt;&lt;item&gt;API password regeneration: Self-service API password updates&lt;/item&gt;&lt;item&gt;Password change: Users can change their own passwords&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Package importer: Included package importer from existing NuGet server&lt;/item&gt;
      &lt;item&gt;Docker image available&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;npm install -g nuget-server&lt;/code&gt;
    &lt;p&gt;For using Docker images, refer to a separate chapter.&lt;/p&gt;
    &lt;code&gt;# Start server on default port 5963
nuget-server

# Custom port
nuget-server --port 3000

# Multiple options
nuget-server --port 3000 --config-file config/config.json --users-file config/users.json&lt;/code&gt;
    &lt;p&gt;The NuGet V3 API is served on the &lt;code&gt;/v3&lt;/code&gt; path.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Default nuget-server served URL (Show UI): &lt;code&gt;http://localhost:5963&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Actual NuGet V3 API endpoint: &lt;code&gt;http://localhost:5963/v3/index.json&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The default URL provided by nuget-server can be changed using the &lt;code&gt;--base-url&lt;/code&gt; option.
This is particularly necessary when public endpoint service using a reverse proxy. For details, refer to below chapter.&lt;/p&gt;
    &lt;p&gt;nuget-server only supports the NuGet V3 API. Therefore, NuGet clients must always access it using the V3 API.&lt;/p&gt;
    &lt;p&gt;If you do not explicitly specify to use the V3 API, some implementations may fall back to the V3 API while others may not, potentially causing unstable behavior. Therefore, you must always specify it. Example below.&lt;/p&gt;
    &lt;p&gt;Add as package source:&lt;/p&gt;
    &lt;p&gt;For HTTP endpoints:&lt;/p&gt;
    &lt;code&gt;dotnet nuget add source http://localhost:5963/v3/index.json \
  -n "local" --protocol-version 3 --allow-insecure-connections&lt;/code&gt;
    &lt;p&gt;For HTTPS endpoints:&lt;/p&gt;
    &lt;code&gt;dotnet nuget add source https://packages.example.com/v3/index.json \
  -n "packages" --protocol-version 3&lt;/code&gt;
    &lt;p&gt;Or specify in &lt;code&gt;nuget.config&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;&amp;lt;?xml version="1.0" encoding="utf-8"?&amp;gt;
&amp;lt;configuration&amp;gt;
  &amp;lt;packageSources&amp;gt;
    &amp;lt;add key="local" value="http://localhost:5963/v3/index.json"
      protocolVersion="3" allowInsecureConnections="true" /&amp;gt;
  &amp;lt;/packageSources&amp;gt;
&amp;lt;/configuration&amp;gt;&lt;/code&gt;
    &lt;p&gt;Upload packages by &lt;code&gt;HTTP POST&lt;/code&gt; method, using cURL or any HTTP client with &lt;code&gt;/api/publish&lt;/code&gt; endpoint:&lt;/p&gt;
    &lt;code&gt;# Upload "MyPackage.1.0.0.nupkg" file
curl -X POST http://localhost:5963/api/publish \
  --data-binary @MyPackage.1.0.0.nupkg \
  -H "Content-Type: application/octet-stream"&lt;/code&gt;
    &lt;p&gt;You may be dissatisfied with publishing using this method. The dotnet command includes &lt;code&gt;dotnet nuget push&lt;/code&gt;, which is the standard approach.
However, in my experience, this protocol uses &lt;code&gt;multipart/form-data&lt;/code&gt; for transmission, which has caused issues with gateway services, reverse proxies, load balancers, and similar components.
Therefore, the current nuget-server does not implement this method and instead uses the simplest binary transmission procedure.&lt;/p&gt;
    &lt;p&gt;Another advantage is that when authentication is enabled, you don't need to manage Basic authentication and V3 API keys separately. You might still feel issue with managing read operations and publish operation with the same key, but in that case, you can simply separate the users.&lt;/p&gt;
    &lt;p&gt;For authentication feature, please refer to below chapter.&lt;/p&gt;
    &lt;p&gt;By default, packages are stored in the &lt;code&gt;./packages&lt;/code&gt; directory relative to where you run nuget-server.
You can customize this location using the &lt;code&gt;--package-dir&lt;/code&gt; option:&lt;/p&gt;
    &lt;code&gt;# Use default ./packages directory
nuget-server

# Use custom directory (relative or absolute path)
nuget-server --package-dir /another/package/location&lt;/code&gt;
    &lt;p&gt;Packages are stored in the filesystem using the following structure:&lt;/p&gt;
    &lt;code&gt;packages/
‚îú‚îÄ‚îÄ PackageName/
‚îÇ   ‚îú‚îÄ‚îÄ 1.0.0/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PackageName.1.0.0.nupkg
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PackageName.nuspec
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ icon.png            # Package icon (if present)
‚îÇ   ‚îî‚îÄ‚îÄ 2.0.0/
‚îÇ       ‚îú‚îÄ‚îÄ PackageName.2.0.0.nupkg
‚îÇ       ‚îú‚îÄ‚îÄ PackageName.nuspec
‚îÇ       ‚îî‚îÄ‚îÄ icon.jpg            # Package icon (if present)
‚îî‚îÄ‚îÄ AnotherPackage/
    ‚îî‚îÄ‚îÄ 1.5.0/
        ‚îú‚îÄ‚îÄ AnotherPackage.1.5.0.nupkg
        ‚îú‚îÄ‚îÄ AnotherPackage.nuspec
        ‚îî‚îÄ‚îÄ icon.png            # Package icon (if present)
&lt;/code&gt;
    &lt;p&gt;You can backup the package directory using simply &lt;code&gt;tar&lt;/code&gt; or other achiver:&lt;/p&gt;
    &lt;code&gt;cd /your/server/base/dir
tar -cf - ./packages | lz4 &amp;gt; backup-packages.tar.lz4&lt;/code&gt;
    &lt;p&gt;Restore is simply extract it and re-run nuget-server with the same package directory configuration, because nuget-server does not use any specialized storage such as databases.&lt;/p&gt;
    &lt;p&gt;nuget-server supports configuration through command-line options, environment variables, and JSON file.&lt;/p&gt;
    &lt;p&gt;Settings are applied in the following order (highest to lowest priority):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Command-line options&lt;/item&gt;
      &lt;item&gt;Environment variables&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;config.json&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;Default values&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can specify a custom configuration file:&lt;/p&gt;
    &lt;code&gt;# Using command line option
nuget-server --config-file /path/to/config.json
# or short alias
nuget-server -c /path/to/config.json

# Using environment variable
export NUGET_SERVER_CONFIG_FILE=/path/to/config.json
nuget-server&lt;/code&gt;
    &lt;p&gt;If not specified, nuget-server looks for &lt;code&gt;./config.json&lt;/code&gt; in the current directory.&lt;/p&gt;
    &lt;p&gt;Create a &lt;code&gt;config.json&lt;/code&gt; file:&lt;/p&gt;
    &lt;code&gt;{
  "port": 5963,
  "baseUrl": "http://localhost:5963",
  "packageDir": "./packages",
  "usersFile": "./users.json",
  "realm": "Awsome nuget-server",
  "logLevel": "info",
  "trustedProxies": ["127.0.0.1", "::1"],
  "authMode": "none",
  "sessionSecret": "&amp;lt;your-secret-here&amp;gt;",
  "passwordMinScore": 2,
  "passwordStrengthCheck": true
}&lt;/code&gt;
    &lt;p&gt;All fields are optional. Only include the settings you want to override. Both &lt;code&gt;packageDir&lt;/code&gt; and &lt;code&gt;usersFile&lt;/code&gt; paths can be absolute or relative. If relative, they are resolved from the directory containing the &lt;code&gt;config.json&lt;/code&gt; file.&lt;/p&gt;
    &lt;p&gt;nuget-server also supports authentication.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Authentication Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Details&lt;/cell&gt;
        &lt;cell role="head"&gt;Auth Initialization&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;none&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Default. No authentication required&lt;/cell&gt;
        &lt;cell&gt;Not required&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;publish&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Authentication required only for package publishing&lt;/cell&gt;
        &lt;cell&gt;Required&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;full&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Authentication required for all operations (must login first)&lt;/cell&gt;
        &lt;cell&gt;Required&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;To enable authentication on the NuGet server, first register an initial user using the &lt;code&gt;--auth-init&lt;/code&gt; option.&lt;/p&gt;
    &lt;p&gt;Create an initial admin user interactively:&lt;/p&gt;
    &lt;code&gt;nuget-server --auth-init&lt;/code&gt;
    &lt;p&gt;This command will:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Prompt for admin username (default: &lt;code&gt;admin&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Prompt for password (with strength checking, masked input)&lt;/item&gt;
      &lt;item&gt;Create &lt;code&gt;users.json&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Exit after initialization (server does not start)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When enabling authentication using a Docker image, use this option to generate the initial user.&lt;/p&gt;
    &lt;code&gt;Initializing authentication...
Enter admin username [admin]:
Enter password: ********
Confirm password: ********

============================================================
Admin user created successfully!
============================================================
Username: admin
Password: *********************
============================================================
&lt;/code&gt;
    &lt;p&gt;Users added with &lt;code&gt;--auth-init&lt;/code&gt; automatically become administrator users.
Administrator users can add or remove other users via the UI. They can also reset user passwords.&lt;/p&gt;
    &lt;p&gt;While administrator users can also be assigned API passwords (described later), we recommend separating users for management whenever possible.&lt;/p&gt;
    &lt;p&gt;The NuGet server distinguishes between the password used to log in to the UI and the password used by NuGet clients when accessing the server. The password used by NuGet clients when accessing the server is called the "API password," and access is granted using the combination of the user and the API password.&lt;/p&gt;
    &lt;p&gt;Please log in by displaying the UI in the browser. Select the ‚ÄúAPI password‚Äù menu from the UI menu to generate an API password. Using this API password will enable access from the NuGet client.&lt;/p&gt;
    &lt;p&gt;Here is an example of using the API password:&lt;/p&gt;
    &lt;code&gt;# Add source with API password
dotnet nuget add source http://localhost:5963/v3/index.json \
  -n "local" \
  -u admin \
  -p xxxxxxxxxxxxxxxxxxxxxx \
  --protocol-version 3 --store-password-in-clear-text --allow-insecure-connections&lt;/code&gt;
    &lt;p&gt;Or specify &lt;code&gt;nuget.config&lt;/code&gt; with credentials:&lt;/p&gt;
    &lt;code&gt;&amp;lt;?xml version="1.0" encoding="utf-8"?&amp;gt;
&amp;lt;configuration&amp;gt;
  &amp;lt;packageSources&amp;gt;
    &amp;lt;add key="local" value="http://localhost:5963/v3/index.json"
      protocolVersion="3" allowInsecureConnections="true" /&amp;gt;
  &amp;lt;/packageSources&amp;gt;
  &amp;lt;packageSourceCredentials&amp;gt;
    &amp;lt;local&amp;gt;
      &amp;lt;add key="Username" value="reader" /&amp;gt;
      &amp;lt;add key="ClearTextPassword" value="xxxxxxxxxxxxxxxxxxxxxx" /&amp;gt;
    &amp;lt;/local&amp;gt;
  &amp;lt;/packageSourceCredentials&amp;gt;
&amp;lt;/configuration&amp;gt;&lt;/code&gt;
    &lt;p&gt;For package publishing:&lt;/p&gt;
    &lt;code&gt;# Publish packages with API password
curl -X POST http://localhost:5963/api/publish \
  -u admin:xxxxxxxxxxxxxxxxxxxxxx \
  --data-binary @MyPackage.1.0.0.nupkg \
  -H "Content-Type: application/octet-stream"&lt;/code&gt;
    &lt;p&gt;When publishing a package, you can send the package by setting Basic authentication in the &lt;code&gt;Authorization&lt;/code&gt; header.&lt;/p&gt;
    &lt;p&gt;nuget-server uses the &lt;code&gt;zxcvbn&lt;/code&gt; library to enforce strong password requirements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Evaluates password strength on a scale of 0-4 (Weak to Very Strong)&lt;/item&gt;
      &lt;item&gt;Default minimum score: 2 (Good)&lt;/item&gt;
      &lt;item&gt;Checks against common passwords, dictionary words, and patterns&lt;/item&gt;
      &lt;item&gt;Provides real-time feedback during password creation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Configure password requirements in &lt;code&gt;config.json&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;{
  "passwordMinScore": 2, // 0-4, default: 2 (Good)
  "passwordStrengthCheck": true // default: true
}&lt;/code&gt;
    &lt;p&gt;The NuGet server stores both "password" and "API password" as SALT hashed information, so no plaintext passwords are ever saved. However, if you do not use HTTPS (TLS), be aware that the &lt;code&gt;Authorization&lt;/code&gt; header will contain the plaintext password, making it vulnerable to sniffing.
When makes public endpoint, protect communications using HTTPS.&lt;/p&gt;
    &lt;p&gt;Import all packages from another NuGet server to your local nuget-server instance. This feature can be used when migrating the foreign NuGet server to nuget-server.&lt;/p&gt;
    &lt;p&gt;Import packages interactively in CLI:&lt;/p&gt;
    &lt;code&gt;nuget-server --import-packages --package-dir ./packages&lt;/code&gt;
    &lt;p&gt;This command will:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Prompt for source NuGet server URL&lt;/item&gt;
      &lt;item&gt;Ask if authentication is required&lt;/item&gt;
      &lt;item&gt;If needed, prompt for username and password (masked input)&lt;/item&gt;
      &lt;item&gt;Discover all packages from the source server&lt;/item&gt;
      &lt;item&gt;Download and import all packages to local storage&lt;/item&gt;
      &lt;item&gt;Display progress for each package (1% intervals)&lt;/item&gt;
      &lt;item&gt;Exit after import (server does not start)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Existing packages with the same version will be overwritten&lt;/item&gt;
      &lt;item&gt;Failed imports are logged with error details&lt;/item&gt;
      &lt;item&gt;Progress is reported at 1% intervals to reduce log noise&lt;/item&gt;
      &lt;item&gt;Package icons are preserved during import&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Parallel downloads are not done. This is to avoid making a large number of requests to the repository.&lt;/p&gt;
    &lt;p&gt;This feature is a type of downloader. Therefore, it does not need to be run on the actual host where it will operate. You can perform the import process in advance on a separate host and then move the &lt;code&gt;packages&lt;/code&gt; directory as-is.&lt;/p&gt;
    &lt;code&gt;Starting package import...
Enter source NuGet server URL [http://host.example.com/repository/nuget/]: https://nexus.example.com/repository/nuget/
Does the server require authentication? [y/N]: y
Enter username: reader
Enter password: **********

============================================================
Import Configuration:
Source: https://nexus.example.com/repository/nuget/
Target: ./packages
Authentication: reader (password hidden)
============================================================

Start importing packages? (existing packages will be overwritten) [y/N]: y

Discovering packages from source server...
Found 125 packages with 563 versions total.
Starting package import...
Progress: 100/563 packages (17%) - MyPackage.Core@1.2.3
Progress: 563/563 packages (100%) - AnotherPackage@2.0.0

============================================================
Import Complete!
============================================================
Total packages: 125
Total versions: 563
Successfully imported: 563
Failed: 0
Time elapsed: 125.3 seconds
============================================================
&lt;/code&gt;
    &lt;p&gt;The server supports running behind a reverse proxy. For example, when you have a public URL like &lt;code&gt;https://nuget.example.com&lt;/code&gt; and run nuget-server on a host within your internal network via a gateway.&lt;/p&gt;
    &lt;p&gt;In such cases, you MUST specify the base URL of the public URL to ensure the NuGet V3 API can provide the correct sub-endpoint address.&lt;/p&gt;
    &lt;p&gt;The server resolves URLs using the following priority order:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fixed base URL (highest priority): When &lt;code&gt;--base-url&lt;/code&gt;option is specified, it always takes precedence&lt;/item&gt;
      &lt;item&gt;Trusted proxy headers: When trusted proxies are configured with &lt;code&gt;--trusted-proxies&lt;/code&gt;:&lt;list rend="ul"&gt;&lt;item&gt;HTTP &lt;code&gt;Forwarded&lt;/code&gt;header (proto, host, port)&lt;/item&gt;&lt;item&gt;Traditional &lt;code&gt;X-Forwarded-*&lt;/code&gt;headers (&lt;code&gt;X-Forwarded-Proto&lt;/code&gt;,&lt;code&gt;X-Forwarded-Host&lt;/code&gt;,&lt;code&gt;X-Forwarded-Port&lt;/code&gt;)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;HTTP &lt;/item&gt;
      &lt;item&gt;Standard request information (fallback): Uses &lt;code&gt;Host&lt;/code&gt;header when proxy headers are not available&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example &lt;code&gt;--base-url&lt;/code&gt; option:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;nuget-server served public base URL: &lt;code&gt;https://packages.example.com&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Actual NuGet V3 API endpoint: &lt;code&gt;https://packages.example.com/v3/index.json&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Configure served base URL (do not include /v3 path)
nuget-server --base-url https://packages.example.com

# Add as NuGet source (HTTPS - no --allow-insecure-connections needed)
dotnet nuget add source https://packages.example.com/v3/index.json \
  -n "packages" --protocol-version 3&lt;/code&gt;
    &lt;p&gt;Another option, you can configure with trusted proxy addresses:&lt;/p&gt;
    &lt;code&gt;# Configure trusted proxies for proper host header handling
nuget-server --trusted-proxies "10.0.0.1,192.168.1.100"&lt;/code&gt;
    &lt;p&gt;Environment variables are also supported:&lt;/p&gt;
    &lt;code&gt;export NUGET_SERVER_BASE_URL=https://packages.example.com
export NUGET_SERVER_TRUSTED_PROXIES=10.0.0.1,192.168.1.100
export NUGET_SERVER_CONFIG_FILE=/path/to/config.json
export NUGET_SERVER_USERS_FILE=/path/to/users.json
export NUGET_SERVER_SESSION_SECRET=your-secret-key-here&lt;/code&gt;
    &lt;p&gt;Docker images are available for multiple architectures:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;linux/amd64&lt;/code&gt;(x86_64)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;linux/arm64&lt;/code&gt;(aarch64)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When pulling the image, Docker automatically selects the appropriate architecture for your platform.&lt;/p&gt;
    &lt;p&gt;Suppose you have configured the following directory structure for persistence (recommended):&lt;/p&gt;
    &lt;code&gt;docker-instance/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ config.json
‚îÇ   ‚îî‚îÄ‚îÄ user.json
‚îî‚îÄ‚îÄ packages/
    ‚îî‚îÄ‚îÄ (package files)
&lt;/code&gt;
    &lt;p&gt;Execute as follows:&lt;/p&gt;
    &lt;code&gt;# Pull and run the latest version
docker run -d -p 5963:5963 \
  -v $(pwd)/data:/data \
  -v $(pwd)/packages:/packages \
  kekyo/nuget-server:latest

# Or with Docker Compose
cat &amp;gt; docker-compose.yml &amp;lt;&amp;lt; EOF
version: '3'
services:
  nuget-server:
    image: kekyo/nuget-server:latest
    ports:
      - "5963:5963"
    volumes:
      - ./data:/data
      - ./packages:/packages
    environment:
      - NUGET_SERVER_AUTH_MODE=publish
EOF

docker-compose up -d&lt;/code&gt;
    &lt;p&gt;Your NuGet server is now available at:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Web UI: &lt;code&gt;http://localhost:5963&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;NuGet V3 API: &lt;code&gt;http://localhost:5963/v3/index.json&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Docker container runs as the &lt;code&gt;nugetserver&lt;/code&gt; user (UID 1001) for security reasons. You need to ensure that the mounted directories have the appropriate permissions for this user to write files.&lt;/p&gt;
    &lt;p&gt;Set proper permissions for mounted directories:&lt;/p&gt;
    &lt;code&gt;# Create directories if they don't exist
mkdir -p ./data ./packages

# Set ownership to UID 1001 (matches the container's nugetserver user)
sudo chown -R 1001:1001 ./data ./packages&lt;/code&gt;
    &lt;p&gt;Important: Without proper permissions, you may encounter &lt;code&gt;500 Permission Denied&lt;/code&gt; errors when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Creating or updating user accounts&lt;/item&gt;
      &lt;item&gt;Publishing packages&lt;/item&gt;
      &lt;item&gt;Writing configuration files&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Run with default settings (port 5963, packages and data stored in mounted volumes)
docker run -p 5963:5963 \
  -v $(pwd)/data:/data \
  -v $(pwd)/packages:/packages \
  kekyo/nuget-server:latest

# With authentication (users.json will be created in /data)
docker run -p 5963:5963 \
  -v $(pwd)/data:/data \
  -v $(pwd)/packages:/packages \
  -e NUGET_SERVER_AUTH_MODE=publish \
  kekyo/nuget-server:latest&lt;/code&gt;
    &lt;p&gt;You can also change settings using environment variables or command-line options, but the easiest way to configure settings is to use &lt;code&gt;config.json&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Since the Docker image has mount points configured, you can mount &lt;code&gt;/data&lt;/code&gt; and &lt;code&gt;/packages&lt;/code&gt; as shown in the example above and place &lt;code&gt;/data/config.json&lt;/code&gt; there to flexibly configure settings. Below is an example of &lt;code&gt;config.json&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;{
  "port": 5963,
  "baseUrl": "http://localhost:5963",
  "realm": "Awsome nuget-server",
  "logLevel": "info",
  "authMode": "publish"
}&lt;/code&gt;
    &lt;p&gt;When initializing credentials or importing packages, configure &lt;code&gt;config.json&lt;/code&gt; and perform the operation via the CLI before launching the Docker image:&lt;/p&gt;
    &lt;code&gt;# Initialize authentication
nuget-server -c ./data/config.json --auth-init&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/data&lt;/code&gt;: Default data directory for&lt;code&gt;config.json&lt;/code&gt;,&lt;code&gt;users.json&lt;/code&gt;and other persistent data&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/packages&lt;/code&gt;: Default package storage directory (mounted to persist packages)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Default behavior: The Docker image runs with &lt;code&gt;--users-file /data/users.json --package-dir /packages&lt;/code&gt; by default.&lt;/p&gt;
    &lt;p&gt;Configuration priority (highest to lowest):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Custom command line arguments (when overriding CMD)&lt;/item&gt;
      &lt;item&gt;Environment variables (e.g., &lt;code&gt;NUGET_SERVER_PACKAGE_DIR&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;config.json&lt;/code&gt;file (if explicitly specified)&lt;/item&gt;
      &lt;item&gt;Default command line arguments in Dockerfile&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Various methods exist for automatically starting containers with systemd. Below is a simple example of configuring a systemd service using Podman. This is a simple service unit file used before quadlets were introduced to Podman. By placing this file and having systemd recognize it, you can automatically start the nuget-server:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;/etc/systemd/system/container-nuget-server.service&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;# container-nuget-server.service

[Unit]
Description=Podman container-nuget-server.service
Documentation=man:podman-generate-systemd(1)
Wants=network-online.target
After=network-online.target
RequiresMountsFor=%t/containers

[Service]
Environment=PODMAN_SYSTEMD_UNIT=%n
Restart=always
RestartSec=30
TimeoutStopSec=70
ExecStart=/usr/bin/podman run \
        --cidfile=%t/%n.ctr-id \
        --cgroups=no-conmon \
        --rm \
        --sdnotify=conmon \
        --replace \
        -d \
        -p 5963:5963 \
        --name nuget_server \
        -v /export/data:/data -v /export/packages:/packages docker.io/kekyo/nuget-server:latest
ExecStop=/usr/bin/podman stop \
        --ignore -t 10 \
        --cidfile=%t/%n.ctr-id
ExecStopPost=/usr/bin/podman rm \
        -f \
        --ignore -t 10 \
        --cidfile=%t/%n.ctr-id
Type=notify
NotifyAccess=all

[Install]
WantedBy=default.target&lt;/code&gt;
    &lt;p&gt;The build of the nuget-server Docker image uses Podman.&lt;/p&gt;
    &lt;p&gt;Use the provided multi-platform build script that uses Podman to build for all supported architectures:&lt;/p&gt;
    &lt;code&gt;# Build for all platforms (local only, no push)
./build-docker-multiplatform.sh

# Build and push to Docker Hub
./build-docker-multiplatform.sh --push

# Build for specific platforms only
./build-docker-multiplatform.sh --platforms linux/amd64,linux/arm64

# Push with custom Docker Hub username
OCI_SERVER_USER=yourusername ./build-docker-multiplatform.sh --push

# Inspect existing manifest
./build-docker-multiplatform.sh --inspect&lt;/code&gt;
    &lt;p&gt;Important: For cross-platform builds, QEMU emulation must be configured first:&lt;/p&gt;
    &lt;code&gt;# Option 1: Use QEMU container (recommended)
sudo podman run --rm --privileged docker.io/multiarch/qemu-user-static --reset -p yes

# Option 2: Install system packages
# Ubuntu/Debian:
sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install -y qemu-user-static
# Fedora/RHEL:
sudo dnf install -y qemu-user-static

# Verify QEMU is working:
podman run --rm --platform linux/arm64 alpine:latest uname -m
# Should output: aarch64&lt;/code&gt;
    &lt;p&gt;Without QEMU, you can only build for your native architecture.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;--auth-init&lt;/code&gt; and &lt;code&gt;--import-packages&lt;/code&gt; options require interactive responses from the operator.
Therefore, attempting to automate these may not work properly.
In such cases, you can provide credentials via environment variables:&lt;/p&gt;
    &lt;code&gt;export NUGET_SERVER_ADMIN_USERNAME=admin
export NUGET_SERVER_ADMIN_PASSWORD=MySecurePassword123!
nuget-server --auth-init --config-file ./config.json&lt;/code&gt;
    &lt;p&gt;This allows initialization in CI/CD pipelines without user interaction.&lt;/p&gt;
    &lt;p&gt;For special configurations (or to support persistent sessions), you can set a fixed session secret. Specify a sufficiently long value for the secret:&lt;/p&gt;
    &lt;code&gt;export NUGET_SERVER_SESSION_SECRET=$(openssl rand -base64 32)
nuget-server&lt;/code&gt;
    &lt;p&gt;(Or use &lt;code&gt;config.json&lt;/code&gt;.)&lt;/p&gt;
    &lt;p&gt;If not set, a random secret is generated (warning will be logged).&lt;/p&gt;
    &lt;p&gt;The server implements a subset of the NuGet V3 API protocol:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Service index: &lt;code&gt;/v3/index.json&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Package content: &lt;code&gt;/v3/package/{id}/index.json&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Package downloads: &lt;code&gt;/v3/package/{id}/{version}/{filename}&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Registration index: &lt;code&gt;/v3/registrations/{id}/index.json&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Under MIT.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45092734</guid></item><item><title>AI enters the grant game, picking winners</title><link>https://www.science.org/content/article/ai-enters-grant-game-picking-winners</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45092880</guid></item><item><title>Git for Music ‚Äì Using Version Control for Music Production (2023)</title><link>https://grechin.org/2023/05/06/git-and-reaper.html</link><description>&lt;doc fingerprint="1774f05c4fe586da"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;git for music. Using version control for music production.&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;last updated on 6 Apr 2024&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Being both a musician and a software engineer, I always felt that these two areas are almost completely separated. My developer skill-set seemed to have little to no use for my work as a musician. Which is a pity considering how cool it would be if there was some kind of a sinergy across these two sides of my life.&lt;/p&gt;
    &lt;p&gt;Recently, though, I have found a useful possibility to utilize something I previously used solely for my development work, namely, git, the version control tool, for my music production.&lt;/p&gt;
    &lt;p&gt;Okay, and now let‚Äôs get to the point and‚Ä¶&lt;/p&gt;
    &lt;head rend="h2"&gt;meet Git for music production&lt;/head&gt;
    &lt;p&gt;Did you notice yourself creating a dozen of versions of your project? Are the names like this familiar to you?&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;my-cool-song-new-vocals-brighter-mix-4.rpp&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Did you ever feel frustrated about unmanageability of all this and how sloppy you project directory ends up looking?&lt;/p&gt;
    &lt;p&gt;This version nightmare problem for software people has a solid and well-recognized solution: version control systems. Such as ‚Äúgit‚Äù, which is not only the most widely used one in the industry, but also completely free, open source and cross platform (that is working flawlessly on Win/Mac/Linux).&lt;/p&gt;
    &lt;p&gt;For music production, I use Reaper, and instead of creating dozens of copies of my project file (&lt;code&gt;my-cool-song.rpp&lt;/code&gt;), such as &lt;code&gt;my-cool-song-new-vocals-brighter-mix-4.rpp&lt;/code&gt;, I simply initialize a git repository in the project folder and put the file under version control. This git repository will be the ‚Äúhome‚Äù for managing the version of our music project.&lt;/p&gt;
    &lt;p&gt;By the way, a good supplementary for this reading could be this video of me going through an example. If you are not fan of watching videos, feel free to read on.&lt;/p&gt;
    &lt;head rend="h2"&gt;My git-based music production workflow&lt;/head&gt;
    &lt;p&gt;Although, when wearing a developer hat, I am normally in linux, for the music production stuff, due to the better availability of plugins and such, Windows is a better option. For Windows, you can install &lt;code&gt;git-bash&lt;/code&gt;, and have all the git functionality at your fingertips through a command-line interface.&lt;/p&gt;
    &lt;p&gt;First, I initialize a repository in the project directory. For me, it is most convenient to use a git bash command line terminal:&lt;/p&gt;
    &lt;code&gt;Acer@DESKTOP-NRN84IB MINGW64 /c/home/music
$ cd test_git_project/

Acer@DESKTOP-NRN84IB MINGW64 /c/home/music/test_git_project
$ git init .
Initialized empty Git repository in C:/home/music/test_git_project/.git/

Acer@DESKTOP-NRN84IB MINGW64 /c/home/music/test_git_project (master)
$
&lt;/code&gt;
    &lt;p&gt;in the example above:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I first navigated to the directory with my project with &lt;code&gt;cd&lt;/code&gt;command&lt;/item&gt;
      &lt;item&gt;initialized a repository with &lt;code&gt;git init .&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;on the last, third line, my command prompt starts having a little &lt;code&gt;(master)&lt;/code&gt;thing, which is the default ‚Äúbranch‚Äù in my repository that Git has created for me&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I also create a &lt;code&gt;.gitignore&lt;/code&gt; file and that this is this particular project file that I want to track, and not any other, such as media or peak files:&lt;/p&gt;
    &lt;code&gt;*

!in_your_eyes_remix_git_managed.rpp
&lt;/code&gt;
    &lt;p&gt;Then I am free to work with the project in my DAW as usual. When I am done working on a specific version, I make a commit and give it a descriptive name, e.g. ‚Äúbass vst settings adjusted‚Äù.&lt;/p&gt;
    &lt;p&gt;Then I can see all the versions of my project in &lt;code&gt;git gui&lt;/code&gt; tool.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;side note: you can use any git frontend, not only &lt;code&gt;git gui&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Not only that, but I can also open any historical version of the project, create branches and so on. In other words, I can fully benefit from the version control system! If you are already using git, you know what I mean.&lt;/p&gt;
    &lt;p&gt;The days of versioned files mess in my project folder are finally gone! I wonder, though, if Reaper developers will be willing to incorporate that into their product one day.&lt;/p&gt;
    &lt;head rend="h3"&gt;Managing other files (WAVs etc.)&lt;/head&gt;
    &lt;p&gt;Git is not super suited for managing big binary files (such as WAV samples and stems), but this is not a problem for me since I only manage the main project file.&lt;/p&gt;
    &lt;p&gt;About other files I do not care. Why? Becase I never remove them. The media files are either WAVs related to this project (and which are therefore kept in the project folder) or samples from my library. In both cases, these files are normally (at least withing the lifespan of the project under construction) never deleted.&lt;/p&gt;
    &lt;p&gt;This approach, which, I guess, I share with most producers, makes it easy to return to any historical version of the project and rely on the media files to be found.&lt;/p&gt;
    &lt;head rend="h3"&gt;collaborating with GIT? Not sure‚Ä¶&lt;/head&gt;
    &lt;p&gt;GIT is not only about versioning, but also about collaboration, with remote repositories and so on. Frankly, I don‚Äôt see it feasible for collaboration over music projects since the project files are normally opaque and we should not expect git or any other version control system to be able to merge/diff them.&lt;/p&gt;
    &lt;p&gt;And let‚Äôs not forget that to be able to work on your project, the collaborator needs to have very close set up: the DAW, the plugins and all the media files.&lt;/p&gt;
    &lt;p&gt;Another note of the remote repositories: I do find it useful that I can push my music project to github and this kind of a backup that will outlive my current PC. This is nice, but we can‚Äôt really consider it a real backup - because of missing media.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tracking TODO items for your music project in GitHub&lt;/head&gt;
    &lt;p&gt;Interesting use-case I‚Äôm currently testing is to have a ‚Äútodo list‚Äù, think of an small per-project issue tracker with a list of things you plan to do later. Just a version-tracked text file of the format similar to this:&lt;/p&gt;
    &lt;code&gt;fix panning issues in chorus TODO
add one more synth layer TODO
&lt;/code&gt;
    &lt;p&gt;Once it‚Äôs in Github, you can update it from anywhere (GitHub allows you to edit files right in the browser), so, basically, you project gets its own, private, read/write website. On the go and got a cool idea? Now you know where to record it (don‚Äôt forget to &lt;code&gt;pull&lt;/code&gt; your update, though, once you are back to your DAW PC).&lt;/p&gt;
    &lt;p&gt;In conclusion, when we inspect this idea of ‚Äúgit for music‚Äù a bit closer, we can see that it does have a few viable applications. Yes, this tool is not magical, but still pretty useful!&lt;/p&gt;
    &lt;p&gt;Thanks for reading.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45092895</guid></item><item><title>Turns out Google made up an elaborate story about me</title><link>https://bsky.app/profile/bennjordan.bsky.social/post/3lxojrbessk2z</link><description>&lt;doc fingerprint="d0eece56e5def5e6"&gt;
  &lt;main&gt;
    &lt;p&gt;This is a heavily interactive web application, and JavaScript is required. Simple HTML interfaces are possible, but that is not what this is. &lt;/p&gt;
    &lt;div&gt;
      &lt;head rend="h3"&gt;Post&lt;/head&gt;
      &lt;p&gt;Benn Jordan&lt;/p&gt;
      &lt;p&gt;bennjordan.bsky.social&lt;/p&gt;
      &lt;p&gt;did:plc:yhpu2wsyvdbsehluhclpqyyk&lt;/p&gt;
      &lt;p&gt;This is SO messed up. ü´• I had a few messages and tags today asking me to clarify my stance on Israel, which was odd as I've been pretty outspoken against genocide and in full support of Palestinian statehood...&lt;/p&gt;
      &lt;p&gt;2025-08-31T07:09:14.503Z&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45092925</guid></item><item><title>Effective learning: Twenty rules of formulating knowledge (1999)</title><link>https://www.supermemo.com/en/blog/twenty-rules-of-formulating-knowledge</link><description>&lt;doc fingerprint="3ed6d2664c2f1e8c"&gt;
  &lt;main&gt;
    &lt;p&gt;Dr Piotr Wozniak, February, 1999 (updated)&lt;/p&gt;
    &lt;p&gt;This article will help you overcome one of the greatest difficulties you will face when trying to accelerate learning: formulating knowledge&lt;/p&gt;
    &lt;p&gt;The speed of learning will depend on the way you formulate the material. The same material can be learned many times faster if well formulated! The difference in speed can be stunning!&lt;/p&gt;
    &lt;p&gt;The rules are listed in the order of importance. Those listed first are most often violated or bring most benefit if complied with!&lt;/p&gt;
    &lt;p&gt;There is an underlying assumption that you will proceed with learning using spaced repetition, i.e. you will not just learn once but you will repeat the material optimally (as in SuperMemo).&lt;/p&gt;
    &lt;p&gt;The 20 rules of formulating knowledge in learning&lt;/p&gt;
    &lt;p&gt;1) Do not learn if you do not understand&lt;/p&gt;
    &lt;p&gt;Trying to learn things you do not understand may seem like an utmost nonsense. Still, an amazing proportion of students commit the offence of learning without comprehension. Very often they have no other choice! The quality of many textbooks or lecture scripts is deplorable while examination deadlines are unmovable.If you are not a speaker of German, it is still possible to learn a history textbook in German. The book can be crammed word for word. However, the time needed for such ‚Äúblind learning‚Äù is astronomical. Even more important: The value of such knowledge is negligible. If you cram a German book on history, you will still know nothing of history.The German history book example is an extreme. However, the materials you learn may often seem well structured and you may tend to blame yourself for lack of comprehension. Soon you may pollute your learning process with a great deal of useless material that treacherously makes you believe ‚Äúit will be useful some day‚Äù. &lt;/p&gt;
    &lt;p&gt;2) Learn before you memorize&lt;/p&gt;
    &lt;p&gt;Before you proceed with memorizing individual facts and rules, you need to build an overall picture of the learned knowledge. Only when individual pieces fit to build a single coherent structure, will you be able to dramatically reduce the learning time. This is closely related to the problem comprehension mentioned in Rule 1: Do not learn if you do not understand. A single separated piece of your picture is like a single German word in the textbook of history.Do not start from memorizing loosely related facts! First read a chapter in your book that puts them together (e.g. the principles of the internal combustion engine). Only then proceed with learning using individual questions and answers (e.g. What moves the pistons in the internal combustion engine?), etc.&lt;/p&gt;
    &lt;p&gt;3) Build upon the basics&lt;/p&gt;
    &lt;p&gt;The picture of the learned whole (as discussed in Rule 2: Learn before you memorize) does not have to be complete to the last detail. Just the opposite, the simpler the picture the better. The shorter the initial chapter of your book the better. Simple models are easier to comprehend and encompass. You can always build upon them later on.Do not neglect the basics. Memorizing seemingly obvious things is not a waste of time! Basics may also appear volatile and the cost of memorizing easy things is little. Better err on the safe side. Remember that usually you spend 50% of your time repeating just 3-5% of the learned material! Basics are usually easy to retain and take a microscopic proportion of your time. However, each memory lapse on basics can cost you dearly!&lt;/p&gt;
    &lt;p&gt;4) Stick to the minimum information principle&lt;/p&gt;
    &lt;p&gt;The material you learn must be formulated in as simple way as it is&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simple is easy&lt;lb/&gt;By definition, simple material is easy to remember. This comes from the fact that its simplicity makes is easy for the brain to process it always in the same way. Imagine a labyrinth. When making a repetition of a piece of material, your brain is running through a labyrinth (you can view a neural network as a tangle of paths). While running through the labyrinth, the brain leaves a track on the walls. If it can run in only one unique way, the path is continuous and easy to follow. If there are many combinations, each run may leave a different trace that will interfere with other traces making it difficult to find the exit. The same happens on the cellular level with different synaptic connections being activated at each repetition of complex material&lt;/item&gt;
      &lt;item&gt;Repetitions of simple items are easier to schedule&lt;lb/&gt;I assume you will make repetitions of the learned material using optimum inter-repetition intervals (as in SuperMemo). If you consider an item that is composed of two sub-items, you will need to make repetitions that are frequent enough to keep the more difficult item in memory. If you split the complex item into sub-items, each can be repeated at its own pace saving your time. Very often, inexperienced students create items that could easily be split into ten or more simpler sub-items! Although the number of items increases, the number of repetitions of each item will usually be small enough to greatly outweigh the cost of (1) forgetting the complex item again and again, (2) repeating it in excessively short intervals or (3) actually remembering it only in part!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here is a striking example:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Ill-formulated knowledge ‚Äì Complex and wordy&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: What are the characteristics of the Dead Sea?&lt;p&gt;A: Salt lake located on the border between Israel and Jordan. Its shoreline is the lowest point on the Earth‚Äôs surface, averaging 396 m below sea level. It is 74 km long. It is seven times as salty (30% by volume) as the ocean. Its density keeps swimmers afloat. Only simple organisms can live in its saline waters&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Well-formulated knowledge ‚Äì Simple and specific&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: Where is the Dead Sea located?&lt;p&gt;A: on the border between Israel and Jordan&lt;/p&gt;&lt;p&gt;Q: What is the lowest point on the Earth‚Äôs surface?&lt;/p&gt;&lt;p&gt;A: The Dead Sea shoreline&lt;/p&gt;&lt;p&gt;Q: What is the average level on which the Dead Sea is located?&lt;/p&gt;&lt;p&gt;A: 400 meters (below sea level)&lt;/p&gt;&lt;p&gt;Q: How long is the Dead Sea?&lt;/p&gt;&lt;p&gt;A: 70 km&lt;/p&gt;&lt;p&gt;Q: How much saltier is the Dead Sea than the oceans?&lt;/p&gt;&lt;p&gt;A: 7 times&lt;/p&gt;&lt;p&gt;Q: What is the volume content of salt in the Dead Sea?&lt;/p&gt;&lt;p&gt;A: 30%&lt;/p&gt;&lt;p&gt;Q: Why can the Dead Sea keep swimmers afloat?&lt;/p&gt;&lt;p&gt;A: due to high salt content&lt;/p&gt;&lt;p&gt;Q: Why is the Dead Sea called Dead?&lt;/p&gt;&lt;p&gt;A: because only simple organisms can live in it&lt;/p&gt;&lt;p&gt;Q: Why only simple organisms can live in the Dead Sea?&lt;/p&gt;&lt;p&gt;A: because of high salt content&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;You might want to experiment and try to learn two subjects using the two above approaches and see for yourself what advantage is brought by minimum information principle. This is particularly visible in the long perspective, i.e. the longer the time you need to remember knowledge, the more you benefit from simplifying your items!&lt;/p&gt;
    &lt;p&gt;Note in the example above how short the questions are. Note also that the answers are even shorter! We want a minimum amount of information to be retrieved from memory in a single repetition! We want answer to be as short as imaginably possible!&lt;/p&gt;
    &lt;p&gt;You will notice that the knowledge learned in the ill-structured example is not entirely equivalent to the well-structured formulation. For example, although you will remember why the Dead Sea can keep swimmers afloat, you may forget that it at all has such a characteristic in the first place! Additionally, rounding 396 to 400 and 74 to 70 produces some loss of information. These can be remedied by adding more questions or making the present ones more precise.&lt;/p&gt;
    &lt;p&gt;You will also lose the ability to fluently recite the description of the Dead Sea when called up to the blackboard by your teachers. I bet, however, that shining in front of the class is not your ultimate goal in learning. To see how to cope with recitations and poems, read further (section devoted to enumerations)&lt;/p&gt;
    &lt;p&gt;5) Cloze deletion is easy and effective&lt;/p&gt;
    &lt;p&gt;Cloze deletion is a sentence with its parts missing and replaced by three dots. Cloze deletion exercise is an exercise that uses cloze deletion to ask the student to fill in the gaps marked with the three dots. For example, Bill ‚Ä¶[name] was the second US president to go through impeachment.If you are a beginner and if you find it difficult to stick to the minimum information principle, use cloze deletion! If you are an advanced user, you will also like cloze deletion. It is a quick and effective method of converting textbook knowledge into knowledge that can be subject to learning based on spaced repetition. Cloze deletion makes the core of the fast reading and learning technique called incremental reading.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Ill-formulated knowledge ‚Äì Complex and wordy&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: What was the history of the Kaleida company?&lt;p&gt;A: Kaleida, funded to the tune of $40 million by Apple Computer and IBM in 1991. Hyped as a red-hot startup, Kaleida‚Äôs mission was to create a multimedia programming language It finally produced one, called Script X. But it took three years. Meanwhile, companies such as Macromedia and Asymetrix had snapped up all the business. Kaleida closed in 1995.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Well-formulated knowledge ‚Äì Simple cloze deletion&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: Kaleida was funded to the tune of ‚Ä¶(amount) by Apple Computer and IBM in 1991&lt;p&gt;A: $40 million&lt;/p&gt;&lt;p&gt;Q: Kaleida was funded to the tune of $40 million by ‚Ä¶(companies) in 1991&lt;/p&gt;&lt;p&gt;A: Apple and IBM&lt;/p&gt;&lt;p&gt;Q: Kaleida was funded to the tune of $40 million by Apple Computer and IBM in ‚Ä¶ (year)&lt;/p&gt;&lt;p&gt;A: 1991&lt;/p&gt;&lt;p&gt;Q: ‚Ä¶(company) mission was to create a multimedia programming language. It finally produced one, called Script X. But it took three years&lt;/p&gt;&lt;p&gt;A: Kaleida‚Äôs&lt;/p&gt;&lt;p&gt;Q: Kaleida‚Äôs mission was to create a ‚Ä¶ It finally produced one, called Script X. But it took three years&lt;/p&gt;&lt;p&gt;A: multimedia programming language&lt;/p&gt;&lt;p&gt;Q: Kaleida‚Äôs mission was to create a multimedia programming language. It finally produced one, called ‚Ä¶ But it took three years&lt;/p&gt;&lt;p&gt;A: Script X&lt;/p&gt;&lt;p&gt;Q: Kaleida‚Äôs mission was to create a multimedia programming language. It finally produced one, called Script X. But it took ‚Ä¶(time)&lt;/p&gt;&lt;p&gt;A: three years&lt;/p&gt;&lt;p&gt;Q: Kaleida‚Äôs mission was to create a multimedia programming language: Script X. But it took three years. Meanwhile, companies such as ‚Ä¶ had snapped up all the business&lt;/p&gt;&lt;p&gt;A: Macromedia/Asymetrix&lt;/p&gt;&lt;p&gt;Q: Kaleida‚Äôs mission was to create Script X. But it took three years. Meanwhile, companies such as Macromedia and Asymetrix had snapped up all the business. Kaleida closed in ‚Ä¶(year)&lt;/p&gt;&lt;p&gt;A: 1995&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Optional: SuperMemo Recipe:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SuperMemo 2002&lt;/cell&gt;
        &lt;cell&gt;SuperMemo 2000&lt;/cell&gt;
        &lt;cell&gt;SuperMemo 98/99&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Creating cloze deletions in new SuperMemos:select the keyword that is to be replaced with tree dots and press Alt+Z&lt;/cell&gt;
        &lt;cell&gt;Generating a cloze deletions from texts placed in the clipboard in SuperMemo 2000:&lt;p&gt;1. Press Ctrl+Alt+N to paste the text to SuperMemo&lt;/p&gt;&lt;p&gt;2. Select the part that is to be replaced with three dots 3. Right-click to open the component menu and select&lt;/p&gt;&lt;p&gt;Reading : Remember cloze (or click one of cloze icons on the reading toolbar)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Cloze deletions in SuperMemo 98/99:&lt;p&gt;1. Press Ctrl+A to add a standard question-and-answer item&lt;/p&gt;&lt;p&gt;2. Paste the text into the question field. This will create the outline of your items&lt;/p&gt;&lt;p&gt;3. Press Ctrl+Alt+U to Duplicate the element&lt;/p&gt;&lt;p&gt;4. Select the part that is to be replaced with three dots&lt;/p&gt;&lt;p&gt;5. Cut the selection to the clipboard (e.g. with Shift+Del)&lt;/p&gt;&lt;p&gt;6. Type in three dots (optionally, add the explanation in parentheses as in above examples)&lt;/p&gt;&lt;p&gt;7. Press Ctrl+T to save the question field and move to the answer field&lt;/p&gt;&lt;p&gt;8. Paste the text cut in Step 5 (e.g. with Shift+Ins or Ctrl+V). Your first item is ready&lt;/p&gt;&lt;p&gt;9. Press PgUp to go back to the outline item created in Step 2&lt;/p&gt;&lt;p&gt;10. Goto Step 3 and continue adding new items&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;6) Use imagery&lt;/p&gt;
    &lt;p&gt;Visual cortex is that part of the brain in which visual stimuli are interpreted. It has been very well developed in the course of evolution and that is why we say one picture is worth a thousand words. Indeed if you look at the number of details kept in a picture and the easiness with which your memory can retain them, you will notice that our verbal processing power is greatly inferior as compared with the visual processing power. The same refers to memory. A graphic representation of information is usually far less volatile.Usually it takes much less time to formulate a simple question-and-answer pair than to find or produce a neat graphic image. This is why you will probably always have to weigh up cost and profits in using graphics in your learning material. Well-employed images will greatly reduce your learning time in areas such as anatomy, geography, geometry, chemistry, history, and many more.The power of imagery explains why the concept of Tony Buzan‚Äôs mind maps is so popular. A mind map is an abstract picture in which connections between its components reflect the logical connections between individual concepts.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Less beneficial formulation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: What African country is located between Kenya, Zambia and Mozambique?A: Tanzania&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Well-formulated knowledge ‚Äì Simple cloze deletion&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: What African country is marked white on the map?&lt;p&gt;A: Tanzania&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;7) Use mnemonic techniques&lt;/p&gt;
    &lt;p&gt;Mnemonic techniques are various techniques that make remembering easier. They are often amazingly effective. For most students, a picture of a 10-year-old memorizing a sequence of 50 playing cards verges on discovering a young genius. It is very surprising then to find out how easy it is to learn the techniques that make it possible with a dose of training. These techniques are available to everyone and do not require any special skills!Before you start believing that mastering such techniques will provide you with an eternal solution to the problem of forgetting, be warned that the true bottleneck towards long-lasting and useful memories is not in quickly memorizing knowledge! This is indeed the easier part. The bottleneck lies in retaining memories for months, years or for lifetime! To accomplish the latter you will need SuperMemo and the compliance with the 20 rules presented herein.There have been dozens of books written about mnemonic techniques. Probably those written by Tony Buzan are most popular and respected. You can search the web for keywords such as: mind maps, peg lists, mnemonic techniques, etc.Experience shows that with a dose of training you will need to consciously apply mnemonic techniques in only 1-5% of your items. With time, using mnemonic techniques will become automatic!&lt;lb/&gt;Exemplary mind map:&lt;/p&gt;
    &lt;p&gt;8) Graphic deletion is as good as cloze deletion&lt;/p&gt;
    &lt;p&gt;Graphic deletion works like cloze deletion but instead of a missing phrase it uses a missing image component. For example, when learning anatomy, you might present a complex illustration. Only a small part of it would be missing. The student‚Äôs job is to name the missing area. The same illustration can be used to formulate 10-20 items! Each item can ask about a specific subcomponent of the image. Graphic deletion works great in learning geography!Exemplary graphic deletion:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SuperMemo 2000/2002&lt;/cell&gt;
        &lt;cell&gt;SuperMemo 99&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;This is how you can quickly generate graphic deletion using a picture from the clipboard:&lt;p&gt;1. Press Shift+Ins to paste the picture to SuperMemo&lt;/p&gt;&lt;p&gt;2. Press Ctrl+Shift+M and choose Occlusion template to apply graphic deletion template&lt;/p&gt;&lt;p&gt;3. SuperMemo 2000 only: Choose Ctrl+Shift+F2 to impose and detach the Occlusion template&lt;/p&gt;&lt;p&gt;4. Fill out the fields and place the occlusion rectangle to cover the appropriate part of the picture (use Alt+click twice to set the rectangle in the dragging mode)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;In SuperMemo 99 you will need a few more steps:&lt;p&gt;1.Create an item containing the following components:&lt;/p&gt;&lt;p&gt;‚Äì question text: What is the name of the area covered with the red rectangle?&lt;/p&gt;&lt;p&gt;‚Äì empty answer text (click Answer on the component menu)&lt;/p&gt;&lt;p&gt;‚Äì your illustration (use Import file on the image component menu)&lt;/p&gt;&lt;p&gt;‚Äì red rectangle component (choose red color with Color on the rectangle component menu)&lt;/p&gt;&lt;p&gt;2. Choose Duplicate on the element menu (e.g. by pressing Ctrl+Alt+U)&lt;/p&gt;&lt;p&gt;3. Ctrl+click the rectangle component twice to place it in the dragging mode&lt;/p&gt;&lt;p&gt;4. Drag and size the red rectangle to cover the area in question&lt;/p&gt;&lt;p&gt;5. Type in the answer in the answer field&lt;/p&gt;&lt;p&gt;6. Press PgUp to go back to the original element created in Step 1&lt;/p&gt;&lt;p&gt;7. Go to Step 2 to add generate more graphic deletions&lt;/p&gt;&lt;p&gt;Note that you could also paint covering rectangles or circles on the original image but this would greatly increase the size of your collection. The above method makes sure that you reuse the same image many times in all items of the same template. For example, the collection Brain Anatomy available from &amp;gt;SuperMemo Library and on SuperMemo MegaMix CD-ROM uses the above technique&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;9) Avoid sets&lt;/p&gt;
    &lt;p&gt;A set is a collection of objects. For example, a set of fruits might be an apple, a pear and a peach. A classic example of an item that is difficult to learn is an item that asks for the list of the members of a set. For example: What countries belong to the European Union? You should avoid such items whenever possible due to the high cost of retaining memories based on sets. If sets are absolutely necessary, you should always try to convert them into enumerations. Enumerations are ordered lists of members (for example, the alphabetical list of the members of the EU). Enumerations are also hard to remember and should be avoided. However, the great advantage of enumerations over sets is that they are ordered and they force the brain to list them always in the same order. An ordered list of countries contains more information than the set of countries that can be listed in any order. Paradoxically, despite containing more information, enumerations are easier to remember. The reason for this has been discussed earlier in the context of the minimum information principle: you should always try to make sure your brain works in the exactly same way at each repetition. In the case of sets, listing members in varying order at each repetition has a disastrous effect on memory. It is nearly impossible to memorize sets containing more than five members without the use of mnemonic techniques, enumeration, grouping, etc. Despite this claim, you will often succeed due to subconsciously mastered techniques that help you go around this problem. Those techniques, however, will fail you all too often. For that reason: Avoid sets! If you need them badly, convert them into enumerations and use techniques for dealing with enumerations&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Ill-formulated knowledge ‚Äì Sets are unacceptable!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: What countries belong to the European Union (2002)?&lt;p&gt;A: Austria, Belgium, Denmark, Finland, France, Germany, Greece, Ireland, Italy, Luxembourg, the Netherlands, Portugal, Spain, Sweden, and the United Kingdom.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Well-formulated knowledge ‚Äì Converting a set into a meaningful listing&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: Which country hosted a meeting to consider the creation of a European Community of Defence in 1951?&lt;p&gt;A: France&lt;/p&gt;&lt;p&gt;Q: Which countries apart from France joined the European Coal and Steel Community in 1952?&lt;/p&gt;&lt;p&gt;A: Germany, Italy and the Benelux&lt;/p&gt;&lt;p&gt;Q: What countries make up the Benelux?&lt;/p&gt;&lt;p&gt;A: Belgium, Luxembourg, and the Netherlands&lt;/p&gt;&lt;p&gt;Q: Whose membership did Charles de Gaulle oppose in the 1960s?&lt;/p&gt;&lt;p&gt;A: that of UK&lt;/p&gt;&lt;p&gt;Q: Which countries joined the EEC along the UK in 1973?&lt;/p&gt;&lt;p&gt;A: Ireland and Denmark&lt;/p&gt;&lt;p&gt;Q: Which country joined the EEC in 1981?&lt;/p&gt;&lt;p&gt;A: Greece&lt;/p&gt;&lt;p&gt;Q: Which countries joined the EEC in 1986?&lt;/p&gt;&lt;p&gt;A: Spain and Portugal&lt;/p&gt;&lt;p&gt;Q: Which countries joined the EU in 1995?&lt;/p&gt;&lt;p&gt;A: Austria, Sweden and Finland&lt;/p&gt;&lt;p&gt;Q: What was the historic course of expansion of the European Union membership?&lt;/p&gt;&lt;p&gt;A: (1) France and (2) Germany, Italy and the Benelux, (3) UK and (4) Ireland and Denmark, (5) Greece, (6) Spain and Portugal and (7) Austria, Sweden and Finland&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note that in the example above, we converted a 15-member set into 9 items, five of which are 2-3 member sets, and one is a six member enumeration. Put it to your SuperMemo, and see how easy it is to generate the list of the European Union members using the historic timeline! Note the tricks used with France and the UK. They joined the union in the company of others but have been listed as separate items to simplify the learning process. Note also that the sum of information included in this well-formulated approach is far greater than that of the original set. Thus along simplicity, we gained some useful knowledge. All individual items effectively comply with the minimum information principle! You could go further by trying to split the Germany-Italy-Benelux set or using mnemonic techniques to memorize the final seven-member enumeration (i.e. the last of the questions above). However, you should take those steps only if you have any problems with retaining the proposed set in memory.&lt;/p&gt;
    &lt;p&gt;10) Avoid enumerations&lt;/p&gt;
    &lt;p&gt;Enumerations are also an example of classic items that are hard to learn. They are still far more acceptable than sets. Avoid enumerations wherever you can. If you cannot avoid them, deal with them using cloze deletions (overlapping cloze deletions if possible). Learning the alphabet can be a good example of an overlapping cloze deletion:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Hard to learn item&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: What is the sequence of letters in the alphabet?&lt;p&gt;A: abcdefghijklmnopqrstuvwxyz&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Easy to learn items&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: What three letters does the alphabet begin with?&lt;p&gt;A: ABCQ: Fill out the missing letters of the alphabet A ‚Ä¶ ‚Ä¶ ‚Ä¶ E&lt;/p&gt;&lt;p&gt;A: B, C, D&lt;/p&gt;&lt;p&gt;Q: Fill out the missing letters of the alphabet B ‚Ä¶ ‚Ä¶ ‚Ä¶ F&lt;/p&gt;&lt;p&gt;A: C, D, E&lt;/p&gt;&lt;p&gt;Q: Fill out the missing letters of the alphabet C ‚Ä¶ ‚Ä¶ ‚Ä¶ G&lt;/p&gt;&lt;p&gt;A: D, E, F&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The above items will make learning the alphabet much faster. The greatest advantage of the above approach is that is it easier for psychological reasons: the student does not have to stop repetitions to recite the whole sequence and can only focus on a small part of the learned material. Still it is recommended that he recite the whole alphabet after making the repetition. However, once all individual pieces are well remembered, reciting the whole should be a pleasant and speedy action that produces little frustration.&lt;lb/&gt;The cloze deletion used above is an overlapping cloze deletion, i.e. the same parts of the enumeration are strengthened in memory using different items (for example, the sequence C-D will be needed to recall the second and the third item). This redundancy does not contradict the minimum information principle because the extra information is added in extra items.&lt;/p&gt;
    &lt;p&gt;You can also deal with enumerations by using grouping like in the case of sets (see the European Union example) but cloze deletions should be simpler and should suffice in most cases.&lt;lb/&gt;Learning poems is an example of learning enumerations (all words and sentences have to be uttered in a predefined sequence); however, due to strong semantic connections, the rhyme and the rhythm, it may often be possible to effectively remember poems without using cloze deletion and without the frustration of forgetting small subcomponents again and again. However, once you notice you stumble with your poem, you should dismember it using cloze deletion and thus make sure that the learning is fast, easy, effective and pleasurable&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;A poem that is hard to remember&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: The credit belongs ‚Ä¶ (Teddy Roosevelt)&lt;p&gt;A: The credit belongs to the man who‚Äôs actually in the arena, whose face is marred by dust and sweat; a man who knows the great enthusiasm and the great devotions, who spends himself in a worthy cause, who in the end knows the triumph of high achievement, so that his place shall never be with those cold and timid souls who know neither victory nor defeat&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;A poem split into easy items&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: The credit belongs ‚Ä¶ (Teddy Roosevelt)&lt;p&gt;A: to the man who‚Äôs actually in the arena&lt;/p&gt;&lt;p&gt;Q: The credit belongs to the man who‚Äôs actually in the arena ‚Ä¶&lt;/p&gt;&lt;p&gt;A: whose face is marred by dust and sweat (a man who knows the great enthusiasm)&lt;/p&gt;&lt;p&gt;Q: whose face is marred by dust and sweat ‚Ä¶ (The credit belongs)&lt;/p&gt;&lt;p&gt;A: a man who knows the great enthusiasm and the great devotions (who spends himself in a worthy cause)&lt;/p&gt;&lt;p&gt;Q: a man who knows the great enthusiasm and the great devotions ‚Ä¶ (The credit belongs)&lt;/p&gt;&lt;p&gt;A: who spends himself in a worthy cause (who in the end knows the triumph of high achievement)&lt;/p&gt;&lt;p&gt;Q: who spends himself in a worthy cause ‚Ä¶ (The credit belongs)&lt;/p&gt;&lt;p&gt;A: who in the end knows the triumph of high achievement (so that his place shall never be), etc. etc.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Does it all sound artificial? It does! But you will never know how effective this approach is until you try it by yourself!&lt;/p&gt;
    &lt;p&gt;11) Combat interference&lt;/p&gt;
    &lt;p&gt;When you learn about similar things you often confuse them. For example, you may have problems distinguishing between the meanings of the words historic and historical. This will even be more visible if you memorize lots of numbers, e.g. optimum dosages of drugs in pharmacotherapy. If knowledge of one item makes it harder to remember another item, we have a case of memory interference. You can often remember an item for years with straight excellent grades until ‚Ä¶ you memorize another item that makes it nearly impossible to remember either! For example, if you learn geography and you memorize that the country located between Venezuela, Suriname and Brazil is Guyana, you are likely to easily recall this fact for years with just a couple of repetitions. However, once you add similar items asking about the location of all these countries, and French Guyana, and Colombia and more, you will suddenly notice strong memory interference and you may experience unexpected forgetting. In simple terms: you will get confused about what is what.&lt;lb/&gt;Interference is probably the single greatest cause of forgetting in collections of an experienced user of SuperMemo. You can never be sure when it strikes, and the only hermetic procedure against it is to detect and eliminate. In other words, in many cases it may be impossible to predict interference at the moment of formulating knowledge. Interference can also occur between remotely related items like Guyana, Guyard and Guyenne, as well as Guyana, kayman and ‚Ä¶ aspirin. It may work differently for you and for your colleague. It very hard to predict.Still you should do your best to prevent interference before it takes its toll. This will make your learning process less stressful and mentally bearable. Here are some tips:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;make items as unambiguous as possible&lt;/item&gt;
      &lt;item&gt;stick to the minimum information principle (many of the remaining rules in this text are based on avoiding interference!)&lt;/item&gt;
      &lt;item&gt;eliminate interference as soon as you spot it, i.e. before it becomes your obsession (e.g. as soon as you see the word inept you think ‚ÄúI know the meanings of inept and inapt but I will never know which is which!‚Äù)&lt;/item&gt;
      &lt;item&gt;in SuperMemo use View : Other browsers : Leeches(Shift+F3) to regularly review and eliminate most difficult items&lt;/item&gt;
      &lt;item&gt;read more: Memory interference&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;12) Optimize wording&lt;/p&gt;
    &lt;p&gt;The wording of your items must be optimized to make sure that in minimum time the right bulb in your brain lights up. This will reduce error rates, increase specificity, reduce response time, and help your concentration.Less optimum item: cloze deletion that is too wordyQ: Aldus invented desktop publishing in 1985 with PageMaker. Aldus had little competition for years, and so failed to improve. Then Denver-based ‚Ä¶ blew past. PageMaker, now owned by Adobe, remains No. 2&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Less optimum item: cloze deletion that is too wordy&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: Aldus invented desktop publishing in 1985 with PageMaker. Aldus had little competition for years, and so failed to improve. Then Denver-based ‚Ä¶ blew past. PageMaker, now owned by Adobe, remains No. 2&lt;p&gt;A: Quark&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Better item: fewer words will speed up learning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: Aldus invented desktop publishing in 1985 with PageMaker but failed to improve. Then ‚Ä¶ blew past (PageMaker remains No. 2)&lt;p&gt;A: Quark&lt;/p&gt;&lt;p&gt;Or better:&lt;/p&gt;&lt;p&gt;Q: Aldus invented desktop publishing with PageMaker but failed to improve. It was soon outdistanced by ‚Ä¶&lt;/p&gt;&lt;p&gt;A: Quark&lt;/p&gt;&lt;p&gt;Or better:&lt;/p&gt;&lt;p&gt;Q: PageMaker failed to improve and was outdistanced by ‚Ä¶&lt;/p&gt;&lt;p&gt;A: Quark&lt;/p&gt;&lt;p&gt;Or better:&lt;/p&gt;&lt;p&gt;Q: PageMaker lost ground to ‚Ä¶&lt;/p&gt;&lt;p&gt;A: Quark&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note that the loss of information content in this item is inconsequential. During repetition you are only supposed to learn the name: Quark. You should not hope that the trailing messages on the ownership of PageMaker and the year of its development will somehow trickle to your memory as a side effect. You should decide if the other pieces of information are important to you and if so, store them in separate items (perhaps reusing the above text, employing cloze deletion again and optimizing the wording in a new way). Otherwise the redundant information will only slow down your learning process!&lt;/p&gt;
    &lt;p&gt;13) Refer to other memories&lt;/p&gt;
    &lt;p&gt;Referring to other memories can place your item in a better context, simplify wording, and reduce interference. In the example below, using the words humble and supplicant helps the student focus on the word shamelessly and thus strengthen the correct semantics. Better focus helps eliminating interference. Secondly, the use of the words humble and supplicant makes it possible to avoid interference of cringing with these words themselves. Finally, the proposed wording is shorter and more specific. Naturally, the rules basics-to-details and do not learn what you do not understand require that the words humble and supplicant be learned beforehand (or at least at the same time)&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Item subject to strong interference&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: derog: adj: shamelessly conscious of one‚Äôs failings and asking in a begging way&lt;p&gt;A: cringing&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Item that uses interfering memories to amplify the correct meaning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: derog: adj: shamelessly humble and supplicant&lt;p&gt;A: cringing&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;14) Personalize and provide examples&lt;/p&gt;
    &lt;p&gt;One of the most effective ways of enhancing memories is to provide them with a link to your personal life. In the example below you will save time if you use a personal reference rather than trying to paint a picture that would aptly illustrate the question&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Item subject to strong interference&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: What is the name of a soft bed without arms or back?&lt;p&gt;A: divan&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Item that uses interfering memories to amplify the correct meaning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: What is the name of a soft bed without arms or back? (like the one at Robert‚Äôs parents)&lt;p&gt;A: divan&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;&lt;lb/&gt;If you remember exactly what kind of soft bed can be found in Robert‚Äôs parents‚Äô apartment you will save time by not having to dig exactly into the semantics of the definition and/or looking for an appropriate graphic illustration for the piece of furniture in question. Personalized examples are very resistant to interference and can greatly reduce your learning time&lt;/p&gt;
    &lt;p&gt;15) Rely on emotional states&lt;/p&gt;
    &lt;p&gt;If you can illustrate your items with examples that are vivid or even shocking, you are likely to enhance retrieval (as long as you do not overuse same tools and fall victim of interference!). Your items may assume bizarre form; however, as long as they are produced for your private consumption, the end justifies the means. Use objects that evoke very specific and strong emotions: love, sex, war, your late relative, object of your infatuation, Linda Tripp, Nelson Mandela, etc. It is well known that emotional states can facilitate recall; however, you should make sure that you are not deprived of the said emotional clues at the moment when you need to retrieve a given memory in a real-life situation&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Harder item&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: a light and joking conversation&lt;p&gt;A: banter&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Easier item&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: a light and joking conversation (e.g. Mandela and de Klerk in 1992)&lt;p&gt;A: banter&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;&lt;lb/&gt;If you have vivid and positive memories related to the meetings between Nelson Mandela and F.W. de Klerk, you are likely to quickly grasp the meaning of the definition of banter. Without the example you might struggle with interference from words such as badinage or even chat. There is no risk of irrelevant emotional state in this example as the state helps to define the semantics of the learned concept! A well-thought example can often reduce your learning time several times! I have recorded examples in which an item without an example was forgotten 20 times within one year, while the same item with a subtle interference-busting example was not forgotten even once in ten repetitions spread over five years. This is roughly equivalent to 25-fold saving in time in the period of 20 years! Such examples are not rare! They are most effectively handled with the all the preceding rules targeted on simplicity and against the interference&lt;/p&gt;
    &lt;p&gt;16) Context cues simplify wording&lt;/p&gt;
    &lt;p&gt;You can use categories in SuperMemo 2000/2002, provide different branches of knowledge with a different look (different template), use reference labels (Title, Author, Date, etc.) and clearly label subcategories (e.g. with strings such as chem for chemistry, math for mathematics, etc.). This will help you simplify the wording of your items as you will be relieved from the need to specify the context of your question. In the example below, the well-defined prefix bioch: saves you a lot of typing and a lot of reading while still making sure you do not confuse the abbreviation GRE with Graduate Record Examination. Note that in the recommended case, you process the item starting from the label bioch which puts your brain immediately in the right context. While processing the lesser optimum case, you will waste precious milliseconds on flashing the standard meaning of GRE and ‚Ä¶ what is worse ‚Ä¶ you will light up the wrong areas of your brain that will now perhaps be prone to interference!&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Wordy item can cause accidental lapses through interference&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: What does GRE stand for in biochemistry?&lt;p&gt;A: glucocorticoid response element&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Context-labeled items increase success rate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: bioch: GRE&lt;p&gt;A: glucocorticoid response element&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;17) Redundancy does not contradict minimum information principle&lt;/p&gt;
    &lt;p&gt;Redundancy in simple terms is more information than needed or duplicate information, etc. Redundancy does not have to contradict the minimum information principle and may even be welcome. The problem of redundancy is too wide for this short text. Here are some examples that are only to illustrate that minimum information principle cannot be understood as minimum number of characters or bits in your collections or even items:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;passive and active approach: if you learn a foreign language, e.g. Esperanto, you will often build word pairs such as phone-telefono, language-lingvo, hope-esperanto, etc. These pairs require active recall of the foreign word. Active recall does not, however, guarantee passive recognition and you may fail with telefono-phone, lingvo-language, or esperanto-hope. Adding new elements with swapped questions and answers may in some cases be redundant but it does not contradict the minimum information principle! Your items are still as simple as possible. You just get more of themIn SuperMemo 2000/2002, you can quickly generate swapped word-pair items with Duplicate (Ctrl+Alt+D) and Swap (Ctrl+Shift+S)&lt;/item&gt;
      &lt;item&gt;reasoning cues: you will often want to boost your reasoning ability by asking about a solution to the problem. Instead of just memorizing the answer you would like to quickly follow the reasoning steps (e.g. solve a simple mathematical equation) and generate the answer. In such a case, providing the hint on the reasoning steps in the answer will only serve helping you always follow the right path at repetitions&lt;/item&gt;
      &lt;item&gt;derivation steps: in more complex problems to solve, memorizing individual derivation steps is always highly recommended (e.g. solving complex mathematical problems). It is not cramming! It is making sure that the brain can always follow the fastest path while solving the problem. For more on boosting creativity and intelligence read: Roots of genius and creativity, as well as more specific: Derivation, reasoning and intelligence&lt;/item&gt;
      &lt;item&gt;multiple semantic representation: very often the same knowledge can be represented and viewed from different angles. Memorizing different representations of the same fact or rule is recommended in cases where a given memory is of high value. This will increase the expected recall rate (beyond that specified with the forgetting index)!&lt;/item&gt;
      &lt;item&gt;flexible repetition: if there are many valid responses to the same question make sure that your representation makes it possible to identify the equivalence and reward you with good grades by providing just one of the equivalent choices. For example, if you learn a language, it rarely make sense to learn all synonyms that meet a definition of a concept. It is more adequate to consider a single synonym as the sufficient answer (e.g. a mark made by ink spilt on sth = blot/blob/blotch)&lt;/item&gt;
      &lt;item&gt;more&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;18) Provide sources&lt;/p&gt;
    &lt;p&gt;Except for well-tested and proven knowledge (such as 2+2=4), it is highly recommended that you include sources from which you have gathered your knowledge. In real-life situation you will often be confronted with challenges to your knowledge. Sources can come to your rescue. You will also find that facts and figures differ depending on the source. You can really be surprised how frivolously reputable information agencies publish figures that are drastically different from other equally reputable sources. Without SuperMemo, those discrepancies are often difficult to notice: before you encounter the new fact, the old one is often long forgotten. With sources provided, you will be able to make more educated choices on which pieces of information are more reliable. Adding reliability labels may also be helpful (e.g. Watch out!, Other sources differ!, etc.). Sources should accompany your items but should not be part of the learned knowledge (unless it is critical for you to be able to recall the source whenever asked).&lt;/p&gt;
    &lt;p&gt;19) Provide date stamping&lt;/p&gt;
    &lt;p&gt;Knowledge can be relatively stable (basic math, anatomy, taxonomy, physical geography, etc.) and highly volatile (economic indicators, high-tech knowledge, personal statistics, etc.). It is important that you provide your items with time stamping or other tags indicating the degree of obsolescence. In case of statistical figures, you might stamp them with the year they have been collected. When learning software applications, it is enough you stamp the item with the software version. Once you have newer figures you can update your items. Unfortunately, in most cases you will have to re-memorize knowledge that became outdated. Date stamping is useful in editing and verifying your knowledge; however, you will rarely want to memorize stamping itself. If you would like to remember the changes of a given figure in time (e.g. GNP figures over a number of years), the date stamping becomes the learned knowledge itself.&lt;/p&gt;
    &lt;p&gt;20) Prioritize&lt;/p&gt;
    &lt;p&gt;You will always face far more knowledge that you will be able to master. That is why prioritizing is critical for building quality knowledge in the long-term. The way you prioritize will affect the way your knowledge slots in. This will also affect the speed of learning (e.g. see: learn basics first). There are many stages at which prioritizing will take place; only few are relevant to knowledge representation, but all are important:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Prioritizing sources ‚Äì there will always be a number of sources of your knowledge. If you are still at student years: these will most likely be books and notes pertaining to different subjects. Otherwise you will probably rely more on journals, Internet, TV, newspapers, encyclopedias, dictionaries, etc. It is always worth being aware what is the optimum proportion of time devoted to those varied sources. As you progress with learning, you will quickly develop a good sense of which learning slots bring better results and which might be extended at the cost of others&lt;/item&gt;
      &lt;item&gt;Extracting knowledge ‚Äì unless you are about to pass an important exam, it nearly never makes sense to memorize whole books or whole articles. You will need to extract those parts that are most likely to impact the quality of your knowledge. You can do it by (1) marking paragraphs in a book or journal, (2) pasting relevant web pages to SuperMemo, (3) pasting relevant passages to SuperMemo, (4) typing facts and figures directly to SuperMemo notes, etc. You will need some experience before you can accurately measure how much knowledge you can indeed transfer to your brain and what degree of detail you can feasibly master. Your best way to prioritize the flow of knowledge into your memory is to use incremental reading tools&lt;/item&gt;
      &lt;item&gt;Transferring knowledge to SuperMemo ‚Äì you may try to stick with the 20 rules of formulating knowledge at the moment of introducing your material to SuperMemo. However, you can also literally transfer your notes or import whole files and later use the mechanisms provided by SuperMemo to determine the order of processing the imported material. Probably the best criterion for choosing between formulating or just importing is the time needed for accurately formulating the item or items. If formulation requires more knowledge, more time, comparing with other sources, etc. you can just import. Otherwise, if you believe that formulating an accurate item is a matter of seconds, formulate it&lt;/item&gt;
      &lt;item&gt;Formulating items ‚Äì make sure that explanatory or optional components of the answer are placed in the parentheses so that your attention is focused on the most important part of the item. The parts in the parentheses can be read after the repetition to strengthen the memory in its context&lt;/item&gt;
      &lt;item&gt;Using forgetting index ‚Äì you can use the forgetting index to prioritize pending items. The sequence of repetitions will naturally be determined by SuperMemo; however, you can request higher retention level for items that are more important and lower retention level for items of lower priority&lt;/item&gt;
      &lt;item&gt;Learning ‚Äì the process of prioritizing does not end with the onset of repetitions. Here are the tools you can use to continue setting your priorities while the learning process is under way:&lt;list rend="ul"&gt;&lt;item&gt;Remember (Ctrl+M) ‚Äì re-memorize items of high priority that have changed or which are extremely important to your knowledge at a given moment. If you choose Ctrl+M you will be able to determine the next interval for the currently reviewed item (its repetition counter will be reset to zero). It is recommended that you always re-memorize items whose content has changed significantly&lt;/item&gt;&lt;item&gt;Reschedule (Ctrl+J) ‚Äì manually schedule the date of the next repetition&lt;/item&gt;&lt;item&gt;Execute repetition (Ctrl+Shift+R) ‚Äì manually execute a repetition even before the repetition‚Äôs due date (e.g. when reviewing particularly important material)&lt;/item&gt;&lt;item&gt;Forget (Ctrl+R)- remove the current item from the learning process and place it at the end of the pending queue&lt;/item&gt;&lt;item&gt;Dismiss (Ctrl+D) ‚Äì ignore the current item in the learning process altogether&lt;/item&gt;&lt;item&gt;Delete (Ctrl+Shift+Del) ‚Äì remove the current item from your collection&lt;/item&gt;&lt;item&gt;Change the forgetting index of memorized items or change the ordinal of pending items (Ctrl+Shift+P)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Summary&lt;/p&gt;
    &lt;p&gt;Here again are the twenty rules of formulating knowledge. You will notice that the first 16 rules revolve around making memories simple! Some of the rules strongly overlap. For example: do not learn if you do not understand is a form of applying the minimum information principle which again is a way of making things simple:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Do not learn if you do not understand&lt;/item&gt;
      &lt;item&gt;Learn before you memorize ‚Äì build the picture of the whole before you dismember it into simple items in SuperMemo. If the whole shows holes, review it again!&lt;/item&gt;
      &lt;item&gt;Build upon the basics ‚Äì never jump both feet into a complex manual because you may never see the end. Well remembered basics will help the remaining knowledge easily fit in&lt;/item&gt;
      &lt;item&gt;Stick to the minimum information principle ‚Äì if you continue forgetting an item, try to make it as simple as possible. If it does not help, see the remaining rules (cloze deletion, graphics, mnemonic techniques, converting sets into enumerations, etc.)&lt;/item&gt;
      &lt;item&gt;Cloze deletion is easy and effective ‚Äì completing a deleted word or phrase is not only an effective way of learning. Most of all, it greatly speeds up formulating knowledge and is highly recommended for beginners&lt;/item&gt;
      &lt;item&gt;Use imagery ‚Äì a picture is worth a thousand words&lt;/item&gt;
      &lt;item&gt;Use mnemonic techniques ‚Äì read about peg lists and mind maps. Study the books by Tony Buzan. Learn how to convert memories into funny pictures. You won‚Äôt have problems with phone numbers and complex figures&lt;/item&gt;
      &lt;item&gt;Graphic deletion is as good as cloze deletion ‚Äì obstructing parts of a picture is great for learning anatomy, geography and more&lt;/item&gt;
      &lt;item&gt;Avoid sets ‚Äì larger sets are virtually un-memorizable unless you convert them into enumerations!&lt;/item&gt;
      &lt;item&gt;Avoid enumerations ‚Äì enumerations are also hard to remember but can be dealt with using cloze deletion&lt;/item&gt;
      &lt;item&gt;Combat interference ‚Äì even the simplest items can be completely intractable if they are similar to other items. Use examples, context cues, vivid illustrations, refer to emotions, and to your personal life&lt;/item&gt;
      &lt;item&gt;Optimize wording ‚Äì like you reduce mathematical equations, you can reduce complex sentences into smart, compact and enjoyable maxims&lt;/item&gt;
      &lt;item&gt;Refer to other memories ‚Äì building memories on other memories generates a coherent and hermetic structure that forgetting is less likely to affect. Build upon the basics and use planned redundancy to fill in the gaps&lt;/item&gt;
      &lt;item&gt;Personalize and provide examples ‚Äì personalization might be the most effective way of building upon other memories. Your personal life is a gold mine of facts and events to refer to. As long as you build a collection for yourself, use personalization richly to build upon well established memories&lt;/item&gt;
      &lt;item&gt;Rely on emotional states ‚Äì emotions are related to memories. If you learn a fact in the sate of sadness, you are more likely to recall it if when you are sad. Some memories can induce emotions and help you employ this property of the brain in remembering&lt;/item&gt;
      &lt;item&gt;Context cues simplify wording ‚Äì providing context is a way of simplifying memories, building upon earlier knowledge and avoiding interference&lt;/item&gt;
      &lt;item&gt;Redundancy does not contradict minimum information principle ‚Äì some forms of redundancy are welcome. There is little harm in memorizing the same fact as viewed from different angles. Passive and active approach is particularly practicable in learning word-pairs. Memorizing derivation steps in problem solving is a way towards boosting your intellectual powers!&lt;/item&gt;
      &lt;item&gt;Provide sources ‚Äì sources help you manage the learning process, updating your knowledge, judging its reliability, or importance&lt;/item&gt;
      &lt;item&gt;Provide date stamping ‚Äì time stamping is useful for volatile knowledge that changes in time&lt;/item&gt;
      &lt;item&gt;Prioritize ‚Äì effective learning is all about prioritizing. In incremental reading you can start from badly formulated knowledge and improve its shape as you proceed with learning (in proportion to the cost of inappropriate formulation). If need be, you can review pieces of knowledge again, split it into parts, reformulate, reprioritize, or delete. See also: Incremental reading, Devouring knowledge, Flow of knowledge, Using tasklists&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45093022</guid></item><item><title>Cloudflare Radar: AI Insights</title><link>https://radar.cloudflare.com/ai-insights</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45093090</guid></item><item><title>Isolated(any)</title><link>https://nshipster.com/isolated-any/</link><description>&lt;doc fingerprint="3e129a13cc2d9639"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;@isolated(any)&lt;/head&gt;&lt;p&gt;Ahh, &lt;code&gt;@isolated(any)&lt;/code&gt;.
                It‚Äôs an attribute of contradictions.
                You might see it a lot, but it‚Äôs ok to ignore it.
                You don‚Äôt need to use it, but I think it should be used more.
                It must always take an argument, but that argument cannot vary.&lt;/p&gt;&lt;p&gt;Confusing? Definitely. But we‚Äôll get to it all.&lt;/p&gt;&lt;p&gt;To understand why &lt;code&gt;@isolated(any)&lt;/code&gt; was introduced,
                we need to take a look at async functions.&lt;/p&gt;&lt;code&gt;let respond&lt;/code&gt;&lt;p&gt;This is about as simple a function type as we can get. But, things start to get a little more interesting when we look at how a function like this is used. A variable with this type must always be invoked with &lt;code&gt;await&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;await respond&lt;/code&gt;&lt;p&gt;This, of course, makes sense. All async functions must be called with &lt;code&gt;await&lt;/code&gt;.
                But! Consider this:&lt;/p&gt;&lt;code&gt;let send&lt;/code&gt;&lt;p&gt;The explicit types are there to help make what‚Äôs going on clear. We first define a synchronous function that must run on the &lt;code&gt;Main&lt;/code&gt;.
              And then we assign that to a plain old,
              non-&lt;code&gt;Main&lt;/code&gt; async function.
            We‚Äôve changed so much that you might find it surprising this even compiles.&lt;/p&gt;&lt;p&gt;Remember what &lt;code&gt;await&lt;/code&gt; actually does. It allows the current task to suspend. That doesn‚Äôt just let the task wait for future work to complete. It also is an opportunity to change isolation. This makes async functions very flexible!&lt;/p&gt;&lt;p&gt;Just like a dispatcher doesn‚Äôt sit there doing nothing while waiting for the ambulance to arrive, a suspended task doesn‚Äôt block its thread. When the dispatcher puts you on hold to coordinate with the ambulance team, that‚Äôs the isolation switch - they‚Äôre transferring your request to a different department that specializes in that type of work.&lt;/p&gt;&lt;head rend="h2"&gt;But change to where, exactly?&lt;/head&gt;&lt;p&gt;Ok, so we know that async functions, because they must always be &lt;code&gt;await&lt;/code&gt;ed, gain a lot of flexibility. We are close, but have to go just a little further to find the motivation for this attribute.&lt;/p&gt;&lt;code&gt;func dispatch&lt;/code&gt;&lt;p&gt;We now have a function that accepts other functions as arguments. It‚Äôs possible to pass in lots of different kinds of functions to &lt;code&gt;dispatch&lt;/code&gt;. They could be async functions themselves, or even be synchronous. And they can be isolated to any actor. All thanks to the power of &lt;code&gt;await&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Except there‚Äôs a little problem now. Have a look at &lt;code&gt;dispatch&lt;/code&gt; on its own:&lt;/p&gt;&lt;code&gt;func dispatch&lt;/code&gt;&lt;p&gt;The type of &lt;code&gt;responder&lt;/code&gt; fully describes everything about this function,
        except for one thing.
        We have no way to know its isolation.
        That information is only available at callsites.
        The isolation is still present,
        so the right thing happens at runtime.
        It‚Äôs just not possible to inspect it statically or even programmatically.
        If you‚Äôve encountered type erasure before,
        this should seem familiar.
        The flexibility of &lt;code&gt;async&lt;/code&gt; has come with a price -
        a loss of information.&lt;/p&gt;&lt;p&gt;This is where &lt;code&gt;@isolated(any)&lt;/code&gt; comes in.&lt;/p&gt;&lt;head rend="h2"&gt;&lt;code&gt;@isolated(any)&lt;/code&gt;&lt;/head&gt; Using &lt;p&gt;We can change the definition of &lt;code&gt;dispatch&lt;/code&gt; to fix this.&lt;/p&gt;&lt;code&gt;func dispatch&lt;/code&gt;
    &lt;p&gt;When you apply &lt;code&gt;@isolated(any)&lt;/code&gt; to a function type, it does two things. Most importantly, it gives you access to a special &lt;code&gt;isolation&lt;/code&gt; property. You can use this property to inspect the isolation of the function. The isolation could be an actor. Or it could be non-isolated. This is expressible in the type system with &lt;code&gt;(any Actor)?&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Functions with properties felt really strange to me at first. But, after thinking for a minute, it became quite natural. Why not? It‚Äôs just a type like any other. In fact, we can simulate how this all works with another feature: &lt;code&gt;call&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;struct Isolated&lt;/code&gt;
&lt;p&gt;This analogy is certainly not perfect, but it‚Äôs close enough that it might help.&lt;/p&gt;&lt;p&gt;There is one other subtle change that &lt;code&gt;@isolated(any)&lt;/code&gt; makes to a function
  that you should be aware of.
  Its whole purpose is to capture the isolation of a function.
  Since that could be anything,
  callsites need an opportunity to switch.
  And that means an &lt;code&gt;@isolated(any)&lt;/code&gt; function must be called with an &lt;code&gt;await&lt;/code&gt; ‚Äî
  even if it isn‚Äôt itself explicitly async.&lt;/p&gt;&lt;code&gt;func dispatch&lt;/code&gt;
&lt;p&gt;This makes synchronous functions marked with &lt;code&gt;@isolated(any)&lt;/code&gt; a little strange.
  They still must be called with &lt;code&gt;await&lt;/code&gt;,
  yet they aren‚Äôt allowed to suspend internally?&lt;/p&gt;&lt;p&gt;As it turns out, there are some valid (if rare) situations where such an arrangement can make sense. But adding this kind of constraint to your API should at least merit some extra documentation.&lt;/p&gt;&lt;head rend="h2"&gt;How @isolated(any) Affects Callers&lt;/head&gt;&lt;p&gt;All of the task creation APIs ‚Äî &lt;code&gt;Task&lt;/code&gt; initializers and &lt;code&gt;Task&lt;/code&gt; ‚Äî
make use of &lt;code&gt;@isolated(any)&lt;/code&gt;.
These are used a lot
and are usually encountered very early on when learning about concurrency.
So, it‚Äôs completely natural to run into this attribute and think:&lt;/p&gt;&lt;p&gt;‚ÄúUgh another thing to understand!‚Äù&lt;/p&gt;&lt;p&gt;It‚Äôs reasonable because the components of a function type dictate how it can be used. They are all essential qualities for API consumers. They are the interface.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Parameters&lt;/item&gt;&lt;item&gt;Return value&lt;/item&gt;&lt;item&gt;Does it throw?&lt;/item&gt;&lt;item&gt;Is it async?&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This is not an exhaustive list, but what‚Äôs important is all of these are things callers must care about. Except for &lt;code&gt;@isolated(any)&lt;/code&gt;, which is the opposite.
  It doesn‚Äôt affect callers at all.&lt;/p&gt;&lt;p&gt;This, I think, is the root of a lot of confusion around &lt;code&gt;@isolated(any)&lt;/code&gt;.
  Unlike other qualities of a function,
  this attribute is used to capture information for the API producer.&lt;/p&gt;&lt;p&gt;I‚Äôm so close to saying ‚Äúyou can and should just ignore &lt;code&gt;@isolated(any)&lt;/code&gt;‚Äú.
  But I just cannot quite go that far,
  because there is one situation you should be aware of.&lt;/p&gt;&lt;head rend="h2"&gt;Scheduling&lt;/head&gt;&lt;p&gt;To help understand when you should be thinking about using &lt;code&gt;@isolated(any)&lt;/code&gt;,
  I‚Äôm going to quote
  the proposal:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;This allows the API to make more intelligent scheduling decisions about the function.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I‚Äôve highlighted ‚Äúintelligent scheduling‚Äù, because this is the key component of &lt;code&gt;@isolated(any)&lt;/code&gt;.
  The attribute gives you access to the isolation of a function argument.
  But what would you use that for?&lt;/p&gt;&lt;p&gt;Did you know that, before Swift 6.0, the ordering of the following code was undefined?&lt;/p&gt;&lt;code&gt;@Main&lt;/code&gt;
&lt;p&gt;Ordering turns out to be a very tricky topic when working with unstructured tasks. And while it will always require care, Swift 6.0 did improve the situation. We now have some stronger guarantees about scheduling work on the &lt;code&gt;Main&lt;/code&gt;,
and &lt;code&gt;@isolated(any)&lt;/code&gt; was needed to make that possible.&lt;/p&gt;&lt;p&gt;Take a look at this:&lt;/p&gt;&lt;code&gt;@Main&lt;/code&gt;
&lt;p&gt;These are three ways to achieve the same goal. But, there is a subtle difference in how the last form is scheduled. &lt;code&gt;Task&lt;/code&gt; takes an &lt;code&gt;@isolated(any)&lt;/code&gt; function
  so it can look at its isolation
  and synchronously submit it to an actor.
  This is how ordering can be preserved!
  But, it cannot do that in the last case.
  That closure passed into &lt;code&gt;Task&lt;/code&gt; isn‚Äôt actually itself &lt;code&gt;Main&lt;/code&gt; ‚Äî
it has inherited nonisolated from the enclosing function.&lt;/p&gt;&lt;p&gt;I think it might help to translate this into GCD.&lt;/p&gt;&lt;code&gt;func dispatch&lt;/code&gt;
&lt;p&gt;Look really closely at that last one! What we are doing there is introducing a new async closure that then calls our &lt;code&gt;Main&lt;/code&gt; function.
There are two steps.
This doesn‚Äôt always matter,
but it certainly could.
And if you need to precisely schedule asynchronous work,
&lt;code&gt;@isolated(any)&lt;/code&gt; can help.&lt;/p&gt;&lt;head rend="h2"&gt;isolated(all)&lt;/head&gt;&lt;p&gt;All this talk about &lt;code&gt;@isolated(any)&lt;/code&gt; got me thinking‚Ä¶&lt;/p&gt;&lt;p&gt;It‚Äôs kinda strange that only some functions get to have this &lt;code&gt;isolation&lt;/code&gt; property.
  It would certainly feel more consistent to me if all functions had it.
  In fact, I think we can go further.
  I can imagine a future where an explicit &lt;code&gt;@isolated(any)&lt;/code&gt;
  isn‚Äôt even necessary for async functions.
  As far as I can tell, there is no downside.&lt;/p&gt;&lt;p&gt;And a little less syntactic noise would be nice. Perhaps one day!&lt;/p&gt;&lt;head rend="h2"&gt;isolated(some)&lt;/head&gt;&lt;p&gt;We do have to talk about that &lt;code&gt;any&lt;/code&gt;.
  It‚Äôs surprising that this attribute requires an argument,
  yet permits only one possible value.
  The reason here comes down to future considerations.&lt;/p&gt;&lt;p&gt;The concrete actor type that this &lt;code&gt;isolation&lt;/code&gt; property returns
  is always &lt;code&gt;(any Actor)?&lt;/code&gt;.
  This is the most generic type for isolation and matches the &lt;code&gt;#isolation&lt;/code&gt; macro.
  Today, there is no way to constrain a function to only specific actor types,
  such as &lt;code&gt;@isolated(My&lt;/code&gt;.
The &lt;code&gt;any&lt;/code&gt; keyword here was chosen to mirror how protocols handle this.
But accepting an argument leaves the door open
to more sophisticated features in the future.&lt;/p&gt;&lt;p&gt;And that really fits the spirit of &lt;code&gt;@isolated(any)&lt;/code&gt;.
  Doing a little work now in exchange for flexibility down the road.&lt;/p&gt;&lt;p&gt;Because you‚Äôll see it in many foundational concurrency APIs, it‚Äôs very natural to feel like you must understand &lt;code&gt;@isolated(any)&lt;/code&gt;.
  I‚Äôm 100% behind technical curiosity!
  In this case, however, it is not required.
  For the most part, you can just ignore this attribute.
  You will rarely, if ever, need to use it yourself.&lt;/p&gt;&lt;p&gt;But if you ever find yourself capturing isolated functions and passing them along to other APIs that use &lt;code&gt;@isolated(any)&lt;/code&gt;,
  you should consider adopting it.
  It could prove useful.
  It‚Äôs even a source-compatible change
  to add or remove this attribute from an async function.&lt;/p&gt;&lt;p&gt;So there you have it.&lt;/p&gt;&lt;p&gt;As with many parts of the concurrency system, there‚Äôs a surprising depth to &lt;code&gt;@isolated(any)&lt;/code&gt;.
  Thankfully, from a practical perspective,
  we can enjoy the ordering guarantees of task creation
  that it enables without needing to master it.
  And one less thing on this journey is most welcome.&lt;/p&gt;&lt;p&gt;Isolated maybe, but never alone.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45093590</guid></item><item><title>Cloudflare Search Engine Market Share 2025Q2</title><link>https://radar.cloudflare.com/reports/search-engine-market-share-2025-q2</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45093693</guid></item></channel></rss>