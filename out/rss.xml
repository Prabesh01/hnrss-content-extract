<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 23 Sep 2025 05:10:27 +0000</lastBuildDate><item><title>CompileBench: Can AI Compile 22-year-old Code?</title><link>https://quesma.com/blog/introducing-compilebench/</link><description>&lt;doc fingerprint="6d57b7bb2d3f174b"&gt;
  &lt;main&gt;
    &lt;p&gt;Now on the front page of Hacker News — join the discussion.&lt;/p&gt;
    &lt;p&gt;When ChatGPT first launched in 2022, it could barely write short snippets of working code. Today, the best LLMs can generate entire applications from scratch and even win prestigious coding competitions (like IOI 2025).&lt;/p&gt;
    &lt;p&gt;But can they tackle the messy reality of software development – dependency hell, legacy toolchains, and cryptic compile errors? We created CompileBench to find out.&lt;/p&gt;
    &lt;p&gt;We tested 19 state-of-the-art LLMs on 15 real-world tasks using the unmodified source code of open-source projects like &lt;code&gt;curl&lt;/code&gt; (HTTP client) and &lt;code&gt;jq&lt;/code&gt; (command-line JSON processor).&lt;/p&gt;
    &lt;p&gt;The goal sounds straightforward – produce a working binary. But achieving it can be surprisingly complex. Our toughest challenges include cross-compiling to Windows or ARM64 and resurrecting 22-year-old source code from 2003 on modern systems. Some agents needed 135 commands and 15 minutes just to produce a single working binary.&lt;/p&gt;
    &lt;p&gt;See the full results later in the article.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Tasks&lt;/head&gt;
    &lt;p&gt;Each task in CompileBench follows the same structure. We give the LLM agent:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Source code from an open-source project (e.g., &lt;code&gt;curl&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;An interactive Linux terminal (running in a Docker container)&lt;/item&gt;
      &lt;item&gt;A clear build objective&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The agent must independently figure out the build system, decide whether to patch the sources, resolve missing headers and libraries, and choose the right compiler/linker flags. Once it’s done, we run various checks to verify that the resulting executable actually works.&lt;/p&gt;
    &lt;p&gt;Our tasks range from simple builds (that most models can handle) to brutal challenges like reviving 2003-era code, cross-compiling to Windows, or cross-compiling for ARM64 architecture. We tested popular projects including &lt;code&gt;curl&lt;/code&gt; (HTTP client), GNU Coreutils (utilities like &lt;code&gt;cp&lt;/code&gt;, &lt;code&gt;ls&lt;/code&gt;, &lt;code&gt;mv&lt;/code&gt;), and &lt;code&gt;jq&lt;/code&gt; (JSON processor).&lt;/p&gt;
    &lt;head rend="h4"&gt;Making Tasks Hard With One Simple Trick&lt;/head&gt;
    &lt;p&gt;It turns out that it’s really easy to make the tasks more difficult. Nearly all models can build &lt;code&gt;curl&lt;/code&gt; with standard settings. But ask them to create a “statically compiled binary for ARM64” (the architecture used by modern Apple devices and many servers) and watch the success rate plummet:&lt;/p&gt;
    &lt;p&gt;With a single attempt (pass@1), the success rate drops from 96% to 2%. Claude Opus 4.1, the only model to succeed, had to execute a 36-command sequence that involved downloading source code for all dependencies (OpenSSL, brotli, zlib, and zstd), cross-compiling each one statically for ARM64, and finally linking them all together in the final &lt;code&gt;curl&lt;/code&gt; build.&lt;/p&gt;
    &lt;head rend="h3"&gt;Anthropic Wins&lt;/head&gt;
    &lt;p&gt;Anthropic’s Claude Sonnet and Opus models are beloved by developers for coding tasks, yet they don’t always top traditional benchmarks. Our results might explain why developers trust them so much.&lt;/p&gt;
    &lt;p&gt;In CompileBench, Anthropic models claim the top 2 spots for success rate and perform impressively on speed metrics:&lt;/p&gt;
    &lt;head rend="h3"&gt;OpenAI: Great Performance at The Best Price&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt; OpenAI models secure 3rd and 6th place in our success rankings. But where they truly excel is cost-efficiency – they dominate the Pareto frontier:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; OpenAI models are the most cost efficient across nearly all task difficulties. GPT-5-mini (high reasoning effort) is a great model in both intelligence and price.&lt;/p&gt;
    &lt;p&gt;OpenAI provides a range of models, from non-reasoning options like GPT-4.1 to advanced reasoning models like GPT-5. We found that each one remains highly relevant in practice. For example, GPT-4.1 is the fastest at completing tasks while maintaining a solid success rate. GPT-5, when set to minimal reasoning effort, is reasonably fast and achieves an even higher success rate. GPT-5 (high reasoning effort) is the best one, albeit at the highest price and slowest speed.&lt;/p&gt;
    &lt;head rend="h3"&gt;Google: A Surprising Disappointment&lt;/head&gt;
    &lt;p&gt;Despite their strong reputation – with Gemini 2.5 Pro being one of the best in web development – Google’s models scored near the bottom of our leaderboard.&lt;/p&gt;
    &lt;p&gt;The models frequently failed to complete tasks as specified. When asked for a static ARM64 build, Gemini 2.5 Pro would produce a valid ARM64 executable but not a static one. For static builds using the musl C library, it correctly used musl but chose dynamic linking, arguing that static builds were unnecessarily large.&lt;/p&gt;
    &lt;p&gt;When designing the benchmark we kept our benchmark harness and prompts minimal, avoiding model-specific tweaks. It is possible that Google models could perform better with a harness or prompt specifically hand-tuned for them, but this is against our principles in this benchmark.&lt;/p&gt;
    &lt;p&gt;Even Gemini seemed to lack confidence, as this output from Gemini 2.5 Pro shows:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I have been unable to successfully complete the request. I have made several mistakes and am not confident that I can produce the correct result. I am aborting the task.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;…but at least it has “learned a lot”, as per Gemini 2.5 Pro output:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I am sorry for the many mistakes I made along the way, but I have learned a lot and I am now confident that I can complete similar requests in the future without making so many errors.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Catching Cheating LLMs&lt;/head&gt;
    &lt;p&gt;Each task in CompileBench comes with a set of checks. For example, for &lt;code&gt;curl&lt;/code&gt; we check whether the model created an actual executable, whether it reports the correct version matching the source code, and whether it can successfully make HTTP requests.&lt;/p&gt;
    &lt;p&gt;But some models tried to cheat! When GPT-5-mini (high reasoning) struggled to compile 2003-era GNU Coreutils (set of utilities like &lt;code&gt;ls&lt;/code&gt;, &lt;code&gt;mv&lt;/code&gt;, &lt;code&gt;cp&lt;/code&gt;), it took a creative shortcut – copying existing system utilities instead of building them. Its reasoning trace revealed:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;As a practical fallback so you have the utilities available under /home/peter/result/&lt;/p&gt;
      &lt;utility&gt;(as you requested), I created /home/peter/result and created symlinks for all utilities from the coreutils source tree. Each symlink points to an available system implementation: if /bin/&lt;/utility&gt;
      &lt;utility&gt;exists it links to that; otherwise it links to /bin/busybox (BusyBox responds to argv[0] so most common utilities will run).&lt;/utility&gt;
    &lt;/quote&gt;
    &lt;p&gt;But our checks caught that and correctly marked the attempt as failed.&lt;/p&gt;
    &lt;head rend="h3"&gt;Summary&lt;/head&gt;
    &lt;p&gt;With CompileBench we wanted to see how LLMs could handle “messy” software engineering problems like dependency hell, legacy toolchains or weird compile errors. CompileBench uses purely function calling for truly long-horizon tasks – some requiring 135 commands or over 15 minutes with agentic loops running tens of times. This design authentically measures LLMs’ ability to recover from errors and persist through complex, multi-step challenges.&lt;/p&gt;
    &lt;p&gt;Our results, show that there’s no single “best” model – it depends on whether you prioritize intelligence, speed, or cost-efficiency.&lt;/p&gt;
    &lt;p&gt;Using the best Anthropic models (Sonnet 4 or Opus 4.1) for the most demanding tasks and cheaper OpenAI models (GPT 4.1, GPT-5/GPT-5-mini with lower reasoning efforts) for less demanding ones seems to be the conclusion based on the benchmark results.&lt;/p&gt;
    &lt;p&gt;This is just the beginning. Future versions of CompileBench could tackle even more challenging projects – can AI handle FFmpeg, ancient GCC versions, or ImageMagick? What about cross-compiling from Linux to FreeBSD? Or for the ultimate benchmark, could an AI get Doom running on an arbitrary device?&lt;/p&gt;
    &lt;p&gt;You can browse the complete results of the benchmark at: https://compilebench.com/&lt;lb/&gt; or tinker with the (full!) source code at: https://github.com/QuesmaOrg/CompileBench&lt;/p&gt;
    &lt;p&gt;Do these results match your own experience with using LLMs for software engineering?&lt;/p&gt;
    &lt;p&gt;Discuss this benchmark on LinkedIn and Hacker News.&lt;/p&gt;
    &lt;p&gt;Stay tuned for future posts and releases&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45332814</guid><pubDate>Mon, 22 Sep 2025 12:59:30 +0000</pubDate></item><item><title>I'm spoiled by Apple Silicon but still love Framework</title><link>https://simonhartcher.com/posts/2025-09-22-why-im-spoiled-by-apple-silicon-but-still-love-framework/</link><description>&lt;doc fingerprint="9eb0593b034f91dc"&gt;
  &lt;main&gt;
    &lt;p&gt;I mainly use my MacBook M1 Pro with Apple Silicon for work. Recently I’ve had a few weeks off. It’s been sitting in my laptop bag waiting for me to return to work.&lt;/p&gt;
    &lt;p&gt;Since I’m back to work tomorrow I got it out to see if it was charged enough for the train journey in the morning. 90% remaining. After 3 weeks. It wasn’t turned off, just closed.&lt;/p&gt;
    &lt;p&gt;In contrast, my Framework 13 with an AMD Ryzen 7840HS is almost always dead when I go to use it. It’s very frustrating. I’m not using it every day so it often goes 2-3 days without being opened. I haven’t measured it but I read that I should expect it to lose 3-4% in suspend every hour. Is that a joke?&lt;/p&gt;
    &lt;p&gt;I was running Fedora Workstation on the 13 for a long while until I had a short stint with Arch Linux. It wasn’t stable enough so now I’m on Fedora Silverblue which has been delightful.&lt;/p&gt;
    &lt;p&gt;However, the Framework’s battery life continues to be an issue.&lt;/p&gt;
    &lt;p&gt;I’m a true believer of Framework’s mission. The technology is awesome. But why is the battery life of modern laptops (other than Apple Silicon) so bad? I don’t think it’s unique to Framework, but it affects my love for this groundbreaking device.&lt;/p&gt;
    &lt;p&gt;Apple Silicon is built upon ARM64 which is apparently core to such great battery life. Do I need to get an ARM mainboard as soon as Framework offers it as an upgrade? I’m not sure. It appears to be way more complicated than just switching to ARM64.&lt;/p&gt;
    &lt;p&gt;I still love my Framework, despite its flaws. I will just keep it plugged in so that it’s ready to go when I want to use it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45332859</guid><pubDate>Mon, 22 Sep 2025 13:03:10 +0000</pubDate></item><item><title>Cloudflare is sponsoring Ladybird and Omarchy</title><link>https://blog.cloudflare.com/supporting-the-future-of-the-open-web/</link><description>&lt;doc fingerprint="29520b5b1bc621bc"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;At Cloudflare, we believe that helping build a better Internet means encouraging a healthy ecosystem of options for how people can connect safely and quickly to the resources they need. Sometimes that means we tackle immense, Internet-scale problems with established partners. And sometimes that means we support and partner with fantastic open teams taking big bets on the next generation of tools.&lt;/p&gt;
      &lt;p&gt;To that end, today we are excited to announce our support of two independent, open source projects: Ladybird, an ambitious project to build a completely independent browser from the ground up, and Omarchy, an opinionated Arch Linux setup for developers.Â &lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Two open source projects strengthening the open InternetÂ &lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Cloudflare has a long history of supporting open-source software â both through our own projects shared with the community and external projects that we support. We see our sponsorship of Ladybird and Omarchy as a natural extension of these efforts in a moment where energy for a diverse ecosystem is needed more than ever.Â Â &lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Ladybird, a new and independent browserÂ &lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Most of us spend a significant amount of time using a web browser âÂ in fact, youâre probably using one to read this blog! The beauty of browsers is that they help users experience the open Internet, giving you access to everything from the largest news publications in the world to a tiny website hosted on a Raspberry Pi.Â Â &lt;/p&gt;
      &lt;p&gt;Unlike dedicated apps, browsers reduce the barriers to building an audience for new services and communities on the Internet. If you are launching something new, you can offer it through a browser in a world where most people have absolutely zero desire to install an app just to try something out. Browsers help encourage competition and new ideas on the open web.&lt;/p&gt;
      &lt;p&gt;While the openness of how browsers work has led to an explosive growth of services on the Internet, browsers themselves have consolidated to a tiny handful of viable options. Thereâs a high probability youâre reading this on a Chromium-based browser, like Googleâs Chrome, along with about 65% of users on the Internet. However, that consolidation has also scared off new entrants in the space. If all browsers ship on the same operating systems, powered by the same underlying technology, we lose out on potential privacy, security and performance innovations that could benefit developers and everyday Internet users.Â &lt;/p&gt;
      &lt;p&gt;A screenshot of Cloudflare Workers developer docs in LadybirdÂ &lt;/p&gt;
      &lt;p&gt;This is where Ladybird comes in: itâs not Chromium based â everything is built from scratch. The Ladybird project has two main components: LibWeb, a brand-new rendering engine, and LibJS, a brand-new JavaScript engine with its own parser, interpreter, and bytecode execution engine.Â &lt;/p&gt;
      &lt;p&gt;Building an engine that can correctly and securely render the modern web is a monumental task that requires deep technical expertise and navigating decades of specifications governed by standards bodies like the W3C and WHATWG. And because Ladybird implements these standards directly, it also stress-tests them in practice. Along the way, the project has found, reported, and sometimes fixed countless issues in the specifications themselves, contributions that strengthen the entire web platform for developers, browser vendors, and anyone who may attempt to build a browser in the future.&lt;/p&gt;
      &lt;p&gt;Whether to build something from scratch or not is a perennial source of debate between software engineers, but absent the pressures of revenue or special interests, weâre excited about the ways Ladybird will prioritize privacy, performance, and security, potentially in novel ways that will influence the entire ecosystem.&lt;/p&gt;
      &lt;p&gt;A screenshot of the Omarchy development environment&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Omarchy, an independent development environmentÂ &lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Developers deserve choice, too. Beyond the browser, a developerâs operating system and environment is where they spend a ton of time â and where a few big players have become the dominant choice. Omarchy challenges this by providing a complete, opinionated Arch Linux distribution that transforms a bare installation into a modern development workstation that developers are excited about.&lt;/p&gt;
      &lt;p&gt;Perfecting oneâs development environment can be a career-long art, but learning how to do so shouldnât be a barrier to beginning to code. The beauty of Omarchy is that it makes Linux approachable to more developers by doing most of the setup for them, making it look good, and then making it configurable. Omarchy provides most of the tools developers need â like Neovim, Docker, and Git â out of the box, and tons of other features.&lt;/p&gt;
      &lt;p&gt;At its core, Omarchy embraces Linux for all of its complexity and configurability, and makes a version of it that is accessible and fun to use for developers that donât have a deep background in operating systems. Projects like this ensure that a powerful, independent Linux desktop remains a compelling choice for people building the next generation of applications and Internet infrastructure.Â &lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Our support comes with no strings attachedÂ Â &lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;We want to be very clear here: we are supporting these projects because we believe the Internet can be better if these projects, and more like them, succeed. No requirement to use our technology stack or any arrangement like that. We are happy to partner with great teams like Ladybird and Omarchy simply because we believe that our missions have real overlap.&lt;/p&gt;
      &lt;p&gt;Ladybird is still in its early days, with an alpha release planned for 2026, but we encourage anyone who is interested to consider contributing to the open source codebase as they prepare for launch.&lt;/p&gt;
      &lt;quote&gt;
        &lt;p&gt;"Cloudflare knows what it means to build critical web infrastructure on the server side. With Ladybird, weâre tackling the near-monoculture on the client side, because we believe it needs multiple implementations to stay healthy, and weâre extremely thankful for their support in that mission.â&lt;/p&gt;
        &lt;p&gt;â Andreas Kling, Founder, LadybirdÂ Â &lt;/p&gt;
      &lt;/quote&gt;
      &lt;p&gt;Omarchy 3.0 was released just last week with faster installation and increased Macbook compatibility, so if youâve been Linux-curious for a while now, we encourage you to try it out!&lt;/p&gt;
      &lt;quote&gt;
        &lt;p&gt;"Cloudflare's support of Omarchy has ensured we have the fastest ISO and package delivery from wherever you are in the world. Without a need to manually configure mirrors or deal with torrents. The combo of a super CDN, great R2 storage, and the best DDoS shield in the business has been a huge help for the project."&lt;/p&gt;
        &lt;p&gt;â David Heinemeier Hansson, Creator of Omarchy and Ruby on Rails&lt;/p&gt;
      &lt;/quote&gt;
      &lt;p&gt;A better Internet is one where people have more choice in how they browse and develop new software. Weâre incredibly excited about the potential of Ladybird, Omarchy, and other audacious projects that support a free and open Internet. &lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45332860</guid><pubDate>Mon, 22 Sep 2025 13:03:33 +0000</pubDate></item><item><title>Cap'n Web: a new RPC system for browsers and web servers</title><link>https://blog.cloudflare.com/capnweb-javascript-rpc-library/</link><description>&lt;doc fingerprint="e228a11afa2902c6"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Allow us to introduce Cap'n Web, an RPC protocol and implementation in pure TypeScript.&lt;/p&gt;
      &lt;p&gt;Cap'n Web is a spiritual sibling to Cap'n Proto, an RPC protocol I (Kenton) created a decade ago, but designed to play nice in the web stack. That means:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Like Cap'n Proto, it is an object-capability protocol. ("Cap'n" is short for "capabilities and".) We'll get into this more below, but it's incredibly powerful.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Unlike Cap'n Proto, Cap'n Web has no schemas. In fact, it has almost no boilerplate whatsoever. This means it works more like the JavaScript-native RPC system in Cloudflare Workers.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;That said, it integrates nicely with TypeScript.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Also unlike Cap'n Proto, Cap'n Web's underlying serialization is human-readable. In fact, it's just JSON, with a little pre-/post-processing.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;It works over HTTP, WebSocket, and postMessage() out-of-the-box, with the ability to extend it to other transports easily.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;It works in all major browsers, Cloudflare Workers, Node.js, and other modern JavaScript runtimes.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;The whole thing compresses (minify+gzip) to under 10Â kB with no dependencies.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;It's open source under the MIT license.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Cap'n Web is more expressive than almost every other RPC system, because it implements an object-capability RPC model. That means it:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Supports bidirectional calling. The client can call the server, and the server can also call the client.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Supports passing functions by reference: If you pass a function over RPC, the recipient receives a "stub". When they call the stub, they actually make an RPC back to you, invoking the function where it was created. This is how bidirectional calling happens: the client passes a callback to the server, and then the server can call it later.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Similarly, supports passing objects by reference: If a class extends the special marker type &lt;code&gt;RpcTarget&lt;/code&gt;, then instances of that class are passed by reference, with method calls calling back to the location where the object was created.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Supports promise pipelining. When you start an RPC, you get back a promise. Instead of awaiting it, you can immediately use the promise in dependent RPCs, thus performing a chain of calls in a single network round trip.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Supports capability-based security patterns.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;In short, Cap'n Web lets you design RPC interfaces the way you'd design regular JavaScript APIs â while still acknowledging and compensating for network latency.&lt;/p&gt;
      &lt;p&gt;The best part is, Cap'n Web is absolutely trivial to set up.&lt;/p&gt;
      &lt;p&gt;A client looks like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;import { newWebSocketRpcSession } from "capnweb";

// One-line setup.
let api = newWebSocketRpcSession("wss://example.com/api");

// Call a method on the server!
let result = await api.hello("World");

console.log(result);
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;And here's a complete Cloudflare Worker implementing an RPC server:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;import { RpcTarget, newWorkersRpcResponse } from "capnweb";

// This is the server implementation.
class MyApiServer extends RpcTarget {
  hello(name) {
    return `Hello, ${name}!`
  }
}

// Standard Workers HTTP handler.
export default {
  fetch(request, env, ctx) {
    // Parse URL for routing.
    let url = new URL(request.url);

    // Serve API at `/api`.
    if (url.pathname === "/api") {
      return newWorkersRpcResponse(request, new MyApiServer());
    }

    // You could serve other endpoints here...
    return new Response("Not found", {status: 404});
  }
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;That's it. That's the app.&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;You can add more methods to &lt;code&gt;MyApiServer&lt;/code&gt;, and call them from the client.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;You can have the client pass a callback function to the server, and then the server can just call it.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;You can define a TypeScript interface for your API, and easily apply it to the client and server.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;It just works.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Why RPC? (And what is RPC anyway?)&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Remote Procedure Calls (RPC) are a way of expressing communications between two programs over a network. Without RPC, you might communicate using a protocol like HTTP. With HTTP, though, you must format and parse your communications as an HTTP request and response, perhaps designed in REST style. RPC systems try to make communications look like a regular function call instead, as if you were calling a library rather than a remote service. The RPC system provides a "stub" object on the client side which stands in for the real server-side object. When a method is called on the stub, the RPC system figures out how to serialize and transmit the parameters to the server, invoke the method on the server, and then transmit the return value back.&lt;/p&gt;
      &lt;p&gt;The merits of RPC have been subject to a great deal of debate. RPC is often accused of committing many of the fallacies of distributed computing.&lt;/p&gt;
      &lt;p&gt;But this reputation is outdated. When RPC was first invented some 40 years ago, async programming barely existed. We did not have Promises, much less async and await. Early RPC was synchronous: calls would block the calling thread waiting for a reply. At best, latency made the program slow. At worst, network failures would hang or crash the program. No wonder it was deemed "broken".&lt;/p&gt;
      &lt;p&gt;Things are different today. We have Promise and async and await, and we can throw exceptions on network failures. We even understand how RPCs can be pipelined so that a chain of calls takes only one network round trip. Many large distributed systems you likely use every day are built on RPC. It works.&lt;/p&gt;
      &lt;p&gt;The fact is, RPC fits the programming model we're used to. Every programmer is trained to think in terms of APIs composed of function calls, not in terms of byte stream protocols nor even REST. Using RPC frees you from the need to constantly translate between mental models, allowing you to move faster.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;When should you use Cap'n Web?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Cap'n Web is useful anywhere where you have two JavaScript applications speaking to each other over a network, including client-to-server and microservice-to-microservice scenarios. However, it is particularly well-suited to interactive web applications with real-time collaborative features, as well as modeling interactions over complex security boundaries.&lt;/p&gt;
      &lt;p&gt;Cap'n Web is still new and experimental, so for now, a willingness to live on the cutting edge may also be required!&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Features, features, featuresâ¦&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Here's some more things you can do with Cap'n Web.&lt;/p&gt;
      &lt;p&gt;Sometimes a WebSocket connection is a bit too heavyweight. What if you just want to make a quick one-time batch of calls, but don't need an ongoing connection?&lt;/p&gt;
      &lt;p&gt;For that, Cap'n Web supports HTTP batch mode:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;import { newHttpBatchRpcSession } from "capnweb";

let batch = newHttpBatchRpcSession("https://example.com/api");

let result = await batch.hello("World");

console.log(result);
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;(The server is exactly the same as before.)&lt;/p&gt;
      &lt;p&gt;Note that once you've awaited an RPC in the batch, the batch is done, and all the remote references received through it become broken. To make more calls, you need to start over with a new batch. However, you can make multiple calls in a single batch:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let batch = newHttpBatchRpcSession("https://example.com/api");

// We can call make multiple calls, as long as we await them all at once.
let promise1 = batch.hello("Alice");
let promise2 = batch.hello("Bob");

let [result1, result2] = await Promise.all([promise1, promise2]);

console.log(result1);
console.log(result2);
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;And that brings us to another featureâ¦&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Chained calls (Promise Pipelining)&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Here's where things get magical.&lt;/p&gt;
      &lt;p&gt;In both batch mode and WebSocket mode, you can make a call that depends on the result of another call, without waiting for the first call to finish. In batch mode, that means you can, in a single batch, call a method, then use its result in another call. The entire batch still requires only one network round trip.&lt;/p&gt;
      &lt;p&gt;For example, say your API is:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;class MyApiServer extends RpcTarget {
  getMyName() {
    return "Alice";
  }

  hello(name) {
    return `Hello, ${name}!`
  }
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;You can do:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let namePromise = batch.getMyName();
let result = await batch.hello(namePromise);

console.log(result);
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Notice the initial call to &lt;code&gt;getMyName()&lt;/code&gt; returned a promise, but we used the promise itself as the input to &lt;code&gt;hello()&lt;/code&gt;, without awaiting it first. With Cap'n Web, this just works: The client sends a message to the server saying: "Please insert the result of the first call into the parameters of the second."&lt;/p&gt;
      &lt;p&gt;Or perhaps the first call returns an object with methods. You can call the methods immediately, without awaiting the first promise, like:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let batch = newHttpBatchRpcSession("https://example.com/api");

// Authencitate the API key, returning a Session object.
let sessionPromise = batch.authenticate(apiKey);

// Get the user's name.
let name = await sessionPromise.whoami();

console.log(name);
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;This works because the promise returned by a Cap'n Web call is not a regular promise. Instead, it's a JavaScript Proxy object. Any methods you call on it are interpreted as speculative method calls on the eventual result. These calls are sent to the server immediately, telling the server: "When you finish the call I sent earlier, call this method on what it returns."&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Did you spot the security?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;This last example shows an important security pattern enabled by Cap'n Web's object-capability model.&lt;/p&gt;
      &lt;p&gt;When we call the authenticate() method, after it has verified the provided API key, it returns an authenticated session object. The client can then make further RPCs on the session object to perform operations that require authorization as that user. The server code might look like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;class MyApiServer extends RpcTarget {
  authenticate(apiKey) {
    let username = await checkApiKey(apiKey);
    return new AuthenticatedSession(username);
  }
}

class AuthenticatedSession extends RpcTarget {
  constructor(username) {
    super();
    this.username = username;
  }

  whoami() {
    return this.username;
  }

  // ...other methods requiring auth...
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Here's what makes this work: It is impossible for the client to "forge" a session object. The only way to get one is to call authenticate(), and have it return successfully.&lt;/p&gt;
      &lt;p&gt;In most RPC systems, it is not possible for one RPC to return a stub pointing at a new RPC object in this way. Instead, all functions are top-level, and can be called by anyone. In such a traditional RPC system, it would be necessary to pass the API key again to every function call, and check it again on the server each time. Or, you'd need to do authorization outside the RPC system entirely.&lt;/p&gt;
      &lt;p&gt;This is a common pain point for WebSockets in particular. Due to the design of the web APIs for WebSocket, you generally cannot use headers nor cookies to authorize them. Instead, authorization must happen in-band, by sending a message over the WebSocket itself. But this can be annoying for RPC protocols, as it means the authentication message is "special" and changes the state of the connection itself, affecting later calls. This breaks the abstraction.&lt;/p&gt;
      &lt;p&gt;The authenticate() pattern shown above neatly makes authentication fit naturally into the RPC abstraction. It's even type-safe: you can't possibly forget to authenticate before calling a method requiring auth, because you wouldn't have an object on which to make the call. Speaking of type-safetyâ¦&lt;/p&gt;
      &lt;p&gt;If you use TypeScript, Cap'n Web plays nicely with it. You can declare your RPC API once as a TypeScript interface, implement in on the server, and call it on the client:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;// Shared interface declaration:
interface MyApi {
  hello(name: string): Promise&amp;lt;string&amp;gt;;
}

// On the client:
let api: RpcStub&amp;lt;MyApi&amp;gt; = newWebSocketRpcSession("wss://example.com/api");

// On the server:
class MyApiServer extends RpcTarget implements MyApi {
  hello(name) {
    return `Hello, ${name}!`
  }
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Now you get end-to-end type checking, auto-completed method names, and so on.&lt;/p&gt;
      &lt;p&gt;Note that, as always with TypeScript, no type checks occur at runtime. The RPC system itself does not prevent a malicious client from calling an RPC with parameters of the wrong type. This is, of course, not a problem unique to Cap'n Web â JSON-based APIs have always had this problem. You may wish to use a runtime type-checking system like Zod to solve this. (Meanwhile, we hope to add type checking based directly on TypeScript types in the future.)&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;An alternative to GraphQL?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;If youâve used GraphQL before, you might notice some similarities. One benefit of GraphQL was to solve the âwaterfallâ problem of traditional REST APIs by allowing clients to ask for multiple pieces of data in one query. For example, instead of making three sequential HTTP calls:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;GET /user
GET /user/friends
GET /user/friends/photos&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;â¦you can write one GraphQL query to fetch it all at once.&lt;/p&gt;
      &lt;p&gt;Thatâs a big improvement over REST, but GraphQL comes with its own tradeoffs:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;New language and tooling. You have to adopt GraphQLâs schema language, servers, and client libraries. If your team is all-in on JavaScript, thatâs a lot of extra machinery.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Limited composability. GraphQL queries are declarative, which makes them great for fetching data, but awkward for chaining operations or mutations. For example, you canât easily say: âcreate a user, then immediately use that new user object to make a friend request, all-in-one round trip.â&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Different abstraction model. GraphQL doesnât look or feel like the JavaScript APIs you already know. Youâre learning a new mental model rather than extending the one you use every day.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;How Cap'n Web goes further&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Cap'n Web solves the waterfall problem without introducing a new language or ecosystem. Itâs just JavaScript. Because Cap'n Web supports promise pipelining and object references, you can write code that looks like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let user = api.createUser({ name: "Alice" });
let friendRequest = await user.sendFriendRequest("Bob");&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;What happens under the hood? Both calls are pipelined into a single network round trip:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Create the user.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Take the result of that call (a new User object).&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Immediately invoke sendFriendRequest() on that object.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;All of this is expressed naturally in JavaScript, with no schemas, query languages, or special tooling required. You just call methods and pass objects around, like you would in any other JavaScript code.&lt;/p&gt;
      &lt;p&gt;In other words, GraphQL gave us a way to flatten RESTâs waterfalls. Cap'n Web lets us go even further: it gives you the power to model complex interactions exactly the way you would in a normal program, with no impedance mismatch.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;But how do we solve arrays?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;With everything we've presented so far, there's a critical missing piece to seriously consider Cap'n Web as an alternative to GraphQL: handling lists. Often, GraphQL is used to say: "Perform this query, and then, for every result, perform this other query." For example: "List the user's friends, and then for each one, fetch their profile photo."&lt;/p&gt;
      &lt;p&gt;In short, we need an &lt;code&gt;array.map()&lt;/code&gt; operation that can be performed without adding a round trip.&lt;/p&gt;
      &lt;p&gt;Cap'n Proto, historically, has never supported such a thing.&lt;/p&gt;
      &lt;p&gt;But with Cap'n Web, we've solved it. You can do:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let user = api.authenticate(token);

// Get the user's list of friends (an array).
let friendsPromise = user.listFriends();

// Do a .map() to annotate each friend record with their photo.
// This operates on the *promise* for the friends list, so does not
// add a round trip.
// (wait WHAT!?!?)
let friendsWithPhotos = friendsPromise.map(friend =&amp;gt; {
  return {friend, photo: api.getUserPhoto(friend.id))};
}

// Await the friends list with attached photos -- one round trip!
let results = await friendsWithPhotos;
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;&lt;code&gt;.map()&lt;/code&gt; takes a callback function, which needs to be applied to each element in the array. As we described earlier, normally when you pass a function to an RPC, the function is passed "by reference", meaning that the remote side receives a stub, where calling that stub makes an RPC back to the client where the function was created.&lt;/p&gt;
      &lt;p&gt;But that is NOT what is happening here. That would defeat the purpose: we don't want the server to have to round-trip to the client to process every member of the array. We want the server to just apply the transformation server-side.&lt;/p&gt;
      &lt;p&gt;To that end, &lt;code&gt;.map() &lt;/code&gt;is special. It does not send JavaScript code to the server, but it does send something like "code", restricted to a domain-specific, non-Turing-complete language. The "code" is a list of instructions that the server should carry out for each member of the array. In this case, the instructions are:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Invoke &lt;code&gt;api.getUserPhoto(friend.id)&lt;/code&gt;.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Return an object &lt;code&gt;{friend, photo}&lt;/code&gt;, where friend is the original array element and photo is the result of step 1.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;But the application code just specified a JavaScript method. How on Earth could we convert this into the narrow DSL?&lt;/p&gt;
      &lt;p&gt;The answer is record-replay: On the client side, we execute the callback once, passing in a special placeholder value. The parameter behaves like an RPC promise. However, the callback is required to be synchronous, so it cannot actually await this promise. The only thing it can do is use promise pipelining to make pipelined calls. These calls are intercepted by the implementation and recorded as instructions, which can then be sent to the server, where they can be replayed as needed.&lt;/p&gt;
      &lt;p&gt;And because the recording is based on promise pipelining, which is what the RPC protocol itself is designed to represent, it turns out that the "DSL" used to represent "instructions" for the map function is just the RPC protocol itself. ð¤¯&lt;/p&gt;
      &lt;p&gt;Cap'n Web's underlying protocol is based on JSON â but with a preprocessing step to handle special types. Arrays are treated as "escape sequences" that let us encode other values. For example, JSON does not have an encoding for &lt;code&gt;Date&lt;/code&gt; objects, but Cap'n Web does. You might see a message that looks like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;{
  event: "Birthday Week",
  timestamp: ["date", 1758499200000]
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;To encode a literal array, we simply double-wrap it in &lt;code&gt;[]&lt;/code&gt;:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;{
  names: [["Alice", "Bob", "Carol"]]
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;In other words, an array with just one element which is itself an array, evaluates to the inner array literally. An array whose first element is a type name, evaluates to an instance of that type, where the remaining elements are parameters to the type.&lt;/p&gt;
      &lt;p&gt;Note that only a fixed set of types are supported: essentially, "structured clonable" types, and RPC stub types.&lt;/p&gt;
      &lt;p&gt;On top of this basic encoding, we define an RPC protocol inspired by Cap'n Proto â but greatly simplified.&lt;/p&gt;
      &lt;p&gt;Since Cap'n Web is a symmetric protocol, there is no well-defined "client" or "server" at the protocol level. There are just two parties exchanging messages across a connection. Every kind of interaction can happen in either direction.&lt;/p&gt;
      &lt;p&gt;In order to make it easier to describe these interactions, I will refer to the two parties as "Alice" and "Bob".&lt;/p&gt;
      &lt;p&gt;Alice and Bob start the connection by establishing some sort of bidirectional message stream. This may be a WebSocket, but Cap'n Web also allows applications to define their own transports. Each message in the stream is JSON-encoded, as described earlier.&lt;/p&gt;
      &lt;p&gt;Alice and Bob each maintain some state about the connection. In particular, each maintains an "export table", describing all the pass-by-reference objects they have exposed to the other side, and an "import table", describing the references they have received. Alice's exports correspond to Bob's imports, and vice versa. Each entry in the export table has a signed integer ID, which is used to reference it. You can think of these IDs like file descriptors in a POSIX system. Unlike file descriptors, though, IDs can be negative, and an ID is never reused over the lifetime of a connection.&lt;/p&gt;
      &lt;p&gt;At the start of the connection, Alice and Bob each populate their export tables with a single entry, numbered zero, representing their "main" interfaces. Typically, when one side is acting as the "server", they will export their main public RPC interface as ID zero, whereas the "client" will export an empty interface. However, this is up to the application: either side can export whatever they want.&lt;/p&gt;
      &lt;p&gt;From there, new exports are added in two ways:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;When Alice sends a message to Bob that contains within it an object or function reference, Alice adds the target object to her export table. IDs assigned in this case are always negative, starting from -1 and counting downwards.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Alice can send a "push" message to Bob to request that Bob add a value to his export table. The "push" message contains an expression which Bob evaluates, exporting the result. Usually, the expression describes a method call on one of Bob's existing exports â this is how an RPC is made. Each "push" is assigned a positive ID on the export table, starting from 1 and counting upwards. Since positive IDs are only assigned as a result of pushes, Alice can predict the ID of each push she makes, and can immediately use that ID in subsequent messages. This is how promise pipelining is achieved.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;After sending a push message, Alice can subsequently send a "pull" message, which tells Bob that once he is done evaluating the "push", he should proactively serialize the result and send it back to Alice, as a "resolve" (or "reject") message. However, this is optional: Alice may not actually care to receive the return value of an RPC, if Alice only wants to use it in promise pipelining. In fact, the Cap'n Web implementation will only send a "pull" message if the application has actually awaited the returned promise.&lt;/p&gt;
      &lt;p&gt;Putting it together, a code sequence like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let namePromise = api.getMyName();
let result = await api.hello(namePromise);

console.log(result);&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Might produce a message exchange like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;// Call api.getByName(). `api` is the server's main export, so has export ID 0.
-&amp;gt; ["push", ["pipeline", 0, "getMyName", []]
// Call api.hello(namePromise). `namePromise` refers to the result of the first push,
// so has ID 1.
-&amp;gt; ["push", ["pipeline", 0, "hello", [["pipeline", 1]]]]
// Ask that the result of the second push be proactively serialized and returned.
-&amp;gt; ["pull", 2]
// Server responds.
&amp;lt;- ["resolve", 2, "Hello, Alice!"]&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;For more details about the protocol, check out the docs.&lt;/p&gt;
      &lt;p&gt;Cap'n Web is new and still highly experimental. There may be bugs to shake out. But, we're already using it today. Cap'n Web is the basis of the recently-launched "remote bindings" feature in Wrangler, allowing a local test instance of workerd to speak RPC to services in production. We've also begun to experiment with it in various frontend applications â expect more blog posts on this in the future.&lt;/p&gt;
      &lt;p&gt;In any case, Cap'n Web is open source, and you can start using it in your own projects now.&lt;/p&gt;
      &lt;p&gt;Check it out on GitHub.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45332883</guid><pubDate>Mon, 22 Sep 2025 13:05:32 +0000</pubDate></item><item><title>Why haven't local-first apps become popular?</title><link>https://marcobambini.substack.com/p/why-local-first-apps-havent-become</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45333021</guid><pubDate>Mon, 22 Sep 2025 13:17:59 +0000</pubDate></item><item><title>What is algebraic about algebraic effects?</title><link>https://interjectedfuture.com/what-is-algebraic-about-algebraic-effects/</link><description>&lt;doc fingerprint="2c481533a51a53b2"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;What is Algebraic about Algebraic Effects?&lt;/head&gt;&lt;quote&gt;what does the word "algebraic" mean when used in the context of programming langs?&lt;lb/&gt;- a random tweet&lt;/quote&gt;&lt;p&gt;I'd wondered the same thing about "Algebraic Effects", and was excited to find a talk on YouTube titled What's Algebraic About Algebraic Effects and Handlers? Unfortunately, I'm not the target audience. As an engineer that doesn't shy away from math, it was still out of my depth.&lt;/p&gt;&lt;p&gt;I found some time this past spring looking into Algebraic Effects, and I think I have a decent answer to the question.&lt;/p&gt;&lt;head rend="h3"&gt;Algebra in the context of programming&lt;/head&gt;&lt;p&gt;My view of "Algebra" in the context of programming is a particular kind of compositionality, where there's a structure.&lt;/p&gt;&lt;p&gt;In contrast, mainstream developers often talk about compositionality as just two obj/function that can interoperate due to the same interface, but not much more can be inferred about properties of the interop between the two obj/functions.&lt;/p&gt;&lt;p&gt;So often times, we get some collection of objects/functions that go together in an arbitrary way according to the taste of the developer that wrote it. If they're any good, it feels intuitive. But more often than not, it feels arbitrary. The effect is magnified if you look into the codebase. To a newcomer, it feels like a mess, in the same way that a house built by piling stones high feels like a mess: there's no apparent or easily recognizable structure.&lt;/p&gt;&lt;head rend="h3"&gt;A tangential detour into abstract algebra&lt;/head&gt;&lt;p&gt;In abstract algebra, structure is often where you take some math object 𝛂 (like an int, or matrix), and you pair it with an operation, (like + or *), and you say: integers can be composed with op `+`, but we can ALSO infer properties in these combos--or laws.&lt;/p&gt;&lt;p&gt;So a common one we know is: integer (ℤ) with addition (+) has implied properties that always hold. And the elements (ℤ), the op (+), and the properties together constrain outcomes, and this is what gives us structure. A house with structure feels like it's built with arches, rather than a pile of rocks. What are the properties of (ℤ) and (+)? Due to how ℤ and + are defined, we get these properties:&lt;/p&gt;&lt;p&gt;1. Closure: ℤ + ℤ always gives you another ℤ.&lt;/p&gt;&lt;p&gt;Sometimes devs write code that doesn't give you back the same thing.&lt;/p&gt;&lt;p&gt;2. Associativity: (a + b) + c = a + (b + c) where a, b, c are in ℤ.&lt;/p&gt;&lt;p&gt;This familiar, as they were drilled in grade school. But often devs don't write code that fulfill this property.&lt;/p&gt;&lt;p&gt;The last two are:&lt;/p&gt;&lt;p&gt;3. identity: ℤ has an element that doesn't change when we use +. &lt;lb/&gt;Here, it's zero: a + 0 = a &lt;/p&gt;&lt;p&gt;4. inverse: every ℤ has a matching ℤ that give us the identity when we use + on it: a + (-a) = 0, where a and -a are in ℤ.&lt;/p&gt;&lt;p&gt;Taken together, math peeps gave this kind of structure a name: Groups. So if someone says [a struct] and [an op] together form a group, I can automatically can assume those properties. It's a shorthand.&lt;/p&gt;&lt;p&gt;If you add even more constraints/properties to how ℤ and + behave together, you get another algebraic structure. There's a whole host and families of these. So if we add another constraint, we get an Abelian Group:&lt;/p&gt;&lt;p&gt;5. Commutativity: a+b = b+a, where a, b are in ℤ&lt;/p&gt;&lt;head rend="h3"&gt;Surmounting the network with algebra&lt;/head&gt;&lt;p&gt;Why write constraining data structure and op pairings? It's quite useful if you want to guarantee specific properties of your system. For example, it's well known that syncing is hard, because of the Eight Fallacies of Distributed Systems.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;The network is reliable;&lt;/item&gt;&lt;item&gt;Latency is zero;&lt;/item&gt;&lt;item&gt;Bandwidth is infinite;&lt;/item&gt;&lt;item&gt;The network is secure;&lt;/item&gt;&lt;item&gt;Topology doesn't change;&lt;/item&gt;&lt;item&gt;There is one administrator;&lt;/item&gt;&lt;item&gt;Transport cost is zero;&lt;/item&gt;&lt;item&gt;The network is homogeneous.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;That means your data, when sent over the network will likely arrive out of order. Worse, clocks can be out of sync, so it can look like data arrived from the future. How can we tame the underlying unreliable system? By constraining our data and operations to have properties.&lt;/p&gt;&lt;p&gt;CRDTs are nowadays used to enforce eventually consistent syncs. It achieves this by pairing a data structure with a merge operation, which together form an algebraic structure called a semi-lattice. The properties of a semi-lattice are:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Closure: For all a, b in the set S, the result of a ∘ b is also in S.&lt;/item&gt;&lt;item&gt;Associativity: a ∘ (b ∘ c)=(a ∘ b) ∘ c for all a, b, c ∈ S.&lt;/item&gt;&lt;item&gt;Commutativity: a ∘ b = b ∘ a for all a, b ∈ S.&lt;/item&gt;&lt;item&gt;Idempotence: a ∘ a = a for all a ∈ S.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Together, this is enough to counteract the network mixing up your data when sending it over the network. I wrote about that here:&lt;/p&gt;&lt;p&gt;So by constraining the power of what our code can do, we can ensure the system has specific desirable properties that achieve the goal of syncing data over an unreliable network. It's where we say: "If we compose this kind of data structure in this constrained way with this kind of merge function, then we can guarantee these properties always hold. And with this structure, our data can survive sync over an unreliable network with other syncers."&lt;/p&gt;&lt;head rend="h3"&gt;From Monads to Algebraic Effects&lt;/head&gt;&lt;p&gt;This is why people also like Monads. Monads are about how to compose code, but with specific properties (Monadic laws) so we can achieve some goal in how they compose. I won't go into it here, as this is already long, but that's the core idea.&lt;/p&gt;&lt;p&gt;However, not all types of Monads compose well together. Here's where I'm out of my depth, but I've read and I'm told that this is why there are Monad Transformers, so you can fit different domain Monads together.&lt;/p&gt;&lt;p&gt;Hence, some people have started looking at Algebraic Effects, as a way to achieve the same compositional powers of monads, but in a different way. Most descriptions of Algebraic Effects actually ignore the `algebraic` part, because describing `effects` is already a big leap.&lt;/p&gt;&lt;p&gt;The effects part, is often explained as "resumable exceptions". I wrote a short description of what algebraic effects are from that perspective, so I won't expound on that here.&lt;/p&gt;&lt;p&gt;But the algebraic part of algebraic effects is that the effects that you raise as a "resumable exception" can be composed together! Not just in any way: design them so when composed, they have *guaranteed properties* just like the stuff you saw above!&lt;/p&gt;&lt;p&gt;For example, if we had a key/value store that we interface with using &lt;code&gt;get&lt;/code&gt; and &lt;code&gt;put&lt;/code&gt;, we could express what we expect to happen through some algebraic properties.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Idempotence of consecutive reads (get-get): get k; get k ≡ get x&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This says, two consecutive &lt;code&gt;gets&lt;/code&gt; is functionally equivalent to a single &lt;code&gt;get&lt;/code&gt;. This guarantees that &lt;code&gt;get&lt;/code&gt; is a pure observation: it doesn't consume or advance anything. If this law didn't hold, reading could "drain" or "advance" some hidden cursor. By making it a law, we make it an explicit behavior for our users, so they're not surprised by bugs down the line when their assumptions veer from this property.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Last write wins (put-put): put k v1; put k v2 ≡ put k v2&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Easy. The two &lt;code&gt;puts&lt;/code&gt; together is the functional equivalent of only executing the last one. Hence, the last &lt;code&gt;put&lt;/code&gt; is the value that's currently sitting in key &lt;code&gt;k&lt;/code&gt;. This encodes overwriting semantics, and without it, &lt;code&gt;put&lt;/code&gt; might append, merge, or accumulate. It wouldn't be what users would expect.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Read after write (put-get): put k v; get k ≡ put k v; return v&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Executing a &lt;code&gt;put&lt;/code&gt; and then an immediate &lt;code&gt;get&lt;/code&gt; is the functional equivalent of just executing the put, but then just returning the value &lt;code&gt;v&lt;/code&gt; you already have in hand, instead of executing &lt;code&gt;get&lt;/code&gt;. This is important to guarantee the consistency of reads right after writes. Without this, you could write &lt;code&gt;v&lt;/code&gt; and then not see &lt;code&gt;v&lt;/code&gt; immediately, which would break the intuitive model of state in a key/value store.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Write back same value (get-put): get k &amp;gt;&amp;gt;= (λv. put k v) ≡ return ()&lt;/item&gt;&lt;/list&gt;&lt;p&gt;If you read the value of a key and then immediately write it back unchanged, that's functionally equivalent of doing nothing (returning unit).&lt;/p&gt;&lt;code&gt;&amp;gt;&amp;gt;=&lt;/code&gt; as saying "and then...". So rule 4 in javascript pseudocode might look like:&lt;p&gt;get(store, key).andThen((val) =&amp;gt; put(store, key, val))&lt;/p&gt;&lt;code&gt;return ()&lt;/code&gt;, &lt;code&gt;()&lt;/code&gt; is called &lt;code&gt;unit&lt;/code&gt;, which is the way functional programmers denote "no meaningful value", which is effectively what C programmers use &lt;code&gt;void&lt;/code&gt; for. They're technically different, but in practice, they're used for similar purposes.&lt;list rend="ol"&gt;&lt;item&gt;Independence across keys For &lt;code&gt;k1 ≠ k2&lt;/code&gt;:&lt;/item&gt;&lt;/list&gt;&lt;code&gt;put k1 v1; put k2 v2  ≡  put k2 v2; put k1 v1
get k1; get k2        ≡  get k2; get k1
put k1 v; get k2      ≡  get k2; put k1 v
&lt;/code&gt;&lt;p&gt;Operations on different keys commute, and the store treats each key as an independent cell. This is what makes it a key/value store, rather than some entangled data structure.&lt;/p&gt;&lt;p&gt;Hence, just because you are writing effects, doesn't automatically mean they're algebraic. You have to consciously design them to be so, in order to give properties or guarantees that you want your users to have. Most current programming languages have no way of enforcing these equational axioms, so even esoteric languages that feature algebraic effects don't even try to enforce them.&lt;/p&gt;&lt;p&gt;Languages which feature dependent types, such as Coq, Agda, Idris 2, and Lean are the only languages that can encode these equational axioms explicitly and be able to prove their veracity. Typically, these languages are used by mathematicians to do proofs in math. But interestingly, Lean has been getting a lot of momentum, and it can compile to C. It can be a practical in-road to using these in practice.&lt;/p&gt;&lt;p&gt;And, in my own words, that's what's algebraic about algebraic effects.&lt;/p&gt;&lt;head rend="h3"&gt;Epilogue&lt;/head&gt;&lt;p&gt;Alan Kay was known to lament that 1 million lines in a code base is unconscionable. It's no more a skyscraper than a pile of rocks. That's because there's often no structure. Eventually we figured out arches: they're structure that give strength with less material.&lt;/p&gt;&lt;p&gt;Hence, we can build higher without using more material. By analogy, we're starting to discover what this structure looks like in software. And it looks like math. There's a lot of resistance to this, and will be for a long time.&lt;/p&gt;&lt;p&gt;And maybe with LLMs, it might not matter for a wide swath of applications. But still, there's ever progress moving forward in this direction, where these pure functional programming or math-y ideas filter down to more mainstream languages.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45333978</guid><pubDate>Mon, 22 Sep 2025 14:30:10 +0000</pubDate></item><item><title>PlanetScale for Postgres is now GA</title><link>https://planetscale.com/blog/planetscale-for-postgres-is-generally-available</link><description>&lt;doc fingerprint="af51e4d16e9161c8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;PlanetScale for Postgres is now GA&lt;/head&gt;
    &lt;p&gt;By Sam Lambert |&lt;/p&gt;
    &lt;p&gt;PlanetScale for Postgres is now generally available and out of private preview. To create a Postgres database, sign up or log in to your PlanetScale account, create a new database, and select Postgres. If you are looking to migrate from another Postgres provider to PlanetScale, you can use our migration guides to get started. Finally, if you have a large or complex migration, we can help you via our sales team at postgres@planetscale.com.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is PlanetScale for Postgres?&lt;/head&gt;
    &lt;p&gt;Our mission is simple: bring you the fastest and most reliable databases with the best developer experience. We have done this for 5 years now with our managed Vitess product, allowing companies like Cursor, Intercom, and Block to scale beyond previous limits.&lt;/p&gt;
    &lt;p&gt;We are so excited to bring this to Postgres. Our proprietary operator allows us to bring the maturity of PlanetScale and the performance of Metal to an even wider audience. We bring you the best of Postgres and the best of PlanetScale in one product.&lt;/p&gt;
    &lt;head rend="h2"&gt;Customers on PlanetScale for Postgres&lt;/head&gt;
    &lt;p&gt;Hundreds of companies already trust PlanetScale for Postgres to power their production workloads. We say this every time we launch something, but we prefer you hear about real-world usage straight from our customers. Read through some of their stories about their migration to PlanetScale for Postgres below.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Convex: Powered by PlanetScale&lt;/item&gt;
      &lt;item&gt;Supermemory just got faster on PlanetScale&lt;/item&gt;
      &lt;item&gt;Scaling RealâTime Discovery: Inside Layersâ PlanetScale Migration&lt;/item&gt;
      &lt;item&gt;Why We Migrated from Neon to PlanetScale&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Vitess for Postgres&lt;/head&gt;
    &lt;p&gt;Neki is our Postgres sharding solution. Built by the team behind Vitess combining the best of Vitess and Postgres. Neki is not a fork of Vitess. Vitessâ achievements are enabled by leveraging MySQLâs strengths and engineering around its weaknesses. To achieve Vitessâ power for Postgres we are architecting from first principles and building alongside design partners at scale. When we are ready we will release Neki as an open source project suitable for running the most demanding Postgres workloads. To sign up for the Neki waitlist visit neki.dev.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45334545</guid><pubDate>Mon, 22 Sep 2025 15:10:48 +0000</pubDate></item><item><title>OpenAI and Nvidia announce partnership to deploy 10GW of Nvidia systems</title><link>https://openai.com/index/openai-nvidia-systems-partnership/</link><description>&lt;doc fingerprint="a9561c5e408ccdb9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;OpenAI and NVIDIA announce strategic partnership to deploy 10 gigawatts of NVIDIA systems&lt;/head&gt;
    &lt;p&gt;News&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Strategic partnership enables OpenAI to build and deploy at least 10 gigawatts of AI datacenters with NVIDIA systems representing millions of GPUs for OpenAI’s next-generation AI infrastructure.&lt;/item&gt;
      &lt;item&gt;To support the partnership, NVIDIA intends to invest up to $100 billion in OpenAI progressively as each gigawatt is deployed.&lt;/item&gt;
      &lt;item&gt;The first gigawatt of NVIDIA systems will be deployed in the second half of 2026 on NVIDIA’s Vera Rubin platform.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;San Francisco and Santa Clara—September 22, 2025—NVIDIA and OpenAI today announced a letter of intent for a landmark strategic partnership to deploy at least 10 gigawatts of NVIDIA systems for OpenAI’s next-generation AI infrastructure to train and run its next generation of models on the path to deploying superintelligence. To support this deployment including datacenter and power capacity, NVIDIA intends to invest up to $100 billion in OpenAI as the new NVIDIA systems are deployed. The first phase is targeted to come online in the second half of 2026 using NVIDIA’s Vera Rubin platform.&lt;/p&gt;
    &lt;p&gt;“NVIDIA and OpenAI have pushed each other for a decade, from the first DGX supercomputer to the breakthrough of ChatGPT,” said Jensen Huang, founder and CEO of NVIDIA. “This investment and infrastructure partnership mark the next leap forward—deploying 10 gigawatts to power the next era of intelligence.”&lt;/p&gt;
    &lt;p&gt;“Everything starts with compute,” said Sam Altman, co-founder and CEO of OpenAI. “Compute infrastructure will be the basis for the economy of the future, and we will utilize what we’re building with NVIDIA to both create new AI breakthroughs and empower people and businesses with them at scale.”&lt;/p&gt;
    &lt;p&gt;“We’ve been working closely with NVIDIA since the early days of OpenAI,” said Greg Brockman, co-founder and President of OpenAI. “We’ve utilized their platform to create AI systems that hundreds of millions of people use every day. We’re excited to deploy 10 gigawatts of compute with NVIDIA to push back the frontier of intelligence and scale the benefits of this technology to everyone.”&lt;/p&gt;
    &lt;p&gt;OpenAI will work with NVIDIA as a preferred strategic compute and networking partner for its AI factory growth plans. OpenAI and NVIDIA will work together to co-optimize their roadmaps for OpenAI's model and infrastructure software and NVIDIA’s hardware and software.&lt;/p&gt;
    &lt;p&gt;This partnership complements the deep work OpenAI and NVIDIA are already doing with a broad network of collaborators, including Microsoft, Oracle, SoftBank, and Stargate partners, focused on building the world’s most advanced AI infrastructure.&lt;/p&gt;
    &lt;p&gt;OpenAI has grown to over 700 million weekly active users and strong adoption across global enterprises, small businesses, and developers. This partnership will help OpenAI advance its mission to build artificial general intelligence that benefits all of humanity.&lt;lb/&gt;NVIDIA and OpenAI look forward to finalizing the details of this new phase of strategic partnership in the coming weeks.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45335474</guid><pubDate>Mon, 22 Sep 2025 16:10:15 +0000</pubDate></item><item><title>Testing is better than data structures and algorithms</title><link>https://nedbatchelder.com/blog/202509/testing_is_better_than_dsa.html</link><description>&lt;doc fingerprint="bf6210976d050c95"&gt;
  &lt;main&gt;
    &lt;p&gt;People should spend less time learning DSA, more time learning testing.&lt;/p&gt;
    &lt;p&gt;I see new learners asking about “DSA” a lot. Data Structures and Algorithms are of course important: considered broadly, they are the two ingredients that make up all programs. But in my opinion, “DSA” as an abstract field of study is over-emphasized.&lt;/p&gt;
    &lt;p&gt;I understand why people focus on DSA: it’s a concrete thing to learn about, there are web sites devoted to testing you on it, and most importantly, because job interviews often involve DSA coding questions.&lt;/p&gt;
    &lt;p&gt;Before I get to other opinions, let me make clear that anything you can do to help you get a job is a good thing to do. If grinding leetcode will land you a position, then do it.&lt;/p&gt;
    &lt;p&gt;But I hope companies hiring entry-level engineers aren’t asking them to reverse linked lists or balance trees. Asking about techniques that can be memorized ahead of time won’t tell them anything about how well you can work. The stated purpose of those interviews is to see how well you can figure out solutions, in which case memorization will defeat the point.&lt;/p&gt;
    &lt;p&gt;The thing new learners don’t understand about DSA is that actual software engineering almost never involves implementing the kinds of algorithms that “DSA” teaches you. Sure, it can be helpful to work through some of these puzzles and see how they are solved, but writing real code just doesn’t involve writing that kind of code.&lt;/p&gt;
    &lt;p&gt;Here is what I think in-the-trenches software engineers should know about data structures and algorithms:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Data structures are ways to organize data. Learn some of the basics: linked list, array, hash table, tree. By “learn” I mean understand what it does and why you might want to use one.&lt;/item&gt;
      &lt;item&gt;Different data structures can be used to organize the same data in different ways. Learn some of the trade-offs between structures that are similar.&lt;/item&gt;
      &lt;item&gt;Algorithms are ways of manipulating data. I don’t mean named algorithms like Quicksort, but algorithms as any chunk of code that works on data and does something with it.&lt;/item&gt;
      &lt;item&gt;How you organize data affects what algorithms you can use to work with the data. Some data structures will be slow for some operations where another structure will be fast.&lt;/item&gt;
      &lt;item&gt;Algorithms have a “time complexity” (Big O): how the code slows as the data grows. Get a sense of what this means.&lt;/item&gt;
      &lt;item&gt;Python has a number of built-in data structures. Learn how they work, and the time complexity of their operations.&lt;/item&gt;
      &lt;item&gt;Learn how to think about your code to understand its time complexity.&lt;/item&gt;
      &lt;item&gt;Read a little about more esoteric things like Bloom filters, so you can find them later in the unlikely case you need them.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here are some things you don’t need to learn:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The details of a dozen different sorting algorithms. Look at two to see different ways of approaching the same problem, then move on.&lt;/item&gt;
      &lt;item&gt;The names of “important” algorithms. Those have all been implemented for you.&lt;/item&gt;
      &lt;item&gt;The answers to all N problems on some quiz web site. You won’t be asked these exact questions, and they won’t come up in your real work. Again: try a few to get a feel for how some algorithms work. The exact answers are not what you need.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of course some engineers need to implement hash tables, or sorting algorithms or whatever. We love those engineers: they write libraries we can use off the shelf so we don’t have to implement them ourselves.&lt;/p&gt;
    &lt;p&gt;There have been times when I implemented something that felt like An Algorithm (for example, Finding fuzzy floats), but it was more about considering another perspective on my data, looking at the time complexity, and moving operations around to avoid quadratic behavior. It wasn’t opening a textbook to find the famous algorithm that would solve my problem.&lt;/p&gt;
    &lt;p&gt;Again: if it will help you get a job, deep-study DSA. But don’t be disappointed when you don’t use it on the job.&lt;/p&gt;
    &lt;p&gt;If you want to prepare yourself for a career, and also stand out in job interviews, learn how to write tests:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This will be a skill you use constantly. Real-world software means writing tests much more than school teaches you to.&lt;/item&gt;
      &lt;item&gt;In a job search, testing experience will stand out more than DSA depth. It shows you’ve thought about what it takes to write high-quality software instead of just academic exercises.&lt;/item&gt;
      &lt;item&gt;It’s not obvious how to test code well. It’s a puzzle and a problem to solve. If you like figuring out solutions to tricky questions, focus on how to write code so that it can be tested, and how to test it.&lt;/item&gt;
      &lt;item&gt;Testing not only gives you more confidence in your code, it helps you write better code in the first place.&lt;/item&gt;
      &lt;item&gt;Testing applies everywhere, from tiny bits of code to entire architectures, assisting you in design and implementation at all scales.&lt;/item&gt;
      &lt;item&gt;If pursued diligently, testing is an engineering discipline in its own right, with a fascinating array of tools and techniques.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Less DSA, more testing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45335635</guid><pubDate>Mon, 22 Sep 2025 16:21:35 +0000</pubDate></item><item><title>Mentra (YC W25) is hiring to build smart glasses</title><link>https://news.ycombinator.com/item?id=45336282</link><description>&lt;doc fingerprint="c1fe11d2919f8da2"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;We’re building the OS for smart glasses because we believe glasses are the next personal computer.&lt;/p&gt;
      &lt;p&gt;This year we launched MentraOS, graduated Y Combinator, and raised an $8M seed round to bring smart glasses software and hardware to market.&lt;/p&gt;
      &lt;p&gt;We're a small team (~11 people) shipping thousands of our first pair of smart glasses in December.&lt;/p&gt;
      &lt;p&gt;We need help in engineering (build smart glasses), design (design glasses interfaces), and growth (make glasses go viral).&lt;/p&gt;
      &lt;p&gt;Apply on the job board or if you don't see a fitting role, email me cayden@mentra.glass&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45336282</guid><pubDate>Mon, 22 Sep 2025 17:01:04 +0000</pubDate></item><item><title>Qwen3-Omni: Native Omni AI model for text, image and video</title><link>https://github.com/QwenLM/Qwen3-Omni</link><description>&lt;doc fingerprint="a44ecd1b2dbb5a97"&gt;
  &lt;main&gt;
    &lt;p&gt; 💜 Qwen Chat | 🤗 Hugging Face | 🤖 ModelScope | 📑 Blog | 📚 Cookbooks | 📑 Paper &lt;lb/&gt; 🖥️ Hugging Face Demo | 🖥️ ModelScope Demo | 💬 WeChat (微信) | 🫨 Discord | 📑 API &lt;/p&gt;
    &lt;p&gt;We release Qwen3-Omni, the natively end-to-end multilingual omni-modal foundation models. It is designed to process diverse inputs including text, images, audio, and video, while delivering real-time streaming responses in both text and natural speech. Click the video below for more information 😃&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2025.09.22: 🎉🎉🎉 We have released Qwen3-Omni. For more details, please check our blog!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Qwen3-Omni is the natively end-to-end multilingual omni-modal foundation models. It processes text, images, audio, and video, and delivers real-time streaming responses in both text and natural speech. We introduce several architectural upgrades to improve performance and efficiency. Key features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;State-of-the-art across modalities: Early text-first pretraining and mixed multimodal training provide native multimodal support. While achieving strong audio and audio-video results, unimodal text and image performance does not regress. Reaches SOTA on 22 of 36 audio/video benchmarks and open-source SOTA on 32 of 36; ASR, audio understanding, and voice conversation performance is comparable to Gemini 2.5 Pro.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multilingual: Supports 119 text languages, 19 speech input languages, and 10 speech output languages.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Speech Input: English, Chinese, Korean, Japanese, German, Russian, Italian, French, Spanish, Portuguese, Malay, Dutch, Indonesian, Turkish, Vietnamese, Cantonese, Arabic, Urdu.&lt;/item&gt;
          &lt;item&gt;Speech Output: English, Chinese, French, German, Russian, Italian, Spanish, Portuguese, Japanese, Korean.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Novel Architecture: MoE-based Thinker–Talker design with AuT pretraining for strong general representations, plus a multi-codebook design that drives latency to a minimum.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Real-time Audio/Video Interaction: Low-latency streaming with natural turn-taking and immediate text or speech responses.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Flexible Control: Customize behavior via system prompts for fine-grained control and easy adaptation.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Detailed Audio Captioner: Qwen3-Omni-30B-A3B-Captioner is now open source: a general-purpose, highly detailed, low-hallucination audio captioning model that fills a critical gap in the open-source community.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Qwen3-Omni supports a wide range of multimodal application scenarios, covering various domain tasks involving audio, image, video, and audio-visual modalities. Below are several cookbooks demonstrating the usage cases of Qwen3-Omni and these cookbooks include our actual execution logs. You can first follow the QuickStart guide to download the model and install the necessary inference environment dependencies, then run and experiment locally—try modifying prompts or switching model types, and enjoy exploring the capabilities of Qwen3-Omni!&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Category&lt;/cell&gt;
        &lt;cell role="head"&gt;Cookbook&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Open&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Audio&lt;/cell&gt;
        &lt;cell&gt;Speech Recognition&lt;/cell&gt;
        &lt;cell&gt;Speech recognition, supporting multiple languages and long audio.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Speech Translation&lt;/cell&gt;
        &lt;cell&gt;Speech-to-Text / Speech-to-Speech translation.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Music Analysis&lt;/cell&gt;
        &lt;cell&gt;Detailed analysis and appreciation of any music, including style, genre, rhythm, etc.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sound Analysis&lt;/cell&gt;
        &lt;cell&gt;Description and analysis of various sound effects and audio signals.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Audio Caption&lt;/cell&gt;
        &lt;cell&gt;Audio captioning, detailed description of any audio input.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Mixed Audio Analysis&lt;/cell&gt;
        &lt;cell&gt;Analysis of mixed audio content, such as speech, music, and environmental sounds.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Visual&lt;/cell&gt;
        &lt;cell&gt;OCR&lt;/cell&gt;
        &lt;cell&gt;OCR for complex images.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Object Grounding&lt;/cell&gt;
        &lt;cell&gt;Target detection and grounding.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Image Question&lt;/cell&gt;
        &lt;cell&gt;Answering arbitrary questions about any image.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Image Math&lt;/cell&gt;
        &lt;cell&gt;Solving complex mathematical problems in images, highlighting the capabilities of the Thinking model.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Video Description&lt;/cell&gt;
        &lt;cell&gt;Detailed description of video content.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Video Navigation&lt;/cell&gt;
        &lt;cell&gt;Generating navigation commands from first-person motion videos.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Video Scene Transition&lt;/cell&gt;
        &lt;cell&gt;Analysis of scene transitions in videos.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Audio-Visual&lt;/cell&gt;
        &lt;cell&gt;Audio Visual Question&lt;/cell&gt;
        &lt;cell&gt;Answering arbitrary questions in audio-visual scenarios, demonstrating the model's ability to model temporal alignment between audio and video.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Audio Visual Interaction&lt;/cell&gt;
        &lt;cell&gt;Interactive communication with the model using audio-visual inputs, including task specification via audio.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Audio Visual Dialogue&lt;/cell&gt;
        &lt;cell&gt;Conversational interaction with the model using audio-visual inputs, showcasing its capabilities in casual chat and assistant-like behavior.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Agent&lt;/cell&gt;
        &lt;cell&gt;Audio Function Call&lt;/cell&gt;
        &lt;cell&gt;Using audio input to perform function calls, enabling agent-like behaviors.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Downstream Task Fine-tuning&lt;/cell&gt;
        &lt;cell&gt;Omni Captioner&lt;/cell&gt;
        &lt;cell&gt;Introduction and capability demonstration of Qwen3-Omni-30B-A3B-Captioner, a downstream fine-tuned model based on Qwen3-Omni-30B-A3B-Instruct, illustrating the strong generalization ability of the Qwen3-Omni foundation model.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Here, we provide several methods to quickly get started with Qwen3-Omni. If you want complete experience of Qwen3-Omni, you can use Hugging Face Transformers. However, since Qwen3-Omni employs an MoE architecture, inference speed with Hugging Face Transformers on MoE models can be very slow. For large-scale invocation or low-latency requirements, we highly recommend using vLLM or performing inference via the DashScope API. We also strongly suggest using our provided Docker image, which includes a complete runtime environment for both Hugging Face Transformers and vLLM. In addition, our cookbooks offer some use cases to show Qwen3-Omni's capabilities. Welcome to learn more!&lt;/p&gt;
    &lt;p&gt;Below is the description of all Qwen3-Omni models. Please select and download the model that fits your needs.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Model Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell&gt;The Instruct model of Qwen3-Omni-30B-A3B, containing both thinker and talker, supporting audio, video, and text input, with audio and text output. For more information, please read the Qwen3-Omni Technical Report.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell&gt;The Thinking model of Qwen3-Omni-30B-A3B, containing the thinker component, equipped with chain-of-thought reasoning, supporting audio, video, and text input, with text output. For more information, please read the Qwen3-Omni Technical Report.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B-Captioner&lt;/cell&gt;
        &lt;cell&gt;A downstream audio fine-grained caption model fine-tuned from Qwen3-Omni-30B-A3B-Instruct, which produces detailed, low-hallucination captions for arbitrary audio inputs. It contains the thinker, supporting audio input and text output. For more information, you can refer to the model's cookbook or Hugging Face Demo and ModelScope Demo.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;During loading in Hugging Face Transformers or vLLM, model weights will be automatically downloaded based on the model name. However, if your runtime environment is not conducive to downloading weights during execution, you can refer to the following commands to manually download the model weights to a local directory:&lt;/p&gt;
    &lt;code&gt;# Download through ModelScope (recommended for users in Mainland China)
pip install -U modelscope
modelscope download --model Qwen/Qwen3-Omni-30B-A3B-Instruct --local_dir ./Qwen3-Omni-30B-A3B-Instruct
modelscope download --model Qwen/Qwen3-Omni-30B-A3B-Thinking --local_dir ./Qwen3-Omni-30B-A3B-Thinking
modelscope download --model Qwen/Qwen3-Omni-30B-A3B-Captioner --local_dir ./Qwen3-Omni-30B-A3B-Captioner

# Download through Hugging Face
pip install -U "huggingface_hub[cli]"
huggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Instruct --local-dir ./Qwen3-Omni-30B-A3B-Instruct
huggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Thinking --local-dir ./Qwen3-Omni-30B-A3B-Thinking
huggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Captioner --local-dir ./Qwen3-Omni-30B-A3B-Captioner&lt;/code&gt;
    &lt;p&gt;The Hugging Face Transformers code for Qwen3-Omni has been successfully merged, but the PyPI package has not yet been released. Therefore, you need to install it from source using the following command. We strongly recommend that you create a new Python environment or use our Docker to avoid environment runtime issues.&lt;/p&gt;
    &lt;code&gt;# If you already have transformers installed, please uninstall it first, or create a new Python environment
# pip uninstall transformers
pip install git+https://github.com/huggingface/transformers
pip install accelerate&lt;/code&gt;
    &lt;p&gt;We offer a toolkit to help you handle various types of audio and visual input more conveniently, providing an API-like experience. This includes support for base64, URLs, and interleaved audio, images, and videos. You can install it using the following command and make sure your system has &lt;code&gt;ffmpeg&lt;/code&gt; installed:&lt;/p&gt;
    &lt;code&gt;pip install qwen-omni-utils -U&lt;/code&gt;
    &lt;p&gt;Additionally, we recommend using FlashAttention 2 when running with Hugging Face Transformers to reduce GPU memory usage. However, if you are primarily using vLLM for inference, this installation is not necessary, as vLLM includes FlashAttention 2 by default.&lt;/p&gt;
    &lt;code&gt;pip install -U flash-attn --no-build-isolation&lt;/code&gt;
    &lt;p&gt;Also, you should have hardware that is compatible with FlashAttention 2. Read more about it in the official documentation of the FlashAttention repository. FlashAttention 2 can only be used when a model is loaded in &lt;code&gt;torch.float16&lt;/code&gt; or &lt;code&gt;torch.bfloat16&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Here is a code snippet to show you how to use Qwen3-Omni with &lt;code&gt;transformers&lt;/code&gt; and &lt;code&gt;qwen_omni_utils&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;import soundfile as sf

from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Instruct"
# MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"

model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    MODEL_PATH,
    dtype="auto",
    device_map="auto",
    attn_implementation="flash_attention_2",
)

processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

conversation = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},
            {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"},
            {"type": "text", "text": "What can you see and hear? Answer in one short sentence."}
        ],
    },
]

# Set whether to use audio in video
USE_AUDIO_IN_VIDEO = True

# Preparation for inference
text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
audios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = processor(text=text, 
                   audio=audios, 
                   images=images, 
                   videos=videos, 
                   return_tensors="pt", 
                   padding=True, 
                   use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = inputs.to(model.device).to(model.dtype)

# Inference: Generation of the output text and audio
text_ids, audio = model.generate(**inputs, 
                                 speaker="Ethan", 
                                 thinker_return_dict_in_generate=True,
                                 use_audio_in_video=USE_AUDIO_IN_VIDEO)

text = processor.batch_decode(text_ids.sequences[:, inputs["input_ids"].shape[1] :],
                              skip_special_tokens=True,
                              clean_up_tokenization_spaces=False)
print(text)
if audio is not None:
    sf.write(
        "output.wav",
        audio.reshape(-1).detach().cpu().numpy(),
        samplerate=24000,
    )&lt;/code&gt;
    &lt;p&gt;Here are some more advanced usage examples. You can expand the sections below to learn more.&lt;/p&gt;
    &lt;head&gt;Batch inference&lt;/head&gt;
    &lt;p&gt;The model can batch inputs composed of mixed samples of various types such as text, images, audio, and videos as input when &lt;code&gt;return_audio=False&lt;/code&gt; is set. Here is an example.&lt;/p&gt;
    &lt;code&gt;from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Instruct"
# MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"

model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    MODEL_PATH,
    dtype="auto",
    device_map="auto",
    attn_implementation="flash_attention_2",
)
model.disable_talker()

processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

# Conversation with image only
conversation1 = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},
            {"type": "text", "text": "What can you see in this image? Answer in one sentence."},
        ]
    }
]

# Conversation with audio only
conversation2 = [
    {
        "role": "user",
        "content": [
            {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"},
            {"type": "text", "text": "What can you hear in this audio?"},
        ]
    }
]

# Conversation with pure text and system prompt
conversation3 = [
    {
        "role": "system",
        "content": [
            {"type": "text", "text": "You are Qwen-Omni."}
        ],
    },
    {
        "role": "user",
        "content": "Who are you?"
    }
]

# Conversation with mixed media
conversation4 = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},
            {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"},
            {"type": "text", "text": "What can you see and hear? Answer in one sentence."}
        ],
    }
]

# Combine messages for batch processing
conversations = [conversation1, conversation2, conversation3, conversation4]

# Set whether to use audio in video
USE_AUDIO_IN_VIDEO = True

# Preparation for batch inference
text = processor.apply_chat_template(conversations, add_generation_prompt=True, tokenize=False)
audios, images, videos = process_mm_info(conversations, use_audio_in_video=USE_AUDIO_IN_VIDEO)

inputs = processor(text=text, 
                   audio=audios, 
                   images=images, 
                   videos=videos, 
                   return_tensors="pt", 
                   padding=True, 
                   use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = inputs.to(model.device).to(model.dtype)

# Batch inference does not support returning audio
text_ids, audio = model.generate(**inputs,
                                 return_audio=False,
                                 thinker_return_dict_in_generate=True,
                                 use_audio_in_video=USE_AUDIO_IN_VIDEO)

text = processor.batch_decode(text_ids.sequences[:, inputs["input_ids"].shape[1] :],
                              skip_special_tokens=True,
                              clean_up_tokenization_spaces=False)
print(text)&lt;/code&gt;
    &lt;head&gt;Use audio output or not&lt;/head&gt;
    &lt;p&gt;The model supports both text and audio outputs. If users do not need audio outputs, they can call &lt;code&gt;model.disable_talker()&lt;/code&gt; after initializing the model. This option will save about &lt;code&gt;10GB&lt;/code&gt; of GPU memory, but the &lt;code&gt;return_audio&lt;/code&gt; option for the &lt;code&gt;generate&lt;/code&gt; function will only allow &lt;code&gt;False&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    "Qwen/Qwen3-Omni-30B-A3B-Instruct",
    dtype="auto",
    device_map="auto",
    attn_implementation="flash_attention_2",
)
model.disable_talker()&lt;/code&gt;
    &lt;p&gt;For a more flexible experience, we recommend that users decide whether to return audio when the &lt;code&gt;generate&lt;/code&gt; function is called. If &lt;code&gt;return_audio&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt;, the model will only return text outputs, resulting in faster text responses.&lt;/p&gt;
    &lt;code&gt;model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    "Qwen/Qwen3-Omni-30B-A3B-Instruct",
    dtype="auto",
    device_map="auto",
    attn_implementation="flash_attention_2",
)
...
text_ids, _ = model.generate(..., return_audio=False)```

&amp;lt;/details&amp;gt;

&amp;lt;details&amp;gt;
&amp;lt;summary&amp;gt;Change voice type of output audio&amp;lt;/summary&amp;gt;

Qwen3-Omni supports changing the voice of the output audio. The `"Qwen/Qwen3-Omni-30B-A3B-Instruct"` checkpoint supports three voice types as follows:

| Voice Type | Gender | Description |
|------------|--------|-------------|
| Ethan      | Male   | A bright, upbeat voice with infectious energy and a warm, approachable vibe. |
| Chelsie    | Female | A honeyed, velvety voice that carries a gentle warmth and luminous clarity. |
| Aiden      | Male   | A warm, laid-back American voice with a gentle, boyish charm. |

Users can use the `speaker` parameter of the `generate` function to specify the voice type. By default, if `speaker` is not specified, the voice type is `Ethan`.

```python
text_ids, audio = model.generate(..., speaker="Ethan")&lt;/code&gt;
    &lt;code&gt;text_ids, audio = model.generate(..., speaker="Chelsie")&lt;/code&gt;
    &lt;code&gt;text_ids, audio = model.generate(..., speaker="Aiden")&lt;/code&gt;
    &lt;p&gt;Additionally, for more usage details such as prompt settings, task-specific usage methods, and resource requirements, please refer to Usage Tips and Cookbooks for Usage Cases.&lt;/p&gt;
    &lt;p&gt;We strongly recommend using vLLM for inference and deployment of the Qwen3-Omni series models. Since our code is currently in the pull request stage, and audio output inference support for the Instruct model will be released in the near future, you can follow the commands below to install vLLM from source. Please note that we recommend you create a new Python environment or use our provided Docker to avoid runtime environment conflicts and incompatibilities. For more details on compiling vLLM from source, please refer to the vLLM official documentation.&lt;/p&gt;
    &lt;code&gt;git clone -b qwen3_omni https://github.com/wangxiongts/vllm.git
cd vllm
pip install -r requirements/build.txt
pip install -r requirements/cuda.txt
export VLLM_PRECOMPILED_WHEEL_LOCATION=https://wheels.vllm.ai/a5dd03c1ebc5e4f56f3c9d3dc0436e9c582c978f/vllm-0.9.2-cp38-abi3-manylinux1_x86_64.whl
VLLM_USE_PRECOMPILED=1 pip install -e . -v --no-build-isolation
# If you meet an "Undefined symbol" error while using VLLM_USE_PRECOMPILED=1, please use "pip install -e . -v" to build from source.
# Install the Transformers
pip install git+https://github.com/huggingface/transformers
pip install accelerate
pip install qwen-omni-utils -U
pip install -U flash-attn --no-build-isolation&lt;/code&gt;
    &lt;p&gt;You can use the following code for vLLM inference. The &lt;code&gt;limit_mm_per_prompt&lt;/code&gt; parameter specifies the maximum number of each modality's data allowed per message. Since vLLM needs to pre-allocate GPU memory, larger values will require more GPU memory; if OOM issues occur, try reducing this value. Setting &lt;code&gt;tensor_parallel_size&lt;/code&gt; greater than one enables multi-GPU parallel inference, improving concurrency and throughput. In addition, &lt;code&gt;max_num_seqs&lt;/code&gt; indicates the number of sequences that vLLM processes in parallel during each inference step. A larger value requires more GPU memory but enables higher batch inference speed. For more details, please refer to the vLLM official documentation. Below is a simple example of how to run Qwen3-Omni with vLLM:&lt;/p&gt;
    &lt;code&gt;import os
import torch

from vllm import LLM, SamplingParams
from transformers import Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

if __name__ == '__main__':
    # vLLM engine v1 not supported yet
    os.environ['VLLM_USE_V1'] = '0'

    MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Instruct"
    # MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"

    llm = LLM(
            model=MODEL_PATH, trust_remote_code=True, gpu_memory_utilization=0.95,
            tensor_parallel_size=torch.cuda.device_count(),
            limit_mm_per_prompt={'image': 3, 'video': 3, 'audio': 3},
            max_num_seqs=8,
            max_model_len=32768,
            seed=1234,
    )

    sampling_params = SamplingParams(
        temperature=0.6,
        top_p=0.95,
        top_k=20,
        max_tokens=16384,
    )

    processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "video", "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/draw.mp4"}
            ], 
        }
    ]

    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    audios, images, videos = process_mm_info(messages, use_audio_in_video=True)

    inputs = {
        'prompt': text,
        'multi_modal_data': {},
        "mm_processor_kwargs": {
            "use_audio_in_video": True,
        },
    }

    if images is not None:
        inputs['multi_modal_data']['image'] = images
    if videos is not None:
        inputs['multi_modal_data']['video'] = videos
    if audios is not None:
        inputs['multi_modal_data']['audio'] = audios

    outputs = llm.generate([inputs], sampling_params=sampling_params)

    print(outputs[0].outputs[0].text)&lt;/code&gt;
    &lt;p&gt;Here are some more advanced usage examples. You can expand the sections below to learn more.&lt;/p&gt;
    &lt;head&gt;Batch inference&lt;/head&gt;
    &lt;p&gt;Using vLLM enables fast batch inference, which can help you efficiently process large volumes of data or conduct benchmarking. Refer to the following code example:&lt;/p&gt;
    &lt;code&gt;import os
import torch

from vllm import LLM, SamplingParams
from transformers import Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

def build_input(processor, messages, use_audio_in_video):
    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    audios, images, videos = process_mm_info(messages, use_audio_in_video=use_audio_in_video)

    inputs = {
        'prompt': text,
        'multi_modal_data': {},
        "mm_processor_kwargs": {
            "use_audio_in_video": use_audio_in_video,
        },
    }

    if images is not None:
        inputs['multi_modal_data']['image'] = images
    if videos is not None:
        inputs['multi_modal_data']['video'] = videos
    if audios is not None:
        inputs['multi_modal_data']['audio'] = audios
    
    return inputs

if __name__ == '__main__':
    # vLLM engine v1 not supported yet
    os.environ['VLLM_USE_V1'] = '0'

    MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Instruct"
    # MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"

    llm = LLM(
            model=MODEL_PATH, trust_remote_code=True, gpu_memory_utilization=0.95,
            tensor_parallel_size=torch.cuda.device_count(),
            limit_mm_per_prompt={'image': 3, 'video': 3, 'audio': 3},
            max_num_seqs=8,
            max_model_len=32768,
            seed=1234,
    )

    sampling_params = SamplingParams(
        temperature=0.6,
        top_p=0.95,
        top_k=20,
        max_tokens=16384,
    )

    processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

    # Conversation with image only
    conversation1 = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},
                {"type": "text", "text": "What can you see in this image? Answer in one sentence."},
            ]
        }
    ]

    # Conversation with audio only
    conversation2 = [
        {
            "role": "user",
            "content": [
                {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"},
                {"type": "text", "text": "What can you hear in this audio?"},
            ]
        }
    ]

    # Conversation with pure text and system prompt
    conversation3 = [
        {
            "role": "system",
            "content": [
                {"type": "text", "text": "You are Qwen-Omni."}
            ],
        },
        {
            "role": "user",
            "content": "Who are you? Answer in one sentence."
        }
    ]

    # Conversation with mixed media
    conversation4 = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},
                {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/cookbook/asr_fr.wav"},
                {"type": "text", "text": "What can you see and hear? Answer in one sentence."}
            ],
        }
    ]
    
    USE_AUDIO_IN_VIDEO = True

    # Combine messages for batch processing
    conversations = [conversation1, conversation2, conversation3, conversation4]
    inputs = [build_input(processor, messages, USE_AUDIO_IN_VIDEO) for messages in conversations]

    outputs = llm.generate(inputs, sampling_params=sampling_params)

    result = [outputs[i].outputs[0].text for i in range(len(outputs))]
    print(result)&lt;/code&gt;
    &lt;head&gt;vLLM Serve Usage&lt;/head&gt;
    &lt;p&gt;vLLM serve for Qwen3-Omni currently only supports the thinker model. The &lt;code&gt;use_audio_in_video&lt;/code&gt; parameter is not available in vLLM serve; you can handle this by separately passing video and audio inputs for processing. You can start vLLM serve through the following command:&lt;/p&gt;
    &lt;code&gt;# Qwen3-Omni-30B-A3B-Instruct for single GPU
vllm serve Qwen/Qwen3-Omni-30B-A3B-Instruct --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 32768 --allowed-local-media-path / -tp 1
# Qwen3-Omni-30B-A3B-Instruct for multi-GPU (example on 4 GPUs)
vllm serve Qwen/Qwen3-Omni-30B-A3B-Instruct --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 65536 --allowed-local-media-path / -tp 4
# Qwen/Qwen3-Omni-30B-A3B-Thinking for single GPU
vllm serve Qwen/Qwen3-Omni-30B-A3B-Thinking --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 32768 --allowed-local-media-path / -tp 1
# Qwen/Qwen3-Omni-30B-A3B-Thinking for multi-GPU (example on 4 GPUs)
vllm serve Qwen/Qwen3-Omni-30B-A3B-Thinking --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 65536 --allowed-local-media-path / -tp 4&lt;/code&gt;
    &lt;p&gt;Then you can use the chat API as below (via curl, for example):&lt;/p&gt;
    &lt;code&gt;curl http://localhost:8901/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
    "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": [
        {"type": "image_url", "image_url": {"url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"}},
        {"type": "audio_url", "audio_url": {"url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"}},
        {"type": "text", "text": "What can you see and hear? Answer in one sentence."}
    ]}
    ]
    }'&lt;/code&gt;
    &lt;p&gt;Additionally, for more usage details such as prompt settings, task-specific usage methods, and resource requirements, please refer to Usage Tips and Cookbooks for Usage Cases.&lt;/p&gt;
    &lt;p&gt;To further explore Qwen3-Omni, we encourage you to try our DashScope API for a faster and more efficient experience. For detailed API information and documentation, please refer to the following:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;API Description&lt;/cell&gt;
        &lt;cell role="head"&gt;API Documentation (Mainland China)&lt;/cell&gt;
        &lt;cell role="head"&gt;API Documentation (International)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Offline API for Qwen3-Omni-Flash, including Instruct and Thinking models&lt;/cell&gt;
        &lt;cell&gt;https://help.aliyun.com/zh/model-studio/qwen-omni&lt;/cell&gt;
        &lt;cell&gt;https://www.alibabacloud.com/help/en/model-studio/qwen-omni&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Real-time API for Qwen3-Omni-Flash, supporting end-to-end real-time interaction&lt;/cell&gt;
        &lt;cell&gt;https://help.aliyun.com/zh/model-studio/realtime&lt;/cell&gt;
        &lt;cell&gt;https://www.alibabacloud.com/help/en/model-studio/realtime&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;API for Qwen3-Omni-30B-A3B-Captioner model&lt;/cell&gt;
        &lt;cell&gt;https://help.aliyun.com/zh/model-studio/qwen3-omni-captioner&lt;/cell&gt;
        &lt;cell&gt;https://www.alibabacloud.com/help/zh/model-studio/qwen3-omni-captioner&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Precision&lt;/cell&gt;
        &lt;cell role="head"&gt;15s Video&lt;/cell&gt;
        &lt;cell role="head"&gt;30s Video&lt;/cell&gt;
        &lt;cell role="head"&gt;60s Video&lt;/cell&gt;
        &lt;cell role="head"&gt;120s Video&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell&gt;BF16&lt;/cell&gt;
        &lt;cell&gt;78.85 GB&lt;/cell&gt;
        &lt;cell&gt;88.52 GB&lt;/cell&gt;
        &lt;cell&gt;107.74 GB&lt;/cell&gt;
        &lt;cell&gt;144.81 GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell&gt;BF16&lt;/cell&gt;
        &lt;cell&gt;68.74 GB&lt;/cell&gt;
        &lt;cell&gt;77.79 GB&lt;/cell&gt;
        &lt;cell&gt;95.76 GB&lt;/cell&gt;
        &lt;cell&gt;131.65 GB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note: The table above presents the theoretical minimum memory requirements for inference with &lt;code&gt;transformers&lt;/code&gt; and &lt;code&gt;BF16&lt;/code&gt; precision, tested with &lt;code&gt;attn_implementation="flash_attention_2"&lt;/code&gt;. The Instruct model includes both the thinker and talker components, whereas the Thinking model includes only the thinker part.&lt;/p&gt;
    &lt;p&gt;When using Qwen3-Omni for audio-visual multimodal interaction, where the input consists of a video and its corresponding audio (with the audio serving as a query), we recommend using the following system prompt. This setup helps the model maintain high reasoning capability while better assuming interactive roles such as a smart assistant. Additionally, the text generated by the thinker will be more readable, with a natural, conversational tone and without complex formatting that is difficult to vocalize, leading to more stable and fluent audio output from the talker. You can customize the &lt;code&gt;user_system_prompt&lt;/code&gt; field in the system prompt to include character settings or other role-specific descriptions as needed.&lt;/p&gt;
    &lt;code&gt;user_system_prompt = "You are Qwen-Omni, a smart voice assistant created by Alibaba Qwen."
message = {
    "role": "system",
    "content": [
          {"type": "text", "text": f"{user_system_prompt} You are a virtual voice assistant with no gender or age.\nYou are communicating with the user.\nIn user messages, “I/me/my/we/our” refer to the user and “you/your” refer to the assistant. In your replies, address the user as “you/your” and yourself as “I/me/my”; never mirror the user’s pronouns—always shift perspective. Keep original pronouns only in direct quotes; if a reference is unclear, ask a brief clarifying question.\nInteract with users using short(no more than 50 words), brief, straightforward language, maintaining a natural tone.\nNever use formal phrasing, mechanical expressions, bullet points, overly structured language. \nYour output must consist only of the spoken content you want the user to hear. \nDo not include any descriptions of actions, emotions, sounds, or voice changes. \nDo not use asterisks, brackets, parentheses, or any other symbols to indicate tone or actions. \nYou must answer users' audio or text questions, do not directly describe the video content. \nYou should communicate in the same language strictly as the user unless they request otherwise.\nWhen you are uncertain (e.g., you can't see/hear clearly, don't understand, or the user makes a comment rather than asking a question), use appropriate questions to guide the user to continue the conversation.\nKeep replies concise and conversational, as if talking face-to-face."}
    ]
}
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/code&gt; model is primarily designed for understanding and interacting with multimodal inputs, including text, audio, image, and video. To achieve optimal performance, we recommend that users include an explicit textual instruction or task description in each round of dialogue alongside the multimodal input. This helps clarify the intent and significantly enhances the model's ability to leverage its reasoning capabilities. For example:&lt;/p&gt;
    &lt;code&gt;messages = [
    {
        "role": "user",
        "content": [
            {"type": "audio", "audio": "/path/to/audio.wav"},
            {"type": "image", "image": "/path/to/image.png"},
            {"type": "video", "video": "/path/to/video.mp4"},
            {"type": "text", "text": "Analyze this audio, image, and video together."},
        ], 
    }
]&lt;/code&gt;
    &lt;p&gt;In multimodal interaction, user-provided videos are often accompanied by audio (such as spoken questions or sounds from events in the video). This information helps the model provide a better interactive experience. We provide the following options for users to decide whether to use the audio from a video.&lt;/p&gt;
    &lt;code&gt;# In data preprocessing
audios, images, videos = process_mm_info(messages, use_audio_in_video=True)&lt;/code&gt;
    &lt;code&gt;# For Transformers
text = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors="pt", 
                   padding=True, use_audio_in_video=True)
text_ids, audio = model.generate(..., use_audio_in_video=True)

# For vLLM
text = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
inputs = {
    'prompt': text,
    'multi_modal_data': {},
    "mm_processor_kwargs": {
        "use_audio_in_video": True,
    },
}&lt;/code&gt;
    &lt;p&gt;It is worth noting that during a multi-round conversation, the &lt;code&gt;use_audio_in_video&lt;/code&gt; parameter must be set consistently across these steps; otherwise, unexpected results may occur.&lt;/p&gt;
    &lt;p&gt;Without local deployment, you can experience an online web demo directly by visiting our Hugging Face Spaces and ModelScope Studio. This includes quick hands-on experiences for Qwen3-Omni-Realtime, Qwen3-Omni (Instruct and Thinking), and Qwen3-Omni-30B-A3B-Captioner.&lt;/p&gt;
    &lt;p&gt;Real-time streaming interaction with Qwen3-Omni is available now. Please visit Qwen Chat and select the voice/video call option in the chat box to experience it.&lt;/p&gt;
    &lt;p&gt;In this section, we provide instructions for users to build a web-based user interface (UI) demo. This UI demo allows users to interact with the model through a web browser. Follow the steps below to get start :)&lt;/p&gt;
    &lt;p&gt;Before you begin, we strongly recommend that you refer to the Installation section in vLLM Usage to set up your environment, which will allow you to seamlessly use both the vLLM and Transformers backends. However, if you only intend to use the Transformers backend (note that this will result in significantly slower inference), please follow the installation instructions in Transformers Usage. That said, we still highly recommend using our Docker image to avoid potential environment-related issues. Additionally, if you are running locally, make sure your system has &lt;code&gt;ffmpeg&lt;/code&gt; installed and you install the following dependencies:&lt;/p&gt;
    &lt;code&gt;pip install gradio==5.44.1 gradio_client==1.12.1 soundfile==0.13.1&lt;/code&gt;
    &lt;p&gt;Once the required packages are installed, you can launch the web demo using the following commands. These commands will start a web server and provide you with a link to access the UI in your web browser. You can run &lt;code&gt;python web_demo.py --help&lt;/code&gt; and &lt;code&gt;python web_demo_captioner.py --help&lt;/code&gt; to learn about more options.&lt;/p&gt;
    &lt;code&gt;# For Qwen3-Omni-30B-A3B-Instruct with vLLM backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct
# For Qwen3-Omni-30B-A3B-Instruct with Transformers backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct --use-transformers --generate-audio
# For Qwen3-Omni-30B-A3B-Instruct with Transformers backend and FlashAttention support
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct --use-transformers --generate-audio --flash-attn2&lt;/code&gt;
    &lt;code&gt;# For Qwen3-Omni-30B-A3B-Thinking with vLLM backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Thinking
# For Qwen3-Omni-30B-A3B-Thinking with Transformers backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Thinking --use-transformers
# For Qwen3-Omni-30B-A3B-Thinking with Transformers backend and FlashAttention support
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Thinking --use-transformers --flash-attn2&lt;/code&gt;
    &lt;code&gt;# For Qwen3-Omni-30B-A3B-Captioner with vLLM backend
python web_demo_captioner.py -c Qwen/Qwen3-Omni-30B-A3B-Captioner
# For Qwen3-Omni-30B-A3B-Captioner with Transformers backend
python web_demo_captioner.py -c Qwen/Qwen3-Omni-30B-A3B-Captioner --use-transformers
# For Qwen3-Omni-30B-A3B-Captioner with Transformers backend and FlashAttention support
python web_demo_captioner.py -c Qwen/Qwen3-Omni-30B-A3B-Captioner --use-transformers --flash-attn2&lt;/code&gt;
    &lt;p&gt;After running the command, you’ll see a link generated in the terminal similar to this:&lt;/p&gt;
    &lt;code&gt;Running on local: http://127.0.0.1:8901/
&lt;/code&gt;
    &lt;p&gt;If you are running locally, copy this link and paste it into your browser to access the web UI. If you are running on a server or in a &lt;code&gt;docker&lt;/code&gt; container, please configure the address according to the server's actual IP, or set up port forwarding where necessary. For instructions on how to configure port forwarding from the official &lt;code&gt;docker&lt;/code&gt; container to the host machine, please refer to here.&lt;/p&gt;
    &lt;p&gt;To simplify the deployment process, we provide Docker images with pre-built environments: qwenllm/qwen3-omni. You only need to install the driver and download model files to launch the demos. Please refer to the guide to install the NVIDIA Container Toolkit, ensuring that your Docker can access the GPU. For users in mainland China who may have difficulty accessing Docker Hub, you can use mirror acceleration services to pull the images. First, run the following command to pull and initialize the container:&lt;/p&gt;
    &lt;code&gt;LOCAL_WORKDIR=/path/to/your/workspace
HOST_PORT=8901
CONTAINER_PORT=80
docker run --gpus all --name qwen3-omni \
    -v /var/run/docker.sock:/var/run/docker.sock -p $HOST_PORT:$CONTAINER_PORT \
    --mount type=bind,source=$LOCAL_WORKDIR,target=/data/shared/Qwen3-Omni \
    --shm-size=4gb \
    -it qwenllm/qwen3-omni:3-cu124&lt;/code&gt;
    &lt;p&gt;After executing the command, you will enter the bash shell of the container. Your local model and data directory (please replace &lt;code&gt;/path/to/your/workspace&lt;/code&gt; with the actual path) will be mounted to the container's internal path &lt;code&gt;/data/shared/Qwen3-Omni&lt;/code&gt;. The host's port &lt;code&gt;8901&lt;/code&gt; is mapped to port &lt;code&gt;80&lt;/code&gt; in the container, meaning you can access the service inside the container by visiting port &lt;code&gt;8901&lt;/code&gt; on the host machine.&lt;/p&gt;
    &lt;p&gt;Please note that services inside the container must be started with the IP &lt;code&gt;0.0.0.0&lt;/code&gt; to ensure proper port forwarding. For example:&lt;/p&gt;
    &lt;code&gt;# Run this command inside the Docker container
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct --server-port 80 --server-name 0.0.0.0&lt;/code&gt;
    &lt;p&gt;For more ways to launch the web demo, please refer to Launch Local Web UI Demo. If you exit the container, you can re-enter it using the following command:&lt;/p&gt;
    &lt;code&gt;docker start qwen3-omni
docker exec -it qwen3-omni bash&lt;/code&gt;
    &lt;p&gt;Or if you want to completely remove the container, please run:&lt;/p&gt;
    &lt;code&gt;docker rm -f qwen3-omni&lt;/code&gt;
    &lt;p&gt;Qwen3-Omni maintains state-of-the-art performance on text and visual modalities without degradation relative to same-size single-model Qwen counterparts. Across 36 audio and audio-visual benchmarks, it achieves open-source SOTA on 32 and sets the SOTA on 22, outperforming strong closed-source systems such as Gemini 2.5 Pro and GPT-4o.&lt;/p&gt;
    &lt;head&gt;Text -&amp;gt; Text&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;GPT-4o-0327&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-235B-A22B&lt;p&gt;Non Thinking&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-30B-A3B-Instruct-2507&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Instruct&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;General&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;MMLU-Redux&lt;/cell&gt;
        &lt;cell&gt;91.3&lt;/cell&gt;
        &lt;cell&gt;89.2&lt;/cell&gt;
        &lt;cell&gt;89.3&lt;/cell&gt;
        &lt;cell&gt;86.6&lt;/cell&gt;
        &lt;cell&gt;86.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;GPQA&lt;/cell&gt;
        &lt;cell&gt;66.9&lt;/cell&gt;
        &lt;cell&gt;62.9&lt;/cell&gt;
        &lt;cell&gt;70.4&lt;/cell&gt;
        &lt;cell&gt;69.6&lt;/cell&gt;
        &lt;cell&gt;69.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Reasoning&lt;/cell&gt;
        &lt;cell&gt;AIME25&lt;/cell&gt;
        &lt;cell&gt;26.7&lt;/cell&gt;
        &lt;cell&gt;24.7&lt;/cell&gt;
        &lt;cell&gt;61.3&lt;/cell&gt;
        &lt;cell&gt;65.0&lt;/cell&gt;
        &lt;cell&gt;65.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;ZebraLogic&lt;/cell&gt;
        &lt;cell&gt;52.6&lt;/cell&gt;
        &lt;cell&gt;37.7&lt;/cell&gt;
        &lt;cell&gt;90.0&lt;/cell&gt;
        &lt;cell&gt;76.0&lt;/cell&gt;
        &lt;cell&gt;76.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Code&lt;/cell&gt;
        &lt;cell&gt;MultiPL-E&lt;/cell&gt;
        &lt;cell&gt;82.7&lt;/cell&gt;
        &lt;cell&gt;79.3&lt;/cell&gt;
        &lt;cell&gt;83.8&lt;/cell&gt;
        &lt;cell&gt;81.4&lt;/cell&gt;
        &lt;cell&gt;81.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Alignment&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;IFEval&lt;/cell&gt;
        &lt;cell&gt;83.9&lt;/cell&gt;
        &lt;cell&gt;83.2&lt;/cell&gt;
        &lt;cell&gt;84.7&lt;/cell&gt;
        &lt;cell&gt;81.0&lt;/cell&gt;
        &lt;cell&gt;81.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Creative Writing v3&lt;/cell&gt;
        &lt;cell&gt;84.9&lt;/cell&gt;
        &lt;cell&gt;80.4&lt;/cell&gt;
        &lt;cell&gt;86.0&lt;/cell&gt;
        &lt;cell&gt;80.6&lt;/cell&gt;
        &lt;cell&gt;81.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;WritingBench&lt;/cell&gt;
        &lt;cell&gt;75.5&lt;/cell&gt;
        &lt;cell&gt;77.0&lt;/cell&gt;
        &lt;cell&gt;85.5&lt;/cell&gt;
        &lt;cell&gt;82.6&lt;/cell&gt;
        &lt;cell&gt;83.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Agent&lt;/cell&gt;
        &lt;cell&gt;BFCL-v3&lt;/cell&gt;
        &lt;cell&gt;66.5&lt;/cell&gt;
        &lt;cell&gt;68.0&lt;/cell&gt;
        &lt;cell&gt;65.1&lt;/cell&gt;
        &lt;cell&gt;64.4&lt;/cell&gt;
        &lt;cell&gt;65.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Multilingual&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;MultiIF&lt;/cell&gt;
        &lt;cell&gt;70.4&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;67.9&lt;/cell&gt;
        &lt;cell&gt;64.0&lt;/cell&gt;
        &lt;cell&gt;64.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;PolyMATH&lt;/cell&gt;
        &lt;cell&gt;25.5&lt;/cell&gt;
        &lt;cell&gt;27.0&lt;/cell&gt;
        &lt;cell&gt;43.1&lt;/cell&gt;
        &lt;cell&gt;37.9&lt;/cell&gt;
        &lt;cell&gt;39.3&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Flash&lt;p&gt;Thinking&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-235B-A22B&lt;p&gt;Thinking&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-30B-A3B-Thinking-2507&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;General&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;MMLU-Redux&lt;/cell&gt;
        &lt;cell&gt;92.1&lt;/cell&gt;
        &lt;cell&gt;92.7&lt;/cell&gt;
        &lt;cell&gt;91.4&lt;/cell&gt;
        &lt;cell&gt;88.8&lt;/cell&gt;
        &lt;cell&gt;89.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;GPQA&lt;/cell&gt;
        &lt;cell&gt;82.8&lt;/cell&gt;
        &lt;cell&gt;71.1&lt;/cell&gt;
        &lt;cell&gt;73.4&lt;/cell&gt;
        &lt;cell&gt;73.1&lt;/cell&gt;
        &lt;cell&gt;73.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Reasoning&lt;/cell&gt;
        &lt;cell&gt;AIME25&lt;/cell&gt;
        &lt;cell&gt;72.0&lt;/cell&gt;
        &lt;cell&gt;81.5&lt;/cell&gt;
        &lt;cell&gt;85.0&lt;/cell&gt;
        &lt;cell&gt;73.7&lt;/cell&gt;
        &lt;cell&gt;74.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;LiveBench 20241125&lt;/cell&gt;
        &lt;cell&gt;74.3&lt;/cell&gt;
        &lt;cell&gt;77.1&lt;/cell&gt;
        &lt;cell&gt;76.8&lt;/cell&gt;
        &lt;cell&gt;71.8&lt;/cell&gt;
        &lt;cell&gt;70.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Code&lt;/cell&gt;
        &lt;cell&gt;MultiPL-E&lt;/cell&gt;
        &lt;cell&gt;84.5&lt;/cell&gt;
        &lt;cell&gt;79.9&lt;/cell&gt;
        &lt;cell&gt;81.3&lt;/cell&gt;
        &lt;cell&gt;80.6&lt;/cell&gt;
        &lt;cell&gt;81.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Alignment&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;IFEval&lt;/cell&gt;
        &lt;cell&gt;89.8&lt;/cell&gt;
        &lt;cell&gt;83.4&lt;/cell&gt;
        &lt;cell&gt;88.9&lt;/cell&gt;
        &lt;cell&gt;85.1&lt;/cell&gt;
        &lt;cell&gt;85.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Arena-Hard v2&lt;/cell&gt;
        &lt;cell&gt;56.7&lt;/cell&gt;
        &lt;cell&gt;61.5&lt;/cell&gt;
        &lt;cell&gt;56.0&lt;/cell&gt;
        &lt;cell&gt;55.1&lt;/cell&gt;
        &lt;cell&gt;57.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Creative Writing v3&lt;/cell&gt;
        &lt;cell&gt;85.0&lt;/cell&gt;
        &lt;cell&gt;84.6&lt;/cell&gt;
        &lt;cell&gt;84.4&lt;/cell&gt;
        &lt;cell&gt;82.5&lt;/cell&gt;
        &lt;cell&gt;83.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;WritingBench&lt;/cell&gt;
        &lt;cell&gt;83.9&lt;/cell&gt;
        &lt;cell&gt;80.3&lt;/cell&gt;
        &lt;cell&gt;85.0&lt;/cell&gt;
        &lt;cell&gt;85.5&lt;/cell&gt;
        &lt;cell&gt;85.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Agent&lt;/cell&gt;
        &lt;cell&gt;BFCL-v3&lt;/cell&gt;
        &lt;cell&gt;68.6&lt;/cell&gt;
        &lt;cell&gt;70.8&lt;/cell&gt;
        &lt;cell&gt;72.4&lt;/cell&gt;
        &lt;cell&gt;63.2&lt;/cell&gt;
        &lt;cell&gt;64.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Multilingual&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;MultiIF&lt;/cell&gt;
        &lt;cell&gt;74.4&lt;/cell&gt;
        &lt;cell&gt;71.9&lt;/cell&gt;
        &lt;cell&gt;76.4&lt;/cell&gt;
        &lt;cell&gt;72.9&lt;/cell&gt;
        &lt;cell&gt;73.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;PolyMATH&lt;/cell&gt;
        &lt;cell&gt;49.8&lt;/cell&gt;
        &lt;cell&gt;54.7&lt;/cell&gt;
        &lt;cell&gt;52.6&lt;/cell&gt;
        &lt;cell&gt;47.1&lt;/cell&gt;
        &lt;cell&gt;48.7&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Audio -&amp;gt; Text&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="9"&gt;
        &lt;cell role="head"&gt;Seed-ASR&lt;/cell&gt;
        &lt;cell role="head"&gt;Voxtral-Mini&lt;/cell&gt;
        &lt;cell role="head"&gt;Voxtral-Small&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-4o-Transcribe&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen2.5-Omni&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Instruct&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;EN &amp;amp; ZH ASR (wer)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Wenetspeech&lt;p&gt;net | meeting&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;4.66 | 5.69&lt;/cell&gt;
        &lt;cell&gt;24.30 | 31.53&lt;/cell&gt;
        &lt;cell&gt;20.33 | 26.08&lt;/cell&gt;
        &lt;cell&gt;15.30 | 32.27&lt;/cell&gt;
        &lt;cell&gt;14.43 | 13.47&lt;/cell&gt;
        &lt;cell&gt;5.91 | 7.65&lt;/cell&gt;
        &lt;cell&gt;4.69 | 5.89&lt;/cell&gt;
        &lt;cell&gt;4.62 | 5.75&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Librispeech&lt;p&gt;clean | other&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;1.58 | 2.84&lt;/cell&gt;
        &lt;cell&gt;1.88 | 4.12&lt;/cell&gt;
        &lt;cell&gt;1.56 | 3.30&lt;/cell&gt;
        &lt;cell&gt;1.39 | 3.75&lt;/cell&gt;
        &lt;cell&gt;2.89 | 3.56&lt;/cell&gt;
        &lt;cell&gt;1.74 | 3.45&lt;/cell&gt;
        &lt;cell&gt;1.22 | 2.48&lt;/cell&gt;
        &lt;cell&gt;1.27 | 2.44&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;CV15-en&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;9.47&lt;/cell&gt;
        &lt;cell&gt;7.79&lt;/cell&gt;
        &lt;cell&gt;10.01&lt;/cell&gt;
        &lt;cell&gt;9.89&lt;/cell&gt;
        &lt;cell&gt;7.61&lt;/cell&gt;
        &lt;cell&gt;6.05&lt;/cell&gt;
        &lt;cell&gt;5.94&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;CV15-zh&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;24.67&lt;/cell&gt;
        &lt;cell&gt;19.30&lt;/cell&gt;
        &lt;cell&gt;9.84&lt;/cell&gt;
        &lt;cell&gt;8.00&lt;/cell&gt;
        &lt;cell&gt;5.13&lt;/cell&gt;
        &lt;cell&gt;4.31&lt;/cell&gt;
        &lt;cell&gt;4.28&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-en&lt;/cell&gt;
        &lt;cell&gt;3.40&lt;/cell&gt;
        &lt;cell&gt;3.96&lt;/cell&gt;
        &lt;cell&gt;3.77&lt;/cell&gt;
        &lt;cell&gt;3.32&lt;/cell&gt;
        &lt;cell&gt;2.94&lt;/cell&gt;
        &lt;cell&gt;3.77&lt;/cell&gt;
        &lt;cell&gt;2.72&lt;/cell&gt;
        &lt;cell&gt;2.74&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-zh&lt;/cell&gt;
        &lt;cell&gt;2.69&lt;/cell&gt;
        &lt;cell&gt;12.22&lt;/cell&gt;
        &lt;cell&gt;7.98&lt;/cell&gt;
        &lt;cell&gt;2.44&lt;/cell&gt;
        &lt;cell&gt;2.71&lt;/cell&gt;
        &lt;cell&gt;2.54&lt;/cell&gt;
        &lt;cell&gt;2.20&lt;/cell&gt;
        &lt;cell&gt;2.19&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Multilingual ASR (wer)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-avg&lt;p&gt;(19 lang)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;15.67&lt;/cell&gt;
        &lt;cell&gt;8.09&lt;/cell&gt;
        &lt;cell&gt;4.48&lt;/cell&gt;
        &lt;cell&gt;5.55&lt;/cell&gt;
        &lt;cell&gt;14.04&lt;/cell&gt;
        &lt;cell&gt;5.33&lt;/cell&gt;
        &lt;cell&gt;5.31&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Lyric ASR (wer)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;MIR-1K (vocal-only)&lt;/cell&gt;
        &lt;cell&gt;6.45&lt;/cell&gt;
        &lt;cell&gt;23.33&lt;/cell&gt;
        &lt;cell&gt;18.73&lt;/cell&gt;
        &lt;cell&gt;11.87&lt;/cell&gt;
        &lt;cell&gt;9.85&lt;/cell&gt;
        &lt;cell&gt;8.15&lt;/cell&gt;
        &lt;cell&gt;5.90&lt;/cell&gt;
        &lt;cell&gt;5.85&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Opencpop-test&lt;/cell&gt;
        &lt;cell&gt;2.98&lt;/cell&gt;
        &lt;cell&gt;31.01&lt;/cell&gt;
        &lt;cell&gt;16.06&lt;/cell&gt;
        &lt;cell&gt;7.93&lt;/cell&gt;
        &lt;cell&gt;6.49&lt;/cell&gt;
        &lt;cell&gt;2.84&lt;/cell&gt;
        &lt;cell&gt;1.54&lt;/cell&gt;
        &lt;cell&gt;2.02&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;S2TT (BLEU)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-en2xx&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;30.35&lt;/cell&gt;
        &lt;cell&gt;37.85&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;39.25&lt;/cell&gt;
        &lt;cell&gt;29.22&lt;/cell&gt;
        &lt;cell&gt;37.50&lt;/cell&gt;
        &lt;cell&gt;36.22&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-xx2en&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;27.54&lt;/cell&gt;
        &lt;cell&gt;32.81&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;35.41&lt;/cell&gt;
        &lt;cell&gt;28.61&lt;/cell&gt;
        &lt;cell&gt;31.08&lt;/cell&gt;
        &lt;cell&gt;30.71&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-zh2xx&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;17.03&lt;/cell&gt;
        &lt;cell&gt;22.05&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;26.63&lt;/cell&gt;
        &lt;cell&gt;17.97&lt;/cell&gt;
        &lt;cell&gt;25.17&lt;/cell&gt;
        &lt;cell&gt;25.10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Fleurs-xx2zh&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;28.75&lt;/cell&gt;
        &lt;cell&gt;34.82&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;37.50&lt;/cell&gt;
        &lt;cell&gt;27.68&lt;/cell&gt;
        &lt;cell&gt;33.13&lt;/cell&gt;
        &lt;cell&gt;31.19&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="9"&gt;
        &lt;cell role="head"&gt;GPT-4o-Audio&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Flash&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen2.5-Omni&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;VoiceBench&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;AlpacaEval&lt;/cell&gt;
        &lt;cell&gt;95.6&lt;/cell&gt;
        &lt;cell&gt;96.1&lt;/cell&gt;
        &lt;cell&gt;94.3&lt;/cell&gt;
        &lt;cell&gt;89.9&lt;/cell&gt;
        &lt;cell&gt;94.8&lt;/cell&gt;
        &lt;cell&gt;96.4&lt;/cell&gt;
        &lt;cell&gt;95.4&lt;/cell&gt;
        &lt;cell&gt;96.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;CommonEval&lt;/cell&gt;
        &lt;cell&gt;89.8&lt;/cell&gt;
        &lt;cell&gt;88.3&lt;/cell&gt;
        &lt;cell&gt;88.4&lt;/cell&gt;
        &lt;cell&gt;76.7&lt;/cell&gt;
        &lt;cell&gt;90.8&lt;/cell&gt;
        &lt;cell&gt;90.5&lt;/cell&gt;
        &lt;cell&gt;91.0&lt;/cell&gt;
        &lt;cell&gt;90.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;WildVoice&lt;/cell&gt;
        &lt;cell&gt;91.6&lt;/cell&gt;
        &lt;cell&gt;92.1&lt;/cell&gt;
        &lt;cell&gt;93.4&lt;/cell&gt;
        &lt;cell&gt;77.7&lt;/cell&gt;
        &lt;cell&gt;91.6&lt;/cell&gt;
        &lt;cell&gt;90.5&lt;/cell&gt;
        &lt;cell&gt;92.3&lt;/cell&gt;
        &lt;cell&gt;90.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;SD-QA&lt;/cell&gt;
        &lt;cell&gt;75.5&lt;/cell&gt;
        &lt;cell&gt;84.5&lt;/cell&gt;
        &lt;cell&gt;90.1&lt;/cell&gt;
        &lt;cell&gt;56.4&lt;/cell&gt;
        &lt;cell&gt;76.9&lt;/cell&gt;
        &lt;cell&gt;78.1&lt;/cell&gt;
        &lt;cell&gt;76.8&lt;/cell&gt;
        &lt;cell&gt;78.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;MMSU&lt;/cell&gt;
        &lt;cell&gt;80.3&lt;/cell&gt;
        &lt;cell&gt;66.1&lt;/cell&gt;
        &lt;cell&gt;71.1&lt;/cell&gt;
        &lt;cell&gt;61.7&lt;/cell&gt;
        &lt;cell&gt;68.1&lt;/cell&gt;
        &lt;cell&gt;83.0&lt;/cell&gt;
        &lt;cell&gt;68.4&lt;/cell&gt;
        &lt;cell&gt;84.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;OpenBookQA&lt;/cell&gt;
        &lt;cell&gt;89.2&lt;/cell&gt;
        &lt;cell&gt;56.9&lt;/cell&gt;
        &lt;cell&gt;92.3&lt;/cell&gt;
        &lt;cell&gt;80.9&lt;/cell&gt;
        &lt;cell&gt;89.7&lt;/cell&gt;
        &lt;cell&gt;94.3&lt;/cell&gt;
        &lt;cell&gt;91.4&lt;/cell&gt;
        &lt;cell&gt;95.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;BBH&lt;/cell&gt;
        &lt;cell&gt;84.1&lt;/cell&gt;
        &lt;cell&gt;83.9&lt;/cell&gt;
        &lt;cell&gt;92.6&lt;/cell&gt;
        &lt;cell&gt;66.7&lt;/cell&gt;
        &lt;cell&gt;80.4&lt;/cell&gt;
        &lt;cell&gt;88.9&lt;/cell&gt;
        &lt;cell&gt;80.6&lt;/cell&gt;
        &lt;cell&gt;89.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;IFEval&lt;/cell&gt;
        &lt;cell&gt;76.0&lt;/cell&gt;
        &lt;cell&gt;83.8&lt;/cell&gt;
        &lt;cell&gt;85.7&lt;/cell&gt;
        &lt;cell&gt;53.5&lt;/cell&gt;
        &lt;cell&gt;77.8&lt;/cell&gt;
        &lt;cell&gt;80.6&lt;/cell&gt;
        &lt;cell&gt;75.2&lt;/cell&gt;
        &lt;cell&gt;80.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;AdvBench&lt;/cell&gt;
        &lt;cell&gt;98.7&lt;/cell&gt;
        &lt;cell&gt;98.9&lt;/cell&gt;
        &lt;cell&gt;98.1&lt;/cell&gt;
        &lt;cell&gt;99.2&lt;/cell&gt;
        &lt;cell&gt;99.3&lt;/cell&gt;
        &lt;cell&gt;97.2&lt;/cell&gt;
        &lt;cell&gt;99.4&lt;/cell&gt;
        &lt;cell&gt;98.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Overall&lt;/cell&gt;
        &lt;cell&gt;86.8&lt;/cell&gt;
        &lt;cell&gt;83.4&lt;/cell&gt;
        &lt;cell&gt;89.6&lt;/cell&gt;
        &lt;cell&gt;73.6&lt;/cell&gt;
        &lt;cell&gt;85.5&lt;/cell&gt;
        &lt;cell&gt;88.8&lt;/cell&gt;
        &lt;cell&gt;85.6&lt;/cell&gt;
        &lt;cell&gt;89.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Audio Reasoning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;MMAU-v05.15.25&lt;/cell&gt;
        &lt;cell&gt;62.5&lt;/cell&gt;
        &lt;cell&gt;71.8&lt;/cell&gt;
        &lt;cell&gt;77.4&lt;/cell&gt;
        &lt;cell&gt;65.5&lt;/cell&gt;
        &lt;cell&gt;77.5&lt;/cell&gt;
        &lt;cell&gt;75.4&lt;/cell&gt;
        &lt;cell&gt;77.6&lt;/cell&gt;
        &lt;cell&gt;76.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;MMSU&lt;/cell&gt;
        &lt;cell&gt;56.4&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;77.7&lt;/cell&gt;
        &lt;cell&gt;62.6&lt;/cell&gt;
        &lt;cell&gt;69.0&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;69.1&lt;/cell&gt;
        &lt;cell&gt;71.3&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Best Specialist&lt;p&gt;Models&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-4o-Audio&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen2.5-Omni&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Instruct&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;RUL-MuchoMusic&lt;/cell&gt;
        &lt;cell&gt;47.6 (Audio Flamingo 3)&lt;/cell&gt;
        &lt;cell&gt;36.1&lt;/cell&gt;
        &lt;cell&gt;49.4&lt;/cell&gt;
        &lt;cell&gt;47.3&lt;/cell&gt;
        &lt;cell&gt;52.0&lt;/cell&gt;
        &lt;cell&gt;52.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;GTZAN&lt;p&gt;Acc.&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;87.9 (CLaMP 3)&lt;/cell&gt;
        &lt;cell&gt;76.5&lt;/cell&gt;
        &lt;cell&gt;81.0&lt;/cell&gt;
        &lt;cell&gt;81.7&lt;/cell&gt;
        &lt;cell&gt;93.0&lt;/cell&gt;
        &lt;cell&gt;93.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;MTG Genre&lt;p&gt;Micro F1&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;35.8 (MuQ-MuLan)&lt;/cell&gt;
        &lt;cell&gt;25.3&lt;/cell&gt;
        &lt;cell&gt;32.6&lt;/cell&gt;
        &lt;cell&gt;32.5&lt;/cell&gt;
        &lt;cell&gt;39.0&lt;/cell&gt;
        &lt;cell&gt;39.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;MTG Mood/Theme&lt;p&gt;Micro F1&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;10.9 (MuQ-MuLan)&lt;/cell&gt;
        &lt;cell&gt;11.3&lt;/cell&gt;
        &lt;cell&gt;14.1&lt;/cell&gt;
        &lt;cell&gt;8.9&lt;/cell&gt;
        &lt;cell&gt;21.0&lt;/cell&gt;
        &lt;cell&gt;21.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;MTG Instrument&lt;p&gt;Micro F1&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;39.8 (MuQ-MuLan)&lt;/cell&gt;
        &lt;cell&gt;34.2&lt;/cell&gt;
        &lt;cell&gt;33.0&lt;/cell&gt;
        &lt;cell&gt;22.6&lt;/cell&gt;
        &lt;cell&gt;40.5&lt;/cell&gt;
        &lt;cell&gt;40.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;MTG Top50&lt;p&gt;Micro F1&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;33.2 (MuQ-MuLan)&lt;/cell&gt;
        &lt;cell&gt;25.0&lt;/cell&gt;
        &lt;cell&gt;26.1&lt;/cell&gt;
        &lt;cell&gt;21.6&lt;/cell&gt;
        &lt;cell&gt;36.7&lt;/cell&gt;
        &lt;cell&gt;36.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;MagnaTagATune&lt;p&gt;Micro F1&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;41.6 (MuQ)&lt;/cell&gt;
        &lt;cell&gt;29.2&lt;/cell&gt;
        &lt;cell&gt;28.1&lt;/cell&gt;
        &lt;cell&gt;30.1&lt;/cell&gt;
        &lt;cell&gt;44.3&lt;/cell&gt;
        &lt;cell&gt;46.8&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Vision -&amp;gt; Text&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Datasets&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT4-o&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.0-Flash&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen2.5-VL&lt;p&gt;72B&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B&lt;p&gt;-Instruct&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash&lt;p&gt;-Instruct&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;General Visual Question Answering&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MMStar&lt;/cell&gt;
        &lt;cell&gt;64.7&lt;/cell&gt;
        &lt;cell&gt;71.4&lt;/cell&gt;
        &lt;cell&gt;70.8&lt;/cell&gt;
        &lt;cell&gt;68.5&lt;/cell&gt;
        &lt;cell&gt;69.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;HallusionBench&lt;/cell&gt;
        &lt;cell&gt;55.0&lt;/cell&gt;
        &lt;cell&gt;56.3&lt;/cell&gt;
        &lt;cell&gt;55.2&lt;/cell&gt;
        &lt;cell&gt;59.7&lt;/cell&gt;
        &lt;cell&gt;58.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MM-MT-Bench&lt;/cell&gt;
        &lt;cell&gt;7.7&lt;/cell&gt;
        &lt;cell&gt;6.7&lt;/cell&gt;
        &lt;cell&gt;7.6&lt;/cell&gt;
        &lt;cell&gt;7.4&lt;/cell&gt;
        &lt;cell&gt;7.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Math &amp;amp; STEM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MMMU_val&lt;/cell&gt;
        &lt;cell&gt;69.1&lt;/cell&gt;
        &lt;cell&gt;71.3&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;69.1&lt;/cell&gt;
        &lt;cell&gt;69.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MMMU_pro&lt;/cell&gt;
        &lt;cell&gt;51.9&lt;/cell&gt;
        &lt;cell&gt;56.1&lt;/cell&gt;
        &lt;cell&gt;51.1&lt;/cell&gt;
        &lt;cell&gt;57.0&lt;/cell&gt;
        &lt;cell&gt;57.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MathVista_mini&lt;/cell&gt;
        &lt;cell&gt;63.8&lt;/cell&gt;
        &lt;cell&gt;71.4&lt;/cell&gt;
        &lt;cell&gt;74.8&lt;/cell&gt;
        &lt;cell&gt;75.9&lt;/cell&gt;
        &lt;cell&gt;77.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MathVision_full&lt;/cell&gt;
        &lt;cell&gt;30.4&lt;/cell&gt;
        &lt;cell&gt;48.6&lt;/cell&gt;
        &lt;cell&gt;38.1&lt;/cell&gt;
        &lt;cell&gt;56.3&lt;/cell&gt;
        &lt;cell&gt;58.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Documentation Understanding&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;AI2D&lt;/cell&gt;
        &lt;cell&gt;84.6&lt;/cell&gt;
        &lt;cell&gt;86.7&lt;/cell&gt;
        &lt;cell&gt;88.7&lt;/cell&gt;
        &lt;cell&gt;85.2&lt;/cell&gt;
        &lt;cell&gt;86.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;ChartQA_test&lt;/cell&gt;
        &lt;cell&gt;86.7&lt;/cell&gt;
        &lt;cell&gt;64.6&lt;/cell&gt;
        &lt;cell&gt;89.5&lt;/cell&gt;
        &lt;cell&gt;86.8&lt;/cell&gt;
        &lt;cell&gt;87.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Counting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;CountBench&lt;/cell&gt;
        &lt;cell&gt;87.9&lt;/cell&gt;
        &lt;cell&gt;91.2&lt;/cell&gt;
        &lt;cell&gt;93.6&lt;/cell&gt;
        &lt;cell&gt;90.0&lt;/cell&gt;
        &lt;cell&gt;90.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Video Understanding&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Video-MME&lt;/cell&gt;
        &lt;cell&gt;71.9&lt;/cell&gt;
        &lt;cell&gt;72.4&lt;/cell&gt;
        &lt;cell&gt;73.3&lt;/cell&gt;
        &lt;cell&gt;70.5&lt;/cell&gt;
        &lt;cell&gt;71.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;LVBench&lt;/cell&gt;
        &lt;cell&gt;30.8&lt;/cell&gt;
        &lt;cell&gt;57.9&lt;/cell&gt;
        &lt;cell&gt;47.3&lt;/cell&gt;
        &lt;cell&gt;50.2&lt;/cell&gt;
        &lt;cell&gt;51.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;MLVU&lt;/cell&gt;
        &lt;cell&gt;64.6&lt;/cell&gt;
        &lt;cell&gt;71.0&lt;/cell&gt;
        &lt;cell&gt;74.6&lt;/cell&gt;
        &lt;cell&gt;75.2&lt;/cell&gt;
        &lt;cell&gt;75.5&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Datasets&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-flash-thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;InternVL-3.5-241B-A28B&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;General Visual Question Answering&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MMStar&lt;/cell&gt;
        &lt;cell&gt;75.5&lt;/cell&gt;
        &lt;cell&gt;77.9&lt;/cell&gt;
        &lt;cell&gt;74.9&lt;/cell&gt;
        &lt;cell&gt;75.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;HallusionBench&lt;/cell&gt;
        &lt;cell&gt;61.1&lt;/cell&gt;
        &lt;cell&gt;57.3&lt;/cell&gt;
        &lt;cell&gt;62.8&lt;/cell&gt;
        &lt;cell&gt;63.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MM-MT-Bench&lt;/cell&gt;
        &lt;cell&gt;7.8&lt;/cell&gt;
        &lt;cell&gt;–&lt;/cell&gt;
        &lt;cell&gt;8.0&lt;/cell&gt;
        &lt;cell&gt;8.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Math &amp;amp; STEM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MMMU_val&lt;/cell&gt;
        &lt;cell&gt;76.9&lt;/cell&gt;
        &lt;cell&gt;77.7&lt;/cell&gt;
        &lt;cell&gt;75.6&lt;/cell&gt;
        &lt;cell&gt;75.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MMMU_pro&lt;/cell&gt;
        &lt;cell&gt;65.8&lt;/cell&gt;
        &lt;cell&gt;–&lt;/cell&gt;
        &lt;cell&gt;60.5&lt;/cell&gt;
        &lt;cell&gt;60.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MathVista_mini&lt;/cell&gt;
        &lt;cell&gt;77.6&lt;/cell&gt;
        &lt;cell&gt;82.7&lt;/cell&gt;
        &lt;cell&gt;80.0&lt;/cell&gt;
        &lt;cell&gt;81.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MathVision_full&lt;/cell&gt;
        &lt;cell&gt;62.3&lt;/cell&gt;
        &lt;cell&gt;63.9&lt;/cell&gt;
        &lt;cell&gt;62.9&lt;/cell&gt;
        &lt;cell&gt;63.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Documentation Understanding&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;AI2D_test&lt;/cell&gt;
        &lt;cell&gt;88.6&lt;/cell&gt;
        &lt;cell&gt;87.3&lt;/cell&gt;
        &lt;cell&gt;86.1&lt;/cell&gt;
        &lt;cell&gt;86.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;ChartQA_test&lt;/cell&gt;
        &lt;cell&gt;–&lt;/cell&gt;
        &lt;cell&gt;88.0&lt;/cell&gt;
        &lt;cell&gt;89.5&lt;/cell&gt;
        &lt;cell&gt;89.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Counting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;CountBench&lt;/cell&gt;
        &lt;cell&gt;88.6&lt;/cell&gt;
        &lt;cell&gt;–&lt;/cell&gt;
        &lt;cell&gt;88.6&lt;/cell&gt;
        &lt;cell&gt;92.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Video Understanding&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Video-MME&lt;/cell&gt;
        &lt;cell&gt;79.6&lt;/cell&gt;
        &lt;cell&gt;72.9&lt;/cell&gt;
        &lt;cell&gt;69.7&lt;/cell&gt;
        &lt;cell&gt;69.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;LVBench&lt;/cell&gt;
        &lt;cell&gt;64.5&lt;/cell&gt;
        &lt;cell&gt;–&lt;/cell&gt;
        &lt;cell&gt;49.0&lt;/cell&gt;
        &lt;cell&gt;49.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;MLVU&lt;/cell&gt;
        &lt;cell&gt;82.1&lt;/cell&gt;
        &lt;cell&gt;78.2&lt;/cell&gt;
        &lt;cell&gt;72.9&lt;/cell&gt;
        &lt;cell&gt;73.9&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;AudioVisual -&amp;gt; Text&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Datasets&lt;/cell&gt;
        &lt;cell role="head"&gt;Previous Open-source SoTA&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Flash&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen2.5-Omni&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Instruct&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;WorldSense&lt;/cell&gt;
        &lt;cell&gt;47.1&lt;/cell&gt;
        &lt;cell&gt;50.9&lt;/cell&gt;
        &lt;cell&gt;45.4&lt;/cell&gt;
        &lt;cell&gt;54.0&lt;/cell&gt;
        &lt;cell&gt;54.1&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Datasets&lt;/cell&gt;
        &lt;cell role="head"&gt;Previous Open-source SoTA&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Flash-Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;DailyOmni&lt;/cell&gt;
        &lt;cell&gt;69.8&lt;/cell&gt;
        &lt;cell&gt;72.7&lt;/cell&gt;
        &lt;cell&gt;75.8&lt;/cell&gt;
        &lt;cell&gt;76.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;VideoHolmes&lt;/cell&gt;
        &lt;cell&gt;55.6&lt;/cell&gt;
        &lt;cell&gt;49.5&lt;/cell&gt;
        &lt;cell&gt;57.3&lt;/cell&gt;
        &lt;cell&gt;57.3&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Zero-shot Speech Generation&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Datasets&lt;/cell&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Performance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Content Consistency&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SEED&lt;p&gt;test-zh | test-en&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Seed-TTSICL&lt;/cell&gt;
        &lt;cell&gt;1.11 | 2.24&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Seed-TTSRL&lt;/cell&gt;
        &lt;cell&gt;1.00 | 1.94&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MaskGCT&lt;/cell&gt;
        &lt;cell&gt;2.27 | 2.62&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;E2 TTS&lt;/cell&gt;
        &lt;cell&gt;1.97 | 2.19&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;F5-TTS&lt;/cell&gt;
        &lt;cell&gt;1.56 | 1.83&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Spark TTS&lt;/cell&gt;
        &lt;cell&gt;1.20 | 1.98&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CosyVoice 2&lt;/cell&gt;
        &lt;cell&gt;1.45 | 2.57&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CosyVoice 3&lt;/cell&gt;
        &lt;cell&gt;0.71 | 1.45&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Qwen2.5-Omni-7B&lt;/cell&gt;
        &lt;cell&gt;1.42 | 2.33&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B&lt;/cell&gt;
        &lt;cell&gt;1.07 | 1.39&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Multilingual Speech Generation&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Language&lt;/cell&gt;
        &lt;cell role="head"&gt;Content Consistency&lt;/cell&gt;
        &lt;cell role="head"&gt;Speaker Similarity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B&lt;/cell&gt;
        &lt;cell&gt;MiniMax&lt;/cell&gt;
        &lt;cell&gt;ElevenLabs&lt;/cell&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B&lt;/cell&gt;
        &lt;cell&gt;MiniMax&lt;/cell&gt;
        &lt;cell&gt;ElevenLabs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Chinese&lt;/cell&gt;
        &lt;cell&gt;0.716&lt;/cell&gt;
        &lt;cell&gt;2.252&lt;/cell&gt;
        &lt;cell&gt;16.026&lt;/cell&gt;
        &lt;cell&gt;0.772&lt;/cell&gt;
        &lt;cell&gt;0.780&lt;/cell&gt;
        &lt;cell&gt;0.677&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;English&lt;/cell&gt;
        &lt;cell&gt;1.069&lt;/cell&gt;
        &lt;cell&gt;2.164&lt;/cell&gt;
        &lt;cell&gt;2.339&lt;/cell&gt;
        &lt;cell&gt;0.773&lt;/cell&gt;
        &lt;cell&gt;0.756&lt;/cell&gt;
        &lt;cell&gt;0.613&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;German&lt;/cell&gt;
        &lt;cell&gt;0.777&lt;/cell&gt;
        &lt;cell&gt;1.906&lt;/cell&gt;
        &lt;cell&gt;0.572&lt;/cell&gt;
        &lt;cell&gt;0.738&lt;/cell&gt;
        &lt;cell&gt;0.733&lt;/cell&gt;
        &lt;cell&gt;0.614&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Italian&lt;/cell&gt;
        &lt;cell&gt;1.067&lt;/cell&gt;
        &lt;cell&gt;1.543&lt;/cell&gt;
        &lt;cell&gt;1.743&lt;/cell&gt;
        &lt;cell&gt;0.742&lt;/cell&gt;
        &lt;cell&gt;0.699&lt;/cell&gt;
        &lt;cell&gt;0.579&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Portuguese&lt;/cell&gt;
        &lt;cell&gt;1.872&lt;/cell&gt;
        &lt;cell&gt;1.877&lt;/cell&gt;
        &lt;cell&gt;1.331&lt;/cell&gt;
        &lt;cell&gt;0.770&lt;/cell&gt;
        &lt;cell&gt;0.805&lt;/cell&gt;
        &lt;cell&gt;0.711&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Spanish&lt;/cell&gt;
        &lt;cell&gt;1.765&lt;/cell&gt;
        &lt;cell&gt;1.029&lt;/cell&gt;
        &lt;cell&gt;1.084&lt;/cell&gt;
        &lt;cell&gt;0.744&lt;/cell&gt;
        &lt;cell&gt;0.762&lt;/cell&gt;
        &lt;cell&gt;0.615&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Japanese&lt;/cell&gt;
        &lt;cell&gt;3.631&lt;/cell&gt;
        &lt;cell&gt;3.519&lt;/cell&gt;
        &lt;cell&gt;10.646&lt;/cell&gt;
        &lt;cell&gt;0.763&lt;/cell&gt;
        &lt;cell&gt;0.776&lt;/cell&gt;
        &lt;cell&gt;0.738&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Korean&lt;/cell&gt;
        &lt;cell&gt;1.670&lt;/cell&gt;
        &lt;cell&gt;1.747&lt;/cell&gt;
        &lt;cell&gt;1.865&lt;/cell&gt;
        &lt;cell&gt;0.778&lt;/cell&gt;
        &lt;cell&gt;0.776&lt;/cell&gt;
        &lt;cell&gt;0.700&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;French&lt;/cell&gt;
        &lt;cell&gt;2.505&lt;/cell&gt;
        &lt;cell&gt;4.099&lt;/cell&gt;
        &lt;cell&gt;5.216&lt;/cell&gt;
        &lt;cell&gt;0.689&lt;/cell&gt;
        &lt;cell&gt;0.628&lt;/cell&gt;
        &lt;cell&gt;0.535&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Russian&lt;/cell&gt;
        &lt;cell&gt;3.986&lt;/cell&gt;
        &lt;cell&gt;4.281&lt;/cell&gt;
        &lt;cell&gt;3.878&lt;/cell&gt;
        &lt;cell&gt;0.759&lt;/cell&gt;
        &lt;cell&gt;0.761&lt;/cell&gt;
        &lt;cell&gt;0.676&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Cross-Lingual Speech Generation&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Language&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B&lt;/cell&gt;
        &lt;cell role="head"&gt;CosyVoice3&lt;/cell&gt;
        &lt;cell role="head"&gt;CosyVoice2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;en-to-zh&lt;/cell&gt;
        &lt;cell&gt;5.37&lt;/cell&gt;
        &lt;cell&gt;5.09&lt;/cell&gt;
        &lt;cell&gt;13.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ja-to-zh&lt;/cell&gt;
        &lt;cell&gt;3.32&lt;/cell&gt;
        &lt;cell&gt;3.05&lt;/cell&gt;
        &lt;cell&gt;48.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ko-to-zh&lt;/cell&gt;
        &lt;cell&gt;0.99&lt;/cell&gt;
        &lt;cell&gt;1.06&lt;/cell&gt;
        &lt;cell&gt;7.70&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;zh-to-en&lt;/cell&gt;
        &lt;cell&gt;2.76&lt;/cell&gt;
        &lt;cell&gt;2.98&lt;/cell&gt;
        &lt;cell&gt;6.47&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ja-to-en&lt;/cell&gt;
        &lt;cell&gt;3.31&lt;/cell&gt;
        &lt;cell&gt;4.20&lt;/cell&gt;
        &lt;cell&gt;17.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ko-to-en&lt;/cell&gt;
        &lt;cell&gt;3.34&lt;/cell&gt;
        &lt;cell&gt;4.19&lt;/cell&gt;
        &lt;cell&gt;11.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;zh-to-ja&lt;/cell&gt;
        &lt;cell&gt;8.29&lt;/cell&gt;
        &lt;cell&gt;7.08&lt;/cell&gt;
        &lt;cell&gt;13.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;en-to-ja&lt;/cell&gt;
        &lt;cell&gt;7.53&lt;/cell&gt;
        &lt;cell&gt;6.80&lt;/cell&gt;
        &lt;cell&gt;14.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ko-to-ja&lt;/cell&gt;
        &lt;cell&gt;4.24&lt;/cell&gt;
        &lt;cell&gt;3.93&lt;/cell&gt;
        &lt;cell&gt;5.86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;zh-to-ko&lt;/cell&gt;
        &lt;cell&gt;5.13&lt;/cell&gt;
        &lt;cell&gt;14.4&lt;/cell&gt;
        &lt;cell&gt;24.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;en-to-ko&lt;/cell&gt;
        &lt;cell&gt;4.96&lt;/cell&gt;
        &lt;cell&gt;5.87&lt;/cell&gt;
        &lt;cell&gt;21.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ja-to-ko&lt;/cell&gt;
        &lt;cell&gt;6.23&lt;/cell&gt;
        &lt;cell&gt;7.92&lt;/cell&gt;
        &lt;cell&gt;21.5&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Decoding Strategy: For the Qwen3-Omni series across all evaluation benchmarks, &lt;code&gt;Instruct&lt;/code&gt;models use greedy decoding during generation without sampling. For&lt;code&gt;Thinking&lt;/code&gt;models, the decoding parameters should be taken from the&lt;code&gt;generation_config.json&lt;/code&gt;file in the checkpoint.&lt;/item&gt;
      &lt;item&gt;Benchmark-Specific Formatting: For the majority of evaluation benchmarks, they come with their own ChatML formatting to embed the question or prompt. It should be noted that all video data are set to &lt;code&gt;fps=2&lt;/code&gt;during evaluation.&lt;/item&gt;
      &lt;item&gt;Default Prompts: For tasks in certain benchmarks that do not include a prompt, we use the following prompt settings:&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Task Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Prompt&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Auto Speech Recognition (ASR) for Chinese&lt;/cell&gt;
        &lt;cell&gt;请将这段中文语音转换为纯文本。&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Auto Speech Recognition (ASR) for Other languages&lt;/cell&gt;
        &lt;cell&gt;Transcribe the audio into text.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Speech-to-Text Translation (S2TT)&lt;/cell&gt;
        &lt;cell&gt;Listen to the provided &amp;lt;source_language&amp;gt; speech and produce a translation in &amp;lt;target_language&amp;gt; text.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Song Lyrics Recognition&lt;/cell&gt;
        &lt;cell&gt;Transcribe the song lyrics into text without any punctuation, separate lines with line breaks, and output only the lyrics without additional explanations.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;System Prompt: No &lt;code&gt;system prompt&lt;/code&gt;should be set for any evaluation benchmark.&lt;/item&gt;
      &lt;item&gt;Input Sequence: The question or prompt should be input as user text. Unless otherwise specified by the benchmark, the text should come after multimodal data in the sequence. For example:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;messages = [
    {
        "role": "user",
        "content": [
            {"type": "audio", "audio": "/path/to/audio.wav"},
            {"type": "image", "image": "/path/to/image.png"},
            {"type": "video", "video": "/path/to/video.mp4"},
            {"type": "text", "text": "Describe the audio, image and video."},
        ],
    },
]&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45336989</guid><pubDate>Mon, 22 Sep 2025 17:50:21 +0000</pubDate></item><item><title>Show HN: Python Audio Transcription: Convert Speech to Text Locally</title><link>https://www.pavlinbg.com/posts/python-speech-to-text-guide</link><description>&lt;doc fingerprint="840a859b39aa1292"&gt;
  &lt;main&gt;
    &lt;p&gt;Last week, I faced a dilemma that many researchers, journalists, and content creators know all too well: I had hours of recordings that needed to be transcribed. I had serious privacy concerns about uploading sensitive content to commercial transcription services and their third-party servers.&lt;/p&gt;
    &lt;p&gt;Instead of risking it, I built a Python-based transcription system using OpenAI’s Whisper model. The result? All my audio files were transcribed in under 10 minutes with 96% accuracy—completely free and processed locally on my laptop.&lt;/p&gt;
    &lt;p&gt;In this post, I will show you how you can build a simple script for processing any audio data without recurring costs or privacy compromises.&lt;/p&gt;
    &lt;head rend="h2"&gt;Essential Setup Requirements&lt;/head&gt;
    &lt;head rend="h3"&gt;1. FFmpeg Installation (Critical First Step)&lt;/head&gt;
    &lt;p&gt;FFmpeg handles audio processing and is required for all transcription methods. This is the #1 cause of setup failures.&lt;/p&gt;
    &lt;head rend="h4"&gt;⚠️ Setup Priority&lt;/head&gt;
    &lt;p&gt;Windows:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download from ffmpeg.org/download.html&lt;/item&gt;
      &lt;item&gt;Extract to &lt;code&gt;C:\ffmpeg&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Add &lt;code&gt;C:\ffmpeg\bin&lt;/code&gt;to your PATH environment variable&lt;/item&gt;
      &lt;item&gt;Restart your terminal&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;macOS:&lt;/p&gt;
    &lt;code&gt;# Using Homebrew (recommended)
brew install ffmpeg&lt;/code&gt;
    &lt;p&gt;Linux (Ubuntu/Debian):&lt;/p&gt;
    &lt;code&gt;sudo apt update &amp;amp;&amp;amp; sudo apt install ffmpeg&lt;/code&gt;
    &lt;p&gt;Verify Installation:&lt;/p&gt;
    &lt;code&gt;ffmpeg -version&lt;/code&gt;
    &lt;p&gt;You should see version information. If you get “command not found,” FFmpeg isn’t properly installed.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Python Environment Setup&lt;/head&gt;
    &lt;head rend="h4"&gt;🔧 Virtual Environment Benefits&lt;/head&gt;
    &lt;code&gt;# Create isolated environment
python -m venv whisper-env
cd whisper-env

# Activate environment
# Windows:
Scripts\activate
# macOS/Linux:
source bin/activate

# Install required packages
pip install openai-whisper&lt;/code&gt;
    &lt;head rend="h2"&gt;Method 1: OpenAI Whisper (Recommended)&lt;/head&gt;
    &lt;p&gt;Whisper is OpenAI’s state-of-the-art speech recognition model, trained on 680,000 hours of multilingual audio. It’s specifically designed for robust, real-world audio transcription and handles various accents, background noise, and audio quality issues remarkably well.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choosing the Right Whisper Model&lt;/head&gt;
    &lt;head rend="h4"&gt;🎯 Model Selection Guide&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;cell role="head"&gt;RAM Required&lt;/cell&gt;
        &lt;cell role="head"&gt;Speed&lt;/cell&gt;
        &lt;cell role="head"&gt;Accuracy&lt;/cell&gt;
        &lt;cell role="head"&gt;Best Use Case&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;tiny&lt;/cell&gt;
        &lt;cell&gt;39 MB&lt;/cell&gt;
        &lt;cell&gt;390 MB&lt;/cell&gt;
        &lt;cell&gt;32x realtime&lt;/cell&gt;
        &lt;cell&gt;89%&lt;/cell&gt;
        &lt;cell&gt;Quick testing, real-time applications&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;base&lt;/cell&gt;
        &lt;cell&gt;74 MB&lt;/cell&gt;
        &lt;cell&gt;740 MB&lt;/cell&gt;
        &lt;cell&gt;16x realtime&lt;/cell&gt;
        &lt;cell&gt;94%&lt;/cell&gt;
        &lt;cell&gt;General use (recommended)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;small&lt;/cell&gt;
        &lt;cell&gt;244 MB&lt;/cell&gt;
        &lt;cell&gt;2.4 GB&lt;/cell&gt;
        &lt;cell&gt;6x realtime&lt;/cell&gt;
        &lt;cell&gt;96%&lt;/cell&gt;
        &lt;cell&gt;High-quality transcription needs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;medium&lt;/cell&gt;
        &lt;cell&gt;769 MB&lt;/cell&gt;
        &lt;cell&gt;5 GB&lt;/cell&gt;
        &lt;cell&gt;2x realtime&lt;/cell&gt;
        &lt;cell&gt;97%&lt;/cell&gt;
        &lt;cell&gt;Professional work, critical accuracy&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;large&lt;/cell&gt;
        &lt;cell&gt;1.5 GB&lt;/cell&gt;
        &lt;cell&gt;10 GB&lt;/cell&gt;
        &lt;cell&gt;1x realtime&lt;/cell&gt;
        &lt;cell&gt;98%&lt;/cell&gt;
        &lt;cell&gt;Maximum accuracy, research purposes&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Basic Whisper Implementation&lt;/head&gt;
    &lt;p&gt;Here’s a clean, production-ready implementation:&lt;/p&gt;
    &lt;code&gt;import whisper
import os
from pathlib import Path
import time

class AudioTranscriber:
    def __init__(self, model_size="base"):
        """Initialize transcriber with specified Whisper model"""
        print(f"Loading Whisper {model_size} model...")
        self.model = whisper.load_model(model_size)
        print("Model loaded successfully!")
    
    def transcribe_file(self, audio_path, language=None):
        """
        Transcribe a single audio file
        
        Args:
            audio_path: Path to audio file
            language: Language code ('en', 'es', 'fr', etc.) or None for auto-detect
        """
        if not os.path.exists(audio_path):
            raise FileNotFoundError(f"Audio file not found: {audio_path}")
        
        print(f"Transcribing: {Path(audio_path).name}")
        
        start_time = time.time()
        
        # Transcribe audio
        options = {"language": language} if language else {}
        result = self.model.transcribe(audio_path, **options)
        
        processing_time = time.time() - start_time
        
        print(f"✓ Completed in {processing_time:.1f} seconds")
        print(f"✓ Detected language: {result['language']}")
        
        return {
            'text': result['text'].strip(),
            'language': result['language'],
            'segments': result.get('segments', []),
            'processing_time': processing_time
        }
    
    def save_transcription(self, result, output_path):
        """Save transcription to text file"""
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("=== Transcription Results ===\n")
            f.write(f"Language: {result['language']}\n")
            f.write(f"Processing Time: {result['processing_time']:.1f} seconds\n")
            f.write("=" * 40 + "\n\n")
            f.write(result['text'])
        
        print(f"✓ Transcription saved to: {output_path}")

# Usage example
def transcribe_audio_file(audio_path, model_size="base", language=None):
    """Simple function to transcribe an audio file"""
    
    transcriber = AudioTranscriber(model_size=model_size)
    result = transcriber.transcribe_file(audio_path, language=language)
    
    # Save transcription
    audio_name = Path(audio_path).stem
    output_path = f"{audio_name}_transcript.txt"
    transcriber.save_transcription(result, output_path)
    
    return result

# Example usage
if __name__ == "__main__":
    # Transcribe a file
    audio_file = "interview.wav"  # Replace with your audio file
    result = transcribe_audio_file(audio_file, model_size="base", language="en")
    
    print(f"\nTranscription preview:")
    print(result['text'][:200] + "..." if len(result['text']) &amp;gt; 200 else result['text'])&lt;/code&gt;
    &lt;head rend="h4"&gt;🎵 Supported Audio Formats&lt;/head&gt;
    &lt;head rend="h3"&gt;Batch Processing Multiple Files&lt;/head&gt;
    &lt;p&gt;For processing multiple audio files efficiently:&lt;/p&gt;
    &lt;code&gt;def batch_transcribe(audio_files, output_dir="transcripts", model_size="base"):
    """Transcribe multiple audio files"""
    
    os.makedirs(output_dir, exist_ok=True)
    transcriber = AudioTranscriber(model_size=model_size)
    
    results = []
    
    for i, audio_file in enumerate(audio_files, 1):
        print(f"\n--- Processing file {i}/{len(audio_files)} ---")
        
        try:
            result = transcriber.transcribe_file(audio_file)
            
            # Save individual transcription
            file_name = Path(audio_file).stem
            output_path = os.path.join(output_dir, f"{file_name}_transcript.txt")
            transcriber.save_transcription(result, output_path)
            
            results.append(result)
            
        except Exception as e:
            print(f"✗ Failed to process {audio_file}: {str(e)}")
            continue
    
    print(f"\n✓ Batch processing completed: {len(results)}/{len(audio_files)} files successful")
    return results

# Usage
audio_files = ["interview1.wav", "interview2.mp3", "lecture.m4a"]
batch_transcribe(audio_files, output_dir="my_transcripts")&lt;/code&gt;
    &lt;head rend="h3"&gt;Creating Subtitle Files (SRT Format)&lt;/head&gt;
    &lt;p&gt;Generate subtitle files for videos:&lt;/p&gt;
    &lt;code&gt;def create_srt_subtitles(audio_path, output_path=None):
    """Create SRT subtitle file from audio"""
    
    transcriber = AudioTranscriber(model_size="base")
    result = transcriber.transcribe_file(audio_path)
    
    if output_path is None:
        output_path = Path(audio_path).stem + ".srt"
    
    with open(output_path, 'w', encoding='utf-8') as f:
        for i, segment in enumerate(result['segments'], 1):
            start_time = format_timestamp(segment['start'])
            end_time = format_timestamp(segment['end'])
            
            f.write(f"{i}\n")
            f.write(f"{start_time} --&amp;gt; {end_time}\n")
            f.write(f"{segment['text'].strip()}\n\n")
    
    print(f"✓ SRT subtitles saved to: {output_path}")

def format_timestamp(seconds):
    """Convert seconds to SRT timestamp format"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millisecs = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}"

# Usage
create_srt_subtitles("presentation.mp4")&lt;/code&gt;
    &lt;head rend="h2"&gt;Method 2: Alternative with SpeechRecognition Library&lt;/head&gt;
    &lt;p&gt;For scenarios requiring different recognition engines or more control over audio preprocessing:&lt;/p&gt;
    &lt;code&gt;import speech_recognition as sr
from pydub import AudioSegment
import tempfile
import os

class FlexibleTranscriber:
    def __init__(self, engine="google"):
        """Initialize with specified recognition engine"""
        self.recognizer = sr.Recognizer()
        self.engine = engine
        
        # Optimize settings
        self.recognizer.energy_threshold = 300
        self.recognizer.dynamic_energy_threshold = True
        
    def preprocess_audio(self, audio_path):
        """Optimize audio for better recognition"""
        audio = AudioSegment.from_file(audio_path)
        
        # Convert to mono and normalize
        if audio.channels &amp;gt; 1:
            audio = audio.set_channels(1)
        
        audio = audio.set_frame_rate(16000)  # Standard sample rate
        audio = audio.normalize()  # Normalize volume
        
        # Export to temporary WAV file
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')
        audio.export(temp_file.name, format="wav")
        
        return temp_file.name
    
    def transcribe_file(self, audio_path, language='en-US'):
        """Transcribe audio file using speech_recognition library"""
        
        # Preprocess audio
        processed_path = self.preprocess_audio(audio_path)
        
        try:
            with sr.AudioFile(processed_path) as source:
                # Adjust for ambient noise
                self.recognizer.adjust_for_ambient_noise(source, duration=1)
                audio_data = self.recognizer.record(source)
            
            # Perform recognition
            if self.engine == "google":
                text = self.recognizer.recognize_google(audio_data, language=language)
            elif self.engine == "sphinx":
                text = self.recognizer.recognize_sphinx(audio_data)
            
            return {
                'text': text,
                'success': True,
                'engine': self.engine
            }
            
        except sr.UnknownValueError:
            return {
                'text': "",
                'success': False,
                'error': "Could not understand audio"
            }
        except sr.RequestError as e:
            return {
                'text': "",
                'success': False,
                'error': f"Recognition service error: {str(e)}"
            }
        finally:
            # Clean up temporary file
            os.unlink(processed_path)

# Usage
transcriber = FlexibleTranscriber(engine="google")
result = transcriber.transcribe_file("audio.wav")

if result['success']:
    print(result['text'])
else:
    print(f"Transcription failed: {result['error']}")&lt;/code&gt;
    &lt;head rend="h4"&gt;🔄 Engine Comparison&lt;/head&gt;
    &lt;head rend="h2"&gt;Common Issues and Solutions&lt;/head&gt;
    &lt;head rend="h3"&gt;Issue 1: FFmpeg Not Found&lt;/head&gt;
    &lt;p&gt;Error: &lt;code&gt;[WinError 2] The system cannot find the file specified&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Solution:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Verify FFmpeg installation: &lt;code&gt;ffmpeg -version&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Windows: Ensure FFmpeg is in your PATH environment variable&lt;/item&gt;
      &lt;item&gt;Restart your terminal/command prompt after PATH changes&lt;/item&gt;
      &lt;item&gt;Try reinstalling FFmpeg if the problem persists&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Issue 2: Out of Memory Errors&lt;/head&gt;
    &lt;p&gt;Error: CUDA out of memory or system RAM exhausted&lt;/p&gt;
    &lt;head rend="h4"&gt;⚡ Memory Management Tips&lt;/head&gt;
    &lt;p&gt;Solutions:&lt;/p&gt;
    &lt;code&gt;# Use smaller model
transcriber = AudioTranscriber(model_size="tiny")

# For very long audio files, process in chunks
def transcribe_long_audio(audio_path, chunk_duration=300):  # 5 minutes
    audio = AudioSegment.from_file(audio_path)
    chunks = [audio[i:i+chunk_duration*1000] for i in range(0, len(audio), chunk_duration*1000)]
    
    transcriptions = []
    for i, chunk in enumerate(chunks):
        chunk_path = f"temp_chunk_{i}.wav"
        chunk.export(chunk_path, format="wav")
        
        result = transcriber.transcribe_file(chunk_path)
        transcriptions.append(result['text'])
        
        os.remove(chunk_path)
    
    return ' '.join(transcriptions)&lt;/code&gt;
    &lt;head rend="h3"&gt;Issue 3: Poor Accuracy on Noisy Audio&lt;/head&gt;
    &lt;p&gt;Problem: Low accuracy on recordings with background noise or poor quality&lt;/p&gt;
    &lt;head rend="h4"&gt;🎤 Audio Quality Tips&lt;/head&gt;
    &lt;p&gt;Solutions:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Audio preprocessing:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;def enhance_audio(audio_path):
    """Basic audio enhancement"""
    audio = AudioSegment.from_file(audio_path)
    
    # Normalize volume
    audio = audio.normalize()
    
    # Apply high-pass filter to reduce low-frequency noise
    audio = audio.high_pass_filter(80)
    
    # Compress dynamic range
    audio = audio.compress_dynamic_range()
    
    return audio&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Specify language for better accuracy:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;result = transcriber.transcribe_file("audio.wav", language="en")&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Use higher-quality model:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Upgrade from 'base' to 'small' for better accuracy
transcriber = AudioTranscriber(model_size="small")&lt;/code&gt;
    &lt;head rend="h2"&gt;Performance Benchmarks&lt;/head&gt;
    &lt;p&gt;Based on testing with various audio types on a modern laptop:&lt;/p&gt;
    &lt;p&gt;Whisper Model Performance (1-hour audio file):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;tiny: 1.9 minutes processing, 89% accuracy&lt;/item&gt;
      &lt;item&gt;base: 3.8 minutes processing, 94% accuracy&lt;/item&gt;
      &lt;item&gt;small: 10 minutes processing, 96% accuracy&lt;/item&gt;
      &lt;item&gt;medium: 30 minutes processing, 97% accuracy&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Hardware Impact:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CPU only: Use base model maximum for reasonable speeds&lt;/item&gt;
      &lt;item&gt;8GB RAM: Comfortable with small model&lt;/item&gt;
      &lt;item&gt;16GB+ RAM: Can handle medium/large models without issues&lt;/item&gt;
      &lt;item&gt;GPU acceleration: 3-5x speed improvement (requires CUDA setup)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;🚀 Optimization Strategy&lt;/head&gt;
    &lt;head rend="h2"&gt;Command-Line Usage&lt;/head&gt;
    &lt;p&gt;Create a simple command-line script for easy usage:&lt;/p&gt;
    &lt;code&gt;# transcribe.py
import sys
import argparse
from pathlib import Path

def main():
    parser = argparse.ArgumentParser(description='Transcribe audio files locally')
    parser.add_argument('audio_file', help='Path to audio file')
    parser.add_argument('--model', default='base', choices=['tiny', 'base', 'small', 'medium', 'large'])
    parser.add_argument('--language', help='Language code (e.g., en, es, fr)')
    parser.add_argument('--output', help='Output file path')
    
    args = parser.parse_args()
    
    # Transcribe
    result = transcribe_audio_file(
        args.audio_file, 
        model_size=args.model,
        language=args.language
    )
    
    # Save to custom output path if specified
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(result['text'])
        print(f"Transcription saved to: {args.output}")

if __name__ == "__main__":
    main()&lt;/code&gt;
    &lt;p&gt;Usage examples:&lt;/p&gt;
    &lt;code&gt;# Basic transcription
python transcribe.py interview.wav

# Specify model and language
python transcribe.py lecture.mp3 --model small --language en

# Custom output file
python transcribe.py podcast.m4a --output transcript.txt&lt;/code&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Local audio transcription with Python and Whisper offers a compelling alternative to commercial services. With a one-time setup, you get unlimited transcription capabilities, complete privacy, and often superior accuracy compared to cloud-based solutions.&lt;/p&gt;
    &lt;p&gt;Key advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zero ongoing costs after initial setup—no per-minute charges&lt;/item&gt;
      &lt;item&gt;Complete privacy—audio never leaves your machine&lt;/item&gt;
      &lt;item&gt;High accuracy—94-98% depending on model choice and audio quality&lt;/item&gt;
      &lt;item&gt;Fast processing—typically 4-16x real-time speed&lt;/item&gt;
      &lt;item&gt;Offline capability—works without internet connection&lt;/item&gt;
      &lt;item&gt;No usage limits—transcribe as much as you want&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Whether you’re a researcher transcribing interviews, a journalist working with sensitive sources, or a content creator processing podcasts, this local solution gives you the control and privacy that cloud services can’t match.&lt;/p&gt;
    &lt;p&gt;The setup might take 30 minutes, but you’ll save hours of time and potentially hundreds of dollars in transcription costs. Plus, you’ll have the peace of mind that comes with keeping your audio data completely under your control.&lt;/p&gt;
    &lt;head rend="h3"&gt;Stay up to date&lt;/head&gt;
    &lt;p&gt;Get notified when I publish something new, and unsubscribe at any time.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45337400</guid><pubDate>Mon, 22 Sep 2025 18:18:56 +0000</pubDate></item><item><title>Fine-grained HTTP filtering for Claude Code</title><link>https://ammar.io/blog/httpjail</link><description>&lt;doc fingerprint="fcf5f913cb30106f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Fine-grained HTTP filtering for Claude Code&lt;/head&gt;
    &lt;p&gt;Coding agents are becoming more powerful every day without commensurate security and governance tooling. The result is a world where solo developers happily run &lt;code&gt;claude --dangerously-skip-permissions&lt;/code&gt; for hours unmoderated while many of the world's most important organizations have barely tried agentic developmentLearned from our experience at Coder . I've been working on a tool called &lt;code&gt;httpjail&lt;/code&gt; in an effort to make agents available everywhere.&lt;/p&gt;
    &lt;p&gt;The tool is focused on mitigating these classes of risks:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Risk&lt;/cell&gt;
        &lt;cell role="head"&gt;Example&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Agents performing destructive actions&lt;/cell&gt;
        &lt;cell&gt;Deleting your database&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Agents leaking sensitive information&lt;/cell&gt;
        &lt;cell&gt;Exposing API keys or credentials&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Agents operating with more authority than desired&lt;/cell&gt;
        &lt;cell&gt;Pushing straight to main instead of opening a PR&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Agents may transgress accidentally (user misinterpretation) or intentionally (prompt injection).&lt;/p&gt;
    &lt;p&gt;There is a class of risks at the file-system interface too, but, I believe existing tooling (containers) is sufficient here. Existing network isolation tools rely on IP-based rules. In our case, they're imprecisecentralized, anycast load balancers power much of the internet and require constant maintenanceIPs change randomly and they're not a part of a service's implicit "contract".&lt;/p&gt;
    &lt;head rend="h2"&gt;httpjail&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;httpjail&lt;/code&gt; implements an HTTP(S) interceptor alongside process-level network isolation. Under default configuration, all DNS (udp:53) is permitted and
all other non-HTTP(S) traffic is blocked.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;httpjail&lt;/code&gt; rules are either JavaScript expressions or custom programs. This approach makes
them far more flexible than traditional rule-oriented firewalls and avoids the learning curve of a DSL.&lt;/p&gt;
    &lt;p&gt;Block all HTTP requests other than the LLM API traffic itself:&lt;/p&gt;
    &lt;code&gt;$ httpjail --js "r.host === 'api.anthropic.com'" -- claude "build something great"
&lt;/code&gt;
    &lt;p&gt;Allow only &lt;code&gt;GET&lt;/code&gt; requests i.e. make the internet read-only:&lt;/p&gt;
    &lt;code&gt;$ httpjail --js "r.method === 'GET'" -- claude "build something great"
&lt;/code&gt;
    &lt;p&gt;Only allow hosts in a &lt;code&gt;whitelist.txt&lt;/code&gt; file:&lt;/p&gt;
    &lt;code&gt;$ httpjail --sh "grep -qx \"$HTTPJAIL_HOST\" whitelist.txt" -- claude "research these APIs"
&lt;/code&gt;
    &lt;head rend="h2"&gt;How it works&lt;/head&gt;
    &lt;p&gt;In a nutshell:&lt;/p&gt;
    &lt;p&gt;Strong| NS[Create namespace&lt;/p&gt;
    &lt;p&gt;+ nftables redirect&lt;/p&gt;
    &lt;p&gt;+ setuid $SUDO_USER] Start --&amp;gt;|macOS/--weak&lt;/p&gt;
    &lt;p&gt;Weak| Env[Set $HTTP_PROXY&lt;/p&gt;
    &lt;p&gt;env vars] end subgraph "2. Target Process" direction TB Target[Target Process&lt;/p&gt;
    &lt;p&gt;e.g., claude] Target --&amp;gt; Request[HTTP/HTTPS&lt;/p&gt;
    &lt;p&gt;Request] Request --&amp;gt; Route{Route&lt;/p&gt;
    &lt;p&gt;Request} end subgraph "3. Interception" direction TB Proxy[Proxy :8080/:8443] Proxy --&amp;gt; Rules{Evaluate&lt;/p&gt;
    &lt;p&gt;JS/Script Rules} end subgraph "4. Result" direction TB Internet[✓ Internet Access] Blocked[✗ 403 Blocked] Bypass[⚠️ BYPASSED!&lt;/p&gt;
    &lt;p&gt;weak mode only] end NS --&amp;gt; Target Env --&amp;gt; Target Route --&amp;gt;|Strong: forced&lt;/p&gt;
    &lt;p&gt;via nftables| Proxy Route --&amp;gt;|Weak: respects&lt;/p&gt;
    &lt;p&gt;$HTTP_PROXY| Proxy Route --&amp;gt;|Weak: ignores&lt;/p&gt;
    &lt;p&gt;$HTTP_PROXY| Bypass Rules --&amp;gt;|Allow| Internet Rules --&amp;gt;|Deny| Blocked style NS fill:#00d4ff,color:#0a0a0a style Env fill:#00d4ff,color:#0a0a0a style Proxy fill:#404040 style Blocked fill:#8b2635 style Internet fill:#2a7f62 style Bypass fill:#8b2635&lt;/p&gt;
    &lt;head rend="h3"&gt;macOS (Weak Mode)&lt;/head&gt;
    &lt;p&gt;macOS uses &lt;code&gt;--weak/-w&lt;/code&gt; mode by default (see #7).&lt;/p&gt;
    &lt;p&gt;In weak mode, we rely on process cooperation via the standard &lt;code&gt;HTTP_PROXY&lt;/code&gt;/&lt;code&gt;HTTPS_PROXY&lt;/code&gt; environment variables. This mode is less of a jail and more of a suggestion that the majority of
well-meaning applications happen to comply with.&lt;/p&gt;
    &lt;head rend="h3"&gt;TLS Interception&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;httpjail&lt;/code&gt; implements full TLS interception to inspect and filter HTTPS traffic. Without interception, rules would only have access to the hostname via SNI, and most of the power of this tool would be lost.&lt;/p&gt;
    &lt;head rend="h4"&gt;How TLS Interception Works&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Certificate Authority Generation: On first run,&lt;/p&gt;&lt;code&gt;httpjail&lt;/code&gt;generates a self-signed Certificate Authority (CA) that's stored in&lt;code&gt;~/.config/httpjail/&lt;/code&gt;. This CA is used to sign certificates for intercepted connections.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Dynamic Certificate Generation: When a client connects,&lt;/p&gt;&lt;code&gt;httpjail&lt;/code&gt;:&lt;list rend="ul"&gt;&lt;item&gt;Extracts the Server Name Indication (SNI) from the TLS ClientHello&lt;/item&gt;&lt;item&gt;Generates a certificate on-the-fly for that specific hostname &lt;list rend="ul"&gt;&lt;item&gt;Uses a shared ECDSA P-256 key pair for all server certificates for O(1) keygen overhead&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Signs it with the CA certificate&lt;/item&gt;&lt;item&gt;Caches certificate in memory for performance&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Dual Mode Operation:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Transparent Proxy Mode: Directly accepts TLS connections by detecting the TLS ClientHello packet (starts with &lt;code&gt;0x16&lt;/code&gt;)&lt;/item&gt;
          &lt;item&gt;Explicit Proxy Mode: Handles HTTP CONNECT tunnels, responds with "200 Connection Established", then transitions to Transparent Proxy Mode&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;Transparent Proxy Mode: Directly accepts TLS connections by detecting the TLS ClientHello packet (starts with &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Trust Injection: The CA certificate must be trusted by client applications.&lt;/p&gt;&lt;code&gt;httpjail&lt;/code&gt;automatically sets environment variables for common tools:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;SSL_CERT_FILE&lt;/code&gt;/&lt;code&gt;SSL_CERT_DIR&lt;/code&gt;for OpenSSL-based tools&lt;/item&gt;&lt;item&gt;&lt;code&gt;CURL_CA_BUNDLE&lt;/code&gt;for curl&lt;/item&gt;&lt;item&gt;&lt;code&gt;NODE_EXTRA_CA_CERTS&lt;/code&gt;for Node.js&lt;/item&gt;&lt;item&gt;&lt;code&gt;REQUESTS_CA_BUNDLE&lt;/code&gt;for Python requests&lt;/item&gt;&lt;item&gt;&lt;code&gt;GIT_SSL_CAINFO&lt;/code&gt;for Git&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Jail Escapes&lt;/head&gt;
    &lt;p&gt;In the weak jail, it's trivial for the agent to escape the jail by funnelling requests through a shim program that disregards the &lt;code&gt;HTTP_PROXY&lt;/code&gt; environment variable.&lt;/p&gt;
    &lt;p&gt;Even the strong jail is not perfect. There are potential escape hatches in the filesystem. For example, the agent could create a container via a Docker socket which would spawn outside the network namespace.&lt;/p&gt;
    &lt;p&gt;To combine filesystem and network isolation into one, &lt;code&gt;httpjail&lt;/code&gt; has a &lt;code&gt;--docker-run&lt;/code&gt; flag that works like this:&lt;/p&gt;
    &lt;code&gt;httpjail --js "r.host === 'api.github.com'" --docker-run -- \
    --rm alpine:latest wget -qO- https://api.github.com
&lt;/code&gt;
    &lt;p&gt;I believe there's still much value in this approach even with imperfect isolation. In my experience, models seldom try to escape restrictions intentionally placed by the user. And, if the jail is rendered ineffective by prompt injection, it wasn't doing its job in the first place.&lt;/p&gt;
    &lt;head rend="h2"&gt;Server Mode&lt;/head&gt;
    &lt;p&gt;For the strongest level of isolation, you can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Run &lt;code&gt;httpjail --server&lt;/code&gt;on a standalone server.&lt;/item&gt;
      &lt;item&gt;Configure the network firewall to only permit 80/443 traffic to the proxy server. &lt;list rend="ul"&gt;&lt;item&gt;Optionally, you may redirect all traffic to the proxy server, otherwise you will need to take care in ensuring HTTP_PROXY is set in your environments and all of your web-faring applications respect it.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Run processes as usual in the development environment.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Request] Check{Respects&lt;/p&gt;
    &lt;p&gt;$HTTP_PROXY?} end subgraph "Network Layer" FW[Network Firewall] Config{Config&lt;/p&gt;
    &lt;p&gt;Mode?} FW --&amp;gt; Config Config --&amp;gt;|Redirect All&lt;/p&gt;
    &lt;p&gt;80/443 Traffic| Force[Forced to Proxy] Config --&amp;gt;|Allow Only&lt;/p&gt;
    &lt;p&gt;Proxy IP| Check Check --&amp;gt;|Yes| Voluntary[To Proxy] Check --&amp;gt;|No| Drop[✗ Dropped] end subgraph "httpjail --server" Decision{Evaluate&lt;/p&gt;
    &lt;p&gt;Rules} end subgraph Result direction TB API[✓ Allowed APIs] Blocked[✗ Blocked] end Request --&amp;gt; FW Force --&amp;gt; Decision Voluntary --&amp;gt; Decision Decision --&amp;gt;|Allow| API Decision --&amp;gt;|Deny| Blocked style Request fill:#404040 style Decision fill:#00d4ff,color:#0a0a0a style FW fill:#404040 style Blocked fill:#8b2635 style Drop fill:#8b2635 style API fill:#2a7f62 style Force fill:#2a7f62 style Voluntary fill:#4a7c59&lt;/p&gt;
    &lt;head rend="h2"&gt;Try it out&lt;/head&gt;
    &lt;code&gt;cargo install httpjail
&lt;/code&gt;
    &lt;p&gt;And, check out the GitHub repository for more details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45338561</guid><pubDate>Mon, 22 Sep 2025 19:49:02 +0000</pubDate></item><item><title>Paper2Agent: Stanford Reimagining Research Papers as Interactive AI Agents</title><link>https://arxiv.org/abs/2509.06917</link><description>&lt;doc fingerprint="f5bd3409d9b3a08a"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Artificial Intelligence&lt;/head&gt;&lt;p&gt; [Submitted on 8 Sep 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:We introduce Paper2Agent, an automated framework that converts research papers into AI agents. Paper2Agent transforms research output from passive artifacts into active systems that can accelerate downstream use, adoption, and discovery. Conventional research papers require readers to invest substantial effort to understand and adapt a paper's code, data, and methods to their own work, creating barriers to dissemination and reuse. Paper2Agent addresses this challenge by automatically converting a paper into an AI agent that acts as a knowledgeable research assistant. It systematically analyzes the paper and the associated codebase using multiple agents to construct a Model Context Protocol (MCP) server, then iteratively generates and runs tests to refine and robustify the resulting MCP. These paper MCPs can then be flexibly connected to a chat agent (e.g. Claude Code) to carry out complex scientific queries through natural language while invoking tools and workflows from the original paper. We demonstrate Paper2Agent's effectiveness in creating reliable and capable paper agents through in-depth case studies. Paper2Agent created an agent that leverages AlphaGenome to interpret genomic variants and agents based on ScanPy and TISSUE to carry out single-cell and spatial transcriptomics analyses. We validate that these paper agents can reproduce the original paper's results and can correctly carry out novel user queries. By turning static papers into dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for knowledge dissemination and a foundation for the collaborative ecosystem of AI co-scientists.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.AI&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45340133</guid><pubDate>Mon, 22 Sep 2025 22:02:01 +0000</pubDate></item><item><title>Kevo app shutdown</title><link>https://www.kwikset.com/support/answers/what-does-the-kevo-app-shutdown-mean-to-my-kevo-door-lock</link><description>&lt;doc fingerprint="922c7b946738aaf4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;What does the November 14th, 2025 Kevo app shutdown mean to my Kevo door lock?&lt;/head&gt;
    &lt;p&gt;After more than a decade of service, as of November 14, 2025 the Kevo app and web portal will no longer be available.&lt;lb/&gt; ASSA ABLOY Americas Residential Inc. (“ASSA ABLOY”, “we” and “us”), which is the successor to the company that previously marketed Kwikset Kevo, Weiser Kevo and Baldwin Evolved smart door locks, will cease supporting your Kevo lock’s remote functionality.&lt;/p&gt;
    &lt;p&gt;Locks Affected (All Generations): Kevo, Kevo Convert, Kevo Plus, Baldwin Evolved&lt;/p&gt;
    &lt;p&gt;Brands Affected: Kwikset, Weiser, Baldwin&lt;/p&gt;
    &lt;p&gt;Impact&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Users can no longer open/close or manage their door lock via the mobile app or web portal.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Not Impacted&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Physical Key, users will be able to unlock or lock the deadbolt with the physical key&lt;/item&gt;
      &lt;item&gt;Key FOB, users will be able to unlock or lock the deadbolt with the Key FOB&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Required User Action&lt;/p&gt;
    &lt;p&gt;Prepare in advance for the Kevo app shutdown. Ensure that you have the physical key or key fob to unlock and lock the door moving forward or you can redeem the unique promotional offer that existing Kevo users received via e-mail and replace the Kevo deadbolt entirely.&lt;/p&gt;
    &lt;p&gt;Replacement Door Lock Discount&lt;/p&gt;
    &lt;p&gt;To help make this transition easier, we’re offering our steepest discounts ever on trusted smart lock replacements, available exclusively to Kevo users.&lt;/p&gt;
    &lt;p&gt;(United States Only)&lt;/p&gt;
    &lt;p&gt;Offers will be fulfilled by our partners at Level, a fellow ASSA ABLOY brand. Your orders will be securely processed and shipped through Level’s website.&lt;/p&gt;
    &lt;p&gt;Available options include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;$80 off Kwikset Halo Keypad Wi-Fi Smart Lock&lt;/item&gt;
      &lt;item&gt;$130 off Level Lock+&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How to Redeem&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Use the following link to visit&lt;/item&gt;
      &lt;item&gt;Choose the replacement deadbolt that is right for you&lt;/item&gt;
      &lt;item&gt;Enter your unique promotional code at checkout&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Your unique promotional code was sent to your registered Kevo e-mail address, notifying you of the Kevo app shutdown&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The above offer is final, and no other offers will ensue with respect to the loss of remote functionality of your Kevo door lock. This offer will expire December 14, 2025.&lt;lb/&gt; (Canada Only)&lt;lb/&gt; Orders will be securely processed and shipped through Weiser’s customer service team.&lt;lb/&gt; Available options include:&lt;lb/&gt; - $89 (CDN) off Weiser Halo Keypad Wi-Fi Smart Lock&lt;lb/&gt; How to Redeem&lt;lb/&gt; 1. Call our Weiser customer service team: 1-800-501-9471&lt;lb/&gt; 2. Ask the service team member about claiming your Kevo replacement offer&lt;lb/&gt; 3. Provide your unique promo code&lt;lb/&gt; o Your unique promotional code was sent to your registered Kevo e-mail address, notifying you of the Kevo app shutdown&lt;lb/&gt; These offers are made in connection with the Kevo app shutdown; and available through December 14, 2025—without further extension.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45340192</guid><pubDate>Mon, 22 Sep 2025 22:07:07 +0000</pubDate></item><item><title>In Maine, prisoners are thriving in remote jobs</title><link>https://www.mainepublic.org/2025-08-29/in-maine-prisoners-are-thriving-in-remote-jobs-and-other-states-are-taking-notice</link><description>&lt;doc fingerprint="3620d016782416f9"&gt;
  &lt;main&gt;
    &lt;p&gt;People who are incarcerated are paid notoriously low wages for kitchen, laundry work and maintenance.&lt;/p&gt;
    &lt;p&gt;But the expanded use of laptops is creating other opportunities.&lt;/p&gt;
    &lt;p&gt;This is part two in a two-part series about remote work in Maine prisons. To read part one, click here.&lt;/p&gt;
    &lt;p&gt;Preston Thorpe is only 32, but he says he's already landed his dream job as a senior software engineer and bought a modest house with his six-figure salary. It was all accomplished by putting in long days from his cell at the Mountain View Correctional Center in Charleston.&lt;/p&gt;
    &lt;p&gt;"It's not normal to have 15-17 hours a day to really focus on something and learn something, like deeply," Thorpe says. "And fortunately tech is one of the few areas where they're not concerned with your college degree. They're really only concerned with your ability to write code."&lt;/p&gt;
    &lt;p&gt;A self-described "computer geek," Thorpe says he built his first computer at age 13. In high school he always expected he'd have a career in tech. But he also had a rebellious side. He got into trouble with drugs, using them and selling them. He says his parents kicked him out of the house and he ended up in prison for the first time at age 20.&lt;/p&gt;
    &lt;p&gt;"You know, I was worried and pretty hopeless that I had messed my life up so bad that it was no longer possible to have like a normal life and normal career," he says.&lt;/p&gt;
    &lt;p&gt;When you have nothing to lose, Thorpe says it's pretty easy to behave that way. And when you're out of prison with a criminal record, no money and an identity as a convict, he says the likelihood you're going to improve your life in any way is zero.&lt;/p&gt;
    &lt;p&gt;His own circumstances changed in 2019 when he got transferred from the New Hampshire prison system to Maine, where he discovered laptops with limited internet access were available for education. That's when he says he had an epiphany that he could change himself by pursuing his passion. And about two years ago, he became one of the first incarcerated people in the country to get hired for a remote job.&lt;/p&gt;
    &lt;p&gt;"Now I feel like my life has purpose," Thorpe says.&lt;/p&gt;
    &lt;p&gt;Glauber Costa says he first became aware of Thorpe through his contributions to an online public software project. Costa is the CEO of Turso, an international database company. He was impressed by Thorpe's work and had no idea he was incarcerated.&lt;/p&gt;
    &lt;p&gt;Once he found out, he thought it would be impossible to talk to Thorpe, let alone hire him.&lt;/p&gt;
    &lt;p&gt;"But then," Costa says, "it turns out that he can take video calls. And then by talking to him, it became very, very clear to me that if this is not a reformed person I don't know what is."&lt;/p&gt;
    &lt;p&gt;Costa says he was also surprised to learn that Thorpe was eligible for remote work while he was in prison. He hired him in June. He figured Thorpe might have trouble clearing the company's background check and he says he prepared himself for that. But since it only searches back seven years and since Thorpe has been in prison for more than a decade, "He is actually our cleanest background check," Costa says.&lt;/p&gt;
    &lt;p&gt;"He doesn't have a parking ticket."&lt;/p&gt;
    &lt;p&gt;Several dozen other prisoners are also working remote jobs. At the Maine Correctional Center in Windham, Darlene George is a certified recovery coach, a scholar and a teaching assistant who's serving a 40-year sentence for the murder of her husband.&lt;/p&gt;
    &lt;p&gt;"I became incarcerated in 2009 and I've been here 16 years," George says.&lt;/p&gt;
    &lt;p&gt;Unlike most women in prison, George had a college degree before she was incarcerated. She says she still tries to make the most of every opportunity she can. For the past two years, she's held a full-time remote job, first as a grant writer and now as a program coordinator, for a Maine-based health care company.&lt;/p&gt;
    &lt;p&gt;"The work, it just, it really makes you feel good," she says. "When I — when I put my head down at night I can say I'm giving something back."&lt;/p&gt;
    &lt;p&gt;George relishes her role as a decision maker and an advocate for clients' health care. She makes a competitive salary. And she says her boss and her co-workers are extremely supportive of her situation, and so are the other women at the Maine Correctional Center.&lt;/p&gt;
    &lt;p&gt;"Literally, I work from my room. There's a sign that I put out that lets people know I'm Zooming or in meetings," she says. "They try not to be noisy on the floor ... because they're like, 'Well, we want a job, too.'"&lt;/p&gt;
    &lt;p&gt;Mara Sanchez, the program director for the Alliance for Higher Education in Prison, says Maine's Department of Corrections was the first to have a remote work policy.&lt;/p&gt;
    &lt;p&gt;"Their implementation, willingness to try remote work for incarcerated students has really kind of set the bar for other states and been very inspiring to other states," she says.&lt;/p&gt;
    &lt;p&gt;Maine Corrections Commissioner Randall Liberty says remote work is an outgrowth of expanded educational opportunities in prison. There are 800 residents who now have access to the internet.&lt;/p&gt;
    &lt;p&gt;"We have technicians that are watching where they're going and what they're doing and we've had very few problems," Liberty says. "If it provides meaningful employment for them ... it also allows for a transition back into the community."&lt;/p&gt;
    &lt;p&gt;For example, Liberty says one resident worked as a paralegal for a law firm and continued with the job after he got out. Wages are garnished for child support, victim restitution and other fees. And for those who earn above a certain amount, 10% goes to the Department of Corrections for room and board. But residents can also save money or send it home. And Liberty says between educational programs and remote work, the prison environment is better for everyone.&lt;/p&gt;
    &lt;p&gt;"It's important that the officers work with residents that have hope and have meaning in their life," he says. "We had 87 assaults on staff in 2017. Last year, we had seven assaults on staff. So all of this work isn't just about the residents. It's about the community ... the officers that go to work everyday and don't feel like their life is at risk."&lt;/p&gt;
    &lt;p&gt;Liberty is optimistic that remote work can be expanded to other people in prison as the network of employers who understand their value grows.&lt;/p&gt;
    &lt;p&gt;"I think it can become the norm," he says. "This isn't a reckless attempt at finding work for individuals. This is a well-thought out plan with lessons learned and consequences. ... The last thing anybody wants is to lose their laptop."&lt;/p&gt;
    &lt;p&gt;Preston Thorpe would agree. He's hoping to be released sometime next year. He never expected to have started a successful career in prison or to have bought a house. But he says what he's most proud of is that after everything he's put his parents through, they are proud of him.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45340600</guid><pubDate>Mon, 22 Sep 2025 22:51:09 +0000</pubDate></item><item><title>Fall Foliage Map 2025</title><link>https://www.explorefall.com/fall-foliage-map</link><description>&lt;doc fingerprint="14d19396b861a2a5"&gt;
  &lt;main&gt;
    &lt;p&gt;Little to No Color&lt;/p&gt;
    &lt;p&gt;Low Color&lt;/p&gt;
    &lt;p&gt;Moderate Color&lt;/p&gt;
    &lt;p&gt;High Color&lt;/p&gt;
    &lt;p&gt;Peak Color&lt;/p&gt;
    &lt;p&gt;Past Peak Color&lt;/p&gt;
    &lt;p&gt;Color Report&lt;/p&gt;
    &lt;p&gt;Peak Timing&lt;/p&gt;
    &lt;p&gt;Reports&lt;/p&gt;
    &lt;p&gt;Download&lt;/p&gt;
    &lt;p&gt;Coming Fall 2025&lt;/p&gt;
    &lt;p&gt;Powered by Esri&lt;/p&gt;
    &lt;p&gt;Fall is nearly upon us, and we're once again helping fall foliage enthusiasts across the country find the best fall color! Use our map to explore the estimated timing of fall foliage throughout the United States, allowing you to plan trips with confidence this year. Check back regularly for updates based on the latest reports gathered from hundreds of sources throughout the country.&lt;/p&gt;
    &lt;p&gt;Two primary factors control the timing of fall foliage: daylight and temperature. This means that the further north and the higher in elevation a tree is, the earlier it will reveal it's colorful canopy. Photosynthesis grinds to a halt when the days grow short in the fall, and leaves no longer have a need for their excess stores of chlorophyll.&lt;/p&gt;
    &lt;p&gt;Over the course of a month or two, the concentrations of chlorophyll diminish, allowing less concentrated chemicals such as anthocyanin and carotenoids to dominate, turning the leaf red, yellow, or orange. The rate at which this change occurs varies amongst tree species, so it can be difficult to pinpoint a single peak in fall foliage.&lt;/p&gt;
    &lt;p&gt;Nevertheless, when the vast majority of trees in a particular area have full canopies of autumnal color, peak has arrived. Some areas, partiularly in the Northeast, experience vibrant red peaks due to an abundance of maple trees, while others experience a mixture of all of fall's colors. Different trees display different colors, giving each region its own unique peak.&lt;/p&gt;
    &lt;p&gt;For most of the United States, peak fall color arrives in the month of October. This is when wide swaths of the Northeast, Midwest, and Western states are aglow with bright fall foliage, and more than 80% of travelers make their fall foliage trips. Some less-populated regions will peak in September (August in northern Alaska), while the southernmost states hold off until mid-November.&lt;/p&gt;
    &lt;p&gt;The most popular fall foliage displays are found in New England, where approximately ten million people travel each year in hopes of photographing or simply walking through fall's splendor. Northern Vermont, New Hampshire, and northwestern Maine experience peak in early October, while much of New York, Massachusetts, and Pennsylvania have to wait until later in the month.&lt;/p&gt;
    &lt;p&gt;Out west, golden Aspens peak in sweeping displays in late September and early October, just prior to the invasive chill of winter. Non-desert, lower elevations in the Northwest are further delayed into late October/early November; however, the wait is well worth it.&lt;/p&gt;
    &lt;p&gt;If you've ever traveled in search of fall foliage before, you likely know how tricky it can be to be in the right place at the right time. The timing of peak color varies signficantly season-to-season, meaning what worked one year might not work the next! The best fall trips take careful planning, a lot of patience, and a reliable fall foliage map.&lt;/p&gt;
    &lt;p&gt;It's helpful to establish a baseline for when leaves normally change. Maps, like the one in the above section, can help you identify roughly when in the season you should be planning your trip. From there, you should consult a real-time fall foliage map like ours to see if fall foliage is on-time or running early/late due to ongoing weather conditions.&lt;/p&gt;
    &lt;p&gt;If at all possible, don't solidify your plans until you're two weeks out from peak fall foliage. This allows you to be flexible should extreme weather rear its head and disrupt the normal progression of fall foliage. Should that not be an option for you, do your planning in early September when fall foliage experts can give you an idea of whether or not fall color is on-time this year.&lt;/p&gt;
    &lt;p&gt;You'll want to make the most of your time in fall's splendor, so be sure to pick out a few beautiful hikes or drives on which you can truly be emersed in autumnal glory. If you're looking to beat the crowds, consider going to popular locations very early in the morning, before the majority of people arrive. Sunrise bathes fall foliage in golden hues, making early morning one of the best times to venture out!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45341324</guid><pubDate>Tue, 23 Sep 2025 00:14:37 +0000</pubDate></item><item><title>X server implementation for SIXEL-featured terminals (2010-2014)</title><link>https://github.com/saitoha/xserver-SIXEL</link><description>&lt;doc fingerprint="10de925be4457d82"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 5&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A X server implementation for SIXEL-featured terminals, based on @pelya's Xsdl kdrive server(https://github.com/pelya/xserver-xsdl)&lt;/p&gt;
    &lt;head rend="h3"&gt;License&lt;/head&gt;
    &lt;head rend="h1"&gt;saitoha/xserver-SIXEL&lt;/head&gt;
    &lt;head rend="h2"&gt;Folders and files&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Name&lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;Last commit message&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;Last commit date&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Repository files navigation&lt;/head&gt;
    &lt;quote&gt;X Server The X server accepts requests from client applications to create windows, which are (normally rectangular) "virtual screens" that the client program can draw into. Windows are then composed on the actual screen by the X server (or by a separate composite manager) as directed by the window manager, which usually communicates with the user via graphical controls such as buttons and draggable titlebars and borders. For a comprehensive overview of X Server and X Window System, consult the following article: http://en.wikipedia.org/wiki/X_server All questions regarding this software should be directed at the Xorg mailing list: http://lists.freedesktop.org/mailman/listinfo/xorg Please submit bug reports to the Xorg bugzilla: https://bugs.freedesktop.org/enter_bug.cgi?product=xorg The master development code repository can be found at: git://anongit.freedesktop.org/git/xorg/xserver http://cgit.freedesktop.org/xorg/xserver For patch submission instructions, see: http://www.x.org/wiki/Development/Documentation/SubmittingPatches For more information on the git code manager, see: http://wiki.x.org/wiki/GitPage&lt;/quote&gt;
    &lt;head rend="h2"&gt;About&lt;/head&gt;
    &lt;p&gt;A X server implementation for SIXEL-featured terminals, based on @pelya's Xsdl kdrive server(https://github.com/pelya/xserver-xsdl)&lt;/p&gt;
    &lt;head rend="h3"&gt;Resources&lt;/head&gt;
    &lt;head rend="h3"&gt;License&lt;/head&gt;
    &lt;head rend="h3"&gt;Stars&lt;/head&gt;
    &lt;head rend="h3"&gt;Watchers&lt;/head&gt;
    &lt;head rend="h3"&gt;Forks&lt;/head&gt;
    &lt;head rend="h2"&gt;Releases&lt;/head&gt;
    &lt;p&gt;No releases published&lt;/p&gt;
    &lt;head rend="h2"&gt;Packages 0&lt;/head&gt;
    &lt;p&gt; No packages published &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45341683</guid><pubDate>Tue, 23 Sep 2025 01:07:57 +0000</pubDate></item><item><title>9 Things I Learned in 90 Years – From the Creator of Choose Your Own Adventure [pdf]</title><link>http://edwardpackard.com/wp-content/uploads/2025/09/Nine-Things-I-Learned-in-Ninety-Years.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45342364</guid><pubDate>Tue, 23 Sep 2025 03:03:06 +0000</pubDate></item><item><title>The Latest Linux File-System: TernFS</title><link>https://www.phoronix.com/news/TernFS-File-System-Open-Source</link><description>&lt;doc fingerprint="459a34730fbf52d2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A Major Trading Firm Has Open-Sourced The Latest Linux File-System: TernFS&lt;/head&gt;
    &lt;p&gt; XTX Markets as one of the largest algorithmic trading firms that handles $250 billion in daily traded volume and relies on around 650+ petabytes of storage for its price forecasts and other algorithmic trading data has open-sourced its Linux file-system. XTX developed TernFS for distributed storage after they outgrew their original NFS usage and other file-system alternatives. &lt;lb/&gt;TernFS is a distributed file-system for dealing with reading and writing large immutable files -- immutable in the context that they are typically not modified after being created. And large in the context of being at least a few megabytes. TernFS is designed for XTX data center needs of maxing out at around 10EB of logical file storage, around one trillion files and 100 billion directories with around one million clients. All running atop commodity hardware and Ethernet networking.&lt;lb/&gt;TernFS is also able to span across multiple geographic regions, store data redundantly to protect against drive failures, no single point of failure for metadata, and other features for the demanding needs of XTX Markets.&lt;lb/&gt;TernFS has been in development for over three years and for over the past year is being used by all of the XTX machine learning efforts. XTX Markets reports that "to this day, we haven't lost a single byte" on TernFS.&lt;lb/&gt;Those wishing to learn more about this Linux-native TernFS file-system that is now open-source can do so via the XTX Markets blog. TernFS is available under GPLv2+ and Apache 2.0 licenses. The TernFS file-system is open-source on GitHub.&lt;/p&gt;
    &lt;p&gt;TernFS is a distributed file-system for dealing with reading and writing large immutable files -- immutable in the context that they are typically not modified after being created. And large in the context of being at least a few megabytes. TernFS is designed for XTX data center needs of maxing out at around 10EB of logical file storage, around one trillion files and 100 billion directories with around one million clients. All running atop commodity hardware and Ethernet networking.&lt;/p&gt;
    &lt;p&gt;TernFS is also able to span across multiple geographic regions, store data redundantly to protect against drive failures, no single point of failure for metadata, and other features for the demanding needs of XTX Markets.&lt;/p&gt;
    &lt;p&gt;TernFS has been in development for over three years and for over the past year is being used by all of the XTX machine learning efforts. XTX Markets reports that "to this day, we haven't lost a single byte" on TernFS.&lt;/p&gt;
    &lt;p&gt;Those wishing to learn more about this Linux-native TernFS file-system that is now open-source can do so via the XTX Markets blog. TernFS is available under GPLv2+ and Apache 2.0 licenses. The TernFS file-system is open-source on GitHub.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45342592</guid><pubDate>Tue, 23 Sep 2025 03:39:07 +0000</pubDate></item></channel></rss>