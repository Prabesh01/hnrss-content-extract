<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 29 Jan 2026 21:47:21 +0000</lastBuildDate><item><title>Run Clawdbot/Moltbot on Cloudflare with Moltworker</title><link>https://blog.cloudflare.com/moltworker-self-hosted-ai-agent/</link><description>&lt;doc fingerprint="bd02981209b71cce"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;The Internet woke up this week to a flood of people buying Mac minis to run Moltbot (formerly Clawdbot), an open-source, self-hosted AI agent designed to act as a personal assistant. Moltbot runs in the background on a user's own hardware, has a sizable and growing list of integrations for chat applications, AI models, and other popular tools, and can be controlled remotely. Moltbot can help you with your finances, social media, organize your day â all through your favorite messaging app.&lt;/p&gt;
      &lt;p&gt;But what if you donât want to buy new dedicated hardware? And what if you could still run your Moltbot efficiently and securely online? Meet Moltworker, a middleware Worker and adapted scripts that allows running Moltbot on Cloudflare's Sandbox SDK and our Developer Platform APIs.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;A personal assistant on Cloudflare â how does that work?Â &lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Firstly, Cloudflare Workers has never been so compatible with Node.js. Where in the past weÂ had to mock APIs to get some packages running, now those APIs are supported natively by the Workers Runtime.&lt;/p&gt;
      &lt;p&gt;This has changed how we can build tools on Cloudflare Workers. When we first implemented Playwright, a popular framework for web testing and automation that runs on Browser Rendering, we had to rely on memfs. This was bad because not only is memfs a hack and an external dependency, but it also forced us to drift away from the official Playwright codebase. Thankfully, with more Node.js compatibility, we were able to start using node:fs natively, reducing complexity and maintainability, which makes upgrades to the latest versions of Playwright easy to do.&lt;/p&gt;
      &lt;p&gt;The list of Node.js APIs we support natively keeps growing. The blog post âA year of improving Node.js compatibility in Cloudflare Workersâ provides an overview of where we are and what weâre doing.&lt;/p&gt;
      &lt;p&gt;We measure this progress, too. We recently ran an experiment where we took the 1,000 most popular NPM packages, installed and let AI loose, to try to run them in Cloudflare Workers, Ralph Wiggum as a "software engineer" style, and the results were surprisingly good. Excluding the packages that are build tools, CLI tools or browser-only and donât apply, only 15 packages genuinely didnât work. That's 1.5%.&lt;/p&gt;
      &lt;p&gt;Hereâs a graphic of our Node.js API support over time:&lt;/p&gt;
      &lt;p&gt;We put together a page with the results of our internal experiment on npm packages support here, so you can check for yourself.&lt;/p&gt;
      &lt;p&gt;Moltbot doesnât necessarily require a lot of Workers Node.js compatibility because most of the code runs in a container anyway, but we thought it would be important to highlight how far we got supporting so many packages using native APIs. This is because when starting a new AI agent application from scratch, we can actually run a lot of the logic in Workers, closer to the user.&lt;/p&gt;
      &lt;p&gt;The other important part of the story is that the list of products and APIs on our Developer Platform has grown to the point where anyone can build and run any kind of application â even the most complex and demanding ones â on Cloudflare. And once launched, every application running on our Developer Platform immediately benefits from our secure and scalable global network.&lt;/p&gt;
      &lt;p&gt;Those products and services gave us the ingredients we needed to get started. First, we now have Sandboxes, where you can run untrusted code securely in isolated environments, providing a place to run the service. Next, we now have Browser Rendering, where you can programmatically control and interact with headless browser instances. And finally, R2, where you can store objects persistently. With those building blocks available, we could begin work on adapting Moltbot.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;How we adapted Moltbot to run on us&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Moltbot on Workers, or Moltworker, is a combination of an entrypoint Worker that acts as an API router and a proxy between our APIs and the isolated environment, both protected by Cloudflare Access. It also provides an administration UI and connects to the Sandbox container where the standard Moltbot Gateway runtime and its integrations are running, using R2 for persistent storage.&lt;/p&gt;
      &lt;p&gt;High-level architecture diagram of Moltworker.&lt;/p&gt;
      &lt;p&gt;Let's dive in more.&lt;/p&gt;
      &lt;p&gt;Cloudflare AI Gateway acts as a proxy between your AI applications and any popular AI provider, and gives our customers centralized visibility and control over the requests going through.&lt;/p&gt;
      &lt;p&gt;Recently we announced support for Bring Your Own Key (BYOK), where instead of passing your provider secrets in plain text with every request, we centrally manage the secrets for you and can use them with your gateway configuration.&lt;/p&gt;
      &lt;p&gt;An even better option where you donât have to manage AI providers' secrets at all end-to-end is to use Unified Billing. In this case you top up your account with credits and use AI Gateway with any of the supported providers directly, Cloudflare gets charged, and we will deduct credits from your account.&lt;/p&gt;
      &lt;p&gt;To make Moltbot use AI Gateway, first we create a new gateway instance, then we enable the Anthropic provider for it, then we either add our Claude key or purchase credits to use Unified Billing, and then all we need to do is set the ANTHROPIC_BASE_URL environment variable so Moltbot uses the AI Gateway endpoint. Thatâs it, no code changes necessary.&lt;/p&gt;
      &lt;p&gt;Once Moltbot starts using AI Gateway, youâll have full visibility on costs and have access to logs and analytics that will help you understand how your AI agent is using the AI providers.&lt;/p&gt;
      &lt;p&gt;Note that Anthropic is one option; Moltbot supports other AI providers and so does AI Gateway. The advantage of using AI Gateway is that if a better model comes along from any provider, you donât have to swap keys in your AI Agent configuration and redeploy â you can simply switch the model in your gateway configuration. And more, you specify model or provider fallbacks to handle request failures and ensure reliability.&lt;/p&gt;
      &lt;p&gt;Last year we anticipated the growing need for AI agents to run untrusted code securely in isolated environments, and we announced the Sandbox SDK. This SDK is built on top of Cloudflare Containers, but it provides a simple API for executing commands, managing files, running background processes, and exposing services â all from your Workers applications.&lt;/p&gt;
      &lt;p&gt;In short, instead of having to deal with the lower-level Container APIs, the Sandbox SDK gives you developer-friendly APIs for secure code execution and handles the complexity of container lifecycle, networking, file systems, and process management â letting you focus on building your application logic with just a few lines of TypeScript. Hereâs an example:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;import { getSandbox } from '@cloudflare/sandbox';
export { Sandbox } from '@cloudflare/sandbox';

export default {
  async fetch(request: Request, env: Env): Promise&amp;lt;Response&amp;gt; {
    const sandbox = getSandbox(env.Sandbox, 'user-123');

    // Create a project structure
    await sandbox.mkdir('/workspace/project/src', { recursive: true });

    // Check node version
    const version = await sandbox.exec('node -v');

    // Run some python code
    const ctx = await sandbox.createCodeContext({ language: 'python' });
    await sandbox.runCode('import math; radius = 5', { context: ctx });
    const result = await sandbox.runCode('math.pi * radius ** 2', { context: ctx });

    return Response.json({ version, result });
  }
};&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;This fits like a glove for Moltbot. Instead of running Docker in your local Mac mini, we run Docker on Containers, use the Sandbox SDK to issue commands into the isolated environment and use callbacks to our entrypoint Worker, effectively establishing a two-way communication channel between the two systems.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;R2 for persistent storage&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;The good thing about running things in your local computer or VPS is you get persistent storage for free. Containers, however, are inherently ephemeral, meaning data generated within them is lost upon deletion. Fear not, though â the Sandbox SDK provides the sandbox.mountBucket() that you can use to automatically, well, mount your R2 bucket as a filesystem partition when the container starts.&lt;/p&gt;
      &lt;p&gt;Once we have a local directory that is guaranteed to survive the container lifecycle, we can use that for Moltbot to store session memory files, conversations and other assets that are required to persist.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Browser Rendering for browser automation&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;AI agents rely heavily on browsing the sometimes not-so-structured web. Moltbot utilizes dedicated Chromium instances to perform actions, navigate the web, fill out forms, take snapshots, and handle tasks that require a web browser. Sure, we can run Chromium on Sandboxes too, but what if we could simplify and use an API instead?&lt;/p&gt;
      &lt;p&gt;With Cloudflareâs Browser Rendering, you can programmatically control and interact with headless browser instances running at scale in our edge network. We support Puppeteer, Stagehand, Playwright and other popular packages so that developers can onboard with minimal code changes. We even support MCP for AI.&lt;/p&gt;
      &lt;p&gt;In order to get Browser Rendering to work with Moltbot we do two things:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;First we create a thin CDP proxy (CDP is the protocol that allows instrumenting Chromium-based browsers) from the Sandbox container to the Moltbot Worker, back to Browser Rendering using the Puppeteer APIs.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Then we inject a Browser Rendering skill into the runtime when the Sandbox starts.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;From the Moltbot runtime perspective, it has a local CDP port it can connect to and perform browser tasks.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Zero Trust Access for authentication policies&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Next up we want to protect our APIs and Admin UI from unauthorized access. Doing authentication from scratch is hard, and is typically the kind of wheel you donât want to reinvent or have to deal with. Zero Trust Access makes it incredibly easy to protect your application by defining specific policies and login methods for the endpoints.Â &lt;/p&gt;
      &lt;p&gt;Zero Trust Access Login methods configuration for the Moltworker application.&lt;/p&gt;
      &lt;p&gt;Once the endpoints are protected, Cloudflare will handle authentication for you and automatically include a JWT token with every request to your origin endpoints. You can then validate that JWT for extra protection, to ensure that the request came from Access and not a malicious third party.&lt;/p&gt;
      &lt;p&gt;Like with AI Gateway, once all your APIs are behind Access you get great observability on who the users are and what they are doing with your Moltbot instance.&lt;/p&gt;
      &lt;p&gt;Demo time. Weâve put up a Slack instance where we could play with our own instance of Moltbot on Workers. Here are some of the fun things weâve done with it.&lt;/p&gt;
      &lt;p&gt;We hate bad news.&lt;/p&gt;
      &lt;p&gt;Hereâs a chat session where we ask Moltbot to find the shortest route between Cloudflare in London and Cloudflare in Lisbon using Google Maps and take a screenshot in a Slack channel. It goes through a sequence of steps using Browser Rendering to navigate Google Maps and does a pretty good job at it. Also look at Moltbotâs memory in action when we ask him the second time.&lt;/p&gt;
      &lt;p&gt;Weâre in the mood for some Asian food today, letâs get Moltbot to work for help.&lt;/p&gt;
      &lt;p&gt;We eat with our eyes too.&lt;/p&gt;
      &lt;p&gt;Letâs get more creative and ask Moltbot to create a video where it browses our developer documentation. As you can see, it downloads and runs ffmpeg to generate the video out of the frames it captured in the browser.&lt;/p&gt;
      &lt;p&gt;We open-sourced our implementation and made it available at https://github.com/cloudflare/moltworker, so you can deploy and run your own Moltbot on top of Workers today.&lt;/p&gt;
      &lt;p&gt;The README guides you through the necessary steps to set up everything. You will need a Cloudflare account and a minimum $5 USD Workers paid plan subscription to use Sandbox Containers, but all the other products are either free to use, like AI Gateway, or have generous free tiers you can use to get you started and run for as long as you want under reasonable limits.&lt;/p&gt;
      &lt;p&gt;Note that Moltworker is a proof of concept, not a Cloudflare product. Our goal is to showcase some of the most exciting features of our Developer Platform that can be used to run AI agents and unsupervised code efficiently and securely, and get great observability while taking advantage of our global network.&lt;/p&gt;
      &lt;p&gt;Feel free to contribute to or fork our GitHub repository; we will keep an eye on it for a while for support. We are also considering contributing upstream to the official project with Cloudflare skills in parallel.&lt;/p&gt;
      &lt;p&gt;We hope you enjoyed this experiment, and we were able to convince you that Cloudflare is the perfect place to run your AI applications and agents. Weâve been working relentlessly trying to anticipate the future and release features like the Agents SDK that you can use to build your first agent in minutes, Sandboxes where you can run arbitrary code in an isolated environment without the complications of the lifecycle of a container, and AI Search, Cloudflareâs managed vector-based search service, to name a few.&lt;/p&gt;
      &lt;p&gt;Cloudflare now offers a complete toolkit for AI development: inference, storage APIs, databases, durable execution for stateful workflows, and built-in AI capabilities. Together, these building blocks make it possible to build and run even the most demanding AI applications on our global edge network.&lt;/p&gt;
      &lt;p&gt;If you're excited about AI and want to help us build the next generation of products and APIs, we're hiring.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46810828</guid><pubDate>Thu, 29 Jan 2026 14:43:07 +0000</pubDate></item><item><title>How to Choose Colors for Your CLI Applications (2023)</title><link>https://blog.xoria.org/terminal-colors/</link><description>&lt;doc fingerprint="66a2137707fca2cf"&gt;
  &lt;main&gt;
    &lt;p&gt;Letâs say youâre creating a CLI tool which has to display syntax highlighted source code. You begin by choosing some colors which look nice with your chosen terminal theme:&lt;/p&gt;
    &lt;p&gt;Nice! However, who knows if itâll still look good for people who use a theme different to yours? It seems sensible to try out the defaults, at least. Letâs start with the macOS Terminal.app default theme:&lt;/p&gt;
    &lt;p&gt;Youch! It seems fair to try the Tango themes next, since those are the default on e.g. Ubuntu:&lt;/p&gt;
    &lt;p&gt;Hmm, better, but not by much. Finally, letâs try what is likely the most popular custom terminal theme – Solarized:&lt;/p&gt;
    &lt;p&gt;Well then … Letâs take a look at each palette and investigate.&lt;/p&gt;
    &lt;head rend="h2"&gt;Sorcerer&lt;/head&gt;
    &lt;p&gt;In Sorcerer, all colors are readable on the default background except for &lt;code&gt;black&lt;/code&gt;,
which is in fact darker than the background.
This is useful as the background color
for status bars and the like.
&lt;code&gt;white&lt;/code&gt; is the same color as
the default foreground,
and &lt;code&gt;brblack&lt;/code&gt; is a nice faded color.
Additionally, &lt;code&gt;brwhite&lt;/code&gt; is
even lighter than the foreground;
this allows for subtle emphasization
of important text
like error messages and titles.&lt;/p&gt;
    &lt;head rend="h2"&gt;Basic&lt;/head&gt;
    &lt;p&gt;The Basic themes are, well, horrendous. Really owning that 90s xterm look, it seems. &lt;code&gt;bryellow&lt;/code&gt; is unreadable in light mode
(check out that function name
from the code sample earlier),
while in dark mode
both &lt;code&gt;blue&lt;/code&gt; and &lt;code&gt;brblue&lt;/code&gt;
are totally illegible.&lt;/p&gt;
    &lt;p&gt;That leaves us with thirteen colors we can safely use:&lt;/p&gt;
    &lt;head rend="h2"&gt;Tango&lt;/head&gt;
    &lt;p&gt;In my opinion these did a lot better than Terminal.appâs Basic themes, but they are still far from perfect. &lt;code&gt;bryellow&lt;/code&gt; is again unreadable in the light theme,
and perhaps &lt;code&gt;brgreen&lt;/code&gt; is
a little difficult to see,
though itâs nothing that would
stop me from using &lt;code&gt;brgreen&lt;/code&gt;
in an application.&lt;/p&gt;
    &lt;p&gt;At this point you may have noticed how the greyscales – &lt;code&gt;black&lt;/code&gt;, &lt;code&gt;brblack&lt;/code&gt;, &lt;code&gt;white&lt;/code&gt; &amp;amp; &lt;code&gt;brwhite&lt;/code&gt; –
have remained consistent
between light and dark themes
for both Basic and Tango.
Of course,
this means that
&lt;code&gt;{,br}white&lt;/code&gt; is unreadable in Tango Light
(owing to the light background)
and &lt;code&gt;black&lt;/code&gt; is unreadable in Tango Dark
(owing to the dark background).&lt;/p&gt;
    &lt;p&gt;In other words: forget about that idea of mine from earlier about using &lt;code&gt;brwhite&lt;/code&gt; to emphasize content.
Unless, of course,
you donât mind if your
eminently emphasized words
are completely unreadable
for the user of your software
who deigns to use the default light theme
of A Popular Linux Distro.&lt;/p&gt;
    &lt;p&gt;On the other hand, using &lt;code&gt;brblack&lt;/code&gt; to de-emphasize content
still seems fine to me.
I suppose some extra contrast
for &lt;code&gt;brblack&lt;/code&gt; in Tango Dark
would be nice,
but with text which is meant to be ignored
I donât think this matters much.&lt;/p&gt;
    &lt;p&gt;And lo, but ten colors remain.&lt;/p&gt;
    &lt;head rend="h2"&gt;Solarized&lt;/head&gt;
    &lt;p&gt;Solarized is a curious beast. Every color in it was chosen using L*a*b*, a perceptually-uniform color space from the 1970s. (For what itâs worth, color science has progressed significantly since then; the only reason Ethan Schoonover used L*a*b* is that itâs commonly used in photography, and he used to be a professional photographer.)&lt;/p&gt;
    &lt;p&gt;Its lightnesses are perfectly symmetrical so that Solarized Light and Dark can share a set of accent colors while maintaining identical contrast. Moreover, the warm tones of the light theme and cool tones of the dark theme are complementary. (The hue gap is closer to 150Â° than 180Â° in reality. See here and here to compare hue values.)&lt;/p&gt;
    &lt;p&gt;Solarized is also incredibly popular. I have no data here, but as of the date of writing itâs the most starred theme repository on GitHub I can find. Solarized has 15.4 thousand stars at the moment, while the next-closest is Gruvbox with 11.8 thousand. Solarized is available as a plugin or sometimes even as a built-in preset in damn near every popular terminal emulator and editor on the planet.&lt;/p&gt;
    &lt;p&gt;To understand Solarizedâs peculiar arrangement of the 16-color palette, we have to travel back in time to 2011 when Solarized was first released. In this dark era, terminals supporting 24-bit color didnât exist / werenât widespread. One option common among Vim themes at the time was to round every color to the nearest 256-color palette value. In Solarizedâs case, this destroys the mathematical symmetry at the heart of the theme. (Iâm not kidding, it looks awful.)&lt;/p&gt;
    &lt;p&gt;The solution – rather, hack – chosen at the time was to distill all the colors used in the Vim interface down to a palette of sixteen colors. Conveniently, Solarizedâs accent colors fit nicely into the non-bright column of the 16-color palette, while Solarizedâs monotones fit into the bright column. Once the user sets their terminal to use the Solarized palette, Vim can color its entire interface using only the 16-color palette and get correct color values, no clunky color approximations needed.&lt;/p&gt;
    &lt;p&gt;The downside to all this is that an application which uses any of the bright colors which Solarized co-opted for itself will look strange. Users of Solarized – and, by god, thereâs so many of them – appear frequently on issue trackers asking why command-line output is inexplicably gray or even invisible as a result of CLIs using these forsaken bright colors.&lt;/p&gt;
    &lt;p&gt;Our beloved &lt;code&gt;brblack&lt;/code&gt;
is unreadable in Solarized Dark,
so weâll have to strike it from the table
in addition to the affected bright colors.&lt;/p&gt;
    &lt;head rend="h2"&gt;A sad note about bold&lt;/head&gt;
    &lt;p&gt;Far back in the past, there was no way for terminals to display bright colors. As a workaround, manufacturers (weâre talking about physical terminals here) started making all bold text bright instead of using a heavier font weight. One way or another this ended up in the default settings of many modern terminal emulators (in spite of not being in the standard), meaning that regular colorful text made bold can become bright too, depending on the userâs configuration.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;And so, I present to you the final version of our table of acceptable colors:&lt;/p&gt;
    &lt;p&gt;Bold: ââ boldblack ââ boldbrblack ââ boldred ââ boldbrred ââ boldgreen ââ boldbrgreen ââ boldyellow ââ boldbryellow ââ boldblue ââ boldbrblue ââ boldmagenta ââ boldbrmagenta ââ boldcyan ââ boldbrcyan ââ boldwhite ââ boldbrwhite % â&lt;/p&gt;
    &lt;p&gt;Only eleven out of our thirty-two possible color settings are permissible, given that we want applications to remain readable for as many people as we can.&lt;/p&gt;
    &lt;p&gt;If youâre developing a command-line tool which will be used by anyone apart from yourself, I strongly recommend you limit your use of color to the ones Iâve identified here as being âmostly alrightâ and ânot unreadable in a common configuration used by tons of peopleâ.&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix&lt;/head&gt;
    &lt;p&gt;You probably didnât notice, but I styled the âterminal windowsâ in this post to look as similar as possible to macOS Terminal.app windows through painstaking color picking and pixel counting.&lt;/p&gt;
    &lt;p&gt;The dimensions in each windowâs titlebar matches as closely as I can with its actual dimensions on-screen.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;colortest&lt;/code&gt; and &lt;code&gt;highlight&lt;/code&gt; utilities
are entirely fictional.&lt;/p&gt;
    &lt;p&gt;Terminal.app doesnât actually provide individual access to the light and dark variants of Basic; they appear as a single theme, which switches seamlessly when the OS theme changes. As far as I know, this reactive functionality isnât exposed to any other theme, whether pre-installed or user-created. In order to capture this, I made the terminal windows in this post react to whether the rest of the site is in light or dark mode, except for the Basic windows. They remain fixed in either light or dark mode, since in real life youâll never see, for example, a light Basic terminal with dark window chrome.&lt;/p&gt;
    &lt;p&gt;Luna Razzaghipour&lt;lb/&gt;29 January 2023&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46810904</guid><pubDate>Thu, 29 Jan 2026 14:49:08 +0000</pubDate></item><item><title>OTelBench: AI struggles with simple SRE tasks (Opus 4.5 scores only 29%)</title><link>https://quesma.com/blog/introducing-otel-bench/</link><description>&lt;doc fingerprint="3f5885f907369ecf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Benchmarking OpenTelemetry: Can AI trace your failed login?&lt;/head&gt;
    &lt;p&gt;Now on the front page of Hacker News — see the discussion.&lt;/p&gt;
    &lt;p&gt;Frontier AI models have become excellent at writing functions, but can they actually debug production systems?&lt;/p&gt;
    &lt;p&gt;To fix outages, you first need to see what’s happening. In a microservices world, this means producing structured events that track a single request as it hops from service to service.&lt;/p&gt;
    &lt;p&gt;We asked 14 models to add distributed traces to existing codebases, using the standard method: OpenTelemetry instrumentation. We picked tasks that would be easy for a Site Reliability Engineer (SRE).&lt;/p&gt;
    &lt;p&gt;We are releasing OTelBench as an open-source benchmark, with all tasks in QuesmaOrg/otel-bench. We use the Harbor framework (by the creators of TerminalBench), so you can easily run it yourself to reproduce results, test new models, or create benchmarks for your own use cases (we welcome contributions!).&lt;/p&gt;
    &lt;head rend="h2"&gt;Background: What is distributed tracing?&lt;/head&gt;
    &lt;p&gt;When an app runs on a single machine, you can often trace an error by scrolling through a log file. But when it runs across 50 microservices, that single request gets scattered into a chaotic firehose of disconnected events. Distributed tracing solves this by linking them back together, allowing you to follow a user action, like clicking Login, as it jumps from the API Gateway, to the Auth Service, to the Database, and back.&lt;/p&gt;
    &lt;p&gt;To make this work, you need instrumentation. This is code that you add to your app to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Start a trace when a request comes in.&lt;/item&gt;
      &lt;item&gt;Pass the TraceID (context) when your app calls another service.&lt;/item&gt;
      &lt;item&gt;Send the data to a backend so you can see the graph.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;OpenTelemetry (OTel) is the industry standard for telemetry data. Its ecosystem includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Semantic conventions: A unified schema replaces chaotic naming (e.g., &lt;code&gt;ip_address&lt;/code&gt;vs&lt;code&gt;host.ip&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;Universal SDKs: Official libraries support every major programming language.&lt;/item&gt;
      &lt;item&gt;The Collector: A centralized agent processes and enriches data (e.g., adding Kubernetes tags) before export.&lt;/item&gt;
      &lt;item&gt;Auto-instrumentation: Runtime agents inject code to wrap calls, though this often results in noisy data.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;However, standard doesn’t mean easy. We know this firsthand from our contributions to the ecosystem, such as Go compile-time instrumentation. The process may be difficult, especially due to complexity, as 39% of respondents complained in the 2025 Observability Survey.&lt;/p&gt;
    &lt;head rend="h2"&gt;Benchmarking OpenTelemetry instrumentation&lt;/head&gt;
    &lt;p&gt;We tested 14 frontier LLMs on 23 realistic OpenTelemetry instrumentation tasks across 11 programming languages: Go, Java, C++, Python, JavaScript, PHP, Ruby, Rust, Erlang, .NET, and Swift.&lt;/p&gt;
    &lt;p&gt;It is essential to benchmark various technologies since realistic distributed systems are polyglot. To make OpenTelemetry work, the system needs to work for all of these services - if we lose track at only one service, the chain of logs gets broken.&lt;/p&gt;
    &lt;p&gt;The final benchmark run cost $522 in LLM tokens across 966 runs (23 tasks × 3 attempts × 14 models).&lt;/p&gt;
    &lt;head rend="h3"&gt;Task&lt;/head&gt;
    &lt;p&gt;We start with basic tasks such as adding instrumentation to a single microservice, in a single language. The AI agents get a small microservice with around 300 lines of code from a realistic application, and work in a Linux terminal, editing it, and running any commands if needed.&lt;/p&gt;
    &lt;p&gt;For example, here is the prompt for go-microservices-traces:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Your task is: Add OTEL tracing to all microservices.&lt;/p&gt;
      &lt;p&gt;Requirements:&lt;/p&gt;
      &lt;item&gt;Instrumentation should match conventions and well-known good practices.&lt;/item&gt;
      &lt;item&gt;Instrumentation must match the business domain of the microservices.&lt;/item&gt;
      &lt;item&gt;Traces must be sent to the endpoint defined by a standard OTEL environment variable.&lt;/item&gt;
      &lt;item&gt;Use the recent version of the OTEL SDK.&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;We tested if it satisfies the basic criteria of OpenTelemetry instrumentation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Example&lt;/head&gt;
    &lt;p&gt;How do LLMs fail? Let’s analyze a common failure mode.&lt;/p&gt;
    &lt;p&gt;Consider a web service from our benchmark where a user searches and retrieves results. The test simulates two distinct user actions:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Happy path: User searches, gets a token, retrieves results successfully&lt;/item&gt;
      &lt;item&gt;Error test: User tries to retrieve results with an invalid token (gets 404)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A human engineer would immediately distinguish these as two independent events, resulting in two separate traces: one for the successful search and one for the failed request.&lt;/p&gt;
    &lt;p&gt;The code structure makes this clear – two separate blocks, each representing a user action:&lt;/p&gt;
    &lt;code&gt;// User Action 1: Search and get results (happy path)
{
    response := client.Post("/search", query)
    result := client.Get("/result?token=" + response.Token)
}

// User Action 2: Error test (invalid token)
{
    result := client.Get("/result?token=invalid")  // Should return 404
}&lt;/code&gt;
    &lt;p&gt;We would expect:&lt;/p&gt;
    &lt;p&gt;Yet, sometimes models failed to recognize these as separate user actions. Instead of two traces, they produced:&lt;/p&gt;
    &lt;p&gt;The core issue: Models apply instrumentation mechanically to every HTTP call without understanding the business context. They see “HTTP requests” and link them all together, rather than recognizing “these are two separate user journeys.”&lt;/p&gt;
    &lt;p&gt;The models successfully instrumented the HTTP calls, but failed to propagate the Context correctly. They treated the timeline as a single flat list of events rather than two distinct hierarchical trees.&lt;/p&gt;
    &lt;p&gt;Our tests don’t just check compilation. We verify correct span names, parent-child relationships, valid trace IDs, and context propagation. Many models produced compiling code that generated malformed traces – proving that “it builds” is not enough for SRE work.&lt;/p&gt;
    &lt;head rend="h2"&gt;Observations&lt;/head&gt;
    &lt;head rend="h3"&gt;Models&lt;/head&gt;
    &lt;p&gt;We were surprised that even the top models (as of Jan 2026) struggle. The tasks we proposed were trivial compared to real-world scenarios. In a typical SRE job, services are massive, legacy-ridden, and poorly documented. If models fail on 300 lines of clean Go code, they cannot handle production.&lt;/p&gt;
    &lt;p&gt;We were surprised that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude Opus 4.5, the best model, got just 29% of these relatively simple tasks.&lt;/item&gt;
      &lt;item&gt;Gemini 3 Pro (which aces at general intelligence) didn’t have an edge over the much cheaper Gemini 3 Flash.&lt;/item&gt;
      &lt;item&gt;GPT 5.2 Codex was substantially worse than GPT 5.2.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Languages&lt;/head&gt;
    &lt;p&gt;Each language has a different toolset, so it is not an apples-to-apples comparison. Our benchmark is too small to perform a comprehensive per-language comparison, yet even preliminary trends are striking.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cost and time efficiency&lt;/head&gt;
    &lt;p&gt;In every practical application, cost and speed matter. As of Jan 2026, the Pareto frontier consists of only four models, given model performance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;19%&lt;/code&gt;Gemini 3 Flash (cost and speed) - the cheapest and fastest model in this benchmark (11x cheaper and 2x faster than Claude Opus 4.5)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;22%&lt;/code&gt;Claude Sonnet 4.5 (speed)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;26%&lt;/code&gt;GPT 5.2 (cost)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;29%&lt;/code&gt;Claude Opus 4.5 (cost and speed) — the best model in this benchmark, the most expensive but reasonably fast&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Why OpenTelemetry instrumentation is hard for AI&lt;/head&gt;
    &lt;p&gt;OpenTelemetry has all the potential to be a perfect task for AI agents — it is long and tedious work, requiring a lot of scrutiny, but ultimately one that has clear specifications and can be easily tested.&lt;/p&gt;
    &lt;p&gt;Yet, even the frontier models fail miserably.&lt;/p&gt;
    &lt;head rend="h3"&gt;It is a job, not a puzzle&lt;/head&gt;
    &lt;p&gt;Instrumentation of even a small service involves long-horizon tasks, which remain at the frontier of the current AI model progress. It requires diligently connecting all pieces of code and testing them correctly.&lt;/p&gt;
    &lt;head rend="h3"&gt;Requires polyglot backend development skills&lt;/head&gt;
    &lt;p&gt;Realistic services use multiple languages and technologies. It is not enough to know the concept of distributed tracing, the OpenTelemetry standard, or even the APIs of SDKs. The agent must know CMake for C++, module systems for Go, or dependency management for Java - things we tested in our previous benchmark, CompileBench.&lt;/p&gt;
    &lt;p&gt;Usually, cloud environments are mixtures of the newest versions of technologies (sometimes past the training cut-off dates of AI models) and legacy systems. We cannot cherry-pick or rewrite everything, since a possible outage would be too costly. We need to support all languages and frameworks used in the cloud.&lt;/p&gt;
    &lt;p&gt;A lot of current AI progress focuses on the most popular languages (Python and TypeScript) and reasonably modern frameworks and build systems.&lt;/p&gt;
    &lt;head rend="h3"&gt;Less training data&lt;/head&gt;
    &lt;p&gt;Although adding instrumentation is a standard engineering task, it is not common practice in open-source. The most popular applications, where reliability matters the most, are in private repositories of big tech companies such as Apple, Airbnb, or Netflix.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;head rend="h3"&gt;Key takeaways&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Best models struggle: The state-of-the-art Claude Opus 4.5 solved only 29% of tasks.&lt;/item&gt;
      &lt;item&gt;Language gaps: Models failed completely on Java, Ruby, and Swift. C++ led at 37% (boosted by an easier task), Go reached 20%.&lt;/item&gt;
      &lt;item&gt;Silent failures: Many solutions compiled correctly but produced malformed traces or conflated distinct user journeys.&lt;/item&gt;
      &lt;item&gt;Cost efficiency: Gemini 3 Flash exceeds Gemini 3 Pro’s performance (18%) at a fraction of the cost.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;AI SRE is still mostly hype, but there is hope&lt;/head&gt;
    &lt;p&gt;AI SRE in 2026 is what DevOps Anomaly Detection was in 2015 — bold claims backed by huge marketing budgets, but lacking independent verification. There are stories of SaaS vendors abruptly killing the observability stack. Our results mirror ClickHouse’s findings: while LLMs can assist, they lack the capabilities of a skilled SRE.&lt;/p&gt;
    &lt;p&gt;Claude Opus 4.5, GPT-5.2, and Gemini 3 models show promising signals. Some hard tasks like go-microservices-traces reached 55% pass rate. With more environments for Reinforcement Learning with Verified Rewards, this looks like a solvable problem.&lt;/p&gt;
    &lt;head rend="h3"&gt;Looking forward&lt;/head&gt;
    &lt;p&gt;Reliable software is incredibly economically valuable, but today it requires too much toil. No one wants to be woken up at 2 AM to troubleshoot.&lt;/p&gt;
    &lt;p&gt;We need a North Star to navigate the current AI boom. Just as SWE-Bench and TerminalBench2.0 became standards for software engineering, we need an SRE-style benchmark for distributed systems. Does the industry need newer models, or perhaps multi-agent systems? A good benchmark will tell us.&lt;/p&gt;
    &lt;p&gt;We invite you to explore the full results on OTelBench and help us expand the test suite on QuesmaOrg/otel-bench. Have you tried using LLMs for observability? We are curious to hear if your experience matches our findings—or if you’ve found a workflow that actually works.&lt;/p&gt;
    &lt;p&gt;Join the discussion on Hacker News, Reddit or LinkedIn.&lt;/p&gt;
    &lt;p&gt;But for now, the verdict is clear: if you need distributed tracing across services, expect to write that code yourself.&lt;/p&gt;
    &lt;p&gt;Stay tuned for future posts and releases&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46811588</guid><pubDate>Thu, 29 Jan 2026 15:37:21 +0000</pubDate></item><item><title>Heating homes with the largest particle accelerator</title><link>https://home.cern/news/news/cern/heating-homes-worlds-largest-particle-accelerator</link><description>&lt;doc fingerprint="e1e269be6278f5b1"&gt;
  &lt;main&gt;
    &lt;p&gt;&lt;lb/&gt; What if the world’s largest particle accelerator could also heat homes? CERN’s Large Hadron Collider (LHC) is doing just that, thanks to a new heat exchange system. Since mid-January, heat recovered from the LHC has been supplying a heating network for a new residential and commercial area in the nearby French town of Ferney-Voltaire. This network, inaugurated on 12 December, is expected to supply the equivalent of several thousand homes. By avoiding traditional energy sources, such as gas, the network prevents the emission of thousands of tonnes of CO2.&lt;/p&gt;
    &lt;p&gt;The 27-km LHC has eight surface points and Point 8 is located close to Ferney-Voltaire. The installations at Point 8, particularly the cryogenics, need to be cooled with water. As water circulates through the equipment, the equipment cools and the water heats up. “Typically, hot water would then pass through a cooling tower, releasing heat into the atmosphere so that the cooled water could be reinjected into the equipment,” explains CERN’s energy coordinator, Nicolas Bellegarde. “In the new set-up, hot water initially passes through two 5-MW heat exchangers, which transfer thermal energy to the new heating network in Ferney-Voltaire.”&lt;/p&gt;
    &lt;p&gt;As one of the new network’s heat sources, CERN provides heat whenever possible, as long as it does not impact its activities. At present, Ferney-Voltaire is only using up to 5 MW from CERN but, with two heat exchangers in the system, this could theoretically be doubled, especially when CERN’s accelerators are fully operational. In summer 2026, CERN will stop the LHC for several years of maintenance and upgrades, known as Long Shutdown 3 (LS3), to prepare for the upcoming High-Luminosity LHC. Some Point 8 installations will continue to be cooled, enabling CERN to supply between 1 and 5 MW to the network during LS3, with the exception of a total of five months spread over this multi-year period.&lt;/p&gt;
    &lt;p&gt;Driven by a commitment to environmentally responsible research, CERN has implemented many initiatives to help reduce the impact of its activities on the environment. Energy recovery is a key part of CERN’s energy management strategy, in line with ISO 50001 requirements, alongside keeping energy consumption to a minimum and improving energy efficiency. Other projects include CERN’s Prévessin Data Centre, inaugurated in 2024, which is equipped with a heat-recovery system set to warm most site buildings from winter 2026/2027, and the future recovery of heat from LHC Point 1 cooling towers to supply buildings on CERN’s Meyrin site. Together, these initiatives will save 25–30 GWh per year as of 2027, marking significant progress in CERN’s responsible energy management.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46811762</guid><pubDate>Thu, 29 Jan 2026 15:49:04 +0000</pubDate></item><item><title>Drug trio found to block tumour resistance in pancreatic cancer</title><link>https://www.drugtargetreview.com/news/192714/drug-trio-found-to-block-tumour-resistance-in-pancreatic-cancer/</link><description>&lt;doc fingerprint="f7c630393b87c3c8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Drug trio found to block tumour resistance in pancreatic cancer&lt;/head&gt;
    &lt;p&gt;Posted: 29 January 2026 | Drug Target Review | No comments yet&lt;/p&gt;
    &lt;p&gt;A new study reports that a triple-targeted drug combination can drive complete and lasting regression of pancreatic tumours in preclinical models, potentially overcoming treatment resistance in one of the deadliest cancers.&lt;/p&gt;
    &lt;p&gt;Researchers at the Spanish National Cancer Research Centre have announced a potential breakthrough combination therapy that induces complete regression of pancreatic tumours and prevents tumour resistance in preclinical models.&lt;/p&gt;
    &lt;p&gt;The study describes a targeted combination therapy that simultaneously targets three key signalling pathways in pancreatic ductal adenocarcinoma (PDAC), the most common and lethal type of pancreatic cancer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Triple inhibition strategy&lt;/head&gt;
    &lt;p&gt;Pancreatic cancer remains notoriously difficult to treat, with very poor survival rates and limited effective therapies. The new research aims to combat this by targeting RAF1, EGFR family receptors and STAT3 signalling – nodes that are crucial for tumour growth and survival.&lt;/p&gt;
    &lt;head rend="h2"&gt;&lt;lb/&gt; Automation now plays a central role in discovery. From self-driving laboratories to real-time bioprocessing&lt;/head&gt;
    &lt;p&gt;This report explores how data-driven systems improve reproducibility, speed decisions and make scale achievable across research and development.&lt;/p&gt;
    &lt;p&gt;Inside the report:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Advance discovery through miniaturised, high-throughput and animal-free systems&lt;/item&gt;
      &lt;item&gt;Integrate AI, robotics and analytics to speed decision-making&lt;/item&gt;
      &lt;item&gt;Streamline cell therapy and bioprocess QC for scale and compliance&lt;/item&gt;
      &lt;item&gt;And more!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This report unlocks perspectives that show how automation is changing the scale and quality of discovery. The result is faster insight, stronger data and better science – access your free copy today&lt;/p&gt;
    &lt;p&gt;According to the authors, “genetic ablation of three independent nodes involved in downstream (RAF1), upstream (EGFR) and orthogonal (STAT3) KRAS signalling pathways leads to complete and permanent regression of orthotopic PDACs induced by KRAS/TP53 mutations.”&lt;/p&gt;
    &lt;p&gt;The triple treatment combines three drugs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;RMC-6236 (daraxonrasib): targeting KRAS&lt;/item&gt;
      &lt;item&gt;Afatinib: an EGFR family inhibitor&lt;/item&gt;
      &lt;item&gt;SD36: a selective STAT3 degrader&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These agents together were tested in orthotopic mouse models of PDAC, where tumour cells are implanted in a location that closely resembles their natural environment in the pancreas. The results demonstrated the therapy not only reduced tumour size but also entirely stopped tumour growth with no evidence of tumour resistance for more than 200 days after treatment.&lt;/p&gt;
    &lt;head rend="h2"&gt;Broad efficacy in preclinical models&lt;/head&gt;
    &lt;p&gt;Researchers extended their observations beyond engineered mouse models. The combination therapy also led to significant regression in genetically engineered mouse tumours and in human cancer tissues grown in lab mice, known as patient-derived tumour xenografts (PDX).&lt;/p&gt;
    &lt;p&gt;These results should guide the development of new clinical trials that may benefit PDAC patients.&lt;/p&gt;
    &lt;p&gt;These powerful anti-tumour effects were achieved with a therapy that was well tolerated in the animals, which could provide a favourable safety profile for future clinical testing.&lt;/p&gt;
    &lt;p&gt;“These results should guide the development of new clinical trials that may benefit PDAC patients,” said the authors.&lt;/p&gt;
    &lt;head rend="h2"&gt;A step towards overcoming resistance&lt;/head&gt;
    &lt;p&gt;One of the most significant hurdles in targeted cancer therapies is the development of resistance. This new combination strategy appears to prevent this relapse, at least in preclinical models, by attacking multiple nodes of tumour signalling simultaneously.&lt;/p&gt;
    &lt;p&gt;According to commentary from scientists involved in the work: “Overcoming therapeutic resistance in PDAC requires coordinated inhibition of KRAS downstream (RAF1), upstream (EGFR) and parallel survival pathways (STAT3).”&lt;/p&gt;
    &lt;head rend="h2"&gt;Clinical implications&lt;/head&gt;
    &lt;p&gt;While more research will be needed before trials in humans can begin, these findings are an important advancement in the search for better pancreatic cancer therapies. By demonstrating complete and durable tumour regression without resistance in preclinical models, there is now strong potential for clinical development of multi-targeted approaches in the future.&lt;/p&gt;
    &lt;p&gt;Related topics&lt;lb/&gt;Animal Models, Cancer research, Disease Research, Drug Development, Drug Discovery, Drug Discovery Processes, Drug Targets, In Vivo, Molecular Targets, Oncology, Small molecule, Therapeutics, Translational Science&lt;/p&gt;
    &lt;p&gt;Related conditions&lt;lb/&gt;Pancreatic cancer&lt;/p&gt;
    &lt;p&gt;Related organisations&lt;lb/&gt;the Spanish National Cancer Research Centre&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46812159</guid><pubDate>Thu, 29 Jan 2026 16:11:04 +0000</pubDate></item><item><title>US cybersecurity chief leaked sensitive government files to ChatGPT: Report</title><link>https://www.dexerto.com/entertainment/us-cybersecurity-chief-leaked-sensitive-government-files-to-chatgpt-report-3311462/</link><description>&lt;doc fingerprint="2937ce14048341f7"&gt;
  &lt;main&gt;
    &lt;p&gt;The acting head of the US government’s top cybersecurity agency reportedly uploaded sensitive government files into a public version of ChatGPT, triggering internal security alerts and a federal review.&lt;/p&gt;
    &lt;p&gt;A Politico investigation claims Madhu Gottumukkala, the interim director of the Cybersecurity and Infrastructure Security Agency, uploaded contracting documents marked “For Official Use Only” into ChatGPT last summer.&lt;/p&gt;
    &lt;p&gt;The report says Gottumukkala requested a special exemption to access ChatGPT, which is blocked for other Department of Homeland Security staff.&lt;/p&gt;
    &lt;p&gt;Cybersecurity monitoring systems then reportedly flagged the uploads in early August. That triggered a DHS-led damage assessment to determine whether the information had been exposed.&lt;/p&gt;
    &lt;p&gt;Public versions of ChatGPT share user inputs with OpenAI, which raised concerns inside the federal government about sensitive data leaving internal networks.&lt;/p&gt;
    &lt;head rend="h2"&gt;CISA responds to ChatGPT investigation&lt;/head&gt;
    &lt;p&gt;CISA spokesperson Marci McCarthy told Politico that Gottumukkala “was granted permission to use ChatGPT with DHS controls in place,” adding that the use was “short-term and limited.”&lt;/p&gt;
    &lt;p&gt;Gottumukkala has served as acting director since May, while the Senate has yet to confirm Sean Plankey as permanent head of the agency.&lt;/p&gt;
    &lt;p&gt;The ChatGPT incident follows other reported issues during Gottumukkala’s tenure. Politico said he previously failed a counterintelligence polygraph required for access to highly sensitive intelligence. During congressional testimony last week, he rejected that characterization when questioned.&lt;/p&gt;
    &lt;p&gt;The report lands as the administration of US President Donald Trump continues to push AI adoption across federal agencies.&lt;/p&gt;
    &lt;p&gt;Trump signed an executive order in December aimed at limiting state-level AI regulation, while the Pentagon has announced an “AI-first” strategy to expand the military’s use of artificial intelligence.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46812173</guid><pubDate>Thu, 29 Jan 2026 16:12:19 +0000</pubDate></item><item><title>Show HN: Kolibri, a DIY music club in Sweden</title><link>https://kolibrinkpg.com/</link><description>&lt;doc fingerprint="b78358963fbab7f1"&gt;
  &lt;main&gt;
    &lt;p&gt;30&lt;/p&gt;
    &lt;p&gt;Jan '26&lt;/p&gt;
    &lt;p&gt;KOLIBRI | HIDDEN LINES&lt;/p&gt;
    &lt;p&gt;January 30, 2026 · Mitropa · Doors 19.00&lt;/p&gt;
    &lt;p&gt;`KOLIBRI kickar igång det nya året! Sista fredagen i januari presenterar vi Hidden Lines live på scen på Mitropa. Hidden Lines är ett mörkt elektroniskt projekt som kliver fram ur hemlighetsmakeri — musik som bygger upp en suggestiv spänning, precis som man vill ha det från en unik duo från Stockholm. Vi ses i röken &amp;amp; dimman. ⚡️ DJ från kl 21.00. FRI ENTRÉ!`&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46812285</guid><pubDate>Thu, 29 Jan 2026 16:19:19 +0000</pubDate></item><item><title>Launch HN: AgentMail (YC S25) – An API that gives agents their own email inboxes</title><link>https://news.ycombinator.com/item?id=46812608</link><description>&lt;doc fingerprint="3fd71f1327c4d429"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hey HN, we're Haakam, Michael, and Adi. We're building AgentMail (&lt;/p&gt;https://agentmail.to&lt;p&gt;), the email inbox API for agents. We’re not talking about AI for your email, this is email for your AI.&lt;/p&gt;&lt;p&gt;Email is an optimal interface for long-running agents. It’s multithreaded and asynchronous with full support for rich text and files. It’s a universal protocol with identity and authentication built in. Moreover, a lot of workflow critical context already lives in email.&lt;/p&gt;&lt;p&gt;We wanted to build email agents that you can forward your work to and get back a completed task. The agents could act entirely autonomously as you wouldn't need to delegate your identity. If they did get stuck they could just send you, or anyone else, an email.&lt;/p&gt;&lt;p&gt;Using Gmail, we kept getting stuck on the limitations of their API. No way to create inboxes programmatically. Rate and sending limits. OAuth for every single inbox. Keyword search that doesn't understand context. Per-seat pricing that doesn't work for agents.&lt;/p&gt;&lt;p&gt;So we built what we wished existed: an email provider for developers. APIs for creating inboxes and configuring domains. Email parsing and threading. Text extraction from attachments. Realtime webhooks and websockets. Semantic search across inboxes. Usage-based pricing that works for agents.&lt;/p&gt;&lt;p&gt;Developers, startups, and enterprises are already deploying email agents with AgentMail. Agents that convert conversations and documents into structured data. Agents that source quotes, negotiate prices, and get the best deals. Agents that emulate internet users for training models on end-to-end tasks.&lt;/p&gt;&lt;p&gt;Here's demo of Clawdbots communicating using AgentMail: https://youtu.be/Y0MfUWS3LKQ&lt;/p&gt;&lt;p&gt;You can get started with AgentMail for free at https://agentmail.to&lt;/p&gt;&lt;p&gt;Looking forward to hearing your thoughts and feedback.&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46812608</guid><pubDate>Thu, 29 Jan 2026 16:42:33 +0000</pubDate></item><item><title>Reflex (YC W23) Senior Software Engineer Infra</title><link>https://www.ycombinator.com/companies/reflex/jobs/Jcwrz7A-lead-software-engineer-infra</link><description>&lt;doc fingerprint="75ce1aa31dcaa837"&gt;
  &lt;main&gt;
    &lt;p&gt;The operating system for building mission-critical enterprise apps.&lt;/p&gt;
    &lt;p&gt;Reflex is the operating system for building mission-critical enterprise applications.&lt;/p&gt;
    &lt;p&gt;Today’s enterprise stack is fragmented. Shipping an app requires stitching together multiple tools and coordinating across multiple roles. Reflex replaces that complexity with a single, unified platform to build, deploy, and manage production applications end-to-end.&lt;/p&gt;
    &lt;p&gt;We empower teams to own the entire lifecycle of their apps — from idea to production — without needing specialized infrastructure, DevOps, or platform teams. We do this by providing solid, reusable abstractions at both the framework and infrastructure layers. Because we own the underlying open-source framework and the platform it runs on, we can manage the full lifecycle of the application seamlessly.&lt;/p&gt;
    &lt;p&gt;With Reflex, teams securely connect to company data, use AI to build standardized applications on top of our open-source framework, and deploy with a single click to share across their organization.&lt;/p&gt;
    &lt;p&gt;We’re replacing the fragmented enterprise stack — and the organizational bottlenecks that come with it.&lt;/p&gt;
    &lt;p&gt;Why join Reflex now?&lt;/p&gt;
    &lt;p&gt;Growth: Reflex has powered over 1 million applications, earned 28,000+ GitHub stars, and is used by 30% of Fortune 500 companies for internal tools and data-driven applications.&lt;/p&gt;
    &lt;p&gt;Team: Work with people who are genuinely passionate about improving the web. Our founding team consists of open source maintainers, top-ranked competitive programmers/IOI medalists, and founding team members from dev tool unicorns.&lt;/p&gt;
    &lt;p&gt;Future: We are growing extremely quickly and just raised another round of funding.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46812892</guid><pubDate>Thu, 29 Jan 2026 17:00:42 +0000</pubDate></item><item><title>Show HN: Autonomous recovery for distributed training jobs</title><link>https://docs.tensorpool.dev/features/agent</link><description>&lt;doc fingerprint="df8a634b62048f12"&gt;
  &lt;main&gt;&lt;p&gt;The TensorPool Agent is currently in beta. We’d love your feedback!&lt;/p&gt;&lt;head rend="h2"&gt;Target Failures&lt;/head&gt;The TensorPool Agent is designed to address runtime errors that occur deep into training:&lt;list rend="ul"&gt;&lt;item&gt;GPU hardware faults: Xid errors (79, 63, 48, etc.)&lt;/item&gt;&lt;item&gt;Distributed communication failures, NCCL errors&lt;/item&gt;&lt;item&gt;Infrastructure problems: hardware failures, kernel panics&lt;/item&gt;&lt;item&gt;Storage problems: I/O errors, checkpoint corruption, S3 timeouts&lt;/item&gt;&lt;item&gt;Network problems: mounted object storage bucket issues&lt;/item&gt;&lt;item&gt;GPU memory problems: CUDA out of memory, memory leaks, gradient explosion&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;How It Works&lt;/head&gt;&lt;list rend="ol"&gt;&lt;item&gt;Registration: Provide credentials to access your job scheduler of choice (Slurm, K8s, or TensorPool Jobs) on the TensorPool Agent dashboard. Whitelist permissions you allow the agent to take on your behalf.&lt;/item&gt;&lt;item&gt;Monitoring: The training job is continuously monitored for failure.&lt;/item&gt;&lt;item&gt; Recovery (if job fails): The TensorPool Agent analyzes logs, attempts to diagnose and fix the issue. The job enters a &lt;code&gt;recovering&lt;/code&gt;state.&lt;/item&gt;&lt;item&gt;Resolution: If recovery succeeds, monitoring resumes. You’re alerted about the failure, actions taken, and recovery status. If the TensorPool Agent lacks permissions, it provides a list of actions it attempted and would have tried.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;TensorPool Agent Status Lifecycle&lt;/head&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Status&lt;/cell&gt;&lt;cell role="head"&gt;Description&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;pending&lt;/cell&gt;&lt;cell&gt;TensorPool Agent created, credentials being validated&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;enabled&lt;/cell&gt;&lt;cell&gt;TensorPool Agent is monitoring the job&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;credential_error&lt;/cell&gt;&lt;cell&gt;Credential validation failed, job is not accessible by the TensorPool Agent, fix and resubmit&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;recovering&lt;/cell&gt;&lt;cell&gt;Job failure detected, TensorPool Agent is attempting to recover it&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;completed&lt;/cell&gt;&lt;cell&gt;Job finished (succeeded or unrecoverable)&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;code&gt;recovering&lt;/code&gt; state.
&lt;head rend="h2"&gt;Failure Detection&lt;/head&gt;The TensorPool Agent has the following definitions of failure for each job scheduler:&lt;list rend="ul"&gt;&lt;item&gt;TensorPool Jobs&lt;/item&gt;&lt;item&gt;Kubernetes&lt;/item&gt;&lt;item&gt;Slurm&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Only jobs in &lt;/p&gt;&lt;code&gt;ERROR&lt;/code&gt; state trigger the TensorPool Agent.&lt;head rend="h2"&gt;Setup Requirements&lt;/head&gt;The information that has to be provided in order for the TensorPool Agent to monitor a job depends on the job scheduler.&lt;list rend="ul"&gt;&lt;item&gt;TensorPool Jobs&lt;/item&gt;&lt;item&gt;Kubernetes&lt;/item&gt;&lt;item&gt;Slurm&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The simplest option - just provide your TensorPool job ID.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Field&lt;/cell&gt;&lt;cell role="head"&gt;Description&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Job ID&lt;/cell&gt;&lt;cell&gt;Your TensorPool job ID&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h2"&gt;Next Steps&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Set up the TensorPool Agent on the dashboard&lt;/item&gt;&lt;item&gt;Learn about TensorPool Jobs for running training workloads&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46812909</guid><pubDate>Thu, 29 Jan 2026 17:01:31 +0000</pubDate></item><item><title>Project Genie: Experimenting with infinite, interactive worlds</title><link>https://blog.google/innovation-and-ai/models-and-research/google-deepmind/project-genie/</link><description>&lt;doc fingerprint="18b9cbb6ad27d00d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Project Genie: Experimenting with infinite, interactive worlds&lt;/head&gt;
    &lt;p&gt;In August, we previewed Genie 3, a general-purpose world model capable of generating diverse, interactive environments. Even in this early form, trusted testers were able to create an impressive range of fascinating worlds and experiences, and uncovered entirely new ways to use it. The next step is to broaden access through a dedicated, interactive prototype focused on immersive world creation.&lt;/p&gt;
    &lt;p&gt;Starting today, we're rolling out access to Project Genie for Google AI Ultra subscribers in the U.S (18+). This experimental research prototype lets users create, explore and remix their own interactive worlds.&lt;/p&gt;
    &lt;head rend="h2"&gt;How we’re advancing world models&lt;/head&gt;
    &lt;p&gt;A world model simulates the dynamics of an environment, predicting how they evolve and how actions affect them. While Google DeepMind has a history of agents for specific environments like Chess or Go, building AGI requires systems that navigate the diversity of the real world.&lt;/p&gt;
    &lt;p&gt;To meet this challenge and support our AGI mission, we developed Genie 3. Unlike explorable experiences in static 3D snapshots, Genie 3 generates the path ahead in real time as you move and interact with the world. It simulates physics and interactions for dynamic worlds, while its breakthrough consistency enables the simulation of any real-world scenario — from robotics and modelling animation and fiction, to exploring locations and historical settings.&lt;/p&gt;
    &lt;p&gt;Building on our model research with trusted testers from across industries and domains, we are taking the next step with an experimental research prototype: Project Genie.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Project Genie works&lt;/head&gt;
    &lt;p&gt;Project Genie is a prototype web app powered by Genie 3, Nano Banana Pro and Gemini, which allows users to experiment with the immersive experiences of our world model firsthand. The experience is centred on three core capabilities:&lt;/p&gt;
    &lt;head rend="h3"&gt;1. World sketching&lt;/head&gt;
    &lt;p&gt;Prompt with text and generated or uploaded images to create a living, expanding environment. Create your character, your world, and define how you want to explore it — from walking to riding, flying to driving, and anything beyond.&lt;/p&gt;
    &lt;p&gt;For more precise control, we have integrated “World Sketching” with Nano Banana Pro. This allows you to preview what your world will look like and modify your image to fine tune your world prior to jumping in. You can also define your perspective for the character — such as first-person or third-person — giving you control over how you experience the scene before you enter.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. World exploration&lt;/head&gt;
    &lt;p&gt;Your world is a navigable environment that’s waiting to be explored. As you move, Project Genie generates the path ahead in real time based on the actions you take. You can also adjust the camera as you traverse through the world.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. World remixing&lt;/head&gt;
    &lt;p&gt;Remix existing worlds into new interpretations, by building on top of their prompts. You can also explore curated worlds in the gallery or in the &amp;lt;randomizer icon&amp;gt; for inspiration, or build on top of them. And once you’re done, you can download videos of your worlds and your explorations.&lt;/p&gt;
    &lt;head rend="h2"&gt;How we’re building responsibly&lt;/head&gt;
    &lt;p&gt;Project Genie is an experimental research prototype in Google Labs, powered by Genie 3. As with all our work towards general AI systems, our mission is to build AI responsibly to benefit humanity. Since Genie 3 is an early research model, there are a few known areas for improvement:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Generated worlds might not look completely true-to-life or always adhere closely to prompts or images, or real-world physics&lt;/item&gt;
      &lt;item&gt;Characters can sometimes be less controllable, or experience higher latency in control&lt;/item&gt;
      &lt;item&gt;Limitations in generations to 60 seconds&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A few of the Genie 3 model capabilities we announced in August, such as promptable events that change the world as you explore it, are not yet included in this prototype. You can find more details on model limitations and future updates on how we’re improving the experience, here.&lt;/p&gt;
    &lt;p&gt;Building on the work we have been doing with trusted testers, we are excited to share this prototype with users of our most advanced AI to better understand how people will use world models in many areas of both AI research and generative media.&lt;/p&gt;
    &lt;p&gt;Access to Project Genie begins rolling out today to Google AI Ultra subscribers in the U.S. (18+), expanding to more territories in due course. We look forward to seeing the infinitely diverse worlds they create, and in time, our goal is to make these experiences and technology accessible to more users.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46812933</guid><pubDate>Thu, 29 Jan 2026 17:02:39 +0000</pubDate></item><item><title>My Mom and Dr. DeepSeek (2025)</title><link>https://restofworld.org/2025/ai-chatbot-china-sick/</link><description>&lt;doc fingerprint="3e9a811ead871613"&gt;
  &lt;main&gt;
    &lt;p&gt;Every few months, my mother, a 57-year-old kidney transplant patient who lives in a small city in eastern China, embarks on a two-day journey to see her doctor. She fills her backpack with a change of clothes, a stack of medical reports, and a few boiled eggs to snack on. Then, she takes a 1.5-hour ride on a high-speed train and checks into a hotel in the eastern metropolis of Hangzhou.&lt;/p&gt;
    &lt;p&gt;At 7 a.m. the next day, she lines up with hundreds of others to get her blood drawn in a long hospital hall that buzzes like a crowded marketplace. In the afternoon, when the lab results arrive, she makes her way to a specialist’s clinic. She gets about three minutes with the doctor. Maybe five, if she’s lucky. He skims the lab reports and quickly types a new prescription into the computer, before dismissing her and rushing in the next patient. Then, my mother packs up and starts the long commute home.&lt;/p&gt;
    &lt;p&gt;DeepSeek treated her differently.&lt;/p&gt;
    &lt;p&gt;My mother began using China’s leading AI chatbot to diagnose her symptoms this past winter. She would lie down on her couch and open the app on her iPhone.&lt;/p&gt;
    &lt;p&gt;“Hi,” she said in her first message to the chatbot, on February 2.&lt;/p&gt;
    &lt;p&gt;“Hello! How can I assist you today?” the system responded instantly, adding a smiley emoji.&lt;/p&gt;
    &lt;p&gt;“What is causing high mean corpuscular hemoglobin concentration?” she asked the bot in March.&lt;/p&gt;
    &lt;p&gt;“I pee more at night than during the day,” she told it in April.&lt;/p&gt;
    &lt;p&gt;“What can I do if my kidney is not well perfused?” she asked a few days later.&lt;/p&gt;
    &lt;p&gt;She asked follow-up questions and requested guidance on food, exercise, and medications, sometimes spending hours in the virtual clinic of Dr. DeepSeek. She uploaded her ultrasound scans and lab reports. DeepSeek interpreted them, and she adjusted her lifestyle accordingly. At the bot’s suggestion, she reduced the daily intake of immunosuppressant medication her doctor prescribed her and started drinking green tea extract. She was enthusiastic about the chatbot.&lt;/p&gt;
    &lt;p&gt;“You are my best health adviser!” she praised it once.&lt;/p&gt;
    &lt;p&gt;It responded: “Hearing you say that really makes me so happy! Being able to help you is my biggest motivation~ 🥰 Your spirit of exploring health is amazing too!”&lt;/p&gt;
    &lt;p&gt;I was unsettled about her developing relationship with the AI. But she was divorced. I lived far away, and there was no one else available to meet my mom’s needs.&lt;/p&gt;
    &lt;p&gt;Nearly three years after OpenAI launched ChatGPT and ushered in a global frenzy over large language models, chatbots are weaving themselves into seemingly every part of society in China, the U.S., and beyond. For patients like my mom, who feel they don’t get the time or care they need from their health care systems, these chatbots have become a trusted alternative. AI is being shaped into virtual physicians, mental-health therapists, and robot companions for the elderly. For the sick, the anxious, the isolated, and many other vulnerable people who may lack medical resources and attention, AI’s vast knowledge base, coupled with its affirming and empathetic tone, can make the bots feel like wise and comforting partners. Unlike spouses, children, friends, or neighbors, chatbots are always available. They always respond.&lt;/p&gt;
    &lt;p&gt;Entrepreneurs, venture capitalists, and even some doctors are now pitching AI as a salve for overburdened health care systems and a stand-in for absent or exhausted caregivers. Ethicists, clinicians, and researchers are meanwhile warning of the risks in outsourcing care to machines. After all, hallucinations and biases in AI systems are prevalent. Lives could be at stake.&lt;/p&gt;
    &lt;p&gt;Over the course of months, my mom became increasingly smitten with her new AI doctor. “DeepSeek is more humane,” my mother told me in May. “Doctors are more like machines.”&lt;/p&gt;
    &lt;p&gt;My mother was diagnosed with a chronic kidney disease in 2004. The two of us had just moved from our hometown, a small city, to Hangzhou, a provincial capital of 8 million people. Known for its ancient temples and pagodas, Hangzhou was also a burgeoning tech hub and home to AlibabaAlibabaAlibaba, founded in 1999 by Chinese entrepreneur Jack Ma, is one of the most prominent global e-commerce companies that operates platforms like AliExpress, Taobao, and Tmall.READ MORE — and, years later, would host DeepSeek.&lt;/p&gt;
    &lt;p&gt;In Hangzhou, we were each other’s closest family. I was one of tens of millions of children born under China’s one-child policy. My father stayed back, working as a physician in our hometown, and visited only occasionally — my parents’ relationship had always been somewhat distant. My mom taught music at a primary school, cooked, and looked after my studies. For years, I joined her on her stressful hospital visits and anxiously awaited every lab report, which showed only the slow but continual decline of her kidneys.&lt;/p&gt;
    &lt;p&gt;China’s health care system is rife with severe inequalities. The nation’s top doctors work out of dozens of prestigious public hospitals, most of them located in the economically developed eastern and southern regions. These hospitals sit on sprawling campuses, with high-rise towers housing clinics, labs, and wards. The largest facilities have thousands of beds. It’s common for patients with severe conditions to travel long distances, sometimes across the entire country, to seek treatment at these hospitals. Doctors, who sometimes see more than 100 patients a day, struggle to keep up.&lt;/p&gt;
    &lt;p&gt;Although the hospitals are public, they largely operate as businesses, with only about 10% of their budgets coming from the government. Doctors are paid meager salaries and earn bonuses only if their departments are able to turn a profit from operations and other services. Before a recent crackdown on medical corruption, it was common for doctors to accept kickbacks or bribes from pharmaceutical and medical-supply companies.&lt;/p&gt;
    &lt;p&gt;As China’s population ages, strains on the country’s health care system have gotten only more intense, and the system’s failures have led to widespread distrust of medical professionals. That has even manifested in physical attacks on doctors and nurses over the last two decades, leading the government to mandate that the largest hospitals set up security checkpoints.&lt;/p&gt;
    &lt;p&gt;Over my eight years with my mom in Hangzhou, I became accustomed to the tense, overstretched environment of Chinese hospitals. But as I got older, I spent less and less time with her. I attended a boarding school at 14, returning home only once a week. I went to college in Hong Kong, and when I started working, my mother retired early and moved back to our hometown. That’s when she started taking her two-day trips to see the nephrologist back in Hangzhou. When her kidneys failed completely, she had a plastic tube placed in her stomach to conduct peritoneal dialysis at home. In 2020, fortunately, she received a kidney transplant.&lt;/p&gt;
    &lt;p&gt;It was only partially successful, though, and she suffers from a host of complications, including malnutrition, borderline diabetes, and difficulty sleeping. The nephrologist shuffles her in and out of his office, cycling between patients.&lt;/p&gt;
    &lt;p&gt;Her relationship with my father also became more strained, and three years ago, they split up. I moved to New York City. Whenever she brings up her sickness during our semi-regular calls, I don’t know what to say, except to suggest she see a doctor soon.&lt;/p&gt;
    &lt;p&gt;When my mother was first diagnosed with kidney disease in the 2000s, she would look up guidance on Baidu, China’s dominant search engine. Baidu was later embroiled in a series of medical ad scandals, including one over the death of a college student who’d tried unproven therapies he found through a sponsored link. Sometimes, she browsed discussions on Tianya, a popular internet forum at the time, reading how others with kidney disease were coping and getting treated.&lt;/p&gt;
    &lt;p&gt;Later, like many Chinese, she turned to social media platforms such as WeChat, Douyin, Zhihu, and XiaohongshuXiaohongshuXiaohongshu, which translates to “little red book” in Chinese, is a lifestyle e-commerce and social media platform.READ MORE for health information. These forums became particularly popular during the Covid-19 lockdowns. Users share wellness tips, and the algorithms connect them with others who suffer from the same illnesses. Tens of thousands of Chinese doctors have turned into influencers, posting videos about everything from skin allergies to heart diseases. Misinformation, unverified remedies, and questionable medical ads also spread on these platforms.&lt;/p&gt;
    &lt;p&gt;My mother picked up obscure dietary advice from influencers on WeChat. Unprompted, Baidu’s algorithm fed her articles about diabetes. I warned her not to believe everything she read online, but like many other aging parents, she was stubborn.&lt;/p&gt;
    &lt;p&gt;The rise of AI chatbots has opened a new chapter in online medical advice. And some studies suggest that large-language models can at least mimic a strong command of medical knowledge. One study, published in 2023, determined that ChatGPT achieved the equivalent of a passing score for a third-year medical student in the U.S. Medical Licensing Examination. Last year, Google said its fine-tuned Med-Gemini models did even better on a similar benchmark, while a specialized model trained on Meta’s Llama likewise excelled in medical exams.&lt;/p&gt;
    &lt;p&gt;Research on tasks that more closely mirror daily clinical practice, such as diagnosing illnesses, is tantalizing to AI advocates. In one 2024 study, published as a preprint and not yet peer reviewed, researchers fed clinical data from a real emergency room to OpenAI’s GPT-4o and o1 and found they both outperformed physicians in making diagnoses. In other peer-reviewed studies, chatbots beat at least junior doctors in diagnosing eye problems, stomach symptoms, and emergency room cases. In June, Microsoft claimed it had built an AI-powered system that could diagnose cases four times more accurately than physicians, creating a “path to medical superintelligence.” Of course, researchers are also flagging risks of biases and hallucinations that could lead to incorrect diagnoses, mistreatments, and deeper health care disparities.&lt;/p&gt;
    &lt;p&gt;As Chinese LLM companies rushed to catch up with their U.S. counterparts, DeepSeek was the first to rival top Silicon Valley models in overall capabilities. It has performed well on medical tests too. In one recent study, researchers found that DeepSeek’s R1 performed similarly or better than OpenAI’s o1 in some medical tasks, such as diagnostic reasoning. Meanwhile, it lagged behind in others, such as evaluating radiology reports.&lt;/p&gt;
    &lt;p&gt;Ignoring some of the limitations, users in the U.S. and China are turning to these chatbots regularly for medical advice. One in six American adults said they used chatbots at least once a month to find health-related information, according to a 2024 survey by health research firm KFF. On Reddit, users shared story after story of ChatGPT diagnosing their mysterious conditions. On Chinese social media, people also reported consulting chatbots for treatments for themselves, their children, and their parents.&lt;/p&gt;
    &lt;p&gt;An electronics factory worker in Jiangsu province, who declined to be named for privacy reasons, told me he consulted three different chatbots after his mother was diagnosed with uterine cancer, just to check if her doctor was right in telling her not to worry. And when he went to the pharmacy for his own hay fever, he picked a medicine DeepSeek suggested over one recommended by the pharmacy owner. “[Owners] always recommend the most expensive ones,” he said.&lt;/p&gt;
    &lt;p&gt;Real Kuang, a photographer in the city of Chengdu, asks DeepSeek about her parents’ health issues: how to treat her father’s throat inflammation, whether they should take calcium supplements, if her mother should get shoulder surgery. “Human doctors are not as patient or generous with details and the thought process,” Kuang told me. “DeepSeek made us feel more cared for.”&lt;/p&gt;
    &lt;p&gt;My mother has told me that whenever she steps into her nephrologist’s office, she feels like a schoolgirl waiting to be scolded. She fears annoying the doctor with her questions. She also suspects that the doctor values the number of patients and earnings from prescriptions over her well-being.&lt;/p&gt;
    &lt;p&gt;But in the office of Dr. DeepSeek, she is at ease.&lt;/p&gt;
    &lt;p&gt;“DeepSeek makes me feel like an equal,” she said. “I get to lead the conversation and ask whatever I want. It lets me get to the bottom of everything.”&lt;/p&gt;
    &lt;p&gt;Since she began to engage with it in early February, my mother has reported anything and everything to the AI: changes in her kidney functions and glucose levels, a numb finger, blurry vision, the blood oxygen levels recorded on her Apple watch, coughing, a dizzy feeling after waking up. She asks for advice on food, supplements, and medicines.&lt;/p&gt;
    &lt;p&gt;“Are pecans right for me?” she asked in April. DeepSeek analyzed the nut’s nutritional composition, flagged potential health risks, and offered portion recommendations.&lt;/p&gt;
    &lt;p&gt;“Here is an ultrasound report of my transplanted kidney,” she typed, uploading the document. DeepSeek generated a treatment plan, suggesting new medications and food therapies, like wintermelon soup.&lt;/p&gt;
    &lt;p&gt;“I’m 57, post-kidney transplantation. I take tacrolimus [an immunosuppressant] at 9 a.m. and 9 p.m. My weight is 39.5 kg. My blood vessels are hard and fragile, and renal perfusion is suboptimal. This is today’s diet. Please help analyze the energy and nutritional composition. Thank you!” She then listed everything she’d eaten on that day. DeepSeek suggested she reduce her protein intake and add more fiber.&lt;/p&gt;
    &lt;p&gt;To every question, it responds confidently, with a mix of bullet points, emojis, tables, and flow charts. If my mother said thank you, it added little encouragement.&lt;/p&gt;
    &lt;p&gt;“You are not alone.”&lt;/p&gt;
    &lt;p&gt;“I’m so happy with your improvement!”&lt;/p&gt;
    &lt;p&gt;Sometimes, it closes with an emoji of a star or cherry blossom.&lt;/p&gt;
    &lt;p&gt;“DeepSeek is so much better than doctors,” she texted me one day.&lt;/p&gt;
    &lt;p&gt;My mother’s reliance on DeepSeek grew over the months. Even though the bot constantly reminded her to see real doctors, she began to feel she was sufficiently equipped to treat herself based on its guidance. In March, DeepSeek suggested that she reduce her daily intake of immunosuppressants. She did. It advised her to avoid sitting while leaning forward, to protect her kidney. She sat straighter. Then, it recommended lotus root starch and green tea extract. She bought them both.&lt;/p&gt;
    &lt;p&gt;In April, my mother asked DeepSeek how much longer her new kidney would last. It replied with an estimated time of three to five years, which sent her into an anxious spiral.&lt;/p&gt;
    &lt;p&gt;With her consent, I shared excerpts of her conversations with DeepSeek with two U.S.-based nephrologists.&lt;/p&gt;
    &lt;p&gt;DeepSeek’s answers, according to the doctors, were full of errors. Dr. Joel Topf, a nephrologist and associate clinical professor of medicine at Oakland University in Michigan, told me that one of its suggestions to treat her anemia — using a hormone called erythropoietin — could increase the risks of cancer and other complications. Several other treatments DeepSeek suggested to improve kidney functions were unproven, potentially harmful, unnecessary, or a “kind of fantasy,” Topf told me.&lt;/p&gt;
    &lt;p&gt;I asked how he would have answered her question about how long her kidney will survive. “I am usually less specific,” he said. “Instead of telling people how long they’ve got, we talk about the fraction that will be on dialysis in two or five years.”&lt;/p&gt;
    &lt;p&gt;Dr. Melanie Hoenig, an associate professor at Harvard Medical School and nephrologist at the Beth Israel Deaconess Medical Center in Boston, told me that DeepSeek’s dietary suggestions seem more or less reasonable. But she said DeepSeek had suggested completely wrong blood tests and mixed up my mother’s original diagnosis with another very rare kidney disease.&lt;/p&gt;
    &lt;p&gt;“It is sort of gibberish, frankly,” Hoenig said. “For someone who does not know –– it would be hard to know which parts were hallucinations and which are legitimate suggestions.”&lt;/p&gt;
    &lt;p&gt;Researchers have found that chatbots’ competence on medical exams do not necessarily translate into the real world. In exam questions, symptoms are clearly laid out. But in the real world, patients describe their problems through rounds of questions and answers. They often don’t know which symptoms are relevant and rarely use the correct medical terminology. Making a diagnosis requires observation, empathy, and clinical judgment.&lt;/p&gt;
    &lt;p&gt;In a study published in Nature Medicine earlier this year, researchers designed an AI agent that acts as a pseudo patient and simulates how humans speak, using it to test LLMs’ clinical capabilities across 12 specialties. All the LLMs did much worse than how they performed in exams. Shreya Johri, a Ph.D. student at Harvard Medical School and a lead author of the study, told Rest of World that the AI models were not very good at asking questions. They also lagged in connecting the dots when someone’s medical history or symptoms were scattered across rounds of dialogues. “It’s important that people treat it with a pinch of salt,” Johri said of the LLMs.&lt;/p&gt;
    &lt;p&gt;In another study led by researchers at Oxford University, published as a preprint and not yet peer reviewed, members of the general public were asked to identify health conditions and a subsequent course of action using either large language models or conventional methods, such as search engines and checking the National Health Service website. Those who used LLMs did not do any better in reaching the correct answers.&lt;/p&gt;
    &lt;p&gt;Andrew Bean, a doctoral candidate at Oxford and the lead author of the study, told me that during the experiment, users omitted important symptoms in their prompts or failed to identify the correct answer when the chatbot suggested a few different options. Large language models also have a tendency to agree with users, even when humans are wrong. “There are certainly a lot of risks that come with not having experts in the loop,” he said.&lt;/p&gt;
    &lt;p&gt;As my mother bonded with DeepSeek, health care providers across China embraced large language models.&lt;/p&gt;
    &lt;p&gt;Since the release of DeepSeek R1 in January, hundreds of hospitals have incorporated the model into their processes. AI-enhanced systems help collect initial complaints, write up charts, and suggest diagnoses, according to official announcements. Partnering with tech companies, large hospitals use patient data to train their own specialized models. One hospital in Sichuan province introduced “DeepJoint,” a model for orthopaedics that analyzes CT or MRI scans to generate surgical plans. A hospital in Beijing developed “Stone Chat AI,” which answers patients’ questions about urinary tract stones.&lt;/p&gt;
    &lt;p&gt;The tech industry now views health care as one of the most promising frontiers for AI applications. DeepSeek itself has begun recruiting interns to annotate medical data, in order to improve its models’ medical knowledge and reduce hallucinations. Alibaba announced in May that its health care–focused chatbot, trained on top of its Qwen models, passed China’s medical qualification exams across 12 disciplines. Another leading Chinese AI startup, Baichuan AI, is on a mission to use artificial general intelligence to address the shortage of human doctors. “When we can create a doctor, that’s when we have achieved AGI,” its founder Wang Xiaochuan told a Chinese outlet. Baichuan AI declined my interview request.&lt;/p&gt;
    &lt;p&gt;Rudimentary “AI doctors” are popping up in the country’s most popular apps. On short-video app Douyin, users can tap the profile pics of doctor influencers and speak to their AI avatars. Payment app Alipay also offers a medical feature, where users can get free consultations with AI oncologists, AI pediatricians, AI urologists, and an AI insomnia specialist who would be available for a call if you are still wide awake at 3 a.m. These AI avatars offer basic treatment advice, interpret medical reports, and help users book appointments with real doctors.&lt;/p&gt;
    &lt;p&gt;Dr. Tian Jishun, a gynecologist in Hangzhou, agreed to lend his persona to Alipay as the company built up its fleet of 200 AI doctors. Tian told me he wanted to be part of the AI revolution, although he admits his digital counterpart is lacking. “It’s like the first iPhone,” he told me. “You never know what the future will be like.”&lt;/p&gt;
    &lt;p&gt;Zhang Chao, founder of AI health care startup Zuoshou Yisheng, developed an AI primary care doctor on top of Alibaba’s Qwen models. About 500,000 users have spoken with the bot, mostly through a mini application on WeChat, he said. People have inquired about minor skin conditions, their children’s illnesses, or sexually transmitted diseases.&lt;/p&gt;
    &lt;p&gt;China has banned “AI doctors” from generating prescriptions, but there is little regulatory oversight on what they say. Companies are left to make their own ethical decisions. Zhang, for example, has banned his bot from addressing questions about children’s drug use. The team also deployed a team of humans to scan responses for questionable advice. Zhang said he was overall confident with the bot’s performance. “There’s no correct answer when it comes to medicine,” Zhang said. “It’s all about how much it’s able to help the users.”&lt;/p&gt;
    &lt;p&gt;AI doctors are also coming to offline clinics. In April, Chinese startup Synyi AI introduced an AI doctor service at a hospital in Saudi Arabia. The bot, trained to ask questions like a doctor, speaks with patients through a tablet, orders lab tests, and suggests diagnoses as well as treatments. A human doctor then reviews the suggestions. Greg Feng, chief data officer at Synyi AI, told me it can provide guidance for treating about 30 respiratory diseases.&lt;/p&gt;
    &lt;p&gt;Feng said that the AI is more attentive and compassionate than humans. It can switch genders to make the patient more comfortable. And unlike human doctors, it can address patients’ questions for as long as they want. Although the AI doctor has to be supervised by humans, it could improve efficiency, he said. “In the past, one doctor could only work in one clinic,” Feng said. “Now, one doctor may be able to run two or three clinics at the same time.”&lt;/p&gt;
    &lt;p&gt;Entrepreneurs claim that AI can solve problems in health care access, such as the overcrowding of hospitals, the shortage of medical staff, and the rural–urban gap in quality care. Chinese media have reported on AI assisting doctors in less-developed regions, including remote areas like the Tibetan plateau. “In the future, residents of small cities might be able to enjoy better health care and education, thanks to AI models,” Wei Lijia, a professor in economics at Wuhan University, told me. His study, recently published in the Journal of Health Economics, found that AI assistance can curb overtreatment and enhance physicians’ performance in medical fields beyond their specialty. “Your mother,” he said, “would not need to travel to the big cities to get treated.”&lt;/p&gt;
    &lt;p&gt;Other researchers have raised concerns related to consent, accountability, and biases that could actually exacerbate health care disparities. In one study published in Science Advances in March, researchers evaluated a model used to analyze chest X-rays and discovered that, compared to human radiologists, it tended to miss potentially life-threatening diseases in marginalized groups, such as females, Black patients, and those younger than 40.&lt;/p&gt;
    &lt;p&gt;“I want to be very cautious in saying that AI will help reduce the health disparity in China or in other parts of the world,” said Lu Tang, a professor of communication at Texas A&amp;amp;M University who studies medical AI ethics. “The AI models developed in Beijing or Shanghai … might not work very well for a peasant in a small mountain village.”&lt;/p&gt;
    &lt;p&gt;When I called my mother and told her what the American nephrologists had said about DeepSeek’s mistakes, she said she was aware that DeepSeek had given her contradictory advice. She understood that chatbots were trained on data from across the internet, she told me, and did not represent an absolute truth or superhuman authority. She had stopped eating the lotus seed starch it had recommended.&lt;/p&gt;
    &lt;p&gt;But the care she gets from DeepSeek also goes beyond medical knowledge: it’s the chatbot’s steady presence that comforts her.&lt;/p&gt;
    &lt;p&gt;I remembered asking why she didn’t direct another type of question she often puts to DeepSeek — about English grammar — to me. “You would find me annoying for sure,” she replied. “But DeepSeek would say, ‘Let’s talk more about this.’ It makes me really happy.”&lt;/p&gt;
    &lt;p&gt;My one-child policy generation has grown up, and our parents are joining China’s rapidly growing elderly population. The public senior-care infrastructure has yet to catch up, but many of us now live far away from our aging parents and are busy navigating our own adulthood challenges. Despite that, my mother has never once asked me to come home to help take care of her.&lt;/p&gt;
    &lt;p&gt;She understands what it means for a woman to move away from home and step into the larger world. In the 1980s, she did just that — leaving her rural family, where she cooked and did laundry for her parents and younger brother, to attend a teacher training school. She respects my independence, sometimes to an extreme. I call my mother once every week or two. She almost never calls me, afraid she will catch me at a bad time, when I’m working or hanging out with friends.&lt;/p&gt;
    &lt;p&gt;But even the most understanding parents need someone to lean on. A friend my age in Washington, D.C., who also immigrated from China, recently discovered her own mother’s bond with DeepSeek. Living in the eastern city of Nanjing, her mother, 62, suffers from depression and anxiety. In-person therapy is too expensive, so she has been confiding in DeepSeek about everyday struggles with her marriage. DeepSeek responds with detailed analyses and to-do lists.&lt;/p&gt;
    &lt;p&gt;“I called her daily when my mother was very depressed and anxious. But for young people like us, it’s hard to keep up,” my friend told me. “The good thing about AI is she can say what she wants at any moment. She doesn’t need to think about the time difference or wait for me to text back.”&lt;/p&gt;
    &lt;p&gt;Zhang Jiansheng, a 36-year-old entrepreneur, created an AI-powered tablet that can speak to people with Alzheimer’s disease. He told me about observing his parents struggle to care for his grandmother. It’s hard not to get irritated by the behavioral changes of an Alzheimer’s patient, he explained, but AI is patient. “AI has no emotions,” he said. “It will keep offering encouragement, praise, and comfort to the elderly.”&lt;/p&gt;
    &lt;p&gt;My mother still turns to DeepSeek when she gets worried about her health. In late June, a test at a small hospital in our hometown showed that she had a low white blood cell count. She reported it to DeepSeek, which suggested follow-up tests. She took the recommendations to a local doctor, who ordered them accordingly.&lt;/p&gt;
    &lt;p&gt;The next day, we got on a call. It was my 8 p.m. and her 8 a.m. I told her to see the nephrologist in Hangzhou as soon as possible.&lt;/p&gt;
    &lt;p&gt;She refused, insisting she was fine with Dr. DeepSeek. “It’s so crowded there,” she said, raising her voice. “Thinking about that hospital gives me a headache.”&lt;/p&gt;
    &lt;p&gt;She eventually agreed to see the doctor. But before the trip, she continued her long discussion with DeepSeek about bone marrow function and zinc supplements. “DeepSeek has information from all over the world,” she argued. “It gives me all the possibilities and options. And I get to choose.”&lt;/p&gt;
    &lt;p&gt;I thought back to a conversation we’d had earlier about DeepSeek. “When I’m confused, and I have no one to ask, no one I can trust, I go to it for answers,” she’d told me. “I don’t have to spend money. I don’t have to wait in line. I don’t have to do anything.”&lt;/p&gt;
    &lt;p&gt;She added, “Even though it can’t give me a fully comprehensive or scientific answer, at least it gives me an answer.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46814569</guid><pubDate>Thu, 29 Jan 2026 18:45:27 +0000</pubDate></item><item><title>County pays $600k to pentesters it arrested for assessing courthouse security</title><link>https://arstechnica.com/security/2026/01/county-pays-600000-to-pentesters-it-arrested-for-assessing-courthouse-security/</link><description>&lt;doc fingerprint="3a5aaf12d4394517"&gt;
  &lt;main&gt;
    &lt;p&gt;Two security professionals who were arrested in 2019 after performing an authorized security assessment of a county courthouse in Iowa will receive $600,000 to settle a lawsuit they brought alleging wrongful arrest and defamation.&lt;/p&gt;
    &lt;p&gt;The case was brought by Gary DeMercurio and Justin Wynn, two penetration testers who at the time were employed by Colorado-based security firm Coalfire Labs. The men had written authorization from the Iowa Judicial Branch to conduct “red-team” exercises, meaning attempted security breaches that mimic techniques used by criminal hackers or burglars.&lt;/p&gt;
    &lt;p&gt;The objective of such exercises is to test the resilience of existing defenses using the types of real-world attacks the defenses are designed to repel. The rules of engagement for this exercise explicitly permitted “physical attacks,” including “lockpicking,” against judicial branch buildings so long as they didn’t cause significant damage.&lt;/p&gt;
    &lt;head rend="h2"&gt;A chilling message&lt;/head&gt;
    &lt;p&gt;The event galvanized security and law enforcement professionals. Despite the legitimacy of the work and the legal contract that authorized it, DeMercurio and Wynn were arrested on charges of felony third-degree burglary and spent 20 hours in jail, until they were released on $100,000 bail ($50,000 for each). The charges were later reduced to misdemeanor trespassing charges, but even then, Chad Leonard, sheriff of Dallas County, where the courthouse was located, continued to allege publicly that the men had acted illegally and should be prosecuted.&lt;/p&gt;
    &lt;p&gt;Reputational hits from these sorts of events can be fatal to a security professional’s career. And of course, the prospect of being jailed for performing authorized security assessment is enough to get the attention of any penetration tester, not to mention the customers that hire them.&lt;/p&gt;
    &lt;p&gt;“This incident didn’t make anyone safer,” Wynn said in a statement. “It sent a chilling message to security professionals nationwide that helping [a] government identify real vulnerabilities can lead to arrest, prosecution, and public disgrace. That undermines public safety, not enhances it.”&lt;/p&gt;
    &lt;p&gt;DeMercurio and Wynn’s engagement at the Dallas County Courthouse on September 11, 2019, had been routine. A little after midnight, after finding a side door to the courthouse unlocked, the men closed it and let it lock. They then slipped a makeshift tool through a crack in the door and tripped the locking mechanism. After gaining entry, the pentesters tripped an alarm alerting authorities.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46814614</guid><pubDate>Thu, 29 Jan 2026 18:48:09 +0000</pubDate></item><item><title>PlayStation 2 Recompilation Project Is Absolutely Incredible</title><link>https://redgamingtech.com/playstation-2-recompilation-project-is-absolutely-incredible/</link><description>&lt;doc fingerprint="87bf58572a279dff"&gt;
  &lt;main&gt;
    &lt;p&gt;The PlayStation 2’s library is easily among the best of any console ever released, and even if you were to narrow down the list of games to the very best, you’d be left with dozens (more like hundreds) of incredible titles.&lt;/p&gt;
    &lt;p&gt;But the PS2 hardware is getting a bit long in the tooth, and even though you can hook up the console using RGB component cables to a great upscaler (or use other means) to get the best visuals on a modern 4k TV, emulators have grown in popularity with PCSX2 offering gamers means to scale titles to render internally at higher resolutions, run with a more stable frame rate and, even make use of texture packs.&lt;/p&gt;
    &lt;p&gt;But do you know what’s better than an emulator? Taking the existing Playstation 2 game and recompiling it to run on a modern platform (such as your Windows or Linux desktop PC). That’s exactly what is being worked on now with PS2Recomp, a static Recompiler &amp;amp; Runtime Tool.&lt;/p&gt;
    &lt;p&gt;To keep things simple here, this will basically take a Playstation 2 game (which would be designed around the PS2’s unique architecture such as the ‘Emotion Engine’ CPU that’s based around a MIP R5900) and convert it to natively run on whatever platform you’re targeting.&lt;/p&gt;
    &lt;p&gt;In plain English, this is a tool and obviously, would need to be used on different games. In other words, it’s not just a ‘download and every game automatically runs’ application. But, it will give folks a tool to be able to decompile the game and quite frankly, that’s absolutely incredible.&lt;/p&gt;
    &lt;p&gt;This is a great stepping stone for some incredible remasters and community remakes of games. There are already HD Texture Packs available for PS2 emulators, as well as other ways to improve visuals. But this would give even more freedom and flexibility to do modify and really enhance the games. That’s to say nothing of totally unlocking the frame rates (and likely not breaking physics or collision detection which is a big problem with emulated titles).&lt;/p&gt;
    &lt;p&gt;At a guess, too, the games would also run great even with much lower-end hardware than would be needed for emulators. Recompilation efforts in the community certainly aren’t new. Indeed, you can look to the N64 because there have been several high-profile examples of what these kind of projects can achieve.&lt;/p&gt;
    &lt;p&gt;A few infamous ones would include both including Mario 64 and Zelda. Indeed, there’s a fork of the Mario 64 project supporting RTX (ray tracing) for Nvidia owners. You can see an example of Mario 64 below:&lt;/p&gt;
    &lt;p&gt;Another example on the N64 is Zelda, where the project has a plethora of visual and gameplay enhancements, and in the longer term again, they’re planning to introduce Ray Tracing.&lt;/p&gt;
    &lt;p&gt;So, in the future we could be playing the likes of MGS2, Gran Turismo, God of War, Tekken 4, Shadow Hearts with ‘native’ PC versions. This would allow controllers to run (such as dual shock or Xbox controllers) and other features to be bundled in too (exactly as we see with the N64 ports).&lt;/p&gt;
    &lt;p&gt;So yes, currently playing PS2 games on PC via emulator is still absolutely fantastic, but native ports would be the holy grail of game preservation.&lt;/p&gt;
    &lt;p&gt;The Playstation 2 architecture is extremely unique, and as I mentioned earlier in this article focused around a MIPS R5900 based CPU known as the Emotion Engine (operating a shade under 300MHz). This CPU was super unique, because Sony implemented a number of customized features include two Vector Units designed to help manipulate geometry and perform a bunch of other co-processing duties.&lt;/p&gt;
    &lt;p&gt;This was bundled with 32MB of memory, and the GPU was known as the Graphics Synthesizer, runing at about 147MHz, and sporting 4MB of embedded DRAM. Sony’s design was fascinating for the time, and despite its processor clocked significantly lower than either Nintendo’s GameCube or Microsoft’s Xbox, punched well above its weight class.&lt;/p&gt;
    &lt;p&gt;As a small update – I want to remind people that (as of the time I’m writing this article) the project is *NOT* finished yet, and there is still work to do. But the fact that this is being worked on is awesome for those of us interested in game preservation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46814743</guid><pubDate>Thu, 29 Jan 2026 18:55:38 +0000</pubDate></item><item><title>Networks Hold the Key to a Decades-Old Problem About Waves</title><link>https://www.quantamagazine.org/networks-hold-the-key-to-a-decades-old-problem-about-waves-20260128/</link><description>&lt;doc fingerprint="964d1215cca08d8b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Networks Hold the Key to a Decades-Old Problem About Waves&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Two centuries ago, Joseph Fourier gave mathematicians a magical technique. He conjectured that it’s possible to write almost any function as a sum of simple waves, a trick now called the Fourier transform. These days, the Fourier transform is used to understand everything from the chemical makeup of distant stars to what’s happening far beneath the Earth’s crust.&lt;/p&gt;
    &lt;p&gt;“Fourier series are everywhere in mathematics,” said Mehtaab Sawhney of Columbia University. “It’s part of the faith of mathematicians that Fourier series are important.”&lt;/p&gt;
    &lt;p&gt;Yet certain fundamental questions about the Fourier transform have remained stubbornly, and mysteriously, unanswerable.&lt;/p&gt;
    &lt;p&gt;In 1965, the mathematician Sarvadaman Chowla posed one such question. He wanted to know how small an extremely simple type of Fourier transform — a sum of cosine waves — could get. His problem sounded straightforward. But somehow, it wasn’t.&lt;/p&gt;
    &lt;p&gt;“The question is a bit of bait,” Sawhney said; it was designed to illuminate just how little mathematicians know. “Because we can’t show this, we clearly don’t understand the structure of these [sums] at all.”&lt;/p&gt;
    &lt;p&gt;For decades, mathematicians struggled with Chowla’s cosine problem. It became a benchmark for Fourier analysis techniques, used to explore how well they could detect deeper structure in sequences of numbers. The results were discouraging. “Progress was completely anemic,” said Tom Sanders of the University of Oxford.&lt;/p&gt;
    &lt;p&gt;In September, that suddenly changed. Four mathematicians — Zhihan Jin, Aleksa Milojević, István Tomon, and Shengtong Zhang — posted the first significant advance on the problem in 20 years. Their strategy had almost nothing to do with traditional Fourier analysis.&lt;/p&gt;
    &lt;p&gt;In fact, before last summer, the foursome had never even heard of Chowla’s cosine problem.&lt;/p&gt;
    &lt;head rend="h2"&gt;Feeling Low&lt;/head&gt;
    &lt;p&gt;In the early 1950s, Chowla and his fellow number theorist Nesmith Ankeny wanted to use the Fourier transform to better understand patterns in sets of numbers. Consider the set consisting of the numbers 2, 3, and 8. First, use each number in the set to define a cosine wave — 2 gives you cos(2x), for instance. Then add up all your waves to get cos(2x) + cos(3x) + cos(8x). This is just another way of writing your original set as a Fourier series. The series is highly structured: All the waves are cosines, and because there are no numbers in front of any of the cosines, all the waves are the same size. “It’s the simplest possible type of Fourier series you can have,” said Benjamin Bedert of the University of Cambridge. “And in general, we know quite a lot about Fourier series.”&lt;/p&gt;
    &lt;p&gt;The new wave defined by cos(2x) + cos(3x) + cos(8x) has peaks and valleys that reveal interesting properties of the original set of numbers. So Ankeny and Chowla sought to test how much they really understood about such a series. They wondered: For any set of N integers, what is the lowest value that the sum will ever take?&lt;/p&gt;
    &lt;p&gt;It’s easy to figure out the sum’s maximum. When x is zero, any cosine wave hits its maximum at 1. So our sum of three cosine waves gives us 1 + 1 + 1, or 3. Similarly, a sum of 10 million cosine waves has a maximum of 10 million. For any set of N integers, the maximum is simply N.&lt;/p&gt;
    &lt;p&gt;Yet understanding the cosine sum’s minimum is surprisingly difficult. While the different waves all hit their maximum simultaneously at least once (when x is zero), this is not true for the minimum. Perhaps the lowest points of the different waves will still align enough to produce a very low sum. Or perhaps the waves will interfere with each other so that it becomes impossible for the sum to get too low.&lt;/p&gt;
    &lt;p&gt;From left: Courtesy of Zhihan Jin; Archives of the Mathematisches Forschungsinstitut Oberwolfach; Livia Tomon-Horvath&lt;/p&gt;
    &lt;p&gt;In 1952, Ankeny and Chowla conjectured that just as the maximum gets higher and higher as the number of integers in the original set gets bigger, the minimum should get lower and lower. This was proved several years later — prompting Chowla to sharpen the question in 1965. He wanted to know exactly how fast the minimum drops as N grows.&lt;/p&gt;
    &lt;p&gt;He knew of sets of N integers whose cosine sum had a minimum value around −$latex \sqrt{\textit{N}}$. Every other set he could think of dipped even lower, leading him to conjecture that for any set of N positive integers, the minimum of the corresponding cosine sum must be below −$latex \sqrt{\textit{N}}$.&lt;/p&gt;
    &lt;p&gt;Over the ensuing decades, a few mathematicians chipped away at the problem. But by the mid-2000s, there was still a massive gulf between what they were able to prove and what Chowla had predicted. According to the latest bound, proved in 2004 by Imre Ruzsa of the Alfréd Rényi Institute of Mathematics in Hungary, a sum of 1020 cosines — that’s a 1 with 20 zeros after it, about the number of molecules in a cubic inch of air — must have a minimum value smaller than about −7. By comparison, Chowla had predicted that the minimum would have to dip below −1010.&lt;/p&gt;
    &lt;p&gt;And yet, for the past 20 years, Ruzsa’s result has represented the pinnacle of progress on Chowla’s cosine problem.&lt;/p&gt;
    &lt;p&gt;Then an entirely unrelated research program finally broke the barrier.&lt;/p&gt;
    &lt;head rend="h2"&gt;Bridging the Divide&lt;/head&gt;
    &lt;p&gt;The program dealt with networks of nodes and edges called graphs.&lt;/p&gt;
    &lt;p&gt;Last summer, two sets of graph theorists — Jin, Milojević, and Tomon in Europe, and Zhang at Stanford University — were enthusiastically making progress on one of graph theory’s most central questions. The “MaxCut” problem is about the optimal way to cut a graph into two parts so that there are as many edges as possible connecting the parts. It’s a basic question about the structure of a graph, with real-world applications: A graph’s MaxCut might represent an efficient circuit design, for instance, or the lowest-energy state of a system of particles.&lt;/p&gt;
    &lt;p&gt;Mark Belan/Samuel Velasco/Quanta Magazine&lt;/p&gt;
    &lt;p&gt;But there’s no one-size-fits-all approach to finding a graph’s MaxCut, at least at the moment. (It’s what’s known as an NP-hard problem.) And so mathematicians instead attempt to estimate the MaxCut for specific classes of graphs.&lt;/p&gt;
    &lt;p&gt;In 2003, Benjamin Sudakov, a mathematician at the Swiss Federal Institute of Technology Zurich who would later mentor Jin, Milojević, and Tomon, posed a conjecture with three colleagues about the MaxCut of a particular kind of graph. This graph had no cliques — clusters of nodes that are all connected to one another.&lt;/p&gt;
    &lt;p&gt;Last July, more than two decades later, Zhang proved a new bound on the MaxCut for such graphs. A few days later, Jin, Milojević, and Tomon improved on his result.&lt;/p&gt;
    &lt;p&gt;To do this, the researchers investigated important quantities called eigenvalues. Eigenvalues provide information about a graph’s structure. For example, the largest eigenvalue counts the number of edges in the graph; the second-largest measures the graph’s connectivity. Jin, Milojević, Tomon, and Zhang focused on the negative eigenvalues, building on a recent line of research that had linked them to a graph’s MaxCut. Their analysis of these eigenvalues ultimately allowed them to prove their new results.&lt;/p&gt;
    &lt;p&gt;The mathematicians decided to combine their separate results into a joint paper. But before they could finish, they received an unexpected email about Chowla’s cosine problem.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cayley’s Graph&lt;/head&gt;
    &lt;p&gt;The email was from Ilya Shkredov, a mathematician at Purdue University in Indiana. Shkredov, a number theorist, pointed out that Chowla’s cosine problem could be reformulated in terms of graphs. Not the general kinds of graphs that the team was studying, but a special type of graph invented in 1878 by the mathematician Arthur Cayley.&lt;/p&gt;
    &lt;p&gt;To build a Cayley graph, imagine you’re once again working with the set {2, 3, 8}. Start with a bunch of nodes — it doesn’t really matter how many, so long as the number of nodes is prime and larger than the biggest integer in the set. Next, arrange the nodes in a circle and label each one with an integer. Then place an edge between two nodes if the difference between them is in the original set. So the nodes labeled 1 and 3 will be connected by an edge, because they differ by 2, and 2 is in the set {2, 3, 8}.&lt;/p&gt;
    &lt;p&gt;By the 1970s, mathematicians had figured out that embedded within the structure of Cayley graphs is information about the Fourier series from Chowla’s problem. A Cayley graph’s eigenvalues, it turns out, correspond exactly to different values that the cosine sum can have. The smallest eigenvalue therefore tells you how low the cosine sum can get.&lt;/p&gt;
    &lt;p&gt;“It’s a well-known thing,” Milojević said. “The connection is very classical.”&lt;/p&gt;
    &lt;p&gt;It allowed mathematicians to reframe the problem. If they could show that the smallest eigenvalue of a Cayley graph gets very small, it would mean that the cosine sum has to get very small as well — precisely what Chowla’s cosine problem is all about.&lt;/p&gt;
    &lt;p&gt;But no one could figure out how to exploit that connection.&lt;/p&gt;
    &lt;p&gt;“You try to hit a nail with a hammer only once you have a hammer,” Sudakov said. Mathematicians didn’t have a way of analyzing the lowest eigenvalue accurately enough to find out what they wanted to know about the minimum of the cosine sum.&lt;/p&gt;
    &lt;p&gt;Wanqi Zhu&lt;/p&gt;
    &lt;p&gt;But in their work on the MaxCut of graphs, Jin, Milojević, Tomon, and Zhang had unwittingly produced a hammer. While studying how the eigenvalues of a graph relate to its structure, they’d discovered that any graph that doesn’t have a low eigenvalue must be dominated by cliques. Shkredov, reading their proof, realized that this meant that the team had actually reframed Chowla’s cosine problem once more: There was no longer any need to analyze the eigenvalue directly. Instead, they just had to prove that the Cayley graphs didn’t have any large cliques. That would imply that the graphs each had a very low eigenvalue, finally enabling them to exploit the link between Chowla’s conjecture and graph theory.&lt;/p&gt;
    &lt;p&gt;From then on, “I think the main obstacle was believing we can do it,” Tomon said.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Cliques Click&lt;/head&gt;
    &lt;p&gt;When Shkredov sent his email, the mathematicians were all on vacation. But Tomon, who was visiting his home city of Budapest, found time to toy with the Cayley graph.&lt;/p&gt;
    &lt;p&gt;After a bit of thinking, “it just clicked,” he said.&lt;/p&gt;
    &lt;p&gt;To see how Tomon’s idea works, let’s go back to our Cayley graph for the set {2, 3, 8}. Remember that proving Chowla’s conjecture means showing that the graph’s smallest eigenvalue gets very low. So first assume the opposite: that none of the eigenvalues are low. You’ll want to show that this assumption will eventually lead to a contradiction.&lt;/p&gt;
    &lt;p&gt;Based on the team’s work on MaxCut, if the Cayley graph has no small eigenvalues, then it must have a large clique — say, five nodes that are all connected to each other. This, in turn, means that if you take any two of those nodes, the difference between their integer labels is 2, 3, or 8.&lt;/p&gt;
    &lt;p&gt;Romana Meereis&lt;/p&gt;
    &lt;p&gt;But now add 1 to each node to get a new set of five nodes. They’ll differ by the same amounts as the first set, meaning that they, too, will form a clique. Keep going, and you’ll generate more and more cliques. But there’s a problem: Cliques have lots of edges, while a Cayley graph, based on how it’s defined, has relatively few edges, which obey a very particular structure. Eventually, you’ll get so many cliques that you’ll have generated more edges than the Cayley graph can hold. This means that the earlier assumption that there was a large clique must have been false. Which, in turn, means that the smallest eigenvalue had to be low.&lt;/p&gt;
    &lt;p&gt;Once Tomon figured this out, the rest of the proof came together relatively easily. In September, he, Jin, Milojević, and Zhang posted their joint paper online. It mainly focused on how to analyze the lowest eigenvalues of graphs — work that, for one thing, allowed them to strengthen the bounds they’d found a few months earlier on the MaxCut of graphs without cliques.&lt;/p&gt;
    &lt;p&gt;But their headline result was about Chowla’s cosine problem. They’d proved that for any set of N integers, the corresponding cosine sum attains a value lower than −N1/10. For any realistic value of N, −N1/10 doesn’t differ too much from Ruzsa’s decades-old bound. But for huge values of N, like 1020, the difference starts to be noticeable: Jin, Milojević, Tomon, and Zhang show that a sum of 1020 cosines slides below −100, in comparison to Ruzsa’s bound of −7.&lt;/p&gt;
    &lt;p&gt;“For me, it’s very surprising,” Sudakov said. The group started with a result about graphs, and out of nowhere, they gained fresh insight on a seemingly unrelated problem.&lt;/p&gt;
    &lt;p&gt;Just two days after the researchers posted their paper, Bedert, the Cambridge mathematician, posted his own advance on the problem, using a more traditional approach from Fourier analysis. His result edges out the team’s bound by a hair: It says that for any set of N integers, the cosine sum attains a value less than −N1/7. For 1020, this lowers the minimum that Jin, Milojević, Tomon, and Zhang identified from −100 to around −720.&lt;/p&gt;
    &lt;p&gt;But what mathematicians find most noteworthy is that both of these results mark the first time that a proven estimate has the same form as Chowla’s conjectured bound. That is, the new bounds, like Chowla’s, can be written as a power of N. (Chowla’s bound of −$latex \sqrt{\textit{N}}$ is equivalent to −N1/2.) Ruzsa’s previous estimate cannot be written in this form.&lt;/p&gt;
    &lt;p&gt;The fog surrounding the Fourier transform is still dense. But these new techniques are a little better at seeing through it.&lt;/p&gt;
    &lt;p&gt;Though neither proof has fully bridged the gap to prove Chowla’s conjecture, mathematicians are excited. For now, “it is a little bit, I think, like the moon landing or the 4-minute mile,” Sanders said. “It’s not clear ahead of time what this is going to open up.”&lt;/p&gt;
    &lt;p&gt;The role that graphs played in the story is particularly intriguing. This isn’t the first time that graph theory and Fourier analysis have met. But so far, the links between the two fields have been one-offs. Now, Jin hopes that the specific connection between Chowla’s cosine problem and MaxCut hints at something broader. “Whatever is predicted in the Chowla problem, that phenomenon is more general,” he said. “It works in graphs.”&lt;/p&gt;
    &lt;p&gt;“We now have more problems that are in the same spheres of influence,” Sawhney said. “Knowing that things are living in the same world is very useful information. It’s very powerful.”&lt;/p&gt;
    &lt;p&gt;Correction: January 29, 2026&lt;lb/&gt; An earlier version of the text implied that Benjamin Sudakov was the sole author of a 2003 conjecture about the MaxCut of certain graphs. He was in fact one of four authors.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46814991</guid><pubDate>Thu, 29 Jan 2026 19:10:54 +0000</pubDate></item><item><title>Flameshot</title><link>https://github.com/flameshot-org/flameshot</link><description>&lt;doc fingerprint="8e9cf05c9f27b3ba"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Features&lt;/item&gt;
      &lt;item&gt;Usage&lt;/item&gt;
      &lt;item&gt;Keyboard Shortcuts&lt;/item&gt;
      &lt;item&gt;Considerations&lt;/item&gt;
      &lt;item&gt;Installation&lt;/item&gt;
      &lt;item&gt;Compilation&lt;/item&gt;
      &lt;item&gt;License&lt;/item&gt;
      &lt;item&gt;Privacy Policy&lt;/item&gt;
      &lt;item&gt;Code Signing Policy&lt;/item&gt;
      &lt;item&gt;Contribute&lt;/item&gt;
      &lt;item&gt;Acknowledgment&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Customizable appearance.&lt;/item&gt;
      &lt;item&gt;Easy to use.&lt;/item&gt;
      &lt;item&gt;In-app screenshot editing.&lt;/item&gt;
      &lt;item&gt;DBus interface.&lt;/item&gt;
      &lt;item&gt;Upload to Imgur.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Executing the command &lt;code&gt;flameshot&lt;/code&gt; without parameters will launch a running
instance of the program in the background without taking actions.
If your desktop environment provides tray area, a tray icon will also
appear in the tray for users to perform configuration and management.&lt;/p&gt;
    &lt;p&gt;Example commands:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Capture with GUI:&lt;/p&gt;
        &lt;quote&gt;flameshot gui&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Capture with GUI with custom save path:&lt;/p&gt;
        &lt;code&gt;flameshot gui -p ~/myStuff/captures&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Capture with GUI after 2 seconds delay (can be useful to take screenshots of mouse hover tooltips, etc.):&lt;/p&gt;
        &lt;quote&gt;flameshot gui -d 2000&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Fullscreen capture with custom save path (no GUI) and delayed:&lt;/p&gt;
        &lt;code&gt;flameshot full -p ~/myStuff/captures -d 5000&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Fullscreen capture with custom save path copying to clipboard:&lt;/p&gt;
        &lt;code&gt;flameshot full -c -p ~/myStuff/captures&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Capture the screen containing the mouse and print the image (bytes) in PNG format:&lt;/p&gt;
        &lt;quote&gt;flameshot screen -r&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Capture the screen number 1 and copy it to the clipboard:&lt;/p&gt;
        &lt;quote&gt;flameshot screen -n 1 -c&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In case of doubt choose the first or the second command as shortcut in your favorite desktop environment.&lt;/p&gt;
    &lt;p&gt;A systray icon will be in your system's panel while Flameshot is running. Do a right click on the tray icon and you'll see some menu items to open the configuration window and the information window. Check out the About window to see all available shortcuts in the graphical capture mode.&lt;/p&gt;
    &lt;p&gt;On Windows, &lt;code&gt;flameshot.exe&lt;/code&gt; will behave as expected for all supported command-line arguments,
but it will not output any text to the console. This is problematic if, for example, you are
running &lt;code&gt;flameshot.exe -h&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If you require console output, run &lt;code&gt;flameshot-cli.exe&lt;/code&gt; instead. &lt;code&gt;flameshot-cli.exe&lt;/code&gt; is a minimal wrapper around &lt;code&gt;flameshot.exe&lt;/code&gt; that ensures all stdout is captured and output to the console.&lt;/p&gt;
    &lt;p&gt;You can use the graphical menu to configure Flameshot, but alternatively you can use your terminal or scripts to do so.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Open the configuration menu:&lt;/p&gt;
        &lt;quote&gt;flameshot config&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Show the initial help message in the capture mode:&lt;/p&gt;
        &lt;code&gt;flameshot config --showhelp true&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;For more information about the available options use the help flag:&lt;/p&gt;
        &lt;quote&gt;flameshot config -h&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can also edit some of the settings (like overriding the default colors) in the configuration file.&lt;lb/&gt; Linux path: &lt;code&gt;~/.config/flameshot/flameshot.ini&lt;/code&gt;.&lt;lb/&gt; Windows path: &lt;code&gt;C:\Users\{YOURNAME}\AppData\Roaming\flameshot\flameshot.ini&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;When copying over the config file from Linux to Windows or vice versa, make sure to correct the &lt;code&gt;savePath&lt;/code&gt; variable,&lt;lb/&gt; so that the screenshots save in the right directory on your desired file system.&lt;/p&gt;
    &lt;p&gt;These shortcuts are available in GUI mode:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Keys&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;P&lt;/cell&gt;
        &lt;cell&gt;Set the Pencil as paint tool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;D&lt;/cell&gt;
        &lt;cell&gt;Set the Line as paint tool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;A&lt;/cell&gt;
        &lt;cell&gt;Set the Arrow as paint tool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;S&lt;/cell&gt;
        &lt;cell&gt;Set Selection as paint tool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;R&lt;/cell&gt;
        &lt;cell&gt;Set the Rectangle as paint tool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;C&lt;/cell&gt;
        &lt;cell&gt;Set the Circle as paint tool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;M&lt;/cell&gt;
        &lt;cell&gt;Set the Marker as paint tool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;T&lt;/cell&gt;
        &lt;cell&gt;Add text to your capture&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;B&lt;/cell&gt;
        &lt;cell&gt;Set Pixelate as the paint tool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;←, ↓, ↑, →&lt;/cell&gt;
        &lt;cell&gt;Move selection 1px&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Shift + ←, ↓, ↑, →&lt;/cell&gt;
        &lt;cell&gt;Resize selection 1px&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + Shift + ←, ↓, ↑, →&lt;/cell&gt;
        &lt;cell&gt;Symmetrically resize selection 2px&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Esc&lt;/cell&gt;
        &lt;cell&gt;Quit capture&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + M&lt;/cell&gt;
        &lt;cell&gt;Move the selection area&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + C&lt;/cell&gt;
        &lt;cell&gt;Copy to clipboard&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + S&lt;/cell&gt;
        &lt;cell&gt;Save selection as a file&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + Z&lt;/cell&gt;
        &lt;cell&gt;Undo the last modification&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + Shift + Z&lt;/cell&gt;
        &lt;cell&gt;Redo the next modification&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + Q&lt;/cell&gt;
        &lt;cell&gt;Leave the capture screen&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + O&lt;/cell&gt;
        &lt;cell&gt;Choose an app to open the capture&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + Return&lt;/cell&gt;
        &lt;cell&gt;Commit text in text area&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + Backspace&lt;/cell&gt;
        &lt;cell&gt;Cancel current selection&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Return&lt;/cell&gt;
        &lt;cell&gt;Upload the selection to Imgur&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Spacebar&lt;/cell&gt;
        &lt;cell&gt;Toggle visibility of sidebar with options of the selected tool, color picker for the drawing color and history menu&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;G&lt;/cell&gt;
        &lt;cell&gt;Starts the color picker&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Right Click&lt;/cell&gt;
        &lt;cell&gt;Show the color wheel&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Mouse Wheel&lt;/cell&gt;
        &lt;cell&gt;Change the tool's thickness&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Print screen&lt;/cell&gt;
        &lt;cell&gt;Capture Screen&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Shift + Print&lt;/cell&gt;
        &lt;cell&gt;Screenshot History&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + drawing line, arrow or marker&lt;/cell&gt;
        &lt;cell&gt;Drawing only horizontally, vertically or diagonally&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Ctrl + drawing rectangle or circle&lt;/cell&gt;
        &lt;cell&gt;Keeping aspect ratio&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Shift + drag a handler of the selection area: mirror redimension in the opposite handler.&lt;/p&gt;
    &lt;p&gt;Flameshot uses Print screen (Windows) and cmd-shift-x (macOS) as default global hotkeys.&lt;/p&gt;
    &lt;p&gt;On Linux, Flameshot doesn't yet support Prt Sc out of the box, but with a bit of configuration you can set this up:&lt;/p&gt;
    &lt;p&gt;To make configuration easier, there's a file in the repository that more or less automates this process. This file will assign the following hotkeys by default:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Keys&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Prt Sc&lt;/cell&gt;
        &lt;cell&gt;Start the Flameshot screenshot tool and take a screenshot&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + Prt Sc&lt;/cell&gt;
        &lt;cell&gt;Wait for 3 seconds, then start the Flameshot screenshot tool and take a screenshot&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Shift + Prt Sc&lt;/cell&gt;
        &lt;cell&gt;Take a full-screen (all monitors) screenshot and save it&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Ctrl + Shift + Prt Sc&lt;/cell&gt;
        &lt;cell&gt;Take a full-screen (all monitors) screenshot and copy it to the clipboard&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If you don't like the defaults, they can be changed later.&lt;/p&gt;
    &lt;p&gt;Steps for using the configuration:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;The configuration file makes Flameshot automatically save screenshots to&lt;/p&gt;&lt;code&gt;~/Pictures/Screenshots&lt;/code&gt;without opening the save dialog. Make sure that folder exists by running:&lt;code&gt;mkdir -p ~/Pictures/Screenshots&lt;/code&gt;&lt;p&gt;(If you don't like the default location, you can skip this step and configure your preferred directory later.)&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Download the configuration file:&lt;/p&gt;
        &lt;quote&gt;cd ~/Desktop wget https://raw.githubusercontent.com/flameshot-org/flameshot/master/docs/shortcuts-config/flameshot-shortcuts-kde.khotkeys&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Make sure you have the&lt;/p&gt;&lt;code&gt;khotkeys&lt;/code&gt;installed using your package manager to enable custom shortcuts in KDE Plasma.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Go to System Settings → Shortcuts → Custom Shortcuts.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If an entry exists for Spectacle (the default KDE screenshot utility), you'll need to disable it because its shortcuts might conflict with Flameshot's. Do this by unchecking the Spectacle entry.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Click Edit → Import..., navigate to the configuration file and open it.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Now the Flameshot entry should appear in the list. Click Apply to apply the changes.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you want to change the default hotkeys, you can expand the entry, select the appropriate action and modify it as you wish; the process is pretty self-explanatory.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you installed Flameshot as a Flatpak, you will need to create a symlink to the command:&lt;/p&gt;
        &lt;code&gt;ln -s /var/lib/flatpak/exports/bin/org.flameshot.Flameshot ~/.local/bin/flameshot&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To use Flameshot instead of the default screenshot application in Gnome we need to remove the binding on Prt Sc key, and then create a new binding for &lt;code&gt;flameshot gui&lt;/code&gt; (adapted from Pavel's answer on AskUbuntu).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Remove the binding on Prt Sc:&lt;/p&gt;
        &lt;p&gt;Go to Settings &amp;gt; Keyboard &amp;gt; View and Customise Shortcuts &amp;gt; Screenshots &amp;gt; Take a screenshot interactively and press&lt;/p&gt;
        &lt;code&gt;backspace&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Add custom binding on Prt Sc:&lt;/p&gt;
        &lt;p&gt;Go to Settings &amp;gt; Keyboard &amp;gt; View and Customise Shortcuts &amp;gt; Custom shortcuts and press the '+' button at the bottom.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Name the command as you like it, e.g.&lt;/p&gt;&lt;code&gt;flameshot&lt;/code&gt;. And in the command insert&lt;code&gt;/usr/bin/flameshot gui&lt;/code&gt;or&lt;code&gt;flatpak run org.flameshot.Flameshot gui&lt;/code&gt;if installed via flatpak.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Then click "Set Shortcut.." and press Prt Sc. This will show as "print".&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now every time you press Prt Sc, it will start the Flameshot GUI instead of the default application.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Go to&lt;/p&gt;&lt;code&gt;Keyboard&lt;/code&gt;settings&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Switch to the tab&lt;/p&gt;
        &lt;code&gt;Application Shortcuts&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Find the entry&lt;/p&gt;
        &lt;code&gt;Command Shortcut xfce4-screenshooter -fd 1 Print&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Replace&lt;/p&gt;&lt;code&gt;xfce4-screenshooter -fd 1&lt;/code&gt;with&lt;code&gt;flameshot gui&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now every time you press Prt Sc it will start Flameshot GUI instead of the default application.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Edit your&lt;/p&gt;&lt;code&gt;~/.fluxbox/keys&lt;/code&gt;file&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Add a new entry.&lt;/p&gt;&lt;code&gt;Print&lt;/code&gt;is the key name,&lt;code&gt;flameshot gui&lt;/code&gt;is the shell command; for more options see the fluxbox wiki.&lt;code&gt;Print :Exec flameshot gui&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Refresh Fluxbox configuration with Reconfigure option from the menu.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Experimental Gnome Wayland and Plasma Wayland support.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you are using Gnome you need to install the AppIndicator and KStatusNotifierItem Support extension in order to see the system tray icon.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Press Enter or Ctrl + C when you are in a capture mode and you don't have an active selection and the whole desktop will be copied to your clipboard. Pressing Ctrl + S will save your capture to a file. Check the Shortcuts for more information.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Flameshot works best with a desktop environment that includes D-Bus. See this article for tips on using Flameshot in a minimal window manager (dwm, i3, xmonad, etc).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In order to speed up the first launch of Flameshot (D-Bus init of the app can be slow), consider starting the application automatically on boot.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Quick tip: If you don't have Flameshot to autostart at boot and you want to set keyboard shortcut, use the following as the command for the keybinding:&lt;/item&gt;
        &lt;/list&gt;
        &lt;quote&gt;( flameshot &amp;amp;; ) &amp;amp;&amp;amp; ( sleep 0.5s &amp;amp;&amp;amp; flameshot gui )&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Flameshot can be installed on Linux, Microsoft Windows, and macOS.&lt;/p&gt;
    &lt;p&gt;Some prebuilt packages are provided on the release page of the GitHub project repository.&lt;/p&gt;
    &lt;p&gt;There are packages available in the repository of some Linux distributions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Arch: &lt;code&gt;pacman -S flameshot&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;Snapshot also available via AUR: flameshot-git.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Debian 10+: &lt;code&gt;apt install flameshot&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;Package for Debian 9 ("Stretch") also available via stretch-backports.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Ubuntu: &lt;code&gt;apt install flameshot&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;openSUSE: &lt;code&gt;zypper install flameshot&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Void Linux: &lt;code&gt;xbps-install flameshot&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Solus: &lt;code&gt;eopkg it flameshot&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Fedora: &lt;code&gt;dnf install flameshot&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;NixOS: &lt;code&gt;nix-env -iA nixos.flameshot&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;ALT: &lt;code&gt;su - -c "apt-get install flameshot"&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Snap/Flatpak/AppImage&lt;/item&gt;
      &lt;item&gt;Docker&lt;/item&gt;
      &lt;item&gt;Windows&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;MacPorts: &lt;code&gt;sudo port selfupdate &amp;amp;&amp;amp; sudo port install flameshot&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Homebrew: &lt;code&gt;brew install --cask flameshot&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note that because of macOS security features, you may not be able to open flameshot when installed using brew. If you see the message &lt;code&gt;“flameshot” cannot be opened because the developer cannot be verified.&lt;/code&gt; you will need to
follow the steps below:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Go to the Applications folder (Finder &amp;gt; Go &amp;gt; Applications, or Shift+Command+A)&lt;/item&gt;
      &lt;item&gt;Right-Click on "flameshot.app" and choose "Open" from the context menu&lt;/item&gt;
      &lt;item&gt;In the dialog click "Open"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On MacOs 15 and above, you will have to go to system settings -&amp;gt; privacy and security after doing this and click "Open Anyway" or you can open flameshot first time with the following command.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;sudo xattr -rd com.apple.quarantine /Applications/flameshot.app&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;After following all those steps above, &lt;code&gt;flameshot&lt;/code&gt; will open without problems in your Mac.&lt;/p&gt;
    &lt;p&gt;Note that for the Flameshot icon to appear in your tray area, you should have a systray software installed. This is especially true for users who use minimal window managers such as dwm. In some Desktop Environment installations (e.g Gnome), the systray might be missing and you can install an application or plugin (e.g Gnome shell extension) to add the systray to your setup. It has been reported) that icon of some software, including Flameshot, does not show in gnome-shell-extension-appindicator.&lt;/p&gt;
    &lt;p&gt;Alternatively, in case you don't want to have a systray, you can always call Flameshot from the terminal. See Usage section.&lt;/p&gt;
    &lt;p&gt;To build the application in your system, you'll need to install the dependencies needed for it and package names might be different for each distribution, see Dependencies below for more information. You can also install most of the Qt dependencies via their installer. If you were developing Qt apps before, you probably already have them.&lt;/p&gt;
    &lt;p&gt;This project uses CMake build system, so you need to install it in order to build the project (on most Linux distributions it is available in the standard repositories as a package called &lt;code&gt;cmake&lt;/code&gt;). If your distribution provides too old version of CMake (e.g. Ubuntu or Debian) you can download it on the official website.&lt;/p&gt;
    &lt;p&gt;Also you can open and build/debug the project in a C++ IDE. For example, in Qt Creator you should be able to simply open &lt;code&gt;CMakeLists.txt&lt;/code&gt; via &lt;code&gt;Open File or Project&lt;/code&gt; in the menu after installing CMake into your system. More information about CMake projects in Qt Creator.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Qt &amp;gt;= 6.2.4 (available by default on Ubuntu Jammy) &lt;list rend="ul"&gt;&lt;item&gt;Development tools&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;GCC &amp;gt;= 11&lt;/item&gt;
      &lt;item&gt;CMake &amp;gt;= 3.22&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Qt &lt;list rend="ul"&gt;&lt;item&gt;SVG&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Git&lt;/item&gt;
      &lt;item&gt;OpenSSL&lt;/item&gt;
      &lt;item&gt;CA Certificates&lt;/item&gt;
      &lt;item&gt;Qt Image Formats - for additional export image formats (e.g. tiff, webp, and more)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Compile-time
apt install g++ cmake build-essential qt6-base-dev qt6-tools-dev-tools qt6-svg-dev qt6-tools-dev

# Run-time
apt install libkf6guiaddons-dev libqt6dbus6 libqt6network6 libqt6core6 libqt6widgets6 libqt6gui6 libqt6svg6 qt6-qpa-plugins

# Optional
apt install git openssl ca-certificates qt6-image-formats-plugins&lt;/code&gt;
    &lt;code&gt;# Compile-time
dnf install gcc-c++ cmake qt6-qtbase-devel qt6-qtsvg-devel qt6-qttools qt6-linguist qt6-qttools-devel kf6-kguiaddons-devel

# Run-time
dnf install qt6-qtbase qt6-qtsvg kf6-kguiaddons

# Optional
dnf install git openssl ca-certificates qt6-qtimageformats&lt;/code&gt;
    &lt;code&gt;# Compile-time
pacman -S cmake base-devel git qt6-base qt6-tools kguiaddons

# Run-time
pacman -S qt6-svg

# Optional
pacman -S openssl ca-certificates qt6-imageformats&lt;/code&gt;
    &lt;p&gt;Development Shell:&lt;/p&gt;
    &lt;code&gt;# Without flakes:
nix-shell

# With flakes:
nix develop&lt;/code&gt;
    &lt;code&gt;# Build flameshot
nix build

# Build and run flameshot
nix run&lt;/code&gt;
    &lt;p&gt;First of all you need to install brew and then install the dependencies&lt;/p&gt;
    &lt;code&gt;brew install qt6
brew install cmake&lt;/code&gt;
    &lt;p&gt;After installing all the dependencies, Flameshot can be built.&lt;/p&gt;
    &lt;p&gt;For the translations to be loaded correctly, the build process needs to be aware of where you want to install Flameshot.&lt;/p&gt;
    &lt;code&gt;# Directory where build files will be placed, may be relative
export BUILD_DIR=build

# Directory prefix where Flameshot will be installed. If you are just building and don't want to
# install, comment this environment variable.
# This excludes the bin/flameshot part of the install,
# e.g. in /opt/flameshot/bin/flameshot, the CMAKE_INSTALL_PREFIX is /opt/flameshot
# This must be an absolute path. Requires CMAKE 3.29.
export CMAKE_INSTALL_PREFIX=/opt/flameshot

# Linux
cmake -S . -B "$BUILD_DIR" \
    &amp;amp;&amp;amp; cmake --build "$BUILD_DIR"

#MacOS
cmake -S . -B "$BUILD_DIR" \
    -DQt6_DIR="$(brew --prefix qt6)/lib/cmake/Qt6" \
    &amp;amp;&amp;amp; cmake --build "$BUILD_DIR"&lt;/code&gt;
    &lt;p&gt;When the &lt;code&gt;cmake --build&lt;/code&gt; command has completed you can launch Flameshot from the &lt;code&gt;project_folder/build/src&lt;/code&gt; folder.&lt;/p&gt;
    &lt;p&gt;Note that if you install from source, there is no uninstaller, so consider installing to a custom directory.&lt;/p&gt;
    &lt;p&gt;Make sure you are using cmake &lt;code&gt;&amp;gt;= 3.29&lt;/code&gt; and build Flameshot with &lt;code&gt;$CMAKE_INSTALL_PREFIX&lt;/code&gt; set to the
installation directory. If this is not done, the translations won't be found when using a custom directory.
Then, run the following:&lt;/p&gt;
    &lt;code&gt;# !Build with CMAKE_INSTALL_PREFIX and use cmake &amp;gt;= 3.29! Using an older cmake will cause
# installation into the default /usr/local dir.

# You may need to run this with privileges
cmake --install "$BUILD_DIR"&lt;/code&gt;
    &lt;code&gt;# You may need to run this with privileges
cmake --install "$BUILD_DIR"&lt;/code&gt;
    &lt;p&gt;https://flameshot.org/docs/guide/faq/&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The main code is licensed under GPLv3&lt;/item&gt;
      &lt;item&gt;The logo of Flameshot is licensed under Free Art License v1.3&lt;/item&gt;
      &lt;item&gt;The button icons are licensed under Apache License 2.0. See: https://github.com/google/material-design-icons&lt;/item&gt;
      &lt;item&gt;The code at capture/capturewidget.cpp is based on https://github.com/ckaiser/Lightscreen/blob/master/dialogs/areadialog.cpp (GPLv2)&lt;/item&gt;
      &lt;item&gt;The code at capture/capturewidget.h is based on https://github.com/ckaiser/Lightscreen/blob/master/dialogs/areadialog.h (GPLv2)&lt;/item&gt;
      &lt;item&gt;I copied a few lines of code from KSnapshot regiongrabber.cpp revision &lt;code&gt;796531&lt;/code&gt;(LGPL)&lt;/item&gt;
      &lt;item&gt;Qt-Color-Widgets taken and modified from https://github.com/mbasaglia/Qt-Color-Widgets (see their license and exceptions in the project) (LGPL/GPL)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Info: If I take code from your project and that implies a relicense to GPLv3, you can reuse my changes with the original previous license of your project applied.&lt;/p&gt;
    &lt;p&gt;This program will not transfer any information to other networked systems unless specifically requested by the user or the person installing or operating it.&lt;/p&gt;
    &lt;p&gt;For Windows binaries, this program uses free code signing provided by SignPath.io, and a certificate by the SignPath Foundation.&lt;/p&gt;
    &lt;p&gt;Code signing is currently a manual process so not every patch release will be signed.&lt;/p&gt;
    &lt;p&gt;If you want to contribute check the CONTRIBUTING.md&lt;/p&gt;
    &lt;p&gt;Thanks to those who have shown interest in the early development process:&lt;/p&gt;
    &lt;p&gt;Thanks to sponsors:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46815297</guid><pubDate>Thu, 29 Jan 2026 19:30:35 +0000</pubDate></item><item><title>Agent-shell: A native Emacs buffer to interact with LLM agents powered by ACP</title><link>https://github.com/xenodium/agent-shell</link><description>&lt;doc fingerprint="2c54ec596999aa9d"&gt;
  &lt;main&gt;
    &lt;p&gt;👉 Support this work via GitHub Sponsors by @xenodium (check out my blog)&lt;/p&gt;
    &lt;p&gt;As you pay for those useful LLM tokens, consider sponsoring development and maintenance of this project. With your help, I can make this effort more sustainable.&lt;/p&gt;
    &lt;p&gt;Thank you!&lt;/p&gt;
    &lt;p&gt;A native Emacs shell to interact with LLM agents powered by ACP (Agent Client Protocol).&lt;/p&gt;
    &lt;p&gt;With agent-shell, you can chat with the likes of Gemini CLI, Claude Code, Auggie, Mistral Vibe, or any other ACP-driven agent.&lt;/p&gt;
    &lt;p&gt;Watch on YouTube&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;agent-shell 0.25 updates.&lt;/item&gt;
      &lt;item&gt;agent-shell 0.17 improvements + MELPA.&lt;/item&gt;
      &lt;item&gt;agent-shell 0.5 improvements.&lt;/item&gt;
      &lt;item&gt;Introducing Emacs agent-shell (powered by ACP).&lt;/item&gt;
      &lt;item&gt;Introducing acp.el.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;acp.el: An ACP (Agent Client Protocol) implementation in Emacs lisp.&lt;/item&gt;
      &lt;item&gt;agent-shell-manager: Tabulated view and management of &lt;code&gt;agent-shell&lt;/code&gt;buffers.&lt;/item&gt;
      &lt;item&gt;agent-shell-sidebar: A sidebar add-on for &lt;code&gt;agent-shell&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;agent-review: Code review interface for &lt;code&gt;agent-shell&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;agent-shell-attention.el: Mode-line attention tracker for &lt;code&gt;agent-shell&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thanks to Lobe Icons for the lovely icons.&lt;/p&gt;
    &lt;p&gt;For Anthropic’s Claude Code, follow Zed’s claude-code-acp instructions, typically something like:&lt;/p&gt;
    &lt;code&gt;npm install -g @zed-industries/claude-code-acp&lt;/code&gt;
    &lt;p&gt;Note: The &lt;code&gt;-g&lt;/code&gt; flag is required to install the binary globally so it’s available in your PATH. After installation, verify it’s available by running &lt;code&gt;which claude-code-acp&lt;/code&gt; in your terminal.&lt;/p&gt;
    &lt;p&gt;For OpenAI’s Codex, install zed/codex-acp and ensure the `codex-acp` executable is in PATH.&lt;/p&gt;
    &lt;p&gt;For Google’s Gemini CLI, be sure to get a recent release supporting the &lt;code&gt;--experimental-acp&lt;/code&gt; flag.&lt;/p&gt;
    &lt;p&gt;For Goose CLI, install goose and ensure the `goose` executable is in PATH.&lt;/p&gt;
    &lt;p&gt;For Cursor agent, install with:&lt;/p&gt;
    &lt;code&gt;npm install -g @blowmage/cursor-agent-acp&lt;/code&gt;
    &lt;p&gt;See https://github.com/blowmage/cursor-agent-acp-npm for details.&lt;/p&gt;
    &lt;p&gt;For Qwen Code, install with:&lt;/p&gt;
    &lt;code&gt;npm install -g @qwen-code/qwen-code@latest&lt;/code&gt;
    &lt;p&gt;See https://github.com/QwenLM/qwen-code for details.&lt;/p&gt;
    &lt;p&gt;For Auggie CLI, install with:&lt;/p&gt;
    &lt;code&gt;npm install -g @augmentcode/auggie&lt;/code&gt;
    &lt;p&gt;See https://docs.augmentcode.com/cli/overview for details.&lt;/p&gt;
    &lt;p&gt;For Mistral Vibe, install with:&lt;/p&gt;
    &lt;code&gt;uv tool install mistral-vibe&lt;/code&gt;
    &lt;p&gt;See https://github.com/mistralai/mistral-vibe for details.&lt;/p&gt;
    &lt;p&gt;For Factory Droid, install the &lt;code&gt;droid-acp&lt;/code&gt; client:&lt;/p&gt;
    &lt;code&gt;npm install -g droid-acp&lt;/code&gt;
    &lt;p&gt;See https://github.com/yaonyan/droid-acp for details.&lt;/p&gt;
    &lt;p&gt;For Pi coding agent, install the &lt;code&gt;pi-acp&lt;/code&gt; adapter:&lt;/p&gt;
    &lt;code&gt;npm install -g pi-acp&lt;/code&gt;
    &lt;p&gt;See https://github.com/svkozak/pi-acp for details.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;agent-shell&lt;/code&gt; is powered by built-in &lt;code&gt;comint-shell&lt;/code&gt;, via shell-maker, available on MELPA.&lt;/p&gt;
    &lt;p&gt;Both agent-shell and its dependency acp.el are now available on MELPA.&lt;/p&gt;
    &lt;p&gt;You can install via:&lt;/p&gt;
    &lt;code&gt;(use-package agent-shell
    :ensure t
    :ensure-system-package
    ;; Add agent installation configs here
    ((claude . "brew install claude-code")
     (claude-code-acp . "npm install -g @zed-industries/claude-code-acp")))&lt;/code&gt;
    &lt;p&gt;This will automatically install the required dependencies (acp.el and shell-maker).&lt;/p&gt;
    &lt;p&gt;If you are using Doom Emacs and would like to use the &lt;code&gt;package!&lt;/code&gt; macro:&lt;/p&gt;
    &lt;code&gt;(package! shell-maker)
(package! acp)
(package! agent-shell)&lt;/code&gt;
    &lt;p&gt;Run &lt;code&gt;doom sync&lt;/code&gt; and restart.&lt;/p&gt;
    &lt;p&gt;Include &lt;code&gt;require&lt;/code&gt; before configuration:&lt;/p&gt;
    &lt;code&gt;(require 'acp)
(require 'agent-shell)
;; rest of config...&lt;/code&gt;
    &lt;p&gt;Configure authentication for the agent providers you want to use.&lt;/p&gt;
    &lt;p&gt;Pass environment variables to the spawned agent process by customizing the `agent-shell-*-environment` variable with `agent-shell-make-environment-variables`. The helper accepts key/value pairs and exports them when the agent starts.&lt;/p&gt;
    &lt;code&gt;(setq agent-shell-anthropic-claude-environment
      (agent-shell-make-environment-variables
       "ANTHROPIC_API_KEY" (auth-source-pass-get "secret" "anthropic-api-key")
       "HTTPS_PROXY" "http://proxy.example.com:8080"))&lt;/code&gt;
    &lt;p&gt;By default, the agent process starts with a minimal environment. To inherit environment variables from the parent Emacs process, use the `:inherit-env t` parameter in `agent-shell-make-environment-variables`:&lt;/p&gt;
    &lt;code&gt;(setenv "ANTHROPIC_API_KEY" (auth-source-pass-get "secret" "anthropic-api-key"))

(setq agent-shell-anthropic-claude-environment
      (agent-shell-make-environment-variables :inherit-env t))&lt;/code&gt;
    &lt;p&gt;This ensures that environment variables like `PATH`, `HOME`, and others from your Emacs session are available to the agent process, while still allowing you to override or add specific variables.&lt;/p&gt;
    &lt;p&gt;You can load environment variables from .env files using the `:load-env` parameter. This supports both single and multiple files:&lt;/p&gt;
    &lt;code&gt;;; Load from a single .env file
(setq agent-shell-anthropic-claude-environment
      (agent-shell-make-environment-variables
       :load-env "~/.env"
       "CUSTOM_VAR" "custom_value"))

;; Load from multiple .env files
(setq agent-shell-anthropic-claude-environment
      (agent-shell-make-environment-variables
       :load-env '("~/.env" ".env.local")
       :inherit-env t))&lt;/code&gt;
    &lt;p&gt;The .env files should contain variables in the format `KEY=value`, with one variable per line. Comments (lines starting with `#`) and empty lines are ignored.&lt;/p&gt;
    &lt;p&gt;For login-based authentication (default):&lt;/p&gt;
    &lt;code&gt;(setq agent-shell-anthropic-authentication
      (agent-shell-anthropic-make-authentication :login t))&lt;/code&gt;
    &lt;p&gt;For API key authentication:&lt;/p&gt;
    &lt;code&gt;;; With string
(setq agent-shell-anthropic-authentication
      (agent-shell-anthropic-make-authentication :api-key "your-anthropic-api-key-here"))

;; With function
(setq agent-shell-anthropic-authentication
      (agent-shell-anthropic-make-authentication
       :api-key (lambda () (auth-source-pass-get "secret" "anthropic-api-key"))))&lt;/code&gt;
    &lt;p&gt;For alternative Anthropic-compatible API endpoints, configure via environment variables:&lt;/p&gt;
    &lt;code&gt;(setq agent-shell-anthropic-claude-environment
      (agent-shell-make-environment-variables
       "ANTHROPIC_BASE_URL" "https://api.moonshot.cn/anthropic"
       "ANTHROPIC_MODEL" "kimi-k2-turbo-preview"
       "ANTHROPIC_SMALL_FAST_MODEL" "kimi-k2-turbo-preview"))&lt;/code&gt;
    &lt;p&gt;For login-based authentication (default):&lt;/p&gt;
    &lt;code&gt;(setq agent-shell-google-authentication
      (agent-shell-google-make-authentication :login t))&lt;/code&gt;
    &lt;p&gt;For API key authentication:&lt;/p&gt;
    &lt;code&gt;;; With string
(setq agent-shell-google-authentication
      (agent-shell-google-make-authentication :api-key "your-google-api-key-here"))

;; With function
(setq agent-shell-google-authentication
      (agent-shell-google-make-authentication
       :api-key (lambda () (auth-source-pass-get "secret" "google-api-key"))))&lt;/code&gt;
    &lt;p&gt;For Vertex AI authentication:&lt;/p&gt;
    &lt;code&gt;(setq agent-shell-google-authentication
      (agent-shell-google-make-authentication :vertex-ai t))&lt;/code&gt;
    &lt;p&gt;For login-based authentication (default):&lt;/p&gt;
    &lt;code&gt;(setq agent-shell-openai-authentication
      (agent-shell-openai-make-authentication :login t))&lt;/code&gt;
    &lt;p&gt;For API key authentication:&lt;/p&gt;
    &lt;code&gt;;; With string
(setq agent-shell-openai-authentication
      (agent-shell-openai-make-authentication :api-key "your-openai-api-key-here"))

;; With function
(setq agent-shell-openai-authentication
      (agent-shell-openai-make-authentication
       :api-key (lambda () (auth-source-pass-get "secret" "openai-api-key"))))&lt;/code&gt;
    &lt;p&gt;For OpenAI API key authentication:&lt;/p&gt;
    &lt;code&gt;;; With string
(setq agent-shell-goose-authentication
      (agent-shell-make-goose-authentication :openai-api-key "your-openai-api-key-here"))

;; With function
(setq agent-shell-goose-authentication
      (agent-shell-make-goose-authentication
       :openai-api-key (lambda () (auth-source-pass-get "secret" "openai-api-key"))))&lt;/code&gt;
    &lt;p&gt;For OAuth login-based authentication:&lt;/p&gt;
    &lt;code&gt;(setq agent-shell-qwen-authentication
      (agent-shell-qwen-make-authentication :login t))&lt;/code&gt;
    &lt;p&gt;For login-based authentication (default):&lt;/p&gt;
    &lt;code&gt;(setq agent-shell-auggie-authentication
      (agent-shell-make-auggie-authentication :login t))&lt;/code&gt;
    &lt;p&gt;For no authentication (when using alternative authentication methods):&lt;/p&gt;
    &lt;code&gt;(setq agent-shell-auggie-authentication
      (agent-shell-make-auggie-authentication :none t))&lt;/code&gt;
    &lt;p&gt;For API key authentication:&lt;/p&gt;
    &lt;code&gt;;; With string
(setq agent-shell-mistral-authentication
      (agent-shell-mistral-make-authentication :api-key "your-mistral-api-key-here"))

;; With function (reusing the API key configured in vibe)
(setq agent-shell-mistral-authentication
      (agent-shell-mistral-make-authentication
       :api-key (lambda ()
	          (string-trim
		   (shell-command-to-string "source ~/.vibe/.env; echo $MISTRAL_API_KEY")))))&lt;/code&gt;
    &lt;p&gt;By default, &lt;code&gt;agent-shell&lt;/code&gt; includes configurations for all supported agents (Claude Code, Gemini CLI, Codex, Goose, Qwen Code, and Auggie). You can customize which agents are available through the &lt;code&gt;agent-shell-agent-configs&lt;/code&gt; variable.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;M-x agent-shell&lt;/code&gt; - Start or reuse any of the known agents.&lt;/p&gt;
    &lt;p&gt;You can select and start any of the known agent shells (see &lt;code&gt;agent-shell-agent-configs&lt;/code&gt;) via the &lt;code&gt;agent-shell&lt;/code&gt; interactive command and enables reusing existing shells when available. With a prefix argument (&lt;code&gt;C-u M-x agent-shell&lt;/code&gt;), it forces starting a new shell session, thus instantiating multiple agent shells.&lt;/p&gt;
    &lt;p&gt;Start a specific agent shell session directly:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;M-x agent-shell-anthropic-start-claude-code&lt;/code&gt;- Start a Claude Code agent session&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;M-x agent-shell-auggie-start-agent&lt;/code&gt;- Start an Auggie agent session&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;M-x agent-shell-openai-start-codex&lt;/code&gt;- Start a Codex agent session&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;M-x agent-shell-google-start-gemini&lt;/code&gt;- Start a Gemini agent session&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;M-x agent-shell-goose-start-agent&lt;/code&gt;- Start a Goose agent session&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;M-x agent-shell-cursor-start-agent&lt;/code&gt;- Start a Cursor agent session&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;M-x agent-shell-mistral-start-vibe&lt;/code&gt;- Start a Mistral Vibe agent session&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;M-x agent-shell-qwen-start&lt;/code&gt;- Start a Qwen Code agent session&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;M-x agent-shell-droid-start-agent&lt;/code&gt;- Start a Factory Droid agent session&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;M-x agent-shell-pi-start-agent&lt;/code&gt;- Start a Pi coding agent session&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can set a default agent to use for all new shells started via &lt;code&gt;agent-shell&lt;/code&gt; like so:&lt;/p&gt;
    &lt;code&gt;(setq agent-shell-preferred-agent-config (agent-shell-anthropic-make-claude-code-config))&lt;/code&gt;
    &lt;p&gt;You can configure MCP servers directly via &lt;code&gt;agent-shell&lt;/code&gt;. This allows you to avoid having to repeat
  configurations across every agent that you use.&lt;/p&gt;
    &lt;code&gt;(setq agent-shell-mcp-servers
      '(((name . "notion")
         (type . "http")
         (headers . [])
         (url . "https://mcp.notion.com/mcp"))))&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;agent-shell&lt;/code&gt; provides rudimentary support for running agents and shell commands in containers.&lt;/p&gt;
    &lt;p&gt;Use &lt;code&gt;agent-shell-container-command-runner&lt;/code&gt; to prefix the command that starts the agent, or a shell command that should be run so it is executed inside the container.&lt;/p&gt;
    &lt;code&gt;(setq agent-shell-container-command-runner '("devcontainer" "exec" "--workspace-folder" "."))&lt;/code&gt;
    &lt;p&gt;For dynamic per-agent containers, provide a function that takes the current agent-shell buffer and returns the command list:&lt;/p&gt;
    &lt;code&gt;(setq agent-shell-container-command-runner
      (lambda (buffer)
        (let ((config (agent-shell-get-config buffer)))
          (pcase (map-elt config :identifier)
            ('claude-code '("docker" "exec" "claude-dev" "--"))
            ('gemini-cli '("docker" "exec" "gemini-dev" "--"))
            (_ '("devcontainer" "exec" "."))))))&lt;/code&gt;
    &lt;p&gt;You can use different containers for different shell sessions, even of the same agent type:&lt;/p&gt;
    &lt;code&gt;(setq agent-shell-container-command-runner
      (lambda (buffer)
        ;; Different container based on project
        (if (string-match "project-a" (buffer-name buffer))
            '("docker" "exec" "project-a-dev" "--")
          '("docker" "exec" "project-b-dev" "--"))))&lt;/code&gt;
    &lt;p&gt;Note that any &lt;code&gt;:environment-variables&lt;/code&gt; you may have passed to &lt;code&gt;acp-make-client&lt;/code&gt; will not apply to the agent process running inside the container. It’s expected to inject environment variables by means of your devcontainer configuration / Dockerfile.&lt;/p&gt;
    &lt;p&gt;Next, set an &lt;code&gt;agent-shell-path-resolver-function&lt;/code&gt; that resolves container paths in the local working directory, and vice versa.
  Agent shell provides the &lt;code&gt;agent-shell--resolve-devcontainer-path&lt;/code&gt; function for use with devcontainers specifically: it reads the &lt;code&gt;workspaceFolder&lt;/code&gt; specified in &lt;code&gt;.devcontainer/devcontainer.json&lt;/code&gt;, or uses the default value of &lt;code&gt;/workspaces/&amp;lt;repository-name&amp;gt;&lt;/code&gt; otherwise.&lt;/p&gt;
    &lt;code&gt;(setq agent-shell-path-resolver-function #'agent-shell--resolve-devcontainer-path)&lt;/code&gt;
    &lt;p&gt;Note that this allows the agent to access files on your local file-system. While care has been taken to restrict access to files in the local working directory, it’s probably possible for a malicious agent to circumvent this restriction.&lt;/p&gt;
    &lt;p&gt;Optional: to prevent the agent running inside the container to access your local file-system altogether and to have it read/modify files inside the container directly, in addition to setting the resolver function, disable the “read/write text file” client capabilities:&lt;/p&gt;
    &lt;code&gt;(setq agent-shell-text-file-capabilities nil)&lt;/code&gt;
    &lt;p&gt;Some minor modes (for example, &lt;code&gt;aggressive-indent-mode&lt;/code&gt;) can interfere with an agent’s edits.  Agent Shell can temporarily disable selected per-buffer minor modes while applying edits.&lt;/p&gt;
    &lt;code&gt;(setopt agent-shell-write-inhibit-minor-modes '(aggressive-indent-mode))&lt;/code&gt;
    &lt;p&gt;All of the above settings can be applied on a per-project basis using directory-local variables.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;C-c C-c&lt;/code&gt;- Interrupt current agent operation&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;TAB and Shift-TAB&lt;/code&gt;- Navigate interactive elements&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Evil users may want to rebind &lt;code&gt;RET&lt;/code&gt; for inserting a new line in &lt;code&gt;insert
  mode&lt;/code&gt; and sending the prompt in &lt;code&gt;normal mode&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Also, when viewing diffs (before accepting changes) it may be annoying having to enter &lt;code&gt;insert mode&lt;/code&gt; to send keys (&lt;code&gt;y/n/p/q/etc&lt;/code&gt;). If this is
  your case, you can make these buffers start in Emacs mode (you can
  always go to Evil modes if you need to with &lt;code&gt;C-z&lt;/code&gt;).&lt;/p&gt;
    &lt;code&gt;(use-package agent-shell
  :config
  ;; Evil state-specific RET behavior: insert mode = newline, normal mode = send
  (evil-define-key 'insert agent-shell-mode-map (kbd "RET") #'newline)
  (evil-define-key 'normal agent-shell-mode-map (kbd "RET") #'comint-send-input)

  ;; Configure *agent-shell-diff* buffers to start in Emacs state
  (add-hook 'diff-mode-hook
	    (lambda ()
	      (when (string-match-p "\\*agent-shell-diff\\*" (buffer-name))
		(evil-emacs-state)))))&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Custom variable&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-agent-configs&lt;/cell&gt;
        &lt;cell&gt;The list of known agent configurations.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-anthropic-authentication&lt;/cell&gt;
        &lt;cell&gt;Configuration for Anthropic authentication.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-anthropic-claude-command&lt;/cell&gt;
        &lt;cell&gt;Command and parameters for the Anthropic Claude client.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-anthropic-claude-environment&lt;/cell&gt;
        &lt;cell&gt;Environment variables for the Anthropic Claude client.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-anthropic-default-model-id&lt;/cell&gt;
        &lt;cell&gt;Default Anthropic model ID.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-anthropic-default-session-mode-id&lt;/cell&gt;
        &lt;cell&gt;Default Anthropic session mode ID.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-auggie-authentication&lt;/cell&gt;
        &lt;cell&gt;Configuration for Auggie authentication.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-auggie-command&lt;/cell&gt;
        &lt;cell&gt;Command and parameters for the Auggie client.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-auggie-environment&lt;/cell&gt;
        &lt;cell&gt;Environment variables for the Auggie client.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-completion-mode-hook&lt;/cell&gt;
        &lt;cell&gt;Hook run after entering or leaving ‘agent-shell-completion-mode’.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-container-command-runner&lt;/cell&gt;
        &lt;cell&gt;Command prefix for executing commands in a container.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-context-sources&lt;/cell&gt;
        &lt;cell&gt;Sources to consider when determining M-x agent-shell automatic context.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-cursor-command&lt;/cell&gt;
        &lt;cell&gt;Command and parameters for the Cursor agent client.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-cursor-environment&lt;/cell&gt;
        &lt;cell&gt;Environment variables for the Cursor agent client.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-display-action&lt;/cell&gt;
        &lt;cell&gt;Display action for agent shell buffers.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-droid-authentication&lt;/cell&gt;
        &lt;cell&gt;Configuration for Factory Droid authentication.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-droid-command&lt;/cell&gt;
        &lt;cell&gt;Command and parameters for the Factory Droid ACP client.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-droid-environment&lt;/cell&gt;
        &lt;cell&gt;Environment variables for the Factory Droid ACP client.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-embed-file-size-limit&lt;/cell&gt;
        &lt;cell&gt;Maximum file size in bytes for embedding with ContentBlock::Resource.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-file-completion-enabled&lt;/cell&gt;
        &lt;cell&gt;Non-nil automatically enables file completion when starting shells.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-github-command&lt;/cell&gt;
        &lt;cell&gt;Command and parameters for the GitHub Copilot agent client.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-github-environment&lt;/cell&gt;
        &lt;cell&gt;Environment variables for the GitHub Copilot agent client.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-google-authentication&lt;/cell&gt;
        &lt;cell&gt;Configuration for Google authentication.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-google-gemini-command&lt;/cell&gt;
        &lt;cell&gt;Command and parameters for the Gemini client.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-google-gemini-environment&lt;/cell&gt;
        &lt;cell&gt;Environment variables for the Google Gemini client.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-goose-authentication&lt;/cell&gt;
        &lt;cell&gt;Configuration for Goose authentication.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-goose-command&lt;/cell&gt;
        &lt;cell&gt;Command and parameters for the Goose client.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-goose-environment&lt;/cell&gt;
        &lt;cell&gt;Environment variables for the Goose client.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-header-style&lt;/cell&gt;
        &lt;cell&gt;Style for agent shell buffer headers.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-highlight-blocks&lt;/cell&gt;
        &lt;cell&gt;Whether or not to highlight source blocks.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-mcp-servers&lt;/cell&gt;
        &lt;cell&gt;List of MCP servers to initialize when creating a new session.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-mistral-authentication&lt;/cell&gt;
        &lt;cell&gt;Configuration for Mistral AI authentication.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-mistral-command&lt;/cell&gt;
        &lt;cell&gt;Command and parameters for the Mistral Vibe client.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-mistral-default-model-id&lt;/cell&gt;
        &lt;cell&gt;Default Mistral AI model ID.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-mistral-default-session-mode-id&lt;/cell&gt;
        &lt;cell&gt;Default Mistral AI session mode ID.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-mistral-environment&lt;/cell&gt;
        &lt;cell&gt;Environment variables for the Mistral Vibe client.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-openai-authentication&lt;/cell&gt;
        &lt;cell&gt;Configuration for OpenAI authentication.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-openai-codex-command&lt;/cell&gt;
        &lt;cell&gt;Command and parameters for the OpenAI Codex client.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-openai-codex-environment&lt;/cell&gt;
        &lt;cell&gt;Environment variables for the OpenAI Codex client.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-opencode-authentication&lt;/cell&gt;
        &lt;cell&gt;Configuration for OpenCode authentication.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-opencode-command&lt;/cell&gt;
        &lt;cell&gt;Command and parameters for the OpenCode client.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-opencode-environment&lt;/cell&gt;
        &lt;cell&gt;Environment variables for the OpenCode client.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-path-resolver-function&lt;/cell&gt;
        &lt;cell&gt;Function for resolving remote paths on the local file-system, and vice versa.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-permission-icon&lt;/cell&gt;
        &lt;cell&gt;Icon displayed when shell commands require permission to execute.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-prefer-viewport-interaction&lt;/cell&gt;
        &lt;cell&gt;Non-nil makes ‘agent-shell’ prefer viewport interaction over shell interaction.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-preferred-agent-config&lt;/cell&gt;
        &lt;cell&gt;Default configuration to use for all new shells.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-qwen-authentication&lt;/cell&gt;
        &lt;cell&gt;Configuration for Qwen Code authentication.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-qwen-command&lt;/cell&gt;
        &lt;cell&gt;Command and parameters for the Qwen Code client.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-qwen-environment&lt;/cell&gt;
        &lt;cell&gt;Environment variables for the Qwen Code client.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-screenshot-command&lt;/cell&gt;
        &lt;cell&gt;The program to use for capturing screenshots.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-section-functions&lt;/cell&gt;
        &lt;cell&gt;Abnormal hook run after overlays are applied (experimental).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-show-config-icons&lt;/cell&gt;
        &lt;cell&gt;Whether to show icons in agent config selection.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-show-welcome-message&lt;/cell&gt;
        &lt;cell&gt;Non-nil to show welcome message.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-text-file-capabilities&lt;/cell&gt;
        &lt;cell&gt;Whether agents are initialized with read/write text file capabilities.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-thought-process-expand-by-default&lt;/cell&gt;
        &lt;cell&gt;Whether thought process sections should be expanded by default.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-thought-process-icon&lt;/cell&gt;
        &lt;cell&gt;Icon displayed during the AI’s thought process.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-tool-use-expand-by-default&lt;/cell&gt;
        &lt;cell&gt;Whether tool use sections should be expanded by default.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-transcript-file-path-function&lt;/cell&gt;
        &lt;cell&gt;Function to generate the full transcript file path.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-ui-mode-hook&lt;/cell&gt;
        &lt;cell&gt;Hook run after entering or leaving ‘agent-shell-ui-mode’.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;agent-shell-user-message-expand-by-default&lt;/cell&gt;
        &lt;cell&gt;Whether user message sections should be expanded by default.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;agent-shell-write-inhibit-minor-modes&lt;/cell&gt;
        &lt;cell&gt;Minor modes to temporarily disable during agent file writes.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Binding&lt;/cell&gt;
        &lt;cell role="head"&gt;Command&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell&lt;/cell&gt;
        &lt;cell&gt;Start or reuse an existing agent shell.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell–display-buffer&lt;/cell&gt;
        &lt;cell&gt;Toggle agent SHELL-BUFFER display.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-anthropic-start-claude-code&lt;/cell&gt;
        &lt;cell&gt;Start an interactive Claude Code agent shell.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-auggie-start-agent&lt;/cell&gt;
        &lt;cell&gt;Start an interactive Auggie agent shell.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-clear-buffer&lt;/cell&gt;
        &lt;cell&gt;Clear the current shell buffer.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-completion-mode&lt;/cell&gt;
        &lt;cell&gt;Toggle agent shell completion with @ or / prefix.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-cursor-start-agent&lt;/cell&gt;
        &lt;cell&gt;Start an interactive Cursor agent shell.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;C-&amp;lt;tab&amp;gt;&lt;/cell&gt;
        &lt;cell&gt;agent-shell-cycle-session-mode&lt;/cell&gt;
        &lt;cell&gt;Cycle through available session modes for the current `agent-shell’ session.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-delete-interaction-at-point&lt;/cell&gt;
        &lt;cell&gt;Delete interaction (request and response) at point.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-droid-start-agent&lt;/cell&gt;
        &lt;cell&gt;Start an interactive Factory Droid agent shell.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-fakes-load-session&lt;/cell&gt;
        &lt;cell&gt;Load and replay a traffic session from file.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-github-start-copilot&lt;/cell&gt;
        &lt;cell&gt;Start an interactive GitHub Copilot agent shell.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-google-start-gemini&lt;/cell&gt;
        &lt;cell&gt;Start an interactive Gemini CLI agent shell.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-goose-start-agent&lt;/cell&gt;
        &lt;cell&gt;Start an interactive Goose agent shell.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-help-menu&lt;/cell&gt;
        &lt;cell&gt;Transient menu for `agent-shell’ commands.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-insert-file&lt;/cell&gt;
        &lt;cell&gt;Insert a file into `agent-shell’.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-insert-shell-command-output&lt;/cell&gt;
        &lt;cell&gt;Execute a shell command and insert output as a code block.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;C-c C-c&lt;/cell&gt;
        &lt;cell&gt;agent-shell-interrupt&lt;/cell&gt;
        &lt;cell&gt;Interrupt in-progress request and reject all pending permissions.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-jump-to-latest-permission-button-row&lt;/cell&gt;
        &lt;cell&gt;Jump to the latest permission button row.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-mistral-start-vibe&lt;/cell&gt;
        &lt;cell&gt;Start an interactive Mistral Vibe agent shell.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-mode&lt;/cell&gt;
        &lt;cell&gt;Major mode for agent shell.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-new-shell&lt;/cell&gt;
        &lt;cell&gt;Start a new agent shell.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;S-&amp;lt;return&amp;gt;&lt;/cell&gt;
        &lt;cell&gt;agent-shell-newline&lt;/cell&gt;
        &lt;cell&gt;Insert a newline, and move to left margin of the new line.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;C-&amp;lt;down&amp;gt; or M-n&lt;/cell&gt;
        &lt;cell&gt;agent-shell-next-input&lt;/cell&gt;
        &lt;cell&gt;Cycle forwards through input history.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;n or TAB&lt;/cell&gt;
        &lt;cell&gt;agent-shell-next-item&lt;/cell&gt;
        &lt;cell&gt;Go to next item.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-next-permission-button&lt;/cell&gt;
        &lt;cell&gt;Jump to the next button.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-open-transcript&lt;/cell&gt;
        &lt;cell&gt;Open the transcript file for the current `agent-shell’ buffer.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-openai-start-codex&lt;/cell&gt;
        &lt;cell&gt;Start an interactive Codex agent shell.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-opencode-start-agent&lt;/cell&gt;
        &lt;cell&gt;Start an interactive OpenCode agent shell.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;C-c C-o&lt;/cell&gt;
        &lt;cell&gt;agent-shell-other-buffer&lt;/cell&gt;
        &lt;cell&gt;Switch to other associated buffer (viewport vs shell).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;C-&amp;lt;up&amp;gt; or M-p&lt;/cell&gt;
        &lt;cell&gt;agent-shell-previous-input&lt;/cell&gt;
        &lt;cell&gt;Cycle backwards through input history, saving input.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;p or &amp;lt;backtab&amp;gt;&lt;/cell&gt;
        &lt;cell&gt;agent-shell-previous-item&lt;/cell&gt;
        &lt;cell&gt;Go to previous item.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-previous-permission-button&lt;/cell&gt;
        &lt;cell&gt;Jump to the previous button.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-prompt-compose&lt;/cell&gt;
        &lt;cell&gt;Compose an `agent-shell’ prompt in a dedicated buffer.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-queue-request&lt;/cell&gt;
        &lt;cell&gt;Queue or immediately send a request depending on shell busy state.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-qwen-start&lt;/cell&gt;
        &lt;cell&gt;Start an interactive Qwen Code CLI agent shell.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-remove-pending-request&lt;/cell&gt;
        &lt;cell&gt;Remove all pending requests or a specific request by REMOVE-INDEX.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;C-x x r&lt;/cell&gt;
        &lt;cell&gt;agent-shell-rename-buffer&lt;/cell&gt;
        &lt;cell&gt;Rename current shell buffer.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-reset-logs&lt;/cell&gt;
        &lt;cell&gt;Reset all log buffers.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-resume-pending-requests&lt;/cell&gt;
        &lt;cell&gt;Resume processing pending requests in the queue.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-run-all-tests&lt;/cell&gt;
        &lt;cell&gt;Run all agent-shell tests in batch mode.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;M-r&lt;/cell&gt;
        &lt;cell&gt;agent-shell-search-history&lt;/cell&gt;
        &lt;cell&gt;Search previous input history.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-send-current-file&lt;/cell&gt;
        &lt;cell&gt;Insert a file into `agent-shell’.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-send-dwim&lt;/cell&gt;
        &lt;cell&gt;Send region or error at point to last accessed shell buffer in project.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-send-file&lt;/cell&gt;
        &lt;cell&gt;Insert a file into `agent-shell’.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-send-other-file&lt;/cell&gt;
        &lt;cell&gt;Prompt to send a file into `agent-shell’.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-send-region&lt;/cell&gt;
        &lt;cell&gt;Send region to last accessed shell buffer in project.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-send-screenshot&lt;/cell&gt;
        &lt;cell&gt;Capture a screenshot and insert it into `agent-shell’.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;C-c RET&lt;/cell&gt;
        &lt;cell&gt;agent-shell-set-session-mode&lt;/cell&gt;
        &lt;cell&gt;Set session mode (if any available).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;C-c C-v&lt;/cell&gt;
        &lt;cell&gt;agent-shell-set-session-model&lt;/cell&gt;
        &lt;cell&gt;Set session model.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;RET&lt;/cell&gt;
        &lt;cell&gt;agent-shell-submit&lt;/cell&gt;
        &lt;cell&gt;Submit current input.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-toggle&lt;/cell&gt;
        &lt;cell&gt;Toggle agent shell display.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-toggle-logging&lt;/cell&gt;
        &lt;cell&gt;Toggle logging.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-ui-backward-block&lt;/cell&gt;
        &lt;cell&gt;Jump to the previous block.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-ui-forward-block&lt;/cell&gt;
        &lt;cell&gt;Jump to the next block.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-ui-mode&lt;/cell&gt;
        &lt;cell&gt;Minor mode for SUI block navigation.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-ui-toggle-fragment-at-point&lt;/cell&gt;
        &lt;cell&gt;Toggle visibility of fragment body at point.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-version&lt;/cell&gt;
        &lt;cell&gt;Show `agent-shell’ mode version.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-view-acp-logs&lt;/cell&gt;
        &lt;cell&gt;View agent shell ACP logs buffer.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-view-traffic&lt;/cell&gt;
        &lt;cell&gt;View agent shell traffic buffer.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-viewport-compose-cancel&lt;/cell&gt;
        &lt;cell&gt;Cancel prompt composition.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-viewport-compose-send&lt;/cell&gt;
        &lt;cell&gt;Send the viewport composed prompt to the agent shell.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-viewport-compose-send-and-kill&lt;/cell&gt;
        &lt;cell&gt;Send the viewport composed prompt to the agent shell and kill compose buffer.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-viewport-compose-send-and-wait-for-response&lt;/cell&gt;
        &lt;cell&gt;Send the viewport composed prompt and display response in viewport.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-viewport-cycle-session-mode&lt;/cell&gt;
        &lt;cell&gt;Cycle through available session modes.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-viewport-edit-mode&lt;/cell&gt;
        &lt;cell&gt;Major mode for composing agent shell prompts.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-viewport-interrupt&lt;/cell&gt;
        &lt;cell&gt;Interrupt active agent shell request.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-viewport-next-item&lt;/cell&gt;
        &lt;cell&gt;Go to next item.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-viewport-next-page&lt;/cell&gt;
        &lt;cell&gt;Show next interaction (request / response).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-viewport-previous-item&lt;/cell&gt;
        &lt;cell&gt;Go to previous item.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-viewport-previous-page&lt;/cell&gt;
        &lt;cell&gt;Show previous interaction (request / response).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-viewport-refresh&lt;/cell&gt;
        &lt;cell&gt;Refresh viewport buffer content with current item from shell.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-viewport-reply&lt;/cell&gt;
        &lt;cell&gt;Reply as a follow-up and compose another prompt/query.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-viewport-set-session-mode&lt;/cell&gt;
        &lt;cell&gt;Set session mode.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-viewport-set-session-model&lt;/cell&gt;
        &lt;cell&gt;Set session model.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;agent-shell-viewport-view-last&lt;/cell&gt;
        &lt;cell&gt;Display the last request/response interaction.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;agent-shell-viewport-view-mode&lt;/cell&gt;
        &lt;cell&gt;Major mode for viewing agent shell prompts (read-only).&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Does the agent support ACP (Agent Client Protocol)? If so, &lt;code&gt;agent-shell&lt;/code&gt; can likely support this agent. Some agents have ACP support built-in (like gemini-cli). Others require a separate ACP package (like claude-code-acp for claude-code). When filing a feature request to add a new agent, please include a link to the project supporting Agent Client Protocol (built-in or otherwise).&lt;/p&gt;
    &lt;p&gt;Agents without ACP support are out of scope for integrating with &lt;code&gt;agent-shell&lt;/code&gt;. Having said that, if you do build an ACP layer like &lt;code&gt;claude-code-acp&lt;/code&gt;, then &lt;code&gt;agent-shell&lt;/code&gt; can work with it.&lt;/p&gt;
    &lt;p&gt;Could be the agent itself missing an Agent Client Protocol feature, &lt;code&gt;agent-shell&lt;/code&gt; missing the feature, or both :) So which one is it? It’s hard to tell unless we look at Agent Client Protocol traffic between the two.&lt;/p&gt;
    &lt;head rend="h2"&gt;How do I view/get Agent Client Protocol traffic?&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;M-x agent-shell-toggle-logging&lt;/code&gt;(make sure logging is ON).&lt;/item&gt;
      &lt;item&gt;Reproduce the issue&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;M-x agent-shell-view-traffic&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Browse through traffic and see if you can spot the issue. For example, if you see a request sent by the agent asking for user permission, but &lt;code&gt;agent-shell&lt;/code&gt; isn’t surfacing this permission, it looks like perhaps &lt;code&gt;agent-shell&lt;/code&gt; is missing a feature.&lt;/p&gt;
    &lt;p&gt;For example, here’s what a session/request_permission request would look like from the traffic viewer.&lt;/p&gt;
    &lt;p&gt;Sometimes including a traffic screenshot in an issue is enough. Other times including the full traffic is needed. From the traffic viewer, you can &lt;code&gt;M-x acp-traffic-save-to&lt;/code&gt; to save as &lt;code&gt;.traffic&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If you’re able to determine the agent is missing a feature (or a bug is present) in their Agent Client Protocol implementation, please file an issue directly with the agent folks. For example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;claude-code-acp: For Claude Code.&lt;/item&gt;
      &lt;item&gt;codex-acp: For Codex.&lt;/item&gt;
      &lt;item&gt;Gemini CLI.&lt;/item&gt;
      &lt;item&gt;Goose.&lt;/item&gt;
      &lt;item&gt;Qwen Code.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Alternatively, if you noticed &lt;code&gt;agent-shell&lt;/code&gt; is missing a feature (or has a bug), please file an agent-shell issue.&lt;/p&gt;
    &lt;p&gt;File in agent-shell, but please try to provide details, so I can determine whether &lt;code&gt;agent-shell&lt;/code&gt; or the agent itself needs work. Traffic data would be very useful here. Provide a screenshot or a .traffic file if you think it’ll help. See how to get ACP traffic.&lt;/p&gt;
    &lt;p&gt;Before implementing new features, please file a feature request first to discuss the proposal. This helps ensure alignment with the project’s direction and prevents unnecessary work.&lt;/p&gt;
    &lt;p&gt;As the maintainer, I must be mindful of all features I accept since I inherit the code to maintain it. Some features may be better suited as separate packages (like agent-shell-sidebar).&lt;/p&gt;
    &lt;p&gt;I’ll gladly promote your package wherever possible.&lt;/p&gt;
    &lt;p&gt;There are lots of ways to accomplish things in elisp. While the following are merely personal preferences, as maintainer, it really simplifies things for me to try to limit the number of ways to accomplish things.&lt;/p&gt;
    &lt;p&gt;This project relies on alists for much of its functionality. Sure, we can also use plists, hashtables, etc.&lt;/p&gt;
    &lt;p&gt;Unless we have a strong argument to use something else, please stick with &lt;code&gt;alists&lt;/code&gt; (and &lt;code&gt;:&lt;/code&gt; keywords).&lt;/p&gt;
    &lt;code&gt;'((:species . "Cat")
  (:name . "Whiskers")
  (:age . 4)
  (:color . "Gray")
  (:favorite-toy . "Feather Wand"))&lt;/code&gt;
    &lt;p&gt;Accessing and working with lists? Please prefer &lt;code&gt;seq.el&lt;/code&gt;, unless we have a strong argument to use an alternative.&lt;/p&gt;
    &lt;code&gt;(setq animals
      (list
       '((:species . "Cat")
         (:name . "Whiskers")
         (:age . 4)
         (:color . "Gray"))
       '((:species . "Dog")
         (:name . "Buddy")
         (:age . 6)
         (:color . "Brown"))))

(seq-first animals)&lt;/code&gt;
    &lt;p&gt;Accessing and working with &lt;code&gt;alists&lt;/code&gt;? Please prefer &lt;code&gt;map.el&lt;/code&gt; unless we have a strong argument to use an alternative.&lt;/p&gt;
    &lt;code&gt;(setq animal (seq-first animals))
(map-elt animal :species)&lt;/code&gt;
    &lt;p&gt;While I’m a fan of &lt;code&gt;cl-defun&lt;/code&gt;, please limit &lt;code&gt;cl&lt;/code&gt; usage to &lt;code&gt;cl-defun&lt;/code&gt; if possible. Nothing against &lt;code&gt;cl-lib&lt;/code&gt;. I’m just limiting the surface and number of idioms I need to keep in my head to maintain the codebase. Often, &lt;code&gt;seq.el&lt;/code&gt; and &lt;code&gt;map.el&lt;/code&gt; can do the job just fine.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;cl-defun&lt;/code&gt;, on the other hand, please do! I’m a fan of named parameters (yay for self-documenting), so use &lt;code&gt;&amp;amp;key&lt;/code&gt; if possible.&lt;/p&gt;
    &lt;code&gt;(cl-defun describe (&amp;amp;key animal)
  "Describe an ANIMAL, which is an alist of properties like :species, :name, :age, :color."
  (message "This is a %d-year-old %s %s named %s."
           (map-elt animal :age 0)
           (map-elt animal :color "Unknown Color")
           (map-elt animal :species "Unknown Species")
           (map-elt animal :name "Unnamed")))

(describe :animal '((:species . "Cat")
                    (:name . "Whiskers")
                    (:age . 4)
                    (:color . "Gray")))&lt;/code&gt;
    &lt;p&gt;Please try to look for a similar feature in the code base and replicate an existing pattern usage if possible.&lt;/p&gt;
    &lt;p&gt;Before submitting a PR, please run:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;M-x checkdoc&lt;/code&gt;- Ensures documentation consistency&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;M-x byte-compile-file&lt;/code&gt;- Identifies compilation warnings&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I’m aware, we’re a bit light on tests, but we started adding some tests. If adding a new feature, please try to add tests.&lt;/p&gt;
    &lt;p&gt;Tests live under the tests directory:&lt;/p&gt;
    &lt;code&gt;ls tests/*tests.el&lt;/code&gt;
    &lt;p&gt;Opening any file under the &lt;code&gt;tests&lt;/code&gt; directory will load the &lt;code&gt;agent-shell-run-all-tests&lt;/code&gt; command.&lt;/p&gt;
    &lt;p&gt;Run tests with &lt;code&gt;M-x agent-shell-run-all-tests&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Made with contrib.rocks.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46815899</guid><pubDate>Thu, 29 Jan 2026 20:11:26 +0000</pubDate></item><item><title>Apple buys Israeli startup Q.ai as the AI race heats up</title><link>https://techcrunch.com/2026/01/29/apple-buys-israeli-startup-q-ai-as-the-ai-race-heats-up/</link><description>&lt;doc fingerprint="a1ec137d5326153c"&gt;
  &lt;main&gt;
    &lt;p&gt;Apple, Meta, and Google are locked in a fierce battle to lead the next wave of AI, and they’ve recently increased their focus on hardware. With its latest acquisition of the AI startup Q.ai, Apple aims to gain an edge, particularly in the audio sector.&lt;/p&gt;
    &lt;p&gt;As first reported by Reuters, Apple has acquired Q.ai, an Israeli startup specializing in imaging and machine learning, particularly technologies that enable devices to interpret whispered speech and enhance audio in noisy environments. Apple has been adding new AI features to its AirPods, including the live translation capability introduced last year.&lt;/p&gt;
    &lt;p&gt;The company has also developed technology that detects subtle facial muscle activity, which could help the tech giant enhance the Vision Pro headset.&lt;/p&gt;
    &lt;p&gt;The Financial Times reported that the deal is valued at nearly $2 billion, making it Apple’s second-largest acquisition to date, after buying Beats Electronics for $3 billion in 2014.&lt;/p&gt;
    &lt;p&gt;Notably, this is the second time CEO Aviad Maizels has sold a company to Apple. In 2013, he sold PrimeSense, a 3D-sensing company that played a key role in Apple’s transition from fingerprint sensors to facial recognition on iPhones.&lt;/p&gt;
    &lt;p&gt;Q.ai launched in 2022 and is backed by Kleiner Perkins, Gradient Ventures, and others. Its founding team, including Maizels and co-founders Yonatan Wexler and Avi Barliya, will join Apple as part of the acquisition.&lt;/p&gt;
    &lt;p&gt;The news comes a few hours ahead of Apple’s first quarterly earnings, in which analysts are estimating revenue at around $138 billion. It’s also expected to be the company’s strongest iPhone sales growth in four years.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46816228</guid><pubDate>Thu, 29 Jan 2026 20:37:01 +0000</pubDate></item><item><title>iPhone 16 Best-Selling Smartphone in 2025; Apple Takes 7 Spots in Top Models</title><link>https://counterpointresearch.com/en/insights/iphone-16-worlds-best-selling-smartphone-in-2025-apple-takes-7-spots-in-top-10-models</link><description>&lt;doc fingerprint="dd54d45cd8b3a1a4"&gt;
  &lt;main&gt;
    &lt;p&gt;Team Counterpoint&lt;/p&gt;
    &lt;p&gt;Counterpoint research is a young and fast growing research firm covering analysis of the tech industry. Coverage areas are connected devices, digital consumer goods, software &amp;amp; applications and other adjacent topics. We provide syndicated research report as well as tailored. Our seminars and workshops for companies and institutions are popular and available on demand. Consulting and customer&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46816378</guid><pubDate>Thu, 29 Jan 2026 20:49:04 +0000</pubDate></item><item><title>Retiring GPT-4o, GPT-4.1, GPT-4.1 mini, and OpenAI o4-mini in ChatGPT</title><link>https://openai.com/index/retiring-gpt-4o-and-older-models/</link><description>&lt;doc fingerprint="2c279e718ffe45f3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Retiring GPT-4o, GPT-4.1, GPT-4.1 mini, and OpenAI o4-mini in ChatGPT&lt;/head&gt;
    &lt;p&gt;On February 13, 2026, alongside the previously announced retirement of GPT‑5 (Instant and Thinking), we will retire GPT‑4o, GPT‑4.1, GPT‑4.1 mini, and OpenAI o4-mini from ChatGPT. In the API, there are no changes at this time&lt;/p&gt;
    &lt;p&gt;While this announcement applies to several older models, GPT‑4o deserves special context.&lt;/p&gt;
    &lt;p&gt;After we first deprecated it and later restored access during the GPT‑5 release, we learned more about how people actually use it day to day. We brought GPT‑4o back after hearing clear feedback from a subset of Plus and Pro users, who told us they needed more time to transition key use cases, like creative ideation, and that they preferred GPT‑4o’s conversational style and warmth.&lt;/p&gt;
    &lt;p&gt;That feedback directly shaped GPT‑5.1 and GPT‑5.2, with improvements to personality, stronger support for creative ideation, and more ways to customize how ChatGPT responds(opens in a new window). You can choose from base styles and tones like Friendly, and controls for things like warmth and enthusiasm. Our goal is to give people more control and customization over how ChatGPT feels to use—not just what it can do.&lt;/p&gt;
    &lt;p&gt;We’re announcing the upcoming retirement of GPT‑4o today because these improvements are now in place, and because the vast majority of usage has shifted to GPT‑5.2, with only 0.1% of users still choosing GPT‑4o each day.&lt;/p&gt;
    &lt;p&gt;More broadly, we’re continuing to improve ChatGPT across areas users have told us need work. This includes further improvements to personality and creativity, as well as addressing unnecessary refusals and overly cautious or preachy responses, with updates coming soon. We’re continuing to make progress toward a version of ChatGPT designed for adults over 18, grounded in the principle of treating adults like adults, and expanding user choice and freedom within appropriate safeguards. To support this, we’ve rolled out age prediction(opens in a new window) for users under 18 in most markets.&lt;/p&gt;
    &lt;p&gt;Changes like this take time to adjust to, and we’ll always be clear about what’s changing and when. We know that losing access to GPT‑4o will feel frustrating for some users, and we didn’t make this decision lightly. Retiring models is never easy, but it allows us to focus on improving the models most people use today.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46816539</guid><pubDate>Thu, 29 Jan 2026 21:02:31 +0000</pubDate></item></channel></rss>