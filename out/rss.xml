<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 17 Sep 2025 15:37:23 +0000</lastBuildDate><item><title>Doom crash after 2.5 years of real-world runtime confirmed on real hardware</title><link>https://lenowo.org/viewtopic.php?t=31</link><description>&lt;doc fingerprint="dc4f80d558235299"&gt;
  &lt;main&gt;
    &lt;p&gt;Two and a half years ago, I started my now longest real-world software experiment. I had read an article about how DOOMs engine works and noticed how a variable for tracking the demo kept being incremented even after the next demo started. This variable was compared with a second one storing its previous value. The issue here being, each incrementation would cause the variable to slowly get closer to an overflow, realistically this would never happen in a normal scenario, although it got me curious on just how long it would take until the game would crash due to this.&lt;lb/&gt; I did a few calculations, I don't remember the specifics of it sadly as it has been over two years since that point and I sadly did not document it back then (or I did, but on a partition I no longer have access to) but I remember having gotten roughly 2 1/2 years of possible runtime before an overflow. Obviously, I wanted to know if this would actually happen in the real game on real hardware.&lt;lb/&gt; So I set up DOOM on a small PDA, powered through a DIY 18650 based UPS which itself was connected to the USB port of my router for a constant 5V supply. I left the system running and mostly forgot about it.&lt;lb/&gt; ... Until today when I noticed a pop-up appearing on the device, not long ago from posting this to the board. The game had crashed, only hours after the two and a half year mark, proving that the variable did indeed overflow and cause the expected hard crash of the game: &lt;/p&gt;
    &lt;head rend="h2"&gt;DOOM crash after 2.5 years of real-world runtime confirmed on real hardware&lt;/head&gt;
    &lt;head rend="h3"&gt;DOOM crash after 2.5 years of real-world runtime confirmed on real hardware&lt;/head&gt;
    &lt;p&gt;~-~-~ MSD - Making your old devices useful again since 2022! ~-~-~&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45268269</guid><pubDate>Tue, 16 Sep 2025 21:24:23 +0000</pubDate></item><item><title>AMD Open Source Driver for Vulkan project is discontinued</title><link>https://github.com/GPUOpen-Drivers/AMDVLK/discussions/416</link><description>&lt;doc fingerprint="8fdbcb577bc7b5ab"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AMDVLK open-source project is discontinued #416&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;In a move to streamline development and strengthen our commitment to the open-source community, AMD is unifying its Linux Vulkan driver strategy and has decided to discontinue the AMDVLK open-source project, throwing our full support behind the RADV driver as the officially supported open-source Vulkan driver for Radeon™ graphics adapters.&lt;/p&gt;
          &lt;p&gt;This consolidation allows us to focus our resources on a single, high-performance codebase that benefits from the incredible work of the entire open-source community. We invite developers and users alike to utilize the RADV driver and contribute to its future.&lt;/p&gt;
          &lt;p&gt;We are excited about this focused path forward and are committed to the continued success of open-source Vulkan on Radeon.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;head rend="h2"&gt;Replies: 6 comments 8 replies&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;What does this mean for AMDPAL and in particular ROCm on Windows?&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;To clarify, does this mean that AMD will be allocating more engineering resources towards RADV?&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Why pal also got archieved?&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Hi all. that is, AMDVLK is no more? and then which Vulkan driver should I use?&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Is this a sign that RADV might come to Windows in the future? The Vulkan driver for Windows is based on AMDVLK...&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;How about the pro vulkan driver?&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45270087</guid><pubDate>Wed, 17 Sep 2025 00:31:04 +0000</pubDate></item><item><title>I got the highest score on ARC-AGI again swapping Python for English</title><link>https://jeremyberman.substack.com/p/how-i-got-the-highest-score-on-arc-agi-again</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45270649</guid><pubDate>Wed, 17 Sep 2025 01:53:47 +0000</pubDate></item><item><title>GNU Midnight Commander</title><link>https://midnight-commander.org/</link><description>&lt;doc fingerprint="cc41321c83a3eac7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Welcome to Midnight Commander&lt;/head&gt;
    &lt;p&gt;GNU Midnight Commander (or &lt;code&gt;mc&lt;/code&gt;) is a visual, dual-pane file manager. It is released under the GNU General Public License and therefore qualifies as Free Software.&lt;/p&gt;
    &lt;p&gt;Midnight Commander is a feature-rich, full-screen, text-mode application that allows you to copy, move, and delete files and entire directory trees, search for files, and execute commands in the subshell. Internal viewer, editor and diff viewer are included.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;mc&lt;/code&gt; uses versatile text interface libraries such as ncurses or S-Lang, which allows it to work on a regular console, inside an X Window terminal, over &lt;code&gt;ssh&lt;/code&gt; connections, and in all kinds of remote shells.&lt;/p&gt;
    &lt;head rend="h2"&gt;Installation&lt;/head&gt;
    &lt;p&gt;The easiest way to install &lt;code&gt;mc&lt;/code&gt; is to use your system package manager:&lt;/p&gt;
    &lt;code&gt;# apt-get install mc
&lt;/code&gt;
    &lt;code&gt;# dnf install mc
&lt;/code&gt;
    &lt;code&gt;# pkg install mc
&lt;/code&gt;
    &lt;code&gt;% brew install midnight-commander
&lt;/code&gt;
    &lt;p&gt;Our source releases are kindly mirrored by OSU OSL. Our canonical repository is hosted on GitHub. See the Source code page for details.&lt;/p&gt;
    &lt;head rend="h2"&gt;Documentation&lt;/head&gt;
    &lt;p&gt;The primary way to learn about &lt;code&gt;mc&lt;/code&gt; is to use the context-sensitive online help available via F1.&lt;/p&gt;
    &lt;p&gt;We also have extensive manual pages, which are the primary source of official documentation:&lt;/p&gt;
    &lt;code&gt;$ man mc
&lt;/code&gt;
    &lt;p&gt;... or read the latest development version online.&lt;/p&gt;
    &lt;code&gt;$ man mcedit
&lt;/code&gt;
    &lt;p&gt;... or read the latest development version online.&lt;/p&gt;
    &lt;code&gt;$ man mcview
&lt;/code&gt;
    &lt;p&gt;... or read the latest development version online.&lt;/p&gt;
    &lt;code&gt;$ man mcdiff
&lt;/code&gt;
    &lt;p&gt;... or read the latest development version online.&lt;/p&gt;
    &lt;head rend="h2"&gt;Color schemes&lt;/head&gt;
    &lt;p&gt;Midnight Commander supports theming! Check out the skins that come with the distribution or develop your own:&lt;/p&gt;
    &lt;head rend="h2"&gt;Contributing &amp;amp; support&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For support, see the Communication page.&lt;/item&gt;
      &lt;item&gt;To contribute to &lt;code&gt;mc&lt;/code&gt;, proceed to the "Development" section.&lt;/item&gt;
      &lt;item&gt;Release notes for the development version are collected on the wiki.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45271481</guid><pubDate>Wed, 17 Sep 2025 03:54:06 +0000</pubDate></item><item><title>The Asus Gaming Laptop ACPI Firmware Bug: A Deep Technical Investigation</title><link>https://github.com/Zephkek/Asus-ROG-Aml-Deep-Dive</link><description>&lt;doc fingerprint="a41ebc5c01b4858a"&gt;
  &lt;main&gt;
    &lt;p&gt;You own a high-end ASUS ROG laptop perhaps a Strix, Scar, or Zephyrus. It's specifications are impressive: an RTX 30/40 series GPU, a top-tier Intel processor, and plenty of RAM. Yet, it stutters during basic tasks like watching a YouTube video, audio crackles and pops on Discord calls, the mouse cursor freezes for a split second, just long enough to be infuriating.&lt;/p&gt;
    &lt;p&gt;You've likely tried all the conventional fixes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Updating every driver imaginable, multiple times.&lt;/item&gt;
      &lt;item&gt;Performing a "clean" reinstallation of Windows.&lt;/item&gt;
      &lt;item&gt;Disabling every conceivable power-saving option.&lt;/item&gt;
      &lt;item&gt;Manually tweaking processor interrupt affinities.&lt;/item&gt;
      &lt;item&gt;Following convoluted multi-step guides from Reddit threads.&lt;/item&gt;
      &lt;item&gt;Even installing Linux, only to find the problem persists.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If none of that worked, it's because the issue isn't with the operating system or a driver. The problem is far deeper, embedded in the machine's firmware, the BIOS.&lt;/p&gt;
    &lt;p&gt;The first tool in any performance investigator's toolkit for these symptoms is LatencyMon. It acts as a canary in the coal mine for system-wide latency issues. On an affected ASUS Zephyrus M16, the results are immediate and damning:&lt;/p&gt;
    &lt;code&gt;CONCLUSION
Your system appears to be having trouble handling real-time audio and other tasks. 
You are likely to experience buffer underruns appearing as drop outs, clicks or pops.

HIGHEST MEASURED INTERRUPT TO PROCESS LATENCY
Highest measured interrupt to process latency (μs):   65,816.60
Average measured interrupt to process latency (μs):   23.29

HIGHEST REPORTED ISR ROUTINE EXECUTION TIME
Highest ISR routine execution time (μs):              536.80
Driver with highest ISR routine execution time:       ACPI.sys

HIGHEST REPORTED DPC ROUTINE EXECUTION TIME  
Highest DPC routine execution time (μs):              5,998.83
Driver with highest DPC routine execution time:       ACPI.sys
&lt;/code&gt;
    &lt;p&gt;The data clearly implicates &lt;code&gt;ACPI.sys&lt;/code&gt;. However, the per-CPU data reveals a more specific pattern:&lt;/p&gt;
    &lt;code&gt;CPU 0 Interrupt cycle time (s):                       208.470124
CPU 0 ISR highest execution time (μs):                536.804674
CPU 0 DPC highest execution time (μs):                5,998.834725
CPU 0 DPC total execution time (s):                   90.558238
&lt;/code&gt;
    &lt;p&gt;CPU 0 is taking the brunt of the impact, spending over 90 seconds processing interrupts while other cores remain largely unaffected. This isn't a failure of load balancing; it's a process locked to a single core.&lt;/p&gt;
    &lt;p&gt;A similar test on a Scar 15 from 2022 shows the exact same culprit: high DPC latency originating from &lt;code&gt;ACPI.sys&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;It's easy to blame a Windows driver, but &lt;code&gt;ACPI.sys&lt;/code&gt; is not a typical driver. It primarily functions as an interpreter for ACPI Machine Language (AML), the code provided by the laptop's firmware (BIOS). If &lt;code&gt;ACPI.sys&lt;/code&gt; is slow, it's because the firmware is feeding it inefficient or flawed AML code to execute. These slowdowns are often triggered by General Purpose Events (GPEs) and traffic from the Embedded Controller (EC). To find the true source, we must dig deeper.&lt;/p&gt;
    &lt;p&gt;To understand what &lt;code&gt;ACPI.sys&lt;/code&gt; is doing during these latency spikes, we can use Event Tracing for Windows (ETW) to capture detailed logs from the ACPI providers.&lt;/p&gt;
    &lt;code&gt;# Find the relevant ACPI ETW providers
logman query providers | findstr /i acpi
# This returns two key providers:
# Microsoft-Windows-Kernel-Acpi {C514638F-7723-485B-BCFC-96565D735D4A}
# Microsoft-ACPI-Provider {DAB01D4D-2D48-477D-B1C3-DAAD0CE6F06B}

# Start a comprehensive trace session
logman start ACPITrace -p {DAB01D4D-2D48-477D-B1C3-DAAD0CE6F06B} 0xFFFFFFFF 5 -o C:\Temp\acpi.etl -ets
logman update ACPITrace -p {C514638F-7723-485B-BCFC-96565D735D4A} 0xFFFFFFFF 5 -ets

# Then once we're done we can stop the trace and check the etl file and save the data in csv format aswell.
logman stop ACPITrace -ets
tracerpt C:\Temp\acpi.etl -o C:\Temp\acpi_events.csv -of CSV&lt;/code&gt;
    &lt;p&gt;Analyzing the resulting trace file in the Windows Performance Analyzer reveals a crucial insight. The spikes aren't random; they are periodic, occurring like clockwork every 30 to 60 seconds.&lt;/p&gt;
    &lt;p&gt;Random interruptions often suggest hardware faults or thermal throttling. A perfectly repeating pattern points to a systemic issue, a timer or a scheduled event baked into the system's logic.&lt;/p&gt;
    &lt;p&gt;The raw event data confirms this pattern:&lt;/p&gt;
    &lt;code&gt;Clock-Time (100ns),        Event,                      Kernel(ms), CPU
134024027290917802,       _GPE._L02 started,          13.613820,  0
134024027290927629,       _SB...BAT0._STA started,    0.000000,   4
134024027290932512,       _GPE._L02 finished,         -,          6
&lt;/code&gt;
    &lt;p&gt;The first event, &lt;code&gt;_GPE._L02&lt;/code&gt;, is an interrupt handler that takes 13.6 milliseconds to execute. For a high-priority interrupt, this is an eternity and is catastrophic for real-time system performance.&lt;/p&gt;
    &lt;p&gt;Deeper in the trace, another bizarre behavior emerges; the system repeatedly attempts to power the discrete GPU on and off, even when it's supposed to be permanently active.&lt;/p&gt;
    &lt;code&gt;Clock-Time,                Event,                    Duration
134024027315051227,       _SB.PC00.GFX0._PS0 start, 278μs     # GPU Power On
134024027315155404,       _SB.PC00.GFX0._DOS start, 894μs     # Display Output Switch
134024027330733719,       _SB.PC00.GFX0._PS3 start, 1364μs    # GPU Power Off
[~15 seconds later]
134024027607550064,       _SB.PC00.GFX0._PS0 start, 439μs     # Power On Again!
134024027607657368,       _SB.PC00.GFX0._DOS start, 1079μs    # Display Output Switch
134024027623134006,       _SB.PC00.GFX0._PS3 start, 394μs     # Power Off Again!
...
&lt;/code&gt;
    &lt;p&gt;This power cycling is nonsensical because the laptop is configured for a scenario where it is impossible: The system is in Ultimate Mode (via a MUX switch) with an external display connected.&lt;/p&gt;
    &lt;p&gt;In this mode:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The discrete NVIDIA GPU (dGPU) is the only active graphics processor.&lt;/item&gt;
      &lt;item&gt;The integrated Intel GPU (iGPU) is completely powered down and bypassed.&lt;/item&gt;
      &lt;item&gt;The dGPU is wired directly to the internal and external displays.&lt;/item&gt;
      &lt;item&gt;There is no mechanism for switching between GPUs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Yet, the firmware ignores MUX state nudging the iGPU path (GFX0) and, worse, engaging dGPU cut/notify logic (PEGP/PEPD) every 15–30 seconds. The dGPU in mux mode isn't just "preferred" - it's the ONLY path to the display. There's no fallback, and no alternative. When the firmware sends &lt;code&gt;_PS3&lt;/code&gt; (power off), it's attempting something architecturally impossible.&lt;/p&gt;
    &lt;p&gt;Most of the time, hardware sanity checks refuse these nonsensical commands, but even failed attempts introduce latency spikes causing audio dropouts, input lag, and accumulating performance degradation. Games freeze mid-session, videos buffer indefinitely, system responsiveness deteriorates until restart.&lt;/p&gt;
    &lt;p&gt;Sometimes, under specific thermal conditions or race conditions, the power-down actually succeeds. When the firmware manages to power down the GPU that's driving the display, the sequence is predictable and catastrophic:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Firmware OFF attempt - cuts the dgpu path via PEG1.DGCE&lt;/item&gt;
      &lt;item&gt;Hardware complies - safety checks fail or timing aligns&lt;/item&gt;
      &lt;item&gt;Display signal cuts - monitors go black&lt;/item&gt;
      &lt;item&gt;User input triggers wake - mouse/keyboard activity&lt;/item&gt;
      &lt;item&gt;Windows calls &lt;code&gt;PowerOnMonitor()&lt;/code&gt;- attempt display recovery&lt;/item&gt;
      &lt;item&gt;NVIDIA driver executes &lt;code&gt;_PS0&lt;/code&gt;- GPU power on command&lt;/item&gt;
      &lt;item&gt;GPU enters impossible state - firmware insists OFF, Windows needs ON&lt;/item&gt;
      &lt;item&gt;Driver thread blocks indefinitely - waiting for GPU response&lt;/item&gt;
      &lt;item&gt;30-second watchdog expires - Windows gives up&lt;/item&gt;
      &lt;item&gt;System crashes with BSOD&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;5: kd&amp;gt; !analyze -v
*******************************************************************************
*                                                                             *
*                        Bugcheck Analysis                                    *
*                                                                             *
*******************************************************************************

WIN32K_POWER_WATCHDOG_TIMEOUT (19c)
Win32k did not turn the monitor on in a timely manner.
Arguments:
Arg1: 0000000000000050, Calling monitor driver to power on.
Arg2: ffff8685b1463080, Pointer to the power request worker thread.
Arg3: 0000000000000000
Arg4: 0000000000000000
...
STACK_TEXT:  
fffff685`3a767130 fffff800`94767be0     : 00000000`00000047 00000000`00000000 00000000`00000000 00000000`00000000 : nt!KiSwapContext+0x76
fffff685`3a767270 fffff800`94726051     : ffff8685`b1463080 00000027`00008b94 fffff685`3a767458 fffff800`00000000 : nt!KiSwapThread+0x6a0
fffff685`3a767340 fffff800`94724ed3     : fffff685`00000000 00000000`00000043 00000000`00000002 0000008a`fbf50968 : nt!KiCommitThreadWait+0x271
fffff685`3a7673e0 fffff800`9471baf2     : fffff685`3a7675d0 02000000`0000001b 00000000`00000000 fffff800`94724500 : nt!KeWaitForSingleObject+0x773
fffff685`3a7674d0 fffff800`9471b7d5     : ffff8685`9cbec810 fffff685`3a7675b8 00000000`00010224 fffff800`00000003 : nt!ExpWaitForFastResource+0x92
fffff685`3a767580 fffff800`9471b49d     : 00000000`00000000 ffff8685`9cbec850 ffff8685`b1463080 00000000`00000000 : nt!ExpAcquireFastResourceExclusiveSlow+0x1e5
fffff685`3a767630 fffff800`28faca9b     : fffff800`262ee9c8 00000000`00000003 ffff8685`9cbec810 02000000`00000065 : nt!ExAcquireFastResourceExclusive+0x1bd
fffff685`3a767690 fffff800`28facbe5     : ffff8685`b31de000 00000000`00000000 ffffd31d`9a05244f 00000000`00000000 : win32kbase!&amp;lt;lambda_63b61c2369133a205197eda5bd671ee7&amp;gt;::&amp;lt;lambda_invoker_cdecl&amp;gt;+0x2b
fffff685`3a7676c0 fffff800`28e5f864     : ffffad0c`94d10878 fffff685`3a767769 ffffad0c`94d10830 ffff8685`b31de000 : win32kbase!UserCritInternal::`anonymous namespace'::EnterCritInternalEx+0x4d
fffff685`3a7676f0 fffff800`28e5f4ef     : 00000000`00000000 00000000`00000000 fffff800`262ee9c8 00000000`00000000 : win32kbase!DrvSetWddmDeviceMonitorPowerState+0x354
fffff685`3a7677d0 fffff800`28e2abab     : ffff8685`b31de000 00000000`00000000 ffff8685`b31de000 00000000`00000000 : win32kbase!DrvSetMonitorPowerState+0x2f
fffff685`3a767800 fffff800`28ef22fa     : 00000000`00000000 fffff685`3a7678d9 00000000`00000001 00000000`00000001 : win32kbase!PowerOnMonitor+0x19b
fffff685`3a767870 fffff800`28ef13dd     : ffff8685`94a40700 ffff8685`a2eb31d0 00000000`00000001 00000000`00000020 : win32kbase!xxxUserPowerEventCalloutWorker+0xaaa
fffff685`3a767940 fffff800`4bab21c2     : ffff8685`b1463080 fffff685`3a767aa0 00000000`00000000 00000000`00000020 : win32kbase!xxxUserPowerCalloutWorker+0x13d
fffff685`3a7679c0 fffff800`26217f3a     : 00000000`00000000 00000000`00000000 00000000`00000000 00000000`00000000 : win32kfull!NtUserUserPowerCalloutWorker+0x22
fffff685`3a7679f0 fffff800`94ab8d55     : 00000000`000005bc 00000000`00000104 ffff8685`b1463080 00000000`00000000 : win32k!NtUserUserPowerCalloutWorker+0x2e
fffff685`3a767a20 00007ff8`ee71ca24     : 00000000`00000000 00000000`00000000 00000000`00000000 00000000`00000000 : nt!KiSystemServiceCopyEnd+0x25
000000cc`d11ffbc8 00000000`00000000     : 00000000`00000000 00000000`00000000 00000000`00000000 00000000`00000000 : 0x00007ff8`ee71ca24

...
&lt;/code&gt;
    &lt;p&gt;The crash dump confirms the thread is stuck in &lt;code&gt;win32kbase!DrvSetWddmDeviceMonitorPowerState&lt;/code&gt;, waiting for the NVIDIA driver to respond. It can't because it's caught between a confused power state, windows wanting to turn on the GPU while the firmware is arming the GPU cut off.&lt;/p&gt;
    &lt;p&gt;GPEs are the firmware's mechanism for signaling hardware events to the operating system. They are essentially hardware interrupts that trigger the execution of ACPI code. The trace data points squarely at &lt;code&gt;_GPE._L02&lt;/code&gt; as the source of our latency.&lt;/p&gt;
    &lt;p&gt;A closer look at the timing reveals a consistent and problematic pattern:&lt;/p&gt;
    &lt;code&gt;_GPE._L02 Event Analysis from ROG Strix Trace:

Event 1 @ Clock 134024027290917802
  Duration: 13,613,820 ns (13.61ms)
  Triggered: Battery and AC adapter status checks

Event 2 @ Clock 134024027654496591  
  Duration: 13,647,255 ns (13.65ms)
  Triggered: Battery and AC adapter status checks
  
Event 3 @ Clock 134024028048493318
  Duration: 13,684,515 ns (13.68ms)  
  Triggered: Battery and AC adapter status checks

Interval between events: ~36-39 seconds
Consistency: The duration is remarkably stable and the interval is periodic.
&lt;/code&gt;
    &lt;p&gt;Every single time the lengthy &lt;code&gt;_GPE._L02&lt;/code&gt; event fires, it triggers the exact same sequence of ACPI method calls.&lt;/p&gt;
    &lt;p&gt;The pattern is undeniable:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A hardware interrupt fires &lt;code&gt;_GPE._L02&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The handler executes methods to check battery status.&lt;/item&gt;
      &lt;item&gt;Shortly thereafter, the firmware attempts to change the GPU's power state.&lt;/item&gt;
      &lt;item&gt;The system runs normally for about 30-60 seconds.&lt;/item&gt;
      &lt;item&gt;The cycle repeats.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To analyze the code responsible for this behavior, we must extract and decompile the ACPI tables provided by the BIOS to the operating system.&lt;/p&gt;
    &lt;code&gt;# Extract all ACPI tables into binary .dat files
acpidump -b

# Output includes:
# DSDT.dat - The main Differentiated System Description Table
# SSDT1.dat ... SSDT17.dat - Secondary System Description Tables

# Decompile the main table into human-readable ACPI Source Language (.dsl)
iasl -d DSDT.dsl&lt;/code&gt;
    &lt;p&gt;This decompiled ASL provides a direct view into the firmware's executable logic. It is a precise representation of the exact instructions that the ACPI.sys driver is fed by the firmware and executes at the highest privilege level within the Windows kernel. Any logical flaws found in this code are the direct cause of the system's behavior.&lt;/p&gt;
    &lt;p&gt;Searching the decompiled &lt;code&gt;DSDT.dsl&lt;/code&gt; file, we find the definition for our problematic GPE handler:&lt;/p&gt;
    &lt;code&gt;Scope (_GPE)
{
    Method (_L02, 0, NotSerialized)  // _Lxx: Level-Triggered GPE
    {
        \_SB.PC00.LPCB.ECLV ()
    }
}&lt;/code&gt;
    &lt;p&gt;This code is simple: when the &lt;code&gt;_L02&lt;/code&gt; interrupt occurs, it calls a single method, &lt;code&gt;ECLV&lt;/code&gt;. The "L" prefix in &lt;code&gt;_L02&lt;/code&gt; signifies that this is a level-triggered interrupt, meaning it will continue to fire as long as the underlying hardware condition is active. This is a critical detail.&lt;/p&gt;
    &lt;p&gt;Following the call to &lt;code&gt;ECLV()&lt;/code&gt;, we uncover a deeply flawed implementation that is the direct cause of the system-wide stuttering.&lt;/p&gt;
    &lt;code&gt;Method (ECLV, 0, NotSerialized)  // Starting at line 099244
{
    // Main loop - continues while events exist OR sleep events are pending
    // AND we haven't exceeded our time budget (TI3S &amp;lt; 0x78)
    While (((CKEV() != Zero) || (SLEC != Zero)) &amp;amp;&amp;amp; (TI3S &amp;lt; 0x78))
    {
        Local1 = One
        While (Local1 != Zero)
        {
            Local1 = GEVT()    // Get next event from queue
            LEVN (Local1)      // Process the event
            TIMC += 0x19       // Increment time counter by 25
            
            // This is where it gets really bad
            If ((SLEC != Zero) &amp;amp;&amp;amp; (Local1 == Zero))
            {
                // No events but sleep events pending
                If (TIMC == 0x19)
                {
                    Sleep (0x64)    // Sleep for 100 milliseconds!!!
                    TIMC = 0x64     // Set time counter to 100
                    TI3S += 0x04    // Increment major counter by 4
                }
                Else
                {
                    Sleep (0x19)    // Sleep for 25 milliseconds!!!
                    TI3S++          // Increment major counter by 1
                }
            }
        }
    }
    
    // Here's where it gets even worse
    If (TI3S &amp;gt;= 0x78)  // If we hit our time budget (120)
    {
        TI3S = Zero
        If (EEV0 == Zero)
        {
            EEV0 = 0xFF    // Force another event to be pending!
        }
    }
}&lt;/code&gt;
    &lt;p&gt;This short block of code violates several fundamental principles of firmware and kernel programming.&lt;/p&gt;
    &lt;p&gt;Wtf 1: Sleeping in an Interrupt Context&lt;/p&gt;
    &lt;code&gt;Sleep (0x64)    // 100ms sleep
Sleep (0x19)    // 25ms sleep&lt;/code&gt;
    &lt;p&gt;An interrupt handler runs at a very high priority to service hardware requests quickly. The &lt;code&gt;Sleep()&lt;/code&gt; function completely halts the execution of the CPU core it is running on (CPU 0 in this case). While CPU 0 is sleeping, it cannot:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Process any other hardware interrupts.&lt;/item&gt;
      &lt;item&gt;Allow the kernel to schedule other threads.&lt;/item&gt;
      &lt;item&gt;Update system timers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;Clarification: These Sleep() calls live in the ACPI GPE handling path for the GPE L02, these calls get executed at PASSIVE_LEVEL after the SCI/GPE is acknowledged so it's not a raw ISR (because i don't think windows will even allow that) but analyzing this further while the control method runs the GPE stays masked and the ACPI/EC work is serialized. With the Sleep() calls inside that path and the self rearm it seems to have the effect of making ACPI.sys get tied up in long periodic bursts (often on CPU 0) which still have the same effect on the system.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Wtf 2: Time-Sliced Interrupt Processing The entire loop is designed to run for an extended period, processing events in batches. It's effectively a poorly designed task scheduler running inside an interrupt handler, capable of holding a CPU core hostage for potentially seconds at a time.&lt;/p&gt;
    &lt;p&gt;Wtf 3: Self-Rearming Interrupt&lt;/p&gt;
    &lt;code&gt;If (EEV0 == Zero)
{
    EEV0 = 0xFF    // Forces all EC event bits on
}&lt;/code&gt;
    &lt;p&gt;This logic ensures that even if the Embedded Controller's event queue is empty, the code will create a new, artificial event. This guarantees that another interrupt will fire shortly after, creating the perfectly periodic pattern of ACPI spikes observed in the traces.&lt;/p&gt;
    &lt;p&gt;The LEVN() method takes an event and routes it:&lt;/p&gt;
    &lt;code&gt;Method (LEVN, 1, NotSerialized)
  {
      If ((Arg0 != Zero))
      {
          MBF0 = Arg0
          P80B = Arg0
          Local6 = Match (LEGA, MEQ, Arg0, MTR, Zero, Zero)
          If ((Local6 != Ones))
          {
              LGPA (Local6)
          }
      }
  }
&lt;/code&gt;
    &lt;p&gt;The LGPA() method is a giant switch statement handling different events:&lt;/p&gt;
    &lt;code&gt;Method (LGPA, 1, Serialized)  // Line 098862
{
    Switch (ToInteger (Arg0))
    {
        Case (Zero)  // Most common case - power event
        {
            DGD2 ()       // GPU-related function
            ^EC0._QA0 ()  // EC query method
            PWCG ()       // Power change - this is our battery polling
        }
        
        Case (0x18)  // GPU-specific event
        {
            If (M6EF == One)
            {
                Local0 = 0xD2
            }
            Else
            {
                Local0 = 0xD1
            }
            NOD2 (Local0)  // Notify GPU driver
        }
        
        Case (0x1E)  // Another GPU event
        {
            Notify (^^PEG1.PEGP, 0xD5)  // Direct GPU notification
            ROCT = 0x55                  // Sets flag for follow-up
        }
       
    }
}&lt;/code&gt;
    &lt;p&gt;This shows a direct link: a GPE fires, and the dispatch logic calls functions related to battery polling and GPU notifications.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;PWCG()&lt;/code&gt; method, called by multiple event types, is responsible for polling the battery and AC adapter status.&lt;/p&gt;
    &lt;code&gt;Method (PWCG, 0, NotSerialized)
{
    Notify (ADP0, Zero)      // Tell OS to check the AC adapter
    ^BAT0._BST ()            // Execute the Battery Status method
    Notify (BAT0, 0x80)      // Tell OS the battery status has changed
    ^BAT0._BIF ()            // Execute the Battery Information method  
    Notify (BAT0, 0x81)      // Tell OS the battery info has changed
}&lt;/code&gt;
    &lt;p&gt;Which we can see here:&lt;/p&gt;
    &lt;p&gt;Each of these operations requires communication with the Embedded Controller, adding to the workload inside the already-stalled interrupt handler.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;NOD2()&lt;/code&gt; method sends notifications to the GPU driver.&lt;/p&gt;
    &lt;code&gt;Method (NOD2, 1, Serialized)
{
    If ((Arg0 != DNOT))
    {
        DNOT = Arg0
        Notify (^^PEG1.PEGP, Arg0)
    }

    If ((ROCT == 0x55))
    {
        ROCT = Zero
        Notify (^^PEG1.PEGP, 0xD1) // Hardware-Specific
    }
}&lt;/code&gt;
    &lt;p&gt;These notifications (&lt;code&gt;0xD1&lt;/code&gt;, &lt;code&gt;0xD2&lt;/code&gt;, etc.) are hardware-specific signals that tell the NVIDIA driver to re-evaluate its power state, which prompts driver power-state re-evaluation; in traces this surfaces as iGPU GFX0._PSx/_DOS toggles plus dGPU state changes via PEPD._DSM/DGCE.&lt;/p&gt;
    &lt;p&gt;Here's where a simple but catastrophic oversight in the firmware's logic causes system-wide failure. High-end ASUS gaming laptops feature a MUX (Multiplexer) switch, a piece of hardware that lets the user choose between two distinct graphics modes:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Optimus Mode: The power-saving default. The integrated Intel GPU (iGPU) is physically connected to the display. The powerful NVIDIA GPU (dGPU) only renders demanding applications when needed, passing finished frames to the iGPU to be drawn on screen.&lt;/item&gt;
      &lt;item&gt;Ultimate/Mux Mode: The high-performance mode. The MUX switch physically rewires the display connections, bypassing the iGPU entirely and wiring the NVIDIA dGPU directly to the screen. In this mode, the dGPU is not optional; it is the only graphics processor capable of outputting an image.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Any firmware managing this hardware must be aware of which mode the system is in. Sending a command intended for one GPU to the other is futile and, in some cases, dangerous. Deep within the ACPI code, a hardware status flag named &lt;code&gt;HGMD&lt;/code&gt; is used to track this state. To understand the flaw, we first need to decipher what &lt;code&gt;HGMD&lt;/code&gt; means, and the firmware itself gives us the key.&lt;/p&gt;
    &lt;p&gt;For screen brightness to work, the command must be sent to the GPU that is physically controlling the display backlight. A command sent to the wrong GPU will simply do nothing. Therefore, the brightness control method (&lt;code&gt;BRTN&lt;/code&gt;) must be aware of the MUX switch state to function at all. It is the firmware's own Rosetta Stone.&lt;/p&gt;
    &lt;code&gt;// Brightness control - CORRECTLY checks for mux mode
Method (BRTN, 1, Serialized)  // Line 034003
{
    If (((DIDX &amp;amp; 0x0F0F) == 0x0400))
    {
        If (HGMD == 0x03)  // 0x03 = Ultimate/Mux mode
        {
            // In mux mode, notify discrete GPU
            Notify (\_SB.PC00.PEG1.PEGP.EDP1, Arg0)
        }
        Else
        {
            // In Optimus, notify integrated GPU
            Notify (\_SB.PC00.GFX0.DD1F, Arg0)
        }
    }
}&lt;/code&gt;
    &lt;p&gt;The logic here is flawless and revealing. The code uses the &lt;code&gt;HGMD&lt;/code&gt; flag to make a binary decision. If &lt;code&gt;HGMD&lt;/code&gt; is &lt;code&gt;0x03&lt;/code&gt;, it sends the command to the NVIDIA GPU. If not, it sends it to the Intel GPU. The firmware itself, through this correct implementation, provides the undeniable definition: &lt;code&gt;HGMD == 0x03&lt;/code&gt; means the system is in Ultimate/Mux Mode.&lt;/p&gt;
    &lt;p&gt;This perfect, platform-aware logic is completely abandoned in the critical code paths responsible for power management. The &lt;code&gt;LGPA&lt;/code&gt; method, which is called by the stutter-inducing interrupt, dispatches power-related commands to the GPU without ever checking the MUX mode.&lt;/p&gt;
    &lt;code&gt;// GPU power notification - NO MUX CHECK!
Case (0x18)
{
    // This SHOULD have: If (HGMD != 0x03)
    // But it doesn't, so it runs even in mux mode
    If (M6EF == One)
    {
        Local0 = 0xD2
    }
    Else
    {
        Local0 = 0xD1
    }
    NOD2 (Local0)  // Notifies GPU regardless of mode
}&lt;/code&gt;
    &lt;p&gt;This is not a single typo. A second, parallel power management system in the firmware exhibits the exact same flaw. The Platform Extension Plug-in Device (&lt;code&gt;PEPD&lt;/code&gt;) is used by Windows to manage system-wide power states, such as turning off displays during modern standby.&lt;/p&gt;
    &lt;code&gt;Device (PEPD)  // Line 071206
{
    Name (_HID, "INT33A1")  // Intel Power Engine Plugin
    
    Method (_DSM, 4, Serialized)  // Device Specific Method
    {
        // ... lots of setup code ...
        
        // Arg2 == 0x05: "All displays have been turned off"
        If ((Arg2 == 0x05))
        {
            // Prepare for aggressive power saving
            If (CondRefOf (\_SB.PC00.PEG1.DHDW))
            {
                ^^PC00.PEG1.DHDW ()         // GPU pre-shutdown work
                ^^PC00.PEG1.DGCE = One      // Set "GPU Cut Enable" flag
            }
            
            If (S0ID == One)  // If system supports S0 idle
            {
                GUAM (One)    // Enter low power mode
            }
            
            ^^PC00.DPOF = One  // Display power off flag
            
            // Tell USB controller about display state
            If (CondRefOf (\_SB.PC00.XHCI.PSLI))
            {
                ^^PC00.XHCI.PSLI (0x05)
            }
        }
        
        // Arg2 == 0x06: "A display has been turned on"
        If ((Arg2 == 0x06))
        {
            // Wake everything back up
            If (CondRefOf (\_SB.PC00.PEG1.DGCE))
            {
                ^^PC00.PEG1.DGCE = Zero     // Clear "GPU Cut Enable"
            }
            
            If (S0ID == One)
            {
                GUAM (Zero)   // Exit low power mode
            }
            
            ^^PC00.DPOF = Zero  // Display power on flag
            
            If (CondRefOf (\_SB.PC00.XHCI.PSLI))
            {
                ^^PC00.XHCI.PSLI (0x06)
            }
        }
    }
}&lt;/code&gt;
    &lt;p&gt;Once again, the firmware prepares to cut power to the discrete GPU without first checking if it's the only GPU driving the displays. This demonstrates that the Mux Mode Confusion is a systemic design flaw. The firmware is internally inconsistent, leading it to issue self-destructive commands that try to cripple the system.&lt;/p&gt;
    &lt;p&gt;Traces from multiple ASUS gaming laptop models confirm this is not an isolated issue.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Trace Duration: 4.1 minutes&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;_GPE._L02&lt;/code&gt;Events: 7&lt;/item&gt;
      &lt;item&gt;Avg. GPE Duration: 1.56ms (lower, but still unacceptably high)&lt;/item&gt;
      &lt;item&gt;Avg. Interval: 39.4 seconds (nearly identical periodic nature)&lt;/item&gt;
      &lt;item&gt;GPU Power Cycles: 8&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Trace Duration: 19.9 minutes&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;_GPE._L02&lt;/code&gt;Events: 3&lt;/item&gt;
      &lt;item&gt;Avg. GPE Duration: 2.94ms&lt;/item&gt;
      &lt;item&gt;GPU Power Cycles: 197 (far more frequent)&lt;/item&gt;
      &lt;item&gt;ASUS WMI Calls: 2,370 (a massive number, indicating software amplification)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Microsoft has a built-in "smooth video" check. It plays HD video in full screen and watches for hiccups. If the PC drops frames, crackles, or any driver pauses for more than a few milliseconds, it fails. That’s Microsoft’s baseline for what "smooth" should look like.&lt;/p&gt;
    &lt;p&gt;Why it matters here:&lt;/p&gt;
    &lt;p&gt;ASUS firmware is causing millisecond-long pauses. Those pauses are exactly the kind that make this test fail i.e., the same stutters and audio pops regular users notice on YouTube/Netflix and games; this firmware violates fundemental standards.&lt;/p&gt;
    &lt;p&gt;Despite being different models, all affected systems exhibit the same core flaws:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;_GPE._L02&lt;/code&gt;handlers take milliseconds to execute instead of microseconds.&lt;/item&gt;
      &lt;item&gt;The GPEs trigger unnecessary battery polling.&lt;/item&gt;
      &lt;item&gt;The firmware attempts to power cycle the GPU while in a fixed MUX mode.&lt;/item&gt;
      &lt;item&gt;The entire process is driven by a periodic, timer-like trigger.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This bug is a cascade of firmware design failures.&lt;/p&gt;
    &lt;p&gt;On windows, the LXX / EXX run at PASSIVE_LEVEL via ACPI.sys but while a GPE control method runs the firing GPE stays masked and ACPI/EC work is serialized. ASUS's dispatch from GPE._L02 to ECLV loops, calls Sleep(25/100ms) and re-arms the EC stretching that masked window into tens of milliseconds (which would explain the 13ms CPU time in ETW (Kernel ms) delay for GPE Events) and producing a periodic ACPI.sys burst that causes the latency problems on the system.The correct behavior is to latch or clear the event, exit the method, and signal a driver with Notify for any heavy work; do not self-rearm or sleep in this path at all.&lt;/p&gt;
    &lt;p&gt;The firmware artificially re-arms the interrupt, creating an endless loop of GPEs instead of clearing the source and waiting for the next legitimate hardware event. This transforms a hardware notification system into a disruptive, periodic timer.&lt;/p&gt;
    &lt;p&gt;The code that sends GPU power notifications does not check if the system is in MUX mode, a critical state check that is correctly performed in other parts of the firmware. This demonstrates inconsistency and a lack of quality control.&lt;/p&gt;
    &lt;p&gt;This issue is not new or isolated. User reports documenting identical symptoms with high ACPI.sys DPC latency, periodic stuttering, and audio crackling have been accumulating since at least 2021 across ASUS's entire gaming laptop lineup.&lt;/p&gt;
    &lt;p&gt;August 2021: The First Major Reports&lt;lb/&gt; The earliest documented cases appear on the official ASUS ROG forums. A G15 Advantage Edition (G513QY) owner reports "severe DPC latency from ACPI.sys" with audio dropouts occurring under any load condition. The thread, last edited in March 2024, shows the issue remains unresolved after nearly three years.&lt;/p&gt;
    &lt;p&gt;Reddit users simultaneously report identical ACPI.sys latency problems alongside NVIDIA driver issues; the exact symptoms described in this investigation.&lt;/p&gt;
    &lt;p&gt;2021-2023: Spreading Across Models&lt;lb/&gt; Throughout this period, the issue proliferates across ASUS's gaming lineup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ROG Strix models experience micro-stutters&lt;/item&gt;
      &lt;item&gt;TUF Gaming series reports throttling for seconds at a time&lt;/item&gt;
      &lt;item&gt;G18 models exhibit the characteristic 45-second periodic stuttering&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2023-2024: The Problem Persists in New Models&lt;lb/&gt; Even the latest generations aren't immune:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2023 Zephyrus G16 owners report persistent audio issues&lt;/item&gt;
      &lt;item&gt;2023 G16 models continue experiencing audio pops/crackles&lt;/item&gt;
      &lt;item&gt;2024 Intel G16 models require workarounds for audio stuttering&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The evidence is undeniable:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Measured Proof: GPE handlers are measured blocking a CPU core for over 13 milliseconds.&lt;/item&gt;
      &lt;item&gt;Code Proof: The decompiled firmware explicitly contains &lt;code&gt;Sleep()&lt;/code&gt;calls within an interrupt handler.&lt;/item&gt;
      &lt;item&gt;Logical Proof: The code lacks critical checks for the laptop's hardware state (MUX mode).&lt;/item&gt;
      &lt;item&gt;Systemic Proof: The issue is reproducible across different models and BIOS versions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Until a fix is implemented, millions of buyers of Asus laptops from approx. 2021 to present day are facing stutters on the simplest of tasks, such as watching YouTube, for the simple mistake of using a sleep call inside of an inefficient interrupt handler and not checking the GPU environment properly.&lt;/p&gt;
    &lt;p&gt;The code is there. The traces prove it. ASUS must fix its firmware.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;ASUS has not responded to this investigation or the documented firmware issues at the time of publication, will update this if anything changes.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Investigation conducted using the Windows Performance Toolkit, ACPI table extraction tools, and Intel ACPI Component Architecture utilities. All code excerpts are from official ASUS firmware. Traces were captured on multiple affected systems, all showing consistent behavior. I used an LLM for wording. The research, traces, and AML decomp are mine. Every claim is verified and reproducible if you follow the steps in the article; logs and commands are in the repo. If you think something's wrong, cite the exact timestamp/method/line. "AI wrote it" is not an argument.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45271484</guid><pubDate>Wed, 17 Sep 2025 03:54:36 +0000</pubDate></item><item><title>Notion API importer, with Databases to Bases conversion bounty</title><link>https://github.com/obsidianmd/obsidian-importer/issues/421</link><description>&lt;doc fingerprint="b02cd2bc20f33688"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 136&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;p&gt;Currently Importer supports converting Notion's HTML exports to Markdown via #14.&lt;/p&gt;
    &lt;p&gt;Unfortunately Notion's file-based export options don't include necessary data to recreate Databases.&lt;/p&gt;
    &lt;p&gt;This new importer would use the Notion API to download files progressively, and convert Databases to Bases as &lt;code&gt;.base&lt;/code&gt; files using the YAML syntax.&lt;/p&gt;
    &lt;p&gt;Closes #415&lt;/p&gt;
    &lt;head rend="h2"&gt;Bounty&lt;/head&gt;
    &lt;p&gt;See the Contribution guidelines for how to claim this bounty.&lt;/p&gt;
    &lt;p&gt;Bounty: $5,000 USD&lt;lb/&gt; Timeframe: 30 days&lt;/p&gt;
    &lt;head rend="h2"&gt;Requirements&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Uses Notion API (integration token) incorporating changes from new data source object introduced 2025-09.&lt;/item&gt;
      &lt;item&gt;Properly converts files to Obsidian-flavored Markdown, including tables, to-do lists, etc&lt;/item&gt;
      &lt;item&gt; Support for images and attachments. Embed links converted to Markdown format &lt;code&gt;!()[image.png]&lt;/code&gt;and placed in the user's defined attachment location (Settings → File &amp;amp; links)&lt;/item&gt;
      &lt;item&gt;Provide working test cases, ideally a reproducible data import that can be used on Notion. Alternatively a test account you can share with us via DM.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Databases to Bases&lt;/head&gt;
    &lt;p&gt;Some exploration is required before implementation because Databases and Bases work a bit differently. Notion's Databases start out as empty, whereas a Base starts out with all of the user's files, then narrows down using filters.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Determine an approach for importing databases and files&lt;/item&gt;
      &lt;item&gt;Determine what database features can be imported: views, columns, groups, summaries, formulas, etc.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Metadata&lt;/head&gt;
    &lt;head rend="h2"&gt;Metadata&lt;/head&gt;
    &lt;head rend="h3"&gt;Assignees&lt;/head&gt;
    &lt;head rend="h3"&gt;Labels&lt;/head&gt;
    &lt;head rend="h3"&gt;Type&lt;/head&gt;
    &lt;head rend="h3"&gt;Projects&lt;/head&gt;
    &lt;p&gt;Status&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45271942</guid><pubDate>Wed, 17 Sep 2025 05:11:50 +0000</pubDate></item><item><title>Murex – An intuitive and content aware shell for a modern command line</title><link>https://murex.rocks/</link><description>&lt;doc fingerprint="76c4cc9c753539fe"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Murex.Rocks&lt;/head&gt;
    &lt;p&gt;An intuitive and content aware shell for a modern command line&lt;/p&gt;
    &lt;head rend="h2"&gt;A Modern shell for the rest of us&lt;/head&gt;
    &lt;p&gt;Murex carries tons of unique features. Some highlights include...&lt;/p&gt;
    &lt;head rend="h3"&gt;Content Aware&lt;/head&gt;
    &lt;p&gt;Native support for manipulating data formats such as JSON, YAML, CSV, and others. This allows for seamless integration and manipulation of data in various formats. &lt;lb/&gt; Data types can be explicitly cast and reformatted, but also inferred if preferred.&lt;/p&gt;
    &lt;head rend="h3"&gt;Expressions&lt;/head&gt;
    &lt;p&gt;Smarter handling of variables and expressions to avoid accidental bugs caused by spaces or incorrect syntax. Resulting in a more reliable and predictable scripting experience. &lt;lb/&gt; Never worry about file names with weird characters, nor running equations in "bc" again.&lt;/p&gt;
    &lt;head rend="h3"&gt;Smartly Interactive&lt;/head&gt;
    &lt;p&gt;A uniquely intuitive interactive shell. With command line hints pulled from man pages, AI LLMs, and other intelligent integrations. &lt;lb/&gt; Navigating the command line is faster, more intuitive and efficient than ever before.&lt;/p&gt;
    &lt;head rend="h3"&gt;Easily Extended&lt;/head&gt;
    &lt;p&gt;The built-in package manager makes it very easy to share your configuration, import other peoples namespaced modules, and port your environment between different machines. &lt;lb/&gt; Configure once, use everywhere.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting Started&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Read the language tour to get started.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The Rosetta Stone is a great cheatsheet for those wishing to skip the tutorials and jump straight in. This guide includes comparisons with Bash.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The Interactive Shell guide walks you through using Murex as a command line as opposed to a scripting language.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Screenshots&lt;/head&gt;
    &lt;p&gt;Check out the Language Tour and Interactive Shell guides!&lt;/p&gt;
    &lt;head rend="h2"&gt;Easy to Install&lt;/head&gt;
    &lt;p&gt;Install &lt;code&gt;murex&lt;/code&gt; from your favorite package manager:&lt;/p&gt;
    &lt;code&gt;# via Homebrew:
brew install murex

# via MacPorts:
port install murex
&lt;/code&gt;
    &lt;code&gt;# From AUR: https://aur.archlinux.org/packages/murex
wget -O PKGBUILD 'https://aur.archlinux.org/cgit/aur.git/plain/PKGBUILD?h=murex'
makepkg --syncdeps --install 
&lt;/code&gt;
    &lt;code&gt;pkg install murex
&lt;/code&gt;
    &lt;p&gt;More options are available in the INSTALL document.&lt;/p&gt;
    &lt;p&gt;This document was generated from gen/root/README_doc.yaml.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45272480</guid><pubDate>Wed, 17 Sep 2025 06:32:17 +0000</pubDate></item><item><title>Why We're Building Stategraph: Terraform State as a Distributed Systems Problem</title><link>https://stategraph.dev/blog/why-stategraph/</link><description>&lt;doc fingerprint="a51827402d82cda9"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Why We're Building Stategraph: Terraform State as a Distributed Systems Problem&lt;/head&gt;&lt;p&gt;The Terraform ecosystem has spent a decade working around a fundamental architectural mismatch: we're using filesystem semantics to solve a distributed systems problem. The result is predictable and painful.&lt;/p&gt;&lt;p&gt;When we started building infrastructure automation at scale, we discovered that Terraform's state management exhibits all the classic symptoms of impedance mismatch between data representation and access patterns. Teams implement increasingly elaborate workarounds: state file splitting, wrapper orchestration, external locking mechanisms. These aren't solutions; they're evidence that we're solving the wrong problem.&lt;/p&gt;&lt;p&gt;Stategraph addresses this by treating state for what it actually is: a directed acyclic graph of resources with partial update semantics, not a monolithic document.&lt;/p&gt;&lt;head rend="h2"&gt;The Pathology of File-Based State&lt;/head&gt;&lt;p&gt;Terraform state, at its core, is a coordination problem. Multiple actors (engineers, CI systems, drift detection) need to read and modify overlapping subsets of infrastructure state concurrently. This is a well-studied problem in distributed systems, with established solutions around fine-grained locking, multi-version concurrency control, and transaction isolation.&lt;/p&gt;&lt;p&gt;Instead, Terraform implements the simplest possible solution: a global mutex on a JSON file.&lt;/p&gt;&lt;head rend="h4"&gt;Observation&lt;/head&gt;&lt;p&gt;The probability of lock contention in a shared state file increases super-linearly with both team size and resource count. At 100 resources and 5 engineers, you're coordinating 500 potential interaction points through a single mutex.&lt;/p&gt;&lt;p&gt;Consider the actual data access patterns in a typical Terraform operation:&lt;/p&gt;&lt;head rend="h4"&gt;Current Model&lt;/head&gt;&lt;p&gt; Read: 100%&lt;lb/&gt;Lock: 100%&lt;lb/&gt;Modify: 0.5% &lt;/p&gt;&lt;head rend="h4"&gt;Actual Requirement&lt;/head&gt;&lt;p&gt; Read: 3%&lt;lb/&gt;Lock: 3%&lt;lb/&gt;Modify: 3% &lt;/p&gt;&lt;p&gt;This mismatch between granularity of operation and granularity of locking is the root cause of every Terraform scaling problem. It violates the fundamental principle of isolation in concurrent systems: non-overlapping operations should not block each other.&lt;/p&gt;&lt;p&gt;The standard response, splitting state files, doesn't solve the problem. It redistributes it. Now you have N coordination problems instead of one, plus the additional complexity of managing cross-state dependencies. You've traded false contention for distributed transaction coordination, which is arguably worse.&lt;/p&gt;&lt;head rend="h2"&gt;State as a Graph: The Natural Representation&lt;/head&gt;&lt;p&gt;Infrastructure state is inherently a directed graph. Resources have dependencies, which form edges. Changes propagate along these edges. Terraform already knows this: the internal representation is a graph, and the planner performs graph traversal. But at the storage layer, we flatten this rich structure into a blob.&lt;/p&gt;&lt;p&gt;This is akin to storing a B-tree in a CSV file. You can do it, but you're destroying the very properties that make the data structure useful.&lt;/p&gt;&lt;p&gt;When state is properly normalized into a graph database, several properties emerge naturally:&lt;/p&gt;&lt;p&gt;Subgraph isolation: Operations on disjoint subgraphs are inherently parallelizable. If Team A is modifying RDS instances and Team B is updating CloudFront distributions, there's no shared state to coordinate.&lt;/p&gt;&lt;p&gt;Precise locking: We can implement row-level locking on resources and edge-level locking on dependencies. Lock acquisition follows the dependency graph, preventing deadlocks through consistent ordering.&lt;/p&gt;&lt;p&gt;Incremental refresh: Given a change set, we can compute the minimal refresh set by traversing the dependency graph. Most changes affect a small cone of resources, not the entire state space.&lt;/p&gt;&lt;head rend="h2"&gt;Concurrency Control Through Proper Abstractions&lt;/head&gt;&lt;p&gt;The distributed systems community solved these problems decades ago. Multi-version concurrency control (MVCC) allows readers to proceed without blocking writers. Write-ahead logging provides durability without sacrificing performance. Transaction isolation levels let operators choose their consistency guarantees.&lt;/p&gt;&lt;p&gt;Stategraph implements these patterns at the Terraform state layer:&lt;/p&gt;&lt;head rend="h4"&gt;Traditional: Global Lock&lt;/head&gt;&lt;head rend="h4"&gt;Stategraph: Subgraph Isolation&lt;/head&gt;&lt;p&gt;Each operation acquires locks only on its subgraph. The lock manager uses the dependency graph to ensure consistent ordering, preventing deadlocks. Readers use MVCC to access consistent snapshots without blocking writers.&lt;/p&gt;&lt;head rend="h4"&gt;Implementation Detail&lt;/head&gt;&lt;p&gt;Lock acquisition follows a strict partial order derived from the resource dependency graph. Resources are locked in topological order, with ties broken by resource ID. This guarantees deadlock freedom without requiring global coordination.&lt;/p&gt;&lt;p&gt;The result is dramatic improvement in concurrent throughput:&lt;/p&gt;&lt;head rend="h5"&gt;Transaction A&lt;/head&gt;&lt;head rend="h5"&gt;Transaction B&lt;/head&gt;&lt;head rend="h5"&gt;Transaction C&lt;/head&gt;&lt;p&gt;Three teams, three transactions, zero contention. This isn't possible with file-based state, regardless of how you split it.&lt;/p&gt;&lt;head rend="h2"&gt;The Refresh Problem&lt;/head&gt;&lt;p&gt;Terraform refresh is O(n) in the number of resources, regardless of change scope. Change one security group rule and you still walk the entire state. That's an algorithmic bottleneck, not just an implementation detail.&lt;/p&gt;&lt;head rend="h4"&gt;File-Based State&lt;/head&gt;&lt;p&gt; Changing 1 resource&lt;lb/&gt; Refreshing all 30 &lt;/p&gt;&lt;head rend="h4"&gt;Graph State&lt;/head&gt;&lt;p&gt; Changing 1 resource&lt;lb/&gt; Refreshing only 3 &lt;/p&gt;&lt;p&gt;With a graph representation, refresh work can be scoped to the affected subgraph instead of the entire state. Most changes touch only a small fraction of resources, not everything.&lt;/p&gt;&lt;head rend="h2"&gt;Why We Built This&lt;/head&gt;&lt;p&gt;At Terrateam, we've watched hundreds of teams struggle with the same fundamental problems. They start with a single state file, hit scaling limits, split their state, discover coordination complexity, build orchestration layers, and eventually resign themselves to living with the pain.&lt;/p&gt;&lt;p&gt;This is a solvable problem. The computer science is well-understood. The implementation is straightforward once you acknowledge that state management is a distributed systems problem, not a file storage problem.&lt;/p&gt;&lt;p&gt;Stategraph isn't revolutionary. It's the application of established distributed systems principles to a problem that's been mischaracterized since its inception. We're not inventing new algorithms; we're applying the right ones.&lt;/p&gt;&lt;head rend="h4"&gt;Design Principle&lt;/head&gt;&lt;p&gt;The storage layer should match the access patterns. Terraform state exhibits graph traversal patterns, partial update patterns, and concurrent access patterns. The storage layer should be a graph database with ACID transactions and fine-grained locking. Anything else is impedance mismatch.&lt;/p&gt;&lt;p&gt;The infrastructure industry has accepted file-based state as an immutable constraint for too long. It's not. It's a choice, and it's the wrong one for systems at scale.&lt;/p&gt;&lt;head rend="h2"&gt;Technical Implementation&lt;/head&gt;&lt;p&gt;Stategraph is implemented as a PostgreSQL schema with a backend that speaks the Terraform/OpenTofu remote backend protocol. We chose PostgreSQL for its robust MVCC, proven scalability, and operational familiarity. The schema normalizes state into three primary relations:&lt;/p&gt;&lt;p&gt;resources: one row per resource, with type, provider, and attribute columns.&lt;lb/&gt; dependencies: edge table representing the resource dependency graph.&lt;lb/&gt; transactions: append-only log of all state mutations with full attribution.&lt;/p&gt;&lt;p&gt;The backend extends Terraform's protocol with graph-aware operations. Lock acquisition and state queries operate directly on the database representation of the graph, enabling precision and concurrency that file-based backends can't provide.&lt;/p&gt;&lt;p&gt;This isn't a wrapper or an orchestrator. It's a replacement for the storage layer that preserves Terraform's execution model while fixing its coordination problems.&lt;/p&gt;&lt;head rend="h2"&gt;Adoption Path&lt;/head&gt;&lt;p&gt;Stategraph reads existing tfstate files and constructs the graph representation automatically. No changes to Terraform configurations are required. The backend protocol is unchanged. From Terraform's perspective, Stategraph is just another backend, like S3 or GCS.&lt;/p&gt;&lt;p&gt;But from an operational perspective, everything changes. Lock contention disappears. Refresh times drop by orders of magnitude. Teams stop blocking each other. State becomes queryable, auditable, and comprehensible.&lt;/p&gt;&lt;p&gt;We're not asking teams to rewrite their infrastructure. We're asking them to store it properly.&lt;/p&gt;&lt;quote&gt;The question isn't whether Terraform state should be a graph. It already is. The question is whether we'll continue pretending it's a file.&lt;/quote&gt;&lt;head rend="h3"&gt;Technical Preview&lt;/head&gt;&lt;p&gt;Stategraph is in active development. We're working with design partners to validate the approach at scale.&lt;/p&gt;Get Updates&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45273352</guid><pubDate>Wed, 17 Sep 2025 08:38:17 +0000</pubDate></item><item><title>Alibaba's New AI Chip Unveiled: Key Specifications Comparable to H20</title><link>https://news.futunn.com/en/post/62202518/alibaba-s-new-ai-chip-unveiled-key-specifications-comparable-to</link><description>&lt;doc fingerprint="ccca7d4c469478ff"&gt;
  &lt;main&gt;
    &lt;p&gt;On September 16, according to a report aired on CCTV News, a comparison of key parameters between domestically produced cards and NV cards revealed that the latest PPU chip developed by Pingtouge, a subsidiary of Alibaba and designed for artificial intelligence, surpasses NVIDIA's A800 in all major parameter metrics and is comparable to the H20. Compared with other domestic AI chips, Pingtouge’s PPU also mostly leads in these indicators.&lt;/p&gt;
    &lt;p&gt;In terms of specific parameters:&lt;lb/&gt;Memory: Alibaba Pingtouge PPU is equipped with 96GB of HBM2e, surpassing NVIDIA A800's 80GB HBM2e and matching the memory capacity of NVIDIA H20. However, H20 integrates HBM3, which is one generation ahead;**&lt;lb/&gt;Inter-chip interconnect bandwidth: Alibaba Pingtouge PPU reaches up to 700GB/s, higher than A800’s 400GB/s, but slightly lower than H20;&lt;lb/&gt;Interface: Alibaba Pingtouge PPU supports PCIe 5.0×15, superior to A800’s PCIe 4.0×16 and on par with H20;&lt;lb/&gt;Power consumption: Alibaba Pingtouge PPU maintains the same level as NVIDIA A800 at 400W, lower than H20’s 550W.&lt;/p&gt;
    &lt;p&gt;According to the report, the list of signed agreements for China Unicom’s Sanjiangyuan Green Electricity Intelligent Computing Center project showcases collaborations with multiple domestic AI chip brands under signed or proposed agreements.&lt;lb/&gt;Among them, the total number of devices in signed projects amounts to 1,747 units, comprising 22,832 computing cards with a total computing power of 3,479P. Specifically:&lt;lb/&gt;Alibaba Cloud Ten Thousand Cards: A total of 1,024 devices, 16,384 Pingtouge computing cards, with computing power reaching 1,945P;&lt;lb/&gt;Chinese Academy of Sciences: A total of 512 devices, 4,096 Muxi computing cards, with computing power reaching 984P;&lt;lb/&gt;Beijing Jingtai: A total of 83 devices, 1,328 BR100 computing power cards, with a computing power of 450P;&lt;lb/&gt;Zhonghao Core Electronics: A total of 128 devices, with a computing power of 200P.&lt;/p&gt;
    &lt;p&gt;In addition, the total computing power of the proposed contracted projects is 2,002P, including computing power cards from Taichu Yuanjie, Suizhi Technology, and Moore Threads.&lt;/p&gt;
    &lt;p&gt;Editor/Rocky&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45273747</guid><pubDate>Wed, 17 Sep 2025 09:45:44 +0000</pubDate></item><item><title>PureVPN IPv6 Leak</title><link>https://anagogistis.com/posts/purevpn-ipv6-leak/</link><description>&lt;doc fingerprint="fc9cf519ac23292a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;PureVPN IPv6 leak&lt;/head&gt;
    &lt;p&gt;In late August 2025, I submitted two security reports to PureVPN under their VDP. Three weeks later, I’ve received no response, so I decided to publish the findings to inform other users.&lt;/p&gt;
    &lt;p&gt;The issues affect both their GUI (v2.10.0) and CLI (v2.0.1) clients on Linux (tested on Ubuntu 24.04.3 LTS, kernel 6.8.0, iptables-nft backend). Hereâs what I found.&lt;/p&gt;
    &lt;head rend="h2"&gt;1. IPv6 Leaks Off-Tunnel&lt;/head&gt;
    &lt;p&gt;After toggling Wi-Fi or resuming from suspend, the PureVPN client fails to restore IPv6 protections:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;CLI (IKS enabled): The client auto-reconnects and reports status as “connected”, yet the system regains a default IPv6 route via Router Advertisements (&lt;/p&gt;&lt;code&gt;fe80::1&lt;/code&gt;). Since&lt;code&gt;ip6tables&lt;/code&gt;&lt;code&gt;OUTPUT&lt;/code&gt;remains&lt;code&gt;ACCEPT&lt;/code&gt;(default), egress resumes off-tunnel.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;GUI (IKS enabled): When the GUI detects a disconnection, it blocks IPv4 and displays the “VPN session disconnected” dialog. However, IPv6 remains functional until the user explicitly clicks&lt;/p&gt;&lt;code&gt;Reconnect&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Real-world effect: I was able to browse IPv6-preferred sites and send/receive email (Thunderbird) with my ISPâs IPv6 address while the client UI claimed I was protected.&lt;/p&gt;
    &lt;head rend="h2"&gt;2. Host Firewall Reset and Not Restored&lt;/head&gt;
    &lt;p&gt;At connect time, PureVPN wipes the user’s &lt;code&gt;iptables&lt;/code&gt; configuration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;INPUT&lt;/code&gt;is set to&lt;code&gt;ACCEPT&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;All &lt;code&gt;-A&lt;/code&gt;rules are flushed (UFW, Docker jumps, user rules, etc.)&lt;/item&gt;
      &lt;item&gt;After disconnect, these changes are not reverted&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Result: the system remains more exposed after using the VPN than before. This defeats the point of using UFW or a local deny policy and contradicts user expectations.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;# Baseline protections
$ sudo iptables -P INPUT DROP
$ sudo iptables -I INPUT -p icmp -j DROP

# Connect to VPN
$ purevpn-cli -c US
$ sudo iptables -S | head -3
-P INPUT ACCEPT
-P FORWARD DROP
-P OUTPUT ACCEPT
$ sudo iptables -S | grep icmp
# (no output â rule was wiped)

# Disconnect
$ purevpn-cli -d
$ sudo iptables -S | head -3
-P INPUT ACCEPT
-P FORWARD DROP
-P OUTPUT ACCEPT
# All wiped. INPUT = ACCEPT
&lt;/code&gt;
    &lt;head rend="h2"&gt;TL;DR&lt;/head&gt;
    &lt;p&gt;PureVPN:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Does not properly implement an IPv6 kill-switch&lt;/item&gt;
      &lt;item&gt;Leaves IPv6 egress open after reconnects or IKS events&lt;/item&gt;
      &lt;item&gt;Wipes your firewall state (&lt;code&gt;iptables&lt;/code&gt;) and does not restore it&lt;/item&gt;
      &lt;item&gt;Applies broad &lt;code&gt;ACCEPT&lt;/code&gt;policies to make things work&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both issues have real-world impact. Privacy claims are undermined when your real IPv6 leaks and your firewall state is lost.&lt;/p&gt;
    &lt;p&gt;I submitted full technical reports and screencasts to security@purevpn.com. No acknowledgment to date.&lt;/p&gt;
    &lt;p&gt;Use with caution.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45273897</guid><pubDate>Wed, 17 Sep 2025 10:10:14 +0000</pubDate></item><item><title>Determination of the fifth Busy Beaver value</title><link>https://arxiv.org/abs/2509.12337</link><description>&lt;doc fingerprint="a8be7401d4f7b0af"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Logic in Computer Science&lt;/head&gt;&lt;p&gt; [Submitted on 15 Sep 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Determination of the fifth Busy Beaver value&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:We prove that $S(5) = 47,176,870$ using the Coq proof assistant. The Busy Beaver value $S(n)$ is the maximum number of steps that an $n$-state 2-symbol Turing machine can perform from the all-zero tape before halting, and $S$ was historically introduced by Tibor Radó in 1962 as one of the simplest examples of an uncomputable function. The proof enumerates $181,385,789$ Turing machines with 5 states and, for each machine, decides whether it halts or not. Our result marks the first determination of a new Busy Beaver value in over 40 years and the first Busy Beaver value ever to be formally verified, attesting to the effectiveness of massively collaborative online research (bbchallenge$.$org).&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.LO&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45273999</guid><pubDate>Wed, 17 Sep 2025 10:26:18 +0000</pubDate></item><item><title>Apple Photos App Corrupts Images</title><link>https://tenderlovemaking.com/2025/09/17/apple-photos-app-corrupts-images/</link><description>&lt;doc fingerprint="7241bf3571248d9f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Apple Photos App Corrupts Images&lt;/head&gt;Sep 17, 2025 @ 8:59 am&lt;p&gt;The Apple Photos app sometimes corrupts images when importing from my camera. I just wanted to make a blog post about it in case anyone else runs into the problem. I’ve seen other references to this online, but most of the people gave up trying to fix it, and none of them went as far as I did to debug the issue.&lt;/p&gt;&lt;p&gt;I’ll try to describe the problem, and the things I’ve tried to do to fix it. But also note that I’ve (sort of) given up on the Photos app too. Since I can’t trust it to import photos from my camera, I switched to a different workflow.&lt;/p&gt;&lt;p&gt;Here is a screenshot of a corrupted image in the Photos app:&lt;/p&gt;&lt;head rend="h2"&gt;How I used to import images&lt;/head&gt;&lt;p&gt;I’ve got an OM System OM-1 camera. I used to shoot in RAW + jpg, then when I would import to Photos app, I would check the “delete photos after import” checkbox in order to empty the SD card. Turns out “delete after import” was a huge mistake.&lt;/p&gt;&lt;head rend="h2"&gt;Getting corrupted images&lt;/head&gt;&lt;p&gt;I’m pretty sure I’d been getting corrupted images for a while, but it would only be 1 or 2 images out of thousands, so I thought nothing of it (it was probably my fault anyway, right?)&lt;/p&gt;&lt;p&gt;But the problem really got me upset when last year I went to a family member’s wedding and took tons of photos. Apple Photos combines RAW + jpg photos so you don’t have a bunch of duplicates, and when you view the images in the photos app, it just shows you the jpg version by default. After I imported all of the wedding photos I noticed some of them were corrupted. Upon closer inspection, I found that it sometimes had corrupted the jpg, sometimes corrupted the RAW file, and sometimes both. Since I had been checking the “delete after import” box, I didn’t know if the images on the SD card were corrupted before importing or not. After all, the files had been deleted so there was no way to check.&lt;/p&gt;&lt;p&gt;I estimate I completely lost about 30% of the images I took that day.&lt;/p&gt;&lt;p&gt;Losing so many photos really rattled me, but I wanted to figure out the problem so I didn’t lose images in the future.&lt;/p&gt;&lt;head rend="h2"&gt;Narrowing down the problem&lt;/head&gt;&lt;p&gt;I was worried this was somehow a hardware problem. Copying files seems so basic, I didn’t think there was any way a massively deployed app like Photos could fuck it up (especially since its main job is managing photo files). So, to narrow down the issue I changed out all of the hardware. Here are all the things I did:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Switched USB-C cables&lt;/item&gt;&lt;item&gt;Bought a new SD card direct from the manufacturer (to eliminate the possibility of buying a bootleg SD card)&lt;/item&gt;&lt;item&gt;Switched to only shooting in RAW (if importing messes up 30% of my images, but I cut the number of images I import by half, then that should be fewer corrupted images right? lol)&lt;/item&gt;&lt;item&gt;Bought a new laptop&lt;/item&gt;&lt;item&gt;Bought a new camera: the OM System OM-1 MKii&lt;/item&gt;&lt;/list&gt;&lt;p&gt;I did each of these steps over time, as to only change one variable at a time, and still the image corruption persisted. I didn’t really want to buy a new camera, the MKii is not really a big improvement over the OM-1, but we had a family trip coming up and the idea that pressing the shutter button on the camera might not actually record the image didn’t sit well with me.&lt;/p&gt;&lt;head rend="h2"&gt;Finally a smoking gun&lt;/head&gt;&lt;p&gt;Since I had replaced literally all of the hardware involved, I knew it must be a software problem. I stopped checking the “delete after import” button, and started reviewing all of the photos after import. After verifying none of them were corrupt, then I would format the SD card. I did this for months without finding any corrupt files. At this point I figured it was somehow a race condition or something when copying the photo files and deleting them at the same time.&lt;/p&gt;&lt;p&gt;However, after I got home from RailsConf and imported my photos, I found one corrupt image (the one above). I was able to verify that the image was not corrupt on the SD card, so the camera was working fine (meaning I probably didn’t need to buy a new camera body at all).&lt;/p&gt;&lt;p&gt;I tried deleting the corrupt file and re-importing the original to see if it was something about that particular image, but it re-imported just fine. In other words, it seems like the Photos app will corrupt files randomly.&lt;/p&gt;&lt;p&gt;I don’t know if this is a problem that is specific to OM System cameras, and I’m not particularly interested in investing in a new camera system just to find out.&lt;/p&gt;&lt;p&gt;If I compare the corrupted image with the non-corrupted image, the file sizes are exactly the same, but the bytes are different:&lt;/p&gt;&lt;p&gt;Checksums:&lt;/p&gt;&lt;code&gt;aaron@tc ~/Downloads&amp;gt; md5sum P7110136-from-camera.ORF Exports/P7110136.ORF 
17ce895fd809a43bad1fe8832c811848  P7110136-from-camera.ORF
828a33005f6b71aea16d9c2f2991a997  Exports/P7110136.ORF
&lt;/code&gt;&lt;p&gt;File sizes:&lt;/p&gt;&lt;code&gt;aaron@tc ~/Downloads&amp;gt; ls -al P7110136-from-camera.ORF Exports/P7110136.ORF
-rw-------@ 1 aaron  staff  18673943 Jul 12 04:38 Exports/P7110136.ORF
-rwx------  1 aaron  staff  18673943 Jul 17 09:29 P7110136-from-camera.ORF*
&lt;/code&gt;&lt;p&gt;The &lt;code&gt;P7110136-from-camera.ORF&lt;/code&gt; is the non-corrupted file, and &lt;code&gt;Exports/P7110136.ORF&lt;/code&gt; is the corrupted file from Photos app.
Here’s a screenshot of the preview of the non-corrupted photo:&lt;/p&gt;&lt;p&gt;Here is the binary diff between the files. I ran both files through &lt;code&gt;xxd&lt;/code&gt; then diffed them.&lt;/p&gt;&lt;head rend="h2"&gt;My new workflow&lt;/head&gt;&lt;p&gt;I’m not going to put any more effort into debugging this problem, but I wanted to blog about it in case anyone else is seeing the issue. I take a lot of photos, and to be frank, most of them are not very good. I don’t want to look through a bunch of bad photos every time I look at my library, so culling photos is important. Culling photos in the Photos app is way too cumbersome, so I’ve switched to using Darktable.&lt;/p&gt;&lt;p&gt;My current process is:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Import images to Darktable&lt;/item&gt;&lt;item&gt;Delete the ones I don’t like&lt;/item&gt;&lt;item&gt;Process ones I do like&lt;/item&gt;&lt;item&gt;Export both the jpg and the original raw file&lt;/item&gt;&lt;item&gt;Import those to the Photos app so they’re easy to view and share&lt;/item&gt;&lt;item&gt;Periodically format my SD card&lt;/item&gt;&lt;/list&gt;&lt;p&gt;I’ve not seen any file corruption when importing to Darktable, so I am convinced this is a problem with the Photos app. But now, since all of my images land in Darktable before making their way to the Photos app, I don’t really care anymore. The bad news is that I’ve spent a lot of time and money trying to debug this. I guess the good news is that now I have redundant hardware!&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45274277</guid><pubDate>Wed, 17 Sep 2025 11:07:44 +0000</pubDate></item><item><title>Procedural Island Generation (III)</title><link>https://brashandplucky.com/2025/09/17/procedural-island-generation-iii.html</link><description>&lt;doc fingerprint="9da72cd84b738ab5"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Procedural Island Generation (III)&lt;/head&gt;&lt;p&gt;This post continues from Part II, where we established the paint map foundation and mountain ridge system. Now we’ll add detailed noise layers, distance-based mountain peaks, and do blending to create the final terrain elevation.&lt;/p&gt;&lt;head rend="h2"&gt;Paint Map (recap)&lt;/head&gt;&lt;p&gt;Before applying noise layers, we start with the foundation established in Part I - the paint map that defines our base land/water distribution:&lt;/p&gt;&lt;p&gt;For visualization throughout this series, we’ll be using the magma palette from matplotlib, which I patched to artificially darken the ocean areas to highlight the coastline:&lt;/p&gt;&lt;p&gt;Note that we’ll be sampling the paint map per Delaunay triangle (at each triangle’s centroid):&lt;/p&gt;&lt;p&gt;Remember that the paint map provides the broad strokes: positive values for land, negative for ocean, with smooth transitions between them. Now we’ll enhance it with noise layers to create realistic terrain detail.&lt;/p&gt;&lt;head rend="h2"&gt;Multi-Scale Noise Layers&lt;/head&gt;&lt;p&gt;We will layer multiple octaves of Simplex noise at different frequencies over the broad strokes provided by the paint map. Each will contribute different detail scales to the final terrain.&lt;/p&gt;&lt;p&gt;mapgen4 by @redblobgames in particular uses six layers:&lt;/p&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell role="head"&gt;Layer&lt;/cell&gt;&lt;cell role="head"&gt;Frequency&lt;/cell&gt;&lt;cell role="head"&gt;Description&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;n₀&lt;/cell&gt;&lt;cell&gt;1x&lt;/cell&gt;&lt;cell&gt;Lowest frequency&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;n₁&lt;/cell&gt;&lt;cell&gt;2x&lt;/cell&gt;&lt;cell&gt;Low frequency&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;n₂&lt;/cell&gt;&lt;cell&gt;4x&lt;/cell&gt;&lt;cell&gt;Medium-low frequency&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;n₄&lt;/cell&gt;&lt;cell&gt;16x&lt;/cell&gt;&lt;cell&gt;Medium-high frequency&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;n₅&lt;/cell&gt;&lt;cell&gt;32x&lt;/cell&gt;&lt;cell&gt;High frequency&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;n₆&lt;/cell&gt;&lt;cell&gt;64x&lt;/cell&gt;&lt;cell&gt;Highest frequency&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Notice the gap in numbering (n₃ is missing). This would correspond to frequency 8x, which we don’t use.&lt;/p&gt;&lt;head rend="h3"&gt;Coastal Noise Enhancement&lt;/head&gt;&lt;p&gt;mapgen4 starts with coastal noise enhancement. This provides control over the variation at coastlines while keeping inland elevation unaffected:&lt;/p&gt;\[e = \text{Paint map from Part I}\] \[e_{coast} = e + \alpha \cdot (1 - e^4) \cdot \left(n_4 + \frac{n_5}{2} + \frac{n_6}{4}\right)\]&lt;p&gt;The term \((1 - e^4)\) creates a bell curve that peaks at \(e=0\) (coastline) and decreases rapidly for \(\lvert e \rvert &amp;gt; 0\). This modulates an fBm-like combination of our three highest frequency noise layers.&lt;/p&gt;&lt;p&gt;What matters here isn’t the exact formula or amplitudes, but the core principle: applying high-frequency detail specifically where land meets water.&lt;/p&gt;\[e_{tmp} = \begin{cases} e &amp;amp; \text{if } e_{coast} &amp;gt; 0 \\ e_{coast} &amp;amp; \text{if } e_{coast} \leq 0 \end{cases}\]&lt;head rend="h2"&gt;Mountain Distance Field&lt;/head&gt;&lt;p&gt;Mountains need special pre-processing. If you remember from Part I in the swarm of seed points we tagged some as mountain peaks. Here we will pre-compute a distance field from every regular seed point to the closest mountain peak point.&lt;/p&gt;&lt;p&gt;We compute distance through the mesh topology of the Delaunay triangulation using BFS (breadth-first search). i.e., we don’t use Euclidean distance. This creates more organic mountain shapes that follow the terrain’s natural connectivity.&lt;/p&gt;&lt;p&gt;The algorithm spreads outward from mountain peaks:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Start at triangles containing mountain seed points (distance = 0)&lt;/item&gt;&lt;item&gt;Visit neighboring triangles, incrementing distance by a randomized amount&lt;/item&gt;&lt;item&gt;The randomization creates natural ridge patterns instead of perfect cones&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Here’s the magic formula used for distance increment in each step:&lt;/p&gt;\[\Delta = s \cdot (1 + j \cdot r)\]&lt;p&gt;Where:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;\(s\) = spacing between triangles (uses configured Poisson disk separation)&lt;/item&gt;&lt;item&gt;\(j\) = jaggedness parameter (0 = true topological distance, 1 = very irregular)&lt;/item&gt;&lt;item&gt;\(r \in [-1,1]\) = random factor using triangular distribution&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The triangular distribution &lt;code&gt;rand() - rand()&lt;/code&gt; clusters values near zero while allowing occasional larger variations. This looks more natural than uniform randomness.&lt;/p&gt;&lt;p&gt;I implemented Fisher-Yates shuffling when visiting neighbor triangles. Instead of processing neighbors in a fixed order (which would create directional bias), the order is randomly shuffled each time. This ensures mountain ridges branch out organically in all directions rather than following predictable patterns.&lt;/p&gt;&lt;p&gt;After computing distances this way, we normalize them (by the max dist, for example):&lt;/p&gt;&lt;head rend="h2"&gt;Elevation Blending&lt;/head&gt;&lt;p&gt;The final elevation combines all components through weighted blending:&lt;/p&gt;\[e_{final} = \begin{cases} \text{lerp}(e_{coast}^2, e_{hill}, e_{mountain}) &amp;amp; \text{if } e_{coast} &amp;gt; 0 \\ e_{coast} \cdot (\rho + n_1) &amp;amp; \text{if } e_{coast} \leq 0 \end{cases}\]&lt;p&gt;Where:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;\(e_{hill} = h \cdot (1 + \text{lerp}(\frac{1 + n_0}{2}, n_4, n_2))\) =&amp;gt; hill elevation with noise-modulated height&lt;/item&gt;&lt;item&gt;\(e_{mountain} = 1 - \frac{\mu}{2^\sigma} \cdot d_m\) =&amp;gt; mountain elevation from distance field&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The quadratic blend weight produces smooth transitions from hills near the coast through mixed terrain at mid-elevations to pure mountains at peaks.&lt;/p&gt;&lt;p&gt;With (editable) parameters:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;\(\alpha\): Coastal noise strength (0.01)&lt;/item&gt;&lt;item&gt;\(h\): Hill height scale (0.02)&lt;/item&gt;&lt;item&gt;\(\rho\): Ocean depth multiplier (1.5)&lt;/item&gt;&lt;item&gt;\(\mu\): Mountain slope (17.6)&lt;/item&gt;&lt;item&gt;\(\sigma\): Mountain sharpness (9.8)&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Interactive Parameter Exploration&lt;/head&gt;&lt;head rend="h2"&gt;Region (vs. Triangle) Elevation&lt;/head&gt;&lt;p&gt;So far we’ve computed elevation for triangles. But our Voronoi regions (from Part I) also need elevations for certain stages in the rest of the series.&lt;/p&gt;&lt;p&gt;Each seed point defines a Voronoi region and serves as a vertex in multiple Delaunay triangles. To assign elevation to a Voronoi region, we average the elevations of all triangles that share its seed point as a vertex.&lt;/p&gt;&lt;head rend="h2"&gt;Next Steps&lt;/head&gt;&lt;p&gt;With elevation complete, our island has shape but lacks the defining features carved by water. Part IV will simulate the hydrological cycle: rainfall patterns influenced by topography, rivers flowing from peaks to ocean, and valleys carved by erosion.&lt;/p&gt;&lt;head rend="h2"&gt;Valuable Resources&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Terrain from Noise - Amit Patel’s Red Blob Games guide to layering noise for terrain&lt;/item&gt;&lt;item&gt;Polygonal Map Generation - Red Blob Games on Voronoi-based terrain (mapgen4 inspiration)&lt;/item&gt;&lt;item&gt;Distance Fields for Terrain - Red Blob Games on using distance fields in terrain generation&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45275049</guid><pubDate>Wed, 17 Sep 2025 12:29:49 +0000</pubDate></item><item><title>Tau² Benchmark: How a Prompt Rewrite Boosted GPT-5-Mini by 22%</title><link>https://quesma.com/blog/tau2-benchmark-improving-results-smaller-models/</link><description>&lt;doc fingerprint="3f0f65d32893063a"&gt;
  &lt;main&gt;
    &lt;p&gt;Now on the front page of Hacker News — join the discussion.&lt;/p&gt;
    &lt;p&gt;In a recent post, we introduced the Tau² benchmark, a framework for benchmaring LLMs. Today we’re sharing a surprising discovery we made while using it: a simple prompt rewrite boosted a small model’s success rate by over 20%. This post is a deep-dive on how we found and fixed this performance bottleneck by making subtle changes to agent policies.&lt;/p&gt;
    &lt;head rend="h2"&gt;Benchmarking LLMs with Tau²&lt;/head&gt;
    &lt;p&gt;On the recent OpenAI Summer Update, we have seen that GPT-5 model has made significant strides in agentic tasks. To validate these claims, they’ve turned to the Tau² benchmark, which simulates real-world agent interactions across various domains like telecom, retail, and airlines.&lt;/p&gt;
    &lt;p&gt;Before moving any further, we have to establish that GPT-5 showed significant improvement only in one benchmark domain - which is Telecom. The other ones have been somehow overlooked during model presentation - therefore we won’t bother about them either (😉).&lt;/p&gt;
    &lt;p&gt;In agentic interactions, accuracy is non-negotiable, but model speed is equally vital for user experience. Therefore, it makes sense to consider alternatives to flagship models, such as the recently introduced GPT-5-mini.&lt;/p&gt;
    &lt;p&gt;GPT-5-mini offers significant advantages: it’s roughly twice as fast in latency and noticeably more efficient in throughput. While delivering 85–95% of the full GPT-5’s performance, it is also five times cheaper.&lt;/p&gt;
    &lt;p&gt;Therefore, we ran an experiment to explore two things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How well GPT-5-mini performs on this benchmark.&lt;/item&gt;
      &lt;item&gt;Whether we can improve its results by making subtle changes to the domain, such as modifying agent policies or task descriptions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Baseline: Expect GPT-5-mini to Fail 45% of the Time&lt;/head&gt;
    &lt;p&gt;Firstly, we’re going to establish the benchmark for the GPT-5-mini model. As the telecom benchmark contains over 100 tests, we’ll use their subset. Luckily, the telecom_small task set comes in handy with just 20 test scenarios.&lt;/p&gt;
    &lt;p&gt;Running the benchmark with:&lt;/p&gt;
    &lt;code&gt;tau2 run \
    --domain telecom \
    --agent-llm gpt-5-mini \
    --user-llm gpt-5-mini \
    --num-trials 2 --task-set-name telecom_small&lt;/code&gt;
    &lt;p&gt;Our results are:&lt;/p&gt;
    &lt;p&gt;We ended up running 40 simulations:&lt;/p&gt;
    &lt;p&gt;The initial success rate was low: just 55%. The GPT-5-mini with its limited reasoning capabilities doesn’t even get close to flagship GPT-5.&lt;/p&gt;
    &lt;p&gt;There’s an additional interesting metric this benchmark has introduced, which is pass^k. This measures how well an agent can perform when it’s challenged with the same task k times. I like to think of it as the reliability of the AI Agent.&lt;lb/&gt; Another intriguing aspect of this benchmark are tasks which failed for all given trials - which could imply that the AI Agent is simply not capable of handling at all. This can happen due to multiple factors - reasoning might be too difficult, user ask could not be specific enough, etc.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Hack: Using Claude to Rewrite Prompts for GPT-5-mini&lt;/head&gt;
    &lt;p&gt;When hacking the AI Agent for &lt;code&gt;GPT-5-mini&lt;/code&gt; model we aimed to answer three questions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Can I improve the overall success rate when using this limited model?&lt;/item&gt;
      &lt;item&gt;Are we able to “unlock” more tasks which agents are capable of handling?&lt;/item&gt;
      &lt;item&gt;Does agent reliability improve?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The beauty of generative AI is that we could offload lots of work and allow ourselves a little laziness. Therefore, we asked Claude to analyze the AI agent policies in the telecom domain, which are the building blocks for the agent prompt. From there, we asked specifically to assume that these policies are going to feed an AI Agent prompt, which will be using faster, non-flagship LLM. Output documents (&lt;code&gt;main_policy.md&lt;/code&gt; and &lt;code&gt;tech_support_manual.md&lt;/code&gt;) will have to be rephrased so that they are easier to follow for the agent and result in improving its behaviour.&lt;/p&gt;
    &lt;p&gt;Here’s the “executive summary” of improvements it made:&lt;/p&gt;
    &lt;code&gt;I've created two AI-optimized versions of the telecom domain documentation:

Key Improvements Made:

1. Structure &amp;amp; Flow
   - Decision Trees: Clear branching logic with ├── and └── notation
   - Sequential Steps: Numbered, ordered procedures instead of scattered explanations
   - Prerequisites: Explicit dependency checks before proceeding

2. AI Agent Optimizations
   - Tool Call Clarity: Exact function names and parameters
   - Binary Decisions: Clear yes/no conditions instead of ambiguous language
   - Error Handling: Specific failure conditions and next steps
   - Verification Steps: "Recheck" instructions after each fix

3. Cognitive Load Reduction
   - Reference Tables: Quick lookup for tools and purposes
   - Pattern Recognition: Common issue combinations and their solutions
   - Critical Reminders: Common AI mistakes section to prevent errors

4. Actionable Language
   - Removed verbose explanations mixed with instructions
   - Consolidated multiple documents' logic into single workflows
   - Used imperative commands: "Check X", "If Y then Z"
   - Added immediate verification steps&lt;/code&gt;
    &lt;p&gt;We’ve moved from lengthy policy declarations to clear, directive instructions. Instead of wordy statements, the new style uses step-by-step logic, conditions, and patterns. For prompts, this works more like a checklist — “Check X → If Y, then Z” — rather than vague, descriptive policies.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Result: 22% Accuracy Boost and 50% Fewer Unsolvable Tasks&lt;/head&gt;
    &lt;p&gt;Let’s review what our improved AI agent results look like:&lt;/p&gt;
    &lt;p&gt;The new prompts led to a significant performance boost. Pass^k metrics surged:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;k=1 from 0.55 to 0.675 (a 22.73% improvement) → In plain terms, GPT-5-mini now succeeds on 67.5% of tasks instead of 55%.&lt;/item&gt;
      &lt;item&gt;k=2 from 0.4 to 0.5 (a 25% improvement) → Meaning retries became more effective too.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For context, flagship GPT-5 scores ~97% on this benchmark, o3 comes in at 58%, and GPT-4.1 at 34%. With our optimized prompts, GPT-5-mini not only jumped well above its own baseline but also outperformed o3, landing much closer to GPT-5 than before.&lt;/p&gt;
    &lt;p&gt;The side-by-side comparison shows exactly where the gains came from. On the left side of the screen you’ll see the “stock” AI agent results, on the right - our AI agent improved for GPT-5-mini.&lt;/p&gt;
    &lt;p&gt;The screenshot above outlines that with our updated prompts and policies, we managed to “unlock” some of the tests which were previously always failing due to GPT-5-mini’s limited capabilities. Now there are only 3 tasks, which the agent didn’t manage to solve at all within the given 2 trials - compared to 6.&lt;/p&gt;
    &lt;head rend="h2"&gt;Key Takeaways for Your Own Models&lt;/head&gt;
    &lt;p&gt;This experiment shows that thoughtful prompt design can meaningfully boost the performance of smaller models like GPT-5-mini. By restructuring policies into clear, step-by-step instructions, we not only improved success rates but also “unlocked” tasks that previously seemed unsolvable for the model.&lt;/p&gt;
    &lt;p&gt;The key was in simplifying language, reducing ambiguity, and breaking down reasoning into explicit, actionable steps. Smaller models struggle with long-winded or fuzzy policies, but thrive when given structured flows, binary decisions, and lightweight verification steps.&lt;/p&gt;
    &lt;p&gt;The takeaway is clear: using a frontier model to automatically optimize prompts can unlock major improvements for smaller LLMs. With strategic optimization, lightweight models can deliver decent results at a fraction of the cost — making them a compelling alternative when efficiency and affordability matter as much as accuracy.&lt;/p&gt;
    &lt;p&gt;If you found this helpful, let us know! Prompt engineering is still an open playground, and we’re excited to see what creative approaches others are exploring in this space.&lt;/p&gt;
    &lt;p&gt;Discuss it on LinkedIn, X or Hacker News.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45275354</guid><pubDate>Wed, 17 Sep 2025 13:03:24 +0000</pubDate></item><item><title>Bringing fully autonomous rides to Nashville, in partnership with Lyft</title><link>https://waymo.com/blog/2025/09/waymo-is-coming-to-nashville-in-partnership-with-lyft</link><description>&lt;doc fingerprint="3c2ac04af1a4a895"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Bringing fully autonomous rides to Nashville, in partnership with Lyft&lt;/head&gt;
    &lt;p&gt;We’re on our way to Music City! We’re excited to bring the magic of Waymo’s fully autonomous ride-hailing service to riders in Nashville, in partnership with Lyft.&lt;/p&gt;
    &lt;p&gt;Our generalizable Waymo Driver has become even more capable as we’ve scaled to hundreds of thousands of fully autonomous rides each week across five major U.S. cities. We’ll start fully autonomous operations in Nashville in the coming months, and open to public riders next year. We’ll do so by pairing our world-leading technology and seamless ride-hailing service with Lyft’s proven track record of fleet management through its Flexdrive subsidiary.&lt;/p&gt;
    &lt;p&gt;We’re also excited to offer riders in Nashville even more ways to ride with Waymo. Riders will hail via the Waymo app, and as our service grows, riders will also be able to use the Lyft app to match with a Waymo vehicle. We’re thrilled for even more people to have access to our ride-hailing service, as we work towards our mission to be the world’s most trusted driver.&lt;/p&gt;
    &lt;p&gt;“We’re delighted to partner with Lyft and launch in Nashville next year, as we continue to scale our Waymo ride-hailing service to more people in more places,” said Waymo co-CEO Tekedra Mawakana. “Lyft’s extensive fleet management capabilities through Flexdrive make them an ideal partner for expanding to Nashville. We can’t wait to introduce Music City’s residents and visitors to the convenient, consistent, safe, and magical Waymo experience.”&lt;/p&gt;
    &lt;p&gt;"This partnership brings together best-in-class autonomous vehicles with best-in-class customer experience," said Lyft CEO David Risher. "Waymo has proven that its autonomous technology works at scale. When combined with Lyft's customer-obsession and world-class fleet management capabilities, it's two great tastes that go great together."&lt;/p&gt;
    &lt;p&gt;With more than 100 million fully autonomous miles driven on public roads, the data shows Waymo’s technology is significantly safer than human drivers in the areas where we operate. Nashville joins a growing list of cities that will soon have access to Waymo.&lt;/p&gt;
    &lt;p&gt;“As families and businesses move to Tennessee in record numbers, our state continues to lead the nation in finding innovative solutions to transportation challenges," said Governor Bill Lee. "By leveraging private sector technologies like Waymo's fully autonomous vehicles, we're exploring possibilities we couldn't achieve on our own, and further accelerating economic growth. I look forward to Waymo's launch in The Volunteer State.”&lt;/p&gt;
    &lt;p&gt;We’re looking forward to serving the people of Nashville soon. If you’re interested in following our journey or want to help bring Waymo to your city next, sign up at waymo.com/updates.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45275415</guid><pubDate>Wed, 17 Sep 2025 13:10:45 +0000</pubDate></item><item><title>Firefox 143 for Android to introduce DoH</title><link>https://blog.mozilla.org/en/firefox/dns-android/</link><description>&lt;doc fingerprint="9f6ab3c3bc9540af"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Firefox DNS privacy: Faster than ever, now on Android&lt;/head&gt;
    &lt;p&gt;All web browsing starts with a DNS query to find the IP address for the desired service or website. For much of the internet’s history, this query is sent in the clear. DNS-over-HTTPS (DoH) plugs this privacy leak by encrypting the DNS messages, so no one on the network, not your internet service provider or a free public WiFi provider, can eavesdrop on your browsing.&lt;lb/&gt;In 2020, Firefox became the first browser to roll out DoH by default, starting in the United States and in 2023, we announced the Firefox DoH-by-default rollout in Canada, powered by our trusted partner, the Canadian Internet Registration Authority (CIRA).&lt;/p&gt;
    &lt;p&gt;This year, we’ve built on that foundation and delivered major performance improvements and mobile support, ensuring more Firefox users benefit from privacy without compromise.&lt;/p&gt;
    &lt;head rend="h2"&gt;Introducing DoH for Android&lt;/head&gt;
    &lt;p&gt;After bringing encrypted DNS protection to millions of desktop users, we’re now extending the same to mobile. Firefox users who have been waiting for DoH on Android can now turn it on and browse with the same privacy protections as on their desktops.&lt;/p&gt;
    &lt;p&gt;Starting with this week’s release of Firefox 143 for Android, users can choose to enable DoH in Firefox on their mobile devices by selecting “Increased Protection” DoH configuration. Performance testing with Firefox DoH partners is currently underway. If DoH is as fast as we expect, we plan to enable it by default for Android users in certain regions, similar to desktop users. Until then, these configuration options provide you the choice to opt in early.&lt;/p&gt;
    &lt;head rend="h2"&gt;DoH performance breakthroughs in 2025&lt;/head&gt;
    &lt;p&gt;DNS resolution speed is critical to the browsing experience — when web pages involve multiple DNS queries, the speed difference compounds and can cause page loads to be slow. Since we first rolled out DoH in Canada, we’ve worked closely with CIRA for reliability and performance measurements. Through our strong collaboration with them and their technology partner Akamai, Firefox DoH lookups are now 61% faster year-to-date for the 75th percentile.&lt;/p&gt;
    &lt;p&gt;With these performance improvements, DoH resolution time is now within a millisecond or two of native DNS resolution. This is a big win because Firefox users in Canada now get the privacy of encrypted DNS with no performance penalty.&lt;/p&gt;
    &lt;p&gt;Although the investigation and analysis started with the desire to improve DoH in Firefox, the benefits didn’t end there. Our collaboration also improved CIRA DoH performance for many of its DNS users, including Canadian universities, as well as other DNS providers relying on CIRA’s or Akamai’s server implementations.&lt;/p&gt;
    &lt;p&gt;This is a win not just for Firefox users, but for the many other users around the globe.&lt;/p&gt;
    &lt;head rend="h2"&gt;Robust privacy on your terms&lt;/head&gt;
    &lt;p&gt;We have always approached DoH with an emphasis on transparency, user choice, and strong privacy safeguards. Firefox gives users meaningful control over how their DNS traffic is handled: Users can opt out, choose their own resolver, or adjust DoH protection levels, and Firefox makes it clear what DoH is doing and why it matters.&lt;/p&gt;
    &lt;p&gt;Firefox enforces strict requirements for DNS resolvers before trusting them with your browsing. Not every DNS provider can become a DoH provider in Firefox — only those that meet and attest to Mozilla’s rigorous Trusted Recursive Resolver (TRR) policy through a legally binding contract.&lt;/p&gt;
    &lt;head rend="h2"&gt;Prioritizing your privacy and speed&lt;/head&gt;
    &lt;p&gt;Our work with DoH this year shows what’s possible when privacy and performance go hand-in-hand. We’ve proven that encrypted DNS can be fast, reliable, and available on desktop and Android. Just as importantly, we’ve shown that partnerships grounded in open standards and accountability can deliver benefits not only to Firefox users but to the wider internet.&lt;/p&gt;
    &lt;p&gt;As we look forward, our commitment stays the same: Privacy should be the default, speed should never be a compromise, and the web should remain open and accessible to everyone. Choosing Firefox means choosing a browser that is built for you and for a better internet.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45275444</guid><pubDate>Wed, 17 Sep 2025 13:14:04 +0000</pubDate></item><item><title>SQLiteData: A fast, lightweight replacement for SwiftData using SQL and CloudKit</title><link>https://github.com/pointfreeco/sqlite-data</link><description>&lt;doc fingerprint="278a2af8febb569a"&gt;
  &lt;main&gt;
    &lt;p&gt;A fast, lightweight replacement for SwiftData, powered by SQL and supporting CloudKit synchronization.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Learn more&lt;/item&gt;
      &lt;item&gt;Overview&lt;/item&gt;
      &lt;item&gt;Quick start&lt;/item&gt;
      &lt;item&gt;Performance&lt;/item&gt;
      &lt;item&gt;SQLite knowledge required&lt;/item&gt;
      &lt;item&gt;Overview&lt;/item&gt;
      &lt;item&gt;Demos&lt;/item&gt;
      &lt;item&gt;Documentation&lt;/item&gt;
      &lt;item&gt;Installation&lt;/item&gt;
      &lt;item&gt;Community&lt;/item&gt;
      &lt;item&gt;License&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This library was motivated and designed over the course of many episodes on Point-Free, a video series exploring advanced programming topics in the Swift language, hosted by Brandon Williams and Stephen Celis. To support the continued development of this library, subscribe today.&lt;/p&gt;
    &lt;p&gt;SQLiteData is a fast, lightweight replacement for SwiftData, including CloudKit synchronization (and even CloudKit sharing) that deploys all the way back to the iOS 13 generation of targets. To populate data from the database you can use &lt;code&gt;@Table&lt;/code&gt; and &lt;code&gt;@FetchAll&lt;/code&gt;, which are
similar to SwiftData's &lt;code&gt;@Model&lt;/code&gt; and &lt;code&gt;@Query&lt;/code&gt;:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;SQLiteData&lt;/cell&gt;
        &lt;cell role="head"&gt;SwiftData&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;@FetchAll
var items: [Item]

@Table
struct Item {
  let id: UUID
  var title = ""
  var isInStock = true
  var notes = ""
}&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;@Query
var items: [Item]

@Model
class Item {
  var title: String
  var isInStock: Bool
  var notes: String
  init(
    title: String = "",
    isInStock: Bool = true,
    notes: String = ""
  ) {
    self.title = title
    self.isInStock = isInStock
    self.notes = notes
  }
}&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Both of the above examples fetch items from an external data store using Swift data types, and both are automatically observed by SwiftUI so that views are recomputed when the external data changes, but SQLiteData is powered directly by SQLite and is usable from UIKit, &lt;code&gt;@Observable&lt;/code&gt; models, and
more.&lt;/p&gt;
    &lt;p&gt;For more information on SQLiteData's querying capabilities, see Fetching model data.&lt;/p&gt;
    &lt;p&gt;Before SQLiteData's property wrappers can fetch data from SQLite, you need to provide–at runtime–the default database it should use. This is typically done as early as possible in your app's lifetime, like the app entry point in SwiftUI, and is analogous to configuring model storage in SwiftData:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;SQLiteData&lt;/cell&gt;
        &lt;cell role="head"&gt;SwiftData&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;@main
struct MyApp: App {
  init() {
    prepareDependencies {
      let db = try! DatabaseQueue(
        // Create/migrate a database
        // connection
      )
      $0.defaultDatabase = db
    }
  }
  // ...
}&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;@main
struct MyApp: App {
  let container = {
    // Create/configure a container
    try! ModelContainer(/* ... */)
  }()

  var body: some Scene {
    WindowGroup {
      ContentView()
        .modelContainer(container)
    }
  }
}&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;For more information on preparing a SQLite database, see Preparing a SQLite database.&lt;/p&gt;
    &lt;p&gt;This &lt;code&gt;defaultDatabase&lt;/code&gt; connection is used implicitly by SQLiteData's strategies, like
&lt;code&gt;@FetchAll&lt;/code&gt; and &lt;code&gt;@FetchOne&lt;/code&gt;, which are similar to SwiftData's
&lt;code&gt;@Query&lt;/code&gt; macro, but more powerful:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;SQLiteData&lt;/cell&gt;
        &lt;cell role="head"&gt;SwiftData&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;@FetchAll
var items: [Item]

@FetchAll(Item.order(by: \.title))
var items

@FetchAll(Item.where(\.isInStock))
var items



@FetchAll(Item.order(by: \.isInStock))
var items

@FetchOne(Item.count())
var itemsCount = 0&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;@Query
var items: [Item]

@Query(sort: [SortDescriptor(\.title)])
var items: [Item]

@Query(filter: #Predicate&amp;lt;Item&amp;gt; {
  $0.isInStock
})
var items: [Item]

// No @Query equivalent of ordering
// by boolean column.

// No @Query equivalent of counting
// entries in database without loading
// all entries.&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;And you can access this database throughout your application in a way similar to how one accesses a model context, via a property wrapper:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;SQLiteData&lt;/cell&gt;
        &lt;cell role="head"&gt;SwiftData&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;@Dependency(\.defaultDatabase)
var database

let newItem = Item(/* ... */)
try database.write { db in
  try Item.insert { newItem }
    .execute(db))
}&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;@Environment(\.modelContext)
var modelContext

let newItem = Item(/* ... */)
modelContext.insert(newItem)
try modelContext.save()&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;For more information on how SQLiteData compares to SwiftData, see Comparison with SwiftData.&lt;/p&gt;
    &lt;p&gt;Further, if you want to synchronize the local database to CloudKit so that it is available on all your user's devices, simply configure a &lt;code&gt;SyncEngine&lt;/code&gt; in the entry point of the app:&lt;/p&gt;
    &lt;code&gt;@main
struct MyApp: App {
  init() {
    prepareDependencies {
      $0.defaultDatabase = try! appDatabase()
      $0.defaultSyncEngine = SyncEngine(
        for: $0.defaultDatabase,
        tables: Item.self
      )
    }
  }
  // ...
}&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;For more information on synchronizing the database to CloudKit and sharing records with iCloud users, see CloudKit Synchronization.&lt;/p&gt;
    &lt;p&gt;This is all you need to know to get started with SQLiteData, but there's much more to learn. Read the articles below to learn how to best utilize this library:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fetching model data&lt;/item&gt;
      &lt;item&gt;Observing changes to model data&lt;/item&gt;
      &lt;item&gt;Preparing a SQLite database&lt;/item&gt;
      &lt;item&gt;Dynamic queries&lt;/item&gt;
      &lt;item&gt;CloudKit Synchronization&lt;/item&gt;
      &lt;item&gt;Comparison with SwiftData&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SQLiteData leverages high-performance decoding from StructuredQueries to turn fetched data into your Swift domain types, and has a performance profile similar to invoking SQLite's C APIs directly.&lt;/p&gt;
    &lt;p&gt;See the following benchmarks against Lighter's performance test suite for a taste of how it compares:&lt;/p&gt;
    &lt;code&gt;Orders.fetchAll                           setup    rampup   duration
   SQLite (generated by Enlighter 1.4.10) 0        0.144    7.183
   Lighter (1.4.10)                       0        0.164    8.059
┌──────────────────────────────────────────────────────────────────┐
│  SQLiteData (1.0.0)                     0        0.172    8.511  │
└──────────────────────────────────────────────────────────────────┘
   GRDB (7.4.1, manual decoding)          0        0.376    18.819
   SQLite.swift (0.15.3, manual decoding) 0        0.564    27.994
   SQLite.swift (0.15.3, Codable)         0        0.863    43.261
   GRDB (7.4.1, Codable)                  0.002    1.07     53.326
&lt;/code&gt;
    &lt;p&gt;SQLite is one of the most established and widely distributed pieces of software in the history of software. Knowledge of SQLite is a great skill for any app developer to have, and this library does not want to conceal it from you. So, we feel that to best wield this library you should be familiar with the basics of SQLite, including schema design and normalization, SQL queries, including joins and aggregates, and performance, including indices.&lt;/p&gt;
    &lt;p&gt;With some basic knowledge you can apply this library to your database schema in order to query for data and keep your views up-to-date when data in the database changes, and you can use StructuredQueries to build queries, either using its type-safe, discoverable query building APIs, or using its &lt;code&gt;#sql&lt;/code&gt; macro for writing safe SQL strings.&lt;/p&gt;
    &lt;p&gt;This repo comes with lots of examples to demonstrate how to solve common and complex problems with SQLiteData. Check out this directory to see them all, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Case Studies&lt;/p&gt;&lt;lb/&gt;Demonstrates how to solve some common application problems in an isolated environment, in both SwiftUI and UIKit. Things like animations, dynamic queries, database transactions, and more.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;CloudKitDemo&lt;/p&gt;&lt;lb/&gt;A simplified demo that shows how to synchronize a SQLite database to CloudKit and how to share records with other iCloud users. See our dedicated articles on CloudKit Synchronization and CloudKit Sharing for more information.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Reminders&lt;/p&gt;&lt;lb/&gt;A rebuild of Apple's Reminders app that uses a SQLite database to model the reminders, lists and tags. It features many advanced queries, such as searching, stats aggregation, and multi-table joins. It also features CloudKit synchronization and sharing.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;SyncUps&lt;/p&gt;&lt;lb/&gt;This application is a faithful reconstruction of one of Apple's more interesting sample projects, called Scrumdinger, and uses SQLite to persist the data for meetings. We have also added CloudKit synchronization so that all changes are automatically made available on all of the user's devices.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The documentation for releases and &lt;code&gt;main&lt;/code&gt; are available here:&lt;/p&gt;
    &lt;p&gt;You can add SQLiteData to an Xcode project by adding it to your project as a package…&lt;/p&gt;
    &lt;p&gt;…and adding the &lt;code&gt;SQLiteData&lt;/code&gt; product to your target.&lt;/p&gt;
    &lt;p&gt;If you want to use SQLiteData in a SwiftPM project, it's as simple as adding it to your &lt;code&gt;Package.swift&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;dependencies: [
  .package(url: "https://github.com/pointfreeco/sqlite-data", from: "1.0.0")
]&lt;/code&gt;
    &lt;p&gt;And then adding the following product to any target that needs access to the library:&lt;/p&gt;
    &lt;code&gt;.product(name: "SQLiteData", package: "sqlite-data"),&lt;/code&gt;
    &lt;p&gt;If you want to discuss this library or have a question about how to use it to solve a particular problem, there are a number of places you can discuss with fellow Point-Free enthusiasts:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;For long-form discussions, we recommend the discussions tab of this repo.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;For casual chat, we recommend the Point-Free Community Slack.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This library is released under the MIT license. See LICENSE for details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45275582</guid><pubDate>Wed, 17 Sep 2025 13:27:26 +0000</pubDate></item><item><title>UUIDv47: Store UUIDv7 in DB, emit UUIDv4 outside (SipHash-masked timestamp)</title><link>https://github.com/stateless-me/uuidv47</link><description>&lt;doc fingerprint="2c041edee4ad24ea"&gt;
  &lt;main&gt;
    &lt;p&gt;uuidv47 lets you store sortable UUIDv7 in your database while emitting a UUIDv4-looking façade at your API boundary. It does this by XOR-masking only the UUIDv7 timestamp field with a keyed SipHash-2-4 stream tied to the UUID’s own random bits.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Header-only C (C89) · zero deps&lt;/item&gt;
      &lt;item&gt;Deterministic, invertible mapping (exact round-trip)&lt;/item&gt;
      &lt;item&gt;RFC-compatible version/variant bits (v7 in DB, v4 on the wire)&lt;/item&gt;
      &lt;item&gt;Key-recovery resistant (SipHash-2-4, 128-bit key)&lt;/item&gt;
      &lt;item&gt;Full tests provided&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Why&lt;/item&gt;
      &lt;item&gt;Quick start&lt;/item&gt;
      &lt;item&gt;Public API&lt;/item&gt;
      &lt;item&gt;Specification &lt;list rend="ul"&gt;&lt;item&gt;UUIDv7 bit layout&lt;/item&gt;&lt;item&gt;Façade mapping (v7 ↔ v4)&lt;/item&gt;&lt;item&gt;SipHash message derived from random&lt;/item&gt;&lt;item&gt;Invertibility&lt;/item&gt;&lt;item&gt;Collision analysis&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Security model&lt;/item&gt;
      &lt;item&gt;Build, test, coverage&lt;/item&gt;
      &lt;item&gt;Integration tips&lt;/item&gt;
      &lt;item&gt;Performance notes&lt;/item&gt;
      &lt;item&gt;FAQ&lt;/item&gt;
      &lt;item&gt;License&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;DB-friendly: UUIDv7 is time-ordered → better index locality &amp;amp; pagination.&lt;/item&gt;
      &lt;item&gt;Externally neutral: The façade hides timing patterns and looks like v4 to clients/systems.&lt;/item&gt;
      &lt;item&gt;Secret safety: Uses a PRF (SipHash-2-4). Non-crypto hashes are not suitable when the key must not leak.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;#include &amp;lt;stdio.h&amp;gt;
#include "uuidv47.h"

int main(void){
  const char* s = "00000000-0000-7000-8000-000000000000";
  uuid128_t v7;
  if (!uuid_parse(s, &amp;amp;v7)) return 1;
  uuidv47_key_t key = { .k0 = 0x0123456789abcdefULL, .k1 = 0xfedcba9876543210ULL };
  uuid128_t facade = uuidv47_encode_v4facade(v7, key);
  uuid128_t back = uuidv47_decode_v4facade(facade, key);

  char a[37], b[37], c[37];
  uuid_format(&amp;amp;v7, a);
  uuid_format(&amp;amp;facade, b);
  uuid_format(&amp;amp;back, c);
  printf("v7 (DB) : %s\n", a);
  printf("v4 (API): %s\n", b);
  printf("back    : %s\n", c);
}&lt;/code&gt;
    &lt;p&gt;Build &amp;amp; run with the provided Makefile: make test make coverage sudo make install&lt;/p&gt;
    &lt;code&gt;typedef struct { uint8_t  b[16]; } uuid128_t;
typedef struct { uint64_t k0, k1; } uuidv47_key_t;

uuid128_t uuidv47_encode_v4facade(uuid128_t v7, uuidv47_key_t key);
uuid128_t uuidv47_decode_v4facade(uuid128_t v4_facade, uuidv47_key_t key);
int  uuid_version(const uuid128_t* u);
void set_version(uuid128_t* u, int ver);
void set_variant_rfc4122(uuid128_t* u);
bool uuid_parse (const char* str, uuid128_t* out);
void uuid_format(const uuid128_t* u, char out[37]);&lt;/code&gt;
    &lt;p&gt;UUIDv7 bit layout:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ts_ms_be: 48-bit big-endian timestamp&lt;/item&gt;
      &lt;item&gt;ver: high nibble of byte 6 = 0x7 (v7) or 0x4 (façade)&lt;/item&gt;
      &lt;item&gt;rand_a: 12 random bits&lt;/item&gt;
      &lt;item&gt;var: RFC variant (0b10)&lt;/item&gt;
      &lt;item&gt;rand_b: 62 random bits&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Façade mapping:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Encode: ts48 ^ mask48(R), set version=4&lt;/item&gt;
      &lt;item&gt;Decode: encTS ^ mask48(R), set version=7&lt;/item&gt;
      &lt;item&gt;Random bits unchanged&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SipHash input: 10 bytes from random field: msg[0] = (byte6 &amp;amp; 0x0F) msg[1] = byte7 msg[2] = (byte8 &amp;amp; 0x3F) msg[3..9] = bytes9..15&lt;/p&gt;
    &lt;p&gt;Invertibility: XOR mask is reversible with known key.&lt;/p&gt;
    &lt;p&gt;Collision analysis: Injective mapping. Only risk is duplicate randoms per ms.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Goal: Secret key unrecoverable even with chosen inputs.&lt;/item&gt;
      &lt;item&gt;Achieved: SipHash-2-4 is a keyed PRF.&lt;/item&gt;
      &lt;item&gt;Keys: 128-bit. Derive via HKDF.&lt;/item&gt;
      &lt;item&gt;Rotation: store small key ID outside UUID.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;make test
make coverage
make debug
sudo make install
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Do encode/decode at API boundary.&lt;/item&gt;
      &lt;item&gt;For Postgres, write tiny C extension.&lt;/item&gt;
      &lt;item&gt;For sharding, hash v4 façade with xxh3 or SipHash.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SipHash-2-4 on 10-byte message is extremely fast. No allocations.&lt;/p&gt;
    &lt;p&gt;Q: Why not xxHash with a secret? A: Not a PRF; secret can leak. Use SipHash.&lt;/p&gt;
    &lt;p&gt;Q: Is façade indistinguishable from v4? A: Yes, variable bits uniform, version/variant set to v4.&lt;/p&gt;
    &lt;p&gt;MIT, Copyright (c) 2025 Stateless Limited&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45275973</guid><pubDate>Wed, 17 Sep 2025 14:02:29 +0000</pubDate></item><item><title>YouTube addresses lower view counts which seem to be caused by ad blockers</title><link>https://9to5google.com/2025/09/16/youtube-lower-view-counts-ad-blockers/</link><description>&lt;doc fingerprint="3c188c5db8917253"&gt;
  &lt;main&gt;
    &lt;p&gt;Over the past month or so, many YouTubers have been reporting major drops to their video view counts. Theories have run wild, but there’s one explanation involving ad blockers that makes the most sense, but YouTube isn’t confirming anything directly.&lt;/p&gt;
    &lt;p&gt;Since mid-August, many YouTubers have noticed their view counts are considerably lower than they were before, in some cases with very drastic drops. The reason for the drop, though, has been shrouded in mystery for many creators.&lt;/p&gt;
    &lt;p&gt;The most likely explanation seems to be that YouTube is not counting views properly for users with an ad blocker enabled, another step in the platform’s continued war on ad blockers. This was first realized by Josh Strife Hayes, who noticed that view counts on TV, phones, and tablets have been steady, while views on computers have dropped by around 50% since the mid-August trend started. TechLinked, a channel in the Linus Tech Tips family, confirmed similar numbers within its statistics.&lt;/p&gt;
    &lt;p&gt;This aligns with one of the possible explanations that YouTube itself hinted at in an acknowledgement of lower view counts.&lt;/p&gt;
    &lt;p&gt;Google says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Viewers Using Ad Blockers &amp;amp; Other Content Blocking Tools: Ad blockers and other extensions can impact the accuracy of reported view counts. Channels whose audiences include a higher proportion of users utilizing such tools may see more fluctuations in traffic related to updates to these tools.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Top comment by bkpickell&lt;/head&gt;
    &lt;p&gt;Maybe if there weren't ads every two minutes and ten minute sponsored ads in the middle of the videos, people wouldn't need ad blockers. And stop the unskipable ads.&lt;/p&gt;
    &lt;p&gt;The rest of the post addresses prior speculation that YouTube’s new AI-powered age verification tools were to blame – which YouTube adamantly says is not the case – while also offering other possible explanations such as “seasonal viewing habits” and competition on the platform.&lt;/p&gt;
    &lt;p&gt;YouTube says “there is no systemic issue that is impacting creators” regarding lower view counts.&lt;/p&gt;
    &lt;p&gt;This ad blocker situation does seem the most likely explanation, though. In a prior video, Linus Tech Tips had noted that while view counts were down, ad revenue was not. If computer views are the only ones down, it stands to reason that viewers using an ad blocker are not being counted correctly, especially if ad revenue isn’t taking a hit from the lower view counts. YouTube’s hint that ad blockers “can impact the accuracy of reported view counts” certainly suggests this is possible, even if it’s not firm confirmation.&lt;/p&gt;
    &lt;head rend="h2"&gt;More on YouTube:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;YouTube for Android TV, Google TV will now let you test new features in beta&lt;/item&gt;
      &lt;item&gt;YouTube recommending awful videos? Here’s how to fix that&lt;/item&gt;
      &lt;item&gt;YouTube rolls out new profanity guidelines for creators&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Follow Ben: Twitter/X, Threads, Bluesky, and Instagram&lt;/p&gt;
    &lt;p&gt;FTC: We use income earning auto affiliate links. More.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45276262</guid><pubDate>Wed, 17 Sep 2025 14:29:10 +0000</pubDate></item><item><title>A single adblock filter may have caused YouTube's global view drop</title><link>https://github.com/easylist/easylist/issues/22375</link><description>&lt;doc fingerprint="7f7a53c6accc0eaa"&gt;
  &lt;main&gt;
    &lt;p&gt;Explanation and data(not mine): https://www.youtube.com/watch?v=YX1eEe8erkQ commit made on 2025-09-11 3:44 UCT that has high likelihood of correlation with drop in views as it's one of default blocklists in UBO ~~https://github.com/easylist/easylist/commit/2d39de407dc96904f00ec91b89d5fa91d29fbfa0~~ Per Srcxtchy investigation this is the real commit: https://github.com/easylist/easylist/commit/8f56190d0eef68b5b137d6c644962e9d0485a4fd&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45276614</guid><pubDate>Wed, 17 Sep 2025 14:56:20 +0000</pubDate></item></channel></rss>