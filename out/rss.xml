<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 23 Dec 2025 23:10:31 +0000</lastBuildDate><item><title>Local AI is driving the biggest change in laptops in decades</title><link>https://spectrum.ieee.org/ai-models-locally</link><description>&lt;doc fingerprint="beb9ddddccb6a4ee"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Odds are the PC in your office today isn’t ready to run AI large language models (LLMs).&lt;/p&gt;
      &lt;p&gt;Today, most users interact with LLMs via an online, browser-based interface. The more technically inclined might use an application programming interface or command line interface. In either case, the queries are sent to a data center, where the model is hosted and run. It works well, until it doesn’t; a data-center outage can take a model offline for hours. Plus, some users might be unwilling to send personal data to an anonymous entity.&lt;/p&gt;
      &lt;p&gt;Running a model locally on your computer could offer significant benefits: lower latency, better understanding of your personal needs, and the privacy that comes with keeping your data on your own machine.&lt;/p&gt;
      &lt;p&gt;However, for the average laptop that’s over a year old, the number of useful AI models you can run locally on your PC is close to zero. This laptop might have a four- to eight-core processor (CPU), no dedicated graphics chip (GPU) or neural-processing unit (NPU), and 16 gigabytes of RAM, leaving it underpowered for LLMs.&lt;/p&gt;
      &lt;p&gt;Even new, high-end PC laptops, which often include an NPU and a GPU, can struggle. The largest AI models have over a trillion parameters, which requires memory in the hundreds of gigabytes. Smaller versions of these models are available, even prolific, but they often lack the intelligence of larger models, which only dedicated AI data centers can handle.&lt;/p&gt;
      &lt;p&gt;The situation is even worse when other AI features aimed at making the model more capable are considered. Small language models (SLMs) that run on local hardware either scale back these features or omit them entirely. Image and video generation are difficult to run locally on laptops, too, and until recently they were reserved for high-end tower desktop PCs.&lt;/p&gt;
      &lt;p&gt;That’s a problem for AI adoption.&lt;/p&gt;
      &lt;p&gt;To make running AI models locally possible, the hardware found inside laptops and the software that runs on it will need an upgrade. This is the beginning of a shift in laptop design that will give engineers the opportunity to abandon the last vestiges of the past and reinvent the PC from the ground up.&lt;/p&gt;
      &lt;head rend="h2"&gt;NPUs enter the chat&lt;/head&gt;
      &lt;p&gt;The most obvious way to boost a PC’s AI performance is to place a powerful NPU alongside the CPU.&lt;/p&gt;
      &lt;p&gt;An NPU is a specialized chip designed for the matrix multiplication calculations that most AI models rely on. These matrix operations are highly parallelized, which is why GPUs (which were already better at highly parallelized tasks than CPUs) became the go-to option for AI data centers.&lt;/p&gt;
      &lt;p&gt;However, because NPUs are designed specifically to handle these matrix operations—and not other tasks, like 3D graphics—they’re more power efficient than GPUs. That’s important for accelerating AI on portable consumer technology. NPUs also tend to provide better support for low-precision arithmetic than laptop GPUs. AI models often use low-precision arithmetic to reduce computational and memory needs on portable hardware, such as laptops.&lt;/p&gt;
      &lt;p&gt;“With the NPU, the entire structure is really designed around the data type of tensors [a multidimensional array of numbers],” said Steven Bathiche, technical fellow at Microsoft. “NPUs are much more specialized for that workload. And so we go from a CPU that can handle three [trillion] operations per second (TOPS), to an NPU” in Qualcomm’s Snapdragon X chip, which can power Microsoft’s Copilot+ features. This includes Windows Recall, which uses AI to create a searchable timeline of a user’s usage history by analyzing screenshots, and Windows Photos’ Generative erase, which can remove the background or specific objects from an image.&lt;/p&gt;
      &lt;p&gt;While Qualcomm was arguably the first to provide an NPU for Windows laptops, it kickstarted an NPU TOPS arms race that also includes AMD and Intel, and the competition is already pushing NPU performance upward.&lt;/p&gt;
      &lt;p&gt;In 2023, prior to Qualcomm’s Snapdragon X, AMD chips with NPUs were uncommon, and those that existed delivered about 10 TOPS. Today, AMD and Intel have NPUs that are competitive with Snapdragon, providing 40 to 50 TOPS.&lt;/p&gt;
      &lt;p&gt;Dell’s upcoming Pro Max Plus AI PC will up the ante with a Qualcomm AI 100 NPU that promises up to 350 TOPS, improving performance by a staggering 35 times compared with that of the best available NPUs just a few years ago. Drawing that line up and to the right implies that NPUs capable of thousands of TOPS are just a couple of years away.&lt;/p&gt;
      &lt;p&gt;How many TOPS do you need to run state-of-the-art models with hundreds of millions of parameters? No one knows exactly. It’s not possible to run these models on today’s consumer hardware, so real-world tests just can’t be done. But it stands to reason that we’re within throwing distance of those capabilities. It’s also worth noting that LLMs are not the only use case for NPUs. Vinesh Sukumar, Qualcomm’s head of AI and machine learning product management, says AI image generation and manipulation is an example of a task that’s difficult without an NPU or high-end GPU.&lt;/p&gt;
      &lt;head rend="h2"&gt;Building balanced chips for better AI&lt;/head&gt;
      &lt;p&gt;Faster NPUs will handle more tokens per second, which in turn will deliver a faster, more fluid experience when using AI models. Yet there’s more to running AI on local hardware than throwing a bigger, better NPU at the problem.&lt;/p&gt;
      &lt;p&gt;Mike Clark, corporate fellow design engineer at AMD, says that companies that design chips to accelerate AI on the PC can’t put all their bets on the NPU. That’s in part because AI isn’t a replacement for, but rather an addition to, the tasks a PC is expected to handle.&lt;/p&gt;
      &lt;p&gt;“We must be good at low latency, at handling smaller data types, at branching code—traditional workloads. We can’t give that up, but we still want to be good at AI,” says Clark. He also noted that “the CPU is used to prepare data” for AI workloads, which means an inadequate CPU could become a bottleneck.&lt;/p&gt;
      &lt;p&gt;NPUs must also compete or cooperate with GPUs. On the PC, that often means a high-end AMD or Nvidia GPU with large amounts of built-in memory. The Nvidia GeForce RTX 5090’s specifications quote an AI performance up to 3,352 TOPS, which leaves even the Qualcomm AI 100 in the dust.&lt;/p&gt;
      &lt;p&gt;That comes with a big caveat, however: power. Though extremely capable, the RTX 5090 is designed to draw up to 575 watts on its own. Mobile versions for laptops are more miserly but still draw up to 175 W, which can quickly drain a laptop battery.&lt;/p&gt;
      &lt;p&gt;Simon Ng, client AI product manager at Intel, says the company is “seeing that the NPU will just do things much more efficiently at lower power.” Rakesh Anigundi, AMD’s director of product management for Ryzen AI, agrees. He adds that low-power operation is particularly important because AI workloads tend to take longer to run than other demanding tasks, like encoding a video or rendering graphics. “You’ll want to be running this for a longer period of time, such as an AI personal assistant, which could be always active and listening for your command,” he says.&lt;/p&gt;
      &lt;p&gt;These competing priorities mean chip architects and system designers will need to make tough calls about how to allocate silicon and power in AI PCs, especially those that often rely on battery power, such as laptops.&lt;/p&gt;
      &lt;p&gt;“We have to be very deliberate in how we design our system-on-a-chip to ensure that a larger SoC can perform to our requirements in a thin and light form factor,” said Mahesh Subramony, senior fellow design engineer at AMD.&lt;/p&gt;
      &lt;head rend="h2"&gt;When it comes to AI, memory matters&lt;/head&gt;
      &lt;p&gt;Squeezing an NPU alongside a CPU and GPU will improve the average PC’s performance in AI tasks, but it’s not the only revolutionary change AI will force on PC architecture. There’s another that’s perhaps even more fundamental: memory.&lt;/p&gt;
      &lt;p&gt;Most modern PCs have a divided memory architecture rooted in decisions made over 25 years ago. Limitations in bus bandwidth led GPUs (and other add-in cards that might require high-bandwidth memory) to move away from accessing a PC’s system memory and instead rely on the GPU’s own dedicated memory. As a result, powerful PCs typically have two pools of memory, system memory and graphics memory, which operate independently.&lt;/p&gt;
      &lt;p&gt;That’s a problem for AI. Models require large amounts of memory, and the entire model must load into memory at once. The legacy PC architecture, which splits memory between the system and the GPU, is at odds with that requirement.&lt;/p&gt;
      &lt;p&gt;“When I have a discrete GPU, I have a separate memory subsystem hanging off it,” explained Joe Macri, vice president and chief technology officer at AMD. “When I want to share data between our [CPU] and GPU, I’ve got to take the data out of my memory, slide it across the PCI Express bus, put it in the GPU memory, do my processing, then move it all back.” Macri said this increases power draw and leads to a sluggish user experience.&lt;/p&gt;
      &lt;p&gt;The solution is a unified memory architecture that provides all system resources access to the same pool of memory over a fast, interconnected memory bus. Apple’s in-house silicon is perhaps the most well-known recent example of a chip with a unified memory architecture. However, unified memory is otherwise rare in modern PCs.&lt;/p&gt;
      &lt;p&gt;AMD is following suit in the laptop space. The company announced a new line of APUs targeted at high-end laptops, Ryzen AI Max, at CES (Consumer Electronics Show) 2025.&lt;/p&gt;
      &lt;p&gt;Ryzen AI Max places the company’s Ryzen CPU cores on the same silicon as Radeon-branded GPU cores, plus an NPU rated at 50 TOPS, on a single piece of silicon with a unified memory architecture. Because of this, the CPU, GPU, and NPU can all access up to a maximum of 128 GB of system memory, which is shared among all three. AMD believes this strategy is ideal for memory and performance management in consumer PCs. “By bringing it all under a single thermal head, the entire power envelope becomes something that we can manage,” said Subramony.&lt;/p&gt;
      &lt;p&gt;The Ryzen AI Max is already available in several laptops, including the HP Zbook Ultra G1a and the Asus ROG Flow Z13. It also powers the Framework Desktop and several mini desktops from less well-known brands, such as the GMKtec EVO-X2 AI mini PC.&lt;/p&gt;
      &lt;p&gt;Intel and Nvidia will also join this party, though in an unexpected way. In September, the former rivals announced an alliance to sell chips that pair Intel CPU cores with Nvidia GPU cores. While the details are still under wraps, the chip architecture will likely include unified memory and an Intel NPU.&lt;/p&gt;
      &lt;p&gt;Chips like these stand to drastically change PC architecture if they catch on. They’ll offer access to much larger pools of memory than before and integrate the CPU, GPU, and NPU into one piece of silicon that can be closely monitored and controlled. These factors should make it easier to shuffle an AI workload to the hardware best suited to execute it at a given moment.&lt;/p&gt;
      &lt;p&gt;Unfortunately, they’ll also make PC upgrades and repairs more difficult, as chips with a unified memory architecture typically bundle the CPU, GPU, NPU, and memory into a single, physically inseparable package on a PC mainboard. That’s in contrast with traditional PCs, where the CPU, GPU, and memory can be replaced individually.&lt;/p&gt;
      &lt;head rend="h2"&gt;Microsoft’s bullish take on AI is rewriting Windows&lt;/head&gt;
      &lt;p&gt;MacOS is well regarded for its attractive, intuitive user interface, and Apple Silicon chips have a unified memory architecture that can prove useful for AI. However, Apple’s GPUs aren’t as capable as the best ones used in PCs, and its AI tools for developers are less widely adopted.&lt;/p&gt;
      &lt;p&gt;Chrissie Cremers, cofounder of the AI-focused marketing firm Aigency Amsterdam, told me earlier this year that although she prefers macOS, her agency doesn’t use Mac computers for AI work. “The GPU in my Mac desktop can hardly manage [our AI workflow], and it’s not an old computer,” she said. “I’d love for them to catch up here, because they used to be the creative tool.”&lt;/p&gt;
      &lt;p&gt; Dan Page&lt;/p&gt;
      &lt;p&gt;That leaves an opening for competitors to become the go-to choice for AI on the PC—and Microsoft knows it.&lt;/p&gt;
      &lt;p&gt;Microsoft launched Copilot+ PCs at the company’s 2024 Build developer conference. The launch had problems, most notably the botched release of its key feature, Windows Recall, which uses AI to help users search through anything they’ve seen or heard on their PC. Still, the launch was successful in pushing the PC industry toward NPUs, as AMD and Intel both introduced new laptop chips with upgraded NPUs in late 2024.&lt;/p&gt;
      &lt;p&gt;At Build 2025, Microsoft also revealed Windows’ AI Foundry Local, a “runtime stack” that includes a catalog of popular open-source large language models. While Microsoft’s own models are available, the catalog includes thousands of open-source models from Alibaba, DeepSeek, Meta, Mistral AI, Nvidia, OpenAI, Stability AI, xAI, and more.&lt;/p&gt;
      &lt;p&gt;Once a model is selected and implemented into an app, Windows executes AI tasks on local hardware through the Windows ML runtime, which automatically directs AI tasks to the CPU, GPU, or NPU hardware best suited for the job.&lt;/p&gt;
      &lt;p&gt;AI Foundry also provides APIs for local knowledge retrieval and low-rank adaptation (LoRA), advanced features that let developers customize the data an AI model can reference and how it responds. Microsoft also announced support for on-device semantic search and retrieval-augmented generation, features that help developers build AI tools that reference specific on-device information.&lt;/p&gt;
      &lt;p&gt;“[AI Foundry] is about being smart. It’s about using all the processors at hand, being efficient, and prioritizing workloads across the CPU, the NPU, and so on. There’s a lot of opportunity and runway to improve,” said Bathiche.&lt;/p&gt;
      &lt;head rend="h3"&gt;Toward AGI on PCs&lt;/head&gt;
      &lt;p&gt;The rapid evolution of AI-capable PC hardware represents more than just an incremental upgrade. It signals a coming shift in the PC industry that’s likely to wipe away the last vestiges of the PC architectures designed in the ’80s, ’90s, and early 2000s.&lt;/p&gt;
      &lt;p&gt;The combination of increasingly powerful NPUs, unified memory architectures, and sophisticated software-optimization techniques is closing the performance gap between local and cloud-based AI at a pace that has surprised even industry insiders, such as Bathiche.&lt;/p&gt;
      &lt;p&gt;It will also nudge chip designers toward ever-more-integrated chips that have a unified memory subsystem and to bring the CPU, GPU, and NPU onto a single piece of silicon—even in high-end laptops and desktops. AMD’s Subramony said the goal is to have users “carrying a mini workstation in your hand, whether it’s for AI workloads, or for high compute. You won’t have to go to the cloud.”&lt;/p&gt;
      &lt;p&gt;A change that massive won’t happen overnight. Still, it’s clear that many in the PC industry are committed to reinventing the computers we use every day in a way that optimizes for AI. Qualcomm’s Vinesh Sukumar even believes affordable consumer laptops, much like data centers, should aim for AGI.&lt;/p&gt;
      &lt;p&gt;“I want a complete artificial general intelligence running on Qualcomm devices,” he said. “That’s what we’re trying to push for.” &lt;/p&gt;
      &lt;p&gt;This article appears in the December 2025 print issue.&lt;/p&gt;
      &lt;div&gt;
        &lt;p&gt;From Your Site Articles&lt;/p&gt;
        &lt;p&gt;Related Articles Around the Web&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46360856</guid><pubDate>Tue, 23 Dec 2025 00:12:16 +0000</pubDate></item><item><title>iOS 26.3 brings AirPods-like pairing to third-party devices in EU under DMA</title><link>https://www.macrumors.com/2025/12/22/ios-26-3-dma-airpods-pairing/</link><description>&lt;doc fingerprint="b4eea82d9326b9c8"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h1"&gt;iOS 26.3 Brings AirPods-Like Pairing to Third-Party Devices in EU Under DMA&lt;/head&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt;The European Commission today praised the interoperability changes that Apple is introducing in iOS 26.3, once again crediting the Digital Markets Act (DMA) with bringing "new opportunities" to European users and developers.&lt;/p&gt;
          &lt;p&gt;&lt;lb/&gt;The Digital Markets Act requires Apple to provide third-party accessories with the same capabilities and access to device features that Apple's own products get. In iOS 26.3, EU wearable device makers can now test proximity pairing and improved notifications.&lt;/p&gt;
          &lt;p&gt;Here are the new capabilities that Apple is adding:&lt;/p&gt;
          &lt;list rend="ul"&gt;
            &lt;item&gt;Proximity pairing - Devices like earbuds will be able to pair with an iOS device in an AirPods-like way by bringing the accessory close to an iPhone or iPad to initiate a simple, one-tap pairing process. Pairing third-party devices will no longer require multiple steps.&lt;/item&gt;
            &lt;item&gt;Notifications - Third-party accessories like smart watches will be able to receive notifications from the iPhone. Users will be able to view and react to incoming notifications, which is functionality normally limited to the Apple Watch. Notifications can only be forwarded to one connected device at a time, and turning on notifications for a third-party device disables notifications to an Apple Watch.&lt;/item&gt;
          &lt;/list&gt;
          &lt;p&gt;The European Commission says that developers can test third-party TVs, smart watches, and headphones with the new features in iOS 26.3, with the functionality to be "fully available in Europe" in 2026.&lt;/p&gt;
          &lt;p&gt;iOS 26.3 offers "another step towards a more inter-connected digital ecosystem to the benefit of all EU citizens," according to the European Commission. iOS 26.3 is expected to launch at the end of January.&lt;/p&gt;
          &lt;p&gt;The changes to proximity pairing and notifications are only available for device makers and iPhone and iPad users in the European Union.&lt;/p&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;head rend="h2"&gt;Popular Stories&lt;/head&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple hasn't updated the Apple TV 4K since 2022, and 2025 was supposed to be the year that we got a refresh. There were rumors suggesting Apple would release the new Apple TV before the end of 2025, but it looks like that's not going to happen now. Subscribe to the MacRumors YouTube channel for more videos. Bloomberg's Mark Gurman said several times across 2024 and 2025 that Apple would...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;You'd think things would be slowing down heading into the holidays, but this week saw a whirlwind of Apple leaks and rumors while Apple started its next cycle of betas following last week's release of iOS 26.2 and related updates. This week also saw the release of a new Apple Music integration with ChatGPT, so read on below for all the details on this week's biggest stories! Top Stories i...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Next year's iPhone 18 Pro and iPhone 18 Pro Max will be equipped with under-screen Face ID, and the front camera will be moved to the top-left corner of the screen, according to a new report from The Information's Wayne Ma and Qianer Liu. As a result of these changes, the report said the iPhone 18 Pro models will not have a pill-shaped Dynamic Island cutout at the top of the screen....&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;The European Commission today praised the interoperability changes that Apple is introducing in iOS 26.3, once again crediting the Digital Markets Act (DMA) with bringing "new opportunities" to European users and developers. The Digital Markets Act requires Apple to provide third-party accessories with the same capabilities and access to device features that Apple's own products get. In iOS...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Since the beginning of December, Apple has been pushing iPhone users who opted to stay on iOS 18 to install iOS 26 instead. Apple started by making the iOS 18 upgrades less visible, and has now transitioned to making new iOS 18 updates unavailable on any device capable of running iOS 26. If you have an iPhone 11 or later, Apple is no longer offering new versions of iOS 18, even though there...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Earlier this month, Apple released iOS 26.2, following more than a month of beta testing. It is a big update, with many new features and changes for iPhones. iOS 26.2 adds a Liquid Glass slider for the Lock Screen's clock, offline lyrics in Apple Music, and more. Below, we have highlighted a total of eight new features. Liquid Glass Slider on Lock Screen A new slider in the Lock...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple is significantly increasing its reliance on Samsung for iPhone memory as component prices surge, according to The Korea Economic Daily. Apple is said to be expanding the share of iPhone memory it sources from Samsung due to rapidly rising memory prices. The shift is expected to result in Samsung supplying roughly 60% to 70% of the low-power DRAM used in the iPhone 17, compared with a...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;While the iPhone 18 Pro and iPhone 18 Pro Max are not expected to launch for another nine months, there are already plenty of rumors about the devices. Below, we have recapped 12 features rumored for the iPhone 18 Pro models. The same overall design is expected, with 6.3-inch and 6.9-inch display sizes, and a "plateau" housing three rear cameras Under-screen Face ID Front camera in...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46362927</guid><pubDate>Tue, 23 Dec 2025 06:22:21 +0000</pubDate></item><item><title>10 years bootstrapped: €6.5M revenue with a team of 13</title><link>https://www.datocms.com/blog/a-look-back-at-2025</link><description>&lt;doc fingerprint="2f3948de99b426d0"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;The DatoCMS Blog&lt;/head&gt;
    &lt;head rend="h1"&gt;A look back at 2025&lt;/head&gt;
    &lt;p&gt;As 2025 comes to a close, it's once again time to reflect. Itâs been another packed twelve months, and itâs great to look back at everything we achieved, day by day. (Yes, we're patting ourselves on the back. It's our blog, we're allowed to.)&lt;/p&gt;
    &lt;p&gt;Want to take a walk down memory lane? Here are previous editions: 2024, 2023, 2022, 2021, 2020.&lt;/p&gt;
    &lt;head rend="h2"&gt;Financials: Strong growth, with best-in-class margins&lt;/head&gt;
    &lt;p&gt;This year, we reached â¬6.5 million in revenue, a solid 10% year-over-year growth. Not that many companies still have double-digit growth after ten years! Most are either dead, laying off half their teams, acqui-hired, or pivoting to AI-something.&lt;/p&gt;
    &lt;p&gt;With our continued focus on sustainable operations and disciplined execution, we achieved an EBIT margin of 65%. To put this in perspective: while most SaaS companies celebrate 20-30% margins, and industry leaders hover around 40%, DatoCMS has reached a level of profitability that places us in the top 5% of SaaS companies globally.&lt;/p&gt;
    &lt;p&gt;For those familiar with SaaS metrics, the "Rule of 40" states that growth rate plus profit margin should exceed 40%. Ours is 75%. We're not bragging (okay, we're bragging a little) but it turns out that not burning through VC cash on ping-pong tables and "growth at all costs" actually works.&lt;/p&gt;
    &lt;head rend="h2"&gt;Partners: More and more of you are joining us&lt;/head&gt;
    &lt;p&gt;With 185 agency partners now fully enrolled in our partner network (!!!), we're genuinely blown away. These are people who build websites for a living, with real deadlines and real clients breathing down their necks. They don't have time for tools that get in the way â and they chose us. We don't take that for granted.&lt;/p&gt;
    &lt;p&gt;This year, we doubled down on making your work more visible. All that real work for real clients? It adds up â we now have 340 projects in the showcase (63 added this year alone!), enough that we had to revamp the page with proper filters so people can actually find things.&lt;/p&gt;
    &lt;p&gt;And what projects they are. Youâve used DatoCMS to power offline wayfinding. Youâve helped shape the early days of the entire GraphQL community. Heck, one of you even took a day to graffiti the streets of Switzerland about us â which is either peak brand loyalty or a cry for help, we're not sure. Either way, never felt so loved.&lt;/p&gt;
    &lt;p&gt;If you're an agency and you're not in the partner program yet â come on. We're not collecting logos here. We want to build a real relationship, learn what's slowing you down, and give you the perfect tool to ship quality work fast and painlessly. Half the features we shipped this year came from partner feedback. You're literally shaping the product. That's the whole point. No awkward sales calls, promise, we hate those too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Product: Another incredible round of improvements&lt;/head&gt;
    &lt;p&gt;2025 has been another year of relentless shipping. We didn't just focus on one area â we improved the entire stack, from the way developers write code to how editors manage content, all while hardening security and preparing for the AI era (gosh, we said it, now we need to wash our mouths).&lt;/p&gt;
    &lt;p&gt;Here is an exhaustive look at everything we shipped this year, grouped by how they help you:&lt;/p&gt;
    &lt;head rend="h6"&gt;Type Safety &amp;amp; Developer Confidence&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Records, finally typed â The biggest DX win of the year. The JavaScript client now supports full end-to-end type safety, generating types directly from your schema for real autocomplete and compile-time safety. No more&lt;/p&gt;&lt;code&gt;any&lt;/code&gt;types haunting your dreams.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Reactive Plugins â Plugin settings are now synced in real-time across users, preventing configuration conflicts when multiple people are working on complex setups simultaneously.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h6"&gt;AI &amp;amp; LLM Readiness&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;LLM-Ready Documentation â We made our docs AI-friendly with&lt;/p&gt;&lt;code&gt;llms-full.txt&lt;/code&gt;and a "Copy as Markdown" feature on every page, so you can easily feed context to ChatGPT or Claude. Because let's be honest, that's how half of you read documentation now anyway.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;MCP Server â We released a Model Context Protocol (MCP) server that enables AI assistants to interact directly with your DatoCMS projects. It works. Sometimes. We wrote a whole blog post about the "sometimes" part.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;AI Translations â Bulk-translate entire records with OpenAI, Claude, Gemini, or DeepL. Finally, a reason to stop copy-pasting into Google Translate.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Structured Text to Markdown â A new package that turns Structured Text fields back into clean, CommonMark-compatible Markdown: perfect for LLM pipelines or migration scripts.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h6"&gt;Content Editing Experience&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Inline Blocks in Structured Text â One of our most requested features! You can now insert blocks directly inside Structured Text fields â perfect for inline links, mentions, or notes â unlocking infinite nesting possibilities.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tabular View for Trees â Hierarchical models got a massive upgrade with a new Tabular View, bringing custom columns, pagination, and sorting to tree structures.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Favorite Locales â Editors can now pin their most-used languages to the top of the UI, hiding the noise of unused locales in massive multi-language projects. Finally, some peace for the people managing 40+ locales.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Enhanced Previews â We introduced inline previews for blocks and link fields, letting you see colors, dates, and images directly in the list view without clicking through.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Single Block Presentation â You can now use a Single Block field as a model's presentation title or image, perfect for models where the main info is nested inside a block.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Improved Link Field Filtering â Link fields now correctly filter records by the current locale, eliminating confusion when referencing localized content.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Fixed Headers â We unified the UI with fixed headers across all sections, ensuring that save and publish buttons are always within reach. A small change that sounds boring until you realize how much scrolling it saves.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h6"&gt;API &amp;amp; Tooling Power&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;New CLI cma:call command â You can now call any API method directly from the terminal without writing custom scripts, thanks to dynamic discovery of API resources.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Filter uploads by path â We added a new path filter to the GraphQL API, allowing you to query assets based on their storage path with inclusion, exclusion, and exact matching.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Increased GraphQL Pagination â We bumped the maximum number of items you can fetch in a single GraphQL query from 100 to 500, reducing the number of requests needed for large datasets. Five times more stuff in one go â you're welcome.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Site Search Decoupled â Site Search is now an independent entity, separate from Build Triggers. You can control indexing explicitly and access detailed crawler logs to debug robots.txt and sitemap issues.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Enhanced Build Triggers Activity â We enhanced the Activity view to show events beyond the 30-item limit, with better filtering and detailed logs for every operation.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h6"&gt;Security &amp;amp; Governance&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Access to CDA Playground with Limited Permissions â Developers can now use the GraphQL Playground without needing full API token management permissions, safer for contractors and temporary access.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;All API Tokens are Deletable â For better security hygiene, you can now delete any API token, including the default read-only ones generated by the system.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;API Token Last Used Time â You can now see when each API token was last used directly in Project Settings, making it easy to identify stale tokens and clean up ones that haven't been active in months. Or years. We don't judge.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No Default Full-Access Token â New projects no longer come with a full-access API token by default, encouraging the principle of least privilege from day one.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Improved Roles &amp;amp; Permissions â We revamped the roles interface to clearly show inherited permissions and human-readable summaries of what a user can actually do.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h6"&gt;Workflow &amp;amp; Quality Control&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;DatoCMS Recipes &amp;amp; Import/Export â We launched a marketplace of reusable project "recipes" â pre-built models and blocks you can install into any project to save setup time, powered by the new Schema Import/Export plugin.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Dedicated SEO Fallback Options â We decoupled SEO metadata from internal preview fields, allowing you to set specific fallbacks for SEO titles and images without affecting the CMS UI.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Force Validations on Publishing â You can now prevent the publishing of records that don't meet current validation rules â crucial when you've tightened schema requirements on existing content.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Save Invalid Drafts â Conversely, you can now save drafts even if they are invalid, allowing editors to save their work-in-progress without being blocked by strict validation rules until they are ready to publish. Because sometimes "half-done" is better than "lost."&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Draft Mode by Default â To encourage better editorial workflows, "Draft/Published" mode is now the default setting for all new models.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Smart Confirmation Guardrails â Destructive actions now calculate their impact before execution. If you're about to delete something used in 10+ records, we force a typed confirmation to prevent accidents. We've all been there. This is us protecting you from yourself.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;...and we also cleaned up some tech debt by sunsetting legacy batch endpoints and removing unused CI triggers, keeping the platform lean and fast.&lt;/p&gt;
    &lt;head rend="h2"&gt;Plugins: The ecosystem keeps growing&lt;/head&gt;
    &lt;p&gt;30 new public plugins landed in the marketplace this year â plus countless private ones we'll never see. The community (and our support team!) keeps surprising us with stuff we didn't even know we needed.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;AI Translations â Bulk-translate entire records with OpenAI, Claude, Gemini, or DeepL. Finally, a reason to stop copy-pasting into Google Translate.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Schema Import/Export â Move models between projects without losing your mind. The backbone of our new Recipes feature.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Asset Optimization â Mass-optimize your media library and watch your storage bill shrink.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Custom Text Styles â Add custom marks and styles to Structured Text. Your designers will love you.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Phone Number, Zoned DateTime Picker, Bulk Change Author â Small tools that solve real annoyances.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Infrastructure: The journey to independence&lt;/head&gt;
    &lt;p&gt;This year, DatoCMS handled an average of 3.5B API calls/month (+80%), while serving 500TB of traffic/month and 4.5M optimized video views/month. At the same time, we executed the most ambitious engineering project in our history: a complete migration from Heroku to a custom Kubernetes cluster on AWS.&lt;/p&gt;
    &lt;p&gt;For almost ten years, managed hosting served us well â but by mid-2024, we had hit a ceiling. Costs were rising while our need for granular control grew. We realized we were paying a premium for convenience we no longer needed. It was time to build our own home.&lt;/p&gt;
    &lt;p&gt;The journey began back in October 2024, kicking off a nine-month marathon. We spent the winter prototyping (experimenting with everything from bare metal to alternative PaaS providers â some of which shall remain unnamed to protect the guilty), the spring architecting, and the early summer stress-testing.&lt;/p&gt;
    &lt;p&gt;After months of planning, we flipped the switch on Saturday, June 7th. We prepared for a battle, but we mostly ended up watching dashboards. Aside from a tiny detail that cost us exactly 1 minute of downtime, the transition was flawless. By the time we turned the writes back on, every byte of data had been successfully secured in AWS.&lt;/p&gt;
    &lt;p&gt;The results were immediate and startling:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Speed: Response times for the Content Delivery API (CDA) were halved instantly.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Efficiency: We are now running on 64GB RAM database instances on AWS that handle traffic better than the 256GB instances we used on Heroku. Yes, you read that right. Four times less RAM, better performance.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It was a massive bet, but looking at the metrics today, it is undeniably one of the best wins of our year.&lt;/p&gt;
    &lt;p&gt;We didn't just move servers and DBs; while moving our core applications to AWS EKS was the main event, we executed a total overhaul of the ecosystem surrounding it:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Infrastructure as Code: We codified our entire environment using Terraform, giving us a reproducible, version-controlled blueprint of our infrastructure that eliminates manual configuration drift.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;CDN Caching: We switched from Fastly to Cloudflare for our CDN cache, implementing smarter caching rules that improved our hit ratio from 85% to 97%.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Storage: We migrated from AWS S3 to Cloudflare R2, eliminating massive egress fees and optimizing asset delivery. Goodbye, AWS data transfer bills. We won't miss you.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Observability: We ditched expensive CloudWatch logs for a custom Prometheus &amp;amp; Loki stack, slashing our monitoring bills to near zero while improving data quality.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Developer Experience: To tame Kubernetes complexity, we built cubo, a custom&lt;/p&gt;&lt;code&gt;kubectl&lt;/code&gt;wrapper tailored around our needs that handles everything from generating K8S manifests and orchestrating rollouts to managing cronjobs, real-time logs, and one-off commands, preserving the "git push" and CLI simplicity we loved on Heroku.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Bottom Line: We lowered overall infrastructure costs by over 25%, reduced Content Delivery API latency by 50%, expanded Realtime API capacity by 10Ã, and gained full control across every infrastructure layer. And we kept our sanity. Mostly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Beyond code: Taking control of the books&lt;/head&gt;
    &lt;p&gt;While liberating ourselves from managed hosting, we made another quiet move: we fully internalized our accounting. For years, we outsourced this to external firms â the typical setup where you hand over receipts and hope for the best. But as we grew, flying blind between quarterly reports became untenable.&lt;/p&gt;
    &lt;p&gt;Now we run everything in-house with full visibility into our finances at any moment. No more waiting for external accountants to reconcile things. Same philosophy as the infrastructure migration: control beats convenience when you're building for the long term.&lt;/p&gt;
    &lt;head rend="h2"&gt;Team: Still small by design&lt;/head&gt;
    &lt;p&gt;This year marked our 10th anniversary â a decade of surviving frontend trends, CMS wars, and the occasional existential crisis about whether "headless" is still a cool term. To celebrate, we flew our entire team to the Tuscan countryside to eat, drink, and ride quad bikes. You can read the full story of our trip (and our "25% Matteo concentration rate") here: Dato Turns 10.&lt;/p&gt;
    &lt;p&gt;Despite our growth in revenue and traffic, we remain a team of just 13 people. This isn't an accident â it's a deliberate choice.&lt;/p&gt;
    &lt;p&gt;As we wrote in "How can you be eight people?" (well, now thirteen), building a massive organization is optional. We choose to ignore the pressure to maximize headcount or chase VC funding. Instead, we focus on what actually matters: a solid product, a healthy work-life balance, and staying profitable on our own terms. We don't mind "leaving a little water in the cloth" if it means we get to keep building the software we love, the way we want to build it.&lt;/p&gt;
    &lt;head rend="h2"&gt;What's next?&lt;/head&gt;
    &lt;p&gt;No idea. And honestly, we like it that way.&lt;/p&gt;
    &lt;p&gt;We're not going to pretend we have a five-year vision carved in stone or a slide deck about "the future of content." We'll keep shipping what matters, keep ignoring the hype cycles, and keep cashing checks instead of burning through runway.&lt;/p&gt;
    &lt;p&gt;That said... we may have a few things cooking that we're genuinely excited about. But we're not going to jinx it by overpromising â you'll see them when they ship.&lt;/p&gt;
    &lt;p&gt;Well, see you in 2026. We'll still be here. Probably still 13 people. Definitely still not taking ourselves too seriously. ð§¡&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Financials: Strong growth, with best-in-class margins&lt;/item&gt;
      &lt;item&gt;Partners: More and more of you are joining us&lt;/item&gt;
      &lt;item&gt;Product: Another incredible round of improvements&lt;/item&gt;
      &lt;item&gt;Type Safety &amp;amp; Developer Confidence&lt;/item&gt;
      &lt;item&gt;AI &amp;amp; LLM Readiness&lt;/item&gt;
      &lt;item&gt;Content Editing Experience&lt;/item&gt;
      &lt;item&gt;API &amp;amp; Tooling Power&lt;/item&gt;
      &lt;item&gt;Security &amp;amp; Governance&lt;/item&gt;
      &lt;item&gt;Workflow &amp;amp; Quality Control&lt;/item&gt;
      &lt;item&gt;Plugins: The ecosystem keeps growing&lt;/item&gt;
      &lt;item&gt;Infrastructure: The journey to independence&lt;/item&gt;
      &lt;item&gt;Beyond code: Taking control of the books&lt;/item&gt;
      &lt;item&gt;Team: Still small by design&lt;/item&gt;
      &lt;item&gt;What's next?&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46363319</guid><pubDate>Tue, 23 Dec 2025 07:50:03 +0000</pubDate></item><item><title>Instant database clones with PostgreSQL 18</title><link>https://boringsql.com/posts/instant-database-clones/</link><description>&lt;doc fingerprint="bd19a93f8d981b8a"&gt;
  &lt;main&gt;
    &lt;p&gt;Have you ever watched long running migration script, wondering if it's about to wreck your data? Or wish you can "just" spin a fresh copy of database for each test run? Or wanted to have reproducible snapshots to reset between runs of your test suite, (and yes, because you are reading boringSQL) needed to reset the learning environment?&lt;/p&gt;
    &lt;p&gt;When your database is a few megabytes, &lt;code&gt;pg_dump&lt;/code&gt; and restore works fine. But
what happens when you're dealing with hundreds of megabytes/gigabytes - or more?
Suddenly "just make a copy" becomes a burden.&lt;/p&gt;
    &lt;p&gt;You've probably noticed that PostgreSQL connects to &lt;code&gt;template1&lt;/code&gt; by default. What
you might have missed is that there's a whole templating system hiding in plain
sight. Every time you run&lt;/p&gt;
    &lt;code&gt;CREATE DATABASE dbname;
&lt;/code&gt;
    &lt;p&gt;PostgreSQL quietly clones standard system database &lt;code&gt;template1&lt;/code&gt; behind the
scenes. Making it same as if you would use&lt;/p&gt;
    &lt;code&gt;CREATE DATABASE dbname TEMPLATE template1;
&lt;/code&gt;
    &lt;p&gt;The real power comes from the fact that you can replace &lt;code&gt;template1&lt;/code&gt; with any
database. You can find more at Template Database
documentation.&lt;/p&gt;
    &lt;p&gt;In this article, we will cover a few tweaks that turn this templating system into an instant, zero-copy database cloning machine.&lt;/p&gt;
    &lt;head rend="h2"&gt;CREATE DATABASE ... STRATEGY&lt;/head&gt;
    &lt;p&gt;Before PostgreSQL 15, when you created a new database from a template, it operated strictly on the file level. This was effective, but to make it reliable, Postgres had to flush all pending operations to disk (using &lt;code&gt;CHECKPOINT&lt;/code&gt;) before taking a consistent snapshot. This created a massive I/O
spike - a "Checkpoint Storm" - that could stall your production traffic.&lt;/p&gt;
    &lt;p&gt;Version 15 of PostgreSQL introduced new parameter &lt;code&gt;CREATE DATABASE ... STRATEGY = [strategy]&lt;/code&gt; and at the same time changed the default behaviour how the new
databases are created from templates. The new default become &lt;code&gt;WAL_LOG&lt;/code&gt; which
copies block-by-block via the Write-Ahead Log (WAL), making I/O sequential (and
much smoother) and support for concurrency without facing latency spike. This
prevented the need to CHECKPOINT but made the database cloning operation
potentially significantly slower. For an empty &lt;code&gt;template1&lt;/code&gt;, you won't notice the
difference. But if you try to clone a 500GB database using WAL_LOG, you are
going to be waiting a long time.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;STRATEGY&lt;/code&gt; parameter allows us to switch back to the original method
&lt;code&gt;FILE_COPY&lt;/code&gt; to keep the behaviour, and speed. And since PostgreSQL 18, this
opens the whole new set of options.&lt;/p&gt;
    &lt;head rend="h2"&gt;FILE_COPY&lt;/head&gt;
    &lt;p&gt;Because the &lt;code&gt;FILE_COPY&lt;/code&gt; strategy is a proxy to operating system file operations,
we can change how the OS handles those files.&lt;/p&gt;
    &lt;p&gt;When using standard file system (like &lt;code&gt;ext4&lt;/code&gt;), PostgreSQL reads every byte of
the source file and writes it to a new location. It's a physical copy. However
starting with PostgreSQL 18 - &lt;code&gt;file_copy_method&lt;/code&gt; gives you options to switch
that logic; while default option remains &lt;code&gt;copy&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;With modern filesystems (like ZFS, XFS with reflinks, APFS, etc.) you can switch it to &lt;code&gt;clone&lt;/code&gt; and leverage &lt;code&gt;CLONE&lt;/code&gt; (&lt;code&gt;FICLONE&lt;/code&gt; on Linux) operation for almost
instant operation. And it won't take any additional space.&lt;/p&gt;
    &lt;p&gt;All you have to do is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Linux with XFS or ZFS support (we will use XFS for the demostration) or similar operating system. MacOS APFS is also fully supported. FreeBSD with ZFS also supported (which normally would be my choice, but haven't got time to test so far)&lt;/item&gt;
      &lt;item&gt;PostgreSQL cluster on that file system&lt;/item&gt;
      &lt;item&gt;update the configuration &lt;code&gt;file_copy_method = clone&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;and reload the configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The benchmark&lt;/head&gt;
    &lt;p&gt;We need some dummy data to copy. This is the only part of the tutorial where you have to wait. Let's generate a ~6GB database.&lt;/p&gt;
    &lt;code&gt;CREATE DATABASE source_db;
\c source_db

CREATE TABLE boring_data (
    id serial PRIMARY KEY,
    payload text
);

-- generate 50m rows
INSERT INTO boring_data (payload)
SELECT md5(random()::text) || md5(random()::text)
FROM generate_series(1, 50000000);

-- force a checkpoint
CHECKPOINT;
&lt;/code&gt;
    &lt;p&gt;You can verify the database now has roughly 6GB of data.&lt;/p&gt;
    &lt;code&gt;Name              | source_db
Owner             | postgres
Encoding          | UTF8
Locale Provider   | libc
Collate           | en_US.UTF-8
Ctype             | en_US.UTF-8
Locale            |
ICU Rules         |
Access privileges |
Size              | 6289 MB
Tablespace        | pg_default
Description       |
&lt;/code&gt;
    &lt;p&gt;While enabling &lt;code&gt;\timing&lt;/code&gt; you can test the default (WAL_LOG) strategy. And on my
test volume (relatively slow storage) I get&lt;/p&gt;
    &lt;code&gt;CREATE DATABASE slow_copy TEMPLATE source_db;
CREATE DATABASE
Time: 67000.615 ms (01:07.001)
&lt;/code&gt;
    &lt;p&gt;Now, let's verify our configuration is set for speed:&lt;/p&gt;
    &lt;code&gt;show file_copy_method;
 file_copy_method
------------------
 clone
(1 row)
&lt;/code&gt;
    &lt;p&gt;Let's request the semi-instant clone of the same database, without taking extra disk space at the same time.&lt;/p&gt;
    &lt;code&gt;CREATE DATABASE fast_clone TEMPLATE source_db STRATEGY=FILE_COPY;
CREATE DATABASE
Time: 212.053 ms
&lt;/code&gt;
    &lt;p&gt;That's a quite an improvement, isn't it?&lt;/p&gt;
    &lt;head rend="h2"&gt;Working with cloned data&lt;/head&gt;
    &lt;p&gt;That was the simple part. But what is happening behind the scenes?&lt;/p&gt;
    &lt;p&gt;When you clone a database with &lt;code&gt;file_copy_method = clone&lt;/code&gt;, PostgreSQL doesn't
duplicate any data. The filesystem creates new metadata entries that point to
the same physical blocks. Both databases share identical storage.&lt;/p&gt;
    &lt;p&gt;This can create some initial confusion. If you ask PostgreSQL for the size:&lt;/p&gt;
    &lt;code&gt;SELECT pg_database_size('source_db') as source,
       pg_database_size('fast_clone') as clone;
&lt;/code&gt;
    &lt;p&gt;PostgreSQL reports both as ~6GB because that's the logical size - how much data each database "contains" - i.e. logical size.&lt;/p&gt;
    &lt;code&gt;-[ RECORD 1 ]------
source | 6594041535
clone  | 6594041535
&lt;/code&gt;
    &lt;p&gt;The interesting part happens when you start writing. PostgreSQL doesn't update tuples in place. When you UPDATE a row, it writes a new tuple version somewhere (often a different page entirely) and marks the old one as dead. The filesystem doesn't care about PostgreSQL internals - it just sees writes to 8KB pages. Any write to a shared page triggers a copy of that entire page.&lt;/p&gt;
    &lt;p&gt;A single UPDATE will therefore trigger copy-on-write on multiple pages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the page holding the old tuple&lt;/item&gt;
      &lt;item&gt;the page receiving the new tuple&lt;/item&gt;
      &lt;item&gt;index pages if any indexed columns changed&lt;/item&gt;
      &lt;item&gt;FSM and visibility map pages as PostgreSQL tracks free space&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And later, VACUUM touches even more pages while cleaning up dead tuples. In this case diverging quickly from the linked storage.&lt;/p&gt;
    &lt;head rend="h2"&gt;XFS proof&lt;/head&gt;
    &lt;p&gt;Using the database OID and relfilenode we can verify the both databases are now sharing physical blocks.&lt;/p&gt;
    &lt;code&gt;root@clone-demo:/var/lib/postgresql# sudo filefrag -v /var/lib/postgresql/18/main/base/16402/16404
Filesystem type is: 58465342
File size of /var/lib/postgresql/18/main/base/16402/16404 is 1073741824 (262144 blocks of 4096 bytes)
 ext:     logical_offset:        physical_offset: length:   expected: flags:
   0:        0..    2031:   10471550..  10473581:   2032:             shared
   1:     2032..   16367:   10474098..  10488433:  14336:   10473582: shared
   2:    16368..   32751:   10497006..  10513389:  16384:   10488434: shared
   3:    32752..   65519:   10522066..  10554833:  32768:   10513390: shared
   4:    65520..  129695:   10571218..  10635393:  64176:   10554834: shared
   5:   129696..  195231:   10635426..  10700961:  65536:   10635394: shared
   6:   195232..  262143:   10733730..  10800641:  66912:   10700962: last,shared,eof
/var/lib/postgresql/18/main/base/16402/16404: 7 extents found
root@clone-demo:/var/lib/postgresql#
root@clone-demo:/var/lib/postgresql#
root@clone-demo:/var/lib/postgresql# sudo filefrag -v /var/lib/postgresql/18/main/base/16418/16404
Filesystem type is: 58465342
File size of /var/lib/postgresql/18/main/base/16418/16404 is 1073741824 (262144 blocks of 4096 bytes)
 ext:     logical_offset:        physical_offset: length:   expected: flags:
   0:        0..    2031:   10471550..  10473581:   2032:             shared
   1:     2032..   16367:   10474098..  10488433:  14336:   10473582: shared
   2:    16368..   32751:   10497006..  10513389:  16384:   10488434: shared
   3:    32752..   65519:   10522066..  10554833:  32768:   10513390: shared
   4:    65520..  129695:   10571218..  10635393:  64176:   10554834: shared
   5:   129696..  195231:   10635426..  10700961:  65536:   10635394: shared
   6:   195232..  262143:   10733730..  10800641:  66912:   10700962: last,shared,eof
/var/lib/postgresql/18/main/base/16418/16404: 7 extents found
&lt;/code&gt;
    &lt;p&gt;All it takes is to update some rows using&lt;/p&gt;
    &lt;code&gt;update boring_data set payload = 'new value' || id where id IN (select id from boring_data limit 20);
&lt;/code&gt;
    &lt;p&gt;and the situation will start to change.&lt;/p&gt;
    &lt;code&gt;root@clone-demo:/var/lib/postgresql# sudo filefrag -v /var/lib/postgresql/18/main/base/16402/16404
Filesystem type is: 58465342
File size of /var/lib/postgresql/18/main/base/16402/16404 is 1073741824 (262144 blocks of 4096 bytes)
 ext:     logical_offset:        physical_offset: length:   expected: flags:
   0:        0..      39:   10471550..  10471589:     40:
   1:       40..    2031:   10471590..  10473581:   1992:             shared
   2:     2032..   16367:   10474098..  10488433:  14336:   10473582: shared
   3:    16368..   32751:   10497006..  10513389:  16384:   10488434: shared
   4:    32752..   65519:   10522066..  10554833:  32768:   10513390: shared
   5:    65520..  129695:   10571218..  10635393:  64176:   10554834: shared
   6:   129696..  195231:   10635426..  10700961:  65536:   10635394: shared
   7:   195232..  262143:   10733730..  10800641:  66912:   10700962: last,shared,eof
/var/lib/postgresql/18/main/base/16402/16404: 7 extents found
root@clone-demo:/var/lib/postgresql# sudo filefrag -v /var/lib/postgresql/18/main/base/16418/16404
Filesystem type is: 58465342
File size of /var/lib/postgresql/18/main/base/16418/16404 is 1073741824 (262144 blocks of 4096 bytes)
 ext:     logical_offset:        physical_offset: length:   expected: flags:
   0:        0..      39:   10297326..  10297365:     40:
   1:       40..    2031:   10471590..  10473581:   1992:   10297366: shared
   2:     2032..   16367:   10474098..  10488433:  14336:   10473582: shared
   3:    16368..   32751:   10497006..  10513389:  16384:   10488434: shared
   4:    32752..   65519:   10522066..  10554833:  32768:   10513390: shared
   5:    65520..  129695:   10571218..  10635393:  64176:   10554834: shared
   6:   129696..  195231:   10635426..  10700961:  65536:   10635394: shared
   7:   195232..  262143:   10733730..  10800641:  66912:   10700962: last,shared,eof
/var/lib/postgresql/18/main/base/16418/16404: 8 extents found
root@clone-demo:/var/lib/postgresql#
&lt;/code&gt;
    &lt;p&gt;In this case extent 0 no longer has shared flag, first 40 blocks size (with default size 4KB) now diverge, making it total of 160KB. Each database now has its own copy at different physical address. The remaining extents are still shared.&lt;/p&gt;
    &lt;head rend="h2"&gt;Things to be aware of&lt;/head&gt;
    &lt;p&gt;Cloning is tempting but there's one serious limitation you need to be aware if you ever attempt to do it in production. The source database can't have any active connections during cloning. This is a PostgreSQL limitation, not a filesystem one. For production use, this usually means you create a dedicated template database rather than cloning your live database directly. Or given the relatively short time the operation takes you have to schedule the cloning in times where you can temporary block/terminate all connections.&lt;/p&gt;
    &lt;p&gt;Other limitation is that the cloning only works within a single filesystem. If your databases spans multiple table spaces on different mount points, cloning will fall back to regular physical copy.&lt;/p&gt;
    &lt;p&gt;Finally, in most managed cloud environments (AWS RDS, Google Cloud SQL), you will not have access to the underlying filesystem to configure this. You are stuck with their proprietary (and often billed) functionality. But for your own VMs or bare metal? Go ahead and try it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46363360</guid><pubDate>Tue, 23 Dec 2025 07:58:25 +0000</pubDate></item><item><title>Font with Built-In Syntax Highlighting (2024)</title><link>https://blog.glyphdrawing.club/font-with-built-in-syntax-highlighting/</link><description>&lt;doc fingerprint="a22f9c78ab11c3ad"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Syntax Highlighting in Hand-Coded Websites&lt;/head&gt;
    &lt;head rend="h3"&gt;The problem&lt;/head&gt;
    &lt;p&gt;I have been trying to identify practical reasons why hand-coding websites with HTML and CSS is so hard (by hand-coding, I mean not relying on frameworks, generators or 3rd party scripts that modify the DOM).&lt;/p&gt;
    &lt;p&gt;Let's say, I want to make a blog. What are the actual things that prevent me from making—and maintaining—it by hand? What would it take to clear these roadblocks?&lt;/p&gt;
    &lt;p&gt;There are many, of course, but for a hand-coded programming oriented blog one of these roadblocks is syntax highlighting.&lt;/p&gt;
    &lt;p&gt;When I display snippets of code, I want to make the code easy to read and understand by highlighting it with colors. To do that, I would normally need to use a complex syntax highlighter library, like Prism or highlight.js. These scripts work by scanning and chopping up the code into small language-specific patterns, then wrapping each part in tags with special styling that creates the highlighted effect, and then injecting the resulting HTML back into the page.&lt;/p&gt;
    &lt;p&gt;But, I want to write code by hand. I don't want any external scripts to inject things I didn't write myself. Syntax highlighters also add to the overall complexity and bloat of each page, which I'm trying to avoid. I want to keep things as simple as possible.&lt;/p&gt;
    &lt;head rend="h3"&gt;Leveraging OpenType features to build a simple syntax highlighter inside the font&lt;/head&gt;
    &lt;p&gt;This lead me to think: could it be possible to build syntax highlighting directly into a font, skipping JavaScript altogether? Could I somehow leverage OpenType features, by creating colored glyphs with the COLR table, and identifying and substituting code syntax with contextual alternates?&lt;/p&gt;
    &lt;code&gt;&amp;lt;div class="spoilers"&amp;gt;
  &amp;lt;strong&amp;gt;Yes, it's possible!&amp;lt;/strong&amp;gt;
  &amp;lt;small&amp;gt;...to some extent =)&amp;lt;/small&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;
    &lt;p&gt;The colors in the HTML snippet above comes from within the font itself, the code is plain text, and requires no JavaScript.&lt;/p&gt;
    &lt;p&gt;To achieve that, I modified an open source font Monaspace Krypton to include colored versions of each character, and then used OpenType contextual alternates to essentially find &amp;amp; replace specific strings of text based on HTML, CSS and JS syntax. The result is a simple syntax highlighter, built-in to the font itself.&lt;/p&gt;
    &lt;p&gt;If you want to try it yourself, download the font: FontWithASyntaxHighlighter-Regular.woff2&lt;/p&gt;
    &lt;p&gt;And include the following bits of CSS:&lt;/p&gt;
    &lt;code&gt;@font-face {
  font-family: 'FontWithASyntaxHighlighter';
  src: 
    url('/FontWithASyntaxHighlighter-Regular.woff2') 
    format('woff2')
  ;
}
code {
  font-family: "FontWithASyntaxHighlighter", monospace;
}
&lt;/code&gt;
    &lt;p&gt;And that's it!&lt;/p&gt;
    &lt;head rend="h2"&gt;What are the Pros and Cons of this method?&lt;/head&gt;
    &lt;p&gt;This method opens up some interesting possibilities...&lt;/p&gt;
    &lt;head rend="h3"&gt;Pros&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install is as easy as using any custom font.&lt;/item&gt;
      &lt;item&gt;Works without JavaScript.&lt;/item&gt;
      &lt;item&gt;Works without CSS themes.&lt;/item&gt;
      &lt;item&gt;...but can be themed with CSS.&lt;/item&gt;
      &lt;item&gt;It's fast.&lt;/item&gt;
      &lt;item&gt;Snippets of code can be put into &lt;code&gt;&amp;lt;pre&amp;gt;&lt;/code&gt;and&lt;code&gt;&amp;lt;code&amp;gt;&lt;/code&gt;, with no extra classes or&lt;code&gt;&amp;lt;span&amp;gt;&lt;/code&gt;s.&lt;/item&gt;
      &lt;item&gt;Clean HTML source code.&lt;/item&gt;
      &lt;item&gt;Works everywhere that supports OpenType features, like InDesign.&lt;/item&gt;
      &lt;item&gt;Doesn't require maintenance or updating.&lt;/item&gt;
      &lt;item&gt;Works in &lt;code&gt;&amp;lt;textarea&amp;gt;&lt;/code&gt;and&lt;code&gt;&amp;lt;input&amp;gt;&lt;/code&gt;! Syntax highlighting inside&lt;code&gt;&amp;lt;textarea&amp;gt;&lt;/code&gt;has been previously impossible, because textareas and inputs can only contain plain text. This is where the interesting possibilities lie. As a demo, I made this tiny HTML, CSS &amp;amp; JS sandbox, with native undo and redo, in a single, web component.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;tiny HTML &amp;amp; CSS sandbox =)&lt;/p&gt;
    &lt;head rend="h3"&gt;Cons&lt;/head&gt;
    &lt;p&gt;There are, of course, some limitations to this method. It is not a direct replacement to the more robust syntax highligting libraries, but works well enough for simple needs.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Making modifications to the syntax highligher, like adding more language supports or changing the look of the font, requires modifying the font file. This requires some knowledge of font production, which most people don't have.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It only works where OpenType is supported. Fortunately, that's all major browsers and most modern programs. However, something like PowerPoint doesn't support OpenType.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Finding patterns in text with OpenType contextual alternates is quite basic, and is no match for scripts that use regular expressions. For example, words within&lt;/p&gt;&lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt;tags that are JS keywords will be always highlighted:&lt;code&gt;&amp;lt;p&amp;gt;if I throw this Object through the window, catch it, for else it’ll continue to Infinity &amp;amp; break&amp;lt;/p&amp;gt;&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multiline highlighting with manual line breaks will sadly not work.&lt;/p&gt;
        &lt;p&gt;This is common, for example, in comment blocks and template literals:&lt;/p&gt;
        &lt;code&gt;&amp;lt;!-- This line gets highlighted... but not this, because I made a manual line break... --&amp;gt;&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How does it actually work?&lt;/head&gt;
    &lt;p&gt;Here's roughly how it works. There are two features in OpenType that make this possible: OpenType COLR table and contextual alternates.&lt;/p&gt;
    &lt;head rend="h3"&gt;OpenType COLR table&lt;/head&gt;
    &lt;p&gt;OpenType COLR table makes multi-colored fonts possible. There is a good guide on creating a color font using Glyphs.&lt;/p&gt;
    &lt;p&gt;I made a palette with 8 colors.&lt;/p&gt;
    &lt;p&gt;I duplicated letters &lt;code&gt;A&lt;/code&gt; &lt;code&gt;–&lt;/code&gt; &lt;code&gt;Z&lt;/code&gt;, numbers &lt;code&gt;0&lt;/code&gt; &lt;code&gt;–&lt;/code&gt; &lt;code&gt;9&lt;/code&gt; and the characters &lt;code&gt;.&lt;/code&gt; &lt;code&gt;#&lt;/code&gt; &lt;code&gt;*&lt;/code&gt; &lt;code&gt;-&lt;/code&gt; and &lt;code&gt;_&lt;/code&gt; four times. Each duplicated character is then suffixed with .alt, .alt2, .alt3 or .alt4, and then assigned a color from the palette. For example, all .alt1 glyphs are &lt;code&gt;this&lt;/code&gt; color.&lt;/p&gt;
    &lt;p&gt;I also duplicated all characters twice, and gave them suffices .alt1 and .alt5 and assigned them colors used in &lt;code&gt;&amp;lt;!-- comment blocks --&amp;gt;&lt;/code&gt; and &lt;code&gt;"strings within quotes"&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;The two other colors I used for symbols &lt;code&gt;&amp;amp; | $ + − = ~ [] () {} / ; : " @ %&lt;/code&gt; and &lt;code&gt;'&lt;/code&gt;, and they are always in one color. Numbers &lt;code&gt;0 1 2 3 4 5 6 7 8 9&lt;/code&gt; are also always a certain color, unless overriden by other rules.&lt;/p&gt;
    &lt;head rend="h3"&gt;OpenType contextual alternates&lt;/head&gt;
    &lt;p&gt;The second required feature is OpenType contextual alternates. Here's a great introductory guide to advanced contextual alternates for Glyphs.&lt;/p&gt;
    &lt;p&gt;Contextual alternates makes characters "aware" of their adjacent characters. An example would be fonts that emulate continuous hand writing, where how a letter connects depends on which letter it connects to. There is a nice article covering possible uses here.&lt;/p&gt;
    &lt;head rend="h4"&gt;JavaScript syntax rules&lt;/head&gt;
    &lt;p&gt;The core feature of contextual alternates is substituting glyphs. Here is a simplified code for finding the JavaScript keyword &lt;code&gt;if&lt;/code&gt; and substituting the letters i and f with their colored variant:&lt;/p&gt;
    &lt;code&gt;sub i' f by i.alt2;
sub i.alt2 f' by f.alt2;
&lt;/code&gt;
    &lt;p&gt;In English:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;When i is followed by f, substitute the default i with an alternate (i.alt2).&lt;/item&gt;
      &lt;item&gt;When i.alt2 is followed by f, substitute the default f with an alternate (f.alt2).&lt;/item&gt;
      &lt;item&gt;As a result, every "if" in text gets substituted with &lt;code&gt;if&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;OpenType doesn't support many-to-many substitutions directly, but @behdad on Mastodon had a great suggestion: keywords could be elegantly colored by chaining contextual substitutions.&lt;/p&gt;
    &lt;p&gt;To do this, I made a lookup which substitutes each letter with its colored variant.&lt;/p&gt;
    &lt;code&gt;lookup ALT_SUBS {
    sub a by a.alt; 
    sub b by b.alt; 
    sub c by c.alt; 
    [etc.]
    sub Y by Y.alt;
    sub Z by Z.alt;
} ALT_SUBS;
&lt;/code&gt;
    &lt;p&gt;I moved this lookup rule to the Prefix section, which just means it doesn't get applied automatically unlike the other lookups.&lt;/p&gt;
    &lt;p&gt;Then, I made a lookup rule for each keyword in the contextual alternates section. Here's one for &lt;code&gt;console&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;lookup console {
    ignore sub @AllLetters c' o' n' s' o' l' e';
    ignore sub c' o' n' s' o' l' e' @AllLetters;
    sub c' lookup ALT_SUBS
        o' lookup ALT_SUBS
        n' lookup ALT_SUBS
        s' lookup ALT_SUBS
        o' lookup ALT_SUBS
        l' lookup ALT_SUBS
        e' lookup ALT_SUBS;
} console;
&lt;/code&gt;
    &lt;p&gt;First two lines tells it to ignore strings like &lt;code&gt;Xconsole&lt;/code&gt; or &lt;code&gt;consoles&lt;/code&gt;, but not if there's a period like &lt;code&gt;console.log()&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The third line starts by replacing the first letter 'c' with its colored variant &lt;code&gt;c&lt;/code&gt;, by using definitions from the other lookup table "ALT_SUBS". This repeats until each letter is replaced by its color variant, and the result is &lt;code&gt;console&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Identifying JavaScript keywords is fairly straightforward. Logic is the same for each keyword, and I used a python script to generate them.&lt;/p&gt;
    &lt;head rend="h4"&gt;HTML &amp;amp; CSS syntax rules&lt;/head&gt;
    &lt;p&gt;But for HTML and CSS... I had to get a bit more creative. There are simply too many keywords for both HTML and CSS combined. Making a separate rule for each keyword would inflate the file size.&lt;/p&gt;
    &lt;p&gt;Instead, I came up with this monstrosity. Here's how I find CSS value functions:&lt;/p&gt;
    &lt;code&gt;lookup CssParamCalt useExtension {
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' parenleft by @CssParamAlt4;
} CssParamCalt;
&lt;/code&gt;
    &lt;p&gt;@CssParam is a custom OpenType glyph class I've set up. It includes the characters &lt;code&gt;A&lt;/code&gt; &lt;code&gt;–&lt;/code&gt; &lt;code&gt;Z&lt;/code&gt;, &lt;code&gt;a&lt;/code&gt; &lt;code&gt;–&lt;/code&gt; &lt;code&gt;z&lt;/code&gt;, and &lt;code&gt;-&lt;/code&gt;, which are all the possible characters used in CSS value function names. Because the longest possible CSS value function name is &lt;code&gt;repeating-linear-gradient()&lt;/code&gt;, with 25 letters, the first line of the lookup starts with @CssParam repeated 25 times, followed by parenleft (&lt;code&gt;(&lt;/code&gt;). This lookup will match any word up to 25 letters long, if it's immediately followed by an opening parenthesis. When a match occurs, it substitutes the matched text with its alternate color form (@CssParamAlt4).&lt;/p&gt;
    &lt;p&gt;This lookup works for both CSS and JavaScript. It will colorize standard CSS functions like &lt;code&gt;rgb()&lt;/code&gt; as well as custom JavaScript functions like &lt;code&gt;myFunction()&lt;/code&gt;. The result is a semi-flexible syntax highlighter that doesn't require complex parsing. The downside is that if you have a really long function name, it stops working midway: &lt;code&gt;aReallyLongFunctionNameStopsWorkingMidway()&lt;/code&gt;. I've repeated the same principle for finding HTML tags and attributes, and for CSS selectors and parameters.&lt;/p&gt;
    &lt;head rend="h4"&gt;Unknown length rules&lt;/head&gt;
    &lt;p&gt;Comment blocks and strings between quotes also required extra care, because their length can be anything. OpenType doesn't support loops or anything resembling regular expressions. For example, I can't just tell it to simply substitute everything it finds between two quotes.&lt;/p&gt;
    &lt;p&gt;However, I got a great suggestion from @penteract on Hacker News to use a finite state machine for these kinds of situations. Here our aim is to colorize eveything between /* and */ gray:&lt;/p&gt;
    &lt;code&gt;lookup CSScomment useExtension {
  // stop if we encounter a colored */
  ignore sub asterisk.alt1 slash.alt1 @All';

  // color first letter after /*
  sub slash asterisk @All' by @AllAlt1;
  sub slash asterisk space @All' by @AllAlt1;
  
  // color /* itself
  sub slash' asterisk by slash.alt1;
  sub slash.alt1 asterisk' by asterisk.alt1;
  
  // finite state machine to color rest of the characters
  // or until ignore condition is met
  sub @AllAlt1 @All' by @AllAlt1;
} CSScomment;
&lt;/code&gt;
    &lt;p&gt;The last line is the important one. The lookup will just continue replacing characters if the previous character is already colored.&lt;/p&gt;
    &lt;head rend="h3"&gt;End note&lt;/head&gt;
    &lt;p&gt;The full process is a little bit too convoluted to go into step-by-step, but if you're a type designer who wants to do this with their own font, don't hesitate to contact me.&lt;/p&gt;
    &lt;p&gt;I'm also not an OpenType expert, so I'm sure the substitution logics could be improved upon. If you're interested in learning more about OpenType, I recommend reading The OpenType Cookbook and the complete OpenType™ Feature File Specification.&lt;/p&gt;
    &lt;p&gt;If you have any ideas, suggestions or feedback, let me know. You can reach me at &lt;code&gt;hlotvonen@gmail.com&lt;/code&gt; or leave a comment on Mastodon.&lt;/p&gt;
    &lt;head rend="h2"&gt;Changing the color theme&lt;/head&gt;
    &lt;p&gt;You can change the color theme with CSS &lt;code&gt;override-colors&lt;/code&gt;! Browser support is great at ~94%.&lt;/p&gt;
    &lt;code&gt;
        var const let for while
        function() linear-gradient()
        .myDiv{ background-color: pink; }
        console.log("hello", true)
        /* comment */
        &amp;amp; | $ + − = ~ [] () {} / ; : " @ % 
        0 1 2 3 4 5 6 7 8 9
      &lt;/code&gt;
    &lt;head rend="h2"&gt;Alternative built-in color themes&lt;/head&gt;
    &lt;p&gt;Additionally, two alternative color themes Night Owl, and Light Owl were added by niutech. You can download them from the FontWithASyntaxHighlighter GitHub page. Night Owl theme is made by Sarah Drasner.&lt;/p&gt;
    &lt;p&gt;In order to modify the built-in color palette, you have to edit the font source file. To do so, you can edit the color palettes values in lines 112-120 of the FontWithASyntaxHighlighter.glyphs file and then build the font with fontmake.&lt;/p&gt;
    &lt;head rend="h2"&gt;Projects using this font&lt;/head&gt;
    &lt;p&gt;Here's some cool projects that are using or are inspired by this font:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Holograph is a visual coding tool built on tldraw &amp;amp; its GitHub page&lt;/item&gt;
      &lt;item&gt;@celine/celine is library for building reactive HTML notebooks &amp;amp; its GitHub page &amp;amp; blogpost&lt;/item&gt;
      &lt;item&gt;Shaders art made with pure CSS &amp;amp; its GitHub Page&lt;/item&gt;
      &lt;item&gt;Mdit, a simple Markdown previewer &amp;amp; its GitHub Page&lt;/item&gt;
      &lt;item&gt;Web Component for making a Textarea element into a syntax highlighted codeblock&lt;/item&gt;
      &lt;item&gt;It might also be used as an example for displaying the potential uses for color fonts in the W3C CSS Fonts Module Level 4 specification&lt;/item&gt;
      &lt;item&gt;
        &lt;code-pen&gt;Web Component with syntax highlighting&lt;/code-pen&gt;
      &lt;/item&gt;
      &lt;item&gt;An OpenType font with built-in TEX syntax highlighting&lt;/item&gt;
      &lt;item&gt;Labelary ZPL viewer &amp;amp; editor&lt;/item&gt;
      &lt;item&gt;garten.salat.dev blog&lt;/item&gt;
      &lt;item&gt;L’invasion du HTML mutant&lt;/item&gt;
      &lt;item&gt;(Did you make a project using this font, or know a project that uses it? Let me know please!)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Potential future&lt;/head&gt;
    &lt;p&gt;Many people suggested that this concept could be taken one step further with harfbuzz-wasm. With harfbuzz-wasm a real parser could be used instead of my crazy opentype lookup rules. Essentially, all the cons could be eliminated... Any harfbuzz-wasm experts who wants to take this on?&lt;/p&gt;
    &lt;head rend="h2"&gt;Licence&lt;/head&gt;
    &lt;p&gt;The original font (MonaSpace) has SIL open font license v1.1, which carries over to my modified version. So, you're free to use the font in any way that the SIL v1.1 license permits.&lt;/p&gt;
    &lt;p&gt;As for the code examples, they are MIT licensed. The tiny sandbox web component can be found here: https://github.com/hlotvonen/tinybox&lt;/p&gt;
    &lt;head rend="h2"&gt;Source&lt;/head&gt;
    &lt;p&gt;The original source .glyphs file is hosted in this GitHub repository. UFO files were kindly added by niutech. Or, you can modify the font with some scripting &amp;amp; build with fontmake.&lt;/p&gt;
    &lt;head rend="h2"&gt;More examples&lt;/head&gt;
    &lt;code&gt;as, in, of, if, for, while, finally, var, new, function,
do, return, void, else, break, catch, instanceof, with,
throw, case, default, try, switch, continue, typeof, delete,
let, yield, const, class, get, set, debugger, async, await,
static, import, from, export, extends

true, false, null, undefined, NaN, Infinity

Object, Function, Boolean, Symbol, Math, Date, Number, BigInt, 
String, RegExp, Array, Float32Array, Float64Array, Int8Array, 
Uint8Array, Uint8ClampedArray, Int16Array, Int32Array, Uint16Array, 
Uint32Array, BigInt64Array, BigUint64Array, Set, Map, WeakSet,
WeakMap, ArrayBuffer, SharedArrayBuffer, Atomics, DataView, 
JSON, Promise, Generator, GeneratorFunction, AsyncFunction, 
Reflect, Proxy, Intl, WebAssembly, Error, EvalError, InternalError, 
RangeError, ReferenceError, SyntaxError, TypeError, URIError, 
setInterval, setTimeout, clearInterval, clearTimeout, require, 
exports, eval, isFinite, isNaN, parseFloat, parseInt, decodeURI, 
decodeURIComponent, encodeURI, encodeURIComponent, escape, 
unescape, arguments, this, super, console, window, document, 
localStorage, sessionStorage, module, global
&lt;/code&gt;
    &lt;code&gt;&amp;lt;!-- this is a comment! --&amp;gt;
/* and this */
// and this
&amp;lt;!-- however...
it breaks when your code goes to a newline.
there's no way to keep context line to line...
--&amp;gt;
&lt;/code&gt;
    &lt;code&gt;&amp;lt;!-- can't disable highlighting JS keywords in between tags --&amp;gt;
&amp;lt;p&amp;gt;
  give me a break...
&amp;lt;/p&amp;gt;
&lt;/code&gt;
    &lt;code&gt;&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html lang="en"&amp;gt;
&amp;lt;head&amp;gt;
  &amp;lt;meta charset="UTF-8"&amp;gt;
  &amp;lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&amp;gt;
  &amp;lt;title&amp;gt;Syntax Highlighter Example&amp;lt;/title&amp;gt;
  &amp;lt;style&amp;gt;
    body {
      background-color: rgb(255, 0, 0);
      font-family: 'Arial Narrow', sans-serif;
      line-height: 1.44;
      color: #333;
    }
  &amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
  &amp;lt;header&amp;gt;
    &amp;lt;h1&amp;gt;Welcome to the Syntax Highlighter Test&amp;lt;/h1&amp;gt;
  &amp;lt;/header&amp;gt;
  &amp;lt;nav&amp;gt;
    &amp;lt;ul&amp;gt;
      &amp;lt;li&amp;gt;&amp;lt;a href="#section1"&amp;gt;Section 1&amp;lt;/a&amp;gt;
    &amp;lt;/ul&amp;gt;
  &amp;lt;/nav&amp;gt;
  &amp;lt;main&amp;gt;
    &amp;lt;section id="section1"&amp;gt;
      &amp;lt;h2&amp;gt;Section 1&amp;lt;/h2&amp;gt;
      &amp;lt;p&amp;gt;This is a &amp;lt;span class="highlight"&amp;gt;highlighted&amp;lt;/span&amp;gt; paragraph.&amp;lt;/p&amp;gt;
      &amp;lt;img src="/api/placeholder/300/200" alt="Placeholder image"&amp;gt;
    &amp;lt;/section&amp;gt;
  &amp;lt;/main&amp;gt;
  &amp;lt;script&amp;gt;
    console.log("This is a JavaScript comment");
    function greet(name) {
      return `Hello, ${name}!`;
    }
    document.addEventListener('DOMContentLoaded', () =&amp;gt; {
      console.log(greet('Syntax Highlighter'));
    });
  &amp;lt;/script&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;
    &lt;code&gt;.crazyBackground {
  /* don't try this at home */
  background:
    radial-gradient(
      100% 50% at 50% 50%,
      hsl(90 90% 45%) 0% 5%,
      hsl(250 70% 40%) 50%,
      hsl(50 50% 50%)
    ),
    radial-gradient(
      100% 100% at 50% 25%,
      hsl(90 40% 85%) 30%,
      hsl(40 80% 20%) 60% 90%,
      transparent
    ),
    linear-gradient(
      90deg,
      hsl(150 90% 90%) 0 10%,
      hsl(10 10% 20%),
      hsl(150 90% 90%) 90% 100%
    )
  ;
  background-size:
    5% 10%,
    10% 200%,
    25% 100%
  ;
  background-blend-mode:
    color-dodge,
    difference,
    normal
  ;
  animation: fire2 60s linear infinite;
}

@keyframes fire2 {
  from {
    background-position: 0% 0%, 0 30%, 0 0;
  }

  to {
    background-position: 0% -100%, -100% 30%, 200% 0%;
  }
}
&lt;/code&gt;
    &lt;code&gt;// Variables and constants
let variable = 'Hello';
const CONSTANT = 42;

// Template literals
const name = 'World';
console.log(`${variable}, ${name}!`);

// Function declaration
function greet(name) {
  return `Hello, ${name}!`;
}

// Arrow function
const multiply = (a, b) =&amp;gt; a * b;

// Class definition
class Person {
  constructor(name, age) {
    this.name = name;
    this.age = age;
  }
  sayHello() {
    console.log(`Hello, my name is ${this.name}`);
  }
}

// Object literal
const config = {
  apiKey: 'abc123',
  maxRetries: 3,
  timeout: 5000
};

// Array methods
const numbers = [1, 2, 3, 4, 5];
const doubled = numbers.map(num =&amp;gt; num * 2);
const sum = numbers.reduce((acc, curr) =&amp;gt; acc + curr, 0);

// Async/await
async function fetchData(url) {
  try {
    const response = await fetch(url);
    const data = await response.json();
    return data;
  } catch (error) {
    console.error('Error fetching data:', error);
  }
}

// Destructuring
const { apiKey, maxRetries } = config;
const [first, second, ...rest] = numbers;

// Spread operator
const newArray = [...numbers, 6, 7, 8];

// Conditional (ternary) operator
const isAdult = age &amp;gt;= 18 ? 'Adult' : 'Minor';

// Switch statement
function getDayName(dayNumber) {
  switch (dayNumber) {
    case 0: return 'Sunday';
    case 1: return 'Monday';
    // ... other cases
    default: return 'Invalid day';
  }
}

// Regular expression
const emailRegex = /^[^\s@]+@[^\s@]+\.[^\s@]+$/;

// Symbol
const uniqueKey = Symbol('description');

// Set and Map
const uniqueNumbers = new Set(numbers);
const userRoles = new Map([['admin', 'full'], ['user', 'limited']]);

// Promises
const promise = new Promise((resolve, reject) =&amp;gt; {
  setTimeout(() =&amp;gt; resolve('Done!'), 1000);
});

// Export statement
export { greet, Person };
&lt;/code&gt;
    &lt;head rend="h2"&gt;Discussion&lt;/head&gt;
    &lt;p&gt;I received a lot of great feedback from the discussions at Mastodon and Hacker News.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Thanks to jfk13 on hn, and @pixelambacht on Mastodon for pointing out that 'calt' is turned on by default, and that 'colr' is not an opentype feature that needs to be "turned on".&lt;/p&gt;
    &lt;p&gt;Thanks to penteract on hn and @behdad on Mastodon for suggesting better substitution rules.&lt;/p&gt;
    &lt;p&gt;Thanks to @kizu and @pixelambacht on Mastodon for suggesting color theming with &lt;code&gt;override-colors&lt;/code&gt; CSS rule.&lt;/p&gt;
    &lt;p&gt;As said earlier, if you have any ideas, suggestions or feedback, let me know. You can reach me at &lt;code&gt;hlotvonen@gmail.com&lt;/code&gt; or leave a comment on Mastodon.&lt;/p&gt;
    &lt;p&gt;Thanks to all who sent emails, messages and commented!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46364131</guid><pubDate>Tue, 23 Dec 2025 10:28:09 +0000</pubDate></item><item><title>Meta is using the Linux scheduler designed for Valve's Steam Deck on its servers</title><link>https://www.phoronix.com/news/Meta-SCX-LAVD-Steam-Deck-Server</link><description>&lt;doc fingerprint="b748768b3c946698"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Meta Is Using The Linux Scheduler Designed For Valve's Steam Deck On Its Servers&lt;/head&gt;
    &lt;p&gt; An interesting anecdote from this month's Linux Plumbers Conference in Tokyo is that Meta (Facebook) is using the Linux scheduler originally designed for the needs of Valve's Steam Deck... On Meta Servers. Meta has found that the scheduler can actually adapt and work very well on the hyperscaler's large servers. &lt;lb/&gt;SCX-LAVD as the Latency-criticality Aware Virtual Deadline scheduler has worked out very well for the needs of Valve's Steam Deck with similar or better performance than EEVDF. SCX-LAVD has been worked on by Linux consulting firm Igalia under contract for Valve. SCX-LAVD has also seen varying use by the CachyOS Handheld Edition, Bazzite, and other Linux gaming software initiatives.&lt;lb/&gt;It turns out that besides working well on handhelds, SCX-LAVD can also end up working well on large servers too. The presentation at LPC 2025 by Meta engineers was in fact titled "How do we make a Steam Deck scheduler work on large servers." At Meta they have explored SCX_LAVD as a "default" fleet scheduler for their servers that works for a range of hardware and use-cases for where they don't need any specialized scheduler.&lt;lb/&gt;They call this scheduler built atop sched_ext as "Meta's New Default Scheduler". LAVD they found to work well across the growing CPU and memory configurations of their servers, nice load balancing between CCX/LLC boundaries, and more. Those wishing to learn more about Meta's use and research into SCX-LAVD can find the Linux Plumbers Conference presentation embedded below along with the slide deck.&lt;/p&gt;
    &lt;p&gt;SCX-LAVD as the Latency-criticality Aware Virtual Deadline scheduler has worked out very well for the needs of Valve's Steam Deck with similar or better performance than EEVDF. SCX-LAVD has been worked on by Linux consulting firm Igalia under contract for Valve. SCX-LAVD has also seen varying use by the CachyOS Handheld Edition, Bazzite, and other Linux gaming software initiatives.&lt;/p&gt;
    &lt;p&gt;It turns out that besides working well on handhelds, SCX-LAVD can also end up working well on large servers too. The presentation at LPC 2025 by Meta engineers was in fact titled "How do we make a Steam Deck scheduler work on large servers." At Meta they have explored SCX_LAVD as a "default" fleet scheduler for their servers that works for a range of hardware and use-cases for where they don't need any specialized scheduler.&lt;/p&gt;
    &lt;p&gt;They call this scheduler built atop sched_ext as "Meta's New Default Scheduler". LAVD they found to work well across the growing CPU and memory configurations of their servers, nice load balancing between CCX/LLC boundaries, and more. Those wishing to learn more about Meta's use and research into SCX-LAVD can find the Linux Plumbers Conference presentation embedded below along with the slide deck.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46366998</guid><pubDate>Tue, 23 Dec 2025 17:08:34 +0000</pubDate></item><item><title>Fabrice Bellard Releases MicroQuickJS</title><link>https://github.com/bellard/mquickjs/blob/main/README.md</link><description>&lt;doc fingerprint="3b74d0de090e4684"&gt;
  &lt;main&gt;
    &lt;p&gt;We read every piece of feedback, and take your input very seriously.&lt;/p&gt;
    &lt;p&gt;To see all available qualifiers, see our documentation.&lt;/p&gt;
    &lt;p&gt;There was an error while loading. Please reload this page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46367224</guid><pubDate>Tue, 23 Dec 2025 17:33:42 +0000</pubDate></item><item><title>Towards a secure peer-to-peer app platform for Clan</title><link>https://clan.lol/blog/towards-app-platform-vmtech/</link><description>&lt;doc fingerprint="97bdc952c9982edd"&gt;
  &lt;main&gt;
    &lt;p&gt;While most of the existing Clan framework is dedicated to machine and service management, there’s more on the horizon. Our mission is to make sure peer-to-peer, user-controlled, community software can beat Big Tech solutions. That’s why we’re working on platform fundamentals that would open the way for our FOSS stack to match the usability and convenience of proprietary platforms.&lt;/p&gt;
    &lt;p&gt;Unfortunately, the FOSS world is still lagging behind commercial platforms in some important aspects:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Web and mobile apps are strongly sandboxed, so while they can get very aggressive in snooping on the data they are allowed to access, the enforcement of the isolation model is very robust — and there is a model for sharing data that makes the isolated applications actually useful.. &lt;list rend="ul"&gt;&lt;item&gt;Meanwhile in the FOSS world, it’s still extremely common to run software with full access to the user’s account. The only project that has built anything close to a similar platform for local software is Flatpak, which is still not perfect and its main repo has a very lax policy;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Centralized Web services can have “multiple instances” simply by switching accounts; self-hosting Web services is trivially multi-instance; even Android now provides a multi-instance facility.. &lt;list rend="ul"&gt;&lt;item&gt;Meanwhile local software often doesn’t have a global database, but when it does, it can be impossible to make it multi-instance without advanced knowledge;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Commercial apps come with their own always-online remote servers. Users don’t have to think about connecting the clients to the servers, it’s all pre-connected! &lt;list rend="ul"&gt;&lt;item&gt;Meanwhile decentralized community software is stuck between various bad options. Supporting multiple commercial backends is tedious and defeats the point anyway. Self-hosting traditional web servers can get complex and unreliable, and exposes attack surface to the public Web. Direct peer-to-peer connections can be hard to set up and unreliable too, and typically don’t provide asynchronous communication.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So… What do we need to make it possible for communities to share apps install and load quickly, already pre-connected to network services; are isolated to a worry-free level of security, and yet allow for enough sharing via explicit permissions to make them useful?&lt;/p&gt;
    &lt;p&gt;The first piece of the puzzle is, unsurprisingly, Nix. The entire Clan project is built on Nix, and the future app platform is no exception. Nix makes it possible to quickly fetch and run any software – thanks to caching, as long as we steer everyone towards using very few common versions of the nixpkgs tree, most downloads could be almost as fast as web app loads.&lt;/p&gt;
    &lt;p&gt;Then we have to add a microVM hypervisor with Wayland and GPU virtualization and a side of D-Bus portals… and we can finally get a glimpse of the future!&lt;/p&gt;
    &lt;head rend="h2"&gt;microVMs&lt;/head&gt;
    &lt;p&gt;Secure isolation is essential for any modern app platform. Hardware-based virtualization is a lot more confidence-inspiring than shared-kernel isolation mechanisms like Linux namespaces. But it’s not only a security measure. Running apps in VMs also improves environment consistency/reproducibility by ensuring everyone runs the same kernel — which can also give us portability, since it enables running on completely different host OSes as well.&lt;/p&gt;
    &lt;p&gt;If your experience with virtualization on desktop has only been with booting entire Linux distros under something like VirtualBox, you might be very skeptical of the same technology being involved in launching applications all the time. But that’s not at all inherent to the use of KVM!&lt;/p&gt;
    &lt;p&gt;Conventional VMs feel “heavy” —slow to launch, big RAM footprint, extra background CPU usage, fixed storage allocation, usually not very well integrated with the host desktop— only because their goal is to simulate a whole another computer within your existing computer. For app isolation, we don’t need that, so the whole stack can be vastly simplified and optimized for high performance and low overhead. The microVM idea was first popularized by AWS’s Firecracker on the server side, powering instantly-launching event/request handlers in Lambda. A microVM boots directly into the kernel (skipping firmware) and does not emulate any legacy PC hardware, which results in very fast boot times, on the order of a couple hundred milliseconds.&lt;/p&gt;
    &lt;p&gt;Now, has this been used on the client side already? Yes, most prominently by Asahi Linux, motivated by a technical restriction that was preventing Apple machines from playing legacy Windows games. That’s the muvm project, powered by libkrun – a Firecracker-like VMM provided as a dynamic library so that different frontends could be built. For our platform development, we have indeed adopted muvm (after submitting some changes that make it more useful for us), combining it with namespace-based Bubblewrap to make a script that runs NixOS system closures in microVMs.&lt;/p&gt;
    &lt;p&gt;…Wait, did someone mention playing games– like, highly GPU-demanding games? In a VM? Without a dedicated GPU?&lt;/p&gt;
    &lt;head rend="h2"&gt;Desktop and GPU support&lt;/head&gt;
    &lt;p&gt;In the Beginning (of virtio-gpu), there was the Framebuffer. An emulated computer monitor, a single rectangle representing the entire graphical output of the VM. Then there was VirGL, a way to forward the OpenGL API across the VM boundary to make the host render on its GPU on behalf of the VM, so that 3D graphics could be displayed on the emulated monitor. It wasn’t super fast, it wasn’t compatible with the latest GL extensions, it wasn’t very secure, but it was something. With the advent of Vulkan, Venus was started as the Vulkan version of the same thing.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the Chrome OS team was working on adding support for Linux apps. While it was initially based on namespaces, they quickly started working on switching to hardware virtualization. The virtio-gpu device was extended to support arbitrary “cross-domain” protocols, making it possible —with some wrapping-unwrapping— to forward Unix domain sockets that pass certain types of file descriptors (shared memory and DMA-BUF) to the guest. (Well, initially it was a whole separate virtual device but let’s skip over that.) Google’s crosvm supports connecting to the host Wayland socket to that facility, and the team wrote Sommelier as the guest-side proxy that exposes a normal Wayland socket to guest apps.&lt;/p&gt;
    &lt;p&gt;The part of crosvm responsible for handling the virtio-gpu device was written as a reusable library called Rutabaga (now living outside of the CrOS repos), and integrated into other VMMs such as good old Qemu. Sommelier was packaged by various Linux distros as well, and one enthusiast wrote an entire alternative to Sommelier.&lt;/p&gt;
    &lt;p&gt;Meanwhile, there was also a lot to improve in terms of accessing the GPU. As we’ve mentioned already, API forwarding solutions like VirGL/Venus leave a lot to be desired. PCIe passthrough requires a dedicated GPU, or SR-IOV support which GPU vendors have mostly restricted to enterprise models. However… of course a better way was possible! Rob Clark presented DRM native contexts at XDC 2022: this approach essentially paravirtualizes the kernel-space GPU driver, letting the guest submit hardware-specific commands that the host would run in separate contexts (relying on the same separation as between programs on the host). That’s the approach that was picked up by the Asahi Linux project for gaming because of the amazing performance it allows for, but it’s also intended to be a stronger security boundary due to providing way less attack surface on the host (it’s all I/O management rather than implementing complex APIs).&lt;/p&gt;
    &lt;p&gt;So, was it possible to take all of this technology and use it? Well… it required quite a bit of debugging and fixing everywhere – but that’s exactly why I joined! So far I’ve discovered that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the rutabaga_gfx integration in QEMU (which was the thing we tried to use initially) and other C consumers was broken with the latest versions due to an &lt;code&gt;ifdef&lt;/code&gt;mistake (fixed)&lt;/item&gt;
      &lt;item&gt;it’s not documented everywhere that kernel &amp;gt;= 6.13 is required to be able to touch AMD GPU memory from KVM guests in any way&lt;/item&gt;
      &lt;item&gt;Sommelier was assuming Chromium OS kernel patches and misinterpreting &lt;code&gt;ioctl&lt;/code&gt;responses on regular mainline Linux&lt;/item&gt;
      &lt;item&gt;libkrun’s internal version of rutabaga_gfx contained a tiny strange API modification incompatible with Sommelier/proxy-virtwl and didn’t handle memfd seals (fixed)&lt;/item&gt;
      &lt;item&gt;RADV (Radeon Vulkan driver in Mesa) only recognized PCI devices including for virtgpu, ignoring the &lt;code&gt;virtio-mmio&lt;/code&gt;setup used by libkrun (fixed)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And we’re continuing with more work in this area.&lt;/p&gt;
    &lt;head rend="h2"&gt;D-Bus / XDG Desktop Portals&lt;/head&gt;
    &lt;p&gt;Application isolation is great, but completely isolated applications tend to have limited usefulness. That’s why we’re also integrating desktop portals that Flatpak uses —at least the file-opening / document portal— into the microVM-based platform.&lt;/p&gt;
    &lt;p&gt;The sidebus project is inspired by Spectrum’s setup for using the document portal with virtiofs to dynamically expose chosen files to the guest, using vsock as the D-Bus transport. It is based on the busd broker library, and uses the portal frontend on the host for perfect integration with arbitrary desktop environments.&lt;/p&gt;
    &lt;p&gt;With the switch to libkrun however, we are looking at the possibility of making the Camera and Screencast portals working, with full hardware acceleration – by switching to virtgpu cross-domain as the transport instead of vsock. Currently libkrun already has added some PipeWire support to its copy of rutabaga_gfx, however that’s fixed to one system-wide socket. How these portals work is that for every request they pass a new restricted PipeWire remote socket over D-Bus. So we’re looking to make rutabaga’s cross-domain sockets more generic, to be able to just pass through that whole chain of file descriptor passing.&lt;/p&gt;
    &lt;p&gt;(And yes, lots of people are worried about PipeWire attack surface — it’s definitely possible to mitigate that with a proxy on the host that would only allow a small validated subset of the PipeWire protocol.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;We’re looking to finally make a peer-to-peer community software platform that’s competitive with commercial ones in terms of security, usability and convenience. If you want to try it out now, you can! Just follow the installation instructions on our munix project. Note that it’s still actively being developed, so if you find any issues, please open up a bug report!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46367232</guid><pubDate>Tue, 23 Dec 2025 17:34:22 +0000</pubDate></item><item><title>We replaced H.264 streaming with JPEG screenshots (and it worked better)</title><link>https://blog.helix.ml/p/we-mass-deployed-15-year-old-screen</link><description>&lt;doc fingerprint="26854c146509ce1d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;We Mass-Deployed 15-Year-Old Screen Sharing Technology and It's Actually Better&lt;/head&gt;
    &lt;head rend="h3"&gt;Or: How JPEG Screenshots Defeated Our Beautiful H.264 WebCodecs Pipeline&lt;/head&gt;
    &lt;p&gt;Part 2 of our video streaming saga. Read Part 1: How we replaced WebRTC with WebSockets →&lt;/p&gt;
    &lt;head rend="h2"&gt;The Year is 2025 and We’re Sending JPEGs&lt;/head&gt;
    &lt;p&gt;Let me tell you about the time we spent three months building a gorgeous, hardware-accelerated, WebCodecs-powered, 60fps H.264 streaming pipeline over WebSockets...&lt;/p&gt;
    &lt;p&gt;...and then replaced it with &lt;code&gt;grim | curl&lt;/code&gt; when the WiFi got a bit sketchy.&lt;/p&gt;
    &lt;p&gt;I wish I was joking.&lt;/p&gt;
    &lt;head rend="h2"&gt;Act I: Hubris (Also Known As “Enterprise Networking Exists”)&lt;/head&gt;
    &lt;p&gt;We’re building Helix, an AI platform where autonomous coding agents work in cloud sandboxes. Users need to watch their AI assistants work. Think “screen share, but the thing being shared is a robot writing code.”&lt;/p&gt;
    &lt;p&gt;Last week, we explained how we replaced WebRTC with a custom WebSocket streaming pipeline. This week: why that wasn’t enough.&lt;/p&gt;
    &lt;p&gt;The constraint that ruined everything: It has to work on enterprise networks.&lt;/p&gt;
    &lt;p&gt;You know what enterprise networks love? HTTP. HTTPS. Port 443. That’s it. That’s the list.&lt;/p&gt;
    &lt;p&gt;You know what enterprise networks hate?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;UDP — Blocked. Deprioritized. Dropped. “Security risk.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;WebRTC — Requires TURN servers, which requires UDP, which is blocked&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Custom ports — Firewall says no&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;STUN/ICE — NAT traversal? In my corporate network? Absolutely not&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Literally anything fun — Denied by policy&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We tried WebRTC first. Worked great in dev. Worked great in our cloud. Deployed to an enterprise customer.&lt;/p&gt;
    &lt;p&gt;“The video doesn’t connect.”&lt;/p&gt;
    &lt;p&gt;checks network — Outbound UDP blocked. TURN server unreachable. ICE negotiation failing.&lt;/p&gt;
    &lt;p&gt;We could fight this. Set up TURN servers. Configure enterprise proxies. Work with IT departments.&lt;/p&gt;
    &lt;p&gt;Or we could accept reality: Everything must go through HTTPS on port 443.&lt;/p&gt;
    &lt;p&gt;So we built a pure WebSocket video pipeline:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;H.264 encoding via GStreamer + VA-API (hardware acceleration, baby)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Binary frames over WebSocket (L7 only, works through any proxy)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;WebCodecs API for hardware decoding in the browser&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;60fps at 40Mbps with sub-100ms latency&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We were so proud. We wrote Rust. We wrote TypeScript. We implemented our own binary protocol. We measured things in microseconds.&lt;/p&gt;
    &lt;p&gt;Then someone tried to use it from a coffee shop.&lt;/p&gt;
    &lt;head rend="h2"&gt;Act II: Denial&lt;/head&gt;
    &lt;p&gt;“The video is frozen.”&lt;/p&gt;
    &lt;p&gt;“Your WiFi is bad.”&lt;/p&gt;
    &lt;p&gt;“No, the video is definitely frozen. And now my keyboard isn’t working.”&lt;/p&gt;
    &lt;p&gt;checks the video&lt;/p&gt;
    &lt;p&gt;It’s showing what the AI was doing 30 seconds ago. And the delay is growing.&lt;/p&gt;
    &lt;p&gt;Turns out, 40Mbps video streams don’t appreciate 200ms+ network latency. Who knew.&lt;/p&gt;
    &lt;p&gt;When the network gets congested:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Frames buffer up in the TCP/WebSocket layer&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;They arrive in-order (thanks TCP!) but increasingly delayed&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Video falls further and further behind real-time&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You’re watching the AI type code from 45 seconds ago&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;By the time you see a bug, the AI has already committed it to main&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Everything is terrible forever&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;“Just lower the bitrate,” you say. Great idea. Now it’s 10Mbps of blocky garbage that’s still 30 seconds behind.&lt;/p&gt;
    &lt;head rend="h2"&gt;Act III: Bargaining&lt;/head&gt;
    &lt;p&gt;We tried everything:&lt;/p&gt;
    &lt;p&gt;“What if we only send keyframes?”&lt;/p&gt;
    &lt;p&gt;This was our big brain moment. H.264 keyframes (IDR frames) are self-contained. No dependencies on previous frames. Just drop all the P-frames on the server side, send only keyframes, get ~1fps of corruption-free video. Perfect for low-bandwidth fallback!&lt;/p&gt;
    &lt;p&gt;We added a &lt;code&gt;keyframes_only&lt;/code&gt; flag. We modified the video decoder to check &lt;code&gt;FrameType::Idr&lt;/code&gt;. We set GOP to 60 (one keyframe per second at 60fps). We tested.&lt;/p&gt;
    &lt;p&gt;We got exactly ONE frame.&lt;/p&gt;
    &lt;p&gt;One single, beautiful, 1080p IDR frame. Then silence. Forever.&lt;/p&gt;
    &lt;code&gt;[WebSocket] Keyframe received (frame 121), sending
[WebSocket] ...
[WebSocket] ...
[WebSocket] It's been 14 seconds why is nothing else coming
[WebSocket] Failed to send audio frame: Closed&lt;/code&gt;
    &lt;p&gt;checks Wolf logs — encoder still running&lt;/p&gt;
    &lt;p&gt;checks GStreamer pipeline — frames being produced&lt;/p&gt;
    &lt;p&gt;checks Moonlight protocol layer — nothing coming through&lt;/p&gt;
    &lt;p&gt;We’re using Wolf, an excellent open-source game streaming server (seriously, the documentation is great). But our WebSocket streaming layer sits on top of the Moonlight protocol, which is reverse-engineered from NVIDIA GameStream. Somewhere in that protocol stack, something decides that if you’re not consuming P-frames, you’re not ready for more frames. Period.&lt;/p&gt;
    &lt;p&gt;We poked around for an hour or two, but without diving deep into the Moonlight protocol internals, we weren’t going to fix this. The protocol wanted all its frames, or no frames at all.&lt;/p&gt;
    &lt;p&gt;“What if we implement proper congestion control?”&lt;/p&gt;
    &lt;p&gt;looks at TCP congestion control literature&lt;/p&gt;
    &lt;p&gt;closes tab&lt;/p&gt;
    &lt;p&gt;“What if we just... don’t have bad WiFi?”&lt;/p&gt;
    &lt;p&gt;stares at enterprise firewall that’s throttling everything&lt;/p&gt;
    &lt;head rend="h2"&gt;Act IV: Depression&lt;/head&gt;
    &lt;p&gt;One late night, while debugging why the stream was frozen again, I opened our screenshot debugging endpoint in a browser tab:&lt;/p&gt;
    &lt;code&gt;GET /api/v1/external-agents/abc123/screenshot?format=jpeg&amp;amp;quality=70&lt;/code&gt;
    &lt;p&gt;The image loaded instantly.&lt;/p&gt;
    &lt;p&gt;A pristine, 150KB JPEG of the remote desktop. Crystal clear. No artifacts. No waiting for keyframes. No decoder state. Just... pixels.&lt;/p&gt;
    &lt;p&gt;I refreshed. Another instant image.&lt;/p&gt;
    &lt;p&gt;I mashed F5 like a degenerate. 5 FPS of perfect screenshots.&lt;/p&gt;
    &lt;p&gt;I looked at my beautiful WebCodecs pipeline. I looked at the JPEGs. I looked at the WebCodecs pipeline again.&lt;/p&gt;
    &lt;p&gt;No.&lt;/p&gt;
    &lt;p&gt;No, we are not doing this.&lt;/p&gt;
    &lt;p&gt;We are professionals. We implement proper video codecs. We don’t spam HTTP requests for individual frames like it’s 2009.&lt;/p&gt;
    &lt;head rend="h2"&gt;Act V: Acceptance&lt;/head&gt;
    &lt;p&gt;typescript&lt;/p&gt;
    &lt;code&gt;// Poll screenshots as fast as possible (capped at 10 FPS max)
const fetchScreenshot = async () =&amp;gt; {
  const response = await fetch(`/api/v1/external-agents/${sessionId}/screenshot`)
  const blob = await response.blob()
  screenshotImg.src = URL.createObjectURL(blob)
  setTimeout(fetchScreenshot, 100) // yolo
}&lt;/code&gt;
    &lt;p&gt;We did it. We’re sending JPEGs.&lt;/p&gt;
    &lt;p&gt;And you know what? It works perfectly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why JPEGs Actually Slap&lt;/head&gt;
    &lt;p&gt;Here’s the thing about our fancy H.264 pipeline:&lt;/p&gt;
    &lt;p&gt;PropertyH.264 StreamJPEG SpamBandwidth40 Mbps (constant)100-500 Kbps (varies with complexity)StateStateful (corrupt = dead)Stateless (each frame independent)Latency sensitivityVery highDoesn’t careRecovery from packet lossWait for keyframe (seconds)Next frame (100ms)Implementation complexity3 months of Rust&lt;code&gt;fetch()&lt;/code&gt; in a loop&lt;/p&gt;
    &lt;p&gt;A JPEG screenshot is self-contained. It either arrives complete, or it doesn’t. There’s no “partial decode.” There’s no “waiting for the next keyframe.” There’s no “decoder state corruption.”&lt;/p&gt;
    &lt;p&gt;When the network is bad, you get... fewer JPEGs. That’s it. The ones that arrive are perfect.&lt;/p&gt;
    &lt;p&gt;And the size! A 70% quality JPEG of a 1080p desktop is like 100-150KB. A single H.264 keyframe is 200-500KB. We’re sending LESS data per frame AND getting better reliability.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Hybrid: Have Your Cake and Eat It Too&lt;/head&gt;
    &lt;p&gt;We didn’t throw away the H.264 pipeline. We’re not complete animals.&lt;/p&gt;
    &lt;p&gt;Instead, we built adaptive switching:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Good connection (RTT &amp;lt; 150ms): Full 60fps H.264, hardware decoded, buttery smooth&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Bad connection detected: Pause video, switch to screenshot polling&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Connection recovers: User clicks to retry video&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The key insight: we still need the WebSocket for input.&lt;/p&gt;
    &lt;p&gt;Keyboard and mouse events are tiny. Like, 10 bytes each. The WebSocket handles those perfectly even on a garbage connection. We just needed to stop sending the massive video frames.&lt;/p&gt;
    &lt;p&gt;So we added one control message:&lt;/p&gt;
    &lt;p&gt;json&lt;/p&gt;
    &lt;code&gt;{"set_video_enabled": false}&lt;/code&gt;
    &lt;p&gt;Server receives this, stops sending video frames. Client polls screenshots instead. Input keeps flowing. Everyone’s happy.&lt;/p&gt;
    &lt;p&gt;15 lines of Rust. I am not joking.&lt;/p&gt;
    &lt;p&gt;rust&lt;/p&gt;
    &lt;code&gt;if !video_enabled.load(Ordering::Relaxed) {
    continue; // skip frame, it's screenshot time baby
}&lt;/code&gt;
    &lt;head rend="h2"&gt;The Oscillation Problem (Lol)&lt;/head&gt;
    &lt;p&gt;We almost shipped a hilarious bug.&lt;/p&gt;
    &lt;p&gt;When you stop sending video frames, the WebSocket becomes basically empty. Just tiny input events and occasional pings.&lt;/p&gt;
    &lt;p&gt;The latency drops dramatically.&lt;/p&gt;
    &lt;p&gt;Our adaptive mode sees low latency and thinks: “Oh nice! Connection recovered! Let’s switch back to video!”&lt;/p&gt;
    &lt;p&gt;Video resumes. 40Mbps floods the connection. Latency spikes. Mode switches to screenshots.&lt;/p&gt;
    &lt;p&gt;Latency drops. Mode switches to video.&lt;/p&gt;
    &lt;p&gt;Latency spikes. Mode switches to screenshots.&lt;/p&gt;
    &lt;p&gt;Forever. Every 2 seconds.&lt;/p&gt;
    &lt;p&gt;The fix was embarrassingly simple: once you fall back to screenshots, stay there until the user explicitly clicks to retry.&lt;/p&gt;
    &lt;p&gt;typescript&lt;/p&gt;
    &lt;code&gt;setAdaptiveLockedToScreenshots(true) // no oscillation for you
```

We show an amber icon and a message: "Video paused to save bandwidth. Click to retry."

Problem solved. User is in control. No infinite loops.

---

## Ubuntu Doesn't Ship JPEG Support in grim Because Of Course It Doesn't

Oh, you thought we were done? Cute.

`grim` is a Wayland screenshot tool. Perfect for our needs. Supports JPEG output for smaller files.

Except Ubuntu compiles it without libjpeg.
```
$ grim -t jpeg screenshot.jpg
error: jpeg support disabled&lt;/code&gt;
    &lt;p&gt;incredible&lt;/p&gt;
    &lt;p&gt;So now our Dockerfile has a build stage that compiles grim from source:&lt;/p&gt;
    &lt;p&gt;dockerfile&lt;/p&gt;
    &lt;code&gt;FROM ubuntu:25.04 AS grim-build
RUN apt-get install -y meson ninja-build libjpeg-turbo8-dev ...
RUN git clone https://git.sr.ht/~emersion/grim &amp;amp;&amp;amp; \
    meson setup build -Djpeg=enabled &amp;amp;&amp;amp; \
    ninja -C build
```

We're building a screenshot tool from source so we can send JPEGs in 2025. This is fine.

---

## The Final Architecture
```
┌─────────────────────────────────────────────────────────────┐
│                     User's Browser                          │
├─────────────────────────────────────────────────────────────┤
│  WebSocket (always connected)                               │
│  ├── Video frames (H.264) ──────────── when RTT &amp;lt; 150ms    │
│  ├── Input events (keyboard/mouse) ── always               │
│  └── Control messages ─────────────── {"set_video_enabled"} │
│                                                              │
│  HTTP (screenshot polling) ──────────── when RTT &amp;gt; 150ms    │
│  └── GET /screenshot?quality=70                             │
└─────────────────────────────────────────────────────────────┘&lt;/code&gt;
    &lt;p&gt;Good connection: 60fps H.264, hardware accelerated, beautiful Bad connection: 2-10fps JPEGs, perfectly reliable, works everywhere&lt;/p&gt;
    &lt;p&gt;The screenshot quality adapts too:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Frame took &amp;gt;500ms? Drop quality by 10%&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Frame took &amp;lt;300ms? Increase quality by 5%&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Target: minimum 2 FPS, always&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Lessons Learned&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Simple solutions often beat complex ones. Three months of H.264 pipeline work. One 2am hacking session the night before production deployment: “what if we just... screenshots?”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Graceful degradation is a feature. Users don’t care about your codec. They care about seeing their screen and typing.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;WebSockets are for input, not necessarily video. The input path staying responsive is more important than video frames.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Ubuntu packages are missing random features. Always check. Or just build from source like it’s 2005.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Measure before optimizing. We assumed video streaming was the only option. It wasn’t.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Try It Yourself&lt;/head&gt;
    &lt;p&gt;Helix is open source: github.com/helixml/helix&lt;/p&gt;
    &lt;p&gt;The shameful-but-effective screenshot code:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;api/cmd/screenshot-server/main.go&lt;/code&gt;— 200 lines of Go that changed everything&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;MoonlightStreamViewer.tsx&lt;/code&gt;— React component with adaptive logic&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;websocket-stream.ts&lt;/code&gt;— WebSocket client with&lt;code&gt;setVideoEnabled()&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The beautiful H.264 pipeline we’re still proud of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;moonlight-web-stream/&lt;/code&gt;— Rust WebSocket server&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Still used when your WiFi doesn’t suck&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We’re building Helix, open-source AI infrastructure that works in the real world — even on terrible WiFi. We started by killing WebRTC, then we killed our replacement. Sometimes the 15-year-old solution is the right one.&lt;/p&gt;
    &lt;p&gt;Star us on GitHub: github.com/helixml/helix&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46367475</guid><pubDate>Tue, 23 Dec 2025 18:00:31 +0000</pubDate></item><item><title>An initial analysis of the discovered Unix V4 tape</title><link>https://www.spinellis.gr/blog/20251223/?yc261223</link><description>&lt;doc fingerprint="7758171347b74c05"&gt;
  &lt;main&gt;&lt;p&gt;Several news outlets reported the discovery of a 1970s Fourth Edition Research Unix magnetic tape at the University of Utah in July 2025 and its successful restoration. This is a significant find, because up to now only the Fourth Editionâs manual was thought to have survived. Over the past few days I incorporated the tapeâs source code into the Unix History Repository hosted on GitHub (see it here) and studied the codeâs composition.&lt;/p&gt;&lt;p&gt;The Fourth Research Edition Unix came out of the famous AT&amp;amp;T Bell Laboratories in November 1973. A significant development it introduced was the rewriting of large parts of the systemâs kernel in a high-level language (early C) rather than PDP-11 assembly language. The tape contains a complete system dump, including both source code and the compiled binaries and kernel. For inclusion in the Unix history repository, I removed the binaries, to match what is normally put under source code version control.&lt;/p&gt;&lt;code&gt;find $dir -name '*.[oa]' | xargs rm
rm -rf $dir/bin $dir/usr/bin $dir/usr/games $dir/lib $dir/dev
rm $dir/etc/{lpd,init,msh,getty,mkfs,mknod,glob,update,umount,mount}
rm $dir/unix
rm $dir/usr/mdec/[tm]boot $dir/usr/sys/conf/mkconf $dir/usr/fort/fc1
rm $dir/usr/c/cvopt $dir/usr/lib/suftab&lt;/code&gt;&lt;p&gt; As with other source code snapshots included in the Unix history repository, the (synthetic) Git commit timestamps are derived from the file timestamps while the commit authors are derived from a manually-created map file. I updated the existing V4 author map file based on information I had gathered for preceding and following Unix Research editions. I explicitly put &lt;code&gt;ken,dmr&lt;/code&gt;
(Ken Thompson and Dennis Ritchie the systemâs main developers)
in all source code files where I lacked author
information (this is also the default introduced via a &lt;code&gt;.*&lt;/code&gt; regular expression)
to mark missing details.
Two members of the original Bell Labs Unix development team kindly
provided me information to fill some details, such as
the developer of the SNOBOL III interpreter (Ken Thompson)
and the implementer of the math library and emulator (Robert H. Morris).&lt;/p&gt;&lt;p&gt;Some have claimed that the tapeâs contents are very close to the Fifth Edition rather to what really was the Fourth Edition. The reason for this claim is that, in contrast to Unix manual editions (which were formally numbered and give the Unix Research Editions their name) distributed software tapes were mostly a copy of whatever was at the time in the (single) Unix development computer. I set out to see the differences between the two versions. First, I looked at the base file names included in the two.&lt;/p&gt;&lt;code&gt;normalize()
{
sed 's|.*/||' | sort -u
   }

comm  -3 \
&amp;lt;(git ls-tree -r --name-only Research-V4-Snapshot-Development | normalize) \
   &amp;lt;(git ls-tree -r --name-only Research-V5-Snapshot-Development | normalize)   &lt;/code&gt;&lt;p&gt;The above command, which outputs files whose base file name occurs only in one of the two releases, shows only the following files introduced in the Fifth Edition.&lt;/p&gt;&lt;code&gt;        c13.c
        c21.c
        c2h.c
        cmp.c
        ldfps.s&lt;/code&gt;&lt;p&gt;So, the C compiler grew by a few files, and the &lt;code&gt;cmp&lt;/code&gt; (compare) utility
was written in C.&lt;/p&gt;&lt;p&gt;To dig deeper I then run &lt;code&gt;git blame&lt;/code&gt; on each file of the two editions,
to see what parts of preceding editions they incorporated.&lt;/p&gt;&lt;code&gt;# For each edition
for ref in Research-V4-Snapshot-Development \
; do
   Research-V5-Snapshot-Development echo $ref
   # For all the edition's files
   git ls-tree -r --name-only $ref |
   # Exclude administrative files introduced in the repo.
     grep -Ev 'README|LICENSE|\.pdf|\.ref' |
     # Run git-blame on each.
     xargs -I '{}' git blame -M -M -C -C $ref -- '{}' |
     sort |
     # Sum lines for each commit
     uniq -c |
     # Obtain lines and provenance of each commit; output totals.
     awk '{("git show " $2 "| awk '\''/Synthesized-from:/{print $2}'\''") | getline ver; total[ver] += $1 }
           END {for (v in total) print v, total[v]}'
done&lt;/code&gt;&lt;p&gt;The output gave me the following Fourth Editionâs composition in terms of code lines:&lt;/p&gt;&lt;code&gt;v4 75676
v3 6590
v2 168&lt;/code&gt;&lt;p&gt;This shows a lot of new material and about 10% coming from earlier editions.&lt;/p&gt;&lt;p&gt;The corresponding output for the Fifth Edition is as follows.&lt;/p&gt;&lt;code&gt;v5 11181
v4 52238
v3 3296
v2 168&lt;/code&gt;&lt;p&gt;This shows that 52 thousand lines of the Fourth Edition are indeed part of the Fifth Edition, but the Fifth Edition also introduces about eleven new thousand lines of code. This is not an insignificant amount.&lt;/p&gt;&lt;p&gt;Finally, I also looked at the average timestamp of the files included in each release.&lt;/p&gt;&lt;code&gt;# For each Research Edition
for v in $(seq 1 7) ; do
ref=Research-V$v-Snapshot-Development
   printf '%s\t' $ref
   # List all files
   git ls-tree -r --name-only $ref |
   # Exclude administrative files introduced in the repo.
     grep -Ev 'README|LICENSE|\.pdf|\.ref' |
     # Output each file's commit time.
     xargs -I@ git log -1 --format=%at $ref -- @ |
     # Obtain average value and format it as a date.
     date -I -d @$(awk '{s += $1} END {printf("%.0f", s / NR)}')
     done&lt;/code&gt;&lt;p&gt;Here are the results.&lt;/p&gt;&lt;code&gt;V1        1972-06-20
V2        1972-05-31
V3        1973-03-10
V4        1974-03-06
V5        1974-11-28
V6        1975-06-15
V7        1979-01-25&lt;/code&gt;&lt;p&gt;The results indicate that the Fourth Edition precedes the Fifth Edition by about about eight months â a significant period for the pace by which the system was evolving at the time. (The results also show that I need to examine the apparent timing mismatch between the First and Second Editions.)&lt;/p&gt;Comments Post Toot! Tweet&lt;p&gt;Last modified: Tuesday, December 23, 2025 8:18 pm&lt;/p&gt;&lt;p&gt;Unless otherwise expressly stated, all original material on this page created by Diomidis Spinellis is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46367744</guid><pubDate>Tue, 23 Dec 2025 18:22:23 +0000</pubDate></item><item><title>LAVD: Meta's New Default Scheduler [pdf]</title><link>https://lpc.events/event/19/contributions/2099/attachments/1875/4020/lpc-2025-lavd-meta.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46368235</guid><pubDate>Tue, 23 Dec 2025 19:04:13 +0000</pubDate></item><item><title>Help My c64 caught on fire</title><link>https://c0de517e.com/026_c64fire.htm</link><description>&lt;doc fingerprint="f2f8d1d66eb7a2e8"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt; I flew back to Italy for the Christmas holidays, as I usually do. Here I have my childhood c64, on which I learend how to program, and which in the last few years I took to refurbishing.&lt;/p&gt;&lt;p&gt; In general, everytime I'm back to my parent's place I spend some time fixing and sorting out things, and this is one of them. &lt;/p&gt;&lt;p&gt; Now it works like a charm, and of course I also added some &lt;/p&gt;bells and whistles&lt;p&gt;, mostly stuff that allows me to easily tranfer programs from a PC (a kung-fu cart and a pi1541) - so this year I thought it was time to actually do something with it! &lt;/p&gt;&lt;p&gt; I decided to turn it into a cozy fireplace: &lt;/p&gt;(click on the image for the full-color version)&lt;p&gt; I was quite surprised that it worked, that I managed to complete this project in a few hours over two days, and that it was, most of all, fun! &lt;/p&gt;&lt;p&gt; I expected... more friction, having to work with arcane cross-platform toolchains and the like, but instead I completed almost of all it in a web-based IDE/emulator combo! &lt;/p&gt;&lt;p&gt; Moreover, I am far from being an expert when it comes to c64 coding. Yes, I used to program with one... when I was six or seven! And yes, I do follow its demoscene, and over the years I've read quite a bit about its chips and inner workings, but I've never written any demo effect for it. &lt;/p&gt;&lt;p&gt; In other words... if I managed, you can too! That's what made me want to write this post... &lt;/p&gt;All you need for Christmas...&lt;p&gt; ...is a 6502. &lt;/p&gt;&lt;p&gt; Here, I'll give you a crash course on the c64 - the key points of what I knew before starting this. &lt;/p&gt;&lt;p&gt; If you know how modern CPUs work, and how to optimize for them - try to forget all of that. The 64 comes from an era where RAM was faster than compute. Lookup tables are your friend, as it is fully unrolling loops / code generation. We have no caches! &lt;/p&gt;&lt;p&gt; Don't be surprised then to learn that the famous 6502 CPU has only one arithmetic register, the "accumulator". There are also two "index" registers used to offset memory locations, a status register, and a program counter - but you can't do math on any of these, at best, test or increment/decrement.&lt;/p&gt;&lt;p&gt; Moreover, the program counter is the only 16-bit register, all the others are 8-bit. &lt;/p&gt;&lt;p&gt; But who needs registers when you have fast RAM? Memory is your register file: pretty much all 6502 instructions operate between the accumulator and memory locations (or numeric constants, that are still memory, just part of the instruction itself). &lt;/p&gt;&lt;p&gt; To further facilitate this, the 6502 comes with a rich set of addressing modes, most instructions can fetch memory in few different ways: at absolute addresses, at addresses offset with one of the two index registers or even at indirect addresses (addresses contained in memory locations).&lt;/p&gt;&lt;p&gt; There is also a special memory area, called the "zero page", the first 256 bytes of memory (a page is, unsurprisingly, 256 bytes), which has an extra optimization: addressing there takes one cycle less, because the address can be encoded in the instruction in a single byte instead of two. &lt;/p&gt;&lt;p&gt; Have a look at the &lt;/p&gt;6502 instruction set&lt;p&gt;, it's very simple! Won't take more than 15 minutes to skim over them all. &lt;/p&gt;The plan. &lt;p&gt; Where things get less simple is to deal with all the c64 custom chips, the SID (sound) and the VIC-II (graphics). That's how demo-scene effects are done! Manipulating these chips in very precise ways to cause them to generate crazy stuff, most of which was never considered possible by the c64 designers back then! &lt;/p&gt;&lt;p&gt; The average c64 demo effect is all about this - generating lookup tables and unrolled assembly to then be able to exactly time internal chip status changes as the video signal is being generated line by line ("racing the beam"). Usually, what is shown on screen is not at all what it seems - i.e. it's not how you would create a similar effect on a PC. &lt;/p&gt;We won't be porting Second Reality...&lt;p&gt; I know almost nothing of any of this - so my plan was to avoid it all! I wanted to set the VIC-II in some graphic mode that gave me a decently simple, linear framebuffer, and from there on write the code like I would have done on any other computer, hoping that a fire effect is simple enough to compute that the 64 would just be able to cope with it. &lt;/p&gt;&lt;p&gt; Luckily, there is one such mode. The default, vanilla, character mode! Here, we have 40x25 characters on screen, chosen from a set of 256. So, one byte per "pixel", and 1000 pixels in total - great! &lt;/p&gt;The default "petscii" character set.&lt;p&gt; Now, the default character set does not really lean itself to creating a demo effect, but I knew I could create a custom one. My idea was to simply making a dither pattern, and as much as possible work like I had a 8-bit "grayscale" screen. &lt;/p&gt;&lt;p&gt; C64 characters are 8x8, so I could create a dither pattern that has a number of "on" pixels in the character corresponding to its position in the charset: 0 being fully off (background color), 64 being fully on (foreground color), and everything in between being a mix.&lt;/p&gt;&lt;p&gt; Of course though this would give us only 64 values, ideally, I wanted to utilize the whole 8-bit space... On obvious idea is that we could add colors to the mix. For example, having the first 64 values go from black (background) to brown (foreground), then the next 64 have brown as background and red as foreground, then red and yellow, and finally yellow and white. &lt;/p&gt;My custom charset. Made by hand in C64Studio.&lt;p&gt; This can't be achieved in the default character mode, unfortunately, as only the foreground color is controllable per-character (via another memory location, still using one byte per character - easy), while the background is shared. Luckily though, there is an &lt;/p&gt;"extended color mode"&lt;p&gt; that fits the bill exactly. In this mode the character set is limited to 64, but the two high bits of each character can be used to control the background color, while the foreground remains in the separate memory location as usual. &lt;/p&gt;Implementation. &lt;p&gt; All development was done in the &lt;/p&gt;retrogamecoders c64 IDE&lt;p&gt;, which handly couples the &lt;/p&gt;cc65 compiler&lt;p&gt; with an emulator. &lt;/p&gt;&lt;p&gt; Writing c64 code in C is generally a terrible idea, and cc65 is not even the "best" compiler out there (is quite old and barely does optimize the generated code - &lt;/p&gt;llvm-mos&lt;p&gt; might be better), but being able to test in C and then gradually "port" to assembly was crucial for a noob like me. If I were to keep working on this, I'd probably move to &lt;/p&gt;Kick Assembler&lt;p&gt;, which is particularly suited for the kind of code-generation that you want to do in demo-coding. &lt;/p&gt;&lt;p&gt; Anyhow, here are the .c files, step by step, in chronological order. You can just copy and paste them in the retrogamecoders IDE and see how things work, start tinkering if you like! Enjoy! &lt;/p&gt; First test of the ECM charset idea. Slowest fire-effect ever in pure C. Still in C. Starting to "unroll". Starting to port to assembly. Ported to assembly, with side-by-side C. Assembly only, some more ECM tricks. "Final" version. &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46368300</guid><pubDate>Tue, 23 Dec 2025 19:09:45 +0000</pubDate></item><item><title>Un-Redactor</title><link>https://github.com/kvthweatt/unredactor</link><description>&lt;doc fingerprint="b6e47833deff88e0"&gt;
  &lt;main&gt;
    &lt;p&gt;A PDF editing tool that lets you put your own information over a redaction box.&lt;/p&gt;
    &lt;p&gt;This tool is for forensics purposes. It does not "recover" data truly destroyed by redaction tools.&lt;/p&gt;
    &lt;p&gt;What it does do is allow you to write over a redaction box. Like white-out.&lt;/p&gt;
    &lt;p&gt;You can select a redaction box, and select all of the exact dimensions and replace at once.&lt;/p&gt;
    &lt;p&gt;I am not responsible for your use of this tool.&lt;/p&gt;
    &lt;p&gt;Republishing altered documents is illegal, and you should not use this to do so.&lt;/p&gt;
    &lt;p&gt;By using this tool you claim all legal liability for any documents you create with it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46368471</guid><pubDate>Tue, 23 Dec 2025 19:26:36 +0000</pubDate></item><item><title>Terrence Malick's Disciples</title><link>https://yalereview.org/article/bilge-ebiri-terrence-malick</link><description>&lt;doc fingerprint="37c09103add60b3d"&gt;
  &lt;main&gt;
    &lt;p&gt;In the winter of 2024, the photographer and filmmaker RaMell Ross released Nickel Boys, a masterful adaptation of a novel by Colson Whitehead. In a fragmentary, impressionistic style, the film portrays the friendship of two African American teens at a brutal Florida reform academy during the Jim Crow era. Acclaimed as a visionary movie, it ended up on many critics’ best-of-the-year lists and earned an Oscar nomination for Best Picture.&lt;/p&gt;
    &lt;p&gt;Ross is a fiercely independent artist. His first film, the lyrical 2018 documentary Hale County This Morning, This Evening, was also nominated for an Oscar. Afterward, he refused Hollywood’s overtures for years. So why did he take a meeting with the producers who reached out to him about making a studio-financed, big-budget adaptation of Nickel Boys? Ross’s explanation was simple: because one of them had produced Terrence Malick’s 2011 film, The Tree of Life.&lt;/p&gt;
    &lt;p&gt;Ross’s reverence for Malick is plain in his films, which, like Malick’s, rely on extended montages of the everyday and do away with the conventional rules of cinematic storytelling, hovering instead between distant, melancholy reverie and hyperfocused, lived-in specificity. And he is not the only recent filmmaker who has fallen under Malick’s spell. Indeed, Malick’s sensibility, visual style, and working methods have had a profound influence on some of today’s best and most interesting directors.&lt;/p&gt;
    &lt;p&gt;Take Chloé Zhao, the director of the Oscar-winning Nomadland (2020). Her early films, all set in the American heartland, were regularly compared to Malick’s, and she herself pointed to The Tree of Life and Malick’s 2005 film, The New World, as influences on her 2021 Marvel superhero movie, Eternals. Those overtones persist in her latest, Hamnet, a film about the death of William Shakespeare’s only son and his subsequent creation of Hamlet. The movie may take place in Elizabethan England, but it is replete with lyrical passages and visions of nature that recall Malick’s work.&lt;/p&gt;
    &lt;p&gt;The same is true of the director Clint Bentley’s newest film, Train Dreams, an adaptation of Denis Johnson’s 2011 novella about the unremarkable life of a logger and railroad worker in the early years of the twentieth century. Weaving episodes from its character’s life into an elegiac collage that incorporates domestic bliss, harrowing tragedy, and melancholic resignation, Train Dreams—which premiered at the Sundance Film Festival in January and was quickly acquired by Netflix—unfolds across 102 minutes, yet seems to contain a whole world. Its protagonist, played by a reserved Joel Edgerton, is a simple man who occasionally questions his place in the universe but never understands it, save for a brief moment near the end when he takes a ride in an airplane—something he’s never done before—and, in one shining (and recognizably Malickian) instant, sees the shape of his life and feels something like transcendence.&lt;/p&gt;
    &lt;p&gt;Malick’s influence is intriguing in part because he is not an obvious choice for filmmakers to emulate. He has had, to be sure, a fascinating career: a publicity-shy Harvard philosophy grad, Rhodes Scholar, former MIT lecturer, and New Yorker writer, he made two brilliant and highly acclaimed films in the 1970s—the lovers-on-the-run drama Badlands and the visually striking romantic tragedy Days of Heaven—before stepping away from filmmaking for twenty years. In 1998, he returned with The Thin Red Line, a dreamy, diffuse adaptation of James Jones’s World War II novel, and followed that with two more ruminative epics: The New World, about the settlement of Jamestown and the romance between John Smith and Pocahontas, and The Tree of Life, a massive autobiographical film that frames a mid-century Texas coming-of-age tale against the spectacular origins of the universe and of life on Earth. His films since then have been less ambitious in scope but, in some ways, more stylistically bold.&lt;/p&gt;
    &lt;p&gt;Many of Malick’s films have been critically acclaimed, and two have received Oscar nominations for Best Picture (albeit without much chance of winning). But none could be called box-office hits, and some have been savaged by critics. Indeed, thanks to his fondness for oblique storytelling, poetic voice-over, and overt spiritual themes, Malick’s oeuvre has become one of the more contentious in cinema. Each new release inspires debate over whether the film at hand is a deep, philosophical masterpiece or boring, pretentious drivel. Young directors looking for heroes tend not to gravitate toward divisive religious artists whose movies don’t make money or win awards. So what accounts for Malick’s impact on twenty-first-century American film?&lt;/p&gt;
    &lt;p&gt;particularly since his return to filmmaking, Malick has sought to reconnect American cinema to a lost spirituality, earnestly tackling questions about faith and the design of the world at a time when most mainstream cinema has avoided such topics. Malick is a devout Episcopalian. But the spirituality in his films is rarely illustrative or prescriptive. He doesn’t use religion as a cudgel or a doctrinaire superstructure with which to explain the world. Rather, he sees it as an inner light in people. In The Thin Red Line, for instance, soldiers, in voice-over, speak solemnly of inner longing. These otherwise inarticulate men’s voices read heartfelt love letters, or dabble in poetry, or edge their way into philosophical inquiries about the cruelty and redemptiveness of nature. A soldier remembers his mother reaching for an angel at the instant of her death; another recalls the serenity he experienced with his wife before he had to leave her behind. The effect is like eavesdropping on a kind of Emersonian oversoul. Malick endows even his most minor characters with humanity, which he views as a kind of holiness. Amid the gaunt and haunted faces of these soldiers, Malick finds grace.&lt;/p&gt;
    &lt;p&gt;This kind of earnestness stood out in an age of relentless irony and snark. It served as a corrective to the glossy productions of Hollywood in its imperial phase, that period of the late 1990s and early 2000s, when budgets ballooned and American cinema, armed with state-of-the-art CGI and desperate to service a growing international market, became increasingly driven by fantasy spectacle and special effects. Malick’s films were a rebuke to even the hip grittiness of independent films of the era. He had an eye for light and an ear for music, he immersed viewers in color and texture, and he used his classical scores to underscore the glory of what he saw. Handcrafted, personal, achingly sincere, and at times proudly “flawed,” his pictures stood out against both the mainstream and the underground.&lt;/p&gt;
    &lt;p&gt;By the end, we are overwhelmed with emotion for this unremarkable life lived in near anonymity.&lt;/p&gt;
    &lt;p&gt;This proved irresistible for a certain kind of filmmaker frustrated with the options available to them. In 2000, for instance, the director David Gordon Green released George Washington, a drifting, multicharacter drama featuring young African American kids in a dead-end North Carolina steel town. Despite his impoverished setting, Green avoids miserabilist clichés and gives his characters a romantic grandeur. He takes their hopes and desires at face value. The title comes from the fact that one of the kids, named George, dreams of being president of the United States, a fact that Green does not treat with bitter irony or fashionable cynicism.&lt;/p&gt;
    &lt;p&gt;Malick’s effect on George Washington is undeniable—rare was the review that didn’t mention the connection—and it is also clear in Green’s second feature, All the Real Girls (2003), an atmospheric and largely uneventful romance defined by the passions of the two shy lovers at its center. Noel (Zooey Deschanel) and Paul (Paul Schneider), like Malick’s characters in Badlands and Days of Heaven, are not extroverted or articulate. But Green’s film thrums with a visual splendor that reflects the characters’ longing, turning another depressed Southern town into a vibrant emotional landscape.&lt;/p&gt;
    &lt;p&gt;Zhao’s films also highlight the great beauty of the otherwise unremarkable. Her masterpiece, 2017’s The Rider, follows a wounded rodeo cowboy (played by Brady Jandreau, a real-life rodeo star who sustained a career-ending head injury) from a Lakota Sioux reservation in South Dakota as he struggles with his inability to ride again. The film is made up of small moments, highlighting brief interactions and quotidian actions, but Zhao’s shooting and cutting, much like Malick’s, elevate these scenes toward the transcendent, finding a sacredness in the existence of a character who has lost his sense of purpose.&lt;/p&gt;
    &lt;p&gt;The same could be said of Bentley’s Train Dreams, which follows a man with very little direction in the world: he’s an orphan, raised in poverty, who finds work as a logger and spends his years felling trees and building railroads. Though he sees racism and murder around him, he can do nothing about it. He finds happiness by starting a family but then loses that family to a raging wildfire. The film’s rhythms are not those of a typical drama; for all the squalls of guilt and grief, the movie moves with a steady cadence that suggests that the mysteries, tragedies, and glories of life are all part of the same thing. This seems like it would result in a cold, opaque film, yet by the end, we are overwhelmed with emotion for this unremarkable life lived in near anonymity, a life that is more like our own than we might want to admit.&lt;/p&gt;
    &lt;p&gt;You can also see Malick’s philosophical influence in three films directed by Laura Dunn (all of which he produced): The Unforeseen (2007), about the dire social and environmental consequences of a mining company’s development of a vast patch of Austin real estate,Look &amp;amp; See: A Portrait of Wendell Berry (2016), about the life of the titular Kentucky farmer, writer, and activist, and All Illusions Must Be Broken (2024), about the American cultural anthropologist Ernest Becker’s ideas around the human denial of mortality and self-knowledge. In each, Dunn portrays a society that is fraying at the seams owing to its increasing disconnection from the natural world and the organic patterns of life. Her films avoid the density of political and philosophical jargon. Instead, they create meaning through images of ordinary people: children playing, adults working in the fields, reconnecting viewers with a different state of being. The films’ form embodies her overall thesis that, despite our endless efforts to deny it, we humans are not separate from nature but inextricably part of it.&lt;/p&gt;
    &lt;p&gt;malick’s humanism is refracted through his visual style—the aspect of his films that’s most obviously influential. He loves to shoot with natural light whenever possible: “Vermeer yourself ” is a common direction he gives to actors, indicating that they should lean into the available light during a take. His fondness for shooting at the “magic hour,” that time when the sun is setting and the sky emits a distinctive dark glow, is legendary. He also talks about “quail hunting”: capturing unscripted moments when the light happens to be perfect and you find something unexpected and real. Then there are “rabbit holes”: quick scenes and exchanges shot when the light isn’t perfect. Natural metaphors, found moments, a dogged pursuit of real light—the way Malick approaches the act of shooting enacts his philosophical view of the world.&lt;/p&gt;
    &lt;p&gt;The lilting, fairy-tale surfaces of that film speak to a search for beauty that the characters cannot find.&lt;/p&gt;
    &lt;p&gt;Malick’s influence on the way movies look has become a cliché. (A short 2015 video titled “Not Directed by Terrence Malick,” compiled by Jacob T. Swinney, features a collection of clips of films apparently influenced by Malick; it includes movies like Up in the Air, Beasts of No Nation, and Ex Machina.) But anybody with some skill can shoot with natural light or cut away to a field of wheat. What distinguishes Malick’s work—what makes it truly revelatory to viewers—emerges from the harmony between a film’s images and its sensibility. In George Washington, Green frames his characters in gorgeous light and scores their interactions with symphonic drones that suggest something heroic. And in Nickel Boys, Ross tells a tale filled with injustice, racism, torture, and murder—a story that should be the very height of despair—yet finds an almost overwhelming humanity with his probing camera. Like Malick in The Thin Red Line, Ross sees evidence of grace in the basest of places.&lt;/p&gt;
    &lt;p&gt;By contrast, it’s jarring—if fascinating—when a film’s visual approach borrows from Malick but doesn’t match the sensibility at work. That’s the case with The Assassination of Jesse James by the Coward Robert Ford (2007), a remarkably beautiful Western directed by Andrew Dominik, who worked as an uncredited cameraman on The New World. The film has a twilight grandeur and a fascination with the natural world that suggests Dominik learned quite a bit working for Malick. But despite the unmistakable surface similarities, Dominik’s dark moral vision bears little resemblance to Malick’s. The outlaws of the James gang live in a universe of endless, savage scrutiny, fearful of both the law and their own viral, panopticist distrust, with each member set against the others. The lilting, fairy-tale surfaces of that film speak to a search for beauty that the characters cannot find; Dominik longs for Malick’s vision of grace but sees no evidence of it. Or maybe he just doesn’t really want to find it.&lt;/p&gt;
    &lt;p&gt;malick’s working style is also appealing to many filmmakers. He shoots incessantly, improvises constantly, pays more attention to capturing footage of flora and fauna than he does to scripted scenes with actors, and then spends months in postproduction with teams of editors assembling his movies in unorthodox ways. This approach is inviting not just because it is unusually creative and collaborative but because it is rooted in the nature of cinema itself.&lt;/p&gt;
    &lt;p&gt;Malick does not rely on the nineteenth-century theatrical conventions that most moviemakers remain bound to, with their focus on acts and protagonists and inciting incidents and A and B storylines. His films also avoid the novelistic, flowing instead like a series of thoughts, or memories, or maybe rivers. His is an intuitive and almost abstract filmmaking process that deprioritizes the presentational and the narrative. Malick focuses on collecting images, ideas, offhand moments, and sounds that can then be used during editing, applied almost like brushstrokes in a painting.&lt;/p&gt;
    &lt;p&gt;He also welcomes spontaneous suggestions on set and encourages experimentation. There are three editors credited on The Thin Red Line, four on The New World, and five on The Tree of Life; for the latter, the director reportedly invited students from the University of Southern California and the University of Texas at Austin to come in and try their hand at cutting footage. “Kids don’t censor themselves—their brains are in a different place,” the editor Billy Weber, one of Malick’s longtime collaborators, told me at the time. “So we had students give it a shot. And we’d have interns come in at night and cut scenes.” This is, to be clear, nothing like the way most other movies are put together. All too often, a film production is like a train that can’t be stopped or set on a different course once it leaves the station. But Malick has found ways to guide the train gently off the tracks—and in new, unexpected, undiscovered directions. It’s easy to see why other directors might be drawn to his less regimented approach.&lt;/p&gt;
    &lt;p&gt;This working process re-creates the filmmaking method that Malick chanced upon with Days of Heaven, which, for all the acclaim it garnered, was something of a salvage job. Coming off the critical success of Badlands, Malick had gone into Days of Heaven with a dense, detailed, ambitious script. But as described in John Bleasdale’s excellent 2024 biography, The Magic Hours: The Films and Hidden Life of Terrence Malick, the director found himself unhappy with the results he was getting over the course of production. He didn’t like how his dialogue sounded. His scenes felt phony. The shoot ran wildly over budget and behind schedule, as the Canada-based production team spent days trying to get the light perfect, and Malick’s original cinematographer, Néstor Almendros (who would go on to win an Oscar for the film), left halfway through and was replaced by Haskell Wexler. Meanwhile, Malick sent a photographer friend to capture nature footage that he could intersperse throughout the movie.&lt;/p&gt;
    &lt;p&gt;Along the way, the director found himself fascinated with the off-the-cuff observations made by another one of his leads, fifteen-year-old Linda Manz, and recorded her describing scenes from the movie in her own words; he eventually shaped that into one of the most indelible voice-over narrations in cinema history, an offbeat series of childlike reflections that provide a poetic counterpoint to the elemental storyline. Everything about Malick’s evolving approach speaks to a heightened sense of possibility, and to a desire to reinvigorate the frustrating rhythm of film production with openness, spontaneity, and discovery.&lt;/p&gt;
    &lt;p&gt;What’s remarkable about this approach is that despite his seemingly scattershot and impulsive methods, Malick’s films possess an aesthetic unity. Ross suggests something similar when talking about his own work. In an interview around the release of Nickel Boys, he described to me the collage-like quality of his film: “It’s jumping time, and jumping textures, and jumping images, and points of view, and focal lengths, and sounds, but also it’s coherent.”&lt;/p&gt;
    &lt;p&gt;influence can be a straitjacket. In 2014, A. J. Edwards, who had worked as an editor on two of Malick’s films, released The Better Angels, a black-and-white meditation on Abraham Lincoln’s years as a young man living in rural Indiana. It’s a bold movie in many ways, almost confrontationally nonnarrative and context-free; aside from a brief coda set immediately after his assassination, we see almost nothing of Lincoln as a grown man or president. And yet there’s a curious emptiness at its heart. Filled with handheld reveries bathed in heavenly light, it replicates the yearning style of Malick’s work without the instances of genuine humanity that undergird his cinematic tapestries; though the characters in The Better Angels are based on real historical personages, they never come across as real people.&lt;/p&gt;
    &lt;p&gt;A similar emptiness afflicts David Lowery’s crime melodrama Ain’t Them Bodies Saints (2013), which has ravishing cinematography, ethereal music, and an elliptical narrative, all of which clearly owe something to Malick’s work. It follows the return of a fugitive to the woman he once loved and the child he has never met, and the tortured romance that ensues. Lowery explained at the time that he was not interested in another story about a crime but instead wanted to explore its emotional aftermath. But for all its loveliness, the film’s glancing storytelling has the opposite effect of Malick’s openness to the world; it dulls the senses, makes the characters and their feelings seem smaller and less significant. (Lowery’s more recent films owe little to Malick and are the better for it.)&lt;/p&gt;
    &lt;p&gt;Think of it this way: What use is Malick’s liberated style of working if a filmmaker merely replicates it? The most successful Malickian films borrow from his work but find ways to transcend it and to convey new ideas. Take RaMell Ross. In his first film, he used fleeting, beautiful glimpses into mundane moments to convey, in just seventy-six short minutes, the arc of his subjects’ lives. In Nickel Boys, he expanded the fragmented lyricism of his earlier film by crossing it with a first-person camera: the story is told almost entirely through shots that appropriate the perspectives of the two characters. The result is a work that is immersive and experiential, otherworldly and mythic. It’s also entirely his.&lt;/p&gt;
    &lt;p&gt;malick has always been frustrated with the typical methods of making movies. In fact, he seems to become restless even with his own methods. If he’s helped liberate other filmmakers, he has also continuously sought to liberate himself. That may be why what has remained constant throughout his work has been change. Even if certain aspects of his films—his love of natural light, his attention to found moments, his use of voice-over—have recurred, his subject matter, and his style, have never been fixed. The films that made his reputation in the 1970s—Badlands and Days of Heaven—are very different from the epics he made after his return to filmmaking, which are even less conventional in terms of narrative. The earlier pictures, compact and diamond-sharp, dance around their ideas, and their young protagonists don’t always grasp the gravity of their stories. Holly, the narrator of Badlands, is just as likely to talk about a movie star or a photograph as she is to talk about the fact that her boyfriend is a serial killer; Linda, the narrator of Days of Heaven, generally talks about everything and anything aside from the fact that her brother and his lover are cruelly betraying a dying man. This is very different from the prayerlike directness we find in the voice-overs for The Thin Red Line, The New World, and The Tree of Life. Many-voiced and at times even rambling, those later period films are pointedly diffuse: each scene, each thought feels like it could expand into a whole other movie; all the characters seem so resolutely alive.&lt;/p&gt;
    &lt;p&gt;Malick then pivoted again, following his trio of epics with a trilogy of low-budget works—To the Wonder (2012), Knight of Cups (2015), and Song to Song (2017)—which seemed at times to be not-so-veiled dramatizations of events from the director’s own life. These works were Malick’s first films to be set in something like the present. They are messier, more frenzied. The characters in these later pictures are rootless, always searching. And the filmmaking in them is centered more on movement than meditations on nature.&lt;/p&gt;
    &lt;p&gt;In his vision, our endless seeking makes us human and therefore holy.&lt;/p&gt;
    &lt;p&gt;To the Wonder, for instance, tells the story of the breakdown of the marriage between an American man and a Ukrainian woman after they return from Paris (where they met) to his home in Oklahoma. The film eschews dialogue, relying instead on characters’ movements to express their emotions and changing relationships. A mother and daughter, newly arrived in the United States, twirl and leap through the aisles of an enormous supermarket, the likes of which they’ve never seen before; the stolid shoulders of a frustrated husband dominate the foreground of the frame, while his effervescent wife moves daintily before him; a flirtation is expressed with a quick curtsy, shame with a penitent bow. It can almost be seen as a dance film.&lt;/p&gt;
    &lt;p&gt;Instructing the film’s team of editors, Malick gave them copies of Margaret Anne Doody’s introduction to a Penguin Classics edition of Samuel Richardson’s 1740 epistolary novel, Pamela, and pointed them to a line about how the author loved “the formless, the radiant zigzag becoming.” The phrase “radiant zigzag becoming” became their own unofficial title for the film, the editors told me; it spoke to the project’s energetic sense of movement. It also reflected the fact that Malick’s characters were always in the process of self-actualizing without ever fully doing so.&lt;/p&gt;
    &lt;p&gt;Something similar could be said for Malick’s films themselves. To the Wonder, in fact, led directly to one of the most intriguing of Malick-influenced movies, a hybrid on multiple levels. In 2018, the veteran photographer and documentarian Eugene Richards premiered a mesmerizing forty-three-minute film called Thy Kingdom Come, which consists of footage Richards shot for To the Wonder, featuring Javier Bardem as a priest who has lost his faith ministering to the impoverished residents of an Oklahoma town.&lt;/p&gt;
    &lt;p&gt;Malick had Bardem go into real people’s lives—into trailer-park homes, a county jail, a homeless shelter—and had Richards document those people speaking to the actor’s clearly fictional priest. Only a small portion of the footage would be used in the finished feature, so Richards and Bardem developed a plan to make a separate film out of the material. In Thy Kingdom Come, Bardem says little; most of the picture consists of these people—drug addicts, inmates, a homeless couple, a former Ku Klux Klan member, a woman grieving a dead baby, and more—describing their experiences and their thoughts. There is no narrative, nor even much of an emotional through line. Aside from a couple of brief exchanges, there is barely any mention of God. And yet spirituality is ever present. These people know the priest isn’t real, but they open up to him as if he were; they do not, in any way, seem to be acting.&lt;/p&gt;
    &lt;p&gt;“Is this a true story?” Bardem asks in the opening narration. “Yes, I would say so. Is the priest a real priest? No. But it’s as if they were waiting for him.” The onrush of faces and lives that then ensues suggests the anticipation goes both ways: it’s as if the film were waiting for them. These people might not have found grace, but the camera eye—Richards’s but also Malick’s—finds grace in them. Thus, this riff on Malick reveals something essential about Malick’s work. In his vision, our endless seeking makes us human and therefore holy. The search for God is not a search for meaning; the meaning lies in the search itself. Through the films he’s made and the ways he’s made them, Malick has turned cinema into the vessel for that search.&lt;/p&gt;
    &lt;p&gt;Bilge Ebiri is a film critic for Vulture and New York magazine. His work has also appeared in The New York Times and the Criterion Collection.&lt;/p&gt;
    &lt;p&gt;A Literary Gift in Print&lt;/p&gt;
    &lt;p&gt;Give a year of The Yale Review—four beautifully printed issues featuring new literature and ideas.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46368557</guid><pubDate>Tue, 23 Dec 2025 19:35:20 +0000</pubDate></item><item><title>Fixed-Wing Runway Design</title><link>https://www.wbdg.org/building/aviation/fixed-wing-runway-design</link><description>&lt;doc fingerprint="769c6e994ef32e0f"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Overview&lt;/head&gt;
    &lt;head rend="h4"&gt;Within This Page&lt;/head&gt;
    &lt;p&gt;Runways are the primary operating surface at airfields and essential to fixed-wing aircraft operations. Fixed-wing runways are built in a variety of lengths, widths, and pavement types depending on a large number of factors, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Aircraft Type (Operating Characteristics, Wingspan, Weight)&lt;/item&gt;
      &lt;item&gt;Mission&lt;/item&gt;
      &lt;item&gt;Number of Operations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The primary reference describing requirements for DoD Fixed Wing Runways is Chapter 3 of UFC 3-260-01, Airfield and Heliport Planning and Design. Other special use runways (Landing Zones, STOVL Facilities and UAS runways) are defined in Chapters 7, 8 and 9 of the UFC.&lt;/p&gt;
    &lt;p&gt;Each DoD Service classifies fixed-wing runways into two primary categories—Class A or Class B—depending primarily on the type(s) of aircraft using the runway. That classification drives many required construction features of the runway, including length, width, transverse slopes, and longitudinal grades. It should be noted that civilian runways are classified by a very different system, defined in Federal Aviation Administration Advisory Circular 150/5300-13, Airport Design, with a classification system based on the critical aircraft's wingspan and landing approach speed.&lt;/p&gt;
    &lt;p&gt;Fixed-wing runways are usually constructed with a rigid pavement surface (Portland cement concrete) or flexible pavement surface (asphalt cement concrete), but in special cases may be surfaced with compacted soil, aggregates, or segmented aluminum mats, depending on the mission requirements.&lt;/p&gt;
    &lt;p&gt;Taxiways are used by aircraft to enter and exit a runway and transit to an aircraft parking position. Taxiways connect directly to runways, most often at the runway ends.&lt;/p&gt;
    &lt;p&gt;In addition to the runway pavement surface, there are many ground surface areas immediately surrounding the runway that improve safety for the operating aircraft by limiting the risk of damage should an aircraft accidentally depart from the runway surface.&lt;/p&gt;
    &lt;p&gt;Not only must objects be restricted from close proximity to the runway surface, the airspace surrounding a runway must also be protected from development that encroaches on the airspace needed for safe aircraft operations. The protected areas are defined by what are known as "imaginary surfaces." These are generally planar or conical surfaces in the air, defined by a length, width, and slope up to a specified elevation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Runway Design&lt;/head&gt;
    &lt;p&gt;There are many different factors that impact runway design and are dependent on many different data inputs. All components should be determined early in the planning process to avoid unexpected challenges or constraints later in the design development.&lt;/p&gt;
    &lt;head rend="h3"&gt;Runway Heading&lt;/head&gt;
    &lt;p&gt;Runways are oriented to provide the best conditions for an aircraft on takeoff and landing. An aircraft moving directly into the wind has the highest airspeed across the wing, thereby increasing lift, and the least sideways forces on the aircraft. Therefore, the ideal orientation of the runway (often referred to as the heading) is determined by analyzing historical wind data (10 years or more) at a location. Wind heading and speed data is graphically displayed on a Wind Rose, and this tool can be used to determine a runway heading that provides the greatest percentage of time with favorable winds for aircraft operations. The objective is to find a heading that allows operations more than 95% of the time with a crosswind less than 19.5 km/hr (10.5 knot). When a single runway cannot provide this coverage, then a crosswind runway may be required. UFC 3-260-01, Appendix B, Section 4 explains this process in detail.&lt;/p&gt;
    &lt;p&gt;In addition to prevailing winds, other factors may affect the selection of a runway heading, including terrain, obstructions, restricted airspace, noise effects, built-up areas, or operational procedures.&lt;/p&gt;
    &lt;head rend="h3"&gt;Runway Length&lt;/head&gt;
    &lt;p&gt;Each service determines the required runway length using their own procedures that generally take into account the mission aircraft performance requirements, altitude, and typical temperature range.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Air Force: For both Class A and B runways, the length will be determined by the Major Command responsible for the airfield.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Army: The Class A runway length requirement is listed in UFC 3-260-01, Table 3-3, but for Class B Runways, runway length is determined by the using aircraft operator, in conjunction with HQ Department of the Army.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Navy and Marine Corps: UFC 2-000-05N describes the process for determining runway length to accommodate the critical aircraft in both takeoff and landing operations under stated load and environmental conditions. Minimum lengths by aircraft type are listed in Table 11110-1; then adjusted for altitude, temperature, and effective gradient with a safety factor applied to the result.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Runway Width&lt;/head&gt;
    &lt;p&gt;For the DoD, the runway width is dependent on the type of aircraft planned to use the airfield. Table 3-1 in UFC 3-260-01 classifies aircraft into Class A or B. Then Table 3-2 defines the required runway width for each class. There are some exceptions to the standard widths, as defined in Item 2. Runways for Bombers like the B-52 are 300-ft wide, and some training runways for small aircraft are only 75-ft wide. Special-use runways (Landing Zones, Short Takeoff and Vertical Landing (STOVL) facilities, and Unmanned Aircraft Systems (UAS) runways have their own requirements, defined in UFC 3-260-01, Chapters 7, 8 and 9.)&lt;/p&gt;
    &lt;head rend="h3"&gt;Clear Zones and Accident Potential Zones&lt;/head&gt;
    &lt;p&gt;Clear Zones are areas on the ground at the ends of runways that have a high potential for accidents. Other uses of the clear zone are restricted to be compatible with aircraft operations. Each DoD service defines clear zones differently, so UFC 3-260-01, Table 3-5 should be carefully considered to provide the appropriate dimensions. Clear Zones should be owned and controlled by the agency to prevent incompatible development within the areas.&lt;/p&gt;
    &lt;p&gt;Accident Potential Zones (APZs) are land-use control areas, mandated by the Air Installation Compatibility Use Zones (AICUZ) program, and intended to promote only compatible development in areas under the approach and departure surfaces for fixed-wing runways. The APZs usually stretch beyond the base property boundaries, so coordination with the local communities is essential to avoid building high population development in these areas where an aircraft accident is more likely to occur.&lt;/p&gt;
    &lt;head rend="h3"&gt;Imaginary Surfaces and Obstructions&lt;/head&gt;
    &lt;p&gt;The area above the ground surrounding a runway that must be kept clear of objects that might damage an aircraft operating around the airfield (approach, departure or circling) is defined by Imaginary Surfaces (planar or conical surfaces in the airspace). An object that projects above an imaginary surface is an obstruction. Typical terminology for imaginary surfaces includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Primary surface&lt;/item&gt;
      &lt;item&gt;Approach-departure surface&lt;/item&gt;
      &lt;item&gt;Inner horizontal surface&lt;/item&gt;
      &lt;item&gt;Conical surface&lt;/item&gt;
      &lt;item&gt;Outer horizontal surface&lt;/item&gt;
      &lt;item&gt;Transitional surface&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each of these surfaces is defined in UFC 3-260-01, Chapter 3.&lt;/p&gt;
    &lt;head rend="h3"&gt;Grades&lt;/head&gt;
    &lt;p&gt;There are strict requirements for the slope or grade of the runway pavement surfaces and ground surfaces surrounding the runway. These surfaces are dependent on the performance requirements of the aircraft (Class A or B) and to promote good drainage as well as aircraft safety in the event that an aircraft accidentally departs from the runway surface. UFC 3-260-01, Chapter 3 defines the requirements for the grades of the following items:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Longitudinal Grades and Rate of Grade Change for Runways, Shoulders, Overruns and Lateral Clearance Zone&lt;/item&gt;
      &lt;item&gt;Transverse Slopes for Runway, Shoulders, Overruns and Lateral Clearance Zone&lt;/item&gt;
      &lt;item&gt;Longitudinal and Transverse Grades in the Clear Zone&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Pavement Thickness Design&lt;/head&gt;
    &lt;p&gt;Runway pavements for military airfields are designed following the procedures described in UFC 3-260-02, Pavement Design for Airfields. Typically, runways are constructed with a flexible pavement structure (asphaltic concrete pavement) or rigid pavement structure (Portland cement concrete).&lt;/p&gt;
    &lt;p&gt;For pavement thickness design, each Service divides an airfield and the runway into different types of traffic areas, (e.g., Type A, Type C, Primary, or Secondary). The traffic type then correlates to different traffic patterns (aircraft load, number of repetitions, and the typical "wander" of the aircraft traffic). When traffic is combined with the subgrade strength, the required pavement thickness can be determined.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pavement Markings&lt;/head&gt;
    &lt;p&gt;To improve the visibility of runways during both day and night, standard markings are painted on the pavement surface. There are three primary runway marking schemes—Visual, Non-Precision Instrument, and Precision Instrument—with progressively more markings. Runway markings are white (as compared to yellow markings on taxiways) and include embedded glass beads to provide reflectivity. At each end, the runway heading number is painted, and when there are one or more parallel runways, a left (L), right (R) or center (C) designation is also applied.&lt;/p&gt;
    &lt;p&gt;UFC 3-260-04, Airfield and Heliport Marking, fully defines pavement marking requirements for Army and Air Force fixed-wing runways. Navy and Marine Corps requirements are defined in NAVAIR 51-50AAA-2, General Requirements for Shorebased Airfield Marking and Lighting. Each Service's requirements very closely match the Federal Aviation Administration runway marking requirements.&lt;/p&gt;
    &lt;head rend="h3"&gt;Runway Lighting and Signs&lt;/head&gt;
    &lt;p&gt;For low visibility and night operations on runways, lights and signs are used to provide visibility of the runway to pilots when operating on the ground and in the air. The most basic fixed-wing runway lighting system consists of edge lights, threshold lights and end lights, used to outline the lateral and longitudinal limits of the usable surface of the runway. These lights are required for visual flight rules (VFR) night operations and for all categories of instrument operations. In some cases where minimal visibility operational capability is needed, the runway perimeter lighting is augmented with touchdown zone and centerline lighting in-pavement light fixtures.&lt;/p&gt;
    &lt;p&gt;Approach light systems provide visual guidance to pilots aligning their aircraft with the runway and attempting final corrections before landing at night or during low visibility. There are several different types of approach lighting systems (MALSR, SSALR, ALSF-1, ALSF-2), each with a different number of lights and different configurations.&lt;/p&gt;
    &lt;p&gt;UFC 3-535-01, Visual Air Navigation Facilities, fully defines lighting and signage requirements for Army and Air Force fixed-wing runways. Navy and Marine Corps requirements are defined in NAVAIR 51-50AAA-2, General Requirements for Shorebased Airfield Marking and Lighting. Each Service's requirements very closely match the Federal Aviation Administration runway lighting and signage requirements.&lt;/p&gt;
    &lt;head rend="h3"&gt;Electronic Navigational Aids (NAVAIDs)&lt;/head&gt;
    &lt;p&gt;Some aircraft are equipped with electronic devices that can use radio signals to provide direction, distance, and glide slope data to help the pilot guide the aircraft to the runway. These systems are called NAVAIDs and consist of a wide variety of antennas installed in various configurations surrounding the runway. For example, an Instrument Landing System (ILS) consists of a Localizer antenna and Glide Slope antenna. The Localizer transmits a radio signal down the centerline of the runway into the approach zone, and the pilot can use the signal to align on the runway. The Glide Slope transmits a radio signal upwards from the runway surface at the correct approach angle the aircraft should follow to touchdown at the appropriate location on the runway surface. Many different types of NAVAID systems have been developed over the years and are deployed at DoD installations. UFC 4-141-10, Airfield Operations Support Facilities describes the different types of NAVAIDs, including installation requirements for each system.&lt;/p&gt;
    &lt;head rend="h2"&gt;Relevant Codes and Standards&lt;/head&gt;
    &lt;head rend="h3"&gt;Federal Aviation Administration&lt;/head&gt;
    &lt;head rend="h3"&gt;Naval Air Systems Command&lt;/head&gt;
    &lt;head rend="h3"&gt;Unified Facility Criteria&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;UFC 2-000-05N Facility Planning for Navy and Marine Corps Shore Installations&lt;/item&gt;
      &lt;item&gt;UFC 3-260-01 Airfield and Heliport Planning and Design&lt;/item&gt;
      &lt;item&gt;UFC 3-260-02 Pavement Design for Airfields&lt;/item&gt;
      &lt;item&gt;UFC 3-260-04 Airfield and Heliport Marking&lt;/item&gt;
      &lt;item&gt;UFC 3-535-01 Visual Air Navigation Facilities&lt;/item&gt;
      &lt;item&gt;UFC 4-141-10 Airfield Operations Support Facilities&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46368608</guid><pubDate>Tue, 23 Dec 2025 19:41:14 +0000</pubDate></item><item><title>HTTP Caching, a Refresher</title><link>https://danburzo.ro/http-caching-refresher/</link><description>&lt;doc fingerprint="b55d69584f161f32"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;HTTP caching, a refresher&lt;/head&gt;
    &lt;p&gt;This is a reading of RFC 9111 (2022), the latest iteration of the HTTP Caching standard.&lt;/p&gt;
    &lt;p&gt;It defines the &lt;code&gt;Cache-Control&lt;/code&gt; HTTP header as a way to prescribe how caches should store and reuse HTTP responses, with regards to not just the browser cache, but to any other intermediary caches, such as proxies and content delivery networks, that may exist between the client and the origin server.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;Cache-Control&lt;/code&gt; header accepts a set of comma-separated directives, some of which are meant to be added to HTTP requests, and others to HTTP responses. A typical response header:&lt;/p&gt;
    &lt;code&gt;HTTP/2 200
Cache-Control: max-age=0, must-revalidate
&lt;/code&gt;
    &lt;p&gt;Some of these directives specifically target shared caches, that is intermediary caches that serve the same cached responses to many users, while others also apply to private caches such as the browser cache.&lt;/p&gt;
    &lt;head rend="h2"&gt;Whatâs fresh?&lt;/head&gt;
    &lt;p&gt;Whenever the cache receives a request, it must figure out if the cached response is still fresh and can therefore be reused without incurring the performance tax of an HTTP request, or whether it has gone stale and should be validated with the server.&lt;/p&gt;
    &lt;p&gt;To decide on freshness, the cache compares the age of the response to the responseâs so-called freshness timeline.&lt;/p&gt;
    &lt;p&gt;The age of a cached response is the time elapsed since it was last generated or revalidated by the origin server. To the time spent in its own cache, the browser will add any &lt;code&gt;Age: &amp;lt;seconds&amp;gt;&lt;/code&gt; header received from intermediary caches.&lt;/p&gt;
    &lt;p&gt;The freshness timeline is a duration beyond which the cached response is to be considered stale. Itâs usually signaled by the server via the appropriate response headers, but may also be guesstimated by the cache in the absence of explicit, valid cues. In order of precedence:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the server establishes a freshness timeline, in seconds, with the &lt;code&gt;Cache-Control: max-age=&amp;lt;number&amp;gt;&lt;/code&gt;directive on the response; otherwise,&lt;/item&gt;
      &lt;item&gt;the cache falls back to computing the interval between the &lt;code&gt;Expires: &amp;lt;date&amp;gt;&lt;/code&gt;and&lt;code&gt;Date: &amp;lt;date&amp;gt;&lt;/code&gt;response headers, if available; otherwise,&lt;/item&gt;
      &lt;item&gt;if thereâs no &lt;code&gt;Expires&lt;/code&gt;header, the response lacks an explicit expiration, and a heuristic freshness based on the&lt;code&gt;Last-Modified&lt;/code&gt;response header might be applicable.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For shared caches, the special &lt;code&gt;s-maxage=&amp;lt;number&amp;gt;&lt;/code&gt; directive takes precedence over all others.&lt;/p&gt;
    &lt;head rend="h2"&gt;Going past expiration&lt;/head&gt;
    &lt;p&gt;Just because a response has gone stale, it doesnât mean it needs to be thrown out.&lt;/p&gt;
    &lt;p&gt;When it receives a request for a stale cached response, the cache should validate it with its upstream server. Although validation always generates an HTTP request, it avoids a data transfer when thereâs no newer version of the cached response on the server, so it can still be faster than a regular request.&lt;/p&gt;
    &lt;p&gt;Validation uses a mechanism known as a conditional HTTP request, which includes one or more special headers called preconditions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;if the precondition with the highest precedence is met, the server responds with HTTP 200 OK and an updated response body; otherwise,&lt;/item&gt;
      &lt;item&gt;it responds with HTTP 304 Not Modified and an empty body, confirming the existing response can be reused.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To generate the preconditions needed for these conditional requests, which the server uses to compare the cached response to the freshest version available, responses must be tagged in a way thatâs unique to each version:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;historically, this was done with the &lt;code&gt;Last-Modified: &amp;lt;date&amp;gt;&lt;/code&gt;header, corresponding to the latest update to the content;&lt;/item&gt;
      &lt;item&gt;a more flexible and robust alternative is the &lt;code&gt;ETag: "&amp;lt;value&amp;gt;"&lt;/code&gt;header, which stores an arbitrary ASCII string that uniquely identifies the response. This string is usually a hash incorporating one or more aspects: the modification time, the file size, and the file content.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When performing the validation, the cached response headers are mirrored as preconditions for the conditional request:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Last-Modified: &amp;lt;date&amp;gt;&lt;/code&gt;becomes&lt;code&gt;If-Modified-Since: &amp;lt;date&amp;gt;&lt;/code&gt;;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ETag: "&amp;lt;value&amp;gt;"&lt;/code&gt;becomes&lt;code&gt;If-None-Match: "&amp;lt;value&amp;gt;"&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When both preconditions are present, only &lt;code&gt;If-None-Match&lt;/code&gt; is evaluated.&lt;/p&gt;
    &lt;p&gt;Regardless of the result of the validation request, the cached response headers are updated with the new values received from the server, and the fresh-o-meter on the cached response is reset.&lt;/p&gt;
    &lt;p&gt;Certain caches may be set up to serve stale responses in some circumstances, such as when losing the connection to the server or in the event of an HTTP 5xx server error. There are also &lt;code&gt;Cache-Control&lt;/code&gt; directives that influence how stale responses are handled, covered in the next section.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cache-Control response directives&lt;/head&gt;
    &lt;p&gt;
      &lt;code&gt;max-age=&amp;lt;number&amp;gt;&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;max-age&lt;/code&gt; response directive defines the responseâs freshness timeline in seconds, after which the response should be considered stale. â RFC 9111 Â§ 5.2.2.1&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;must-revalidate&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;must-revalidate&lt;/code&gt; response directive indicates that the cache must not reuse a stale response until itâs been successfully validated by the origin server. â RFC 9111 Â§ 5.2.2.2&lt;/p&gt;
    &lt;p&gt;If the server throws an error, the cache must surface that instead of reusing a stale response. If the cache is disconnected, it must produce an error with the HTTP 504 Gateway Timeout status code, or another more applicable error code.&lt;/p&gt;
    &lt;p&gt;Side effects: &lt;code&gt;must-revalidate&lt;/code&gt; is one of the directives, along with &lt;code&gt;s-maxage&lt;/code&gt; and &lt;code&gt;public&lt;/code&gt;, that allow shared caches to store and reuse a response to a request containing an &lt;code&gt;Authorization&lt;/code&gt; header, which they are generally prohibited from doing.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;no-cache&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;no-cache&lt;/code&gt; response directive indicates that the cache must not reuse any response until itâs successfully validated by the origin server. â RFC 9111 Â§ 5.2.2.4&lt;/p&gt;
    &lt;p&gt;This is similar to &lt;code&gt;must-revalidate&lt;/code&gt; but refers to all cached responses, not just stale ones. In effect, &lt;code&gt;no-cache&lt;/code&gt; is a sort of &lt;code&gt;max-age=0, must-revalidate&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;no-store&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;no-store&lt;/code&gt; response directive indicates that private and shared caches must not store any part of the request or the response, and to never reuse the response. The standard is quick to warn that the effect is not guaranteed:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;âMUST NOT storeâ in this context means that the cache MUST NOT intentionally store the information in non-volatile storage and MUST make a best-effort attempt to remove the information from volatile storage as promptly as possible after forwarding it. This directive is not a reliable or sufficient mechanism for ensuring privacy. In particular, malicious or compromised caches might not recognize or obey this directive, and communications networks might be vulnerable to eavesdropping. â RFC 9111 Â§ 5.2.2.4&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Side effects: The directive can also influence non-HTTP caches. Most browsers will exclude from the back/forward cache pages having the &lt;code&gt;no-store&lt;/code&gt; response directive. Chrome, however, has recently started to make some of these pages eligible for bfcache when the browser deems it safe.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;must-understand&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;must-understand&lt;/code&gt; response directive indicates that the cache shouldnât store or reuse responses with HTTP status codes whose semantics the cache doesnât understand and conform to. The directive is meant to future-proof existing implementations from status codes that might have special requirements in regards to caching. â RFC 9111 Â§ 5.2.2.3&lt;/p&gt;
    &lt;p&gt;Itâs recommended to use &lt;code&gt;must-understand, no-store&lt;/code&gt; together as a fallback, and caches are encouraged to ignore the &lt;code&gt;no-store&lt;/code&gt; directive if they do understand the semantics of the HTTP status code. This ensures older caches that donât recognize the &lt;code&gt;must-understand&lt;/code&gt; directive donât cache the response at all, although by 2025 this should be an exceedingly rare sight.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;private&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;private&lt;/code&gt; response directive indicates that the response is meant for a single user. â RFC 9111 Â§ 5.2.2.7.&lt;/p&gt;
    &lt;p&gt;Therefore:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;a shared cache must not store the response; and&lt;/item&gt;
      &lt;item&gt;a private cache may store the response even if the response wouldnât otherwise be heuristically cacheable.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The &lt;code&gt;private&lt;/code&gt; directive can be used to guard against other directives that might inadvertently make authenticated responses available to shared caches.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;public&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;public&lt;/code&gt; response directive indicates two things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;a shared cache may store and reuse a response to a request containing an &lt;code&gt;Authorization&lt;/code&gt;header, which itâs generally prohibited from doing; and&lt;/item&gt;
      &lt;item&gt;a private cache may store the response even if the response wouldnât otherwise be heuristically cacheable.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;s-maxage=&amp;lt;number&amp;gt;&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;s-maxage=&amp;lt;number&amp;gt;&lt;/code&gt; response directive is analogous to &lt;code&gt;max-age&lt;/code&gt;, but only affects shared caches. â RFC 9111 Â§ 5.2.2.10.&lt;/p&gt;
    &lt;p&gt;The directive also incorporates the semantics of the &lt;code&gt;proxyârevalidate&lt;/code&gt; response directive, in that a shared cache must not use a stale response until it has been successfully validated with the origin server.&lt;/p&gt;
    &lt;p&gt;Side effects: &lt;code&gt;s-maxage&lt;/code&gt; is one of the directives, along with &lt;code&gt;must-revalidate&lt;/code&gt; and &lt;code&gt;public&lt;/code&gt;, that allow shared caches to store and reuse a response to a request containing an &lt;code&gt;Authorization&lt;/code&gt; header, which they are generally prohibited from doing.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;proxy-revalidate&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;proxy-revalidate&lt;/code&gt; response directive is analogous to &lt;code&gt;must-revalidate&lt;/code&gt;, but only affects shared caches. â RFC 9111 Â§ 5.2.2.8.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;no-transform&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;no-transform&lt;/code&gt; response directive indicates that intermediaries, regardless of whether they implement a cache or not, must not transform the response content, such as optimizing images or compressing stylesheets and scripts. â RFC 9111 Â§ 5.2.2.6.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;stale-while-revalidate=&amp;lt;number&amp;gt;&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;stale-while-revalidate&lt;/code&gt; response directive was defined in RFC 5861: HTTP Cache-Control Extensions for Stale Content (2010). It indicates that the cache may use a cached response if it hasnât exceeded its freshness lifetime by more than the specified number of seconds.&lt;/p&gt;
    &lt;p&gt;Whenever the presence of this directive causes a stale response to be served, the cache should trigger a background revalidation of the response.&lt;/p&gt;
    &lt;p&gt;The author of the RFC, Mark Nottingham, has written a rationale for this directive.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;stale-if-error=&amp;lt;number&amp;gt;&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Also defined in the RFC 5861 extension, the &lt;code&gt;stale-if-error&lt;/code&gt; response directive indicates that the cache may use a cached response if it hasnât exceeded its freshness lifetime by more than the specified number of seconds, if the attempt to validate the stale response results in an error.&lt;/p&gt;
    &lt;p&gt;HTTP Caching Tests suggests this directive is not well supported.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cache-Control request directives&lt;/head&gt;
    &lt;p&gt;As web developers, we most often deal with &lt;code&gt;Cache-Control&lt;/code&gt; in HTTP responses, but this header can also be included on HTTP requests. Browsers, for example, use them when the user refreshes the page.&lt;/p&gt;
    &lt;p&gt;When used in HTTP requests, &lt;code&gt;Cache-Control&lt;/code&gt; directives express the clientâs preference in regards to the freshness or age of the response. Caches reconcile these requests with the &lt;code&gt;Cache-Control&lt;/code&gt; response directives of its cached responses.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Note: the&lt;/p&gt;&lt;code&gt;cache&lt;/code&gt;option for&lt;code&gt;fetch()&lt;/code&gt;has a separate set of values that map to&lt;code&gt;Cache-Control&lt;/code&gt;request directives, but the mappings are not always intuitive. For example,&lt;code&gt;cache: 'no-cache'&lt;/code&gt;maps to&lt;code&gt;Cache-Control: max-age=0&lt;/code&gt;. For the curious, the mappings are defined here. You can always set your&lt;code&gt;Cache-Control&lt;/code&gt;headers directly with the&lt;code&gt;headers&lt;/code&gt;option.&lt;/quote&gt;
    &lt;head rend="h3"&gt;
      &lt;code&gt;max-age=&amp;lt;number&amp;gt;&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;max-age&lt;/code&gt; request directive indicates that the client wants a fresh response whose age is less than or equal to the specified number of seconds. When combined with &lt;code&gt;max-stale&lt;/code&gt;, the client will accept some stale responses. â RFC 9111 Â§ 5.2.1.1.&lt;/p&gt;
    &lt;head rend="h3"&gt;
      &lt;code&gt;max-stale=&amp;lt;number&amp;gt;&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;max-stale&lt;/code&gt; request directive indicates that the client will accept a stale response that has exceeded its freshness lifetime by no more than the specified number of seconds. When used without an argument, &lt;code&gt;max-stale&lt;/code&gt; indicates that the client will accept any stale response, no matter how old. â RFC 9111 Â§ 5.2.1.2&lt;/p&gt;
    &lt;head rend="h3"&gt;
      &lt;code&gt;min-fresh=&amp;lt;number&amp;gt;&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;min-fresh&lt;/code&gt; request directive indicates that the client prefers a response that still has at least the specified number of seconds of freshness left. â RFC 9111 Â§ 5.2.1.3&lt;/p&gt;
    &lt;head rend="h3"&gt;
      &lt;code&gt;no-cache&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;no-cache&lt;/code&gt; request directive indicates that the client prefers caches not to use a stored response without successfully validating it with the origin server. â RFC 9111 Â§ 5.2.1.4&lt;/p&gt;
    &lt;head rend="h3"&gt;
      &lt;code&gt;no-store&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;no-store&lt;/code&gt; request directive indicates that a cache must not store any part of either this request or any response to it. The same caveats as to its response counterpart apply. â RFC 9111 Â§ 5.2.1.5&lt;/p&gt;
    &lt;p&gt;If a cache serves this request with a response that was previously stored, the &lt;code&gt;no-store&lt;/code&gt; request directive doesnât cause the cache to remove the response after serving it.&lt;/p&gt;
    &lt;head rend="h3"&gt;
      &lt;code&gt;no-transform&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;no-transform&lt;/code&gt; request directive indicates that the client is asking for intermediaries to avoid transforming the content. â RFC 9111 Â§ 5.2.1.6&lt;/p&gt;
    &lt;head rend="h3"&gt;
      &lt;code&gt;only-if-cached&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;only-if-cached&lt;/code&gt; request directive indicates that the client only wants a stored response. Caches should respond with either a stored response that satisfies all the other constraints, or an HTTP 504 Gateway Timeout status code. â RFC 9111 Â§ 5.2.1.7&lt;/p&gt;
    &lt;head rend="h3"&gt;
      &lt;code&gt;stale-if-error=&amp;lt;number&amp;gt;&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;Similarly to its response counterpart, the &lt;code&gt;stale-if-error&lt;/code&gt; request directive indicates that the client will accept a stale response that has exceeded its freshness lifetime by no more than the specified number of seconds, if an attempt to validate it resulted in a server error.&lt;/p&gt;
    &lt;head rend="h2"&gt;Browser refresh mechanisms&lt;/head&gt;
    &lt;p&gt;Browsers typically offer two refresh mechanisms:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;soft reloads, triggered by the reload button, a corresponding menu item and keyboard shortcut, and the pull-to-refresh gesture in mobile browsers, are meant to get an updated representation of the page, for example getting the latest posts on a social media timeline.&lt;/item&gt;
      &lt;item&gt;hard reloads, enabled with a modifier key, skip the cache altogether and are meant to fix interrupted loads, outdated cached responses, and other broken states.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Hereâs how some browsers on macOS implement these behaviors.&lt;/p&gt;
    &lt;head rend="h3"&gt;Soft reloads&lt;/head&gt;
    &lt;p&gt;Triggered by Ctrl + R on Windows/Linux and Command + R on macOS.&lt;/p&gt;
    &lt;p&gt;Firefox triggers a conditional request to revalidate the cached response for the main resource (the HTML file). Sub-resources such as stylesheets, scripts, and images are reloaded as usual, according to their cache directives.&lt;/p&gt;
    &lt;p&gt;Chrome behaves similarly, with the difference that the validation request for the main resource also includes a &lt;code&gt;Cache-Control: max-age=0&lt;/code&gt; directive (which canât hurt).&lt;/p&gt;
    &lt;p&gt;Instead of revalidating its cached response, Safari performs a non-conditional request for the main resource, then loads sub-resources as usual.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hard reloads&lt;/head&gt;
    &lt;p&gt;Triggered by Ctrl + Shift + R on Windows/Linux and Command + Shift + R on macOS except Safari, which uses Command + Option + R. (If youâve applied your muscle memory to Safari before, you know all too well that the common shortcut opens Reader Mode insteadâ¦)&lt;/p&gt;
    &lt;p&gt;On a hard reload, all three browsers trigger non-conditional requests with the &lt;code&gt;Cache-Control: no-cache&lt;/code&gt; directive on the HTML page and its sub-resources.&lt;/p&gt;
    &lt;p&gt;Curiously, once you perform a hard reload in Safari, subsequent soft reloads will still use the &lt;code&gt;Cache-Control: no-cache&lt;/code&gt; request directive to fetch the main resource, which is probably an unintended, but otherwise benign behavior.&lt;/p&gt;
    &lt;head rend="h3"&gt;The &lt;code&gt;immutable&lt;/code&gt; response directive&lt;/head&gt;
    &lt;p&gt;Reloading a web page didnât always work like this. Historically, when performing a soft reload, all the sub-resources would be revalidated along with the main resource, in effect freshening up the cache for the current page.&lt;/p&gt;
    &lt;p&gt;Circa 2015, Facebook was seeing several HTTP 304 Not Modified responses on long-lived resources like scripts and stylesheets whenever a user would refresh their feed page with the browserâs reload button.&lt;/p&gt;
    &lt;p&gt;To address this issue, Patrick McManus from Mozilla proposed the &lt;code&gt;immutable&lt;/code&gt; response directive, which later became RFC 8246: HTTP Immutable Responses.&lt;/p&gt;
    &lt;p&gt;The directive indicates that the origin server wonât update a resource during the freshness lifetime of the cached response, so a cache shouldnât issue conditional requests for responses that are still fresh when the user reloads the page, unless the user really, really wants an updated response (e.g. a hard reload).&lt;/p&gt;
    &lt;p&gt;Around the time that support for &lt;code&gt;immutable&lt;/code&gt; landed in Firefox 49 and Facebook began to use it to great effect, Chrome introduced a new way to perform reloads that solved the problem without introducing additional directives: instead of revalidating everything on a soft reload, just revalidate the main resource and load sub-resources as usual. Safari switched over to the new reload policy soon after [Webkit#169756], and Firefox eventually did with Firefox 100.&lt;/p&gt;
    &lt;p&gt;That leaves the &lt;code&gt;immutable&lt;/code&gt; directive in an awkward place. Safari added support [Webkit#167497] but Chrome representatives remain unconvinced that it offers a significant benefit on top of the current reload behavior [Chromium#41253661].&lt;/p&gt;
    &lt;head rend="h2"&gt;Caching responses to authenticated requests&lt;/head&gt;
    &lt;p&gt;One of the more confusing aspects of HTTP caching is how various &lt;code&gt;Cache-Control&lt;/code&gt; response directives affect the way shared caches treat responses to requests that contain an &lt;code&gt;Authorization&lt;/code&gt; header, which are understood as specific to a single user.&lt;/p&gt;
    &lt;p&gt;As per RFC 9111 Â§ 3.5, shared caches are not allowed to store these responses &lt;quote&gt;unless the response contains a Cache-Control field with a response directive that allows it to be stored by a shared cache, and the cache conforms to the requirements of that directive for that response.&lt;/quote&gt;&lt;/p&gt;
    &lt;p&gt;The three directives that enable shared caches to store authenticated responses, and which must therefore be carefully evaluated before deploying, are:&lt;/p&gt;
    &lt;p&gt;Conversely, a &lt;code&gt;private&lt;/code&gt; directive prevents any other directive from making authenticated responses eligible to shared caches.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;I wrote this article to clarify for myself what the various cache directives stand for and how they overlap and interact. It only covers the main ideas, without delving into the more obscure corners of HTTP semantics. I approached the subject with a âclear cacheâ, and mainly used the normative references (RFC 9111 and its extensions), aided by various guides from different eras:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Caching Tutorial for Web Authors and Webmasters (1998â) by Mark Nottingham;&lt;/item&gt;
      &lt;item&gt;Caching best practices &amp;amp; max-age gotchas (2016) by Jake Archibald;&lt;/item&gt;
      &lt;item&gt;Prevent unnecessary network requests with the HTTP Cache (2018) by Ilya Grigorik and Jeff Posnik;&lt;/item&gt;
      &lt;item&gt;Cache control for civilians (2019â2025) and Why Do We Have a Cache-Control Request Header? (2025) by Harry Roberts;&lt;/item&gt;
      &lt;item&gt;Cache-Control Recommendations (2021) by April King;&lt;/item&gt;
      &lt;item&gt;Web Caching on MDN.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Whatâs interesting about these guides is that the recommendations donât just encode an interpretation of the specs, but also incorporate safeguards against non-conformant or outdated browser caches and intermediares.&lt;/p&gt;
    &lt;p&gt;In light of developments as recent as 2022, it would be cool to figure out to what extent things have improved, and which of these safeguards can be discarded. HTTP Caching Tests seems to be a good resource for assessing the situation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46368616</guid><pubDate>Tue, 23 Dec 2025 19:41:39 +0000</pubDate></item><item><title>I didn't realize my LG TV was spying on me until I turned off this setting</title><link>https://www.pocket-lint.com/lg-tv-turn-off-live-plus/</link><description>&lt;doc fingerprint="d0fcd0966b03abba"&gt;
  &lt;main&gt;
    &lt;p&gt;When I first set up my LG TV, my main focus was ensuring the picture quality was perfect. To LG's credit, the TV automatically detected all of my devices -- my PC, PS5, Switch 2, and Fire TV Stick 4K Max -- and applied the best settings for each. The image looked immaculate right out of the box, so I didn't spend much time digging through the settings menu and instead jumped straight into movies and gaming.&lt;/p&gt;
    &lt;p&gt;However, about a week later, while I was trying to disable ads on the home screen, I stumbled across a setting I didn't even know existed and instantly knew I wanted to be disabled. It's called Live Plus.&lt;/p&gt;
    &lt;p&gt;If you've never heard of Live Plus before, it's a feature on LG smart TVs that uses ACR (automatic content recognition) to analyze what's displayed on your screen (via The Markup). LG then uses that data to offer "personalized services," including content recommendations and advertisements.&lt;/p&gt;
    &lt;p&gt;Naturally, reading what the feature was on my TV was a bit of a shock. I wasn't thrilled to discover a setting enabled by default that essentially spies on my screen just for ads, but at the same time, I felt a wave of relief when I realized how easy it is to turn off. If you don't want your LG TV quietly snooping on what you watch and using it to serve you ads, here's how to turn Live Plus off.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to turn off Live Plus on your LG TV&lt;/head&gt;
    &lt;head rend="h3"&gt;Other TV brands have a similar setting&lt;/head&gt;
    &lt;p&gt;Thankfully, LG doesn't make disabling Live Plus too hard, though you do have to click through a few menus. If you want to turn it off, here's how:&lt;/p&gt;
    &lt;p&gt;1. Press the Settings button on your remote (the gear icon).&lt;/p&gt;
    &lt;p&gt;2. When the side menu pops up, select Settings.&lt;/p&gt;
    &lt;p&gt;3. Chose the General option.&lt;/p&gt;
    &lt;p&gt;4. Scroll down and select System.&lt;/p&gt;
    &lt;p&gt;5, Select Additional Settings.&lt;/p&gt;
    &lt;p&gt;6. Toggle Live Plus off.&lt;/p&gt;
    &lt;p&gt;In the Settings menu on its TVs, LG says, "By turning Live Plus on, you understand that the content displayed on your TV can be recognized, and that the viewing information may be used to provide you with an enhanced viewing experience and personalized services including content recommendations and advertisements."&lt;/p&gt;
    &lt;p&gt;Fortunately, once you've toggled Live Plus off, you no longer have to worry about your TV screen constantly being read to see what you're watching and to give you targeted ads.&lt;/p&gt;
    &lt;p&gt;...to be clear, this isn't just an LG problem -- other smart TV brands do the same thing with similar ACR (automatic content recognition) settings.&lt;/p&gt;
    &lt;p&gt;While it's frustrating that a setting like this exists in the first place, I was at least relieved by how easy it was to turn off. And to be clear, this isn't just an LG problem -- other smart TV brands do the same thing with similar ACR (automatic content recognition) settings.&lt;/p&gt;
    &lt;p&gt;On Samsung smart TVs, for example, you can disable targeted ads by going to Privacy Choices, selecting Terms and Conditions, and toggling off Viewing Information Services and Internet-Based Advertisement Services. On Roku TVs, ACR can be turned off by disabling Use info from TV inputs, which is tucked away in the settings menu under Smart TV Experience.&lt;/p&gt;
    &lt;p&gt;In other LG-related news, DirecTV recently launched its live TV app on the LG webOS app store, and LG recently made the controversial move to automatically install Copilot on all LG Smart TVs with its latest webOS update.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46369860</guid><pubDate>Tue, 23 Dec 2025 21:47:33 +0000</pubDate></item><item><title>X-ray: a Python library for finding bad redactions in PDF documents</title><link>https://github.com/freelawproject/x-ray</link><description>&lt;doc fingerprint="4e945bd17ca78994"&gt;
  &lt;main&gt;
    &lt;p&gt;x-ray is a Python library for finding bad redactions in PDF documents.&lt;/p&gt;
    &lt;p&gt;At Free Law Project, we collect millions of PDFs. An ongoing problem is that people fail to properly redact things. Instead of doing it the right way, they just draw a black rectangle or a black highlight on top of black text and call it a day. Well, when that happens you just select the text under the rectangle, and you can read it again. Not great.&lt;/p&gt;
    &lt;p&gt;After witnessing this problem for years, we decided it would be good to figure out how common it is, so, with some help, we built this simple tool. You give the tool the path to a PDF. It tells you if it has worthless redactions in it.&lt;/p&gt;
    &lt;p&gt;Right now, &lt;code&gt;x-ray&lt;/code&gt; works pretty well and we are using it to analyze documents
in our collections. It could be better though. Bad redactions take many forms.
See the issues tab for other examples we don't yet support. We'd love your
help solving some of tougher cases.&lt;/p&gt;
    &lt;p&gt;With uv, do:&lt;/p&gt;
    &lt;code&gt;uv add x-ray
&lt;/code&gt;
    &lt;p&gt;With pip, that'd be:&lt;/p&gt;
    &lt;code&gt;pip install x-ray
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;uvx&lt;/code&gt; lets you run this without even installing it. For example, here's an amicus brief we filed that doesn't have any bad redactions:&lt;/p&gt;
    &lt;code&gt;uvx --from x-ray xray https://storage.courtlistener.com/recap/gov.uscourts.ca3.125346/gov.uscourts.ca3.125346.45.0.pdf
{}
&lt;/code&gt;
    &lt;p&gt;Once you do install x-ray, you can easily use it on the command line. Once installed, just:&lt;/p&gt;
    &lt;code&gt;% xray path/to/your/file.pdf
{
  "1": [
    {
      "bbox": [
        58.550079345703125,
        72.19873046875,
        75.65007781982422,
        739.3987426757812
      ],
      "text": "The Ring travels by way of Cirith Ungol"
    }
  ]
}&lt;/code&gt;
    &lt;p&gt;Or if you have the file on a server somewhere, give it a URL. If it starts with &lt;code&gt;https://&lt;/code&gt;, it will be interpreted as a PDF to download. Here's congressional testimonry our directory made (it doesn't have any bad redactions):&lt;/p&gt;
    &lt;code&gt;% xray https://free.law/pdf/congressional-testimony-michael-lissner-free-law-project-hearing-on-ethics-and-transparency-2021-10-26.pdf
{}&lt;/code&gt;
    &lt;p&gt;A fun trick you can do is to make a file with one URL per line, call it &lt;code&gt;urls.txt&lt;/code&gt;. Then you can run this to check each URL:&lt;/p&gt;
    &lt;code&gt;xargs -n 1 xray  &amp;lt; urls.txt&lt;/code&gt;
    &lt;p&gt;However you run &lt;code&gt;xray&lt;/code&gt; on the command line, you'll get JSON as output. When you have that, you can use it with tools like &lt;code&gt;jq&lt;/code&gt;. The format is as follows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It's a dict.&lt;/item&gt;
      &lt;item&gt;The keys are page numbers.&lt;/item&gt;
      &lt;item&gt;Each page number maps to a list of dicts.&lt;/item&gt;
      &lt;item&gt;Each of those dicts maps to two keys.&lt;/item&gt;
      &lt;item&gt;The first key is &lt;code&gt;bbox&lt;/code&gt;. This is a four-tuple that indicates the x,y positions of the upper left corner and then lower right corners of the bad redaction.&lt;/item&gt;
      &lt;item&gt;The second key is &lt;code&gt;text&lt;/code&gt;. This is the text under the bad rectangle.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Simple enough.&lt;/p&gt;
    &lt;p&gt;You can also use it as a Python module, if you prefer the long-form:&lt;/p&gt;
    &lt;code&gt;% python -m xray some-file.pdf
&lt;/code&gt;
    &lt;p&gt;But that's not as easy to remember.&lt;/p&gt;
    &lt;p&gt;If you want a bit more, you can, of course, use &lt;code&gt;xray&lt;/code&gt; in Python:&lt;/p&gt;
    &lt;code&gt;from pprint import pprint
import xray
bad_redactions = xray.inspect("some/path/to/your/file.pdf")  # Pathlib works too
pprint(bad_redactions)
{1: [{'bbox': (58.550079345703125,
               72.19873046875,
               75.65007781982422,
               739.3987426757812),
      'text': 'Aragorn is the one true king.'}]}&lt;/code&gt;
    &lt;p&gt;The output is the same as above, except it's a Python object, not a JSON object.&lt;/p&gt;
    &lt;p&gt;If you already have the file contents as a &lt;code&gt;bytes&lt;/code&gt; object, that'll work too:&lt;/p&gt;
    &lt;code&gt;some_bytes = requests.get("https://lotr-secrets.com/some-doc.pdf").content
bad_redactions = xray.inspect(some_bytes)&lt;/code&gt;
    &lt;p&gt;Note that because the &lt;code&gt;inspect&lt;/code&gt; method uses the same signature no matter what,
the type of the object you give it is essential:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Input&lt;/cell&gt;
        &lt;cell role="head"&gt;&lt;code&gt;xray&lt;/code&gt;'s Assumption&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;str&lt;/code&gt; or Pathlib &lt;code&gt;Path&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;local file&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;str&lt;/code&gt; that starts with &lt;code&gt;https://&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;URL to download&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;bytes&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;PDF in memory&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This means that if you provide the filename on disk as a bytes object instead of a &lt;code&gt;str&lt;/code&gt;, it's not going to work. This will fail:&lt;/p&gt;
    &lt;code&gt;xray.inspect(b"some-file-path.pdf")&lt;/code&gt;
    &lt;p&gt;That's pretty much it. There are no configuration files or other variables to learn. You give it a file name. If there is a bad redaction in it, you'll soon find out.&lt;/p&gt;
    &lt;p&gt;Under the covers, &lt;code&gt;xray&lt;/code&gt; uses the high-performant PyMuPDF project to parse PDFs. It has been a wonderful project to work with.&lt;/p&gt;
    &lt;p&gt;You can read the source to see how it works, but the general idea is to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Find rectangles in a PDF.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Find letters in the same location&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Render the rectangle as an image&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Inspect the rectangle to see if it's all one color. If it is, then that's a bad redaction. If not, then we assume you can see a mix of text and drawings, indicating a redaction that's OK.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The PDF format is a big and complicated one, so it's difficult to do all this perfectly. We do our best, but there's always more to do to make it better. Donations and sponsored work help.&lt;/p&gt;
    &lt;p&gt;Please see the issues list on Github for things we need, or start a conversation if you have questions. Before you do your first contribution, we'll need a signed contributor license agreement. See the template in the repo.&lt;/p&gt;
    &lt;p&gt;Releases happen automatically via Github Actions. To trigger an automated build:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Update CHANGES.md&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Update the version in pyproject.toml&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tag the commit with something like "v0.0.0".&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you wish to create a new version manually, the process is:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Update version info in&lt;/p&gt;
        &lt;code&gt;pyproject.toml&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Configure your Pypi credentials with Poetry&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Build and publish the version:&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;poetry publish --build&lt;/code&gt;
    &lt;p&gt;This repository is available under the permissive BSD license, making it easy and safe to incorporate in your own libraries.&lt;/p&gt;
    &lt;p&gt;Pull and feature requests welcome. Online editing in GitHub is possible (and easy!).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46369923</guid><pubDate>Tue, 23 Dec 2025 21:54:30 +0000</pubDate></item><item><title>Texas App Store Age Verification Law Blocked by Federal Judge</title><link>https://www.macrumors.com/2025/12/23/texas-app-store-law-blocked/</link><description>&lt;doc fingerprint="b6c4f825132611dc"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h1"&gt;Texas App Store Age Verification Law Blocked by Federal Judge&lt;/head&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt;A Texas federal judge today blocked an App Store age verification law that was set to go into effect on January 1, 2026, which means Apple may not have to support the changes after all.&lt;/p&gt;
          &lt;p&gt;&lt;lb/&gt;The Texas App Store Accountability Act (SB2420) requires Apple and other app marketplaces to confirm user age when a person creates an Apple Account. Apple Accounts for users under 18 would need to join a Family Sharing group, with new controls available for parents and restrictions for minors.&lt;/p&gt;
          &lt;p&gt;In a preliminary injunction that delays the implementation of the act, Judge Robert Pitman said that it violates the First Amendment and is "more likely than not unconstitutional."&lt;/p&gt;
          &lt;quote&gt;
            &lt;p&gt;The Act is akin to a law that would require every bookstore to verify the age of every customer at the door and, for minors, require parental consent before the child or teen could enter and again when they try to purchase a book. As set out below, the Court finds a likelihood that, when considered on the merits, SB 2420 violates the First Amendment.&lt;/p&gt;
          &lt;/quote&gt;
          &lt;p&gt;The injunction was in response to a motion filed by the Computer and Communications Industry Association (CCIA), a group that includes Apple and Google. Today's decision is a win for Apple, as Apple has been fighting against age assurance requirements in Texas and other states. Apple says that the Texas law impacts user privacy.&lt;/p&gt;
          &lt;quote&gt;
            &lt;p&gt;While we share the goal of strengthening kids' online safety, we are concerned that SB2420 impacts the privacy of users by requiring the collection of sensitive, personally identifiable information to download any app, even if a user simply wants to check the weather or sports scores.&lt;/p&gt;
          &lt;/quote&gt;
          &lt;p&gt;The court will move on to determining whether the law is facially invalid, which would mean that it is unconstitutional and will be entirely thrown out.&lt;/p&gt;
          &lt;p&gt;Note: Due to the political or social nature of the discussion regarding this topic, the discussion thread is located in our Political News forum. All forum members and site visitors are welcome to read and follow the thread, but posting is limited to forum members with at least 100 posts.&lt;/p&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;head rend="h2"&gt;Popular Stories&lt;/head&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple hasn't updated the Apple TV 4K since 2022, and 2025 was supposed to be the year that we got a refresh. There were rumors suggesting Apple would release the new Apple TV before the end of 2025, but it looks like that's not going to happen now. Subscribe to the MacRumors YouTube channel for more videos. Bloomberg's Mark Gurman said several times across 2024 and 2025 that Apple would...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;You'd think things would be slowing down heading into the holidays, but this week saw a whirlwind of Apple leaks and rumors while Apple started its next cycle of betas following last week's release of iOS 26.2 and related updates. This week also saw the release of a new Apple Music integration with ChatGPT, so read on below for all the details on this week's biggest stories! Top Stories i...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Next year's iPhone 18 Pro and iPhone 18 Pro Max will be equipped with under-screen Face ID, and the front camera will be moved to the top-left corner of the screen, according to a new report from The Information's Wayne Ma and Qianer Liu. As a result of these changes, the report said the iPhone 18 Pro models will not have a pill-shaped Dynamic Island cutout at the top of the screen....&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;The European Commission today praised the interoperability changes that Apple is introducing in iOS 26.3, once again crediting the Digital Markets Act (DMA) with bringing "new opportunities" to European users and developers. The Digital Markets Act requires Apple to provide third-party accessories with the same capabilities and access to device features that Apple's own products get. In iOS...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Since the beginning of December, Apple has been pushing iPhone users who opted to stay on iOS 18 to install iOS 26 instead. Apple started by making the iOS 18 upgrades less visible, and has now transitioned to making new iOS 18 updates unavailable on any device capable of running iOS 26. If you have an iPhone 11 or later, Apple is no longer offering new versions of iOS 18, even though there...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Earlier this month, Apple released iOS 26.2, following more than a month of beta testing. It is a big update, with many new features and changes for iPhones. iOS 26.2 adds a Liquid Glass slider for the Lock Screen's clock, offline lyrics in Apple Music, and more. Below, we have highlighted a total of eight new features. Liquid Glass Slider on Lock Screen A new slider in the Lock...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple is significantly increasing its reliance on Samsung for iPhone memory as component prices surge, according to The Korea Economic Daily. Apple is said to be expanding the share of iPhone memory it sources from Samsung due to rapidly rising memory prices. The shift is expected to result in Samsung supplying roughly 60% to 70% of the low-power DRAM used in the iPhone 17, compared with a...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;While the iPhone 18 Pro and iPhone 18 Pro Max are not expected to launch for another nine months, there are already plenty of rumors about the devices. Below, we have recapped 12 features rumored for the iPhone 18 Pro models. The same overall design is expected, with 6.3-inch and 6.9-inch display sizes, and a "plateau" housing three rear cameras Under-screen Face ID Front camera in...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46370012</guid><pubDate>Tue, 23 Dec 2025 22:03:46 +0000</pubDate></item><item><title>Microspeak: North Star – The Old New Thing (2015)</title><link>https://devblogs.microsoft.com/oldnewthing/20151103-00/?p=91861</link><description>&lt;doc fingerprint="740e9f0f4f6e4c55"&gt;
  &lt;main&gt;
    &lt;p&gt;I noted it in the interview with the Defrag Tools show, but I’ll make a proper Microspeak for it. Today’s term is North star.&lt;/p&gt;
    &lt;p&gt;This term rose quickly to prominence in October 2015. My research suggests that it had been simmering below the surface for about a year. For example, here’s an isolated citation from May 2015:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The best you can do is paint a compelling picture of an improved world (your north star), and plan the long journey to it.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This citation is interesting because it seems to give a definition for “north star”: It means “a compelling picture of an improved world”.&lt;/p&gt;
    &lt;p&gt;The term has become wildly popular of late at Microsoft. I guess a major executive used the term recently, so now it’s suddenly the cool thing to say.&lt;/p&gt;
    &lt;p&gt;We had a team meeting a little while ago. One of the agenda items was “Longer term North star topics”, which was itself rather intriguing. During the meeting, I noted¹ the following uses of the term:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;There may be changes along the way, but your north star of the feature is intact.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;We have to decide where we want to go as a north star.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I raised my hand. “What do you mean by north star? Because if you follow the north star, you end up at the north pole, and not where you actually want to go.”&lt;/p&gt;
    &lt;p&gt;The speaker seemed a bit frustrated by this question. “Who is this idiot who doesn’t know what a north star is? Certainly this person hasn’t been in all the meetings I’ve been in, where people are saying ‘north star’ all over the place.”&lt;/p&gt;
    &lt;p&gt;The speaker noted that I might want to look it up in the dictionary, because it would have told me that the north star is the goal you have beyond your immediate goal. It’s a guiding principle that keeps you on the right path for your journey. (Curiously, this definition doesn’t appear anywhere in any online dictionary I could find. It also doesn’t match the citation at the top of this article.)&lt;/p&gt;
    &lt;p&gt;So there you go. An explicit definition, as provided by somebody who used the term. I embarrassed myself in front of my whole team for you.&lt;/p&gt;
    &lt;p&gt;Bonus chatter: Later that same day, a top executive sent mail to the entire company. It too used the term “north star”:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;With Microsoft’s mission as our north star—to empower every person and every organization on the planet to achieve more—we have a…&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;¹ Yes, when I attend meetings, one of the things I pay particular attention to is new jargon, so I can add it to my collection of citations. If you see me pull out my phone and jot something down, it’s either because I’m writing down a question to ask later, or I’m preserving something you said so I can add it to my Microspeak citations.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46370180</guid><pubDate>Tue, 23 Dec 2025 22:23:49 +0000</pubDate></item></channel></rss>