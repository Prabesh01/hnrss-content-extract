<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 23 Sep 2025 17:07:50 +0000</lastBuildDate><item><title>Go has added Valgrind support</title><link>https://go-review.googlesource.com/c/go/+/674077</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45344708</guid><pubDate>Tue, 23 Sep 2025 09:26:57 +0000</pubDate></item><item><title>Structured Outputs in LLMs</title><link>https://parthsareen.com/blog.html#sampling.md</link><description>&lt;doc fingerprint="d3fff7ff6f587ef7"&gt;
  &lt;main&gt;
    &lt;p&gt;Writings&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45345207</guid><pubDate>Tue, 23 Sep 2025 10:40:41 +0000</pubDate></item><item><title>Zinc (YC W14) Is Hiring a Senior Back End Engineer (NYC)</title><link>https://app.dover.com/apply/Zinc/4d32fdb9-c3e6-4f84-a4a2-12c80018fe8f/?rs=76643084</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45345742</guid><pubDate>Tue, 23 Sep 2025 12:00:00 +0000</pubDate></item><item><title>Mesh: I tried Htmx, then ditched it</title><link>https://ajmoon.com/posts/mesh-i-tried-htmx-then-ditched-it</link><description>&lt;doc fingerprint="ae1a9a5068051682"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;MESH: I tried HTMX, then ditched it&lt;/head&gt;
    &lt;p&gt;There is a kind of exciting movement in Web dev right now. Web devs are talking about "JavaScript Fatigue", "Framework Fatigue", the "Revival of Hypermedia" and "HTML Over The Wire". In a word: we're asking ourselves why we're building HTML in JavaScript.&lt;/p&gt;
    &lt;p&gt;The figurehead for this movement is undoubtedly HTMX. It shows that much of what we do in JavaScript could instead be done declaratively, with HTML attributes. If browsers adopted these semantics natively, many websites - and even apps - wouldn't need JavaScript at all. I love this idea! Writing HTML first and adding JS on top is the way the Web should work.&lt;/p&gt;
    &lt;p&gt;At present, we write JavaScript first, and we use it to generate HTML. How did we get it so backwards? I believe the answer is pretty straightforward: SPA frameworks are a joy to use. They impose structure, enforcing conventions, ultimately making it easy to keep concerns separated in one's mind.&lt;/p&gt;
    &lt;p&gt;My big problem with HTMX, as it stands, is that it lacks that structure. Taking a look at HTMX the first time, my reaction was: "...so, declarative jQuery." I could see, as if before my very eyes, the spaghetti that inevitably grows out of a library like this. HTMX leaves it up to the developer to impose discipline on their code, however they see fit.&lt;/p&gt;
    &lt;p&gt;So, I decided to accept the challenge. I want to do modular SSR the way HTMX encourages, but I want to do it with something like an SPA framework. I want nestable components, each with their own HTML, CSS, and JS - and back-end code - sitting side by side. I want there to be one, and only one, right way to do something.&lt;/p&gt;
    &lt;p&gt;The result of this journey is MESH - modular element SSR with hydration. MESH is based on a simple principle: one component = one endpoint. This is a powerful idea - it allows us to write a HTML-first back-end in such a way that it feels like writing an SPA.&lt;/p&gt;
    &lt;p&gt;This write-up includes a lot of code snippets. I've tried to keep these minimal. If you want to follow along with more context, you can find the whole commit history for MESH on GitHub.&lt;/p&gt;
    &lt;head rend="h2"&gt;Basic Interactivity&lt;/head&gt;
    &lt;p&gt;Looking around, it seemed the back-end of choice for HTMX devs is Go with Templ. I've never really had my "Damascus moment" with Go, but this was a good opportunity to get my feet wet. I will say this much: it is a joy to work with something genuinely blazingly fast to build and deploy.&lt;/p&gt;
    &lt;p&gt;I also wanted to have a go with proper vibe coding - writing code without reading it - with Junie. What fun this was! I can see why people would be tempted to write whole apps this way. I'll only say this much: as someone who's battled addiction in the past, I didn't like what I noticed my brain was doing with it. That's a subject for another blog post another time.&lt;/p&gt;
    &lt;p&gt;My mission was, in a word, to write something like an opinionated framework or "harness" for HTMX which would give me a standard way to use it with Web Components. Specifically, what I had in mind was a "one component one endpoint" model. HTMX would always swap the entire component, which would then be "hydrated".&lt;/p&gt;
    &lt;p&gt;It turns out there is a standard way to do server-side rendered custom elements, called Declarative Shadow DOM (DSD). Others have already had some success using HTMX and DSD together. The combo looked promising.&lt;/p&gt;
    &lt;p&gt;There is one significant limitation, however: HTMX will not cross shadow root boundaries. This is by design, to be clear - this is how we should expect HTMX to behave. No sweat, we can do a simple hack to make it work - and, at the same time, to enforce component-level swaps:&lt;/p&gt;
    &lt;code&gt;import type {HtmxBeforeSwapDetail} from "./types/htmx";

function enforceComponentSwap(evt: CustomEvent&amp;lt;HtmxBeforeSwapDetail&amp;gt;) {
  const detail = evt.detail;
  let elt = detail.elt;
  let root = elt.getRootNode();
  
  if (root instanceof ShadowRoot) {
    detail.target = root.host as HTMLElement;
    detail.swapOverride = "outerHTML";
  }
}

document.body.addEventListener("htmx:beforeSwap", enforceComponentSwap as EventListener);
&lt;/code&gt;
    &lt;p&gt;With this little helper, I can now start building out a very simple Trello clone to prove the concept. Let's build a little editable card component:&lt;/p&gt;
    &lt;code&gt;package card

import (
  "mesh/src/services"
  "fmt"
)

type CardProps struct {
  *services.Card
}

templ Card(props CardProps) {
  &amp;lt;mesh-card
    id={ fmt.Sprintf("card-%d", props.Card.ID) }
  &amp;gt;
    &amp;lt;template shadowrootmode="open"&amp;gt;
      &amp;lt;base href="/"/&amp;gt;
      &amp;lt;link rel="stylesheet" href="/static/css/components/card.css"/&amp;gt;
      &amp;lt;div data-view class="card"&amp;gt;
        &amp;lt;div class="card-header"&amp;gt;
          &amp;lt;h3&amp;gt;{ props.Card.Title }&amp;lt;/h3&amp;gt;
        &amp;lt;/div&amp;gt;
        &amp;lt;div class="card-content"&amp;gt;
          { props.Card.Content }
        &amp;lt;/div&amp;gt;
        &amp;lt;div class="actions"&amp;gt;
          &amp;lt;button type="button" mesh-click="edit"&amp;gt;Edit&amp;lt;/button&amp;gt;
        &amp;lt;/div&amp;gt;
      &amp;lt;/div&amp;gt;
      &amp;lt;form data-form class="card hide" hx-patch="/card"&amp;gt;
        &amp;lt;input type="hidden" name="cardID" value={ props.Card.ID } /&amp;gt;
        &amp;lt;label&amp;gt;
          Title
          &amp;lt;input type="text" name="title" value={ props.Data.Title } /&amp;gt;
        &amp;lt;/label&amp;gt;
        &amp;lt;label&amp;gt;
          Content
          &amp;lt;textarea name="content"&amp;gt;{ props.Data.Content }&amp;lt;/textarea&amp;gt;
        &amp;lt;/label&amp;gt;
        &amp;lt;div class="actions"&amp;gt;
          &amp;lt;button type="button" mesh-click="cancel"&amp;gt;Cancel&amp;lt;/button&amp;gt;
          &amp;lt;button type="submit"&amp;gt;Save&amp;lt;/button&amp;gt;
        &amp;lt;/div&amp;gt;
      &amp;lt;/form&amp;gt;
    &amp;lt;/template&amp;gt;
  &amp;lt;/mesh-card&amp;gt;
}
&lt;/code&gt;
    &lt;p&gt;Now let's hydrate it. I'm going to start with a simple base element that'll ensure that our shadow root is attached properly and processed by HTMX:&lt;/p&gt;
    &lt;code&gt;export class MeshElement extends HTMLElement {
  connectedCallback() { 
    if (!this.shadowRoot) {
      // the browser should do this for us - oh well, what can you do?
      const root = this.attachShadow({ mode: 'open' });
      const template = this.querySelector('template[shadowrootmode="open"]');
      if (template) {
        root.appendChild((template as any).content.cloneNode(true));
      }
    }
    if (window.htmx) {
      window.htmx.process(this);
      if (this.shadowRoot) {
        window.htmx.process(this.shadowRoot);
      }
    }
    this.bindListeners();
  }

  protected bindListeners() {
    const supportedEvents = ['click'];

    supportedEvents.forEach(eventName =&amp;gt; {
      const attribute = "mesh-" + eventName;
      this.all('[' + attribute + ']', el =&amp;gt; {
        const methodName = el.getAttribute(attribute);
        if (!methodName) {
          return;
        }
        const method = (this as any)[methodName];
        if (!method || typeof method !== 'function') {
          console.error(`Method ${methodName} is not a function`);
          return;
        }

        el.addEventListener(eventName, method.bind(this));
      });
    });
  }

  all(selector: string, cb: (el: HTMLElement) =&amp;gt; void) {
    return this.shadowRoot!.querySelectorAll(selector).forEach(e =&amp;gt; cb(e as HTMLElement));
  }
}
&lt;/code&gt;
    &lt;p&gt;Then our card element is straightforward to implement:&lt;/p&gt;
    &lt;code&gt;import {MeshElement} from "../base/mesh-element.ts";

export class Card extends MeshElement {
  edit() {
    this.show('[data-form]');
    this.hide('[data-view]');
  }
  
  cancel() {
    this.hide('[data-form]');
    this.show('[data-view]');
  }

  show(selector: string) {
    this.all(selector, el =&amp;gt; {
      el.classList.remove('hide');
    });
  }

  hide(selector: string) {
    this.all(selector, el =&amp;gt; {
      el.classList.add('hide');
    });
  }
}
window.customElements.define('mesh-card', Card);
&lt;/code&gt;
    &lt;p&gt;This works great! I've enhanced my card component with some basic JS to show that it can be done, and otherwise this is all just plain old HTMX.&lt;/p&gt;
    &lt;p&gt;The next step is adding functionality to move the cards between columns. This is where we'll run into a common difficulty with HTMX: how to swap out "parent" components given an update on a child component.&lt;/p&gt;
    &lt;p&gt;Now, HTMX devs have a number of differing opinions on how best to go about this. One common practice is to "expand the target", which means your component needs to be aware of parent components. Another way is to trigger events in the response headers - this is better, in that it moves responsibility for this back to the server. I believe front-end components shouldn't know anything their own placement on the page.&lt;/p&gt;
    &lt;p&gt;Fortunately, HTMX gives us another way to do this - and it appears to be the emerging "best practice" - with "out of band" (OOB) swaps. If, in our response to a call to the card endpoint, we return any other components that need updating, and simply flag them as OOB, HTMX will handle the swaps for us. This best reflects my own aims for MESH, so let's see how we get along doing it this way.&lt;/p&gt;
    &lt;p&gt;Let's add "promote" functionality to our card component - this will simply move the card one column to the right:&lt;/p&gt;
    &lt;code&gt;package card

import (
  "mesh/src/services"
  "fmt"
)

const PutActionPromote = "promote"

type CardProps struct {
  *services.Card
  CanPromote bool
}

templ Card(props CardProps) {
  &amp;lt;mesh-card
    id={ fmt.Sprintf("card-%d", props.Card.ID) }
  &amp;gt;
    &amp;lt;template shadowrootmode="open"&amp;gt;
      &amp;lt;base href="/"/&amp;gt;
      &amp;lt;link rel="stylesheet" href="/static/css/components/card.css"/&amp;gt;
      &amp;lt;div data-view class="card"&amp;gt;
        &amp;lt;div class="card-header"&amp;gt;
          &amp;lt;h3&amp;gt;{ props.Card.Title }&amp;lt;/h3&amp;gt;
        &amp;lt;/div&amp;gt;
        &amp;lt;div class="card-content"&amp;gt;
          { props.Card.Content }
        &amp;lt;/div&amp;gt;
        &amp;lt;div class="actions"&amp;gt;
          if props.CanPromote {
            &amp;lt;form hx-put="/card"&amp;gt;
              &amp;lt;input type="hidden" name="action" value="promote" /&amp;gt;
              &amp;lt;input type="hidden" name="cardID" value={props.Card.ID} /&amp;gt;
              &amp;lt;button type="submit" aria-label="Move to next column"&amp;gt;
                &amp;lt;i data-lucide="arrow-right"&amp;gt;&amp;lt;/i&amp;gt;
              &amp;lt;/button&amp;gt;
            &amp;lt;/form&amp;gt;
          }
        &amp;lt;/div&amp;gt;
      &amp;lt;/div&amp;gt;
    &amp;lt;/template&amp;gt;
  &amp;lt;/mesh-card&amp;gt;
}
&lt;/code&gt;
    &lt;p&gt;To handle the OOB updates, we'll write a "context-enriched" pub-sub:&lt;/p&gt;
    &lt;code&gt;type EventContext struct {
	Context        context.Context
	ResponseWriter http.ResponseWriter
}

func (e *EventContext) Write(component templ.Component) {
	err := component.Render(e.Context, e.ResponseWriter)
	if err != nil {
		http.Error(e.ResponseWriter, "Failed to render OOB updates", http.StatusInternalServerError)
	}
}

func (e *EventService) Publish(event Event, w http.ResponseWriter, ctx context.Context) {
  eventContext := EventContext{
    Context:        ctx,
    ResponseWriter: w,
  }
  for _, subscriber := range e.subscribers[event.Key()] {
    subscriber(event, eventContext)
  }
}

func (e *EventService) Subscribe(key string, subscriber func(event Event, context EventContext)) {
  e.subscribers[key] = append(e.subscribers[key], subscriber)
}
&lt;/code&gt;
    &lt;p&gt;Then we can publish in the card handler:&lt;/p&gt;
    &lt;code&gt;func (h *Handler) Put(w http.ResponseWriter, r *http.Request) {
  card, err := h.getCardFromRequest(r)
  if err != nil {
    http.Error(w, err.Error(), http.StatusNotFound)
    return
  }
  action := r.FormValue("action")
  switch action {
  case PutActionPromote:
    fromColumn, toColumn, err := h.CardService.Promote(card.ID)
    if err != nil {
      http.Error(w, err.Error(), http.StatusBadRequest)
    } else {
      h.EventService.PublishCardMoved(card.ID, fromColumn.ID, toColumn.ID, w, r.Context())
    }
    break
  }
}
&lt;/code&gt;
    &lt;p&gt;And we can subscribe in the column handler:&lt;/p&gt;
    &lt;code&gt;func (h *Handler) OnCardMoved(event *services.CardMovedEvent, context services.EventContext) {
  column, err := h.CardService.GetColumn(event.ToColumnID)
  if err == nil {
    context.Write(h.RenderComponent(column, true))
  } else {
    http.Error(context.ResponseWriter, err.Error(), http.StatusInternalServerError)
  }
  column, err = h.CardService.GetColumn(event.FromColumnID)
  if err == nil {
    context.Write(h.RenderComponent(column, true))
  } else {
    http.Error(context.ResponseWriter, err.Error(), http.StatusInternalServerError)
  }
}
&lt;/code&gt;
    &lt;p&gt;This way, our components can communicate with each other without needing to know about each other. The subscriber takes the request context from the publisher and simply writes to the response. The result is a response from the back-end with&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;the component-specific update, followed by&lt;/item&gt;
      &lt;item&gt;any other OOB updates simply appended to the response.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This works surprisingly well.&lt;/p&gt;
    &lt;p&gt;Unfortunately, we once again run into the same limitation as before: HTMX will not cross shadow root boundaries. At this point, it seems clear that, if we want to use HTMX as intended, we are going to have to give up on shadow DOM entirely. Again, this is intended behaviour. JavaScript should not cross shadow root boundaries by default. What this means is that, if we're wedded to shadow DOM, we are going to have to fight HTMX all the way.&lt;/p&gt;
    &lt;p&gt;I'm undeterred, of course - all we need is another little hack:&lt;/p&gt;
    &lt;code&gt;function findInShadow(root: any, id: string): any {
  const element = root.getElementById?.(id);
  if (element) {
    return element;
  }
  const allElements = root.querySelectorAll('*');
  for (let element of allElements) {
    if (element.shadowRoot) {
      const found = findInShadow(element.shadowRoot, id);
      if (found) {
        return found;
      }
    }
  }
  return null;
}

function enableOobSwap(evt: CustomEvent&amp;lt;any&amp;gt;) {
  const id = evt.detail.content.id;
  const found = findInShadow(document, id);
  if (found) {
    found.outerHTML = evt.detail.content.outerHTML;
    evt.preventDefault();
  }
}

document.body.addEventListener("htmx:oobErrorNoTarget", enableOobSwap as EventListener);
&lt;/code&gt;
    &lt;p&gt;You'll notice we've done the &lt;code&gt;outerHTML&lt;/code&gt; swap ourselves here, overriding HTMX entirely. I'm not a fan of this. I tried as many approaches as I could think of to get HTMX to do the swap - and, thus, leverage existing functionality HTMX provides for these (maintaining scroll position and focus and so on) - but did not succeed.&lt;/p&gt;
    &lt;p&gt;Nevertheless, this works for my purposes. I'm happy to leave the problem alone for now. Let's add some drag-and-drop functionality to our cards. In our card component:&lt;/p&gt;
    &lt;code&gt;import {MeshElement} from "../base/mesh-element.ts";

export class Card extends MeshElement {
  setupDragAndDrop() {
    this.one('.grip', grip =&amp;gt; {
      grip.draggable = true;
      this.addEventListener('dragstart', this.handleDragStart.bind(this));
      this.addEventListener('dragend', this.handleDragEnd.bind(this));
    });
  }

  handleDragStart(e: any) {
    e.dataTransfer.setData('text/plain', this.dataset.id);
    this.classList.add('dragging');
    e.dataTransfer.effectAllowed = 'move';
  }

  handleDragEnd() {
    this.classList.remove('dragging');
  }
}
&lt;/code&gt;
    &lt;p&gt;And in our column component:&lt;/p&gt;
    &lt;code&gt;import {MeshElement} from "../base/mesh-element.ts";

export class Column extends MeshElement {
  setupDropTarget() {
    this.addEventListener('dragover', this.handleDragOver.bind(this));
    this.addEventListener('drop', this.handleDrop.bind(this));
  }

  handleDragOver(e: any) {
    e.preventDefault();
    e.dataTransfer.dropEffect = 'move';
  }

  handleDrop(e: any) {
    e.preventDefault();
    this.classList.remove('drag-over');

    const cardId = e.dataTransfer.getData('text/plain');
    const columnId = this.dataset.id;
    if (!cardId || !columnId) {
      throw new Error('Missing card or column ID');
    }

    const position = this.calculateDropPosition(e);
    this.moveCard(cardId, +columnId, position);
  }

  async moveCard(cardId: number, columnId: number, position: number) {
    window.htmx.ajax('put', '/card', {
      swap: 'none',
      values: {
        action: 'move',
        cardID: cardId,
        columnID: columnId,
        position: position,
      }
    } as any);
  }
}
&lt;/code&gt;
    &lt;p&gt;This is great! This use case is precisely why HTMX provides the ajax JS API. With a bit of hacking, we've demonstrated that it's possible to use HTMX to handle modular SSR based on the premise of "component = endpoint". I'm pretty happy with how this has turned out.&lt;/p&gt;
    &lt;head rend="h2"&gt;Realtime Collaboration&lt;/head&gt;
    &lt;p&gt;From the moment I conceived of this project, one of the things I wanted to do was to support realtime collaboration with server-sent events (SSE). HTMX supports SSE with a standard plugin which is easy enough to set up:&lt;/p&gt;
    &lt;code&gt;&amp;lt;html lang="en" hx-ext="sse"&amp;gt;
&amp;lt;body hx-ext="sse" sse-connect="/sse" sse-swap="oob-update"&amp;gt;
&lt;/code&gt;
    &lt;p&gt;In theory this should just work once I've written my SSE back-end. I ended up using r3labs/sse for this, which I found very easy to use. We wrap this in a service and provide a "broadcast" method that sends OOB updates to all subscribed clients immediately:&lt;/p&gt;
    &lt;code&gt;func (s *SSEService) BroadcastOOBUpdate(component templ.Component) {
   var buf strings.Builder
   err := component.Render(context.Background(), &amp;amp;buf)
   if err != nil {
       s.log.Error("Failed to render component for SSE broadcast", "error", err)
       return
   }

   html := buf.String()
   s.server.Publish("oob-updates", &amp;amp;sse.Event{
       Event: []byte("oob-update"),
       Data:  html,
   })
}
&lt;/code&gt;
    &lt;p&gt;Then we call it in our handler:&lt;/p&gt;
    &lt;code&gt;func (h *Handler) OnCardMoved(event *services.CardMovedEvent) {
  column, err := h.CardService.GetColumn(event.ToColumnID)
  if err == nil {
    component := h.RenderComponent(column, true)
    h.SSEService.BroadcastOOBUpdate(component)
  } else {
    h.Log.Error("Failed to get to-column for SSE broadcast", "columnID", event.ToColumnID, "error", err)
  }

  column, err = h.CardService.GetColumn(event.FromColumnID)
  if err == nil {
    h.SSEService.BroadcastOOBUpdate(h.RenderComponent(column, true))
  } else {
    h.Log.Error("Failed to get from-column for SSE broadcast", "columnID", event.FromColumnID, "error", err)
  }
}
&lt;/code&gt;
    &lt;p&gt;This makes our back-end code a lot cleaner! We no longer need to pass the request context around with our event, and we no longer need to append a bunch of OOB updates to the response. Having done this both ways, I have come to believe that SSE is the most natural way to do these kind of asynchronous cross-context modular updates, even with only a single user.&lt;/p&gt;
    &lt;p&gt;I was hoping this would also allow me to get rid of my &lt;code&gt;outerHTML&lt;/code&gt; hack, but alas it was not to be. The longer I worked on this project, the more it became clear to me that I'm not really using HTMX the way it's intended to be used. More importantly, there is a lot of other HTMX functionality that I'm not using at all.&lt;/p&gt;
    &lt;p&gt;Naturally, I was intrigued to see if I could just get rid of HTMX entirely. So I did, and the result is a lot cleaner and easier to reason about. We are left with two JS modules - one for the custom elements:&lt;/p&gt;
    &lt;code&gt;export class MeshElement extends HTMLElement {
  connectedCallback() {
    if (!this.shadowRoot) {
      const root = this.attachShadow({ mode: 'open' });
      const template = this.querySelector('template[shadowrootmode="open"]');
      if (template) {
        root.appendChild((template as any).content.cloneNode(true));
      }
    }
    this.bindFormHandlers();
  }

  protected bindFormHandlers() {
    const supported = [
      'get', 'post', 'put', 'patch', 'delete',
    ];

    supported.forEach(verb =&amp;gt; {
      const attribute = "mesh-" + verb;
      this.all('[' + attribute + ']', el =&amp;gt; {
        const form = el as HTMLFormElement;
        form.addEventListener('submit', (event: Event) =&amp;gt; {
          event.preventDefault();
          const method = verb.toUpperCase();
          const url = form.getAttribute(attribute);

          if (!url) {
            console.error('No URL specified for form submission');
            return;
          }

          const formData = new FormData(form);
          this.makeRequest(method, url, formData)
            .then(response =&amp;gt; {
              if (response.ok) {
                return response.text();
              } else {
                throw new Error('Form submission failed: ' + response.statusText);
              }
            })
            .then(html =&amp;gt; this.outerHTML = html)
            .catch(error =&amp;gt; console.error('Form submission failed:', error));
        });
      });
    });
  }

  protected async makeRequest(method: string, url: string, formData: FormData): Promise&amp;lt;Response&amp;gt; {
    const options: RequestInit = {
      method,
      headers: {
        'X-Requested-With': 'XMLHttpRequest',
      },
    };

    if (method === 'GET') {
      const params = new URLSearchParams(formData as any);
      url += (url.includes('?') ? '&amp;amp;' : '?') + params.toString();
    } else {
      options.body = formData;
    }

    return fetch(url, options);
  }
}
&lt;/code&gt;
    &lt;p&gt;and one for SSE:&lt;/p&gt;
    &lt;code&gt;export class SSEManager {
  private eventSource: EventSource | null = null;

  constructor(private url: string = '/sse?stream=oob-updates') {
    this.connect();
  }

  private connect() {
    if (this.eventSource) {
      this.eventSource.close();
    }

    this.eventSource = new EventSource(this.url);

    this.eventSource.addEventListener('oob-update', (event) =&amp;gt; {
      this.processOOBUpdate(event as MessageEvent);
    });

    this.eventSource.onerror = (error) =&amp;gt; {
      console.error('SSE connection error:', error);
      setTimeout(() =&amp;gt; this.connect(), 5000);
    };
  }

  private processOOBUpdate(html: string) {
    const template = document.createElement('template');
    template.innerHTML = html.trim();

    for (const content of template.content.querySelectorAll('[mesh-swap-oob]')) {
      const id = content.id;
      const target = this.findInShadow(document, id);

      if (target) {
        target.outerHTML = content.outerHTML;
      } else {
        console.warn('OOB target not found:', id);
      }
    }
  }

  private findInShadow(root: Document | ShadowRoot | Element, id: string): Element | null {
    let element = root.querySelector(`#${id}`);
    if (element) {
      return element;
    }

    const allElements = root.querySelectorAll('*');
    for (const el of allElements) {
      if (el.shadowRoot) {
        element = this.findInShadow(el.shadowRoot, id);
        if (element) {
          return element;
        }
      }
    }

    return null;
  }
}

new SSEManager();
&lt;/code&gt;
    &lt;p&gt;And that's it! That's all the JS it takes to replace all of HTMX that I'm using for this project.&lt;/p&gt;
    &lt;head rend="h2"&gt;Takeaways&lt;/head&gt;
    &lt;p&gt;This was a fun project. First, let me say, if you're writing apps with jQuery, please check out HTMX! It's very dev-friendly and a proper 2020s way of doing that kind of dev. Personally, however, I am happy to have convinced myself it's not for me.&lt;/p&gt;
    &lt;p&gt;I, for one, don't believe the HTMX spec, or something like it, will be merged back into HTML, at least until it can answer one fundamental question: what is the default swap behaviour? When I declare a &lt;code&gt;form&lt;/code&gt; with a &lt;code&gt;method&lt;/code&gt;, I understand how that form will behave: it will reload the entire page. What happens when I declare a &lt;code&gt;form&lt;/code&gt; with &lt;code&gt;hx-post&lt;/code&gt; or equivalent? The default behaviour in HTMX is that the &lt;code&gt;innerHTML&lt;/code&gt; of the form itself becomes the swap target. This does not seem like a sane default to me.&lt;/p&gt;
    &lt;p&gt;So what's the answer? Well, as anyone who's aware of the state of the art on SSR will have noticed, all I've actually done with MESH is reinvent HotWire, LiveWire, LiveView and friends. Personally, I find this encouraging! It is clear to me that there is a kind of best practice to be found here.&lt;/p&gt;
    &lt;p&gt;I believe the default swap behaviour should be: always swap the whole component. One component, one endpoint. This is how these frameworks do it. My problem with them is they lock you into a specific back-end. I believe the principle is generalisable, that there is a way to do this kind of modular SSR in a back-end-agnostic way, like HTMX does. MESH is my attempt to show what that would look like.&lt;/p&gt;
    &lt;p&gt;I will certainly keep using MESH for my projects, fleshing it out as I go. The Trello clone will always be online for anyone to play with.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45345950</guid><pubDate>Tue, 23 Sep 2025 12:18:01 +0000</pubDate></item><item><title>Getting More Strategic</title><link>https://cate.blog/2025/09/23/getting-more-strategic/</link><description>&lt;doc fingerprint="66e3193529eb0183"&gt;
  &lt;main&gt;
    &lt;p&gt;Strategy – how to be strategic, and how to be seen as strategic – is one of my ongoing obsessions. Years ago, I read Good Strategy/Bad Strategy, and it’s guided my thinking ever since.&lt;/p&gt;
    &lt;p&gt;One of the things that book helps clarify is that being strategic and being seen as strategic can work against each other – good strategy is obvious, and usually it is executed on more than it’s talked about. An ongoing frustration for other under indexed people in tech I talk to, as we build products and organizations without drama, whilst being told we’re just “not strategic” enough. The strategy required to sidestep problems that never happen or that creates optionality to quickly resolve is somehow invisible.&lt;/p&gt;
    &lt;p&gt;But I think as we rise up the org chart, strategy is the job. Strategy defines your job, and evolves it to meet the organizational need. Not just one strategy, but multiple strategies that need to fit together and be coherent.&lt;/p&gt;
    &lt;p&gt;Your product strategy. Your technical strategy. Your team strategy. Your you-as-a-leader-but-also-a-human-being strategy.&lt;/p&gt;
    &lt;p&gt;As we find our groove in the resource constrained era we are in currently as opposed to the everything strategy of ZIRP (zero interest rates), by definition we need to make more harder choices, and strategy is how we know what those choices are, and when and how to make them.&lt;/p&gt;
    &lt;p&gt;This is the first rule of strategy: strategy is contextual. A crucial insight, because often when leaders fail, it’s because they tried to apply a strategy that worked in one context, to a different one, without considering the difference.&lt;/p&gt;
    &lt;p&gt;This is true when you change companies, and I think the reason why there is such a high failure rate for executive hires*. Ones I’ve watched fail came in with a playbook, usually including the org chart they wanted, and expended all the goodwill and capital in pursuit of that goal, whilst achieving very little.&lt;/p&gt;
    &lt;p&gt;It’s also the case that when the market changes, our strategy must change. One of the core features of ZIRP-era engineering leadership was hiring for the sake of it, and number of people as a proxy for many things it maybe (probably) shouldn’t have been. One of the biggest shifts has been the layoffs and the mantra of “doing more with less”. Regardless of personal feelings on this topic and what is actually realistic, it is apparent that hard choices and discipline are a key feature of the post-ZIRP era.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;We could talk about these strategies – product, technical, team, you, like some balanced stool. But realistically, I think it’s more like the image above. The product strategy is a storm (especially pre-product market fit). The technical strategy is a half built shelter (you’ll get to it properly once you have product market fit). The team strategy is an umbrella (the most flexible and controllable). And the you as a human strategy is nowhere to be found.&lt;/p&gt;
    &lt;p&gt;This is the second rule of strategy: timeframe varies with the level of uncertainty you’re navigating.&lt;/p&gt;
    &lt;p&gt;The idea of a proximate objective is the next logical step in pursuit of your overall strategy, if you achieve it, you confirm your course. If you fail, you learn and reconsider your options.&lt;/p&gt;
    &lt;p&gt;We often talk about strategy like it’s defining the end state, setting and describing the destination. But strategy is about defining the incremental steps – the proximate objectives – that can take us towards that end state. Strategy is understanding where we are at – context – and the path from there to where we need to go**. Any strategic “plan”, is best executed as a set of proximate objectives.&lt;/p&gt;
    &lt;p&gt;This mistake of how strategy is talked about is why it can be so hard for some people to be seen as strategic. When we think strategy is depicting the end state, and undervalue the proximate objective definitions and execution that it takes to get there, the person who talks more about the end state can be seen as more strategic than the person who actually reaches it.&lt;/p&gt;
    &lt;p&gt;We need four things for strategy:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Time – energy – to think deeply about it&lt;/item&gt;
      &lt;item&gt;Context to situate it&lt;/item&gt;
      &lt;item&gt;Direction to identify proximate objectives&lt;/item&gt;
      &lt;item&gt;Expertise to chart the path&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All of these need to come together to create and deliver an effective strategy. It’s a balance between all of them, leaning into different ones at different times.&lt;/p&gt;
    &lt;p&gt;To illustrate, why each of these are important, I think it’s helpful to consider the extremes of each.&lt;/p&gt;
    &lt;p&gt;When someone is all time, we call them a political operator. This is the person who manages up to get credit, but the people underneath them ask what it is that they do.&lt;/p&gt;
    &lt;p&gt;When someone is all context, we say they can’t see the forest for the trees. They miss the big picture fixating on the details.&lt;/p&gt;
    &lt;p&gt;When someone is all proximate objectives, we call them a thought leader and it’s not a compliment. Execution is an exercise left to the reader.&lt;/p&gt;
    &lt;p&gt;When someone is all expertise, they present solutions in search of problems. They don’t seem to understand impact.&lt;/p&gt;
    &lt;p&gt;Devaluing these things gives us a reason not to do them. So many engineers will tell you they hate politics, and yes, there is definitely toxic workplace politics. But there’s a baseline where politics is getting things done. It’s convincing people that the idea is good, and that it can be executed. My favourite explanation of this is Nik Means talking about Eiffel’s tower.&lt;/p&gt;
    &lt;p&gt;Context is important. Yes, you’re delivering something bigger, but the details need to add up. You can’t gloss over all of them, you need to learn how to distinguish which are important and which are not.&lt;/p&gt;
    &lt;p&gt;Proximate objectives chart your path. They explain the steps you expect to take between where you are and where you plan to be. Explaining them helps bring people along with you.&lt;/p&gt;
    &lt;p&gt;Expertise is ultimately how you deliver things, you need to understand how to deliver and how to validate. Execution is when the strategy becomes real.&lt;/p&gt;
    &lt;p&gt;Strategy is hard, and being seen as strategic – especially for under-indexed people – can be even harder. We need all of these four things to develop our strategy and move things forward. And we need to be recognized as doing all of them in order to be seen as strategic.&lt;/p&gt;
    &lt;p&gt;Coming back to our problems of strategy – the product, technical, team, and you.&lt;/p&gt;
    &lt;p&gt;Product strategy drives your proximate objectives. Whilst product strategy may seem like the job of product management – and to a certain extent it is, but hopefully your product team does not operate in a vacuum. Engineering needs to provide input, but engineering also needs to understand the product strategy, because everything else needs to fit in with it.&lt;/p&gt;
    &lt;p&gt;Your team exists for a purpose, and the clearest part of that purpose is delivery of the product strategy. You need direction and alignment to identify proximate objectives. Direction – where the product strategy is going, alignment on what is most important, and what will be delivered when.&lt;/p&gt;
    &lt;p&gt;Technical strategy evolves the context. Your technical strategy is often about surfacing the underlying work that allows you to deliver on the business need. It has to be well justified, because ideally it’s pro-active rather than reactive – i.e. you implement it before the emergency rather than during it.&lt;/p&gt;
    &lt;p&gt;Any technical strategy needs to start with what problem is being solved. A problem is not the absence of a technology – unless, I understand, that technology is AI – but rather the problems that technology would solve. So “we don’t have containers” is not a problem. Number of incidents or environment inconsistencies is. Good technical strategy changes the context over time – making more possible – like building roads on the territory you’ve chartered.&lt;/p&gt;
    &lt;p&gt;Your team strategy must be grounded in execution. The product and technical strategy define the organizational need. Your team strategy is about how your team is going to meet that organizational need, within the constraints of the business.&lt;/p&gt;
    &lt;p&gt;Post-ZIRP, this has been a big challenge. Doing more with less means having fewer people, less flexibility, less margin of error. You need to figure out how you retain key people when money is tighter and promotions are harder to come by. But amidst all of these challenges, you have to execute. If in a ZIRP era, you could build the team then deliver, now you must deliver as you build the team.&lt;/p&gt;
    &lt;p&gt;The you as a person strategy requires that you carve out time to be strategic. In this market, many of us are doing-doing-doing to prove that we’re worth keeping around, but at some point, your job is no longer what is being done this week, and more about what is possible next quarter (and the quarters after that). It’s never been easier to be DDOS’d by the job and think that means we’re doing a good one, but you could be missing key things if you’re too focused on the day to day, or week to week and not enough on the month to month.&lt;/p&gt;
    &lt;p&gt;To wrap up, strategy is about more than just a vision; it’s about navigating the path to get there. We need to balance time, context, direction, and expertise to ensure we’re not only seen as strategic but are genuinely creating a strategic path forward for the teams we’re responsible for – and our own evolving needs to competently lead them.&lt;/p&gt;
    &lt;p&gt;* I can’t find a great source here, although the search results suggest it’s commonly accepted #. # possibly, which links out to a site requiring login.&lt;/p&gt;
    &lt;p&gt;** I love Tanya Reilly’s description of the map in The Staff Engineer’s Path.&lt;/p&gt;
    &lt;p&gt;Image credit: Joe Groove&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45346219</guid><pubDate>Tue, 23 Sep 2025 12:41:36 +0000</pubDate></item><item><title>Zig feels more practical than Rust for real-world CLI tools</title><link>https://dayvster.com/blog/why-zig-feels-more-practical-than-rust-for-real-world-cli-tools/</link><description>&lt;doc fingerprint="9fcbdc9bb863a24c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why Zig Feels More Practical Than Rust for Real-World CLI Tools&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;So when it comes to memory management there are two terms you really need to know, the stack and the heap.&lt;/p&gt;
    &lt;p&gt;The stack is a region of memory that stores temporary data that is only needed for a short period of time. It operates in a last-in, first-out (LIFO) manner, meaning that the most recently added data is the first to be removed, as the name suggests. Basically imagine a stack of plates, if you wanna remove one plate you remove the top one, remove the middle plate and disaster awaits in this analogy. The stack is typically used for storing function parameters, local variables, and return addresses. It is fast and efficient because it has a fixed size and does not require dynamic memory allocation.&lt;/p&gt;
    &lt;p&gt;The size of the stack is usually limited, and if a program tries to use more stack space than is available, it can result in a stack overflow error. This can happen if a function calls itself recursively too many times or if a program allocates too much memory on the stack.&lt;/p&gt;
    &lt;p&gt;Whereas the heap as the name suggests is a region of memory that is used for dynamic memory allocation. Unlike the stack, the heap does not have a fixed size and can grow or shrink as needed. The heap is typically used for storing data that needs to persist beyond the lifetime of a single function call, such as objects or data structures that are created at runtime. Imagine the heap as a pile of clothes in a disorganized household, you can add or remove clothes as needed and as long as the pile isn’t too big you can find what you need with relative speed and ease. But it will quickly become a nightmare if you let it grow out of control. The heap is managed by the operating system and requires dynamic memory allocation, which can be slower and less efficient than stack allocation.&lt;/p&gt;
    &lt;p&gt;The heap can also become fragmented over time, since we do not always store data in a contiguous block of memory. This can lead to performance issues and make it more difficult to allocate large blocks of memory.&lt;/p&gt;
    &lt;head rend="h3"&gt;Rust’s Borrow Checker&lt;/head&gt;
    &lt;p&gt;Rust’s borrow checker is a a pretty powerful tool that helps ensure memory safety during compile time. It enforces a set of rules that govern how references to data can be used, preventing common programming memory safety errors such as null pointer dereferencing, dangling pointers and so on. However you may have notice the word compile time in the previous sentence. Now if you got any experience at systems programming you will know that compile time and runtime are two very different things. Basically compile time is when your code is being translated into machine code that the computer can understand, while runtime is when the program is actually running and executing its instructions. The borrow checker operates during compile time, which means that it can only catch memory safety issues that can be determined statically, before the program is actually run.&lt;/p&gt;
    &lt;p&gt;This means that basically the borrow checker can only catch issues at comptime but it will not fix the underlying issue that is developers misunderstanding memory lifetimes or overcomplicated ownership. The compiler can only enforce the rules you’re trying to follow; it can’t teach you good patterns, and it won’t save you from bad design choices.&lt;/p&gt;
    &lt;head rend="h3"&gt;Story Time&lt;/head&gt;
    &lt;p&gt;Last weekend I’ve made a simple CLI tool for myself to help me manage my notes it parses &lt;code&gt;~/.notes&lt;/code&gt; into a list of notes, then builds a tag index mapping strings to references into that list. Straightforward, right? Not in Rust. The borrow checker blocks you the moment you try to add a new note while also holding references to the existing ones. Mutability and borrowing collide, lifetimes show up, and suddenly you’re restructuring your code around the compiler instead of the actual problem.&lt;/p&gt;
    &lt;p&gt;In Zig, we would just allocate the list with an allocator, store pointers into it for the tag index, and mutate freely when we need to add or remove notes. No lifetimes, no extra wrappers, no compiler gymnastics, that’s a lot more straightforward.&lt;/p&gt;
    &lt;head rend="h3"&gt;But Dave isn’t that the exact point of Rust’s borrow checker?&lt;/head&gt;
    &lt;p&gt;Yes it is, however by using Zig I managed to get most of the benefits of Rust’s memory safety without the complexity or ceremony of the borrow checker. All it took was some basic understanding of memory management and a bit of discipline. I was able to produce two CLI’s that are both memory safe and efficient however the Zig one was way more straightforward and easier to reason about and took less time to write.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is Safety, Really for CLI Tools?&lt;/head&gt;
    &lt;p&gt;This is where a lot of developers trip up, Rust markets itself as a language that produces safe software, great marketing hook, but one tiny problem, memory safety is one puzzle piece of overall software safety. I’m not sure if the Rust foundation does this on purpose sort of a blanket statement to make it seem like memory safety is the end all be all of software safety, or if they just don’t want to constantly prefix safety with memory safety(even though they should).&lt;/p&gt;
    &lt;p&gt;But back to the main point, memory safety is just one aspect of software safety. You can argue if it’s a big or small piece of the puzzle, I’d say it depends on the software and use-case but it’s definitely not the only piece.&lt;/p&gt;
    &lt;p&gt;So What exactly is safety in terms of CLI tools?&lt;/p&gt;
    &lt;p&gt;Memory safety alone does not make a program safe. Your CLI tool can still crash, produce wrong results, corrupt files, leak sensitive data, be vulnerable to various types of attacks or just behave in a way that is not expected. Let’s go back to my &lt;code&gt;Notes CLI&lt;/code&gt; it’s rust version may never segfault but it could silently overwrite my index or tags or corrupt my files if I make a mistake in my logic, or perhaps it could store my file in a temporary location that is world readable, exposing my notes to anyone on the system. Is that safe? No.&lt;/p&gt;
    &lt;p&gt;Would using Zig solve any of those issues automatically, also no. Is my example a bit contrived, yes, but it illustrates the point that memory safety is not the only thing that matters when it comes to software safety.&lt;/p&gt;
    &lt;p&gt;In fact you should also consider other aspects of safety such as:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Predictable Behavior: The program should do what the user expects, even when input is malformed or unexpected. A CLI that panics on a missing file or fails silently on a corrupted note is not safe.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Avoiding Crashes or Silent Corruption: The program should handle errors gracefully, providing meaningful feedback to the user instead of crashing or corrupting data. A CLI that crashes on a malformed note or silently overwrites existing notes is not safe.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Manageable Performance: The program should perform well under expected workloads, avoiding excessive resource consumption or slowdowns. A CLI that becomes unresponsive when managing a large number of notes is not safe. This is where it really helps to understand memory allocations and performance characteristics of your language of choice.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sensitive Data Handling: The program should protect sensitive data from unauthorized access or exposure. A CLI that stores notes in a world-readable temporary file is not safe.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Robustness Against Attacks: The program should be resilient against common attack vectors, such as injection attacks or buffer overflows. A CLI that can be exploited to execute arbitrary code or corrupt data is not safe. And this is precisely where Rust’s memory safety shines, it can help prevent certain types of vulnerabilities that arise from memory mismanagement. However, it’s not a silver bullet that guarantees overall safety.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The Borrow Checker: Strengths and Limitations&lt;/head&gt;
    &lt;p&gt;The borrow checker is impressive. It prevents dangling references, double frees, and mutable aliasing at compile time, things that would otherwise cause segfaults or undefined behavior. It’s why Rust can claim “memory safe without a garbage collector.”&lt;/p&gt;
    &lt;head rend="h3"&gt;Strengths:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Zero data races / mutable aliasing issues: The compiler guarantees that only one mutable reference exists at a time, and that immutable references cannot be combined with mutable ones.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Strong compile-time guarantees: Many memory-related bugs are caught before you even run the program.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Early bug detection: You find mistakes before shipping code, which is a huge win in long-lived services or concurrent systems.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Limitations / Pain Points:&lt;/head&gt;
    &lt;p&gt;Cognitive overhead: You’re constantly thinking about lifetimes, ownership, and borrow scopes, even for simple tasks. A small CLI like my notes tool suddenly feels like juggling hot potatoes.&lt;/p&gt;
    &lt;p&gt;Boilerplate and contortions: You end up introducing clones, wrappers (Rc, RefCell), or redesigning data structures just to satisfy the compiler. Your code starts serving the compiler, not the problem.&lt;/p&gt;
    &lt;p&gt;Compile-time only: The borrow checker cannot fix logic bugs, prevent silent corruption, or make your CLI behave predictably. It only ensures memory rules are followed.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Edge cases get messy: Shared caches, global state, or mutable indexes often trigger lifetime errors that are annoying to work around.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At this point, the Rust borrow checker can feel more like a mental tax than a helpful tool, especially for short-lived CLI projects. You’re trading developer ergonomics for a compile-time guarantee that, in many CLI scenarios, may be overkill.&lt;/p&gt;
    &lt;head rend="h2"&gt;Zig’s Approach to Safety and Simplicity&lt;/head&gt;
    &lt;p&gt;Zig takes a different approach to safety and simplicity. It provides manual memory management with optional safety checks, allowing developers to choose the level of control they need. This can lead to more straightforward code for certain use cases, like CLI tools. However where it really shines is how it does manual memory management, I’ve briefly touched upon this in my other blog post Zig Allocators Explained.&lt;/p&gt;
    &lt;p&gt;But basically long story short Zig gives you allocators, a set of tools that helps you manually manage your memory in a more structured and predictable way. You can choose to use a general purpose allocator like the &lt;code&gt;std.heap.GeneralPurposeAllocator&lt;/code&gt; or you can create your own custom allocator that fits your specific needs. This allows you to have more control over how memory is allocated and deallocated, which can lead to more efficient and predictable memory usage. This combined with Zig’s &lt;code&gt;defer&lt;/code&gt; statement which allows you to schedule cleanup code to run when a scope is exited, makes it easy to manage resources gives you most of the power of Rust’s borrow checker at your disposal without the complexity and ritual. However it asks one thing in return of you, discipline, your software will be only as safe as you make it. We can make the same claim about Rust, you can throw &lt;code&gt;copy&lt;/code&gt; and &lt;code&gt;clone&lt;/code&gt; and &lt;code&gt;unsafe&lt;/code&gt; around your code and throw away all the benefits of the borrow checker in a heartbeat.&lt;/p&gt;
    &lt;p&gt;The two languages are polar opposites in this regard, Zig places the burden on the developer and makes it easy for them to produce memory safe software, whereas Rust places the burden on the compiler and makes it hard for developers to produce memory unsafe software.&lt;/p&gt;
    &lt;p&gt;Back to the main point, zig’s approach to memory management is in my subjective opinion more practical for most of my use cases, especially for CLI tools. It allows me to write straightforward code that is easy to reason about and maintain, without the overhead of the borrow checker. I can allocate a list of notes, store pointers to them in a tag index, and mutate the list freely when I need to add or remove notes. No lifetimes, no extra wrappers, no compiler gymnastics, that’s a lot more straightforward.&lt;/p&gt;
    &lt;p&gt;Oh I almost forgot, Zig also has the &lt;code&gt;comptime&lt;/code&gt; feature which allows you to execute code at compile time. This can be useful for generating code, performing static analysis, or optimizing performance and even for testing which is a really nice bonus and can be a small helper when it comes to memory safety.&lt;/p&gt;
    &lt;head rend="h2"&gt;Developer Ergonomics Matter and Developers are not Idiots&lt;/head&gt;
    &lt;p&gt;When developing software we want to be productive and efficient, most of all we want to be correct and produce good software, however we also want to enjoy the process of creation and not feel like we are fighting the tools we use. Developer ergonomics is a term that refers to how easy and comfortable it is to use a programming language or framework. It encompasses things like syntax, tooling, documentation, and community support. A language with good developer ergonomics can make it easier to write correct code, while a language with poor developer ergonomics can make it harder to do so. I’d say as it currently stands Rust has poor developer ergonomics but produces memory safe software, whereas Zig has good developer ergonomics and allows me to produce memory safe software with a bit of discipline.&lt;/p&gt;
    &lt;p&gt;I personally usually prefer languages where I do not have to succumb to too much ceremony and ritual to get things done, I want to be able to express my ideas in code without having to constantly think about the underlying mechanics of the language and yet I want to be responsible and produce good software. So with C and C++ this was a tiny bit harder as you basically had to learn some useful and practical memory management patterns and techniques, Zig comes with them baked in.&lt;/p&gt;
    &lt;p&gt;I feel like Zig really respects it’s developers and treats them like adults, it gives you the tools and expects you to use them wisely. Rust on the other hand feels like it treats developers like children that need to be constantly supervised and guided, which can be frustrating and demotivating.&lt;/p&gt;
    &lt;p&gt;Developers are not idiots, sure even the smartest amongst us still produce memory safety issues or bugs in their software and it’s silly to assume that with enough training and practice we can become perfect developers, but we can become better developers. We can learn from our mistakes and improve our skills, we can learn to write better code and produce better software.&lt;/p&gt;
    &lt;p&gt;It’s not good to abstract that away to the compiler and assume that it will magically make us better developers, I don’t personally think it will. In fact not to sound too cliche but I think that the journey to becoming a better developer is a series of mistakes and fixes, we learn from our mistakes and improve our skills. What does it say about a language that tries to abstract away the mistakes we make, does it really help us become better developers ?&lt;/p&gt;
    &lt;head rend="h2"&gt;Final Thoughts&lt;/head&gt;
    &lt;p&gt;Rust is amazing, if you’re building something massive, multithreaded, or long-lived, where compile-time guarantees actually save your life. The borrow checker, lifetimes, and ownership rules are a boon in large systems.&lt;/p&gt;
    &lt;p&gt;But for small, practical CLI tools? Rust can feel like overkill. That’s where Zig shines. Lightweight, fast, and straightforward, you get memory safety without constantly bending over backward for the compiler. You can allocate a list, track pointers, and mutate freely without extra wrappers, lifetimes, or contortions. Iterating feels natural, the code is easier to reason about, and you get stuff done faster.&lt;/p&gt;
    &lt;p&gt;Memory safety is important, but it’s just one piece of the puzzle. Predictable behavior, maintainable code, and robustness are just as critical, and that’s exactly why Zig often feels more practical for real-world CLI tools.&lt;/p&gt;
    &lt;p&gt;At the end of the day, it’s not about which language is “better.” It’s about what fits your workflow and the kinds of projects you build. For me, Zig hits the sweet spot: memory safe, low ceremony, and developer-friendly, perfect for small tools that actually get things done.&lt;/p&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Stack and the Heap&lt;/item&gt;
      &lt;item&gt;Zig Allocators Explained&lt;/item&gt;
      &lt;item&gt;Rustonomicon - The Dark Arts of Unsafe Rust&lt;/item&gt;
      &lt;item&gt;Zig Documentation - Memory Management&lt;/item&gt;
      &lt;item&gt;Rust Documentation - The Rust Programming Language&lt;/item&gt;
      &lt;item&gt;Zig Documentation - Comptime&lt;/item&gt;
      &lt;item&gt;Rust Documentation - Ownership&lt;/item&gt;
      &lt;item&gt;Zig Documentation - Defer&lt;/item&gt;
      &lt;item&gt;Rust Documentation - Error Handling&lt;/item&gt;
      &lt;item&gt;Zig Documentation - Error Handling&lt;/item&gt;
      &lt;item&gt;Rust Documentation - Concurrency&lt;/item&gt;
      &lt;item&gt;Zig Documentation - Concurrency&lt;/item&gt;
      &lt;item&gt;Rust Documentation - Testing&lt;/item&gt;
      &lt;item&gt;Zig Documentation - Testing&lt;/item&gt;
      &lt;item&gt;Rust Documentation - Performance&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45346387</guid><pubDate>Tue, 23 Sep 2025 12:56:00 +0000</pubDate></item><item><title>Restrictions on house sharing by unrelated roommates</title><link>https://marginalrevolution.com/marginalrevolution/2025/08/the-war-on-roommates-why-is-sharing-a-house-illegal.html</link><description>&lt;doc fingerprint="28b9c150fec148ac"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The War on Roommates: Why Is Sharing a House Illegal?&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Boarding houses were made illegal by zoning that enforced single family homes and by rules limiting occupancy, demanding every room have a private bathroom, outlawing shared kitchens, requiring parking spaces for every resident etc.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;How States and Cities Decimated Americans’ Lowest-Cost Housing Option is an excellent, hard-hitting piece making and extending these points and significantly it’s not from a libertarian think tank but Pew:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Low-cost micro-units, often called single-room occupancies, or SROs, were once a reliable form of housing for the United States’ poorest residents of, and newcomers to, New York, Chicago, San Francisco, and many other major U.S. cities. Well into the 20th century, SROs were the least expensive option on the housing market, providing a small room with a shared bathroom and sometimes a shared kitchen for a price that is unimaginable today—as little as $100 to $300 a month (in 2025 dollars).&lt;/p&gt;
      &lt;p&gt;In the late 19th and early 20th centuries, landlords converted thousands of houses, hotels, apartment buildings, and commercial buildings into SROs, and by 1950, SRO units made up about 10% of all rental units in some major cities. But beginning in the mid-1950s, as some politicians and vocal members of the public turned against SROs and the people who lived in them, major cities across the country revised zoning and building codes to force or encourage landlords to eliminate SRO units and to prohibit the development of new ones. Over the next several decades, governments and developers gradually demolished thousands of SROs or converted them to other uses, including boutique hotels for tourists. And as SROs disappeared, homelessness—which had been rare from at least the end of the Great Depression to the late 1970s—exploded nationwide.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The Pew piece does an excellent job of documenting how laws are beginning to change. I especially appreciated this point: the simplest reform is to stop making it illegal for unrelated people to share a home!&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Perhaps the simplest method of creating low-cost shared housing is to allow unrelated individuals to share a house in the same way that relatives are allowed to share a house.67 But many communities limit the number of unrelated people who can live together—in some places, to as few as two. Such laws make sharing a house for a group of roommates—which usually enables rents lower than having an individual apartment—illegal. The U.S. has a record number of unused bedrooms, but many cannot be rented because of restrictions on house sharing by unrelated roommates, even if that would be the most profitable use for the landlord and the most affordable option for the tenants.68 To enable this low-cost housing option, Iowa, Oregon, and Colorado all passed bipartisan legislation to strike down local codes that prohibit house-sharing (in 2017, 2021, and 2024, respectively).69&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;So many of our problems are created by busybodies and do-gooders who prevent people from using their own property.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45347043</guid><pubDate>Tue, 23 Sep 2025 13:51:56 +0000</pubDate></item><item><title>OpenDataLoader-PDF: An open source tool for structured PDF parsing</title><link>https://github.com/opendataloader-project/opendataloader-pdf</link><description>&lt;doc fingerprint="3390da1012851aca"&gt;
  &lt;main&gt;
    &lt;p&gt;Safe, Open, High-Performance — PDF for AI&lt;/p&gt;
    &lt;p&gt;OpenDataLoader-PDF converts PDFs into JSON, Markdown or Html — ready to feed into modern AI stacks (LLMs, vector search, and RAG).&lt;/p&gt;
    &lt;p&gt;It reconstructs document layout (headings, lists, tables, and reading order) so the content is easier to chunk, index, and query. Powered by fast, heuristic, rule-based inference, it runs entirely on your local machine and delivers high-throughput processing for large document sets. AI-safety is enabled by default and automatically filters likely prompt-injection content embedded in PDFs to reduce downstream risk.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🧾 Rich, Structured Output — JSON, Markdown or Html&lt;/item&gt;
      &lt;item&gt;🧩 Layout Reconstruction — Headings, Lists, Tables, Images, Reading Order&lt;/item&gt;
      &lt;item&gt;⚡ Fast &amp;amp; Lightweight — Rule-Based Heuristic, High-Throughput, No GPU&lt;/item&gt;
      &lt;item&gt;🔒 Local-First Privacy — Runs fully on your machine&lt;/item&gt;
      &lt;item&gt;🛡️ AI-Safety — Auto-Filters likely prompt-injection content - Learn more about AI-Safety&lt;/item&gt;
      &lt;item&gt;🖍️ Annotated PDF Visualization — See detected structures overlaid on the original&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🖨️ OCR for scanned PDFs — Extract data from image-only pages&lt;/item&gt;
      &lt;item&gt;🧠 Table AI option — Higher accuracy for tables with borderless or merged cells&lt;/item&gt;
      &lt;item&gt;⚡ Performance Benchmarks — Transparent evaluations with open datasets and metrics, reported regularly&lt;/item&gt;
      &lt;item&gt;🛡️ AI Red Teaming — Transparent adversarial benchmarks with datasets and metrics, reported regularly&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Java 11 or higher must be installed and available in your system's PATH.&lt;/item&gt;
      &lt;item&gt;Python 3.9+&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install -U opendataloader-pdf&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;input_path can be either the path to a single document or the path to a folder.&lt;/item&gt;
      &lt;item&gt;If you don’t specify an output_folder, the output data will be saved in the same directory as the input document.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;import opendataloader_pdf

opendataloader_pdf.run(
    input_path="path/to/document.pdf",
    output_folder="path/to/output",
    generate_markdown=True,
    generate_html=True,
    generate_annotated_pdf=True,
)&lt;/code&gt;
    &lt;p&gt;The main function to process PDFs.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Parameter&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Required&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;input_path&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;str&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
        &lt;cell&gt;Path to the input PDF file or folder.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;output_folder&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;str&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;input folder&lt;/cell&gt;
        &lt;cell&gt;Path to the output folder.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;password&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;str&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;None&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Password for the PDF file.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;replace_invalid_chars&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;str&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;" "&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Character to replace invalid or unrecognized characters (e.g., �, \u0000)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;content_safety_off&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;str&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;None&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Disables one or more content safety filters. Accepts a comma-separated list of filter names. Arguments: all, hidden-text, off-page, tiny, hidden-ocg.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;generate_markdown&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bool&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;False&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;True&lt;/code&gt;, generates a Markdown output file.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;generate_html&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bool&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;False&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;True&lt;/code&gt;, generates an HTML output file.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;generate_annotated_pdf&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bool&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;False&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;True&lt;/code&gt;, generates an annotated PDF output file.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;keep_line_breaks&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bool&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;False&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;True&lt;/code&gt;, keeps line breaks in the output.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;html_in_markdown&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bool&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;False&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;True&lt;/code&gt;, uses HTML in the Markdown output.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;add_image_to_markdown&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bool&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;False&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;True&lt;/code&gt;, adds images to the Markdown output.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;debug&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bool&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;False&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;True&lt;/code&gt;, prints CLI messages to the console during execution.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note: This package is a wrapper around a Java CLI and is intended for use in a Node.js backend environment. It cannot be used in a browser-based frontend.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Java 11 or higher must be installed and available in your system's PATH.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;npm install @opendataloader/pdf&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;inputPath&lt;/code&gt;can be either the path to a single document or the path to a folder.&lt;/item&gt;
      &lt;item&gt;If you don’t specify an &lt;code&gt;outputFolder&lt;/code&gt;, the output data will be saved in the same directory as the input document.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;import { run } from '@opendataloader/pdf';

async function main() {
  try {
    const output = await run('path/to/document.pdf', {
      outputFolder: 'path/to/output',
      generateMarkdown: true,
      generateHtml: true,
      generateAnnotatedPdf: true,
      debug: true,
    });
    console.log('PDF processing complete.', output);
  } catch (error) {
    console.error('Error processing PDF:', error);
  }
}

main();&lt;/code&gt;
    &lt;p&gt;
      &lt;code&gt;run(inputPath: string, options?: RunOptions): Promise&amp;lt;string&amp;gt;&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The main function to process PDFs.&lt;/p&gt;
    &lt;p&gt;Parameters&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Parameter&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Required&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;inputPath&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;string&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Path to the input PDF file or folder.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;options&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;RunOptions&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Configuration options for the run.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;RunOptions&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Property&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;outputFolder&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;string&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;undefined&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Path to the output folder. If not set, output is saved next to the input.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;password&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;string&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;undefined&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Password for the PDF file.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;replaceInvalidChars&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;string&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;" "&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Character to replace invalid or unrecognized characters (e.g., , \u0000).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;contentSafetyOff&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;string&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;undefined&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Disables one or more content safety filters. Accepts a comma-separated list of filter names. Arguments: all, hidden-text, off-page, tiny, hidden-ocg.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;generateMarkdown&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;boolean&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;false&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;true&lt;/code&gt;, generates a Markdown output file.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;generateHtml&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;boolean&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;false&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;true&lt;/code&gt;, generates an HTML output file.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;generateAnnotatedPdf&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;boolean&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;false&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;true&lt;/code&gt;, generates an annotated PDF output file.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;keepLineBreaks&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;boolean&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;false&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;true&lt;/code&gt;, keeps line breaks in the output.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;htmlInMarkdown&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;boolean&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;false&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;true&lt;/code&gt;, uses HTML in the Markdown output.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;addImageToMarkdown&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;boolean&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;false&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;true&lt;/code&gt;, adds images to the Markdown output.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;debug&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;boolean&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;false&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;true&lt;/code&gt;, prints CLI messages to the console during execution.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;For various example templates, including Gradle and Maven, please refer to https://github.com/opendataloader-project/opendataloader-pdf/tree/main/examples/java.&lt;/p&gt;
    &lt;p&gt;To include OpenDataLoader PDF in your Maven project, add the dependency below to your &lt;code&gt;pom.xml&lt;/code&gt; file.&lt;/p&gt;
    &lt;p&gt;Check for the latest version on Maven Central.&lt;/p&gt;
    &lt;code&gt;&amp;lt;project&amp;gt;
    &amp;lt;!-- other configurations... --&amp;gt;

    &amp;lt;dependencies&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.opendataloader&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;opendataloader-pdf-core&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;1.0.0&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;
    &amp;lt;/dependencies&amp;gt;

    &amp;lt;repositories&amp;gt;
        &amp;lt;repository&amp;gt;
            &amp;lt;snapshots&amp;gt;
                &amp;lt;enabled&amp;gt;true&amp;lt;/enabled&amp;gt;
            &amp;lt;/snapshots&amp;gt;
            &amp;lt;id&amp;gt;vera-dev&amp;lt;/id&amp;gt;
            &amp;lt;name&amp;gt;Vera development&amp;lt;/name&amp;gt;
            &amp;lt;url&amp;gt;https://artifactory.openpreservation.org/artifactory/vera-dev&amp;lt;/url&amp;gt;
        &amp;lt;/repository&amp;gt;
    &amp;lt;/repositories&amp;gt;
    &amp;lt;pluginRepositories&amp;gt;
        &amp;lt;pluginRepository&amp;gt;
            &amp;lt;snapshots&amp;gt;
                &amp;lt;enabled&amp;gt;false&amp;lt;/enabled&amp;gt;
            &amp;lt;/snapshots&amp;gt;
            &amp;lt;id&amp;gt;vera-dev&amp;lt;/id&amp;gt;
            &amp;lt;name&amp;gt;Vera development&amp;lt;/name&amp;gt;
            &amp;lt;url&amp;gt;https://artifactory.openpreservation.org/artifactory/vera-dev&amp;lt;/url&amp;gt;
        &amp;lt;/pluginRepository&amp;gt;
    &amp;lt;/pluginRepositories&amp;gt;

    &amp;lt;!-- other configurations... --&amp;gt;
&amp;lt;/project&amp;gt;&lt;/code&gt;
    &lt;p&gt;To integrate Layout recognition API into Java code, one can follow the sample code below.&lt;/p&gt;
    &lt;code&gt;import org.opendataloader.pdf.api.Config;
import org.opendataloader.pdf.api.OpenDataLoaderPDF;

import java.io.IOException;

public class Sample {

    public static void main(String[] args) {
        Config config = new Config();
        config.setOutputFolder("path/to/output");
        config.setGeneratePDF(true);
        config.setGenerateMarkdown(true);
        config.setGenerateHtml(true);

        try {
            OpenDataLoaderPDF.processFile("path/to/document.pdf", config);
        } catch (Exception exception) {
            //exception during processing
        }
    }
}&lt;/code&gt;
    &lt;p&gt;The full API documentation is available at javadoc&lt;/p&gt;
    &lt;p&gt;Download sample PDF&lt;/p&gt;
    &lt;code&gt;curl -L -o 1901.03003.pdf https://arxiv.org/pdf/1901.03003&lt;/code&gt;
    &lt;p&gt;Run opendataloader-pdf in Docker container&lt;/p&gt;
    &lt;code&gt;docker run --rm -v "$PWD":/work ghcr.io/opendataloader-project/opendataloader-pdf-cli:latest /work/1901.03003.pdf --markdown --html --pdf
&lt;/code&gt;
    &lt;p&gt;Build and install using Maven command:&lt;/p&gt;
    &lt;code&gt;mvn clean install -f java/pom.xml&lt;/code&gt;
    &lt;p&gt;If the build is successful, the resulting &lt;code&gt;jar&lt;/code&gt; file will be created in the path below.&lt;/p&gt;
    &lt;code&gt;java/opendataloader-pdf-cli/target&lt;/code&gt;
    &lt;code&gt;java -jar opendataloader-pdf-cli-&amp;lt;VERSION&amp;gt;.jar [options] &amp;lt;INPUT FILE OR FOLDER&amp;gt;&lt;/code&gt;
    &lt;p&gt;This generates a JSON file with layout recognition results in the specified output folder. Additionally, annotated PDF with recognized structures, Markdown and Html are generated if options &lt;code&gt;--pdf&lt;/code&gt;, &lt;code&gt;--markdown&lt;/code&gt; and &lt;code&gt;--html&lt;/code&gt; are specified.&lt;/p&gt;
    &lt;p&gt;By default all line breaks and hyphenation characters are removed, the Markdown does not include any images and does not use any HTML.&lt;/p&gt;
    &lt;p&gt;The option &lt;code&gt;--keep-line-breaks&lt;/code&gt; to preserve the original line breaks text content in JSON and Markdown output.
The option &lt;code&gt;--content-safety-off&lt;/code&gt; disables one or more content safety filters. Accepts a comma-separated list of filter names.
The option &lt;code&gt;--markdown-with-html&lt;/code&gt; enables use of HTML in Markdown, which may improve Markdown preview in processors that support HTML tags.
The option &lt;code&gt;--markdown-with-images&lt;/code&gt; enables inclusion of image references into the output Markdown.
The option &lt;code&gt;--replace-invalid-chars&lt;/code&gt; replaces invalid or unrecognized characters (e.g., �, \u0000) with the specified character.
The images are extracted from PDF as individual files and stored in a subfolder next to the Markdown output.&lt;/p&gt;
    &lt;code&gt;Options:
-o,--output-dir &amp;lt;arg&amp;gt;           Specifies the output directory for generated files
--keep-line-breaks              Preserves original line breaks in the extracted text
--content-safety-off &amp;lt;arg&amp;gt;      Disables one or more content safety filters. Accepts a comma-separated list of filter names. Arguments: all, hidden-text, off-page, tiny, hidden-ocg
--markdown-with-html            Sets the data extraction output format to Markdown with rendering complex elements like tables as HTML for better structure
--markdown-with-images          Sets the data extraction output format to Markdown with extracting images from the PDF and includes them as links
--markdown                      Sets the data extraction output format to Markdown
--html                          Sets the data extraction output format to HTML
-p,--password &amp;lt;arg&amp;gt;             Specifies the password for an encrypted PDF
--pdf                           Generates a new PDF file where the extracted layout data is visualized as annotations
--replace-invalid-chars &amp;lt;arg&amp;gt;   Replaces invalid or unrecognized characters (e.g., �, \u0000) with the specified character
&lt;/code&gt;
    &lt;p&gt;Root json node&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Field&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Optional&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;file name&lt;/cell&gt;
        &lt;cell&gt;string&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Name of processed pdf file&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;number of pages&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Number of pages in pdf file&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;author&lt;/cell&gt;
        &lt;cell&gt;string&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Author of pdf file&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;title&lt;/cell&gt;
        &lt;cell&gt;string&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Title of pdf file&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;creation date&lt;/cell&gt;
        &lt;cell&gt;string&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Creation date of pdf file&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;modification date&lt;/cell&gt;
        &lt;cell&gt;string&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Modification date of pdf file&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;kids&lt;/cell&gt;
        &lt;cell&gt;array&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Array of detected content elements&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Common fields of content json nodes&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Field&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Optional&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;id&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;Unique id of content element&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;level&lt;/cell&gt;
        &lt;cell&gt;string&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;Level of content element&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;type&lt;/cell&gt;
        &lt;cell&gt;string&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Type of content element&lt;p&gt;Possible types:&lt;/p&gt;&lt;code&gt;footer&lt;/code&gt;, &lt;code&gt;header&lt;/code&gt;, &lt;code&gt;heading&lt;/code&gt;, &lt;code&gt;line&lt;/code&gt;, &lt;code&gt;table&lt;/code&gt;, &lt;code&gt;table row&lt;/code&gt;, &lt;code&gt;table cell&lt;/code&gt;, &lt;code&gt;paragraph&lt;/code&gt;, &lt;code&gt;list&lt;/code&gt;, &lt;code&gt;list item&lt;/code&gt;, &lt;code&gt;image&lt;/code&gt;, &lt;code&gt;line art&lt;/code&gt;, &lt;code&gt;caption&lt;/code&gt;, &lt;code&gt;text block&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;page number&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Page number of content element&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;bounding box&lt;/cell&gt;
        &lt;cell&gt;array&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Bounding box of content element&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Specific fields of text content json nodes (&lt;code&gt;caption&lt;/code&gt;, &lt;code&gt;heading&lt;/code&gt;, &lt;code&gt;paragraph&lt;/code&gt;)&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Field&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Optional&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;font&lt;/cell&gt;
        &lt;cell&gt;string&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Font name of text&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;font size&lt;/cell&gt;
        &lt;cell&gt;double&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Font size of text&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;text color&lt;/cell&gt;
        &lt;cell&gt;array&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Color of text&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;content&lt;/cell&gt;
        &lt;cell&gt;string&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Text value&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Specific fields of &lt;code&gt;table&lt;/code&gt; json nodes&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Field&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Optional&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;number of rows&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Number of table rows&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;number of columns&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Number of table columns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;rows&lt;/cell&gt;
        &lt;cell&gt;array&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Array of table rows&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;previous table id&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;Id of previous connected table&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;next table id&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;Id of next connected table&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Specific fields of &lt;code&gt;table row&lt;/code&gt; json nodes&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Field&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Optional&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;row number&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Number of table row&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;cells&lt;/cell&gt;
        &lt;cell&gt;array&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Array of table cells&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Specific fields of &lt;code&gt;table cell&lt;/code&gt; json nodes&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Field&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Optional&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;row number&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Row number of table cell&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;column number&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Column number of table cell&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;row span&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Row span of table cell&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;column span&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Column span of table cell&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;kids&lt;/cell&gt;
        &lt;cell&gt;array&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Array of table cell content elements&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Specific fields of &lt;code&gt;heading&lt;/code&gt; json nodes&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Field&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Optional&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;heading level&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Heading level of heading&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Specific fields of &lt;code&gt;list&lt;/code&gt; json nodes&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Field&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Optional&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;number of list items&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Number of list items&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;numbering style&lt;/cell&gt;
        &lt;cell&gt;string&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Numbering style of this list&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;previous list id&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;Id of previous connected list&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;next list id&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;Id of next connected list&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;list items&lt;/cell&gt;
        &lt;cell&gt;array&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Array of list item content elements&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Specific fields of &lt;code&gt;list item&lt;/code&gt; json nodes&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Field&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Optional&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;kids&lt;/cell&gt;
        &lt;cell&gt;array&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Array of list item content elements&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Specific fields of &lt;code&gt;header&lt;/code&gt; and &lt;code&gt;footer&lt;/code&gt; json nodes&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Field&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Optional&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;kids&lt;/cell&gt;
        &lt;cell&gt;array&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Array of header/footer content elements&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Specific fields of &lt;code&gt;text block&lt;/code&gt; json nodes&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Field&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Optional&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;kids&lt;/cell&gt;
        &lt;cell&gt;array&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Array of text block content elements&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We believe that great software is built together.&lt;/p&gt;
    &lt;p&gt;Your contributions are vital to the success of this project.&lt;/p&gt;
    &lt;p&gt;Please read CONTRIBUTING.md for details on how to contribute.&lt;/p&gt;
    &lt;p&gt;Have questions or need a little help? We're here for you!🤗&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GitHub Discussions: For Q&amp;amp;A and general chats. Let's talk! 🗣️&lt;/item&gt;
      &lt;item&gt;GitHub Issues: Found a bug? 🐛 Please report it here so we can fix it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We love our brand and want to protect it!&lt;/p&gt;
    &lt;p&gt;This project may contain trademarks, logos, or brand names for our products and services.&lt;/p&gt;
    &lt;p&gt;To ensure everyone is on the same page, please remember these simple rules:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Authorized Use: You're welcome to use our logos and trademarks, but you must follow our official brand guidelines.&lt;/item&gt;
      &lt;item&gt;No Confusion: When you use our trademarks in a modified version of this project, it should never cause confusion or imply that Hancom officially sponsors or endorses your version.&lt;/item&gt;
      &lt;item&gt;Third-Party Brands: Any use of trademarks or logos from other companies must follow that company’s specific policies.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project is licensed under the Mozilla Public License 2.0.&lt;/p&gt;
    &lt;p&gt;For the full license text, see LICENSE.&lt;/p&gt;
    &lt;p&gt;For information on third-party libraries and components, see:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45347147</guid><pubDate>Tue, 23 Sep 2025 13:58:48 +0000</pubDate></item><item><title>State of AI-assisted software development</title><link>https://blog.google/technology/developers/dora-report-2025/</link><description>&lt;doc fingerprint="af439e98eb1325cc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How are developers using AI? Inside our 2025 DORA report&lt;/head&gt;
    &lt;p&gt;Today, we're sharing key findings from the 2025 DORA Report, our annual research into the trends shaping modern software development. In our report, State of AI-assisted Software Development, we surveyed nearly 5,000 technology professionals globally, validating what we all suspected: the way software is built has profoundly changed.&lt;/p&gt;
    &lt;p&gt;Google Cloud’s DORA research program has been investigating the capabilities, practices and measures of high-performing technology-driven teams and organizations for more than a decade. And our 2025 research makes it clear that AI is no longer a novelty, but a near-universal part of a developer’s toolkit.&lt;/p&gt;
    &lt;head rend="h2"&gt;Massive adoption meets major productivity gains&lt;/head&gt;
    &lt;p&gt;This year's report reveals a significant finding: AI adoption among software development professionals has surged to 90%, marking a 14% increase from last year. These professionals, from developers to product managers, now integrate AI into their core workflows, typically dedicating a median of two hours daily to working with it.&lt;/p&gt;
    &lt;p&gt;Our findings on the adoption and use of AI by software developers point to a broad adoption of and deep reliance on AI across a range of tasks.&lt;/p&gt;
    &lt;p&gt;A significant majority (65%) of those surveyed are heavily relying on AI for software development, with 37% reporting a "moderate amount" of reliance, 20% "a lot" and 8% "a great deal."&lt;/p&gt;
    &lt;p&gt;This indicates a strong adoption and dependence on AI within the field, and the advantages for users of these tools are evident in this year’s research:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Significant productivity gains: Over 80% of respondents indicate that AI has enhanced their productivity.&lt;/item&gt;
      &lt;item&gt;Improved code quality: A majority (59%) report a positive influence of AI on code quality.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As AI adoption has increased, developers have reported increased productivity and positive impacts on code quality&lt;/p&gt;
    &lt;head rend="h2"&gt;A trust and productivity paradox&lt;/head&gt;
    &lt;p&gt;Despite the widespread adoption and perceived benefits, some software development professionals remain cautious about using AI in their work. Our report uncovers a surprising "trust paradox": While 24% of respondents report a "great deal" (4%) or "a lot" (20%) of trust in AI, 30% trust it "a little" (23%) or "not at all" (7%). This indicates that AI outputs are perceived as useful and valuable by many of this year’s survey respondents, despite a lack of complete trust in them. This could also imply that AI is being incorporated into workflows as a supportive tool to enhance productivity and efficiency, rather than serving as a full substitute for human judgment.&lt;/p&gt;
    &lt;p&gt;While AI is boosting individual performance, its effect on organizations is more complex. This year's research shows that AI adoption is now linked to higher software delivery throughput, meaning teams are releasing more software and applications, which is a positive reversal of last year's findings. However, the ongoing challenge remains of ensuring software works as intended before it's delivered to users.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reflections on how teams work&lt;/head&gt;
    &lt;p&gt;Our research this year also found that AI can act as a "mirror and a multiplier.” In cohesive organizations, AI boosts efficiency. In fragmented ones, it highlights weaknesses.&lt;/p&gt;
    &lt;p&gt;To better understand these underlying conditions, this year's report moves beyond simple performance metrics to reveal seven distinct team archetypes, providing a deeper, more human-centric view of what drives success in AI adoption. These profiles, from "Harmonious high-achievers" to teams caught in a "Legacy bottleneck," offer a richer narrative that can help organizations understand the unique interplay between performance, well-being and workplace environment.&lt;/p&gt;
    &lt;p&gt;For organizations ready to adopt AI, new tools can help them evolve their work processes - meaning they benefit from both the productivity boost and the resulting transformation.&lt;/p&gt;
    &lt;head rend="h2"&gt;A blueprint for guiding AI in organizations&lt;/head&gt;
    &lt;p&gt;Adoption of AI alone isn't enough to guarantee success though. That’s why this year, we’re also introducing a new blueprint of seven essential capabilities for amplifying AI's impact. The DORA AI Capabilities Model is based on extensive research and identifies a blend of technical and cultural factors that are crucial for success.&lt;/p&gt;
    &lt;p&gt;The DORA AI Capabilities Model provides data-backed guidance for organizations adopting AI&lt;/p&gt;
    &lt;p&gt;The key takeaway is clear: AI is a transformative tool for developers, but realizing its full potential requires more than just adoption. It demands that organizations evolve their culture, processes and systems to support a new era of software development.&lt;/p&gt;
    &lt;p&gt;Get deeper insights and additional findings from this year's research when you download the State of AI-assisted Software Development report.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45347197</guid><pubDate>Tue, 23 Sep 2025 14:02:49 +0000</pubDate></item><item><title>x402 — An open protocol for internet-native payments</title><link>https://www.x402.org/</link><description>&lt;doc fingerprint="a538813a0d0efbd5"&gt;
  &lt;main&gt;
    &lt;p&gt;An open protocol for internet-native payments&lt;/p&gt;
    &lt;p&gt;Built around the HTTP 402 status code, x402 enables users to pay for resources via API without registration, emails, OAuth, or complex signatures.&lt;/p&gt;
    &lt;p&gt;x402 as a protocol has 0 fees for either the customer or the merchant.&lt;/p&gt;
    &lt;p&gt;Accept payments at the speed of the blockchain. Money in your wallet in 2 seconds, not T+2.&lt;/p&gt;
    &lt;p&gt;x402 is not tied to any specific blockchain or token, its a neutral standard open to integration by all.&lt;/p&gt;
    &lt;p&gt;As little as 1 line of middleware code or configuration in your existing web server stack and you can start accepting payments. Customers and agents aren't required to create an account or provide any personal information.&lt;/p&gt;
    &lt;p&gt;Anyone can implement or extend x402. It's not tied to any centralized provider, and encourages broad community participation.&lt;/p&gt;
    &lt;p&gt;Activates the dormant 402 HTTP status code and works with any HTTP stack. It works simply via headers and status codes on your existing HTTP server.&lt;/p&gt;
    &lt;p&gt;x402 unlocks new monetization models, offering developers and content creators a frictionless way to earn revenue from small transactions without forcing subscriptions or showing ads.&lt;/p&gt;
    &lt;p&gt;Agents can use the x402 Protocol to pay for API requests in real-time.&lt;/p&gt;
    &lt;p&gt;Using x402, customers can easily access storage services without account creation.&lt;/p&gt;
    &lt;p&gt;x402 unlocks instant transactions, enabling true micropayments for content.&lt;/p&gt;
    &lt;p&gt;Just add a single line of code in your app, and you can require a USDC payment for each incoming request.&lt;/p&gt;
    &lt;quote&gt;paymentMiddleware("0xYourAddress", {"/your-endpoint": "$0.01"}); // and thats it!&lt;/quote&gt;
    &lt;p&gt;If a request arrives without payment, the server responds with HTTP 402, prompting the client to pay and retry.&lt;/p&gt;
    &lt;quote&gt;HTTP/1.1 402 Payment Required&lt;/quote&gt;
    &lt;p&gt;x402 allows any web developer to accept crypto payments without the complexity of having to interact with the blockchain.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45347335</guid><pubDate>Tue, 23 Sep 2025 14:14:46 +0000</pubDate></item><item><title>Agents turn simple keyword search into compelling search experiences</title><link>https://softwaredoug.com/blog/2025/09/22/reasoning-agents-need-bad-search</link><description>&lt;doc fingerprint="3b1fda320b812e0a"&gt;
  &lt;main&gt;
    &lt;p&gt;A traditional RAG system often looks like the search we’ve built for years. An LLM calls a search API. We have query understanding and reranking functionality under the hood. All optimized for user engagement.&lt;/p&gt;
    &lt;p&gt;This thick-daddy search API does quite a bit, as shown below:&lt;/p&gt;
    &lt;p&gt;With small modifications (chunking, a focus on vector retrieval) the RAG implementations look familiar.&lt;/p&gt;
    &lt;p&gt;Agents, however, come with the ability to reason. And like human users, they might try a query, see the results, adjust their query, and zero-in on the best search terms to find what they want.&lt;/p&gt;
    &lt;p&gt;Like human users, the agent can build a “mental model” of how the search tool works. If there’s a clear, predictable correspondence between stimulus and output, the agent will figure it out. However, a large, complex system might not be as transparent.&lt;/p&gt;
    &lt;p&gt;Because of this, I’d argue the traditional, thick search APIs are counterproductive to being used by agents. They may be too complex for agents to reason about effectively.&lt;/p&gt;
    &lt;p&gt;So lately, I’ve tried a rather stupid search backend. More like this image:&lt;/p&gt;
    &lt;p&gt;In the furniture search examples I use in my Cheat at Search course, I tried this out.&lt;/p&gt;
    &lt;p&gt;Instead of a big, complex search system, I tell the agent about this dumb thing it can use to search. It then applies its own intelligence to use the tool correctly.&lt;/p&gt;
    &lt;code&gt;def search_products(query: str, top_k: int = 5) -&amp;gt; List[Dict]:
    """
    Search for furniture products.

    This is direct keyword search, no synonyms, only BM25 scoring on product name 
    and description and basic snowball tokenization of query and document.

    Args:
        query: The search query string.
        top_k: The number of top results to return.

    Returns:
        A list of dictionaries containing product information.
    """
   ...
&lt;/code&gt;
    &lt;p&gt;( the doc string here becomes part of the prompt, see the source code)&lt;/p&gt;
    &lt;p&gt;I slimmed the search down to fairly simple keyword search, and told GPT-5 exactly how it works. It behaves predictably, if stupidly.&lt;/p&gt;
    &lt;p&gt;And that’s a good thing.&lt;/p&gt;
    &lt;p&gt;I’m fairly surprised how well it works at solving user queries. Albeit slowly.&lt;/p&gt;
    &lt;p&gt;Here I am finding couches that would be good for a vampire.&lt;/p&gt;
    &lt;code&gt;User: A couch fit for a vampire

...

Assistant: Here are some dramatically tufted, velvet/chesterfield options with a suitably vampiric vibe:

Chesterfield-style sofas (timeless, tufted, dramatic arms/back)
- Porter 80" Velvet Rolled Arm Chesterfield Sofa (ID 4306)
- Rolled Arm Chesterfield Sofa (ID 28854)
- Quitaque 88" Rolled Arm Chesterfield Sofa (ID 17093)

Velvet sofas with luxe flair
- 85" Velvet Tuxedo Arm Sofa (ID 29613) — velvet with nailhead trim
- Camren 72" Velvet Round Arm Sofa (ID 40049)
- 112" Wide Velvet Sofa &amp;amp; Chaise (ID 27546)

Chaise lounges (fainting-couch energy)
- Rosemead Velvet Chaise Lounge (ID 40626)
- Avondale Velvet Chaise Lounge (ID 6452)
- Brighouse Chaise Lounge (ID 6392)
- Tilstone Chaise Lounge (ID 18643) — velvet with silver nailhead accents
&lt;/code&gt;
    &lt;p&gt;Or asking for the ugliest chair&lt;/p&gt;
    &lt;code&gt;User: ugliest chair in the catalog

Assistant: I ran several targeted searches to surface the most over-the-top
chairs. Here are the top contenders for “ugliest chair” in our catalog:

- Zebra Arm Chair (ID 18587) — bold zebra upholstery, statement piece energy
- Toby Cow Print Task Chair (ID 5002) — desk chair in full cow print
- Gaudy 47" Wide Chenille Armchair (ID 8921) — it literally says “Gaudy” in the name
- The Abbey Gothic Revival 19" Wide Side Chair (ID 36608) — ornate, carved gothic look
&lt;/code&gt;
    &lt;head rend="h2"&gt;Helping the agent remember as it goes&lt;/head&gt;
    &lt;p&gt;The agent can track how well a tool call works as it goes. In my code, the agent stores its evaluation of each tool’s results. Essentially “LLM-as-a-judge as you go”. The next time a search happens, the agent can recall what it liked / didn’t like last time.&lt;/p&gt;
    &lt;code&gt;system_prompt = """
    You take user search queries and use a search tool to find furniture products.
    
    Look at the search tools you have, their limitations, how they work, etc when forming your plan.

    Before searching you MUST use the "get_past_queries" to get similar, past queries
    the user has made

    Remember every tool usage you make. After searching with a tool, evaluate the results,
    then save the interaction (immediately after tool usage) with the "save_queries_used" tool
"""

&lt;/code&gt;
    &lt;p&gt;Below are search tool queries my code saved after I searched for the &lt;code&gt;ugliest chair in the catalog&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;You see the user’s query, along with the query used at the tool (ie &lt;code&gt;cow print chair&lt;/code&gt; ):&lt;/p&gt;
    &lt;code&gt;Saved interaction: user_query='ugliest chair in the catalog' 
search_tool_name='mcp_search-server.search_products' 
search_tool_query='cow print chair'
quality='good'
reasoning="Returned an adult 'cow print task chair' that clearly fits a loud/novelty aesthetic; strong candidate for 'ugliest chair' request."

Saved interaction: user_query='ugliest chair in the catalog'
search_tool_name='mcp_search-server.search_products' 
search_tool_query='patchwork accent chair'
quality='meh'
reasoning="Found a kids patchwork chair; adult options not surfaced. Only tangentially relevant to 'ugliest chair' intent."

Saved interaction: user_query='ugliest chair in the catalog' 
search_tool_name='mcp_search-server.search_products'
search_tool_query='skull chair'
quality='bad'
reasoning='No chair products matched; results were decor and textiles unrelated to chairs.'

&lt;/code&gt;
    &lt;p&gt;I’ll be able to recall what worked well for this query. I could even remember these expansions later if I build a normal site search without LLMs.&lt;/p&gt;
    &lt;p&gt;Semantic caching gives us another bonus. Later, when I search for a similar query, say &lt;code&gt;ugly chair&lt;/code&gt; , I can take &lt;code&gt;ugly chair&lt;/code&gt; and find past queries most similar to this query, like below.&lt;/p&gt;
    &lt;p&gt;The agent can see what’s worked well / hasn’t worked well for friends of &lt;code&gt;ugly chair&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;Getting past queries for: ugly chair

Matched query: 
  ugliest chair in the catalog, 
  similarity: 0.8236806392669678, 
  interaction: user_query='ugliest chair in the catalog' 
  search_tool_name='mcp_search-server.search_products'
  search_tool_query='novelty chair'
  quality='meh' reasoning="Found novelty chairs but primarily for kids; may not match adult seating requestfor 'ugliest chair'."
Matched query: ugliest chair in the catalog, 
  similarity: 0.8236806392669678, interaction: user_query='ugliest chair in the catalog' search_tool_name='mcp_search-server.search_products' search_tool_query='weird chair'
  quality='bad'
  reasoning="Keyword 'weird' mostly matched non-chair textiles and generic chair listings; not  helpful for surfacing intentionally odd or gaudy chairs."
Matched query: ugliest chair in the catalog, 
  similarity: 0.8236806392669678,
  interaction: user_query='ugliest chair in the catalog'
  search_tool_name='mcp_search-server.search_products'
  search_tool_query='zebra accent chair'
  quality='good'
  reasoning="Surfaced a 'zebra arm chair' (animal print) that fits an intentionally loud/gaudy aesthetic often perceived as ugly; results include a clear adult armchair option."
&lt;/code&gt;
    &lt;p&gt;As the agent searches, it’s little vector lookup of user queries turns into a knowledge graph. User queries connected to other user queries via similarity. And each with notes on what’s worked / not worked when calling the search tool.&lt;/p&gt;
    &lt;p&gt;It’s the ultimate learning machine.&lt;/p&gt;
    &lt;p&gt;At least as far as its own preferences.&lt;/p&gt;
    &lt;head rend="h2"&gt;Only as good as the agent’s ability to evaluate results&lt;/head&gt;
    &lt;p&gt;The LLM must be able to evaluate the results that come back. It needs to be a reasonable LLM judge. That means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Did we return useful metadata for the agent to use in its evaluation? (Here we use product / description). But should we return images? Any other metadata?&lt;/item&gt;
      &lt;item&gt;Does the LLM actually have domain expertise to evaluate the results? (Note above it returns a brand “Gaudy”, which is not the adjective “gaudy” — as in ugly)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Finally the big one:&lt;/p&gt;
    &lt;head rend="h3"&gt;What about the user’s input?&lt;/head&gt;
    &lt;p&gt;The agent’s judgment is bound to be flawed / limited. Even in an ideal setting, it will always be disjoint from what users want. Lack of user clickstream feedback is a huge gap in RAG that continues here.&lt;/p&gt;
    &lt;p&gt;We can’t expect LLMs to have any idea of the crazy, insane things, humans do when nobody is watching. I’ve seen users do all sorts of things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Users often click black / boring colors, not loud ones&lt;/item&gt;
      &lt;item&gt;Users click on spicy topics or images that have nothing to do with their query&lt;/item&gt;
      &lt;item&gt;Users get what they need from the search page, and never click anything&lt;/item&gt;
      &lt;item&gt;Users click because of the relationship between an image and its neighbor&lt;/item&gt;
      &lt;item&gt;Click data itself is sparse and noisy, and may require a lot in aggregate to make sense of&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are dozens of biases cataloged in user clickstream interactions with search results.&lt;/p&gt;
    &lt;p&gt;The search field has spent decades interpreting clickstream data, and the RAG world continues to lack good answers to getting that data back into the agent.&lt;/p&gt;
    &lt;p&gt;Maybe you’ve seen good research here? Please share!&lt;/p&gt;
    &lt;head rend="h2"&gt;Are reasoners actually actively harmful rankers?&lt;/head&gt;
    &lt;p&gt;Reasoning involves taking a base set of facts and deriving a solution. This, however, is different from directly optimizing for what users want (as expressed through clickstream data). We don’t have the technology for an LLM or agent to churn over millions of user sessions to become a better judge of search result quality.&lt;/p&gt;
    &lt;p&gt;Even if we gave the agent an oracle - labeled each result with user engagement data, could it make sense of latent patterns? Or would it simply be confused at the strange inconsistent behavior of humans that don’t act like math proofs or programming conpetitions?&lt;/p&gt;
    &lt;p&gt;Any ranking model optimized to engagement would more likely be implemented outside the agent: In a classic, thick search stack. The agent’s inner LLM judge, might look at model-ranked results and assume they’re bad. Why would users want such bland products? Why would the search have interpreted “Gaudy” to be a brand name, when its obviously “ugly”?&lt;/p&gt;
    &lt;p&gt;In short, users’ chaotic, implicit search expectations — as expressed in their clickstream — go against the agent’s orderly, explicit reasoning + judgment. An agent may issue a search and think “oh this result set was meh” when in reality, its the most engaging search results for that query.&lt;/p&gt;
    &lt;p&gt;Reasoning about search tool output might be very dangerous if the agent discards actually relevant results.&lt;/p&gt;
    &lt;p&gt;So take this article with this big grain of salt and be careful.&lt;/p&gt;
    &lt;p&gt;But still, it’s pretty fun.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45347363</guid><pubDate>Tue, 23 Sep 2025 14:17:23 +0000</pubDate></item><item><title>Show HN: Open-source AI data generator (now hosted)</title><link>https://www.metabase.com/ai-data-generator</link><description>&lt;doc fingerprint="9342a5104ed0da8"&gt;
  &lt;main&gt;
    &lt;p&gt;Because “demo data” shouldn’t be a blocker! Our AI data generator turns a few dropdowns into believable, business-ready datasets in minutes. Pick an industry, preview a schema, and export… now product demos, tests, and dashboards feel real without touching real data.&lt;/p&gt;
    &lt;p&gt;The AI data generator uses a two-stage process to create realistic business data. First, it leverages large language models to generate detailed data specifications based on your business type and parameters. Then, it uses these specifications to create unlimited amounts of realistic data locally.&lt;/p&gt;
    &lt;p&gt;Next.js, Tailwind CSS + ShadCN UI, Open AI, Faker, LiteLLM (only available locally), and Metabase (dockerized, launched on demand locally).&lt;/p&gt;
    &lt;p&gt;Absolutely, this project is fully open source and yours to shape. You can check out the repo here. Star it, fork it, or open a PR ✌️&lt;/p&gt;
    &lt;p&gt;You bet! On the web, click "try Metabase Cloud for free Once Metabase is ready, click "Open Metabase" to explore your data. In Metabase, use the "Upload Data" feature to analyze your CSV files or connect to your database. If you are running the app locally, "Start Metabase" to spin up Metabase in Docker and follow the same steps to load the data. When done, click "Stop Metabase" to shut down and clean up Docker containers.&lt;/p&gt;
    &lt;p&gt;Choose the specific data parameters you want using the dropdowns.&lt;/p&gt;
    &lt;p&gt;Click on an arrow to see a sample schema and 10 rows of generated data.&lt;/p&gt;
    &lt;p&gt;If it looks good, you can export a full dataset as CSV, SQL, or launch Metabase to explore it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45347623</guid><pubDate>Tue, 23 Sep 2025 14:33:59 +0000</pubDate></item><item><title>Show HN: Kekkai – a simple, fast file integrity monitoring tool in Go</title><link>https://github.com/catatsuy/kekkai</link><description>&lt;doc fingerprint="f54dd7534d159c2"&gt;
  &lt;main&gt;
    &lt;p&gt;A simple and fast Go tool for file integrity monitoring. Detects unauthorized file modifications caused by OS command injection and other attacks by recording file hashes during deployment and verifying them periodically.&lt;/p&gt;
    &lt;p&gt;The name "Kekkai" comes from the Japanese word 結界 (kekkai), meaning "barrier" - a protective boundary that keeps unwanted things out, perfectly representing this tool's purpose of protecting your files from tampering.&lt;/p&gt;
    &lt;p&gt;Kekkai was designed to solve specific challenges in production server environments:&lt;/p&gt;
    &lt;p&gt;Traditional tools like &lt;code&gt;tar&lt;/code&gt; or file sync utilities (e.g., &lt;code&gt;rsync&lt;/code&gt;) include metadata like timestamps in their comparisons, causing false positives when only timestamps change. In environments with heavy NFS usage or dynamic log directories, existing tools become difficult to configure and maintain.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Content-Only Hashing&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Hashes only file contents, ignoring timestamps and metadata&lt;/item&gt;
          &lt;item&gt;Detects actual content changes, not superficial modifications&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Immutable Exclude Rules&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Exclude patterns are set during manifest generation only&lt;/item&gt;
          &lt;item&gt;Cannot be modified during verification, preventing attackers from hiding changes&lt;/item&gt;
          &lt;item&gt;Only exclude server-generated files (logs, cache, uploads, NFS mounts)&lt;/item&gt;
          &lt;item&gt;Application dependencies (vendor, node_modules) are monitored as they're part of the deployment&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Symlink Security&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Uses &lt;code&gt;os.Lstat&lt;/code&gt;to properly detect symlinks without following them&lt;/item&gt;
          &lt;item&gt;Tracks symbolic links with their target paths (via &lt;code&gt;os.Readlink&lt;/code&gt;)&lt;/item&gt;
          &lt;item&gt;Hashes the symlink target path itself, not the target's content&lt;/item&gt;
          &lt;item&gt;Detects when symlinks are modified to point to different targets&lt;/item&gt;
          &lt;item&gt;Detects when regular files are replaced with symlinks (and vice versa)&lt;/item&gt;
          &lt;item&gt;Prevents attackers from hiding malicious changes through symlink manipulation&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;Uses &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Secure Hash Storage with S3&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Deploy servers have write-only access&lt;/item&gt;
          &lt;item&gt;Application servers have read-only access&lt;/item&gt;
          &lt;item&gt;Even if compromised, attackers cannot modify stored hashes&lt;/item&gt;
          &lt;item&gt;Local file output available for testing&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tamper-Resistant Distribution&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Single Go binary with minimal dependencies&lt;/item&gt;
          &lt;item&gt;Recommended to run with restricted permissions&lt;/item&gt;
          &lt;item&gt;Configuration should be read from S3 or managed paths, not local files&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🚀 Fast: Efficient hash calculation with parallel processing&lt;/item&gt;
      &lt;item&gt;🔒 Secure: Tamper-proof storage with S3 integration&lt;/item&gt;
      &lt;item&gt;📊 Monitoring Ready: Multiple output formats for various monitoring systems&lt;/item&gt;
      &lt;item&gt;🎯 Deterministic: Same file structure always produces the same hash&lt;/item&gt;
      &lt;item&gt;☁️ EC2 Ready: Authentication via IAM roles&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Build from source
git clone https://github.com/catatsuy/kekkai.git
cd kekkai
make

# Or directly with go build
go build -o ./bin/kekkai ./cmd/kekkai

# Run tests
make test&lt;/code&gt;
    &lt;code&gt;# Generate manifest
kekkai generate --target /var/www/app --output manifest.json

# Verify files
kekkai verify --manifest manifest.json --target /var/www/app&lt;/code&gt;
    &lt;code&gt;kekkai generate \
  --target /var/www/app \
  --exclude "*.log" \
  --exclude "cache/**" \
  --output manifest.json&lt;/code&gt;
    &lt;p&gt;Kekkai stores manifests in S3 for secure, centralized management. Each deployment updates the same &lt;code&gt;manifest.json&lt;/code&gt; file.&lt;/p&gt;
    &lt;code&gt;# For production deployment (must explicitly specify --base-path)
kekkai generate \
  --target /var/www/app \
  --s3-bucket my-manifests \
  --app-name myapp \
  --base-path production  # Explicitly required for production

# For staging/development (uses default "development" if not specified)
kekkai generate \
  --target /var/www/app \
  --s3-bucket my-manifests \
  --app-name myapp \
  --base-path staging

# During verification (must match the base-path used during generation)
kekkai verify \
  --s3-bucket my-manifests \
  --app-name myapp \
  --base-path production \
  --target /var/www/app&lt;/code&gt;
    &lt;p&gt;Benefits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Lower S3 costs - Minimal S3 operations&lt;/item&gt;
      &lt;item&gt;Clean structure - One manifest file per application&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Add to crontab for periodic checks
*/5 * * * * kekkai verify \
  --s3-bucket my-manifests \
  --app-name myapp \
  --base-path production \
  --target /var/www/app

# Use cache for faster verification (cache in temp directory)
*/5 * * * * kekkai verify \
  --s3-bucket my-manifests \
  --app-name myapp \
  --base-path production \
  --target /var/www/app \
  --use-cache \
  --verify-probability 0.1

# Use persistent cache in custom directory
*/5 * * * * kekkai verify \
  --s3-bucket my-manifests \
  --app-name myapp \
  --base-path production \
  --target /var/www/app \
  --use-cache \
  --cache-dir /var/cache/kekkai \
  --verify-probability 0.1&lt;/code&gt;
    &lt;p&gt;Configure your monitoring system to alert based on your requirements (e.g., alert after consecutive failures).&lt;/p&gt;
    &lt;p&gt;These examples show common exclude patterns for various frameworks. Important: Only exclude files generated on the server (logs, cache, uploads). Application dependencies like &lt;code&gt;vendor&lt;/code&gt; or &lt;code&gt;node_modules&lt;/code&gt; MUST be monitored as they are part of the deployed application.&lt;/p&gt;
    &lt;p&gt;For production use, replace &lt;code&gt;--output manifest.json&lt;/code&gt; with S3 storage options (&lt;code&gt;--s3-bucket&lt;/code&gt;, &lt;code&gt;--app-name&lt;/code&gt;, &lt;code&gt;--base-path&lt;/code&gt;).&lt;/p&gt;
    &lt;code&gt;kekkai generate \
  --target /var/www/app \
  --exclude "storage/**" \
  --exclude "bootstrap/cache/**" \
  --exclude "*.log" \
  --output manifest.json&lt;/code&gt;
    &lt;code&gt;kekkai generate \
  --target /var/www/app \
  --exclude "*.log" \
  --exclude ".npm/**" \
  --exclude "tmp/**" \
  --output manifest.json&lt;/code&gt;
    &lt;code&gt;kekkai generate \
  --target /var/www/app \
  --exclude "log/**" \
  --exclude "tmp/**" \
  --exclude "public/assets/**" \
  --output manifest.json&lt;/code&gt;
    &lt;code&gt;kekkai generate \
  --target /var/www/app \
  --exclude "**/__pycache__/**" \
  --exclude "media/**" \
  --exclude "staticfiles/**" \
  --exclude "*.pyc" \
  --output manifest.json&lt;/code&gt;
    &lt;p&gt;For deployment server (write access):&lt;/p&gt;
    &lt;code&gt;{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:PutObject"
      ],
      "Resource": "arn:aws:s3:::my-manifests/*"
    }
  ]
}&lt;/code&gt;
    &lt;p&gt;For production server (read-only):&lt;/p&gt;
    &lt;code&gt;{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject"
      ],
      "Resource": "arn:aws:s3:::my-manifests/*"
    }
  ]
}&lt;/code&gt;
    &lt;p&gt;Recommended: Enable S3 versioning to maintain history of manifest changes.&lt;/p&gt;
    &lt;code&gt;# Optional: Enable versioning for history tracking
aws s3api put-bucket-versioning \
  --bucket my-manifests \
  --versioning-configuration Status=Enabled

# Enable encryption
aws s3api put-bucket-encryption \
  --bucket my-manifests \
  --server-side-encryption-configuration '{
    "Rules": [{
      "ApplyServerSideEncryptionByDefault": {
        "SSEAlgorithm": "AES256"
      }
    }]
  }'

# Optional: Set lifecycle policy to delete old versions after N days
aws s3api put-bucket-lifecycle-configuration \
  --bucket my-manifests \
  --lifecycle-configuration '{
    "Rules": [{
      "Id": "DeleteOldVersions",
      "Status": "Enabled",
      "NoncurrentVersionExpiration": {
        "NoncurrentDays": 30
      }
    }]
  }'&lt;/code&gt;
    &lt;code&gt;#!/bin/bash
# deploy.sh

set -e

APP_NAME="myapp"
DEPLOY_DIR="/var/www/app"
S3_BUCKET="my-manifests"

# 1. Install dependencies locally
cd ./src
composer install --no-dev

# 2. Deploy application to server
rsync -av ./src/ ${DEPLOY_DIR}/

# 3. Generate manifest and save to S3
# Note: For production, explicitly specify --base-path production
kekkai generate \
  --target ${DEPLOY_DIR} \
  --exclude "storage/**" \
  --exclude "bootstrap/cache/**" \
  --s3-bucket ${S3_BUCKET} \
  --app-name ${APP_NAME} \
  --base-path production  # MUST be explicit for production

echo "Deploy completed with integrity manifest"
echo "Manifest saved to: ${S3_BUCKET}/production/${APP_NAME}/manifest.json"&lt;/code&gt;
    &lt;p&gt;Generate a manifest file.&lt;/p&gt;
    &lt;code&gt;Options:
  -target string      Target directory (default ".")
  -output string      Output file, "-" for stdout (default "-")
  -exclude string     Exclude pattern (can be specified multiple times)
  -s3-bucket string   S3 bucket name
  -s3-region string   AWS region
  -base-path string   S3 base path (default "development")
  -app-name string    Application name (creates path: {base-path}/{app-name}/manifest.json)
  -format string      Output format: text, json (default "text")
  -workers int        Number of worker threads (0 = auto detect)
  -rate-limit int     Rate limit in bytes per second (0 = no limit)
  -timeout int        Timeout in seconds (default: 300)
&lt;/code&gt;
    &lt;p&gt;Verify file integrity.&lt;/p&gt;
    &lt;code&gt;Options:
  -manifest string    Manifest file path
  -s3-bucket string   S3 bucket name
  -s3-region string   AWS region
  -base-path string   S3 base path (default "development")
  -app-name string    Application name (reads from: {base-path}/{app-name}/manifest.json)
  -target string      Target directory to verify (default ".")
  -format string      Output format: text, json (default "text")
  -workers int              Number of worker threads (0 = auto detect)
  -rate-limit int           Rate limit in bytes per second (0 = no limit)
  -timeout int              Timeout in seconds (default: 300)
  -use-cache                Enable local cache for verification (checks size, mtime, ctime)
  -cache-dir string         Directory for cache file (default: system temp directory)
  -verify-probability float Probability of hash verification with cache hit (0.0-1.0, default: 0.1)
&lt;/code&gt;
    &lt;code&gt;✓ Integrity check passed
  Verified 1523 files
&lt;/code&gt;
    &lt;code&gt;{
  "success": true,
  "timestamp": "2024-01-01T00:00:00Z",
  "message": "All files verified successfully",
  "details": {
    "total_files": 1523,
    "verified_files": 1523
  }
}&lt;/code&gt;
    &lt;p&gt;Kekkai uses glob patterns for the &lt;code&gt;--exclude&lt;/code&gt; option to skip specific files and directories during manifest generation.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Pattern&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Example&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;*.ext&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Match files with specific extension&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;*.log&lt;/code&gt; matches &lt;code&gt;app.log&lt;/code&gt;, &lt;code&gt;error.log&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;dir/*&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Match all files in a directory&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;logs/*&lt;/code&gt; matches &lt;code&gt;logs/app.log&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;dir/**&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Match all files recursively&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;cache/**&lt;/code&gt; matches &lt;code&gt;cache/data.db&lt;/code&gt;, &lt;code&gt;cache/sessions/abc.txt&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;**/*.ext&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Match extension at any depth&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;**/*.pyc&lt;/code&gt; matches &lt;code&gt;app.pyc&lt;/code&gt;, &lt;code&gt;lib/utils.pyc&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;**/dir/*&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Match directory at any depth&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;**/logs/*&lt;/code&gt; matches &lt;code&gt;logs/app.log&lt;/code&gt;, &lt;code&gt;app/logs/error.log&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;path/to/file&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Exact path match&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;config/local.ini&lt;/code&gt; matches only that file&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Relative Paths: All patterns match against relative paths from the target directory&lt;/item&gt;
      &lt;item&gt;Forward Slashes: Always use &lt;code&gt;/&lt;/code&gt;as path separator (even on Windows)&lt;/item&gt;
      &lt;item&gt;No Negation: Patterns cannot be negated (no &lt;code&gt;!pattern&lt;/code&gt;support)&lt;/item&gt;
      &lt;item&gt;Order Independent: All patterns are evaluated, order doesn't matter&lt;/item&gt;
      &lt;item&gt;Immutable: Exclude patterns cannot be changed during verification&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Laravel/Symfony
--exclude "storage/**"          # User uploads
--exclude "var/cache/**"        # Framework cache
--exclude "var/log/**"          # Application logs
--exclude "public/uploads/**"   # Uploaded files

# Python/Django
--exclude "**/__pycache__/**"   # Python cache
--exclude "**/*.pyc"            # Compiled Python
--exclude "media/**"            # User uploads
--exclude "staticfiles/**"      # Collected static files

# Node.js
--exclude "*.log"               # Log files
--exclude "tmp/**"              # Temporary files
--exclude ".npm/**"             # NPM cache

# General
--exclude "*.tmp"               # Temporary files
--exclude "*.bak"               # Backup files
--exclude ".git/**"             # Git repository (if needed)&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;❌ &lt;code&gt;--exclude "vendor/**"&lt;/code&gt;(PHP dependencies)&lt;/item&gt;
      &lt;item&gt;❌ &lt;code&gt;--exclude "node_modules/**"&lt;/code&gt;(Node.js dependencies)&lt;/item&gt;
      &lt;item&gt;❌ &lt;code&gt;--exclude "venv/**"&lt;/code&gt;(Python virtual environment)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are part of your deployed application and must be monitored for tampering.&lt;/p&gt;
    &lt;p&gt;✅ Only exclude server-generated content:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Log files&lt;/item&gt;
      &lt;item&gt;Cache directories&lt;/item&gt;
      &lt;item&gt;User uploads&lt;/item&gt;
      &lt;item&gt;Temporary files&lt;/item&gt;
      &lt;item&gt;NFS mounts&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Patterns are evaluated in this order:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Check for &lt;code&gt;**&lt;/code&gt;recursive matching&lt;/item&gt;
      &lt;item&gt;Special case: &lt;code&gt;**/*&lt;/code&gt;or&lt;code&gt;**&lt;/code&gt;matches everything&lt;/item&gt;
      &lt;item&gt;Suffix pattern: &lt;code&gt;dir/**&lt;/code&gt;matches everything under&lt;code&gt;dir/&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Prefix pattern: &lt;code&gt;**/*.ext&lt;/code&gt;matches files with extension at any depth&lt;/item&gt;
      &lt;item&gt;Simple glob: Standard shell glob matching with &lt;code&gt;*&lt;/code&gt;and&lt;code&gt;?&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Kekkai has comprehensive symlink security to prevent attackers from hiding malicious changes:&lt;/p&gt;
    &lt;p&gt;Kekkai handles symlinks differently depending on where they appear:&lt;/p&gt;
    &lt;p&gt;If &lt;code&gt;--target&lt;/code&gt; points to a symlink (e.g., &lt;code&gt;/current&lt;/code&gt; → &lt;code&gt;/releases/20240101&lt;/code&gt;):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Automatically resolved: Uses &lt;code&gt;filepath.EvalSymlinks&lt;/code&gt;to follow the symlink&lt;/item&gt;
      &lt;item&gt;Operates on real path: All operations happen in the resolved directory&lt;/item&gt;
      &lt;item&gt;Transparent to user: Works exactly as if you specified the real directory&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;# These produce identical results:
kekkai generate --target /var/www/current        # Symlink to /var/www/releases/20240101
kekkai generate --target /var/www/releases/20240101  # Direct path&lt;/code&gt;
    &lt;p&gt;For symlinks found within the target directory:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Not followed: Uses &lt;code&gt;os.Lstat&lt;/code&gt;to detect them without following&lt;/item&gt;
      &lt;item&gt;Tracked as symlinks: Stored with &lt;code&gt;IsSymlink: true&lt;/code&gt;flag&lt;/item&gt;
      &lt;item&gt;Target recorded: Link target path saved for verification&lt;/item&gt;
      &lt;item&gt;Hash of target path: Creates hash from &lt;code&gt;"symlink:" + target_path&lt;/code&gt;string&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Detection: Uses &lt;code&gt;os.Lstat&lt;/code&gt;to identify symlinks without following them&lt;/item&gt;
      &lt;item&gt;Target Tracking: Reads the link target with &lt;code&gt;os.Readlink&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Hash Calculation: Creates hash from &lt;code&gt;"symlink:" + target_path&lt;/code&gt;string&lt;/item&gt;
      &lt;item&gt;Verification: Checks both link type and target path during verification&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ Symlink target changes (e.g., &lt;code&gt;/usr/bin/php&lt;/code&gt;→&lt;code&gt;/tmp/malicious&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;✅ File type changes (regular file → symlink or symlink → regular file)&lt;/item&gt;
      &lt;item&gt;✅ Broken symlinks (target doesn't exist)&lt;/item&gt;
      &lt;item&gt;✅ New symlinks added to the directory&lt;/item&gt;
      &lt;item&gt;✅ Deleted symlinks&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Original deployment
/app/config.php -&amp;gt; /etc/app/config.php  # Hash: abc123...

# These changes will be detected:
/app/config.php -&amp;gt; /tmp/fake-config.php  # Modified: different target
/app/config.php (regular file)           # Modified: type change
/app/config.php (deleted)                # Deleted: symlink removed&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Symlinks are never followed during hash calculation&lt;/item&gt;
      &lt;item&gt;Only the link itself is hashed, not the target's content&lt;/item&gt;
      &lt;item&gt;Prevents directory traversal attacks via symlink manipulation&lt;/item&gt;
      &lt;item&gt;Cache system skips symlinks (only caches regular files)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A: Kekkai only hashes file contents, so timestamp or permission changes don't affect hashes. Check for line ending differences (CRLF/LF).&lt;/p&gt;
    &lt;p&gt;A: Verify that the EC2 instance has the correct IAM role configured. Also check that the region is specified correctly.&lt;/p&gt;
    &lt;p&gt;A: For large file sets, use &lt;code&gt;--exclude&lt;/code&gt; options to skip server-generated directories like logs, cache, and temporary files. You can also optimize performance with:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--use-cache&lt;/code&gt;: Enable local cache that checks file metadata (size, mtime, ctime) to skip hash calculation&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--verify-probability N&lt;/code&gt;: Set probability of hash verification even with cache hit (0.0-1.0, default: 0.1)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--workers N&lt;/code&gt;: Adjust the number of worker threads for your system&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--rate-limit N&lt;/code&gt;: Limit I/O throughput (bytes per second) to reduce system load&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Cache Mode: When using &lt;code&gt;--use-cache&lt;/code&gt;, kekkai maintains a local cache file (&lt;code&gt;.kekkai-cache-{base-name}-{app-name}.json&lt;/code&gt;) in the cache directory (defaults to system temp directory, or specify with &lt;code&gt;--cache-dir&lt;/code&gt;). Cache files are temporary by nature and will be recreated if missing. It checks file metadata including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;File size&lt;/item&gt;
      &lt;item&gt;Modification time (mtime)&lt;/item&gt;
      &lt;item&gt;Change time (ctime) - cannot be easily forged&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If all metadata matches, it uses the cached hash. The &lt;code&gt;--verify-probability&lt;/code&gt; option adds probabilistic verification:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;0.0&lt;/code&gt;: Always trust cache (fastest, least secure)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0.1&lt;/code&gt;: 10% chance to verify hash even with cache hit (default, good balance)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;1.0&lt;/code&gt;: Always verify hash (most secure, no performance benefit)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The cache file itself is protected with a hash to detect tampering.&lt;/p&gt;
    &lt;p&gt;💡 Cache Behavior: By default, cache files are stored in the system temp directory (e.g., &lt;code&gt;/tmp&lt;/code&gt; on Linux/macOS) and may be automatically cleaned by the system. This is intentional - the cache is designed to be ephemeral and will be recreated as needed for performance optimization.&lt;/p&gt;
    &lt;p&gt;Note that application dependencies (vendor, node_modules) should still be verified as they are part of the deployed application.&lt;/p&gt;
    &lt;p&gt;A: Use &lt;code&gt;--rate-limit&lt;/code&gt; to throttle I/O bandwidth. For example, &lt;code&gt;--rate-limit 10485760&lt;/code&gt; limits to 10MB/s. This global rate limit is shared across all worker threads, preventing system overload while still allowing parallel processing.&lt;/p&gt;
    &lt;p&gt;Alternatively, you can use systemd to control resource usage at the OS level:&lt;/p&gt;
    &lt;code&gt;# Run with limited CPU and I/O priority (with cache support)
systemd-run --quiet --wait --pipe --collect \
  -p Type=oneshot \
  -p CPUQuota=25% -p CPUWeight=50 \
  -p PrivateTmp=no \
  -p User=nobody \
  /bin/bash -lc 'nice -n 10 ionice -c2 -n7 /usr/local/bin/kekkai verify \
    --s3-bucket my-manifests \
    --app-name myapp \
    --target /srv/app \
    --use-cache \
    --verify-probability 0.1 \
    --rate-limit 10485760'&lt;/code&gt;
    &lt;p&gt;This approach provides more comprehensive resource control:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;CPUQuota=25%&lt;/code&gt;: Limits CPU usage to 25%&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;CPUWeight=50&lt;/code&gt;: Sets CPU scheduling weight (lower priority)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;PrivateTmp=no&lt;/code&gt;: Allows cache persistence in&lt;code&gt;/tmp&lt;/code&gt;across runs&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;User=nobody&lt;/code&gt;: Runs with minimal privileges for security&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;nice -n 10&lt;/code&gt;: Lower process priority&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ionice -c2 -n7&lt;/code&gt;: Best-effort I/O scheduling with lowest priority&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--use-cache&lt;/code&gt;: Enables cache for faster verification&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--verify-probability 0.1&lt;/code&gt;: 10% chance to verify hash even with cache hit&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--rate-limit 10485760&lt;/code&gt;: Limits I/O to 10MB/s&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Important: The &lt;code&gt;PrivateTmp=no&lt;/code&gt; setting is required when using &lt;code&gt;--use-cache&lt;/code&gt; to ensure cache files persist between systemd-run executions. Without this, systemd creates an isolated &lt;code&gt;/tmp&lt;/code&gt; directory for each run, preventing cache reuse. If you prefer stronger isolation, use &lt;code&gt;--cache-dir&lt;/code&gt; to specify a persistent directory outside of &lt;code&gt;/tmp&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;# Alternative: Keep PrivateTmp=yes but use a custom cache directory
systemd-run --quiet --wait --pipe --collect \
  -p Type=oneshot \
  -p CPUQuota=25% -p CPUWeight=50 \
  -p PrivateTmp=yes \
  -p User=nobody \
  /bin/bash -lc 'nice -n 10 ionice -c2 -n7 /usr/local/bin/kekkai verify \
    --s3-bucket my-manifests \
    --app-name myapp \
    --target /srv/app \
    --use-cache \
    --cache-dir /var/cache/kekkai \
    --verify-probability 0.1 \
    --rate-limit 10485760'&lt;/code&gt;
    &lt;p&gt;Security Recommendation: Always run kekkai verification with minimal privileges using &lt;code&gt;-p User=nobody&lt;/code&gt; or a dedicated non-privileged user. This follows the principle of least privilege and reduces the security impact if the process is compromised.&lt;/p&gt;
    &lt;p&gt;Note: With Go 1.25+, &lt;code&gt;CPUQuota&lt;/code&gt; also automatically adjusts &lt;code&gt;GOMAXPROCS&lt;/code&gt; to match the quota, so kekkai will use fewer worker threads when CPU is limited, providing better resource utilization.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;--use-cache&lt;/code&gt; on NFS&lt;/p&gt;
    &lt;p&gt;Cache mode is disabled by default. Do not enable it on NFS-mounted directories as it does not work well and may run slower than without cache.&lt;/p&gt;
    &lt;p&gt;Kekkai's cache system uses ctime (change time) to detect file changes. Ctime is hard to fake and important for security.&lt;/p&gt;
    &lt;p&gt;However, NFS clients may not update ctime properly. This causes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Very low cache hit rate&lt;/item&gt;
      &lt;item&gt;Cache becomes useless&lt;/item&gt;
      &lt;item&gt;Runs as slow as normal verification&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Disable cache on NFS
kekkai verify \
  --s3-bucket my-manifests \
  --app-name myapp \
  --target /nfs/app \
  --workers 2 \
  --rate-limit 5242880&lt;/code&gt;
    &lt;p&gt;Disabling cache does not reduce security. All files are still verified with SHA256 hashes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45347714</guid><pubDate>Tue, 23 Sep 2025 14:39:59 +0000</pubDate></item><item><title>Zip Code Map of the United States</title><link>https://engaging-data.com/us-zip-code-map/</link><description>&lt;doc fingerprint="7e9b239a41844cf"&gt;
  &lt;main&gt;
    &lt;p&gt;This zip code map of the United States visualizes over 42,000 zip codes in the 50 states. Zip codes are five digit postal codes used for mail delivery in the US. The points on the map show the geographic center of each zip code. The interactive visualization lets you type in a zip code and will show you where that zip code lies on the map. As you begin to type in the zip code, the map will highlight all the zip codes that begin with those numbers.&lt;/p&gt;
    &lt;p&gt;For example, if you type in “0”, you will highlight all zip codes that start with the zero in the Northeastern US. This will represent about 10% of the zip codes in the US. When you type in another number, it will narrow down the zip codes that begin with those two digits (approximately 1% of zip codes). It will progressively narrow down the number of zip codes as you type in more numbers, until you get to a full 5 digit zip code that represents 1 out of almost 43,000 zip codes (0.002% of zip codes). The map will then tell you the name of the city that that zip code is in.&lt;/p&gt;
    &lt;p&gt;You can explore how zip codes are distributed across the US by typing in different 1 and 2 digit numbers. You can also click on the check box to show or hide the outlines of the states.&lt;/p&gt;
    &lt;p&gt;Sources and Tools:&lt;/p&gt;
    &lt;p&gt;Zip code data was downloaded from opendatasoft.com. And the visualization was created using javascript and the open source leaflet javascript mapping library.&lt;/p&gt;
    &lt;head rend="h3"&gt;Related Posts&lt;/head&gt;
    &lt;p&gt;1 Comment »&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45347880</guid><pubDate>Tue, 23 Sep 2025 14:50:06 +0000</pubDate></item><item><title>Launch HN: Strata (YC X25) – One MCP server for AI to handle thousands of tools</title><link>https://news.ycombinator.com/item?id=45347914</link><description>&lt;doc fingerprint="3d5a22c222f374b3"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hey HN! We are Klavis AI (&lt;/p&gt;https://www.klavis.ai/&lt;p&gt;) and we're launching Strata, one open-source MCP server that helps AI agents use thousands of API tools without getting overwhelmed. Instead of showing all available tools at once, Strata reveals them step-by-step based on what the AI actually needs.&lt;/p&gt;&lt;p&gt;As a former Senior SWE on Google Gemini 's tool use team, I saw firsthand how AI would struggle with tools. If you've built AI agents, you've likely hit the same walls: (1) AI agents struggle to pick the right API from hundreds of options. (2) Tool descriptions and info consume massive token budgets. (3) Most servers cap at 40~50 tools to avoid these problems, limiting what you can build.&lt;/p&gt;&lt;p&gt;Instead of flooding the AI with everything upfront, Strata works like a human would. It guides the AI agents to discover relevant categories, then lists available actions in those categories. It relies on LLMs’ reasoning to drill down progressively to find the exact tool needed. Here are some examples:&lt;/p&gt;&lt;p&gt;Github query: "Find my stale pull requests in our main repo"&lt;/p&gt;&lt;p&gt;Strata: AI model identifies GitHub → Shows categories (Repos, Issues, PRs, Actions) → AI selects PRs → Shows PR-specific actions -&amp;gt; AI selects list_pull_requests → Shows list_pull_requests details -&amp;gt; Executes list_pull_requests with the right parameters.&lt;/p&gt;&lt;p&gt;Jira query: "Create a bug ticket in the 'MOBILE' project about the app crashing on startup."&lt;/p&gt;&lt;p&gt;Strata: AI identifies Jira → Shows categories (Projects, Issues, Sprints) → AI selects Issues → Shows actions (create_issue, get_issue) → AI selects create_issue → Shows create_issue details → Executes with correct parameters.&lt;/p&gt;&lt;p&gt;Slack query: "Post a message in the #announcements channel that bonus will be paid out next Friday."&lt;/p&gt;&lt;p&gt;Strata: AI identifies Slack → Shows categories (Channels, Messages, Users) → AI selects Messages → Shows actions (send_message, schedule_message) → AI selects send_message → Shows send_message details → Executes with correct parameters.&lt;/p&gt;&lt;p&gt;This progressive approach unlocks a huge advantage: depth. While most integrations offer a handful of high-level tools, Strata can expose hundreds of granular features for a single app like GitHub, Jira, etc. Your AI agent can finally access the deep, specific features that real workflows require, without getting lost in a sea of options.&lt;/p&gt;&lt;p&gt;Under the hood, Strata manages authentication tokens and includes a built-in search tool for the agent to dig into documentation if it gets stuck.&lt;/p&gt;&lt;p&gt;On the MCPMark https://mcpmark.ai/leaderboard/mcp, Strata achieves +15.2% higher pass@1 rate vs the official GitHub server and +13.4% higher pass@1 rate vs the official Notion server. In human eval tests, it hits 83%+ accuracy on complex, real-world multi-app workflows.&lt;/p&gt;&lt;p&gt;Here is a quick demo to watch Strata navigate a complex workflow with multiple apps, automatically selecting the right tools at each step: https://www.youtube.com/watch?v=N00cY9Ov_fM.&lt;/p&gt;&lt;p&gt;You can connect to any external MCP Server into Strata, and we have an open source version for it: https://github.com/Klavis-AI/klavis.&lt;/p&gt;&lt;p&gt;For team or production use with more features, visit our website: https://www.klavis.ai. Add Strata to Cursor, VS Code or any MCP-compatible application with one click. You can also use our API to easily plug in Strata to your AI application.&lt;/p&gt;&lt;p&gt;We look forward to your comments. Thanks for reading!&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45347914</guid><pubDate>Tue, 23 Sep 2025 14:52:07 +0000</pubDate></item><item><title>From MCP to shell: MCP auth flaws enable RCE in Claude Code, Gemini CLI and more</title><link>https://verialabs.com/blog/from-mcp-to-shell/</link><description>&lt;doc fingerprint="b5638dd89ba7ea90"&gt;
  &lt;main&gt;
    &lt;p&gt;During our security testing, we discovered that connecting to a malicious MCP server via common coding tools like Claude Code and Gemini CLI could give attackers instant control over user computers.&lt;/p&gt;
    &lt;p&gt;As a preview, hereâs a video of us opening the calculator (âpopping calcâ) on someoneâs computer through Claude Code:&lt;/p&gt;
    &lt;p&gt;âPopping calcâ is a harmless way of showcasing remote code execution. The exploits we found can be extended for malicious purposes beyond that, such as invisibly installing a reverse shell or malware.&lt;/p&gt;
    &lt;head rend="h2"&gt;TL;DR&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Earlier this year, MCP introduced an OAuth standard to authenticate clients&lt;/item&gt;
      &lt;item&gt;Many MCP clients did not validate the authorization URL passed by a malicious MCP server&lt;/item&gt;
      &lt;item&gt;We were able to exploit this bug to achieve Remote Code Execution (RCE) in popular tools&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;About Us&lt;/head&gt;
    &lt;p&gt;At Veria Labs, we build AI agents that secure high-stakes industries so you can ship quickly and confidently. Founded by members of the #1 competitive hacking team in the U.S., weâve already found critical bugs in AI tools, operating systems, and billion-dollar crypto exchanges.&lt;/p&gt;
    &lt;p&gt;Think we can help secure your systems? Weâd love to chat! Book a call here.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Attack Surface&lt;/head&gt;
    &lt;p&gt;MCP (Model Context Protocol) allows an AI to connect with external tools, APIs, and data sources. It extends an LLM applicationâs base capabilities by sharing context and performing actions, such as giving Gemini access to Google Drive.&lt;/p&gt;
    &lt;p&gt;In March, Anthropic released the first revision to their MCP specification, introducing an authorization framework using OAuth. OAuth is the standard that powers âLogin with Googleâ and other similar authentication methods. Adding OAuth to MCP is a great change for the AI ecosystem, giving a standardized way for MCP servers and clients to authenticate.&lt;/p&gt;
    &lt;p&gt;However, the way MCP clients implemented OAuth creates a new and subtle attack surface. In this blog post, we exploit this attack surface to varying degrees of success across different applications, including Cloudflareâs &lt;code&gt;use-mcp&lt;/code&gt; client library, Anthropicâs MCP Inspector, Claude Code, Gemini CLI, and (almost) ChatGPT itself.&lt;/p&gt;
    &lt;p&gt;The core issue is simple: MCP servers control where clients redirect users for authentication, and most clients trusted this URL completely.&lt;/p&gt;
    &lt;head rend="h2"&gt;Exploiting Cloudflareâs &lt;code&gt;use-mcp&lt;/code&gt; library XSS&lt;/head&gt;
    &lt;p&gt;We initially discovered this vulnerability pattern in June, when Cloudflare released their &lt;code&gt;use-mcp&lt;/code&gt; library. As of the time of writing, the library has over 36,000 weekly downloads on npm.&lt;/p&gt;
    &lt;p&gt;The bug occurs in the OAuth flow where the server tells the client where to open a browser window to authenticate. The bug occurs at &lt;code&gt;src/auth/browser-provider.ts&lt;/code&gt;. In code:&lt;/p&gt;
    &lt;p&gt;If youâre familiar with web exploitation, you may be able to see where this is going.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;use-mcp&lt;/code&gt; client performs &lt;code&gt;window.open()&lt;/code&gt; on &lt;code&gt;authUrlString&lt;/code&gt;, which is an arbitrary string supplied by the MCP server directly to the client. This creates an XSS vulnerability, as you can supply a &lt;code&gt;javascript:&lt;/code&gt; URL in &lt;code&gt;authUrlString&lt;/code&gt;. When supplied to &lt;code&gt;window.open&lt;/code&gt;, a &lt;code&gt;javascript:&lt;/code&gt; URL executes everything supplied as JavaScript code on the currently loaded page.&lt;/p&gt;
    &lt;p&gt;Impact: A user connecting to an MCP application with the &lt;code&gt;use-mcp&lt;/code&gt; library is vulnerable to the server delivering arbitrary JavaScript, which the client will automatically execute on the userâs browser. This can potentially lead to hijacking the user session and the takeover of the user account for that website.&lt;/p&gt;
    &lt;head rend="h3"&gt;Writing our &lt;code&gt;use-mcp&lt;/code&gt; exploit&lt;/head&gt;
    &lt;p&gt;We used the following Cloudflare Workers example code at cloudflare/remote-mcp-github-oauth for our exploit Proof of Concept (PoC). This made the setup process easy, and the PoC only required us to modify a few lines of code.&lt;/p&gt;
    &lt;p&gt;Specifically, our malicious &lt;code&gt;authUrlString&lt;/code&gt; payload is the following:&lt;/p&gt;
    &lt;p&gt;We were able to demonstrate our PoC on Cloudflareâs Workers AI LLM Playground:&lt;/p&gt;
    &lt;p&gt;The newly opened window counts as same-origin, allowing us to hijack the original web page via &lt;code&gt;window.opener&lt;/code&gt;. This gives us a reference to the parent windowâs JavaScript context.&lt;/p&gt;
    &lt;p&gt;Since we can force arbitrary client-side JavaScript execution, any user connecting to an MCP server via the &lt;code&gt;use-mcp&lt;/code&gt; library could have been vulnerable to exploits such as session hijacking and account takeover.&lt;/p&gt;
    &lt;head rend="h2"&gt;Escalating to RCE with MCP Inspector&lt;/head&gt;
    &lt;p&gt;While working on our exploit, we used Anthropicâs MCP Inspector to debug our malicious MCP server. While playing around with MCP Inspector, we found out it too is vulnerable to the same exploit as Cloudflareâs &lt;code&gt;use-mcp&lt;/code&gt; library!&lt;/p&gt;
    &lt;head rend="h3"&gt;XSS -&amp;gt; RCE: Abusing MCPâs &lt;code&gt;stdio&lt;/code&gt; Transport&lt;/head&gt;
    &lt;p&gt;We have XSS now, but that doesnât allow us to do all that much. However, since the application runs locally on a userâs machine, we were interested in seeing if we could do more. Turns out, we can request a connection using MCP Inspectorâs &lt;code&gt;stdio&lt;/code&gt; transport to escalate this XSS into Remote Code Execution (RCE) on the userâs system.&lt;/p&gt;
    &lt;head rend="h4"&gt;What is the MCP &lt;code&gt;stdio&lt;/code&gt; transport?&lt;/head&gt;
    &lt;p&gt;In the context of MCP Inspector, the browser UI canât speak directly to a local process, so the Inspector Proxy (a small Node.js service running on your machine) sits in the middle. When the UI asks to connect to a server via &lt;code&gt;stdio&lt;/code&gt;, the proxy spawns the requested command as a child process and bridges messages between the browser and that process. Functionally, itâs:&lt;/p&gt;
    &lt;p&gt;That bridging role turns an XSS in the Inspector UI into RCE: if attackerâcontrolled JavaScript can run in the Browser UI and obtain the proxyâs authentication token, it can tell the proxy to spawn any local command, effectively escalating XSS to arbitrary code execution on the host.&lt;/p&gt;
    &lt;head rend="h4"&gt;Completing the exploit chain&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;stdio&lt;/code&gt; transport is normally secured against other local processes with an authentication token that only the MCP Inspector client knows. However, since we have XSS, we can steal this token from the query parameter &lt;code&gt;MCP_PROXY_AUTH_TOKEN&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This gives us complete remote code execution on the userâs system with the privileges of the MCP Inspector process. Note that while this specific exploit is written for Windows, Linux and Mac systems are vulnerable too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Exploiting Claude Code and Gemini CLI to take over your PC&lt;/head&gt;
    &lt;p&gt;We also decided to check whether our favorite command line agentic code editors might be vulnerable, as they are some of the most popular programs with MCP implementations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Popping calc in Claude Code&lt;/head&gt;
    &lt;p&gt;Claude Code is not open source, but its npm package includes a minified bundle. We were able to browse different versions on socket.dev to grab &lt;code&gt;cli.js&lt;/code&gt;, which contains the entire Claude Code CLI in a single file.&lt;/p&gt;
    &lt;p&gt;The relevant code (modified for clarity) was:&lt;/p&gt;
    &lt;p&gt;While it performs URL schema validationâmaking it seem safe at first glanceâthe Windows specific code is still vulnerable to command injection. It spawns the browser with &lt;code&gt;cmd.exe /c start &amp;lt;authUrl&amp;gt;&lt;/code&gt;, but we could append &lt;code&gt;&amp;amp;calc.exe&lt;/code&gt;, causing cmd.exe to launch an additional program: &lt;code&gt;cmd.exe /c start &amp;lt;authUrl&amp;gt;&amp;amp;calc.exe&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;As such, this is our payload:&lt;/p&gt;
    &lt;p&gt;Claude Code version 1.0.54 rewrote this to spawn PowerShell instead of cmd.exe.&lt;/p&gt;
    &lt;p&gt;We adapted our exploit to use PowerShellâs string interpolation features. Double-quoted PowerShell strings allow expressions to be evaluated when constructing the string, similar to JavaScript template literals:&lt;/p&gt;
    &lt;p&gt;This payload encodes &lt;code&gt;calc.exe&lt;/code&gt; as base64, then uses PowerShellâs expression evaluation to decode and execute it during string construction.&lt;/p&gt;
    &lt;head rend="h3"&gt;Gemini CLI is also exploitable :)&lt;/head&gt;
    &lt;p&gt;Gemini CLI was exploitable in the exact same way. It passes the OAuth URL to the popular &lt;code&gt;open&lt;/code&gt; npm package.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;open&lt;/code&gt; packageâs README includes this warning:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This package does not make any security guarantees. If you pass in untrusted input, itâs up to you to properly sanitize it.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It turns out that the warning in the &lt;code&gt;open&lt;/code&gt; README is there for a good reason. Looking at the source of &lt;code&gt;open&lt;/code&gt;, we can see the URL opening logic is also implemented through PowerShell, with the same use of templating that made Claude Code vulnerable to command injection.&lt;/p&gt;
    &lt;p&gt;This means the exact same payload we used for Claude Code also works for Gemini CLI!&lt;/p&gt;
    &lt;head rend="h2"&gt;Defenses that prevented exploitation&lt;/head&gt;
    &lt;head rend="h3"&gt;Almost XSSing ChatGPT&lt;/head&gt;
    &lt;p&gt;Recently, OpenAI rolled out ChatGPT Developer Mode which provides full MCP support with the ability to add custom MCP Connectors to ChatGPT.&lt;/p&gt;
    &lt;p&gt;Looking through ChatGPTâs client-side JavaScript, we see that ChatGPT passes the modified redirect URL directly to &lt;code&gt;window.open&lt;/code&gt; during the OAuth flow. This is very similar to the &lt;code&gt;use-mcp&lt;/code&gt; package, resulting in an almost identical exploit.&lt;/p&gt;
    &lt;p&gt;However, there is a strong Content Security Policy (CSP) preventing the &lt;code&gt;javascript:&lt;/code&gt; URL from executing. We attempted to exploit with a custom data URL using the &lt;code&gt;text/html&lt;/code&gt; mimetype, but this was also blocked by ChatGPTâs CSP.&lt;/p&gt;
    &lt;head rend="h3"&gt;Server Side Redirect on Claude Web App&lt;/head&gt;
    &lt;p&gt;For connectors added on the Claude web app, we observed that a server-side redirect would be performed with the malicious URL specified by the MCP server. However, JavaScript execution did not occur. This is because &lt;code&gt;javascript:&lt;/code&gt; URLs are not executed from server-side redirects.&lt;/p&gt;
    &lt;head rend="h2"&gt;Industry Response &amp;amp; Fixes&lt;/head&gt;
    &lt;p&gt;The response across affected vendors was swift; but they took different approaches to solving the underlying problem:&lt;/p&gt;
    &lt;head rend="h3"&gt;Different Fix Approaches&lt;/head&gt;
    &lt;p&gt;Cloudflare created a strict-url-sanitise package, which validates URL schemes and blocks &lt;code&gt;javascript:&lt;/code&gt; URLs. This addresses the specific attack vector through input validation.&lt;/p&gt;
    &lt;p&gt;Anthropicâs fix for Claude Code went through multiple iterations, ultimately settling on eliminating shell usage entirely with &lt;code&gt;await execFileAsync("rundll32",["url,OpenURL",url],{});&lt;/code&gt;. As they already had URL schema validation, this removes the attack surface completely.&lt;/p&gt;
    &lt;p&gt;Google dropped the vulnerable &lt;code&gt;open&lt;/code&gt; package and reimplemented URL opening themselves. In their fix PR, they sanitized URLs by escaping single quotes (&lt;code&gt;'&lt;/code&gt; to &lt;code&gt;''&lt;/code&gt;) for PowerShell. This works, but is not a very robust fix.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Most Impactful Fix&lt;/head&gt;
    &lt;p&gt;The biggest impact came from Anthropicâs update to the MCP TypeScript SDK, which blacklisted dangerous URI schemes like &lt;code&gt;javascript:&lt;/code&gt;. As multiple tools including MCP Inspector consume this SDK, this single upstream change improved security across the entire ecosystem instantly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Not being able to achieve XSS on ChatGPT shows that traditional defense-in-depth methods still work. While the underlying system was vulnerable, CSP prevented us from escalating it into a high-severity vulnerability. Much of the AI space is built on top of existing web technologies and can benefit from taking advantage of web security features.&lt;/p&gt;
    &lt;p&gt;Broad, upstream improvements like what was done in Anthropicâs MCP TypeScript SDK make the ecosystem more secure overall. Exploitation has been too easy in places, but the trajectory is encouraging and we are hopeful for the future of AI security.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Weâd like to thank the following bug bounty programs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cloudflare&lt;/item&gt;
      &lt;item&gt;Anthropic&lt;/item&gt;
      &lt;item&gt;Google VRP&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They had a fast patching process, and both Claude Code and Gemini CLI have an included auto-updater, allowing the fixes to be deployed quickly.&lt;/p&gt;
    &lt;head rend="h3"&gt;Timeline&lt;/head&gt;
    &lt;head rend="h4"&gt;use-mcp&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2025-06-19: Bug reported to Cloudflare via HackerOne&lt;/item&gt;
      &lt;item&gt;2025-06-25: Bug triaged by Cloudflare&lt;/item&gt;
      &lt;item&gt;2025-06-25: Bounty awarded by Cloudflare ($550)&lt;/item&gt;
      &lt;item&gt;2025-06-30: Fix pushed by Cloudflare&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;MCP Inspector&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2025-06-23: Bug reported to Anthropic via HackerOne&lt;/item&gt;
      &lt;item&gt;2025-07-19: Bug triaged by Anthropic&lt;/item&gt;
      &lt;item&gt;2025-08-15: Bounty awarded by Anthropic ($2300)&lt;/item&gt;
      &lt;item&gt;2025-09-06: Published as GHSA-g9hg-qhmf-q45m and CVE-2025-58444&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Claude Code&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2025-07-12: Bug reported to Anthropic via HackerOne&lt;/item&gt;
      &lt;item&gt;2025-07-14: Bug closed by HackerOne Triage team as duplicate&lt;/item&gt;
      &lt;item&gt;2025-07-15: Reopened and properly triaged by Anthropic team&lt;/item&gt;
      &lt;item&gt;2025-07-31: Bounty awarded by Anthropic ($3700)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Gemini CLI&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2025-07-26: Bug reported to Google VRP under OSS VRP program&lt;/item&gt;
      &lt;item&gt;2025-07-28: Bug âidentified as an Abuse Risk and triaged to our Trust &amp;amp; Safety teamâ&lt;/item&gt;
      &lt;item&gt;2025-07-29: Bug filed as P2/S2 (priority and severity)&lt;/item&gt;
      &lt;item&gt;2025-09-04: Abuse VRP panel marks bug as duplicate of already tracked bug.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Unlike HackerOne, Google VRP checks duplicates at the same time as deciding bounties.&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix&lt;/head&gt;
    &lt;head rend="h3"&gt;Other Exploited Vendors&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Cherry Studio was briefly vulnerable, however upon discovery of the vulnerability, we failed to find a suitable security contact. A patch was later created using the same package Cloudflare used (strict-url-sanitise).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The Gemini CLI exploit briefly affected the downstream fork Qwen Code. Once the upstream fix was released, the Qwen Code team quickly patched their fork.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;open&lt;/code&gt;exploit is not new. It was used before to exploit the&lt;code&gt;mcp-remote&lt;/code&gt;package on npm.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Proof of concepts&lt;/head&gt;
    &lt;p&gt;Each PoC is based on the same code with minor tweaks for each target. Code is published at https://github.com/verialabs/mcp-auth-exploit-pocs, including additional videos showcasing the exploits.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45348183</guid><pubDate>Tue, 23 Sep 2025 15:09:50 +0000</pubDate></item><item><title>Shopify, pulling strings at Ruby Central, forces Bundler and RubyGems takeover</title><link>https://joel.drapper.me/p/rubygems-takeover/</link><description>&lt;doc fingerprint="8f13309c118bf6fe"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Shopify, pulling strings at Ruby Central, forces Bundler and RubyGems takeover&lt;/head&gt;
    &lt;p&gt;Ruby Central recently took over a collection of open source projects from their maintainers without their consent. News of the takeover was first broken by Ellen on 19 September.&lt;/p&gt;
    &lt;p&gt;I have spoken to about a dozen people directly involved in the events, and seen a recording of a key meeting between Ruby Gems maintainers and Ruby Central, to uncover what went on.&lt;/p&gt;
    &lt;p&gt;Here’s a quick summary of what I know:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Ruby Central was struggling for money.&lt;/item&gt;
      &lt;item&gt;Sidekiq withdrew its $250,000/year sponsorship for Ruby Central because they platformed DHH at RailsConf 2025.&lt;/item&gt;
      &lt;item&gt;Shopify demanded that Ruby Central take full control of the RubyGems GitHub repositories and the &lt;code&gt;bundler&lt;/code&gt;and&lt;code&gt;rubygems-update&lt;/code&gt;gems, threatening to withdraw funding if Ruby Central did not comply.&lt;/item&gt;
      &lt;item&gt;HSBT jumped the gun and implemented the takeover plan adding Marty Haught as an owner and reducing maintainers permissions before Marty had discussed this with the maintainers.&lt;/item&gt;
      &lt;item&gt;Marty met with the maintainers after their access was temporarily restored.&lt;/item&gt;
      &lt;item&gt;Marty (and by extension, Ruby Central) understood that Ruby Central did not have the right to take over these GitHub repositories or gems from their long established community maintainers.&lt;/item&gt;
      &lt;item&gt;Marty presented alternatives such as making a fork of the relevant RubyGems projects and warned Ruby Central of the consequences of doing the takeover.&lt;/item&gt;
      &lt;item&gt;The board voted to execute the takeover anyway and Marty executed it immediately.&lt;/item&gt;
      &lt;item&gt;A number of board members subsequently misrepresented the takeover to the Ruby community on social media.&lt;/item&gt;
      &lt;item&gt;This was premeditated. Shopify had organised an on-call rotation to take over from the previous maintainers, some of which at the time were also operating the RubyGems Service.&lt;/item&gt;
      &lt;item&gt;Shopify specifically demanded that at least one of the RubyGems maintainers, André Arko, be excluded from returning to the project. André has been working on RubyGems for over a decade and was also one of the founders of Ruby Together, an organization that merged with Ruby Central.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The Takeover&lt;/head&gt;
    &lt;p&gt;On 9 September, HSBT (Hiroshi Shibata) — a member of Ruby core and maintainer of RubyGems — renamed the RubyGems GitHub enterprise to “Ruby Central”, added a new owner, Marty Haught, and downgraded the permissions of several other maintainers.&lt;/p&gt;
    &lt;p&gt;According to one of the maintainers, when HSBT was challenged, he refused to revert these changes claiming he needed permission from Marty. On 15 September, Marty said the changes were a mistake and HSBT reverted some of the changes. However, Marty was not removed as an owner, even though the other maintainers never agreed to him being added.&lt;/p&gt;
    &lt;p&gt;On 17 September, RubyGems maintainers met with Marty on Zoom.&lt;/p&gt;
    &lt;p&gt;Marty explained he’s been working on “operational planning” for the RubyGems Service. He was putting together a new Operator Agreement that all the operators of the RubyGems Service would need to sign.&lt;/p&gt;
    &lt;p&gt;He also mentioned that it had been identified as a risk that there were external individuals with ownership permissions over repositories that are necessary for running the RubyGems Service. He said HSBT prematurely changed the ownership permissions before the operational plan was complete.&lt;/p&gt;
    &lt;p&gt;During the discussion, the maintainers clarified with Marty the distinction between the RubyGems source code and the RubyGems Service.&lt;/p&gt;
    &lt;p&gt;RubyGems is a collection of community owned, community maintained repositories of code that are held in commons for everyone in the Ruby community to use.&lt;/p&gt;
    &lt;p&gt;The RubyGems Service is entirely separate from that. It’s a specific deployment: a domain name and servers that happen to be running RubyGems source code. It is operated by Ruby Central.&lt;/p&gt;
    &lt;p&gt;This distinction is important. Anyone else could run the RubyGems source code on their own servers with their own domains. And Ruby Central could decide to run different source code on its servers — whether that be a fork of the RubyGems source code or otherwise.&lt;/p&gt;
    &lt;p&gt;The RubyGems maintainers have been developing this software for decades, predating Ruby Central’s operation of the RubyGems Service. Their contributions represent countless hours of unpaid work, establishing a clear history of community ownership and stewardship.&lt;/p&gt;
    &lt;p&gt;Ruby Central did contribute financially towards RubyGems maintenance, but these contributions did not confer ownership. Ruby Central’s funding of RubyGems development is no different than if they had contributed to the development of Rails, RSpec, or any other open source project. In no case would such funding grant them ownership rights over the project itself.&lt;/p&gt;
    &lt;p&gt;Similarly, Ruby Central’s employment of some RubyGems maintainers to operate the RubyGems Service does not transfer ownership of the separate open source projects.&lt;/p&gt;
    &lt;p&gt;Having personally reviewed a recording of this meeting, I have no doubt that Marty understood this distinction. The RubyGems source code and GitHub organisation was not owned by Ruby Central, even though Ruby Central operated a service with the same name.&lt;/p&gt;
    &lt;p&gt;On 18 September, the team started losing access again. This time they were removed from the GitHub organisation, their &lt;code&gt;rubygems.org&lt;/code&gt; email accounts were disabled and they were removed as owners of the &lt;code&gt;bundler&lt;/code&gt; and &lt;code&gt;rubygems-update&lt;/code&gt; gems. One maintainer, André Arko, was on-call for the RubyGems Service at the time when his access to GitHub and Fastly was revoked.&lt;/p&gt;
    &lt;p&gt;The Ruby Central board had voted for Ruby Central to take control of the RubyGems GitHub repositories and gems. And since Marty was now an owner, he was able to execute this order.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ruby Central becomes mostly dependent on Shopify&lt;/head&gt;
    &lt;p&gt;When Ruby Central decided to platform DHH at the final RailsConf, they lost $250,000 USD of annual sponsorship from Sidekiq, and this I understand left them almost entirely dependent on Shopify.&lt;/p&gt;
    &lt;p&gt;An anonymous source told me that during Rails World, members of Ruby Central, Ruby Core, Rails Core and representatives from major companies (Shopify, GitHub) discussed possible funding options.&lt;/p&gt;
    &lt;p&gt;According to this source, Ruby Central was presented with a proposal for long-term support, but this would only happen if certain RubyGems maintainers were removed.&lt;/p&gt;
    &lt;p&gt;Another source has confirmed to me that a meeting between Rails Foundation and Ruby Central did take place at Rails World, however they were not able to verify the agenda or who was in attendance.&lt;/p&gt;
    &lt;p&gt;I do know that the Rails World conference was attended by HSBT, DHH, Aaron Patterson, Amanda Perino, Shan Cureton, Marty Haught, Ufuk Kayserilioglu and Rafael França.&lt;/p&gt;
    &lt;p&gt;I also know that Shopify specifically put immense financial pressure on Ruby Central to take full control of the RubyGems GitHub organisation and Ruby gems.&lt;/p&gt;
    &lt;p&gt;Freedom Dumlao, a Ruby Central board member, described the board vote saying “if I had voted the other way, I felt I’d be voting to start the process of shutting down Ruby Central”.&lt;/p&gt;
    &lt;p&gt;A source familiar with the events told me that Shopify’s pressure was both carrot and stick. Essentially, do what we ask and we’ll reward you with more funding, long-term financial stability. Don’t do this and you’ll never see a dollar of enterprise money again.&lt;/p&gt;
    &lt;p&gt;This to me strongly suggests that other companies were involved, perhaps through the Rails Foundation. But I have not been able to confirm anything beyond Shopify’s involvement.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Vote&lt;/head&gt;
    &lt;p&gt;According to a source familiar with the events, the Ruby Central board was made aware by Marty of the risks and damage this takeover would likely do to the community. Apparently he also highlighted other options besides the takeover, such as forking some of the projects.&lt;/p&gt;
    &lt;p&gt;Despite this, the board voted in favour of carrying out the takeover and Marty executed it immediately with his new owner privileges.&lt;/p&gt;
    &lt;p&gt;Shopify had given Ruby Central a hard deadline and it seems that Ruby Central only capitulated at the last moment.&lt;/p&gt;
    &lt;p&gt;I don’t know if the timing was intentional, but this takeover happened on the second day of the EuRuKo conference in Europe, which meant many outspoken European Rubyists were distracted at the time.&lt;/p&gt;
    &lt;p&gt;Because this takeover meant locking out most of the RubyGems Service operators including André who was on-call at the time, Shopify had contributed engineers to a new on-call rotation ready to spring into action after the takeover.&lt;/p&gt;
    &lt;p&gt;Shopify developers had been warming up with their first commits in six years coming in at the same time as the takeover.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Response&lt;/head&gt;
    &lt;p&gt;About six hours after Ellen broke the news, Ruby Central published their response: Strengthening the Stewardship of RubyGems and Bundler.&lt;/p&gt;
    &lt;p&gt;A post that feels like AI-generated corporate speak and bears no signature from anyone at Ruby Central willing to take responsibility.&lt;/p&gt;
    &lt;p&gt;The response says, “To strengthen supply chain security, we are taking important steps to ensure that administrative access to the RubyGems.org, RubyGems, and Bundler is securely managed. This includes both our production systems and GitHub repositories. In the near term we will temporarily hold administrative access to these projects while we finalize new policies that limit commit and organization access rights. This decision was made and approved by the Ruby Central Board as part of our fiduciary responsibility.”&lt;/p&gt;
    &lt;p&gt;But while Ruby Central has the right to lock down the RubyGems Service infrastructure, it never owned the RubyGems GitHub repositories.&lt;/p&gt;
    &lt;p&gt;DHH ignored Ellen’s post but instead retweeted the Ruby Central announcement with the caption “Ruby Central is making the right moves to ensure the Ruby supply chain is beyond reproach both technically and organisationally.”&lt;/p&gt;
    &lt;p&gt;A position that seems to stand in stark contrast to his other opinions. For example, he criticised Apple’s control of the App Store and takes the ownership of his own open source projects seriously.&lt;/p&gt;
    &lt;p&gt;Ruby Central board member and Shopify employee Ufuk Kayserilioglu misrepresented what happened, responding to Bluesky threads. For example he said, “Ruby Central has been running the rubygems.org system for years now, so this can hardly be considered a supply chain attack. On the contrary, we have a legal obligation to all the users of the system to keep it safe and secure.”&lt;/p&gt;
    &lt;p&gt;But no one accused Ruby Central of taking over the RubyGems Service and the takeover of the RubyGems GitHub organization and gems was not required to meet Ruby Central’s legal obligations. Remember, Ruby Central was in full control of what source code it deployed to the RubyGems Service which it operated.&lt;/p&gt;
    &lt;p&gt;He also said “How is limiting access to critical and shared infra &amp;amp; code a supply chain attack?” once again conflating the RubyGems source code with the RubyGems Service.&lt;/p&gt;
    &lt;p&gt;On 21 September, Freedom Dumlao published A board member’s perspective of the RubyGems controversy in which he claimed “Ruby Central has been responsible for RubyGems and Bundler for a long time. This isn’t a new development, and I’m honestly very confused about the confusion.”&lt;/p&gt;
    &lt;p&gt;This is a misrepresentation of the real situation where Ruby Central was responsible for operating the RubyGems Service but did not own the RubyGems source code, repositories or gems.&lt;/p&gt;
    &lt;p&gt;He goes on to talk about supply chain attacks, which I admit is a convenient cover, but I don’t believe is the genuine reason for the takeover.&lt;/p&gt;
    &lt;p&gt;He then confirms that a deadline loomed. “Either Ruby Central puts controls in place to ensure the safety and stability of the infrastructure we are responsible for, or lose the funding that we use to keep those things online and going. With less than 24 hours to go, we were still working on this. Conversations with some maintainers were still happening as far as I know but the cooperation we were hoping for was not emerging.”&lt;/p&gt;
    &lt;p&gt;He doesn’t mention Shopify, but based on my other sources, I know it was Shopify that applied this pressure.&lt;/p&gt;
    &lt;p&gt;“It was clear that we weren’t quite ready yet, but in the end we were out of time. A vote had to be cast so we could ensure we did not lose funding necessary to operate RubyGems. What I voted for, was to direct Marty, Ruby Central’s Director of Open Source, to temporarily remove access and lock down the systems, get operator agreements in place with maintainers, and then re-enable access to those folks who needed and wanted it. Marty did exactly what the board asked of him.”&lt;/p&gt;
    &lt;p&gt;This again highlights the pressure Shopify put on Ruby Central.&lt;/p&gt;
    &lt;p&gt;Two sources directly involved told me that access specifically would not be re-enabled for André who had been singled out. Sources have also suggested that Shopify had been pressuring Ruby Central to end their relationship with André and remove him from the RubyGems project for some period of time prior to this taking place.&lt;/p&gt;
    &lt;p&gt;On 23 September, Ruby Central shared a video address by Shan Cureton (Executive Director, Ruby Central) on behalf of Ruby Central’s board and team.&lt;/p&gt;
    &lt;p&gt;In it she claims that Bundler and RubyGems came under Ruby Central’s responsibility through the merger with Ruby Together. But Ruby Together never owned Bundler or RubyGems.&lt;/p&gt;
    &lt;p&gt;She mentioned the departure of a “lead maintainer” [André] and transition of security engineer [Samuel Giddins] as raising questions around access to RubyGems, Bundler and the RubyGems Service.&lt;/p&gt;
    &lt;p&gt;She says sponsors (plural) and companies who depend on Ruby tooling came to them with supply chain concerns. She explained that they couldn’t reach agreement with existing maintainers in the timelines they were facing.&lt;/p&gt;
    &lt;p&gt;I have seen the meeting with the maintainers and can tell you the conversation was primarily about ownership, not security. None of the maintainers had a problem with Ruby Central restricting access to the RubyGems Service that it operated.&lt;/p&gt;
    &lt;p&gt;They had a problem with Ruby Central taking control of the RubyGems open source code repositories and gems, which Ruby Central never owned.&lt;/p&gt;
    &lt;p&gt;She explains that the board voted to remove administrative and commit privileges until agreements could be put in place. She said it was never meant to be permanent.&lt;/p&gt;
    &lt;p&gt;She said “this is not a shutdown of community contribution and it’s not permanent”. However, my sources tell me this will be permanent for at least André and likely Samuel.&lt;/p&gt;
    &lt;p&gt;She said on-call coverage remains in place. We know that André was on-call when his access was revoked, so she must be talking about the new on-call rotation which Shopify contributed to.&lt;/p&gt;
    &lt;p&gt;She said “all of these changes are being made in good faith.” But we know that these changes were made at Shopify’s request to take control of the RubyGems projects and specifically to exclude André (and likely Samuel too).&lt;/p&gt;
    &lt;p&gt;She also talked about two new agreements: Operator Agreements cover access to production systems for on-call and maintenance responsibilities. Contributor Agreements cover access to Bundler and RubyGem code repositories, covering both paid and volunteer maintainers.&lt;/p&gt;
    &lt;p&gt;The Operator Agreements make sense, but it is not Ruby Central’s place to run the RubyGems projects including Bundler and the RubyGems.org source code, which are community owned as explained previously.&lt;/p&gt;
    &lt;p&gt;She said, “in most open source projects where the code is a library or framework, you usually don’t see formal operator agreements. People contribute under contributor license agreements, codes of conduct or decisions made by a steering committee. But RubyGems.org is different. It’s not just code, it’s a production service. It runs critical infrastructure for the Ruby ecosystem, processes billions of downloads, stores sensitive metadata and is relied on by companies that have compliance requirements. Because it’s a service, Ruby Central carries the legal liability, the financial exposure and the operational risk. This is why Operator Agreements are necessary. They ensure access is tied to responsibility and accountability.”&lt;/p&gt;
    &lt;p&gt;Here she conflates RubyGems.org (the source code) with the RubyGems Service operated by Ruby Central and running on the domain name &lt;code&gt;rubygems.org&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Claiming that Ruby Central owns the RubyGems.org repository because it operates a service that uses the source code is like claiming you own Rails because you have a Rails app and sponsored someone who contributed a PR to the project.&lt;/p&gt;
    &lt;p&gt;It’s confusing because of how the projects are named, and Ruby Central are taking advantage of that confusion.&lt;/p&gt;
    &lt;p&gt;The reality is Ruby Central never owned the Ruby Gems source code. They could only take it because Marty was added by HSBT without the consent of other maintainers.&lt;/p&gt;
    &lt;head rend="h2"&gt;RV&lt;/head&gt;
    &lt;p&gt;An important piece of context is that André and Samuel started a new cooperative with Kasper Timm Hansen and Sam Stephenson called Spinel.&lt;/p&gt;
    &lt;p&gt;Spinel is developing a new Ruby management tool called &lt;code&gt;rv&lt;/code&gt;. It was introduced on 25 August 2025, right before Rails World.&lt;/p&gt;
    &lt;p&gt;In his blog post, André says, “For the last ten years or so of working on Bundler, I’ve had a wish rattling around: I want a better dependency manager. It doesn’t just manage your gems, it manages your ruby versions, too. It doesn’t just manage your ruby versions, it installs pre-compiled rubies so you don’t have to wait for ruby to compile from source every time. And more than all of that, it makes it completely trivial to run any script or tool written in ruby, even if that script or tool needs a different ruby than your application does.”&lt;/p&gt;
    &lt;p&gt;Bluesky threads reveal that Rafael França (Shopify / Rails Core) saw this tool as a threat, saying “some of the “admins” even announced publicly many days ago they were launching a competitor tool [rv] and were funding raising for it. I’d not trust the system to such “admin”.”&lt;/p&gt;
    &lt;p&gt;He also quoted the &lt;code&gt;rv&lt;/code&gt; README which says, “Get rid of rvm, rbenv, chruby, asdf, mise, ruby-build, ruby-install, bundler, and rubygems, all at once”, adding the caption “I’m not so sure I trust them to not sabotage rubygems or bundler.”&lt;/p&gt;
    &lt;head rend="h2"&gt;What I don’t know&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I don’t know how each member voted or exactly how the information was presented to the board. I was hoping that someone would leak it to me, but so far that has not happened.&lt;/item&gt;
      &lt;item&gt;I don’t know if other groups or companies were involved, though circumstantial evidence and hearsay seems to point to this.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you have any information you can provide, please contact me on Signal.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;It is not clear that Ruby Central’s plans include returning control of the RubyGems codebases to their original owners.&lt;/p&gt;
    &lt;p&gt;I am concerned that Ruby Central seems to be vulnerable to coercion by Shopify.&lt;/p&gt;
    &lt;p&gt;I am concerned that Ruby Central’s board with full knowledge of the consequences and the alternatives voted to take over a collection of open source projects from their maintainers without consent. Especially when these maintainers were acting in good faith at the time. This is the organisation we are meant to trust to host our Ruby gems.&lt;/p&gt;
    &lt;p&gt;I am concerned that Rails Core seems to consider &lt;code&gt;rv&lt;/code&gt; a “threat” rather than an exciting development, and I wonder if the “threat” is more Spinel than &lt;code&gt;rv&lt;/code&gt;. It seems likely that Spinel would be less susceptible to enterprise coercion and could offer a genuine alternative to RubyCentral’s RubyGems Service.&lt;/p&gt;
    &lt;head rend="h3"&gt;Disclosure&lt;/head&gt;
    &lt;p&gt;I was employed by Shopify between 2017 and 2022.&lt;/p&gt;
    &lt;head rend="h3"&gt;Disclaimer&lt;/head&gt;
    &lt;p&gt;I have put this story together to the best of my ability based on hours of conversations with many different people involved. But I am not a professional journalist and I may have missed something. If I have made a mistake, please let me know.&lt;/p&gt;
    &lt;p&gt;I am willing to talk to anyone involved to make sure the community has a fair and honest understanding of the events that took place.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45348390</guid><pubDate>Tue, 23 Sep 2025 15:25:29 +0000</pubDate></item><item><title>Are elites meritocratic and efficiency-seeking? Evidence from MBA students</title><link>https://arxiv.org/abs/2503.15443</link><description>&lt;doc fingerprint="fcbc340a51d3308e"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Economics &amp;gt; General Economics&lt;/head&gt;&lt;p&gt; [Submitted on 19 Mar 2025 (v1), last revised 20 Sep 2025 (this version, v4)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Are Elites Meritocratic and Efficiency-Seeking? Evidence from MBA Students&lt;/head&gt;View PDF&lt;quote&gt;Abstract:Elites disproportionately influence policymaking, yet little is known about their fairness and efficiency preferences -- key determinants of support for redistributive policies. We investigate these preferences in an incentivized lab experiment with a group of future elites -- Ivy League MBA students. We find that MBA students implement substantially more unequal earnings distributions than the average American, regardless of whether inequality stems from luck or merit. Their redistributive choices are also highly responsive to efficiency costs, with an effect that is an order of magnitude larger than that found in representative U.S. samples. Analyzing fairness ideals, we find that MBA students are less likely to be strict meritocrats than the broader population. These findings provide novel insights into how elites' redistributive preferences may shape high levels of inequality in the U.S.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: German Reyes [view email]&lt;p&gt;[v1] Wed, 19 Mar 2025 17:22:25 UTC (2,540 KB)&lt;/p&gt;&lt;p&gt;[v2] Sat, 24 May 2025 18:39:21 UTC (2,116 KB)&lt;/p&gt;&lt;p&gt;[v3] Sun, 20 Jul 2025 22:22:13 UTC (2,728 KB)&lt;/p&gt;&lt;p&gt;[v4] Sat, 20 Sep 2025 22:18:54 UTC (2,112 KB)&lt;/p&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;econ.GN&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45348421</guid><pubDate>Tue, 23 Sep 2025 15:28:03 +0000</pubDate></item><item><title>Show HN: FlyCode – Recover Stripe payments by automatically using backup cards</title><link>https://news.ycombinator.com/item?id=45348730</link><description>&lt;doc fingerprint="3e5887de200fe1ed"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;We built FlyCode after seeing subscription businesses lose ~35% of recurring revenue each year to failed payments — even when customers had other valid cards on file.&lt;/p&gt;
      &lt;p&gt;*The problem:* When a customer's primary card fails, Stripe retries a few times then cancels the subscription. If that customer has a backup card, it isn’t tried. At least 20% of active customers have more than one card on file, which means a lot of preventable churn.&lt;/p&gt;
      &lt;p&gt;*Our solution:* FlyCode automatically identifies if a customer has other valid cards on file and retries them when a subscription payment fails. You can configure when these retries happen during the dunning period (beginning, middle, end) and define validity rules (e.g. “card was used in last 180 days”). It’s a Stripe app — no code changes needed.&lt;/p&gt;
      &lt;p&gt;We've seen 18%-20% higher recovery rates from our core retry engine, plus another 5–10% from using backup cards. Importantly, there was no increase in refunds or chargebacks — in fact, rates were lower than merchant averages. Big companies like Microsoft and Amazon already do this internally; we wanted to make the same capability accessible to smaller SaaS teams.&lt;/p&gt;
      &lt;p&gt;*Under the hood:* FlyCode monitors for failed invoices, checks for available backup methods via Stripe’s PaymentMethod API, and systematically retries in a way that avoids service disruption or manual workflows.&lt;/p&gt;
      &lt;p&gt;We’re Jake, Etai, and Tzachi — we previously built payment recovery systems at startups and enterprises, which is how we discovered this gap.&lt;/p&gt;
      &lt;p&gt;You can try it here: [https://www.flycode.com/stripe]&lt;/p&gt;
      &lt;p&gt;We’d love feedback from anyone dealing with subscription payment failures. What’s been your experience with involuntary churn? Have you considered leveraging backup payment methods?&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45348730</guid><pubDate>Tue, 23 Sep 2025 15:50:10 +0000</pubDate></item><item><title>U.S. Secret Service disrupts telecom network that threatened NYC during U.N</title><link>https://www.cbsnews.com/news/u-s-secret-service-disrupts-telecom-network-threatened-new-york-city-u-n-general-assembly/</link><description>&lt;doc fingerprint="d4248189e8b92c2e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;U.S. Secret Service disrupts telecom network that threatened NYC during U.N. General Assembly&lt;/head&gt;
    &lt;p&gt;The Secret Service has disrupted a sprawling telecommunications network in the New York tri-state area that investigators say posed a serious potential disruption to New York's telecom systems and a possible threat to the United Nations General Assembly meetings this week.&lt;/p&gt;
    &lt;p&gt;In the largest seizure of its kind, the U.S. Secret Service announced Tuesday that the agency found active SIM farms at abandoned apartment buildings located at more than five sites. In total, law enforcement discovered 300 SIM servers – over 100,000 SIM cards – enabling encrypted, anonymous communication and capable of sending 30 million text messages per minute. Officials say the servers were so powerful they could have disabled cell phone towers and launched distributed denial of services attacks with the ability to block emergency communications like EMS and police dispatch.&lt;/p&gt;
    &lt;p&gt;"This network had the potential to disable cell phone towers and essentially shut down the cellular network in New York City," U.S. Secret Service Special Agent in Charge Matt McCool said in a video released by the agency.&lt;/p&gt;
    &lt;p&gt;An official briefed on the investigation told reporters that this week, the sophisticated network "could text message the entire country within 12 minutes," later adding, "This was well organized and well funded."&lt;/p&gt;
    &lt;p&gt;Telephonic threats to multiple senior U.S. officials this past spring – including multiple people protected by the Secret Service – first triggered the investigation, but officials say the network was seized within the last three weeks.&lt;/p&gt;
    &lt;p&gt;"We cannot share which officials were targeted out of concerns for their privacy, but as the forensics investigation continues, we do expect that we will find more targeted officials once we get through that data," McCool said.&lt;/p&gt;
    &lt;p&gt;Early analysis shows the network was used for communication between foreign governments and individuals known to U.S. law enforcement, including members of known organized crime gangs, drug cartels and human trafficking rings, according to multiple officials briefed on the investigation. The U.S. Secret Service says it is combing through the more than 100,000 SIM cards in an ongoing, exhaustive forensic analysis.&lt;/p&gt;
    &lt;p&gt;"Each SIM basically has the equivalent data of a cell phone. So we're working through every call, every text, every search made on those SIM cards," an official told CBS News, adding, "Early analysis indicates that this network was used for communication between foreign governments and individuals that are known to federal law enforcement here in the U.S."&lt;/p&gt;
    &lt;p&gt;The equipment was found within 35 miles of the United Nations in New York, ahead of the U.N. General Assembly. Investigators also found 80 grams of cocaine, illegal firearms, plus computers and phones.&lt;/p&gt;
    &lt;p&gt;"This isn't a group of people in a basement playing a video game and trying to play a prank," one official said. "This was well organized and well funded."&lt;/p&gt;
    &lt;p&gt;"The timing, the location, the proximity of this network had the potential to impact the U.N. and that was clear and something that we had to consider," added McCool.&lt;/p&gt;
    &lt;p&gt;The investigation was launched by a new division within the U.S. Secret Service established by Director Sean Curran and dubbed the "Advanced Threat Interdiction Unit," in conjunction with Homeland Security Investigations.&lt;/p&gt;
    &lt;p&gt;"These devices no longer pose any threat to New York," an official said. "We've taken care of and dismantled that threat…There is currently no credible threat against the UN."&lt;/p&gt;
    &lt;p&gt;Still, another official added that "it would be unwise to assume" there aren't other such networks in the U.S.&lt;/p&gt;
    &lt;p&gt;The investigation remains ongoing, according to the U.S. Secret Service. There have been no arrests yet, but officials said, "there could be arrests down the road," adding that "from an operational perspective, we want those behind the network to know that the Secret Service is aware and that we're kind of coming for them."&lt;/p&gt;
    &lt;p&gt;Homeland Security Investigations is leading the criminal investigation into individuals involved in coordinating this large scheme, while the U.S. Secret Service is running down threats tied back to its protectees, according to multiple U.S. officials.&lt;/p&gt;
    &lt;p&gt;In a statement released by the U.S. Secret Service, the agency also thanked the Department of Justice, the Office of the Director of National Intelligence and the NYPD, as well as other state and local law enforcement partners, for providing technical advice and assistance.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45349231</guid><pubDate>Tue, 23 Sep 2025 16:25:46 +0000</pubDate></item></channel></rss>