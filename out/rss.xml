<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 26 Sep 2025 16:11:54 +0000</lastBuildDate><item><title>Investigating a Forged PDF</title><link>https://mjg59.dreamwidth.org/73317.html</link><description>&lt;doc fingerprint="7096a97f0e007743"&gt;
  &lt;main&gt;&lt;head rend="h3"&gt;Investigating a forged PDF&lt;/head&gt;Sep. 24th, 2025 12:24 pm&lt;p&gt; mjg59&lt;/p&gt;&lt;p&gt;I had to rent a house for a couple of months recently, which is long enough in California that it pushes you into proper tenant protection law. As landlords tend to do, they failed to return my security deposit within the 21 days required by law, having already failed to provide the required notification that I was entitled to an inspection before moving out. Cue some tedious argumentation with the letting agency, and eventually me threatening to take them to small claims court.&lt;lb/&gt;This post is not about that.&lt;lb/&gt;Now, under Californian law, the onus is on the landlord to hold and return the security deposit - the agency has no role in this. The only reason I was talking to them is that my lease didn't mention the name or address of the landlord (another legal violation, but the outcome is just that you get to serve the landlord via the agency). So it was a bit surprising when I received an email from the owner of the agency informing me that they did not hold the deposit and so were not liable - I already knew this.&lt;lb/&gt;The odd bit about this, though, is that they sent me another copy of the contract, asserting that it made it clear that the landlord held the deposit. I read it, and instead found a clause reading&lt;lb/&gt;Ok, fair enough, there's an addendum that says the landlord has it (I've removed the landlord's name, it's present in the original).&lt;lb/&gt;Except. I had no recollection of that addendum. I went back to the copy of the contract I had and discovered:&lt;lb/&gt;Huh! But obviously I could just have edited that to remove it (there's no obvious reason for me to, but whatever), and then it'd be my word against theirs. However, I'd been sent the document via RightSignature, an online document signing platform, and they'd added a certification page that looked like this:&lt;lb/&gt;Interestingly, the certificate page was identical in both documents, including the checksums, despite the content being different. So, how do I show which one is legitimate? You'd think given this certificate page this would be trivial, but RightSignature provides no documented mechanism whatsoever for anyone to verify any of the fields in the certificate, which is annoying but let's see what we can do anyway.&lt;lb/&gt;First up, let's look at the PDF metadata. pdftk has a dump_data command that dumps the metadata in the document, including the creation date and the modification date. My file had both set to identical timestamps in June, both listed in UTC, corresponding to the time I'd signed the document. The file containing the addendum? The same creation time, but a modification time of this Monday, shortly before it was sent to me. This time, the modification timestamp was in Pacific Daylight Time, the timezone currently observed in California. In addition, the data included two ID fields, ID0 and ID1. In my document both were identical, in the one with the addendum ID0 matched mine but ID1 was different.&lt;lb/&gt;These ID tags are intended to be some form of representation (such as a hash) of the document. ID0 is set when the document is created and should not be modified afterwards - ID1 initially identical to ID0, but changes when the document is modified. This is intended to allow tooling to identify whether two documents are modified versions of the same document. The identical ID0 indicated that the document with the addendum was originally identical to mine, and the different ID1 that it had been modified.&lt;lb/&gt;Well, ok, that seems like a pretty strong demonstration. I had the "I have a very particular set of skills" conversation with the agency and pointed these facts out, that they were an extremely strong indication that my copy was authentic and their one wasn't, and they responded that the document was "re-sealed" every time it was downloaded from RightSignature and that would explain the modifications. This doesn't seem plausible, but it's an argument. Let's go further.&lt;lb/&gt;My next move was pdfalyzer, which allows you to pull a PDF apart into its component pieces. This revealed that the documents were identical, other than page 3, the one with the addendum. This page included tags entitled "touchUp_TextEdit", evidence that the page had been modified using Acrobat. But in itself, that doesn't prove anything - obviously it had been edited at some point to insert the landlord's name, it doesn't prove whether it happened before or after the signing.&lt;lb/&gt;But in the process of editing, Acrobat appeared to have renamed all the font references on that page into a different format. Every other page had a consistent naming scheme for the fonts, and they matched the scheme in the page 3 I had. Again, that doesn't tell us whether the renaming happened before or after the signing. Or does it?&lt;lb/&gt;You see, when I completed my signing, RightSignature inserted my name into the document, and did so using a font that wasn't otherwise present in the document (Courier, in this case). That font was named identically throughout the document, except on page 3, where it was named in the same manner as every other font that Acrobat had renamed. Given the font wasn't present in the document until after I'd signed it, this is proof that the page was edited after signing.&lt;lb/&gt;But eh this is all very convoluted. Surely there's an easier way? Thankfully yes, although I hate it. RightSignature had sent me a link to view my signed copy of the document. When I went there it presented it to me as the original PDF with my signature overlaid on top. Hitting F12 gave me the network tab, and I could see a reference to a base.pdf. Downloading that gave me the original PDF, pre-signature. Running sha256sum on it gave me an identical hash to the "Original checksum" field. Needless to say, it did not contain the addendum.&lt;lb/&gt;Why do this? The only explanation I can come up with (and I am obviously guessing here, I may be incorrect!) is that International Executive Rentals realised that they'd sent me a contract which could mean that they were liable for the return of my deposit, even though they'd already given it to my landlord, and after realising this added the addendum, sent it to me, and assumed that I just wouldn't notice (or that, if I did, I wouldn't be able to prove anything). In the process they went from an extremely unlikely possibility of having civil liability for a few thousand dollars (even if they were holding the deposit it's still the landlord's legal duty to return it, as far as I can tell) to doing something that looks extremely like forgery.&lt;lb/&gt;There's a hilarious followup. After this happened, the agency offered to do a screenshare with me showing them logging into RightSignature and showing the signed file with the addendum, and then proceeded to do so. One minor problem - the "Send for signature" button was still there, just below a field saying "Uploaded: 09/22/25". I asked them to search for my name, and it popped up two hits - one marked draft, one marked completed. The one marked completed? Didn't contain the addendum.&lt;/p&gt;&lt;p&gt;This post is not about that.&lt;/p&gt;&lt;p&gt;Now, under Californian law, the onus is on the landlord to hold and return the security deposit - the agency has no role in this. The only reason I was talking to them is that my lease didn't mention the name or address of the landlord (another legal violation, but the outcome is just that you get to serve the landlord via the agency). So it was a bit surprising when I received an email from the owner of the agency informing me that they did not hold the deposit and so were not liable - I already knew this.&lt;/p&gt;&lt;p&gt;The odd bit about this, though, is that they sent me another copy of the contract, asserting that it made it clear that the landlord held the deposit. I read it, and instead found a clause reading&lt;/p&gt;&lt;quote&gt;SECURITY: The security deposit will secure the performance of Tenant’s obligations. IER may, but will not be obligated to, apply all portions of said deposit on account of Tenant’s obligations. Any balance remaining upon termination will be returned to Tenant. Tenant will not have the right to apply the security deposit in payment of the last month’s rent. Security deposit held at IER Trust Account., where IER is International Executive Rentals, the agency in question. Why send me a contract that says you hold the money while you're telling me you don't? And then I read further down and found this:&lt;/quote&gt;&lt;p&gt;Ok, fair enough, there's an addendum that says the landlord has it (I've removed the landlord's name, it's present in the original).&lt;/p&gt;&lt;p&gt;Except. I had no recollection of that addendum. I went back to the copy of the contract I had and discovered:&lt;/p&gt;&lt;p&gt;Huh! But obviously I could just have edited that to remove it (there's no obvious reason for me to, but whatever), and then it'd be my word against theirs. However, I'd been sent the document via RightSignature, an online document signing platform, and they'd added a certification page that looked like this:&lt;/p&gt;&lt;p&gt;Interestingly, the certificate page was identical in both documents, including the checksums, despite the content being different. So, how do I show which one is legitimate? You'd think given this certificate page this would be trivial, but RightSignature provides no documented mechanism whatsoever for anyone to verify any of the fields in the certificate, which is annoying but let's see what we can do anyway.&lt;/p&gt;&lt;p&gt;First up, let's look at the PDF metadata. pdftk has a dump_data command that dumps the metadata in the document, including the creation date and the modification date. My file had both set to identical timestamps in June, both listed in UTC, corresponding to the time I'd signed the document. The file containing the addendum? The same creation time, but a modification time of this Monday, shortly before it was sent to me. This time, the modification timestamp was in Pacific Daylight Time, the timezone currently observed in California. In addition, the data included two ID fields, ID0 and ID1. In my document both were identical, in the one with the addendum ID0 matched mine but ID1 was different.&lt;/p&gt;&lt;p&gt;These ID tags are intended to be some form of representation (such as a hash) of the document. ID0 is set when the document is created and should not be modified afterwards - ID1 initially identical to ID0, but changes when the document is modified. This is intended to allow tooling to identify whether two documents are modified versions of the same document. The identical ID0 indicated that the document with the addendum was originally identical to mine, and the different ID1 that it had been modified.&lt;/p&gt;&lt;p&gt;Well, ok, that seems like a pretty strong demonstration. I had the "I have a very particular set of skills" conversation with the agency and pointed these facts out, that they were an extremely strong indication that my copy was authentic and their one wasn't, and they responded that the document was "re-sealed" every time it was downloaded from RightSignature and that would explain the modifications. This doesn't seem plausible, but it's an argument. Let's go further.&lt;/p&gt;&lt;p&gt;My next move was pdfalyzer, which allows you to pull a PDF apart into its component pieces. This revealed that the documents were identical, other than page 3, the one with the addendum. This page included tags entitled "touchUp_TextEdit", evidence that the page had been modified using Acrobat. But in itself, that doesn't prove anything - obviously it had been edited at some point to insert the landlord's name, it doesn't prove whether it happened before or after the signing.&lt;/p&gt;&lt;p&gt;But in the process of editing, Acrobat appeared to have renamed all the font references on that page into a different format. Every other page had a consistent naming scheme for the fonts, and they matched the scheme in the page 3 I had. Again, that doesn't tell us whether the renaming happened before or after the signing. Or does it?&lt;/p&gt;&lt;p&gt;You see, when I completed my signing, RightSignature inserted my name into the document, and did so using a font that wasn't otherwise present in the document (Courier, in this case). That font was named identically throughout the document, except on page 3, where it was named in the same manner as every other font that Acrobat had renamed. Given the font wasn't present in the document until after I'd signed it, this is proof that the page was edited after signing.&lt;/p&gt;&lt;p&gt;But eh this is all very convoluted. Surely there's an easier way? Thankfully yes, although I hate it. RightSignature had sent me a link to view my signed copy of the document. When I went there it presented it to me as the original PDF with my signature overlaid on top. Hitting F12 gave me the network tab, and I could see a reference to a base.pdf. Downloading that gave me the original PDF, pre-signature. Running sha256sum on it gave me an identical hash to the "Original checksum" field. Needless to say, it did not contain the addendum.&lt;/p&gt;&lt;p&gt;Why do this? The only explanation I can come up with (and I am obviously guessing here, I may be incorrect!) is that International Executive Rentals realised that they'd sent me a contract which could mean that they were liable for the return of my deposit, even though they'd already given it to my landlord, and after realising this added the addendum, sent it to me, and assumed that I just wouldn't notice (or that, if I did, I wouldn't be able to prove anything). In the process they went from an extremely unlikely possibility of having civil liability for a few thousand dollars (even if they were holding the deposit it's still the landlord's legal duty to return it, as far as I can tell) to doing something that looks extremely like forgery.&lt;/p&gt;&lt;p&gt;There's a hilarious followup. After this happened, the agency offered to do a screenshare with me showing them logging into RightSignature and showing the signed file with the addendum, and then proceeded to do so. One minor problem - the "Send for signature" button was still there, just below a field saying "Uploaded: 09/22/25". I asked them to search for my name, and it popped up two hits - one marked draft, one marked completed. The one marked completed? Didn't contain the addendum.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45381010</guid><pubDate>Fri, 26 Sep 2025 00:14:11 +0000</pubDate></item><item><title>My Deus Ex lipsyncing fix mod</title><link>https://www.joewintergreen.com/my-deus-ex-lipsyncing-fix-mod-making-of/</link><description>&lt;doc fingerprint="43251212cb9cd376"&gt;
  &lt;main&gt;
    &lt;p&gt;Back in 2021 I made a mod for Deus Ex 1 that fixes the lipsyncing and blinking, which, I betcha didn’t know, was broken since ship. Everything I wrote about it is on Twitter, and it oughta be somewhere else, so here’s a post about it. The mod itself can be downloaded here.&lt;/p&gt;
    &lt;p&gt;I guess I was playing DX1 and thinking, geez, was this lipsync always this bad? In a weird way? It’s insta-snapping mouth shapes, but they’re not always the same mouth shapes. Is this broken? I couldn’t find anything online about it, but I did find this article: an interview with Chris Norden, a coder on DX, where he goes into the lipsyncing and how it was, at one point, super elaborate and amazing, and they had to pare it back for performance reasons. I thought I’d check how much of this was done in Unrealscript (since the C++ source for DX is nowhere) and whether I could un-pare it. It turns out it was an extremely simple fix to get it as good as I got it, and I think that’s as good as you can get it until someone leaks the source code.&lt;/p&gt;
    &lt;p&gt;I’d messed around with lipsyncing stuff before and was familiar with the broad strokes of how it tends to work via my intense familiarity with Half-Life 2: you figure out, hopefully automatically, the sounds (phonemes) present in a sound file (“oo”, “ah”, whatever) and map those to mouth shapes (visemes), then when the audio plays, move the mouth into the right shape for the phoneme we’re in at this moment. The figuring-out process is called “phoneme extraction”, at least by Valve, and Valve do this offline, because it takes a sec. In Valve’s case they append this phoneme information to the end of the .wav file, and it looks like this:&lt;/p&gt;
    &lt;code&gt;PLAINTEXT
{
Okay, I don't blame you for hesitating, but if we're gonna do this thing, then let's just get through it. 
}
WORDS
{
WORD Okay 0.064 0.224
{
111 ow 0.014 0.096 1.000
107 k 0.096 0.142 1.000
101 ey 0.142 0.220 1.000
}
WORD I 0.224 0.352
{
593 ay 0.220 0.310 1.000
105 iy 0.310 0.364 1.000
}
WORD don't 0.352 0.496
{
100 d 0.364 0.396 1.000
111 ow 0.396 0.456 1.000
110 n 0.456 0.496 1.000
}
&lt;/code&gt;
    &lt;p&gt;, etc. Phonemes, start times, end times. Easy!&lt;/p&gt;
    &lt;p&gt;My assumption is that the reason Deus Ex’s super cool lipsyncing was too expensive to ship was, they don’t seem to save this information anywhere, so I guess they were figuring out the phonemes in realtime. If correct, this is sort of a bummer – doing what Valve did would have scooped the whole cost out. Maybe there was more to it.&lt;/p&gt;
    &lt;p&gt;Anyway, the Unrealscript. Deus Ex is pre-Unreal having skeletal animation, it’s all vertex animation. The character heads have a few: relevant here, 7 visemes and a blink. &lt;code&gt;nextphoneme&lt;/code&gt; is set from somewhere outside this code (probably a cpp audio system I can’t access) to A, E, F, M, O, T or U, which it doesn’t matter which is which and I don’t remember, or X, which is nothing (close mouth). Then this Unrealscript on the character sets the head’s anim sequence to the appropriate pose. This all happens on tick, but only if &lt;code&gt;IsSpeaking&lt;/code&gt;. We have a &lt;code&gt;tweentime&lt;/code&gt; we’re using to blend between these poses, so we should be seeing nice smooth blending, the lack of which is why I’m here in the first place! So what’s the problem?&lt;/p&gt;
    &lt;p&gt;The main thing is a dodgy frame rate check:&lt;/p&gt;
    &lt;code&gt;// update the animation timers that we are using
	animTimer[0] += deltaTime;
	animTimer[1] += deltaTime;
	animTimer[2] += deltaTime;

	if (bIsSpeaking)
	{
		// if our framerate is high enough (&amp;gt;20fps), tween the lips smoothly
		if (Level.TimeSeconds - animTimer[3]  &amp;lt; 0.05)
			tweentime = 0;
		else
			tweentime = 0.1;
&lt;/code&gt;
    &lt;p&gt;“tweentime” is how long it takes to blend to the next viseme in seconds; if 0, it’s an instant snap. The intent here is to skip blending entirely if our framerate is so low that it looks better snapping the lips around than showing any in-between poses, only it doesn’t work. The code is keeping &lt;code&gt;Level.TimeSeconds&lt;/code&gt; from the previous frame and subtracting that from the current &lt;code&gt;Level.TimeSeconds&lt;/code&gt; to get deltatime, which if it’s less than 0.05, we’re assumed to be getting less than 20fps. So it’s flipped.&lt;/p&gt;
    &lt;p&gt;Also, 0.1 is just way too fast a value, which I suspect a reason for that I’ll come back to*. I increased it to 0.35 to make the blends take long enough to really see.&lt;/p&gt;
    &lt;p&gt;With that fixed, the lipsync is smooth! Hooray! But it’s not perfect: at the end of a line, when the audio finishes, we don’t smoothly close the mouth; we snap the mouth shut instantly. This is because we’re only doing any blending if &lt;code&gt;bIsSpeaking=true&lt;/code&gt;, which it suddenly isn’t. The perf hit of this function no longer matters at all, so I just skip that check too: every character always gets to run lipsync. Tweentime is also local to this function and initialises at 0, so I had to set it to 0.3 to get blending even when we have no phoneme.&lt;/p&gt;
    &lt;p&gt;Blinking was also way too fast, so fast as to be invisible, so I slowed it down a ton. Now you can see ’em blinkin’.&lt;/p&gt;
    &lt;p&gt;So now we have nice blinking and smooth mouth movement, but there’s one thing that still sucks: presumably as part of the optimisation that made this ship at all, &lt;code&gt;nextphoneme&lt;/code&gt; does not update every tick, or anywhere near every tick. It doesn’t even update at a fixed rate – sometimes you’ll get a good amount of updates in a sentence, sometimes one or two. This means that all the smooth blending in the world won’t get you a correct result unless you happen to get lucky: JC can be speaking the M in “a bomb” and you’re still back on the “a”. As far as I can tell there’s no way to fix this right now – the code that updates the phonemes just needs to do it every tick, and it don’t, and it’s not Unrealscript so I can’t touch it. If the time between phoneme updates was at least consistent, you could set tweentime to that duration and make your blend take as long as it takes for a new phoneme to show up, but it ain’t. So close!&lt;/p&gt;
    &lt;p&gt;*In the interview where Norden alludes to this amazing lipsync demo they had going on before they optimised it down, I assume it was initially getting a new phoneme every tick, and that is probably when they set 0.1 seconds as a blend duration. If you’re getting constant new phonemes, blending super fast to the next one makes sense; it’s only when you’re not that a slower blend time looks good.&lt;/p&gt;
    &lt;p&gt;There’s a lot of jank to this code. The silliest thing about it might be that it lives in &lt;code&gt;ScriptedPawn&lt;/code&gt;, Deus Ex’s NPC class, which does not share an immediate parent with the player character, so this whole function is just duplicated between the two classes.&lt;/p&gt;
    &lt;p&gt;Anyway, here’s the whole function after I futzed with it.&lt;/p&gt;
    &lt;code&gt;// lip synching support - DEUS_EX CNN
//
function LipSynch(float deltaTime)
{
	local name animseq;
	local float rnd;
	local float tweentime;

	// update the animation timers that we are using
	animTimer[0] += deltaTime;
	animTimer[1] += deltaTime;
	animTimer[2] += deltaTime;

	if (bIsSpeaking)
	{
		// if our framerate is high enough (&amp;gt;20fps), tween the lips smoothly
		
//JOE CHANGE: 
//This used to set tweentime to 0 (no blend) if it thought FPS was low, else 0.1. It was 
//backwards though, the result was the opposite. 
//Even 0.1 is too fast to look good though. Anyway, skip the check, we don't care
//
//		if (Level.TimeSeconds - animTimer[3]  &amp;lt; 0.05)
//			tweentime = 0.4;
//		else
			tweentime = 0.36;

//Also, ideally tweentime would be the duration until the next time we get a phoneme update?
//But I don't know where that update comes from at the moment

		// the last animTimer slot is used to check framerate
		animTimer[3] = Level.TimeSeconds;

		if (nextPhoneme == "A")
			animseq = 'MouthA';
		else if (nextPhoneme == "E")
			animseq = 'MouthE';
		else if (nextPhoneme == "F")
			animseq = 'MouthF';
		else if (nextPhoneme == "M")
			animseq = 'MouthM';
		else if (nextPhoneme == "O")
			animseq = 'MouthO';
		else if (nextPhoneme == "T")
			animseq = 'MouthT';
		else if (nextPhoneme == "U")
			animseq = 'MouthU';
		else if (nextPhoneme == "X")
			animseq = 'MouthClosed';

		if (animseq != '')
		{
					if (lastPhoneme != nextPhoneme)
			{
				lastPhoneme = nextPhoneme;
				TweenBlendAnim(animseq, tweentime);
				TimeLastPhoneme = Level.TimeSeconds;
			}
		
		}
		

//		if ((Level.TimeSeconds - TimeLastPhoneme) &amp;gt;= tweentime*0.8 &amp;amp;&amp;amp; TimeLastPhoneme != 0)
//		{
//		TweenBlendAnim('MouthClosed', 0.2);
//		nextPhoneme = "X";
//		lastPhoneme = "A";
//		TimeLastPhoneme = Level.TimeSeconds;
//		}
	}
	else
	if (bWasSpeaking)
	{
		bWasSpeaking = false;
		
//JOE: I added this tweentime set. Without it it was 0 as initialised, so the jaw snapped shut

		tweentime = 0.3;
		TweenBlendAnim('MouthClosed', tweentime);
	}

	// blink randomly
	if (animTimer[0] &amp;gt; 0.5)
	{
		animTimer[0] = 0;
		if (FRand() &amp;lt; 0.4)
			PlayBlendAnim('Blink', 0.2, 0.1, 1);
	}

	LoopHeadConvoAnim();
	LoopBaseConvoAnim();
}
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45382397</guid><pubDate>Fri, 26 Sep 2025 03:45:54 +0000</pubDate></item><item><title>A platform-jumping prince – History of Prince of Persia's 1990s Ports</title><link>https://www.jordanmechner.com/en/latest-news/#a-platform-jumping-prince</link><description>&lt;doc fingerprint="a7c6792f1b92179c"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;A platform-jumping prince&lt;/head&gt;
    &lt;p&gt;"Which is your favorite/definitive version of the original Prince of Persia game?"&lt;/p&gt;
    &lt;p&gt;I get this question surprisingly often, considering it's been 35 years. I figured it deserves a blog post.&lt;/p&gt;
    &lt;head rend="h4"&gt;Apple II&lt;/head&gt;
    &lt;p&gt;The Apple II version was the original. It's the only version I programmed myself; Prince of Persia's gameplay, graphics, animation and music were all created on the Apple II. I spent three years sweating over every byte (from 1986 to 1989), so it's close to my heart in a way no other version can be. That said...&lt;/p&gt;
    &lt;head rend="h4"&gt;DOS/Windows&lt;/head&gt;
    &lt;p&gt;The 1990 PC version, developed in parallel with the Apple II and shipped a few months later, took advantage of the PC's improved graphics and sound capabilities to deliver the Prince of Persia most players remember (in CGA, EGA, or VGA). My dad, Francis Mechner, re-orchestrated his music (previously limited by the Apple II's tinny built-in speaker) for MIDI synthesizers. The Broderbund in-house team, led by programmer Lance Groody, with Leila Joslyn on art, Tom Rettig on sound, and me as director, stayed faithful to the Apple game while upping the quality in every dimension. The digitized spike and slicer sound effects that traumatized many an elementary-school gamer originated with the PC version. If someone asked me the best way to play old-school PoP online today, I'd likely recommend the DOS version.&lt;/p&gt;
    &lt;p&gt;In 1990, C-family programming languages were the future, 6502 machine language the past. For good reasons, nearly all subsequent ports of PoP took the PC version as their starting point, rather than the Apple II.&lt;/p&gt;
    &lt;head rend="h4"&gt;Amiga&lt;/head&gt;
    &lt;p&gt;The Amiga port was developed by Dan Gorlin (of Choplifter fame), in parallel with the PC version, using the graphics and sound assets developed by the Broderbund team.&lt;/p&gt;
    &lt;p&gt;Danny was one of my game-author heroes. Playing Choplifter, as a 17-year-old college freshman in 1982, blew me away and set me on the creative path that would lead to Karateka. I was star-struck that he agreed to port PoP to Amiga. He did an impeccable job, working alone at home, using the state-of-the-art development system he'd built for his games Airheart and Typhoon Thompson.&lt;/p&gt;
    &lt;p&gt;In a detail perhaps mainly interesting to lawyers, Amiga was one of three PoP versions (Apple II and Macintosh were the others) that I was contractually responsible for delivering to Broderbund, rather than their doing the development. This meant me driving to Danny's house for meetings instead of to Broderbund, and that I was on the hook in case the project fell behind schedule or something went wrong. Fortunately, with Danny, all was smooth sailing.&lt;/p&gt;
    &lt;head rend="h4"&gt;Commodore 64&lt;/head&gt;
    &lt;p&gt;One port that didn't get greenlit was the Commodore 64. Like the Apple II, the C64 had its heyday in the mid-1980s. By 1990, Broderbund (and most U.S. retailers) considered the C64 and Apple II outdated platforms; sales numbers were dwindling by the month. Broderbund couldn't escape publishing PoP on Apple, since it was the lead platform I created the game on, but they had little interest in a C64 version. It would have been a tough port in any case. To fit PoP into 64K of memory, with the Commodore's technical limitations, needed an ace 6502 programmer.&lt;/p&gt;
    &lt;p&gt;In a twist I'd never have predicted, an unofficial, fan-made C64 port was finally done in 2011, over 20 years later, and a Commodore Plus/4 port just last year. I hope my Apple II source code was helpful.&lt;/p&gt;
    &lt;head rend="h4"&gt;Macintosh&lt;/head&gt;
    &lt;p&gt;In 1984, Apple unveiled the Macintosh computer (with a now-legendary Super Bowl ad). Still in college, and flush with Karateka royalties, I took advantage of the student discount to purchase a 128K Mac — keeping my Apple IIe for games. (A computer with no lowercase, and enough RAM to hold four pages of text, isn't ideal for writing term papers.) I loved my Mac, and faithfully upgraded my system every time they did: Mac Plus, SE, II, IIci, LC. By 1990, I was proudly Mac-only.&lt;/p&gt;
    &lt;p&gt;But the games market was overwhelmingly PC. Broderbund estimated Mac's games market share as 5% of DOS/Windows. Since I believed in the Mac more than they did, it made sense for me to take on the port, as I'd done with Amiga. I subcontracted it to Presage Software, a group of ex-Broderbund programmers I'd known since Karateka days.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Fun fact: the previous occupant of Presage's San Rafael office was George Lucas's Industrial Light &amp;amp; Magic.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Presage had an excellent, seasoned lead Mac programmer in Scott Shumway; but whereas Danny met his Amiga milestones promptly, Scott's Mac milestones receded like the horizon as they approached. With each new Mac model release — black-and-white, then color, then a different-sized screen — Presage had to redo the bit-mapped PoP graphics for the new configuration. While Prince of Persia's Apple, Amiga and PC versions languished on store shelves (the game wasn't a hit in its first two years), the Mac release date slipped from 1990 to 1991, then to 1992.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Fun fact #2: the young graphic artist who up-rezzed the Mac sprites, Mike Kennedy, went on to found the comics imprint Magnetic Press. We met again in 2024, when Magnetic published my graphic novel Monte Cristo.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Ironically, the Mac delays turned out to be a blessing in disguise. By the time the port was finally finished, almost two years late, Broderbund marketing had noticed that despite PoP's lackluster U.S. sales, its overseas and console versions were doing surprisingly well. Maybe the game had untapped potential?&lt;/p&gt;
    &lt;p&gt;Broderbund took the gamble of combining PoP's Mac release with a PC re-release in a bigger, hourglass-shaped "candy box" designed by the San Francisco firm Wong &amp;amp; Yeo. The dual Mac-PC release in the new box turned the prince's fortunes around. PoP not only became the #1-selling Mac game, it went from ice-cold to hot on PC as well. To 1992 Mac owners who'd been using their machines mainly for work, a game like PoP was a welcome diversion.&lt;/p&gt;
    &lt;p&gt;The Mac port was terrific. A sign of its quality is that we adopted its revamped prince (sporting a vest, turban and shoes) for the sequel, Prince of Persia 2: The Shadow and The Flame.&lt;/p&gt;
    &lt;p&gt;But I still think the original Apple and PC graphics play best. The CRT blur and fat pixels smoothed over animated glitches, enhancing the illusion of life. Higher resolution leaves less to the imagination. (The same can be said of photography and cinema.)&lt;/p&gt;
    &lt;head rend="h4"&gt;Other ports&lt;/head&gt;
    &lt;p&gt;Between 1990 and 1993, more computer and console ports of PoP than I can list — Nintendo NES, Game Boy, SEGA Game Gear, Genesis, Master System, Amstrad CPC, Atari ST, NEC PC-9801, FM Towns, Sam Coupé — were developed by teams in Japan, Europe, and elsewhere. Usually, by the time someone handed me a controller to playtest a build, it was too late for my feedback to matter, so I rarely played beyond the first level or two. I don't remember enough specifics of those versions to compare them; I'll leave that to players who know them better.&lt;/p&gt;
    &lt;p&gt;There is one unforgettable exception.&lt;/p&gt;
    &lt;head rend="h4"&gt;Super Nintendo&lt;/head&gt;
    &lt;p&gt;In March 1992, I moved to Paris for a year (to learn French and 16mm filmmaking). Soon after my arrival, a colleague at Activision invited me to visit their office. They showed me the Super Nintendo version of PoP, developed by Arsys and published by NCS in Japan. Activision was lobbying Broderbund for the rights to publish it in Europe and the U.S. It wasn't my call, but they hoped I'd put in a word.&lt;/p&gt;
    &lt;p&gt;I wrote in my journal that day:&lt;/p&gt;
    &lt;p&gt;"Wow! It was like a brand new game. For the first time I felt what it's really like to play Prince of Persia, when you're not the author and don't already know by rote what's lurking around every corner."&lt;/p&gt;
    &lt;p&gt;Arsys had done more than a straight port; they'd expanded the game from 12 levels to 20, adding new enemies, traps, setpieces, and new music. I didn't play all the way through — a half-hour in Activision's office only scratched the surface — but I'll never forget the delighted thrill of being surprised playing my own game. You can see and play it in your browser here.&lt;/p&gt;
    &lt;p&gt;Elaborate production values and doubled playtime helped make SNES PoP a huge hit. I especially loved the fantastic box artwork by Katsuya Terada.&lt;/p&gt;
    &lt;p&gt;A recent feature article in Time Extension revealed behind-the-scenes details about the SNES development that I hadn't known — including that game producer Keiichi Onogi traveled to the U.S. to visit Broderbund in 1991, hoping to get my feedback. (I missed his visit.) The article is a fascinating time capsule and testament to how special that port was.&lt;/p&gt;
    &lt;head rend="h4"&gt;...And onwards&lt;/head&gt;
    &lt;p&gt;The SNES, so different from the original Apple/DOS version, gave me my first taste of a feeling I would grow used to in decades to come: playing and enjoying new Prince of Persia games that were made by others. With the exception of The Sands of Time (2003), where I was part of a Ubisoft Montreal team, the more recent modern PoP games don't have my fingerprints on them.&lt;/p&gt;
    &lt;p&gt;I suspect that for many reading this post, your answer to "Which is your favorite PoP?" will be the same as mine: Whichever version we played, for hours on end, at a formative age when playing and finishing a game mattered intensely. The real value is in the ingenuity and imagination you brought to the effort, and in your own memories tied to that time.&lt;/p&gt;
    &lt;p&gt;Thanks for reading this post. If you'd like a deeper dive into the story behind Prince of Persia's creation, I've published two books on the subject: my old journals (1985-1993), and my new graphic novel Replay. You can check them out here. Archival materials about PoP (including the Apple II source code) can be found in this website's Library.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45382645</guid><pubDate>Fri, 26 Sep 2025 04:29:41 +0000</pubDate></item><item><title>No reachable chess position with more than 218 moves</title><link>https://lichess.org/@/Tobs40/blog/there-is-no-reachable-chess-position-with-more-than-218-moves/a5xdxeqs</link><description>&lt;doc fingerprint="13011afe410486f7"&gt;
  &lt;main&gt;&lt;p&gt;Created by the author using GIMP and freely available images.&lt;/p&gt;&lt;head rend="h1"&gt;There is no reachable chess position with more than 218 moves.&lt;/head&gt;Stop searching, we had it right for 60 years.&lt;p&gt;Ever since Nenad Petrović, grandmaster of chess composition, published his 218 move composition in 1964, people have tried to come up with a better one. Last month, I joined the hunt and, being a computer scientist, I decided to settle this question once and for all, using computers. You can give it a try yourself. Try to find a position with more moves than the one below.&lt;/p&gt;&lt;p&gt;Spoiler: You won't.&lt;/p&gt;&lt;p&gt;&lt;lb/&gt;Reachable chess position with 218 moves for White, published by Petrović in 1964.&lt;/p&gt;&lt;p&gt;...but how can we know for sure?&lt;/p&gt;&lt;p&gt;By checking all approximately 8.7x10^45 reachable chess positions?&lt;lb/&gt;Yeah that's not gonna happen...&lt;/p&gt;&lt;p&gt;That's 8.7 billion billion billion billion billion and it's enough to scare even the mightiest of supercomputers. In fact, cracking AES-128 encryption would be easier.&lt;/p&gt;&lt;p&gt;Fortunately, we can use the power of Math!&lt;/p&gt;&lt;p&gt;Follow me along and perhaps you can compute yourself a world record afterwards :)&lt;/p&gt;&lt;head rend="h2"&gt;Using the power of Math&lt;/head&gt;&lt;p&gt;&lt;lb/&gt;Red is the official color of Math, at least in my elementary school.&lt;/p&gt;&lt;p&gt;We're gonna tackle this problem from the white side, i.e., it is White to move. Except for rare exceptions regarding the reachability of positions, this is equivalent.&lt;/p&gt;&lt;p&gt;Since proving that a position is reachable is complicated, we're gonna search through all ways of placing pieces on the board and filter out the non-reachable ones later on, if needed.&lt;/p&gt;&lt;p&gt;Obviously, 99.9% of the positions suck at having lots of moves, they are not even close to 218. We just need a way to skip them and pray that there exists enough electricity to check the rest.&lt;/p&gt;&lt;p&gt;Let's begin with a couple of useful observations.&lt;/p&gt;&lt;head rend="h4"&gt;Useless pieces&lt;/head&gt;&lt;p&gt;A black piece does not improve the number of moves, most of the time. Its existence only benefits the number of moves if it increases White's number of moves, i.e., at least one of the following is true:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;It can be taken by a white pawn, giving said pawn more moves&lt;/item&gt;&lt;item&gt;It protects the black king from check, making an otherwise illegal position with lots of moves legal&lt;/item&gt;&lt;item&gt;It frees a white piece from being pinned to the white king, thus giving it more moves&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Otherwise, it is useless, at best, and thus can be removed.&lt;/p&gt;&lt;p&gt;&lt;lb/&gt;I inserted this picture for the sole purpose of making this article look less text-heavy.&lt;/p&gt;&lt;head rend="h4"&gt;Too powerful pieces&lt;/head&gt;&lt;p&gt;Next, we observe that, if piece counts permit, we can always replace a black piece, with the exception of the black king, with a strictly less powerful one. That is, a piece that has a subset of the moves of the original piece. For example, a queen with a pawn/bishop/rook or a bishop with a pawn. Except if a pawn is on the seventh rank, since in that case, each of its moves to the promotion square counts as 4 separata moves. The only way for Black's moves to affect White's number of moves is by pinning White's pieces or preventing the white king from stepping on a certain square. Both of these things can't get worse if the black piece has less moves than before.&lt;/p&gt;&lt;head rend="h4"&gt;Too weak pieces?&lt;/head&gt;&lt;p&gt;The other way around, it is not so easy, however. You'd think that you can just replace white rooks and white bishops with white queens if counts permit, but the problem is this: How do you ensure that you cannot capture the black king, making the position illegal? Maybe the optimal solution has a rook instead of a queen to avoid just that?&lt;/p&gt;&lt;p&gt;Well, you might say "Let's just place a black piece in between then".&lt;/p&gt;&lt;p&gt;And you would be wrong unless you can tell me why you haven't just blocked some other white piece and thus reduced the number of moves in total. Or what your plan is, if there is no space to put something in between. You just made the position illegal, duh!&lt;/p&gt;&lt;head rend="h4"&gt;No checks, thank you&lt;/head&gt;&lt;p&gt;Finally, we can get rid of checks. If the black king is in check, it means that the position is illegal, since it is White's turn to move and they could just capture the black king. Not okay! So that can't be it. On the other hand, if the white king is in check, then the number of White's moves is severely restricted and we can easily prove that we cannot reach 218 moves. There are three ways to get out of check:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Move the king&lt;/item&gt;&lt;item&gt;Capture the attacker&lt;/item&gt;&lt;item&gt;Block the attack&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Moving the king gives 8 moves at best. Since any square can be reached by at most 16 pieces at the same time (8 knights and 8 other pieces straight or diagonally), capturing gives 16 moves at best. We can have at most 6 squares to block an attack, so that's an additional 6 x 16 = 96 moves. So at best 8+16+96 = 120 moves, far less than 218, and thus we do not need to consider positions in which either king is in check.&lt;/p&gt;&lt;p&gt;So is this it Tobi? Can we now call NASA and ask for their supercomputer?&lt;/p&gt;&lt;p&gt;Nope, it's still absolutely hopeless, there are waaaaay too many positions left.&lt;lb/&gt;We have to skip even more positions.&lt;/p&gt;&lt;head rend="h2"&gt;Introducing Chess with Cheating: Partial Pieces and Moves&lt;/head&gt;&lt;p&gt;&lt;lb/&gt;Reminder: Replace rook pictures by something more interesting.&lt;/p&gt;&lt;p&gt;While searching through all possible piece configurations, we would ideally like to have some provably correct way of telling whether we can still reach 218 moves, so we can stop trying and save ourselves an astronomically large amount of work. The better the method is, the more work we save. But it also needs to be fast, since we need to run it millions and billions of times.&lt;/p&gt;&lt;p&gt;A common technique in optimization is to allow fractional decisions. Instead of a piece being either on e4 or not, it can be 27.3% there and 72.7% not there. This enables us to just "swim" through the solution space towards the optimal solution instead of trying all combinations. The drawback is that we usually end up with a way too good solution with most decisions being fractional. But if that doesn't get us beyond 218 moves, we know we can stop trying.&lt;/p&gt;&lt;p&gt;Obviously, these kinds of algorithms are already implemented in state-of-the-art solvers like Gurobi, so all we need to do is model our problem to its likings (as a so-called integer programming problem), tell it to maximize the number of moves and pray. Finally, after ~55 000 seconds, it crashed.&lt;lb/&gt;Not enough memory, but also hopelessly far away from completing the proof. Extrapolating the runtime led to an estimated runtime of ~6 years. Yeah, let's not go there. Going back to making more observations:&lt;/p&gt;&lt;head rend="h4"&gt;Checking more positions to make things less slow&lt;/head&gt;&lt;p&gt;To reduce the size of the model, I bent some chess rules, simplifying the search space:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;I allowed castling if king and rook were on the right squares, not requiring anything else&lt;/item&gt;&lt;item&gt;I stopped caring about pieces moving despite being pinned&lt;/item&gt;&lt;item&gt;I stopped caring whether the white king is in check or walks into check&lt;/item&gt;&lt;item&gt;I allowed white pawns to always capture when standing on the fifth rank so I'd not have to check en passant.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;All of these things are unlikely to happen in 218+ move positions and after a solution has been found, I can still check and discard it if it has less moves than it claims to have.&lt;/p&gt;&lt;p&gt;I started again and this time, memory wasn't a problem, but the progress was awfully slow with ~29 days remaining. The world could not wait this long and neither could I. I pressed cancel.&lt;/p&gt;&lt;head rend="h4"&gt;Preventing white magic&lt;/head&gt;&lt;p&gt;&lt;lb/&gt;The optimal fractional solution for the empty board. Most pieces are spread out over multiple squares and have fractional numbers of moves available that sum to ~305. This does prove that the real solution can't have more than 305 moves. Still quite a bit from proving 218 to be the optimum.&lt;/p&gt;&lt;p&gt;Our current way of cheating is too different from reality, causing the solver to have to search through way too many board configurations. In our case, the optimal solution to the easy problem flooded the center with half queens and half knights sharing the same squares, having half pieces move through other half pieces with half moves. Gurobi thinks that the above position has 305 moves in total, which is far away from 218 and a bad upper bound.&lt;/p&gt;&lt;p&gt;In order to cut off this crazy solution, I added a "redundant" constraint, saying that at most one piece in total can move from one direction onto a particular square. Which got us a new hallucination...&lt;/p&gt;&lt;p&gt;&lt;lb/&gt;The second try. This one has 271 2/3 moves which proves that there is no solution with more than 271 moves.&lt;/p&gt;&lt;p&gt;Now we have 271 2/3 moves, which is much better. It's a bit like having to try all passwords with 53 characters vs. all passwords with 87 characters, the latter being many magnitudes harder.&lt;/p&gt;&lt;p&gt;Wait a minute, White can't have 4 kings.&lt;/p&gt;&lt;p&gt;White does not have four kings. White has 3 times 24.6% kings and one 26.2% king. Also, the queen on g3 barely exists, it's 0.8% but apparently it somehow contributes to the total sum of moves :)&lt;/p&gt;&lt;p&gt;With this improved model, I tried again and after ~23 000 seconds, Gurobi solved it to optimality!&lt;/p&gt;&lt;head rend="h2"&gt;Results&lt;/head&gt;&lt;p&gt;Sadly, instead of finding me a fancy position with 219 moves and making my name immortal, Gurobi gave me the following 12 representative positions (of 40,000 in total) with 218 moves each:&lt;/p&gt;&lt;p&gt;All 12 positions seem trivially reachable. I only constructed a proof game for one of the positions, since that is sufficient for proving our claim. If you don't believe that this is possible, keep in mind that White's last or second last move might have been a capture. Or click on the link :)&lt;/p&gt;&lt;p&gt;Sadly not a world record, but at least we now know for certain that 218 is the limit.&lt;/p&gt;&lt;p&gt;And you smart chess move 3.7 bit compression people and chess engine developers, you can finally stop worrying. 256 moves will be enough. You're welcome :-)&lt;/p&gt;&lt;p&gt;Except if you allow non-reachable positions, in which case you might want to read on :P&lt;/p&gt;&lt;head rend="h2"&gt;Other stuff solved along the way&lt;/head&gt;&lt;p&gt;I also confirmed the optimality of the 144 move record without promotions. Since Gurobi did not find any position with more than 144 moves, that means that there also is no reachable position with more than 144 moves. Hence, 144 moves is the best we can do and "Jenő Bán", a chess composer from Hungary, found one in 1960 already:&lt;/p&gt;&lt;p&gt;&lt;lb/&gt;144 moves for White, no promotions. Created by Hungarian chess composer "Jenő Bán". Here is a proof game, demonstrating that the position is reachable.&lt;/p&gt;&lt;p&gt;Also, I confirmed the optimality of the following illegal position, which has 288 moves for White.&lt;/p&gt;&lt;p&gt;&lt;lb/&gt;Illegal position with 288 moves for White. Corner queens can be replaced with bishops.&lt;/p&gt;&lt;p&gt;Now have a guess at what the best non-reachable legal position looks like ;)&lt;lb/&gt;Yep, you're right, cramming the kings into the corners for 271 moves.&lt;/p&gt;&lt;p&gt;&lt;lb/&gt;Legal but non-reachable position with 271 moves for White. Corner queens can be replaced with bishops.&lt;/p&gt;&lt;head rend="h2"&gt;Future plans&lt;/head&gt;&lt;p&gt;My code snippet is freely available at Github. If you manage to do something cool based on it, please let the world know :)&lt;/p&gt;&lt;p&gt;Fun problems enthusiasts could try to tackle next using similar techniques:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Most captures&lt;/item&gt;&lt;item&gt;Most stalemates&lt;/item&gt;&lt;item&gt;Most checks&lt;/item&gt;&lt;item&gt;Most checkmates&lt;/item&gt;&lt;item&gt;Most mates in two&lt;/item&gt;&lt;item&gt;...&lt;/item&gt;&lt;item&gt;Most ... under &amp;lt;condition&amp;gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Some of these might be hard, extremely hard or practically impossible to solve with current technology. I don't think that integer linear programming is a suitable approach for all of them, one likely has to develop a custom algorithm for computing good upper bounds, based on creative mathematical insights.&lt;/p&gt;&lt;p&gt;Good luck to those who dare to try solving one of these ^^&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45382755</guid><pubDate>Fri, 26 Sep 2025 04:47:40 +0000</pubDate></item><item><title>Translating a Fortran F-16 Simulator to Unity3D</title><link>https://vazgriz.com/762/f-16-flight-sim-in-unity-3d/</link><description>&lt;doc fingerprint="3633da10e06c133a"&gt;
  &lt;main&gt;&lt;p&gt;I recently purchased the textbook “Aircraft Control and Simulation” by Brian L. Stevens, Frank L. Lewis, and Eric N. Johnson1. This book covers the control and simulation of aircraft. It’s really dense and frankly hard to understand. As far as aerodynamics texts go, it’s pretty typical.&lt;/p&gt;&lt;p&gt;One interesting item in the appendices of the book is the source code for the simulation of an F-16. It has a flight model, based on scale model wind tunnel data. The flight model consists of a dozen lookup tables and the math equations to make it fly.&lt;/p&gt;&lt;p&gt;The only problem: it’s written entirely in Fortran.&lt;/p&gt;&lt;p&gt;The source code is available on Github.&lt;/p&gt;&lt;p&gt;You can play the finished project right now on itch.io&lt;/p&gt;&lt;p&gt;Or watch the demo on Youtube:&lt;/p&gt;&lt;p&gt;While I am a professional software engineer and I have worked in the aerospace industry, that doesn’t mean that I understand what I’m doing.&lt;/p&gt;&lt;p&gt;Table of Contents&lt;/p&gt;&lt;head rend="h1"&gt;Introduction&lt;/head&gt;&lt;p&gt;In previous posts on this blog2 3 4, I covered the development of a flight simulator based on the lift equation and hand-tuned parameters. This gives the game designer direct control over a lot of flight parameters. For example, you can directly choose the turn rate and the G-limit of the aircraft, allowing the designer to easily tune the corner speed. This works well for game development, since the designer, and ultimately the player, care more about these high-level parameters.&lt;/p&gt;&lt;p&gt;But real aircraft are designed from the other direction, starting from low-level parameters such as the size, shape, and position of airfoils. Engineers tune every aspect of the aircraft in order to reach those high-level behaviors. But every design decision has trade offs and reaching the goal for one parameter means compromising another. An airliner is designed very differently from a fighter jet because of this.&lt;/p&gt;&lt;p&gt;Simulating all of the low-level parameters is difficult. It’s possible to simulate air flow over the vehicle using computational fluid dynamics (CFD), but this kind of software is difficult to write and even more difficult to verify.&lt;/p&gt;&lt;p&gt;The F-16 flight model from the textbook does not simulate the low-level parameters, but it also doesn’t simulate the high-level parameters either. It sits somewhere in between, so it serves as a useful stepping stone from my previous projects. This project will explore more advanced flight dynamics and explain the limitations of the old flight model as well as the new one.&lt;/p&gt;&lt;head rend="h1"&gt;Aerospace Conventions&lt;/head&gt;&lt;head rend="h2"&gt;Coordinate System&lt;/head&gt;&lt;p&gt;Before we can write any code, we need to understand the conventions used for mathematically modeling aircraft that are used in the aerospace industry. The textbook uses aerospace conventions and to use them in this project, we must convert them to Unity conventions.&lt;/p&gt;&lt;p&gt;The first convention is the coordinate system axes. If you’ve ever visited a graphics programming forum, you might have seen people arguing over how the X, Y, and Z axes should be arranged in their game. Especially whether to use a right handed or left handed, and Y-up or Z-up coordinate system.&lt;/p&gt;&lt;p&gt;This chart by Freya Holmer5 shows the axis choices made by a variety of 3D software tools.&lt;/p&gt;&lt;p&gt;The aerospace industry takes a different path. The most common coordinate system for aircraft is right handed, X forward, Y right, and Z down. This is completely different from every tool shown above. The textbook defines all of it’s math using this convention.&lt;/p&gt;&lt;p&gt;Luckily, translating between two coordinate systems is easy. You just swap the components around and then add or remove minus signs until it all works. Every calculation made by the textbook’s code can be easily translated into Unity’s coordinate system and vice versa.&lt;/p&gt;&lt;p&gt;Writing functions to do this is simple:&lt;/p&gt;&lt;quote&gt;public static Vector3 ConvertVectorToAerospace(Vector3 vector) { return new Vector3(vector.z, vector.x, -vector.y); } public static Vector3 ConvertVectorToUnity(Vector3 vector) { return new Vector3(vector.y, -vector.z, vector.x); }&lt;/quote&gt;&lt;p&gt;When translating euler angles, torque, or other angular values, one additional negation is needed:&lt;/p&gt;&lt;quote&gt;public static Vector3 ConvertAngleToAerospace(Vector3 angle) { // negate when switching handedness return -ConvertVectorToAerospace(angle); } public static Vector3 ConvertAngleToUnity(Vector3 angle) { // negate when switching handedness return -ConvertVectorToUnity(angle); }&lt;/quote&gt;&lt;head rend="h2"&gt;Units&lt;/head&gt;&lt;p&gt;For completely inscrutable reasons, American aerospace texts (and the industry!) insist on using US customary units for everything. All math is defined with these units. Distance is measured in feet. Mass is measured in slugs.&lt;/p&gt;&lt;p&gt;What the hell is a slug? A slug is the unit of mass in the US system. This is the equivalent unit of the kilogram in the metric system. Remember that weight and mass are not the same thing.&lt;/p&gt;\(1 \, \text{kg} * 9.81 \, \text{m/s}^2 = 9.81 \, \text{N}\) \(1 \, \text{slug} * 32.17 \, \text{ft/s}^2 = 32.17 \, \text{lb}\)&lt;p&gt;Mass is the measure of how much an object resists linear force. Moment of inertia is how much the object resists rotational force or torque. The unit for moment of inertia in metric is kg-m2. Thus, the equivalent unit in customary is slug-ft2.&lt;/p&gt;&lt;p&gt;You want to measure how much air mass is in a given volume? That’s gonna be slugs/ft3.&lt;/p&gt;&lt;p&gt;Speed is mostly measured in feet per second, unless you want to know the speed of the aircraft. Then you use knots, which means nautical miles per hour. Importantly, a nautical mile is not the same as a regular mile. A regular mile is 5,280 feet. A nautical mile is ~6,076 feet or exactly 1,852 meters (???).&lt;/p&gt;&lt;p&gt;Do you want to know how fast your ship is sailing? Just throw out this piece of wood tied to a spool of rope. The rope has knots tied at regular intervals. Count the number of knots that unspool in a given time frame. That’s how many knots your ship is making.&lt;/p&gt;&lt;p&gt;Finally, temperature is measured in degrees Rankine. You know how the Kelvin scale is just the Celsius scale adjusted so that 0 Kelvin equals absolute zero? Well Rankine is the same concept applied to Fahrenheit.&lt;/p&gt;\(0 \, \text{R} = \text{absolute zero} \\ 534 \, \text{R} = 75 \, \text{F} = \text{room temperature}\)&lt;p&gt;How the fuck did we ever build the SR-71? 💀&lt;/p&gt;&lt;p&gt;Unity by convention uses metric for all physics units. The flight model in the textbook uses US customary units, so every input and output of this system has to be converted. So we have to add functions to handle converting to and from US customary units. This is easy enough since conversion is just a multiplication or division operation.&lt;/p&gt;&lt;head rend="h2"&gt;Terminology&lt;/head&gt;&lt;p&gt;There are several terms used in aerospace that I need to define. I have used equivalent terms in the previous project, but I will clarify them here.&lt;/p&gt;&lt;p&gt;Alpha (α) refers to the angle of attack.&lt;/p&gt;&lt;p&gt;Beta (β) refers to the angle of side slip.&lt;/p&gt;&lt;p&gt;Longitudinal axis is the X-axis, from tail to nose.&lt;/p&gt;&lt;p&gt;Normal axis is the Z- axis, the vertical axis pointing downwards.&lt;/p&gt;&lt;p&gt;Lateral axis is the Y-axis, or side axis, pointing right.&lt;/p&gt;&lt;p&gt;Phi (φ) is the aircraft’s roll around the X axis.&lt;/p&gt;&lt;p&gt;Theta (θ) is the aircraft’s pitch around the Y axis.&lt;/p&gt;&lt;p&gt;Psi (ψ) is the aircraft’s yaw around the Z axis.&lt;/p&gt;&lt;p&gt;P, Q, and R refer to the angular velocity around the X, Y, and Z axes respectively.&lt;/p&gt;&lt;p&gt;In general you will find that aerodynamics texts are allergic to good variable names. I suspect this is a form of gatekeeping. Or perhaps the authors have to pay by the letter to publish.&lt;/p&gt;&lt;head rend="h1"&gt;Air Data&lt;/head&gt;&lt;p&gt;Airplanes need air to fly [citation needed]. Every behavior of a plane is determined by the movement of air. Therefore it is critically important, for real and simulated planes, to be able to measure the air flowing around it.&lt;/p&gt;&lt;p&gt;Real planes need to measure static and dynamic air pressure to determine how fast the plane is moving. Static pressure is measured by a static pressure port. It’s the pressure that you would measure if you just lifted a pressure meter to the same altitude as the plane. Static pressure decreases with altitude.&lt;/p&gt;&lt;p&gt;Dynamic pressure measures the pressure added by the plane’s forward motion. This requires a pitot tube to measure. As the plane moves forward it rams air into the pitot tube and increases the pressure above the static pressure. The pitot measures the total pressure of the air. By subtracting the static pressure, we can obtain the dynamic pressure.&lt;/p&gt;&lt;p&gt;The static and dynamic pressures can then be used to calculate many of the variables the pilot needs to fly. Most important are the airspeed and altitude of the aircraft. Specifically, these values can be used to calculate the indicated airspeed of the aircraft. Indicated airspeed is calculated directly from the dynamic pressure.&lt;/p&gt;&lt;p&gt;At most subsonic speeds, the dynamic pressure of air flowing over the wings is the most important variable in flight. A plane’s performance can be defined in terms of indicated airspeed. For example, a plane may have a stall speed of 100 knots indicated airspeed. This means that no matter what altitude the plane is at, the indicated airspeed will be 100 when the plane stalls.&lt;/p&gt;&lt;p&gt;This is important since it gives a consistent number for the stall speed regardless of atmospheric conditions. The pressure and density of the air can vary based on weather, temperature, and other factors. So the true airspeed when a stall occurs can be very inconsistent. But as long as the pilot knows the indicated airspeed, they know how their plane will behave.&lt;/p&gt;&lt;p&gt;For this simulator, the calculation has to work backwards. We know the true airspeed and altitude of the plane from the velocity and position of the rigidbody. From that, we can calculate the dynamic pressure. This dynamic pressure is then used for later calculations in the flight model. Additionally, the plane’s speed in mach is calculated here as well.&lt;/p&gt;&lt;p&gt;The original Fortran source code is given:&lt;/p&gt;&lt;quote&gt;SUBROUTINE ADC(VT,ALT,AMACH,QBAR) DATA R0/2.377E-3/ TFAC = 1.0 - 0.703E-5 * ALT T = 519.0 * TFAC IF (ALT .GE. 35000.0) T= 390.0 RHO = R0 * (TFAC**4.14) AMACH= VT/SQRT(1.4*1716.3*T) QBAR = 0.5*RHO*VT*VT C PS = 1715.0 * RHO * T RETURN END&lt;/quote&gt;&lt;p&gt;It turns out, Fortran is actually pretty good at translating formulas. So this code is not as difficult to read as I expected.&lt;/p&gt;&lt;p&gt;To translate this to Unity, we create a class AirDataComputer to perform these calculations. The output of the calculation is the AirData struct.&lt;/p&gt;&lt;quote&gt;public struct AirData { public float altitudeMach; public float qBar; } public class AirDataComputer { /// &amp;lt;summary&amp;gt; /// Density in slugs/ft^3 /// &amp;lt;/summary&amp;gt; public const float SeaLevelDensity = 2.377e-3f; public const float MaxAltitude = 35000.0f; /// &amp;lt;summary&amp;gt; /// Calculates air data based on velocity and altitude /// &amp;lt;/summary&amp;gt; /// &amp;lt;param name="velocity"&amp;gt;Velocity in ft/s&amp;lt;/param&amp;gt; /// &amp;lt;param name="altitude"&amp;gt;Altitude in ft&amp;lt;/param&amp;gt; /// &amp;lt;returns&amp;gt;Air data&amp;lt;/returns&amp;gt; public AirData CalculateAirData(float velocity, float altitude) { ... } }&lt;/quote&gt;&lt;p&gt;Here we can see where the US customary units are used. The density of air at sea level is defined in slugs/ft3. The altitude is defined in feet. Theoretically, these values could be defined using metric. But the implementation of the function depends on even more values defined in customary.&lt;/p&gt;&lt;quote&gt;const float baseTemperature = 519.0f; // sea level temp in R const float minTemperature = 390.0f; // minimum temp in R const float temperatureGradient = 0.703e-5f; // gradient in R / ft altitude = Mathf.Clamp(altitude, 0, MaxAltitude); // calculate temperature in Rankine float temperatureFactor = 1.0f - (temperatureGradient * altitude); float T = Mathf.Max(minTemperature, baseTemperature * temperatureFactor);&lt;/quote&gt;&lt;p&gt;These calculations simulate the change in atmospheric conditions at different altitudes. Particularly important is how the temperature drops at higher altitudes. The temperature gradient approximates the decreases in temperature (in Rankine) as altitude increases.&lt;/p&gt;&lt;p&gt;This flight model supports altitudes up to 35,000 ft. Altitudes above this are not supported. At any altitude above this, the plane will behave as if it were at 35,000 ft. This is because the temperatures at this altitude no longer consistently decrease, as it does in the lower atmosphere. A more advanced atmosphere model would need to be used.&lt;/p&gt;&lt;p&gt;Temperature factor does not drop below about 0.75 in this range, so the resulting temperature T does not fall below 390 R.&lt;/p&gt;&lt;quote&gt;const float gamma = 1.4f; // ratio of specific heats const float gasConstant = 1716.3f; float speedOfSound = Mathf.Sqrt(gamma * gasConstant * T); float altitudeMach = velocity / speedOfSound;&lt;/quote&gt;&lt;p&gt;Now we can calculate the speed of sound at the plane’s current altitude and use it to find the plane’s Mach number. The speed of sound varies with density, which varies with temperature. The speed of sound is equal to the square root of the ratio of specific heat, called gamma, times the gas constant, gasConstant, times the absolute temperature, T.7&lt;/p&gt;&lt;p&gt;Once the speed of sound is known, calculating the Mach number is just a simple division.&lt;/p&gt;&lt;quote&gt;const float densityPower = 4.14f; float rho = SeaLevelDensity * Mathf.Pow(temperatureFactor, densityPower); float qBar = 0.5f * rho * velocity * velocity;&lt;/quote&gt;&lt;p&gt;And finally the dynamic pressure is calculated from the temperature factor. I’ll admit, I don’t understand why exactly the formula is designed this way. It seems to calculate a density factor, called rho, based solely on the temperature factor, raised to an arbitrary value, densityPower.&lt;/p&gt;&lt;p&gt;The NASA reference provides a similar formula using metric units and using a different arbitrary power. I guess this value is just what results from using customary🤷♂️&lt;/p&gt;&lt;p&gt;In any case, this gives us the two air data values we need for the rest of the simulation, dynamic pressure and mach number.&lt;/p&gt;&lt;head rend="h1"&gt;Table Interpolation&lt;/head&gt;&lt;p&gt;Throughout this flight model, various forms of table lookups are used to determine the aircraft’s behavior. Lookup tables are commonly used in flight simulators to represent complex curves and functions. In fact, Unity’s AnimationCurve class in the previous project is used to define a few lookup tables, such as lift coefficient.&lt;/p&gt;&lt;p&gt;This animation curve serves as a 1 dimensional lookup table. The input dimension is AOA and the output value is lift coefficient.&lt;/p&gt;&lt;p&gt;Fortran code doesn’t have the luxury of using AnimationCurves, but a simple table of values with an interpolation function is almost as powerful.&lt;/p&gt;&lt;head rend="h2"&gt;1D Lookup Table&lt;/head&gt;&lt;p&gt;The interpolation functions provided by the textbook look something like this:&lt;/p&gt;&lt;quote&gt;FUNCTION LOOKUP(ALPHA, RESULT) REAL A(-2:9) C DATA A / .770,.241,-.100,-.416,-.731,-1.053, &amp;amp; -1.366,-1.646,-1.917,-2.120,-2.248,-2.229 / C S = 0.2*ALPHA K = INT(S) IF(K.LE.-2) K=-1 IF(K.GE.9) K=8 DA = S - FLOAT(K) L = K + INT(SIGN(1.1,DA)) RESULT = A(K) + ABS(DA)*(A(L)-A(K)) RETURN END&lt;/quote&gt;&lt;p&gt;This function takes alpha (AOA) and uses it to lookup a value from the table. Alpha is a float that can have any value from [-10, 45]. The table “A” represents values for every 5 degree increment of alpha. Note that Fortran supports arrays with an arbitrary starting index, in this case -2. So this table supports indices in the range [-2, 9].&lt;/p&gt;&lt;p&gt;This first step is multiplying alpha by a scaling value to create a float S, which maps alpha to the range [-2, 9]. An integer index K is created from S and then clamped to values one less than the table’s index range. The value DA is calculated as the difference between S and K.&lt;/p&gt;&lt;p&gt;The value L is calculated to be one index away from K, in the same direction as S. So now we have two indices to the table, K and L, which we use to read two values from the table, A(K) and A(L). DA is then used to blend between these table values and produce the final result.&lt;/p&gt;&lt;p&gt;This has two effects. The first is the simplest to understand. If alpha falls within the input range of the table, L and K are selected as the closest table values. For example, if alpha is 12, the two indices would be 2 and 3. The difference between S and K would be less than 1. The values A(2) and A(3) can be read from the table and then interpolated based on the value of DA. This is a fairly normal interpolation calculation.&lt;/p&gt;&lt;p&gt;The other effect is what happens when alpha is outside of the input range of the table. K is guaranteed to not be the first or last index, and L is allowed to be one index off of K. L and K are still valid indices, but the value of DA may be larger than 1. This means when we interpolate between A(L) and A(K), we can extrapolate values for inputs beyond the range of the table.&lt;/p&gt;&lt;p&gt;This means our lookup table can handle values outside of it’s input range. But there is still a limitation. As the input value gets further away from the input range, the extrapolated values will become more and more unrealistic. This allows our plane to fly slightly outside the flight envelope of the lookup tables.&lt;/p&gt;&lt;p&gt;I translated this function into C# like this:&lt;/p&gt;&lt;quote&gt;public static float ReadTable(float[] table, int i, int start) { return table[i - start]; } public static (int k0, int k1, float t) GetLookUpIndex(float value, float scale, int min, int max) { float scaled = value * scale; int K0 = Mathf.Clamp((int)scaled, min, max); float T = scaled - K0; int K1 = K0 + (int)Mathf.Sign(T); return (K0, K1, T); } public static float LinearLookup(float value, float scale, float[] table, int min, int max) { (int k0, int k1, float kT) = GetLookUpIndex(value, scale, min + 1, max - 1); float T = ReadTable(table, k0, min); float U = ReadTable(table, k1, min); float result = T + Math.Abs(kT) * (U - T); return result; }&lt;/quote&gt;&lt;p&gt;GetLookUpIndex calculates K, L, and DA. These variables are renamed to k0, k1, and kT respectively.&lt;/p&gt;&lt;p&gt;ReadTable is a function that maps array indices to a new range, to support arbitrary starting indices like Fortran. (C# surprisingly supports this feature natively, but who actually uses that?)&lt;/p&gt;&lt;p&gt;LinearLookup reads the k0 and k1 values from the array and performs the interpolation. This allows us to calculate values for any input to the lookup table.&lt;/p&gt;&lt;p&gt;Note that the expression “T + Math.Abs(kT) * (U – T)” is effectively equivalent to Mathf.LerpUnclamped.&lt;/p&gt;&lt;head rend="h2"&gt;2D Lookup Table&lt;/head&gt;&lt;p&gt;All of the above code is needed to perform a one dimensional table lookup. Performing this kind of table lookup with two input dimensions is called a bilinear interpolation. Extending this to two dimensions is not that much more complicated.&lt;/p&gt;&lt;p&gt;The two input values to the table form a two dimensional space. Our input values form a two dimensional point. Instead of selecting two array indices K and L, we need to select four array indices. These four indices form a box around our input point. We simply perform 2 one dimensional lookups, and then interpolate between them to produce the final value.&lt;/p&gt;&lt;p&gt;The 2 one dimensional lookups are marked in red. The final interpolation is marked in blue.&lt;/p&gt;&lt;p&gt;Implementing this in C# is a simple extension of the LinearLookup function:&lt;/p&gt;&lt;quote&gt;public static float BilinearLookup(float xValue, float xScale, float yValue, float yScale, float[,] table, int xMin, int xMax, int yMin, int yMax) { (int x0, int x1, float xT) = GetLookUpIndex(xValue, xScale, xMin + 1, xMax - 1); (int y0, int y1, float yT) = GetLookUpIndex(yValue, yScale, yMin + 1, yMax - 1); float T = ReadTable(table, x0, y0, xMin, yMin); float U = ReadTable(table, x0, y1, xMin, yMin); float V = T + Math.Abs(xT) * (ReadTable(table, x1, y0, xMin, yMin) - T); float W = U + Math.Abs(xT) * (ReadTable(table, x1, y1, xMin, yMin) - U); float result = V + (W - V) * Math.Abs(yT); return result; }&lt;/quote&gt;&lt;p&gt;A bilinear interpolation is a very common operation in computer graphics. This is how textures are sampled when placed on 3D geometry.&lt;/p&gt;&lt;p&gt;In the next section, we will see that the engine thrust calculation interpolates between the output of 2 two dimensional tables. Adding this third interpolation means this calculation is now a trilinear interpolation. Interpolating between two tables is how mipmaps are blended together in computer graphics. How neat is that?&lt;/p&gt;&lt;head rend="h1"&gt;Engine&lt;/head&gt;&lt;p&gt;The next system we’re going to add is the engine. In my previous project, the engine was dead simple. The player selected a throttle value from [0, 1], which is multiplied by the plane’s total thrust. This works fine for that simulation and even gives us the ability to reduce thrust to zero, so the plane becomes a glider.&lt;/p&gt;&lt;p&gt;However, it is not a realistic simulation of how a jet engine works. In reality, a jet engine still produces some thrust at idle throttle. And there are more factors that affect thrust output than just the throttle setting.&lt;/p&gt;&lt;p&gt;The thrust output of a jet engine decreases with altitude and increases with speed. As altitude increases, the air gets thinner and the jet engine becomes weaker. But as speed increases, dynamic pressure, and thus pressure in the engine, increases and the engine becomes stronger. These two effects need to be considered at the same time to find the thrust output at any given moment.&lt;/p&gt;&lt;p&gt;Additionally, we have to consider how jet engines behave in terms of RPM. Just like piston engines (like in a typical car), jet engines have rotating components whose speed increases with throttle. The max RPM of a jet is much higher than a piston engine, however the range of possible RPM is smaller.&lt;/p&gt;&lt;p&gt;The engine in an F-16 has a maximum RPM of about 14,000. This is at the maximum non-afterburner power, called military power. When throttle is reduced to the lowest setting, idle, the RPM falls to about 8,400 RPM or about 60% of the max. Planes of course do not have a transmission like a car does, so this range of RPM also covers the range of thrust needed at all stages of flight.&lt;/p&gt;&lt;p&gt;At idle throttle, the engine runs at 60% max RPM, but only produces 8% of max thrust. At military power, the engine runs at 100% RPM and produces 100% thrust.&lt;/p&gt;&lt;p&gt;Military power is selected when the pilot moves the throttle lever to 77% of it’s max setting. Pushing the throttle beyond that engages the afterburner and produces even more thrust. Setting the throttle lever to 100% is called max power. Max power provides about 57% more thrust than military power. Engine RPM does not increase when using afterburner.&lt;/p&gt;&lt;p&gt;A significant difference between a piston engine and a jet engine is how fast the engine can change RPM. In a car, you can put the transmission in neutral and rev the engine up and down very quickly. But a jet engine is much slower to respond to changes in throttle, regardless of how fast the pilot moves the throttle lever. Generally, it can take several seconds to go from idle to military power or vice versa.&lt;/p&gt;&lt;p&gt;The reasons why jet engines are slower to change RPM are complicated. The change in throttle is managed by a computer to avoid compressor stall, which can cause damage or shut down of the engine. This computer will change engine parameters slowly to avoid compressor stall or any other problems that might be caused by moving the throttle too quickly.&lt;/p&gt;&lt;head rend="h2"&gt;Power&lt;/head&gt;&lt;p&gt;The behavior of the jet engine is included in the textbook’s flight model. RPM is not explicitly modeled, but is abstracted as power. The pilot chooses a commanded power level and the engine’s current power setting will move towards this over time. This behavior is spring-like, thus a larger difference will cause the current power setting to change faster. It takes about 2 seconds to increase from idle to military power in this flight model.&lt;/p&gt;&lt;p&gt;The first step is to translate the player’s throttle setting into engine power. This is a fairly simple function that maps military power, or 77% throttle, to 50% power. Full afterburner, or 100%, is mapped to 100% power. This is called the “throttle gearing”, but don’t confuse that with a car’s gearing. It’s much simpler.&lt;/p&gt;&lt;quote&gt;FUNCTION TGEAR(THTL) ! Power command v. thtl. relationship IF(THTL.LE.0.77) THEN TGEAR = 64.94*THTL ELSE TGEAR = 217.38*THTL-117.38 END IF RETURN END&lt;/quote&gt;&lt;p&gt;In C#, this is translated as:&lt;/p&gt;&lt;quote&gt;public static float CalculateThrottleGear(float throttle) { // maps throttle 0 - 0.77 to power 0% - 50% // maps throttle 0.77 - 1.0 to power 50% - 100% float power; if (throttle &amp;lt;= militaryPowerThrottle) { power = 64.94f * throttle; } else { power = 217.38f * throttle - 117.38f; } return power; }&lt;/quote&gt;&lt;p&gt;Those constants might seem weird, but they just define two lines with different slopes. The two lines intersect when the throttle is 0.77.&lt;/p&gt;&lt;p&gt;The player’s throttle setting is used to calculate the commanded power level. The rate of change of engine power also depends on the current power level. This rate is calculated in the functions PDOT and RTAU:&lt;/p&gt;&lt;quote&gt;FUNCTION PDOT(P3,P1) ! PDOT= rate of change of power IF (P1.GE.50.0) THEN ! P3= actual power, P1= power command IF (P3.GE.50.0) THEN T=5.0 P2=P1 ELSE P2=60.0 T=RTAU(P2-P3) END IF ELSE IF (P3.GE.50.0) THEN T=5.0 P2=40.0 ELSE P2=P1 T=RTAU(P2-P3) END IF END IF PDOT=T*(P2-P3) RETURN END FUNCTION RTAU(DP) ! used by function PDOT IF (DP.LE.25.0) THEN RTAU=1.0 ! reciprocal time constant ELSE IF (DP.GE.50.0)THEN RTAU=0.1 ELSE RTAU=1.9-.036*DP END IF RETURN END&lt;/quote&gt;&lt;p&gt;PDOT means power rate of change. In C#, this is translated as:&lt;/p&gt;&lt;quote&gt;float CalculatePowerRateOfChange(float actualPower, float commandPower) { // calculates how fast power output should change based on commanded power float T; float p2; if (commandPower &amp;gt;= 50.0) { if (actualPower &amp;gt;= 50.0) { T = 5.0f; p2 = commandPower; } else { p2 = 60.0f; T = CalculateRTau(p2 - actualPower); } } else { if (actualPower &amp;gt;= 50.0) { T = 5.0f; p2 = 40.0f; } else { p2 = commandPower; T = CalculateRTau(p2 - actualPower); } } float pdot = T * (p2 - actualPower); return pdot; } float CalculateRTau(float deltaPower) { float rTau; if (dp &amp;lt;= 25.0) { rTau = 1.0f; } else if (dp &amp;gt;= 50.0) { rTau = 0.1f; } else { rTau = 1.9f - 0.036f * dp; } return rTau; }&lt;/quote&gt;&lt;p&gt;Power rate of change is the velocity of the power level. The most important line is this:&lt;/p&gt;&lt;quote&gt;float pdot = T * (p2 - actualPower);&lt;/quote&gt;&lt;p&gt;The velocity depends on the quantity (p2 – actualPower). Let’s call this value deltaPower. A larger deltaPower means a larger velocity. This is scaled by the factor T. The complexity comes from selecting the values for p2 and T. p2 is sometimes the commandPower value. T is sometimes the result of calling CalculateRTau.&lt;/p&gt;&lt;p&gt;These values are selected by the if statements above. These check for two conditions, the commandedPower being above 50%, and the actualPower being above 50%. This is checking whether the afterburner is being requested, and whether the afterburner is currently active. Remember that afterburner starts at 77% throttle, but 50% power.&lt;/p&gt;&lt;p&gt;If the afterburner is not active, then the T is given the value of CalculateRTau. If it is active, then T is given the constant value of 5.0. This matches with our expectation of how the engine’s RPM changes. When not in afterburner, the engine RPM should change slowly, thus power changes slowly. When in afterburner, fuel flow into the afterburner can change quickly, thus power changes quickly.&lt;/p&gt;&lt;p&gt;If we look at the function CalculateRTau, we can see that T can vary in the range [0.1, 1.0]. This depends on deltaPower. When the engine is not in afterburner, T can be at most 1.0. In afterburner, T is 5.0. That means power can change about 5 times faster when in afterburner. When multiplied with deltaPower, pdot can be as large as 250% per second.&lt;/p&gt;&lt;p&gt;The smallest value of T occurs when deltaPower is 50 or greater. This occurs when actualPower is 0 and commanded power is 50%, for example. This will cause the power rate of change to be quite small at only 6% per second. Note that this is simply the instantaneous rate of change. As the actual power rises, T will become larger and the rate of change will increase.&lt;/p&gt;&lt;p&gt;Now the reason why p2 is used instead of commandedPower is to handle the case where commandedPower is over 50% and actualPower is below 50%, or vice versa. The pilot is requesting afterburner, but the engine has not reached military power yet. In that case, deltaPower would become very large and the simulation would change power levels too quickly. To avoid this, an arbitrary constant is chosen that is on the opposite side of 50%, but not very far.&lt;/p&gt;&lt;p&gt;So if the actualPower is 0%, but commandedPower is 100%, p2 is set to the value of 60. This limits deltaPower to a maximum value of 60, instead of 100. And in the case where actualPower is 100% and commandedPower is 0%, deltaPower is limited to -60.&lt;/p&gt;&lt;p&gt;Another behavior of this code is that CalculateRTau does not handle cases where deltaPower is negative. In this case, the function returns 1, the highest value it can return. This means that the power can decrease 10 times faster than it can increase, in the most extreme case.&lt;/p&gt;&lt;p&gt;I don’t know if this is an intentional effect. This may match the behavior of real jet engines, or it may be an oversight by the authors. You can play with the behavior by adding a few calls to Mathf.Abs().&lt;/p&gt;&lt;p&gt;The practical effect of all this is that the plane’s power will lag behind the player’s throttle setting. The pilot needs to make sure that they provide enough time for the power level to change when moving the throttle.&lt;/p&gt;&lt;p&gt;The HUD for this project is mostly reused from the previous flight sim project. But the throttle indicator must be updated, since it can’t show the difference between commanded power and current power.&lt;/p&gt;&lt;p&gt;Previously, the red bar used to show the player’s throttle setting. This worked fine since power lag was not modeled. In this project, the red bar shows the engine’s current power level. I added a triangle marker to show the commanded power setting.&lt;/p&gt;&lt;p&gt;As you move the throttle, you’ll see that current power level changes quickly when there is a large difference from commanded power, and it slows down as it approaches. And when the engine enters afterburner, the power level changes very quickly.&lt;/p&gt;&lt;head rend="h2"&gt;Thrust&lt;/head&gt;&lt;p&gt;Engine power is a fairly abstract variable in this flight model. It doesn’t really correspond to any physical variable. Once we calculate the current power, we use it to find the thrust generated by the engine. Thrust in this flight model is defined in terms of pounds-force (lbf).&lt;/p&gt;&lt;p&gt;Thrust is defined by a group of look up tables. Each table has two dimensions as input, mach number and altitude, and the output is thrust. This gives us different thrust values in different flight conditions. Mach is input as 0.0 to 1.0 mach, in increments of 0.2 mach. Altitude is input as 0 to 50,000 ft, in increments of 10,000 ft. In other words, the table has dimensions 6×6.&lt;/p&gt;&lt;p&gt;The lookup tables in this flight model correspond to idle power, military power, and max power (full afterburner). The engine’s power value is used to perform a third interpolation between the output values of these tables. This makes the thrust calculation a trilinear interpolation.&lt;/p&gt;&lt;p&gt;At idle throttle, the thrust output has 100% influence from the idle table. When the throttle is halfway to military power, the output has 50% influence from the idle table and 50% influence from the military table. Above military power, the output will have some influence from the military power table and the max power table.&lt;/p&gt;&lt;p&gt;The code to read one table in Fortran is given:&lt;/p&gt;&lt;quote&gt;DATA A/ [IDLE TABLE OMITTED] DATA B/ [MIL TABLE OMITTED] DATA C/ [MAX TABLE OMITTED] H=0.0001*ALT I=INT(H) IF (I.GE.5) I=4 DH=H-FLOAT(I) RM=5.*RMACH M=INT(RM) IF (M.GE.5) M=4 DM=RM-FLOAT(M) CDH=1.0-DH&lt;/quote&gt;&lt;p&gt;These parameters are used to perform the table lookups:&lt;/p&gt;&lt;quote&gt;TMIL= S + (T-S)*DM IF (POW.LT.50.0) THEN S= A(I,M)*CDH + A(I+1,M)*DH T= A(I,M+1)*CDH + A(I+1,M+1)*DH TIDL= S + (T-S)*DM THRUST= TIDL + (TMIL-TIDL)*POW/50.0 ELSE S= C(I,M)*CDH + C(I+1,M)*DH T= C(I,M+1)*CDH + C(I+1,M+1)*DH TMAX= S + (T-S)*DM THRUST= TMIL + (TMAX-TMIL)*(POW-50.0)*0.02 END IF&lt;/quote&gt;&lt;p&gt;The output of the military power table, TMIL, is always calculated. If the power level is under 50, then the idle table is calculated as well, TIDL. Otherwise the max table is calculated, TMAX. The output of the two table lookups is then interpolated again to calculate the final thrust value, THRUST.&lt;/p&gt;&lt;p&gt;Altogether, this forms a trilinear lookup. To translate this to C#, we call BilinearLookup twice. Then those two results are interpolated based on the power level:&lt;/p&gt;&lt;quote&gt;float InterpolateThrust(float thrust1, float thrust2, float power) { float result = Mathf.LerpUnclamped(thrust1, thrust2, power * 0.02f); return result; } float CalculateThrust(float power, float altitude, float rMach) { float a = Mathf.Max(0, altitude); float m = Mathf.Max(0, rMach); float thrust; float thrustMilitary = Table.BilinearLookup(a, 0.0001f, m, 5, militaryPowerTable, 0, 6, 0, 6); // perform trilinear interpolation if (power &amp;lt; 50.0) { float thrustIdle = Table.BilinearLookup(a, 0.0001f, m, 5, idlePowerTable, 0, 6, 0, 6); thrust = InterpolateThrust(thrustIdle, thrustMilitary, power); } else { float thrustMax = Table.BilinearLookup(a, 0.0001f, m, 5, maxPowerTable, 0, 6, 0, 6); thrust = InterpolateThrust(thrustMilitary, thrustMax, power - 50.0f); } return thrust; }&lt;/quote&gt;&lt;p&gt;The output of this calculation is the plane’s thrust in pounds-force. A simple unit conversion allows us to apply it in newtons to a Unity rigidbody:&lt;/p&gt;&lt;quote&gt;void UpdateThrust(float dt) { engine.ThrottleCommand = Throttle; engine.Mach = Mach; engine.Altitude = AltitudeFeet; engine.Update(dt); Rigidbody.AddRelativeForce(new Vector3(0, 0, engine.Thrust * poundsForceToNewtons)); }&lt;/quote&gt;&lt;head rend="h1"&gt;Forces&lt;/head&gt;&lt;head rend="h2"&gt;Lift force vs Normal force&lt;/head&gt;&lt;p&gt;In the previous flight sim project, we calculated a plane’s lift force using the angle of attack and an AnimationCurve. This is the very core of the flight simulator and is what enables flight. The flight model from the textbook does not calculate lift force.&lt;/p&gt;&lt;p&gt;Instead what this flight model calculates is normal force. Recall that lift force is perpendicular to the aircraft’s velocity vector. Normal force is perpendicular to the aircraft’s nose. This distinction is subtle at a low angle of attack, but it becomes significant at a high angle of attack.&lt;/p&gt;&lt;p&gt;There are two more analogous forces to consider, drag and axial force. Drag is always exactly opposite to the aircraft’s velocity vector while axial force is opposite the aircraft’s nose. Lift and drag are perpendicular to each other and form one set of forces. Normal and axial form another perpendicular set. It’s important to understand that these two sets of forces are equally valid. In fact, they are simply the consequence of choosing different basis vectors for measuring force.&lt;/p&gt;&lt;p&gt;And of course there is the side force that points to the right. These forces are applied on the normal, side, and longitudinal (axial) axes, which are equivalent to the X, Y, and Z axes.&lt;/p&gt;&lt;p&gt;Imagine all of the forces being produced by the aircraft are summed into a single force vector. This vector would be strongly vertical, because the plane is generating enough lift to support it’s own weight, and somewhat backwards because of drag. When this vector is projected onto the lift vector, the result is the lift force. When it’s projected onto the normal vector, the result is the normal force.&lt;/p&gt;&lt;p&gt;Choosing to represent these forces as lift/drag or normal/axial is arbitrary. The textbook flight model only deals with normal/axial force. I suspect that’s because it’s easier to measure the physical forces when using normal/axial forces in a wind tunnel, since those are always aligned with the plane’s local axes.&lt;/p&gt;&lt;p&gt;The normal force is very similar to lift for low angles of attack. Lift force peaks at the stall AOA and then declines. Normal force similarly peaks at stall AOA, but it then increases again to peak at 90 AOA, with an even higher force. 90 degrees AOA means the plane is falling downwards belly first, so it’s no longer producing lift over the wings. Instead the normal vector and the drag vector are now aligned. All of the drag force projected onto the normal vector results in a large normal force.&lt;/p&gt;&lt;p&gt;We can calculate the lift force from the normal and axial force. Both normal and axial force may contribute to the lift force, so a complete projection needs to use both. This is the formula:&lt;/p&gt;\(\text{Lift} = \text{normal} * \cos{(\text{alpha})} – \text{axial} * \sin{(\text{alpha})}\)&lt;p&gt;When we apply this formula to the normal force from the textbook, this is the result:&lt;/p&gt;&lt;p&gt;Oh wait, that’s upside down. Recall that the Z axis points downward in this coordinate system. So a negative Z value is an upwards force. Still though, the chart is a little confusing. I inverted the values below to make it more intuitive.&lt;/p&gt;&lt;p&gt;We can see at 90 degrees AOA, the normal force stays relatively high while the lift force drops to zero. This roughly matches with the chart from aerospaceweb.org above.&lt;/p&gt;&lt;p&gt;Also note that the textbook only provides table values up to 45 degrees AOA. The extrapolation of the table lookup function is what allows us to have normal force values up to 90 degrees AOA. Additionally, the table only goes down to -10 degrees AOA. We can extrapolate further, but the data will be inaccurate by -30 degrees AOA. Large negative AOA values will quickly become inaccurate. So when you’re flying, don’t do that.&lt;/p&gt;&lt;p&gt;Anyways, adding these forces to our simulator is easy. The functions are fairly simple. They are called CZ, CY, and CX. These calculate the coefficients of force on the Z, Y, and X axes respectively. Note that these functions are the coefficients, not the force values themselves. They are used to calculate the force later on.&lt;/p&gt;&lt;p&gt;CZ or the normal coefficient is calculated like this:&lt;/p&gt;&lt;quote&gt;FUNCTION CZ(ALPHA,BETA,EL) REAL A(-2:9) C DATA A/ [TABLE OMITTED] C S = 0.2*ALPHA K = INT(S) IF(K.LE.-2) K=-1 IF(K.GE.9) K=8 DA = S - FLOAT(K) L = K + INT(SIGN(1.1,DA)) S = A(K) + ABS(DA)*(A(L)-A(K)) CZ = S*(1-(BETA/57.3)**2) - .19*(EL/25.0) C RETURN END&lt;/quote&gt;&lt;p&gt;The bulk of this code is just the table interpolation function. The table only depends on ALPHA and the output is S. The only new part here is the last line, where CZ is assigned a value. S is reduced based on the value of BETA and another term is subtracted based on EL, the elevator angle.&lt;/p&gt;&lt;p&gt;This is very easy to translate to C#:&lt;/p&gt;&lt;quote&gt;float GetZAxisForceCoefficient(float alpha, float beta, float elevator) { float S = Table.LinearLookup(alpha, 0.2f, zAxisTable, -2, 10); float CZ = S * (1 - Mathf.Pow(beta * Mathf.Deg2Rad, 2)) - 0.19f * (elevator / 25.0f); return CZ; }&lt;/quote&gt;&lt;p&gt;CY or the side coefficient is even simpler. It doesn’t even have a lookup table. Side force is perpendicular to both normal and axial force.&lt;/p&gt;&lt;quote&gt;FUNCTION CY(BETA,AIL,RDR) CY = -.02*BETA + .021*(AIL/20.0) + .086*(RDR/30.0) C RETURN END&lt;/quote&gt;&lt;p&gt;Side coefficient depends solely on beta, aileron angle, and rudder angle.&lt;/p&gt;&lt;p&gt;In C#:&lt;/p&gt;&lt;quote&gt;float GetYAxisForceCoefficient(float beta, float aileron, float rudder) { float CY = -0.02f * beta + 0.021f * (aileron / 20.0f) + 0.086f * (rudder / 30.0f); return CY; }&lt;/quote&gt;&lt;p&gt;CX or the axial coefficient is basically what creates drag on the aircraft. This function is a little more complicated since it performs a bilinear interpolation, with alpha and elevator angle as the inputs.&lt;/p&gt;&lt;quote&gt;FUNCTION CX(ALPHA,EL) REAL A(-2:9,-2:2) C DATA A/ [TABLE OMITTED] C S = 0.2*ALPHA K = INT(S) IF(K.LE.-2) K=-1 IF(K.GE.9) K=8 DA = S - FLOAT(K) L = K + INT(SIGN(1.1,DA)) S = EL/12.0 M = INT(S) IF(M.LE.-2) M=-1 IF(M.GE.2) M=1 DE = S - FLOAT(M) N = M + INT(SIGN(1.1,DE)) V = A(K,M) + ABS(DA)*(A(L,M)-A(K,M)) W = A(K,N) + ABS(DA)*(A(L,N)-A(K,N)) CX = V + (W-V)*ABS(DE) C RETURN END&lt;/quote&gt;&lt;p&gt;Thanks to the table lookup functions, this is easy to translate to C#:&lt;/p&gt;&lt;quote&gt;float GetXAxisForceCoefficient(float alpha, float elevator) { float result = Table.BilinearLookup(alpha, 0.2f, elevator, 1f / 12f, xAxisTable, -2, 9, -2, 2); return result; }&lt;/quote&gt;&lt;p&gt;These three functions define all of the linear force coefficients applied to the aircraft during flight. None of these will rotate the aircraft. That is handled by a different and more complicated set of calculations.&lt;/p&gt;&lt;head rend="h1"&gt;Moments&lt;/head&gt;&lt;p&gt;Moment is another word for torque. (There is a subtle difference, but who cares?🤓) The F-16 flight model uses another set of look up tables to compute the moment for the aircraft.&lt;/p&gt;&lt;p&gt;In the previous flight sim, torque was not actually calculated. Instead, the flight model calculates the angular acceleration directly. This ignores the mass of the plane when applying the torque. This is a simplification. A more realistic flight model would take into account the mass of the aircraft when applying torque.&lt;/p&gt;&lt;p&gt;Note that mass is not sufficient to model rotations. When it comes to rotation, the analogy to mass is called moment of inertia. Just like mass is the property that measures an object’s resistance to force, moment of inertia is the resistance to torque. But unlike mass, moment of inertia can differ on all 3 axes. This means a torque on the X axis will result in a different angular acceleration than the same torque on the Y axis, for example.&lt;/p&gt;&lt;p&gt;Moment of inertia is a four-dimensional value, called AXX, AYY, AZZ, and AXZ in the textbook code. This flight model contains it’s own calculations for angular velocity using these moment of inertia values.&lt;/p&gt;&lt;p&gt;The flight model contains several functions that calculate the moment of the aircraft. The three basic functions are called CM, CL, and CN. These calculate the moments around the Y, X, and Z axes, AKA pitch, roll, and yaw, respectively. (L stands for longitudinal, which is the X axis. N stands for normal, which is the Z axis. M stands for… something)&lt;/p&gt;&lt;p&gt;These functions are all simple look up tables. CM (pitch) uses alpha and elevator angle as the input. CL (roll) and CN (yaw) use alpha and beta as the input. CM is basically the same as the other lookup table functions. CL and CN are similar to each other since they both use a symmetric table. This is because the plane is symmetric on the lateral axis, so a single table can represent the left and right sides. Their final output is then multiplied by the sign of beta.&lt;/p&gt;&lt;quote&gt;FUNCTION CL(ALPHA,BETA) REAL A(-2:9,0:6) DATA A/ [DATA OMITTED] S = 0.2*ALPHA K = INT(S) IF(K.LE.-2) K=-1 IF(K.GE.9) K=8 DA = S - FLOAT(K) L = K + INT(SIGN(1.1,DA)) S = .2*ABS(BETA) M = INT(S) IF(M.EQ.0) M=1 IF(M.GE.6) M=5 DB = S - FLOAT(M) N = M + INT(SIGN(1.1,DB)) T = A(K,M) U = A(K,N) V = T + ABS(DA)*(A(L,M) - T) W = U + ABS(DA)*(A(L,N) - U) DUM = V + (W - V) * ABS(DB) CL = DUM + SIGN(1.0,BETA) RETURN END&lt;/quote&gt;&lt;p&gt;Note that there is an error in the textbook code. The final operation “CL = DUM + SIGN(…)” should use multiplication instead of addition. Otherwise this operation doesn’t make any sense.&lt;/p&gt;&lt;p&gt;When translated into C#:&lt;/p&gt;&lt;quote&gt;float GetYAxisMoment(float alpha, float elevator) { float result = Table.BilinearLookup(alpha, 0.2f, elevator, 1f / 12f, yMomentTable, -2, 9, -2, 2); return result; } float GetXAxisMoment(float alpha, float beta) { float DUM = Table.BilinearLookup(alpha, 0.2f, Mathf.Abs(beta), 0.2f, xMomentTable, -2, 9, 0, 7); float CL = DUM * Mathf.Sign(beta); return CL; } float GetZAxisMoment(float alpha, float beta) { float DUM = Table.BilinearLookup(alpha, 0.2f, Mathf.Abs(beta), 0.2f, zMomentTable, -2, 9, 0, 7); float CL = DUM * Mathf.Sign(beta); return CL; }&lt;/quote&gt;&lt;p&gt;Notice that CM takes “elevator” as an argument, so this is where the elevator’s turning effect is calculated. But CL and CN do not take any control surface as an argument. These functions only apply moment based on alpha and beta. For example, at high angles of sideslip, the plane tends to roll. On real planes, this is caused by wing sweep. In this flight model, it’s caused by the CL function.&lt;/p&gt;&lt;p&gt;Elevators are applied in CM, but rudder and ailerons are not. Those are actually handled by four more functions, called DLDA, DLDR, DNDA, and DNDR. The names are cryptic, but it just means which axis is affected from which control surface.&lt;/p&gt;&lt;p&gt;The “L” stands for longitudinal, so DLDA is the longitudinal moment from the ailerons, A. DLDR is the longitudinal moment from the rudder, R. The “N” stands for normal, so those functions are the normal axis moment from aileron and rudders.&lt;/p&gt;&lt;p&gt;These four functions are eventually summed with the CL and CN functions above. These functions mean that roll is affected by aileron and rudder, and yaw is affected by aileron and rudder.&lt;/p&gt;&lt;head rend="h2"&gt;Damping&lt;/head&gt;&lt;p&gt;There is one more set of coefficients that must be calculated. These are the damping coefficients and they depend solely on alpha. These values are stored in 9 distinct 1D lookup tables. The code for these lookups is the same as the other lookup code.&lt;/p&gt;&lt;p&gt;These values are stored in an array of length 9 called D.&lt;/p&gt;&lt;p&gt;Damping is the moment that opposes the angular velocity of an aircraft, essentially angular drag. They affect the other moment values in somewhat complex ways. For example, some of them are combined with the plane’s current bank value to affect the roll moment.&lt;/p&gt;&lt;p&gt;What isn’t clear is what the damping values actually represent. In the C# code, I added these comments explaining their meaning:&lt;/p&gt;&lt;quote&gt;// D[0] = CXq // D[1] = CYr // D[2] = CYp // D[3] = CZq // D[4] = Clr // D[5] = Clp // D[6] = Cmq // D[7] = Cnr // D[8] = Cnp&lt;/quote&gt;&lt;p&gt;Hope this helps!&lt;/p&gt;&lt;p&gt;The best I can tell is that “CXq” is the damping moment on the X axis relative to q, which is the angular velocity around the Y axis. The other damping values follow this naming scheme.&lt;/p&gt;&lt;p&gt;This is yet another example of aerodynamics texts with poor variable names.&lt;/p&gt;&lt;head rend="h1"&gt;Complete Flight Model&lt;/head&gt;&lt;p&gt;With all of the individual coefficients defined, we can now implement the complete flight model for the F-16. This flight model actually contains it’s own physics integrator. The text provides it’s own code for calculating velocity and angular velocity from the aerodynamic forces.&lt;/p&gt;&lt;p&gt;Strictly speaking, we don’t need to use this code since Unity allows us to provide those same forces and then performs the physics calculation for us. Setting the mass is easy enough, we just have to convert slugs to kilograms. The textbook code calculates acceleration by dividing the force by the aircraft mass. We simply omit this division, convert the forces to newtons, and apply it to the rigidbody.&lt;/p&gt;&lt;p&gt;However the moment of inertia is more complicated. The textbook provides the 4 dimensional MOI values, but Unity expects a 3 dimensional inertia tensor. That inertia tensor is then rotated by a quaternion called “inertiaTensorRotation”. I have no idea how to calculate this quaternion from the textbook’s provided value.&lt;/p&gt;&lt;p&gt;Therefore, we continue to use the textbook’s code for applying moment and simply apply the resulting angular acceleration to the rigidbody.&lt;/p&gt;&lt;p&gt;The Fortran code for the flight model is concise, yet scrutable. The first step is to read the plane’s current state from the input state vector X. This is simply an array that contains all of the relevant data for this frame.&lt;/p&gt;&lt;quote&gt;VT= X(1); ALPHA= X(2)*RTOD; BETA= X(3)*RTOD PHI=X(4); THETA= X(5); PSI= X(6) P= X(7); Q= X(8); R= X(9); ALT= X(12); POW= X(13)&lt;/quote&gt;&lt;p&gt;VT is the plane’s velocity in feet per second.&lt;/p&gt;&lt;p&gt;ALPHA and BETA are the plane’s AOA and AOS in degrees. RTOD is the constant to convert from radians to degrees.&lt;/p&gt;&lt;p&gt;PHI, THETA, and PSI are the plane’s roll, pitch, and yaw in radians.&lt;/p&gt;&lt;p&gt;P, Q, and R are the plane’s angular velocities (or roll rate, pitch rate, and yaw rate) in radians per second.&lt;/p&gt;&lt;p&gt;ALT is the altitude in feet.&lt;/p&gt;&lt;p&gt;POW is the current power level of the engine (0 – 100).&lt;/p&gt;&lt;p&gt;The air data computer (ADC) and engine model are then called using these variables:&lt;/p&gt;&lt;quote&gt;CALL ADC(VT,ALT,AMACH,QBAR); CPOW= TGEAR(THTL) XD(13) = PDOT(POW,CPOW); T= THRUST(POW,ALT,AMACH)&lt;/quote&gt;&lt;p&gt;The ADC function populates the variables AMACH and QBAR, which are the altitude mach and dynamic pressure.&lt;/p&gt;&lt;p&gt;CPOW is the pilot’s commanded power setting. That is, the power level returned by calling the throttle gear function, TGEAR, on the throttle lever position, THTL.&lt;/p&gt;&lt;p&gt;The array XD is the output state vector. Specifically, it holds the calculated derivative for every input value. XD(13) is set to the value calculated by PDOT, which is the velocity of the power level.&lt;/p&gt;&lt;p&gt;T is the thrust in pounds-force output by the engine, calculated using the power level, altitude, and altitude mach.&lt;/p&gt;&lt;p&gt;Then the aerodynamic coefficients are calculated using the force and moment functions:&lt;/p&gt;&lt;quote&gt;CXT = CX (ALPHA,EL) CYT = CY (BETA,AIL,RDR) CZT = CZ (ALPHA,BETA,EL) DAIL= AIL/20.0; DRDR= RDR/30.0 CLT = CL(ALPHA,BETA) + DLDA(ALPHA,BETA)*DAIL &amp;amp; + DLDR(ALPHA,BETA)*DRDR CMT = CM(ALPHA,EL) CNT = CN(ALPHA,BETA) + DNDA(ALPHA,BETA)*DAIL &amp;amp; + DNDR(ALPHA,BETA)*DRDR&lt;/quote&gt;&lt;p&gt;The values CXT, CYT, and CZT are the coefficients on the X, Y, and Z axes, calculated by calling their respective coefficient functions.&lt;/p&gt;&lt;p&gt;EL, AIL, and RDR are the current position of the elevators, ailerons, and rudder in degrees. DAIL and DRDR are simply the angle of these surfaces divided by the max angle. Their range is [-1, 1].&lt;/p&gt;&lt;p&gt;The values CLT, CMT, and CNT are the moment coefficients on the longitudinal, m’lateral, and normal axes. Note that CM calculates moment caused by the elevator position. The effects of the other control surfaces are calculated in the DLDA, DLDR, DNDA, and DNDR functions.&lt;/p&gt;&lt;p&gt;Then some other values are calculated and the damping coefficients are added to the above values:&lt;/p&gt;&lt;quote&gt;TVT= 0.5/VT; B2V= B*TVT; CQ= CBAR*Q*TVT CALL DAMP(ALPHA,D) CXT= CXT + CQ * D(1) CYT= CYT + B2V * ( D(2)*R + D(3)*P ) CZT= CZT + CQ * D(4) CLT= CLT + B2V * ( D(5)*R + D(6)*P ) CMT= CMT + CQ * D(7) + CZT * (XCGR-XCG) CNT= CNT + B2V*(D(8)*R + D(9)*P) - CYT*(XCGR-XCG) * CBAR/B&lt;/quote&gt;&lt;p&gt;I’ll be honest, I straight up don’t know what any of these values are or why they are being applied like this. The effect appears to be angular damping (AKA angular drag) which opposes the plane’s angular velocity.&lt;/p&gt;&lt;p&gt;The value (XCGR – XCG) is the center of gravity reference minus the current center of gravity. This allows us to alter the center of gravity of the aircraft and see how that affects stability.&lt;/p&gt;&lt;p&gt;XCGR is 0.35 for this flight model. XCG is 0.35 by default. XCG is the normalized position of the center of gravity, with a possible range of [0, 1]. This means that when XCG is 0.35, the term (XCGR – XCG) becomes zero and the aircraft is balanced around it’s center of gravity.&lt;/p&gt;&lt;p&gt;The center of gravity term affects CMT and CNT, which are the pitch and yaw axes. The roll axis is not affected.&lt;/p&gt;&lt;p&gt;The next block of code is fun:&lt;/p&gt;&lt;quote&gt;CBTA = COS(X(3)); U=VT*COS(X(2))*CBTA V= VT * SIN(X(3)); W=VT*SIN(X(2))*CBTA STH = SIN(THETA); CTH= COS(THETA); SPH= SIN(PHI) CPH = COS(PHI) ; SPSI= SIN(PSI); CPSI= COS(PSI) QS = QBAR * S ; QSB= QS * B; RMQS= QS/MASS GCTH = GD * CTH ; QSPH= Q * SPH AY = RMQS*CYT ; AZ= RMQS * CZT&lt;/quote&gt;&lt;p&gt;This writhing mass of arithmetic is simply pre-calculating a lot of the values that are used to calculate the aerodynamic forces. Some of these values are used in multiple places, so to avoid repeating them, they are pulled out of those equations and placed here.&lt;/p&gt;&lt;p&gt;This is essentially the “common subexpression” optimization pass of a compiler, but applied manually.&lt;/p&gt;&lt;p&gt;The important variables are U, V, and W, which is the plane’s velocity on the X, Y, and Z axes respectively.&lt;/p&gt;&lt;p&gt;QS is QBAR (dynamic pressure) times S (wing area).&lt;/p&gt;&lt;p&gt;Now the aerodynamic forces are calculated:&lt;/p&gt;&lt;quote&gt;UDOT = R*V - Q*W - GD*STH + (QS * CXT + T)/MASS VDOT = P*W - R*U + GCTH * SPH + AY WDOT = Q*U - P*V + GCTH * CPH + AZ DUM = (U*U + W*W) xd(1) = (U*UDOT + V*VDOT + W*WDOT)/VT xd(2) = (U*WDOT - W*UDOT) / DUM xd(3) = (VT*VDOT- V*XD(1)) * CBTA / DUM&lt;/quote&gt;&lt;p&gt;Once again, I don’t actually understand what I’m reading. UDOT etc are the accelerations on each axis. These values are then used to update the output state vector xd(1), xd(2), and xd(3), which are the VT, ALPHA, and BETA that will be used in the next frame.&lt;/p&gt;&lt;p&gt;It appears that this flight model is calculating the change in alpha and beta directly from the change in velocity. This is not necessary in C#, since we can calculate alpha and beta fresh in each frame.&lt;/p&gt;&lt;p&gt;But I don’t fully understand how UDOT is calculated. R and Q are angular velocities, so multiplying them with linear velocity doesn’t make any sense. Perhaps this is some physics equation that I’m not familiar with.&lt;/p&gt;&lt;p&gt;GD * STH is the gravity acceleration times sin(theta). This is simply how gravity is applied. When the plane is level (theta = 0), sin(theta) is 0. The plane experiences no gravity acceleration on the X axis (the forward axis). When the plane is pointed straight down, sin(theta) = 1, so the plane experiences the full force of gravity pulling on the X axis.&lt;/p&gt;&lt;p&gt;A similar calculation is made for every axis.&lt;/p&gt;&lt;p&gt;For UDOT, the final term is (QS * CXT + T) / MASS. This is the coefficient CXT plus the thrust from the engine, divided by mass. VDOT and WDOT have similar final terms, made more difficult to read by the common subexpression optimization.&lt;/p&gt;&lt;p&gt;Ignoring the other terms, the 3 accelerations can be written:&lt;/p&gt;&lt;quote&gt;UDOT = (QS * CXT + T)/MASS VDOT = AY WDOT = AZ&lt;/quote&gt;&lt;p&gt;Then the variables can be expanded and rewritten:&lt;/p&gt;&lt;quote&gt;UDOT = (QBAR * S * CXT + T) / MASS VDOT = (QBAR * S * CYT) / MASS WDOT = (QBAR * S * CZT) / MASS&lt;/quote&gt;&lt;p&gt;This is simply the force coefficient times QBAR (dynamic pressure) times S (wing area). Then thrust is added to the X axis. This is how all forces are applied to the aircraft.&lt;/p&gt;&lt;p&gt;Recall the lift equation from my previous project:&lt;/p&gt;\(L=\frac12\times A\times\rho\times C_L\times v^2\)&lt;list rend="ul"&gt;&lt;item&gt;L is the resulting lift force&lt;/item&gt;&lt;item&gt;A is the surface area&lt;/item&gt;&lt;item&gt;ρ (rho) is the air density&lt;/item&gt;&lt;item&gt;CL is the coefficient of lift&lt;/item&gt;&lt;item&gt;v is the velocity&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The surface area A is equivalent to the wing area S in the Fortran code. CL is equivalent to the variables CXT, CYT, or CZT. The factor ρ * v2 is equivalent to QBAR. Thus we are essentially calculating a lift force on all three axes. But remember that we are specifically calculating normal force, not lift force.&lt;/p&gt;&lt;p&gt;The roll, pitch, and yaw state vectors are then updated:&lt;/p&gt;&lt;quote&gt;xd(4) = P + (STH/CTH)*(QSPH + R*CPH) xd(5) = Q*CPH - R*SPH xd(6) = (QSPH + R*CPH)/CTH&lt;/quote&gt;&lt;p&gt;Once again, these equations make zero sense to me🤷♂️. It’s important for the Fortran code, but we will be calculating roll, pitch, and yaw differently in C#.&lt;/p&gt;&lt;p&gt;Aerodynamic moment is about to be calculated. However this depends on the moment of inertia values and some more values derived from those:&lt;/p&gt;&lt;quote&gt;PARAMETER (AXX=9496.0, AYY= 55814.0, AZZ=63100.0, AXZ= 982.0) PARAMETER (AXZS=AXZ**2, XPQ=AXZ*(AXX-AYY+AZZ),GAM=AXX*AZZ-AXZ**2) PARAMETER (XQR= AZZ*(AZZ-AYY)+AXZS, ZPQ=(AXX-AYY)*AXX+AXZS) PARAMETER ( YPR= AZZ - AXX )&lt;/quote&gt;&lt;p&gt;Now the aerodynamic moment is calculated:&lt;/p&gt;&lt;quote&gt;ROLL = QSB*CLT PITCH = QS *CBAR*CMT YAW = QSB*CNT PQ = p*Q QR = Q*R QHX = Q*HX xd(7) = ( XPQ*PQ - XQR*QR + AZZ*ROLL + AXZ*(YAW + QHX) )/GAM xd(8) = ( YPR*P*R - AXZ*(P**2 - R**2) + PITCH - R*HX )/AYY xd(9) = ( ZPQ*PQ - XPQ*QR + AXZ*ROLL + AXX*(YAW + QHX) )/GAM&lt;/quote&gt;&lt;p&gt;There’s a lot of stuff going on here. The output state vectors are updated using the moment of inertia values as well as HX, which is the angular momentum of the spinning engine mass. I don’t know enough about physics to fully understand why these equations are defined like this.&lt;/p&gt;&lt;p&gt;But we can at least see how the moment coefficients are used if we expand the ROLL, PITCH, and YAW variables:&lt;/p&gt;&lt;quote&gt;ROLL = QBAR * S * B * CLT PITCH = QBAR * S * CBAR * CMT YAW = QBAR * S * B * CNT&lt;/quote&gt;&lt;p&gt;ROLL and YAW depend on B, the wingspan of the plane. Pitch depends on CBAR, the mean aerodynamic chord.&lt;/p&gt;&lt;p&gt;The final step is to calculate the world space position of the aircraft. Since we are using Unity rigidbodies to implement the flight model, this step is not translated to C#. But for reference:&lt;/p&gt;&lt;quote&gt;T1= SPH * CPSI; T2= CPH * STH; T3= SPH * SPSI S1= CTH * CPSI; S2= CTH * SPSI; S3= T1 * STH - CPH * SPSI S4= T3 * STH + CPH * CPSI; S5= SPH * CTH; S6= T2*CPSI + T3 S7= T2 * SPSI - T1; S8= CPH * CTH xd(10) = U * S1 + V * S3 + W * S6 ! North speed xd(11) = U * S2 + V * S4 + W * S7 ! East speed xd(12) = U * STH -V * S5 - W * S8 ! Vertical speed AN = -AZ/GD; ALAT= AY/GD;&lt;/quote&gt;&lt;p&gt;Now we can start translating this into C# using Unity’s physics engine to replace some parts.&lt;/p&gt;&lt;p&gt;A lot of the code can be reused from the previous flight sim project. Using it for this new flight model only requires some conversion into customary units and back. The main class that controls everything is Plane. This class contains instances of the AirDataComputer, Engine, and Aerodynamics, which is where the translated Fortran code lives.&lt;/p&gt;&lt;p&gt;One simplification can be made since we are using Unity physics. We do not need to calculate the acceleration of the aircraft manually. That can be done automatically by the physics engine. However, the moment calculation needs to be copied more or less directly from the textbook.&lt;/p&gt;&lt;p&gt;The air data computer needs to be called:&lt;/p&gt;&lt;quote&gt;void UpdateAirData() { float speed = LocalVelocity.magnitude; // m/s float speedFeet = speed * metersToFeet; AltitudeFeet = Rigidbody.position.y * metersToFeet; airData = airDataComputer.CalculateAirData(speedFeet, AltitudeFeet); }&lt;/quote&gt;&lt;p&gt;Then the engine needs to be updated and the thrust force applied:&lt;/p&gt;&lt;quote&gt;void UpdateThrust(float dt) { engine.ThrottleCommand = Throttle; engine.Mach = Mach; engine.Altitude = AltitudeFeet; engine.Update(dt); Rigidbody.AddRelativeForce(new Vector3(0, 0, engine.Thrust * poundsForceToNewtons)); }&lt;/quote&gt;&lt;p&gt;For the aerodynamics class, a struct with all relevant aerodynamic state is passed, similar to the state vector in the Fortran code.&lt;/p&gt;&lt;quote&gt;public struct AerodynamicState { public Vector4 inertiaTensor; public Vector3 velocity; public Vector3 angularVelocity; public AirData airData; public float altitude; public float alpha; public float beta; public float xcg; public ControlSurfaces controlSurfaces; }&lt;/quote&gt;&lt;p&gt;This is populated by the Plane class, which also handles unit conversions:&lt;/p&gt;&lt;quote&gt;AerodynamicState currentState = new AerodynamicState { inertiaTensor = inertiaTensor, velocity = ConvertVectorToAerospace(LocalVelocity) * metersToFeet, angularVelocity = ConvertAngleToAerospace(LocalAngularVelocity), airData = airData, alpha = alpha, beta = beta, xcg = centerOfGravityPosition, controlSurfaces = ControlSurfaces }; var newState = aerodynamics.CalculateAerodynamics(currentState);&lt;/quote&gt;&lt;p&gt;All of the flight model code is located inside the Aerodynamics class.&lt;/p&gt;&lt;p&gt;First step is to call the aerodynamic coefficient functions from above:&lt;/p&gt;&lt;quote&gt;Vector3 GetForceCoefficient(float alpha, float beta, float aileron, float rudder, float elevator) { return new Vector3( GetXAxisForceCoefficient(alpha, elevator), GetYAxisForceCoefficient(beta, aileron, rudder), GetZAxisForceCoefficient(alpha, beta, elevator) ); } Vector3 GetMomentCoefficient(float alpha, float beta, float elevator) { return new Vector3( GetXAxisMomentCoefficient(alpha, beta), GetYAxisMomentCoefficient(alpha, elevator), GetZAxisMomentCoefficient(alpha, beta) ); } ... public AerodynamicForces CalculateAerodynamics(AerodynamicState currentState) { Vector3 forceCoefficient = GetForceCoefficient( currentState.alpha, currentState.beta, currentState.controlSurfaces.aileron, currentState.controlSurfaces.rudder, currentState.controlSurfaces.elevator ); Vector3 momentCoefficient = GetMomentCoefficient( currentState.alpha, currentState.beta, currentState.controlSurfaces.elevator ); }&lt;/quote&gt;&lt;p&gt;Then we calculate the damping values. This function simply performs the 9 table lookups.&lt;/p&gt;&lt;quote&gt;void CalculateDampingValues(float alpha) { float S = 0.2f * alpha; int K = Mathf.Clamp((int)S, -1, 8); float DA = S - K; int L = K + (int)Mathf.Sign(DA); for (int i = 0; i &amp;lt; 9; i++) { dampingTable[i] = ReadDampTable(dampTable, K, i) + Math.Abs(DA) * (ReadDampTable(dampTable, L, i) - ReadDampTable(dampTable, K, i)); } }&lt;/quote&gt;&lt;p&gt;Then the variables we need later are calculated:&lt;/p&gt;&lt;quote&gt;// calculate variables float P = currentState.angularVelocity.x; // roll rate float Q = currentState.angularVelocity.y; // pitch rate float R = currentState.angularVelocity.z; // yaw rate float airspeed = Mathf.Max(1, currentState.velocity.magnitude); float TVT = 0.5f / airspeed; float B2V = wingSpanFt * TVT; float CQ = CBAR * Q * TVT; float DAIL = currentState.controlSurfaces.aileron / 20.0f; float DRDR = currentState.controlSurfaces.rudder / 30.0f; float QS = currentState.airData.qBar * wingAreaFtSquared; float QSB = QS * wingSpanFt;&lt;/quote&gt;&lt;p&gt;Then damping is applied to the force and moment coefficients:&lt;/p&gt;&lt;quote&gt;// damping float CXT = forceCoefficient.x + CQ * dampingTable[0]; float CYT = forceCoefficient.y + B2V * (dampingTable[1] * R + dampingTable[2] * P); float CZT = forceCoefficient.z + CQ * dampingTable[3]; float CLT = momentCoefficient.x + B2V * (dampingTable[4] * R + dampingTable[5] * P); CLT += GetDLDA(currentState.alpha, currentState.beta) * DAIL; CLT += GetDLDR(currentState.alpha, currentState.beta) * DRDR; float CMT = momentCoefficient.y + CQ * dampingTable[6] + CZT * (XCGR - currentState.xcg); float CNT = momentCoefficient.z + B2V * (dampingTable[7] * R + dampingTable[8] * P) - CYT * (XCGR - currentState.xcg) * CBAR / wingSpanFt; CNT += GetDNDA(currentState.alpha, currentState.beta) * DAIL; CNT += GetDNDR(currentState.alpha, currentState.beta) * DRDR;&lt;/quote&gt;&lt;p&gt;Note that the damping array in Fortran is 1 based, while the same array in C# is 0 based.&lt;/p&gt;&lt;p&gt;Forces are calculated from the force coefficients. Since we are using Unity’s physics to apply gravity, the gravity terms are not included here. The force from engine thrust is applied outside of this class. And force is applied to a rigidbody, so acceleration does not need to be calculated manually. So the force calculations are now very simple:&lt;/p&gt;&lt;quote&gt;// forces // Acceleration in original text. Need to calculate force instead of acceleration float UDOT = QS * CXT; float VDOT = QS * CYT; float WDOT = QS * CZT;&lt;/quote&gt;&lt;p&gt;Moments are calculated using largely the same code as the textbook:&lt;/p&gt;&lt;quote&gt;// moments float ROLL = QSB * CLT; float PITCH = QS * CBAR * CMT; float YAW = QSB * CNT; float PQ = P * Q; float QR = Q * R; float QHX = Q * HX; // calculate inertia values float AXX = currentState.inertiaTensor.x; float AYY = currentState.inertiaTensor.y; float AZZ = currentState.inertiaTensor.z; float AXZ = currentState.inertiaTensor.w; float AXZS = AXZ * AXZ; float XPQ = AXZ * (AXX - AYY + AZZ); float GAM = AXX * AZZ - AXZS; float XQR = AZZ * (AZZ - AYY) + AXZS; float ZPQ = (AZZ - AYY) * AXX + AXZS; float YPR = AZZ - AXX; float rollAccel = ((XPQ * PQ) - (XQR * QR) + (AZZ * ROLL) + (AXZ * (YAW + QHX))) / GAM; float pitchAccel = ((YPR * P * R) - (AXZ * (P * P - R * R)) + PITCH - (R * HX)) / AYY; float yawAccel = ((ZPQ * PQ) - (XPQ * QR) + (AXZ * ROLL) + (AXX * (YAW + QHX))) / GAM;&lt;/quote&gt;&lt;p&gt;Finally, the force and angular acceleration is returned:&lt;/p&gt;&lt;quote&gt;public AerodynamicForces CalculateAerodynamics(AerodynamicState currentState) { ... result.force = new Vector3(UDOT, VDOT, WDOT); result.angularAcceleration = new Vector3(rollAccel, pitchAccel, yawAccel); return result; }&lt;/quote&gt;&lt;p&gt;Then in the Plane class, the force and angular acceleration can be applied to the rigidbody:&lt;/p&gt;&lt;quote&gt;// aeroForces in pounds var forces = ConvertVectorToUnity(aeroForces) * poundsForceToNewtons; Rigidbody.AddRelativeForce(forces); // aeroAngularAcceleration changes angular velocity directly Vector3 avCorrection = ConvertAngleToUnity(aeroAngularAcceleration); Rigidbody.AddRelativeTorque(avCorrection, ForceMode.Acceleration); lastAngularAcceleration = avCorrection;&lt;/quote&gt;&lt;p&gt;The plane is now able to fly. But if you try flying it right now, you will quickly find that it is impossible to fly by hand.&lt;/p&gt;&lt;head rend="h1"&gt;Stability&lt;/head&gt;&lt;p&gt;One important aerodynamic effect not modeled in my previous flight sim project is stability. Stability is the behavior of an aircraft when it’s disturbed from it’s flight path. More specifically, it’s how the aircraft behaves when it’s nose vector doesn’t match it’s velocity vector. Stability is the force that pulls the nose vector back towards the velocity vector.&lt;/p&gt;&lt;p&gt;For most aircraft, stability is created by the stabilizers in the tail. A stabilizer is simply a small airfoil (wing). Even without the pilot giving input, the stabilizers act like the fins of a dart. As the plane increases it’s Angle of Attack, the horizontal stabilizer will produce a lift force at the rear of the plane. This creates a torque that pulls the plane’s nose back towards the velocity vector, thus reducing the AOA. Likewise, the vertical stabilizers will create a torque that reduces Angle of Slip.&lt;/p&gt;&lt;p&gt;Keep in mind that stability depends on AOA, just as lift does. When the aircraft has a large AOA, the wings produce lift, which brings the velocity vector towards the nose vector. The stabilizers create torque which brings the nose vector towards the velocity vector. These two forces balance out somewhere and the aircraft will take a new attitude with a new velocity.&lt;/p&gt;&lt;p&gt;However, the aircraft needs to maintain a non-zero AOA to create enough lift to fly straight and level. How does it maintain this AOA when stability works to reduce AOA to zero? The stabilizers can be trimmed to hold a specific AOA. This means that the stabilizers produce zero torque at this particular non-zero AOA. In some planes this must be done manually by the pilot, but in the F-16 this is done automatically by the FCS.&lt;/p&gt;&lt;p&gt;The two forces of lift and stability combine to produce the “feel” of an aircraft’s controls. The tendency for the nose to be pulled towards the velocity vector is called positive stability. Most non-fighter aircraft are designed to have positive stability to maximize safety and ease of flying.&lt;/p&gt;&lt;p&gt;But fighter aircraft like the F-16 are different. These aircraft are often designed to have neutral or even negative stability. Neutral stability means that the aircraft will hold it’s current attitude. Negative stability means that the aircraft will rotate even further away from the velocity vector, at an increasing rate.&lt;/p&gt;&lt;p&gt;The previous flight sim does not model this at all. There is no torque that changes the plane’s attitude except for the steering force. So the behavior is best described as neutrally stable.&lt;/p&gt;&lt;p&gt;This F-16 flight model does include stability. But keep in mind that the real F-16 was designed to have relaxed static stability. This means that it is positively stable, but weakly so. This makes the aircraft more maneuverable and better at retaining energy while turning. But flying an aircraft like this is difficult or even impossible for a human pilot. The plane will depart from steady flight from the smallest stick input or wind gust. The only way a human can handle an aircraft like this during long and stressful missions is with a computerized flight control system.&lt;/p&gt;&lt;head rend="h1"&gt;Flight Control System&lt;/head&gt;&lt;p&gt;A flight control system (FCS) is a computer located between the flight stick and the control surfaces. This computer translates the pilot’s input on the flight stick into control surface movement. It can react to disturbances in the plane’s attitude more quickly and precisely than a human can.&lt;/p&gt;&lt;p&gt;In my previous flight sim project, the control surfaces were purely cosmetic. The actual method used to turn the vehicle was by applying torque directly to the center of mass. That torque was calculated to create a certain amount of angular acceleration without exceeding the plane’s turn rate limit.&lt;/p&gt;&lt;p&gt;For example, the plane had a turn rate on the pitch axis of 60 degrees per second and an acceleration of 120 degrees per second per second. The plane’s turn rate never leaves the range [-60, 60]. Actually, no torque is ever calculated. Unity provides a function to apply angular acceleration directly, ignoring moment of inertia. I chose this behavior to make it easy to both understand the code and to fly the plane.&lt;/p&gt;&lt;p&gt;But this F-16 simulator does depend on the position of the control surfaces. Instead of specifying the acceleration directly, this simulator specifies the torque (moment) and calculates the resulting acceleration. This is more accurate to how serious simulators work and how real planes fly, but this makes controlling the plane more difficult.&lt;/p&gt;&lt;p&gt;The steering system in the previous flight sim project was essentially a perfect FCS that could always achieve the turn rate chosen by the pilot. This is helped by the fact that that simulator does not model aerodynamic stability or instability at all. Spinning out of control was simply not possible.&lt;/p&gt;&lt;p&gt;This F-16 simulator is more difficult to control both because of the more accurate control surfaces and because of the modeled stability. You can actually try to fly this F-16 manually, by disabling the FCS in the config menu.&lt;/p&gt;&lt;p&gt;You will quickly find that the F-16 is almost impossible to fly manually. Every small disturbance from straight and level flight will create small torques that turn your plane unexpectedly. If you try to correct it with the control stick, you will almost certainly overcorrect and send the plane into a new and exciting attitude. This is called pilot induced oscillation.&lt;/p&gt;&lt;p&gt;It simply isn’t possible for a human to react quickly and precisely enough to fly this aircraft. You may be able to fly straight and level with some effort, but you will quickly lose control if you attempt any maneuver. This is indeed a property of the real F-16.&lt;/p&gt;&lt;p&gt;The textbook provides no Fortran code for the FCS. From here on out, it’s my own original code.&lt;/p&gt;&lt;head rend="h2"&gt;PID Controllers&lt;/head&gt;&lt;p&gt;The steering system from the previous project cannot be reused. The solution is to use PID controllers, a topic I’ve covered on this blog before.11&lt;/p&gt;&lt;p&gt;To be more specific, steering in the previous flight sim was easy because we could read the angular velocity of the aircraft and apply a torque that directly countered any undesired movement. This F-16 flight model does not allow us to apply torques directly. We can only set the angle of the control surfaces. This is the problem that PID controllers are good at solving.&lt;/p&gt;&lt;p&gt;Adding the PID controllers is simple. The pilot’s control input is used to select a target angular velocity for the plane, for the 3 axes of rotation. This is given to three independent PID controllers. The output of the PID controllers set the target position for the control surface.&lt;/p&gt;&lt;p&gt;The control surface positions are then passed into the flight model inside AerodynamicState.&lt;/p&gt;&lt;quote&gt;Vector3 targetAV = Vector3.Scale(controlInput, steeringSpeed * steeringSpeedFactor); var accel = lastAngularAcceleration * Mathf.Rad2Deg * dt; controlSurfaceTarget = new Vector3( pitchController.Calculate(dt, av.x, accel.x, targetAV.x), -yawController.Calculate(dt, av.y, accel.y, targetAV.y), rollController.Calculate(dt, av.z, accel.z, targetAV.z) ); var current = ControlSurfaces; ControlSurfaces = new ControlSurfaces( Utilities.MoveTo(current.elevator, controlSurfaceTarget.x, elevatorSpeed, dt, -elevatorRange, elevatorRange), Utilities.MoveTo(current.rudder, controlSurfaceTarget.y, rudderSpeed, dt, -rudderRange, rudderRange), Utilities.MoveTo(current.aileron, controlSurfaceTarget.z, aileronSpeed, dt, -aileronRange, aileronRange) ); ... AerodynamicState currentState = new AerodynamicState { controlSurfaces = ControlSurfaces }; var newState = aerodynamics.CalculateAerodynamics(currentState);&lt;/quote&gt;&lt;p&gt;Here the PIDs are named “pitchController”, “yawController”, and “rollController”. They are all tuned separately to handle a single axis.&lt;/p&gt;&lt;p&gt;When the player releases the stick, the PID controllers will attempt to hold an angular velocity of zero. This makes the aircraft feel like it’s neutrally stable. This also acts as a way to trim the aircraft, so that level flight can be maintained without needing to constantly pull the stick. The PID controller will detect an undesired rotation and move the elevators at a slight angle to counter it.&lt;/p&gt;&lt;p&gt;These PID controllers only add a small amount of complexity to the code, but they achieve similar results as the perfect FCS from the previous project. But there are still limitations that prevent it from being a perfect FCS.&lt;/p&gt;&lt;p&gt;First, the PID controllers must be tuned. The output has to be strong enough to quickly respond to pilot inputs, while avoiding oscillation. This is of course a limitation of any PID control system.&lt;/p&gt;&lt;p&gt;Second, the control surfaces move at a finite speed. This means that it will take some time for the control surface to match the FCS’s commands. So the commands will be imperfectly applied to the aircraft.&lt;/p&gt;&lt;p&gt;Third, unlike the previous flight sim, the three axes of rotation are not independent. For example, a large angle of slip will cause the plane to roll. This is due to the swept wings of the F-16. The roll controller will cancel this out somewhat, but a large enough AOS will result in an uncommanded roll.&lt;/p&gt;&lt;p&gt;Even with these limitations, the PID controllers work fairly well at keeping the plane in control.&lt;/p&gt;&lt;p&gt;Additionally, I use a technique called gain scheduling to change the gain parameters of the roll controller. Because roll performance increases with airspeed, we need a way to limit the amount of aileron movement at high speed. I add two animation curves, which take speed as input, and give the P and D gain of the roll controller as output.&lt;/p&gt;&lt;quote&gt;rollController.P = rollPSchedule.Evaluate(Mathf.Max(0, LocalVelocity.z)); rollController.D = rollDSchedule.Evaluate(Mathf.Max(0, LocalVelocity.z)); Vector3 fcsTarget = new Vector3( pitchController.Calculate(dt, av.x, accel.x, targetAV.x), -yawController.Calculate(dt, av.y, accel.y, targetAV.y), rollController.Calculate(dt, av.z, accel.z, targetAV.z) );&lt;/quote&gt;&lt;p&gt;This allows us to change the strength of the roll controller at different speeds. A more advanced FCS might have a gain schedule for each controller, possibly using more inputs than just airspeed. In fact, if there were multiple inputs, we would need a 2D lookup table to calculate the gain schedule.&lt;/p&gt;&lt;p&gt;Because the flight model is a complete description of how the aircraft will respond at different combinations of AOA, AOS, and control input, it is theoretically possible to design an FCS system that perfectly counters all of the unwanted tendencies. However, that is beyond my understanding of aerodynamics and control theory.&lt;/p&gt;&lt;head rend="h2"&gt;G and AOA Limiter&lt;/head&gt;&lt;p&gt;The weakness of PID controllers is that they only control the angular velocity of the plane. This is not sufficient to control the plane. The previous project has a G limiter, which is simple since steering torque is applied directly to the aircraft. Adding a G limiter is more complicated with this F-16 flight model.&lt;/p&gt;&lt;p&gt;Additionally, a critical part of the FCS on a real F-16 is the AOA limiter. Just like the G limiter prevents the pilot from creating excessive G-forces while maneuvering, the AOA limiter prevents excessive AOA. This is because the aircraft becomes so unstable at about 28 degrees AOA that even the FCS can not compensate. And importantly, our flight model only supports a limited range of AOA (up to 45 degrees), so if the pilot goes beyond that, the behavior of the simulator becomes nonsensical. So limiting the AOA to about 25 degrees is important for maintaining stable flight.&lt;/p&gt;&lt;p&gt;The previous flight sim project did not have anything like an AOA limiter. I simply tuned the steering strength so that AOA would not exceed about 15 degrees (unless stalling). And even then, there is no instability caused by high AOA, so nothing bad happens if the pilot exceeds that.&lt;/p&gt;&lt;p&gt;We need a system that prevents the pilot from exceeding 25 degrees AOA. This would be implemented as a multiplier on the pilot’s stick input, just like a G limiter. Since there are two limits, we simply select the more restrictive limit using min(). So if the G limiter says to limit input to 0.75 and the AOA limiter says to limit input to 0.5, the value 0.5 is chosen.&lt;/p&gt;&lt;p&gt;Because this flight model uses lookup tables, there is no simple formula for calculating either the G limiter or AOA limiter. The G limiter from the previous project won’t work here. Additionally, the relationship between steering input and AOA is not simple. There is a feedback loop between AOA and lift. As AOA increases, lift increases. But as lift increases, AOA decreases since lift pulls the plane onto a new velocity vector. Not to mention lift also depends on airspeed and altitude.&lt;/p&gt;&lt;p&gt;Luckily, the F-16 flight model is completely disconnected from Unity’s physics system. We can actually run the flight model as much as we want with any inputs, and use the outputs for any purpose. There is the “main” flight model that syncs with a Unity rigidbody. But we can create “side” flight models to predict future behavior of the plane.&lt;/p&gt;&lt;p&gt;I chose to implement the G and AOA limiters by running a side flight model. This side model takes the pilot’s inputs and simulates the aircraft in a simplified world state. In a single physics update, the main flight model runs once, but the side flight model runs multiple times to predict movement several seconds into the future. Because running the flight model is a few lookups and math operations, running multiple times per frame is dirt cheap.&lt;/p&gt;&lt;p&gt;By running this side model, we can determine how the plane would behave if it flew without any limiters. So if the plane is flying fast enough to pull 16 Gs, the side model will report that. We can use that information to calculate the G limiter for the main model.&lt;/p&gt;&lt;p&gt;The side model is contained in the class SimpleTrimmer. The main function Trim looks like this:&lt;/p&gt;&lt;quote&gt;public SimulatedState Trim(float dt, float timeMax, SimulatedState initialState) { float time = 0; while (time &amp;lt; timeMax) { AerodynamicState aeroState = new AerodynamicState() { ... }; var aeroForces = aerodynamics.CalculateAerodynamics(aeroState); … time += dt; } return state; }&lt;/quote&gt;&lt;p&gt;It just calls CalculateAerodynamics in a loop with it’s own time variable. The timestep can also be different from the main FixedUpdate loop time step. The variable timeMax controls how far into the future the prediction runs. For example, this side model can run at 0.1 second time steps for 5 seconds total.&lt;/p&gt;&lt;p&gt;After one step of the simulation is run, the state variables are updated and fed back into the next step. The maximum G force and AOA of the whole simulation is recorded.&lt;/p&gt;&lt;quote&gt;// rotate velocity by pitchDelta Quaternion newRotation = Quaternion.Euler(0, pitchDelta, 0); Vector3 newVelocity = newRotation * state.velocity; newVelocity.y = 0; newVelocity.z += gravity * dt; Vector3 velNormalized = newVelocity.normalized; // assume airspeed magnitude does not change (no drag, no thrust) state.velocity = velNormalized * airspeed; state.alpha = Mathf.Atan2(velNormalized.z, velNormalized.x) * Mathf.Rad2Deg; state.maxAlpha = Mathf.Max(state.maxAlpha, state.alpha); state.maxAccelerationZ = Mathf.Min(state.maxAccelerationZ, state.acceleration.z);&lt;/quote&gt;&lt;p&gt;This simulation is highly simplified compared to the main flight model. It ignores the pilot’s input except pitch. It ignores angular velocity except for pitch rate. It does not apply drag or any other force that changes airspeed or altitude. It ignores any change to the aircraft’s pitch. Note that the flight model does not care about the orientation of the aircraft to begin with.&lt;/p&gt;&lt;p&gt;The Trim function assumes the pilot is giving a full pitch up or pitch down input and takes the pitch PID controller as a parameter. So this side flight model uses the same PID values as the main model, to prevent the simulation from turning faster than the max turn rate. Since the I term is not used on the PID controller, we can use it without worrying about state.&lt;/p&gt;&lt;p&gt;Gravity as a single float value is also passed as a parameter. This allows the simulation to know how much gravity is affecting the turn on the pitch axis. If the plane is level, this value is 1. If the plane is rolled 90 degrees to the side, this value is 0. If upside down, this value is -1. Gravity on the other axes is ignored.&lt;/p&gt;&lt;p&gt;The larger time step and reduced complexity of simulation means that the side model is not completely accurate to how the plane will fly. But that’s acceptable since we are only using this to estimate the maximum G force and AOA that a turn might create.&lt;/p&gt;&lt;p&gt;After running through a few seconds of simulation on the flight model, the Trim function returns with the max G force and AOA. The FCS then uses these values to calculate the limiting factors for the pilot’s pitch input.&lt;/p&gt;&lt;quote&gt;SimpleTrimmer.SimulatedState state = simpleTrimmer.Trim( trimmerTimeStep, trimmerTime, initialState, maxAV.x * Mathf.Deg2Rad, gravityFactor * metersToFeet, pitchController, centerOfGravityPosition ); float predictedAlpha = state.maxAlpha; float predictedG = -state.maxAccelerationZ * feetToMeters; float aoaPitchMult = CalculateAOALimiter(predictedAlpha); float gLimit = gLimitPitch; // pitch up limit (ie 8G) if (controlInput.x &amp;gt; 0) { gLimit = this.gLimit; // pitch down limit (ie 4G) } float gPitchMult = CalculateGLimiter(predictedG, gLimit);&lt;/quote&gt;&lt;p&gt;The limiting factor for AOA and G force is calculated with a simple function:&lt;/p&gt;&lt;quote&gt;float ApplyLimiter(float value, float limit, float limitStrength) { if (limit &amp;lt;= 0) return 1; if (value &amp;lt; limit) return 1; float error = value - limit; error *= limitStrength; return limit / (limit + error); }&lt;/quote&gt;&lt;p&gt;ApplyLimiter returns a factor in the range [0, 1], which is eventually multiplied with the pilot’s control input. This function then used in the limiter functions:&lt;/p&gt;&lt;quote&gt;float CalculateGLimiter(float predictedG, float gLimit) { float gForce = predictedG / 9.81f; float gPitchMult = ApplyLimiter(gForce, gLimit, gLimitStrength); return gPitchMult; }&lt;/quote&gt;&lt;p&gt;The variable gForce is the predicted max G force from the side model. gLimit is the value chosen as the max G force, for example, 8. If the predicted value is 12, then the variable error will be 12 – 8 = 4. The returned factor would be 8 / (8 + 4) = 8 / 12 = 0.66. limitStrength is used to tune how strongly the error affects the returned limit factor.&lt;/p&gt;&lt;p&gt;If the value is below the limit, the returned factor is 1.&lt;/p&gt;&lt;p&gt;The AOA limiter uses the same function to calculate two limiting factors which are combined:&lt;/p&gt;&lt;quote&gt;float CalculateAOALimiter(float predictedAlpha) { float aoaPitchMult = 1.0f; aoaPitchMult *= ApplyLimiter(predictedAlpha, predictedAoaLimitMax, predictedAoaLimitStrength); float realAOA = AngleOfAttack * Mathf.Rad2Deg; aoaPitchMult *= ApplyLimiter(realAOA, feedbackAoaLimitMax, feedbackAoaLimitStrength); return aoaPitchMult; }&lt;/quote&gt;&lt;p&gt;One limit factor depends on the predicted alpha from the SimpleTrimmer class. The other factor depends on the actual alpha value the plane currently has. This can handle cases where the real alpha is much larger than the predicted value, such as when the plane is already stalling.&lt;/p&gt;&lt;p&gt;Then the AOA and G limiter factors are applied to the pilot’s input:&lt;/p&gt;&lt;quote&gt;float aoaPitchMult = CalculateAOALimiter(predictedAlpha); float gPitchMult = CalculateGLimiter(predictedG, gLimitPitch); float pitchMult = Mathf.Min(aoaPitchMult, gPitchMult); // select whichever limiter is stronger float rollMult = rollPitchFactor.Evaluate(Mathf.Abs(controlInput.x)) * rollAOAFactor.Evaluate(AngleOfAttack * Mathf.Rad2Deg); Vector3 limitedInput = Vector3.Scale(controlInput, new Vector3(pitchMult, 1, rollMult)); Vector3 targetAV = Vector3.Scale(limitedInput, steeringSpeed * steeringSpeedFactor);&lt;/quote&gt;&lt;p&gt;The min() function is used to select whichever limiter factor is strongest. Since I am designing these systems myself, I can tell you there is not a strong reason why I chose min() instead of another multiplication. This is just the formula that felt right when I was testing it.&lt;/p&gt;&lt;p&gt;In fact there are many different ways that the limiting factors could be calculated and combined. I designed the ApplyLimiter function primarily to be easy to tune. These allow me to have separate variables for tuning predicted G, predicted AOA, and feedback AOA limiters.&lt;/p&gt;&lt;p&gt;There is one final limiter above, rollMult. This is controlled by two AnimationCurves, rollPitchFactor and rollAOAFactor. These curves reduce the strength of roll input when the pilot is commanding a strong pitch rotation and when the plane has a high AOA. I added this because rolls felt too sensitive when in a high G or high AOA turn. Tune these to your own taste.&lt;/p&gt;&lt;head rend="h2"&gt;Stick Pusher&lt;/head&gt;&lt;p&gt;The final system to add is a stick pusher. A stick pusher is a device some aircraft have that physically pushes the stick forward to avoid a stall. This doesn’t exist in the real F-16, even digitally, but who cares? It was quick and easy to write.&lt;/p&gt;&lt;p&gt;If the AOA exceeds some threshold, a bias value is added to the pilot’s stick input to push the nose down. This is different from the AOA limiter above, which multiplies the input by a factor [0, 1]. If the pilot is giving an input of 0, then the AOA limiter has no effect. The stick pusher adds the bias value to the pilot’s input, so it will work even when the pilot gives no input.&lt;/p&gt;&lt;p&gt;The stick pusher will apply when the plane is stalling or if the AOA limiter fails to keep the AOA in the safe range.&lt;/p&gt;&lt;p&gt;The code for this is incredibly simple in concept and implementation:&lt;/p&gt;&lt;quote&gt;float CalculateAOAPusher() { float bias = 0.0f; float aoa = AngleOfAttack * Mathf.Rad2Deg; if (aoa &amp;gt; stickPusherThreshold) { float error = aoa - stickPusherThreshold; bias = stickPusherCurve.Evaluate(error); } return Mathf.Min(stickPusherMax, bias); }&lt;/quote&gt;&lt;p&gt;If the AOA is over the stickPushThreshold, add a bias to the player’s input. The more it exceeds the threshold, the stronger the bias. At max strength, the stick pusher can give a full nose down input that can’t be overridden by the pilot.&lt;/p&gt;&lt;p&gt;This value is summed with the pilot’s input before running the PID controllers.&lt;/p&gt;&lt;quote&gt;Vector3 stickPusher = new Vector3(CalculateAOAPusher(), 0, 0); Vector3 limitedInput = Vector3.Scale(controlInput, new Vector3(pitchMult, 1, rollMult)) + stickPusher; Vector3 targetAV = Vector3.Scale(limitedInput, steeringSpeed * steeringSpeedFactor);&lt;/quote&gt;&lt;p&gt;With all of these systems added to the FCS, the plane should be very stable to fly now. Since the side model simulation is simplified, the G and AOA limiters are not perfect. They will sometimes result in those parameters being limited at a value too high or too low. But these systems do work accurately enough to keep the plane stable.&lt;/p&gt;&lt;head rend="h1"&gt;Testing&lt;/head&gt;&lt;p&gt;Of course any implementation can have bugs. We need to test the flight model to make sure it works. This includes the translation of the Fortran flight model, and the code that implements all of this in Unity.&lt;/p&gt;&lt;head rend="h2"&gt;Unit Testing&lt;/head&gt;&lt;p&gt;Because the flight model is separate from Unity’s physics engine, we can actually test it using normal unit testing techniques. Unity provides a unit testing framework based on NUnit, so testing is pretty typical for C#.&lt;/p&gt;&lt;p&gt;The authors of the textbook also helpfully provide a test case to use. They give the inputs to the model (airspeed, control surface position, throttle, etc) and the expected output of the model (forces, moment, angular velocity, etc). This lets us validate that the model is implemented correctly by running a single step of simulation.&lt;/p&gt;&lt;quote&gt;// Textbook provides a table of input values and the expected output // Index Param Input State (X) Output (XD) // 1 0.4 (XCG) 0.9 (throttle) 500 (vt) -75.23724 // 2 20 (elevator) 0.5 (alpha) -0.8813419 // 3 -15 (aileron) -0.2 (beta) -0.4759990 // 4 -20 (rudder) -1 (phi) // 5 1 (theta) // 6 -1 (psi) // 7 0.7 (P) 12.62679 // 8 -0.8 (Q) 0.9649671 // 9 0.9 (R) 0.5809759 // 10 1000 (north) // 11 900 (east) // 12 10000 (alt) 248.1241 // 13 90 (power) -58.68999&lt;/quote&gt;&lt;p&gt;Note that the output values for roll, pitch, and yaw, and north, east, and altitude, are not checked in this test. We are using the Unity rigidbody to handle these, so these values are not even calculated in C#.&lt;/p&gt;&lt;p&gt;Additionally, the table lookup operations are fairly simple C# code, so these functions can also be unit tested. I caught a few bugs in the flight model by adding these tests.&lt;/p&gt;&lt;p&gt;All of the tests are in the ModelTestCase class.&lt;/p&gt;&lt;head rend="h2"&gt;Flight Testing&lt;/head&gt;&lt;p&gt;Of course unit testing can only cover so much. The whole point of this project is to create a flight simulator. The only way to know how the flight model really feels is to fly it. So get out there and start flying it!&lt;/p&gt;&lt;p&gt;I have caught a few bugs in the implementation just by flying it and realizing that some aspect felt weird.&lt;/p&gt;&lt;p&gt;In the aerospace industry, test flights are thoroughly instrumented to gather as much data as possible. Force on every axis, angular velocity, pilot input, GPS track, etc is all recorded and stored for future analysis. It’s possible to write automated tests that read this data and check that values stay within expected bounds.&lt;/p&gt;&lt;p&gt;I have done none of that here. Just have fun flying 🙂&lt;/p&gt;&lt;head rend="h1"&gt;Limitations&lt;/head&gt;&lt;p&gt;The flight model defined in the textbook has several limitations.&lt;/p&gt;&lt;p&gt;The effects of alpha on the flight model is only modeled for the range [-10, 45]. Beta is only modeled for the range [-30, 30]. The flight model supports extrapolating data tables beyond their defined ranges, but the returned values will quickly become nonsensical. This means that if you manage to fly the F-16 beyond the provided ranges for alpha and beta, the flight model will break down and begin behaving non-physically.&lt;/p&gt;&lt;p&gt;In some cases, the aircraft will eventually return to controlled flight. But in other cases, one bad data value used to query the tables will cause increasingly bad data to be stored to the plane’s state. These bad values will quickly grow until the plane is thrown to infinity.&lt;/p&gt;&lt;p&gt;Hopefully, this is not possible when using FCS that I’ve written. But I encourage any readers to try breaking it themselves.&lt;/p&gt;&lt;p&gt;You can also turn off parts of the FCS using the config menu in the top left corner. This allows you to fly the plane completely manually, turn the engine off, or alter the center of gravity.&lt;/p&gt;&lt;p&gt;If the flight model doesn’t bug out from extreme values, then you can actually perform a backflip or a “Kulbit” maneuver with the F-16. I recommend turning off only the pitch axis FCS if you want to try that.&lt;/p&gt;&lt;p&gt;Another limitation is that flaps and slats are not defined in the flight model. The real F-16 uses a single control surface called a “flaperon” that works as both a flap and an aileron. When more lift is needed at low speeds, both flaperons deflect downwards like traditional flaps. Leading edge slats also deflect downwards to increase lift.&lt;/p&gt;&lt;p&gt;The textbook flight model only considers these control surfaces to be ailerons. That is, they always deflect in opposite directions in order to create a roll moment. Only a single “aileron” value is used to represent both left and right, so they cannot be used as flaps. If they were to be used as flaps, then there would need to be a left aileron and right aileron value and the Z axis force coefficient would depend on flaperon position.&lt;/p&gt;&lt;p&gt;The effect of slat position is blended into the existing tables, so there is some effect of slats on Z axis force. But the slat position cannot be animated on the plane’s 3D model since no variable for it exists.&lt;/p&gt;&lt;p&gt;This means that there are reduced high lift devices on the aircraft. The extra lift from flaps cannot be modeled. So the plane’s takeoff speed is much higher than you might expect from the F-16. The textbook only defines a model for flight, not for taxiing or takeoff. Landings feel quite bad because of this.&lt;/p&gt;&lt;p&gt;Another limitation is the lack of landing gear simulation. The landing gear is implemented exactly the same as the previous project: three capsule colliders. There is no simulation of wheel, tire, or suspension behavior. Again, this makes takeoff and landing feel kind of weird. But I have no idea how to write a system like that and it’s out of scope for this project anyways.🤷♂️&lt;/p&gt;&lt;p&gt;Another limitation of the flight model is the inaccuracy when flying super sonic. With real planes, lift and drag forces change drastically as you approach Mach 1. Air accelerates as it passes over the wing. Even while the plane remains subsonic, some parts of the air flow are forced to accelerate above Mach 1. When this air reaches supersonic speeds, shockwaves form over the wing which alters the way air flows around it.&lt;/p&gt;&lt;p&gt;This region, where some air is supersonic and some is not, is called the transonic region. This has a drastic effect on the aircraft’s performance and handling. In particular, the coefficient of drag increases, creating the “sound barrier” effect. The position of lift force on the wing changes, which will change how the plane handles.&lt;/p&gt;&lt;p&gt;None of these effects are included in the textbook’s flight model. These could be modeled by adding another input dimension to the force and moment tables. I suspect these were omitted to keep the flight model simple.&lt;/p&gt;&lt;p&gt;The practical effect is that the flight model only works up to about Mach 0.7. Above that, all of the forces on the aircraft become unrealistic. The behavior of the plane continues to increase smoothly with airspeed as if supersonic effects don’t exist.&lt;/p&gt;&lt;head rend="h1"&gt;Conclusion&lt;/head&gt;&lt;p&gt;I started this project after I got a job in the aerospace industry. The textbook was recommended by my manager, since I was working on real flight control systems. In a way, this article is a summary of everything I’ve learned about flying and software engineering in that job.&lt;/p&gt;&lt;p&gt;The way this F-16 flight model is implemented is very different from my previous project. It is actually close to how professional level sims are written, though simplified to fit in a textbook. Even so, there are still plenty of limitations in the flight model which means the simulation will behave unrealistically beyond the intended flight envelope.&lt;/p&gt;&lt;p&gt;The authors of the textbook based their flight model on a NASA paper12 which measured the aerodynamic properties of a scale model in a wind tunnel. The Nasa paper provides 50 lookup tables. The textbook simplified, approximated, and combined these into only 13 lookup tables.&lt;/p&gt;&lt;p&gt;With only a little more effort, you could write a simulator that uses many more tables to cover a larger flight envelope with more detail. The only limit is the data you have access to and your understanding of aerodynamics.&lt;/p&gt;&lt;p&gt;The FCS I’ve written is much simpler than the real FCS. Theoretically, it would be possible to write code that models the real F-16 FCS and apply it to this flight simulator. But how could you even get that information and who would be crazy enough to try that?&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;“Aircraft Control and Simulation” by Brian L. Stevens, Frank L. Lewis, and Eric N. Johnson ↩︎&lt;/item&gt;&lt;item&gt;https://vazgriz.com/346/flight-simulator-in-unity3d-part-1/ ↩︎&lt;/item&gt;&lt;item&gt;https://vazgriz.com/467/flight-simulator-in-unity3d-part-2/ ↩︎&lt;/item&gt;&lt;item&gt;https://vazgriz.com/503/creating-a-flight-simulator-in-unity3d-part-3/ ↩︎&lt;/item&gt;&lt;item&gt;https://twitter.com/FreyaHolmer/status/1325556229410861056 ↩︎&lt;/item&gt;&lt;item&gt;https://commons.wikimedia.org/wiki/File:Speyer_Handlog.jpg ↩︎&lt;/item&gt;&lt;item&gt;https://www.grc.nasa.gov/www/k-12/VirtualAero/BottleRocket/airplane/sound.html ↩︎&lt;/item&gt;&lt;item&gt;https://en.wikipedia.org/wiki/Bilinear_interpolation ↩︎&lt;/item&gt;&lt;item&gt;https://en.wikipedia.org/wiki/Trilinear_interpolation ↩︎&lt;/item&gt;&lt;item&gt;https://aerospaceweb.org/question/aerodynamics/q0194.shtml ↩︎&lt;/item&gt;&lt;item&gt;https://vazgriz.com/621/pid-controllers/ ↩︎&lt;/item&gt;&lt;item&gt;Simulator study of stall/post-stall characteristics of a fighter airplane with relaxed longitudinal static stability, Nyugen et al, 1979 (https://ntrs.nasa.gov/citations/19800005879) ↩︎&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45383637</guid><pubDate>Fri, 26 Sep 2025 07:06:45 +0000</pubDate></item><item><title>Pop OS 24.04 LTS Beta</title><link>https://system76.com/pop/pop-beta/</link><description>&lt;doc fingerprint="d3f093e026c52dc0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Pop!_OS 24.04 LTS Beta Download&lt;/head&gt;
    &lt;head rend="h2"&gt;Pop!_OS is Getting Beta&lt;/head&gt;
    &lt;p&gt;Pop!_OS 24.04 LTS with the new COSMIC DE, developed by System76, is coming with many new features to explore and discover. Test out the beta as we fine-tune for release.&lt;/p&gt;
    &lt;head rend="h3"&gt;Upgrading from Pop!_OS 22.04 LTS to Pop!_OS 24.04 LTS Beta&lt;/head&gt;
    &lt;head rend="h3"&gt;Pop!_OS 24.04 LTS Beta&lt;/head&gt;
    &lt;p&gt;Filesize: GB&lt;/p&gt;
    &lt;head rend="h3"&gt;Pop!_OS 24.04 LTS Beta with NVIDIA&lt;/head&gt;
    &lt;p&gt;Filesize: GB&lt;/p&gt;
    &lt;head rend="h2"&gt;Release Notes&lt;/head&gt;
    &lt;p&gt;- Pop!_OS 24.04 LTS Beta includes the new COSMIC Desktop Environment designed and developed by System76. COSMIC DE is largely feature complete for the first release and development focus has turned to bug fixes for the final release. - This is a beta release and some bugs are expected. - On occasion, the installer does not start in a virtual machine. Press Super to activate the Launcher and search for "Installer". - Some GNOME apps are replaced by COSMIC apps - GNOME Files (Nautilus) &amp;gt; COSMIC Files - GNOME Terminal &amp;gt; COSMIC Terminal - GNOME Text Editor &amp;gt; COSMIC Text Editor - GNOME Media Player (Totem) &amp;gt; COSMIC Media Player - Pop!_Shop is replaced by COSMIC Store - Key components - COSMIC Epoch 1 Beta - Linux kernel 6.16.3 - Mesa 25.1.5-1 - NVIDIA Driver 580 - libwayland/libwayland-client 1.23.1-3 - libdrm 2.4.125-1 - Dragging and Dropping files from Wayland apps to X11 apps is not currently supported. For instance dragging files from COSMIC Files to Slack. Use the applications upload option as a work-around until the feature is added. - On distributions other than Pop!_OS, Firefox may need a configuration flag set to match COSMIC theming - Go to **```about:config```** and set **```widget.gtk.libadwaita-colors.enabled```** to **```false```** - Google Chrome based browsers - As of Google Chrome version 140, no configuration is necessary for Wayland - For versions prior to 140 and other Chrome based browsers that aren’t updated, setting the **```ozone-platform-hint```** is necessary. Go to chrome://flags in a tab, search for **```ozone-platform-hint```** and change the setting to “auto”. Restart the browser. - Gaming is working well but we expect to need more fixes in our xwayland implementation for the Release Candidate. - Some games may start partially off screen. Press F11 or Super+F11 to full screen the game (Goat Simulator is one example) - Display toggle hotkeys and an on-screen-display is not supported yet. - The “Accent hint” around windows doesn’t match the roundness style setting in Appearance. This is expected for at least COSMIC apps for the Release Candidate. - COSMIC has a built-in screenshot tool. If you require annotations, we recommend Flameshot which can be installed from Flathub via COSMIC Store. Version 13.1 or higher is required for COSMIC. - COSMIC Store doesn’t currently display Flatpak suggested addons for apps. This is planned for the Release Candidate. - Accessibility: The screen reader may not read all COSMIC apps widgets or may read them in an unintuitive direction. We’re working on screen reader flow and navigation for the Release Candidate. - Some application indicators do not appear in the Notification Tray applet. - Switching to an application using its application indicator does not currently work. - Printing support in COSMIC Text Editor is planned for the release candidate - Additional features and bugs expected to be fixed are triaged in the RC column on the [project board](https://github.com/orgs/pop-os/projects/23/views/1)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45384481</guid><pubDate>Fri, 26 Sep 2025 09:20:07 +0000</pubDate></item><item><title>Genode OS Framework</title><link>https://genode.org</link><description>&lt;doc fingerprint="3d9a7bf614438f99"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;We understand the complexity of code and policy as the most fundamental security problem shared by modern general-purpose operating systems. Because of high functional demands and dynamic workloads, however, this complexity cannot be avoided. But it can be organized. Genode is a novel OS architecture that is able to master complexity by applying a strict organizational structure to all software components including device drivers, system services, and applications. The Genode OS framework is an open-source tool kit for building highly secure component-based operating systems. It scales from embedded devices to dynamic general-purpose computing.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;keywords&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;capability-based security, microkernel, principle of least authority, sandboxing, virtualization&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The Books&lt;/head&gt;
    &lt;p&gt;The book "Genode Applications" provides a beginner-friendly starting point for the development and porting of applications to Genode and Sculpt OS in particular. It introduces the Goa SDK, describes key libraries, components, and conventions such as the C runtime, VFS, NIC router, and package management, provides application-debugging aid, and tops off with a collection of advanced tutorials. Download as PDF&lt;/p&gt;
    &lt;p&gt;The book "Genode Foundations" describes the Genode OS framework in a holistic and comprehensive way. It equips the reader with a thorough understanding of the architecture, assists developers with the explanation of the development environment and system configuration, and provides a look under the hood of the framework. Furthermore, it contains the specification of the framework's programming interface. Download as PDF or Browse online...&lt;/p&gt;
    &lt;p&gt;The "Genode Platforms" document complements the Genode Foundations book with low-level hardware-related topics. It is primarily intended for integrators and developers of device drivers. Download as PDF&lt;/p&gt;
    &lt;head rend="h2"&gt;Genode News RSS feed&lt;/head&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Genode OS Framework release 25.08 Aug 28, 2025&lt;/item&gt;
      &lt;item rend="dd-1"&gt;Genode 25.08 introduces a new kernel scheduler for fairness and low latency, explores an alternative to XML, optimizes the block-storage stack, and updates all Linux-based PC drivers to kernel version 6.12. Furthermore, the release unlocks further dynamic scenarios for the latest version 13.0 of the seL4 microkernel.&lt;/item&gt;
      &lt;item rend="dt-2"&gt;Revised Foundations and Applications books Jun 06, 2025&lt;/item&gt;
      &lt;item rend="dd-2"&gt;The "Genode Foundations" and "Genode Applications" books have been updated to match the Genode framework 25.05 and Sculpt OS 25.04.&lt;/item&gt;
      &lt;item rend="dt-3"&gt;Genode OS Framework release 25.05 May 28, 2025&lt;/item&gt;
      &lt;item rend="dd-3"&gt;In tandem with the tool-chain upgrade to GCC 14.2 and binutils 2.44, Genode 25.05 hardens the framework API to foster a programming style that leaves no error condition unconsidered. Further highlights are the rigid sandboxing of the Goa SDK, the consolidation of the two supported TCP/IP stacks, and enhanced graphics drivers.&lt;/item&gt;
      &lt;item rend="dt-4"&gt;Sculpt OS release 25.04 Apr 29, 2025&lt;/item&gt;
      &lt;item rend="dd-4"&gt;Sculpt OS 25.04 brings compatibility with Intel's Meteor-Lake hardware, introduces multi-monitor window management and display rotation, and is accompanied by a major update of the Chromium web engine.&lt;/item&gt;
      &lt;item rend="dt-5"&gt;Genode OS Framework release 25.02 Feb 28, 2025&lt;/item&gt;
      &lt;item rend="dd-5"&gt;Version 25.02 extends Genode's multi-monitor abilities to the window-management level and to virtual machines, features the ports of Qemu and the Chromium web engine version 112, boosts the graphics performance via SIMD optimizations, and brings Sculpt OS to the latest Intel Meteor-Lake Framework laptops as well as F&amp;amp;S MX8MP armStone hardware.&lt;/item&gt;
      &lt;item rend="dt-6"&gt;Road Map for 2025 Jan 22, 2025&lt;/item&gt;
      &lt;item rend="dd-6"&gt;We dedicate the year 2025 to Genode's rigidity, clarity, and performance.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45384653</guid><pubDate>Fri, 26 Sep 2025 09:52:45 +0000</pubDate></item><item><title>ParadeDB (YC S23) Is Hiring Database Internals Engineers</title><link>https://paradedb.notion.site/?source=copy_link</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45385477</guid><pubDate>Fri, 26 Sep 2025 12:00:18 +0000</pubDate></item><item><title>Titanic's Sister, Britannic, Sank in 1916. Divers Have Recovered Artifacts</title><link>https://www.smithsonianmag.com/smart-news/the-titanics-sister-ship-the-britannic-sank-in-1916-for-the-first-time-ever-divers-have-recovered-artifacts-from-its-wreck-180987402/</link><description>&lt;doc fingerprint="3431279307b2c5f7"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;The Titanic’s Sister Ship, the Britannic, Sank in 1916. For the First Time, Divers Have Recovered Artifacts From Its Wreck&lt;/head&gt;&lt;head rend="h2"&gt;The luxury liner was requisitioned as a hospital ship during World War I. Thirty people died after the vessel struck a German naval mine and sank off the coast of Greece&lt;/head&gt;&lt;p&gt;In the early 20th century, the British White Star Line set out to make the largest and most luxurious ocean liners the world had ever seen. Between 1909 and 1914, the shipping line built a trio of upscale vessels: the Olympic, the Titanic and the Britannic.&lt;/p&gt;&lt;p&gt;Within a decade of the Olympic’s maiden voyage in June 1911, two of the three sister vessels had met tragic ends: the Titanic, which famously sank after striking an iceberg in April 1912, and the lesser-known Britannic, which sank in November 1916 after hitting a naval mine in the Aegean Sea during World War I.&lt;/p&gt;&lt;head rend="h4"&gt;Did you know? The long, strange journey of the Titanic’s dead&lt;/head&gt;Rescuers recovered the remains of only 337 of the roughly 1,500 men, women and children who died in the Titanic disaster. Some of the bodies were buried at sea, while others were brought back to land.&lt;p&gt;This week, Greece’s Ministry of Culture announced that deep-sea divers have recovered artifacts from the wreck of the Britannic for the very first time. The finds include the ship’s bell, a pair of binoculars and a navigation lamp.&lt;/p&gt;&lt;p&gt;In May, an 11-member team embarked on a weeklong expedition to the wreck site, which rests at a depth of nearly 400 feet, the Associated Press reports. The highly skilled group of divers used closed-circuit rebreather equipment to safely survey the ship. “Conditions at the wreck site were particularly challenging due to currents, depth and low visibility,” the ministry says in a statement, per a translation by Agence France-Presse.&lt;/p&gt;&lt;p&gt;British historian Simon Mills, who purchased the wreck in 1996, organized the recovery operation. Artifacts raised from the depths include both practical objects and luxury items, like silver-plated trays used in first class, ceramic tiles from a Turkish bath, and a porcelain sink from the ship’s second-class cabins.&lt;/p&gt;&lt;p&gt;The items are currently undergoing conservation in Athens. Afterward, they will be prominently displayed at the National Museum of Underwater Antiquities, which is slated to open in Greece’s largest port city, Piraeus, in 2026.&lt;/p&gt;&lt;p&gt;Unlike the Titanic and the Olympic, the Britannic was never actually deployed as a luxury cruise liner. Instead, the vessel was requisitioned in 1915, operating as the largest hospital ship in the world for almost a year. As Mills explains in an interview with Bloombsbury:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;By the time the Britannic’s keel was being laid, the Titanic was still several months from completion, and the Olympic had been in commercial service for almost six months, but the building and operational experience gained by observing the earlier vessels meant that even at this early stage, there would have been countless improvements incorporated into the Britannic’s designs.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;These safety measures weren’t enough to save the ship from the disaster. While traveling toward the Greek island of Lemnos, the Britannic hit a German naval mine and sank off the island of Kea on November 21, 1916. Of the 1,066 people on board, 30 lost their lives.&lt;/p&gt;&lt;p&gt;“Britannic was the largest vessel to sink during the Great War, [but] there was no catastrophic loss of life, unlike with the Lusitania, and no deliberate targeting of a hospital ship was evident,” notes the Western Front Association on its website. “As a result, the story of the Britannic is now largely forgotten.”&lt;/p&gt;&lt;p&gt;The Titanic’s sinking has long held immense cultural fascination. Last year, coal from the shipwreck sold for £1,500, while the controversial floating board from James Cameron’s 1997 film Titanic fetched $718,750 at auction.&lt;/p&gt;&lt;p&gt;The Olympic, the White Star Line’s lead ship, sailed for 24 years before its retirement in 1935. It served as a troopship during World War I, then returned to passenger service after the war.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45386690</guid><pubDate>Fri, 26 Sep 2025 14:08:33 +0000</pubDate></item><item><title>DeepFabric – Generate High-Quality Synthetic Datasets at Scale</title><link>https://lukehinds.github.io/deepfabric/</link><description>&lt;doc fingerprint="a39866c824b08fe8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Home&lt;/head&gt;
    &lt;p&gt;DeepFabric transforms the process of creating synthetic datasets for language model training, evaluation, and research. Built around the concept of topic-driven data generation, it provides both hierarchical topic trees and experimental graph-based topic modeling to create diverse, contextually rich training examples.&lt;/p&gt;
    &lt;p&gt;The library serves researchers, engineers, and practitioners who need high-quality synthetic data for model distillation, agent evaluation, or statistical research. Whether you're generating conversational datasets, creating domain-specific training examples, or building evaluation benchmarks, DeepFabric provides the tools to scale your data generation process while maintaining quality and diversity.&lt;/p&gt;
    &lt;head rend="h2"&gt;Core Capabilities¶&lt;/head&gt;
    &lt;p&gt;DeepFabric operates through a three-stage pipeline that transforms a simple prompt into a comprehensive dataset. The process begins with topic generation, where the system creates either a hierarchical tree structure or a more complex graph representation of your domain. These topics then feed into the dataset generation engine, which produces contextually appropriate training examples. Finally, the system packages everything into standard formats ready for immediate use.&lt;/p&gt;
    &lt;p&gt;The topic modeling approach sets DeepFabric apart from simple prompt-based generation. Rather than creating isolated examples, the system builds a conceptual map of your domain and generates examples that explore different aspects systematically. This ensures broader coverage and more consistent quality across your dataset.&lt;/p&gt;
    &lt;head rend="h2"&gt;Topic Trees and Graphs¶&lt;/head&gt;
    &lt;p&gt;Traditional topic trees provide a hierarchical breakdown of subjects, ideal for domains with clear categorical structures. The experimental topic graph feature extends this concept by allowing cross-connections between topics, creating more realistic representations of complex domains where concepts naturally interconnect.&lt;/p&gt;
    &lt;p&gt;Both approaches leverage large language models to intelligently expand topics and generate relevant content, but they serve different use cases depending on your domain's structure and complexity requirements.&lt;/p&gt;
    &lt;head&gt;Choosing Between Trees and Graphs&lt;/head&gt;
    &lt;p&gt;Topic trees work well for domains with clear hierarchical relationships, such as academic subjects, product categories, or organizational structures. Topic graphs excel in interconnected domains like research areas, technical concepts, or social phenomena where relationships span multiple categories.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting Started¶&lt;/head&gt;
    &lt;p&gt;The fastest path to your first dataset involves three simple steps: installation, configuration, and generation. The Getting Started section walks through this process with practical examples that you can run immediately.&lt;/p&gt;
    &lt;p&gt;For those preferring configuration-driven workflows, DeepFabric's YAML format provides comprehensive control over every aspect of generation. Developers seeking programmatic integration can access the full API through Python classes that mirror the CLI functionality.&lt;/p&gt;
    &lt;head rend="h2"&gt;Integration Ecosystem¶&lt;/head&gt;
    &lt;p&gt;DeepFabric integrates seamlessly with the modern machine learning ecosystem, including OpenAI, Anthropic, local Ollama instances, and cloud-based solutions. Generated datasets export directly to Hugging Face Hub with automatic dataset cards and metadata.&lt;/p&gt;
    &lt;p&gt;The modular CLI design supports complex workflows through commands like &lt;code&gt;deepfabric validate&lt;/code&gt; for configuration checking, &lt;code&gt;deepfabric visualize&lt;/code&gt; for topic graph exploration, and &lt;code&gt;deepfabric upload&lt;/code&gt; for streamlined dataset publishing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Next Steps¶&lt;/head&gt;
    &lt;p&gt;Begin with the Installation Guide to set up your environment, then follow the First Dataset tutorial to generate your initial synthetic dataset. The Configuration Guide provides comprehensive coverage of YAML options, while the API Reference documents programmatic usage patterns.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45386872</guid><pubDate>Fri, 26 Sep 2025 14:26:44 +0000</pubDate></item><item><title>JWST peers deep into the heart of star formation in our Milky Way galaxy</title><link>https://www.space.com/astronomy/james-webb-space-telescope/james-webb-space-telescope-peers-deep-into-the-heart-of-star-formation-in-our-milky-way-galaxy</link><description>&lt;doc fingerprint="bdf514be94aae2b5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;James Webb Space Telescope peers deep into the heart of star formation in our Milky Way galaxy&lt;/head&gt;
    &lt;p&gt;This massive star-forming cloud is working surprisingly fast.&lt;/p&gt;
    &lt;p&gt;A maelstrom of star formation close to the center of our galaxy has been revealed in two different wavelengths by the James Webb Space Telescope (JWST), its beautiful images highlighting the intensity of star-birth in the region and deepening the mystery of why star formation at the very heart of our galaxy is so sluggish.&lt;/p&gt;
    &lt;p&gt;Sagittarius B2 is a dense cloud of molecular gas located about 390 light-years from the black hole Sagittarius A* at the center of our Milky Way galaxy. At about 150 light-years across and containing enough gas to assemble 3 million sun-like stars, B2 is the largest, most massive and most active star-forming region in our entire galaxy.&lt;/p&gt;
    &lt;p&gt;Yet, B2 is at odds with the rest of the galactic center. As massive as B2 is, it contains only 10% of the molecular gas in the galactic center, gas that forms the building blocks of stars. Still, despite only having a modest fraction of gas relative to the galactic center as a whole, B2 produces half of all the stars there. It is an enduring mystery why B2 has such intense star formation while the rest of the galactic center has proportionately lower rates of star-birth.&lt;/p&gt;
    &lt;p&gt;That's why the new observations by the JWST are so important in understanding what drives and what puts the brakes on star formation in the galactic center.&lt;/p&gt;
    &lt;p&gt;"Webb's powerful infrared instruments provide detail we've never been able to see before, which will help us to understand some of the still-elusive mysteries of massive star formation and why Sagittarius B2 is so much more active than the rest of the galactic center," said study co-author Adam Ginsburg of the University of Florida in a statement.&lt;/p&gt;
    &lt;p&gt;One theory is that powerful, complex magnetic fields that are entwined around the galactic center and its retinue of molecular clouds similar to B2 could play a deciding factor, but the hows and whys there are still to be determined.&lt;/p&gt;
    &lt;p&gt;For its part, JWST can get to the heart of the star formation in B2 thanks to the space telescope's powerful infrared vision that can peer through much of the obscuring dust in the cloud. Presented here are two images, one taken at shorter infrared wavelengths by the JWST's Near Infrared Camera (NIRCam) and the other captured at longer wavelengths by the telescope's Mid-Infrared Instrument (MIRI).&lt;/p&gt;
    &lt;p&gt;Breaking space news, the latest updates on rocket launches, skywatching events and more!&lt;/p&gt;
    &lt;p&gt;In the NIRCam image, we see myriad stars in B2 amid hazy patches of nebulosity. In the darkest areas where we can't see nebulosity there is cosmic dust too dense even for NIRCam to see through.&lt;/p&gt;
    &lt;p&gt;So, we turn to MIRI's image, which is able to penetrate the thicker dust in B2. Here, all but the brightest stars have faded to invisibility since they do not radiate much at these long infrared wavelengths. Meanwhile, the nebulosity across the entire scene has blossomed into life, revealing the true scale of star-birth in the region as each of those bright clouds is being illuminated by the light of very young but massive stars that are still in the process of growing.&lt;/p&gt;
    &lt;p&gt;The aim of the JWST observations is to better understand the history of star formation in B2. Has it been ongoing for many millions of years and many generations of stars, or has it only recently ignited? The answer will help place B2 into context with the rest of the galactic center as astronomers search for clues as to what stifles star-birth at the heart of our galaxy.&lt;/p&gt;
    &lt;p&gt;The findings could have broader repercussions. The intensity of star formation in B2 is believed by astronomers to be similar to conditions in the early universe when the first stars were formed in a flurry of frenzied activity. By learning what governs star formation in the galactic center, we could also be learning about what governed star formation in the aftermath of the Big Bang.&lt;/p&gt;
    &lt;p&gt;A study about these results can be viewed on the paper repository arxiv.&lt;/p&gt;
    &lt;p&gt;Join our Space Forums to keep talking space on the latest missions, night sky and more! And if you have a news tip, correction or comment, let us know at: community@space.com.&lt;/p&gt;
    &lt;p&gt;Keith Cooper is a freelance science journalist and editor in the United Kingdom, and has a degree in physics and astrophysics from the University of Manchester. He's the author of "The Contact Paradox: Challenging Our Assumptions in the Search for Extraterrestrial Intelligence" (Bloomsbury Sigma, 2020) and has written articles on astronomy, space, physics and astrobiology for a multitude of magazines and websites.&lt;/p&gt;
    &lt;p&gt;You must confirm your public display name before commenting&lt;/p&gt;
    &lt;p&gt;Please logout and then login again, you will then be prompted to enter your display name.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45387154</guid><pubDate>Fri, 26 Sep 2025 14:49:05 +0000</pubDate></item><item><title>How to stop AI's "lethal trifecta"</title><link>https://www.economist.com/leaders/2025/09/25/how-to-stop-ais-lethal-trifecta</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45387155</guid><pubDate>Fri, 26 Sep 2025 14:49:07 +0000</pubDate></item><item><title>They don't make 'em like that any more: Dyson Pure Cool-Me personal air purifier</title><link>https://kevinboone.me/cool-me.html</link><description>&lt;doc fingerprint="a4a9945469719cfb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;They don’t make ’em like that any more: Dyson Pure Cool-Me personal air purifier&lt;/head&gt;
    &lt;quote&gt;&lt;p&gt;Note:&lt;/p&gt;&lt;lb/&gt;I have to admit to a certain trepidation here. If you can afford even to contemplate paying nearly £300 for a fan, you’re in a highly fortunate, privileged position. I received mine as a gift, and it’s one of the best presents I ever had; I wouldn’t have spent that amount of money on myself. I’m not altogether happy, singing the praises of a device that nobody truly needs, and that probably only 0.001% of the world’s population would consider a reasonable expense. A shortage of personal air purifiers is most definitely a first-world problem. So, while I think it’s lamentable that Dyson stopped making the Cool-Me – and that nobody else has stepped in to fill the hole in the market more affordably – I’m not surprised to find that Dyson didn’t sell enough of them to keep them in production.&lt;/quote&gt;
    &lt;p&gt;Dyson’s Cool-Me is, or was, a personal fan and air purifier; it’s “Personal” in that it’s really designed to be used by one person at a time. Mine sits on my desk and, without making any noise I can hear over my computer fans, makes my life just a little better. The Cool-Me produces a gentle flow of cooling air at head-and-shoulders level, largely stripped of the pollen and particulates that make the spring months so trying.&lt;/p&gt;
    &lt;p&gt;Like its other fans, all of which are eye-wateringly expensive, Dyson describes the Cool-Me as ‘bladeless’, although that isn’t really the case – rather, the air-moving elements are concealed in the centre of the unit. They draw air through the network of holes in the case, through the particulate filter, and out of the vent at the top. At the lowest setting, the fan is effectively inaudible, but it still does a good job of cooling when you’re sitting right next to it. Close up, the air jet is quite intense, considering the fan is almost silent. At the highest setting the Cool-Me is quite noisy, but still quieter than a conventional fan. The ‘bladeless’ design means that there are no accessible moving parts, so it’s safe around children.&lt;/p&gt;
    &lt;p&gt;On the lowest setting, the Cool-Me’s power consumption is only a few Watts, rising to about 30 Watts at full speed. The low power means that it doesn’t add a lot of heat to the environment, when it’s supposed to be keeping you cool. Because it’s an air cleaner, not just a fan, it makes sense to leave it running for extended periods of time, not just when you’re sitting in front of it. So it’s good that it’s relatively cheap to run: it works out at about 2p for eight hours’ use at the lowest speed. Replacement filters will be a lot more expensive; more on that thorny subject later.&lt;/p&gt;
    &lt;p&gt;The Cool-Me has an ‘oscillating’ mode, where the fan head rotates side-to-side automatically. Conceivably, you could leave it running like this whilst you’re out of the room, so it sprays cleaned air around the environment. Whether this works, I can’t say, and I’ve rarely used this feature. Mostly I just sit the unit a few feet away, so it blows directly towards my chair. Until the ambient temperature reaches about 30 Celsius, I find it as effective as my full-size air conditioner, and a heck of a lot cheaper to run. At higher temperatures the Cool-Me still cools me, but only at much higher fan speeds. As with any fan, a blast of air at head height can dry your eyes uncomfortably, even though it might still make you feel cooler.&lt;/p&gt;
    &lt;p&gt;In short, the Cool-Me is a highly effective machine for providing a modest cooling effect with little noise and low energy consumption, while reducing the misery of hay fever and allergies.&lt;/p&gt;
    &lt;p&gt;The Cool-Me is, of course, not perfect. It has no external controls, just a pointless little remote control, that is supposed to attach to the unit by a magnet. In practice, its job seems to be to drop off into my coffee mug every time I so much as twitch. And, as the range of the remote is (honestly) about 30cm, it’s no more useful than ordinary controls. Despite its low-ish power needs, the Cool-Me’s external power supply is about the size of a house-brick, so it’s unsightly and gets in the way. There’s a measure of up-down adjustment of the airflow, but not really enough. In particular it can’t blow downwards; you only have to look at the fan-head to see why. To use it at night-time to cool my bed area, I have to raise the back of the Cool-Me on a pile of books so it points downwards.&lt;/p&gt;
    &lt;p&gt;The real problem, though, is the filter. The Cool-Me uses a glass HEPA filter, which is claimed to remove ‘99.95% of allergens and pollutants’. Dyson are cagey about how long their filters last, suggesting that they should be replaced about once a year, but they don’t say how much daily usage that corresponds to. There is a filter life gauge on the unit, which you can activate using the remote control. Bear in mind, however, that if you remove the filter to inspect it, that resets the gauge.&lt;/p&gt;
    &lt;p&gt;In all, I find that the filter does, in fact, last about a year with my usage. A replacement filter from Dyson costs – gulp! – £70, and I’m not sure how much longer Dyson will stock them. Dyson does use the same filter in other products, but they all appear to be discontinued, too. The good news is that a number of other suppliers can provide compatible filters, and at a much lower price. Whether these work as well as the original Dyson part, I really couldn’t say. The airflow doesn’t appear to be any lower, nor the fan any noisier. Whether they remove particulates as effectively as the Dyson filters is hard to say.&lt;/p&gt;
    &lt;p&gt;This is all a little irritating, because glass HEPA filter are not easy to clean when they lose their effectiveness.&lt;lb/&gt; They don’t respond to vacuuming as paper filters do, and washing destroys them. There might be some mileage in trying to blast the accumulated contaminants ‘backwards’ out of the filter using an air compressor, although I’ve heard mixed reports about the effectiveness of this method. Now that Dyson no longer makes the Cool-Me, I guess it won’t be too long before the lack of replacement filters turns mine into an expensive paperweight.&lt;/p&gt;
    &lt;p&gt;So there we have it. The Cool-Me wasn’t a hugely innovative device – it didn’t change the world, or even embody any radically new engineering principles. It’s just sat on my desk, day after day, quietly making my life a little better.&lt;/p&gt;
    &lt;p&gt;The downfall of the Cool-Me was surely its price. I can buy a perfectly serviceable desk fan for about £10. It wouldn’t filter the air, it would be noisy, it would dry out my eyes, and Heaven help anybody foolish enough to poke a finger between its blades. Still, three hundred quid was a lot of money for a fan.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45387242</guid><pubDate>Fri, 26 Sep 2025 14:56:04 +0000</pubDate></item><item><title>Did a US Chess Champion Cheat?</title><link>https://www.chicagobooth.edu/review/did-us-chess-champion-cheat</link><description>&lt;doc fingerprint="6956837f6123599f"&gt;
  &lt;main&gt;
    &lt;p&gt;The chess world is no stranger to scandals, from a 1960s fistfight between grand masters Bobby Fischer and Pal Benko to allegations of another grand master, Vladimir Kramnik, cheating in the 2006 world championship by accessing a phone during bathroom breaks. Just this fall, Kramnik expressed “concerns” on X about grand master Daniel Naroditsky and unspecified other players.&lt;/p&gt;
    &lt;p&gt;A prominent recent controversy erupted in November 2023, when Kramnik insinuated on his chess.com blog and in a YouTube video that Hikaru Nakamura cheated. Nakamura is a grand master and five-time US chess champion known for his aggressive style of play. Kramnik’s comments were a reaction to Nakamura’s almost flawless 45.5 out of 46 game winning streak in the high-speed chess.com online blitz tournament, where each player only has a total of three minutes of playing time per game. Kramnik pointed out the statistical improbability of Nakamura’s streak and stated that such a winning run would require the chess prodigy to play at a level higher than his current Elo rating (an estimate of a player’s skill level based on their historical play).&lt;/p&gt;
    &lt;p&gt;Was this just one of those rare streaks that occurs from time to time in sports, or was Nakamura relying on more than just his talent, as Kramnik believed?&lt;/p&gt;
    &lt;p&gt;The statistical elements of the criticism drew the attention of researchers including Shiva Maharaj of the chess school CHESS-ED, Chicago Booth’s Nicholas Polson, and George Mason University’s Vadim Sokolov. Using statistical analysis to investigate the cheating allegation, they find a 99.6 percent probability that Nakamura did not cheat. Moreover, their research highlights how the improper use of statistical evidence can distort or bias interpretations and lead to flawed conclusions.&lt;/p&gt;
    &lt;p&gt;For data, the researchers used Nakamura’s performance in more than 3,500 games he played on chess.com, including the 46 games in question. They also compared Nakamura’s Elo rating with those of his opponents, and find that Nakamura was a much stronger player than those whom he played.&lt;/p&gt;
    &lt;p&gt;This skill imbalance may have been an underestimated factor in the resulting winning streak, the researchers write. After calculating that there was a less than 3 percent chance of Nakamura’s demonstrated winning streak given his opponents’ ratings, the researchers used Bayesian analysis to reexamine the likelihood of it having occurred without cheating. This type of analysis refines an initial hypothesis by continuously incorporating new information to produce a more accurate assessment.&lt;/p&gt;
    &lt;p&gt;As a starting point, the researchers needed an estimate of the level of cheating that occurs in online chess games. Viswanathan Anand, deputy president of the World Chess Federation, stated in a 2022 discussion with the Hindustan Times that the number of online chess games in which cheating occurs “must be 1 in 10,000.” Using this estimated probability as an initial measure, they were able to calculate the high likelihood of Nakamura’s innocence.&lt;/p&gt;
    &lt;p&gt;But what if Anand’s estimate was off and cheating was more prevalent? After all, online games have less oversight than in-person tournaments. Maharaj, Polson, and Sokolov recalculated the probability of Nakamura’s innocence using a number of harsher estimations of online cheating. While the probability of his innocence was lower when it was assumed that cheating occurs in 1 out of every 500 games, the probability was high thereafter and sat in the 98 percent range once that estimate rose to 1 out of 1,500 games. This highlights how crucial the initial assumptions are in any analysis, especially when dealing with probabilities.&lt;/p&gt;
    &lt;head rend="h2"&gt;A real winning streak&lt;/head&gt;
    &lt;p&gt;Although the likelihood of any chess player winning 45.5 out of 46 online chess games is statistically low, the researchers estimate that the probability that grand master Nakamura won that many games without cheating—given his skill and the rarity of cheating among top players—is high.&lt;/p&gt;
    &lt;p&gt;Not only are initial assumptions important; improper use of statistical evidence can lead to misinterpretations. The researchers state that Kramnik’s claim—that the low probability of the streak was evidence of Nakamura’s guilt—falls into what statisticians call the prosecutor’s fallacy, a common misunderstanding in statistical reasoning that confuses the probability of evidence given innocence with the probability of innocence given evidence. Just because an event is unlikely does not mean the opposite of the situation must be true. For example, just because a low probability of a winning streak implies a high probability of cheating, that doesn’t mean Nakamura cheated, despite the streak’s unlikeliness.&lt;/p&gt;
    &lt;p&gt;Nakamura responded to Kramnik’s allegations by arguing that focusing on a particular streak while ignoring other games was cherry-picking. The researchers note that there’s a problem with this argument, too, as it violates the likelihood principle. This principle tells us the interpretation should only rely on the actual data observed, not the context in which it was collected.&lt;/p&gt;
    &lt;p&gt;The researchers also highlighted another statistical concept, Cromwell’s rule. This cautions against assigning 0 percent or 100 percent probability to an event. Even when things seem either impossible or certain, there may be factors that have not yet been considered or nuances in the data that could change our understanding of the probability.&lt;/p&gt;
    &lt;p&gt;Overall, the researchers issue a reminder to be critical consumers of information. No matter how rare or unusual an event may seem, before interpreting the data, consider the assumptions you’re making. Otherwise, the data’s framing can affect the conclusions you draw, even if you had no intention of manipulation. And that can damage reputations.&lt;/p&gt;
    &lt;p&gt;Shiva Maharaj, Nicholas Polson, and Vadim Sokolov, “Kramnik vs. Nakamura: A Chess Scandal,” preprint, arXiv, September 2024.&lt;/p&gt;
    &lt;p&gt;Your Privacy&lt;lb/&gt; We want to demonstrate our commitment to your privacy. Please review Chicago Booth's privacy notice, which provides information explaining how and why we collect particular information when you visit our website.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45387337</guid><pubDate>Fri, 26 Sep 2025 15:02:53 +0000</pubDate></item><item><title>Context is the bottleneck for coding agents now</title><link>https://runnercode.com/blog/context-is-the-bottleneck-for-coding-agents-now</link><description>&lt;doc fingerprint="6750dc7768fd1a4f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Context is the bottleneck for coding agents now&lt;/head&gt;
    &lt;p&gt;Intelligence is rapidly improving with each model release. Just last week it was announced that OpenAI got a perfect score on the 2025 ICPC programming contest, beating every single human contestant. They achieved this using a version (presumably a very high compute version, but still) of their publicly available GPT-5 model.&lt;/p&gt;
    &lt;p&gt;And yet, coding agents are nowhere near capable of replacing software developers. Why is that?&lt;/p&gt;
    &lt;p&gt;I’m going to argue that the limiting factor is no longer raw intelligence, but rather context. Existing coding agents simply do not have enough context about the problems they’re being asked to solve. This severely limits how long they can work effectively without human guidance.&lt;/p&gt;
    &lt;head rend="h2"&gt;Intelligence and context&lt;/head&gt;
    &lt;p&gt;How autonomous are existing coding agents? Let’s think about autonomy as a spectrum and see how far along that spectrum we are.&lt;/p&gt;
    &lt;p&gt;Level 1 - A few lines of code&lt;lb/&gt;This is what autocomplete does, and it works very well.&lt;/p&gt;
    &lt;p&gt;Level 2 - One commit&lt;lb/&gt;Cursor and Claude Code work well for tasks in this size range.&lt;/p&gt;
    &lt;p&gt;Level 3 - One PR&lt;lb/&gt;Devin and other async agents are built to tackle tasks of this size. But do they work reliably? Only on relatively simple tasks.&lt;/p&gt;
    &lt;p&gt;Level 4 - Major feature or refactor&lt;lb/&gt;Doing this autonomously on an existing codebase is beyond the reach of current agents.&lt;/p&gt;
    &lt;p&gt;Level 5 - Entire codebase&lt;lb/&gt;This is what the vibe coding products like Lovable and Replit do now, but it only works because they can start from scratch. The problem is that they usually hit a wall well before they get to a production-ready application.&lt;/p&gt;
    &lt;p&gt;I’d say Level 2 is all we can reliably do on production codebases right now. And even that requires substantial human guidance and review. What will it take to move further along the autonomy spectrum, without sacrificing quality?&lt;/p&gt;
    &lt;p&gt;When an agent fails at a task, the cause is usually one of two things. It’s either an intelligence failure, or it’s a context failure. Either the model didn’t have the information it needed, or it didn’t have the mental horsepower to process that information properly. There are other aspects that can affect performance, such as taste, but if we’re just talking about whether the agent succeeds or fails at a task, it’s sufficient to just consider intelligence and context. Also note that I’m including general world knowledge as part of intelligence, both for simplicity and because I think it’s hard to fully separate those two out.&lt;/p&gt;
    &lt;p&gt;Programming competitions are competitions of intelligence. The entire context needed to solve a problem is provided in the problem statement itself. There’s no existing codebase to understand, no business requirements to consider, and no unwritten development processes you need to follow.&lt;/p&gt;
    &lt;p&gt;The superhuman ICPC performance we saw this week, as well as the IOI gold medal-level performances from last month, strongly suggest that the raw intelligence and general programming knowledge of frontier models is sufficient to automate most software engineering work.&lt;/p&gt;
    &lt;p&gt;Now these performances were achieved using models that are quite a bit stronger than the models used on a daily basis by developers, like Claude 4 and GPT-5. So we can’t quite say that lack of intelligence is never a cause of failure in current coding agents. They still do some pretty dumb stuff sometimes. But as models improve, more and more of the failures in agentic coding are failures of context, not failures of intelligence.&lt;/p&gt;
    &lt;head rend="h2"&gt;What context does a coding agent need?&lt;/head&gt;
    &lt;p&gt;Context isn’t just code. It’s also specs, dev practices, conversations, etc. When human developers write code, they’re drawing from a reservoir of implicit knowledge that goes far beyond what’s visible in the codebase itself. Current coding agents are operating with maybe 20% of this context, at best.&lt;/p&gt;
    &lt;p&gt;What context does an agent need to reliably operate autonomously and ship code that’s as good or better than human developers? It’s the same things a human developer needs.&lt;/p&gt;
    &lt;p&gt;There are the basics:&lt;/p&gt;
    &lt;p&gt;It needs to be able to access all code files&lt;lb/&gt;Most coding agents can already do this.&lt;/p&gt;
    &lt;p&gt;It needs to be able to access documentation&lt;lb/&gt;Most coding agents can do this if set up properly.&lt;/p&gt;
    &lt;p&gt;It needs to be able to run code and see the output&lt;lb/&gt;Most coding agents can already do this pretty well.&lt;/p&gt;
    &lt;p&gt;And then there are the more subtle forms of context:&lt;/p&gt;
    &lt;p&gt;It needs to have a high-level understanding of how the codebase is organized and where different code lives&lt;lb/&gt;This is important for efficient execution and also for making sure you don’t miss things. Most tool-based agents, like Cursor and Claude Code, do not have this. Some agents are provided with something along these lines.&lt;/p&gt;
    &lt;p&gt;It needs to understand all of the existing architectural patterns and conventions in the codebase&lt;lb/&gt;Every codebase has its own dialect. Maybe you always use dependency injection in a specific way. Perhaps there’s an unwritten rule about where business logic lives versus presentation logic. Maybe you have a specific pattern for handling async operations that evolved organically over three years.&lt;/p&gt;
    &lt;p&gt;Current agents struggle here because many of these patterns are emergent properties of the codebase that aren’t documented in any single place. They’re distributed across thousands of commits, pull requests, and code reviews.&lt;/p&gt;
    &lt;p&gt;It needs to understand why things were done the way they were&lt;lb/&gt;Why does the authentication system work the way it does? Because two years ago, there was a security incident that led to a complete redesign. Why don’t we use library X even though it seems perfect? Because it caused production issues in 2022.&lt;/p&gt;
    &lt;p&gt;This tribal knowledge lives in Slack threads, meeting notes, incident post-mortems, and developers’ heads.&lt;/p&gt;
    &lt;p&gt;It needs to understand development and deployment practices&lt;lb/&gt;Testing expectations, style and comment guidelines, etc.&lt;/p&gt;
    &lt;p&gt;Every team has unwritten rules about how code ships. Maybe you deploy to staging-east first because of a subtle dependency. Perhaps certain tests look weird because they’re working around a known race condition. The CI/CD pipeline has manual approval steps that seem redundant but prevent real disasters that happened in the past.&lt;/p&gt;
    &lt;p&gt;Current agents can read your test configs and deployment scripts, but they don’t understand the “why” behind them. They might remove a “redundant” check that’s actually preventing a production issue, or follow official docs that everyone knows are outdated.&lt;/p&gt;
    &lt;p&gt;It needs to understand product and business requirements&lt;lb/&gt;Code doesn’t exist in a vacuum. That seemingly arbitrary validation rule? It’s there because of a regulatory requirement in the EU market. That weird data transformation? It’s handling an edge case for your biggest enterprise customer.&lt;/p&gt;
    &lt;p&gt;I don’t know of any coding agents that are plugged into this kind of data right now.&lt;/p&gt;
    &lt;p&gt;Notice how all the basic forms of context use the word “access” while the more subtle forms of context use the word “understand.” This is important. Most of this context is not written down in a single document that the agent can just read. To the extent that it’s written down at all, it’s often scattered across many different files and apps. Some of that information will be conflicting and out of date. Giving the agent this context is not as simple as just giving it an MCP connector to your Google Drive and Linear accounts. The information needs to be processed and synthesized by the agent.&lt;/p&gt;
    &lt;p&gt;What does this mean for coding agents?&lt;/p&gt;
    &lt;p&gt;First, we need to give them access to way more context. Much of this new context will require sophisticated preprocessing to make it usable, so this is not an easy problem.&lt;/p&gt;
    &lt;p&gt;Second, not everything is written down. That means experienced human developers will still need to fill in the gaps for a very long time to come.&lt;/p&gt;
    &lt;p&gt;Third, agents need to learn to identify when they’re missing context so they can ask for human guidance. Right now they seem to be trained to just plow forward with what they have.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45387374</guid><pubDate>Fri, 26 Sep 2025 15:06:42 +0000</pubDate></item><item><title>Show HN: Dreamtap – Make your AI more creative</title><link>https://dreamtap.xyz/</link><description>&lt;doc fingerprint="94095c1aadabbafb"&gt;
  &lt;main&gt;
    &lt;p&gt;⚡ Stop reading the same story 20 times ⚡ Explore different ideas ⚡ The AIs get bored too! ⚡&lt;/p&gt;
    &lt;p&gt;Dreamtap: Inspire your AI to be more creative&lt;/p&gt;
    &lt;p&gt;Dreamtap is a chatbot plugin that gives your AI wide-ranging sources of inspiration to do better at creative tasks - writing, design, etc.&lt;/p&gt;
    &lt;p&gt;Dreamtap works with Claude, ChatGPT (beta), and other tools that support MCP.&lt;/p&gt;
    &lt;p&gt;Dreamtap is free and doesn't see your chat content or know who you are.&lt;/p&gt;
    &lt;p&gt;Why?&lt;/p&gt;
    &lt;p&gt;When you ask AI to write stories, you've probably noticed they are frequently incredibly similar. Claude, for example, likes lighthouses and cartographers.&lt;/p&gt;
    &lt;p&gt;This is mode collapse – the AI defaulting to the safest, most average patterns in its training data. Every story becomes a slight variation of the same template.&lt;/p&gt;
    &lt;p&gt;How Dreamtap Fixes This&lt;/p&gt;
    &lt;p&gt;Dreamtap injects randomized sources of inspiration before your AI starts generating. Instead of letting the model slide into its default patterns, it inspires the model with some concepts unconnected to your prompt. Claude decides by itself when it needs more inspiration and uses Dreamtap. ChatGPT is a bit worse at this, and you'll need to use the tool manually.&lt;/p&gt;
    &lt;p&gt;The Difference in Practice&lt;/p&gt;
    &lt;p&gt;Below are stories written by Claude with the same prompt: "Write an inspired short story"&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45387421</guid><pubDate>Fri, 26 Sep 2025 15:11:31 +0000</pubDate></item><item><title>Fast UDP I/O for Firefox in Rust</title><link>https://max-inden.de/post/fast-udp-io-in-firefox/</link><description>&lt;doc fingerprint="9cacf1177618bfaa"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Motivation&lt;/head&gt;
    &lt;p&gt;Around 20% of Firefox’s HTTP traffic today uses HTTP/3, which runs over QUIC, which in turn runs over UDP. This translates to substantial UDP I/O activity.&lt;/p&gt;
    &lt;p&gt;Firefox uses NSPR for most of its network I/O. When it comes to UDP I/O, NSPR only offers a limited set of dated APIs, most relevant here &lt;code&gt;PR_SendTo&lt;/code&gt; and &lt;code&gt;PR_RecvFrom&lt;/code&gt;, wrappers around POSIX’s &lt;code&gt;sendto&lt;/code&gt; and &lt;code&gt;recvfrom&lt;/code&gt;.
The N in NSPR stands for Netscape, giving you a hint of its age.&lt;/p&gt;
    &lt;p&gt;Operating systems have evolved since. Many offer multi-message APIs like &lt;code&gt;sendmmsg&lt;/code&gt; and &lt;code&gt;recvmmsg&lt;/code&gt;.
Some offer segmentation offloading like GSO (Generic Segmentation Offload) and GRO (Generic Receive Offload).
Each of these promise significant performance improvements for UDP I/O.&lt;/p&gt;
    &lt;p&gt;Can Firefox benefit from replacing its aging UDP I/O stack with modern system calls?&lt;/p&gt;
    &lt;head rend="h2"&gt;Overview&lt;/head&gt;
    &lt;p&gt;This project began in mid-2024 with the goal of rewriting Firefox’s QUIC UDP I/O stack using modern system calls across all supported operating systems. Beyond performance improvements, we wanted to increase security by using a memory-safe language to do UDP I/O. Firefox’s QUIC state machine itself is implemented in Rust already. We thereby chose Rust for this project as well, giving us both increased security and easy integration with the existing QUIC codebase.&lt;/p&gt;
    &lt;p&gt;Instead of starting from scratch, we built on top of &lt;code&gt;quinn-udp&lt;/code&gt;, the UDP I/O library of the Quinn project, a QUIC implementation in Rust.
This sped up our development efforts significantly.
Big thank you to the Quinn project.
Operating system calls are complex, with a myriad of idiosyncrasies, especially across versions.
Firefox is multi-platform, focusing on Windows, Android, MacOS and Linux as tier 1.
The main complexity though stems from Firefox supporting ancient versions of each of them, e.g.
Android 5.&lt;/p&gt;
    &lt;p&gt;One year later, i.e., mid 2025, this project is now rolling out to the majority of Firefox users. Performance benchmark results are promising. In extreme cases, on purely CPU bound benchmarks, we’re seeing a jump from &amp;lt; 1Gbit/s to 4 Gbit/s. Looking at CPU flamegraphs, the majority of CPU time is now spent in I/O system calls and cryptography code.&lt;/p&gt;
    &lt;p&gt;Below are the many improvements we were able to land, plus the ones we weren’t. I hope other projects in need of fast UDP I/O can benefit from our work. To make their lifes easier, below I am documenting the many learnings we made.&lt;/p&gt;
    &lt;head rend="h2"&gt;The basics&lt;/head&gt;
    &lt;p&gt;To understand the improvements, it’s helpful to first examine how UDP I/O traditionally works and how modern optimizations change this picture.&lt;/p&gt;
    &lt;head rend="h3"&gt;Single datagram&lt;/head&gt;
    &lt;p&gt;Previously Firefox would send (and receive) single UDP datagrams to (and from) the OS via &lt;code&gt;sendto&lt;/code&gt; (and &lt;code&gt;recvfrom&lt;/code&gt;) system call family.
The OS would send (and receive) that UDP datagram to (and from) the network interface card (NIC).
The NIC would send (and receive) it to (and from) the Internet.&lt;/p&gt;
    &lt;p&gt;Thus each datagram would require leaving user space which is cheap for one UDP datagram, but expensive when sending at say a 500 Mbit/s rate. In addition all user space and kernel space overhead independent of the number of bytes sent and received, is paid per datagram, i.e. per &amp;lt; 1500 bytes.&lt;/p&gt;
    &lt;code&gt;    +----------------------+
    |       Firefox        |
    |    +-----------+     |
    |    |   QUIC    |     |
    |    +-----------+     |
    +----------------------+
              |
         [ datagram ]
              |
    === User / Kernel ===
              |
         [ datagram ]
              |
    +----------------------+
    |         OS           |
    +----------------------+
              |
         [ datagram ]
              |
    +----------------------+
    |         NIC          |
    +----------------------+
              |
         [ datagram ]
              |
    +----------------------+
    |      Internet        |
    +----------------------+
&lt;/code&gt;
    &lt;head rend="h3"&gt;Batch of datagrams&lt;/head&gt;
    &lt;p&gt;Instead of sending a single datagram at a time, some operating systems nowadays offer multi-message system call families, e.g. on Linux &lt;code&gt;sendmmsg&lt;/code&gt; and &lt;code&gt;recvmmsg&lt;/code&gt;.
The idea is simple.
Send and receive multiple UDP datagrams at once, save on the costs that are independent of the number of bytes sent and received.&lt;/p&gt;
    &lt;code&gt;    +--------------------------+
    |         Firefox          |
    |      +-----------+       |
    |      |   QUIC    |       |
    |      +-----------+       |
    +--------------------------+
                  |
  [ datagram, datagram, datagram ]
                  |
    ===== User / Kernel =====
                  |
  [ datagram, datagram, datagram ]
                  |
    +--------------------------+
    |           OS             |
    +--------------------------+
                  |
  [ datagram, datagram, datagram ]
                  |
    +--------------------------+
    |           NIC            |
    +--------------------------+
                  |
  [ datagram, datagram, datagram ]
                  |
    +--------------------------+
    |        Internet          |
    +--------------------------+
&lt;/code&gt;
    &lt;head rend="h3"&gt;Single large segmented datagram&lt;/head&gt;
    &lt;p&gt;Some modern operating systems and network interface cards also support system call families with UDP segmentation offloading, e.g. &lt;code&gt;GSO&lt;/code&gt; and &lt;code&gt;GRO&lt;/code&gt; on Linux.
Instead of sending multiple UDP datagrams in a batch, it enables the application to send a single large UDP datagram, i.e. larger than the Maximum Transmission Unit, to the kernel.
Next, either the kernel, but really ideally the network interface card, will segment it into multiple smaller packets, add a header to each and calculates the UDP checksum.
The reverse happens on the receive path, where multiple incoming packets can be coalesced into a single large UDP datagram delivered to the application all at once.&lt;/p&gt;
    &lt;code&gt;    +------------------------------+
    |           Firefox            |
    |        +-----------+         |
    |        |   QUIC    |         |
    |        +-----------+         |
    +------------------------------+
                    |
      [ large segmented datagram ]
                    |
      ====== User / Kernel ======
                    |
      [ large segmented datagram ]
                    |
    +------------------------------+
    |             OS               |
    +------------------------------+
                    |
      [ large segmented datagram ]
                    |
    +------------------------------+
    |             NIC              |
    +------------------------------+
                    |
    [ datagram, datagram, datagram ]
                    |
    +------------------------------+
    |          Internet            |
    +------------------------------+
&lt;/code&gt;
    &lt;p&gt;Note: Unfortunately, Wireshark does not yet support GSO, making network-level debugging more challenging when these optimizations are active.&lt;/p&gt;
    &lt;p&gt;For performance analysis of these different approaches, Cloudflare’s comprehensive study provides excellent benchmarks and detailed explanations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Replacing NSPR in Firefox&lt;/head&gt;
    &lt;p&gt;Batching and segmentation offloading aside for now, first step in the project was to replace usage of NSPR with quinn-udp, still sending and receiving one UDP datagram at a time. We updated the Mozilla QUIC client and server test implementation, then integrated quinn-udp into Firefox itself.&lt;/p&gt;
    &lt;p&gt;Next we rewrote the UDP datagram processing pipeline in the Mozilla QUIC implementation to send and receive batches of datagrams. This is done in a way, such that we can leverage both the multi-message style system calls, as well as the segmentation offloading style, if available. We added this along with various other I/O improvements, e.g. Lars added in-place en-/decryption. Going into detail here is better done in a separate blog post. Let’s focus on UDP I/O here.&lt;/p&gt;
    &lt;p&gt;So far so good. This was the easy part. Up next, the edge cases by platform.&lt;/p&gt;
    &lt;head rend="h2"&gt;Platform details&lt;/head&gt;
    &lt;head rend="h3"&gt;Windows&lt;/head&gt;
    &lt;p&gt;Windows offers &lt;code&gt;WSASendMsg&lt;/code&gt; and &lt;code&gt;WSARecvMsg&lt;/code&gt; to send and receive a single UDP datagram.
That UDP datagram can either be a classic MTU size datagram, or a large segmented datagram.
For the latter, what Linux calls &lt;code&gt;GSO&lt;/code&gt; and &lt;code&gt;GRO&lt;/code&gt;, Windows call &lt;code&gt;USO&lt;/code&gt; and &lt;code&gt;URO&lt;/code&gt;.
As described above, we started off rolling out quinn-udp using single-datagram system calls only.
This went without issues on Windows.&lt;/p&gt;
    &lt;p&gt;Next we tested &lt;code&gt;WSARecvMsg&lt;/code&gt; with &lt;code&gt;URO&lt;/code&gt;, i.e. receiving a batch of inbound datagrams as a single large segmented datagram, but got the following bug report:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;fosstodon.org doesn’t load with network.http.http3.use_nspr_for_io=false on ARM64 Windows&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;fosstodon is a Mastodon server. It is hosted behind the CDN provider Fastly. Fastly is a heavy user of Linux’s GSO, i.e. sends larger UDP datagram trains, perfect to be coalesced into a single large segmented UDP datagram when Firefox receives it. Why would Window’s &lt;code&gt;URO&lt;/code&gt; prevent Firefox from loading the site?&lt;/p&gt;
    &lt;p&gt;After many hours of back and forth with the reporter, luckily a Mozilla employee as well, I ended up buying the exact same laptop, same color, in a desperate attempt to reproduce the issue. Without much luck at first, I eventually needed a Linux command line tool, thus installed WSL, and to my surprise, that triggered the bug (reproducer). Turns out, on Windows on ARM, with WSL enabled, a &lt;code&gt;WSARecvMsg&lt;/code&gt; call with &lt;code&gt;URO&lt;/code&gt; would not return a segment size, thus Firefox was unable to differentiate a single datagram, from a single segmented datagram.
QUIC short header packets don’t carry a length, thus there is no way to tell where one QUIC packet ends and another starts, leading to the above page load failures.&lt;/p&gt;
    &lt;p&gt;We have been in touch with Microsoft since. No progress thus far. Thereby we are keeping &lt;code&gt;URO&lt;/code&gt; on Windows disabled in Firefox for now.&lt;/p&gt;
    &lt;p&gt;After &lt;code&gt;URO&lt;/code&gt; we started using &lt;code&gt;WSASendMsg&lt;/code&gt; &lt;code&gt;USO&lt;/code&gt;, i.e. sending a single large segmented datagram per system call.
But this too we rolled back quickly, seeing increased packet loss on Firefox Windows installations.
In addition, we have at least one report of a user, seeing their network driver crash due to Firefox’s usage of &lt;code&gt;USO&lt;/code&gt;.
More debugging needed.&lt;/p&gt;
    &lt;head rend="h3"&gt;MacOS&lt;/head&gt;
    &lt;p&gt;The transition on MacOS from NSPR to quinn-udp for HTTP/3 QUIC UDP I/O involved switching from the system calls &lt;code&gt;sendto&lt;/code&gt; and &lt;code&gt;recvfrom&lt;/code&gt; to the system calls &lt;code&gt;sendmsg&lt;/code&gt; and &lt;code&gt;recvmsg&lt;/code&gt;.
As with Windows, no issues on this first step, ignoring one report where MacOS 10.15 might be seeing IP packets other than v4 and v6 (fixed since).&lt;/p&gt;
    &lt;p&gt;Unfortunately MacOS does not offer UDP segmentation offloading, neither on the send, nor on the receive side. What it does offer though are two undocumented system calls, namely &lt;code&gt;sendmsg_x&lt;/code&gt; and &lt;code&gt;recvmsg_x&lt;/code&gt;, allowing a user to send and receive batches of UDP datagrams at once.
Lars from Mozilla added it to quinn-udp, exposed behind the &lt;code&gt;fast-apple-datapath&lt;/code&gt; feature flag, off by default.
After multiple iterations with smaller bugfixes (#2154, #2214, #2216 …) we decided to not ship it to users, not knowing how MacOS would behave, in case Apple ever decides to remove it, but with Firefox still calling it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Linux&lt;/head&gt;
    &lt;p&gt;Linux provides the most comprehensive and mature UDP optimization support, offering both multi-message APIs (&lt;code&gt;sendmmsg&lt;/code&gt;/&lt;code&gt;recvmmsg&lt;/code&gt;) and segmentation offloading (GSO/GRO).
The quinn-udp library makes a deliberate choice to prioritize GSO over &lt;code&gt;sendmmsg&lt;/code&gt; for transmission, as GSO typically provides superior performance with diminishing returns when both techniques are combined.
Thus far, this has proven the right choice for Firefox as well.&lt;/p&gt;
    &lt;p&gt;In addition to segmentation offloading being superior in the first place, Firefox uses one UDP socket per connection in order to improve privacy. As each socket gets its own source port it is harder to correlate connections. Why is this relevant here? &lt;code&gt;GSO&lt;/code&gt; (and &lt;code&gt;GRO&lt;/code&gt;) can only segment (and coalesce) datagrams from the same 4-tuple (src IP, src port, dst IP, dst port), &lt;code&gt;sendmmsg&lt;/code&gt; and &lt;code&gt;recvmmsg&lt;/code&gt; on the other hand can send and receive across 4-tuples.
Given that Firefox uses one socket per connection, it cannot make use of that distinct benefit of &lt;code&gt;sendmmsg&lt;/code&gt; (and &lt;code&gt;recvmmsg&lt;/code&gt;), making segmentation offloading yet again the obvious choice for Firefox.&lt;/p&gt;
    &lt;p&gt;Ignoring minor changes required to Firefox’s optional network sandboxing, and an additional at runtime GSO support check, replacing Firefox’s QUIC UDP I/O stack on Linux has been without issues, now enjoying all the benefits of segmentation offloading.&lt;/p&gt;
    &lt;head rend="h3"&gt;Android&lt;/head&gt;
    &lt;p&gt;During the time of this project I learned quickly that (a) Android is not Linux and (b) that Firefox still supports Android 5, …, on x86 (32 bit).&lt;/p&gt;
    &lt;p&gt;On x86, Android dispatches advanced socket calls through &lt;code&gt;socketcall&lt;/code&gt; system call instead of calling e.g. &lt;code&gt;sendmsg&lt;/code&gt; directly.
In addition Android has various default seccomp filters, crashing an app when e.g. not going through the required &lt;code&gt;socketcall&lt;/code&gt; system call.
The combination of the two did cost me a couple of days, resulting in this (basically single line) change in quinn-udp.&lt;/p&gt;
    &lt;p&gt;On Android API level 25 and below, calling &lt;code&gt;sendmsg&lt;/code&gt; with an ECN bit set results in an error &lt;code&gt;EINVAL&lt;/code&gt;.
quinn-udp will now simply retry on &lt;code&gt;EINVAL&lt;/code&gt; disabling various optional settings (e.g. ECN) on the second attempt.&lt;/p&gt;
    &lt;p&gt;Great benefit of the Quinn community is that Firefox will benefit from any improvements made to quinn-udp. For example this excellent find by Thomas where Android in some cases would complain if we did a &lt;code&gt;GSO&lt;/code&gt; with a single segment only.&lt;/p&gt;
    &lt;head rend="h2"&gt;Explicit congestion notifications (ECN)&lt;/head&gt;
    &lt;p&gt;With Firefox using modern system calls across all major operating systems, a nice additional benefit is the ability to send and receive ancillary data like IP ECN. This too came with some minor surprises, but QUIC ECN in Firefox is well on its way now. Firefox Nightly telemetry shows around 50% of all QUIC connections running on ECN outbound capable paths. With L4S and thus ECN becoming more and more relevant in today’s Internet, this is a great step forward.&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;p&gt;We successfully replaced Firefox’s QUIC UDP I/O stack with a modern memory-safe implementation using quinn-udp. Instead of limited and dated system calls like &lt;code&gt;sendto&lt;/code&gt; and &lt;code&gt;recvfrom&lt;/code&gt;, Firefox now uses modern OS specific system calls across all major platforms, resulting in HTTP/3 QUIC throughput improvements when CPU bound, and enabling QUIC ECN support across all major platforms.
Some OS specific optimizations still need more work, e.g. &lt;code&gt;USO&lt;/code&gt; and &lt;code&gt;URO&lt;/code&gt; on Windows.
That said, especially given QUIC’s growing adoption and thus increased UDP usage, I am optimistic that OS and driver support will continue to improve.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45387462</guid><pubDate>Fri, 26 Sep 2025 15:14:44 +0000</pubDate></item><item><title>Better health conversations: Research on a "wayfinding" AI agent based on Gemini</title><link>https://research.google/blog/towards-better-health-conversations-research-insights-on-a-wayfinding-ai-agent-based-on-gemini/</link><description>&lt;doc fingerprint="f448c2f07b73a773"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Towards better health conversations: Research insights on a “wayfinding” AI agent based on Gemini&lt;/head&gt;
    &lt;p&gt;September 25, 2025&lt;/p&gt;
    &lt;p&gt;Mike Schaekermann, Research Scientist, and Rory Sayres, Researcher, Google Research&lt;/p&gt;
    &lt;p&gt;We share user insights from a novel research AI agent that helps people find their way to better health information through proactive conversational guidance, goal understanding, and tailored conversations.&lt;/p&gt;
    &lt;p&gt;The ability to find clear, relevant, and personalized health information is a cornerstone of empowerment for medical patients. Yet, navigating the world of online health information is often a confusing, overwhelming, and impersonal experience. We are met with a flood of generic information that does not account for our unique context, and it can be difficult to know what details are relevant.&lt;/p&gt;
    &lt;p&gt;Large language models (LLMs) have the potential to make this information more accessible and tailored. However, many AI tools today act as passive "question-answerers" — they provide a single, comprehensive answer to an initial query. But this isn't how an expert, like a doctor, helps someone navigate a complex topic. A health professional doesn't just provide a lecture; they ask clarifying questions to understand the full picture, discover a person's goals, and guide them through the information maze. Though this context-seeking is critical, it's a significant design challenge for AI.&lt;/p&gt;
    &lt;p&gt;In “Towards Better Health Conversations: The Benefits of Context-Seeking”, we describe how we designed and tested our “Wayfinding AI”, an early-stage research prototype, based on Gemini, that explores a new approach. Our fundamental thesis is that by proactively asking clarifying questions, an AI agent can better discover a user's needs, guide them in articulating their concerns, and provide more helpful, tailored information. In a series of four mixed-method user experience studies with a total of 163 participants, we examined how people interact with AI for their health questions, and we iteratively designed an agent that users found to be significantly more helpful, relevant, and tailored to their needs than a baseline AI agent.&lt;/p&gt;
    &lt;head rend="h2"&gt;Formative user experience insights: Challenges in finding health information online&lt;/head&gt;
    &lt;p&gt;To better understand the hurdles people face, we interviewed 33 participants about their experiences finding health information online. A key theme quickly emerged: people often struggle to articulate their health concerns. As one participant described, their process was to "...just kind of like throw all the words in there and then I'm just gonna see what comes back." It may be that without a clinical background, it’s difficult to know which details are medically relevant.&lt;/p&gt;
    &lt;p&gt;The people we interviewed were then able to use research prototypes of different chatbots. (The chat histories were not logged.) These participants made up a diverse group and asked health questions on a wide range of topics (e.g., rib pain, vertigo, consistent and unexplained weight gain, tinnitus and surgery; more details in the paper). Our studies revealed that when a chatbot proactively asks clarifying questions, the experience changes dramatically. The majority of participants preferred a "deferred-answer" approach — where the AI asks questions first — over one that gives a comprehensive answer immediately. This conversational style was perceived as more personal and reassuring. As one person noted, "It feels more like the way it would work if you talk to a doctor... it does make me feel a little more confident that it wants to know more before jumping right into an answer." These clarifying questions not only help the AI provide better answers, but also empower users, guiding them to provide more relevant context. We found similar patterns in prior work on AI for dermatology.&lt;/p&gt;
    &lt;p&gt;However, the effectiveness of this clarifying question–based approach depends heavily on the execution — engagement drops if questions are poorly formulated, irrelevant, or buried within long paragraphs of text where they are easily missed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Designing a Wayfinding AI to empower people through personal and proactive conversations&lt;/head&gt;
    &lt;p&gt;Informed by these insights, we designed our Wayfinding AI around three core principles to create a more empowering conversational experience:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Proactive conversational guidance: At each turn, the Wayfinding AI asks up to three targeted questions designed to systematically reduce ambiguity. This helps users articulate their health story more completely and directly incorporates users’ desire for more contextualized answers.&lt;/item&gt;
      &lt;item&gt;Best-effort answers at each turn: Because some health-related questions may not require clarification to get a good answer, the Wayfinding AI provides a "best-effort" answer at every conversational turn, based on the information shared so far, while emphasizing that the answer can be improved if the user can answer one or more of the follow-up questions. This approach gives the user helpful information throughout the conversation, while providing the option to further receive increasingly better answers as the conversation progresses.&lt;/item&gt;
      &lt;item&gt;Transparent reasoning: The Wayfinding AI explains how the user's latest answers have helped refine the previous answer. This makes the AI's reasoning process clear and understandable.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To ensure clarifying questions are never missed within the longer answers in the “best-effort” answers section, we designed an interface with a two-column layout. The conversation and clarifying questions appear in the left column, while best-effort answers and more detailed explanations appear in the right. This separates the interactive conversation from the informational content.&lt;/p&gt;
    &lt;head rend="h2"&gt;Evaluating our Wayfinding AI through a randomized user study&lt;/head&gt;
    &lt;p&gt;To evaluate the potential real-world impact of this agent, we conducted a randomized user study with 130 US-based participants recruited via a third party platform. All participants were 21 years and older, were not health care professionals, and had a health-related question for which they were willing to interact with an AI. To ensure a broad range of health topics, we imposed very few restrictions on which topic would be eligible for the study (details on excluded inquiries are provided in the paper). In a randomized within-subjects design, each participant interacted with both our Wayfinding AI and a baseline Gemini 2.5 Flash model to explore their health topic. After providing informed consent and answering standard demographic questions, participants were instructed to have a conversation spending at least 3 minutes on their question; and then to resume the survey. After interacting with each AI, participants answered questions about their satisfaction with the experience along 6 dimensions: helpfulness, relevance of questions asked, tailoring to their situation, goal understanding, ease of use, and efficiency of getting useful information. They were able to provide open feedback about what they learned, and also had the option to upload their conversation with the AI. Sharing the conversation was not required to complete the survey. At the end of the study, participants were prompted to explicitly compare the two AIs and indicate which they would prefer in terms of each of the six dimensions above. They were also asked, "For a future topic, would you prefer the first or the second AI?" The order of AI exposure (Baseline AI first vs. Wayfinding AI first) was randomized across participants. Throughout the study, participants were instructed to not provide any identifying information about themselves.&lt;/p&gt;
    &lt;head rend="h2"&gt;Helpful and relevant information through goal understanding and tailored conversations&lt;/head&gt;
    &lt;p&gt;As shown below, the results of the study demonstrated that users preferred the Wayfinding AI's approach across several important dimensions, despite its less-familiar two-column interface. Users favored Wayfinding AI for its helpfulness, relevance, ability to understand their goal, and for tailoring the conversation to their specific needs. These findings suggest that the proactive, question-asking behavior of Wayfinding AI successfully created a more personalized and helpful experience for users without introducing undue friction in the user experience.&lt;/p&gt;
    &lt;p&gt;Beyond simply preferring their conversations with the Wayfinding AI, participants had noticeably different conversations. Conversations were longer with the Wayfinding AI, in particular when participants were trying to understand the cause of their symptoms. For those topics, conversations with the Wayfinding AI had 4.96 turns on average, compared to 3.29 for the baseline AI. And the pattern of prompts they provided to each AI looked different across conversations:&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Finding the right health information online can feel like navigating a maze. While AI has the potential to be a powerful guide, our research shows that its success hinges on its ability to move beyond being a passive question-answerer and become an active conversational partner.&lt;/p&gt;
    &lt;p&gt;By designing our Wayfinding AI to be personal and proactive, we demonstrated how asking targeted questions in a well-structured interface can power an experience that users prefer over a more classical, question-answering experience, and thus enable people to obtain more helpful, relevant, and tailored information. The results from our user studies provide strong evidence that this human-centered, conversational approach is a promising direction for the future of AI in health, helping people navigate their health journeys.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;The research described here is joint work across Google Research, Google Health, and partnering teams. We would like to thank Yuexing Hao, Abbi Ward, Amy Wang, Beverly Freeman, Serena Zhan, Diego Ardila, Jimmy Li, I-Ching Lee, Anna Iurchenko, Siyi Kou, Kartikeya Badola, Jimmy Hu, Bhawesh Kumar, Keith Johnson, Supriya Vijay, Justin Krogue, Avinatan Hassidim, Yossi Matias, Dale Webster, Sunny Virmani, Yun Liu, Quang Duong, Fereshteh Mahvar, Laura Vardoulakis, Tiffany Guo, and Meredith Ringel Morris for contributing or reviewing this work. We would also like to thank the participants who contributed to these studies.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45387485</guid><pubDate>Fri, 26 Sep 2025 15:16:57 +0000</pubDate></item><item><title>SpaceX – Evolving the Multi-User Spaceport</title><link>https://www.spacex.com/updates#multiuser-spaceport</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45387494</guid><pubDate>Fri, 26 Sep 2025 15:17:31 +0000</pubDate></item><item><title>Flagship mobile phone with hardware kill switches for privacy</title><link>https://news.itsfoss.com/murena-powered-hiroh-phone/</link><description>&lt;doc fingerprint="bc3d383ed197baff"&gt;
  &lt;main&gt;
    &lt;p&gt;Murena has been making steady progress in the privacy-focused smartphone market with its Fairphone range of devices and their open source, de-Googled operating system, /e/OS.&lt;/p&gt;
    &lt;p&gt;Recently, we covered its launch of the Murena Fairphone 6 (Gen. 6), which combined their ethical hardware with their software ecosystem.&lt;/p&gt;
    &lt;p&gt;Now, they are back, but with a new collaboration. This time, Murena has teamed up with a company called HIROH.&lt;/p&gt;
    &lt;head rend="h2"&gt;📝 HIROH Phone (Powered by Murena): Key Specifications&lt;/head&gt;
    &lt;p&gt;The HIROH Phone comes with some standout privacy features, starting with two dedicated kill switches. One is a hardware switch that cuts circuit power to the cameras and microphones, ensuring they cannot be accessed by apps, the operating system, or hackers.&lt;/p&gt;
    &lt;p&gt;The other is a software switch that disables all wireless communications like 5G, Wi-Fi, Bluetooth, and NFC in one go. Together, these switches make unauthorized access tricky, to say the least.&lt;/p&gt;
    &lt;p&gt;Under the hood, the phone is powered by the MediaTek Dimensity 8300 SoC, paired with 16 GB of RAM and 512 GB of internal storage. These specs place it firmly in flagship territory, making it suitable for heavy multitasking, gaming, and media consumption without sacrificing on performance.&lt;/p&gt;
    &lt;p&gt;On the software side, the HIROH Phone runs /e/OS exclusively. This means no Google services, no hidden trackers, and a Google-free experience. Though, users can still install mainstream apps.&lt;/p&gt;
    &lt;p&gt;This device also packs in other niceties like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Display: 6.67″ AMOLED (protected by Gorilla Glass Victus)&lt;/item&gt;
      &lt;item&gt;Camera: 108 MP, 13 MP, a macro on the rear, and a 32 MP front.&lt;/item&gt;
      &lt;item&gt;Battery: 5,000 mAh Li-ion (33W fast charging)&lt;/item&gt;
      &lt;item&gt;Network Connectivity: 2G, 3G, 4G, and 5G (dual-sim).&lt;/item&gt;
      &lt;item&gt;Wi-Fi: Wi-Fi 6E (802.11ax)&lt;/item&gt;
      &lt;item&gt;Bluetooth: 5.3&lt;/item&gt;
      &lt;item&gt;USB: Type-C (audio, data transfer, and power)&lt;/item&gt;
      &lt;item&gt;NFC: Yes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Interestingly, there isn’t much information available on the HIROH brand right now. Many people have already asked about it in the /e/OS Community, and Manoj Nair from the Murena team has assured everyone that they will be posting a clarification soon.&lt;/p&gt;
    &lt;head rend="h2"&gt;🛒 Getting the HIROH Phone (Powered by Murena)&lt;/head&gt;
    &lt;p&gt;The HIROH Phone (Powered by Murena) is available for pre-sale. You can pay a €/$ 99 deposit to reserve your device and get a voucher to buy it later for €/$ 999, discounted from the retail price of €/$ 1,199.&lt;/p&gt;
    &lt;p&gt;Shipping is expected in January or February 2026. The phone will be available in all countries where Murena products are sold. The first 500 buyers have the chance to grab the Limited Edition Platinum Model, exclusive to Murena’s store.&lt;/p&gt;
    &lt;p&gt;Suggested Read 📖&lt;/p&gt;
    &lt;p&gt;- Even the biggest players in the Linux world don't care about desktop Linux users. We do.&lt;/p&gt;
    &lt;p&gt;- We don't put informational content behind paywall. Your support keeps it open for everyone. Think of it like 'pay it forward'.&lt;/p&gt;
    &lt;p&gt;- Don't like ads? With the Plus membership, you get an ad-free reading experience.&lt;/p&gt;
    &lt;p&gt;- When millions of AI-generated content is being published daily, you read and learn from real human Linux users.&lt;/p&gt;
    &lt;p&gt;- It costs just $2 a month, less than the cost of your favorite burger.&lt;/p&gt;
    &lt;p&gt;Become a Plus Member today and join over 300 people in supporting our work.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45387796</guid><pubDate>Fri, 26 Sep 2025 15:44:12 +0000</pubDate></item></channel></rss>