<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 29 Aug 2025 20:36:40 +0000</lastBuildDate><item><title>If you have a Claude account, they're going to train on your data moving forward</title><link>https://old.reddit.com/r/LocalLLaMA/comments/1n2ubjx/if_you_have_a_claude_personal_account_they_are/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45062738</guid></item><item><title>Meta might be secretly scanning your phone's camera roll</title><link>https://www.zdnet.com/article/meta-might-be-secretly-scanning-your-phones-camera-roll-how-to-check-and-turn-it-off/</link><description>&lt;doc fingerprint="b4b2db1208265aff"&gt;
  &lt;main&gt;
    &lt;p&gt;'ZDNET Recommends': What exactly does it mean?&lt;/p&gt;
    &lt;p&gt;ZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we’re assessing.&lt;/p&gt;
    &lt;p&gt;When you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.&lt;/p&gt;
    &lt;p&gt;ZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.&lt;/p&gt;
    &lt;head rend="h1"&gt;Meta might be secretly scanning your phone's camera roll - how to check and turn it off&lt;/head&gt;
    &lt;p&gt;Follow ZDNET: Add us as a preferred source on Google.&lt;/p&gt;
    &lt;head rend="h3"&gt;ZDNET's key takeaways&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Meta could be scanning your camera roll right now.&lt;/item&gt;
      &lt;item&gt;It's using your photos to provide AI-powered suggestions.&lt;/item&gt;
      &lt;item&gt;Check Facebook settings to turn off the features.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Meta could be analyzing and retaining your phone's photos without your explicit consent.&lt;/p&gt;
    &lt;p&gt;Some Facebook users have noticed that, deep within their app settings, Meta has switched on two toggles that allow it to access their camera roll to offer AI-powered suggestions, including "personalized creative ideas, like travel highlights and collages."&lt;/p&gt;
    &lt;p&gt;Also: How to delete Facebook, Messenger, or Instagram - if you want Meta out of your life&lt;/p&gt;
    &lt;p&gt;The problem? The toggles for the AI suggestion features, called "camera roll sharing suggestions," appear to be turned on for users who claim they haven't seen a pop-up from Facebook asking for permission to enable them. If you get that "cloud processing" pop-up and tap "Allow" on it, you'll agree to Meta's AI Terms of Service and permit your "media and facial features" to be analyzed by AI.&lt;/p&gt;
    &lt;p&gt;Facebook will then use your camera roll images -- including the dates on them and the presence of people or objects -- to suggest collages, themed albums, recap posts, or AI restyled versions of your pictures. These AI suggestions are only visible to you, unless you choose to share them, and Meta says the media won't be used for ad targeting.&lt;/p&gt;
    &lt;p&gt;Also: How to protect your privacy from Facebook - and what doesn't work&lt;/p&gt;
    &lt;p&gt;But, to be clear, you're still giving Meta the right to access and retain your camera roll images, and that could raise serious privacy concerns, especially for users who never knowingly opted in.&lt;/p&gt;
    &lt;p&gt;ZDNET's editorial director found Meta's camera roll sharing suggestions enabled in her Facebook app without her knowledge. I also noticed they were enabled for me, although I vaguely recall seeing a pop-up from Facebook about the new features a few weeks ago. I think I dismissed it quickly, and I can't remember whether I tapped Allow or Don't allow on it.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to stop Facebook from scanning your camera roll&lt;/head&gt;
    &lt;p&gt;Meta said its camera roll sharing suggestions are not enabled by default. If you're worried you dismissed Facebook's pop-up, unknowingly opted-in, and gave access to your camera roll, here's how to check and turn it off.&lt;/p&gt;
    &lt;head rend="h2"&gt;1. Open the Facebook app&lt;/head&gt;
    &lt;p&gt;The settings you'll want to check can be found in the Facebook mobile app.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Grab your iPhone or Android phone.&lt;/item&gt;
      &lt;item&gt;Open the Facebook app. You'll need to be signed into your account.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;2. Go to the Menu &amp;gt; Settings and Privacy&lt;/head&gt;
    &lt;p&gt;Facebook hides most of its settings in the menu -- the three-line hamburger icon in the bottom corner of the app.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Tap the Menu icon in the bottom right corner of the screen.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Once the menu opens, look for Settings and Privacy with a gear icon. This will take you directly to Settings.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;3. Select Settings&lt;/head&gt;
    &lt;p&gt;Once you find and tap Privacy and Settings to expand the dropdown options, tap Settings again.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Under Settings and Privacy, tap Settings.&lt;/item&gt;
      &lt;item&gt;Now, scroll down and look for "Camera roll sharing suggestions."&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;4. Go to Camera roll sharing suggestions&lt;/head&gt;
    &lt;p&gt;Meta placed the toggles that grant it access to your camera roll under the "Camera roll sharing suggestions" setting. You'll need to go there to see if they're on and, if so, switch them off.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Look for the option labeled "Camera roll sharing suggestions" and tap it.&lt;/item&gt;
      &lt;item&gt;This will open a preference page with a couple of toggles.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;5. Turn off both toggles&lt;/head&gt;
    &lt;p&gt;Once you're inside the camera roll sharing suggestions page, notice the two separate switches. If they're blue and the toggle circle is pushed to the right, they're on -- meaning Meta is already processing and retaining your phone's photos. Turn them off so the app can't automatically upload and analyze your camera roll.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find the option labeled "Get camera roll suggestions when you're browsing Facebook." If the switch is on (blue), tap it once to turn it off (gray). This will stop Facebook from using basic camera roll data, such as which videos you've favorited and when photos were taken, to suggest sharing media you haven't yet uploaded.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Find the option labeled "Get creative ideas made for you by allowing camera roll cloud processing." If the switch is on (blue), tap it once to turn it off (gray). This will stop Facebook from continuously uploading media from your camera roll -- and using details like time, location, themes, and the presence of people or objects -- to generate personalized creative ideas such as recaps and AI restylings.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;FAQs&lt;/head&gt;
    &lt;head rend="h3"&gt;Why is Facebook cloud-processing my device's camera roll?&lt;/head&gt;
    &lt;p&gt;Meta is uploading and analyzing your camera roll photos and videos, even ones you haven't posted, in its cloud in order to generate AI-powered suggestions like collages, monthly recaps, themed albums, or AI-restyled versions of your images.&lt;/p&gt;
    &lt;head rend="h3"&gt;Where is this feature being tested?&lt;/head&gt;
    &lt;p&gt;Meta has confirmed the feature is a test, saying, "We're exploring ways to make content sharing easier for people on Facebook by testing suggestions of ready-to-share and curated content from a person's camera roll."&lt;/p&gt;
    &lt;p&gt;The test is currently available in the US and Canada, but it's not available in Illinois or Texas due to those states' privacy laws.&lt;/p&gt;
    &lt;head rend="h3"&gt;Did Facebook ask for my consent before turning this on?&lt;/head&gt;
    &lt;p&gt;Meta is showing a pop-up asking users if they want to enable cloud processing, but some users claim they haven't seen it. Instead, they found the toggles in their settings already switched on by default, raising questions about whether clear consent was given.&lt;/p&gt;
    &lt;head rend="h3"&gt;Can I remove my photos once they've been uploaded?&lt;/head&gt;
    &lt;p&gt;ZDNET's sister site, CNET, reports that Meta pulls from your newer pictures (roughly the last 30 days) and if you disable the feature, your uploaded photos will be deleted after 30 days. The only way to confirm is by downloading your Facebook account data.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why is this a potential privacy issue?&lt;/head&gt;
    &lt;p&gt;It expands Meta's reach beyond the content you've chosen to upload and share online -- into your private, unposted photos and videos. For many, that's a major red flag and a line they're not comfortable crossing, understandably so.&lt;/p&gt;
    &lt;p&gt;Also: What Zuckerberg's 'personal superintelligence' sales pitch leaves out&lt;/p&gt;
    &lt;p&gt;Even if Meta is asking for consent to access your camera roll in order to analyze your phone's photos and provide AI-powered suggestions, the company could have done a better job of being clear and explicit about what it's trying to do.&lt;/p&gt;
    &lt;p&gt;How many users, like me, simply dismissed the consent pop-up without fully realizing what they'd just agreed to?&lt;/p&gt;
    &lt;p&gt;Editor's note: This article was updated on Aug. 24, 2025 to clarify that Meta's camera roll sharing suggestions are not turned on by default and are entirely opt-in. Still, some users say they never knowingly agreed and are finding the features enabled in their settings.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45062910</guid></item><item><title>Sig Sauer citing national security to keep documents from public</title><link>https://practicalshootinginsights.com/eighth-circuit-fmeca-update/</link><description>&lt;doc fingerprint="df1fc178385f5482"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;In the Eighth Circuit, the Fight Over Sig Sauer’s P320 FMECA Goes Public&lt;/head&gt;
    &lt;p&gt;The secrecy battle over the Army’s Failure Modes, Effects, and Criticality Analysis (FMECA) for Sig Sauer’s P320 has followed Glasscock v. Sig Sauer to the Eighth Circuit. A media intervenor is now asking the appellate court to keep key records open—and their brief places Practical Shooting Insights (this site) squarely in the middle of the story.&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s new&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Trace intervenes in the appeal. The newsroom moved to intervene for the limited purpose of opposing sealed filings tied to class certification and the FMECA, arguing the public’s right of access and noting the district court cited the FMECA nine times when it certified the class.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sig Sauer says “national security” and asks for deference to the Army. In opposing intervention, Sig Sauer urges the court to leave FMECA-related material sealed and to give the Army time to weigh in, framing the dispute in terms of protecting “military secrets.”&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A second FMECA document emerges. Sig Sauer’s opposition confirms there are two FMECA records in the class-certification exhibits: a FMECA Spreadsheet and a FMECA Memorandum—the latter not previously described in public filings—raising fresh questions about what the memo contains and who authored it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PSI’s reporting is part of the record. The Trace’s filing tells the court the unredacted FMECA was found on CourtListener, de‑obscured, and published on Practical Shooting Insights, where it “remains available”—and it recounts Sig Sauer’s own executive discussing it on a podcast while pointing viewers to this website.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The FMECA document was previously published here.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Trace’s pitch: This isn’t secret anymore&lt;/head&gt;
    &lt;p&gt;The Trace walks the appellate court through how the FMECA left the bottle: it was posted on this website and then widely republished; a YouTube explainer discussing it surpassed 100,000 views. The filing quotes Sig Sauer’s VP of Consumer Affairs Phil Strader being asked on the Behind the Lens podcast why the FMECA shouldn’t be public and responding, “No, there’s not” (nothing to hide), while directing viewers to this website to see the document and describing its contents.&lt;/p&gt;
    &lt;p&gt;The reporting regarding Phil Strader’s interview was previously published here&lt;/p&gt;
    &lt;p&gt;How many times has the unredacted FMECA been “shared”? The filings don’t give a precise share count. What they do document is widespread republication and discussion, including the 100k‑plus video and multiple re‑posts mirroring the PSI copy. In other words, the genie is out of the bottle.&lt;/p&gt;
    &lt;p&gt;The Trace also points to DoD Instruction 5230.24, the policy Sig Sauer invokes, noting it does not authorize withholding unclassified information about evaluations of performance and reliability of military equipment—and that the PSI‑hosted FMECA bears no DoD distribution markings.&lt;/p&gt;
    &lt;head rend="h2"&gt;Sig Sauer’s response: Let the Army decide—and keep the lid on&lt;/head&gt;
    &lt;p&gt;Sig Sauer tells the Eighth Circuit The Trace lacks standing and that parallel briefing is already underway in the district court. Substantively, Sig Sauer leans on military‑secrets concerns, requests time for the Army to opine on release, and characterizes the FMECA as controlled technical information created under the MHS contract. (The company also recounts how the spreadsheet briefly became public in another case before being pulled down.)&lt;/p&gt;
    &lt;p&gt;Two details in Sig Sauer’s papers matter going forward:&lt;/p&gt;
    &lt;p&gt;1) The “FMECA Memorandum.” Sig Sauer identifies the memo alongside the previously published spreadsheet. If the memo is Sig Sauer‑authored, it could reveal how the company framed the Army analysis internally—an issue directly relevant to notice, risk mitigation, and marketing claims.&lt;/p&gt;
    &lt;p&gt;2) Ongoing Army communications. Sig Sauer’s litigation counsel filed a declaration stating he asked the Army about the FMECA’s distribution status and that key Army decision‑makers were unavailable the week of the deadline; Sig Sauer says the Army may submit information and seeks additional time.&lt;/p&gt;
    &lt;head rend="h2"&gt;The transparency question, distilled&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Is the FMECA “national‑security” material? The Trace says no—and points to DoDI 5230.24’s carve‑out: it does not provide authority to withhold unclassified evaluations of performance and reliability—exactly what a FMECA is. It also underscores the lack of any DoD marking on the PSI copy.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Is secrecy even possible at this point? The record shows the unredacted spreadsheet is online on this website, has been reposted broadly, and has been discussed by Sig Sauer’s own executive on air—who told listeners where to find it. One video discussing it has 100,000+ views.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Why this matters to the class action—and to owners&lt;/head&gt;
    &lt;p&gt;The district court relied on the FMECA repeatedly when certifying the Missouri class, including on notice and risk‑mitigation questions—the very issues consumers care about. Keeping the heart of that analysis under seal on appeal would blunt the public’s ability to scrutinize a product‑safety fight with real‑world consequences.&lt;/p&gt;
    &lt;head rend="h2"&gt;My role, plainly&lt;/head&gt;
    &lt;p&gt;Practical Shooting Insights is an independent site covering the shooting‑sports and firearms industry. The Trace’s filing names PSI as the first publisher of the unredacted spreadsheet and quotes Strader pointing viewers here. I will continue to publish filings and analysis so readers can compare the arguments to the documents themselves.&lt;/p&gt;
    &lt;head rend="h2"&gt;What to watch next&lt;/head&gt;
    &lt;p&gt;1) Whether the Eighth Circuit permits intervention and applies the strong presumption of public access to class‑certification records.&lt;lb/&gt; 2) If the Army weighs in—and on what basis—regarding the FMECA’s distribution status.&lt;lb/&gt; 3) Disclosure of the FMECA Memorandum. If it’s Sig Sauer-authored, it could illuminate internal framing of hazards and fixes—material at the core of consumer‑protection claims.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45063431</guid></item><item><title>Grok Code Fast 1</title><link>https://x.ai/news/grok-code-fast-1</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45063559</guid></item><item><title>Show HN: Sosumi.ai – Convert Apple Developer docs to AI-readable Markdown</title><link>https://sosumi.ai/</link><description>&lt;doc fingerprint="b0cfd92d22047de1"&gt;
  &lt;main&gt;
    &lt;p&gt;Ever notice Claude struggling to write Swift code? It might not be their fault!&lt;/p&gt;
    &lt;p&gt; Apple Developer docs are locked behind JavaScript, making them invisible to most LLMs. If they try to fetch it, all they see is &lt;quote&gt;This page requires JavaScript. Please turn on JavaScript in your browser and refresh the page to view its content.&lt;/quote&gt;&lt;/p&gt;
    &lt;p&gt;This service translates Apple Developer documentation pages into AI-friendly Markdown.&lt;/p&gt;
    &lt;head rend="h2"&gt;HTTP Usage&lt;/head&gt;
    &lt;p&gt;Replace &lt;code&gt;developer.apple.com&lt;/code&gt; with &lt;code&gt;sosumi.ai&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Original&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;code&gt;https://developer.apple.com/documentation/swift/array&lt;/code&gt;
      &lt;/item&gt;
      &lt;item rend="dt-2"&gt;AI-readable&lt;/item&gt;
      &lt;item rend="dd-2"&gt;
        &lt;code&gt;https://sosumi.ai/documentation/swift/array&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Examples&lt;/head&gt;
    &lt;head rend="h2"&gt;MCP Usage&lt;/head&gt;
    &lt;p&gt; Connect your MCP client to &lt;code&gt;https://sosumi.ai/mcp&lt;/code&gt;:
                &lt;/p&gt;
    &lt;code&gt;{
  "mcpServers": {
    "sosumi": {
      "command": "npx",
      "args": [
        "-y",
        "mcp-remote",
        "https://sosumi.ai/mcp"
      ]
    }  }
}&lt;/code&gt;
    &lt;head rend="h3"&gt;Available Resources&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;list rend="dl"&gt;
          &lt;item rend="dt-1"&gt;
            &lt;code&gt;doc://{path}&lt;/code&gt;
          &lt;/item&gt;
          &lt;item rend="dd-1"&gt;Get Apple Developer documentation as markdown &lt;lb/&gt;Example:&lt;code&gt;doc://swift/array&lt;/code&gt;returns Swift Array documentation&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Available Tools&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;list rend="dl"&gt;
          &lt;item rend="dt-1"&gt;
            &lt;code&gt;search&lt;/code&gt;
          &lt;/item&gt;
          &lt;item rend="dd-1"&gt;Search Apple Developer documentation &lt;lb/&gt;Parameters:&lt;code&gt;query&lt;/code&gt;(string)&lt;lb/&gt;Returns structured results with titles, URLs, descriptions, breadcrumbs, and tags&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45063874</guid></item><item><title>Show HN: Find Hidden Gems on HN</title><link>https://pj4533.com/hn-overlooked/</link><description>&lt;doc fingerprint="b0e9d773c204af23"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;This tool helps you discover recent hidden gems on Hacker News – high-effort posts that haven't gotten much attention.&lt;/p&gt;
      &lt;head rend="h3"&gt;Why "Recent"?&lt;/head&gt;
      &lt;p&gt;We search the HN API's Ask, Show, and New story feeds, which typically contain posts from the last 3-7 days. This ensures fresh content while keeping the search fast.&lt;/p&gt;
      &lt;head rend="h3"&gt;Passion Score&lt;/head&gt;
      &lt;p&gt;Posts are ranked by their Passion Score, which identifies high-effort, low-engagement content:&lt;/p&gt;
      &lt;p&gt; Passion Score = (Text Length Score) / (Engagement + 1) &lt;lb/&gt; Where: &lt;lb/&gt;• Text Length Score = min(text_length / 500, 10) &lt;lb/&gt;• Engagement = votes + (comments × 2) &lt;/p&gt;
      &lt;p&gt;Higher scores indicate more "overlooked" posts – substantial writing with minimal recognition. Perfect for finding thoughtful contributions that the community may have missed.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45064210</guid></item><item><title>Flunking my Anthropic interview again</title><link>https://taylor.town/flunking-anthropic</link><description>&lt;doc fingerprint="3275139e35550156"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Curious Case of Flunking My Anthropic Interview (Again)&lt;/head&gt;
    &lt;p&gt;Here's a vague overview of what just happened:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I recently applied for Anthropic's Developer Relations role.&lt;/item&gt;
      &lt;item&gt;My friend who works there gave me a glowing recommendation (thanks again, dude!).&lt;/item&gt;
      &lt;item&gt;I completed their secret take-home assignment.&lt;/item&gt;
      &lt;item&gt;On top of their secret take-home assignment, I independently published diggit.dev and a companion blogpost about my [sincerely] positive experiences with Claude. I was hoping that some unsolicited "extra credit" would make me look like an exceptional/ambitious candidate.&lt;/item&gt;
      &lt;item&gt;I posted diggit.dev to HackerNews and it hit the frontpage!&lt;/item&gt;
      &lt;item&gt;I submitted my take-home assignment and my unsolicited extra credit.&lt;/item&gt;
      &lt;item&gt;They sent me the "unfortunately" email.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Anthropic obviously didn't do anything wrong. I'm just bummed.&lt;/p&gt;
    &lt;p&gt;Claude Code truly is one of my favorite dev tools ever, and if you've suffered through my talks/interviews, you're probably sick of my enthusiasm for software. I was particularly excited to interview with Anthropic because I respect their approach to responsible AI adoption. This very blog is too often a crazed celebration of humans, of software, of AI, of progress, of sincerity -- I, I felt like I was a perfect fit.&lt;/p&gt;
    &lt;p&gt;The first time I flunked an Anthropic interview (ca. 2022), I accidentally clicked a wrong button during their automated coding challenge. It was easy to swallow that failure. I made an honest mistake; I expect companies to reject candidates who make honest mistakes during interviews.&lt;/p&gt;
    &lt;p&gt;This is different. I didn't misclick any buttons. My best wasn't good enough. I'm not good enough.&lt;/p&gt;
    &lt;p&gt;This essay started as a fantasy: some hero at Anthropic reads this on HackerNews and vouches for me and I get the job and I help them guide humanity toward post-scarity AI abundance, forever and ever, amen. I'm ashamed of these thoughts. It's the same folly of explaining to an ex-girlfriend why she's wrong about her own experience.&lt;/p&gt;
    &lt;p&gt;Dating was difficult for me. I don't mind feeling ugly or low-status or whatever -- I know my place. But it hurts to feel seen, feel considered, but ultimately rejected due to mysterious forces: "He's cute, but he's too weird."&lt;/p&gt;
    &lt;p&gt;Yes, I'm weird. My eccentric habits have been an overall boon for my career, for my relationships, for my well-being. But it's moments like these when I just want to turn all my weird off. I want to be a square peg for this square hole and do honest work and feed my family and help humanity thrive.&lt;/p&gt;
    &lt;p&gt;I can't turn my weird off, so I think I defensively dial it up sometimes. I exaggerate my eccentricities. It's easy to swallow criticism when it isn't the real me, when it isn't my best, when it's honest mistakes -- what a load of crap. This is me. This is my best. Hello, world.&lt;/p&gt;
    &lt;p&gt;Now it's all coming back in waves, in gasps -- I spent so much of my life being an unlikable jerk. Becoming somebody else has been slow/painful and I'm so deeply afraid of regressing. Over the past decade, I've been striving to spread joy, to do good, to be better. I'm trying so hard.&lt;/p&gt;
    &lt;p&gt;And all this keyboard vomit is a promise to myself that I'm not giving up. I hate this feeling, and I'm staring these nightmares straight in their stupid eyeballs, and they're not blinking. I am still alive, and I have so much more to do.&lt;/p&gt;
    &lt;p&gt;I'm okay. I mean it. I don't need (or deserve) your sympathy. I'm so lucky to be alive, at this time, at this place, in this body, with these people. My life is great, and it will get even better if I keep putting in this effort.&lt;/p&gt;
    &lt;p&gt;Spewing my insides like this onto The Internet is terrifying, but I suspect many strangers are facing similar feelings. It's rough out there. Whatever it is, wherever you are, I hope this helps. You've got this. You're not alone, and we're only human.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45064284</guid></item><item><title>Deploying DeepSeek on 96 H100 GPUs</title><link>https://lmsys.org/blog/2025-05-05-large-scale-ep/</link><description>&lt;doc fingerprint="ec5d845b04b8c994"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Deploying DeepSeek with PD Disaggregation and Large-Scale Expert Parallelism on 96 H100 GPUs&lt;/head&gt;
    &lt;p&gt;by: The SGLang Team, May 05, 2025&lt;/p&gt;
    &lt;p&gt;DeepSeek is a popular open-source large language model (LLM) praised for its strong performance. However, its large size and unique architecture, which uses Multi-head Latent Attention (MLA) and Mixture of Experts (MoE), require an advanced system for efficient serving at scale. In this blog, we explain how we match DeepSeek's inference system performance with SGLang.&lt;/p&gt;
    &lt;p&gt;Our implementation, shown in the figure above, runs on 12 nodes in the Atlas Cloud, each equipped with 8 H100 GPUs. It uses prefill-decode disaggregation and large-scale expert parallelism (EP), achieving a speed of 52.3k input tokens per second and 22.3k output tokens per second per node for 2000-token input sequences. To the best of our knowledge, this represents the first open-source implementation to nearly match the throughput reported in the official DeepSeek blog at large scale. By deploying this implementation locally, it translates to a cost of $0.20/1M output tokens, which is about one-fifth the cost of the official DeepSeek Chat API. Compared to vanilla tensor parallelism using the same resources, this optimized strategy improves the output throuhgput by up to 5x. This blog dives into our parallelism design, optimization methods, and results. All components of our work are fully open-source, allowing others to explore and build on our efforts. The instructions for reproducing our experiments are fully available here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Highlight&lt;/head&gt;
    &lt;p&gt;✅ SGLang now supports prefill-decode (PD) disaggregation and large-scale EP, including the full functionality of DeepEP, DeepGEMM, and EPLB.&lt;/p&gt;
    &lt;p&gt;✅ Leveraging these new features, our team successfully replicated DeepSeek's inference system using 12 nodes, each with 8 H100 GPUs. In total, SGLang achieves a throughput of 52.3k input tokens per second and 22.3k output tokens per second per node for input sequences of 2000 tokens.&lt;/p&gt;
    &lt;p&gt;✅ This blog explains technical details of our approach, focusing on optimizations for efficiency, peak memory usage reduction, and workload balancing. The profile results show that our implementation achieves nearly on-par performance with the official DeepSeek’s report.&lt;/p&gt;
    &lt;p&gt;✅ All experiments and code are fully open-sourced for community access and further development.&lt;/p&gt;
    &lt;head rend="h2"&gt;Outline&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Parallelism Design&lt;/item&gt;
      &lt;item&gt;Prefill and Decode Disaggregation&lt;/item&gt;
      &lt;item&gt;Large-scale Expert Parallelism&lt;/item&gt;
      &lt;item&gt;Evaluation&lt;/item&gt;
      &lt;item&gt;Toolkits&lt;/item&gt;
      &lt;item&gt;Limitations and Future Work&lt;/item&gt;
      &lt;item&gt;Conclusion&lt;/item&gt;
      &lt;item&gt;Acknowledgment&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Parallelism Design&lt;/head&gt;
    &lt;p&gt;Efficient parallelism is essential to manage the computational complexity and memory demands of DeepSeek's architecture. This section outlines our approach to optimizing key components: attention layers, dense feed-forward networks (FFNs), sparse FFNs, and the language model (LM) head. Each component leverages tailored parallelism strategies to enhance scalability, memory efficiency, and performance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Attention Layers&lt;/head&gt;
    &lt;p&gt;DeepSeek employs Multi-head Latent Attention (MLA) to effectively model complex dependencies within input sequences. To optimize this mechanism, we implement DP Attention, a data parallelism strategy that eliminates KV cache duplication across devices, significantly reducing memory overhead. Introduced in SGLang v0.4, this approach has been extended to support hybrid data and tensor parallelism, offering flexibility for processing small batch sizes efficiently.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dense FFNs&lt;/head&gt;
    &lt;p&gt;Despite using only three dense FFN layers, DeepSeek-V3's computation can significantly increase peak memory usage, potentially leading to system crashes if not carefully managed. To address this, we adopt Data Parallelism (DP) over tensor parallelism (TP), leveraging the following advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enhanced Scalability: With an intermediate dimension of 18,432, high TP degrees (e.g., TP32) result in inefficient fragmentation into small-unit segments (e.g., 576 units), which are not divisible by 128—a common alignment boundary for modern GPUs such as H100. This misalignment hampers computational efficiency and memory utilization. DP provides a more scalable solution by avoiding fragmentation, ensuring balanced workload distribution across devices.&lt;/item&gt;
      &lt;item&gt;Optimized Memory Efficiency: Traditionally, TP reduces memory usage as worker size increases, but this advantage diminishes under DP attention. In a pure TP setup, memory demand for a single-layer Transformer model scales with DP size as: $$\text{Memory}=\frac{N_{\text{param}}}{\text{TP}}+(1+k)N_{\text{hidden_state}}\cdot \text{DP}\notag$$ Here, $N_{\text{hidden_state}}=n_\text{token}\times n_\text{hidden_size}$ is the size of the hidden state on each device (DP rank), $N_{\text{param}}=n_\text{intermediate_size}\times n_\text{hidden_size}$ is the number of model parameters, and $k$ is a coefficient representing extra memory overhead from CUDA Graph duplication. By assuming $\text{DP}=\text{TP}$, this memory usage function is minimized when $\text{TP}=\sqrt{\frac{N_{\text{param}}}{(1+k)N_{\text{hidden_state}}}}$. DeepSeek-V3 uses an intermediate size of 18,432. During the prefill phase, CUDA Graph is typically disabled, so $k = 0$. However, the token size per device can easily exceed 2,048, resulting in an optimal TP size of 3 or less. In the decode phase, a practical configuration might use 128 tokens per device and set $k = 3$. In this case, the memory-optimal TP size is 6. In both phases, a lower TP degree minimizes memory usage per device. As a result, DP may offer a more memory-efficient approach for scaling compared to relying solely on TP.&lt;/item&gt;
      &lt;item&gt;Minimized Communication Overhead: In pure TP, each FFN necessitates two all-reduce operations, resulting in substantial communication overhead. By leveraging DP, we optimize this process to a single reduce-scatter following the prior attention layer and an all-gather before the next, reducing communication costs by 50%. Furthermore, when attention is also computed under pure DP, inter-device communication is entirely eliminated, significantly enhancing overall efficiency.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The integration of DP dense FFN with DP attention is illustrated in the left figure below. Users can enable this feature by setting &lt;code&gt;--moe-dense-tp-size=1&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sparse FFNs&lt;/head&gt;
    &lt;p&gt;In DeepSeek-V3's Mixture of Experts (MoE) architecture, sparse FFNs require substantial expert weights, creating a significant memory bottleneck. To address this, we implement Expert Parallelism (EP), which distributes expert weights across multiple devices. This approach effectively scales memory capacity while maintaining high performance, though it does introduce challenges like irregular all-to-all communication and workload imbalance.&lt;/p&gt;
    &lt;p&gt;The figure in the right figure above illustrates our EP implementation using the DeepEP framework, with further details on our EP design and optimizations provided in the following sections.&lt;/p&gt;
    &lt;head rend="h3"&gt;LM Head&lt;/head&gt;
    &lt;p&gt;The LM head computes output probabilities over a large vocabulary, a resource-intensive operation traditionally handled with vocabulary parallelism to aggregate token logits from TP groups. To enhance scalability and efficiency, we adopt Data Parallelism (DP), mirroring our dense FFN strategy. This reduces memory overhead and simplifies communication across devices, delivering a more streamlined solution.&lt;/p&gt;
    &lt;head rend="h2"&gt;Prefill and Decode Disaggregation&lt;/head&gt;
    &lt;p&gt;LLM inference comprises two distinct phases: Prefill and Decode. The Prefill phase is computation-intensive, processing the entire input sequence, while the Decode phase is memory-intensive, managing the Key-Value (KV) cache for token generation. Traditionally, these phases are handled within a unified engine, where combined scheduling of prefill and decode batches introduces inefficiencies. To address these challenges, we introduce Prefill and Decode (PD) Disaggregation in SGLang.&lt;/p&gt;
    &lt;head rend="h3"&gt;Issues with Unified Scheduling&lt;/head&gt;
    &lt;p&gt;The conventional unified engine, which processes prefill and decode batches together, results in three significant problems:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Prefill Interruption: Incoming prefill batches frequently interrupt ongoing decode batches, causing substantial delays in token generation.&lt;/item&gt;
      &lt;item&gt;DP Attention Imbalance: In DP attention, one DP worker may process a prefill batch while another handles a decode batch simultaneously, leading to increased decode latency.&lt;/item&gt;
      &lt;item&gt;Incompatible with DeepEP: As we will discuss in a later section, DeepEP executes different dispatch modes for prefill and decode, making unified scheduling imcompatible with DeepEP.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;PD Disaggregation resolves these by separating the two stages, enabling tailored optimizations for each.&lt;/p&gt;
    &lt;head rend="h3"&gt;Implementation Details&lt;/head&gt;
    &lt;p&gt;The PD Disaggregation design in SGLang, depicted in the diagram below, interleaves execution between a Prefill Server and a Decode Server:&lt;/p&gt;
    &lt;p&gt;Upon receiving an input request, the workflow proceeds as follows:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A Prefill Server and a Decode Server pair via a handshake, establishing a local sender and receiver, respectively.&lt;/item&gt;
      &lt;item&gt;The Decode Server pre-allocates the KV cache, signaling the Prefill Server to begin the model forward pass and compute the KV caches.&lt;/item&gt;
      &lt;item&gt;Once computed, the data transfers to the Decode Server, which handles iterative token generation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This separation ensures each phase operates under optimal conditions, maximizing GPU resource utilization. To further enhance performance, our implementation incorporates:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Non-blocking Transfer: Data send and receive operations run in a background thread, keeping the scheduler’s event loop uninterrupted.&lt;/item&gt;
      &lt;item&gt;RDMA-Based Transfer: Remote Direct Memory Access (RDMA) leverages queue pairs for connections and scatter-gather elements (SGE) for efficient transfer of non-contiguous memory chunks.&lt;/item&gt;
      &lt;item&gt;Flexible API Integration: SGLang offers adaptable APIs that integrate high-performance RDMA libraries like Mooncake and NIXL, streamlining data transfers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More details can be found in our design document.&lt;/p&gt;
    &lt;head rend="h2"&gt;Large-scale Expert Parallelism&lt;/head&gt;
    &lt;head rend="h3"&gt;Expert Parallelism with DeepEP&lt;/head&gt;
    &lt;p&gt;DeepEP, implemented by the DeepSeek team, is a communication library designed to streamline EP in MoE models. It tackles the challenge of efficiently routing tokens to specific experts across multiple GPUs. By providing optimized communication kernels, DeepEP reduces latency and boosts throughput, making it ideal for large-scale inference tasks.&lt;/p&gt;
    &lt;p&gt;DeepEP provides two specialized dispatch modes to address varying workload demands:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Normal Dispatch: Optimized for handling long input sequences, such as during the prefill phase, this mode prioritizes maximum computational throughput. However, it generates symbolic shapes that are incompatible with CUDA Graph, rendering it less effective for the decode phase, where kernel launch overhead becomes a significant bottleneck.&lt;/item&gt;
      &lt;item&gt;Low-Latency Dispatch: Tailored for generating output tokens during the decode phase, this mode prioritizes minimal delay to ensure real-time performance. It supports CUDA Graph but requires preallocating a fixed memory size. If the memory demand exceeds this preallocation, a runtime error occurs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In SGLang, the integration of DeepEP provides auto mode that dynamically selects between these two dispatch modes based on the workload. However, without PD disaggregation, the auto mode faces a limitation: it cannot simultaneously support both normal dispatch (for prefill) and low-latency dispatch (for decode) within the same communication group. This restriction hinders its compatibility with DP attention, which is crucial for memory-efficient inference. The compatibility of each mode is outlined in the table below:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Long Input&lt;/cell&gt;
        &lt;cell role="head"&gt;Long Output&lt;/cell&gt;
        &lt;cell role="head"&gt;DP Attention&lt;/cell&gt;
        &lt;cell role="head"&gt;CUDA Graph&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Normal&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Low-Latency&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Auto&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;PD disaggregation addresses this by separating prefill and decode phases, allowing normal dispatch for the prefill phase and low-latency dispatch for the decode phase, both under DP attention. This integration optimizes resource utilization and enhances overall performance by aligning the dispatch mode with the specific needs of each phase.&lt;/p&gt;
    &lt;head rend="h3"&gt;DeepGEMM Integration&lt;/head&gt;
    &lt;p&gt;DeepGEMM is another high-efficient library developed by the DeepSeek team, specifically designed to optimize computations in MoE models. It provides two specialized functions for handling MoE-related matrix multiplications (Grouped GEMMs), each tailored to different phases of the inference process.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Grouped GEMMs (contiguous layout): This kernel is designed for dynamic input shapes, making it ideal for the prefill phase of MoE inference. It processes inputs where the data for different experts is concatenated contiguously, allowing for flexible handling of varying input sizes.&lt;/item&gt;
      &lt;item&gt;Grouped GEMMs (masked layout): This kernel assumes a fixed input shape and uses a mask tensor to compute only the valid portions of the input. It is compatible with CUDA Graph, which optimizes kernel launches, making it well-suited for the decode phase where reducing overhead is critical.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;DeepGEMM integrates smoothly with the dispatch modes of DeepEP:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For the contiguous layout kernel, which is used with normal dispatch in the prefill phase, an additional step is required. Since normal dispatch outputs a symbolic shape, a permutation is needed to transform the output into the contiguous format expected by the kernel. We referred to the LightLLM project and implemented a custom Triton kernel for efficient permutation. This kernel ensures that the output from normal dispatch is correctly rearranged, enabling smooth integration with the contiguous GEMM kernel.&lt;/item&gt;
      &lt;item&gt;The masked layout kernel pairs seamlessly with DeepEP’s low-latency dispatch, as both are optimized for the decode phase and support CUDA Graph.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SGLang also integrates DeepGEMM for MoE computation under tensor parallelism. Additionally, DeepGEMM provides a highly efficient general GeMM kernel, which can be activated in SGLang by setting the environment variable &lt;code&gt;SGL_ENABLE_JIT_DEEPGEMM&lt;/code&gt; to 1, offering even greater computational efficiency for non-MoE operations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Two-batch Overlap&lt;/head&gt;
    &lt;p&gt;In multi-node environments, limited communication bandwidth can significantly increase overall latency. To tackle this challenge, we implemented Two-batch Overlap (TBO) following DeepSeek's system design. TBO splits a single batch into two micro-batches, allowing computation and communication to overlap, which also lowers peak memory usage by halving the effective batch size. However, putting TBO into practice introduces specific implementation difficulties.&lt;/p&gt;
    &lt;head rend="h5"&gt;Implementation Challenges&lt;/head&gt;
    &lt;p&gt;Although DeepSeek released the design framework of TBO, there are two slight implementation challenges.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Code Complexity: Directly coding TBO can lead to duplicated logic for managing multiple micro-batches. This increases the complexity of the codebase, making it harder to maintain and prone to errors, especially as the number of micro-batches or overlapping scenarios grows.&lt;/item&gt;
      &lt;item&gt;Synchronization Issues in the Prefill Phase: Achieving effective overlap between computation and communication needs consideration when the normal dispatch in DeepEP block the CPU. This blocking behavior can stall the pipeline, leaving the GPU idle and undermining the performance benefits of TBO.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Abstraction for Clean Implementation&lt;/head&gt;
    &lt;p&gt;To create a more maintainable and reusable codebase, we use an abstraction layer consisting of operations and yield points. This method simplifies development by allowing us to write code as if handling a single micro-batch, while strategically pausing execution by inserting yield points to let other micro-batches proceed. It eliminates code duplication, reduces the potential need for variable postfixes, and efficiently manages cases where some executions complete at a layer's end while others have not. Additionally, it supports easy adaptation to different overlapping region choices or future enhancements, like a three-batch overlap, with minimal code changes. Below is a concise demonstration of this approach:&lt;/p&gt;
    &lt;code&gt;operations = [
    self._forward_attn,
    YieldOperation(),  # Pause execution for other micro-batches
    self._forward_dispatch,
    self._forward_mlp,
    YieldOperation(),  # Another pause point
    self._forward_combine,
]

# Process a single micro-batch without duplicating code
def _forward_attn(self, state):
    state.hidden_states = self.self_attn(state.hidden_states, ...)
&lt;/code&gt;
    &lt;head rend="h5"&gt;Prefill Overlapping Implementation&lt;/head&gt;
    &lt;p&gt;We refine the launch order during the prefill phase to avoid CPU-blocking via the dispatch operation in DeepEP, even though we are using its asynchronous mode. Specifically:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The dispatch operation blocks the CPU until the GPU receives metadata from other ranks to allocate correctly sized tensors.&lt;/item&gt;
      &lt;item&gt;An improper implementation would leave the computation stream idle during this period, as no computation tasks are submitted to the GPU.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To optimize, we prioritize submitting computation tasks to the GPU before launching CPU-blocking communication. This ensures the GPU remains active during communication. As illustrated in the figure below, TBO with a proper launch order, indicated by bolded borders, avoids bubble caused by a CPU-blocking operation (i.e., normal dispatch).&lt;/p&gt;
    &lt;head rend="h3"&gt;Expert Parallelism Load Balancer&lt;/head&gt;
    &lt;p&gt;In MoE models, EP often leads to uneven workload distribution across GPUs. This imbalance forces the system to wait for the slowest GPU computation or communication, wasting compute cycles and increasing memory usage due to expert activations. As the number of GPUs (EP size) increases, the imbalance issue gets more severe.&lt;/p&gt;
    &lt;p&gt;To address this, DeepSeek developed the Expert Parallelism Load Balancer (EPLB). EPLB takes expert distribution statistics as input and computes an optimal arrangement of experts to minimize imbalance. Users can allocate redundant experts (e.g., 32 additional experts), which, when combined with the original 256, create a pool of 288 experts. This pool allows EPLB to strategically place or replicate experts—for instance, duplicating the most frequently used expert multiple times or grouping a moderately used expert with rarely used ones on a single GPU.&lt;/p&gt;
    &lt;p&gt;Beyond balancing workloads, EPLB offers greater flexibility in parallelism design. With the original 256 experts, parallelism sizes are restricted to powers of two. EPLB’s use of 288 experts enables more diverse configurations, such as parallelism sizes of 12 or 72.&lt;/p&gt;
    &lt;p&gt;In the figure below, we demonstrate the effects of scale and EPLB algorithm to the imbalance issue via simulation. We compute GPU balancedness as the ratio between mean computation time and maximum computation time for a MoE layer among GPUs, and we use the number of tokens for a GPU to estimate the computation time for it. As can be seen, utilization rate decreases when the system scales with the number of nodes, and enabling EPLB significantly improves the utilization.&lt;/p&gt;
    &lt;head rend="h5"&gt;EPLB for Real-World Serving&lt;/head&gt;
    &lt;p&gt;For EPLB to be effective, the input distribution must closely match the actual serving workload. Two strategies enhance this alignment:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Increasing Batch Size: Larger batches reduce random fluctuations in expert usage, which improves balance, which can be achieved by scaling the cluster or using techniques like Multi-Token Prediction (MTP).&lt;/item&gt;
      &lt;item&gt;Periodic Rebalancing: Regularly updating the expert arrangement leverages temporal locality but requires efficient reloading of experts. This necessitates minimizing the cost of expert reloading operations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Even with EPLB, some imbalance is inevitable, making further optimization a valuable future direction.&lt;/p&gt;
    &lt;head rend="h5"&gt;Implementation of Rebalancing&lt;/head&gt;
    &lt;p&gt;SGLang implements expert rebalancing in three stages to ensure efficiency and minimal disruption:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;System Loading Stage: Weights are optionally preloaded from disk to main memory for faster rebalancing or kept on disk with memory mapping (mmap) for reduced memory usage.&lt;/item&gt;
      &lt;item&gt;Rebalance Preparation Stage: Required weights are asynchronously transferred to device memory in the background, utilizing free DMA hardware engines without interrupting ongoing GPU operations.&lt;/item&gt;
      &lt;item&gt;Rebalance Execution Stage: A device-to-device copy updates the weights. This step can be further optimized through physical memory rebinding techniques.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This staged approach ensures that rebalancing is both efficient and non-disruptive, maintaining system performance during updates.&lt;/p&gt;
    &lt;head rend="h2"&gt;Evaluation&lt;/head&gt;
    &lt;head rend="h3"&gt;End-to-end Performance&lt;/head&gt;
    &lt;head rend="h5"&gt;Experimental Setup&lt;/head&gt;
    &lt;p&gt;We evaluated the end-to-end performance of different configurations of SGLang using DeepSeek-V3 on a cluster of 12 nodes, connected via InfiniBand and each equipped with 8 H100 GPUs. This evaluation highlights the throughput improvements enabled by our advanced optimization techniques. We compared the following four settings:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SGLang with TP16 x 6: Every two nodes are paired with an independent group, running DeepSeek-V3 inference with a TP size of 16 and DP attention.&lt;/item&gt;
      &lt;item&gt;SGLang with PD Disaggregation: This version incorporates PD disaggregation and full EP optimization. For the EPLB, we adopt a distribution matching the input/output data, as real-time serving statistics are unavailable.&lt;/item&gt;
      &lt;item&gt;SGLang with PD Disaggregation and simulated MTP: To simulate MTP’s effects, we firstly double the batch size and halve the Key-Value KV cache length to maintain the same workload for GroupedGeMM computation and memory access. Moreover, we insert dummy kernels after the real attention computation to ensure the attention phase takes the same time as in DeepSeek’s profile, accurately reflecting the slowdown caused by MTP’s attention mechanism. We conservatively assume a 70% acceptance rate under MTP.&lt;/item&gt;
      &lt;item&gt;DeepSeek Profile Results: Throughput estimates are derived from DeepSeek’s official profiling data.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Performance Analysis of Prefill and Decode Phases&lt;/head&gt;
    &lt;p&gt;To accommodate varying workload demands, we independently evaluated the prefill (P) and decode (D) phases, assuming unlimited resources for the non-tested phase to isolate and maximize the load on the tested nodes—mirroring the setup used by DeepSeek. The results are summarized below:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Prefill Phase: On 4 nodes (4×8×H100, EP32), the system achieved per-node throughputs of 57,674, 54,543, and 50,302 tokens per second for prompt lengths of 1K, 2K, and 4K, respectively. As shown in the bar chart below, this represents up to a 3.3× improvement over the TP16 baseline, largely attributable to the optimized GroupedGeMM kernel (DeepGEMM) and two-batch overlap. Assuming a perfectly balanced workload, our system’s throughput is within 5.6% of DeepSeek's official profile.&lt;/item&gt;
      &lt;item&gt;Decode Phase: Evaluated on 9 nodes (9×8×H100, EP72; half the scale of DeepSeek), the system achieved 22,282 tokens/sec per node for 2K inputs—representing a 5.2× speedup over the TP16 baseline. Under simulated MTP conditions—with attention kernels intentionally slowed to reflect real-world latency—the system sustained a high throughput of 17,373 tokens/sec per node for 4K inputs, just 6.6% below DeepSeek’s official profile. As shown in the figure on the right, these performance gains are largely attributed to 4× larger batch sizes enabled by EP, which enhances scalability by significantly reducing per-GPU memory consumption of model weights.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Profile Results&lt;/head&gt;
    &lt;p&gt;This section compares SGLang’s performance with DeepSeek’s inference system, aligning our experimental setup as closely as possible to DeepSeek’s production environment. We analyze overall throughput and detailed kernel breakdowns, benchmarking against DeepSeek’s blog and public profile data.&lt;/p&gt;
    &lt;head rend="h5"&gt;Overall Throughput&lt;/head&gt;
    &lt;p&gt;For prefill, we tested a scenario with 16,384 tokens per device and an input length of 4,096. Due to uncertainty in DeepSeek’s expert distribution, we evaluated two cases: one with default expert distribution and another with simulated perfect EPLB (random expert selection following group-limited routing semantics) as a performance upper bound.&lt;/p&gt;
    &lt;p&gt;The results are presented below:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;DeepSeek Blog (excl. cache hit)&lt;/cell&gt;
        &lt;cell role="head"&gt;DeepSeek Profile&lt;/cell&gt;
        &lt;cell role="head"&gt;SGLang (Default)&lt;/cell&gt;
        &lt;cell role="head"&gt;SGLang + Simulated Perfect EPLB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Batch Size&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;16,384&lt;/cell&gt;
        &lt;cell&gt;16,384&lt;/cell&gt;
        &lt;cell&gt;16,384&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Input Length&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;4,096&lt;/cell&gt;
        &lt;cell&gt;4,096&lt;/cell&gt;
        &lt;cell&gt;4,096&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Throughput (per node)&lt;/cell&gt;
        &lt;cell&gt;32,206&lt;/cell&gt;
        &lt;cell&gt;62,713&lt;/cell&gt;
        &lt;cell&gt;50,302&lt;/cell&gt;
        &lt;cell&gt;59,337&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;DeepSeek’s profile reports a throughput roughly twice that of its production environment. SGLang with default expert imbalance is 20% slower than DeepSeek’s profile, while the simulated perfect EPLB case narrows the gap to 6%.&lt;/p&gt;
    &lt;p&gt;For decode, the results are shown below:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;DeepSeek Blog&lt;/cell&gt;
        &lt;cell role="head"&gt;DeepSeek Profile&lt;/cell&gt;
        &lt;cell role="head"&gt;SGLang (Default)&lt;/cell&gt;
        &lt;cell role="head"&gt;SGLang + Simulated MTP (Slow Attention)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Batch Size&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;128&lt;/cell&gt;
        &lt;cell&gt;256&lt;/cell&gt;
        &lt;cell&gt;128&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;KV Cache Length&lt;/cell&gt;
        &lt;cell&gt;4,989&lt;/cell&gt;
        &lt;cell&gt;4,096&lt;/cell&gt;
        &lt;cell&gt;2,000&lt;/cell&gt;
        &lt;cell&gt;4,000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Number of Nodes&lt;/cell&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Throughput (per node)&lt;/cell&gt;
        &lt;cell&gt;14,800&lt;/cell&gt;
        &lt;cell&gt;18,598&lt;/cell&gt;
        &lt;cell&gt;22,282&lt;/cell&gt;
        &lt;cell&gt;17,373&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Using half the nodes of DeepSeek, SGLang with simulated MTP is only slightly slower than DeepSeek’s profile. In a higher batch size setting (256 sequences, 2,000 input length), SGLang achieves 22,282 tokens per second per node, demonstrating strong scalability.&lt;/p&gt;
    &lt;head rend="h5"&gt;Detail Breakdown&lt;/head&gt;
    &lt;p&gt;The figure below breaks down kernel execution times for prefill, including unit test results as a theoretical upper bound:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Default EPLB: Communication kernels exhibit longer execution times and higher variance compared to DeepSeek’s profile, likely due to greater expert imbalance. This leads to extended computation stream bubbles, slowing down overall performance.&lt;/item&gt;
      &lt;item&gt;Simulated Perfect EPLB: This setup aligns more closely with DeepSeek’s profile, though discrepancies remain, indicating potential areas for optimization.&lt;/item&gt;
      &lt;item&gt;Comparison with Unit Tests: Both DeepSeek and SGLang have a communication time slower than unit test results, while the latter is achievable when disabling TBO, revealing a potential optimization direction if communication is the bottleneck.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SGLang’s decode kernel breakdown aligns closely with DeepSeek’s, as shown below:&lt;/p&gt;
    &lt;p&gt;Key observations include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Combine Time Discrepancy: SGLang’s combine operation appears 2x slower than DeepSeek’s due to shorter attention computation, causing communication kernels to busy-wait. In the simulated slow attention experiment, combine time matches DeepSeek’s, confirming this hypothesis.&lt;/item&gt;
      &lt;item&gt;MoE Performance: SGLang’s MoE kernels are 25% slower, possibly because DeepSeek’s 18 nodes (versus our 9) distribute experts more efficiently, reducing memory access overhead for GEMM operations.&lt;/item&gt;
      &lt;item&gt;Dispatch Optimization Potential: Both DeepSeek and SGLang show dispatch times of ~0.17ms per layer, but unit tests with DeepEP reveal a potential of 0.06ms occupying SMs. Currently, dispatch spends significant time busy-waiting for data. Inserting slow dummy kernels between send/receive operations reduces dispatch time to 0.09ms, and in-flight duration analysis using unit test data suggests further improvements are possible.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While minor enhancements remain—primarily in kernel fusion under "Other Kernels"—SGLang’s decode performance is largely aligned with DeepSeek’s, with prefill optimization as the next focus.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ablation Study: Two-batch Overlap&lt;/head&gt;
    &lt;head rend="h5"&gt;Impact of Batch Size and Attention Time&lt;/head&gt;
    &lt;p&gt;This section investigates TBO performance across varying batch sizes and simulated MTP scenarios.&lt;/p&gt;
    &lt;p&gt;TBO delivers two significant benefits in the prefill phase, as evidenced by throughput comparisons and memory usage optimizations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Support for Larger Batch Sizes: In the vanilla configuration, each device processes up to 8,192 tokens before encountering out-of-memory (OOM) errors at 16,384 tokens. TBO mitigates this by optimizing memory usage for input tokens, enabling inference with batches as large as 16,384 tokens per device. This further boosts performance to 40.5% increase when comparing the TBO flag with all other configurations made optimal.&lt;/item&gt;
      &lt;item&gt;Enhanced Throughput: By overlapping computation (e.g., attention and MLP phases) with communication (e.g., DeepEP Combine and Dispatch), TBO achieves a 27% to 35% throughput increase compared to the vanilla setup, even when processing the same token count per device.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;TBO’s impact in the decode phase varies by scenario, with performance tied to batch size and attention processing time:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Real Test Cases: Speedup in practical scenarios is contingent on batch size exceeding a threshold between 64 and 128 tokens. Below this, TBO yields minimal or negative gains (e.g., -27% at 32 tokens/device), as small decode batch sizes hinder kernel efficiency. The speedup reaches 25.5% at 256 tokens with a performance of 22,310 tokens per second.&lt;/item&gt;
      &lt;item&gt;Simulated MTP Scenario: TBO provides the most substantial speedup in simulated MTP cases when processing 128 requests to generate 256 tokens per decode step. This is due to prolonged attention processing time, which aligns computation (e.g., DP Attention layers) with DeepEP communication overhead (e.g., combine and dispatch steps). The evaluation shows a 35% speedup at 128 sequences/device, with throughput 17,552 tokens per second compared to 12,929 without TBO.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Detail Breakdown&lt;/head&gt;
    &lt;p&gt;We evaluated three prefill scenarios: TBO with 16k tokens per batch, TBO with 8k tokens, and no-TBO with 8k tokens. The figure below reveals key insights:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;TBO Efficiency: Comparing the 8k cases, TBO improves overall efficiency by overlapping computation and communication, as expected.&lt;/item&gt;
      &lt;item&gt;Batch Size Impact: Reducing the batch size from 16k to 8k with TBO results in a slight slowdown, reflecting diminished kernel efficiency with smaller batches.&lt;/item&gt;
      &lt;item&gt;Kernel Performance: Interestingly, the no-TBO 8k case outperforms the TBO 16k case in per-kernel speed, despite both having an effective batch size of 8k for kernels. This may stem from reduced streaming multiprocessors (SMs) with TBO, potential noisy neighbor effects during overlap, or kernel incompatibility between computation and communication. These findings suggest future optimization directions for SGLang.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For the decode phase, we analyzed three configurations: TBO with a batch size of 256, no-TBO with 256, and no-TBO with 128. The time breakdown is shown below:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;TBO vs. No-TBO (Batch Size 256): Without TBO, communication time increases significantly due to the lack of overlap. However, computation kernels, particularly GEMM, benefit from a larger effective batch size, resulting in faster execution.&lt;/item&gt;
      &lt;item&gt;TBO (256) vs. No-TBO (128): Comparing cases with the same kernel batch size, only non-overlapped communication slows down in the no-TBO setup, while computation remains consistent. Unlike prefill, decode communication kernels either fully utilize SMs (during send/receive) or none (during inflight waiting), avoiding resource contention with computation kernels.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Ablation Study: EPLB&lt;/head&gt;
    &lt;p&gt;This section evaluates the impact of the EPLB on system performance through overall throughput analysis and detailed case studies. Given EPLB's sensitivity to workload distribution and distribution shifts in production environments, we focus on qualitative and generalizable insights rather than real-world performance, which requires production data.&lt;/p&gt;
    &lt;head rend="h5"&gt;Overall Results&lt;/head&gt;
    &lt;p&gt;The figure below illustrates EPLB's effect on throughput in large-scale settings. EPLB delivers a significant speedup of 1.49x (prefill) and 2.54x (decode), as expected, due to its ability to mitigate workload imbalances across GPUs. As the number of ranks scales, imbalances grow, and EPLB effectively addresses this in our large-scale experiments, leading to notable throughput improvements.&lt;/p&gt;
    &lt;head rend="h5"&gt;Case Study: Workload Imbalance Versus Overall Throughput&lt;/head&gt;
    &lt;p&gt;To explore the relationship between workload imbalance and throughput, we conducted a case study using a decode experiment with 1800 input tokens, 100 output tokens, and a batch size of 256. Throughput and balancedness (average token count divided by maximum token count across experts) were plotted against decoding steps:&lt;/p&gt;
    &lt;p&gt;The results reveal a strong correlation between balancedness and throughput, emphasizing the importance of maintaining high balancedness for optimal performance.&lt;/p&gt;
    &lt;head rend="h5"&gt;Case Study: Expert Distribution Statistics&lt;/head&gt;
    &lt;p&gt;The following figure presents expert distribution statistics for prefill and decode sample data:&lt;/p&gt;
    &lt;p&gt;Key observations include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Imbalance in Expert Usage: Most experts are infrequently used, while a small subset is heavily utilized, underscoring the inherent imbalance in MoE models.&lt;/item&gt;
      &lt;item&gt;Prefill vs. Decode Differences: Although prefill and decode distributions share similarities, notable differences exist. This supports the use of PD disaggregation, which enables distinct expert placements for each phase, optimizing performance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These findings highlight EPLB's role in addressing workload imbalances and the value of tailoring expert placement to phase-specific demands.&lt;/p&gt;
    &lt;head rend="h2"&gt;Toolkits&lt;/head&gt;
    &lt;head rend="h3"&gt;Disposable Tensor&lt;/head&gt;
    &lt;p&gt;Memory management in PyTorch can be challenging due to persistent object references, especially in GPU-intensive workflows where CUDA memory is a scarce resource. Consider the following example:&lt;/p&gt;
    &lt;code&gt;def ffn(hidden_state: torch.Tensor, linear1: nn.Linear, linear2: nn.Linear):
    intermediate_state = linear1(hidden_state)
    del hidden_state  # Attempt to free memory, but no effect due to external reference
    return linear2(nn.ReLU(intermediate_state))

hidden_state = ffn(hidden_state, linear1, linear2)
&lt;/code&gt;
    &lt;p&gt;In this code, &lt;code&gt;del hidden_state&lt;/code&gt; is intended to release the memory occupied by &lt;code&gt;hidden_state&lt;/code&gt; after &lt;code&gt;intermediate_state&lt;/code&gt; is computed. However, as &lt;code&gt;hidden_state&lt;/code&gt; is still referenced outside the function, the &lt;code&gt;del&lt;/code&gt; operation has no effect. This increases peak memory usage, risking performance slowdowns or out-of-memory errors.&lt;/p&gt;
    &lt;p&gt;SGLang addresses this with the DisposableTensor class, a subclass of &lt;code&gt;torch.Tensor&lt;/code&gt; which introduces a dispose() method to explicitly and immediately release a tensor’s memory, circumventing Python’s reference counting limitations. Here’s how it works:&lt;/p&gt;
    &lt;code&gt;def ffn(hidden_state: torch.Tensor, linear1: nn.Linear, linear2: nn.Linear):
    intermediate_state = linear1(hidden_state)
    hidden_state.dispose()  # Immediately releases CUDA memory
    return linear2(nn.ReLU(intermediate_state))

# Wrap the tensor in DisposableTensor
hidden_state = DisposableTensor(hidden_state)
hidden_state = ffn(hidden_state, linear1, linear2)
&lt;/code&gt;
    &lt;p&gt;By wrapping &lt;code&gt;hidden_state&lt;/code&gt; in a &lt;code&gt;DisposableTensor&lt;/code&gt; and calling &lt;code&gt;dispose()&lt;/code&gt; when it’s no longer needed, the CUDA memory is freed right away. This ensures that memory is released as soon as the tensor’s role in the computation is complete, reducing peak memory usage and improving overall efficiency.&lt;/p&gt;
    &lt;head rend="h3"&gt;Expert Workload Extraction and Simulation&lt;/head&gt;
    &lt;p&gt;SGLang also includes a toolset for analyzing and simulating expert workload distribution in MoE models. This feature enables users to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dump Expert Workload Statistics: Extract either accumulated statistics or per-batch workload data. Accumulated stats support the EPLB manager for real-time optimization, while per-batch data provides granular insights for analysis and simulation.&lt;/item&gt;
      &lt;item&gt;Simulate Expert Utilization: Model expert balance across various configurations without requiring costly hardware or repeated trials. For instance, users can gather workload data from a modest setup (e.g., 2x8xH100 or 8xH200) and simulate the performance for a large-scale 22-node deployment.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This simulation capability allows users to evaluate how factors like rebalancing frequency, node count, or batch size impact system performance. It’s a cost-effective way to fine-tune configurations before scaling up.&lt;/p&gt;
    &lt;head rend="h2"&gt;Limitations and Future Work&lt;/head&gt;
    &lt;p&gt;While our implementation of SGLang for DeepSeek-V3 inference demonstrates significant throughput improvements, several limitations and areas for future enhancement remain:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Latency Optimization: The current focus on throughput leaves Time to First Token (TTFT) at 2–5 seconds and Inter-Token Latency (ITL) at approximately 100ms, requiring further optimizations for real-time use cases.&lt;/item&gt;
      &lt;item&gt;Sequence Length Constraints: Limited to shorter sequences due to the use of 96 GPUs. Expanding GPU resources would support longer sequences, essential for specific applications.&lt;/item&gt;
      &lt;item&gt;Multi-Token Prediction (MTP) Integration: SGLang supports MTP but lacks full integration with DP attention, reducing efficiency in mixed parallelism configurations.&lt;/item&gt;
      &lt;item&gt;EPLB Distribution: The experiments in this blog utilizes in-distribution data for Expert Parallelism Load Balancer (EPLB), which may not reflect real-world variability. Future work should experiment performances when having distribution shifts.&lt;/item&gt;
      &lt;item&gt;Flexible Tensor Parallelism (TP) Sizes: For DeepSeek-V3, memory-optimal TP sizes for dense FFNs are small but larger than 1. Currently, SGLang only supports pure TP or DP, leading to suboptimal memory use. Flexible TP options are needed.&lt;/item&gt;
      &lt;item&gt;Blackwell Support: Currently, our implementation supports only the NVIDIA Hopper architecture. We are actively working to extend compatibility to the next-generation Blackwell architecture. If you are interested in supporting or sponsoring this development, welcome to contact lmsys.org@gmail.com.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;By leveraging PD disaggregation, EP, and a carefully crafted parallelism design, we’ve reproduced DeepSeek’s inference framework in SGLang with exceptional performance. Our open-source efforts—achieving 52.3k input tokens per second and 22.3k output tokens per second—demonstrate SGLang’s power for large-scale LLM inference. We invite the community to explore, replicate, and extend this work to push the boundaries of efficient AI deployment.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgment&lt;/head&gt;
    &lt;p&gt;We would like to express our heartfelt gratitude to the following teams and collaborators:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SGLang Core Team and Community Contributors — Jingyi Chen, Cheng Wan, Liangsheng Yin, Baizhou Zhang, Ke Bao, Jiexin Liang, Xiaoyu Zhang, Yanbo Yang, Fan Yin, Chao Wang, Laixin Xie, Runkai Tao, Yuhong Guo, Kaihong Zhang, Lei Yu, Yu-Hsuan Tseng, Qilin Tian, Peng Zhang, Yi Zhang, Yineng Zhang, Byron Hsu, and many others.&lt;/item&gt;
      &lt;item&gt;Atlas Cloud Team — Jerry Tang, Wei Xu, Simon Xue, Harry He, Eva Ma, and colleagues — for providing a 96-device NVIDIA H100 cluster and offering responsive engineering support.&lt;/item&gt;
      &lt;item&gt;NVIDIA Solution Architect Team — Xuting Zhou, Jinyan Chen, and colleagues — for their work on the seamless integration of expert parallelism.&lt;/item&gt;
      &lt;item&gt;NVIDIA Enterprise Product Team — Trevor Morris, Elfie Guo, Kaixi Hou, Kushan Ahmadian, and colleagues — for optimizing the DeepSeek R1 kernels.&lt;/item&gt;
      &lt;item&gt;LinkedIn Team — Biao He, Qingquan Song, Chunan Zeng, Yun Dai, Yubo Wang, and colleagues — for optimizing the Flash-Attention 3 backend.&lt;/item&gt;
      &lt;item&gt;Mooncake Team — Shangming Cai, Teng Ma, Mingxing Zhang, and colleagues — for their collaboration on PD disaggregation in SGLang.&lt;/item&gt;
      &lt;item&gt;FlashInfer Team — Zihao Ye, Yong Wu, Yaxing Cai — for additional DeepSeek R1 kernel optimizations.&lt;/item&gt;
      &lt;item&gt;Dynamo Team - Kyle Kranen, Vikram Sharma Mailthody, and colleagues - for extra support on PD disaggregation in SGLang.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thank you all for your invaluable support and collaboration.&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix&lt;/head&gt;
    &lt;p&gt;Related PRs: #1970 #2925 #4068 #4165 #4232 #4390 #4435 #4521 #4654 #4767 #4770 #4836 #4880 #4957 #5068 #5085 #5295 #5415 #5432 #5435 #5530 #5558 #5561 #5626 #5657 #5805 #5819 #5890 DeepEP#142&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45064329</guid></item><item><title>Seedbox Lite: A lightweight torrent streaming app with instant playback</title><link>https://github.com/hotheadhacker/seedbox-lite</link><description>&lt;doc fingerprint="ae3af81d7f622191"&gt;
  &lt;main&gt;
    &lt;p&gt;Stream Torrents Instantly&lt;/p&gt;
    &lt;p&gt;A modern, lightweight torrent streaming application with instant playback&lt;/p&gt;
    &lt;p&gt;Features • Screenshots • Quick Start • Installation • Documentation&lt;/p&gt;
    &lt;p&gt;SeedBox Lite is a cutting-edge torrent streaming platform that allows you to watch movies and TV shows instantly without waiting for complete downloads. Built with modern web technologies, it provides a Netflix-like experience with powerful torrent capabilities.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🎯 Instant Streaming - Start watching immediately as the torrent downloads&lt;/item&gt;
      &lt;item&gt;🔐 Password Protection - Secure access with authentication&lt;/item&gt;
      &lt;item&gt;📱 Mobile Optimized - Perfect responsive design for all devices&lt;/item&gt;
      &lt;item&gt;🎥 Smart Video Player - Advanced player with subtitles and fullscreen support&lt;/item&gt;
      &lt;item&gt;⚡ Fast Setup - Deploy in minutes with Docker or PM2&lt;/item&gt;
      &lt;item&gt;🌐 Cross-Platform - Works on Windows, macOS, and Linux&lt;/item&gt;
      &lt;item&gt;🎨 Modern UI - Clean, intuitive interface inspired by popular streaming services&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Torrent to Stream - Convert any movie/TV torrent to instant streaming&lt;/item&gt;
      &lt;item&gt;Progress Tracking - Real-time download progress and cache management&lt;/item&gt;
      &lt;item&gt;Smart Caching - Intelligent caching system with configurable limits&lt;/item&gt;
      &lt;item&gt;Multiple Formats - Support for MP4, MKV, AVI, and more video formats&lt;/item&gt;
      &lt;item&gt;Subtitle Support - Automatic subtitle detection and loading&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Netflix-Style Interface - Familiar and intuitive design&lt;/item&gt;
      &lt;item&gt;Mobile-First Design - Optimized for smartphones and tablets&lt;/item&gt;
      &lt;item&gt;Native Fullscreen - True fullscreen experience on mobile devices&lt;/item&gt;
      &lt;item&gt;Gesture Controls - Double-tap to fullscreen, intuitive video controls&lt;/item&gt;
      &lt;item&gt;Responsive Layout - Adapts perfectly to any screen size&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Password Authentication - Secure access control&lt;/item&gt;
      &lt;item&gt;CORS Enabled - Cross-origin resource sharing for flexible deployment&lt;/item&gt;
      &lt;item&gt;Health Monitoring - Built-in health checks and monitoring&lt;/item&gt;
      &lt;item&gt;Production Ready - Optimized for production deployments&lt;/item&gt;
      &lt;item&gt;Docker Support - Easy containerized deployment&lt;/item&gt;
      &lt;item&gt;PM2 Integration - Process management for Node.js applications&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;iOS Safari Support - Native fullscreen using WebKit APIs&lt;/item&gt;
      &lt;item&gt;Android Chrome - Optimized for Android mobile browsers&lt;/item&gt;
      &lt;item&gt;Range Requests - HTTP range support for smooth video seeking&lt;/item&gt;
      &lt;item&gt;Mobile Viewport - Proper viewport handling for app-like experience&lt;/item&gt;
      &lt;item&gt;Touch Optimized - Gesture-friendly video controls&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Clone the repository
git clone https://github.com/hotheadhacker/seedbox-lite.git
cd seedbox-lite

# Start with Docker Compose
docker-compose up -d

# Access the application
open http://localhost:5174&lt;/code&gt;
    &lt;code&gt;# Clone and install dependencies
git clone https://github.com/hotheadhacker/seedbox-lite.git
cd seedbox-lite

# Install backend dependencies
cd server &amp;amp;&amp;amp; npm install

# Install frontend dependencies  
cd ../client &amp;amp;&amp;amp; npm install

# Build frontend
npm run build

# Start with PM2
pm2 start ecosystem.config.js&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Node.js 18+&lt;/item&gt;
      &lt;item&gt;npm 8+&lt;/item&gt;
      &lt;item&gt;Docker 20+ (for Docker deployment)&lt;/item&gt;
      &lt;item&gt;PM2 (for PM2 deployment)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ Windows 10/11&lt;/item&gt;
      &lt;item&gt;✅ macOS 10.15+&lt;/item&gt;
      &lt;item&gt;✅ Ubuntu 18.04+&lt;/item&gt;
      &lt;item&gt;✅ Debian 10+&lt;/item&gt;
      &lt;item&gt;✅ CentOS 7+&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ Chrome 90+&lt;/item&gt;
      &lt;item&gt;✅ Firefox 88+&lt;/item&gt;
      &lt;item&gt;✅ Safari 14+&lt;/item&gt;
      &lt;item&gt;✅ Edge 90+&lt;/item&gt;
      &lt;item&gt;✅ Mobile browsers (iOS Safari, Android Chrome)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/hotheadhacker/seedbox-lite.git
cd seedbox-lite&lt;/code&gt;
    &lt;code&gt;# Copy and edit environment variables
cp .env.example .env
nano .env&lt;/code&gt;
    &lt;p&gt;Key Environment Variables:&lt;/p&gt;
    &lt;code&gt;# Server Configuration
NODE_ENV=production
SERVER_PORT=3001
ACCESS_PASSWORD=your_secure_password

# Frontend Configuration  
FRONTEND_URL=http://localhost:5174
VITE_API_BASE_URL=http://localhost:3001

# Docker Ports
BACKEND_PORT=3001
FRONTEND_PORT=5174&lt;/code&gt;
    &lt;code&gt;# Start all services
docker-compose up -d

# Check status
docker-compose ps

# View logs
docker-compose logs -f&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frontend: http://localhost:5174&lt;/item&gt;
      &lt;item&gt;Backend API: http://localhost:3001&lt;/item&gt;
      &lt;item&gt;Default Login: Password set in &lt;code&gt;ACCESS_PASSWORD&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Install Node.js 18+
curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
sudo apt-get install -y nodejs

# Install PM2 globally
npm install -g pm2&lt;/code&gt;
    &lt;code&gt;# Clone repository
git clone https://github.com/hotheadhacker/seedbox-lite.git
cd seedbox-lite

# Install backend dependencies
cd server
npm install
cd ..

# Install and build frontend
cd client
npm install
npm run build
cd ..&lt;/code&gt;
    &lt;code&gt;# Backend environment
cd server
cp .env.example .env
nano .env&lt;/code&gt;
    &lt;p&gt;Backend &lt;code&gt;.env&lt;/code&gt; Configuration:&lt;/p&gt;
    &lt;code&gt;NODE_ENV=production
SERVER_PORT=3001
SERVER_HOST=0.0.0.0
ACCESS_PASSWORD=your_secure_password
FRONTEND_URL=http://localhost:5174&lt;/code&gt;
    &lt;code&gt;# Start backend with PM2
cd server
pm2 start ecosystem.config.js

# Serve frontend with nginx or serve
cd ../client/dist
npx serve -s . -l 5174

# Or use PM2 for frontend
pm2 start "npx serve -s . -l 5174" --name "seedbox-frontend"&lt;/code&gt;
    &lt;code&gt;# View running processes
pm2 list

# View logs
pm2 logs

# Restart services
pm2 restart all

# Save PM2 configuration
pm2 save
pm2 startup&lt;/code&gt;
    &lt;code&gt;git clone https://github.com/hotheadhacker/seedbox-lite.git
cd seedbox-lite

# Install backend dependencies
cd server
npm install

# Install frontend dependencies
cd ../client  
npm install&lt;/code&gt;
    &lt;code&gt;# Backend environment
cd server
cp .env.example .env&lt;/code&gt;
    &lt;p&gt;Development &lt;code&gt;.env&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;NODE_ENV=development
SERVER_PORT=3000
SERVER_HOST=localhost
ACCESS_PASSWORD=seedbox123
FRONTEND_URL=http://localhost:5173&lt;/code&gt;
    &lt;code&gt;# Terminal 1: Start backend
cd server
npm run dev

# Terminal 2: Start frontend  
cd client
npm run dev&lt;/code&gt;
    &lt;code&gt;# Health check
curl http://localhost:3001/api/health
curl http://localhost:5174/health

# API endpoints
curl -X POST http://localhost:3001/api/auth/login \
  -H "Content-Type: application/json" \
  -d '{"password":"your_password"}'

# Cache stats
curl http://localhost:3001/api/cache/stats&lt;/code&gt;
    &lt;code&gt;# Check PM2 status
pm2 list
pm2 logs seedbox-backend
pm2 logs seedbox-frontend

# Test API endpoints
curl http://localhost:3001/api/health
curl http://localhost:5174&lt;/code&gt;
    &lt;code&gt;cd client
npm test

# Run Cypress e2e tests
npm run test:e2e

# Accessibility testing
npm run test:a11y&lt;/code&gt;
    &lt;code&gt;cd server
npm test

# API integration tests
npm run test:integration

# Load testing
npm run test:load&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Variable&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;NODE_ENV&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;production&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Application environment&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;SERVER_PORT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;3001&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Backend server port&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;SERVER_HOST&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;0.0.0.0&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Backend server host&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;ACCESS_PASSWORD&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;seedbox123&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Authentication password&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;MAX_CACHE_SIZE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;5GB&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Maximum cache size&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;CLEANUP_INTERVAL&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;1h&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Cache cleanup interval&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Variable&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;VITE_API_BASE_URL&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;http://localhost:3001&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Backend API URL&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;FRONTEND_URL&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;http://localhost:5174&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Frontend URL&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Variable&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;BACKEND_PORT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;3001&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Docker backend port mapping&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;FRONTEND_PORT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;5174&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Docker frontend port mapping&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;server {
    listen 80;
    server_name your-domain.com;
    
    location / {
        proxy_pass http://localhost:5174;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
    
    location /api/ {
        proxy_pass http://localhost:3001;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}&lt;/code&gt;
    &lt;code&gt;# Install Certbot
sudo apt install certbot python3-certbot-nginx

# Get SSL certificate
sudo certbot --nginx -d your-domain.com

# Auto-renewal
sudo crontab -e
# Add: 0 12 * * * /usr/bin/certbot renew --quiet&lt;/code&gt;
    &lt;code&gt;# Check if ports are in use
lsof -i :3001
lsof -i :5174

# Kill processes using ports
sudo kill -9 $(lsof -ti:3001)
sudo kill -9 $(lsof -ti:5174)&lt;/code&gt;
    &lt;code&gt;# Rebuild containers
docker-compose down
docker-compose up --build

# Clear Docker cache
docker system prune -a

# Check container logs
docker-compose logs seedbox-backend
docker-compose logs seedbox-frontend&lt;/code&gt;
    &lt;code&gt;# Reset PM2
pm2 kill
pm2 start ecosystem.config.js

# Check PM2 logs
pm2 logs --lines 50

# Monitor PM2 processes
pm2 monit&lt;/code&gt;
    &lt;code&gt;# Fix file permissions
sudo chown -R $USER:$USER .
chmod +x deploy.sh

# Docker permission issues
sudo usermod -aG docker $USER
newgrp docker&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ensure CORS is enabled in backend&lt;/item&gt;
      &lt;item&gt;Check video format compatibility&lt;/item&gt;
      &lt;item&gt;Verify range request support&lt;/item&gt;
      &lt;item&gt;Test with different browsers&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;POST /api/auth/login
{
  "password": "your_password"
}&lt;/code&gt;
    &lt;code&gt;GET /api/torrents/search?q=movie+name
POST /api/torrents/add
{
  "magnetLink": "magnet:..."
}&lt;/code&gt;
    &lt;code&gt;GET /api/stream/:torrentId/:fileIndex
Range requests supported for video seeking&lt;/code&gt;
    &lt;code&gt;GET /api/cache/stats
POST /api/cache/clear&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Change default password immediately&lt;/item&gt;
      &lt;item&gt;Use HTTPS in production&lt;/item&gt;
      &lt;item&gt;Keep dependencies updated&lt;/item&gt;
      &lt;item&gt;Enable firewall rules&lt;/item&gt;
      &lt;item&gt;Regular security audits&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The application includes security headers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;X-Frame-Options: SAMEORIGIN&lt;/item&gt;
      &lt;item&gt;X-Content-Type-Options: nosniff&lt;/item&gt;
      &lt;item&gt;X-XSS-Protection: 1; mode=block&lt;/item&gt;
      &lt;item&gt;Referrer-Policy: no-referrer-when-downgrade&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Change default passwords&lt;/item&gt;
      &lt;item&gt;Configure HTTPS/SSL&lt;/item&gt;
      &lt;item&gt;Set up monitoring&lt;/item&gt;
      &lt;item&gt;Configure backups&lt;/item&gt;
      &lt;item&gt;Set up log rotation&lt;/item&gt;
      &lt;item&gt;Configure firewall&lt;/item&gt;
      &lt;item&gt;Test mobile compatibility&lt;/item&gt;
      &lt;item&gt;Verify video streaming&lt;/item&gt;
      &lt;item&gt;Test authentication&lt;/item&gt;
      &lt;item&gt;Monitor performance&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For high-traffic deployments:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use load balancer (nginx/HAProxy)&lt;/item&gt;
      &lt;item&gt;Scale backend horizontally&lt;/item&gt;
      &lt;item&gt;Implement Redis for session storage&lt;/item&gt;
      &lt;item&gt;Use CDN for static assets&lt;/item&gt;
      &lt;item&gt;Monitor resource usage&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fork the repository&lt;/item&gt;
      &lt;item&gt;Create feature branch&lt;/item&gt;
      &lt;item&gt;Make changes&lt;/item&gt;
      &lt;item&gt;Add tests&lt;/item&gt;
      &lt;item&gt;Submit pull request&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;IMPORTANT: Please read this disclaimer carefully before using SeedBox Lite.&lt;/p&gt;
    &lt;p&gt;SeedBox Lite is an open-source project provided for educational and personal use only. We do not endorse, promote, or facilitate copyright infringement, illegal streaming, or piracy in any form. This software is designed to be used with legal content only.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We do not host, store, or distribute any content. All torrents and media are accessed through your own connections.&lt;/item&gt;
      &lt;item&gt;This application is intended for use with content that you have the legal right to access and stream.&lt;/item&gt;
      &lt;item&gt;Users are solely responsible for how they use this software and for ensuring compliance with all applicable laws in their jurisdiction.&lt;/item&gt;
      &lt;item&gt;The creators and contributors of SeedBox Lite take no responsibility for how this software is used.&lt;/item&gt;
      &lt;item&gt;Using torrents to download or share copyrighted materials without permission may be illegal in your country.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By using SeedBox Lite, you acknowledge that you understand these terms and agree to use the software responsibly and legally.&lt;/p&gt;
    &lt;p&gt;This project is licensed under the Custom Non-Commercial License - see the LICENSE file for details.&lt;/p&gt;
    &lt;p&gt;Important License Restrictions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This software is provided for personal, educational, and non-commercial use only&lt;/item&gt;
      &lt;item&gt;Commercial use is strictly prohibited without explicit written permission&lt;/item&gt;
      &lt;item&gt;Redistribution must include this license and copyright notice&lt;/item&gt;
      &lt;item&gt;No warranty or liability is provided with this software&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;WebTorrent for torrent streaming capabilities&lt;/item&gt;
      &lt;item&gt;React team for the amazing framework&lt;/item&gt;
      &lt;item&gt;Docker community for containerization&lt;/item&gt;
      &lt;item&gt;All contributors and users&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Made with ❤️ by hotheadhacker&lt;/p&gt;
    &lt;p&gt;⭐ Star this repo if you find it useful!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45065278</guid></item><item><title>Why AI Isn't Ready to Be a Real Coder</title><link>https://spectrum.ieee.org/ai-for-coding</link><description>&lt;doc fingerprint="3f551019b8e708df"&gt;
  &lt;main&gt;
    &lt;p&gt;Artificial intelligence (AI) has transformed the coding sphere, with AI coding tools completing source code, correcting syntax errors, creating inline documentation, and understanding and answering questions about a codebase. As the technology advances beyond automating programming tasks, the idea of full autonomy looms large. Is AI ready to be a real coder?&lt;/p&gt;
    &lt;p&gt;A new paper says not yet—and maps out exactly why. Researchers from Cornell University, MIT CSAIL, Stanford University, and UC Berkeley highlight key challenges that today’s AI models face and outline promising research directions to tackle them. They presented their work at the 2025 International Conference on Machine Learning.&lt;/p&gt;
    &lt;p&gt;The study offers a clear-eyed reality check amid all the hype. “At some level, the technology is powerful and useful already, and it has gotten to the point where programming without these tools just feels primitive,” says Armando Solar-Lezama, a co-author of the paper and an associate director at MIT CSAIL, where he leads the computer-aided programming group. He argues, however, that AI-powered software development has yet to reach “the point where you can really collaborate with these tools the way you can with a human programmer.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Challenges With AI Coding Tools&lt;/head&gt;
    &lt;p&gt;According to the study, AI still struggles with several crucial facets of coding: sweeping scopes involving huge codebases, the extended context lengths of millions of lines of code, higher levels of logical complexity, and long-horizon or long-term planning about the structure and design of code to maintain code quality.&lt;/p&gt;
    &lt;p&gt;Koushik Sen, a professor of computer science at UC Berkeley and also a co-author of the paper, cites fixing a memory safety bug as an example. (Such bugs can cause crashes, corrupt data, and open security vulnerabilities.) Software engineers might approach debugging by first determining where the error originates, “which might be far away from where it’s crashing, especially in a large codebase,” Sen explains. They’ll also have to understand the semantics of the code and how it works, and make changes based on that understanding. “You might have to not only fix that bug but change the entire memory management,” he adds.&lt;/p&gt;
    &lt;p&gt;These kinds of complex tasks can be difficult for AI development tools to navigate, resulting in hallucinations about where the bug is or its root cause, as well as irrelevant suggestions or code fixes with subtle problems. “There are many failure points, and I don’t think the current LLMs [large language models] are good at handling that,” says Sen.&lt;/p&gt;
    &lt;p&gt;Among the various paths suggested by the researchers toward solving these AI coding challenges—such as training code LLMs to better collaborate with humans and ensuring human oversight for machine-generated code—the human element endures.&lt;/p&gt;
    &lt;p&gt;“A big part of software development is building a shared vocabulary and a shared understanding of what the problem is and how we want to describe these features. It’s about coming up with the right metaphor for the architecture of our system,” Solar-Lezama says. “It’s something that can be difficult to replicate by a machine. Our interfaces with these tools are still quite narrow compared to all the things that we can do when interacting with real colleagues.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Enhancing AI-Human Collaboration in Coding&lt;/head&gt;
    &lt;p&gt;Creating better interfaces, which today are driven by prompt engineering, is integral for developer productivity in the long run. “If it takes longer to explain to the system all the things you want to do and all the details of what you want to do, then all you have is just programming by another name,” says Solar-Lezama.&lt;/p&gt;
    &lt;p&gt;Shreya Kumar, a software engineer and an associate teaching professor in computer science at the University of Notre Dame who was not involved in the research, echoes the sentiment. “The reason we have a programming language is because we need to be unambiguous. But right now, we’re trying to adjust the prompt [in a way] that the tool will be able to understand,” she says. “We’re adapting to the tool, so instead of the tool serving us, we’re serving the tool. And it is sometimes more work than just writing the code.”&lt;/p&gt;
    &lt;p&gt;As the study notes, one way to address the dilemma of human-AI interaction is for AI systems to learn to quantify uncertainty and communicate proactively, asking for clarification or more information when faced with vague instructions or unclear scenarios. Sen adds that AI models might also be “missing context that I have in my mind as a developer—hidden concepts that are embedded in the code but hard to decipher from it. And if I give any hint to the LLM about what is happening, it might actually make better progress.”&lt;/p&gt;
    &lt;p&gt;For Abhik Roychoudhury, a professor of computer science at the National University of Singapore who was also not involved in the research, a crucial aspect missing from the paper and from most AI-backed software development tools entails capturing user intent.&lt;/p&gt;
    &lt;p&gt;“A software engineer is doing a lot of thinking in understanding the intent of the code. This intent inference—what the program is trying to do, what the program is supposed to do, and the deviation between the two—is what helps in a lot of software engineering tasks. If this outlook can be brought in future AI offerings for software engineering, then it will get closer to what the software engineer does.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Where Does AI Coding Go From Here?&lt;/head&gt;
    &lt;p&gt;Roychoudhury also assumes that many of the challenges identified in the paper are either being worked on now or “would be solved relatively quickly” due to the rapid pace of progress in AI for software engineering. Additionally, he believes that an agentic AI approach can help, viewing significant promise in AI agents for processing requirements specifications and ensuring they can be enforced at the code level.&lt;/p&gt;
    &lt;p&gt;“I feel the automation of software engineering via agents is probably irreversible. I would dare say that it is going to happen,” Roychoudhury says.&lt;/p&gt;
    &lt;p&gt;Sen is of the same view but looks beyond agentic AI initiatives. He pinpoints ideas such as evolutionary algorithms to enhance AI coding skills and projects like AlphaEvolve that employ genetic algorithms “to shuffle the solutions, pick the best ones, and then continue improving those solutions. We need to adopt a similar technology for coding agents, where the code is continuously improving in the background.”&lt;/p&gt;
    &lt;p&gt;However, Roychoudhury cautions that the bigger question lies in “whether you can trust the agent, and this issue of trust will be further exacerbated as more and more of the coding gets automated.”&lt;/p&gt;
    &lt;p&gt;That’s why human supervision remains vital. “There should be a check and verify process. If you want a trustworthy system, you do need to have humans in the loop,” says Notre Dame’s Kumar.&lt;/p&gt;
    &lt;p&gt;Solar-Lezama agrees. “I think it’s always going to be the case that we’re ultimately going to want to build software for people, and that means we have to figure out what it is we want to write,” he says. “In some ways, achieving full automation really means that we get to now work at a different level of abstraction.”&lt;/p&gt;
    &lt;p&gt;So while AI may become a “real coder” in the near future, Roychoudhury acknowledges that it probably won’t gain software developers’ complete trust as a team member, and thus might not be allowed to do its tasks fully autonomously. “That team dynamics—when an AI agent can become a member of the team, what kind of tasks will it be doing, and how the rest of the team will be interacting with the agent—is essentially where the human-AI boundary lies,” he says.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AMD Takes Holistic Approach to AI Coding Copilots ›&lt;/item&gt;
      &lt;item&gt;AI Copilots Are Changing How Coding Is Taught ›&lt;/item&gt;
      &lt;item&gt;The Best AI Coding Tools You Can Use Right Now ›&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Rina Diane Caballar is a writer covering tech and its intersections with science, society, and the environment. An IEEE Spectrum Contributing Editor, she's a former software engineer based in Wellington, New Zealand.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45065343</guid></item><item><title>Essential Coding Theory [pdf]</title><link>https://cse.buffalo.edu/faculty/atri/courses/coding-theory/book/web-coding-book.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45065705</guid></item><item><title>Wikipedia as a Graph</title><link>https://wikigrapher.com/paths</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45066060</guid></item><item><title>Offline-First Landscape – 2025</title><link>https://marcoapp.io/blog/offline-first-landscape</link><description>&lt;doc fingerprint="726893d26eaf7fde"&gt;
  &lt;main&gt;
    &lt;head rend="h6"&gt;Engineering&lt;/head&gt;
    &lt;head rend="h1"&gt;Offline-First Landscape&lt;/head&gt;
    &lt;head rend="h6"&gt;Isaac Hinman&lt;/head&gt;
    &lt;head rend="h6"&gt;|&lt;/head&gt;
    &lt;head rend="h6"&gt;Jan 11, 2025&lt;/head&gt;
    &lt;head rend="h3"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;When we set out to build Marco, we knew we were committing to two very difficult requirements: (1) IMAP-based, not API-based, and (2) cross-platform â web, Mac, Windows, Android, iOS.&lt;/p&gt;
    &lt;p&gt;We had a handful of additional ancillary requirements. One of these was offline-first, and I can now say confidently that we drastically underestimated its complexity.&lt;/p&gt;
    &lt;p&gt;I mentioned in a previous blog post that Missive was my daily driver in the recent past. It lacks offline support though, and this is one of the major downfalls of the product.&lt;/p&gt;
    &lt;p&gt;At Marco we believe that full offline support is crucial. "Managing your emails on an airplane with no wifi" is an example use case we frequently come back to. You should be able to read, delete, respond, and organise your emails with no internet connection. When you land and connect to wifi, everything should seamlessly sync.&lt;/p&gt;
    &lt;p&gt;That said, Marco is not a simple todo app. Marco is not an application that starts with zero data and grows in size gradually, as is the case with user-generated content like Notion, etc.&lt;/p&gt;
    &lt;p&gt;Marco is an application that deals with hundreds of MB of data, and hundreds of thousands (or millions) of rows/entities.&lt;/p&gt;
    &lt;p&gt;Essentially this means we are instantly jumping into the top 1% of heavy-duty use cases for offline-first implementations. Over time we realised that this actually rules out almost all available offline-first options.&lt;/p&gt;
    &lt;head rend="h3"&gt;Starting Point: WatermelonDB&lt;/head&gt;
    &lt;p&gt;I spent about a week deeply investigating the offline-first options available to us, in August 2024.&lt;/p&gt;
    &lt;p&gt;We (perhaps naively) had committed to the idea that our offline-first architecture should be database-agnostic â the offline-first logic should "end" at the API layer. We did not want to manage sync tables or schemas in Postgres â we wanted to write API endpoints, and manage our database ourselves.&lt;/p&gt;
    &lt;p&gt;Here's a rundown of the initial offline-first options we looked at:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;FOSS, self-hosted&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Database agnostic&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Been around for ages, used in many production applications&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Not quite FOSS, but has a free Self-Hosted Edition&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Requires Postgres-level integration&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Very complex architecture, requires both changes to Postgres _and_ a separate HA MongoDB deployment cluster&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Appears to have been around quite awhile, but all their case studies are on demo/tiny/side projects&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Looked interesting, but was in the middle of a complete rewrite&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Requires Postgres-level integration&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;New version only handles data sync one way â it does not handle mutations&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are many other options, including RxDB, MongoDB Atlas, Couchbase, and on and on. The three listed above are the options that we deeply investigated. As will become clear, we should have looked further at this stage.&lt;/p&gt;
    &lt;p&gt;We settled on WatermelonDB and built the initial alpha version of Marco on it. The backend implementation is rather simple: there is a "pull" endpoint to GET data, and a "push" endpoint to POST mutations.&lt;/p&gt;
    &lt;p&gt;It is important here to note that although Marco is a native application in some targets, it also must run in a web browser. While we may have access to a filesystem or "true" SQLite in native targets, our common denominator is web, where persistent storage options are very limited.&lt;/p&gt;
    &lt;p&gt;On the (web) frontend, Watermelon uses IndexedDB (as do essentially all other options â even the WASM SQLite options are usually SQLite-on-top-of-IndexedDB). However, it turns out Watermelon faces a serious problem that all other relational frontend databases face â IndexedDB performance is terrible. To solve this, Watermelon uses a LokiJS adapter, which is literally just an in-memory database.&lt;/p&gt;
    &lt;p&gt;Yes, you heard that right. To get around IndexedDB performance issues, Watermelon uses LokiJS to... hold the entire database in memory. When your database size is 100MB+, this starts to become a serious problem.&lt;/p&gt;
    &lt;p&gt;Moreover, clients must pull data before they can push any new mutations, and mutations can easily be clobbered if the row has been updated on the backend before the frontend can push mutations.&lt;/p&gt;
    &lt;p&gt;On top of this, WatermelonDB is not as actively-maintained as it once was. Many issues and PRs are left without a response. For example, chunked initial syncing is not supported out of the box. We opened a PR for this in early December, but it's still not been merged.&lt;/p&gt;
    &lt;p&gt;We got quite far along with the Marco alpha build, and then had a bit of a panic in November. Our confidence in our WatermelonDB-based offline-first approach was decreasing steadily. We began to seriously question if this technology could actually support a rock-solid, modern user experience.&lt;/p&gt;
    &lt;p&gt;We decided we needed to find something better.&lt;/p&gt;
    &lt;head rend="h3"&gt;New Wave of Offline-First&lt;/head&gt;
    &lt;p&gt;This time around, we threw out any preconceptions we had about Postgres, separation of concerns at the API layer, etc. We had completely open minds and desperately wanted to find the "best" solution, no matter what that might look like. We were now extremely clear on the fact that we had a "tough" offline-first use case, and needed some serious help.&lt;/p&gt;
    &lt;p&gt;We discovered a host of "new wave" offline first implementations. We talked with the founders/developers of these projects and found so many extremely intelligent and talented people working on what is a very tough problem.&lt;/p&gt;
    &lt;p&gt;The leaders in this new wave are:&lt;/p&gt;
    &lt;p&gt;While Triplit and InstantDB can be described as "full stack databases", Convex is instead an entire backend solution, including API endpoints, etc. For this reason we excluded Convex, as it seems like a huge leap and essentially 100% lock-in.&lt;/p&gt;
    &lt;head rend="h3"&gt;Problems with Triplit&lt;/head&gt;
    &lt;p&gt;Our first (of several) rewrites was from WatermelonDB to Triplit. Both Triplit and InstantDB use triples to represent data â entities are stored as "triples" of (&amp;lt;entity id&amp;gt;, &amp;lt;entity field&amp;gt;, &amp;lt;entity value&amp;gt;). Such a representation makes syncing a straightforward affair.&lt;/p&gt;
    &lt;p&gt;Triplit's API and DX is best-in-class. However, although we desperately wanted to love the product, we found it unusable for our use case. The server-side implementation was eating gigabytes of RAM while sitting idle and would regularly OOM/crash. The client-side triples implementation would balloon 5MB of JSON into 1GB of SQLite.&lt;/p&gt;
    &lt;p&gt;We believe Triplit is a fantastic choice for any offline-first applications with relatively small storage needs. On top of this, we found the Triplit team to be incredibly talented and hard-working, and truly believe that by mid-2025, it will be a robust and capable product.&lt;/p&gt;
    &lt;p&gt;We absolutely love the developer experience with Triplit, and are rooting for the team to succeed!&lt;/p&gt;
    &lt;p&gt;But we need something which is reliable, highly performant, and battle-tested, now.&lt;/p&gt;
    &lt;head rend="h3"&gt;Problems with InstantDB&lt;/head&gt;
    &lt;p&gt;We next moved onto InstantDB, which can be considered a direct competitor to Triplit. Although both InstantDB and Triplit essentially solve the same problem, we found InstantDB to be a far worse implementation.&lt;/p&gt;
    &lt;p&gt;TypeScript types were non-existent. There was no sort/ordering by fields. There was no support for $like operators, and certainly no full text search. I believe some of these features have since been added â both teams are certainly scrambling to handle a million features and requests.&lt;/p&gt;
    &lt;p&gt;On the backend side of things, there are no webhooks, so it is impossible to respond to mutations in a scalable way. We would have had to build our own singleton subscriber microservice that then translated reactive queries into scalable webhooks. But reactive queries themselves are prone to dropped data, so we would need some sort of polling fallback... The backend story for InstantDB feels extremely incomplete.&lt;/p&gt;
    &lt;p&gt;Even looking past all of this, frontend queries that returned in 2-5ms with Watermelon+LokiJS were taking 200-500ms to return data with InstantDB. This is primarily because InstantDB is not optimistic, and was hitting network to fetch data with almost every request. There is no granular control as to what gets cached on the client side and what does not.&lt;/p&gt;
    &lt;p&gt;InstantDB is another promising product, and our understanding is that some teams are already building production applications on top of it. But it's simply too immature for our use case right now. There's essentially zero backend support, and the frontend UX felt like a massive downgrade coming from Watermelon.&lt;/p&gt;
    &lt;head rend="h3"&gt;Problems with PowerSync&lt;/head&gt;
    &lt;p&gt;Finally, to our great disappointment, we begrudgingly moved on to PowerSync. We were wary of PowerSync from the beginning, and our reluctance proved to be well-founded.&lt;/p&gt;
    &lt;p&gt;Although PowerSync is undoubtedly a mature product, and probably the most capable (on paper) of all options mentioned thus far, we hated every minute of working with it. The underlying tech might be the most production/enterprise friendly, but the DX is the worst by quite a margin.&lt;/p&gt;
    &lt;p&gt;There is a paid SaaS offering, but their pricing model would have made our use case prohibitively expensive. Therefore we needed to self-host PowerSync, which is quite a complex and expensive task in itself. Not only does it require Postgres-level integration, it also needs a HA MongoDB cluster, a lot of arcane yaml configuration, etc. It also required us to completely denormalise our Postgres tables, as relations are not properly supported across the sync buckets.&lt;/p&gt;
    &lt;p&gt;On the frontend side of things, PowerSync runs SQLite in WASM, and although the DX is fairly good, we found horrifying problems like off-by-one bugs in their Drizzle ORM integration, queries returning data from local db very slowly (100ms+), and long initialisation times (45s+ after login to Marco) with the UI view not updating until the sync fully completed.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why So Many Problems?&lt;/head&gt;
    &lt;p&gt;There is a saying: if everyone is an asshole, maybe you're the asshole. Is the problem 5+ offline-first tools, or is it us?&lt;/p&gt;
    &lt;p&gt;Like with most things, the reality is "a bit of both". As mentioned, Marco is an incredibly data-intensive application, and from day one, we were pushing these offline-first tools to their absolute limits.&lt;/p&gt;
    &lt;p&gt;However, we also found the practical limits of these tools to be way lower than one would expect.&lt;/p&gt;
    &lt;p&gt;What is the underlying cause? In my estimation, the root cause is that all of these offline-first tools for web are essentially hacks. Because of Marco's web deployment target, which becomes our common denominator, we must support offline-first in a web browser, and web browsers only really support KV storage via IndexedDB.&lt;/p&gt;
    &lt;p&gt;All attempts to implement relational or graph databases within a web browser are essentially hacks. PowerSync itself is WASM SQLite... On top of IndexedDB. Binary SQLite chunks are literally stored in IndexedDB.&lt;/p&gt;
    &lt;p&gt;There are essentially three different variables:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The underlying (true) data store â this will always be IndexedDB for web implementations&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;How the data is represented for sync purposes&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;How the data is presented to developers&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The new wave of tools are attempting triples/graph implementations, but the story is the same. Browsers only give you a KV API, and anything on top of that will be built in userland and will suffer poor performance once you hit a certain scale. Although triples are easy to store and sync, when you try to jam a relational layer on top, the whole thing starts to fall apart.&lt;/p&gt;
    &lt;p&gt;To be absolutely clear: these relational and graph implementations on top of IndexedDB do not start to show their cracks unless you have 10s/100s of MB of data, or hundreds of thousands of rows/entities. But at a certain scale, performance grinds to a halt and they become unusable. For lesser use cases, Triplit and InstantDB offer exceptional DX and velocity, with almost no drawbacks.&lt;/p&gt;
    &lt;p&gt;At this point, we were starting to pull our hair out, and were wondering if we needed to build our own sync engine. Like many others, we're highly impressed with Linear, but are also aware that their sync engine was a monumental engineering effort.&lt;/p&gt;
    &lt;p&gt;We're only a team of two, and we have a lot to work on besides offline-first itself.&lt;/p&gt;
    &lt;head rend="h3"&gt;Finally, A Solution&lt;/head&gt;
    &lt;p&gt;Some time in early December, I came back across an option which I had glanced over before, but disregarded: Replicache.&lt;/p&gt;
    &lt;p&gt;I think we were initially put off by their strange pricing model and the fact that it's closed-source.&lt;/p&gt;
    &lt;p&gt;I am so glad we took another look.&lt;/p&gt;
    &lt;p&gt;In terms of backend implementation, Replicache is somewhat similar to WatermelonDB, in that you need to implement push and pull endpoints in your backend, and it is otherwise entirely database-agnostic.&lt;/p&gt;
    &lt;p&gt;The frontend is where the crucial difference lies â Replicache is just a KV store. It is a thin layer on top of IndexedDB that adds reactivity and some querying DX. That's it. You get raw &lt;code&gt;get&lt;/code&gt; and &lt;code&gt;set&lt;/code&gt; performance. Some performance benchmarks are outlined here. The perf is truly remarkable.&lt;/p&gt;
    &lt;p&gt;The drawback to this KV approach is that searching/sorting/ordering entities would require scanning through entire collections. This is obviously a non-starter. In other words, if we wanted to use Replicache, we would need to handle indexing and search on our own.&lt;/p&gt;
    &lt;p&gt;We'll post more detailed write-ups on our tech stack in the future, but a quick summary of where we landed on the frontend is: Replicache + Orama. This gives us sophisticated and battle-tested data sync with conflict resolution and rebasing, but also extremely flexible and powerful indexing, full-text search, and more.&lt;/p&gt;
    &lt;p&gt;At the time of writing (January 2025), the Replicache team have just made it completely free and open source. This is because they've just released Zero, which looks extraordinarily compelling and will likely jump into the #1 spot for any offline-first product available.&lt;/p&gt;
    &lt;p&gt;We're eager to try Zero once it's a bit more stable, but for now will build our product on the extremely capable and robust piece of software that is Replicache.&lt;/p&gt;
    &lt;head rend="h3"&gt;Future of Offline-First&lt;/head&gt;
    &lt;p&gt;We embarked on a long and rambling journey through essentially all prior art and work in the offline-first world. It's a very hard problem to solve.&lt;/p&gt;
    &lt;p&gt;The good news is that there are many new teams and projects actively and energetically working on this problem.&lt;/p&gt;
    &lt;p&gt;Imagine a world where, as a fullstack developer, you can read and write data from an SDK in both your backend and your frontend, and they magically sync with each other. All your apps are instantly responsive. All your apps work offline out of the box.&lt;/p&gt;
    &lt;p&gt;This is already possible today with Triplit or InstantDB, if your use case is reasonable. And things are only improving.&lt;/p&gt;
    &lt;p&gt;I believe 2025 will be a year where HTTP/REST APIs will start to feel antiquated. Don't share endpoints â share databases.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45066070</guid></item><item><title>Bourbaki – A Secret Society of Mathematicians</title><link>https://books.google.com/books/about/Bourbaki.html</link><description>&lt;doc fingerprint="5b2e48cf691c42a2"&gt;
  &lt;main&gt;
    &lt;p&gt;Sign in Hidden fields Books Search the world's most comprehensive index of full-text books. My library&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45066113</guid></item><item><title>The web does not need gatekeepers: Cloudflare’s new “signed agents” pitch</title><link>https://positiveblue.substack.com/p/the-web-does-not-need-gatekeepers</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45066258</guid></item><item><title>Thunder Compute (YC S24) Is Hiring</title><link>https://www.ycombinator.com/companies/thunder-compute/jobs/sS6QzTi-founding-developer-advocate-contract-to-hire</link><description>&lt;doc fingerprint="50876e585e37f227"&gt;
  &lt;main&gt;
    &lt;p&gt;Launch GPU instances in one click for 80% less&lt;/p&gt;
    &lt;p&gt;Thunder Compute is the cheapest, easiest GPU cloud for developers. We’re a 4-person, seed-funded team (approaching Series A) with 100%+ MoM growth. 100% in-person, 6 days per week in SF. Our virtualization stack exposes network-attached GPUs over TCP, letting us oversubscribe hardware and pass savings to users.&lt;/p&gt;
    &lt;p&gt;Own DevRel end-to-end. You’ll build and grow our community, ship hands-on demos and templates, teach developers how to run real workloads on Thunder, and bring sharp product feedback straight from Discord to the roadmap. High autonomy, high impact. You’ll report to the CEO.&lt;/p&gt;
    &lt;p&gt;Full-time salary + meaningful equity. Starts as a 2-month contract, in-person optional, will full-time returning 100% in-person. You’ll work closely with the founding team and help define our DevRel function from zero.&lt;/p&gt;
    &lt;p&gt;Send 2–3 links (writing/talks/demos), plus a short note on a community initiative you led and its outcome when you apply.&lt;/p&gt;
    &lt;p&gt;Thunder Compute is the world's cheapest GPU cloud platform for researchers, data scientists, and indie devs. The team is building a proprietary virtualization stack to improve GPU utilization by 5x. Try it today.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45066592</guid></item><item><title>SQLite's Durability Settings Are a Mess</title><link>https://www.agwa.name/blog/post/sqlite_durability</link><description>&lt;doc fingerprint="b0517adf2ea79b48"&gt;
  &lt;main&gt;
    &lt;p&gt;August 29, 2025&lt;/p&gt;
    &lt;head rend="h2"&gt;SQLite's Durability Settings are a Mess&lt;/head&gt;
    &lt;p&gt;One of the most important properties of a database is durability. Durability means that after a transaction commits, you can be confident that, absent catastrophic hardware failure, the changes made by the commit won't be lost. This should remain true even if the operating system crashes or the system loses power soon after the commit. On Linux, and most other Unix operating systems, durability is ensured by calling the fsync system call at the right time.&lt;/p&gt;
    &lt;p&gt;Durability comes at a performance cost, and sometimes applications don't need durability. Some applications can tolerate losing the last several seconds of commits in the event of a power failure, as long as the database doesn't end up corrupted. Thus, databases typically provide knobs to configure if and when they call fsync. This is fine, but it's essential that the database clearly documents what its default durability properties are, and what each configuration setting guarantees.&lt;/p&gt;
    &lt;p&gt;Unfortunately, SQLite's documentation about its durability properties is far from clear. I cannot tell whether SQLite is durable by default, and if not, what are the minimal settings you need to use to ensure durability.&lt;/p&gt;
    &lt;p&gt;The two relevant configuration options are &lt;code&gt;journal_mode&lt;/code&gt; and &lt;code&gt;synchronous&lt;/code&gt;.  &lt;code&gt;journal_mode&lt;/code&gt; has several possible values, but most people use either DELETE or WAL.  &lt;code&gt;synchronous&lt;/code&gt; has four possible values: EXTRA, FULL, NORMAL, and OFF.
&lt;/p&gt;
    &lt;p&gt;This is how I interpret SQLite's documentation after a careful reading:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The default value of&lt;/p&gt;&lt;code&gt;journal_mode&lt;/code&gt;is DELETE:&lt;quote&gt;The DELETE journaling mode is the normal behavior (source; archived)&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The default value of&lt;/p&gt;&lt;code&gt;synchronous&lt;/code&gt;is FULL:&lt;quote&gt;If not overridden at compile-time, the default setting is 2 (FULL) (source; archived)&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The default value of&lt;/p&gt;&lt;code&gt;synchronous&lt;/code&gt;is FULL even in WAL mode:&lt;quote&gt;If not overridden at compile-time, this value is the same as SQLITE_DEFAULT_SYNCHRONOUS. (source; archived)&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;When&lt;/p&gt;&lt;code&gt;journal_mode&lt;/code&gt;is DELETE, you need to set&lt;code&gt;synchronous&lt;/code&gt;to EXTRA to get durability:&lt;quote&gt;EXTRA synchronous is like FULL with the addition that the directory containing a rollback journal is synced after that journal is unlinked to commit a transaction in DELETE mode. EXTRA provides additional durability if the commit is followed closely by a power loss. (source; archived)&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;When&lt;/p&gt;&lt;code&gt;journal_mode&lt;/code&gt;is WAL, FULL is sufficient for durability:&lt;quote&gt;With synchronous=FULL in WAL mode, an additional sync operation of the WAL file happens after each transaction commit. The extra WAL sync following each transaction helps ensure that transactions are durable across a power loss (source; archived)&lt;/quote&gt;&lt;p&gt;Note that this is not mentioned under the definition of FULL, but rather further down in the documentation for&lt;/p&gt;&lt;code&gt;synchronous&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Based on the above, I conclude that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;By default, SQLite is not durable, because the default value of&lt;/p&gt;&lt;code&gt;journal_mode&lt;/code&gt;is DELETE, and the default value of&lt;code&gt;synchronous&lt;/code&gt;is FULL, which doesn't provide durability in DELETE mode.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;If you change&lt;/p&gt;&lt;code&gt;journal_mode&lt;/code&gt;to WAL, then SQLite is durable, because&lt;code&gt;synchronous=FULL&lt;/code&gt;provides durability in WAL mode.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;However, a recent Hacker News comment by a user who credibly claims to be Richard Hipp, the creator of SQLite, says:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;"In its default configuration, SQLite is durable."&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;"If you switch to WAL mode, the default behavior is that transactions ... are not necessarily durable across OS crashes or power failures"&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That's literally the opposite of what the documentation seems to say!&lt;/p&gt;
    &lt;p&gt;A Hacker News commenter who agrees with my reading of the documentation asked Hipp how his comment is consistent with the documentation, but received no reply.&lt;/p&gt;
    &lt;p&gt;Hipp also says that WAL mode used to be durable by default, but it was changed after people complained about poor performance. This surprised me, since I had the impression that SQLite cared deeply about backwards compatibility, and weakening the default durability setting is a nasty breaking change for any application which needs durability.&lt;/p&gt;
    &lt;p&gt;There are a couple other pitfalls around SQLite durability that you should be aware of, though I don't necessarily blame the SQLite project for these:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Libraries that wrap SQLite can override the default value of&lt;/p&gt;&lt;code&gt;synchronous&lt;/code&gt;. For example, the most popular Go driver for SQLite sets it to NORMAL when in WAL mode, which does not provide durability.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;On macOS, fsync is nerfed to make macOS appear faster. If you want a real fsync, you have to make a different, macOS-specific system call. SQLite can do this, but it's off by default.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt; My takeaway is that if you need durability, you'd better set the &lt;code&gt;synchronous&lt;/code&gt; option explicitly because who knows what the default is, or what it will be in the future.  With WAL mode, FULL seems to suffice.  As for DELETE mode, who knows if FULL is enough, so you'd better go with EXTRA to be safe. And if your application might be used on macOS, enable &lt;code&gt;fullfsync&lt;/code&gt;.
&lt;/p&gt;
    &lt;p&gt; The SQLite project ought to clarify their documentation. Since the meaning of &lt;code&gt;synchronous&lt;/code&gt; depends on the value of &lt;code&gt;journal_mode&lt;/code&gt;, I think it would be quite helpful to document the values of &lt;code&gt;synchronous&lt;/code&gt; separately for each possible &lt;code&gt;journal_mode&lt;/code&gt;, rather than mixing it all together.  A table with &lt;code&gt;synchronous&lt;/code&gt; values on one axis and &lt;code&gt;journal_mode&lt;/code&gt; on the other which tells you if the combination provides durability would do wonders.
&lt;/p&gt;
    &lt;p&gt;By the way, there are definitely many applications for which losing a few seconds of data in exchange for better performance is a great tradeoff, which is why SQLite and macOS have made the choices they have made. But programmers need to know what guarantees their tools provide, which is why unclear documentation and breaking previously-held assumptions is not cool.&lt;/p&gt;
    &lt;head rend="h3"&gt;Comments&lt;/head&gt;
    &lt;p&gt;No comments yet.&lt;/p&gt;
    &lt;head rend="h3"&gt;Post a Comment&lt;/head&gt;
    &lt;p&gt;Your comment will be public. To contact me privately, email me. Please keep your comment polite, on-topic, and comprehensible. Your comment may be held for moderation before being published.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45066999</guid></item><item><title>Data engineering and software engineering are converging</title><link>https://clickhouse.com/blog/eight-principles-of-great-developer-experience-for-data-infrastructure</link><description>&lt;doc fingerprint="635bb13bdfb753fa"&gt;
  &lt;main&gt;&lt;quote&gt;&lt;p&gt;TL;DR:&lt;/p&gt;&lt;lb/&gt;· If you’re an engineer building realtime analytics or AI-powered features, you need the right data infrastructure coupled with the right developer experience (DX).&lt;lb/&gt;· A great DX for data infrastructure should empower both software devs and data engineers, while taking inspiration from the best of modern web development (git-native, local-first, everything as code, CI/CD friendly, etc).&lt;lb/&gt;· MooseStack by 514 offers a fully open source implementation of a DX layer for ClickHouse.&lt;/quote&gt;&lt;p&gt;Data engineering and software engineering are converging.&lt;/p&gt;&lt;p&gt;For years, data infrastructure was built for analysts. Warehouses, lakes, BI dashboards—all SQL-first, point-and-click workflows. But today, analytics isn’t just about reporting or data science. Real-time data is at the center of modern user experiences and AI-readiness. SaaS apps are surfacing analytics and AI directly in their UX to drive adoption, engagement, and retention. Enterprises are accelerating their business with AI-powered automations for faster insights, predictions, and operations.&lt;/p&gt;&lt;p&gt;Engineering teams are on the hook to ship data-backed functionality with the same discipline as any other application code. If you’re coming from the software engineering world, you probably start with a transactional database like Postgres, MySQL or Mongo. The tooling is great, and the developer experience is mature—but those systems are built for transactions, not analytics. As cardinality and scan sizes grow, queries bog down. Dashboards spin. AI chat slows to a crawl.&lt;/p&gt;&lt;p&gt;Alternatively, if you’re coming from the data engineering world, you’re probably on managed analytics platforms like Snowflake, BigQuery, or Databricks. These work well for batch ETL and reporting, but fall short when you need freshness, concurrency, or sub-second response times. They’re also full of rough edges for developers. You don’t get a real local environment. Iteration cycles are slow. They’re just not built for the modern software development lifecycle.&lt;/p&gt;&lt;p&gt;So we’re left with a gap—and it’s two-fold: user experience (UX) and developer experience (DX).&lt;/p&gt;&lt;head rend="h3"&gt;1. The user experience (UX) gap #&lt;/head&gt;&lt;p&gt;End users want sub-second analytics at application scale. Enter ClickHouse. ClickHouse offers best in class performance on analytical queries—orders of magnitude faster than transactional databases like Postgres, and many times faster and more cost efficient than cloud data warehouses like Snowflake or Databricks. That means snappy dashboards and conversation-speed AI chat for your UX.&lt;/p&gt;&lt;head rend="h3"&gt;2. The developer experience (DX) gap #&lt;/head&gt;&lt;p&gt;Engineers need the same kind of safe, tight iteration loop that’s been taken for granted in web development for twenty years. Interestingly, ClickHouse can run efficiently at any scale, from megabytes to petabytes of data, and can adapt to any type of deployment—from serverless functions, to a container on your laptop, to a cluster of thousands of servers working together. This makes ClickHouse uniquely suited for both a local-first development workflow and massive production scale. But how to build on ClickHouse’s power and flexibility with a full-blown, modern, open source developer experience? Enter MooseStack.&lt;/p&gt;&lt;head rend="h2"&gt;Adding a modern software DX to ClickHouse #&lt;/head&gt;&lt;p&gt;A great data &amp;amp; analytics DX should serve both (1) data engineers leaning into software development best practices, and (2) software engineers leaning into data, analytics and AI. And it should take inspiration from the most innovative tools driving the modern web development experience—like Ruby on Rails, Next.js, TanStack, and Supabase.&lt;/p&gt;&lt;p&gt;In short, a great data &amp;amp; analytics DX should embrace the following core principles:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Git-based version control &amp;amp; governance&lt;/item&gt;&lt;item&gt;Local-first development&lt;/item&gt;&lt;item&gt;Native programming languages (not YAML)&lt;/item&gt;&lt;item&gt;Infrastructure boilerplate abstractions&lt;/item&gt;&lt;item&gt;Horizontal integration, with modularization&lt;/item&gt;&lt;item&gt;Open source native&lt;/item&gt;&lt;item&gt;AI copilot native&lt;/item&gt;&lt;item&gt;Transparent migrations &amp;amp; integrated CI/CD&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The remainder of this post explores each of these principles in more detail, while also referencing how they are implemented with MooseStack by 514—an open source developer toolkit for building TypeScript or Python apps on ClickHouse and other open source data infrastructure.&lt;/p&gt;&lt;head rend="h2"&gt;1. Git-based version control and governance #&lt;/head&gt;&lt;p&gt;Version control systems like git are at the heart of the modern software development lifecycle. Make changes, track changes, collaborate on code, etc. This is the norm for any software developer, but not so much with many data &amp;amp; analytics platforms, with cloud-based GUIs and point and click interfaces. And if there is code, it’s not always easy to integrate with git—expecially with browser-based code editors, and heavy-handed cloud-to-local-to-cloud workflows.&lt;/p&gt;&lt;p&gt;A great developer experience should be grounded in a code base that is easily tracked and managed with git.&lt;/p&gt;&lt;p&gt;With MooseStack libraries and tools, you can build your entire user-facing analytics app, or even your data warehouse, in native Typescript or Python, with git integration natively supported - it’s just code.&lt;/p&gt;git-based version control means audit trails for your data contracts&lt;head rend="h2"&gt;2. Local dev experience #&lt;/head&gt;&lt;quote&gt;&lt;p&gt;“Data engineering shouldn’t have to trail software development by a decade or more when it comes to developer experience. MooseStack brings the tools and abstractions that you expect from a modern developer framework.”&lt;/p&gt;&lt;lb/&gt;– Pardhu Gunnam, CEO/Creator of Metaphor Data&lt;/quote&gt;&lt;p&gt;The best developer experience for you is in your IDE of choice, not in a browser tab. Web development figured this out a long time ago. You don’t “yolo” changes directly into a live server. You spin up a dev environment on your laptop that mirrors production. You create a branch, make changes, and immediately see what broke in a live preview of your application. Your build logs tell you if you’ve stranded an import or introduced a syntax error. You have a safe environment to freely experiment and break things, knowing that the worst thing that can happen is your code gets into an error state and you just kill the branch and start a fresh one. By the time you merge, you’ve had multiple layers of validation and review. That workflow gives you confidence. You can see exactly how your changes impact the entire system before they ever hit production.&lt;/p&gt;&lt;p&gt;A great developer experience should provide an isolated, production-like environment to freely experiment and immediately see what breaks in a live preview of your application.&lt;/p&gt;&lt;p&gt;With MooseStack, local development is first-class. “Modern” data platforms tend to be large, distributed, and cloud native—Snowflake and BigQuery won’t run on your laptop. But ClickHouse and other next-gen data infrastructure (like Redpanda, for example) can run locally in a container. MooseStack’s local dev server runs your entire analytics stack in one CLI command: &lt;code&gt;moose dev&lt;/code&gt;. Combine that with git-native development, and you’ve got a DX where you can create a branch off main, pull it down, run Moose dev, and all your models running in production are instantly materialized in a local ClickHouse instance for development. Seed it with sample data, and your dev server gives you the full loop—ingest, transform, aggregate, serve—hot-reloaded as you edit code. The same APIs your app calls in production are live in development, so your feature work is always exercising the real data paths and pipelines.&lt;/p&gt;&lt;head rend="h2"&gt;3. Native programming languages (not YAML) #&lt;/head&gt;&lt;p&gt;In our previous post, we dug into the question, “Does OLAP need an ORM?”. Traditional ORMs can sometimes cause more harm than good, e.g., with leaky abstractions, or by hiding SQL performance implications. But the core idea is worth keeping: modeling tables as objects in application code. That pattern gives you type-safety, IDE auto-completion, and immediate visibility when a change in your schema layer breaks an API in your app layer (or the other way around). In web development, if you change a prop in a React component, your IDE and dev server immediately show you which pages are broken. Analytics deserves the same feedback loop.&lt;/p&gt;&lt;p&gt;For many teams, YAML-based DSLs are the first step toward treating data as code. That’s progress: your schema definitions are at least version-controlled and reviewable, instead of being declared directly into a live database. But YAML is a configuration file format, not a programming language. It can’t express complex business logic—no IF statements, no loops, no variables—so non-trivial transformations end up pushed into shell scripts, SQL fragments, or proprietary templating. The result is fragmentation: schemas live in one place, pipelines in another, and there’s no way to reason about them together at compile time.&lt;/p&gt;&lt;p&gt;A great developer experience should leverage the full capabilities of application programming languages, with schemas represented as native types in the same language where you write your application and pipeline logic.&lt;/p&gt;&lt;code&gt;interface DataModel {
columnName: Key&amp;lt;string&amp;gt;;
	secondColumnName: Date;
}

export const my_table = new OlapTable&amp;lt;DataModel&amp;gt;("table_name")
&lt;/code&gt;&lt;p&gt;With MooseStack, everything lives in Typescript or Python: schemas, pipelines, transforms, APIs—all versioned in your repo alongside application logic. Yes, some transformations are still expressed in SQL, but the SQL isn’t floating around as raw strings. It’s written inside language-native templates that reference typed schema objects. Rename a column in your TypeScript interface or Python class, and Moose updates the underlying ClickHouse schema (Moose OLAP) and immediately flags every SQL fragment, stream (Moose Streaming), pipeline (Moose Workflows), or API (Moose APIs) that depends on it. You’re still writing real SQL—but with the safety net and ergonomics of a programming language.&lt;/p&gt;&lt;code&gt;const result = await client.query.execute(sql`
      SELECT 
        ${my_table.columnName} as my_column,
        COUNT(*) as total_records,
      FROM ${my_table} 
      GROUP BY ${my_table.columnName} 
`);
&lt;/code&gt;&lt;p&gt;The payoff is that schemas and pipelines evolve together. Changes are surfaced instantly in your IDE and in your dev loop, not hours later in production. You keep SQL where it belongs—as the lingua franca of analytics—but ground it in the same typed codebase as the rest of your application.&lt;/p&gt;&lt;head rend="h2"&gt;4. Infrastructure boilerplate abstractions #&lt;/head&gt;&lt;quote&gt;&lt;p&gt;“MooseStack abstracts away all the annoying boilerplate, and gives me simple, intuitive primitives to build with, and a local dev server to iterate on.”&lt;/p&gt;&lt;lb/&gt;– David Der, Chief AI officer, SingleStone&lt;/quote&gt;&lt;p&gt;Boilerplate infrastructure code is the worst. You have to get it right, or everything breaks, but it’s hard to get right. And more often than not, there is a best practice way to do it that covers 90%+ of use cases. This is the perfect scenario for abstraction. In modern web development, you don’t configure your router from scratch—you use next.js’s router or TanStack Router, and you use their elegant abstractions. No need to reinvent the wheel every time.&lt;/p&gt;&lt;p&gt;A great developer experience should abstract away commonly used boilerplate code for infrastructure best practices.&lt;/p&gt;&lt;p&gt;Data infrastructure is full of examples like this. Buffering streaming events for batch writes to the database. Runtime data validation and dead letter queueing on data ingestion. Structuring tables for advanced materialized views. MooseStack provides simple abstractions in TypeScript/Python for each of these. So you can focus on the unique business logic of your application, instead of the data infrastructure glue and duct tape.&lt;/p&gt;&lt;code&gt;export const FooPipeline =
 new IngestPipeline&amp;lt;FooDataModel&amp;gt;("myFooPipeline", {
   table: true,
   stream: true,
   api: true,
 });
&lt;/code&gt;&lt;p&gt;For example, MooseStack’s IngestPipeline object automatically wires up a complete ingest pipeline, typed to a particular data model. This includes:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;An ingest API with runtime data validation, automatic OpenAPI documentation, and optional dead letter queuing&lt;/item&gt;&lt;item&gt;A Redpanda/Kafka streaming buffer with at least once delivery and optional streaming transformations&lt;/item&gt;&lt;item&gt;A ClickHouse table with writes automatically batched to maximize performance&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;5. Horizontal integration, with modularization #&lt;/head&gt;&lt;quote&gt;&lt;p&gt;“MooseStack brings together all the modules needed for building end-to-end data services into a simple unified dev framework.”&lt;/p&gt;&lt;lb/&gt;– Scott Haines, Distinguished Software Engineer, Fortune 100 Brand&lt;/quote&gt;&lt;p&gt;Modern web frameworks like Next.js show the power of horizontal integration by bundling routing, rendering, APIs, and deployment into a seamless developer experience. At the same time, tools like TanStack highlight why modularization matters—providing composable, swappable pieces that work across frameworks without lock-in. Data infrastructure tends to be particularly piecemeal, with a sprawling landscape of services surrounding the core database, including streaming, orchestration, connectors, transformations, catalogues, etc.&lt;/p&gt;&lt;p&gt;A great developer experience should leverage integrated workflows for speed, and modular building blocks for long-term flexibility.&lt;/p&gt;&lt;p&gt;MooseStack offers a variety of modules with developer abstractions for all the core parts of a standard analytical backend. These modules can be used independently and swapped out for alternative solutions. For example, you could use the Moose OLAP module to manage your ClickHouse deployment, paired with ClickHouse Cloud’s ClickPipes to bring data in, and FastAPI to layer Python APIs on top. Or if you’ve got an existing ClickHouse cluster, and need to add data from a bespoke source, you could just use the Moose Workflows module to create a custom data connector and pipeline in Typescript. But as you adopt MooseStack across more of your analytical backend, you get the benefits of a unified end-to-end abstraction layer, shared data models and a consistent local development experience.&lt;/p&gt;MooseStack modules and tooling can be used individually, or combined for an end-to-end experience&lt;head rend="h2"&gt;6. Open source native #&lt;/head&gt;&lt;p&gt;The modern web development experience is built on open source frameworks and technologies. Open source tooling reduces vendor lock-in, increases trust and security, encourages innovation, and keeps control in the hands of the developer. ClickHouse is, of course, open source. So why would you want to wrap it in a closed-source developer experience that locks you in to a particular vendor?&lt;/p&gt;&lt;p&gt;A great developer experience should be grounded in open source tooling, offering flexibility and transparency.&lt;/p&gt;&lt;p&gt;MooseStack is open source and MIT-licensed, and integrates with the rest of your open source software stack, including:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Data infrastructure: ClickHouse, Kafka, RedPanda, Temporal, Iceberg, Delta Lake, etc&lt;/item&gt;&lt;item&gt;Full stack frameworks: Next.js, Remix, TanStack, etc&lt;/item&gt;&lt;item&gt;Micro frameworks: Flask, fastAPI, Fastify, etc&lt;/item&gt;&lt;item&gt;API standards: OpenAPI, etc&lt;/item&gt;&lt;item&gt;Language Runtimes: Node and Python&lt;/item&gt;&lt;item&gt;Frontend Clients: React, TanStack Query, Streamlit, etc&lt;/item&gt;&lt;item&gt;Transactional ORMs: Prisma, Drizzle, etc&lt;/item&gt;&lt;item&gt;Typing and data validation: Typia, Pydantic, etc&lt;/item&gt;&lt;item&gt;Libraries: all your favorite TS and Python libraries can be imported&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;7. AI copilot native #&lt;/head&gt;&lt;quote&gt;&lt;p&gt;“Arming our full stack engineers with MooseStack and Sloan AI agents puts data engineering in the ‘full stack’.”&lt;/p&gt;&lt;lb/&gt;– C. Rodes Boyd, Bracket Real Estate&lt;/quote&gt;&lt;p&gt;Whatever your client of choice, you probably have some kind of copilot helping write code, or even an agent creating entire applications. LLM-powered copilots and agents tend to be pretty good at creating web apps (not perfect of course, but moving quickly in the right direction). The performance here is powered by:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Tons of examples to learn from&lt;/item&gt;&lt;item&gt;Tons of frameworks and abstractions to reduce the complexity and the surface area of interaction&lt;/item&gt;&lt;item&gt;Great local dev experience to iterate quickly to functional output&lt;/item&gt;&lt;/list&gt;&lt;p&gt;It turns out, the same things that make a great developer experience for human developers, also make a great developer experience for LLMs and agentic developers.&lt;/p&gt;&lt;p&gt;MooseStack is designed from the ground up to be agentic coder friendly, including:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Following familiar patterns from the transactional world that LLMs are comfortable with&lt;/item&gt;&lt;item&gt;Offering abstractions and reduced surface area to constrain LLMs to viable solutions&lt;/item&gt;&lt;item&gt;Offering great local dev experience to quickly iterate in&lt;/item&gt;&lt;/list&gt;&lt;p&gt;And if you want to supercharge your coding co-pilot, Fiveonefour’s Sloan AI (from the creators of MooseStack) offers agents and tools trained specifically on MooseStack to boost your developer experience further.&lt;/p&gt;Using Sloan’s MCP integration with Cursor to create (and immediately test locally!) a new API endpoint on ClickHouse&lt;head rend="h2"&gt;8. Transparent migrations &amp;amp; integrated CI/CD #&lt;/head&gt;&lt;p&gt;Of course, development doesn’t stop when you push a commit—you need to get to production. In web development, deployment has matured: innovations like automated governance and CI/CD pipelines in GitHub, and preview branches in Vercel and Supabase, give developers confidence that their production deployments won’t break. That confidence comes from transparency: you can see exactly what will happen before code hits prod, and you know you can safely roll it back if something goes wrong.&lt;/p&gt;&lt;p&gt;A great developer experience should bring confidence and transparency to production deployment for data systems.&lt;/p&gt;&lt;p&gt;With data backends, the stakes are even higher. Shipping a half-applied migration isn’t like breaking a web page—it can corrupt or orphan critical datasets. Analytical/OLAP systems are especially fragile here: schema changes are often non-transactional, meaning there’s no easy rollback. A failed &lt;code&gt;ALTER&lt;/code&gt; can leave a table in limbo, requiring you to write and apply your own reverse mutations by hand. Plus, in systems where analysts or external pipelines can also mutate schemas, drift between your code and the live database is common.&lt;/p&gt;&lt;p&gt;MooseStack addresses this head-on with Moose Migrate. Before deploying to production, MooseStack diffs your code against the live schema and generates a migration plan to apply to your production database to update schemas and business logic. You can also generate the plan in advance, to review, edit, and version control your migration. Either way, when you go to deploy, if drift has crept in between your code and the live database state, the migration fails fast, rather than shipping a broken deployment. The result is your application code and schema changes always ship together, in sync.&lt;/p&gt;&lt;p&gt;If you want to go further, Fiveonefour’s Boreal (from the creators of MooseStack) can host and manage your ClickHouse cluster on top of ClickHouse Cloud, along with your data streaming, API endpoints, and pipeline orchestration. Boreal integrates natively with GitHub, so you get one-click deploys, deep integration with your CI/CD workflows, and automatic previews of your dev branches deployed to the cloud. You also get enterprise-grade security, compliance, and observability. Boreal is SOC 2 Type 2 certified and offers logs and metrics endpoints to collect your observability data into your monitoring/alerting tool of choice.&lt;/p&gt;&lt;head rend="h3"&gt;Wrapping up #&lt;/head&gt;&lt;p&gt;The performance and flexibility of ClickHouse unlocks new ways of approaching data engineering, and new ways of integrating analytics and AI into software applications. But taking full advantage requires a modern developer experience layer, not just a powerful engine. That’s our mission at Fiveonefour with open-source MooseStack: to build upon ClickHouse's powerful core, offering a developer experience that feels as productive and familiar as modern web development.&lt;/p&gt;&lt;p&gt;Give the repo a star if you think it's interesting.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;“The developer experience is what really stands out with MooseStack. It’s my go-to now for every new project that needs an analytics backend.”&lt;/p&gt;&lt;lb/&gt;– David Der, Chief AI officer, SingleStone&lt;/quote&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45067867</guid></item><item><title>Do the simplest thing that could possibly work</title><link>https://www.seangoedecke.com/the-simplest-thing-that-could-possibly-work/</link><description>&lt;doc fingerprint="7d78985fad453135"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Do the simplest thing that could possibly work&lt;/head&gt;
    &lt;p&gt;When designing software systems, do the simplest thing that could possibly work.&lt;/p&gt;
    &lt;p&gt;It’s surprising how far you can take this piece of advice. I genuinely think you can do this all the time. You can follow this approach for fixing bugs, for maintaining existing systems, and for architecting new ones.&lt;/p&gt;
    &lt;p&gt;A lot of engineers design by trying to think of the “ideal” system: something well-factored, near-infinitely scalable, elegantly distributed, and so on. I think this is entirely the wrong way to go about software design. Instead, spend that time understanding the current system deeply, then do the simplest thing that could possibly work.&lt;/p&gt;
    &lt;head rend="h3"&gt;Simple can be underwhelming&lt;/head&gt;
    &lt;p&gt;System design requires competence with a lot of different tools: app servers, proxies, databases, caches, queues, and so on. As they gain familiarity with these tools, junior engineers naturally want to use them. It’s fun to construct systems out of many different components! And it feels very satisfying to draw boxes and arrows on a whiteboard - like you’re doing real engineering.&lt;/p&gt;
    &lt;p&gt;However, as with many skills, real mastery often involves learning when to do less, not more. The fight between an ambitious novice and an old master is a well-worn cliche in martial arts movies: the novice is a blur of motion, flipping and spinning. The master is mostly still. But somehow the novice’s attacks never seem to quite connect, and the master’s eventual attack is decisive.&lt;/p&gt;
    &lt;p&gt;In software, this means that great software design looks underwhelming. It doesn’t look like anything much is happening at all. You can tell you’re in the presence of great software design because you start having thoughts like “oh, I didn’t realise the problem was that easy” or “oh nice, you don’t actually have to do anything difficult”.&lt;/p&gt;
    &lt;p&gt;Unicorn is great software design, because it delivers all the most important guarantees in a web server (request isolation, horizontal scaling, crash recovery) by leaning on Unix primitives1. The industry-standard Rails REST API is great software design, because it gives you exactly what you need for a CRUD app in the most boring way possible. I don’t think any of these are impressive software. But they’re impressive feats of design, because they do the simplest thing that could possibly work.&lt;/p&gt;
    &lt;p&gt;You should do that too! Suppose you’ve got a Golang application that you want to add some kind of rate limiting to. What’s the simplest thing that could possibly work? Your first idea might be to add some kind of persistent storage (say, Redis) to track per-user request counts with a leaky-bucket algorithm. That would work! But do you need a whole new piece of infrastructure? What if instead you kept those per-user request counts in-memory? Sure, you’d lose some rate limiting data when the application is restarted, but does that matter? Actually, are you sure your edge proxy2 doesn’t support rate limiting already? Could you just write a couple of lines in a config file instead of implementing the feature at all?&lt;/p&gt;
    &lt;p&gt;Maybe your edge proxy doesn’t support rate limiting. Maybe you can’t track it in-memory because you have too many server instances running in parallel, so the tightest rate limit you could enforce that way is too wide. Maybe it’s a dealbreaker if you ever lose rate limiting data, because people are hammering your service that hard. In that case, the simplest thing that could possibly work is adding persistent storage, so you should go and do that. But if you could do one of the easier approaches, wouldn’t you want to?&lt;/p&gt;
    &lt;p&gt;You really can build a whole application from scratch this way: start with the absolute simplest thing, and then only extend it when you have new requirements that force you to. It sounds silly, but it works. Think of it as taking YAGNI as the ultimate design principle: above single-responsibility, above choosing the best tool for the job, and above “good design”.&lt;/p&gt;
    &lt;head rend="h3"&gt;What’s wrong with doing the simplest thing?&lt;/head&gt;
    &lt;p&gt;Of course, there are three big problems with always doing the simplest thing that could possibly work. The first is that, by not anticipating future requirements, you end up with an inflexible system or a big ball of mud. The second is that it’s not clear what “simplest” means, so at worst I’m saying “to design well, always do good design”. The third is that you ought to be building systems that can scale, not systems that just work right now. Let’s take those objections in order.&lt;/p&gt;
    &lt;head rend="h4"&gt;Big balls of mud&lt;/head&gt;
    &lt;p&gt;To some engineers, “do the simplest thing that could possibly work” sounds like I’m telling them to stop doing engineering. If the simplest thing is usually a quick kludge, does that mean this advice will inevitably lead to a complete mess? We’ve all seen codebases with hacks stacked on top of hacks, and they definitely don’t look like good design.&lt;/p&gt;
    &lt;p&gt;But are hacks simple? I actually don’t think so. The problem with a hack or a kludge is precisely that it isn’t simple: that it adds complexity to the codebase by introducing another thing you have to always remember. Hacks are just easier to think of. Figuring out the proper fix is hard because it requires having to understand the entire codebase (or large sections of it). In fact, the proper fix is almost always much simpler than the hack.&lt;/p&gt;
    &lt;p&gt;It is not easy to do the simplest thing that could possibly work. When you’re looking at a problem, the first few solutions that come to mind are unlikely to be the simplest ones. Figuring out the simplest solution requires considering many different approaches. In other words, it requires doing engineering.&lt;/p&gt;
    &lt;head rend="h4"&gt;What is simplicity?&lt;/head&gt;
    &lt;p&gt;Engineers disagree a lot about what constitutes simple code. If “simplest” already means “with good design”, is it just a tautology to say “you should do the simplest thing that could possibly work?” In other words, is Unicorn really simpler than Puma3? Is adding in-memory rate limiting really simpler than using Redis? Here’s a rough, intuitive definition of simplicity4:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Simple systems have fewer “moving pieces”: fewer things you have to think about when you’re working with them&lt;/item&gt;
      &lt;item&gt;Simple systems are less internally-connected. They are composed from components with clear, straightforward interfaces&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Unix processes are simpler than threads (and thus Unicorn is simpler than Puma) because processes are less connected: they do not share memory. This makes a lot of sense to me! But I don’t think it gives you the tools to figure out what’s simpler in every case.&lt;/p&gt;
    &lt;p&gt;What about in-memory rate limiting vs Redis? On the one hand, in-memory is simpler because you don’t have to think about all the things involved in standing up a separate service with persistent memory. On the other hand, Redis is simpler because the rate limiting guarantees it offers are more straightforward - you don’t have to worry about the case where one server instance thinks a user is rate limited and another one doesn’t.&lt;/p&gt;
    &lt;p&gt;When I’m not sure what “seems” simpler to me, I like to use this tiebreaker: simple systems are stable. If you’re comparing two states of a software system, and one will require more ongoing work if no requirements change, the other one is simpler. Redis must be deployed and maintained, it can have its own incidents, it requires its own monitoring, it requires a separate deployment in any new environments the service finds itself in, and so on. Thus in-memory rate limiting is simpler than Redis5.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why wouldn’t you want to be scalable?&lt;/head&gt;
    &lt;p&gt;A certain type of engineer is now screaming to themselves “but in-memory rate limiting won’t scale!” Doing the simplest thing that could possibly work will emphatically not deliver the most web-scale system. It will deliver a system that works well at the current scale. Is this irresponsible engineering?&lt;/p&gt;
    &lt;p&gt;No. In my view, the cardinal sin of big tech SaaS engineering is an obsession with scale. I’ve seen so much unavoidable pain caused by over-engineering systems to prepare for several orders of magnitude more than the current scale.&lt;/p&gt;
    &lt;p&gt;The main reason to not try this is that it doesn’t work. In my experience, for any non-trivial codebase, you can’t anticipate how it will behave at several orders of magnitude more traffic, because you don’t know ahead of time where all the bottlenecks are going to be. At most you can try to make sure you’re ready for 2x or 5x the current traffic, and then stand by to deal with problems as they come in.&lt;/p&gt;
    &lt;p&gt;The other reason not to try this is that it makes your codebase inflexible. It’s fun to decouple your service into two pieces so they can be scaled independently (I have seen this happen maybe ten times, and I have seen them actually be usefully scaled independently maybe once). But that makes certain features very hard to implement, because they now require coordination over the wire. In the worst case, they require transactions over the wire, which is a genuinely hard engineering problem. Most of the time you just don’t have to do any of this!&lt;/p&gt;
    &lt;head rend="h3"&gt;Final thoughts&lt;/head&gt;
    &lt;p&gt;The longer I spend working in tech, the less optimistic I become about our collective ability to predict where a system is going. It’s hard enough to get your head around where a system currently is. And in fact, that’s the main practical difficulty in doing good design: getting an accurate big-picture understanding of the system. Most design is done without that understanding, and most design is thus pretty bad.&lt;/p&gt;
    &lt;p&gt;There are, broadly speaking, two ways to develop software. The first is to predict what your requirements might look like six months or a year from now, and then design the best system for that purpose. The second is to design the best system for what your requirements actually look like right now: in other words, to do the simplest thing that could possibly work.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;It’s just Unix sockets and forked processes! I love Unicorn.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Every tech company has some kind of edge proxy.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;I do like Puma and think it’s a good web server. There are definitely use cases where you’d pick it over Unicorn (though in those cases I would personally think hard about using a different language than Ruby).&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;I’m influenced here by Rich Hickey’s great talk Simple Made Easy. I don’t agree with all of it (I think familiarity does in fact contribute to simplicity in practice) but it’s definitely worth watching.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Of course, if the system has to scale horizontally more than a little bit, in-memory rate limiting won’t work and must be replaced with something like Redis. But in my experience a Golang service can scale a lot without having to scale horizontally to more than a handful of replicas.&lt;/p&gt;↩&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you liked this post, consider subscribing to email updates about my new posts, or sharing it on Hacker News.&lt;/p&gt;
    &lt;p&gt;August 28, 2025 │ Tags: software design, shipping&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45068091</guid></item><item><title>The No-CPU Amiga Demo Challenge</title><link>https://github.com/askeksa/NoCpuChallenge</link><description>&lt;doc fingerprint="d4aa45094723b8a8"&gt;
  &lt;main&gt;
    &lt;p&gt;This is an open challenge to create demos that run entirely on the Amiga custom chips without involving the CPU.&lt;/p&gt;
    &lt;p&gt;This repository contains the rules of the challenge and a runner application for launching no-CPU demos. This is intended as a standard specification of the no-CPU platform for demo competitions.&lt;/p&gt;
    &lt;p&gt;There will be a dedicated no-CPU Amiga demo competition at Gerp 2026, January 23-25, 2026. In addition, this is an ongoing challenge — an invitation to explore a different kind of demo platform.&lt;/p&gt;
    &lt;p&gt;An invitation demo — itself a no-CPU demo — was released at Evoke 2025. The full source code for the demo is available here.&lt;/p&gt;
    &lt;p&gt;Whenever you release a no-CPU demo, you are encouraged to write a comment about it on the demo announcement issue.&lt;/p&gt;
    &lt;p&gt;There's also a FAQ.&lt;/p&gt;
    &lt;p&gt;The Amiga custom chips (affectionately named Alice, Lisa and Paula in the AGA version of the chipset) were remarkably powerful for their time, enabling the Amiga computers — with their modestly-powered CPUs — to perform graphical and musical feats that required heavy computation on most contemporary platforms.&lt;/p&gt;
    &lt;p&gt;This challenge aims to discover just how powerful these chips really are by exploring what they can do completely on their own, without the CPU even telling them what to do.&lt;/p&gt;
    &lt;p&gt;There have been several demo competitions in the past with a technical theme. Examples include Atari zero bitplane, Atari mixed-resolution, C64 only sprites and C64 border only. This is a similar idea for the Amiga — no CPU, custom chips only.&lt;/p&gt;
    &lt;p&gt;A no-CPU demo takes the form of a raw memory image that specifies the initial contents of chip memory. Together with the initial state of the hardware registers (specified below) this memory image fully defines the demo.&lt;/p&gt;
    &lt;p&gt;The memory image is loaded into memory by a runner application, which serves as the demo executable. You can use the runner as is or modify it to your liking, but in order to qualify as a no-CPU demo according to this challenge, your chip memory image has to work with the official runner (with the same behavior).&lt;/p&gt;
    &lt;p&gt;The maximum size of the chip memory image depends on the targeted Amiga chipset: 512k for OCS, 1MB for ECS (or OCS with ECS Agnus and 512k expansion, likely the most common Amiga 500 configuration), and 2MB for AGA.&lt;/p&gt;
    &lt;p&gt;The audio filter is disabled. Since the filter is controlled via the CIA registers, which the copper does not have access to, the demo does not have the option of enabling the filter.&lt;/p&gt;
    &lt;p&gt;The initial hardware register contents are as follows. Registers with ECS/AGA specific bits are generally initialized to their OCS defaults. This makes it easier to make a demo targeting OCS without worrying about AGA compatibility (as long as you are not using incompatible features, such as the 7 bitplane trick).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Register&lt;/cell&gt;
        &lt;cell role="head"&gt;Address&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
        &lt;cell role="head"&gt;Comment&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;VPOSW&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$dff02a&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$8000&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Long frames&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;COPCON&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$dff02e&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$0002&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Copper danger flag set&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;COP1LC&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$dff080&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$000000&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Copper initially starts at address 0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;DMACON&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$dff096&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$87c0&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Bitplane, copper and blitter DMA enabled, sprite DMA disabled, Blitter Nasty set&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ADKCON&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$dff09e&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$xx00&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;All modulation disabled&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;BPLCON0&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$dff100&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$0200&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;OCS default&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;BPLCON1&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$dff102&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$0000&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;OCS default&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;BPLCON2&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$dff104&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$0024&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;OCS default&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;BPLCON3&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$dff106&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$0c00&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;OCS default&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;BPLCON4&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$dff10c&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$0011&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;OCS default&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;COLOR00&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$dff180&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$000&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Black background&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;BEAMCON0&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$dff1dc&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$0020&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;PAL&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;FMODE&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$dff1fc&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$0000&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;OCS default&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The values of all other registers are undefined.&lt;/p&gt;
    &lt;p&gt;The demo can signal that it has ended by clearing the Blitter Nasty flag (i.e. by executing the copper instruction $0096,$0400). Depending on the hardware configuration, the runner may not actually be able to exit back to the OS, so the demo should still maintain a valid display.&lt;/p&gt;
    &lt;p&gt;If you have questions or comments, you are welcome to open an issue. I am particularly interested in feedback in these areas:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If there is something in the rules or the runner that you think puts undue limits on what no-CPU demos can do under this formalism.&lt;/item&gt;
      &lt;item&gt;If you find a security hole in the runner sandbox, enabling a demo to reactivate the CPU.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45068268</guid></item></channel></rss>