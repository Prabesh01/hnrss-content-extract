<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 02 Oct 2025 23:32:27 +0000</lastBuildDate><item><title>Pre-Record Your Demos</title><link>https://www.steveharrison.dev/pre-record-your-demos/</link><description>&lt;doc fingerprint="71723990b1a6a8e6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Pre-record your demos&lt;/head&gt;
    &lt;p&gt;Meta was in the news recently for two failed live demos:&lt;/p&gt;
    &lt;p&gt;While there were a lot of people who said they'd rather see a live demo than pre-recorded content such as Apple events nowadays, I disagree: I think most demos should be pre-recorded.&lt;/p&gt;
    &lt;p&gt;Here's one of the reasons the Meta demos failed:&lt;/p&gt;
    &lt;p&gt;There are many reasons why live demos can fail, such as:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Network issues (WiFi/server/proxy/certificate/etc. issues)&lt;/item&gt;
      &lt;item&gt;Computer setup issues if presenting from someone else's computer&lt;/item&gt;
      &lt;item&gt;External services going down when you need to do the demo&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When Steve Jobs demoed the iPhone 4 at WWDC2010 (I was lucky enough to be in the room!), there were network issues caused by all the MiFi routers in the audience:&lt;/p&gt;
    &lt;p&gt;And if you're demoing something locally, you need to have your environment set up for the demo. Since most of the time you're demoing something you previously worked on, this prevents you from working on current stuff right up to the demo, because you're worried you might break your local setup for the demo by installing different node modules, switching to different Git branches, etc.&lt;/p&gt;
    &lt;p&gt;This is why I don't ever do live demos now at work. I use Apple's Screenshot app to do a recording beforehand, sometimes with audio if relevant, and then play this movie during my presentation. You can also choose to show mouse clicks in the recording, which helps the audience see what's going on. And when presenting from a recording, you can scrub through it, so the audience doesn't suffer through slow loading screens.&lt;/p&gt;
    &lt;p&gt;Sure, live demos serve a purpose when showing off final, polished products at keynotes—it was cool to see Steve Jobs video call live with Jonny Ive once everyone disabled their MiFi routers. For most demos, though, I think we all save ourselves and our audience some frustration and just present a recording.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45435720</guid><pubDate>Wed, 01 Oct 2025 09:04:10 +0000</pubDate></item><item><title>F3: Open-source data file format for the future [pdf]</title><link>https://db.cs.cmu.edu/papers/2025/zeng-sigmod2025.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45437759</guid><pubDate>Wed, 01 Oct 2025 13:52:41 +0000</pubDate></item><item><title>How Israeli actions caused famine in Gaza, visualized</title><link>https://www.cnn.com/2025/10/02/middleeast/gaza-famine-causes-vis-intl</link><description>&lt;doc fingerprint="edaed098b2dab5fa"&gt;
  &lt;main&gt;
    &lt;p&gt;Israel’s nearly two-year war pushed parts of Gaza into “man-made” famine, according to a report published in August by a United Nations-backed initiative, deepening the Palestinians’ struggle for survival under relentless bombing, mass displacement and the spread of disease.&lt;/p&gt;
    &lt;p&gt;The report by the Integrated Food Security Phase Classification (IPC), a UN-backed expert panel that assesses global food insecurity and malnutrition, helped to fuel growing international outcry over Israel’s campaign in Gaza following the Hamas-led October 7, 2023, attacks – and was cited by some of countries that recently made moves towards formally recognizing a Palestinian state. The IPC forecast that by the end of September nearly a third of Gaza’s total population would face famine conditions, although it has not yet provided an update on that forecast.&lt;/p&gt;
    &lt;p&gt;In Gaza governorate alone – the largest by population of five in the Gaza Strip – more than half a million people were condemned to a cycle of “starvation, destitution and death,” the IPC added. The Israeli assault on Gaza City, which Israeli Prime Minister Benjamin Netanyahu says is targeting one of Hamas’ “remaining strongholds” has choked relief operations for starving Palestinians, according to rights workers.&lt;/p&gt;
    &lt;p&gt;Michael Fakhri, the UN’s special rapporteur on the right to food, accused Israel of using hunger “as a weapon against Palestinians,” in violation of international law.&lt;/p&gt;
    &lt;p&gt;“Israel is using food and aid as a weapon to humiliate, weaken, displace and kill Palestinians in Gaza,” Fakhri told CNN on August 28.&lt;/p&gt;
    &lt;p&gt;Israel rejected the IPC’s findings, with the Israeli agency that oversees the entry of aid into Gaza claiming the report was “false” and based on “partial, biased” data “originating from Hamas.” Netanyahu slammed the UN-backed report, in a statement from his office, adding that “Israel does not have a policy of starvation.”&lt;/p&gt;
    &lt;p&gt;Israel has since insisted that it has stepped up the entry of aid into Gaza. But aid agencies say that Israel’s intensification of the war, particularly around Gaza City, has compounded the misery faced by Palestinians. Here is a look, in five charts, at how the situation described by the IPC materialized.&lt;/p&gt;
    &lt;head rend="h2"&gt;Famine projected to spread to central, southern Gaza&lt;/head&gt;
    &lt;p&gt;The IPC projected that famine would spread to Deir Al-Balah, central Gaza and further south, in Khan Younis by the end of September, affecting nearly 641,000 people.&lt;/p&gt;
    &lt;p&gt;Up to June 2026, at least 132,000 children under the age of five are expected to suffer from acute malnutrition, including more than 41,000 severe cases of children at heightened risk of death, the IPC added.&lt;/p&gt;
    &lt;p&gt;Under the IPC – a five-phase indicator used to measure the severity of food insecurity – a famine can only be declared if three thresholds are met: at least 20% of households face extreme food shortages, the proportion of children assessed as acutely malnourished reaches a certain threshold, and at least two in every 10,000 people die each day from starvation, or from malnutrition and disease.&lt;/p&gt;
    &lt;p&gt;Israel accused the IPC of lowering the second threshold of acutely malnourished children for a famine declaration, which the IPC has denied.&lt;/p&gt;
    &lt;p&gt;Researchers use three methods for assessing child malnutrition – either a child’s height and weight, their BMI, or a child’s mid-upper arm circumference, known as MUAC. The IPC used the latter, a metric employed since 2019, to determine that at least 15% of children aged six to 59 months have a mid-upper arm circumference of less than 125mm or edema, the agency told CNN. The thresholds for famine classification are “standard and were not modified for Gaza,” the IPC told CNN, adding that the MUAC metric “is the measurement most frequently available and has strong correlation with mortality outcomes,” and was also used in famine classifications in Sudan and South Sudan this decade.&lt;/p&gt;
    &lt;p&gt;Human rights advocates say Israel’s destruction of health infrastructure and intensified hostilities have hampered efforts to document the full scope of famine in Gaza.&lt;/p&gt;
    &lt;p&gt;After more than 700 days of war, 455 Palestinians have died of malnutrition or starvation, including 151 children, the health ministry in Gaza reported on October 1. One hundred and seventy-seven of the total number have died of malnutrition or starvation since the IPC confirmed famine on August 15, it said.&lt;/p&gt;
    &lt;head rend="h2"&gt;How much UN aid is getting into Gaza?&lt;/head&gt;
    &lt;p&gt;Israel’s vast web of bureaucratic impediments, including delayed approvals, arduous border checks and the arbitrary rejection of items, throttles the amount of aid that makes it to the other side of the border and sends food costs soaring, the UN and aid agencies say.&lt;/p&gt;
    &lt;p&gt;After visiting the region in late August, US Senators Chris Van Hollen and Jeff Merkley, both Democrats, warned that Netanyahu’s government was “implementing a plan to ethnically cleanse Gaza of Palestinians” and accused Israel of using food “as a weapon of war.” Israel has denied the allegations.&lt;/p&gt;
    &lt;p&gt;“The findings from our trip lead to the inescapable conclusion that the Netanyahu government’s war in Gaza has gone far beyond the targeting of Hamas to imposing collective punishment on the Palestinians there, with the goal of making life for them unsustainable,” said the report, published on September 11. “That is why it restricts the delivery of humanitarian assistance.”&lt;/p&gt;
    &lt;p&gt;Israeli authorities have said trucks “remain uncollected” at the border with Gaza – accusing the UN of failing to coordinate the entry of vehicles into the strip.&lt;/p&gt;
    &lt;p&gt;But Sam Rose, the acting director of affairs for the UN agency for Palestinian refugees (UNRWA) in Gaza, says Israel – which has near-total jurisdiction over what goods enter and exit Gaza – has controlled “to the calorie” the volume, type and overall flow of food into the enclave. “The system is designed not to function smoothly,” he said.&lt;/p&gt;
    &lt;p&gt;Israeli authorities “know and analyze each truck that goes into Gaza, the weight and the calories,” a senior official with COGAT, the Israeli agency that controls the entry of aid into the enclave, said in September. According to a COGAT statement published in response to the IPC famine declaration, “analysis of contents of food aid trucks that entered the Gaza Strip reveal that 4,400 calories per person per day entered Gaza since the beginning of August.”&lt;/p&gt;
    &lt;p&gt;However, as of May, Palestinians were consuming just 1,400 calories per day – or “67 per cent of what a human body needs to survive,” at 2,300 calories, the UN reported in June.&lt;/p&gt;
    &lt;p&gt;Last October, Israel’s government banned UNRWA from operating in areas under its control, a prohibition that went into effect in January, having accused the agency of failing to stop Hamas’ alleged theft of aid. An internal US government review found no evidence of widespread theft by Hamas of US-funded humanitarian aid in Gaza.&lt;/p&gt;
    &lt;p&gt;When the trickle of relief does enter the strip, aid workers face intensified hostilities, damaged roads and limited fuel supplies – impeding internal distribution efforts, minimizing viable routes and blocking access to displaced Palestinians, said Rose.&lt;/p&gt;
    &lt;head rend="h2"&gt;What other ways are there to receive aid?&lt;/head&gt;
    &lt;p&gt;Israel says UN aid makes up only part of the relief that gets into Gaza. A senior COGAT official told a briefing in early September that 27% of the trucks entering Gaza are UN vehicles, claiming it was “a lie” that the UN had brought in 600 aid trucks a day before the war.&lt;/p&gt;
    &lt;p&gt;“There is no famine in Gaza. Period,” the official said, adding that “Israel and the IDF are trying to strengthen the humanitarian situation in Gaza with partners.”&lt;/p&gt;
    &lt;p&gt;In May, the US and Israeli-backed Gaza Humanitarian Foundation (GHF) established a program that now plans to operate up to five distribution sites in the enclave, all but one in southern Gaza – which rely on private military contractors and largely replaced 400 UN-led hubs.&lt;/p&gt;
    &lt;p&gt;Relief and health workers say these other methods of delivering food in Gaza, including the GHF sites and aid pallet drops from planes, are dehumanizing and inaccessible for many Palestinians, and expose them to injury or death.&lt;/p&gt;
    &lt;p&gt;At least 1,172 people were killed “near militarized supply sites” between May 27 and September 9, the UN said on September 10, with another 1,084 deaths along convoy supply routes. In August, UN experts called for the immediate closure of GHF-operated sites in Gaza and accused Israeli forces of opening “indiscriminate fire” on people seeking aid there. The advocates warned the hubs are “especially difficult” for women, children, people with disabilities and elderly Palestinians to access.&lt;/p&gt;
    &lt;p&gt;GHF has defended its work in Gaza and said earlier in September that it was the only organization in Gaza able to deliver food “at scale without interference.” The organization also said that it had “repeatedly sought collaboration with UN agencies and international NGOs to deliver aid side-by-side” but that the UN had “declined those offers.” The Israeli military has acknowledged firing warning shots toward crowds in some instances and denied responsibility for other casualties near aid hubs.&lt;/p&gt;
    &lt;p&gt;The US and Israel plan to set up 12 additional sites across the enclave, an Israeli official told CNN in August. However, there is no indication that the new sites have been established. In September, GHF said it had sought IDF permission to open sites in northern Gaza but that Israel had not granted the permission.&lt;/p&gt;
    &lt;p&gt;“With parents injured and their siblings starving, many teenagers and young adults are taking the risk,” Mohammed Khaleel, an American surgeon who was deployed to Gaza earlier this year, told CNN in August.&lt;/p&gt;
    &lt;p&gt;“We’ve even heard some people report that they will go and accept their fate. Dying from a gunshot may be preferable to dying from starvation,” he added.&lt;/p&gt;
    &lt;head rend="h2"&gt;Farmland is shrinking and becoming increasingly inaccessible&lt;/head&gt;
    &lt;p&gt;Israel’s two-year offensive in Gaza had left just 1.5% of cropland accessible and undamaged as of July 28, according to the UN – largely preventing Palestinians from cultivating produce.&lt;/p&gt;
    &lt;p&gt;That destruction, coupled with Israel’s fishing ban and intensified assault in the north, has further limited the sources of food available to hundreds of thousands of displaced Palestinians.&lt;/p&gt;
    &lt;p&gt;“It is not by chance that Israel has focused its starvation tactics in northern Gaza,” Fakhri, the UN special rapporteur, said. “They have announced their intent to push people from the north to the south of Gaza… Just as now, the focus of their starvation campaign on Gaza City correlates with their invasion plans.”&lt;/p&gt;
    &lt;p&gt;The military’s invasion of Gaza City will collapse an “already fragile” aid supply chain, warned Arif Husain, chief economist at the World Food Programme.&lt;/p&gt;
    &lt;p&gt;Relief agencies need a ceasefire, unimpeded humanitarian access, large-scale multi-sector aid, protection of civilians and infrastructure – and restoration of commercial and local food systems – to reverse famine in Gaza, said Husain.&lt;/p&gt;
    &lt;p&gt;“We are already at the brink. Another escalation – especially in Gaza City – could push the situation into unimaginable catastrophe,” he added. “It will not only result in more deaths but destroy any foundation for future recovery.”&lt;/p&gt;
    &lt;p&gt;CNN’s Ibrahim Dahman, Kareem Khadder and Eyad Kourdi contributed reporting.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45447699</guid><pubDate>Thu, 02 Oct 2025 09:23:50 +0000</pubDate></item><item><title>NL Judge: Meta must respect user's choice of recommendation system</title><link>https://www.bitsoffreedom.nl/2025/10/02/judge-in-the-bits-of-freedom-vs-meta-lawsuit-meta-must-respect-users-choice/</link><description>&lt;doc fingerprint="bc501a786403c13c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Judge in the Bits of Freedom vs. Meta lawsuit: Meta must respect users’ choice&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;02 oktober 2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Today the judge issued a ruling in the summary proceedings brought by digital human rights organisation Bits of Freedom against Meta. The organisation demanded that Meta gives its users on apps such as Instagram and Facebook the option to select a feed that is not based on profiling.&lt;/p&gt;
    &lt;p&gt;Bits of Freedom sued Meta for a breach of the Digital Services Act (DSA). This European legislation is intended to give users more autonomy and control over the major online platforms. One of the core elements of the DSA is that users must have greater influence over the information they see.&lt;/p&gt;
    &lt;p&gt;For many people, and especially for young people, social media platforms are a major source of news and information. Therefore it is crucial that users themselves can decide which content appears on their feed. Without that freedom of choice, participation in the public debate is seriously hampered. That is problematic at any time, but especially so during election periods. In the Netherlands, national elections will be held at the end of this month.&lt;/p&gt;
    &lt;p&gt;The judge states that Meta is indeed acting in violation of the law. He says that “a non‑persistent choice option for a recommendation system runs counter to the purpose of the DSA, which is to give users genuine autonomy, freedom of choice, and control over how information is presented to them.” The judge also concludes that the way Meta has designed its platforms constitutes “a significant disruption of the autonomy of Facebook and Instagram users.” The judge orders Meta to adjust its apps so that the user’s choice is preserved, even when the user navigates to another section or restarts the app.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“We are pleased that the judge now makes clear that Meta must respect the user’s choice,” says Maartje Knaap, spokesperson for Bits of Freedom. “It is absolutely unacceptable that a handful of American tech billionaires determine how we see the world. That concentration of power poses a risk to our democracy. At the same time, it is regrettable that we need to go to court to ensure Meta complies with the law.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Meta has an interest in steering users toward a feed where it can show as many interest‑ and behavior‑based ads as possible. That is the core of Meta’s revenue model. Subtle design techniques push users toward that feed, while the non‑profiled feed is hidden behind a logo, making it hard to find. Users who do choose the alternative timeline also lose direct access to features such as Direct Messages. Moreover, when you open the app, it always starts with Meta’s feed, even if the user selected a different one before. Because of the judge’s ruling, Meta must change its behavior.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“This ruling shows that Meta is not untouchable,” continues Maartje Knaap. “But we are also realistic, this is just a drop in the ocean. There’s still a long way to go. We hope the decision will inspire individuals, civil society organisations, regulators and lawmakers worldwide around the world who are working to rein in Meta’s power. Together we can stand up to a company that has become overwhelmingly powerful.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;You can find the ruling here (in Dutch).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45448326</guid><pubDate>Thu, 02 Oct 2025 11:32:19 +0000</pubDate></item><item><title>N8n added native persistent storage with DataTables</title><link>https://community.n8n.io/t/data-tables-are-here/192256</link><description>&lt;doc fingerprint="bfbc187f418403ca"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Hey everyone &lt;/head&gt;
        &lt;head rend="h2"&gt;We’re super excited to share that starting with v1.113 we’re rolling out data tables (beta) to all plans. &lt;/head&gt;
        &lt;p&gt;Since the very beginning of n8n we’ve heard many of you mention the need for a proper table inside n8n to store data between workflow executions without needing to switch platforms or setting up credentials and now it’s finally here.&lt;/p&gt;
        &lt;p&gt;With data tables you can:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Save specific data from your workflow runs&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Keep data around between multiple executions&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Avoid duplicate runs by tracking execution status&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Store reusable prompts for different workflows&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Collect evaluation data for your AI workflows&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Do lookups, merges, enhancements…&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;…and honestly, probably 100 other creative things we haven’t even thought of yet &lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
        &lt;p&gt; To make sure your instance stays performant, we’ve set a 50MB limit for everyone. If you’re self-hosting (and know what you’re doing), you can change that via the ENV variable &lt;code&gt;N8N_DATA_TABLES_MAX_SIZE_BYTES&lt;/code&gt;&lt;/p&gt;
        &lt;p&gt; Upgrade to 1.113, give data tables a spin, and let us know what you think! What’s missing? What would make it even more useful for you? We’re really curious to hear your ideas and thoughts! &lt;/p&gt;
        &lt;p&gt; Read more about the data tables in the docs here.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 38 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;liam
2&lt;/div&gt;
      &lt;p&gt; 7 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;dszp
3&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;This is absolutely awesome to see, I can’t wait to use these! It’s probably been my number one frustration that saving even a small amount of data between executions for all sorts of purposes requires either integrating PostgreSQL and dealing with schemas, using a third party database or API like Supabase (as handy as they are), or using variables that are powerful but are somewhat clumsy to instantiate and track since they only work in Code nodes and only save data for production executions, making testing hard. Hoping data-tables makes a ton of these things easier! Probably won’t run the new version until it’s in final release rather than pre-release, but this is awesome to see!&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 4 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;bartv
5&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;This is really great - when I migrated from “the other platform” almost 4 years ago, I really felt the pain of not having a simple in-app data storage. I played around with Data tables this weekend and it’s just SUCH a good and fast experience! Kudos to our Product and Engineering teams &lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 3 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Hey all,&lt;/p&gt;
        &lt;p&gt;IMPORTANT NOTE: There is an issue with very large SQLite databases that is causing instances to slow down. Out of an abundance of caution, we are unfortunately removing version 1.113.0 until we fix this issue. We hope to have this released again with a fix within the next couple of days.&lt;/p&gt;
        &lt;p&gt;Very sorry about this!&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 10 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;This is great!&lt;/p&gt;
        &lt;p&gt;It´d be cool for self hosting to be able to add a second DB, where n8n pulls the data from. So one could have performance without having to set up each time a postgres connection.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 2 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;bartv
10&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;Data tables is back on!&lt;/p&gt;
        &lt;p&gt;A patch was released earlier today. It has now been tested and we have high confidence. Please update to 1.113.1 (which is still in beta) to try this feature.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 4 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;TH1
11&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;is that Data Tables only available for the Cloud version? local host will not have Data Tables?&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;liam
12&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;It’s on all plans (cloud and self hosted) starting on version 1.113.1 &lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 2 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;Sujit
13&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;I am unable to see the data tables in my local self hosted n8n. I’ve also updated the docker image to pick the latest one. What am I missing?&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Does this mean we can share data between multiple workflows now? This would make splitting up complex workflows across multiple workflows so much easier.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;I believe this is still only available in the beta version?&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;jabbson
16&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;The fact that the “latest” is not “1.113.1”. The latest is “the latest stable”, where 1.113.1 is not that.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 3 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Very happy to see this feature. I’ve been testing it out, and was commenting feedback on Reddit but someone in the Discord server said the forums is the best place to post this instead. Here’s my list(so far)&lt;/p&gt;
        &lt;list rend="ol"&gt;
          &lt;item&gt;
            &lt;p&gt;When going to the Data Tables tab, “Create Table” is not default on the upper right button, it’s defaulted to “Create Workflow” instead.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Cannot change the data type after a column is created.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Cannot set any of the column’s as primary or unique such as the ID column (To prevent duplicates)&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;For some odd reason, setting a column data type to “number” then pushing data from JSON array into the table, physically opening the table and looking at the rows, the numbers in the “number” data type column are not all together. For example “29683389” shows in the table as “29 683 389”. This isn’t a one off either, ALL rows exhibit the same behavior and ALL columns set as “numbers” too.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Table page can only show 50 rows per page. Which I understand is probably for performance reasons. However, there really needs to be a “search” function for the table to search for data.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Are there any limitations for creating tables?&lt;lb/&gt; or we can create multiples/unlimited (in 50Mb limit)?&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Love this list @compaholic, thanks so much for sharing it.&lt;lb/&gt; 1, 2 and 5 are all planned. For (4), I think that is just a highlighting to make it easier to read that it’s actually 29M. So the number should still be correct.&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;You can created unlimited ones within the storage limit &lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45450044</guid><pubDate>Thu, 02 Oct 2025 14:26:23 +0000</pubDate></item><item><title>Two Amazon delivery drones crash into crane in commercial area of Tolleson, AZ</title><link>https://www.abc15.com/news/region-west-valley/tolleson/two-amazon-delivery-drones-crash-into-crane-in-commercial-area-of-tolleson</link><description>&lt;doc fingerprint="d15faed71076add2"&gt;
  &lt;main&gt;
    &lt;p&gt;TOLLESON, AZ — The Tolleson Police Department is investigating after two Amazon delivery drones crashed on Wednesday morning.&lt;/p&gt;
    &lt;p&gt;Officials say they are working an active investigation after the two drones crashed into a crane that was in a commercial area near 96th Avenue and Roosevelt Street.&lt;/p&gt;
    &lt;p&gt;It's unclear if anyone was injured during the incident.&lt;/p&gt;
    &lt;p&gt;ABC15 reached out to Amazon which provided the following statement: “We’re aware of an incident involving two Prime Air drones in Tolleson, Arizona. We’re currently working with the relevant authorities to investigate.”&lt;/p&gt;
    &lt;p&gt;On Thursday, Amazon told ABC15 operations are set to resume on Friday:&lt;/p&gt;
    &lt;p&gt;“We'll resume drone delivery in Tolleson tomorrow while continuing to support ongoing reviews by relevant agencies. Safety is our top priority, and we've completed our own internal review of this incident and are confident that there wasn't an issue with the drones or the technology that supports them. Nonetheless, we've introduced additional processes like enhanced visual landscape inspections to better monitor for moving obstructions such as cranes.”&lt;/p&gt;
    &lt;p&gt;This is a developing story and will be updated once new information becomes available.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45450449</guid><pubDate>Thu, 02 Oct 2025 14:52:49 +0000</pubDate></item><item><title>Work is not school: Surviving institutional stupidity</title><link>https://www.leadingsapiens.com/surviving-institutional-stupidity/</link><description>&lt;doc fingerprint="37111b1e29438f9c"&gt;
  &lt;main&gt;
    &lt;p&gt;For 16+ years, we master the rules of school. Study hard, get good grades, follow the formula and ultimately merit wins. Then we enter the workforce and none of it works quite like we thought. This becomes painfully obvious as you rise higher in the org.&lt;/p&gt;
    &lt;p&gt;Even seasoned veterans forget this. Recently, a director-level client hit a minor career bump and spiraled into crisis mode, their expectations still anchored in what I call "school rules".&lt;/p&gt;
    &lt;p&gt;Organizations don't run purely on merit or even clear criteria. Although they claim otherwise using buzzwords like “merit” and “data”. That’s only one part of the story, and also what’s visible.&lt;/p&gt;
    &lt;p&gt;The other part, often more consequential, runs on flawed psychology, imperfect decisions, and competing interests. You can call it organizational absurdities. Or more bluntly, institutional stupidity.&lt;/p&gt;
    &lt;p&gt;What follows is a reality check. It’s a “letter to frustrated high-performers” who keep bumping up against these unwritten rules of work. Consider it your guide to staying sane while playing the long game.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If you have to, blame stupidity not malice&lt;/item&gt;
      &lt;item&gt;Organizations are anything but meritocracies&lt;/item&gt;
      &lt;item&gt;Perception matters as much as performance&lt;/item&gt;
      &lt;item&gt;Don’t waste time fighting for “objective fairness.”&lt;/item&gt;
      &lt;item&gt;Positioning what you offer&lt;/item&gt;
      &lt;item&gt;Mind the gap: your standards vs their’s&lt;/item&gt;
      &lt;item&gt;Higher you go, more it’s an inverted funnel&lt;/item&gt;
      &lt;item&gt;Know which game you’re choosing to play&lt;/item&gt;
      &lt;item&gt;Watching your circle of control&lt;/item&gt;
      &lt;item&gt;Keep a balanced portfolio&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;If you have to, blame stupidity not malice&lt;/head&gt;
    &lt;p&gt;Most of what we chalk up to “politics” or “backstabbing”, aka bad intent, is often better explained by stupidity, inertia, bad incentives, fragmented attention, and misaligned maps of reality.&lt;/p&gt;
    &lt;p&gt;People are juggling too much, thinking too little, and rarely stepping back to ask, “What actually makes sense here?”&lt;/p&gt;
    &lt;p&gt;When you assume stupidity instead of malice, you stay above the fray, stop taking slights personally, or turning misjudgments into betrayals. This way we retain agency and choice.&lt;/p&gt;
    &lt;p&gt;Assuming malice turns you into a cynic. In contrast, assuming stupidity keeps you curious. Instead of fighting ghosts, you study the system and ask better questions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What pressures is that person responding to?&lt;/item&gt;
      &lt;item&gt;What game are they trying to win?&lt;/item&gt;
      &lt;item&gt;What am I assuming as rational that’s not?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;No one is out to get you; they’re just out to get through the week. The shift from malice to stupidity gives you just enough distance to be curious instead of reactive.&lt;/p&gt;
    &lt;head rend="h2"&gt;Organizations are anything but meritocracies&lt;/head&gt;
    &lt;p&gt;Managers will claim they are, and we want to believe them. But the reality is that the best don’t always rise. At least not as easily or automatically as we think they should.&lt;/p&gt;
    &lt;p&gt;Sometimes they do. But often, what gets rewarded isn’t performance but proximity to power, timing, perception, and political usefulness.&lt;/p&gt;
    &lt;p&gt;This doesn’t mean performance doesn’t matter. It means performance is necessary but not sufficient. It is the entry ticket that gets you through the door, but does not guarantee a seat at the table.&lt;/p&gt;
    &lt;p&gt;Assuming that excellence is obvious is the fatal error of the conscientious expert. Although it creates value, performance doesn’t automatically generate visibility, influence, or narrative. And those are the currencies that get traded when decisions are made by humans.&lt;/p&gt;
    &lt;p&gt;Merit matters, but it needs a stage and a spotlight. It doesn’t mean becoming a shameless self-promoter. Rather, your work needs a distribution strategy.&lt;/p&gt;
    &lt;head rend="h2"&gt;Perception matters as much as performance&lt;/head&gt;
    &lt;p&gt;In school, everyone was evaluated against an objective criteria by someone paid to assess fairly.&lt;/p&gt;
    &lt;p&gt;In organizations, no such thing exists. Instead, perception is the “data”. And this data is constructed often haphazardly by busy people working off limited inputs. You have to manage the story by shaping impressions intentionally.&lt;/p&gt;
    &lt;p&gt;Not only does perception matter as much as performance but who’s doing the perceiving matters even more.&lt;/p&gt;
    &lt;p&gt;Not all perceivers are created equal. A peer may love your work but they might not be a critical node in the web of influence. Who gets consulted? Do they know what you’ve built, and have they heard your name in relevant contexts?&lt;/p&gt;
    &lt;p&gt;It’s not just “do great work.”; it’s also “do the work that’s perceived as valuable.” This means translating your work’s significance up the chain and shape its interpretation. If not, others will do it for you and they may not be generous, or even accurate.&lt;/p&gt;
    &lt;p&gt;For more, see my last two articles: Schrodinger’s Cat at Work Part I and Schrodinger’s Cat at Work Part II.&lt;/p&gt;
    &lt;head rend="h2"&gt;Don’t waste time fighting for “objective fairness.”&lt;/head&gt;
    &lt;p&gt;On paper, organizations love metrics: KPIs, OKRs, dashboards. They create the appearance of detached objectivity.&lt;/p&gt;
    &lt;p&gt;Meanwhile, subjective decisions are constantly happening behind the scenes. The decisions about who to trust, or who gets a shot are made through informal reputations and shared stories about your value. Then the “data” is used to justify them in retrospect.&lt;/p&gt;
    &lt;p&gt;Rather than rant against the system, get good at reading the underlying subjective logic:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Who does this person trust, and why?&lt;/item&gt;
      &lt;item&gt;What do they consider strategic vs tactical?&lt;/item&gt;
      &lt;item&gt;What would make them feel safe betting on me?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Subjectivity isn’t the enemy. It’s the underlying physics of it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Positioning what you offer&lt;/head&gt;
    &lt;p&gt;You may have had the right message but at the wrong moment, or in the wrong wrapper.&lt;/p&gt;
    &lt;p&gt;Positioning is the context around your contributions: Why now? Why you? Why this way? A good idea, or stellar performance, poorly positioned can seem irrelevant. In contrast, a mediocre one nicely positioned is deemed visionary.&lt;/p&gt;
    &lt;p&gt;It’s not just what you say but also when, how, and through whom you say it.&lt;/p&gt;
    &lt;p&gt;Persistence matters as well. Think in campaigns, not just one-time efforts. In my piece on effective conversations I wrote:&lt;/p&gt;
    &lt;quote&gt;It is the series of messages in different forms that over time makes the difference. More akin to waves shaping the shoreline rather than the occasional once in a lifetime tsunami.&lt;/quote&gt;
    &lt;p&gt;What messages are you sending, are they varied, and are you doing it consistently? This is as true for marketing products as it is for positioning yourself inside organizations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mind the gap: your standards vs their’s&lt;/head&gt;
    &lt;p&gt;Another obvious but forgotten reality of organizational life: Not everyone operates by the same playbook.&lt;/p&gt;
    &lt;p&gt;You prioritize substance and direct contribution, while others focus on visibility and relationship-building in ways that are uncomfortable to you. It’s that colleague who excels at positioning routine work as “strategic”, or the peer who builds influence through cultivating key relationships.&lt;/p&gt;
    &lt;p&gt;And yes, these approaches do yield results.&lt;/p&gt;
    &lt;p&gt;But this doesn’t mean dismissing it as pure politics or simply abandoning your principles. It means understanding the landscape you're operating in.&lt;/p&gt;
    &lt;p&gt;You can’t effectively participate in a game you refuse to see clearly. You're not at a disadvantage because you choose to act with integrity. It’s because you fail to recognize that influence flows through multiple channels and others are willing to play differently.&lt;/p&gt;
    &lt;p&gt;The key is not to match their behavior but to factor it in. Instead of expecting fairness, anticipate asymmetry. And then get creative about how you play.&lt;/p&gt;
    &lt;p&gt;Being ethical doesn’t mean being passive but tactically awake.&lt;/p&gt;
    &lt;head rend="h2"&gt;Higher you go, more it’s an inverted funnel&lt;/head&gt;
    &lt;p&gt;Perhaps the most obvious point but also easy to forget especially if your career has been on autopilot so far.&lt;/p&gt;
    &lt;p&gt;There’s a bottleneck up top: fewer seats, more ambiguity, less structure and high subjectivity. It’s not just hard to get in but even harder to stay clear on what “good” even looks like.&lt;/p&gt;
    &lt;p&gt;This means you can do everything right and still get passed over. That’s not a verdict on your worth or ability, just geometry.&lt;/p&gt;
    &lt;p&gt;It also means that staying the course when things don’t go your way isn’t just a virtue but a practice. To play the long game, you have to keep showing up even after crushing disappointment without getting cynical of the process. Put differently, you need high levels of frustration tolerance.&lt;/p&gt;
    &lt;p&gt;Cliched? Yes, very much so, but also uncommon. It means if you can pull it off, it’s a source of power.&lt;/p&gt;
    &lt;head rend="h2"&gt;Know which game you’re choosing to play&lt;/head&gt;
    &lt;p&gt;There is no one game being played. There are multiple, overlapping games with different scoring systems. Some are playing to build long-term credibility; others for short-term visibility.&lt;/p&gt;
    &lt;p&gt;You can’t play them all and neither should you try.&lt;/p&gt;
    &lt;p&gt;The real problem is we slide into playing someone else's game without realizing it. We adopt the norms and metrics of others without checking if that’s the game we actually want to play, let alone win. So we end up optimizing for a role we don’t respect, or chasing promotions that hollow us out.&lt;/p&gt;
    &lt;p&gt;Whatever you’re doing, own it outright. Not just the upside but also the downside. If you're focused on building something lasting like developing others, or robust systems, you need to accept that visible status markers (titles, promotions, recognition) might not happen right away.&lt;/p&gt;
    &lt;p&gt;The danger isn't which path you pick, whether it's chasing promotions or maintaining your autonomy. The real disaster is to sleepwalk down a path while pretending you had no choice in the matter.&lt;/p&gt;
    &lt;head rend="h2"&gt;Watching your circle of control&lt;/head&gt;
    &lt;p&gt;An easy way to burn out is to focus relentlessly on things you care about but cannot actually influence. Over time, especially in large organizations, it's tempting to attribute everything to forces outside yourself. This induces organizational helplessness. A sense that nothing you do matters unless someone above says so.&lt;/p&gt;
    &lt;p&gt;Fight that, not with bluster, but with deliberate ownership of the space you control and influence. While experienced professionals often have more influence than they think, it's distributed differently than they expect.&lt;/p&gt;
    &lt;p&gt;The key is maintaining an internal locus of control which includes your positioning, relationships, and what you are building.&lt;/p&gt;
    &lt;head rend="h2"&gt;Keep a balanced portfolio&lt;/head&gt;
    &lt;p&gt;This is a well-understood concept in investing but often missing in the context of long careers. If all your identity is wrapped up in organizational validation, you're fragile. This means setbacks don't just rattle your job, it rattles your sense of self.&lt;/p&gt;
    &lt;p&gt;Many mid-career professionals learn this the hard way. To state the obvious: you are not your title, or your most recent performance review.&lt;/p&gt;
    &lt;p&gt;The anti-dote is diversification not of money, but meaning.&lt;/p&gt;
    &lt;p&gt;Rebalancing here means investing in other sources of connection and community. This includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Developing a craft that exists beyond a given employer.&lt;/item&gt;
      &lt;item&gt;Investing in communities that outlast org charts.&lt;/item&gt;
      &lt;item&gt;Projects, relationships, and sources of learning that replenish.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You are building adaptive capacity.&lt;/p&gt;
    &lt;p&gt;A balanced portfolio also helps to play the long game with more psychological courage because your whole life isn't riding on the next promotion cycle or external validation.&lt;/p&gt;
    &lt;head rend="h2"&gt;In closing&lt;/head&gt;
    &lt;p&gt;By recognizing the subjective currents that shape work environments, we can operate within them more skillfully.&lt;/p&gt;
    &lt;p&gt;This isn't cynicism or gaming the system. Rather, it's developing a nuanced understanding of how organizations actually function. This stance equips us to make more intentional choices about how to engage, contribute, and create meaning.&lt;/p&gt;
    &lt;p&gt;As ideal as it sounds, the goal isn't to eliminate organizational absurdities, but to work effectively within and around them. By staying in the game, you find ways to gradually improve the system from within.&lt;/p&gt;
    &lt;p&gt;Organizations are ultimately human constructs. Imperfect, but not immutable.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45450525</guid><pubDate>Thu, 02 Oct 2025 14:58:04 +0000</pubDate></item><item><title>Signal Protocol and Post-Quantum Ratchets</title><link>https://signal.org/blog/spqr/</link><description>&lt;doc fingerprint="ac708592c921c484"&gt;
  &lt;main&gt;
    &lt;p&gt;We are excited to announce a significant advancement in the security of the Signal Protocol: the introduction of the Sparse Post Quantum Ratchet (SPQR). This new ratchet enhances the Signal Protocol’s resilience against future quantum computing threats while maintaining our existing security guarantees of forward secrecy and post-compromise security.&lt;/p&gt;
    &lt;p&gt;The Signal Protocol is a set of cryptographic specifications that provides end-to-end encryption for private communications exchanged daily by billions of people around the world. After its publication in 2013, the open source Signal Protocol was adopted not only by the Signal application but also by other major messaging products. Technical information on the Signal Protocol can be found in the specifications section of our docs site.&lt;/p&gt;
    &lt;p&gt;In a previous blog post, we announced the first step towards advancing quantum resistance for the Signal Protocol: an upgrade called PQXDH that incorporates quantum-resistent cryptographic secrets when chat sessions are established in order to protect against harvest-now-decrypt-later attacks that could allow current chat sessions to become compromised if a sufficiently powerful quantum computer is developed in the future. However, the Signal Protocol isn’t just about protecting cryptographic material and keys at the beginning of a new chat or phone call; it’s also designed to minimize damage and heal from compromise as that conversation continues.&lt;/p&gt;
    &lt;p&gt;We refer to these security goals as Forward Secrecy (FS) and Post-Compromise Security (PCS). FS and PCS can be considered mirrors of each other: FS protects past messages against future compromise, while PCS protects future messages from past compromise. Today, we are happy to announce the next step in advancing quantum resistance for the Signal Protocol: an additional regularly advancing post-quantum ratchet called the Sparse Post Quantum Ratchet, or SPQR. On its own, SPQR provides secure messaging that provably achieves these FS and PCS guarantees in a quantum safe manner. We mix the output of this new ratcheting protocol with Signal’s existing Double Ratchet, in a combination we refer to as the Triple Ratchet.&lt;/p&gt;
    &lt;p&gt;What does this mean for you as a Signal user? First, when it comes to your experience using the app, nothing changes. Second, because of how we’re rolling this out and mixing it in with our existing encryption, eventually all of your conversations will move to this new protocol without you needing to take any action. Third, and most importantly, this protects your communications both now and in the event that cryptographically relevant quantum computers eventually become a reality, and it allows us to maintain our existing security guarantees of forward secrecy and post-compromise security as we proactively prepare for that new world.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Current State of the Signal Protocol&lt;/head&gt;
    &lt;p&gt;The original Signal ratchet uses hash functions for FS and a set of elliptic-curve Diffie Hellman (ECDH) secret exchanges for PCS. The hash functions are quantum safe, but elliptic-curve cryptography is not. An example is in order: our favorite users, Alice and Bob, establish a long-term connection and chat over it regularly. During that session’s lifetime, Alice and Bob regularly agree on new ECDH secrets and use them to “ratchet” their session. Mean ol’ Mallory records the entire (encrypted) communication, and really wants to know what Alice and Bob are talking about.&lt;/p&gt;
    &lt;p&gt;The concept of a “ratchet” is crucial to our current non-quantum FS/PCS protection. In the physical world, a ratchet is a mechanism that allows a gear to rotate forward, but disallows rotation backwards. In the Signal Protocol, it takes on a similar role. When Alice and Bob “ratchet” their session, they replace the set of keys they were using prior with a new set based on both the older secrets and a new one they agree upon. Given access to those new secrets, though, there’s no (non-quantum) way to compute the older secrets. By being “one-way”, this ratcheting mechanism provides FS.&lt;/p&gt;
    &lt;p&gt;The ECDH mechanism allows Alice and Bob to generate new, small (32 bytes) data blobs and attach them to every message. Whenever each party receives a message from the other, they can locally (and relatively cheaply) use this data blob to agree on a new shared secret, then use that secret to ratchet their side of the protocol. Crucially, ECDH also allows Alice and Bob to both agree on the new secret without sending that secret itself over their session, and in fact without sending anything over the session that Mallory could use to determine it. This description of Diffie-Hellman key exchange provides more details on the concepts of such a key exchange, and this description of ECDH provides specific details on the variant used by the current Signal protocol.&lt;/p&gt;
    &lt;p&gt;Sometime midway through the lifetime of Alice and Bob’s session, Mallory successfully breaches the defences of both Alice and Bob, gaining access to all of the (current) secrets used for their session at the time of request. Alice and Bob should have the benefits of Forward Secrecy - they’ve ratcheted sometime recently before the compromise, so no messages earlier than their last ratchet are accessible to Mallory, since ratcheting isn’t reversible. They also retain the benefits of Post-Compromise Security. Their ratcheting after Mallory’s secret access agrees upon new keys that can’t be gleaned just from the captured data they pass between each other, re-securing the session.&lt;/p&gt;
    &lt;p&gt;Should Mallory have access to a quantum computer, though, things aren’t so simple. Because elliptic curve cryptography is not quantum resistant, it’s possible that Mallory could glean access to the secret that Alice and Bob agreed upon, just by looking at the communication between them. Given this, Alice and Bob’s session will never “heal”; Mallory’s access to their network traffic from this point forward will allow her to decrypt all future communications.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mixing In Quantum Security&lt;/head&gt;
    &lt;p&gt;In order to make our security guarantees stand up to quantum attacks, we need to mix in secrets generated from quantum secure algorithms. In PQXDH, we did this by performing an additional round of key agreement during the session-initiating handshake, then mixing the resulting shared secret into the initial secret material used to create Signal sessions. To handle FS and PCS, we need to do continuous key agreement, where over the lifetime of a session we keep generating new shared secrets and mixing those keys into our encryption keys.&lt;/p&gt;
    &lt;p&gt;Luckily there is a tool designed exactly for this purpose: the quantum-secure Key-Encapsulation Mechanism (KEM). KEMs share similar behavior to the Diffie-Hellman mechanisms we described earlier, where two clients provide each other with information, eventually deciding on a shared secret, without anyone who intercepts their communications being able to access that secret. However, there is one important distinction for KEMs - they require ordered, asymmetric messages to be passed between their clients. In ECDH, both clients send the other some public parameters, and both combine these parameters with their locally held secrets and come up with an identical shared secret. In the standardized ML-KEM key-encapsulation mechanism, though, the initiating client generates a pair of encapsulation key (EK) and decapsulation key (DK) (analogous to a public and private key respectively) and sends the EK. The receiving client receives it, generates a secret, and wraps it into a ciphertext (CT) with that key. The initiating client receives that CT and decapsulates with its previously generated DK. In the end, both clients have access to the new, shared secret, just through slightly different means.&lt;/p&gt;
    &lt;p&gt;Wanting to integrate this quantum-secure key sharing into Signal, we could take a simple, naive approach for each session. When Alice initiates a session with Bob,&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Alice, with every message she sends, sends an EK&lt;/item&gt;
      &lt;item&gt;Bob, with every message he receives, generates a secret and a CT, and sends the CT back&lt;/item&gt;
      &lt;item&gt;Alice, on receiving a CT, extracts the secret with her DK and mixes it in&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This initially simple-looking approach, though, quickly runs into a number of issues we’ll need to address to make our protocol actually robust. First, encapsulation keys and CTs are large - over 1000 bytes each for ML-KEM 768, compared to the 32 bytes required for ECDH. Second, while this protocol works well when both clients are online, what happens when a client is offline? Or a message is dropped or reordered? Or Alice wants to send 10 messages before Bob wants to send one?&lt;/p&gt;
    &lt;p&gt;Some of these problems have well-understood solutions, but others have trade-offs that may shine in certain circumstances but fall short in others. Let’s dive in and come to some conclusions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Who Wants What When&lt;/head&gt;
    &lt;p&gt;How does Alice decide what to send based on what Bob needs next, and vice versa? If Bob hasn’t received an EK yet, she shouldn’t send the next one. What does Bob send when he hasn’t yet received an EK from Alice, or when he has, but he’s already responded to it? This is a common problem when remote parties send messages to communicate, so there’s a good, well-understood solution: a state machine. Alice and Bob both keep track of “what state am I in”, and base their decisions on that. When sending or receiving a message, they might also change their state. For example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Alice wants to send a message, but she’s in a StartingA state, so she doesn’t have an EK. So, she generates an EK/DK pair, stores them locally, and transitions to the SendEK state&lt;/item&gt;
      &lt;item&gt;Alice wants to send a message and is in the SendEK state. She sends the EK along with the message&lt;/item&gt;
      &lt;item&gt;Alice wants to send another message, but she’s still in the SendEK state. So, she sends the EK with the new message as well&lt;/item&gt;
      &lt;item&gt;Bob receives the message with the EK. He generates a secret and uses the EK to create a CT. He transitions to the SendingCT state.&lt;/item&gt;
      &lt;item&gt;Bob wants to send a message and he’s in the SendingCT state. He sends the CT along with the message&lt;/item&gt;
      &lt;item&gt;Bob wants to send a message and he’s in the SendingCT state. He sends the CT along with the message&lt;/item&gt;
      &lt;item&gt;… etc …&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By crafting a set of states and transitions, both sides can coordinate what’s sent. Note, though, that even in this simple case, we see problems. For example, we’re sending our (large) EK and (large) CT multiple times.&lt;/p&gt;
    &lt;head rend="h2"&gt;Say (or Send) Less&lt;/head&gt;
    &lt;p&gt;We’ve already mentioned that the size of the data we’re sending has increased pretty drastically, from 32 bytes to over 1000 per message. But bandwidth is expensive, especially on consumer devices like client phones, that may be anywhere in the world and have extremely varied costs for sending bytes over the wire. So let’s discuss strategies for conserving that bandwidth.&lt;/p&gt;
    &lt;p&gt;First, the simplest approach - don’t send a new key with every message. Just, for example, send with every 50 messages, or once a week, or every 50 messages unless you haven’t sent a key in a week, or any other combination of options. All of these approaches tend to work pretty well in online cases, where both sides of a session are communicating in real-time with no message loss. But in cases where one side is offline or loss can occur, they can be problematic. Consider the case of “send a key if you haven’t sent one in a week”. If Bob has been offline for 2 weeks, what does Alice do when she wants to send a message? What happens if we can lose messages, and we lose the one in fifty that contains a new key? Or, what happens if there’s an attacker in the middle that wants to stop us from generating new secrets, and can look for messages that are 1000 bytes larger than the others and drop them, only allowing keyless messages through?&lt;/p&gt;
    &lt;p&gt;Another method is to chunk up a message. Want to send 1000 bytes? Send 10 chunks of 100 bytes each. Already sent 10 chunks? Resend the first chunk, then the second, etc. This smooths out the total number of bytes sent, keeping individual message sizes small and uniform. And often, loss of messages is handled. If chunk 1 was lost, just wait for it to be resent. But it runs into an issue with message loss - if chunk 99 was lost, the receiver has to wait for all of chunks 1-98 to be resent before it receives the chunk it missed. More importantly, if a malicious middleman wants to stop keys from being decided upon, they could always drop chunk 3, never allowing the full key to pass between the two parties.&lt;/p&gt;
    &lt;p&gt;We can get around all of these issues using a concept called erasure codes. Erasure codes work by breaking up a larger message into smaller chunks, then sending those along. Let’s consider our 1000 byte message being sent as 100 byte chunks again. After chunk #10 has been sent, the entirety of the original 1000 byte message has been sent along in cleartext. But rather than just send the first chunk over again, erasure codes build up a new chunk #11, and #12, etc. And they build them in such a way that, once the recipient receives any 10 chunks in any order, they’ll be able to reconstruct the original 1000 byte message.&lt;/p&gt;
    &lt;p&gt;When we put this concept of erasure code chunks together with our previous state machine, it gives us a way to send large blocks of data in small chunks, while handling messages that are dropped. Crucially, this includes messages dropped by a malicious middleman: since any N chunks can be used to recreate the original message, a bad actor would need to drop all messages after #N-1 to disallow the data to go through, forcing them into a complete (and highly noticeable) denial of service. Now, if Alice wants to send an EK to Bob, Alice will:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Transition from the StartingA state to the SendingEK state, by generating a new EK and chunking it&lt;/item&gt;
      &lt;item&gt;While in the SendingEK state, send a new chunk of the EK along with any messages she sends&lt;/item&gt;
      &lt;item&gt;When she receives confirmation that the recipient has received the EK (when she receives a chunk of CT), transition to the ReceivingCT state&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On Bob’s side, he will:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Transition from the StartingB state to the ReceivingEK state when he receives its first EK chunk&lt;/item&gt;
      &lt;item&gt;Keep receiving EK chunks until he has enough to reconstruct the EK&lt;/item&gt;
      &lt;item&gt;At that point, reconstruct the EK, generate the CT, chunk the CT, and transition to the SendingCT state&lt;/item&gt;
      &lt;item&gt;From this point on, he will send a chunk of the CT with every message&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;One interesting way of looking at this protocol so far is to consider the messages flowing from Alice to Bob as potential capacity for sending data associated with post-quantum ratcheting: each message that we send, we could also send additional data like a chunk of EK or of the CT. If we look at Bob’s side, above, we notice that sometimes he’s using that capacity (IE: in step 4 when he’s sending CT chunks) and sometimes he’s not (if he sends a message to Alice during step 2, he has no additional data to send). This capacity is pretty limited, so using more of it gives us the potential to speed up our protocol and agree on new secrets more frequently.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Meditation On How Faster Isn’t Always Better&lt;/head&gt;
    &lt;p&gt;We want to generate shared secrets, then use them to secure messages. So, does that mean that we want to generate shared secrets as fast as possible? Let’s introduce a new term: an epoch. Alice and Bob start their sessions in epoch 0, sending the EKs for epoch 1 (EK#1) and associated ciphertext (CT#1) to each other. Once that process completes, they have a new shared secret they use to enter epoch 1, after which all newly sent messages are protected by the new secret. Each time they generate a new shared secret, they use it to enter a new epoch. Surely, every time we enter a new epoch with a new shared secret, we protect messages before that secret (FS) and after that secret (PCS), so faster generation is better? It seems simple, but there’s an interesting complexity here that deserves attention.&lt;/p&gt;
    &lt;p&gt;First, let’s discuss how to do things faster. Right now, there’s a lot of capacity we’re not using: Bob sends nothing while Alice sends an EK, and Alice sends nothing while Bob sends a CT. Speeding this up isn’t actually that hard. Let’s change things so that Alice sends EK#1, and once Bob acknowledges its receipt, Alice immediately generates and sends EK#2. And once she notices Bob has received that, she generates and sends EK#3, etc. Whenever Alice sends a new message, she always has data to send along with it (new EK chunks), so she’s using its full capacity. Bob doesn’t always have a new CT to send, but he is receiving EKs as fast as Alice can send them, so he often has a new CT to send along.&lt;/p&gt;
    &lt;p&gt;But now let’s consider what happens when an attacker gains access to Alice. Let’s say that Alice has sent EK#1 and EK#2 to Bob, and she’s in the process of sending EK#3. Bob has acknowledged receipt of EK#1 and EK#2, but he’s still in the process of sending CT#1, since in this case Bob sends fewer messages to Alice than vice versa. Because Alice has already generated 3 EKs she hasn’t used, Alice needs to keep the associated DK#1, DK#2, and DK#3 around. So, if at this point someone maliciously gains control of Alice’s device, they gain access to both the secrets associated with the current epoch (here, epoch 0) and to the DKs necessary to reconstruct the secrets to other epochs (here, epochs 1, 2, and 3) using only the over-the-wire CT that Bob has yet to send. This is a big problem: by generating secrets early, we’ve actually made the in-progress epochs and any messages that will be sent within them less secure against this single-point-in-time breach.&lt;/p&gt;
    &lt;p&gt;To test this out, we at Signal built a number of different state machines, each sending different sets of data either in parallel or serially. We then ran these state machines in numerous simulations, varying things like the ratio of messages sent by Alice vs Bob, the amount of data loss or reordering, etc. And while running these simulations, we tracked what epochs’ secrets were exposed at any point in time, assuming an attacker were to breach either Alice’s or Bob’s secret store. The results showed that, in general, while simulations that handled multiple epochs’ secrets in parallel (IE: by sending EK#2 before receipt of CT#1) did generate new epochs more quickly, they actually made more messages vulnerable to a single breach.&lt;/p&gt;
    &lt;head rend="h2"&gt;But Let’s Still Be Efficient&lt;/head&gt;
    &lt;p&gt;This still leaves us with a problem, though: the capacity present in messages we send in either direction is still a precious resource, and we want to use it as efficiently as possible. And our simple approach of Alice’s “send EK, receive CT, repeat” and Bob’s “receive EK, send CT, repeat” leaves lots of time where Alice and Bob have nothing to send, should that capacity be available.&lt;/p&gt;
    &lt;p&gt;To improve our use of our sending capacity, we decided to take a harder look into the ML-KEM algorithm we’re using to share secrets, to see if there was room to improve. Let’s break things down more and share some actual specifics on the ML-KEM algorithm.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Alice generates an EK of 1184 bytes to send to Bob, and an associated DK&lt;/item&gt;
      &lt;item&gt;Bob receives the EK&lt;/item&gt;
      &lt;item&gt;Bob samples a new shared secret (32 bytes), which he encrypts with EK into a CT of 1088 bytes to send to Alice&lt;/item&gt;
      &lt;item&gt;Alice receives the CT, uses the DK to decrypt it, and now also has access to the 32 byte shared secret&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Diving in further, we can break out step #3 into some sub-steps&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Alice generates an EK of 1184 bytes to send to Bob, and an associated DK&lt;/item&gt;
      &lt;item&gt;Bob receives the EK&lt;/item&gt;
      &lt;item&gt;Bob samples a new shared secret (32 bytes), which he encrypts with EK into a CT of 1088 bytes to send to Alice1&lt;list rend="ol"&gt;&lt;item&gt;Bob creates a new shared secret S and sampled randomness R by sampling entropy and combining it with a hash of EK&lt;/item&gt;&lt;item&gt;Bob hashes the EK into a Hash&lt;/item&gt;&lt;item&gt;Bob pulls 32 bytes of the EK, a Seed&lt;/item&gt;&lt;item&gt;Bob uses the Seed and R to generate the majority of the CT&lt;/item&gt;&lt;item&gt;Bob then uses S and EK to generate the last portion of the CT&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Alice receives the CT, uses the DK to decrypt it, and now also has access to the 32 byte shared secret&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Step 3.d, which generates 960 bytes of the 1088-byte CT, only needs 64 bytes of input: a Seed that’s 32 of EK’s bytes, and the hash of EK, which is an additional 32. If we combine these values and send them first, then most of EK and most of the CT can be sent in parallel from Alice to Bob and Bob to Alice respectively. Our more complicated but more efficient secret sharing now looks like this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Alice generates EK and DK. Alice extracts the 32-byte Seed from EK&lt;/item&gt;
      &lt;item&gt;Alice sends 64 bytes EK1 (Seed + Hash(EK)) to Bob. Bob sends nothing during this time.&lt;/item&gt;
      &lt;item&gt;Bob receives the Seed and Hash, and generates the first, largest part of the CT from them (CT1)&lt;/item&gt;
      &lt;item&gt;After this point, Alice sends EK2 (the rest of the EK minus the Seed), while Bob sends CT1&lt;/item&gt;
      &lt;item&gt;Bob eventually receives EK2, and uses it to generate the final portion of the CT (CT2)&lt;/item&gt;
      &lt;item&gt;Once Alice tells Bob that she has received all of CT1, Bob sends Alice CT2. Alice sends nothing during this time.&lt;/item&gt;
      &lt;item&gt;With both sides having all of the pieces of EK and the CT that they need, they extract their shared secret and increment their epoch&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are still places in this algorithm (specifically steps 2 and 6) where one side has nothing to send. But during those times, the other side has only a very small amount of information to send, so the duration of those steps is minimal compared to the rest of the process. Specifically, while the full EK is 37 chunks and the full CT is 34, the two pieces of the new protocol which must be sent without data being received (EK1 and CT2) are 2 and 4 chunks respectively, while the pieces that can be sent while also receiving (EK2 and CT1) are the bulk of the data, at 36 and 30 chunks respectively. Far more of our sending capacity is actually used with this approach.&lt;/p&gt;
    &lt;p&gt;Remember that all of this is just to perform a quantum-safe key exchange that gives us a secret we can mix into the bigger protocol. To help us organize our code, our security proofs, and our understanding better we treat this process as a standalone protocol that we call the ML-KEM Braid.&lt;/p&gt;
    &lt;p&gt;This work was greatly aided by the authors of the libcrux-ml-kem Rust library, who graciously exposed the APIs necessary to work with this incremental version of ML-KEM 768. With this approach completed, we’ve been able to really efficiently use the sending capacity of messages sent between two parties to share secrets as quickly as possible without exposing secrets from multiple epochs to potential attackers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mixing Things Up - The Triple Ratchet&lt;/head&gt;
    &lt;p&gt;There are plenty of details to add to make sure that we reached every corner - check those out in our online protocol documentation - but this basic idea lets us build secure messaging that has post-quantum FS and PCS without using up anyone’s data. We’re not done, though! Remember, at the beginning of this post we said we wanted post-quantum security without taking away our existing guarantees.&lt;/p&gt;
    &lt;p&gt;While today’s Double Ratchet may not be quantum safe, it provides a high level of security today and we believe it will continue to be strong well into the future. We aren’t going to take that away from our users. So what can we do?&lt;/p&gt;
    &lt;p&gt;Our answer ends up being really simple: we run both the Double Ratchet and the Sparse Post Quantum Ratchet alongside each other and mix their keys together, into what we’re calling the Triple Ratchet protocol. When you want to send a message you ask both the Double Ratchet and SPQR “What encryption key should I use for the next message?” and they will both give you a key (along with some other data you need to put in a message header). Instead of either key being used directly, both are passed into a Key Derivation Function - a special function that takes random-enough inputs and produces a secure cryptographic key that’s as long as you need. This gives you a new “mixed” key that has hybrid security. An attacker has to break both our elliptic curve and ML-KEM to even be able to distinguish this key from random bits. We use that mixed key to encrypt our message.&lt;/p&gt;
    &lt;p&gt;Receiving messages is just as easy. We take the message header - remember it has some extra data in it - and send it to the Double Ratchet and SPQR and ask them “What key should I use to decrypt a message with this header?” They both return their keys and you feed them both into that Key Derivation Function to get your decryption key. After that, everything proceeds just like it always has.&lt;/p&gt;
    &lt;head rend="h2"&gt;Heterogeneous Rollout&lt;/head&gt;
    &lt;p&gt;So we’ve got this new, snazzy protocol, and we want to roll it out to all of our users across all of their devices… but none of the devices currently support that protocol. We roll it out to Alice, and Alice tries to talk to Bob, but Alice speaks SPQR and Bob doesn’t. Or we roll it out to Bob, but Alice wants to talk to Bob and Alice doesn’t know the new protocol Bob wants to use. How do we make this work?&lt;/p&gt;
    &lt;p&gt;Let’s talk about the simplest option: allowing downgrades. Alice tries to establish a session with Bob using SPQR and sends a message over it. Bob fails to read the message and establish the session, because Bob hasn’t been upgraded yet. Bob sends Alice an error, so Alice has to try again. This sounds fine, but in practice it’s not tenable. Consider what happens if Alice and Bob aren’t online at the same time… Alice sends a message at 1am, then shuts down. Bob starts up at 3am, sends an error, then shuts down. Alice gets that error when she restarts at 5am, then resends. Bob starts up at 7am and finally gets the message he should have received at 3am, 4 hours behind schedule.&lt;/p&gt;
    &lt;p&gt;To handle this, we designed the SPQR protocol to allow itself to downgrade to not being used. When Alice sends her first message, she attaches the SPQR data she would need to start up negotiation of the protocol. Noticing that downgrades are allowed for this session, Alice doesn’t mix any SPQR key material into the message yet. Bob ignores that data, because it’s in a location he glosses over, but since there’s no mixed in keys yet, he can still decrypt the message. He sends a response that lacks SPQR data (since he doesn’t yet know how to fill it in), which Alice receives. Alice sees a message without SPQR data, and understands that Bob doesn’t speak SPQR yet. So, she downgrades to not using it for that session, and they happily talk without SPQR protection.&lt;/p&gt;
    &lt;p&gt;There’s some scary potential problems here… let’s work through them. First off, can a malicious middleman force a downgrade and disallow Alice and Bob from using SPQR, even if both of them are able to? We protect against that by having the SPQR data attached to the message be MAC’d by the message-wide authentication code - a middleman can’t remove it without altering the whole message in such a way that the other party sees it, even if that other party doesn’t speak SPQR. Second, could some error cause messages to accidentally downgrade sometime later in their lifecycle, due either to bugs in the code or malicious activity? Crucially, SPQR only allows a downgrade when it first receives a message from a remote party. So, Bob can only downgrade if he receives his first message from Alice and notices that she doesn’t support SPQR, and Alice will only downgrade if she receives her first reply from Bob and notices that he doesn’t. After that first back-and-forth, SPQR is locked in and used for the remainder of the session.&lt;/p&gt;
    &lt;p&gt;Finally, those familiar with Signal’s internal workings might note that Signal sessions last a really long time, potentially years. Can we ever say “every session is protected by SPQR”, given that SPQR is only added to new sessions as they’re being initiated? To accomplish this, Signal will eventually (once all clients support the new protocol) roll out a code change that enforces SPQR for all sessions, and that archives all sessions which don’t yet have that protection. After the full rollout of that future update, we’ll be able to confidently assert complete coverage of SPQR.&lt;/p&gt;
    &lt;p&gt;One nice benefit to setting up this “maybe downgrade if the other side doesn’t support things” approach is that it also sets us up for the future: the same mechanisms that allow us to choose between SPQR or no-SPQR are designed to also allow us to upgrade from SPQR to some far-future (as yet unimagined) SPQRv2.&lt;/p&gt;
    &lt;head rend="h2"&gt;Making Sure We Get It Right&lt;/head&gt;
    &lt;p&gt;Complex protocols require extraordinary care. We have to ensure that the new protocol doesn’t lose any of the security guarantees the Double Ratchet gives us. We have to ensure that we actually get the post-quantum protection we’re aiming for. And even then, after we have full confidence in the protocol, we have to make sure that our implementation is correct and robust and stays that way as we maintain it. This is a tall order.&lt;/p&gt;
    &lt;p&gt;To make sure we got this right, we started by building the protocol on a firm foundation of fundamental research. We built on the years of research the academic community has put into secure messaging and we collaborated with researchers from PQShield, AIST, and NYU to explore what was possible with post-quantum secure messaging. In a paper at Eurocrypt 25 we introduced erasure code based chunking and proposed the high-level Triple Ratchet protocol, proving that it gave us the post-quantum security we wanted without taking away any of the security of the classic Double Ratchet. In a follow up paper at USENIX 25, we observed that there are many different ways to design a post-quantum ratchet protocol and we need to pick the one that protects user messages the best. We introduced and analyzed six different protocols and two stood out: one is essentially SPQR, the other is a protocol using a new KEM, called Katana, that we designed just for ratcheting. That second one is exciting, but we want to stick to standards to start!&lt;/p&gt;
    &lt;head rend="h2"&gt;Formal Verification From the Start&lt;/head&gt;
    &lt;p&gt;This research gave us the framework to think about protocol design and prove protocols are secure, but there is a big leap from an academic paper to code. Already when designing PQXDH - a much simpler protocol! - we found that formal verification was an important tool for getting the details right. With the Triple Ratchet we partnered with Cryspen and made formal verification part of the process from the beginning.&lt;/p&gt;
    &lt;p&gt;As we kept finding better protocol candidates - and we implemented around a dozen of them - we modeled them in ProVerif to prove that they had the security properties we needed. Rather than wrapping up a protocol design and performing formal verification as a last step we made it a core part of the design process. Now that the design is settled, this gives us machine verified proof that our protocol has the security properties we demand from it. We wrote our Rust code to closely match the ProVerif models, so it is easy to check that we’re modeling what we implement. In particular, ProVerif is very good at reasoning about state machines, which we’re already using, making the mapping from code to model much simpler.&lt;/p&gt;
    &lt;p&gt;We are taking formal verification further than that, though. We are using hax to translate our Rust implementation into F* on every CI run. Once the F* models are extracted, we prove that core parts of our highly optimized implementation are correct, that function pre-conditions and post-conditions cannot be violated, and that the entire crate is panic free. That last one is a big deal. It is great for usability, of course, because nobody wants their app to crash. But it also matters for correctness. We aggressively add assertions about things we believe must be true when the protocol is running correctly - and we crash the app if they are false. With hax and F*, we prove that those assertions will never fail.&lt;/p&gt;
    &lt;head rend="h2"&gt;Formal Verification Doesn’t Freeze Our Progress&lt;/head&gt;
    &lt;p&gt;Often when people think about formally verified protocol implementations, they imagine a one-time huge investment in verification that leaves you with a codebase frozen in time. This is not the case here. We re-run formal verification in our CI pipeline every time a developer pushes a change to GitHub. If the proofs fail then the build fails, and the developer needs to fix it. In our experience so far, this is usually as simple as adding a pre- or postcondition or returning an error when a value is out of bounds. For us, formal verification is a dynamic part of the development process and ensures that the quality is high on every merge.&lt;/p&gt;
    &lt;head rend="h2"&gt;TLDR&lt;/head&gt;
    &lt;p&gt;Signal is rolling out a new version of the Signal Protocol with the Triple Ratchet. It adds the Sparse Post-Quantum Ratchet, or SPQR, to the existing Double Ratchet to create a new Triple Ratchet which gives our users quantum-safe messaging without taking away any of our existing security promises. It’s being added in such a way that it can be rolled out without disruption. It’s relatively lightweight, not using much additional bandwidth for each message, to keep network costs low for our users. It’s resistant to meddling by malicious middlemen - to disrupt it, all messages after a certain time must be dropped, causing a noticeable denial of service for users. We’re rolling it out slowly and carefully now, but in such a way that we’ll eventually be able to say with confidence “every message sent by Signal is protected by this.” Its code has been formally verified, and will continue to be so even as future updates affect the protocol. It’s the combined effort of Signal employees and external researchers and contributors, and it’s only possible due to the continued work and diligence of the larger crypto community. And as a user of Signal, our biggest hope is that you never even notice or care. Except one day, when headlines scream “OMG, quantum computers are here”, you can look back on this blog post and say “oh, I guess I don’t have to care about that, because it’s already been handled”, as you sip your Nutri-Algae while your self-driving flying car wends its way through the floating tenements of Megapolis Prime.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Those that are interested can look at https://nvlpubs.nist.gov/nistpubs/fips/nist.fips.203.pdf and note that Algorithm 17 uses randomness plus the hash of EK to generate a shared secret and random value, then that random value is used in Algorithm 14 to create c1. The rest of ekPKE is only used by Algorithm 14 to generate c2. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45451527</guid><pubDate>Thu, 02 Oct 2025 16:06:10 +0000</pubDate></item><item><title>Launch HN: Simplex (YC S24) – Browser automation platform for developers</title><link>https://www.simplex.sh/</link><description>&lt;doc fingerprint="f4968ef0cf13274b"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Browser automation&lt;lb/&gt;for developers&lt;/head&gt;&lt;p&gt;Simplex provides all the infrastructure needed for modern browser automation. &lt;lb/&gt; Remote browsers, steerable web agents, and more.&lt;/p&gt;&lt;head rend="h3"&gt;A team from world class institutions&lt;/head&gt;&lt;head rend="h2"&gt;Watch a live demo of Simplex automating&lt;lb/&gt; a real billing portal.&lt;/head&gt;&lt;head rend="h3"&gt;Live Session Stream&lt;/head&gt;&lt;p&gt;Demo Preview&lt;/p&gt;&lt;head rend="h3"&gt;Live Demo Logs&lt;/head&gt;&lt;p&gt;Agent logs will appear here when demo is running&lt;/p&gt;&lt;head rend="h2"&gt;Engineered from the ground up to work with legacy systems.&lt;/head&gt;&lt;p&gt;Reliably automate every legacy portal your customers use.&lt;/p&gt;&lt;head rend="h3"&gt;Billing Portals&lt;/head&gt;&lt;p&gt;Simplex has been used to log into a billing portal and download the list of invoices for a specified customer.&lt;/p&gt;&lt;head rend="h3"&gt;Prior Authorization Portals&lt;/head&gt;&lt;p&gt;Simplex has been used to fill out complex, branching-logic prior authorization forms on medical provider portals.&lt;/p&gt;&lt;head rend="h3"&gt;ERPs&lt;/head&gt;&lt;p&gt;Simplex has been used to automate data entry and download report PDFs across different ERPs.&lt;/p&gt;&lt;head rend="h3"&gt;Government Portals&lt;/head&gt;&lt;p&gt;Simplex has been used to search and extract structured information across public government portals.&lt;/p&gt;&lt;head rend="h3"&gt;TMS/WMS Software&lt;/head&gt;&lt;p&gt;Simplex has been used to log into a TMS portal, create and edit the information for a shipment, then dispatch the shipment.&lt;/p&gt;&lt;head rend="h3"&gt;... and more&lt;/head&gt;&lt;p&gt;with us to discuss your specific use case.&lt;/p&gt;&lt;table&gt;&lt;row span="6"&gt;&lt;cell role="head"&gt;Order ID&lt;/cell&gt;&lt;cell role="head"&gt;Customer&lt;/cell&gt;&lt;cell role="head"&gt;Status&lt;/cell&gt;&lt;cell role="head"&gt;Amount&lt;/cell&gt;&lt;cell role="head"&gt;Last Updated&lt;/cell&gt;&lt;cell role="head"&gt;Actions&lt;/cell&gt;&lt;/row&gt;&lt;row span="6"&gt;&lt;cell&gt;PO-2024-001&lt;/cell&gt;&lt;cell&gt;John Smith&lt;/cell&gt;&lt;cell&gt;PENDING&lt;/cell&gt;&lt;cell&gt;$1,234.56&lt;/cell&gt;&lt;cell&gt;01/15/2024&lt;/cell&gt;&lt;/row&gt;&lt;row span="6"&gt;&lt;cell&gt;PO-2024-002&lt;/cell&gt;&lt;cell&gt;Jane Doe&lt;/cell&gt;&lt;cell&gt;PROCESSING&lt;/cell&gt;&lt;cell&gt;$987.65&lt;/cell&gt;&lt;cell&gt;01/14/2024&lt;/cell&gt;&lt;/row&gt;&lt;row span="6"&gt;&lt;cell&gt;PO-2024-003&lt;/cell&gt;&lt;cell&gt;Bob Johnson&lt;/cell&gt;&lt;cell&gt;COMPLETED&lt;/cell&gt;&lt;cell&gt;$2,345.67&lt;/cell&gt;&lt;cell&gt;01/13/2024&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;PO-2024-004&lt;/cell&gt;&lt;cell&gt;Alice Brown&lt;/cell&gt;&lt;cell&gt;ERROR&lt;/cell&gt;&lt;cell&gt;$876.54&lt;/cell&gt;&lt;cell&gt;01/12/2024&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h2"&gt;Deploy reliably, scale easily.&lt;/head&gt;&lt;head rend="h3"&gt;Run consistent workflows&lt;/head&gt;&lt;p&gt;Simplex automatically caches agent actions. This increases reliability of runs and makes developing flows lightning fast.&lt;/p&gt;&lt;head rend="h3"&gt;Create realtime flows&lt;/head&gt;We've achieved realtime latency to handle complex workflows during phone calls.&lt;p&gt;if low latency flows are a priority for you.&lt;/p&gt;&lt;quote&gt;simplex.click(“New Order”)simplex.click(“Shipment Address”)simplex.type(“”)&lt;/quote&gt;&lt;head rend="h2"&gt;Simplex just works.&lt;/head&gt;&lt;head rend="h3"&gt;Production-ready&lt;/head&gt;&lt;p&gt;Eval harnesses to stress-test your workflows at scale and emulate production conditions.&lt;/p&gt;&lt;head rend="h3"&gt;Authentication Handling&lt;/head&gt;&lt;p&gt;Authentication SDK functions to handle 2FA, login data, and more on your customers' sites. See more here.&lt;/p&gt;&lt;head rend="h3"&gt;Scalable Headless Browsers&lt;/head&gt;&lt;p&gt;Headless browsers that can scale to 100s of concurrent sessions in seconds.&lt;/p&gt;&lt;head rend="h3"&gt;Stealth Mode&lt;/head&gt;&lt;p&gt;Automatic CAPTCHA solving, proxies, and anti-bot protections.&lt;/p&gt;&lt;head rend="h3"&gt;Controllable Workflows&lt;/head&gt;&lt;p&gt;Our web agents are constrained to only take the actions you tell it to.&lt;/p&gt;&lt;head rend="h3"&gt;Optimized Workflows&lt;/head&gt;&lt;p&gt;Automatically cache your workflows for fast and reliable execution in production.&lt;/p&gt;&lt;head rend="h3"&gt;Robust SDKs&lt;/head&gt;&lt;p&gt;Our SDKs are designed to be robust and easy to use. Available in both Python and TypeScript.&lt;/p&gt;&lt;head rend="h3"&gt;Detailed Logging and Replays&lt;/head&gt;&lt;p&gt;View a livestream and live logs of sessions as they happen. Share session replays and detailed agent logs with your team and customers.&lt;/p&gt;&lt;head rend="h2"&gt;Ready to get started?&lt;/head&gt;&lt;p&gt;Book a call with our team to discuss your use case and get started.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45451547</guid><pubDate>Thu, 02 Oct 2025 16:07:23 +0000</pubDate></item><item><title>Playball – Watch MLB games from a terminal</title><link>https://github.com/paaatrick/playball</link><description>&lt;doc fingerprint="607031164bc1d92f"&gt;
  &lt;main&gt;
    &lt;p&gt;Watch MLB games from the comfort of your own terminal&lt;/p&gt;
    &lt;p&gt;MLB Gameday and MLB.tv are great, but sometimes you want to keep an eye on a game a bit more discreetly. &lt;code&gt;playball&lt;/code&gt; puts the game in a terminal window.&lt;/p&gt;
    &lt;p&gt;Just want to try it out?&lt;/p&gt;
    &lt;code&gt;$ npx playball
&lt;/code&gt;
    &lt;p&gt;Ready for the big leagues? Install the package globally&lt;/p&gt;
    &lt;code&gt;$ npm install -g playball
&lt;/code&gt;
    &lt;p&gt;Then run it&lt;/p&gt;
    &lt;code&gt;$ playball
&lt;/code&gt;
    &lt;code&gt;$ docker build -t playball .
$ docker run -it --rm --name playball playball:latest
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;key&lt;/cell&gt;
        &lt;cell role="head"&gt;action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;q&lt;/cell&gt;
        &lt;cell&gt;quit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;c&lt;/cell&gt;
        &lt;cell&gt;go to schedule view&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;s&lt;/cell&gt;
        &lt;cell&gt;go to standings view&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;key&lt;/cell&gt;
        &lt;cell role="head"&gt;action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;↓/j, ↑/k, ←/h, →/l&lt;/cell&gt;
        &lt;cell&gt;change highlighted game&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;enter&lt;/cell&gt;
        &lt;cell&gt;view highlighted game&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;p&lt;/cell&gt;
        &lt;cell&gt;show previous day's schedule/results&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;n&lt;/cell&gt;
        &lt;cell&gt;show next day's schedule&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;t&lt;/cell&gt;
        &lt;cell&gt;return to today's schedule&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;key&lt;/cell&gt;
        &lt;cell role="head"&gt;action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;↓/j, ↑/k&lt;/cell&gt;
        &lt;cell&gt;scroll list of all plays&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Playball can be configured using the &lt;code&gt;config&lt;/code&gt; subcommand. To list the current configuration values run the subcommand with no additional arguments:&lt;/p&gt;
    &lt;code&gt;playball config&lt;/code&gt;
    &lt;p&gt;You should see output similar to:&lt;/p&gt;
    &lt;code&gt;color.ball = green
color.favorite-star = yellow
color.in-play-no-out = blue
color.in-play-out = white
color.in-play-runs-bg = white
color.in-play-runs-fg = black
color.on-base = yellow
color.other-event = white
color.out = red
color.strike = red
color.strike-out = red
color.walk = green
favorites = 
&lt;/code&gt;
    &lt;p&gt;To get the value of a single setting pass the key as an additional argument:&lt;/p&gt;
    &lt;code&gt;playball config color.strike&lt;/code&gt;
    &lt;p&gt;To change a setting pass the key and value as arguments:&lt;/p&gt;
    &lt;code&gt;playball config color.strike blue&lt;/code&gt;
    &lt;p&gt;To revert a setting to its default value provide the key and the &lt;code&gt;--unset&lt;/code&gt; flag:&lt;/p&gt;
    &lt;code&gt;playball config color.strike --unset&lt;/code&gt;
    &lt;p&gt;This table summarizes the available settings:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;key&lt;/cell&gt;
        &lt;cell role="head"&gt;description&lt;/cell&gt;
        &lt;cell role="head"&gt;default&lt;/cell&gt;
        &lt;cell role="head"&gt;allowed values&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.ball&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of dots representing balls in top row of game view&lt;/cell&gt;
        &lt;cell&gt;green&lt;/cell&gt;
        &lt;cell&gt;One of the following: &lt;code&gt;black&lt;/code&gt;, &lt;code&gt;red&lt;/code&gt;, &lt;code&gt;green&lt;/code&gt;, &lt;code&gt;yellow&lt;/code&gt;, &lt;code&gt;blue&lt;/code&gt;, &lt;code&gt;magenta&lt;/code&gt;, &lt;code&gt;cyan&lt;/code&gt;, &lt;code&gt;white&lt;/code&gt;, &lt;code&gt;grey&lt;/code&gt;. Any of those colors may be prefixed by &lt;code&gt;bright-&lt;/code&gt; or &lt;code&gt;light-&lt;/code&gt; (for example &lt;code&gt;bright-green&lt;/code&gt;). The exact color used will depend on your terminal settings. The value &lt;code&gt;default&lt;/code&gt; may be used to specify the default text color for your terminal. Finally hex colors (e.g &lt;code&gt;#FFA500&lt;/code&gt;) can be specified. If your terminal does not support true color, the closest supported color may be used.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.favorite-star&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of star indiciating favorite team in schedule and standing views&lt;/cell&gt;
        &lt;cell&gt;yellow&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.in-play-no-out&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of result where ball was put in play and no out was made (single, double, etc) in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;blue&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.in-play-out&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of result where ball was put in play and an out was made (flyout, fielder's choice, etc) in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;white&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.in-play-runs-bg&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Background color for score update in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;white&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.in-play-runs-fg&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Foreground color for score update in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;black&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.on-base&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of diamonds representing runners on base in top row of game view&lt;/cell&gt;
        &lt;cell&gt;yellow&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.other-event&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of other events (mound visit, injury delay, etc) in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;white&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.out&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of dots representing outs in top row of game view&lt;/cell&gt;
        &lt;cell&gt;red&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.strike&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of dots representing strikes in top row of game view&lt;/cell&gt;
        &lt;cell&gt;red&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.strike-out&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of result where play ends on a strike (strike out) in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;red&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.walk&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of result where play ends on a ball (walk, hit by pitch) in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;green&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;favorites&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Teams to highlight in schedule and standings views&lt;/cell&gt;
        &lt;cell&gt;Any one of the following: &lt;code&gt;ATL&lt;/code&gt;, &lt;code&gt;AZ&lt;/code&gt;, &lt;code&gt;BAL&lt;/code&gt;, &lt;code&gt;BOS&lt;/code&gt;, &lt;code&gt;CHC&lt;/code&gt;, &lt;code&gt;CIN&lt;/code&gt;, &lt;code&gt;CLE&lt;/code&gt;, &lt;code&gt;COL&lt;/code&gt;, &lt;code&gt;CWS&lt;/code&gt;, &lt;code&gt;DET&lt;/code&gt;, &lt;code&gt;HOU&lt;/code&gt;, &lt;code&gt;KC&lt;/code&gt;, &lt;code&gt;LAA&lt;/code&gt;, &lt;code&gt;LAD&lt;/code&gt;, &lt;code&gt;MIA&lt;/code&gt;, &lt;code&gt;MIL&lt;/code&gt;, &lt;code&gt;MIN&lt;/code&gt;, &lt;code&gt;NYM&lt;/code&gt;, &lt;code&gt;NYY&lt;/code&gt;, &lt;code&gt;OAK&lt;/code&gt;, &lt;code&gt;PHI&lt;/code&gt;, &lt;code&gt;PIT&lt;/code&gt;, &lt;code&gt;SD&lt;/code&gt;, &lt;code&gt;SEA&lt;/code&gt;, &lt;code&gt;SF&lt;/code&gt;, &lt;code&gt;STL&lt;/code&gt;, &lt;code&gt;TB&lt;/code&gt;, &lt;code&gt;TEX&lt;/code&gt;, &lt;code&gt;TOR&lt;/code&gt;, &lt;code&gt;WSH&lt;/code&gt;. Or a comma-separated list of multiple (e.g. &lt;code&gt;SEA,MIL&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;git clone https://github.com/paaatrick/playball.git
cd playball
npm install
npm start
&lt;/code&gt;
    &lt;p&gt;Contributions are welcome!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45451577</guid><pubDate>Thu, 02 Oct 2025 16:09:15 +0000</pubDate></item><item><title>Why I chose Lua for this blog</title><link>https://andregarzia.com/2025/03/why-i-choose-lua-for-this-blog.html</link><description>&lt;doc fingerprint="7f443511072312dc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why I chose Lua for this blog&lt;/head&gt;
    &lt;p&gt;This blog used to run using with a stack based on Racket using Pollen and lots of hacks on top of it. At some point I realised that my setup was working against me. The moving parts and workflow I created added too much friction to keep my blog active. That happened mostly because it was a static generator trying to behave as if it was dynamic website with an editing interface. That can be done really well â cue Grav CMS â but that was not the case for me.&lt;/p&gt;
    &lt;p&gt;Once I decided to rewrite this blog as a simpler system, I faced the dilema of what stack to choose. The obvious choice for me would be Javascript, it is the language I use more often and one that I am quite confortable with. Still, I don't think it is a wise choice for the kind of blog I want to maintain.&lt;/p&gt;
    &lt;p&gt;Talking to some friends recently, I noticed that many people I know that have implemented their own blogging systems face many challenges keeping them running over many years. Not because it is hard to keep software running, but because their stack of choice is moving faster than their codebase.&lt;/p&gt;
    &lt;p&gt;This problem is specially prevalent in the Javascript world. It is almost a crime that JS as understood by the browser is this beautiful language with extreme retrocompatibility, while JS as understood and used by the current tooling and workflows is this mess moving at lightspeed. Let me unpack that for a bit.&lt;/p&gt;
    &lt;p&gt;You can open a web page from 1995 on your browser of choice and it will just work because browser vendors try really hard to make sure they don't break the web.&lt;/p&gt;
    &lt;p&gt;Developers who built the whole ecosystem of NodeJS, NPM, and all those libraries and frameworks don't share the same ethos. They all make a big case of semantic versioning and thus being able to handle breaking changes, but they have breaking changes all the time. You'd be hardpressed to actually run some JS code from ten years ago based on NodeJS and NPM. There is a big chance that dependencies might be gone, broken, or it might be incompatible with the current NodeJS.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I know this sounds like FUD, and that for many many projects, maybe even most projects, that will not be the case. But I heard from many people that keeping their blogging systems up to date requires a lot more work than they would like to do and if they don't, then they're screwed.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That is also true about other languages even though many of them move at a slower speed. A friend recently complained about a blogging system he implemented that requires Ruby 2.0 and that keeping that running sucks.&lt;/p&gt;
    &lt;p&gt;I want a simpler blogging system; one that requires minimal changes over time.&lt;/p&gt;
    &lt;head rend="h3"&gt;Now we talk about Lua&lt;/head&gt;
    &lt;p&gt;Lua is a wonderful and nimble language that is often misunderstood.&lt;/p&gt;
    &lt;p&gt;One characteristic that I love about it, is that is evolves very slowly. Lua 5.1 was introduced in 2006, Lua 5.4 which is the current version initial release was in 2020. Yes, there are point released in between, but you can see how much slower it moves when compared to JS.&lt;/p&gt;
    &lt;p&gt;The differences between Lua 5.1 and Lua 5.4 are minimal when compared with how much other languages changed in the same time period.&lt;/p&gt;
    &lt;p&gt;Lua only requires a C89 compiler to bootstrap itself. It is very easy to make Lua work and even easier to make it interface with something.&lt;/p&gt;
    &lt;p&gt;JS is a lot larger than Lua, there is more to understand and more to remember. My blog needs are very simple and Lua can handle them with ease.&lt;/p&gt;
    &lt;head rend="h2"&gt;How this blog works&lt;/head&gt;
    &lt;p&gt;This is an old-school blog. I uses cgi-bin â aka Comon Gateway Interface â scripts to run it. It is a dynamic website with a SQLite database holding its data. When you open a page, it fetches the data from a database and assembles a HTML to send to the browser using Mustache templates.&lt;/p&gt;
    &lt;p&gt;One process per request. Like the old days.&lt;/p&gt;
    &lt;p&gt;You might argue that if I went with NodeJS, I'd be able to serve more requests using fewer resources. That is true. I don't need to serve that many requests though. My peak access was a couple years ago with 50k visitors on a week, even my old Racket blog could handle that fine. The Lua one should handle it too; and if it breaks it breaks. I'm a flawed human being, my code can be flawed too, we're in this together, holding hands.&lt;/p&gt;
    &lt;p&gt;Your blog is your place to experiment and program how you want it. You can drop the JS fatigue, you can drop your fancy Haskell types, you can just do whatever you find fun and keep going (and that includes JS and Haskell if that's your thing. You do you).&lt;/p&gt;
    &lt;p&gt;Cause I'm using Lua, I don't have as many libraries and frameworks available to me as JS people have, but I still have quite a large collection via Luarocks. I try not to add many dependencies to my blog. At the moment there are about ten and that is mostly because Lua is a batteries-not-included language so you start from a minimal core and build things up to suit your needs.&lt;/p&gt;
    &lt;p&gt;For a lot of things I went with the questionable choice of implementing things myself. I got my own little CGI library. It is 200 lines long and does the bare minimum to make this blog work. I got my own little libraries for many things. Micropub and IndieAuth were all implemented by hand.&lt;/p&gt;
    &lt;p&gt;At the moment I'm &lt;del&gt;despairing&lt;/del&gt;&lt;del&gt;frustrated&lt;/del&gt; having a lot of fun implementing WebMentions. Doing the Microformats2 &lt;del&gt;exorcism&lt;/del&gt; extraction on my own is teaching me a lot of things.&lt;/p&gt;
    &lt;p&gt;What I want to say is that by choosing a small language that moves very slowly and very few dependencies, I can keep all of my blogging system in my head. I can make sure it will run without too much change for the next ten or twenty years.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Lua is a lego set, a toolkit, it adapts to you and your needs. I don't need to keep chasing the new shiny or the latest framework du jour. I can focus on making the features I want and actually understanding how they work.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Instead of installing a single dependency in another language and it pulling a hundred of other small dependencies all of which were transpiled into something the engine understands to the point that understanding how all the pieces work and fit together takes more time than to learn a new language, I decided to keep things simple.&lt;/p&gt;
    &lt;p&gt;I got 29 Luarocks installed here and that is for all my Lua projects in this machine. That is my blog, my game development, my own work scripts for my day job. Not even half of those are for my blog.&lt;/p&gt;
    &lt;p&gt;I often see wisdom in websites such as Hacker News and Lobsters around the idea of "choosing boring" because it is proven, safe, easier to maintain. I think that boring is not necessarily applicable to my case. I don't find Lua boring at all, but all that those blog posts talk about that kind of mindset are all applicable to my own choices here.&lt;/p&gt;
    &lt;p&gt;Next time you're building your own blogging software, consider for a bit for how long do you want to maintain it. I first started blogging on macOS 8 in 2001. I choose badly many times and in the end couldn't keep my content moving forward in time with me as softwares I used or created became impossible to run. The last two changes: from JS to Racket and from Racket to Lua have been a lot safer and I managed to carry all my content forward into increasingly simpler setups and workflows.&lt;/p&gt;
    &lt;p&gt;My blogging system is not becoming more complex over the years, it is becoming smaller, because with each change I select a stack that is more nimble and smaller than the one I had before. I don't think I can go smaller than Lua though.&lt;/p&gt;
    &lt;p&gt;By small I mean:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A language I can fully understand and keep on my head.&lt;/item&gt;
      &lt;item&gt;A language that I know how to build the engine and can do it if needed.&lt;/item&gt;
      &lt;item&gt;An engine that requires very few resources and is easy to interface with third-party libraries in native code.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I chose Lua because of all that, and I'm happy with it and hope this engine will see me through the next ten or so years.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45452261</guid><pubDate>Thu, 02 Oct 2025 16:58:55 +0000</pubDate></item><item><title>Liva AI (YC S25) Is Hiring</title><link>https://www.ycombinator.com/companies/liva-ai/jobs/6xM8JYU-founding-operations-lead</link><description>&lt;doc fingerprint="a3de624f74d97bb3"&gt;
  &lt;main&gt;
    &lt;p&gt;Scale AI for video and voice data.&lt;/p&gt;
    &lt;p&gt;The mission at Liva AI (YC S25) is to make AI feel truly human. AI voices and faces today still feel flat and generic, missing emotion, nuance, and identity. We’re fixing that by building the world’s richest library of human voice and video data, fueling the next generation of realistic AI.&lt;/p&gt;
    &lt;p&gt;We’re hiring an extremely organized and committed operator to take on a full-time role. You’ll manage complex projects and people with efficiency, solve problems in uncertain situations, and help us scale fast. Over time, you’ll also play a key role in building the most automated operations system of any data company, translating the workflows you run today into scalable processes and overseeing the internal systems we’re developing.&lt;/p&gt;
    &lt;p&gt;This is a founding role: your work will directly fuel the next generation of AI in a tangible way, while shaping the foundation of how Liva runs at scale.&lt;/p&gt;
    &lt;p&gt;ABOUT THE ROLE&lt;/p&gt;
    &lt;p&gt;What you’ll do:&lt;/p&gt;
    &lt;p&gt;WHAT WE’RE LOOKING FOR&lt;/p&gt;
    &lt;p&gt;Requirements:&lt;/p&gt;
    &lt;p&gt;Nice to have:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; BENEFITS:&lt;/p&gt;
    &lt;p&gt;Liva's mission is to make AI look and sound truly human. The AI voices and faces today feel off, and lack the capability to reflect diverse people across different ethnicities, races, accents, and career professions. We’re fixing that by building the world’s richest library of human voice and video data, fueling the next generation of realistic AI.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45452299</guid><pubDate>Thu, 02 Oct 2025 17:01:16 +0000</pubDate></item><item><title>Babel is why I keep blogging with Emacs</title><link>https://entropicthoughts.com/why-stick-to-emacs-blog</link><description>&lt;doc fingerprint="26d8bd04cf2550b8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why I Keep Blogging With Emacs&lt;/head&gt;
    &lt;p&gt;Every time I look at someone’s simple static site generation setup for their blog, I feel a pang of envy. I’m sure I could make a decent blogging engine in 2,000 lines of code, and it would be something I’d understand, be proud over, able to extend, and willing to share with others.&lt;/p&gt;
    &lt;p&gt; Instead, I write these articles in Org mode, and use mostly the standard Org publishing functions to export them to html. This is sometimes brittle, but most annoyingly, I don’t understand it. I have been asked for details on how my publishing flow works, but the truth is I have no idea what happens when I run the &lt;code&gt;org-publish-current-file&lt;/code&gt; command.
&lt;/p&gt;
    &lt;p&gt; I could find out by tracing the evaluation of the Lisp code that runs on export, but I won’t, because just the html exporting code (&lt;code&gt;ox-html.el&lt;/code&gt;) is 5,000
lines of complexity. The general exporting framework (&lt;code&gt;ox-publish.el&lt;/code&gt; and
&lt;code&gt;ox.el&lt;/code&gt;) is 8,000 lines. The framework depends on Org parsing code
(&lt;code&gt;org-element.el&lt;/code&gt;) which is at least another 9,000 lines. This is over 20,000
lines of complexity I’d need to contend with.
&lt;/p&gt;
    &lt;p&gt;It might seem like a no-brainer to just write that 2,000 line custom static generator and use that instead.&lt;/p&gt;
    &lt;p&gt;Except one thing: Babel.&lt;/p&gt;
    &lt;p&gt;Any lightweight markup format (like Markdown or ReStructuredText or whatever) allows for embedding code blocks, but Org, through Babel, can run that code on export, and then display the output in the published document, even when the output is a table or an image. It supports sessions that lets code reuse definitions from earlier code blocks. It allows for injecting variables from the markup into the code, and vice versa. As a bonus, Org doesn’t require a JavaScript syntax highlighter, because it generates inline styles in the source code.&lt;/p&gt;
    &lt;p&gt;It does this for a large number of languages, although I mainly use it with R for drawing plots. Being able to do this is incredibly convenient, because it makes it trivial to draft data, illustrations, and text at the same time, adjusting both until the article coheres. Having tried it, I cannot see myself living without it.&lt;/p&gt;
    &lt;p&gt;A simple 2,000 line blogging engine would be a fun weekend project. Mirroring the features of Babel I use would turn it into a multi-month endeavour for someone with limited time such as myself. Not going to happen, and I will continue to beat myself up for overcomplicating my publishing workflow.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45453222</guid><pubDate>Thu, 02 Oct 2025 18:06:41 +0000</pubDate></item><item><title>The Answer (1954)</title><link>https://sfshortstories.com/?p=5983</link><description>&lt;doc fingerprint="73199b7271208fff"&gt;
  &lt;main&gt;
    &lt;p&gt;The Answer by Fredric Brown (Angels and Spaceships, 1954) opens with a scientist called Dwar Ev completing a connection and then moving towards a switch:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The switch that would connect, all at once, all of the monster computing machines of all the populated planets in the universe—ninety-six billion planets—into the supercircuit that would connect them all into one super-calculator, one cybernetics machine that would combine all the knowledge of all the galaxies. p. 36&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Ev then asks the super-computer if there is a God, and it replies (spoiler), “Yes, now there is a God”. Then, when Ev rushes towards the switch to turn the computer off, it zaps him with a lightning bolt.&lt;lb/&gt;This is one of these squibs (it is less than a page long) that you find (a) pretty neat when you are twelve, but (b) a not very good gimmick story when older. The real sense of wonder here lies in the idea of ninety-six billion inhabited and interconnected planets.&lt;lb/&gt;* (Mediocre). 250 words. Story link.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45453299</guid><pubDate>Thu, 02 Oct 2025 18:13:24 +0000</pubDate></item><item><title>Gemini 3.0 Pro – early tests</title><link>https://twitter.com/chetaslua/status/1973694615518880236</link><description>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45453448</guid><pubDate>Thu, 02 Oct 2025 18:26:57 +0000</pubDate></item><item><title>OpenAI's H1 2025: $4.3B in income, $13.5B in loss</title><link>https://www.techinasia.com/news/openais-revenue-rises-16-to-4-3b-in-h1-2025</link><description>&lt;doc fingerprint="d267d1111c03bee1"&gt;
  &lt;main&gt;
    &lt;p&gt;If you're seeing this message, that means JavaScript has been disabled on your browser.&lt;/p&gt;
    &lt;p&gt;Please enable JavaScript to make this website work.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45453586</guid><pubDate>Thu, 02 Oct 2025 18:37:28 +0000</pubDate></item><item><title>Why most product planning is bad and what to do about it</title><link>https://blog.railway.com/p/product-planning-improvement</link><description>&lt;doc fingerprint="ad29913538a777e8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why most product planning is bad and what to do about it&lt;/head&gt;
    &lt;p&gt;TL;DR: We tried OKRs, they created more ceremony than clarity. Our solution: Problem Driven Development, a 4-day quarterly process focused on identifying problems (not solutions), prioritizing as a team, and committing publicly. It's kept us shipping at velocity even as we've scaled to 1.7M+ users.&lt;/p&gt;
    &lt;p&gt;For most of my friends and colleagues at mature software companies, there are usually three ways for an item of work to get put on the board to eventually be done.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;There is a giant ceremony that determines what gets done by a grab bag of metrics.&lt;/item&gt;
      &lt;item&gt;A deal gets blocked by a missing feature, and the engineering team scrambles jets to eliminate the blocker.&lt;/item&gt;
      &lt;item&gt;Founder feels like we have to build something.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thats not to say that every company is a disorganized mess or a bureaucratic hell scape but, I have never met any engineer who said: “Wow, I just love my company planning process.”&lt;/p&gt;
    &lt;p&gt;These words are seldom spoken in the english language.&lt;/p&gt;
    &lt;p&gt;Railway, was fast approaching 1. and 2. at the same time.&lt;/p&gt;
    &lt;p&gt;Despite us using excellent tools (shoutout Linear) planning is much as a cultural phenomena as well as an interesting engineering problem. From our perspective, we would finalize the features and the requirements what we would want to build once every 3 months and then at times get blindsided every now and then from a new business priority or an incident. Especially now serving 1.7M+ customers, we have to aggregate our taste, feedback, and opportunities and make a roadmap that will get us everything we ever wanted.&lt;/p&gt;
    &lt;p&gt;This was an issue.&lt;/p&gt;
    &lt;p&gt;Why write about something that you would read in Rand’s? It’s Q4 for those who celebrate, and we felt that reigniting the agile vs. waterfall armistice needed to be torn up. (Besides, we’re blogging like it’s 2005.)&lt;/p&gt;
    &lt;p&gt;…and we have spent the better part of 18 months to improve our planning so that we finally get to a process that is not bad, and if you’d like, you can steal it, so that you can deliver excellent products.&lt;/p&gt;
    &lt;p&gt;Mike Tyson asked about his fight plan against Evander Holyfield responded with; “Everyone has a plan until they get punched in the mouth.”&lt;/p&gt;
    &lt;p&gt;And that quote ever since would be used out of context to fight the implementation of Objectives, Key Results across the world. At Railway, we aren’t Anti-Planning, we are Anti-Bad-Planning and as such, we used to avoid a lot of it because we always felt that a bad process is massively negative to the status quo. However, we did get to the point where we needed to finally harness the collective attention spans of the company and work towards a goal.&lt;/p&gt;
    &lt;p&gt;Railway at the time was (and is) a vision led company. We fundamentally believe that most infrastructure boilerplate shouldn’t exist so that you can work on what matters. Back in ‘21, we went from a world where we could do our work in weeks, to work that required us to think in months. I implemented a somewhat lighter form of SAFE which splits work between “sprints”, which were short pieces of work, then initiatives which helped grouped “epics” which are long running work-streams to deliver a feature.&lt;/p&gt;
    &lt;p&gt;Despite us having a strong founder, we still wanted the ability to have employees bring projects that they would be excited to work on.&lt;/p&gt;
    &lt;p&gt;As with most young companies, we found quickly that new work would come in and unseat the old work and we would begin anew.&lt;/p&gt;
    &lt;p&gt;So then we turned to OKRs.&lt;/p&gt;
    &lt;p&gt;We thought that having some long term objectives will help us focus the company.&lt;/p&gt;
    &lt;p&gt;We implemented them faithfully, we all read the John Doer book. …and it worked… somewhat.&lt;/p&gt;
    &lt;p&gt;We can make this a whole blog post on OKRs. But, trying to hold back here from just ranting and to give you the relevant information.&lt;/p&gt;
    &lt;p&gt;OKRs work REALLY well when you have concrete goals and really simple ways to measure them. Its why they belong at their primary birthplace, the factory. Objectives work when you have something binary or “limited”, like a product existing… or not. Or a product meeting a benchmark… or not. It’s easy to rally a team around them and conquer the world.&lt;/p&gt;
    &lt;p&gt;Where OKRs start to falter, are when you need to use them to prioritize work to meet a “unlimited” objective. Like: “Increase conversion rate of landing page”&lt;/p&gt;
    &lt;p&gt;OKRheads will notice that’s a particularly weak objective, however, teaching an entire organization to all of a sudden be great goal setters, which is what OKRs require you to do made it difficult for engineers to 1) bring them and then 2) plan work around them. Which leads to the first big issue of OKRs- which is that it really depends on the human psychology of the team. Engineers straddle the line between concrete numbers such as uptime, and more creative endeavors such as figuring out how to get a feature implemented right. Whenever the work enters the creative realm, the wheels come off.&lt;/p&gt;
    &lt;p&gt;Which is why outside of the factory, OKRs are great for Sales. You set a number, you get alignment on hitting that number, and then you pick up the phone, email, or LinkedIn until you hit it. The psychology of an account executive matches the planning process.&lt;/p&gt;
    &lt;p&gt;Where OKRs work great, is for alignment. If you are able to set great goals and if you wanted to prescribe a bunch of work toward said goal. …and most startups reach for the High Output Management book (which I love) because there is the lack of alignment between different orgs at a startup. Which is to say, I don’t shame any company reaching for them, the same way newborns have the palmar grasp reflex. We yearn for the alignment.&lt;/p&gt;
    &lt;p&gt;The second big issue for OKRs is that they are, by design, inflexible, once you commit to them. If you spend a week or two planning for them for the year or quarter, and you are midway through realizing that the “KR” part is incorrect, thats a one way door.&lt;/p&gt;
    &lt;p&gt;So we felt pressure into making sure that the OKRs were indeed correct. Which is where you enter the issues of the performative aspects of planning. It felt “mature” for Railway to be having these discussions, even though… it wasn’t productive.&lt;/p&gt;
    &lt;p&gt;At Railway, we would spend a significant amount of time discussing what is a valid OKR, only for us to realize we were two days into the planning process and we’ve yet to decide what we should build for the OKR.&lt;/p&gt;
    &lt;p&gt;This is when Christian, our Head of Operations had an answer for us.&lt;/p&gt;
    &lt;p&gt;Instead of crowd sourcing the OKRs from the company and bubbling them up per function Jake and Christian would work on some top level guidance that would provide a macro view of our finances, priorities, and strategy that we have to keep in mind.&lt;/p&gt;
    &lt;p&gt;We then have a Hex dashboard with topline metrics from different sides of the product and business. From a GTM perspective, it’s engagement, signups, and revenue. From an Engineering perspective, it’s support tickets, uptime, and feature performance.&lt;/p&gt;
    &lt;p&gt;This data would give us a “theme”. (You may have seen them in our launch weeks.)&lt;/p&gt;
    &lt;p&gt;Whenever someone goes on Central Station and requests a feature, depending on how large the work is, that usually determines if that request becomes a “Project”.&lt;/p&gt;
    &lt;p&gt;Then around the start of 2024, Christian spun up a Notion DB, and punched in a simple template for each new entry.&lt;/p&gt;
    &lt;p&gt;Railway at this point and time was organized into three sections of the business. Product Engineering which delivers features and value to our customers via the UI and terminal. Platform which supports the product with the Infrastructure and the APIs to control that Infrastructure. Then Logistics, which is a synthetic team which includes the Support, Marketing, and Sales function working to be the voice of the customer.&lt;/p&gt;
    &lt;p&gt;We would then fill out a “Project Candidate” and then go to bat on trying to figure out if was a priority that tied to the “theme” that was presented.&lt;/p&gt;
    &lt;p&gt;If something is a P0: it’s an existential company risk, we MUST deliver it.&lt;/p&gt;
    &lt;p&gt;Then, a P1: something we need to deliver for this quarter&lt;/p&gt;
    &lt;p&gt;Lastly, P2: a nice to have.&lt;/p&gt;
    &lt;p&gt;This worked well until some cracks formed.&lt;/p&gt;
    &lt;p&gt;At the start of this new process, our team was small enough to list maybe 50 project candidates, discuss as a group and then be on with it but with 200. Spending all day on a call with your co-workers to discuss if something was worth it isn’t the best use of time.&lt;/p&gt;
    &lt;p&gt;Second, Project Candidates would sometimes have full on fleshed out RFC style sections where the author would have a solution ready to go. When a candidate got deprioritized, it’s understandable why someone would feel not great about their hard effort not being reciprocated.&lt;/p&gt;
    &lt;p&gt;Third, to deal with the larger amount of project candidates and additional sources of project candidates, we would set up additional meetings before the planning week that would eat into our engineering cycles. Not that we want to squeeze every engineer for what they are worth, but these negotiation calls would take longer than the main call sometimes. Worse even, was that we would completely reset the board quarter after quarter.&lt;/p&gt;
    &lt;p&gt;Lastly, which was starting to be a bigger issue, was that having Project Candidates being a bottom up process was great when the people who knew that work they needed to do can fit the whole work-stream in their head. But… we were about to embark on a multi-quarter effort that would span all parts of the business to deliver Railway Metal. This system was not great for uncovering planning gaps that required attention from a different team.&lt;/p&gt;
    &lt;p&gt;(With that said, we did ship some really good software thanks to this planning process.)&lt;/p&gt;
    &lt;p&gt;However, Jake, the team, aren’t ones to sit on their laurels so we refined it until we have our current process.&lt;/p&gt;
    &lt;p&gt;So we flipped the project system on it’s head.&lt;/p&gt;
    &lt;p&gt;Instead, we practice what I call: “Problem Driven Development”&lt;/p&gt;
    &lt;p&gt;Rather than spending a loaded amount of time picking feedback items from our feedback systems, and spending a loaded amount of time on trying to flesh out requirements. We collect problems on a continuous process that begins in earnest on the Friday before our planning week.&lt;/p&gt;
    &lt;p&gt;We stopped asking people to propose solutions and started asking them to articulate problems. No more half-baked RFCs that people felt attached to. No more solution-first thinking that locked us into approaches before we understood what we were solving. Just clear problem statements.&lt;/p&gt;
    &lt;p&gt;Here's what a problem looks like in our system:&lt;/p&gt;
    &lt;p&gt;Problem Title: "Users can't debug failed deployments without SSHing into containers"&lt;/p&gt;
    &lt;p&gt;Notice what's missing? Any mention of how we'd solve it. That comes later, after we've committed to solving the problem. Then we turned the two week ceremony into a lightning 4 day sprint.&lt;/p&gt;
    &lt;p&gt;After the problems are listed and filled out fully, each team on Day 1 enters problems on their own. If a problem isn't fleshed out, it's put in the Parking Lot and we kindly nudge the idea person to fill the template.&lt;/p&gt;
    &lt;p&gt;By the time we get into a room together, everyone's had time to process what's on the board.&lt;/p&gt;
    &lt;p&gt;Then on Day 2, the planning captain will have a closed session with the team (with spectators from other teams) to put a best guess priority from P0, P1, and P2. If we find that a problem is contentious or requires an external dependency, we mark it so the day after, we can discuss it.&lt;/p&gt;
    &lt;p&gt;By having each team prioritize independently first, we avoid the tragedy of the commons where everything becomes P0 because someone shouted loudest on the call. Platform can look at their problems and say "yes, API reliability is more important than API versioning right now" without having to negotiate with Product in real-time.&lt;/p&gt;
    &lt;p&gt;(We have spent countless hours talking about difficult questions on a call when we have been able to diffuse rough conversations ahead of time.)&lt;/p&gt;
    &lt;p&gt;On Day 3, we get to a 95 percent certainty on the priorities listed and tie break the dependencies. We also confirm that we have the capacity and staffing to deal with the problems listed on the board. If not, we add a problem for hiring for a specific role.&lt;/p&gt;
    &lt;p&gt;This is where cross-team dependencies surface naturally. When Platform marks "Multi-mount volumes" as P1 and Product marks "HA DBs" as P1, we can see that one blocks the other. We're not discovering this mid-quarter when someone's already three weeks into building the wrong thing.&lt;/p&gt;
    &lt;p&gt;The capacity check is crucial too. We look at people's current commitments, oncall rotations, and whether anyone's about to take parental leave. If we have eight P1 problems and capacity for five, we have an honest conversation about what drops to P2 or what we need to hire for.&lt;/p&gt;
    &lt;p&gt;Then lastly, Day 4, commitment. Everyone looks at the whole list, mention final objections if we should be working on something that we aren't— or vice versa. After we confirm priorities, we assign problems to people by giving them a DRI (Directly Responsible Individual).&lt;/p&gt;
    &lt;p&gt;Then, we look into each other's eyes, as much as anyone can on a video call, and commit.&lt;/p&gt;
    &lt;p&gt;After the starting gun is off, we then write RFCs on problems to see how we can best solve them which eventually become Linear tickets.&lt;/p&gt;
    &lt;p&gt;We're working on open-sourcing our Notion templates because honestly, this isn't rocket science. It's just:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Cataloguing your problems, not solutions&lt;/item&gt;
      &lt;item&gt;Let teams prioritize independently before negotiating&lt;/item&gt;
      &lt;item&gt;Front-load the hard conversations&lt;/item&gt;
      &lt;item&gt;Commit publicly&lt;/item&gt;
      &lt;item&gt;Then, and only then, figure out how to solve it&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But the real hard part was building a culture where people feel safe bringing up problems without having to defend their pet solutions.&lt;/p&gt;
    &lt;p&gt;Keep process to a minimum, focus on performance of shipping, not performative work.&lt;/p&gt;
    &lt;p&gt;And like any good product, our planning process will keep evolving. Maybe in six months we'll realize Problem Driven Development has its own cracks. Theres always plenty to do on planning and I think we generally know how it’ll need to evolve, but we try to take small steps from cycle to cycle. If that happens, we'll write another blog post about what we learned and what we changed.&lt;/p&gt;
    &lt;p&gt;Until then, we're shipping.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45454374</guid><pubDate>Thu, 02 Oct 2025 19:34:08 +0000</pubDate></item><item><title>Anti-aging breakthrough: Stem cells reverse signs of aging in monkeys</title><link>https://www.nad.com/news/anti-aging-breakthrough-stem-cells-reverse-signs-of-aging-in-monkeys</link><description>&lt;doc fingerprint="3648d18e38a2aa99"&gt;
  &lt;main&gt;
    &lt;p&gt;Key Points:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;“Super stem cells” improve the memory of monkeys while protecting against neurodegeneration.&lt;/item&gt;
      &lt;item&gt;The super stem cells prevent age-related bone loss while rejuvenating over 50% of the 61 tissues analyzed.&lt;/item&gt;
      &lt;item&gt;Treatment with stem cells reduces inflammation and senescent cells (cells that accumulate to promote aging).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While small in number, our adult stem cells play a crucial role in regenerating our lost or damaged tissues, rebuilding our body cell by cell. However, with age, our bodies become riddled with inflammation, hardly providing an environment capable of keeping our stem cells healthy. Eventually, our stem cells lose their regenerative capacity, contributing to degenerative aging.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fox, O, Three&lt;/head&gt;
    &lt;p&gt;Hydra are a genus of immortal beings that live forever in freshwater environments like lakes and ponds. What scientists have called “nothing but an active stem cell community,” Hydra can escape death by infinitely regenerating. Their stem cells can continuously proliferate and renew by producing FoxO, a protein they share with humans.&lt;/p&gt;
    &lt;p&gt;In humans, the FoxO protein, specifically the FoxO3 isoform, responds to cellular stress by binding to DNA and turning genes on and off. These genes are involved in numerous cellular processes that promote healthy aging and extended lifespan. This is why the FoxO3 protein’s corresponding gene, FOXO3, is considered a longevity gene.&lt;/p&gt;
    &lt;head rend="h2"&gt;Experimenting with SRCs&lt;/head&gt;
    &lt;p&gt;As FoxO3 assists cells in resisting stressful environments, such as inflamed tissue, Chinese Academy of Sciences researchers engineered human stem cells to have enhanced FoxO3 activity. As published in Cell, these senescence-resistant stem cells (SRCs) were designed to exhibit greater resistance to age-related stress. To test this, cynomolgus monkeys, also known as crab-eating macaques, were first stratified into four groups based on age:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A1: 3-5 years (approximately equivalent to 9-15 human years)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A2: 10-12 years (approximately equivalent to 30-36 human years)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A3: 16-19 years (approximately equivalent to 48-57 human years)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A4: 19-23 years (approximately equivalent to 57-69 human years)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The oldest of the monkeys, the A4 group, were the focus of the study and were subdivided into three groups. One group was injected with saline (salt and water), another group with normal stem cells, and the third with SRCs. The aged monkeys were injected every two weeks for 44 weeks, approximately equivalent to the duration of three human years. On safety, there were no serious adverse events, such as immune system rejection or tumor growth.&lt;/p&gt;
    &lt;head rend="h2"&gt;SRCs Improve Cognition&lt;/head&gt;
    &lt;p&gt;After 44 weeks of biweekly injections, a suite of biological indices was measured from the aged monkeys to assess whether SRCs slow down biological aging. One of these indices was memory retention. To assess memory, the researchers conducted a common experiment called the Wisconsin General Test Apparatus (WGTA).&lt;/p&gt;
    &lt;p&gt;For this experiment, each monkey was trained to retrieve food located outside of one of two identical boxes. During the test session, after each monkey was trained, food was placed next to one of the boxes to keep it hidden. Subsequently, a flap was placed in front of the monkey to block the boxes from view. Three seconds later, when the flap was reopened, each monkey had to remember which of the two boxes contained the food.&lt;/p&gt;
    &lt;p&gt;Remarkably, the monkeys injected with SRCs remembered the location of the food with higher accuracy than the monkeys injected with saline. Moreover, the monkeys injected with normal stem cells exhibited the same level of accuracy as the monkeys injected with saline. These findings suggest that SRCs, and not normal stem cells, improve the memory of aged monkeys.&lt;/p&gt;
    &lt;p&gt;Furthermore, MRI-based structural analysis showed that treatment with SRCs mitigated age-related brain shrinkage. MRI-based experiments also revealed that brain connectivity was restored to that of young (A1 group) monkeys. Namely, the structural connectivity between seven brain regions, including those important for working memory (prefrontal cortex), was rejuvenated with SRC treatment. Overall, these findings suggest that SRC injections improve memory by protecting against neurodegeneration.&lt;/p&gt;
    &lt;head rend="h2"&gt;SRCs Rejuvenate Many Organs and Tissues&lt;/head&gt;
    &lt;p&gt;In addition to the brain, the Academy researchers found that SRC treatment rejuvenated multiple organs and tissues. This is important because the rejuvenation of a given organ or tissue could lead to the reduced risk of its corresponding age-related chronic diseases. For example, the rejuvenating effects of SRCs on the brain could reduce the risk of neurodegenerative disorders like Alzheimer’s and Parkinson’s diseases.&lt;/p&gt;
    &lt;p&gt;One common age-related disease is osteoporosis, characterized by brittle and weak bones that make patients more prone to fractures and deadly falls. Using an X-ray imaging technique called micro-CT, the researchers found evidence for the reversal of age-related bone loss. Namely, while the aged monkeys treated with saline exhibited dental bone loss, the aged monkeys treated with SRCs had teeth more similar to those of young monkeys.&lt;/p&gt;
    &lt;p&gt;To conduct a body-wide assessment, the researchers measured the up- and down-regulation of genes from 10 systems and 61 tissues. Elevations and reductions in gene activation reflect the function (or dysfunction) of cells, tissues, and organ systems. With that said, SRC treatment was shown to rejuvenate over 50% of the tissues examined, with maximal rejuvenation achieved in areas like the hippocampus (the memory consolidation center of the brain), fallopian tubes, and colon. In contrast, the regular stem cells rejuvenated about 30% of the tissues examined.&lt;/p&gt;
    &lt;p&gt;Confirming some of the rejuvenating effects inferred by the gene experiments, the researchers also observed structural changes to various organs and tissues from aged monkeys treated with SRCs. For example, the vascularity of the lung and heart was improved while the thickening of the aorta was reduced. Neurons had longer projections and fewer proteins associated with Alzheimer’s disease (e.g., beta-amyloid and phosphorylated-tau), and the kidney and brain showed less mineralization [abnormal mineral (usually calcium) deposits].&lt;/p&gt;
    &lt;head rend="h2"&gt;SRCs Reduce Cellular Senescence and Inflammation&lt;/head&gt;
    &lt;p&gt;Modern scientists have begun to unravel the underlying causes of aging by identifying commonalities between age-related diseases at the cellular level. Two of the most prominent purported underlying causes of aging are chronic inflammation and senescent cells. With age, senescent cells accumulate throughout the body, promoting inflammation by secreting pro-inflammatory molecules.&lt;/p&gt;
    &lt;p&gt;Eliminating senescent cells, which can be achieved through genetic manipulation or compounds called senolytics, ameliorates age-related diseases and even extends the lifespan of model organisms. Now, the Academy researchers demonstrate that SRCs reduce senescent cells, measured using a blue dye called SA-β-Gal, in multiple organs, including the brain, heart, and lungs. Along those lines, SRC treatment also reduced markers of inflammation and other underlying causes of aging, like DNA damage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Stem Cells Make Sense&lt;/head&gt;
    &lt;p&gt;When it comes to combating degenerative aging, it makes sense that regenerative stem cells are a promising solution. In fact, one of the underlying causes of aging is stem cell exhaustion, whereby stem cells lose their regenerative capacity. While normal stem cells have anti-aging effects, as shown by the Chinese Academy of Sciences researchers, they are not protected against stressors like age-related inflammation. This explains why SRCs provide enhanced regenerative capacity (they withstand the harsh microenvironments induced by aging and cellular senescence).&lt;/p&gt;
    &lt;p&gt;As no serious safety concerns were raised during the study, it would seem that SRCs are well tolerated. However, the long-term effects of the SRC treatment will need further evaluation. The primary concern with injecting stem cells into the bloodstream is that they can trigger the spread of cancer almost anywhere in the body. Nevertheless, the SRCs possess tumor suppression properties, suggesting they may not induce tumor growth. If this ends up being true, we may soon see SRCs being tested in humans.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45454460</guid><pubDate>Thu, 02 Oct 2025 19:39:08 +0000</pubDate></item><item><title>Launching Solveit – an antidote to AI fatigue</title><link>https://www.answer.ai/posts/2025-10-01-solveit-full.html</link><description>&lt;doc fingerprint="29342a5f6be50e99"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Launching Solveit, the antidote to AI fatigue&lt;/head&gt;
    &lt;p&gt;It’s a strange time to be a programmer. It’s easier than ever to get started, but also easier than ever to let AI steer you into a situation where you’re overwhelmed by code you don’t understand. We’ve got an antidote that we’ve been using ourselves with 1000 preview users for the last year. It’s changed our lives at Answer.AI, and hundreds of our users say the same thing. Now we’re ready to share it with you. Signups are open, and will remain so until October 20th. Over five weeks, we’ll give you a taste of how our new approach and platform, “Solveit”, can be applied to everything from programming challenges, web development, and system administration to learning, writing, business, and more.&lt;/p&gt;
    &lt;p&gt;OK, let’s explain what on earth we’re talking about!…&lt;/p&gt;
    &lt;p&gt;At the end of last year, Jeremy Howard (co-founder of fast.ai, Answer.AI, Kaggle, Fastmail, creator of the first LLM…) and I ran a small trial course titled “How To Solve It With Code”. The response was so overwhelming that we had to close signups after just one day. 1000 keen beans joined us for a deep dive into our general approach to solving problems. The first few lessons were taught via the vehicle of the ‘Advent of Code’ programming challenges and run in a new, purpose-built tool called solveit. As the course progressed, we had lots of fun exploring web development, AI, business, writing and more. And the solveit tool became an extremely useful test-bed for ideas around AI-assisted coding, learning and exploration.&lt;/p&gt;
    &lt;p&gt;In the year since, we’ve continued to refine and expand both the process and the platform. We now basically live in the solveit platform. We do all our sysadmin work in it (Solveit itself is hosted on a new horizontally scalable multi-server platform we built and run entirely using Solveit!), host production apps in it (e.g all students in the course can use a Discord AI bot “Discord Buddy” that’s running inside a Solveit dialog!), develop most of our software in it, our legal team does contract drafting in it, we iterate on GUIs in it, and in fact we do the vast majority of our day to day work of all kinds in it.&lt;/p&gt;
    &lt;p&gt;From October 20th for five weeks, Jeremy and I will show you how to use the solveit approach, and give you full access to the platform that powers it (and you’ll have the option to continute to access the lessons and platform afterwards too). Also Eric Ries will join us for lessons about building startups that don’t just make money, but that stick to your vision for how you want to impact the world. You’ll be amongst the first people in the world to have the opportunity to read his new unreleased book.&lt;/p&gt;
    &lt;p&gt;But what IS “the solveit approach”? It isn’t some new AI thing, but actually is based on ideas that are at least 80 years old… To learn more, read on, or watch this video Jeremy and I recorded a few weeks ago.&lt;/p&gt;
    &lt;head rend="h2"&gt;Inspiration from Polya&lt;/head&gt;
    &lt;p&gt;George Polya was a Hungarian mathematician who wrote the influential book “How to Solve It” in 1945. In it, he shares his philosophies on education (focus on active learning, heuristic thinking, and careful questioning to guide students towards discovering answers for themselves) and outlines a four-step problem-solving framework:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Understand the Problem: identify what you’re being asked to do; restate the problem&lt;/item&gt;
      &lt;item&gt;Devise a Plan: draw on similar problems; break down into manageable parts; consider working backward; simplify the problem&lt;/item&gt;
      &lt;item&gt;Carry Out the Plan: verify each step&lt;/item&gt;
      &lt;item&gt;Look Back and Reflect: consider alternatives; extract lessons learned&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;He was focused on mathematics, but as Jeremy and I realized, these ideas translate far beyond maths! It turns out that it actually works great for coding, writing, reading, learning…&lt;/p&gt;
    &lt;p&gt;Of course, you can often just have AI code and write for you. But should you?&lt;/p&gt;
    &lt;p&gt;In most cases, we argue the answer is “no”.&lt;/p&gt;
    &lt;p&gt;There’s a myriad of problems waiting for you if you go down that path: - If you didn’t know the foundations of how to do it before, you don’t now either. You’ve learned nothing - If you keep working this way, you build up more and more code you don’t understand, creating technical and understanding debt that will eventually become crippling - You won’t be building up a foundation to solve harder tasks that neither humans nor AI can one-shot. So you’re limiting yourself to only solving problems that everyone else can trivially solve too. This is not a recipe for personal or organizational success!&lt;/p&gt;
    &lt;p&gt;On the other hand, if you build a discipline of always working to improve your understanding and expertise, you’ll discover that something delightful and amazing happens. Each time you tackle a task, you’ll find it’s a little easier than the last one. These improvements in understanding and capability will multiply, and you’ll find that your own skills develop even faster than AI improves. You’ll focus on using AI to help you dramatically increase your own productivity and abilities, instead of focusing on helping the AI improve its productivity and abilities!&lt;/p&gt;
    &lt;head rend="h2"&gt;Application to Coding: iterative, exploratory coding in notebook-like environments.&lt;/head&gt;
    &lt;p&gt;Let’s consider a quick example of coding the solveit way (without even any AI yet). For 2024’s Advent of Code, Day 1’s solution involves comparing two lists, sorted by value (there’s a whole backstory involving elves, which you can read if you like). Let’s imagine we’ve considered the problem, and are now focused on a small sub-task: extracting the first (sorted) list. We start with the sample data provided:&lt;/p&gt;
    &lt;code&gt;= '3   4\n4   3\n2   5\n1   3\n3   9\n3   3' x &lt;/code&gt;
    &lt;p&gt;Our plan might be:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Split into a list of lines&lt;/item&gt;
      &lt;item&gt;Grab the first number from each line&lt;/item&gt;
      &lt;item&gt;Sort&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After thinking through the plan, we begin working on individual steps. We aim to write no more than a few lines of code at a time, with each piece giving some useful output that you can use to verify that you’re on the right track:&lt;/p&gt;
    &lt;code&gt;= x.splitlines()
 lines 
 lines&amp;gt;&amp;gt;&amp;gt; ['3   4', '4   3', '2   5', '1   3', '3   9', '3   3']&lt;/code&gt;
    &lt;p&gt;Now we build up a list comprehension to get the first elements. We might start with &lt;code&gt;[o for o in lines]&lt;/code&gt; and then add bits one at a time, inspecting the output, building up to:&lt;/p&gt;
    &lt;code&gt;= [int(o.split()[0]) for o in lines]
 l1 
 l1&amp;gt;&amp;gt;&amp;gt; [3, 4, 2, 1, 3, 3]&lt;/code&gt;
    &lt;p&gt;Now sorting:&lt;/p&gt;
    &lt;code&gt;sorted(l1)
&amp;gt;&amp;gt;&amp;gt; [1, 2, 3, 3, 3, 4]&lt;/code&gt;
    &lt;p&gt;Now that we’ve run all the pieces individually, and checked that the outputs are what we’d expect, we can stack them together into a function:&lt;/p&gt;
    &lt;code&gt;def get_list(x):
= x.splitlines()
     lines = [int(o.split()[0]) for o in lines]
     l1 return sorted(l1)
     
 get_list(x)&amp;gt;&amp;gt;&amp;gt; [1, 2, 3, 3, 3, 4]&lt;/code&gt;
    &lt;p&gt;At this point, you’d reflect on the solution, think back to the larger plan, perhaps ask yourself if there are better ways you could do it. You may be thinking that this is far too much work for &lt;code&gt;sorted(int(line.split()[0]) for line in x.splitlines())&lt;/code&gt; – as your skill increases you can tailor the level of granularity, but the idea remains the same: working on small pieces of code, checking the outputs, only combining them into larger functions once you’ve tried them individually, and constantly reflecting back on the larger goal.&lt;/p&gt;
    &lt;p&gt;(We’ll come back to this shortly – but also consider for a moment how integrated AI can fit into the above process. Any time you don’t know how to do something, you can ask for help with just that one little step. Any time you don’t understand how something works, or why it doesn’t, you can have AI help you with that exact piece.)&lt;/p&gt;
    &lt;head rend="h2"&gt;The Power of Fast Feedback Loops&lt;/head&gt;
    &lt;p&gt;The superpower that this kind of live, iterative coding gives you is near-instant feedback loops. Instead of building your giant app, waiting for the code to upload, clicking through to a website and then checking a debug console for errors – you’re inspecting the output of a chunk of code and seeing if it matches what you expected. It’s still possible to make mistakes and miss edge cases, but it is a LOT easier to catch most mistakes early when you code in this way.&lt;/p&gt;
    &lt;p&gt;This idea of setting things up so that you get feedback as soon as possible pops up again and again. Our cofounder Eric Ries talks about this in his book ‘The Lean Startup’, where getting feedback from customers is valuable for quick iteration on product or business ideas. Kaggle pros talk about the importance of fast evals – if you can test an idea in 5 minutes, you can try a lot more ideas than you could if each experiment requires 12 hours of model training.&lt;/p&gt;
    &lt;head rend="h2"&gt;AI: Dialog Engineering Keeps Context Useful&lt;/head&gt;
    &lt;p&gt;One issue with current chat-based models is that once they go off the rails, it’s hard to get back on track. The model is now modelling a language sequence that involves the AI making mistakes – and more mistakes are likely to follow! If you’ve used language models much, then you’ve no doubt experienced this problem many times.&lt;/p&gt;
    &lt;p&gt;There is an interesting mathematical reason that this occurs. The vast majority of language model training is entirely about getting a neural network to predict the next word in a sentence – they are auto-regressive. Although they are later fine-tuned to do more than this, they are still at their heart really wanting to predict the next word of a sentence. In the documents used for training, there are plenty of examples of poor-quality reasoning and mistakes.Therefore, once an AI sees some mistakes in a chat, the most likely next tokens are going to be mistakes as well. That means that every time you are correcting the AI, you are making it more likely for the AI to give bad responses in the future!&lt;/p&gt;
    &lt;p&gt;Because solveit dialogs are fluid and editable, it’s much easier to go back and edit/remove mistakes, dead ends, and unrelated explorations. You can even edit past AI responses, to steer it into the kinds of behaviour you’d prefer. Combine this with the ability to easily hide messages from the AI or to pin messages to keep them in context even as the dialog grows beyond the context window and starts to be truncated, and you have a recipe for continued AI helpfulness as time goes on. We’ve been talking about this as “dialog engineering” for a long time – and it really is key to having AI work sessions that improve as time goes on, rather than degrading.&lt;/p&gt;
    &lt;p&gt;Of course, this is all useful for humans too! The discipline of keeping things tidy, using (collapsible) headings to organise sections, writing notes on what you’re doing or aiming for, and even past questions+answers with the AI all make it a pleasure to pick back up old work.&lt;/p&gt;
    &lt;head rend="h2"&gt;Building an App for Collaboration not Replacement&lt;/head&gt;
    &lt;p&gt;One thing is still (intentionally) hard in solveit though, and that is getting the AI to actually write all of your code in a hands-off way. We’ve made various choices to gently push towards the human remaining in control. Things like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Solveit defaults to code inputs&lt;/item&gt;
      &lt;item&gt;AI outputs code in fenced blocks, but these are not added to your code or run until you choose to do so. There are shortcuts to add them, but this extra step encourages you to read + refactor before mindlessly running&lt;/item&gt;
      &lt;item&gt;In ‘Learning’ mode especially, the AI will gently guide you to writing small steps rather than providing a big chunk of code, unless you really specifically ask it to do so.&lt;/item&gt;
      &lt;item&gt;In ‘Learning’ mode, the AI ‘ghost text’ auto-complete suggestions don’t show unless you trigger them with a keyboard shortcut.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Even the choice to have the editor be fairly small and down at the bottom emphasizes that this is a REPL/dialog, optimised for building small, understandable pieces. It’s entirely possible to practice the solveit approach in other tools, but we’ve also found that a combination of these intentional choices and the extra affordances for dialog engineering rapidly feel indispensible.&lt;/p&gt;
    &lt;head rend="h2"&gt;Learning Trajectory&lt;/head&gt;
    &lt;p&gt;This brings us back to a foundational piece of the solveit approach: a learning mindset. It’s great that we can ask AI to fill in the gaps of our knowledge, or to save some time with fiddly pieces like matplotlib plots or library-specific boilerplate. But when the AI suggests something you don’t know, it is important not to skip it and move on – otherwise that new piece will never be something you learn!&lt;/p&gt;
    &lt;p&gt;We try to build the discipline to stop and explore anytime something like this comes up. Fortunately, it’s really easy to do this – you can add new messages trying out whatever new thing the AI has shown you, asking how it works, getting demo code, and poking it until you’re satisfied. And then the evidence of that side-quest can be collapsed below a heading (for later ref) or deleted, leaving you back in the main flow but with a new piece of knowledge in your brain.&lt;/p&gt;
    &lt;p&gt;Like many programmers, I’ve had my share of existential worries given the rapid rise in AI’s coding ability. What if AI keeps getting better and better, to the point where there’s little point for the average person actually learning to master any of these skills? If you assume your coding skills stay static, and imagine the AI continuing to get better, you may feel kinda bleak. The thing is, skill doesn’t have to be static! And as both you and the AI you’re carefully using get better, you will learn faster and be able to accomplish more and more.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mastery Requires Deliberate Practice&lt;/head&gt;
    &lt;p&gt;This is all hard work. It’s like exercise, or practicing a musical instrument. And like any pursuit of mastery, I don’t know that it’s for everyone. But as we’ve seen from all of the students who invested their time into the first cohort, the effort is well worth it in the end. Just take a look at the project showcase featuring a few hundred (!) things our community has made.&lt;/p&gt;
    &lt;head rend="h2"&gt;Sign up for Solveit&lt;/head&gt;
    &lt;p&gt;If you’re interested in joining us to learn how to use the Solveit approach yourself, head over to our site and sign up: solve.it.com, Signups are open until October 20th, but may close earlier if we fill up, so don’t wait too long!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45455719</guid><pubDate>Thu, 02 Oct 2025 21:21:49 +0000</pubDate></item><item><title>The strangest letter of the alphabet: The rise and fall of yogh</title><link>https://www.deadlanguagesociety.com/p/history-of-letter-yogh</link><description>&lt;doc fingerprint="b6728524aea13b82"&gt;
  &lt;main&gt;
    &lt;p&gt;English spelling has a reputation. And it’s not a good one.&lt;/p&gt;
    &lt;p&gt;It’s full of silent letters, as in numb, knee, and honour. A given sound can be spelled in multiple ways (farm, laugh, photo), and many letters make multiple sounds (get, gist, mirage).&lt;/p&gt;
    &lt;p&gt;English spelling is so complex that we’ve made mastering it into a competitive sport: what would be the point of a spelling bee in a language with a predictable spelling system? Where’s the fun unless you have to sweat a little as you struggle to recall whether this particular word is one where “‘i’ before ‘e’ except after ‘c’” doesn’t apply?&lt;/p&gt;
    &lt;p&gt;In short, English has a complicated writing system.&lt;/p&gt;
    &lt;p&gt;I’ve written about the origin of some of this complexity before, blaming everyone from the French to stingy printers and late medieval yuppies. But I’ve not yet plumbed the depths of this complexity. To do so, I will need to tell you the story of yogh,1 an obscure medieval letter whose rise and fall allows us to peer into this abyss.&lt;/p&gt;
    &lt;p&gt;But like an Icelandic family saga, we begin not with the story of yogh, but with the story of its parent. So allow me to introduce you to the letter ‘g,’ which, as you’ll soon see, is a complicated letter in its own right, dating back to Old English.&lt;/p&gt;
    &lt;p&gt;It starts with the shape of the letter. When modern editors print Old English today, they print nice, modern-looking ‘g’s — that is, the ones we use today, with an open or closed loop on the bottom, depending on the typeface.&lt;/p&gt;
    &lt;p&gt;This modern form of ‘g’ is called the Carolingian ‘g.’ It had its origin in the Carolingian minuscule, the script used by the scribes of the Carolingian Renaissance, the great revival of learning which flourished in the vast realm of Charlemagne (reigned 768–814).2&lt;/p&gt;
    &lt;p&gt;But Old English scribes didn’t write their g-sounds with a Carolingian ‘g.’ The Old English letter ‘g’ was written in a form called the insular ‘g.’ Here’s what it looked like: ‘ᵹ.’ It’s like a mix between a ‘z’ and a ‘3.’&lt;/p&gt;
    &lt;p&gt;Let’s see it in action. When the first lines of Beowulf are written in a modern edition, they look like this:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Hwæt, we Gardena in geardagum&lt;/p&gt;&lt;lb/&gt;þeodcyninga þrym gefrunon,&lt;p&gt;‘How we have heard of the glory of the kings of the spear-Danes in days of old’&lt;/p&gt;&lt;p&gt;(Beowulf 1–2)&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;But in the manuscript, they’re written like this:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;HǷÆT ǷE GARDE&lt;/p&gt;&lt;lb/&gt;na inᵹear daᵹum þeod cyninᵹa&lt;lb/&gt;þrym ᵹefrunon&lt;p&gt;London, British Library, Cotton MS Vitellius A XV, f. 132r.&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;Now, there’s clearly lots of other weird stuff going on in the manuscript, but focus on how the ‘g’ is represented. While the majuscule (capital) ‘G’ in ‘gardena’ is spelled more like a modern ‘g,’ all the others are insular ‘ᵹ.’&lt;/p&gt;
    &lt;p&gt;Interestingly, the Anglo-Saxons did use the Carolingian ‘g’ — just not for Old English. They used it when writing Latin, at least after the late 10th century. This was when the Church in England underwent a set of reforms, which caused a flowering of literature both in Latin and Old English.3 As part of these reforms, the Carolingian minuscule script was adopted for Latin texts.&lt;/p&gt;
    &lt;p&gt;So for a period, both ‘g’ and ‘ᵹ’ were used in England, but generally speaking, each was used for writing the ‘g’ sound in a different language. Simple enough, but the stage was set for things to get a lot more complicated.&lt;/p&gt;
    &lt;p&gt;But for that, we need the help of the Normans.&lt;/p&gt;
    &lt;p&gt;You're reading The Dead Language Society. I'm Colin Gorrie, linguist, ancient language teacher, and your guide through the history of the English language and its relatives.&lt;/p&gt;
    &lt;p&gt;Subscribe for a free issue every Wednesday, or upgrade to support my mission of bringing historical linguistics out of the ivory tower and receive two extra Saturday deep-dives per month.&lt;/p&gt;
    &lt;p&gt;If you upgrade, you’ll also be able to join our ongoing Beowulf Book Club and watch our discussion of the first 915 lines (part 1, part 2) right away.&lt;/p&gt;
    &lt;head rend="h1"&gt;Of course, they would have spelled it ȝoȝ&lt;/head&gt;
    &lt;p&gt;For the history of the English language, no single year was more momentous than 1066. In this year, William, Duke of Normandy, invaded and took the English throne, bringing with him Norman knights, and more importantly for our purposes, Norman scribes.&lt;/p&gt;
    &lt;p&gt;These Norman scribes inherited the writing traditions that the Carolingian renaissance had given birth to. This meant the latest, greatest, 11th-century French versions of the Carolingian minuscule.&lt;/p&gt;
    &lt;p&gt;These weren’t so different from the way Anglo-Saxon scribes had written Latin. But they were very different from the way they had written Old English, especially in the ‘g’ department.&lt;/p&gt;
    &lt;p&gt;But that wasn’t so much of an issue, since these Norman-trained scribes, and those of the generations that came after them, didn’t write much English at all. In fact, writing in English of any kind was very scarce up until the end of the 12th century.&lt;/p&gt;
    &lt;p&gt;Over the course of that tumultuous — and, for English, silent — century, the language had changed a great deal. All the scribes trained in the old, Anglo-Saxon traditions were long dead, so when a new generation of scribes turned their attention once again to English, they had to devise some new strategies for writing it.&lt;/p&gt;
    &lt;p&gt;And this, after a surprisingly long delay, is where we first meet the star of today’s issue: ‘ȝ,’ also known as yogh.&lt;/p&gt;
    &lt;p&gt;Yogh is descended from a variant form of the old insular ‘ᵹ.’ But, while the insular ‘ᵹ’ was thought of as the same letter as the Carolingian ‘g’ in Anglo-Saxon times, the yogh ‘ȝ’ of the 12th century was an entirely different letter from the Carolingian-derived ‘g.’&lt;/p&gt;
    &lt;p&gt;And, stranger still, ‘ȝ’ was used to write two completely different sounds in Middle English, the form of English spoken from around 1100–1450: the y-sound as in young or yesterday, and another sound that English has lost altogether.&lt;/p&gt;
    &lt;p&gt;The other sound that ‘ȝ’ once spelled is the “harsh” or “guttural” sound made in the back of the mouth, which you hear in Scots loch or German Bach.4 This sound is actually the reason for the most famous bit of English spelling chaos: the sometimes-silent, sometimes-not sequence ‘gh’ that you see in laugh, cough, night, and daughter. Maybe one day I’ll tell you that story too.&lt;/p&gt;
    &lt;p&gt;For today, however, just know that the spelling ‘gh,’ which causes spellers so much trouble, was originally a replacement for ‘ȝ’ in these words. But more on that later. Let’s dwell for a moment on the bizarre situation we had in Middle English, where the same letter ‘ȝ’ could represent either a y-sound or that now-vanished gh-sound.&lt;/p&gt;
    &lt;head rend="h1"&gt;But not as bizarre as this painting&lt;/head&gt;
    &lt;p&gt;Or is it actually so bizarre?&lt;/p&gt;
    &lt;p&gt;Modern English spelling is, of course, chaotic. So perhaps it shouldn’t surprise us that we too have a very yogh-like situation with two of our letters: ‘c’ and — wait for it — ‘g.’&lt;/p&gt;
    &lt;p&gt;Each of these regularly represents two not particularly similar sounds. The letter ‘c’ sometimes represents a k-sound, like in cat, and sometimes an s-sound, like in city. The letter ‘g’ is similar: sometimes it writes a true g-sound, like in good, but other times, what it represents is a j-sound, like in gem.&lt;/p&gt;
    &lt;p&gt;If these sounds seem similar to you, pay attention to your tongue as you make each one: the k-sound and the true g-sound are made with the back of your tongue hitting against your soft palate. The s-sound and the ‘j’ sound are made in slightly different places, but in both cases, they use the tip of your tongue coming up against (or close to) just behind your upper teeth.&lt;/p&gt;
    &lt;p&gt;Each of our two double-sounding letters, ‘c’ and ‘g,’ has two variants, which are made at completely different ends of the mouth. This is how yogh worked too: it had one variant made at the back of the mouth (the gh-sound) and the other made towards the front (that’s the y-sound).&lt;/p&gt;
    &lt;p&gt;There’s actually a good linguistic reason why this pattern keeps happening. It’s a sound change called palatalization: this happens when a sound made towards the back of the mouth, like a k- or g-sound, gets pulled forward because it’s next to another sound made at the front of the mouth. Often, these front-of-the-mouth sounds are vowels.&lt;/p&gt;
    &lt;p&gt;In Old English, these front-of-the-mouth vowels (front vowels for short) included the ones spelled ‘i’ and ‘e,’ which sounded like the vowels in bee and bay.5 Sometime in the deep prehistory of the English language, the ‘g’ sound got pulled forward in the mouth to sound like ‘y’ whenever it was next to these front vowels.&lt;/p&gt;
    &lt;p&gt;But when it came time to spell these ‘y’ sounds in the Latin alphabet, they still seemed to the Anglo-Saxon scribes to be versions of ‘g’ sounds. So they spelled them ‘ᵹ’ just like they spelled other ‘g’ sounds. This is why, when you read Old English, you can often replace ‘ᵹ’ (or ‘g’ in modern editions) with ‘y’ and get recognizable Modern English words: ᵹear is year, dæᵹ is day, weᵹ is way.6&lt;/p&gt;
    &lt;p&gt;A process like this happened in the ancestor of French too, just slightly differently. Instead of the ‘g’ being pulled forward into a ‘y’ sound before ‘i’ and ‘e,’ it got pulled forward into a ‘j’ sound.&lt;/p&gt;
    &lt;p&gt;This is why the Modern English ‘g’ represents two sounds, one before the letters ‘i’ and ‘e’, and the other before the other letters. It’s because we took our spelling conventions from how the Norman scribes wrote their language, which was an old form of French.7&lt;/p&gt;
    &lt;p&gt;While the Norman scribes, and the later English scribes they trained, were used to the letter ‘g’ writing two different sounds, neither was a y-sound. They needed another letter for that, and they found one: the old insular ‘ᵹ,’ or rather, its descendant, the yogh ‘ȝ.’&lt;/p&gt;
    &lt;p&gt;So that’s why ‘ȝ’ spelled a ‘y’ sound. To understand why ‘ȝ’ also spelled that vanished ‘gh’ sound, however, we need to go back into the distant history of English, long before it was ever written down. Actually, long before there even was an English.&lt;/p&gt;
    &lt;p&gt;Back then, there was just one single language, which would later split into English, Dutch, German, Swedish, and all the other Germanic languages.&lt;/p&gt;
    &lt;p&gt;In this Proto-Germanic language (as it’s called today), the sound that would become the English g-sound — which the Anglo-Saxons would later spell ‘ᵹ’ — was not the g-sound we have today in words like good or bag. That came later.&lt;/p&gt;
    &lt;p&gt;Instead, the Proto-Germanic ancestor of words like good had a sound very similar to the later Middle English gh-sound at the start.8 Only later would English harden that gh-sound into the g-sound we know today. But this only happened in certain places in the word, especially at the start. At the end of the word, the gh-sound remained. In Old English, both versions were spelled the same: ‘ᵹ.’&lt;/p&gt;
    &lt;p&gt;This means that ‘ᵹ’ actually had three pronunciations in Old English, not two: the y-sound (next to front vowels), the g-sound (at the start of words), and the gh-sound (in the middle of words). So when those Norman-trained scribes turned their attention to writing English in the 12th century, they had no problem writing the g-sound at the start of words with ‘g.’&lt;/p&gt;
    &lt;p&gt;But when they wanted to write the gh-sound, they ran into the same problem they had in writing the y-sound. They didn’t have a good way to write the gh-sound, which didn’t exist in French at the time, so they pressed yogh into service again.&lt;/p&gt;
    &lt;p&gt;And that’s why yogh has two sounds, each of which corresponds to a pronunciation of the Old English letter ‘ᵹ’ that the French scribal tradition couldn’t accept writing with ‘g.’&lt;/p&gt;
    &lt;head rend="h1"&gt;Wait, we’ve been saying ‘Mackenzie’ wrong?&lt;/head&gt;
    &lt;p&gt;When you’re reading Middle English, it can get a bit confusing: Which kind of yogh is which?&lt;/p&gt;
    &lt;p&gt;Look at this line from Sir Gawain and the Green Knight:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Þaȝ ȝe ȝourself be talenttyf, to take hit to yourseluen,&lt;/p&gt;&lt;lb/&gt;‘Though you yourself are eager to accept it [a challenge] personally,’ (Sir Gawain and the Green Knight 350)&lt;/quote&gt;
    &lt;p&gt;In this line, the first yogh makes the gh-sound, while the second and third represent y-sounds. But you just have to know the words in order to figure that out.&lt;/p&gt;
    &lt;p&gt;As inconvenient as this confusion is for us modern readers of Middle English, that isn’t the reason yogh disappeared from the English language. The fate of yogh was sealed by a conspiracy of factors.&lt;/p&gt;
    &lt;p&gt;One is that yogh was never the only alternative for writing the sounds it wrote. The y-sound could also be written in the way the French wrote it, that is, ‘i’ or ‘y.’ The latter is the spelling that English ended up using for this sound, hence yourself instead of ȝourself. The gh-sound could also be written ‘h,’ ‘ȝh,’ or ‘gh.’ The last of these, of course, is what English ended up adopting.&lt;/p&gt;
    &lt;p&gt;But the death blow dealt to yogh was the printing press. The earliest printing press in England was a Flemish import, as were the typefaces. But the yogh letter was unique to English, and like the other letters unique to English, it would be expensive to print. And, as we just saw, there were ready alternatives, so yogh disappeared without a trace… from English.&lt;/p&gt;
    &lt;p&gt;In Scotland, on the other hand, it stuck around for longer. Scots used the combination ‘lȝ’ to represent an ‘ly’ sequence like we have in million, and ‘nȝ’ to represent either the ‘ny’ sequence like in canyon, or an ‘ng’ sound like in singer.&lt;/p&gt;
    &lt;p&gt;And Scottish printers were more eager to keep it than English printers were. So they took advantage of the visual similarity between ‘ȝ’ and ‘z’ — most forms of cursive writing in English still write ‘z’ like ‘ȝ’ — to write their yoghs with ‘z’s.&lt;/p&gt;
    &lt;p&gt;You still see the results of this substitution, ‘lz’ and ‘nz,’ in certain Scottish names. But the ‘z’ has led them to be pronounced in ways that have nothing to do with their traditional forms. So Menzies and Mackenzie were meant to spell things that sounded more like Mingus and Mackenyie.&lt;/p&gt;
    &lt;p&gt;And that’s how one single letter of the Middle English alphabet ended up being pronounced like ‘y,’ ‘gh,’ or even eventually like ‘z.’ I warned you it would be complicated.&lt;/p&gt;
    &lt;p&gt;But the journey through the history of yogh has allowed us to peer down some interesting side alleys of the history of writing, from Carolingian scribal practices to the compromises of Scottish printers.&lt;/p&gt;
    &lt;p&gt;I don’t lament the loss of yogh myself, not nearly as much as I lament the fate of other lost letters. But if the cause of yogh is one ȝou fancy taking up ȝourself, there’s nothing standing in ȝour waȝ (it’s included in many modern fonts), althouȝ I can imagine hiȝer causes to aspire to.&lt;/p&gt;
    &lt;p&gt;The name of the letter yogh is pronounced today in many ways: you can say it to rhyme with log, loch, or brogue.&lt;/p&gt;
    &lt;p&gt;Technically, it was the double-storey, closed-loop ‘g’ that was associated with Carolingian minuscule. The open-loop, single-storey ‘g’ was a later development. Today, they’re seen as more or less interchangeable: in fact, I don’t even know what version you’re seeing when you read this, since it’ll look different on the web and in your email client.&lt;/p&gt;
    &lt;p&gt;If you know phonetic terminology, I’ll be more specific: this sound was the voiceless velar fricative, or [x] in the International Phonetic Alphabet. In some situations, it was likely the voiceless palatal fricative (IPA [ç]).&lt;/p&gt;
    &lt;p&gt;If you want to know why the names of these letters sound completely different in Modern English (‘i’ and ‘e’ sound more like the vowels in buy and bee), let me tell you all about it.&lt;/p&gt;
    &lt;p&gt;Conscientious modern editors (myself included, he said humbly) will spell the ‘g’ that you’re supposed to pronounce like ‘y’ with a little dot on top: ‘ġ.’&lt;/p&gt;
    &lt;p&gt;Ditto for the two pronunciations of ‘c.’&lt;/p&gt;
    &lt;p&gt;Note for nerds: this was the voiced velar fricative [ɣ]. Yogh would later be used for the voiceless velar fricative [x], which actually had a different origin in the ancestor of Old English. But the two sounds ended up sounding identical at the end of a word, so yogh ended up being used for both.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45455882</guid><pubDate>Thu, 02 Oct 2025 21:34:42 +0000</pubDate></item></channel></rss>