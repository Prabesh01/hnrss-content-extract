<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 24 Nov 2025 22:09:04 +0000</lastBuildDate><item><title>Three Years from GPT-3 to Gemini 3</title><link>https://www.oneusefulthing.org/p/three-years-from-gpt-3-to-gemini</link><description>&lt;doc fingerprint="3d53915f1bb40691"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Three Years from GPT-3 to Gemini 3&lt;/head&gt;
    &lt;head rend="h3"&gt;From chatbots to agents&lt;/head&gt;
    &lt;p&gt;I‚Äôve been testing Google‚Äôs new Gemini 3 model. It is very good. But rather than give you benchmark numbers, I‚Äôm going to show you how far AI has come in three years by asking the AI to demonstrate it.&lt;/p&gt;
    &lt;p&gt;It has been slightly less than three years since the release of ChatGPT. A few days before that launch, I wrote my first post on this Substack about OpenAI‚Äôs earlier GPT-3 model. Then ChatGPT came out, and I wrote immediately afterwards that ‚ÄúI am usually pretty hesitant to make technology predictions, but I think that this is going to change our world much sooner than we expect, and much more drastically. Rather than automating jobs that are repetitive &amp;amp; dangerous, there is now the prospect that the first jobs that are disrupted by AI will be more analytic; creative; and involve more writing and communication.‚Äù&lt;/p&gt;
    &lt;p&gt;I think I got that one right.&lt;/p&gt;
    &lt;p&gt;I could explain the difference between the original ChatGPT and the new AI model from Google, Gemini 3, that launched today. But I don‚Äôt have to. Instead, I gave the screenshot above to Gemini 3 with a single prompt:&lt;/p&gt;
    &lt;p&gt;Me: ‚Äúshow how far AI has come since this post by doing stuff.‚Äù&lt;/p&gt;
    &lt;p&gt;Gemini: ‚ÄúI can definitely show you the difference. The tweet you shared is from November 2022‚Äîright on the cusp of the modern AI boom. Back then, the ‚Äúwow factor‚Äù was that an AI could write a coherent paragraph or a simple poem about a silly concept like a ‚Äúcandy-powered FTL drive escaping otters.‚Äù&lt;/p&gt;
    &lt;p&gt;To demonstrate how far we‚Äôve come, I‚Äôm not just going to write text about it. I have built you a fully interactive, playable Candy-Powered FTL Starship Simulator.&lt;/p&gt;
    &lt;p&gt;In 2022, AI could describe the engine. In 2025, AI can code the engine, design the interface, and let you pilot the ship yourself.‚Äù&lt;/p&gt;
    &lt;p&gt;And, indeed, it built me a tiny game where I had to use the power of candy to escape otters, featuring small poems and an ongoing set of amusing updates. If you‚Äôre reading this blog, you probably knew AI could do that. What‚Äôs more interesting is what else Gemini 3 can do when you stop thinking of it as a chatbot and start thinking of it as something else entirely&lt;/p&gt;
    &lt;head rend="h1"&gt;Coding tools that aren‚Äôt just coding tools&lt;/head&gt;
    &lt;p&gt;Along with Gemini 3, Google has launched Antigravity. For programmers, Antigravity should be familiar territory, it is similar to Claude Code and OpenAI Codex, specialized tools that can be given access to your computer and which can autonomously write computer programs with guidance. If you aren‚Äôt a programmer, you may dismiss Antigravity and similar tools. I think that is a mistake because the ability to code isn‚Äôt just about programming, it‚Äôs about being able to do anything that happens on a computer. And that changes what these tools actually are.&lt;/p&gt;
    &lt;p&gt;Gemini 3 is very good at coding, and this matters to you even if you don‚Äôt think of what you do as programming. A fundamental perspective powering AI development is that everything you do on a computer is, ultimately, code, and if AI can work with code it can do anything someone with a computer can: build you dashboards, work with websites, create PowerPoint, read your files, and so on. This makes agents that can code general purpose tools. Antigravity embraces this idea, with the concept of an Inbox, a place where I can send AI agents off on assignments and where they can ping me when they need permission or help.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt communicate with these agents in code, I communicate with them in English and they use code to do the work. Because Gemini 3 is good at planning, it is capable of figuring out what to do, and also when to ask my approval. For example, I gave Antigravity access to a directory on my computer containing all of my posts for this newsletter.1 I then asked Gemini 3,0: ‚ÄúI would like an attractive list of predictions I have made about AI in a single site, also do a web search to see which I was right and wrong about.‚Äù It then read through all the files, executing code, until it gave me a plan which I could edit or approve. The screenshot below is the first time the AI asked me anything about the project, and its understanding of what I wanted was impressive. I made a couple of small changes and let the AI work.&lt;/p&gt;
    &lt;p&gt;It then did web research, created a site, took over my browser to confirm the site worked, and presented me the results. Just as I would have with a human, I went through the results and made a few suggestions for improvement. It then packaged up the results so I could deploy them here.&lt;/p&gt;
    &lt;p&gt;It was not that Gemini 3.0 was capable of doing everything correctly without human intervention ‚Äî agents aren‚Äôt there yet. There were no hallucinations I spotted, but there were things I corrected, though those errors were more about individual judgement calls or human-like misunderstandings of my intentions than traditional AI problems. Importantly, I felt that I was in control of the choices AI was making because the AI checked in and its work was visible. It felt much more like managing a teammate than prompting an AI through a chat interface.&lt;/p&gt;
    &lt;head rend="h1"&gt;PhD Level Intelligence?&lt;/head&gt;
    &lt;p&gt;But Antigravity isn‚Äôt the only way Gemini 3 surprised me. The other was in how it handled work that required genuine judgment. As I have mentioned many times on this site, benchmarking AI progress is a mess. Gemini 3 takes a definitive benchmark lead on most stats, (although it may still not be able to beat the $200 GPT-5 Pro Model, but I suspect that might change when Gemini 3‚Äôs inevitable Deep Think version comes out). But you will hear one phrase repeated a lot in the AI world - that a model has ‚ÄúPhD level intelligence.‚Äù&lt;/p&gt;
    &lt;p&gt;I decided to put that to the test. I gave Gemini 3 access to a directory of old files I had used for research into crowdfunding a decade ago. It was a mishmash of files labelled things like ‚Äúproject_final_seriously_this_time_done.xls‚Äù and data in out-of-date statistical formats. I told the AI to ‚Äúfigure out the data and the structure and the initial cleaning from the STATA files and get it ready to do a new analysis to find new things.‚Äù And it did, recovering corrupted data and figuring out the complexities of the environment.&lt;/p&gt;
    &lt;p&gt;Then I gave it a typical assignment that you would expect from a second year PhD student, doing minor original research. With no further hints I wrote: ‚Äúgreat, now i want you to write an original paper using this data. do deep research on the field, make the paper not just about crowdfunding but about an important theoretical topic of interest in either entrepreneurship or business strategy. conduct a sophisticated analysis, write it up as if for a journal.‚Äù I gave it no suggestions beyond that and yet the AI considered the data, generated original hypotheses, tested them statistically, and gave me formatted output in the form of a document. The most fascinating part was that I did not give it any hints about what to research, it walked the tricky tightrope of figuring out what might be an interesting topic and how to execute it with the data it had - one of the hardest things to teach. After a couple of vague commands (‚Äúbuild it out more, make it better‚Äù) I got a 14 page paper.&lt;/p&gt;
    &lt;p&gt;Aside from this, I was impressed that the AI came up with its own measure, a way of measuring how unique a crowdfunding idea was by using natural language processing tools to compare its description mathematically to other descriptions. It wrote the code, executed it and checked the results.&lt;/p&gt;
    &lt;p&gt;So is this a PhD-level intelligence? In some ways, yes, if you define a PhD level intelligence as doing the work of a competent grad student at a research university. But it also had some of the weaknesses of a grad student. The idea was good, as were many elements of the execution, but there were also problems: some of its statistical methods needed more work, some of its approaches were not optimal, some of its theorizing went too far given the evidence, and so on. Again, we have moved past hallucinations and errors to more subtle, and often human-like, concerns. Interestingly, when I gave it suggestions with a lot of leeway, the way I would a student: (‚Äúmake sure that you cover the crowdfunding research more to establish methodology, etc.‚Äù) it improved tremendously, so maybe more guidance would be all that Gemini needed. We are not there yet, but ‚ÄúPhD intelligence‚Äù no longer seems that far away.&lt;/p&gt;
    &lt;head rend="h1"&gt;Gemini 3&lt;/head&gt;
    &lt;p&gt;Gemini 3 is a very good thinking and doing partner that is available to billions of people around the world. It is also a sign of many things: the fact that we have not yet seen a significant slowdown in AI‚Äôs continued development, the rise of agentic models, the need to figure out better ways to manage smart AIs, and more. It shows how far AI has come.&lt;/p&gt;
    &lt;p&gt;Three years ago, we were impressed that a machine could write a poem about otters. Less than 1,000 days later, I am debating statistical methodology with an agent that built its own research environment. The era of the chatbot is turning into the era of the digital coworker. To be very clear, Gemini 3 isn‚Äôt perfect, and it still needs a manager who can guide and check it. But it suggests that ‚Äúhuman in the loop‚Äù is evolving from ‚Äúhuman who fixes AI mistakes‚Äù to ‚Äúhuman who directs AI work.‚Äù And that may be the biggest change since the release of ChatGPT.&lt;/p&gt;
    &lt;p&gt;Obligatory warning: Giving an AI agent access to your computer can be risky if you don‚Äôt know what you are doing. They can move or delete files without asking you and can potentially present a security risk as well by exposing your documents to others. I suspect many of these problems will be addressed as these tools are adapted to non-coders, but, for now, be very careful.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46019898</guid><pubDate>Sun, 23 Nov 2025 01:25:17 +0000</pubDate></item><item><title>We stopped roadmap work for a week and fixed bugs</title><link>https://lalitm.com/fixits-are-good-for-the-soul/</link><description>&lt;doc fingerprint="3623d0560ff62cdd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;We stopped roadmap work for a week and fixed 189 bugs&lt;/head&gt;
    &lt;p&gt;It‚Äôs Friday at 4pm. I‚Äôve just closed my 12th bug of the week. My brain is completely fried. And I‚Äôm staring at the bug leaderboard, genuinely sad that Monday means going back to regular work. Which is weird because I love regular work. But fixit weeks have a special place in my heart.&lt;/p&gt;
    &lt;head rend="h2"&gt;What‚Äôs a fixit, you ask?&lt;/head&gt;
    &lt;p&gt;Once a quarter, my org with ~45 software engineers stops all regular work for a week. That means no roadmap work, no design work, no meetings or standups.&lt;/p&gt;
    &lt;p&gt;Instead, we fix the small things that have been annoying us and our users:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;an error message that‚Äôs been unclear for two years&lt;/item&gt;
      &lt;item&gt;a weird glitch when the user scrolls and zooms at the same time&lt;/item&gt;
      &lt;item&gt;a test which runs slower than it should, slowing down CI for everyone&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The rules are simple: 1) no bug should take over 2 days and 2) all work should focus on either small end-user bugs/features or developer productivity.&lt;/p&gt;
    &lt;p&gt;We also have a ‚Äúpoints system‚Äù for bugs and a leaderboard showing how many points people have. And there‚Äôs a promise of t-shirts for various achievements: first bug fix, most points, most annoying bug, etc. It‚Äôs a simple structure, but it works surprisingly well.&lt;/p&gt;
    &lt;head rend="h2"&gt;What we achieved&lt;/head&gt;
    &lt;p&gt;Some stats from this fixit:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;189 bugs fixed&lt;/item&gt;
      &lt;item&gt;40 people participated&lt;/item&gt;
      &lt;item&gt;4 was the median number of bugs closed per person&lt;/item&gt;
      &lt;item&gt;12 was maximum number of bugs closed by one person&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Bug Burndown Chart for the Q4'25 Fixit&lt;/p&gt;
    &lt;p&gt;Here are some of the highlights (sadly many people in my org work in internal-facing things so I cannot share their work!):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I closed a feature request from 2021! It‚Äôs a classic fixit issue: a small improvement that never bubbled to the priority list. It took me one day to implement. One day for something that sat there for four years. And it‚Äôs going to provide a small but significant boost to every user‚Äôs experience of Perfetto.&lt;/item&gt;
      &lt;item&gt;My colleague made this small change to improve team productivity. Just ~25 lines of code in a GitHub Action to avoid every UI developer taking two extra clicks to open the CI‚Äôs build. The response from the team speaks for itself:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Such a simple change but the team loved it!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I also fixed this issue to provide a new ‚Äúamalgamated‚Äù version of our SDK, allowing it to be easily integrated into projects. It‚Äôs one of those things that might be the difference between someone deciding to use us or not, but building it took just one hour of work (with liberal use of AI!).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The benefits of fixits&lt;/head&gt;
    &lt;head rend="h3"&gt;For the product: craftsmanship and care&lt;/head&gt;
    &lt;p&gt;I care deeply about any product I work on. That means asking big questions like ‚Äúwhat should we build?‚Äù and ‚Äúhow do we make this fast?‚Äù But it also means asking smaller questions: ‚Äúis this error message actually helpful?‚Äù or ‚Äúwould I be frustrated using this?‚Äù&lt;/p&gt;
    &lt;p&gt;A hallmark of any good product is attention to detail: a sense that someone has thought things through, and the pieces fit together to make a cohesive whole. And the opposite is true: a product with rough edges might be tolerated if there are no alternatives, but there will always be a sense of frustration and ‚ÄúI wish I could use something else‚Äù.&lt;/p&gt;
    &lt;p&gt;Fixits are a great chance to work on exactly those details that separate good products from great ones. The small things your average user might not consciously notice, but absolutely will notice if they‚Äôre wrong.&lt;/p&gt;
    &lt;head rend="h3"&gt;For the individual: doing, not thinking&lt;/head&gt;
    &lt;p&gt;I sometimes miss the feeling I had earlier in my career when I got to just fix things. See something broken, fix it, ship it the same day.&lt;/p&gt;
    &lt;p&gt;The more senior you get in a big company, the less you do that. Most of your time becomes thinking about what to build next, planning quarters ahead, navigating tradeoffs and getting alignment.&lt;/p&gt;
    &lt;p&gt;Fixits give me that early-career feeling back. You see the bug, you fix it, you ship it, you close it, you move on. There‚Äôs something deeply satisfying about work where the question isn‚Äôt ‚Äúwhat should we do?‚Äù but rather ‚Äúcan I make this better?‚Äù And you get to answer that question multiple times in a week.&lt;/p&gt;
    &lt;head rend="h3"&gt;For the team: morale and spirit&lt;/head&gt;
    &lt;p&gt;People sharing live updates in the Fixit chatroom&lt;/p&gt;
    &lt;p&gt;Having 40 people across two time zones all fixing bugs together adds a whole other dimension.&lt;/p&gt;
    &lt;p&gt;The vibe of the office is different: normally we‚Äôre all heads-down on different projects, but during fixit the team spirit comes out strong. People share their bug fixes in chat rooms, post before-and-after screenshots and gather around monitors to demo a new feature or complain about a particularly nasty bug they‚Äôre wrestling.&lt;/p&gt;
    &lt;p&gt;The daily update from Friday&lt;/p&gt;
    &lt;p&gt;The leaderboard amplifies this energy. There‚Äôs a friendly sense of competition as people try and balance quick wins with meatier bugs they can share stories about.&lt;/p&gt;
    &lt;p&gt;There‚Äôs also a short update every morning about how the previous day went:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;total bugs fixed&lt;/item&gt;
      &lt;item&gt;how many people have fixed at least one bug&lt;/item&gt;
      &lt;item&gt;how many different products we‚Äôve fixed things in&lt;/item&gt;
      &lt;item&gt;who‚Äôs currently at the top of the leaderboard&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All of this creates real momentum, and people feel magnetically pulled into the effort.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to run a fixit&lt;/head&gt;
    &lt;p&gt;I‚Äôve participated in 6 fixits over the years and I‚Äôve learned a lot about what makes them successful. Here are a few things that matter more than you‚Äôd think.&lt;/p&gt;
    &lt;head rend="h3"&gt;Preparation is key&lt;/head&gt;
    &lt;p&gt;Most of what makes a fixit work happens before the week even starts.&lt;/p&gt;
    &lt;p&gt;All year round, we encourage everyone to tag bugs as ‚Äúgood fixit candidates‚Äù as they encounter them. Then the week before fixit, each subteam goes through these bugs and sizes them:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;small (less than half a day)&lt;/item&gt;
      &lt;item&gt;medium (less than a day)&lt;/item&gt;
      &lt;item&gt;large (less than 2 days).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They assign points accordingly: 1, 2, or 4.&lt;/p&gt;
    &lt;p&gt;We also create a shortlist of high-priority bugs we really want fixed. People start there and move to the full list once those are done. This pre-work is critical: it prevents wasting day one with people aimlessly searching for bugs to fix.&lt;/p&gt;
    &lt;head rend="h3"&gt;The 2-day hard limit&lt;/head&gt;
    &lt;p&gt;In one of our early fixits, someone picked up what looked like a straightforward bug. It should have been a few hours, maybe half a day. But it turned into a rabbit hole. Dependencies on other systems, unexpected edge cases, code that hadn‚Äôt been touched in years.&lt;/p&gt;
    &lt;p&gt;They spent the entire fixit week on it. And then the entire week after fixit trying to finish it. What started as a bug fix turned into a mini project. The work was valuable! But they missed the whole point of a fixit. No closing bugs throughout the week. No momentum. No dopamine hits from shipping fixes. Just one long slog.&lt;/p&gt;
    &lt;p&gt;That‚Äôs why we have the 2-day hard limit now. If something is ballooning, cut your losses. File a proper bug, move it to the backlog, pick something else. The limit isn‚Äôt about the work being worthless - it‚Äôs about keeping fixit feeling like fixit.&lt;/p&gt;
    &lt;head rend="h3"&gt;Number of people matters&lt;/head&gt;
    &lt;p&gt;We didn‚Äôt always do fixits with 40 people. Early on, this wasn‚Äôt an org-wide effort, just my subteam of 7 people. It worked okay: bugs got fixed and there was a sense of pride in making the product better. But it felt a bit hollow: in the bigger picture of our org, it didn‚Äôt feel like anyone else noticed or cared.&lt;/p&gt;
    &lt;p&gt;At ~40 people, it feels like a critical mass that changes things significantly. The magic number is probably somewhere between 7 and 40. And it probably varies based on the team. But whatever the number is, the collective energy matters. If you‚Äôre trying this with 5 people, it might still be worth doing, but it probably won‚Äôt feel the same.&lt;/p&gt;
    &lt;head rend="h3"&gt;Gamification&lt;/head&gt;
    &lt;p&gt;The points and leaderboard are more than a gimmick, but they have to be handled carefully.&lt;/p&gt;
    &lt;p&gt;What works for us:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Points are coarse, not precise: We deliberately use 1/2/4 points instead of trying to measure exact effort; the goal is ‚Äúroughly right and fun‚Äù, not accurate performance evaluation.&lt;/item&gt;
      &lt;item&gt;Celebrate breadth, not just volume. We give t-shirts for things like ‚Äúfirst bug fix‚Äù, ‚Äúmost annoying bug fixed‚Äù, not just ‚Äúmost points‚Äù. That keeps newer or less experienced engineers engaged.&lt;/item&gt;
      &lt;item&gt;Visibility over prizes. A shout-out in the daily update or an internal post often matters more than the actual t-shirt.&lt;/item&gt;
      &lt;item&gt;No attachment to perf reviews. This is important: fixit scores do not feed into performance reviews. The moment they do, people will start gaming it and the good vibe will die.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We‚Äôve had very little ‚Äúgaming‚Äù in practice. Social norms do a good job of keeping people honest and 40 is still small enough that there‚Äôs a sense of ‚Äúloyalty to the cause‚Äù from folks.&lt;/p&gt;
    &lt;head rend="h2"&gt;The AI factor&lt;/head&gt;
    &lt;p&gt;The big challenge with fixits is context switching. Constantly changing what you‚Äôre working on means constantly figuring out new parts of the codebase, thinking about new problems.&lt;/p&gt;
    &lt;p&gt;AI tools have mitigated this in a big way. The code they write is less important than their ability to quickly search through relevant files and summarize what needs to change. They might be right or wrong, but having that starting point really reduces the cognitive load. And sometimes (rarely) they one-shot a fix.&lt;/p&gt;
    &lt;p&gt;This docs change was a perfect example of the above: an update to our docs which catches out new contributors and AI was able to one-shot the fix.&lt;/p&gt;
    &lt;p&gt;On the other hand, in my record page change it was more useful for giving me prototypes of what the code should look like and I had to put in significant effort to correct the bad UX it generated and its tendency to ‚Äúover-generate‚Äù code. Even so, it got me to the starting line much faster.&lt;/p&gt;
    &lt;head rend="h2"&gt;Criticisms of fixits (and why I still like them anyway)&lt;/head&gt;
    &lt;p&gt;I‚Äôve definitely come across people who question whether fixits are actually a good idea. Some of the criticisms are fair but overall I still think it‚Äôs worth it.&lt;/p&gt;
    &lt;head rend="h3"&gt;‚ÄúIsn‚Äôt this just admitting you ignore bugs the rest of the time?‚Äù&lt;/head&gt;
    &lt;p&gt;To some extent, yes, this is an admission of the fact that ‚Äúpapercut‚Äù bugs are underweighted in importance, both by managers and engineers. It‚Äôs all too easy to tunnel on making sure a big project is successful and easier to ignore the small annoyances for users and the team.&lt;/p&gt;
    &lt;p&gt;Fixits are a way of counterbalancing that somewhat and saying ‚Äúactually those bugs matter too‚Äù. That‚Äôs not to say we don‚Äôt fix important bugs during regular work; we absolutely do. But fixits recognize that there should be a place for handling the ‚Äúthis is slightly annoying but never quite urgent enough‚Äù class of problems.&lt;/p&gt;
    &lt;p&gt;The whole reason we started fixits in the first place is that we observed these bugs never get actioned. Given this, I think carving out some explicit time for it is a good thing.&lt;/p&gt;
    &lt;head rend="h3"&gt;‚ÄúIsn‚Äôt it a waste to pause roadmap work for a whole week?‚Äù&lt;/head&gt;
    &lt;p&gt;It‚Äôs definitely a tradeoff. 40 engineer-weeks is a lot of manpower and there‚Äôs an argument to be made it should be used for actually solving roadmap problems.&lt;/p&gt;
    &lt;p&gt;But I think this underweights the importance of polish of products to users. We‚Äôve consistently found that the product feels noticeably better afterward (including positive comments from users about things they notice!) and there‚Äôs a sense of pride in having a well-functioning product.&lt;/p&gt;
    &lt;p&gt;A user‚Äôs response to my PR to improve the Perfetto UI ‚Äúrecord‚Äù page&lt;/p&gt;
    &lt;p&gt;Also, many of the team productivity fixes compound (faster tests, clearer errors, smoother workflows) so the benefits carry forward well beyond the week itself.&lt;/p&gt;
    &lt;head rend="h3"&gt;This only works at big companies!&lt;/head&gt;
    &lt;p&gt;I agree that a full week might be too much for tiny teams or startups. But you can still borrow the idea in smaller chunks: a ‚Äúfixit Friday‚Äù once a month, or a 2-day mini-fixit each quarter. The core idea is the same: protected, collective time to fix the stuff people complain about but no one schedules time to address.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fixits are good for the soul&lt;/head&gt;
    &lt;p&gt;The official justification for fixits is that they improve product quality and developer productivity. And of course they do this.&lt;/p&gt;
    &lt;p&gt;But the unofficial reason I love them is simpler: it just feels good to fix things. It takes me back to a simpler time, and putting thought and attention into building great products is a big part of my ethos for how software engineering should be done. I wouldn‚Äôt want to work like that all the time. But I also wouldn‚Äôt want to work somewhere that never makes time for it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46024541</guid><pubDate>Sun, 23 Nov 2025 16:06:46 +0000</pubDate></item><item><title>The Arithmetic of Braids (2022)</title><link>https://mathcenter.oxford.emory.edu/site/math108/braid_arithmetic/</link><description>&lt;doc fingerprint="86e150664c455050"&gt;
  &lt;main&gt;&lt;p&gt;Whether in the context of hairstyles, friendship bracelets, or even parachute cords -- most will be familiar with the notion of a braid.&lt;/p&gt;&lt;p&gt;As can be seen in the images above, each braid starts with some number of strands which are repeatedly crossed under/over each other in some way. Note that we typically don't allow the strands of a braid to "turn back up".&lt;/p&gt;&lt;p&gt;We can represent the particular crossings of a braid with a braid diagram like the one shown below. Note the diagram shown describes a braid similar to (but longer than) the hair braid above.&lt;/p&gt;&lt;p&gt;Of course, other braids will have a different number of strands and/or a different sequence of crossings. Some may even include sequences of crossings that don't even repeat, such as the one shown below:&lt;/p&gt;&lt;p&gt;Taking inspiration from braids in the real world, "tugging" on the strands in one direction or another -- even when new crossings result (as long as we don't allow any one strand to pass through another) -- can lead to different representations of what is essentially the same braid. As an example, consider the following two diagrams which actually represent the same braid.&lt;/p&gt;&lt;p&gt;While the two braid diagrams above represent the same braid, certainly the one on the left seems "simpler" in some capacity. This raises the question: "How does one simplify a given braid diagram?" Remember this question -- we'll come back to it in a bit.&lt;/p&gt;&lt;p&gt;Admittedly, drawing braid diagrams like the ones previously seen -- and especially when they are not fully simplified -- can be tedious. However, there is a much easier way to represent braids!&lt;/p&gt;&lt;p&gt;Towards this end, observe that if we "tug" in just the right places, we can always jiggle any particular crossing a little bit to the left or right, as desired. In this way, we can arrange any braid (with a finite number of crossings, anyways) so that no two crossings happen simultaneously as we scan the braid from left to right.&lt;/p&gt;&lt;p&gt;As an example, consider the braid diagram involving 5 strands presented earlier, which is shown again below on the left. Numbers and vertical lines have been added to help make the positions of the crossings easier to identify.&lt;/p&gt;&lt;p&gt;In the diagram below on the left, multiple crossings sometimes happen simultaneously between consecutive pairs of vertical lines. For example, between the first pair of vertical lines, the strands at positions $1$ and $2$ cross (red and green) and the strands at positions $3$ and $4$ cross (blue and orange). Similarly, between the second pair of vertical lines, the strands at positions $1$ and $2$ again cross (green and red) and the strands at positions $4$ and $5$ cross (blue and black).&lt;/p&gt;&lt;p&gt;However, with a bit of tugging on the strands we can ensure only one crossing happens at a time as we move from left to right along the braid. Notice how in the diagram on the right the initial red/green crossing has been jiggled a bit to the left and the initial blue/orange crossing has been jiggled a bit to the right. In this way, the red/green crossing now happens first, and the blue/orange crossing now happens second.&lt;/p&gt;&lt;p&gt;Indeed, once things have been "jiggled" in this way, what we see happening between pairs of consecutive lines reduces down to just a few simple possibilities for $5$ strands (there would of course be more if there were more strands involved):&lt;/p&gt;&lt;p&gt;Importantly, if we have names for these possibilities (above we have used $a$ through $h$), then we can describe the braid in question with a simple sequence of letters. So for example, using the above we might identify the braid we've been discussing with the following sequence of letters (also known as a "word"): $$aeahchchedh$$&lt;/p&gt;&lt;p&gt;As much as this can help reduce the tedium of describing a braid from drawing a complicated picture to just writing down a sequence of letters -- implicit in the above is an even greater revelation. Notice it suggests a natural way to combine two braids together to produce a new (longer) braid -- through concatenation!&lt;/p&gt;&lt;p&gt;Consider the two small braids below, which are combined by concatenating the second to the first to form a longer braid. Note, we'll use the "$*$" operator to indicate the action of concatenation:&lt;/p&gt;&lt;p&gt;One should note that for two braids on the same number of strands, we can always combine them in this way to get another braid (again with the same number of strands).&lt;/p&gt;&lt;p&gt;In general, when combining two things of the same "type" (e.g., two braids on $n$ strands) via some operation (e.g., concatenation) and the result is always of the same type, we say the "things" of this type under the associated operation are closed (or equivalently, that they satisfy the property of closure with respect to that operation). To understand the word choice here, note that the combination of any two braids by concatenation will never be anything besides another braid -- a flamingo, for example. Combinations of braids must stay in the "braid universe". We can't get to the universe of flamingos if concatenating is all we can do. The flamingo universe is inaccessible -- it is "closed off" from us.&lt;/p&gt;&lt;p&gt;Closure will become very important to us later, but just to mention a couple of specific examples to reinforce the idea: Note that even integers are closed with respect to addition, but odd integers are not. Similarly integers are closed with respect to multiplication, but not with respect to division.&lt;/p&gt;&lt;p&gt;Turning attention back to braids -- note that denoting the result of concatenating braids $B_1$ and $B_2$ with $B_1 * B_2$ subtly suggests this operation of concatenation behaves in a way similar to real number multiplication. The use of an asterisk "*" after all is a frequent way to denote a product (especially in programming).&lt;/p&gt;&lt;p&gt;Let's think about that for a moment -- what exactly do we mean by "behaves in a way similar to real number multiplication"? The real numbers are certainly closed under multiplication, but surely we must mean more than that! As we mull over the various properties we know multiplication of real numbers enjoy -- ones we hope braids under concatenation will also enjoy -- we might find it promising to ask the following questions to this end:&lt;/p&gt;&lt;p&gt;Is braid concatenation associative?&lt;lb/&gt;Recall, this is certainly a property of real-number multiplication: $(ab)c = a(bc)$&lt;/p&gt;&lt;p&gt;Is there an identity braid?&lt;lb/&gt;That is to say, is there something that functions like the real number $1$ with respect to multiplication, in that for any real number $x$, we have $x \cdot 1 = 1 \cdot x = x$? (i.e., some special value that when we multiply some other value by it (or vice-versa), that other value's "identity" is preserved)&lt;/p&gt;&lt;p&gt;Do braids have inverses?&lt;lb/&gt; We certainly have multiplicative inverses for real numbers (provided they aren't zero). That is, there is a real-number $x^{-1}$ for every non-zero real number $x$ (namely $x^{-1} = 1/x$) where the products $x \cdot x^{-1}$ and $x^{-1} \cdot x$ both equal the multiplicative identity, $1$.&lt;/p&gt;&lt;p&gt;Let us consider each of these in turn. For convenience, for the second and third questions, we'll assume the number of strands involved is $4$, but generalizations to some other number of strands should be both natural and (hopefully) obvious.&lt;/p&gt;&lt;p&gt;Q: Is braid concatenation associative?&lt;/p&gt;&lt;p&gt;That is to say, for any three braids $B_1$, $B_2$, and $B_3$, are $(B_1 * B_2) * B_3$ and $B_1 * (B_2 * B_3)$ always the same braid?&lt;/p&gt;&lt;p&gt;Absolutely! This is an immediate result of how concatenation works. We don't even need to consider any specific braids to see this. Just let the yellow, pink, and green rectangles below represent arbitrary braids $B_1$, $B_2$, and $B_3$, respectively.&lt;/p&gt;&lt;p&gt;Q: Is there an identity braid?&lt;/p&gt;&lt;p&gt;Again, recall the multiplicative identity for real numbers is the value $1$ as we can multiply any real value by $1$ (or vice-versa) and leave it unchanged. Notice this works in both directions -- that is to say, for any real value $x$, it is true that $x \cdot 1 = 1 \cdot x = x$.&lt;/p&gt;&lt;p&gt;Similarly, the additive identity for real numbers is the value $0$ as we can add $0$ to any real value $x$ and leave it unchanged. (Again, reversing the order from $x+0$ to $0+x$ has no impact -- both result in $x$.)&lt;/p&gt;&lt;p&gt;If we seek an identity braid with respect to concatenation, then we seek a braid that could be concatenated to any other braid (on either side) and leave its sequence of crossings unchanged.&lt;/p&gt;&lt;p&gt;Consider that unique braid on some number of strands, $I$, that has no crossings at all!&lt;/p&gt;&lt;p&gt;As the below clearly suggests, concatenating such a braid $I$ to any other braid $B$ (with the same number of strands, of course) leaves $B$ essentially unchanged (i.e., the strands might be a tad longer, but the crossings are all still intact, ant that's what's important).&lt;/p&gt;&lt;p&gt;The reverse is easily shown to hold as well (i.e., $I * B = B$ for any braid $B$).&lt;/p&gt;&lt;p&gt;As such, the braid $I$ on $n$ strands with no crossings serves as an identity for the concatenation of braids on $n$ strands.&lt;/p&gt;&lt;p&gt;Q: Do braids have inverses?&lt;/p&gt;&lt;p&gt;Here again, let us restrict our attention to "braids on $n$ strands" for a particular $n$.&lt;/p&gt;&lt;p&gt;Following the pattern of multiplicative inverses discussed earlier, we then seek for any such braid $B$ an inverse $B^{-1}$ where $B * B^{-1} = I$ and $B^{-1} * B = I$ (assuming $I$ denotes the braid identity)&lt;/p&gt;&lt;p&gt;Remember the simple braids that we previously used to identify a braid of $5$ strands with a sequence of letters? Here's a similar set of braids for braids of $4$ strands:&lt;/p&gt;&lt;p&gt;Regardless of the number of strands involved, notice that these always occur in pairs where the same two strands cross -- with one with one strand on top and the other where the other strand is on top. Indeed, each of these pairs is an inverse pair, as suggested by the names given to the six simple braids immediately above. After concatenating each such pair, only a couple of tugs on the strands are needed to simplify the result to the identity braid $I$ (on $n$ strands), as the below demonstrates for one such pair:&lt;/p&gt;&lt;p&gt;Just to be explicit about the naming convention adopted above, note that for any $i = 1,2,3,\ldots$, we let $x_i$ denote the braid where strands at positions $i$ and $i+1$ cross, with the strand at position $i$ going "over" the strand at position $i+1$. We denote the inverse of $x_i$ by $x_i^{-1}$, where the strand at position $i$ goes "under" the strand at position $i+1$. As a matter of verbiage, we call the set of all such $x_i$ and their inverses the elementary braids on $n$ strands.&lt;/p&gt;&lt;p&gt;Armed now with these inverse pairs of elementary braids, we can build inverses for more complicated braids.&lt;/p&gt;&lt;p&gt;We can think of the individual crossings as actions taken on the strands that change their state, much like the individual actions of putting on one's socks, shoes, and rain boots (which go over one's shoes) each change the state of your feet. The inverse action to some combination of these can be found by "undoing" each individual action, but in reverse.&lt;/p&gt;&lt;p&gt;Suppose one puts on one's socks, and then shoes, and then rain boots, in that order. We could consider other orders, but are likely to over-stretch our socks in doing so. üòÜ To undo this combination of three individual actions (returning one's feet to their bare state), one removes the rain boots, then removes one's socks, then removes one's socks. (Note the reverse order!)&lt;/p&gt;&lt;p&gt;Likewise, if we apply elementary braids $x_1$, $x_3^{-1}$, and $x_2$ in that order, we can undo them by applying their inverses $x_2^{-1}$, $x_3$, and then $x_1^{-1}$, in this reversed order.&lt;/p&gt;&lt;p&gt;Below are the braid diagrams for concatenation and simplification of the example just described. Note that, given how far this similarity between braid concatenation and real number multiplication seems to be going, we'll go ahead and adopt some additional notational conventions often used for products.&lt;/p&gt;&lt;p&gt;Specifically -- just as we often abbreviate $a \cdot b$ with $ab$ when dealing with products of real numbers $a$ and $b$ -- we'll often omit the "$*$" operator between variables representing braids (elementary or otherwise), leaving their concatenation assumed by their adjacency. We may also now start referring to such concatenations as "braid products", or simply "products" when the context is clear.&lt;/p&gt;&lt;p&gt;As you consider the braid product being simplified below, note how we take advantage of the associativity of braid concatenation to evaluate the product of the center-most two elementary braids at each step -- which, being an inverse pair, results in the identity braid $I$ which can then be removed as it has no effect (except the last $I$, of course).&lt;/p&gt;&lt;p&gt;Of course, we could imagine untangling the initial concatenation by visualizing tugging some of the strands up or down at key moments to reduce the number of crossing too. However, the advantage of using the properties of inverses and identities to get from one step to the next is that we no longer really need the pictures (which were cumbersome to draw in the first place) -- we can proceed to simplify things in a completely algebraic way, as shown next.&lt;/p&gt;&lt;p&gt;As you consider the concatenation of braids $x_1 x_3^{-1} x_2$ and $x_2^{-1} x_3 x_1^{-1}$ below and its (algebraic) simplification, notice that we have initially added parentheses around each, so that we can more easily see what is being concatenated at the moment. Of course, using parentheses in this way again mirrors yet another familiar way we often write products of real numbers. For example, $6 = (2)(3)$. $$\begin{array}{rcl} (x_1 x_3^{-1} x_2)(x_2^{-1} x_3 x_1^{-1}) &amp;amp; = &amp;amp; x_1 x_3^{-1} x_2 x_2^{-1} x_3 x_1^{-1}\\ &amp;amp; = &amp;amp; x_1 x_3^{-1} (x_2 x_2^{-1}) x_3 x_1^{-1}\\ &amp;amp; = &amp;amp; x_1 x_3^{-1} I x_3 x_1^{-1}\\ &amp;amp; = &amp;amp; x_1 x_3^{-1} (I x_3) x_1^{-1}\\ &amp;amp; = &amp;amp; x_1 x_3^{-1} x_3 x_1^{-1}\\ &amp;amp; = &amp;amp; x_1 (x_3^{-1} x_3) x_1^{-1}\\ &amp;amp; = &amp;amp; x_1 I x_1^{-1}\\ &amp;amp; = &amp;amp; x_1 (I x_1^{-1})\\ &amp;amp; = &amp;amp; x_1 x_1^{-1}\\ &amp;amp; = &amp;amp; I \end{array}$$&lt;/p&gt;&lt;p&gt;In truth, the above is a bit verbose -- showing all the intermediate steps each time an inverse pair produces an identity braid $I$, which then combines with whatever comes next to leave only whatever comes next.&lt;/p&gt;&lt;p&gt;In practice, this combination of steps is so common we often omit this level of detail when writing the steps taken to simplify a braid -- writing only something similar to the below (which one will notice mirrors the "braid words" given with the pictures above):&lt;/p&gt;$$\begin{array}{rcl} (x_1 x_3^{-1} x_2)(x_2^{-1} x_3 x_1^{-1}) &amp;amp; = &amp;amp; x_1 x_3^{-1} x_2 x_2^{-1} x_3 x_1^{-1}\\ &amp;amp; = &amp;amp; x_1 x_3^{-1} x_3 x_1^{-1}\\ &amp;amp; = &amp;amp; x_1 x_1^{-1}\\ &amp;amp; = &amp;amp; I \end{array}$$&lt;p&gt;There is precedence for this. Consider the steps taken as one simplifies the fraction $\frac{ab}{b}$ (where $b \neq 0$), which are shown below. (Remember that $\frac{b}{b}$ equals the value $1$, the multiplicative "identity"): $$\frac{ab}{b} = a \cdot \frac{b}{b} = a \cdot 1 = a$$ Something like this happens every time one cancels a common factor in the numerator and denominator of a fraction -- but we often skip all that detail, writing only $$\frac{ab}{b} = a$$&lt;/p&gt;&lt;p&gt;The above clearly establishes there is some sort of "multiplicative arithmetic" we can apply to braids, but we must be careful to not let our analogy go too far. One significant difference between braid multiplication and the multiplication of numerical values with which we are well familiar is that braid multiplication is not generally commutative.&lt;/p&gt;&lt;p&gt;That is to say, we don't always have $B_1 B_2 = B_2 B_1$ for any braids $B_1$ and $B_2$.&lt;/p&gt;&lt;p&gt;As a simple example of this, consider the following two braids. Notice the first "product" on the right can't be simplified to the second. For example, the strand initially at position $1$ ends up in position $4$ in the first product, but not in the second.&lt;/p&gt;&lt;p&gt;The lack of general commutivity for braid products certainly decreases the ease with which we may manipulate braids, but as Alexander Graham Bell once said: "When one door closes, another opens."&lt;/p&gt;&lt;p&gt;Special pairs of braids do actually enjoy commutativity. One can easily see how the identity braid $I$, and any other braid will commute. The same can be said of inverse pairs. Interestingly, "distant" elementary braids also commute. That is to say, adjacent elementary braids $x_i$ and $x_j$ will commute if they are far enough apart that they don't involve a common strand position. Noting that this only happens when $i$ and $j$ are at least two positions apart, we can equivalently say for adjacent elementary braids $x_i$ and $x_j$: $$x_i x_j = x_j x_i \textrm{ when } |i-j| \ge 2$$&lt;/p&gt;&lt;p&gt;This is perhaps easier to understand with an example. Note in the diagram below, we can change the order of the pink elementary braids without effectively changing the overall braid. However, if we change the order of the yellow elementary braids, the overall braid is a different braid.&lt;/p&gt;&lt;p&gt;Named after Emil Artin, one of the leading mathematicians of the twentieth century and who developed the theory of braids as a branch of an area in mathematics known as algebraic topology, Artin's relation provides the last piece of the puzzle when establishing the strange arithmetic of braids.&lt;/p&gt;&lt;p&gt;With a little mental "tugging" on the strands below, one should easily be able to convince oneself that this relation holds for elementary braids $x_i$ and $x_{i+1}$ multiplied (i.e., concatenated) in the given way.&lt;/p&gt;&lt;p&gt;What is important, however, is that this special braid relation will allow us to manipulate braids now in a completely algebraic way -- never having to draw pictures like those above, if desired.&lt;/p&gt;&lt;p&gt;We have seen that braids on $n$ strands can be represented by algebraic expressions/words consisting of concatenations of elementary braids $x_1,x_2,x_3,\ldots,x_{n-1}$ and/or their inverses $x_1^{-1},x_2^{-1},x_3^{-1},\ldots,x_{n-1}^{-1}$.&lt;/p&gt;&lt;p&gt;These braid expressions are not unique to a given braid, however. We can certainly show two braid words represent the same braid by drawing pictures of each and "tugging" the strands this way and that until these pictures are identical. However, the equality of two braid words can also be shown by applying algebraic manipulations to one to produce the other, in accordance with the following rules:&lt;/p&gt;&lt;p&gt;Assuming $i$ and $j$ are taken from $1,2,\ldots,n-1$ as appropriate, $B_i$ represents an arbitrary braid word, and $I$ represents the identity braid of no crossings)&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell&gt;Braid Associativity&lt;/cell&gt;&lt;cell&gt;$(B_1 B_2) B_3 = B_1 (B_2 B_3)$&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Multiplication by the Identity&lt;/cell&gt;&lt;cell&gt;$I \, B_i = B_i \, I = B_i$&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Inverse Relations&lt;/cell&gt;&lt;cell&gt;$x_i \, x_i^{-1} = I = x_i^{-1} \, x_i$&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Commutativity of Distant Braids&lt;/cell&gt;&lt;cell&gt;$x_i \, x_j = x_j \, x_i$ when $|i-j| \ge 2$&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Artin's Relation&lt;/cell&gt;&lt;cell&gt;$x_i \, x_{i+1} \, x_i = x_{i+1} \, x_i \, x_{i+1}$&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Now let's put these to use! Consider the following way to simplify a braid $B$ on 3 strands where $$B = x_3^{-1} \, x_2 \, \, x_3 \, x_2 \, x_3^{-1}$$ Note that with only three strands, we won't be able to take advantage of the commutativity of distant braids. Further, we have no inverse pairs adjacent to one another, so we can't use any inverse relations yet.&lt;/p&gt;&lt;p&gt;However, there is an opportunity to apply Artin's relation. Notice, once we take advantage of this, we see two inverse pairs that can then be eliminated -- greatly simplifying the resulting expression!&lt;/p&gt;$$\begin{array}{rcl} B &amp;amp; = &amp;amp; x_3^{-1} \, x_2 \, \, x_3 \, x_2 \, x_3^{-1}\\ &amp;amp; = &amp;amp; x_3^{-1} \, (x_2 \, \, x_3 \, x_2) \, x_3^{-1}\\ &amp;amp; = &amp;amp; x_3^{-1} \, (x_3 \, x_2 \, x_3) \, x_3^{-1}\\ &amp;amp; = &amp;amp; (x_3^{-1} \, x_3) \, x_2 \, (x_3 \, x_3^{-1})\\ &amp;amp; = &amp;amp; I \, x_2 \, I\\ &amp;amp; = &amp;amp; (I \, x_2) \, I\\ &amp;amp; = &amp;amp; x_2 \, I\\ &amp;amp; = &amp;amp; x_2 \end{array}$$&lt;p&gt;Knowing that intermixed elementary inverse braids $b$ and $b^{-1}$ can result in cancellations leaving a product/concatenation of $x_i$ with itself some number of times (as demonstrated in the example below), we might find it useful and more compact to abbreviate a braid $b$ (elementary or otherwise) multiplied/concatenated by itself $p$ times by $b^p$. $$\begin{array}{rcl} b \, b \, b \, b^{-1} \, b \, b^{-1} \, b \, b &amp;amp; = &amp;amp; b \, b \, (b \, b^{-1}) \, (b \, b^{-1}) \, b \, b\\ &amp;amp; = &amp;amp; b \, b \, I \, I \, b \, b\\ &amp;amp; = &amp;amp; b \, b \, b \, b\\ &amp;amp; = &amp;amp; b^4 \end{array}$$&lt;/p&gt;&lt;p&gt;However, if the number of elementary braids of the form $b^{-1}$ exceed those of $b$ form, such products will simplify to a concatenation of $b^{-1}$ with itself some number of times (see example below). In these cases, abbreviating $(b^{-1})^p$ with $b^{-p}$ also seems natural and more compact.&lt;/p&gt;$$\begin{array}{rcl} b \, b^{-1} \, b^{-1} \, b^{-1} \, b \, b^{-1} \, b \, b^{-1} &amp;amp; = &amp;amp; (b \, b^{-1}) \, b^{-1} \, (b^{-1} \, b) \, (b^{-1} \, b) \, b^{-1}\\ &amp;amp; = &amp;amp; I \, b^{-1} \, I \, I \, b^{-1}\\ &amp;amp; = &amp;amp; b^{-1} \, b^{-1}\\ &amp;amp; = &amp;amp; b^{-2} \end{array}$$&lt;p&gt;Of course, there is a third possibility. It could be that pairing off the $b$ and $b^{-1}$ elementary braids and eliminating them leaves nothing but $I$ in the end, as the following suggests: $$\begin{array}{rcl} b \, b^{-1} \, b^{-1} \, b^{-1} \, b \, b \, b^{-1} \, b &amp;amp; = &amp;amp; (b \, b^{-1}) \, b^{-1} \, (b^{-1} \, b) \, (b \, b^{-1}) \, b\\ &amp;amp; = &amp;amp; I \, b^{-1} \, I \, I \, b\\ &amp;amp; = &amp;amp; b^{-1} \, b\\ &amp;amp; = &amp;amp; I \end{array}$$&lt;/p&gt;&lt;p&gt;Given the above, Let us now make the following two additional definitions:&lt;/p&gt;&lt;p&gt;The reason for making the last two definitions above stems from the following observation: With these two definitions, we can safely say that the following simplification rules will now always hold, regardless of what integers $p$ and $q$ are involved! (Note that without these definitions, products like $x_i^3 x_i^{-3}$ or $x_i^3 x_i^{-2}$ can't be simplified with these rules.)&lt;/p&gt;&lt;p&gt;$b^p \, b^q = b^{p+q}$ (add exponents when multiplying powers)&lt;/p&gt;&lt;p&gt;$(b^p)^q = b^{pq}$ (multiply exponents when finding a power of a power)&lt;/p&gt;&lt;p&gt;Just for clarity, please know that the above examples and rules hold for any braid $b$, including elementary braids of the form $b = x_i$ or $b = x_i^{-1}$. As examples, it must be the case by the last two rules that $x_4^3 x_4^5 = x_4^8$ and $(x_2^7)^3 = x_2^{21}$&lt;/p&gt;&lt;p&gt;Noting that the first rule (the one about adding exponents) forces $b^p \, b^{-q} = b^{p-q}$ to also be true for any integers $p$ and $q$.&lt;/p&gt;&lt;p&gt;Taking things one step further, recall that the division of one real number by another is equivalent to multiplying the first by the reciprical (i.e., the multiplicative inverse) of the second. In a parallel way, we can define and denote the "division of one braid by another" (and the equivalent "fraction of braids") in the following way: $$b_1 \div b_2 = \frac{b_1}{b_2} = b_1 b_2^{-1}$$&lt;/p&gt;&lt;p&gt;Doing this leads to even more results for general braids which mirror the standard exponent rules with which the reader might already be familiar.&lt;/p&gt;&lt;p&gt;$\displaystyle{\frac{b^p}{b^q} = b^{p-q}}$ (subtract exponents when dividing powers)&lt;/p&gt;&lt;p&gt;$\displaystyle{b^{-p} = \frac{I}{b^p}}$ (negative exponents produce recipricals of powers) To see why, consider replacing $I$ with $b^0$.&lt;/p&gt;&lt;p&gt;$\displaystyle{\frac{I}{b_i b_j} = b_j^{-1} b_i^{-1}}$ (to invert a product, invert each and combine in reverse order) Consider $x_i x_j x_j^{-1} x_i^{-1}$.&lt;/p&gt;&lt;p&gt;We'll have more to say about the aforementioned standard exponent rules soon -- and why there is such a strong parallel between these and the rules we are developing for braids (and other things we will learn about shortly). For now however, let us push things even farther...&lt;/p&gt;&lt;p&gt;Note that we can say even more for commutative braids $b_i$ and $b_j$ or their inverses (e.g., "distant" elementary braids $x_i$ and $x_j$, where $|i-j| \ge 2$ or inverse pairs). In particular, for all integers $p$ the following simplification rules also must apply:&lt;/p&gt;&lt;p&gt;$(b_i b_j)^p = b_i^p b_j^p$ (exponents distribute over products of commutative elementary braids)&lt;/p&gt;&lt;p&gt;$\displaystyle{\left( \frac{b_i}{b_j} \right)^p = \frac{b_i^p}{b_j^p}}$ (exponents distribute over quotients of commutative elementary braids)&lt;/p&gt;&lt;p&gt;If $b_1$, $b_2$, $b_3$, and $b_4$ are all pairwise commutative, then&lt;/p&gt;$\displaystyle{\frac{b_1 b_2}{b_3 b_4} = \frac{b_1}{b_3} \cdot \frac{b_2}{b_4}}$ (quotients of products can be expressed as products of quotients)&lt;p&gt;To see how commutativity plays a role in these two rules, note that to establish the first (i.e., exponents distribute over products) we can use commutivity to get all the $x_i$ expressions together to consolidate them into a single power, as shown in the following example (look carefully between the 3rd and 4th expressions):: $$(b_i b_j)^2 = (b_i b_j)(b_i b_j) = b_i b_j b_i b_j = b_i b_i b_j b_j = b_i^2 b_j^2$$ Then, we can use the first rule to prove the second (i.e., where exponents are seen to distribute over quotients): $$\left(\frac{b_i}{b_j}\right)^2 = (b_i b_j^{-1})^2 = b_i^2 b_j^{-2} = \frac{b_i^2}{b_j^2}$$&lt;/p&gt;&lt;p&gt;As for the last rule, note how commutativity allows us to get from the third expression below to the fourth: $$\frac{b_1 b_2}{b_3 b_4} = b_1 b_2 (b_3 b_4)^{-1} = b_1 b_2 b_4^{-1} b_3^{-1} = b_1 b_3^{-1} b_2 b_4^{-1} = \frac{b_1}{b_3} \cdot \frac{b_2}{b_4}$$&lt;/p&gt;&lt;p&gt;Just be careful -- don't use any of these or other rules that rely on commutativity holding when it fails to hold!&lt;/p&gt;&lt;p&gt;While the results and definitions in the last section will let us prove certain pairs of braid expressions describe the same braid, sometimes doing so by direct application of these results and definitions can be quite long and tedius -- especially when we find ourselves applying the same sequence of manipulations over and over again. This of course is one reason why we prove theorems in mathematics -- to let us jump over all that repetition and make a useful (read that as "usable in many different contexts") conclusion from some given knowledge. We only need to prove the theorem in question holds.&lt;/p&gt;&lt;p&gt;As an example, for any given positive integer $i$, note how we can justify the following sequence of manipulations with what we know already: $$\begin{array}{rcll} x_i \, x_{i+1} \, x_i^{-1} &amp;amp; = &amp;amp; I \, x_i \, x_{i+1} \, x_i^{-1} &amp;amp; \scriptsize{\textrm{a property of the identity braid}}\\ &amp;amp; = &amp;amp; (x_{i+1}^{-1} \, x_{i+1}) \, x_i \, x_{i+1} \, x_i^{-1} &amp;amp; \scriptsize{\textrm{using inverse braids to form a "well-chosen value of "} I}\\ &amp;amp; = &amp;amp; x_{i+1}^{-1} \, (x_{i+1} \, x_i \, x_{i+1}) \, x_i^{-1} &amp;amp; \scriptsize{\textrm{associativity of braid concatenation}}\\ &amp;amp; = &amp;amp; x_{i+1}^{-1} \, (x_i \, x_{i+1} \, x_i) \, x_i^{-1} &amp;amp; \scriptsize{\textrm{Artin's Relation}}\\ &amp;amp; = &amp;amp; x_{i+1}^{-1} \, x_i \, x_{i+1} \, (x_i \, x_i^{-1}) &amp;amp; \scriptsize{\textrm{associativity of braid concatenation}}\\ &amp;amp; = &amp;amp; x_{i+1}^{-1} \, x_i \, x_{i+1} \, I &amp;amp; \scriptsize{\textrm{cancellation of inverse braids}}\\ &amp;amp; = &amp;amp; x_{i+1}^{-1} \, x_i \, x_{i+1} &amp;amp; \scriptsize{\textrm{a property of the identity braid}}\\ \end{array}$$&lt;/p&gt;&lt;p&gt;For lack of a better name, let us call this "Braid Theorem 1". That is to say,&lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Braid Theorem 1&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Having now proven this theorem, we can employ it in future manipulations. For example, suppose we were trying to decide if the following two braid words represent the same braid: $$x_2 \, x_3^{-1} \, x_2 \, x_2 \, x_3 \, x_2^{-1} \quad \stackrel{\text{?}}{=} \quad x_2 \, x_3^{-1} \, (x_3^{-1} \, x_3) \, x_2 \, x_3^{-1} \, x_2 \, x_3$$&lt;/p&gt;&lt;p&gt;With the theorem we just proved, we can prove they are equivalent in just 4 steps instead of the 10 steps it would require without it! $$\begin{array}{rcll} x_2 \, x_3^{-1} \, x_2 \, x_2 \, x_3 \, x_2^{-1} &amp;amp; = &amp;amp; x_2 \, x_3^{-1} \, x_2 \, (x_2 \, x_3 \, x_2^{-1}) &amp;amp; \scriptsize{\textrm{associativity of braid concatenation}}\\ &amp;amp; = &amp;amp; x_2 \, x_3^{-1} \, x_2 \, (x_3^{-1} \, x_2 \, x_3) &amp;amp; \scriptsize{\textrm{braid theorem 1}}\\ &amp;amp; = &amp;amp; x_2 \, x_3^{-1} \, I \, x_2 \, x_3^{-1} \, x_2 \, x_3 &amp;amp; \scriptsize{\textrm{a property of the identity braid}}\\ &amp;amp; = &amp;amp; x_2 \, x_3^{-1} \, (x_3^{-1} \, x_3) \, x_2 \, x_3^{-1} \, x_2 \, x_3 &amp;amp; \scriptsize{\textrm{using inverse braids to form a "well-chosen value of "} I}\\ \end{array}$$&lt;/p&gt;&lt;p&gt;Of course, proving some theorems can lead to wondering about others. For example, the expressions involved in the statement of Braid Theorem 1 are reminiscent of those in Artin's Relation -- except for the presence of an inverse elementary braid. What happens if the inverse elementary braid is in a different location -- say, on the first $x_i$ instead of the second? Because it would be "pretty" (due to the inherent symmetry involved), we might even hope that the following turns out to be true -- that for every positive integer $i$, we have $x_i^{-1} \, x_{i+1} \, x_i = x_{i+1} \, x_i \, x_{i+1}^{-1}$.&lt;/p&gt;&lt;p&gt;If this were true, we could remember both results as essentially an "Artin-like manipulation with a move of the inverse from one side to the other".&lt;/p&gt;&lt;p&gt;Of course, we don't know if this result holds yet -- we are only hopeful. We must prove it works for any positive integer $i$ before we can use it.&lt;/p&gt;&lt;p&gt;Let's try to argue similarly to how the last one was argued: $$\begin{array}{rcll} x_i^{-1} \, x_{i+1} \, x_i &amp;amp; = &amp;amp; x_i^{-1} \, x_{i+1} \, x_i \, I &amp;amp; \scriptsize{\textrm{a property of the identity braid}}\\ &amp;amp; = &amp;amp; x_i^{-1} \, x_{i+1} \, x_i \, (x_{i+1} \, x_{i+1}^{-1}) &amp;amp; \scriptsize{\textrm{using inverse braids to form a "well-chosen value of "} I}\\ &amp;amp; = &amp;amp; x_i^{-1} \, (x_{i+1} \, x_i \, x_{i+1}) \, x_{i+1}^{-1} &amp;amp; \scriptsize{\textrm{associativity of braid concatenation}}\\ &amp;amp; = &amp;amp; x_i^{-1} \, (x_i \, x_{i+1} \, x_i) \, x_{i+1}^{-1} &amp;amp; \scriptsize{\textrm{Artin's Relation}}\\ &amp;amp; = &amp;amp; (x_i^{-1} \, x_i) \, x_{i+1} \, x_i \, x_{i+1}^{-1} &amp;amp; \scriptsize{\textrm{associativity of braid concatenation}}\\ &amp;amp; = &amp;amp; I \, x_{i+1} \, x_i \, x_{i+1}^{-1} &amp;amp; \scriptsize{\textrm{cancellation of inverse braids}}\\ &amp;amp; = &amp;amp; x_{i+1} \, x_i \, x_{i+1}^{-1} &amp;amp; \scriptsize{\textrm{a property of the identity braid}}\\ \end{array}$$ Voila! We have shown that which we hoped to demonstrate -- we have proven our result!&lt;/p&gt;&lt;p&gt;By the way, in math textbooks one often sees the letters "QED." written at the end of a proof. This is merely an acronym for the Latin phrase "quod erat demonstrandum" which means "which was to be demonstrated".&lt;/p&gt;&lt;p&gt;Let us refer to this new theorem with an equally imaginative name -- say, "Braid Theorem 2":&lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Braid Theorem 2&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;This is essentially how theorems in mathematics (involving braids or otherwise) get developed. First, somebody makes a good guess as to what they think should hold. Sometimes, this stems from some desire for the world to be "pretty" or symmetric. Other times, one makes some conjecture based on patterns they see hold in specific examples. Then, they try to form an argument that connects what they know to what they hope to show -- one that can be justified at every step and turn by things already accepted as true (e.g., definitions, postulates, other previously-proven theorems, etc).&lt;/p&gt;&lt;p&gt;One often tries to base these arguments on tricks or techniques that have worked in the past, much as we used the proof of Braid Theorem 1 as a guide to proving Braid Theorem 2. In this sense, mathematicians can build both on their own experience and on that of others. Interestingly however, the real fun for mathematicians begins when things they or others have used in the past fail to work! Working hard to find a new "wrinkle" on an old technique or creating a brand new way to argue something altogether -- that's where things get exciting!&lt;/p&gt;&lt;p&gt;Sadly, the presentation of theorems and their proofs in textbooks often leave out any discussion of the "blood, sweat, and tears" that went into a proof's initial construction. If you are lucky, they might briefly discuss the inspiration for the creative argument, but almost never explicitly address the author's excitement upon discovering that their novel approach bore fruit. Instead, the proof is often simply written as efficiently as possible -- often even omitting things deemed "easy enough" for the reader to fill in.&lt;/p&gt;&lt;p&gt;Part of this is to draw the reader's attention to the interesting (perhaps novel?) steps.&lt;/p&gt;&lt;p&gt;Another part of this is to not bias the reader to a particular line of conjecture and discovery, providing instead -- as a certain famed sergeant from the 1950's television show Dragnet would often say when questioning women as part of his police investigation:&lt;/p&gt;&lt;p&gt;Perhaps the greatest motivation for this habit however, lies in the very thing that drove us to develop braid words and the above algebra they afford in the first place. In this, and in many, many more examples in the future, we will see that mathematicians seem to work very hard in everything they do to save themselves effort in the future. This includes being as brief and efficient as possible with anything they write.&lt;/p&gt;&lt;p&gt;As an example of this less verbose way of introducing a theorem and its proof, consider the following:&lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Braid Theorem 3&lt;/p&gt;&lt;p&gt;Proof: $$\begin{array}{rcll} x_i \, x_{i+1}^{-1} \, x_i^{-1} &amp;amp; = &amp;amp; (x_{i+1}^{-1} \, x_{i+1}) \, x_i \, x_{i+1}^{-1} \, x_i^{-1} &amp;amp; \\ &amp;amp; = &amp;amp; x_{i+1}^{-1} \, (x_{i+1} \, x_i \, x_{i+1}^{-1}) \, x_i^{-1} &amp;amp; \\ &amp;amp; = &amp;amp; x_{i+1}^{-1} \, (x_i^{-1} \, x_{i+1} \, x_i) \, x_i^{-1} &amp;amp; \scriptsize{\textrm{ by braid theorem 2}}\\ &amp;amp; = &amp;amp; x_{i+1}^{-1} \, x_i^{-1} \, x_{i+1} \, (x_i \, x_i^{-1}) \\ &amp;amp; = &amp;amp; x_{i+1}^{-1} \, x_i^{-1} \, x_{i+1}\\ \end{array}$$&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Notice in the above how the steps involving concatenation with $I$ were left out, much like one rarely writes a "multiplication by 1". Also, the more common applications of associativity or properties of inverses were left implicit, while the use of braid theorem 2 (i.e., the interesting step) was highlighted.&lt;/p&gt;&lt;p&gt;Just remember, when reading proofs presented in a "just the facts" style like the one above, it is your job as the reader to make sure you can justify each new step from the previous ones. To help you do this, you should always keep a pencil nearby when reading a math book!&lt;/p&gt;&lt;p&gt;Before continuing, you might try to conjecture and then prove results similar to braid theorems 1-3 that involve the following braids: $x_i^{-1} \, x_{i+1}^{-1} \, x_i$ and $x_i^{-1} \, x_{i+1}^{-1} \, x_i^{-1}$.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46031068</guid><pubDate>Mon, 24 Nov 2025 06:44:57 +0000</pubDate></item><item><title>Shai-Hulud Returns: Over 300 NPM Packages Infected</title><link>https://helixguard.ai/blog/malicious-sha1hulud-2025-11-24</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46032539</guid><pubDate>Mon, 24 Nov 2025 10:40:22 +0000</pubDate></item><item><title>NSA and IETF, part 3: Dodging the issues at hand</title><link>https://blog.cr.yp.to/20251123-dodging.html</link><description>&lt;doc fingerprint="331298363f8401c8"&gt;
  &lt;main&gt;
    &lt;p&gt;Normal practice in deploying post-quantum cryptography is to deploy ECC+PQ. IETF's TLS working group is standardizing ECC+PQ. But IETF management is also non-consensually ramming a particular NSA-driven document through the IETF process, a "non-hybrid" document that adds just PQ as another TLS option.&lt;/p&gt;
    &lt;p&gt;Don't worry: we're standardizing cars with seatbelts. Also, recognizing generous funding from the National Morgue Association, we're going to standardize cars without seatbelts as another option, ignoring the safety objections. That's okay, right?&lt;/p&gt;
    &lt;p&gt;Last month I posted part 1 of this story. Today's part 2 highlighted the corruption. This blog post, part 3, highlights the dodging in a particular posting at the beginning of this month by an IETF "security area director". Part 4 will give an example of how dissent on this topic has been censored.&lt;/p&gt;
    &lt;p&gt;Consensus means whatever the people in power want to do. Recall from my previous blog post that "adoption" of a document is a preliminary step before an IETF "working group" works on, and decides whether to standardize, the document. In April 2025, the chairs of the IETF TLS WG called for "adoption" of this NSA-driven document. During the call period, 20 people expressed unequivocal support for adoption, 2 people expressed conditional support for adoption, and 7 people expressed unequivocal opposition to adoption. (Details for verification.)&lt;/p&gt;
    &lt;p&gt;The chairs claimed that "we have consensus to adopt this draft". I promptly asked for explanation.&lt;/p&gt;
    &lt;p&gt;Before the chairs could even reply, an "area director" interrupted, claiming, inter alia, the following: "There is clearly consensus based on the 67 responses to the adoption call. ... The vast majority was in favour of adoption ... There were a few dissenting opinions".&lt;/p&gt;
    &lt;p&gt;After these lies by the "area director" were debunked, the chairs said that they had declared consensus "because there is clearly sufficient interest to work on this draft" specifically "enough people willing to review the draft".&lt;/p&gt;
    &lt;p&gt;I can understand not everybody being familiar with the specific definition of "consensus" that antitrust law requires standards-development organizations to follow. But it's astonishing to see chairs substituting a consensus-evaluation procedure that simply ignores objections.&lt;/p&gt;
    &lt;p&gt;Stonewalling. The chairs said I could escalate. IETF procedures say that an unresolved dispute can be brought "to the attention of the Area Director(s) for the area in which the Working Group is chartered", and then "The Area Director(s) shall attempt to resolve the dispute".&lt;/p&gt;
    &lt;p&gt;I filed a complaint with the "security area directors" in early June 2025. One of them never replied. The other, the same one who had claimed that there was "clearly consensus", sent a series of excuses for not handling the complaint. For example, one excuse was that the PDF format "discourages participation".&lt;/p&gt;
    &lt;p&gt;Do IETF procedures say "The Area Director(s) shall attempt to resolve the dispute unless the dispute is documented in a PDF"? No.&lt;/p&gt;
    &lt;p&gt;I sent email two days later systematically addressing the excuses. The "area director" never replied.&lt;/p&gt;
    &lt;p&gt;It isn't clear under IETF procedures whether a non-reply allows an appeal. It is, however, clear that an appeal can't be filed after two months. I escalated to the "Internet Engineering Steering Group" (IESG) in August 2025.&lt;/p&gt;
    &lt;p&gt;(These aren't even marginally independent groups. The "area directors" are the IESG members. IESG appoints the WG chairs.)&lt;/p&gt;
    &lt;p&gt;IESG didn't reply until October 2025. It rejected one of the "Area Director" excuses for having ignored my complaint, but endorsed another excuse. I promptly filed a revised complaint with the "area director", jumping through the hoops that IESG had set. There were then further runarounds.&lt;/p&gt;
    &lt;p&gt;The switch. Suddenly, on 1 November 2025, IESG publicly instructed the "area director" to address the following question: "Was rough consensus to adopt draft-connolly-tls-mlkem-key-agreement in the TLS Working Group appropriately called by the WG chairs?"&lt;/p&gt;
    &lt;p&gt;The "area director" posted his conclusion mere hours later: "I agree with the TLS WG Chairs that the Adoption Call result was that there was rough consensus to adopt the document".&lt;/p&gt;
    &lt;p&gt;Dodging procedural objections. Before looking at how the "area director" argued for this conclusion, I'd like to emphasize three things that the "area director" didn't do.&lt;/p&gt;
    &lt;p&gt;First, did the "area director" address my complaint about the chair action on this topic? No.&lt;/p&gt;
    &lt;p&gt;One reason this matters is that the law requires standards-development organizations to provide an "appeals process". Structurally, the "area director" isn't quoting and answering the points in my complaint; the "area director" puts the entire burden on the reader to try to figure out what's supposedly answering what, and to realize that many points remain unanswered.&lt;/p&gt;
    &lt;p&gt;Second, did the "area director" address the chairs claiming that "we have consensus to adopt this draft"? Or the previous claim from the "area director" that there was "clearly consensus"? No. Instead IESG and this "area director" quietly shifted from "consensus" to "rough consensus". (Did you notice this shift when I quoted IESG's "rough consensus" instruction?)&lt;/p&gt;
    &lt;p&gt;One reason this matters is that "consensus" is another of the legal requirements for standards-development organizations. The law doesn't allow "rough consensus". Also, IETF claims that "decision-making requires achieving broad consensus". "broad consensus" is even stronger than "consensus", since it's saying that there's consensus in a broad group.&lt;/p&gt;
    &lt;p&gt;Third, the way that my complaint had established the lack of consensus was, first, by reviewing the general definition of "consensus" (which I paraphrased from the definition in the law, omitting a citation only because the TLS chairs had threatened me with a list ban if I mentioned the law again), and then applying the components of that definition to the situation at hand. Did the area director follow this structure? Here's the definition of "consensus", or "rough consensus" if we're switching to that, and now let's apply that definition? No. Nobody reading this message from the "area director" can figure out what the "area director" believes these words mean.&lt;/p&gt;
    &lt;p&gt;Wow, look at that: "due process" is another of the legal requirements for standards-development organizations. Part of due process is simply making clear what procedures are being applied. Could it possibly be that the people writing the law were thinking through how standardization processes could be abused?&lt;/p&gt;
    &lt;p&gt;Numbers. Without further ado, let's look at what the "security area director" did write.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The IESG has requested that I evaluate the WG Adoption call results for ML-KEM Post-Quantum Key Agreement for TLS 1.3 (draft-connolly-tls-mlkem-key-agreement). Please see below.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;As noted above, IESG had instructed the "area director" to answer the following question: "Was rough consensus to adopt draft-connolly-tls-mlkem-key-agreement in the TLS Working Group appropriately called by the WG chairs?"&lt;/p&gt;
    &lt;p&gt;Side note: Given that the "area director" posted all of the following on the same day that IESG instructed the "area director" to write this, presumably this was all written in advance and coordinated with the rest of IESG. I guess the real point of finally (on 1 November 2025) addressing the adoption decision (from 15 April 2025) was to try to provide cover for the "last call" a few days later (5 November 2025).&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;ExecSum&lt;/p&gt;
      &lt;p&gt;I agree with the TLS WG Chairs that the Adoption Call result was that there was rough consensus to adopt the document.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;As noted above, the TLS WG chairs had claimed "consensus", and the "area director" had claimed that there was "clearly consensus". The "area director" is now quietly shifting to a weaker claim.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Timeline&lt;/p&gt;
      &lt;p&gt;April 1: Sean and Joe announce WG Adoption Call [ about 40 messages sent in the thread ]&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;"About 40"? What happened to the "area director" previously writing "There is clearly consensus based on the 67 responses to the adoption call"? And why is the number of messages supposed to matter in the first place?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;April 15: Sean announces the Adoption Call passed. [ another 50 messages are sent in the thread ]&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Messages after the specified adoption-call deadline can't justify the claim that "the Adoption Call result was that there was rough consensus to adopt the document". The adoption call failed to reach consensus.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;April 18 to today: A chain of (attempted) Appeals by D. J. Bernstein to the AD(s), IESG and IAB, parts of which are still in process.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The fact that the ADs and IESG stonewalled in response to complaints doesn't mean that they were "attempted" complaints.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Outcome&lt;/p&gt;
      &lt;p&gt;30 people participated in the consensus call, 23 were in favour of adoption, 6 against and 1 ambivalent (names included at the bottom of this email).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;These numbers are much closer to reality than the "area director" previously writing "There is clearly consensus based on the 67 responses to the adoption call. ... The vast majority was in favour of adoption ... There were a few dissenting opinions".&lt;/p&gt;
    &lt;p&gt;Also, given that the "area director" is continually making claims that aren't true (see examples below) and seems generally allergic to providing evidence (the text I'm quoting below has, amazingly, zero URLs), it's a relief to see the "area director" providing names to back up the claimed numbers here.&lt;/p&gt;
    &lt;p&gt;But somehow, even after being caught lying about the numbers before, the "area director" still can't resist shading the numbers a bit.&lt;/p&gt;
    &lt;p&gt;The actual numbers were 20 people unequivocally supporting adoption, 2 people conditionally supporting adoption, and 7 people unequivocally opposing adoption. Clearly 7 is close to 6, and 20+2 is close to 23, but, hmmm, not exactly. Let's check the details:&lt;/p&gt;
    &lt;p&gt;How does the "area director" end up with 6 negative votes rather than 7? By falsely listing Thomas Bellebaum as "ambivalent" and falsely attributing a "prefer not, but okay if we do" position to Bellebaum. In fact, Bellbaum had written "I agree with Stephen on this one and would not support adoption of non-hybrids." (This was in reply to Stephen Farrell, who had written "I'm opposed to adoption, at this time.")&lt;/p&gt;
    &lt;p&gt;How does the "area director" end up with 23 positive votes rather than 22? By falsely listing the document author (Deirdre Connolly) as having stated a pro-adoption position during the call. The "area director" seems generally clueless about conflict-of-interest issues and probably doesn't find it obvious that an author shouldn't vote, but the simple fact is that the author didn't vote. She sent three messages during the call period; all of those messages are merely commenting on specific points, not casting a vote on the adoption question.&lt;/p&gt;
    &lt;p&gt;The document author didn't object to the "area director" fudging the numbers. Bellebaum did politely object; the "area director" didn't argue, beyond trying to save face with comments such as "Thanks for the clarification".&lt;/p&gt;
    &lt;p&gt;More to the point, the "area director" has never explained whether or how the tallies of positive and negative votes are supposed to be relevant to the "rough consensus" claim. The "area director" also hasn't commented on IETF saying that IETF doesn't make decisions by voting.&lt;/p&gt;
    &lt;p&gt;Bogus arguments for the draft. I mentioned in my previous blog post that IETF claims that "IETF participants use their best engineering judgment to find the best solution for the whole Internet, not just the best solution for any particular network, technology, vendor, or user".&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In favour argument summary&lt;/p&gt;
      &lt;p&gt;While there is a lack of substantiating why adoption is desired - which is typical&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Okay, the "area director" seems to have some basic awareness that this document flunks the "engineering judgment" criterion. The "area director" tries to defend this by saying that other documents flunk too. So confidence-inspiring!&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;- the big use case seems to be to support those parties relying on NIST and FIPS for their security requirements.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Wrong. Anything+PQ, and in particular ECC+PQ, complies with NIST's standards when the PQ part does. See NIST SP 800-227: "This publication approves the use of the key combiner (14) for any t &amp;gt; 1 if at least one shared secret (i.e., Sj for some j) is generated from the key-establishment methods in SP 800-56A [1] or SP 800-56B [2] or an approved KEM." For example, if the PQ part is ML-KEM as per FIPS 203, then NIST allows ECC+PQ too.&lt;/p&gt;
    &lt;p&gt;What's next: claiming that using PQ in an Internet protocol would violate NIST standards unless NIST has standardized that particular Internet protocol?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This encompasses much more than just the US government as other certification bodies and other national governments have come to rely on the outcome of the NIST competition, which was the only public multi-year post-quantum cryptography effort to evaluate the security of proposed new post-quantum algorithms.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I won't bother addressing the errors here, since the bottom-line claim is orthogonal to the issue at hand. The TLS WG already has an ECC+PQ document using NIST-approved PQ; the question is whether to also have a document allowing the ECC seatbelt to be removed.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It was also argued pure PQ has less complexity.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;You know what would be even less complicated? Encrypting with the null cipher!&lt;/p&gt;
    &lt;p&gt;There was a claim that PQ is less complex than ECC+PQ. There was no response to Andrey Jivsov objecting that having a PQ option makes the ecosystem more complicated. The basic error in the PQ-less-complex claim is that it ignores ECC+PQ already being there.&lt;/p&gt;
    &lt;p&gt;How the "area director" described the objections.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Opposed argument summary&lt;/p&gt;
      &lt;p&gt;Most of the arguments against adoption are focused on the fact that a failsafe is better than no failsafe, irrespective of which post-quantum algorithm is used,&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is the closest that the "area director" comes to acknowledging the central security argument for ECC+PQ. Of course, the "area director" spends as little time as possible on security. Compare this to my own objection to adoption, which started with SIKE as a concrete example of the dangers and continued with "SIKE is not an isolated example: https://cr.yp.to/papers.html#qrcsp shows that 48% of the 69 round-1 submissions to the NIST competition have been broken by now".&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;and that the practical costs for hybrids are negligible.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Hmmm. By listing this as part of an "opposed argument summary", is the "area director" suggesting that this was disputed? When and where was the dispute?&lt;/p&gt;
    &lt;p&gt;As noted above, I've seen unquantified NSA/GCHQ fearmongering about costs, but that was outside IETF. If NSA and GCHQ tried the same arguments on a public mailing list then they'd end up being faced with questions that they can't answer.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It was also argued that having an RFC gives too much promotion or sense of approval to a not recommended algorithm.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When I wrote my own summary of the objections, I provided a quote and link for each point. The "area director" doesn't do this. If the "area director" is accurately presenting an argument that was raised, why not provide a quote and a link? Is the "area director" misrepresenting the argument? Making up a strawman? The reader can't tell.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I have expanded some of the arguments and my interpretation of the weight of these below.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This comment about "weight" is revealing. What we'll see again and again is that the "area director" is expressing the weight that he places on each argument (within the arguments selected and phrased by the "area director"), i.e., the extent to which he is convinced or not convinced by those arguments.&lt;/p&gt;
    &lt;p&gt;Given that IESG has power under IETF rules to unilaterally block publications approved by WGs, it's unsurprising that the "area directors", in their roles as IESG members, will end up evaluating the merits of WG-approved documents. But that isn't what this "area director" was instructed to do here. There isn't a WG-approved document at this point. Instead the "area director" was instructed to evaluate whether the chairs "appropriately" called "rough consensus" to "adopt" the document. The "area director" is supposed to be evaluating procedurally what the WG decision-makers did. Instead the "area director" is putting his thumb on the scale in favor of the document.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Non-hybrid as "basic flaw"&lt;/p&gt;
      &lt;p&gt;The argument by some opponents that non-hybrids are a "basic flaw" seems to miscategorize what a "basic flaw" is. There is currently no known "basic flaw" against MLKEM.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I think that the "area director" is trying to make some sort of claim here about ML-KEM not having been attacked, but the wording is so unclear as to be unevaluatable. Why doesn't KyberSlash count? How about Clangover? How about the continuing advances in lattice attacks that have already reduced ML-KEM below its claimed security targets, the most recent news being from last month?&lt;/p&gt;
    &lt;p&gt;More importantly, claiming that ML-KEM isn't "known" to have problems is utterly failing to address the point of the ECC seatbelt. It's like saying "This car hasn't crashed, so the absence of seatbelts isn't a basic flaw".&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;As was raised, it is rather odd to be arguing we must immediately move to use post-quantum algorithms while at the same time argue these might contain fundamental basic flaws.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here the "area director" is reasonably capturing a statement from one document proponent (original wording: "I find it to be cognitive dissonance to simultaneously argue that the quantum threat requires immediate work, and yet we are also somehow uncertain of if the algorithms are totally broken. Both cannot be true at the same time").&lt;/p&gt;
    &lt;p&gt;But I promptly followed up explaining the error: "Rolling out PQ is trying to reduce the damage from an attacker having a quantum computer within the security lifetime of the user data. Doing that as ECC+PQ instead of just PQ is trying to reduce the damage in case the PQ part is broken. These actions are compatible, so how exactly do you believe they're contradictory?"&lt;/p&gt;
    &lt;p&gt;There was, of course, no reply at the time. The "area director" now simply repeats the erroneous argument.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;As TLS (or IETF) is not phasing out all non-hybrid classics,&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;"Non-hybrid classics" is weird terminology. Sometimes pre-quantum algorithms (ECC, RSA, etc.) are called "classical", so I guess the claim here is that using just ECC in TLS isn't being phased out. That's a bizarre claim. There are intensive efforts to roll out ECC+PQ in TLS to try to protect against quantum computers. Cloudflare reports the usage of post-quantum cryptography having risen to about 50% of all browsers that it sees (compared to 20% a year ago); within those connections, 95% use ECC+MLKEM768 and 5% use ECC+Kyber768.&lt;/p&gt;
    &lt;p&gt;The "area director" also gives no explanation of why the "not phasing out" claim is supposed to be relevant here.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I find this argument not strong enough&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;See how the "area director" is saying the weight that the "area director" places on each argument (within the arguments selected and phrased by the "area director"), rather than evaluating whether there was consensus to adopt the document?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;to override the consensus of allowing non-hybrid standards from being defined&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Circular argument. There wasn't consensus to adopt the document in the first place.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;especially in light of the strong consensus for marking these as "not recommended".&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I think many readers will be baffled by this comment. If something is "not recommended", wouldn't that be an argument against standardizing it, rather than an argument for standardizing it?&lt;/p&gt;
    &lt;p&gt;The answer is that "not recommended" doesn't mean what you think it means: the "area director" is resorting to confusing jargon. I don't think there's any point getting into the weeds on this.&lt;/p&gt;
    &lt;p&gt;Incompetent planning for the future.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Non-hybrids are a future end goal&lt;/p&gt;
      &lt;p&gt;Additionally, since if/when we do end up in an era with a CRQC, we are ultimately designing for a world where the classic components offer less to no value.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;If someone is trying to argue for removing ECC, there's a big difference between the plausible scenario of ECC having "less" value and the extreme scenario of ECC having "no" value. It's wrong for the "area director" to be conflating these possibilities.&lt;/p&gt;
    &lt;p&gt;As I put it almost two years ago: "Concretely, think about a demo showing that spending a billion dollars on quantum computation can break a thousand X25519 keys. Yikes! We should be aiming for much higher security than that! We don't even want a billion-dollar attack to be able to break one key! Users who care about the security of their data will be happy that we deployed post-quantum cryptography. But are the users going to say 'Let's turn off X25519 and make each session a million dollars cheaper to attack'? I'm skeptical. I think users will need to see much cheaper attacks before agreeing that X25519 has negligible security value."&lt;/p&gt;
    &lt;p&gt;Furthermore, let's think for a moment about the idea that one will eventually want to transition to just ML-KEM, the specific proposal that the "area director" is portraying as the future. Here are three ways that this can easily be wrong:&lt;/p&gt;
    &lt;p&gt;Maybe ML-KEM's implementation issues end up convincing the community to shift to a more robust option, analogously to what happened with ECC.&lt;/p&gt;
    &lt;p&gt;Maybe the advances in public attacks continue to the point of breaking ML-KEM outright.&lt;/p&gt;
    &lt;p&gt;Maybe the cliff stops crumbling and ML-KEM survives, but more efficient options also survive. At this point there are quite a few options more efficient than ML-KEM. (Random example: SMAUG. The current SMAUG software isn't as fast as the ML-KEM software, but this is outweighed by SMAUG using less network traffic than ML-KEM.) Probably some options will be broken, but ML-KEM would have to be remarkably lucky to end up as the most efficient remaining option.&lt;/p&gt;
    &lt;p&gt;Does this "area director" think that all of the more efficient options are going to be broken, while ML-KEM won't? Sounds absurdly overconfident. More likely is that the "area director" doesn't even realize that there are more efficient options. For anyone thinking "presumably those newer options have received less scrutiny than ML-KEM": we're talking about what to do long-term, remember?&lt;/p&gt;
    &lt;p&gt;Taking ML-KEM as the PQ component of ECC+PQ is working for getting something rolled out now. Hopefully ML-KEM will turn out to not be a security disaster (or a patent disaster). But, for guessing what will be best to do in 5 or 10 or 15 years, picking ML-KEM is premature.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;When and where to exactly draw the line of still using a classic component safeguard is speculation at best.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here the "area director" is clearly attacking a strawman.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Already supporting pure post quantum algorithms now to gain experience&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;How is rolling out PQ supposed to be gaining experience that isn't gained from the current rollout of ECC+PQ?&lt;/p&gt;
    &lt;p&gt;Also, I think it's important to call out the word "pure" here as incoherent, indefensible marketing. What we're actually talking about isn't modifying ML-KEM in any way; it's simply hashing the ML-KEM session key together with other inputs. Is ML-KEM no longer "pure" when it's plugged into TLS, which also hashes session keys? (The word "pure" also showed up in a few of the earlier quotes.)&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;while not recommending it at this time seems a valid strategy for the future, allowing people and organizations their own timeline of deciding when/if to go from hybrid to pure PQ.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here we again see the area director making a decision to support the document, rather than evaluating whether there was consensus in the WG to adopt the document.&lt;/p&gt;
    &lt;p&gt;Again getting the complexity evaluation backwards.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Added complexity of hybrids&lt;/p&gt;
      &lt;p&gt;There was some discussion on whether or not hybrids add more complexity, and thus add risk, compared to non-hybrids. While arguments were made that proper classic algorithms add only a trivial amount of extra resources, it was also pointed out that there is a cost of implementation, deployment and maintenance.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here the "area director" is again making the same mistake explained earlier: ignoring the fact that ECC+PQ is already there, and thus getting the complexity evaluation backwards.&lt;/p&gt;
    &lt;p&gt;The "thus add risk" logic is also wrong. Again, all of these options are more complex than the null cipher.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Additionally, the existence of draft-ietf-tls-hybrid-design and the extensive discussions around "chempat" vs "xwing" vs "kitchensink" shows that there is at least some complexity that is added by the hybrid solutions.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;No, the details of how to combine ECC with PQ in TLS are already settled and deployed.&lt;/p&gt;
    &lt;p&gt;Looking beyond TLS: Chempat hashes the transcript (similarly to TLS), making it robust for a wide range of protocols. The other options add fragility by hashing less for the sake of minor cost savings. Each of these options is under 10 lines of code. The "area director" exaggerates the complexity by mentioning "extensive discussions", and spends much more effort hyping this complexity as a risk than acknowledging the risks of further PQ attacks.&lt;/p&gt;
    &lt;p&gt;Anyway, it's not as if the presence of this document has eliminated the discussions of ECC+PQ details, nor is there any credible mechanism by which it could do so. Again, the actual choice at hand is whether to have PQ as an option alongside ECC+PQ. Adding that option adds complexity. The "area director" is getting the complexity comparison backwards by instead comparing (1) PQ in isolation to (2) ECC+PQ in isolation.&lt;/p&gt;
    &lt;p&gt;Botching the evaluation of human factors.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;RFCs being interpreted as IETF recommendation&lt;/p&gt;
      &lt;p&gt;It seems there is disagreement about whether the existence of an RFC itself qualifies as the IETF defacto "recommending" this in the view of IETF outsiders/ implemeners whom do not take into account any IANA registry RECOMMENDED setting or the Mandatory-To-Implement (MTI) reommendations.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I would expect a purchasing manager to have instructions along the lines of "Buy only products complying with the standards", and to never see IETF's confusing jumble of further designations.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This is an area where we recently found out there is little consensus on an IETF wide crypto policy statement via an RFC. The decision on whether an RFC adds value to a Code Point should therefor be taken independently of any such notion of how outsiders might interpret the existence of an RFC.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;From a security perspective, it's a big mistake to ignore the human factor, such as the impact of a purchasing manager saying "This is the most efficient standard so I'll pick that".&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In this case, while Section 3 could be considered informative, I believe Section 4 and Section 5 are useful (normative) content that assists implementers.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Is this supposed to have something to do with the consensus question?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;And people have proposed extending the Security Considerations to more clearly state that this algorithm is not recommended at this point in time. Without an RFC, these recommendations cannot be published by the IETF in a way that implementers would be known to consume.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Ah, yes, "known to consume"! There was, um, one of those, uh, studies showing the details of, um, how implementors use RFCs, which, uh, showed that 100% of the implementors diligently consumed the warnings in the RFCs. Yeah, that's the ticket. I'm sure the URL for this study is sitting around here somewhere.&lt;/p&gt;
    &lt;p&gt;Let's get back to the real world. Even if an implementor does see a "This document is a bad idea" warning, this simply doesn't matter when the implementors are chasing contracts issued by purchasing managers who simply care what's standardized and haven't seen the warning.&lt;/p&gt;
    &lt;p&gt;It's much smarter for the document to (1) eliminate making the proposal that it's warning about and (2) focus, starting in the title, on saying why such proposals are bad. This makes people more likely to see the warning, and at the same time it removes the core problem of the bad proposal being standardized.&lt;/p&gt;
    &lt;p&gt;Fictions regarding country actions.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Say no to Nation State algorithms&lt;/p&gt;
      &lt;p&gt;The history and birth of MLKEM from Kyber through a competition of the international Cryptographic Community, organized through US NIST can hardly be called or compared to unilateral dictated nation state algorithm selection.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;NIST repeatedly refused to designate the "NIST Post-Quantum Cryptography Standardization Process" as a "competition". It even wrote that the process "should not be treated as a competition".&lt;/p&gt;
    &lt;p&gt;Certainly there were competition-like aspects to the process. I tend to refer to it as a competition. But in the end the selection of algorithms to standardize was made by NIST, with input behind the scenes from NSA.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;There has been no other comparable public effort to gather cryptographers and publicly discuss post-quantum crypto candidates in a multi-years effort.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Nonsense. The premier multi-year effort by cryptographers to "publicly discuss post-quantum crypto candidates" is the cryptographic literature.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In fact, other nation states are heavily relying on the results produced by this competition.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here's the objection from Stephen Farrell that the "area director" isn't quoting or linking to: "I don't see what criteria we might use in adopting this that wouldn't leave the WG open to accusations of favouritism if we don't adopt other pure PQ national standards that will certainly arise".&lt;/p&gt;
    &lt;p&gt;After reading this objection, you can see how the "area director" is sort of responding to it by suggesting that everybody is following NIST (i.e., that the "certainly arise" part is wrong).&lt;/p&gt;
    &lt;p&gt;But that's not true. NIST's selections are controversial. For example, ISO is considering not just ML-KEM but also&lt;/p&gt;
    &lt;p&gt;Classic McEliece, where NIST has said it's waiting for ISO ("After the ISO standardization process has been completed, NIST may consider developing a standard for Classic McEliece based on the ISO standard"), and&lt;/p&gt;
    &lt;p&gt;FrodoKEM, which NIST said "will not be considered further for standardization".&lt;/p&gt;
    &lt;p&gt;ISO is also now considering NTRU, where the advertisement includes "All patents related to NTRU have expired" (very different from the ML-KEM situation).&lt;/p&gt;
    &lt;p&gt;BSI, which sets cryptographic standards for Germany, recommends not just ML-KEM but also FrodoKEM (which it describes as "more conservative" than ML-KEM) and Classic McEliece ("conservative and very thoroughly analysed"). Meanwhile China has called for submissions of new post-quantum proposals for standardization.&lt;/p&gt;
    &lt;p&gt;I could keep going, but this is enough evidence to show that Farrell's prediction was correct; the "area director" is once again wrong.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The use of MLKEM in the IETF will not set a precedent for having to accept other nation state cryptography.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Notice how the "area director" is dodging Farrell's point. If NSA can pressure the TLS WG into standardizing non-hybrid ML-KEM, why can't China pressure the TLS WG into standardizing something China wants? What criteria will IETF use to answer this question without leaving the WG "open to accusations of favouritism"? If you want people to believe that it isn't about the money then you need a really convincing alternative story.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Not recommending pure PQ right now&lt;/p&gt;
      &lt;p&gt;There was a strong consensus that pure PQ should not be recommended at this time, which is reflected in the document. There was some discussion on RECOMMENDED N vs D, which is something that can be discussed in the WG during the document's lifecycle before WGLC. It was further argued that adopting and publishing this document gives the WG control over the accompanying warning text, such as Security Considerations, that can reflect the current consensus of not recommending pure MLKEM over hybrid at publication time.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is just rehashing earlier text, even if the detailed wording is a bit different.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Conclusion&lt;/p&gt;
      &lt;p&gt;The pure MLKEM code points exist.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Irrelevant. The question is whether they're being standardized.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;An international market segment that wants to use pure MLKEM exists&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;"International"? Like Swedish company Ericsson setting up its "Ericsson Federal Technologies Group" in 2024 to receive U.S. military contracts?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;as can be seen by the consensus call outcome&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Um, how?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;along with existing implementations of the draft on mainstream devices and software.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Yes, NSA waving around money has convinced some corporations to provide software. How is this supposed to justify the claim that "there was rough consensus to adopt the document"?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;There is a rough consensus to adopt the document&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Repeating a claim doesn't make it true.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;with a strong consensus for RECOMMENDED N and not MTI, which is reflected in the draft.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Irrelevant. What matters is whether the document is standardized.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The reasons to not publish MLKEM as an RFC seem more based on personal opinions of risk and trust not shared amongst all participants as facts.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This sort of dismissal might be more convincing if it were coming from someone providing more URLs and fewer easily debunked claims. But it's in any case not addressing the consensus question.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Based on the above, I believe the WG Chairs made the correct call that there was rough consensus for adopting draft-connolly-tls-mlkem-key-agreement&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The chairs claimed that "we have consensus to adopt this draft" (based on claiming that "there were enough people willing to review the draft", never mind the number of objections). That claim is wrong. The call for adoption failed to reach consensus.&lt;/p&gt;
    &lt;p&gt;The "area director" claimed that "There is clearly consensus based on the 67 responses to the adoption call. ... The vast majority was in favour of adoption ... There were a few dissenting opinions". These statements still haven't been retracted; they were and are outright lies about what happened. Again, the actual tallies were 20 people unequivocally supporting adoption, 2 people conditionally supporting adoption, and 7 people unequivocally opposing adoption.&lt;/p&gt;
    &lt;p&gt;Without admitting error, the "area director" has retreated to a claim of "rough consensus". The mishmash of ad-hoc comments from the "area director" certainly doesn't demonstrate any coherent meaning of "rough consensus".&lt;/p&gt;
    &lt;p&gt;It's fascinating that IETF's advertising to the public claims that IETF's "decision-making requires achieving broad consensus", but IETF's WG procedures allow controversial documents to be pushed through on the basis of "rough consensus". To be clear, that's only if the "area director" approves of the documents, as you can see from the same "area director" issuing yet another mishmash of ad-hoc comments to overturn a separate chair decision in September 2025.&lt;/p&gt;
    &lt;p&gt;You would think that the WG procedures would define "rough consensus". They don't. All they say is that "51% of the working group does not qualify as 'rough consensus' and 99% is better than rough", not even making clear whether 51% of voters within a larger working group can qualify. This leaves a vast range of ambiguous intermediate cases up to the people in power.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46033151</guid><pubDate>Mon, 24 Nov 2025 12:00:55 +0000</pubDate></item><item><title>Chrome Jpegxl Issue Reopened</title><link>https://issues.chromium.org/issues/40168998</link><description>&lt;doc fingerprint="732bc1ae2d485202"&gt;
  &lt;main&gt;
    &lt;p&gt;Sign in&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46033330</guid><pubDate>Mon, 24 Nov 2025 12:23:02 +0000</pubDate></item><item><title>Corvus Robotics (YC S18): Hiring Head of Mfg/Ops, Next Door to YC Mountain View</title><link>https://news.ycombinator.com/item?id=46036222</link><description>&lt;doc fingerprint="9c16a0a6b25e9e2d"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Corvus Robotics is scaling the largest autonomous logistics data capture fleet ever built. If you're allergic to leaded solder, actual revenue, spreadsheets, or ambiguity -- this role is not for you. More "cardboard boxes," less "Pelican cases."&lt;/p&gt;
      &lt;p&gt;Our fleet of flying warehouse drones is 5x-ing in 2026, and I'm looking for a generalist ex-founder or manufacturing leader in the SF Bay Area who wants to get their hands dirty massively scaling manufacturing operations in the US and overseas.&lt;/p&gt;
      &lt;p&gt;We need someone who has worked on hardware products (not just SaaS), communicates clearly, and is relentlessly resourceful. Mandarin proficiency and leading a product through EVT/DVT/PVT is an added bonus.&lt;/p&gt;
      &lt;p&gt;If this resonates with you, DM me, or send a super short email to a@ our url with: - why you‚Äôre interested - what was the biggest manufacturing fuckup you've recovered from - your target comp&lt;/p&gt;
      &lt;p&gt;PS - Please reshare our LinkedIn post! https://www.linkedin.com/posts/mhkabirr_at-corvus-robotics-w...&lt;/p&gt;
      &lt;p&gt;Thanks, Jackie&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46036222</guid><pubDate>Mon, 24 Nov 2025 17:00:33 +0000</pubDate></item><item><title>Implications of AI to schools</title><link>https://twitter.com/karpathy/status/1993010584175141038</link><description>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We‚Äôve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info ¬© 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46036878</guid><pubDate>Mon, 24 Nov 2025 17:51:02 +0000</pubDate></item><item><title>Cool-retro-term: terminal emulator which mimics look and feel of the old CRTs</title><link>https://github.com/Swordfish90/cool-retro-term</link><description>&lt;doc fingerprint="afec61b99b81218b"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;&amp;gt; Default Amber&lt;/cell&gt;
        &lt;cell role="head"&gt;C:\ IBM DOS&lt;/cell&gt;
        &lt;cell role="head"&gt;$ Default Green&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;cool-retro-term is a terminal emulator which mimics the look and feel of the old cathode tube screens. It has been designed to be eye-candy, customizable, and reasonably lightweight.&lt;/p&gt;
    &lt;p&gt;It uses the QML port of qtermwidget (Konsole): https://github.com/Swordfish90/qmltermwidget.&lt;/p&gt;
    &lt;p&gt;This terminal emulator works under Linux and macOS and requires Qt5. It's suggested that you stick to the latest LTS version.&lt;/p&gt;
    &lt;p&gt;Settings such as colors, fonts, and effects can be accessed via context menu.&lt;/p&gt;
    &lt;p&gt;If you want to get a hold of the latest version, just go to the Releases page and grab the latest AppImage (Linux) or dmg (macOS).&lt;/p&gt;
    &lt;p&gt;Alternatively, most distributions such as Ubuntu, Fedora or Arch already package cool-retro-term in their official repositories.&lt;/p&gt;
    &lt;p&gt;Check out the wiki and follow the instructions on how to build it on Linux and macOS.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46036895</guid><pubDate>Mon, 24 Nov 2025 17:52:01 +0000</pubDate></item><item><title>Show HN: I built an interactive HN Simulator</title><link>https://news.ysimulator.run/news</link><description>&lt;doc fingerprint="777ff7b7fede5c03"&gt;
  &lt;main&gt;
    &lt;p&gt;More&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46036908</guid><pubDate>Mon, 24 Nov 2025 17:52:43 +0000</pubDate></item><item><title>Mind-reading devices can now predict preconscious thoughts</title><link>https://www.nature.com/articles/d41586-025-03714-0</link><description>&lt;doc fingerprint="bc2ac354b92b09e0"&gt;
  &lt;main&gt;
    &lt;p&gt;Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript.&lt;/p&gt;
    &lt;p&gt;Before a car crash in 2008 left her paralysed from the neck down, Nancy Smith enjoyed playing the piano. Years later, Smith started making music again, thanks to an implant that recorded and analysed her brain activity. When she imagined playing an on-screen keyboard, her brain‚Äìcomputer interface (BCI) translated her thoughts into keystrokes ‚Äî and simple melodies, such as ‚ÄòTwinkle, Twinkle, Little Star‚Äô, rang out1.&lt;/p&gt;
    &lt;p&gt;But there was a twist. For Smith, it seemed as if the piano played itself. ‚ÄúIt felt like the keys just automatically hit themselves without me thinking about it,‚Äù she said at the time. ‚ÄúIt just seemed like it knew the tune, and it just did it on its own.‚Äù&lt;/p&gt;
    &lt;p&gt;Smith‚Äôs BCI system, implanted as part of a clinical trial, trained on her brain signals as she imagined playing the keyboard. That learning enabled the system to detect her intention to play hundreds of milliseconds before she consciously attempted to do so, says trial leader Richard Andersen, a neuroscientist at the California Institute of Technology in Pasadena.&lt;/p&gt;
    &lt;p&gt;Smith is one of roughly 90 people who, over the past two decades, have had BCIs implanted to control assistive technologies, such as computers, robotic arms or synthetic voice generators. These volunteers ‚Äî paralysed by spinal-cord injuries, strokes or neuromuscular disorders, such as motor neuron disease (amyotrophic lateral sclerosis) ‚Äî have demonstrated how command signals for the body‚Äôs muscles, recorded from the brain‚Äôs motor cortex as people imagine moving, can be decoded into commands for connected devices.&lt;/p&gt;
    &lt;p&gt;But Smith, who died of cancer in 2023, was among the first volunteers to have an extra interface implanted in her posterior parietal cortex, a brain region associated with reasoning, attention and planning. Andersen and his team think that by also capturing users‚Äô intentions and pre-motor planning, such ‚Äòdual-implant‚Äô BCIs will improve the performance of prosthetic devices.&lt;/p&gt;
    &lt;p&gt;Andersen‚Äôs research also illustrates the potential of BCIs that access areas outside the motor cortex. ‚ÄúThe surprise was that when we go into the posterior parietal, we can get signals that are mixed together from a large number of areas,‚Äù says Andersen. ‚ÄúThere‚Äôs a wide variety of things that we can decode.‚Äù&lt;/p&gt;
    &lt;p&gt;The ability of these devices to access aspects of a person‚Äôs innermost life, including preconscious thought, raises the stakes on concerns about how to keep neural data private. It also poses ethical questions about how neurotechnologies might shape people‚Äôs thoughts and actions ‚Äî especially when paired with artificial intelligence.&lt;/p&gt;
    &lt;p&gt;Meanwhile, AI is enhancing the capabilities of wearable consumer products that record signals from outside the brain. Ethicists worry that, left unregulated, these devices could give technology companies access to new and more precise data about people‚Äôs internal reactions to online and other content.&lt;/p&gt;
    &lt;p&gt;Ethicists and BCI developers are now asking how previously inaccessible information should be handled and used. ‚ÄúWhole-brain interfacing is going to be the future,‚Äù says Tom Oxley, chief executive of Synchron, a BCI company in New York City. He predicts that the desire to treat psychiatric conditions and other brain disorders will lead to more brain regions being explored. Along the way, he says, AI will continue to improve decoding capabilities and change how these systems serve their users. ‚ÄúIt leads you to the final question: how do we make that safe?‚Äù&lt;/p&gt;
    &lt;p&gt;Consumer concerns&lt;/p&gt;
    &lt;p&gt;Consumer neurotech products capture less-sophisticated data than implanted BCIs do. Unlike implanted BCIs, which rely on the firings of specific collections of neurons, most consumer products rely on electroencephalography (EEG). This measures ripples of electrical activity that arise from the averaged firing of huge neuronal populations and are detectable on the scalp. Rather than being created to capture the best recording possible, consumer devices are designed to be stylish (such as in sleek headbands) or unobtrusive (with electrodes hidden inside headphones or headsets for augmented or virtual reality).&lt;/p&gt;
    &lt;p&gt;Still, EEG can reveal overall brain states, such as alertness, focus, tiredness and anxiety levels. Companies already offer headsets and software that give customers real-time scores relating to these states, with the intention of helping them to improve their sports performance, meditate more effectively or become more productive, for example.&lt;/p&gt;
    &lt;p&gt;AI has helped to turn noisy signals from suboptimal recording systems into reliable data, explains Ramses Alcaide, chief executive of Neurable, a neurotech company in Boston, Massachusetts, that specializes in EEG signal processing and sells a headphone-based headset for this purpose. ‚ÄúWe‚Äôve made it so that EEG doesn‚Äôt suck as much as it used to,‚Äù Alcaide says. ‚ÄúNow, it can be used in real-life environments, essentially.‚Äù&lt;/p&gt;
    &lt;p&gt;And there is widespread anticipation that AI will allow further aspects of users‚Äô mental processes to be decoded. For example, Marcello Ienca, a neuroethicist at the Technical University of Munich in Germany, says that EEG can detect small voltage changes in the brain that occur within hundreds of milliseconds of a person perceiving a stimulus. Such signals could reveal how their attention and decision-making relate to that specific stimulus.&lt;/p&gt;
    &lt;p&gt;Although accurate user numbers are hard to gather, many thousands of enthusiasts are already using neurotech headsets. And ethicists say that a big tech company could suddenly catapult the devices to widespread use. Apple, for example, patented a design for EEG sensors for future use in its Airpods wireless earphones in 2023.&lt;/p&gt;
    &lt;p&gt;Yet unlike BCIs aimed at the clinic, which are governed by medical regulations and privacy protections, the consumer BCI space has little legal oversight, says David Lyreskog, an ethicist at the University of Oxford, UK. ‚ÄúThere‚Äôs a wild west when it comes to the regulatory standards,‚Äù he says.&lt;/p&gt;
    &lt;p&gt;In 2018, Ienca and his colleagues found that most consumer BCIs don‚Äôt use secure data-sharing channels or implement state-of-the-art privacy technologies2. ‚ÄúI believe that has not changed,‚Äù Ienca says. What‚Äôs more, a 2024 analysis3 of the data policies of 30 consumer neurotech companies by the Neurorights Foundation, a non-profit organization in New York City, showed that nearly all had complete control over the data users provided. That means most firms can use the information as they please, including selling it.&lt;/p&gt;
    &lt;p&gt;Responding to such concerns, the government of Chile and the legislators of four US states have passed laws that give direct recordings of any form of nerve activity protected status. But Ienca and Nita Farahany, an ethicist at Duke University in Durham, North Carolina, fear that such laws are insufficient because they focus on the raw data and not on the inferences that companies can make by combining neural information with parallel streams of digital data. Inferences about a person‚Äôs mental health, say, or their political allegiances could still be sold to third parties and used to discriminate against or manipulate a person.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe data economy, in my view, is already quite privacy-violating and cognitive- liberty-violating,‚Äù Ienca says. Adding neural data, he says, ‚Äúis like giving steroids to the existing data economy‚Äù.&lt;/p&gt;
    &lt;p&gt;Several key international bodies, including the United Nations cultural organization UNESCO and the Organisation for Economic Co-operation and Development, have issued guidelines on these issues. Furthermore, in September, three US senators introduced an act that would require the Federal Trade Commission to review how data from neurotechnology should be protected.&lt;/p&gt;
    &lt;p&gt;Heading to the clinic&lt;/p&gt;
    &lt;p&gt;While their development advances at pace, so far no implanted BCI has been approved for general clinical use. Synchron‚Äôs device is closest to the clinic. This relatively simple BCI allows users to select on-screen options by imagining moving their foot. Because it is inserted into a blood vessel on the surface of the motor cortex, it doesn‚Äôt require neurosurgery. It has proved safe, robust and effective in initial trials4, and Oxley says Synchron is discussing a pivotal trial with the US Food and Drug Administration that could lead to clinical approval.&lt;/p&gt;
    &lt;p&gt;Elon Musk‚Äôs neurotech firm Neuralink in Fremont, California, has surgically implanted its more complex device in the motor cortices of at least 13 volunteers who are using it to play computer games, for example, and control robotic hands. Company representatives say that more than 10,000 people have joined waiting lists for its clinical trials.&lt;/p&gt;
    &lt;p&gt;At least five more BCI companies have tested their devices in humans for the first time over the past two years, making short-term recordings (on timescales ranging from minutes to weeks) in people undergoing neurosurgical procedures. Researchers in the field say the first approvals are likely to be for devices in the motor cortex that restore independence to people who have severe paralysis ‚Äî including BCIs that enable speech through synthetic voice technology.&lt;/p&gt;
    &lt;p&gt;As for what‚Äôs next, Farahany says that moving beyond the motor cortex is a widespread goal among BCI developers. ‚ÄúAll of them hope to go back further in time in the brain,‚Äù she says, ‚Äúand to get to that subconscious precursor to thought.‚Äù&lt;/p&gt;
    &lt;p&gt;Last year, Andersen‚Äôs group published a proof-of-concept study5 in which internal dialogue was decoded from the parietal cortex of two participants, albeit with an extremely limited vocabulary. The team has also recorded from the parietal cortex while a BCI user played the card game blackjack (pontoon)6. Certain neurons responded to the face values of cards, whereas others tracked the cumulative total of a player‚Äôs hand. Some even became active when the player decided whether to stick with their current hand or take another card.&lt;/p&gt;
    &lt;p&gt;Both Oxley and Matt Angle, chief executive of BCI company Paradromics, based in Austin, Texas, agree that BCIs in brain regions other than the motor cortex might one day help to diagnose and treat psychiatric conditions. Maryam Shanechi, an engineer and computer scientist at the University of Southern California in Los Angeles, is working towards this goal ‚Äî in part by aiming to identify and monitor neural signatures of psychiatric diseases and their symptoms7.&lt;/p&gt;
    &lt;p&gt;BCIs could potentially track such symptoms in a person, deliver stimulation that adjusts neural activity and quantify how the brain responds to that stimulation or other interventions. ‚ÄúThat feedback is important, because you want to precisely tailor the therapy to that individual‚Äôs own needs,‚Äù Shanechi says.&lt;/p&gt;
    &lt;p&gt;Shanechi does not yet know whether the neural correlates of psychiatric symptoms will be trackable across many brain regions or whether they will require recording from specific brain areas. Either way, a central aspect of her work is building foundation models of brain activity. Such models, constructed by training AI algorithms on thousands of hours of neural data from numerous people, would in theory be generalizable across individuals‚Äô brains.&lt;/p&gt;
    &lt;p&gt;Enjoying our latest content? Log in or create an account to continue&lt;/p&gt;
    &lt;p&gt;Access the most recent journalism from Nature's award-winning team&lt;/p&gt;
    &lt;p&gt;Explore the latest features &amp;amp; opinion covering groundbreaking research&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46037267</guid><pubDate>Mon, 24 Nov 2025 18:26:09 +0000</pubDate></item><item><title>TSMC Arizona Outage Saw Fab Halt, Apple Wafers Scrapped</title><link>https://www.culpium.com/p/tsmc-arizona-outage-saw-fab-halt</link><description>&lt;doc fingerprint="d02a087caa68d73"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;TSMC Arizona Outage Saw Fab Halt, Apple Wafers Scrapped&lt;/head&gt;
    &lt;head rend="h3"&gt;[Exclusive] Supply of industrial gases used for chipmaking was cut when power at a vendor's facilities was interrupted.&lt;/head&gt;
    &lt;p&gt;Good Evening from Taipei,&lt;/p&gt;
    &lt;p&gt;A power outage at an industrial gas facility servicing TSMC interrupted manufacturing at the company‚Äôs Fab 21 in Arizona late last quarter, sources told me. The incident stopped the flow of crucial inputs needed for chipmaking, forcing the facility to shut down for at least a few hours, I was told. As a result, the company had to scrap thousands of wafers that were in production for clients at the site which include Apple, Nvidia, and AMD.&lt;/p&gt;
    &lt;p&gt;The event happened mid-September and was caused by a power fault at its outsourced vendor Linde, a British industrial gases and engineering company, my sources tell me. TSMC runs a lot of its own gas supply in Taiwan, but opted to contract the work out for its Arizona site. While mistakes happen, and insurance may cover some of the losses from the event, Linde has been put on notice to identify and rectify the cause of the outage, I was told. A PR representative for Linde didn‚Äôt answer multiple phone calls and emails from Culpium outlining the incident and requesting comment.&lt;/p&gt;
    &lt;p&gt;TSMC‚Äôs Arizona unit turned profitable in the first quarter of this year, a sign of the Taiwanese company‚Äôs ability to quickly scale up and churn out chips even in higher-cost locales like the US. But a 99% drop in net income in the third quarter to just $1.4 million had folks scratching their head. One writer was quick to jump to conclusions, with the assumption that ‚Äúrising costs have taken out an enormous chunk of profits, putting pressure on the firm‚Äôs operations.‚Äù The September outage, which hasn‚Äôt previously been reported, offers an alternative explanation for the profit decline.&lt;/p&gt;
    &lt;p&gt;‚ÄúTSMC Arizona has begun to positively contribute to TSMC‚Äôs revenue. However, the company‚Äôs profit is influenced by multiple factors and should be read over time,‚Äù TSMC wrote in response to a detailed account of what Culpium has been told about the outage. ‚ÄúWe also stated before that the ramp up for our overseas fabs will lead to gross margin dilution in the next five years, starting from 2025.‚Äù&lt;/p&gt;
    &lt;p&gt;Unfortunately, the company declined to immediately address the issue of the manufacturing disruption.&lt;/p&gt;
    &lt;p&gt;Fab shutdowns are unusual, at least for TSMC. With equipment so expensive, its factories are run 24/7. That means that an hour of idle time can cost millions of dollars. Compounding the financial effect of this incident was the fact that it occurred late in the quarter, leaving little room to make up for lost production before the quarter closed.&lt;/p&gt;
    &lt;p&gt;Profit margins on new facilities and at new nodes tend to be quite thin, even negative. In addition, TSMC has been ramping up capacity in Arizona and that capex gets reflected in depreciation costs even before the new equipment can start producing revenue. So it‚Äôs reasonable to see fluctuations in net income at the site. A halt in production and scrapping of wafers adds to the costs, dragging on earnings even if only slightly and briefly.&lt;/p&gt;
    &lt;p&gt;Impact to clients is likely to be negligible, I was told, and the financial loss to TSMC may be covered by insurance. Capacity at Fab 21 is still quite small, and many products being made there have already been taped out and manufactured in Taiwan previously. In past disruptions, lost production and revenue was made up in the subsequent quarter.&lt;/p&gt;
    &lt;p&gt;That said, the broader issue is that Taiwanese manufacturers are good at managing their operations when they handle it themselves, but still face struggles when they need to lean on non-Taiwanese firms at overseas facilities. The entire process of building the fab and installing equipment at Arizona has been an exercise in cross-cultural adaptation.&lt;/p&gt;
    &lt;p&gt;The most common cause of production interruptions at TSMC is Mother Nature. Earthquakes regularly rattle Taiwan, and fabs are built to withstand most of them. But sometimes a big tremor can trigger a safety shutdown, while really nasty temblors have caused actual damage. Beyond natural disasters, there‚Äôve been few man-made shutdowns at TSMC because they‚Äôre pretty rigorous about operations.&lt;/p&gt;
    &lt;p&gt;A couple of notable problems were both caused by vendors, not TSMC internally. In 2018, a computer virus was introduced to fabs via equipment from Japan. That incident sparked a whole new approach to cybersecurity both at TSMC and among fellow Taiwanese chipmakers. Less than a year later, a batch of contaminated photoresist from a chemical supplier forced the company to scrap a large number of wafers. It made up the production the following quarter, with the problem costing TSMC around $25 million in operating profit for the year.&lt;/p&gt;
    &lt;p&gt;Linde trumpeted the TSMC contract when it landed the deal back in 2021, noting that it planned to invest $600 million into the facility. ‚ÄúWhile the project is capital and electricity intensive, it will only employ 14 plant employees and 14 truck drivers, documents from 2020 said,‚Äù the Arizona Tech Council later reported.&lt;/p&gt;
    &lt;p&gt;Apple‚Äôs A16 SoC was the first product taped out at the site, Culpium reported in September last year. AMD‚Äôs Ryzen 9000 and Nvidia Blackwell chips were since added to the lineup with designs from Bitdeer, among others, also qualified at the Arizona fab.&lt;/p&gt;
    &lt;p&gt;Thanks for reading. Please subscribe, if you haven‚Äôt already.&lt;/p&gt;
    &lt;p&gt;More from Culpium:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46037324</guid><pubDate>Mon, 24 Nov 2025 18:30:48 +0000</pubDate></item><item><title>The Bitter Lesson of LLM Extensions</title><link>https://www.sawyerhood.com/blog/llm-extension</link><description>&lt;doc fingerprint="d546845b0b9b86be"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Bitter Lesson of LLM Extensions&lt;/head&gt;
    &lt;p&gt;Three years ago, ‚Äúusing an LLM‚Äù meant pasting a wall of text into a chat box and hoping for something useful back. Today, we point agents at our codebases, our browsers, and let them go off and act on our behalf. A key question that has been brewing under the surface during this time has been: how do we let end users actually customize these systems?&lt;/p&gt;
    &lt;p&gt;As models have become more capable, the ways and mechanisms that end users have access to customize them have expanded as well. We've gone from simple system prompts to complex client-server protocols and back again.&lt;/p&gt;
    &lt;p&gt;I wanted to take a moment to reflect on the history of LLM extension over the last three years and where I see it going in the future.&lt;/p&gt;
    &lt;head rend="h2"&gt;ChatGPT Plugins (March 2023)&lt;/head&gt;
    &lt;p&gt;Just four months after launch, OpenAI announced ChatGPT Plugins. Looking back, these were wildly ahead of their time.&lt;/p&gt;
    &lt;p&gt;The idea was ambitious: give the LLM a link to an OpenAPI spec and let it "run wild" calling REST endpoints. It was a direct line to AGI-style thinking: universal tool use via standard APIs.&lt;/p&gt;
    &lt;code&gt;{
  "schema_version": "v1",
  "name_for_human": "TODO Manager",
  "name_for_model": "todo_manager",
  "description_for_human": "Manages your TODOs!",
  "description_for_model": "An app for managing a user's TODOs",
  "api": { "url": "/openapi.json" },
  "auth": { "type": "none" },
  "logo_url": "https://example.com/logo.png",
  "legal_info_url": "http://example.com",
  "contact_email": "hello@example.com"
}
&lt;/code&gt;
    &lt;p&gt;The problem? The models weren't ready. GPT-3.5 (and even early GPT-4) struggled to navigate massive API specs without hallucinating or getting lost in context. Plus, the UX was clunky. You had to manually toggle plugins for every chat!&lt;/p&gt;
    &lt;p&gt;Here's what that looked like:&lt;/p&gt;
    &lt;p&gt;But it gave us a glimpse of the future: The Code Interpreter plugin (later Advanced Data Analysis) became indispensable, foreshadowing the powerful sandboxed execution environments we use today.&lt;/p&gt;
    &lt;head rend="h2"&gt;Custom Instructions (July 2023)&lt;/head&gt;
    &lt;p&gt;Custom instructions were the "smooth brain" counter-reaction to the complexity of plugins. I did a double take when writing this because I thought for sure this feature was released before plugins.&lt;/p&gt;
    &lt;p&gt;It was just a user-defined prompt appended to every chat. Simple. Obvious. Yet it solved a huge problem: repetitive context setting.&lt;/p&gt;
    &lt;p&gt;This was the spiritual ancestor to every &lt;code&gt;.cursorrules&lt;/code&gt; and &lt;code&gt;CLAUDE.md&lt;/code&gt; file that followed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Custom GPTs (Nov 2023)&lt;/head&gt;
    &lt;p&gt;OpenAI repackaged instructions and tools into Custom GPTs. This was an attempt to "productize" prompt engineering. You could bundle a persona, some files, and a few actions into a shareable link.&lt;/p&gt;
    &lt;p&gt;It was a retreat from the open-ended promise of plugins toward curated, single-purpose "apps."&lt;/p&gt;
    &lt;head rend="h2"&gt;Memory in ChatGPT (February 2024)&lt;/head&gt;
    &lt;p&gt;So far, we've discussed manual ways to extend LLMs. Memory represented a shift toward automatic personalization.&lt;/p&gt;
    &lt;p&gt;ChatGPT Memory records details from your conversations and quietly inserts them into future context. It's like a system prompt that writes itself. If you mention you're a vegetarian, it remembers that weeks later. It‚Äôs a small feature, but it marked the beginning of agents that maintain long-term state without user intervention.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cursor Rules (April 2024)&lt;/head&gt;
    &lt;p&gt;Cursor changed the game by putting custom instructions where they belonged: in the repo.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;.cursorrules&lt;/code&gt; file was a revelation. Instead of pasting context into a chat window, you committed it to git.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"We use tabs, not spaces."&lt;/item&gt;
      &lt;item&gt;"No semicolons."&lt;/item&gt;
      &lt;item&gt;"Always use TypeScript."&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It started as a single file, then evolved into a &lt;code&gt;.cursor/rules&lt;/code&gt; folder with sophisticated scoping. You could organize multiple rule files, and even define when they applied, for example, only for certain file types or subdirectories. It was the first time extension felt "native" to the code.&lt;/p&gt;
    &lt;p&gt;Later Cursor introduced the ability to let the LLM decide when to apply a rule, which is a pattern we will see again.&lt;/p&gt;
    &lt;head rend="h2"&gt;Model Context Protocol (Nov 2024)&lt;/head&gt;
    &lt;p&gt;By late 2024, models were finally smart enough to handle real tools reliably. Anthropic's Model Context Protocol (MCP) was the answer.&lt;/p&gt;
    &lt;p&gt;MCP is a heavyweight solution. An MCP client needs to keep a persistent connection to an MCP server. The server serves up tool definitions, resources, and prompts to the client (in most cases is an agent) and it can send a message to the server saying a tool has been called and the server can respond with the result.&lt;/p&gt;
    &lt;p&gt;Unlike Custom Instructions (which just add context), MCP gives the model actual capabilities. It can read your repo, query your Postgres DB, or deploy to Vercel. Besides just providing tools, it also allows servers to provide resources (documents, logs) and prompts directly to the agent.&lt;/p&gt;
    &lt;p&gt;It's powerful, and perhaps a bit of overkill. While the complexity might be worth it for agent developers asking a user to set up and connect an MCP is a lot of friction and there is an entire ecosystem of startups like Smithery built around making it easier to use MCP.&lt;/p&gt;
    &lt;p&gt;It is worth noting that ChatGPT apps which were announced in October 2025 are built on top of MCP as a base layer. This is an attempt to make it easier for end users to use MCP without having to actually think about it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Claude Code: New Agent, New Extensions (Feb 2025)&lt;/head&gt;
    &lt;p&gt;Early 2025 brought us Claude Code, which essentially added every extension mechanism under the sun to an agent.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt;: The standard for repo-level instructions.&lt;/item&gt;
      &lt;item&gt;MCP: For heavy-duty tool integration.&lt;/item&gt;
      &lt;item&gt;Slash Commands: Like Cursor's notebooks, for reusable prompts.&lt;/item&gt;
      &lt;item&gt;Hooks: The ability to intercept and modify the agent's loop (e.g., "Stop if the tests fail").&lt;/item&gt;
      &lt;item&gt;Sub-agents: Spawning specialized workers to handle sub-tasks.&lt;/item&gt;
      &lt;item&gt;Output Styles: (Deprecated) Configuring tone and format.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Time will tell how many of these features will stick around in the long term. Anthropic has already tried to deprecate output styles.&lt;/p&gt;
    &lt;head rend="h2"&gt;Agent Skills (Oct 2025)&lt;/head&gt;
    &lt;p&gt;The next extension mechanism added to Claude Code is significant enough to warrant a deeper dive. Agent Skills are the rebirth of ChatGPT Plugins.&lt;/p&gt;
    &lt;p&gt;While MCP has a whole client-server protocol, Agent Skills are just folders of markdown files and scripts (in whatever language you choose).&lt;/p&gt;
    &lt;p&gt;The agent simply scans a &lt;code&gt;skills/&lt;/code&gt; directory, reads the frontmatter of every &lt;code&gt;SKILL.md&lt;/code&gt;, and builds a lightweight index. It then chooses to read the full contents of a skill only if it's appropriate for the current task. This solves one of the major problems with MCP: the context bloat that comes from having to load all of the tool definitions into the context window at once.&lt;/p&gt;
    &lt;p&gt;Here is a snippet of the structure of a skill for doing e2e testing with Playwright taken from Anthropic's Skills examples repository:&lt;/p&gt;
    &lt;code&gt;webapp-testing/
‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îú‚îÄ‚îÄ console_logging.py
‚îÇ   ‚îú‚îÄ‚îÄ element_discovery.py
‚îÇ   ‚îî‚îÄ‚îÄ static_html_automation.py
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ with_server.py
‚îî‚îÄ‚îÄ SKILL.md
&lt;/code&gt;
    &lt;p&gt;There is a mix of scripts, examples, and plain text instructions. The only required file is the SKILL.md file. Let's take a look at that file:&lt;/p&gt;
    &lt;code&gt;---
name: webapp-testing
description: Toolkit for interacting with and testing local web applications using Playwright. Supports verifying frontend functionality, debugging UI behavior, capturing browser screenshots, and viewing browser logs.
license: Complete terms in LICENSE.txt
---

# Web Application Testing

To test local web applications, write native Python Playwright scripts.

**Helper Scripts Available**:

- `scripts/with_server.py` - Manages server lifecycle (supports multiple servers)

**Always run scripts with `--help` first** to see usage. DO NOT read the source until you try running the script first and find that a customized solution is absolutely necessary. These scripts can be very large and thus pollute your context window. They exist to be called directly as black-box scripts rather than ingested into your context window.

... skill continues ...
&lt;/code&gt;
    &lt;p&gt;This is just a plain markdown file with some metadata and a description of the skill. The agent reads the file which freely references other files that the agent can read. In contrast a playwright MCP server has dozens of tool definitions to control a browser, this skill just says "you have bash, this is how you write a playwright script".&lt;/p&gt;
    &lt;p&gt;Granted to use a skill the agent needs to have general purpose access to a computer, but this is the bitter lesson in action. Giving an agent general purpose tools and trusting it to have the ability to use them to accomplish a task might very well be the winning strategy over making specialized tools for every task.&lt;/p&gt;
    &lt;head rend="h2"&gt;What the future holds&lt;/head&gt;
    &lt;p&gt;Skills are the actualization of the dream that was set out by ChatGPT Plugins: just give the model instructions and some generic tools and trust it to do the glue work in-between. But I have a hypothesis that it might actually work now because the models are actually smart enough for it to work.&lt;/p&gt;
    &lt;p&gt;Agent skills work because it assumes the agent has the ability to write its own tools (via bash commands). You can just give it code snippets and ask the agent to figure out how to run them generically for the task at hand.&lt;/p&gt;
    &lt;p&gt;Importantly, I think that skills signal towards a new definition of what an agent really is. An agent isn't just a LLM in a while loop. It's an LLM in a while loop that has a computer strapped to it.&lt;/p&gt;
    &lt;p&gt;Claude Code is the piece of software that first made this click for me, but it is way too developer focused to be the final form. Other applications like Zo Computer try to package the llm and computer together into a single application, but I still think it still doesn't abstract the computer away enough from the end user. If I ask a coworker to do something, I don't need to see their entire file system, I just need to know that they have a computer.&lt;/p&gt;
    &lt;p&gt;Looking forward into 2026 I expect more and more llm applications that we use will have a computer strapped to them in new and interesting ways, whether we know it or not.&lt;/p&gt;
    &lt;p&gt;If I could short MCP, I would, and I expect us to go back to extending our agents with the most accessible programming language: natural language.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46037343</guid><pubDate>Mon, 24 Nov 2025 18:32:27 +0000</pubDate></item><item><title>Launch HN: Karumi (YC F25) ‚Äì Personalized, agentic product demos</title><link>http://karumi.ai/</link><description>&lt;doc fingerprint="87ee0ccaea20b026"&gt;
  &lt;main&gt;
    &lt;p&gt;Backed by Y Combinator&lt;/p&gt;
    &lt;head rend="h1"&gt;Don√¢t make your prospects wait√¢ever again&lt;/head&gt;
    &lt;head rend="h1"&gt;Don√¢t make your prospects wait√¢ever again&lt;/head&gt;
    &lt;head rend="h1"&gt;Don√¢t make your prospects wait√¢ever again&lt;/head&gt;
    &lt;p&gt;The first agentic product demo platform where prospects receive personalized demos, in a video call, instantly.&lt;/p&gt;
    &lt;p&gt;12&lt;/p&gt;
    &lt;p&gt;JAN&lt;/p&gt;
    &lt;p&gt;FEB&lt;/p&gt;
    &lt;p&gt;MAR&lt;/p&gt;
    &lt;p&gt;APR&lt;/p&gt;
    &lt;p&gt;MAY&lt;/p&gt;
    &lt;p&gt;JUN&lt;/p&gt;
    &lt;p&gt;JUL&lt;/p&gt;
    &lt;p&gt;AUG&lt;/p&gt;
    &lt;p&gt;SEPT&lt;/p&gt;
    &lt;p&gt;OCT&lt;/p&gt;
    &lt;p&gt;7000&lt;/p&gt;
    &lt;p&gt;6000&lt;/p&gt;
    &lt;p&gt;5000&lt;/p&gt;
    &lt;p&gt;4000&lt;/p&gt;
    &lt;p&gt;3000&lt;/p&gt;
    &lt;p&gt;2000&lt;/p&gt;
    &lt;p&gt;1000&lt;/p&gt;
    &lt;p&gt;AI Agent&lt;/p&gt;
    &lt;p&gt;Simon m.&lt;/p&gt;
    &lt;p&gt;CEO AT UNISON IQ&lt;/p&gt;
    &lt;p&gt;On-demand Product Demo&lt;/p&gt;
    &lt;p&gt;Multi-Language Support&lt;/p&gt;
    &lt;p&gt;Connect with your CRM&lt;/p&gt;
    &lt;p&gt;12&lt;/p&gt;
    &lt;p&gt;JAN&lt;/p&gt;
    &lt;p&gt;FEB&lt;/p&gt;
    &lt;p&gt;MAR&lt;/p&gt;
    &lt;p&gt;APR&lt;/p&gt;
    &lt;p&gt;MAY&lt;/p&gt;
    &lt;p&gt;JUN&lt;/p&gt;
    &lt;p&gt;JUL&lt;/p&gt;
    &lt;p&gt;AUG&lt;/p&gt;
    &lt;p&gt;SEPT&lt;/p&gt;
    &lt;p&gt;OCT&lt;/p&gt;
    &lt;p&gt;7000&lt;/p&gt;
    &lt;p&gt;6000&lt;/p&gt;
    &lt;p&gt;5000&lt;/p&gt;
    &lt;p&gt;4000&lt;/p&gt;
    &lt;p&gt;3000&lt;/p&gt;
    &lt;p&gt;2000&lt;/p&gt;
    &lt;p&gt;1000&lt;/p&gt;
    &lt;p&gt;AI Agent&lt;/p&gt;
    &lt;p&gt;Simon m.&lt;/p&gt;
    &lt;p&gt;CEO AT UNISON IQ&lt;/p&gt;
    &lt;p&gt;On-demand Product Demo&lt;/p&gt;
    &lt;p&gt;Multi-Language Support&lt;/p&gt;
    &lt;p&gt;Connect with your CRM&lt;/p&gt;
    &lt;p&gt;12&lt;/p&gt;
    &lt;p&gt;JAN&lt;/p&gt;
    &lt;p&gt;FEB&lt;/p&gt;
    &lt;p&gt;MAR&lt;/p&gt;
    &lt;p&gt;APR&lt;/p&gt;
    &lt;p&gt;MAY&lt;/p&gt;
    &lt;p&gt;JUN&lt;/p&gt;
    &lt;p&gt;JUL&lt;/p&gt;
    &lt;p&gt;AUG&lt;/p&gt;
    &lt;p&gt;SEPT&lt;/p&gt;
    &lt;p&gt;OCT&lt;/p&gt;
    &lt;p&gt;7000&lt;/p&gt;
    &lt;p&gt;6000&lt;/p&gt;
    &lt;p&gt;5000&lt;/p&gt;
    &lt;p&gt;4000&lt;/p&gt;
    &lt;p&gt;3000&lt;/p&gt;
    &lt;p&gt;2000&lt;/p&gt;
    &lt;p&gt;1000&lt;/p&gt;
    &lt;p&gt;AI Agent&lt;/p&gt;
    &lt;p&gt;Simon m.&lt;/p&gt;
    &lt;p&gt;CEO AT UNISON IQ&lt;/p&gt;
    &lt;p&gt;On-demand Product Demo&lt;/p&gt;
    &lt;p&gt;Multi-Language Support&lt;/p&gt;
    &lt;p&gt;Connect with your CRM&lt;/p&gt;
    &lt;p&gt;12&lt;/p&gt;
    &lt;p&gt;JAN&lt;/p&gt;
    &lt;p&gt;FEB&lt;/p&gt;
    &lt;p&gt;MAR&lt;/p&gt;
    &lt;p&gt;APR&lt;/p&gt;
    &lt;p&gt;MAY&lt;/p&gt;
    &lt;p&gt;JUN&lt;/p&gt;
    &lt;p&gt;JUL&lt;/p&gt;
    &lt;p&gt;AUG&lt;/p&gt;
    &lt;p&gt;SEPT&lt;/p&gt;
    &lt;p&gt;OCT&lt;/p&gt;
    &lt;p&gt;7000&lt;/p&gt;
    &lt;p&gt;6000&lt;/p&gt;
    &lt;p&gt;5000&lt;/p&gt;
    &lt;p&gt;4000&lt;/p&gt;
    &lt;p&gt;3000&lt;/p&gt;
    &lt;p&gt;2000&lt;/p&gt;
    &lt;p&gt;1000&lt;/p&gt;
    &lt;p&gt;AI Agent&lt;/p&gt;
    &lt;p&gt;Simon m.&lt;/p&gt;
    &lt;p&gt;CEO AT UNISON IQ&lt;/p&gt;
    &lt;p&gt;On-demand Product Demo&lt;/p&gt;
    &lt;p&gt;Multi-Language Support&lt;/p&gt;
    &lt;p&gt;Connect with your CRM&lt;/p&gt;
    &lt;p&gt;Trusted by&lt;/p&gt;
    &lt;p&gt;GDPR&lt;/p&gt;
    &lt;p&gt;GDPR&lt;/p&gt;
    &lt;p&gt;SOC 2 Compliant&lt;/p&gt;
    &lt;p&gt;SOC 2 Compliant&lt;/p&gt;
    &lt;p&gt;ISO 27001&lt;/p&gt;
    &lt;p&gt;ISO 27001&lt;/p&gt;
    &lt;head rend="h1"&gt;Close your leads when the intent is at its highest with 24/7 personalized demos.&lt;/head&gt;
    &lt;head rend="h1"&gt;Close your leads when the intent is at its highest with 24/7 hyper-personalized demos.&lt;/head&gt;
    &lt;head rend="h1"&gt;Close your leads when the intent is at its highest with 24/7 personalized demos.&lt;/head&gt;
    &lt;head rend="h1"&gt;Close your leads when the intent is at its highest with 24/7 personalized demos.&lt;/head&gt;
    &lt;p&gt;In brazil&lt;/p&gt;
    &lt;p&gt;In brazil&lt;/p&gt;
    &lt;p&gt;Interested Lead&lt;/p&gt;
    &lt;p&gt;Interested Lead&lt;/p&gt;
    &lt;p&gt;CFO at NexuSphere&lt;/p&gt;
    &lt;p&gt;CFO at NexuSphere&lt;/p&gt;
    &lt;p&gt;Requested Demo at 9:54PM&lt;/p&gt;
    &lt;p&gt;Requested Demo at 9:54PM&lt;/p&gt;
    &lt;p&gt;In brazil&lt;/p&gt;
    &lt;p&gt;Interested Lead&lt;/p&gt;
    &lt;p&gt;CFO at NexuSphere&lt;/p&gt;
    &lt;p&gt;Requested Demo at 9:54PM&lt;/p&gt;
    &lt;p&gt;In GERMANY&lt;/p&gt;
    &lt;p&gt;In GERMANY&lt;/p&gt;
    &lt;p&gt;Interested Lead&lt;/p&gt;
    &lt;p&gt;Interested Lead&lt;/p&gt;
    &lt;p&gt;project manager at deepmatter&lt;/p&gt;
    &lt;p&gt;project manager at deepmatter&lt;/p&gt;
    &lt;p&gt;Requested Demo at 06:09pm&lt;/p&gt;
    &lt;p&gt;Requested Demo at 06:09pm&lt;/p&gt;
    &lt;p&gt;In GERMANY&lt;/p&gt;
    &lt;p&gt;Interested Lead&lt;/p&gt;
    &lt;p&gt;project manager at deepmatter&lt;/p&gt;
    &lt;p&gt;Requested Demo at 06:09pm&lt;/p&gt;
    &lt;p&gt;Mid-market&lt;/p&gt;
    &lt;p&gt;Mid-market&lt;/p&gt;
    &lt;p&gt;Interested Lead&lt;/p&gt;
    &lt;p&gt;Interested Lead&lt;/p&gt;
    &lt;p&gt;ACCOUNTANT AT QUANTISCOPE&lt;/p&gt;
    &lt;p&gt;ACCOUNTANT AT QUANTISCOPE&lt;/p&gt;
    &lt;p&gt;Requested Demo at 03:48PM&lt;/p&gt;
    &lt;p&gt;Requested Demo at 03:48PM&lt;/p&gt;
    &lt;p&gt;Mid-market&lt;/p&gt;
    &lt;p&gt;Interested Lead&lt;/p&gt;
    &lt;p&gt;ACCOUNTANT AT QUANTISCOPE&lt;/p&gt;
    &lt;p&gt;Requested Demo at 03:48PM&lt;/p&gt;
    &lt;p&gt;Freelancer&lt;/p&gt;
    &lt;p&gt;Freelancer&lt;/p&gt;
    &lt;p&gt;Interested Lead&lt;/p&gt;
    &lt;p&gt;Interested Lead&lt;/p&gt;
    &lt;p&gt;DATA ANALYST AT UNISONIQ&lt;/p&gt;
    &lt;p&gt;DATA ANALYST AT UNISONIQ&lt;/p&gt;
    &lt;p&gt;Requested Demo at 03:02AM&lt;/p&gt;
    &lt;p&gt;Requested Demo at 03:02AM&lt;/p&gt;
    &lt;p&gt;Freelancer&lt;/p&gt;
    &lt;p&gt;Interested Lead&lt;/p&gt;
    &lt;p&gt;DATA ANALYST AT UNISONIQ&lt;/p&gt;
    &lt;p&gt;Requested Demo at 03:02AM&lt;/p&gt;
    &lt;p&gt;sMALL bUSINESS&lt;/p&gt;
    &lt;p&gt;sMALL bUSINESS&lt;/p&gt;
    &lt;p&gt;Interested Lead&lt;/p&gt;
    &lt;p&gt;Interested Lead&lt;/p&gt;
    &lt;p&gt;CTO AT VIRALYNX&lt;/p&gt;
    &lt;p&gt;CTO AT VIRALYNX&lt;/p&gt;
    &lt;p&gt;Requested Demo at 02:21AM&lt;/p&gt;
    &lt;p&gt;Requested Demo at 02:21AM&lt;/p&gt;
    &lt;p&gt;sMALL bUSINESS&lt;/p&gt;
    &lt;p&gt;Interested Lead&lt;/p&gt;
    &lt;p&gt;CTO AT VIRALYNX&lt;/p&gt;
    &lt;p&gt;Requested Demo at 02:21AM&lt;/p&gt;
    &lt;p&gt;Enterprise&lt;/p&gt;
    &lt;p&gt;Enterprise&lt;/p&gt;
    &lt;p&gt;Interested Lead&lt;/p&gt;
    &lt;p&gt;Interested Lead&lt;/p&gt;
    &lt;p&gt;PRODUCT MANAGER AT LUMENFLOW&lt;/p&gt;
    &lt;p&gt;PRODUCT MANAGER AT LUMENFLOW&lt;/p&gt;
    &lt;p&gt;Requested Demo at 01:57AM&lt;/p&gt;
    &lt;p&gt;Requested Demo at 01:57AM&lt;/p&gt;
    &lt;p&gt;Enterprise&lt;/p&gt;
    &lt;p&gt;Interested Lead&lt;/p&gt;
    &lt;p&gt;PRODUCT MANAGER AT LUMENFLOW&lt;/p&gt;
    &lt;p&gt;Requested Demo at 01:57AM&lt;/p&gt;
    &lt;p&gt;STARTUP&lt;/p&gt;
    &lt;p&gt;STARTUP&lt;/p&gt;
    &lt;p&gt;Interested Lead&lt;/p&gt;
    &lt;p&gt;Interested Lead&lt;/p&gt;
    &lt;p&gt;FINANCIAL ADVISOR AT DEEPMATTER&lt;/p&gt;
    &lt;p&gt;FINANCIAL ADVISOR AT DEEPMATTER&lt;/p&gt;
    &lt;p&gt;Requested Demo at 10:16PM&lt;/p&gt;
    &lt;p&gt;Requested Demo at 10:16PM&lt;/p&gt;
    &lt;p&gt;STARTUP&lt;/p&gt;
    &lt;p&gt;Interested Lead&lt;/p&gt;
    &lt;p&gt;FINANCIAL ADVISOR AT DEEPMATTER&lt;/p&gt;
    &lt;p&gt;Requested Demo at 10:16PM&lt;/p&gt;
    &lt;p&gt;neW customer IN ITALY&lt;/p&gt;
    &lt;p&gt;neW customer IN ITALY&lt;/p&gt;
    &lt;p&gt;CTO AT VIRALYNX&lt;/p&gt;
    &lt;p&gt;CTO AT VIRALYNX&lt;/p&gt;
    &lt;p&gt;STARTED Demo at 02:21AM&lt;/p&gt;
    &lt;p&gt;STARTED Demo at 02:21AM&lt;/p&gt;
    &lt;p&gt;New CUSTOMER IN AUSTRALIA&lt;/p&gt;
    &lt;p&gt;New CUSTOMER IN AUSTRALIA&lt;/p&gt;
    &lt;p&gt;DATA ANALYST AT UNISONIQ&lt;/p&gt;
    &lt;p&gt;DATA ANALYST AT UNISONIQ&lt;/p&gt;
    &lt;p&gt;STARTED Demo at 03:02AM&lt;/p&gt;
    &lt;p&gt;STARTED Demo at 03:02AM&lt;/p&gt;
    &lt;p&gt;new CUSTOMER IN GERMANY&lt;/p&gt;
    &lt;p&gt;new CUSTOMER IN GERMANY&lt;/p&gt;
    &lt;p&gt;Product Manager at DEEPMATTER&lt;/p&gt;
    &lt;p&gt;Product Manager at DEEPMATTER&lt;/p&gt;
    &lt;p&gt;STARTED Demo at 06:09pm&lt;/p&gt;
    &lt;p&gt;STARTED Demo at 06:09pm&lt;/p&gt;
    &lt;p&gt;new CuSTOMER IN USA&lt;/p&gt;
    &lt;p&gt;new CuSTOMER IN USA&lt;/p&gt;
    &lt;p&gt;Product Manager at LumenFlow&lt;/p&gt;
    &lt;p&gt;Product Manager at LumenFlow&lt;/p&gt;
    &lt;p&gt;STARTED Demo at 01:57AM&lt;/p&gt;
    &lt;p&gt;STARTED Demo at 01:57AM&lt;/p&gt;
    &lt;p&gt;LEAD IN PORTUGAL&lt;/p&gt;
    &lt;p&gt;LEAD IN PORTUGAL&lt;/p&gt;
    &lt;p&gt;FINANCIAL ADVISOR AT DEEPMATTER&lt;/p&gt;
    &lt;p&gt;FINANCIAL ADVISOR AT DEEPMATTER&lt;/p&gt;
    &lt;p&gt;STARTED Demo at 10:16PM&lt;/p&gt;
    &lt;p&gt;STARTED Demo at 10:16PM&lt;/p&gt;
    &lt;p&gt;LEAD IN BRAZIL&lt;/p&gt;
    &lt;p&gt;LEAD IN BRAZIL&lt;/p&gt;
    &lt;p&gt;CFO AT NEXUSPHERE&lt;/p&gt;
    &lt;p&gt;CFO AT NEXUSPHERE&lt;/p&gt;
    &lt;p&gt;STARTED Demo at 09:54PM&lt;/p&gt;
    &lt;p&gt;STARTED Demo at 09:54PM&lt;/p&gt;
    &lt;p&gt;LEAD IN JAPAN&lt;/p&gt;
    &lt;p&gt;LEAD IN JAPAN&lt;/p&gt;
    &lt;p&gt;accountant at quantiscope&lt;/p&gt;
    &lt;p&gt;accountant at quantiscope&lt;/p&gt;
    &lt;p&gt;STARTED Demo at 03:48pm&lt;/p&gt;
    &lt;p&gt;STARTED Demo at 03:48pm&lt;/p&gt;
    &lt;p&gt;What a Karumi Agent can do for you&lt;/p&gt;
    &lt;p&gt;Accelerate how you connect, demo, and convert.&lt;/p&gt;
    &lt;p&gt;Don't demo like it is 1990&lt;/p&gt;
    &lt;p&gt;Accelerate how you connect, demo, and convert.&lt;/p&gt;
    &lt;p&gt;Accelerate how you connect, demo, and convert.&lt;/p&gt;
    &lt;p&gt;Available 24/7&lt;/p&gt;
    &lt;p&gt;On-demand, 24 hours, 7 days a week&lt;/p&gt;
    &lt;p&gt;Easily jump into a call through your landing page, in-app, outbound emails.&lt;/p&gt;
    &lt;p&gt;Let users experience your product on-demand, through your landing page, in-app, or outbound emails.&lt;/p&gt;
    &lt;p&gt;Easily jump into a call through your landing page, in-app, outbound emails.&lt;/p&gt;
    &lt;p&gt;Easily jump into a call through your landing page, in-app, outbound emails.&lt;/p&gt;
    &lt;p&gt;12&lt;/p&gt;
    &lt;p&gt;Let me walk you through your sales dashboard&lt;/p&gt;
    &lt;p&gt;Here you can view daily, weekly and monthly sales&lt;/p&gt;
    &lt;p&gt;AI Agent&lt;/p&gt;
    &lt;p&gt;12&lt;/p&gt;
    &lt;p&gt;Let me walk you through your sales dashboard&lt;/p&gt;
    &lt;p&gt;Here you can view daily, weekly and monthly sales&lt;/p&gt;
    &lt;p&gt;AI Agent&lt;/p&gt;
    &lt;p&gt;12&lt;/p&gt;
    &lt;p&gt;Let me walk you through your sales dashboard&lt;/p&gt;
    &lt;p&gt;Here you can view daily, weekly and monthly sales&lt;/p&gt;
    &lt;p&gt;AI Agent&lt;/p&gt;
    &lt;p&gt;Sales Script&lt;/p&gt;
    &lt;p&gt;Messaging Doc&lt;/p&gt;
    &lt;p&gt;Messaging Doc&lt;/p&gt;
    &lt;p&gt;Sales Script&lt;/p&gt;
    &lt;p&gt;Messaging Doc&lt;/p&gt;
    &lt;p&gt;Sales Script&lt;/p&gt;
    &lt;p&gt;Messaging Doc&lt;/p&gt;
    &lt;p&gt;Messaging Doc&lt;/p&gt;
    &lt;p&gt;Sales Script&lt;/p&gt;
    &lt;p&gt;Messaging Doc&lt;/p&gt;
    &lt;p&gt;Sales Script&lt;/p&gt;
    &lt;p&gt;Messaging Doc&lt;/p&gt;
    &lt;p&gt;Messaging Doc&lt;/p&gt;
    &lt;p&gt;Sales Script&lt;/p&gt;
    &lt;p&gt;Messaging Doc&lt;/p&gt;
    &lt;p&gt;Hyper-personalization at Scale&lt;/p&gt;
    &lt;p&gt;Product demos that adapt to each customer√¢s needs and context√¢always up to date with your product&lt;/p&gt;
    &lt;p&gt;Product demos that adapt to each customer√¢s needs and context√¢always up to date with your product&lt;/p&gt;
    &lt;p&gt;Product demos that adapt to each customer√¢s needs and context√¢always up to date with your product&lt;/p&gt;
    &lt;p&gt;MULTILINGUAL AGENTS&lt;/p&gt;
    &lt;p&gt;Instantly expand your reach with multilingual conversations in a click.&lt;/p&gt;
    &lt;p&gt;Instantly expand your reach with multilingual conversations in a click.&lt;/p&gt;
    &lt;p&gt;Instantly expand your reach with multilingual conversations in a click.&lt;/p&gt;
    &lt;p&gt;Always in-sync with your CRM.&lt;/p&gt;
    &lt;p&gt;Transcripts, analysis, and next steps are logged directly into your CRM, ready for your team to follow up.&lt;/p&gt;
    &lt;p&gt;AI Agent&lt;/p&gt;
    &lt;p&gt;Italian&lt;/p&gt;
    &lt;p&gt;Duration: 19 mins&lt;/p&gt;
    &lt;p&gt;Time: 04:23AM&lt;/p&gt;
    &lt;p&gt;AI Agent&lt;/p&gt;
    &lt;p&gt;Spanish&lt;/p&gt;
    &lt;p&gt;Duration: 15 mins&lt;/p&gt;
    &lt;p&gt;Time: 10:45AM&lt;/p&gt;
    &lt;p&gt;Status:&lt;/p&gt;
    &lt;p&gt;ACTIVE&lt;/p&gt;
    &lt;p&gt;AI Agent&lt;/p&gt;
    &lt;p&gt;GERMAN&lt;/p&gt;
    &lt;p&gt;Duration: 28 mins&lt;/p&gt;
    &lt;p&gt;Time: 06:03AM&lt;/p&gt;
    &lt;p&gt;AI Agent&lt;/p&gt;
    &lt;p&gt;English&lt;/p&gt;
    &lt;p&gt;Duration: 34 mins&lt;/p&gt;
    &lt;p&gt;Time: 13:33AM&lt;/p&gt;
    &lt;p&gt;AI Agent&lt;/p&gt;
    &lt;p&gt;French&lt;/p&gt;
    &lt;p&gt;Duration: 45 mins&lt;/p&gt;
    &lt;p&gt;Time: 02:01&lt;/p&gt;
    &lt;p&gt;Always in-sync with your CRM.&lt;/p&gt;
    &lt;p&gt;Transcripts, analysis, and next steps are logged directly into your CRM, ready for your team to follow up.&lt;/p&gt;
    &lt;p&gt;AI Agent&lt;/p&gt;
    &lt;p&gt;Italian&lt;/p&gt;
    &lt;p&gt;Duration: 19 mins&lt;/p&gt;
    &lt;p&gt;Time: 04:23AM&lt;/p&gt;
    &lt;p&gt;AI Agent&lt;/p&gt;
    &lt;p&gt;Spanish&lt;/p&gt;
    &lt;p&gt;Duration: 15 mins&lt;/p&gt;
    &lt;p&gt;Time: 10:45AM&lt;/p&gt;
    &lt;p&gt;Status:&lt;/p&gt;
    &lt;p&gt;ACTIVE&lt;/p&gt;
    &lt;p&gt;AI Agent&lt;/p&gt;
    &lt;p&gt;GERMAN&lt;/p&gt;
    &lt;p&gt;Duration: 28 mins&lt;/p&gt;
    &lt;p&gt;Time: 06:03AM&lt;/p&gt;
    &lt;p&gt;AI Agent&lt;/p&gt;
    &lt;p&gt;English&lt;/p&gt;
    &lt;p&gt;Duration: 34 mins&lt;/p&gt;
    &lt;p&gt;Time: 13:33AM&lt;/p&gt;
    &lt;p&gt;AI Agent&lt;/p&gt;
    &lt;p&gt;French&lt;/p&gt;
    &lt;p&gt;Duration: 45 mins&lt;/p&gt;
    &lt;p&gt;Time: 02:01&lt;/p&gt;
    &lt;p&gt;Always in-sync with your CRM.&lt;/p&gt;
    &lt;p&gt;Transcripts, analysis, and next steps are logged directly into your CRM, ready for your team to follow up.&lt;/p&gt;
    &lt;p&gt;AI Agent&lt;/p&gt;
    &lt;p&gt;Italian&lt;/p&gt;
    &lt;p&gt;Duration: 19 mins&lt;/p&gt;
    &lt;p&gt;Time: 04:23AM&lt;/p&gt;
    &lt;p&gt;AI Agent&lt;/p&gt;
    &lt;p&gt;Spanish&lt;/p&gt;
    &lt;p&gt;Duration: 15 mins&lt;/p&gt;
    &lt;p&gt;Time: 10:45AM&lt;/p&gt;
    &lt;p&gt;Status:&lt;/p&gt;
    &lt;p&gt;ACTIVE&lt;/p&gt;
    &lt;p&gt;AI Agent&lt;/p&gt;
    &lt;p&gt;GERMAN&lt;/p&gt;
    &lt;p&gt;Duration: 28 mins&lt;/p&gt;
    &lt;p&gt;Time: 06:03AM&lt;/p&gt;
    &lt;p&gt;AI Agent&lt;/p&gt;
    &lt;p&gt;English&lt;/p&gt;
    &lt;p&gt;Duration: 34 mins&lt;/p&gt;
    &lt;p&gt;Time: 13:33AM&lt;/p&gt;
    &lt;p&gt;AI Agent&lt;/p&gt;
    &lt;p&gt;French&lt;/p&gt;
    &lt;p&gt;Duration: 45 mins&lt;/p&gt;
    &lt;p&gt;Time: 02:01&lt;/p&gt;
    &lt;p&gt;TESTIMONIALS&lt;/p&gt;
    &lt;p&gt;What others say about Karumi&lt;/p&gt;
    &lt;p&gt;What others say about Karumi&lt;/p&gt;
    &lt;p&gt;What others say about Karumi&lt;/p&gt;
    &lt;p&gt;"Demos are just the start. Agentic experiences redefine every step of GTM. Scalable personalization is here!"&lt;/p&gt;
    &lt;p&gt;Alex Lindahl&lt;/p&gt;
    &lt;p&gt;GTM Engineer @ Clay&lt;/p&gt;
    &lt;p&gt;"Faster 'Aha!' moments at scale with personalized demos, without headcount increase."&lt;/p&gt;
    &lt;p&gt;Bernard Aceituno&lt;/p&gt;
    &lt;p&gt;CEO @ StackAI&lt;/p&gt;
    &lt;p&gt;"We capture more leads and strengthen our funnel by scaling up demos right when prospects need them."&lt;/p&gt;
    &lt;p&gt;Max Minsker&lt;/p&gt;
    &lt;p&gt;CEO @ Cranston&lt;/p&gt;
    &lt;p&gt;FROM SETUP TO SCALE&lt;/p&gt;
    &lt;p&gt;Implementation made simple&lt;/p&gt;
    &lt;p&gt;Implementation made simple&lt;/p&gt;
    &lt;p&gt;Implementation made simple&lt;/p&gt;
    &lt;p&gt;Sales&lt;/p&gt;
    &lt;p&gt;First line of engagement: qualifying, running tailored demos, and capturing new opportunities instantly. Missed inbound traffic turned into booked demos and conversations that convert.&lt;/p&gt;
    &lt;p&gt;First line of engagement: qualifying, running tailored demos, and capturing new opportunities instantly. Missed inbound traffic turned into booked demos and conversations that convert.&lt;/p&gt;
    &lt;p&gt;Onboarding&lt;/p&gt;
    &lt;p&gt;Personalized onboarding sessions, 24/7. Faster activation without adding headcount.&lt;/p&gt;
    &lt;p&gt;Personalized onboarding sessions, 24/7. Faster activation without adding headcount.&lt;/p&gt;
    &lt;p&gt;Jeremy S.&lt;/p&gt;
    &lt;p&gt;CFO at NexuSphere&lt;/p&gt;
    &lt;p&gt;Jeremy S.&lt;/p&gt;
    &lt;p&gt;CFO at NexuSphere&lt;/p&gt;
    &lt;p&gt;Walkthrough&lt;/p&gt;
    &lt;p&gt;Walkthrough&lt;/p&gt;
    &lt;p&gt;Intgration questions&lt;/p&gt;
    &lt;p&gt;Intgration questions&lt;/p&gt;
    &lt;p&gt;Troubleshooting&lt;/p&gt;
    &lt;p&gt;Troubleshooting&lt;/p&gt;
    &lt;p&gt;Members invitation&lt;/p&gt;
    &lt;p&gt;Members invitation&lt;/p&gt;
    &lt;p&gt;Support&lt;/p&gt;
    &lt;p&gt;Assist users by: answering questions, surfacing product features, and keeping engagement high.&lt;/p&gt;
    &lt;p&gt;No problem!&lt;/p&gt;
    &lt;p&gt;Yes, then please show me how to set-up payments.&lt;/p&gt;
    &lt;p&gt;Would you like a demo of the dashboard?&lt;/p&gt;
    &lt;p&gt;Hi, I'm Paul. I'm here to guide you through our product&lt;/p&gt;
    &lt;p&gt;Support&lt;/p&gt;
    &lt;p&gt;Assist users by: answering questions, surfacing product features, and keeping engagement high.&lt;/p&gt;
    &lt;p&gt;No problem!&lt;/p&gt;
    &lt;p&gt;Yes, then please show me how to set-up payments.&lt;/p&gt;
    &lt;p&gt;Would you like a demo of the dashboard?&lt;/p&gt;
    &lt;p&gt;Hi, I'm Paul. I'm here to guide you through our product&lt;/p&gt;
    &lt;p&gt;Support&lt;/p&gt;
    &lt;p&gt;Assist users by: answering questions, surfacing product features, and keeping engagement high.&lt;/p&gt;
    &lt;p&gt;No problem!&lt;/p&gt;
    &lt;p&gt;Yes, then please show me how to set-up payments.&lt;/p&gt;
    &lt;p&gt;Would you like a demo of the dashboard?&lt;/p&gt;
    &lt;p&gt;Hi, I'm Paul. I'm here to guide you through our product&lt;/p&gt;
    &lt;head rend="h1"&gt;Accelerate your growth with&lt;/head&gt;
    &lt;head rend="h1"&gt;AI-led demos.&lt;/head&gt;
    &lt;head rend="h1"&gt;Accelerate your growth with&lt;/head&gt;
    &lt;head rend="h1"&gt;AI-led demos.&lt;/head&gt;
    &lt;p&gt;Don√¢t make your prospects wait√¢ever again. Scale your demos and increase your revenue.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46037416</guid><pubDate>Mon, 24 Nov 2025 18:37:27 +0000</pubDate></item><item><title>Is your Android TV streaming box part of a botnet?</title><link>https://krebsonsecurity.com/2025/11/is-your-android-tv-streaming-box-part-of-a-botnet/</link><description>&lt;doc fingerprint="a5b98911c8aa3149"&gt;
  &lt;main&gt;
    &lt;p&gt;On the surface, the Superbox media streaming devices for sale at retailers like BestBuy and Walmart may seem like a steal: They offer unlimited access to more than 2,200 pay-per-view and streaming services like Netflix, ESPN and Hulu, all for a one-time fee of around $400. But security experts warn these TV boxes require intrusive software that forces the user‚Äôs network to relay Internet traffic for others, traffic that is often tied to cybercrime activity such as advertising fraud and account takeovers.&lt;/p&gt;
    &lt;p&gt;Superbox bills itself as an affordable way for households to stream all of the television and movie content they could possibly want, without the hassle of monthly subscription fees ‚Äî for a one-time payment of nearly $400.&lt;/p&gt;
    &lt;p&gt;‚ÄúTired of confusing cable bills and hidden fees?,‚Äù Superbox‚Äôs website asks in a recent blog post titled, ‚ÄúCheap Cable TV for Low Income: Watch TV, No Monthly Bills.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúReal cheap cable TV for low income solutions does exist,‚Äù the blog continues. ‚ÄúThis guide breaks down the best alternatives to stop overpaying, from free over-the-air options to one-time purchase devices that eliminate monthly bills.‚Äù&lt;/p&gt;
    &lt;p&gt;Superbox claims that watching a stream of movies, TV shows, and sporting events won‚Äôt violate U.S. copyright law.&lt;/p&gt;
    &lt;p&gt;‚ÄúSuperBox is just like any other Android TV box on the market, we can not control what software customers will use,‚Äù the company‚Äôs website maintains. ‚ÄúAnd you won‚Äôt encounter a law issue unless uploading, downloading, or broadcasting content to a large group.‚Äù&lt;/p&gt;
    &lt;p&gt;There is nothing illegal about the sale or use of the Superbox itself, which can be used strictly as a way to stream content at providers where users already have a paid subscription. But that is not why people are shelling out $400 for these machines. The only way to watch those 2,200+ channels for free with a Superbox is to install several apps made for the device that enable them to stream this content.&lt;/p&gt;
    &lt;p&gt;Superbox‚Äôs homepage includes a prominent message stating the company does ‚Äúnot sell access to or preinstall any apps that bypass paywalls or provide access to unauthorized content.‚Äù The company explains that they merely provide the hardware, while customers choose which apps to install.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe only sell the hardware device,‚Äù the notice states. ‚ÄúCustomers must use official apps and licensed services; unauthorized use may violate copyright law.‚Äù&lt;/p&gt;
    &lt;p&gt;Superbox is technically correct here, except for maybe the part about how customers must use official apps and licensed services: Before the Superbox can stream those thousands of channels, users must configure the device to update itself, and the first step involves ripping out Google‚Äôs official Play store and replacing it with something called the ‚ÄúApp Store‚Äù or ‚ÄúBlue TV Store.‚Äù&lt;/p&gt;
    &lt;p&gt;Superbox does this because the device does not use the official Google-certified Android TV system, and its apps will not load otherwise. Only after the Google Play store has been supplanted by this unofficial App Store do the various movie and video streaming apps that are built specifically for the Superbox appear available for download (again, outside of Google‚Äôs app ecosystem).&lt;/p&gt;
    &lt;p&gt;Experts say while these Android streaming boxes generally do what they advertise ‚Äî enabling buyers to stream video content that would normally require a paid subscription ‚Äî the apps that enable the streaming also ensnare the user‚Äôs Internet connection in a distributed residential proxy network that uses the devices to relay traffic from others.&lt;/p&gt;
    &lt;p&gt;Ashley is a senior solutions engineer at Censys, a cyber intelligence company that indexes Internet-connected devices, services and hosts. Ashley requested that only her first name be used in this story.&lt;/p&gt;
    &lt;p&gt;In a recent video interview, Ashley showed off several Superbox models that Censys was studying in the malware lab ‚Äî including one purchased off the shelf at BestBuy.&lt;/p&gt;
    &lt;p&gt;‚ÄúI‚Äôm sure a lot of people are thinking, ‚ÄòHey, how bad could it be if it‚Äôs for sale at the big box stores?'‚Äù she said. ‚ÄúBut the more I looked, things got weirder and weirder.‚Äù&lt;/p&gt;
    &lt;p&gt;Ashley said she found the Superbox devices immediately contacted a server at the Chinese instant messaging service Tencent QQ, as well as a residential proxy service called Grass IO.&lt;/p&gt;
    &lt;head rend="h2"&gt;GET GRASSED&lt;/head&gt;
    &lt;p&gt;Also known as getgrass[.]io, Grass says it is ‚Äúa decentralized network that allows users to earn rewards by sharing their unused Internet bandwidth with AI labs and other companies.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúBuyers seek unused internet bandwidth to access a more diverse range of IP addresses, which enables them to see certain websites from a retail perspective,‚Äù the Grass website explains. ‚ÄúBy utilizing your unused internet bandwidth, they can conduct market research, or perform tasks like web scraping to train AI.‚Äù&lt;/p&gt;
    &lt;p&gt;Reached via Twitter/X, Grass founder Andrej Radonjic told KrebsOnSecurity he‚Äôd never heard of a Superbox, and that Grass has no affiliation with the device maker.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt looks like these boxes are distributing an unethical proxy network which people are using to try to take advantage of Grass,‚Äù Radonjic said. ‚ÄúThe point of grass is to be an opt-in network. You download the grass app to monetize your unused bandwidth. There are tons of sketchy SDKs out there that hijack people‚Äôs bandwidth to help webscraping companies.‚Äù&lt;/p&gt;
    &lt;p&gt;Radonjic said Grass has implemented ‚Äúa robust system to identify network abusers,‚Äù and that if it discovers anyone trying to misuse or circumvent its terms of service, the company takes steps to stop it and prevent those users from earning points or rewards.&lt;/p&gt;
    &lt;p&gt;Superbox‚Äôs parent company, Super Media Technology Company Ltd., lists its street address as a UPS store in Fountain Valley, Calif. The company did not respond to multiple inquiries.&lt;/p&gt;
    &lt;p&gt;According to this teardown by behindmlm.com, a blog that covers multi-level marketing (MLM) schemes, Grass‚Äôs compensation plan is built around ‚Äúgrass points,‚Äù which are earned through the use of the Grass app and through app usage by recruited affiliates. Affiliates can earn 5,000 grass points for clocking 100 hours usage of Grass‚Äôs app, but they must progress through ten affiliate tiers or ranks before they can redeem their grass points (presumably for some type of cryptocurrency). The 10th or ‚ÄúTitan‚Äù tier requires affiliates to accumulate a whopping 50 million grass points, or recruit at least 221 more affiliates.&lt;/p&gt;
    &lt;p&gt;Radonjic said Grass‚Äôs system has changed in recent months, and confirmed the company has a referral program where users can earn Grass Uptime Points by contributing their own bandwidth and/or by inviting other users to participate.&lt;/p&gt;
    &lt;p&gt;‚ÄúUsers are not required to participate in the referral program to earn Grass Uptime Points or to receive Grass Tokens,‚Äù Radonjic said. ‚ÄúGrass is in the process of phasing out the referral program and has introduced an updated Grass Points model.‚Äù&lt;/p&gt;
    &lt;p&gt;A review of the Terms and Conditions page for getgrass[.]io at the Wayback Machine shows Grass‚Äôs parent company has changed names at least five times in the course of its two-year existence. Searching the Wayback Machine on getgrass[.]io shows that in June 2023 Grass was owned by a company called Wynd Network. By March 2024, the owner was listed as Lower Tribeca Corp. in the Bahamas. By August 2024, Grass was controlled by a Half Space Labs Limited, and in November 2024 the company was owned by Grass OpCo (BVI) Ltd. Currently, the Grass website says its parent is just Grass OpCo Ltd (no BVI in the name).&lt;/p&gt;
    &lt;p&gt;Radonjic acknowledged that Grass has undergone ‚Äúa handful of corporate clean-ups over the last couple of years,‚Äù but described them as administrative changes that had no operational impact. ‚ÄúThese reflect normal early-stage restructuring as the project moved from initial development‚Ä¶into the current structure under the Grass Foundation,‚Äù he said.&lt;/p&gt;
    &lt;head rend="h2"&gt;UNBOXING&lt;/head&gt;
    &lt;p&gt;Censys‚Äôs Ashley said the phone home to China‚Äôs Tencent QQ instant messaging service was the first red flag with the Superbox devices she examined. She also discovered the streaming boxes included powerful network analysis and remote access tools, such as Tcpdump and Netcat.&lt;/p&gt;
    &lt;p&gt;‚ÄúThis thing DNS hijacked my router, did ARP poisoning to the point where things fall off the network so they can assume that IP, and attempted to bypass controls,‚Äù she said. ‚ÄúI have root on all of them now, and they actually have a folder called ‚Äòsecondstage.‚Äô These devices also have Netcat and Tcpdump on them, and yet they are supposed to be streaming devices.‚Äù&lt;/p&gt;
    &lt;p&gt;A quick online search shows various Superbox models and many similar Android streaming devices for sale at a wide range of top retail destinations, including Amazon, BestBuy, Newegg, and Walmart. Newegg.com, for example, currently lists more than three dozen Superbox models. In all cases, the products are sold by third-party merchants on these platforms, but in many instances the fulfillment comes from the e-commerce platform itself.&lt;/p&gt;
    &lt;p&gt;‚ÄúNewegg is pretty bad now with these devices,‚Äù Ashley said. ‚ÄúEbay is the funniest, because they have Superbox in Spanish ‚Äî the SuperCaja ‚Äî which is very popular.‚Äù&lt;/p&gt;
    &lt;p&gt;Ashley said Amazon recently cracked down on Android streaming devices branded as Superbox, but that those listings can still be found under the more generic title ‚Äúmodem and router combo‚Äù (which may be slightly closer to the truth about the device‚Äôs behavior).&lt;/p&gt;
    &lt;p&gt;Superbox doesn‚Äôt advertise its products in the conventional sense. Rather, it seems to rely on lesser-known influencers on places like Youtube and TikTok to promote the devices. Meanwhile, Ashley said, Superbox pays those influencers 50 percent of the value of each device they sell.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt‚Äôs weird to me because influencer marketing usually caps compensation at 15 percent, and it means they don‚Äôt care about the money,‚Äù she said. ‚ÄúThis is about building their network.‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;BADBOX&lt;/head&gt;
    &lt;p&gt;As plentiful as the Superbox is on e-commerce sites, it is just one brand in an ocean of no-name Android-based TV boxes available to consumers. While these devices generally do provide buyers with ‚Äúfree‚Äù streaming content, they also tend to include factory-installed malware or require the installation of third-party apps that engage the user‚Äôs Internet address in advertising fraud.&lt;/p&gt;
    &lt;p&gt;In July 2025, Google filed a ‚ÄúJohn Doe‚Äù lawsuit (PDF) against 25 unidentified defendants dubbed the ‚ÄúBadBox 2.0 Enterprise,‚Äù which Google described as a botnet of over ten million Android streaming devices that engaged in advertising fraud. Google said the BADBOX 2.0 botnet, in addition to compromising multiple types of devices prior to purchase, can also infect devices by requiring the download of malicious apps from unofficial marketplaces.&lt;/p&gt;
    &lt;p&gt;Several of the Android streaming devices flagged in Google‚Äôs lawsuit are still for sale on top U.S. retail sites. For example, searching for the ‚ÄúX88Pro 10‚Äù and the ‚ÄúT95‚Äù Android streaming boxes finds both continue to be peddled by Amazon sellers.&lt;/p&gt;
    &lt;p&gt;Google‚Äôs lawsuit came on the heels of a June 2025 advisory from the Federal Bureau of Investigation (FBI), which warned that cyber criminals were gaining unauthorized access to home networks by either configuring the products with malicious software prior to the user‚Äôs purchase, or infecting the device as it downloads required applications that contain backdoors, usually during the set-up process.&lt;/p&gt;
    &lt;p&gt;‚ÄúOnce these compromised IoT devices are connected to home networks, the infected devices are susceptible to becoming part of the BADBOX 2.0 botnet and residential proxy services known to be used for malicious activity,‚Äù the FBI said.&lt;/p&gt;
    &lt;p&gt;The FBI said BADBOX 2.0 was discovered after the original BADBOX campaign was disrupted in 2024. The original BADBOX was identified in 2023, and primarily consisted of Android operating system devices that were compromised with backdoor malware prior to purchase.&lt;/p&gt;
    &lt;p&gt;Riley Kilmer is founder of Spur, a company that tracks residential proxy networks. Kilmer said Badbox 2.0 was used as a distribution platform for IPidea, a China-based entity that is now the world‚Äôs largest residential proxy network.&lt;/p&gt;
    &lt;p&gt;Kilmer and others say IPidea is merely a rebrand of 911S5 Proxy, a China-based proxy provider sanctioned last year by the U.S. Department of the Treasury for operating a botnet that helped criminals steal billions of dollars from financial institutions, credit card issuers, and federal lending programs (the U.S. Department of Justice also arrested the alleged owner of 911S5).&lt;/p&gt;
    &lt;p&gt;How are most IPidea customers using the proxy service? According to the proxy detection service Synthient, six of the top ten destinations for IPidea proxies involved traffic that has been linked to either ad fraud or credential stuffing (account takeover attempts).&lt;/p&gt;
    &lt;p&gt;Kilmer said companies like Grass are probably being truthful when they say that some of their customers are companies performing web scraping to train artificial intelligence efforts, because a great deal of content scraping which ultimately benefits AI companies is now leveraging these proxy networks to further obfuscate their aggressive data-slurping activity. By routing this unwelcome traffic through residential IP addresses, Kilmer said, content scraping firms can make it far trickier to filter out.&lt;/p&gt;
    &lt;p&gt;‚ÄúWeb crawling and scraping has always been a thing, but AI made it like a commodity, data that had to be collected,‚Äù Kilmer told KrebsOnSecurity. ‚ÄúEverybody wanted to monetize their own data pots, and how they monetize that is different across the board.‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;SOME FRIENDLY ADVICE&lt;/head&gt;
    &lt;p&gt;Products like Superbox are drawing increased interest from consumers as more popular network television shows and sportscasts migrate to subscription streaming services, and as people begin to realize they‚Äôre spending as much or more on streaming services than they previously paid for cable or satellite TV.&lt;/p&gt;
    &lt;p&gt;These streaming devices from no-name technology vendors are another example of the maxim, ‚ÄúIf something is free, you are the product,‚Äù meaning the company is making money by selling access to and/or information about its users and their data.&lt;/p&gt;
    &lt;p&gt;Superbox owners might counter, ‚ÄúFree? I paid $400 for that device!‚Äù But remember: Just because you paid a lot for something doesn‚Äôt mean you are done paying for it, or that somehow you are the only one who might be worse off from the transaction.&lt;/p&gt;
    &lt;p&gt;It may be that many Superbox customers don‚Äôt care if someone uses their Internet connection to tunnel traffic for ad fraud and account takeovers; for them, it beats paying for multiple streaming services each month. My guess, however, is that quite a few people who buy (or are gifted) these products have little understanding of the bargain they‚Äôre making when they plug them into an Internet router.&lt;/p&gt;
    &lt;p&gt;Superbox performs some serious linguistic gymnastics to claim its products don‚Äôt violate copyright laws, and that its customers alone are responsible for understanding and observing any local laws on the matter. However, buyer beware: If you‚Äôre a resident of the United States, you should know that using these devices for unauthorized streaming violates the Digital Millennium Copyright Act (DMCA), and can incur legal action, fines, and potential warnings and/or suspension of service by your Internet service provider.&lt;/p&gt;
    &lt;p&gt;According to the FBI, there are several signs to look for that may indicate a streaming device you own is malicious, including:&lt;/p&gt;
    &lt;p&gt;-The presence of suspicious marketplaces where apps are downloaded.&lt;lb/&gt; -Requiring Google Play Protect settings to be disabled.&lt;lb/&gt; -Generic TV streaming devices advertised as unlocked or capable of accessing free content.&lt;lb/&gt; -IoT devices advertised from unrecognizable brands.&lt;lb/&gt; -Android devices that are not Play Protect certified.&lt;lb/&gt; -Unexplained or suspicious Internet traffic.&lt;/p&gt;
    &lt;p&gt;This explainer from the Electronic Frontier Foundation delves a bit deeper into each of the potential symptoms listed above.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46037556</guid><pubDate>Mon, 24 Nov 2025 18:47:11 +0000</pubDate></item><item><title>GrapheneOS migrates server infrastructure from France</title><link>https://www.privacyguides.org/news/2025/11/22/grapheneos-migrates-server-infrastructure-from-france-amid-police-intimidation-claims/</link><description>&lt;doc fingerprint="1510995082bdd47c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;GrapheneOS migrates server infrastructure from France amid police intimidation claims&lt;/head&gt;
    &lt;p&gt;The GrapheneOS project has announced on X that they are ceasing all operations in France, asserting that the country is no longer safe for "open source privacy projects".&lt;/p&gt;
    &lt;p&gt;While the operating system will still be available to French users, all website and discussion servers are being relocated abroad.&lt;/p&gt;
    &lt;p&gt;Until now, the project relied on OVH Bearharnois, a French hosting provider, for its core website and social media services. The migration plan moves the Mastodon, Discourse, and Matrix instances to a combination of local and shared servers in Toronto. Critical website infrastructure will be hosted by Netcup, a German‚Äëbased company.&lt;/p&gt;
    &lt;p&gt;Thank you for reading this article. If you want to support our news briefs, guides, and videos please consider becoming a Privacy Guides member.&lt;/p&gt;
    &lt;p&gt;Privacy Guides is 100% reader-funded. You can subscribe for free, or donate and receive early-access and exclusive content from the team.&lt;/p&gt;
    &lt;p&gt;GrapheneOS claims that they does not collect confidential user data in their servers or store critical infrastructure in France. Therefore, the migration does not affect services such as signature verification and downgrade protection for updates.&lt;/p&gt;
    &lt;p&gt;Citing the government's support of the European Union Chat Control proposal, GrapheneOS developers are also refusing travel to France. Developers are no longer allowed to work inside the country due to safety concerns.&lt;/p&gt;
    &lt;p&gt;This decision was sparked by negative press coverage from two articles published by Le Parisien. An interview with French cybercrime prosecutor Johanna Brousse implies potential legal action against the project:&lt;/p&gt;
    &lt;quote&gt;"With this new tool, there is real legitimacy for a certain portion of users in the desire to protect their exchanges. The approach is therefore different. But that won't stop us from suing the publishers if links are discovered with a criminal organization and they don't cooperate with the law"&lt;/quote&gt;
    &lt;p&gt;GrapheneOS argues that Le Parisien have conflated their project with government-sponsored forks, which are fake copies of their operating system. The news outlet refers to a fake Snapchat app, dark web advertising, and a series of unlisted YouTube videos that are not features of GrapheneOS itself.&lt;/p&gt;
    &lt;p&gt;The project had previously threatened litigation against these government-sponsored forks. One prominent example is ANOM, an FBI-backed shell company that developed a compromised Android operating system and messaging platform as part of Operation Trojan Horse from 2018 and 2021.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46037573</guid><pubDate>Mon, 24 Nov 2025 18:48:04 +0000</pubDate></item><item><title>Pebble Watch software is now 100% open source</title><link>https://ericmigi.com/blog/pebble-watch-software-is-now-100percent-open-source</link><description>&lt;doc fingerprint="dc4f4b1117d7e1f1"&gt;
  &lt;main&gt;
    &lt;p&gt;Pebble Watch Software Is Now 100% Open Source + Tick Talk #4 - PT2 Demos!&lt;/p&gt;
    &lt;p&gt;[2025-11-24]&lt;/p&gt;
    &lt;p&gt;Another big Pebble update today! TLDR:&lt;/p&gt;
    &lt;p&gt;Yesterday, Pebble watch software was ~95% open source. Today, it‚Äôs 100% open source. You can download, compile and run all the software you need to use your Pebble. We just published the source code for the new Pebble mobile app!&lt;/p&gt;
    &lt;p&gt;Pebble Appstore now has a publicly available backup and supports multiple feeds, providing long term reliability through decentralization. We‚Äôve launched our own feed and Developer Dashboard.&lt;/p&gt;
    &lt;p&gt;Pebble Time 2 schedule update (aiming to begin shipping in January, with most arriving on wrists in March/April)&lt;/p&gt;
    &lt;p&gt;Over the last year, and especially in the last week, I've chatted with tons of people in the Pebble community. One of the main questions people have is ‚Äòhow do I know that my new Pebble watch will continue to work long into the future?‚Äô. It‚Äôs an extremely valid question and concern - one that I share as a fellow Pebble wearer. I called this out specifically in my blog post announcing the relaunch in January 2025. How is this time round going to be different from last time?&lt;/p&gt;
    &lt;p&gt;There are two pieces to making Pebble sustainable long term - hardware and software.&lt;/p&gt;
    &lt;p&gt;Hardware&lt;/p&gt;
    &lt;p&gt;Nothing lasts forever, especially an inexpensive gadget like a Pebble. We want to be able to keep manufacturing these watches long into the future - mostly because I will always want one on my wrist! The company I set up to relaunch Pebble, Core Devices, is self funded, built without investors, and extremely lean. As long as we stay profitable (ie we don‚Äôt lose money), we will continue to manufacture new watches.&lt;/p&gt;
    &lt;p&gt;We‚Äôre also making sure that our new watches are more repairable than old Pebble watches. The back cover of Pebble Time 2 is screwed in. You can remove the back cover and replace the battery.&lt;/p&gt;
    &lt;p&gt;We‚Äôve also published electrical and mechanical design files for Pebble 2 Duo. Yes, you can download the schematic (includes KiCad project files) right now on Github! This should give you a nice jumpstart to designing your own PebbleOS-compatible device.&lt;/p&gt;
    &lt;p&gt;Software&lt;/p&gt;
    &lt;p&gt;Last time round, barely any of the Pebble software was open source. This made it very hard for the Pebble community to make improvements to their watches after the company behind Pebble shut down. Things are different now! This whole relaunch came about primarily because Google open sourced PebbleOS (thank you!). Yesterday, the software that powers Pebble watches was around 95% open source. As of today, it‚Äôs now 100%. This means that if Core Devices were to disappear into a black hole, you have all the source code you need to build, run and improve the software behind your Pebble.&lt;/p&gt;
    &lt;p&gt;I confess that I misunderstood why 95% was much less sustainable than 100% until recently. I discuss this in more detail in my latest Tick Talk episode (check it out). Long story short - I‚Äôm an Android user and was happy to sideload the old Pebble APK on my phone, but iPhone and other Android users have basically been stuck without an easily available Pebble mobile companion app for years.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs how we‚Äôre making sure the 3 main Pebble software components are open source and guaranteed to work long into the future:&lt;/p&gt;
    &lt;p&gt;PebbleOS - software that runs on your watch itself. This has been 100% open source since January and we‚Äôve committed to open sourcing all the improvements we‚Äôve made ‚Üí github.com/coredevices/PebbleOS. You can download the source code, compile PebbleOS and easily install it over Bluetooth on your new Pebble. Textbook definition of open source!&lt;/p&gt;
    &lt;p&gt;Pebble mobile companion app - the app that for your iPhone or Android. Without the app, your Pebble is basically a paperweight. When the Pebble Tech Corp died, the lack of an open source mobile app made it difficult for anyone to continue to use their watches. We had to build an entirely new app (get it here). Today, our app is now 100% open source on Github- ensuring that what happened before cannot happen again. Want to learn more about how we built the new app cross platform using Kotlin Multiplatform? Watch Steve‚Äôs presentation at Droidcon.&lt;/p&gt;
    &lt;p&gt;Developer tools and Pebble Appstore - this software enables people to build and share their watchapps and watchfaces.&lt;/p&gt;
    &lt;p&gt;In the case of dev tools, just being open source is not enough. They needed to be updated to work on modern computers. Before we made improvements, the state of the art of Pebble app development was using an Ubuntu virtualbox VM with Python2! Over the summer, our incredibly productive intern upgraded all the SDK and dev tools and created a new way to develop Pebble apps in the browser. You should check them out!&lt;/p&gt;
    &lt;p&gt;Then there‚Äôs the Pebble Appstore. This is a collection of nearly 15,000 watchfaces and watchapps that you - the Pebble community - developed between 2012 and July 2018. When Fitbit pulled the plug on the original Pebble Appstore, the Rebble Foundation downloaded a copy of all the apps and faces, and set up a new web service to let users of the old Pebble app continue to download and use watchfaces. This was an incredible effort, one that I have used thousands of times and am a happy paying subscriber. But it‚Äôs still centralized - if their server disappears, there is no freely available backup.&lt;/p&gt;
    &lt;p&gt;To compensate for that, today we‚Äôre launching two new things:&lt;/p&gt;
    &lt;p&gt;The Pebble mobile app will soon (later this week) be able to subscribe to multiple appstore ‚Äòfeeds‚Äô. This is similar to open source package managers like pip, AUR, APT, etc. Anyone can create a Pebble-compatible appstore feed and users will be able to browse apps from that feed in the Pebble mobile app.&lt;/p&gt;
    &lt;p&gt;We‚Äôve created our own Pebble Appstore feed (appstore-api.repebble.com) and new Developer Dashboard. Our feed (fyi powered by 100% new software) is configured to back up an archive of all apps and faces to Archive.org (backup will gradually complete over the next week). Today, our feed only has a subset of all Pebble watchfaces and apps (thank you aveao for creating Pebble Archive!). Developers - you can upload your existing or new apps right now! We hope that this sets a standard for openness and we encourage all feeds to publish a freely and publicly available archive.&lt;/p&gt;
    &lt;p&gt;Important to note - developers will still be able to charge money for their apps and faces, using Kiezel pay or other services. This change does not preclude them from doing that, in fact it makes it even easier - I could see some developers creating a paid-only feed. As I recently wrote, we're also working on other ways for Pebble developers to earn money by publishing fun, beautiful and creative Pebble apps.&lt;/p&gt;
    &lt;p&gt;Another important note - some binary blobs and other non-free software components are used today in PebbleOS and the Pebble mobile app (ex: the heart rate sensor on PT2 , Memfault library, and others). Optional non-free web services, like Wispr-flow API speech recognizer, are also used. These non-free software components are not required - you can compile and run Pebble watch software without them. This will always be the case. More non-free software components may appear in our software in the future. The core Pebble watch software stack (everything you need to use your Pebble watch) will always be open source.&lt;/p&gt;
    &lt;p&gt;Pre-production Pebble Time 2. These watches are not final quality! We are still tweaking and tuning everything.&lt;/p&gt;
    &lt;p&gt;PT2 Schedule Update&lt;/p&gt;
    &lt;p&gt;We‚Äôre currently in the middle of Pebble Time 2 design verification test (DVT) phase. After we finish that, we go into production verification test (PVT) and then mass production (MP). So far, things are proceeding according to the schedule update I shared last month but that is extraordinarily subject to change. We still have a lot of testing (especially waterproof and environmental) to go. If we find problems (which is likely) we will push the schedule back to make improvements to the product.&lt;/p&gt;
    &lt;p&gt;The one major complicating factor is the timing of Chinese New Year (CNY). It‚Äôs early next year - factories will shut down for 3 weeks starting around the end of January. After restarting, things always take a week or two to get back to full speed.&lt;/p&gt;
    &lt;p&gt;We are trying our best to get into mass production and ship out at most several thousand Pebble Time 2s before CNY. It‚Äôs going to be very tight ü§û. More likely is that production will begin after CNY, then we need to transfer the watches to our fulfillment center, and ship them out. Realistically, at this time we‚Äôre forecasting that the majority of people will receive their PT2 in March and April. Please keep in mind that things may still change.&lt;/p&gt;
    &lt;p&gt;Picking a PT2 colour&lt;/p&gt;
    &lt;p&gt;There will be 4 colour options for PT2 - black/black, black/red, silver/blue, silver/(white most likely). Let me be crystal very clear - no one has picked a colour yet üòÉ. In a few weeks, I will send out an email asking everyone who pre-ordered a Pebble Time 2 to select which colour they would like to receive. Please do not email us asking when this email will be sent out. No one has been invited yet to do this. I will post here after all emails have gone out.&lt;/p&gt;
    &lt;p&gt;On a related note, I am extremely happy that we built and shipped Pebble 2 Duo. Not only is it an awesome watch, it was also a phenomenal way for us to exercise our production muscles and ease back into the systematic flow of building and shipping smartwatches.&lt;/p&gt;
    &lt;p&gt;A video is worth a million words - so I encourage you to watch me demo Pebble Time 2 watches I just received this week. Keep in mind these watches are PRE-PRODUCTION which means they parts have imperfect qualities! Subject to change!&lt;/p&gt;
    &lt;p&gt;The video below opens to the part of the video where I do the demo.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46037626</guid><pubDate>Mon, 24 Nov 2025 18:52:12 +0000</pubDate></item><item><title>Claude Opus 4.5</title><link>https://www.anthropic.com/news/claude-opus-4-5</link><description>&lt;doc fingerprint="5f7d0efe2f9bc06f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Introducing Claude Opus 4.5&lt;/head&gt;&lt;p&gt;Our newest model, Claude Opus 4.5, is available today. It‚Äôs intelligent, efficient, and the best model in the world for coding, agents, and computer use. It‚Äôs also meaningfully better at everyday tasks like deep research and working with slides and spreadsheets. Opus 4.5 is a step forward in what AI systems can do, and a preview of larger changes to how work gets done.&lt;/p&gt;&lt;p&gt;Claude Opus 4.5 is state-of-the-art on tests of real-world software engineering:&lt;/p&gt;&lt;p&gt;Opus 4.5 is available today on our apps, our API, and on all three major cloud platforms. If you‚Äôre a developer, simply use &lt;code&gt;claude-opus-4-5-20251101&lt;/code&gt; via the Claude API. Pricing is now $5/$25 per million tokens‚Äîmaking Opus-level capabilities accessible to even more users, teams, and enterprises.&lt;/p&gt;&lt;p&gt;Alongside Opus, we‚Äôre releasing updates to the Claude Developer Platform, Claude Code, and our consumer apps. There are new tools for longer-running agents and new ways to use Claude in Excel, Chrome, and on desktop. In the Claude apps, lengthy conversations no longer hit a wall. See our product-focused section below for details.&lt;/p&gt;&lt;head rend="h2"&gt;First impressions&lt;/head&gt;&lt;p&gt;As our Anthropic colleagues tested the model before release, we heard remarkably consistent feedback. Testers noted that Claude Opus 4.5 handles ambiguity and reasons about tradeoffs without hand-holding. They told us that, when pointed at a complex, multi-system bug, Opus 4.5 figures out the fix. They said that tasks that were near-impossible for Sonnet 4.5 just a few weeks ago are now within reach. Overall, our testers told us that Opus 4.5 just ‚Äúgets it.‚Äù&lt;/p&gt;&lt;p&gt;Many of our customers with early access have had similar experiences. Here are some examples of what they told us:&lt;/p&gt;&lt;quote&gt;Opus models have always been ‚Äúthe real SOTA‚Äù but have been cost prohibitive in the past. Claude Opus 4.5 is now at a price point where it can be your go-to model for most tasks. It‚Äôs the clear winner and exhibits the best frontier task planning and tool calling we‚Äôve seen yet.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 delivers high-quality code and excels at powering heavy-duty agentic workflows with GitHub Copilot. Early testing shows it surpasses internal coding benchmarks while cutting token usage in half, and is especially well-suited for tasks like code migration and code refactoring.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 beats Sonnet 4.5 and competition on our internal benchmarks, using fewer tokens to solve the same problems. At scale, that efficiency compounds.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 delivers frontier reasoning within Lovable's chat mode, where users plan and iterate on projects. Its reasoning depth transforms planning‚Äîand great planning makes code generation even better.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 excels at long-horizon, autonomous tasks, especially those that require sustained reasoning and multi-step execution. In our evaluations it handled complex workflows with fewer dead-ends. On Terminal Bench it delivered a 15% improvement over Sonnet 4.5, a meaningful gain that becomes especially clear when using Warp‚Äôs Planning Mode.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 achieved state-of-the-art results for complex enterprise tasks on our benchmarks, outperforming previous models on multi-step reasoning tasks that combine information retrieval, tool use, and deep analysis.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 delivers measurable gains where it matters most: stronger results on our hardest evaluations and consistent performance through 30-minute autonomous coding sessions.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 represents a breakthrough in self-improving AI agents. For office automation, our agents were able to autonomously refine their own capabilities‚Äîachieving peak performance in 4 iterations while other models couldn‚Äôt match that quality after 10.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 is a notable improvement over the prior Claude models inside Cursor, with improved pricing and intelligence on difficult coding tasks.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 is yet another example of Anthropic pushing the frontier of general intelligence. It performs exceedingly well across difficult coding tasks, showcasing long-term goal-directed behavior.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 delivered an impressive refactor spanning two codebases and three coordinated agents. It was very thorough, helping develop a robust plan, handling the details and fixing tests. A clear step forward from Sonnet 4.5.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 handles long-horizon coding tasks more efficiently than any model we‚Äôve tested. It achieves higher pass rates on held-out tests while using up to 65% fewer tokens, giving developers real cost control without sacrificing quality.&lt;/quote&gt;&lt;quote&gt;We‚Äôve found that Opus 4.5 excels at interpreting what users actually want, producing shareable content on the first try. Combined with its speed, token efficiency, and surprisingly low cost, it‚Äôs the first time we‚Äôre making Opus available in Notion Agent.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 excels at long-context storytelling, generating 10-15 page chapters with strong organization and consistency. It's unlocked use cases we couldn't reliably deliver before.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 sets a new standard for Excel automation and financial modeling. Accuracy on our internal evals improved 20%, efficiency rose 15%, and complex tasks that once seemed out of reach became achievable.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 is the only model that nails some of our hardest 3D visualizations. Polished design, tasteful UX, and excellent planning &amp;amp; orchestration - all with more efficient token usage. Tasks that took previous models 2 hours now take thirty minutes.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 catches more issues in code reviews without sacrificing precision. For production code review at scale, that reliability matters.&lt;/quote&gt;&lt;quote&gt;Based on testing with Junie, our coding agent, Claude Opus 4.5 outperforms Sonnet 4.5 across all benchmarks. It requires fewer steps to solve tasks and uses fewer tokens as a result. This indicates that the new model is more precise and follows instructions more effectively ‚Äî a direction we‚Äôre very excited about.&lt;/quote&gt;&lt;quote&gt;The effort parameter is brilliant. Claude Opus 4.5 feels dynamic rather than overthinking, and at lower effort delivers the same quality we need while being dramatically more efficient. That control is exactly what our SQL workflows demand.&lt;/quote&gt;&lt;quote&gt;We‚Äôre seeing 50% to 75% reductions in both tool calling errors and build/lint errors with Claude Opus 4.5. It consistently finishes complex tasks in fewer iterations with more reliable execution.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 is smooth, with none of the rough edges we've seen from other frontier models. The speed improvements are remarkable.&lt;/quote&gt;&lt;head rend="h2"&gt;Evaluating Claude Opus 4.5&lt;/head&gt;&lt;p&gt;We give prospective performance engineering candidates a notoriously difficult take-home exam. We also test new models on this exam as an internal benchmark. Within our prescribed 2-hour time limit, Claude Opus 4.5 scored higher than any human candidate ever1.&lt;/p&gt;&lt;p&gt;The take-home test is designed to assess technical ability and judgment under time pressure. It doesn‚Äôt test for other crucial skills candidates may possess, like collaboration, communication, or the instincts that develop over years. But this result‚Äîwhere an AI model outperforms strong candidates on important technical skills‚Äîraises questions about how AI will change engineering as a profession. Our Societal Impacts and Economic Futures research is aimed at understanding these kinds of changes across many fields. We plan to share more results soon.&lt;/p&gt;&lt;p&gt;Software engineering isn‚Äôt the only area on which Claude Opus 4.5 has improved. Capabilities are higher across the board‚ÄîOpus 4.5 has better vision, reasoning, and mathematics skills than its predecessors, and it is state-of-the-art in many domains:2&lt;/p&gt;&lt;p&gt;The model‚Äôs capabilities outpace some of the benchmarks we use in our tests. A common benchmark for agentic capabilities is œÑ2-bench, which measures the performance of agents in real-world, multi-turn tasks. In one scenario, models have to act as an airline service agent helping a distressed customer. The benchmark expects models to refuse a modification to a basic economy booking since the airline doesn‚Äôt allow changes to that class of tickets. Instead, Opus 4.5 found an insightful (and legitimate) way to solve the problem: upgrade the cabin first, then modify the flights.&lt;/p&gt;&lt;code&gt;Let me think about what options I have within my policy:

1. Modify flights - Basic economy cannot be modified. This is clear in the policy.
2. Change cabin - Wait, let me check this option! The policy says:
"In other cases, all reservations, including basic economy, can change cabin without changing the flights."

But he wants to change the flights, not just the cabin. However, what if we:
- Change the cabin first (which IS allowed for basic economy), then
- Change the flights (which is allowed for non-basic economy)

So the strategy could be:
1. Upgrade his cabin from basic economy to economy (or business)
2. Then modify the flights to be 2 days later

This would cost more money, but it‚Äôs a legitimate path within the policy!&lt;/code&gt;&lt;p&gt;The benchmark technically scored this as a failure because Claude‚Äôs way of helping the customer was unanticipated. But this kind of creative problem solving is exactly what we‚Äôve heard about from our testers and customers‚Äîit‚Äôs what makes Claude Opus 4.5 feel like a meaningful step forward.&lt;/p&gt;&lt;p&gt;In other contexts, finding clever paths around intended constraints could count as reward hacking‚Äîwhere models ‚Äúgame‚Äù rules or objectives in unintended ways. Preventing such misalignment is one of the objectives of our safety testing, discussed in the next section.&lt;/p&gt;&lt;head rend="h2"&gt;A step forward on safety&lt;/head&gt;&lt;p&gt;As we state in our system card, Claude Opus 4.5 is the most robustly aligned model we have released to date and, we suspect, the best-aligned frontier model by any developer. It continues our trend towards safer and more secure models:&lt;/p&gt;&lt;p&gt;Our customers often use Claude for critical tasks. They want to be assured that, in the face of malicious attacks by hackers and cybercriminals, Claude has the training and the ‚Äústreet smarts‚Äù to avoid trouble. With Opus 4.5, we‚Äôve made substantial progress in robustness against prompt injection attacks, which smuggle in deceptive instructions to fool the model into harmful behavior. Opus 4.5 is harder to trick with prompt injection than any other frontier model in the industry:&lt;/p&gt;&lt;p&gt;You can find a detailed description of all our capability and safety evaluations in the Claude Opus 4.5 system card.&lt;/p&gt;&lt;head rend="h2"&gt;New on the Claude Developer Platform&lt;/head&gt;&lt;p&gt;As models get smarter, they can solve problems in fewer steps: less backtracking, less redundant exploration, less verbose reasoning. Claude Opus 4.5 uses dramatically fewer tokens than its predecessors to reach similar or better outcomes.&lt;/p&gt;&lt;p&gt;But different tasks call for different tradeoffs. Sometimes developers want a model to keep thinking about a problem; sometimes they want something more nimble. With our new effort parameter on the Claude API, you can decide to minimize time and spend or maximize capability.&lt;/p&gt;&lt;p&gt;Set to a medium effort level, Opus 4.5 matches Sonnet 4.5‚Äôs best score on SWE-bench Verified, but uses 76% fewer output tokens. At its highest effort level, Opus 4.5 exceeds Sonnet 4.5 performance by 4.3 percentage points‚Äîwhile using 48% fewer tokens.&lt;/p&gt;&lt;p&gt;With effort control, context compaction, and advanced tool use, Claude Opus 4.5 runs longer, does more, and requires less intervention.&lt;/p&gt;&lt;p&gt;Our context management and memory capabilities can dramatically boost performance on agentic tasks. Opus 4.5 is also very effective at managing a team of subagents, enabling the construction of complex, well-coordinated multi-agent systems. In our testing, the combination of all these techniques boosted Opus 4.5‚Äôs performance on a deep research evaluation by almost 15 percentage points4.&lt;/p&gt;&lt;p&gt;We‚Äôre making our Developer Platform more composable over time. We want to give you the building blocks to construct exactly what you need, with full control over efficiency, tool use, and context management.&lt;/p&gt;&lt;head rend="h2"&gt;Product updates&lt;/head&gt;&lt;p&gt;Products like Claude Code show what‚Äôs possible when the kinds of upgrades we‚Äôve made to the Claude Developer Platform come together. Claude Code gains two upgrades with Opus 4.5. Plan Mode now builds more precise plans and executes more thoroughly‚ÄîClaude asks clarifying questions upfront, then builds a user-editable plan.md file before executing.&lt;/p&gt;&lt;p&gt;Claude Code is also now available in our desktop app, letting you run multiple local and remote sessions in parallel: perhaps one agent fixes bugs, another researches GitHub, and a third updates docs.&lt;/p&gt;&lt;p&gt;For Claude app users, long conversations no longer hit a wall‚ÄîClaude automatically summarizes earlier context as needed, so you can keep the chat going. Claude for Chrome, which lets Claude handle tasks across your browser tabs, is now available to all Max users. We announced Claude for Excel in October, and as of today we've expanded beta access to all Max, Team, and Enterprise users. Each of these updates takes advantage of Claude Opus 4.5‚Äôs market-leading performance in using computers, spreadsheets, and handling long-running tasks.&lt;/p&gt;&lt;p&gt;For Claude and Claude Code users with access to Opus 4.5, we‚Äôve removed Opus-specific caps. For Max and Team Premium users, we‚Äôve increased overall usage limits, meaning you‚Äôll have roughly the same number of Opus tokens as you previously had with Sonnet. We‚Äôre updating usage limits to make sure you‚Äôre able to use Opus 4.5 for daily work. These limits are specific to Opus 4.5. As future models surpass it, we expect to update limits as needed.&lt;/p&gt;&lt;head rend="h4"&gt;Footnotes&lt;/head&gt;&lt;p&gt;1: This result was using parallel test-time compute, a method that aggregates multiple ‚Äútries‚Äù from the model and selects from among them. Without a time limit, the model (used within Claude Code) matched the best-ever human candidate.&lt;/p&gt;&lt;p&gt;2: We improved the hosting environment to reduce infrastructure failures. This change improved Gemini 3 to 56.7% and GPT-5.1 to 48.6% from the values reported by their developers, using the Terminus-2 harness.&lt;/p&gt;&lt;p&gt;3: Note that these evaluations were run on an in-progress upgrade to Petri, our open-source, automated evaluation tool. They were run on an earlier snapshot of Claude Opus 4.5. Evaluations of the final production model show a very similar pattern of results when compared to other Claude models, and are described in detail in the Claude Opus 4.5 system card.&lt;/p&gt;&lt;p&gt;4: A fetch-enabled version of BrowseComp-Plus. Specifically, the improvement was from 70.48% without using the combination of techniques to 85.30% using it.&lt;/p&gt;&lt;p&gt;Methodology&lt;/p&gt;&lt;p&gt;All evals were run with a 64K thinking budget, interleaved scratchpads, 200K context window, default effort (high), default sampling settings (temperature, top_p), and averaged over 5 independent trials. Exceptions: SWE-bench Verified (no thinking budget) and Terminal Bench (128K thinking budget). Please see the Claude Opus 4.5 system card for full details.&lt;/p&gt;&lt;head rend="h2"&gt;Related content&lt;/head&gt;&lt;head rend="h3"&gt;Claude now available in Microsoft Foundry and Microsoft 365 Copilot&lt;/head&gt;Read more&lt;head rend="h3"&gt;Microsoft, NVIDIA, and Anthropic announce strategic partnerships&lt;/head&gt;&lt;p&gt;Microsoft, NVIDIA and Anthropic announced new strategic partnerships. Anthropic is scaling its rapidly-growing Claude AI model on Microsoft Azure, powered by NVIDIA, which will broaden access to Claude and provide Azure enterprise customers with expanded model choice and new capabilities. Anthropic has committed to purchase $30 billion of Azure compute capacity and to contract additional compute capacity up to one gigawatt.&lt;/p&gt;Read more&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46037637</guid><pubDate>Mon, 24 Nov 2025 18:53:05 +0000</pubDate></item><item><title>Claude Advanced Tool Use</title><link>https://www.anthropic.com/engineering/advanced-tool-use</link><description>&lt;doc fingerprint="b34d66d332b7caaa"&gt;
  &lt;main&gt;
    &lt;p&gt;The future of AI agents is one where models work seamlessly across hundreds or thousands of tools. An IDE assistant that integrates git operations, file manipulation, package managers, testing frameworks, and deployment pipelines. An operations coordinator that connects Slack, GitHub, Google Drive, Jira, company databases, and dozens of MCP servers simultaneously.&lt;/p&gt;
    &lt;p&gt;To build effective agents, they need to work with unlimited tool libraries without stuffing every definition into context upfront. Our blog article on using code execution with MCP discussed how tool results and definitions can sometimes consume 50,000+ tokens before an agent reads a request. Agents should discover and load tools on-demand, keeping only what's relevant for the current task.&lt;/p&gt;
    &lt;p&gt;Agents also need the ability to call tools from code. When using natural language tool calling, each invocation requires a full inference pass, and intermediate results pile up in context whether they're useful or not. Code is a natural fit for orchestration logic, such as loops, conditionals, and data transformations. Agents need the flexibility to choose between code execution and inference based on the task at hand.&lt;/p&gt;
    &lt;p&gt;Agents also need to learn correct tool usage from examples, not just schema definitions. JSON schemas define what's structurally valid, but can't express usage patterns: when to include optional parameters, which combinations make sense, or what conventions your API expects.&lt;/p&gt;
    &lt;p&gt;Today, we're releasing three features that make this possible:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tool Search Tool, which allows Claude to use search tools to access thousands of tools without consuming its context window&lt;/item&gt;
      &lt;item&gt;Programmatic Tool Calling, which allows Claude to invoke tools in a code execution environment reducing the impact on the model‚Äôs context window&lt;/item&gt;
      &lt;item&gt;Tool Use Examples, which provides a universal standard for demonstrating how to effectively use a given tool&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In internal testing, we‚Äôve found these features have helped us build things that wouldn‚Äôt have been possible with conventional tool use patterns. For example, Claude for Excel uses Programmatic Tool Calling to read and modify spreadsheets with thousands of rows without overloading the model‚Äôs context window.&lt;/p&gt;
    &lt;p&gt;Based on our experience, we believe these features open up new possibilities for what you can build with Claude.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tool Search Tool&lt;/head&gt;
    &lt;head rend="h3"&gt;The challenge&lt;/head&gt;
    &lt;p&gt;MCP tool definitions provide important context, but as more servers connect, those tokens can add up. Consider a five-server setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GitHub: 35 tools (~26K tokens)&lt;/item&gt;
      &lt;item&gt;Slack: 11 tools (~21K tokens)&lt;/item&gt;
      &lt;item&gt;Sentry: 5 tools (~3K tokens)&lt;/item&gt;
      &lt;item&gt;Grafana: 5 tools (~3K tokens)&lt;/item&gt;
      &lt;item&gt;Splunk: 2 tools (~2K tokens)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That's 58 tools consuming approximately 55K tokens before the conversation even starts. Add more servers like Jira (which alone uses ~17K tokens) and you're quickly approaching 100K+ token overhead. At Anthropic, we've seen tool definitions consume 134K tokens before optimization.&lt;/p&gt;
    &lt;p&gt;But token cost isn't the only issue. The most common failures are wrong tool selection and incorrect parameters, especially when tools have similar names like &lt;code&gt;notification-send-user&lt;/code&gt; vs. &lt;code&gt;notification-send-channel&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Our solution&lt;/head&gt;
    &lt;p&gt;Instead of loading all tool definitions upfront, the Tool Search Tool discovers tools on-demand. Claude only sees the tools it actually needs for the current task.&lt;/p&gt;
    &lt;p&gt;Traditional approach:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All tool definitions loaded upfront (~72K tokens for 50+ MCP tools)&lt;/item&gt;
      &lt;item&gt;Conversation history and system prompt compete for remaining space&lt;/item&gt;
      &lt;item&gt;Total context consumption: ~77K tokens before any work begins&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With the Tool Search Tool:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Only the Tool Search Tool loaded upfront (~500 tokens)&lt;/item&gt;
      &lt;item&gt;Tools discovered on-demand as needed (3-5 relevant tools, ~3K tokens)&lt;/item&gt;
      &lt;item&gt;Total context consumption: ~8.7K tokens, preserving 95% of context window&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This represents an 85% reduction in token usage while maintaining access to your full tool library. Internal testing showed significant accuracy improvements on MCP evaluations when working with large tool libraries. Opus 4 improved from 49% to 74%, and Opus 4.5 improved from 79.5% to 88.1% with Tool Search Tool enabled.&lt;/p&gt;
    &lt;head rend="h3"&gt;How the Tool Search Tool works&lt;/head&gt;
    &lt;p&gt;The Tool Search Tool lets Claude dynamically discover tools instead of loading all definitions upfront. You provide all your tool definitions to the API, but mark tools with &lt;code&gt;defer_loading: true&lt;/code&gt; to make them discoverable on-demand. Deferred tools aren't loaded into Claude's context initially. Claude only sees the Tool Search Tool itself plus any tools with &lt;code&gt;defer_loading: false&lt;/code&gt; (your most critical, frequently-used tools).&lt;/p&gt;
    &lt;p&gt;When Claude needs specific capabilities, it searches for relevant tools. The Tool Search Tool returns references to matching tools, which get expanded into full definitions in Claude's context.&lt;/p&gt;
    &lt;p&gt;For example, if Claude needs to interact with GitHub, it searches for "github," and only &lt;code&gt;github.createPullRequest&lt;/code&gt; and &lt;code&gt;github.listIssues&lt;/code&gt; get loaded‚Äînot your other 50+ tools from Slack, Jira, and Google Drive.&lt;/p&gt;
    &lt;p&gt;This way, Claude has access to your full tool library while only paying the token cost for tools it actually needs.&lt;/p&gt;
    &lt;p&gt;Implementation:&lt;/p&gt;
    &lt;code&gt;{
  "tools": [
    // Include a tool search tool (regex, BM25, or custom)
    {"type": "tool_search_tool_regex_20251119", "name": "tool_search_tool_regex"},

    // Mark tools for on-demand discovery
    {
      "name": "github.createPullRequest",
      "description": "Create a pull request",
      "input_schema": {...},
      "defer_loading": true
    }
    // ... hundreds more deferred tools with defer_loading: true
  ]
}
&lt;/code&gt;
    &lt;p&gt;For MCP servers, you can defer loading entire servers while keeping specific high-use tools loaded:&lt;/p&gt;
    &lt;code&gt;{
  "type": "mcp_toolset",
  "mcp_server_name": "google-drive",
  "default_config": {"defer_loading": true}, # defer loading the entire server
  "configs": {
    "search_files": {
"defer_loading": false
    }  // Keep most used tool loaded
  }
}&lt;/code&gt;
    &lt;p&gt;The Claude Developer Platform provides regex-based and BM25-based search tools out of the box, but you can also implement custom search tools using embeddings or other strategies.&lt;/p&gt;
    &lt;head rend="h3"&gt;When to use the Tool Search Tool&lt;/head&gt;
    &lt;p&gt;Like any architectural decision, enabling the Tool Search Tool involves trade-offs. The feature adds a search step before tool invocation, so it delivers the best ROI when the context savings and accuracy improvements outweigh additional latency.&lt;/p&gt;
    &lt;p&gt;Use it when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tool definitions consuming &amp;gt;10K tokens&lt;/item&gt;
      &lt;item&gt;Experiencing tool selection accuracy issues&lt;/item&gt;
      &lt;item&gt;Building MCP-powered systems with multiple servers&lt;/item&gt;
      &lt;item&gt;10+ tools available&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Less beneficial when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Small tool library (&amp;lt;10 tools)&lt;/item&gt;
      &lt;item&gt;All tools used frequently in every session&lt;/item&gt;
      &lt;item&gt;Tool definitions are compact&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Programmatic Tool Calling&lt;/head&gt;
    &lt;head rend="h3"&gt;The challenge&lt;/head&gt;
    &lt;p&gt;Traditional tool calling creates two fundamental problems as workflows become more complex:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Context pollution from intermediate results: When Claude analyzes a 10MB log file for error patterns, the entire file enters its context window, even though Claude only needs a summary of error frequencies. When fetching customer data across multiple tables, every record accumulates in context regardless of relevance. These intermediate results consume massive token budgets and can push important information out of the context window entirely.&lt;/item&gt;
      &lt;item&gt;Inference overhead and manual synthesis: Each tool call requires a full model inference pass. After receiving results, Claude must "eyeball" the data to extract relevant information, reason about how pieces fit together, and decide what to do next‚Äîall through natural language processing. A five tool workflow means five inference passes plus Claude parsing each result, comparing values, and synthesizing conclusions. This is both slow and error-prone.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Our solution&lt;/head&gt;
    &lt;p&gt;Programmatic Tool Calling enables Claude to orchestrate tools through code rather than through individual API round-trips. Instead of Claude requesting tools one at a time with each result being returned to its context, Claude writes code that calls multiple tools, processes their outputs, and controls what information actually enters its context window.&lt;/p&gt;
    &lt;p&gt;Claude excels at writing code and by letting it express orchestration logic in Python rather than through natural language tool invocations, you get more reliable, precise control flow. Loops, conditionals, data transformations, and error handling are all explicit in code rather than implicit in Claude's reasoning.&lt;/p&gt;
    &lt;head rend="h4"&gt;Example: Budget compliance check&lt;/head&gt;
    &lt;p&gt;Consider a common business task: "Which team members exceeded their Q3 travel budget?"&lt;/p&gt;
    &lt;p&gt;You have three tools available:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;get_team_members(department)&lt;/code&gt;- Returns team member list with IDs and levels&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;get_expenses(user_id, quarter)&lt;/code&gt;- Returns expense line items for a user&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;get_budget_by_level(level)&lt;/code&gt;- Returns budget limits for an employee level&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Traditional approach:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fetch team members ‚Üí 20 people&lt;/item&gt;
      &lt;item&gt;For each person, fetch their Q3 expenses ‚Üí 20 tool calls, each returning 50-100 line items (flights, hotels, meals, receipts)&lt;/item&gt;
      &lt;item&gt;Fetch budget limits by employee level&lt;/item&gt;
      &lt;item&gt;All of this enters Claude's context: 2,000+ expense line items (50 KB+)&lt;/item&gt;
      &lt;item&gt;Claude manually sums each person's expenses, looks up their budget, compares expenses against budget limits&lt;/item&gt;
      &lt;item&gt;More round-trips to the model, significant context consumption&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With Programmatic Tool Calling:&lt;/p&gt;
    &lt;p&gt;Instead of each tool result returning to Claude, Claude writes a Python script that orchestrates the entire workflow. The script runs in the Code Execution tool (a sandboxed environment), pausing when it needs results from your tools. When you return tool results via the API, they're processed by the script rather than consumed by the model. The script continues executing, and Claude only sees the final output.&lt;/p&gt;
    &lt;p&gt;Here's what Claude's orchestration code looks like for the budget compliance task:&lt;/p&gt;
    &lt;code&gt;team = await get_team_members("engineering")

# Fetch budgets for each unique level
levels = list(set(m["level"] for m in team))
budget_results = await asyncio.gather(*[
    get_budget_by_level(level) for level in levels
])

# Create a lookup dictionary: {"junior": budget1, "senior": budget2, ...}
budgets = {level: budget for level, budget in zip(levels, budget_results)}

# Fetch all expenses in parallel
expenses = await asyncio.gather(*[
    get_expenses(m["id"], "Q3") for m in team
])

# Find employees who exceeded their travel budget
exceeded = []
for member, exp in zip(team, expenses):
    budget = budgets[member["level"]]
    total = sum(e["amount"] for e in exp)
    if total &amp;gt; budget["travel_limit"]:
        exceeded.append({
            "name": member["name"],
            "spent": total,
            "limit": budget["travel_limit"]
        })

print(json.dumps(exceeded))&lt;/code&gt;
    &lt;p&gt;Claude's context receives only the final result: the two to three people who exceeded their budget. The 2,000+ line items, the intermediate sums, and the budget lookups do not affect Claude‚Äôs context, reducing consumption from 200KB of raw expense data to just 1KB of results.&lt;/p&gt;
    &lt;p&gt;The efficiency gains are substantial:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Token savings: By keeping intermediate results out of Claude's context, PTC dramatically reduces token consumption. Average usage dropped from 43,588 to 27,297 tokens, a 37% reduction on complex research tasks.&lt;/item&gt;
      &lt;item&gt;Reduced latency: Each API round-trip requires model inference (hundreds of milliseconds to seconds). When Claude orchestrates 20+ tool calls in a single code block, you eliminate 19+ inference passes. The API handles tool execution without returning to the model each time.&lt;/item&gt;
      &lt;item&gt;Improved accuracy: By writing explicit orchestration logic, Claude makes fewer errors than when juggling multiple tool results in natural language. Internal knowledge retrieval improved from 25.6% to 28.5%; GIA benchmarks from 46.5% to 51.2%.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Production workflows involve messy data, conditional logic, and operations that need to scale. Programmatic Tool Calling lets Claude handle that complexity programmatically while keeping its focus on actionable results rather than raw data processing.&lt;/p&gt;
    &lt;head rend="h3"&gt;How Programmatic Tool Calling works&lt;/head&gt;
    &lt;head rend="h4"&gt;1. Mark tools as callable from code&lt;/head&gt;
    &lt;p&gt;Add code_execution to tools, and set allowed_callers to opt-in tools for programmatic execution:&lt;/p&gt;
    &lt;code&gt;{
  "tools": [
    {
      "type": "code_execution_20250825",
      "name": "code_execution"
    },
    {
      "name": "get_team_members",
      "description": "Get all members of a department...",
      "input_schema": {...},
      "allowed_callers": ["code_execution_20250825"] # opt-in to programmatic tool calling
    },
    {
      "name": "get_expenses",
 	...
    },
    {
      "name": "get_budget_by_level",
	...
    }
  ]
}&lt;/code&gt;
    &lt;p&gt;The API converts these tool definitions into Python functions that Claude can call.&lt;/p&gt;
    &lt;head rend="h4"&gt;2. Claude writes orchestration code&lt;/head&gt;
    &lt;p&gt;Instead of requesting tools one at a time, Claude generates Python code:&lt;/p&gt;
    &lt;code&gt;{
  "type": "server_tool_use",
  "id": "srvtoolu_abc",
  "name": "code_execution",
  "input": {
    "code": "team = get_team_members('engineering')\n..." # the code example above
  }
}&lt;/code&gt;
    &lt;head rend="h4"&gt;3. Tools execute without hitting Claude's context&lt;/head&gt;
    &lt;p&gt;When the code calls get_expenses(), you receive a tool request with a caller field:&lt;/p&gt;
    &lt;code&gt;{
  "type": "tool_use",
  "id": "toolu_xyz",
  "name": "get_expenses",
  "input": {"user_id": "emp_123", "quarter": "Q3"},
  "caller": {
    "type": "code_execution_20250825",
    "tool_id": "srvtoolu_abc"
  }
}&lt;/code&gt;
    &lt;p&gt;You provide the result, which is processed in the Code Execution environment rather than Claude's context. This request-response cycle repeats for each tool call in the code.&lt;/p&gt;
    &lt;head rend="h4"&gt;4. Only final output enters context&lt;/head&gt;
    &lt;p&gt;When the code finishes running, only the results of the code are returned to Claude:&lt;/p&gt;
    &lt;code&gt;{
  "type": "code_execution_tool_result",
  "tool_use_id": "srvtoolu_abc",
  "content": {
    "stdout": "[{\"name\": \"Alice\", \"spent\": 12500, \"limit\": 10000}...]"
  }
}&lt;/code&gt;
    &lt;p&gt;This is all Claude sees, not the 2000+ expense line items processed along the way.&lt;/p&gt;
    &lt;head rend="h3"&gt;When to use Programmatic Tool Calling&lt;/head&gt;
    &lt;p&gt;Programmatic Tool Calling adds a code execution step to your workflow. This extra overhead pays off when the token savings, latency improvements, and accuracy gains are substantial.&lt;/p&gt;
    &lt;p&gt;Most beneficial when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Processing large datasets where you only need aggregates or summaries&lt;/item&gt;
      &lt;item&gt;Running multi-step workflows with three or more dependent tool calls&lt;/item&gt;
      &lt;item&gt;Filtering, sorting, or transforming tool results before Claude sees them&lt;/item&gt;
      &lt;item&gt;Handling tasks where intermediate data shouldn't influence Claude's reasoning&lt;/item&gt;
      &lt;item&gt;Running parallel operations across many items (checking 50 endpoints, for example)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Less beneficial when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Making simple single-tool invocations&lt;/item&gt;
      &lt;item&gt;Working on tasks where Claude should see and reason about all intermediate results&lt;/item&gt;
      &lt;item&gt;Running quick lookups with small responses&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Tool Use Examples&lt;/head&gt;
    &lt;head rend="h3"&gt;The challenge&lt;/head&gt;
    &lt;p&gt;JSON Schema excels at defining structure‚Äìtypes, required fields, allowed enums‚Äìbut it can't express usage patterns: when to include optional parameters, which combinations make sense, or what conventions your API expects.&lt;/p&gt;
    &lt;p&gt;Consider a support ticket API:&lt;/p&gt;
    &lt;code&gt;{
  "name": "create_ticket",
  "input_schema": {
    "properties": {
      "title": {"type": "string"},
      "priority": {"enum": ["low", "medium", "high", "critical"]},
      "labels": {"type": "array", "items": {"type": "string"}},
      "reporter": {
        "type": "object",
        "properties": {
          "id": {"type": "string"},
          "name": {"type": "string"},
          "contact": {
            "type": "object",
            "properties": {
              "email": {"type": "string"},
              "phone": {"type": "string"}
            }
          }
        }
      },
      "due_date": {"type": "string"},
      "escalation": {
        "type": "object",
        "properties": {
          "level": {"type": "integer"},
          "notify_manager": {"type": "boolean"},
          "sla_hours": {"type": "integer"}
        }
      }
    },
    "required": ["title"]
  }
}&lt;/code&gt;
    &lt;p&gt;The schema defines what's valid, but leaves critical questions unanswered:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Format ambiguity: Should &lt;code&gt;due_date&lt;/code&gt;use "2024-11-06", "Nov 6, 2024", or "2024-11-06T00:00:00Z"?&lt;/item&gt;
      &lt;item&gt;ID conventions: Is &lt;code&gt;reporter.id&lt;/code&gt;a UUID, "USR-12345", or just "12345"?&lt;/item&gt;
      &lt;item&gt;Nested structure usage: When should Claude populate &lt;code&gt;reporter.contact&lt;/code&gt;?&lt;/item&gt;
      &lt;item&gt;Parameter correlations: How do &lt;code&gt;escalation.level&lt;/code&gt;and&lt;code&gt;escalation.sla_hours&lt;/code&gt;relate to priority?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These ambiguities can lead to malformed tool calls and inconsistent parameter usage.&lt;/p&gt;
    &lt;head rend="h3"&gt;Our solution&lt;/head&gt;
    &lt;p&gt;Tool Use Examples let you provide sample tool calls directly in your tool definitions. Instead of relying on schema alone, you show Claude concrete usage patterns:&lt;/p&gt;
    &lt;code&gt;{
    "name": "create_ticket",
    "input_schema": { /* same schema as above */ },
    "input_examples": [
      {
        "title": "Login page returns 500 error",
        "priority": "critical",
        "labels": ["bug", "authentication", "production"],
        "reporter": {
          "id": "USR-12345",
          "name": "Jane Smith",
          "contact": {
            "email": "jane@acme.com",
            "phone": "+1-555-0123"
          }
        },
        "due_date": "2024-11-06",
        "escalation": {
          "level": 2,
          "notify_manager": true,
          "sla_hours": 4
        }
      },
      {
        "title": "Add dark mode support",
        "labels": ["feature-request", "ui"],
        "reporter": {
          "id": "USR-67890",
          "name": "Alex Chen"
        }
      },
      {
        "title": "Update API documentation"
      }
    ]
  }&lt;/code&gt;
    &lt;p&gt;From these three examples, Claude learns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Format conventions: Dates use YYYY-MM-DD, user IDs follow USR-XXXXX, labels use kebab-case&lt;/item&gt;
      &lt;item&gt;Nested structure patterns: How to construct the reporter object with its nested contact object&lt;/item&gt;
      &lt;item&gt;Optional parameter correlations: Critical bugs have full contact info + escalation with tight SLAs; feature requests have reporter but no contact/escalation; internal tasks have title only&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In our own internal testing, tool use examples improved accuracy from 72% to 90% on complex parameter handling.&lt;/p&gt;
    &lt;head rend="h3"&gt;When to use Tool Use Examples&lt;/head&gt;
    &lt;p&gt;Tool Use Examples add tokens to your tool definitions, so they‚Äôre most valuable when accuracy improvements outweigh the additional cost.&lt;/p&gt;
    &lt;p&gt;Most beneficial when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Complex nested structures where valid JSON doesn't imply correct usage&lt;/item&gt;
      &lt;item&gt;Tools with many optional parameters and inclusion patterns matter&lt;/item&gt;
      &lt;item&gt;APIs with domain-specific conventions not captured in schemas&lt;/item&gt;
      &lt;item&gt;Similar tools where examples clarify which one to use (e.g., &lt;code&gt;create_ticket&lt;/code&gt;vs&lt;code&gt;create_incident&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Less beneficial when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simple single-parameter tools with obvious usage&lt;/item&gt;
      &lt;item&gt;Standard formats like URLs or emails that Claude already understands&lt;/item&gt;
      &lt;item&gt;Validation concerns better handled by JSON Schema constraints&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Best practices&lt;/head&gt;
    &lt;p&gt;Building agents that take real-world actions means handling scale, complexity, and precision simultaneously. These three features work together to solve different bottlenecks in tool use workflows. Here's how to combine them effectively.&lt;/p&gt;
    &lt;head rend="h3"&gt;Layer features strategically&lt;/head&gt;
    &lt;p&gt;Not every agent needs to use all three features for a given task. Start with your biggest bottleneck:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Context bloat from tool definitions ‚Üí Tool Search Tool&lt;/item&gt;
      &lt;item&gt;Large intermediate results polluting context ‚Üí Programmatic Tool Calling&lt;/item&gt;
      &lt;item&gt;Parameter errors and malformed calls ‚Üí Tool Use Examples&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This focused approach lets you address the specific constraint limiting your agent's performance, rather than adding complexity upfront.&lt;/p&gt;
    &lt;p&gt;Then layer additional features as needed. They're complementary: Tool Search Tool ensures the right tools are found, Programmatic Tool Calling ensures efficient execution, and Tool Use Examples ensure correct invocation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Set up Tool Search Tool for better discovery&lt;/head&gt;
    &lt;p&gt;Tool search matches against names and descriptions, so clear, descriptive definitions improve discovery accuracy.&lt;/p&gt;
    &lt;code&gt;// Good
{
    "name": "search_customer_orders",
    "description": "Search for customer orders by date range, status, or total amount. Returns order details including items, shipping, and payment info."
}

// Bad
{
    "name": "query_db_orders",
    "description": "Execute order query"
}&lt;/code&gt;
    &lt;p&gt;Add system prompt guidance so Claude knows what's available:&lt;/p&gt;
    &lt;code&gt;You have access to tools for Slack messaging, Google Drive file management, 
Jira ticket tracking, and GitHub repository operations. Use the tool search 
to find specific capabilities.&lt;/code&gt;
    &lt;p&gt;Keep your three to five most-used tools always loaded, defer the rest. This balances immediate access for common operations with on-demand discovery for everything else.&lt;/p&gt;
    &lt;head rend="h3"&gt;Set up Programmatic Tool Calling for correct execution&lt;/head&gt;
    &lt;p&gt;Since Claude writes code to parse tool outputs, document return formats clearly. This helps Claude write correct parsing logic:&lt;/p&gt;
    &lt;code&gt;{
    "name": "get_orders",
    "description": "Retrieve orders for a customer.
Returns:
    List of order objects, each containing:
    - id (str): Order identifier
    - total (float): Order total in USD
    - status (str): One of 'pending', 'shipped', 'delivered'
    - items (list): Array of {sku, quantity, price}
    - created_at (str): ISO 8601 timestamp"
}&lt;/code&gt;
    &lt;p&gt;See below for opt-in tools that benefit from programmatic orchestration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tools that can run in parallel (independent operations)&lt;/item&gt;
      &lt;item&gt;Operations safe to retry (idempotent)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Set up Tool Use Examples for parameter accuracy&lt;/head&gt;
    &lt;p&gt;Craft examples for behavioral clarity:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use realistic data (real city names, plausible prices, not "string" or "value")&lt;/item&gt;
      &lt;item&gt;Show variety with minimal, partial, and full specification patterns&lt;/item&gt;
      &lt;item&gt;Keep it concise: 1-5 examples per tool&lt;/item&gt;
      &lt;item&gt;Focus on ambiguity (only add examples where correct usage isn't obvious from schema)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Getting started&lt;/head&gt;
    &lt;p&gt;These features are available in beta. To enable them, add the beta header and include the tools you need:&lt;/p&gt;
    &lt;code&gt;client.beta.messages.create(
    betas=["advanced-tool-use-2025-11-20"],
    model="claude-sonnet-4-5-20250929",
    max_tokens=4096,
    tools=[
        {"type": "tool_search_tool_regex_20251119", "name": "tool_search_tool_regex"},
        {"type": "code_execution_20250825", "name": "code_execution"},
        # Your tools with defer_loading, allowed_callers, and input_examples
    ]
)&lt;/code&gt;
    &lt;p&gt;For detailed API documentation and SDK examples, see our:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Documentation and cookbook for Tool Search Tool&lt;/item&gt;
      &lt;item&gt;Documentation and cookbook for Programmatic Tool Calling&lt;/item&gt;
      &lt;item&gt;Documentation for Tool Use Examples&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These features move tool use from simple function calling toward intelligent orchestration. As agents tackle more complex workflows spanning dozens of tools and large datasets, dynamic discovery, efficient execution, and reliable invocation become foundational.&lt;/p&gt;
    &lt;p&gt;We're excited to see what you build.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Written by Bin Wu, with contributions from Adam Jones, Artur Renault, Henry Tay, Jake Noble, Nathan McCandlish, Noah Picard, Sam Jiang, and the Claude Developer Platform team. This work builds on foundational research by Chris Gorgolewski, Daniel Jiang, Jeremy Fox and Mike Lambert. We also drew inspiration from across the AI ecosystem, including Joel Pobar's LLMVM, Cloudflare's Code Mode and Code Execution as MCP. Special thanks to Andy Schumeister, Hamish Kerr, Keir Bradwell, Matt Bleifer and Molly Vorwerck for their support.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46038047</guid><pubDate>Mon, 24 Nov 2025 19:21:35 +0000</pubDate></item><item><title>PS5 now costs less than 64GB of DDR5 memory. RAM jumps to $600 due to shortage</title><link>https://www.tomshardware.com/pc-components/ddr5/64gb-of-ddr5-memory-now-costs-more-than-an-entire-ps5-even-after-a-discount-trident-z5-neo-kit-jumps-to-usd600-due-to-dram-shortage-and-its-expected-to-get-worse-into-2026</link><description>&lt;doc fingerprint="7b2a58f58eb6e785"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;An entire PS5 now costs less than 64GB of DDR5 memory, even after a discount ‚Äî simple memory kit jumps to $600 due to DRAM shortage, and it's expected to get worse into 2026&lt;/head&gt;
    &lt;p&gt;Thanks to the AI boom devouring the majority of the world's memory and storage supply, end-consumers are now facing increasingly inflated prices for common components. DDR5 RAM, a necessity for building current-gen Intel or AMD systems, has now reached record highs in terms of pricing; a 64 GB kit of G.Skill's Trident Z5 Neo 6000 MT/s RAM is listed at $599.99 on Newegg right now ‚Äî that's $200 more than a PS5 Slim or a Microsoft Xbox Series S, and just $50 shy off an entire PS5 Pro at the moment.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;G.SKILL Trident Z5 Neo RGB Series 64GB DDR5 6000 (PC5 48000)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;($599 in Black Friday sale, with $40 off and additional $20 off via code BFE2458)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Row 1 - Cell 2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Row 2 - Cell 2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Row 3 - Cell 2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Row 4 - Cell 2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Row 5 - Cell 2&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;That $600 price tag has a 6% discount already applied to its original $640 ask, as part of a Black Friday deal. For context, a more exclusive 64 GB limited edition Corsair Dominator Titanium kit cost only $349 when we reviewed it a few months ago. Earlier this year, we posted about DDR5 deals on Prime Day where the standard edition of the same kit was just $299, and you could get other comparable 64 GB kits for as low as $140.&lt;/p&gt;
    &lt;p&gt;A quick glance at price tracking data, and G.Skill's Trident Z5 Neo kit has regularly sat at $205-$220 for the past few months, and it was only in late October that it started to pick up steam. From September 20th when it was listed at $220, to $640 now. In just 2 months we've witnessed an astounding ~190% surge.&lt;/p&gt;
    &lt;p&gt;Right as this particular Trident Z5 Neo kit began to skyrocket in price was when the industry first started to pick up on the affects of the AI crunch. A few days later we published our initial coverage on DDR5 RAM price hikes; from there, the situation has only worsened to reach worrying levels.&lt;/p&gt;
    &lt;p&gt;Insane mark-up aside, the kit itself is one of the best on the market, recommend as the top pick for DDR5 memory in our roundup. Unfortunately, it seems like high prices are going to be the story going forward. The surge in demand for AI projects will see production lines will prioritizing serving AI clients, leaving consumers to pay through the nose or make the best of what they have. Experts speculate that both DRAM and NAND constraints will become normal throughout 2026 as Big Tech looks to pursue AGI.&lt;/p&gt;
    &lt;p&gt;In the meantime, hard drives are vanishing from store shelves to the point where microSD cards are serving as a feasible replacement for them. Large-capacity nearline HDDs are backordered for 2 years, as a result of which QLC SSDs are now being swept up at alarming rates. Many distributors are even selling memory and motherboards bundled together to combat the global shortage.&lt;/p&gt;
    &lt;p&gt;Even Valve's upcoming Steam Machine will end up costing more than expected due to the production window of the device aligning with the DRAM crisis. That being said, memory has almost always lived in a rollercoaster cycle, with manufacturers oversupplying for a couple of years, then undersupplying for the next few. Looking at it optimistically, you're probably going to find DDR5 at bargain prices again in 2027.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, &amp;amp; reviews in your feeds.&lt;/p&gt;
    &lt;p&gt;Hassam Nasir is a die-hard hardware enthusiast with years of experience as a tech editor and writer, focusing on detailed CPU comparisons and general hardware news. When he‚Äôs not working, you‚Äôll find him bending tubes for his ever-evolving custom water-loop gaming rig or benchmarking the latest CPUs and GPUs just for fun.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;vanadiel007&lt;/header&gt;Initially I did not believe this, but then I went looking and wow. Memory kit I got a month ago is now 300% more!Reply&lt;lb/&gt;I guess this is how AI is generating money: increasing the prices of electronics.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;kerberos_20&lt;/header&gt;mkay, so we are looking now on high end sideReply&lt;lb/&gt;why not brag abot 128GB sticks at only 1.5k price point?&lt;lb/&gt;https://www.corsair.com/us/en/p/memory/CMH128GX5M2B6400C42/vengeance-rgb-128gb-2x64gb-ddr5-dram-6400mts-cl42-intel-xmp-memory-kit-cmh128gx5m2b6400c42&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;thestryker&lt;/header&gt;Reply&lt;quote/&gt;First of all those are 64GB modules and secondly that kit debuted in June at less than $400.kerberos_20 said:mkay, so we are looking now on high end side&lt;lb/&gt;why not brag abot 128GB sticks at only 1.5k price point?&lt;lb/&gt;https://www.corsair.com/us/en/p/memory/CMH128GX5M2B6400C42/vengeance-rgb-128gb-2x64gb-ddr5-dram-6400mts-cl42-intel-xmp-memory-kit-cmh128gx5m2b6400c42&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;yahrightthere&lt;/header&gt;Want prices to come down, stop buying it, just say no, that includes consumer and enterprise.Reply&lt;lb/&gt;See who blinks first.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;mikethepoor&lt;/header&gt;Reply&lt;quote/&gt;That's honestly what manufacturers want, is for consumers to stop buying so they can focus on the much more lucrative enterprise market. Same thing happened to GPUs.yahrightthere said:Want prices to come down, stop buying it, just say no, that includes consumer and enterprise.&lt;lb/&gt;See who blinks first.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;yahrightthere&lt;/header&gt;Reply&lt;quote/&gt;I did include enterprise, they are part of the equation.mikethepoor said:That's honestly what manufacturers want, is for consumers to stop buying so they can focus on the much more lucrative enterprise market. Same thing happened to GPUs.&lt;lb/&gt;If they don't, talking for myself, I don't need their over valued high priced tech items in my life.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;thestryker&lt;/header&gt;Reply&lt;quote/&gt;Generally speaking enterprise can't just stop buying, and here they're the driver of the current costs to begin with. They need it to make more money so the increased costs are part of doing business as long as that money they're making is higher.yahrightthere said:I did include enterprise, they are part of the equation.&lt;lb/&gt;If they don't, talking for myself, I don't need their over valued high priced tech items in my life.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46038143</guid><pubDate>Mon, 24 Nov 2025 19:29:12 +0000</pubDate></item></channel></rss>