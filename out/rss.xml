<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 01 Oct 2025 09:11:45 +0000</lastBuildDate><item><title>An opinionated critique of Duolingo</title><link>https://isomorphism.xyz/blog/2025/duolingo/</link><description>&lt;doc fingerprint="ec79919e09a13ed2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;an opinionated critique of duolingo&lt;/head&gt;
    &lt;p&gt;During the stay-at-home grim days of 2020, I started learning Spanish on Duolingo. Having a working understanding of Spanish seemed like a sensible first step towards opening a taco truck in Mexico, in case I had to run away from my doctoral studies. This July, after about 5 years I decided to end the 1800 day streak that I managed to drag on with numerous streak freezes and minimal effort lessons. While Spanish words look less foreign, and with some focus, I am able to decipher small paragraphs of grammatically simple sentences; the effort was less than a smashing success ‚Äì I certainly could not be writing this essay in Spanish.&lt;/p&gt;
    &lt;p&gt;If Duolingo is known for anything, it has to be their gamification approach. There is no shortage of gamification mechanics on the platform: XP; potions to double your XP; leagues of gold, silver, and all sorts of other metals and minerals; treasure chests; quests; monthly quests; and so on. I have only ever paid little attention to these mechanics. While I still haven‚Äôt entirely rejected the idea that a good RPG could be a good scaffolding to teach a language, I do not think Duolingo is one.&lt;/p&gt;
    &lt;p&gt;Games worth their salt are not created by bolting together a collection of numerical statistics. That is how you get cookie clicker. I did not have a good understanding of how the mechanics work: if I learn 10 words, how many XP do I get for my hard work? Is the Diamond League higher or lower than the Obsidian League? I could have viewed their documentation to figure it out, but there was nothing motivating me to do so. If I collect 100 XP, what does it mean for my language skills? For that matter, why do I collect extra XP when I receive a potion? Can the XP I collect be used in a way to carefully guide me towards the specific language skills I would explore next? Navigating the mechanical gameplay of Duolingo is neither rewarding for its own sake, nor is it helpful towards actually learning a language.&lt;/p&gt;
    &lt;p&gt;Duolingo is not just a poor simulacrum of the mechanical aspects of a game, but also of the social aspects of one. Who are all these people I am on the Silver league with? Having a comparable amount of XP does not give me a sense of social connection with them. When I click a button to congratulate a friend on Duolingo, I do not truly engage with their learning journey. Indeed, it is worse than hearting an instagram photo, or upvoting a reddit thread. In those cases, I am reacting to a sliver of expression from my acquaintance. Here, I am presented only with a pre-rendered text with an abstract numerical statistic. Reacting to it is deliberately frictionless: I am presented with a wall of buttons allowing me to click them with ease and without thought. When Duolingo tells me that so and so sent me a message saying ‚ÄúHey, come back and learn Spanish with me!‚Äù, I don‚Äôt admire how thoughtful and encouraging my friend is; I just notice that they clicked a button to send me a pre-generated message.&lt;/p&gt;
    &lt;p&gt;Interactions on Duolingo were not always of the push-button variety. Duolingo had forums where users would discuss different aspects of their language learning journey. In fact, Duolingo would link each sentence to its own forum thread for discussion ‚Äì discussion, which was at the very least, helpful, and at times, eye-opening. At first, these discussion threads were locked, and later removed. My hypothesis is that for the business geniuses running Duolingo, the forums were assesed to be a liability, having to moderate which were not worth spending the dollars for. The nature of interacting with people ‚Äì friends or strangers, in person or online ‚Äì is that sometimes bad things would happen. Even when there is no abuse or harassment going on, there is always the risk that the other person might greet you with disagreement, or worse, apathy. Many people tend to think that the risks outweigh the benefits.&lt;/p&gt;
    &lt;p&gt;The gamification mechanic that I did latch on to was the Streak. I generally have been critical of the green owl, but I do think that it did help me form a good habit ‚Äì a net positive, despite the minuscule magnitude. Regular Duolingo users will know that the streak can be gamed away in more than one ways. Streak Freezes can be bought using gems (of which I happen to have 24,053 of, somehow) or be gifted by your friends, and equipped 2 at a time. Streaks wouldn‚Äôt have their social effect if there weren‚Äôt enough people with a moderate number of people with decent streaks to be sprinkled around. Maintaining the streak, even without freezes, does not have to mean that you are learning ‚Äì repeating a simple lesson from several units ago would work. My 1800 day streak didn‚Äôt mean that I spent 1800 days learning Spanish; it meant that I spent a large number of days engaging with the platform. I later started peeking into the Japanese and Finnish courses, and the 1800 day number includes them. If you loose interest in languages, Duolingo tells us that spending time with math or music will count towards your Streak.&lt;/p&gt;
    &lt;p&gt;The deficiency of Duolingo‚Äôs pedagogy was first made obvious to me by the excellent audio lessons produced by Language Transfer. Going through the first few lessons of Language Transfer, I was unfazed, observing that I had already learnt what was being taught. Soon, what shocked me was how quickly Language Transfer caught up to what I had managed to learn in a couple years of time. While Duolingo is great at making sure that the user comes back to the app everyday, their pedagogy is subpar. Remaining true to gamification, Duolingo prefers to throw users head-first into translation exercises. If you do not know a word, you hover over it and you arrange a given bag of words into a sentence that is hopefully meaningful. Grammar lessons are extremely minimal. The removal of the forums dedicated to the discussion of specific sentences did not help. Understanding the course outline ‚Äì knowing what is taught where, or reviewing lessons ‚Äì is not easy.&lt;/p&gt;
    &lt;p&gt;Supposedly, the Duolingo philosophy is that if you are exposed to enough sentences, you will eventually learn how to use them. I do believe this is true, and I do believe the exercises are indispensable. However, the whole process could be greatly improved by a few more lessons interspersed in the curriculum telling the student what is going on. To see this, I would ask myself, did I always internalize the grammatically correct structures even in my own mother tongue? I think not, and my language skills have improved when my parents or teachers would point out simple grammatical mistakes. Eggcorns are a closely related amusing phenomena.&lt;/p&gt;
    &lt;p&gt;I cannot tell if Duolingo repeats different concepts in exercises adaptively based on your mastery, or are simply fixed in the course material. Repetition is good for learning but Duolingo‚Äôs repetition can be frustrating. The platform‚Äôs interface is largely built around clicking a bag of jumbled words one at a time to input a translation. Once you learn a concept well enough, most of your mental energy is spent on the finding and clicking of the words rather than the translation. Thankfully, this situation can be made better by dictating your answer, as pointed out by the official blog.&lt;/p&gt;
    &lt;p&gt;Duolingo does include a few other formats for their lessons. There are some stories, which are interspersed in the course. The stories are short, but silly and enjoyable. There are also audio only lessons, which are also shorter and unfortunately, not as fun. From time to time, the regular lessons also ask you to speak to the microphone but in my experience, the audio recognition seems to accept the answer even if I mumble through the words. Duolingo is also known for its usage of bizarre phrases, whose shock value generates social media buzz and may or may not have a positive pedagogical effect.&lt;/p&gt;
    &lt;p&gt;Duolingo is a neat case study in Silicon Valley ideology. Big tech embraces blitz-scaling: the primary goal is neither financial sustainability nor the quality of materials but making the number of users grow. The faux gamification and passive-aggressive messaging may be helpful with little else, but is good for user retention. The expansionism does not stop at growing the number of users; Duolingo has decided that they must loop in music and math learners as well. As we have discussed, the maxim of friction reduction has guided them towards optimizing away authenticity in the user interactions on the platform.&lt;/p&gt;
    &lt;p&gt;In April 2025, Duolingo decided to go AI-first. Supposedly, ‚Äúto teach well, [they] need to create a massive amount of content‚Äù ‚Äì so much so that ‚Äúdoing [it] manually does not scale‚Äù. For the top ten languages, I cannot imagine any reasonable person saying that the lack of study material is the main obstacle towards learning. This statement spells out what the Duolingo executives value. The Duolingo CEO is not shy to admit it. In an interview with NPR, he said the following.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[I]f it‚Äôs our content, as in, like, our learning content, there‚Äôs so much of that - thousands and thousands and thousands of kind of sentences and words and paragraphs. That is mostly done by computers, and we probably spot-check it. But if it‚Äôs things like the user interface of Duolingo, where we say - like, you know, the button says quit, and we have to translate, that is all done with humans. And we spend a lot of effort on that, but that‚Äôs because each one of those is highly valuable.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Yes, the button that says ‚Äòquit‚Äô is more valuable than the learning material, which is only ‚Äòprobably‚Äô spot-checked.&lt;/p&gt;
    &lt;p&gt;After I moved to Japan, I dialed up my efforts to learn Japanese. For a while, I shifted over my Duolingo habits to Japanese. Because Duolingo wasn‚Äôt my only learning material for Japanese, it was glaringly obvious very soon that the Duolingo pedagogy is unhelpful and often misleading. While the Spanish learner has to introduce themselves to a few new concepts (e.g, ser vs estar, or reflexive verbs), the Japanese learner faces an explosion of differences. Japanese has a writing system with three components; generally uses a topic-comment structure and often omits the topic; has a subject-object-verb order; has adjectives which conjugate; a lot of counting suffixes; sentence ending particles and famously, a complex honorific system. Duolingo does not break its gamification fa√ßade to teach the user some of these concepts head-on. Instead, it pretends that translating between Japanese and English is a matter of substituting phrases and shuffling them around.&lt;/p&gt;
    &lt;p&gt;Since I gave up on my Duolingo streak, I have started exploring other avenues to continue learning Japanese. I participate in group lessons with a tutor once a week for an hour. Believe it or not, the tutor has more charm than Falstaff. I regularly do my flashcard kanji study with Wanikani. A newer addition to my study routine is Bunpro. My progress has been slow but evident: when I recognize that the names of the metro stations I frequent break down into simple words, they lose a little bit of their mystery but it is a satisfying revelation.&lt;/p&gt;
    &lt;p&gt;These platforms are a welcome contrast against the techno-accelerationist attitude of Duolingo. Instead of trying to do it all, they are extremely niche: they only teaching one language and Wanikani is focused at teaching a very specific element of it. Wanikani maintains a public API, which makes third-party apps and scripts possible. I praise them for their welcome attitude towards interoperability instead of trying to build a closed ecosystem. Both Wanikani and Bunpro have vibrant user forums. Bunpro makes actual lessons part of their critical path, instead of hoping that the user will eventually figure it out. When a Bunpro user feels that their lesson was not adequate, they do not have to rely on AI generated slop ‚Äì Bunpro directs users to carefully-crafted lessons by other people (see the ‚Äòresource‚Äô section at the end of this page, for example).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45425061</guid><pubDate>Tue, 30 Sep 2025 13:19:53 +0000</pubDate></item><item><title>Software essays that shaped me</title><link>https://refactoringenglish.com/blog/software-essays-that-shaped-me/</link><description>&lt;doc fingerprint="355f991b6f272c96"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Software Essays that Shaped Me&lt;/head&gt;
    &lt;p&gt;I started reading software blogs before I got my first programming job 20 years ago. At this point, I‚Äôve read thousands of blog posts and essays about software, but only a small handful stuck in my mind and changed the way I think.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;‚ÄúThe Joel Test: 12 Steps to Better Code‚Äù by Joel Spolsky (2000)&lt;/item&gt;
      &lt;item&gt;‚ÄúParse, don‚Äôt validate‚Äù by Alexis King (2019)&lt;/item&gt;
      &lt;item&gt;‚ÄúNo Silver Bullet - Essence and Accident in Software Engineering‚Äù by Fred Brooks (1986)&lt;/item&gt;
      &lt;item&gt;‚ÄúChoices‚Äù by Joel Spolsky (2000)&lt;/item&gt;
      &lt;item&gt;‚ÄúApplication compatibility layers are there for the customer, not for the program‚Äù by Raymond Chen (2010)&lt;/item&gt;
      &lt;item&gt;‚ÄúDon‚Äôt Put Logic in Tests‚Äù by Erik Kuefler (2014)&lt;/item&gt;
      &lt;item&gt;‚ÄúA little bit of plain Javascript can do a lot‚Äù by Julia Evans (2020)&lt;/item&gt;
      &lt;item&gt;‚ÄúChoose Boring Technology‚Äù by Dan McKinley (2015)&lt;/item&gt;
      &lt;item&gt;‚ÄúI‚Äôve locked myself out of my digital life‚Äù by Terence Eden (2022)&lt;/item&gt;
      &lt;item&gt;Bonus: Brad Fitzpatrick on parsing user input (2009)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;‚ÄúThe Joel Test: 12 Steps to Better Code‚Äù by Joel Spolsky (2000)üîó&lt;/head&gt;
    &lt;p&gt;Joel Spolsky is the greatest software blogger of all time. His essays have informed so much of my approach to software that it was hard to pick out just one, but ‚ÄúThe Joel Test‚Äù is my favorite.&lt;/p&gt;
    &lt;p&gt;The Joel Test is a set of 12 questions that employers can ask themselves to see how well they‚Äôre investing in their software team:&lt;/p&gt;
    &lt;quote&gt;
      &lt;item&gt;Do you use source control?&lt;/item&gt;
      &lt;item&gt;Can you make a build in one step?&lt;/item&gt;
      &lt;item&gt;Do you make daily builds?&lt;/item&gt;
      &lt;item&gt;Do you have a bug database?&lt;/item&gt;
      &lt;item&gt;Do you fix bugs before writing new code?&lt;/item&gt;
      &lt;item&gt;Do you have an up-to-date schedule?&lt;/item&gt;
      &lt;item&gt;Do you have a spec?&lt;/item&gt;
      &lt;item&gt;Do programmers have quiet working conditions?&lt;/item&gt;
      &lt;item&gt;Do you use the best tools money can buy?&lt;/item&gt;
      &lt;item&gt;Do you have testers?&lt;/item&gt;
      &lt;item&gt;Do new candidates write code during their interview?&lt;/item&gt;
      &lt;item&gt;Do you do hallway usability testing?&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;Some of the questions are dated, but the point was never the questions themselves but rather the meta-point of the questions.&lt;/p&gt;
    &lt;p&gt;Joel was really asking employers: do you respect developers?&lt;/p&gt;
    &lt;p&gt;The questions all assess whether an employer prioritizes their developers‚Äô time and focus over things like cheap office space and short-term deadlines.&lt;/p&gt;
    &lt;p&gt;Joel published this article at the height of the dot-com boom, when skilled developers were a precious resource, but not everyone realized it, including developers themselves.&lt;/p&gt;
    &lt;p&gt;Joel‚Äôs blog always presented programmers as rare, delicate geniuses that employers needed to pursue and pamper. I liked that.&lt;/p&gt;
    &lt;p&gt;Throughout my career, I sought out employers that scored well on the Joel test, and I‚Äôm grateful to Joel for giving me the map to find them.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúParse, don‚Äôt validate‚Äù by Alexis King (2019)üîó&lt;/head&gt;
    &lt;p&gt;This essay is about leveraging the type system in Haskell to ‚Äî wait, wait! Don‚Äôt go to sleep.&lt;/p&gt;
    &lt;p&gt;If you don‚Äôt care about type systems or Haskell, I get it. I don‚Äôt either. But this essay radically changed the way I think about software. You can use Alexis‚Äô technique outside of Haskell in any language that supports static types (e.g., Go, C++, Rust).&lt;/p&gt;
    &lt;p&gt;The highly abridged version of the essay is that whenever you validate any data, you should convert it to a new type.&lt;/p&gt;
    &lt;p&gt;Suppose that your app has a rule limiting usernames to a maximum of 20 alphanumeric characters. The na√Øve solution would be to define a function that looks like this:&lt;/p&gt;
    &lt;code&gt;func validateUsername(username string) error { ... }
&lt;/code&gt;
    &lt;p&gt;With the above function, you run &lt;code&gt;validateUsername&lt;/code&gt; anytime you receive a username from a user.&lt;/p&gt;
    &lt;p&gt;The problem with this approach is that your code is unsafe by default. You have to remember to validate every username you receive, so it‚Äôs easy to create a code path that accidentally processes a username without validating it. And if a nefarious user notices the mistake, they can do tricky things like embed malicious code in the username field or stuff it with a billion characters to fill up your database.&lt;/p&gt;
    &lt;p&gt;Alexis‚Äô solution is to instead use a function like this:&lt;/p&gt;
    &lt;code&gt;func parseUsername(raw string) (Username, error) { ... }
&lt;/code&gt;
    &lt;p&gt;In the rest of your codebase, instead of passing around a &lt;code&gt;string&lt;/code&gt; called ‚Äúusername,‚Äù you use a custom type: &lt;code&gt;Username&lt;/code&gt;. The only function that can create a &lt;code&gt;Username&lt;/code&gt; is &lt;code&gt;parseUsername&lt;/code&gt;, and it applies validation rules before returning a &lt;code&gt;Username&lt;/code&gt; instance.&lt;/p&gt;
    &lt;p&gt;Therefore, if you have a &lt;code&gt;Username&lt;/code&gt; instance, it must contain a valid username. Otherwise, it couldn‚Äôt exist.&lt;/p&gt;
    &lt;p&gt;You can‚Äôt forget to validate a username because untrusted input will always be a &lt;code&gt;string&lt;/code&gt;, and you can‚Äôt pass a &lt;code&gt;string&lt;/code&gt; to a function that expects a &lt;code&gt;Username&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Before Alexis‚Äô essay, I thought type systems were just a fun way to distract language nerds. ‚ÄúParse, don‚Äôt validate‚Äù opened my eyes to how valuable compiler features can be in improving an application‚Äôs security and reliability.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúNo Silver Bullet - Essence and Accident in Software Engineering‚Äù by Fred Brooks (1986)üîó&lt;/head&gt;
    &lt;p&gt;In college, I read The Mythical Man-Month, a collection of essays about software engineering by Fred Brooks, drawing on his experience directing IBM‚Äôs OS/360 project.&lt;/p&gt;
    &lt;p&gt;The essays were hit or miss. Some felt too old to be relevant, even in 2002, but the one that stuck with me was, ‚ÄúNo Silver Bullet.‚Äù&lt;/p&gt;
    &lt;p&gt;The essay argues that you can divide software work into two categories: essential complexity and accidental complexity.&lt;/p&gt;
    &lt;p&gt;Essential complexity is the work that you absolutely have to do, regardless of your tooling and hardware. For example, if you write software that calculates bonuses for salespeople, you have to define formulas for those bonuses and cover all possible edge cases. This work is the same if you have a $5B supercomputer or a $1 microcontroller.&lt;/p&gt;
    &lt;p&gt;Accidental complexity is everything else: dealing with memory leaks, waiting for your code to compile, figuring out how to use a third-party library. The better your tooling and hardware resources, the less time you spend on accidental complexity.&lt;/p&gt;
    &lt;p&gt;Given this model, Brooks concluded that it was impossible for any advancement in tooling or hardware to create a 10x improvement in developer productivity:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;How much of what software engineers now do is still devoted to the accidental, as opposed to the essential? Unless it is more than 9/10 of all effort, shrinking all the accidental activities to zero time will not give an order of magnitude improvement.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Throughout my career, people have been trying to find ways to eliminate programmers from software. For a few years, no-code platforms generated buzz by promising non-programmers all the powers of a seasoned web developer.&lt;/p&gt;
    &lt;p&gt;Brooks‚Äô essay always reassured me that the latest buzzword platforms could never replace developers, as the platforms focused on the accidental, not the essential. Even if the platforms could magically create working code from a functional specification, you still need someone to write the spec:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I believe the hard part of building software to be the specification, design, and testing of this conceptual construct, not the labor of representing it and testing the fidelity of the representation.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Modern AI has thrown a wrench into Brooks‚Äô theory, as it actually does reduce essential complexity. You can hand AI an incomplete or contradictory specification, and the AI will fill in the gaps by cribbing from similar specifications.&lt;/p&gt;
    &lt;p&gt;Even if AI eliminates programming as we know it, Brooks‚Äô essay gives me hope that we‚Äôll still need people to manage essential complexity at whatever level of abstraction that ends up being.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúChoices‚Äù by Joel Spolsky (2000)üîó&lt;/head&gt;
    &lt;p&gt;I said above that it was hard to pick a single favorite Joel Spolsky essay, which is why I‚Äôve chosen two.&lt;/p&gt;
    &lt;p&gt;‚ÄúChoices‚Äù is about creating user interfaces and the subtle costs of giving a user power:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Every time you provide an option, you‚Äôre asking the user to make a decision. That means they will have to think about something and decide about it. It‚Äôs not necessarily a bad thing, but, in general, you should always try to minimize the number of decisions that people have to make.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;As an example, Joel shares a ridiculous dialog that appears in Windows 98 when you try to search the help documentation:&lt;/p&gt;
    &lt;p&gt;The dialog infuriates Joel because it interrupts the user while they‚Äôre trying to get help, and it asks them to make an uninformed about database optimization. Windows was shirking a decision and pushing it onto the user.&lt;/p&gt;
    &lt;p&gt;Joel‚Äôs essay focuses on graphical user interfaces, but I think about it wherever people might encounter my code, including on the command-line or other developers calling functions I wrote. Can I make a useful decision on my user‚Äôs behalf while still giving them power over things they care about? There are countless times where Joel‚Äôs essay has saved me from pushing a decision onto the user that I could make myself.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúApplication compatibility layers are there for the customer, not for the program‚Äù by Raymond Chen (2010)üîó&lt;/head&gt;
    &lt;p&gt;Raymond Chen is one of the longest-serving developers on the Microsoft Windows team. His blog has thousands of informative, entertaining stories about the history of Windows programming, but the one I think back to most is one about compatibility mode in Windows Vista.&lt;/p&gt;
    &lt;p&gt;A customer had contacted Raymond‚Äôs team with this request:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Hi, we have a program that was originally designed for Windows XP and Windows Server 2003, but we found that it runs into difficulties on Windows Vista. We‚Äôve found that if we set the program into Windows XP compatibility mode, then the program runs fine on Windows Vista. What changes do we need to make to our installer so that when the user runs it on Windows Vista, it automatically runs in Windows XP compatibility mode?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Raymond proceeds to characterize the customer‚Äôs request as follows:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I normally toss my garbage on the sidewalk in front of the pet store, and every morning, when they open up, somebody sweeps up the garbage and tosses it into the trash. But the pet store isn‚Äôt open on Sundays, so on Sundays, the garbage just sits there. How can I get the pet store to open on Sundays, too?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I loved this analogy. The metaphor was so funny that I didn‚Äôt realize until just now that Raymond is in the wrong. He‚Äôs making fun of a developer whose sin is expecting Windows not to break their app after a single release.&lt;/p&gt;
    &lt;p&gt;But as is the case with a lot of Raymond Chen‚Äôs writing, it‚Äôs so funny and sharp that I can look past the flaws.&lt;/p&gt;
    &lt;p&gt;Even though I disagree with the specifics, Raymond‚Äôs post is an excellent lesson in influencing user behavior.&lt;/p&gt;
    &lt;p&gt;If you want to nudge the user to do something that helps you, think carefully about the path of least resistance from the user‚Äôs perspective, because that‚Äôs the path they‚Äôll take.&lt;/p&gt;
    &lt;p&gt;If you show the user that dumping garbage on the sidewalk completely solves their problem, they‚Äôre going to keep dumping their garbage on the sidewalk.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúDon‚Äôt Put Logic in Tests‚Äù by Erik Kuefler (2014)üîó&lt;/head&gt;
    &lt;p&gt;I‚Äôve always loved unit testing and took great pride in my test code. That‚Äôs why I was so horrified when this essay appeared in my bathroom and revealed that I‚Äôd been writing awful tests my whole career.&lt;/p&gt;
    &lt;p&gt;Erik‚Äôs essay shows the following unit test, which has a subtle bug:&lt;/p&gt;
    &lt;code&gt;@Test public void shouldNavigateToPhotosPage() {
  String baseUrl = "http://plus.google.com/";
  Navigator nav = new Navigator(baseUrl);
  nav.goToPhotosPage();
  assertEquals(baseUrl + "/u/0/photos", nav.getCurrentUrl());
}
&lt;/code&gt;
    &lt;p&gt;When I first read the essay, I thought, ‚ÄúThat‚Äôs exactly how I write unit tests!‚Äù&lt;/p&gt;
    &lt;p&gt;Why duplicate the &lt;code&gt;http://plus.google.com/&lt;/code&gt; string in two places? Create a single source of truth, just like in production code. I did this all the time, adding helper functions, variables, and loops to eliminate redundancy from my tests.&lt;/p&gt;
    &lt;p&gt;The problem with the approach above is that it masks a subtle bug. It‚Äôs actually asserting that the URL looks like this:&lt;/p&gt;
    &lt;code&gt;http://plus.google.com//u/0/photos
                      ^^
                    whoops
&lt;/code&gt;
    &lt;p&gt;Erik‚Äôs essay made me realize that I shouldn‚Äôt treat test code like production code at all. The two have completely different goals and constraints.&lt;/p&gt;
    &lt;p&gt;Good test code should be, above all, clear. Test code doesn‚Äôt have its own test code, so the only way to verify correctness is by inspection. A test should make it blindingly obvious to the reader what behavior it asserts. In service of that goal, you can accept redundancy to reduce complexity.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúA little bit of plain Javascript can do a lot‚Äù by Julia Evans (2020)üîó&lt;/head&gt;
    &lt;p&gt;As a software engineer, I was embarrassingly late to the web. For the first 10 years of my career, I only wrote code for desktop apps and backend servers. I never bothered with HTML or JavaScript until 2017.&lt;/p&gt;
    &lt;p&gt;By the time I got serious about learning frontend development, my impression was that JavaScript was a mess of a language, hacked together in 10 days, and it had drastically different behavior in different browsers. If I was going to write web apps, I wanted something modern and sleek to protect me from all of JavaScript‚Äôs bile and warts.&lt;/p&gt;
    &lt;p&gt;So, I tried the popular web frameworks of the day: Angular, React, and Vue. I learned enough Vue to make my way around, but I was still spending an enormous amount of my time on dependency issues and framework gotchas. After all the work these frontend frameworks did to fix JavaScript, web programming still sucked.&lt;/p&gt;
    &lt;p&gt;Then, I read Julia‚Äôs essay, and I realized I‚Äôd been so confident that JavaScript needed fixing that I never gave it a chance.&lt;/p&gt;
    &lt;p&gt;At the time, I was working on the prototype of TinyPilot, which would become my first commercially successful software product. TinyPilot had a web interface that I was planning to implement with Vue, but Julia‚Äôs essay inspired me to see how far I could go with plain JavaScript. No framework, no wrapper libraries, no build step, no Node.js, just regular old JavaScript. Okay, not ‚Äúold‚Äù ‚Äî more like ES2018, but you know.&lt;/p&gt;
    &lt;p&gt;I kept expecting to hit some problem where I‚Äôd need to switch to some kind of framework or builder, but it never happened. There were still some gotchas, especially around WebComponents, but it was nothing compared to the suffering I endured with Vue and Angular.&lt;/p&gt;
    &lt;p&gt;I loved being free of the frameworks. When I had a runtime error, the stack trace wasn‚Äôt some minified, transmogrified, tree-shakified fever dream of my code. I was debugging my code, exactly as I wrote it. Why hadn‚Äôt I tried this sooner?&lt;/p&gt;
    &lt;p&gt;My biases about JavaScript were wrong. Modern JavaScript is pretty nice. It absorbed a lot of ideas from wrapper libraries, so now you don‚Äôt need the wrappers. And browsers got their act together to ensure consistent behavior across platforms and devices.&lt;/p&gt;
    &lt;p&gt;I haven‚Äôt integrated a JavaScript framework or build step into any new project since 2020, and I‚Äôve never looked back. Plain JavaScript gets me 90% of the benefit of frameworks with 5% of the headache.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúChoose Boring Technology‚Äù by Dan McKinley (2015)üîó&lt;/head&gt;
    &lt;p&gt;This is an odd essay to include in this list because I‚Äôve never actually read it.&lt;/p&gt;
    &lt;p&gt;People have quoted this essay to me, and once I understood the idea, it felt so intuitive that I didn‚Äôt need to read it. In my interview with CoRecursive podcast host Adam Gordon Bell, he talked about how there are certain non-fiction books where, once you understand the idea, all you need is the title. ‚ÄúChoose Boring Technology‚Äù is that for me.&lt;/p&gt;
    &lt;p&gt;Dan‚Äôs argument is that when you start a new project, you‚Äôre tempted to use cutting-edge technology that has lots of buzz. Google just announced a new database that scales to exabytes, and it‚Äôs 40% faster than Postgres at 20% the cost. You‚Äôd be an idiot to use Postgres when this sexy new alternative is right there!&lt;/p&gt;
    &lt;p&gt;In practice, the new technology has bugs and weaknesses, but they‚Äôre not obvious to you yet; they‚Äôre not obvious to anyone yet. So, when you run into them, you‚Äôre stuck. Postgres has its issues, but after 30 years in the field, it has battle-tested solutions for any problem you‚Äôre likely to encounter.&lt;/p&gt;
    &lt;p&gt;Dan concedes that you should use new technologies sometimes but only strategically and in limited quantities. He suggests that every business gets three ‚Äúinnovation tokens‚Äù to spend. If you want a flashy new database, you‚Äôll have to spend one of your tokens.&lt;/p&gt;
    &lt;p&gt;Dan‚Äôs essay dovetails naturally with Julia‚Äôs essay. I wish I‚Äôd read either of them before I wasted all that time with frontend frameworks.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúI‚Äôve locked myself out of my digital life‚Äù by Terence Eden (2022)üîó&lt;/head&gt;
    &lt;p&gt;Terence Eden is a delightful and eclectic technology blogger. He writes several new posts each week, but the one that impacted me the most was ‚ÄúI‚Äôve locked myself out of my digital life.‚Äù&lt;/p&gt;
    &lt;p&gt;The article plays out what would happen if lightning struck Terence‚Äôs house and destroyed all of his possessions. He keeps his passwords to everything in a password manager, but if all his devices get destroyed, he can‚Äôt access his password manager. And he can‚Äôt fall back to hardware passkeys because those were in his house, too.&lt;/p&gt;
    &lt;p&gt;I always felt like I was pretty safe about my data because I store everything on redundant drives, and I have offsite backups on three continents with two vendors.&lt;/p&gt;
    &lt;p&gt;Terence‚Äôs post got me thinking about the many credible threats that could wipe out all of my devices simultaneously: fire, flood, electrical surge, criminal investigation. All of my data is encrypted with passwords that live in my head, so add to that list memory loss, incapacitation, or death.&lt;/p&gt;
    &lt;p&gt;Online services are bad at helping users recover from disaster. I use several services that assume it‚Äôs impossible for me to ever lose my phone, let alone my email account and every digital device in my possession.&lt;/p&gt;
    &lt;p&gt;Ever since I read Terence‚Äôs essay, I‚Äôve been thinking more about which services and devices are critical to me, and how I could recover from a scenario like the one Terence described. The next time I bought a laptop, I set it up at the library to test whether I could recover access to my password manager and critical accounts without any of the devices in my house.&lt;/p&gt;
    &lt;p&gt;I still could do a better job at digital disaster preparedness, but Terence‚Äôs post always echoes in my head whenever I think about how to secure my devices and data. What if everything was suddenly destroyed?&lt;/p&gt;
    &lt;head rend="h2"&gt;Bonus: Brad Fitzpatrick on parsing user input (2009)üîó&lt;/head&gt;
    &lt;p&gt;It‚Äôs technically not an essay, but there‚Äôs a quote from a software interview I constantly think about.&lt;/p&gt;
    &lt;p&gt;In 2009, as a result of Joel Spolsky‚Äôs gushing review, (yes, again with the Joel), I read Coders at Work, a collection of interviews with accomplished programmers.&lt;/p&gt;
    &lt;p&gt;Brad Fitzpatrick, creator of LiveJournal and Memcached, appears in the book as one of the interviewees. He was only 28 years old at the time, the youngest programmer in the book and also the sweariest and most entertaining.&lt;/p&gt;
    &lt;p&gt;In response to a question about ethics in software engineering, Brad goes on an impassioned rant about input validation:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I would like to ask that everyone is consistent on their credit-card forms to like let me put in fucking spaces or hypens. Computers are good at removing that shit. Like don‚Äôt tell me how to format my numbers.&lt;/p&gt;
      &lt;p&gt;-Brad Fitzpatrick, in Coders at Work&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I think back to this quote whenever I try to paste a phone number into a web form, and it whines that parentheses or spaces aren‚Äôt allowed. Or worse, it truncates my phone number because of the parentheses, and also complains that parentheses aren‚Äôt allowed.&lt;/p&gt;
    &lt;p&gt;Whenever I create input fields in my software and think about unexpected characters, I hear Brad Fitzpatrick say, ‚ÄúComputers are good at removing that shit.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45425568</guid><pubDate>Tue, 30 Sep 2025 14:01:53 +0000</pubDate></item><item><title>Kagi News</title><link>https://blog.kagi.com/kagi-news</link><description>&lt;doc fingerprint="b500b3a787eb238f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing Kagi News&lt;/head&gt;
    &lt;p&gt;A comprehensive daily press review with global news. Fully private, with sources openly curated by our community.&lt;/p&gt;
    &lt;p&gt;News is broken. We all know it, but we‚Äôve somehow accepted it as inevitable. The endless notifications. The clickbait headlines designed to trigger rather than inform, driven by relentless ad monetization. The exhausting cycle of checking multiple apps throughout the day, only to feel more anxious and less informed than when we started. This isn‚Äôt what news was supposed to be. We can do better, and create what news should have been all along: pure, essential information that respects your intelligence and time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our approach: Signal over noise&lt;/head&gt;
    &lt;p&gt;Kagi News operates on a simple principle: understanding the world requires hearing from the world. Every day, our system reads thousands of community curated RSS feeds from publications across different viewpoints and perspectives. We then use AI to distill this massive information into one comprehensive daily briefing, while clearly citing sources.&lt;/p&gt;
    &lt;p&gt;We strive for diversity and transparency of resources and welcome your contributions to widen perspectives. This multi-source approach helps reveal the full picture beyond any single viewpoint.&lt;/p&gt;
    &lt;head rend="h2"&gt;Design principles that put readers first&lt;/head&gt;
    &lt;p&gt;One daily update: We publish once per day around noon UTC, creating a natural endpoint to news consumption. This is a deliberate design choice that turns news from an endless habit into a contained ritual.&lt;/p&gt;
    &lt;p&gt;Five-minute complete understanding: Our briefings cover everything important in just five minutes. No endless scrolling. No attention hijacking. You read, understand, and move on with your day.&lt;/p&gt;
    &lt;p&gt;Diversity over echo chambers: Rather than personalizing feeds to match existing preferences, we expose readers to the full spectrum of global perspectives. This approach breaks down information silos instead of reinforcing them.&lt;/p&gt;
    &lt;p&gt;Privacy by design: Your reading habits belong to you. We don‚Äôt track, profile, or monetize your attention. You remain the customer and not the product.&lt;/p&gt;
    &lt;p&gt;Community-driven sources: Our news sources are open source and community-curated through our public GitHub repository. Anyone can propose additions, flag problems, or suggest improvements.&lt;/p&gt;
    &lt;p&gt;Customizable: In your settings, you can select and reorder categories to match your interests and priorities. You can also adjust the number of stories shown, as well as dragging to re-order various sections, so that your briefing is focused on the depth and topics that matter most to you.&lt;/p&gt;
    &lt;p&gt;News in your language: You can choose your preferred interface and content language. News stories are generated in their original source language, and then translated using Kagi Translate. The default mode shows regional stories in their original language without translation, and all other ones in your browser‚Äôs language.&lt;/p&gt;
    &lt;head rend="h2"&gt;Technical implementation that respects publishers&lt;/head&gt;
    &lt;p&gt;We don‚Äôt scrape content from websites. Instead, we use publicly available RSS feeds that publishers choose to provide. Publishers decide what content appears in their feeds; some include full articles, others only titles or summaries. We respect those choices completely. We‚Äôre working within the ecosystem publishers have created rather than circumventing their intentions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ready to experience news differently?&lt;/head&gt;
    &lt;p&gt;If you‚Äôre tired of news that makes you feel worse about the world while teaching you less about it, we invite you to try a different approach with Kagi News, so download it today:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45426490</guid><pubDate>Tue, 30 Sep 2025 15:09:00 +0000</pubDate></item><item><title>Designing agentic loops</title><link>https://simonwillison.net/2025/Sep/30/designing-agentic-loops/</link><description>&lt;doc fingerprint="b540585d0af512bf"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Designing agentic loops&lt;/head&gt;
    &lt;p&gt;30th September 2025&lt;/p&gt;
    &lt;p&gt;Coding agents like Anthropic‚Äôs Claude Code and OpenAI‚Äôs Codex CLI represent a genuine step change in how useful LLMs can be for producing working code. These agents can now directly exercise the code they are writing, correct errors, dig through existing implementation details, and even run experiments to find effective code solutions to problems.&lt;/p&gt;
    &lt;p&gt;As is so often the case with modern AI, there is a great deal of depth involved in unlocking the full potential of these new tools.&lt;/p&gt;
    &lt;p&gt;A critical new skill to develop is designing agentic loops.&lt;/p&gt;
    &lt;p&gt;One way to think about coding agents is that they are brute force tools for finding solutions to coding problems. If you can reduce your problem to a clear goal and a set of tools that can iterate towards that goal a coding agent can often brute force its way to an effective solution.&lt;/p&gt;
    &lt;p&gt;My preferred definition of an LLM agent is something that runs tools in a loop to achieve a goal. The art of using them well is to carefully design the tools and loop for them to use.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The joy of YOLO mode&lt;/item&gt;
      &lt;item&gt;Picking the right tools for the loop&lt;/item&gt;
      &lt;item&gt;Issuing tightly scoped credentials&lt;/item&gt;
      &lt;item&gt;When to design an agentic loop&lt;/item&gt;
      &lt;item&gt;This is still a very fresh area&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The joy of YOLO mode&lt;/head&gt;
    &lt;p&gt;Agents are inherently dangerous‚Äîthey can make poor decisions or fall victim to malicious prompt injection attacks, either of which can result in harmful results from tool calls. Since the most powerful coding agent tool is ‚Äúrun this command in the shell‚Äù a rogue agent can do anything that you could do by running a command yourself.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;An AI agent is an LLM wrecking its environment in a loop.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Coding agents like Claude Code counter this by defaulting to asking you for approval of almost every command that they run.&lt;/p&gt;
    &lt;p&gt;This is kind of tedious, but more importantly, it dramatically reduces their effectiveness at solving problems through brute force.&lt;/p&gt;
    &lt;p&gt;Each of these tools provides its own version of what I like to call YOLO mode, where everything gets approved by default.&lt;/p&gt;
    &lt;p&gt;This is so dangerous, but it‚Äôs also key to getting the most productive results!&lt;/p&gt;
    &lt;p&gt;Here are three key risks to consider from unattended YOLO mode.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Bad shell commands deleting or mangling things you care about.&lt;/item&gt;
      &lt;item&gt;Exfiltration attacks where something steals files or data visible to the agent‚Äîsource code or secrets held in environment variables are particularly vulnerable here.&lt;/item&gt;
      &lt;item&gt;Attacks that use your machine as a proxy to attack another target‚Äîfor DDoS or to disguise the source of other hacking attacks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you want to run YOLO mode anyway, you have a few options:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Run your agent in a secure sandbox that restricts the files and secrets it can access and the network connections it can make.&lt;/item&gt;
      &lt;item&gt;Use someone else‚Äôs computer. That way if your agent goes rogue, there‚Äôs only so much damage they can do, including wasting someone else‚Äôs CPU cycles.&lt;/item&gt;
      &lt;item&gt;Take a risk! Try to avoid exposing it to potential sources of malicious instructions and hope you catch any mistakes before they cause any damage.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Most people choose option 3.&lt;/p&gt;
    &lt;p&gt;Despite the existence of container escapes I think option 1 using Docker or the new Apple container tool is a reasonable risk to accept for most people.&lt;/p&gt;
    &lt;p&gt;Option 2 is my favorite. I like to use GitHub Codespaces for this‚Äîit provides a full container environment on-demand that‚Äôs accessible through your browser and has a generous free tier too. If anything goes wrong it‚Äôs a Microsoft Azure machine somewhere that‚Äôs burning CPU and the worst that can happen is code you checked out into the environment might be exfiltrated by an attacker, or bad code might be pushed to the attached GitHub repository.&lt;/p&gt;
    &lt;p&gt;There are plenty of other agent-like tools that run code on other people‚Äôs computers. Code Interpreter mode in both ChatGPT and Claude can go a surprisingly long way here. I‚Äôve also had a lot of success (ab)using OpenAI‚Äôs Codex Cloud.&lt;/p&gt;
    &lt;p&gt;Coding agents themselves implement various levels of sandboxing, but so far I‚Äôve not seen convincing enough documentation of these to trust them.&lt;/p&gt;
    &lt;p&gt;Update: It turns out Anthropic have their own documentation on Safe YOLO mode for Claude Code which says:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Letting Claude run arbitrary commands is risky and can result in data loss, system corruption, or even data exfiltration (e.g., via prompt injection attacks). To minimize these risks, use&lt;/p&gt;&lt;code&gt;--dangerously-skip-permissions&lt;/code&gt;in a container without internet access. You can follow this reference implementation using Docker Dev Containers.&lt;/quote&gt;
    &lt;p&gt;Locking internet access down to a list of trusted hosts is a great way to prevent exfiltration attacks from stealing your private source code.&lt;/p&gt;
    &lt;head rend="h4"&gt;Picking the right tools for the loop&lt;/head&gt;
    &lt;p&gt;Now that we‚Äôve found a safe (enough) way to run in YOLO mode, the next step is to decide which tools we need to make available to the coding agent.&lt;/p&gt;
    &lt;p&gt;You can bring MCP into the mix at this point, but I find it‚Äôs usually more productive to think in terms of shell commands instead. Coding agents are really good at running shell commands!&lt;/p&gt;
    &lt;p&gt;If your environment allows them the necessary network access, they can also pull down additional packages from NPM and PyPI and similar. Ensuring your agent runs in an environment where random package installs don‚Äôt break things on your main computer is an important consideration as well!&lt;/p&gt;
    &lt;p&gt;Rather than leaning on MCP, I like to create an AGENTS.md (or equivalent) file with details of packages I think they may need to use.&lt;/p&gt;
    &lt;p&gt;For a project that involved taking screenshots of various websites I installed my own shot-scraper CLI tool and dropped the following in &lt;code&gt;AGENTS.md&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;To take a screenshot, run:

shot-scraper http://www.example.com/ -w 800 -o example.jpg
&lt;/code&gt;
    &lt;p&gt;Just that one example is enough for the agent to guess how to swap out the URL and filename for other screenshots.&lt;/p&gt;
    &lt;p&gt;Good LLMs already know how to use a bewildering array of existing tools. If you say "use playwright python" or "use ffmpeg" most models will use those effectively‚Äîand since they‚Äôre running in a loop they can usually recover from mistakes they make at first and figure out the right incantations without extra guidance.&lt;/p&gt;
    &lt;head rend="h4"&gt;Issuing tightly scoped credentials&lt;/head&gt;
    &lt;p&gt;In addition to exposing the right commands, we also need to consider what credentials we should expose to those commands.&lt;/p&gt;
    &lt;p&gt;Ideally we wouldn‚Äôt need any credentials at all‚Äîplenty of work can be done without signing into anything or providing an API key‚Äîbut certain problems will require authenticated access.&lt;/p&gt;
    &lt;p&gt;This is a deep topic in itself, but I have two key recommendations here:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Try to provide credentials to test or staging environments where any damage can be well contained.&lt;/item&gt;
      &lt;item&gt;If a credential can spend money, set a tight budget limit.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I‚Äôll use an example to illustrate. A while ago I was investigating slow cold start times for a scale-to-zero application I was running on Fly.io.&lt;/p&gt;
    &lt;p&gt;I realized I could work a lot faster if I gave Claude Code the ability to directly edit Dockerfiles, deploy them to a Fly account and measure how long they took to launch.&lt;/p&gt;
    &lt;p&gt;Fly allows you to create organizations, and you can set a budget limit for those organizations and issue a Fly API key that can only create or modify apps within that organization...&lt;/p&gt;
    &lt;p&gt;So I created a dedicated organization for just this one investigation, set a $5 budget, issued an API key and set Claude Code loose on it!&lt;/p&gt;
    &lt;p&gt;In that particular case the results weren‚Äôt useful enough to describe in more detail, but this was the project where I first realized that ‚Äúdesigning an agentic loop‚Äù was an important skill to develop.&lt;/p&gt;
    &lt;head rend="h4"&gt;When to design an agentic loop&lt;/head&gt;
    &lt;p&gt;Not every problem responds well to this pattern of working. The thing to look out for here are problems with clear success criteria where finding a good solution is likely to involve (potentially slightly tedious) trial and error.&lt;/p&gt;
    &lt;p&gt;Any time you find yourself thinking ‚Äúugh, I‚Äôm going to have to try a lot of variations here‚Äù is a strong signal that an agentic loop might be worth trying!&lt;/p&gt;
    &lt;p&gt;A few examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Debugging: a test is failing and you need to investigate the root cause. Coding agents that can already run your tests can likely do this without any extra setup.&lt;/item&gt;
      &lt;item&gt;Performance optimization: this SQL query is too slow, would adding an index help? Have your agent benchmark the query and then add and drop indexes (in an isolated development environment!) to measure their impact.&lt;/item&gt;
      &lt;item&gt;Upgrading dependencies: you‚Äôve fallen behind on a bunch of dependency upgrades? If your test suite is solid an agentic loop can upgrade them all for you and make any minor updates needed to reflect breaking changes. Make sure a copy of the relevant release notes is available, or that the agent knows where to find them itself.&lt;/item&gt;
      &lt;item&gt;Optimizing container sizes: Docker container feeling uncomfortably large? Have your agent try different base images and iterate on the Dockerfile to try to shrink it, while keeping the tests passing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A common theme in all of these is automated tests. The value you can get from coding agents and other LLM coding tools is massively amplified by a good, cleanly passing test suite. Thankfully LLMs are great for accelerating the process of putting one of those together, if you don‚Äôt have one yet.&lt;/p&gt;
    &lt;head rend="h4"&gt;This is still a very fresh area&lt;/head&gt;
    &lt;p&gt;Designing agentic loops is a very new skill‚ÄîClaude Code was first released in just February 2025!&lt;/p&gt;
    &lt;p&gt;I‚Äôm hoping that giving it a clear name can help us have productive conversations about it. There‚Äôs so much more to figure out about how to use these tools as effectively as possible.&lt;/p&gt;
    &lt;head rend="h2"&gt;More recent articles&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude Sonnet 4.5 is probably the "best coding model in the world" (at least for now) - 29th September 2025&lt;/item&gt;
      &lt;item&gt;I think "agent" may finally have a widely enough agreed upon definition to be useful jargon now - 18th September 2025&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45426680</guid><pubDate>Tue, 30 Sep 2025 15:21:23 +0000</pubDate></item><item><title>Leaked Apple M5 9 core Geekbench scores</title><link>https://browser.geekbench.com/v6/cpu/14173685</link><description>&lt;doc fingerprint="47a0626d6e0cbd81"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Upload Date&lt;/cell&gt;
        &lt;cell&gt;September 30 2025 12:36 PM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Views&lt;/cell&gt;
        &lt;cell&gt;22079&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;System Information&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Operating System&lt;/cell&gt;
        &lt;cell&gt;iOS 26.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Model&lt;/cell&gt;
        &lt;cell&gt;iPad17,3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Model ID&lt;/cell&gt;
        &lt;cell&gt;iPad17,3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Motherboard&lt;/cell&gt;
        &lt;cell&gt;J820AP&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;CPU Information&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Name&lt;/cell&gt;
        &lt;cell&gt;ARM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Topology&lt;/cell&gt;
        &lt;cell&gt;1 Processor, 9 Cores&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Identifier&lt;/cell&gt;
        &lt;cell&gt;ARM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Base Frequency&lt;/cell&gt;
        &lt;cell&gt;4.42 GHz&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Cluster 1&lt;/cell&gt;
        &lt;cell&gt;3 Cores&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Cluster 2&lt;/cell&gt;
        &lt;cell&gt;6 Cores&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;L1 Instruction Cache&lt;/cell&gt;
        &lt;cell&gt;128 KB x 1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;L1 Data Cache&lt;/cell&gt;
        &lt;cell&gt;64.0 KB x 1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;L2 Cache&lt;/cell&gt;
        &lt;cell&gt;6.00 MB x 1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Instruction Sets&lt;/cell&gt;
        &lt;cell&gt;neon aes sha1 sha2 neon-fp16 neon-dotprod i8mm sme-i8i32 sme-f32f32 sme2&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Memory Information&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Size&lt;/cell&gt;
        &lt;cell&gt;11.20 GB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Single-Core Score&lt;/cell&gt;
        &lt;cell role="head"&gt;4133&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; File Compression &lt;/cell&gt;
        &lt;cell&gt; 3552 &lt;p&gt;510.1 MB/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Navigation &lt;/cell&gt;
        &lt;cell&gt; 3659 &lt;p&gt;22.0 routes/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; HTML5 Browser &lt;/cell&gt;
        &lt;cell&gt; 4260 &lt;p&gt;87.2 pages/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; PDF Renderer &lt;/cell&gt;
        &lt;cell&gt; 3734 &lt;p&gt;86.1 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Photo Library &lt;/cell&gt;
        &lt;cell&gt; 3719 &lt;p&gt;50.5 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Clang &lt;/cell&gt;
        &lt;cell&gt; 4649 &lt;p&gt;22.9 Klines/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Text Processing &lt;/cell&gt;
        &lt;cell&gt; 3822 &lt;p&gt;306.1 pages/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Asset Compression &lt;/cell&gt;
        &lt;cell&gt; 3547 &lt;p&gt;109.9 MB/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Object Detection &lt;/cell&gt;
        &lt;cell&gt; 6032 &lt;p&gt;180.5 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Background Blur &lt;/cell&gt;
        &lt;cell&gt; 4104 &lt;p&gt;17.0 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Horizon Detection &lt;/cell&gt;
        &lt;cell&gt; 4139 &lt;p&gt;128.8 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Object Remover &lt;/cell&gt;
        &lt;cell&gt; 5276 &lt;p&gt;405.6 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; HDR &lt;/cell&gt;
        &lt;cell&gt; 4678 &lt;p&gt;137.3 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Photo Filter &lt;/cell&gt;
        &lt;cell&gt; 5061 &lt;p&gt;50.2 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Ray Tracer &lt;/cell&gt;
        &lt;cell&gt; 3302 &lt;p&gt;3.20 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt; Structure from Motion &lt;/cell&gt;
        &lt;cell&gt; 3836 &lt;p&gt;121.4 Kpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Multi-Core Score&lt;/cell&gt;
        &lt;cell role="head"&gt;15437&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; File Compression &lt;/cell&gt;
        &lt;cell&gt; 12308 &lt;p&gt;1.73 GB/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Navigation &lt;/cell&gt;
        &lt;cell&gt; 17065 &lt;p&gt;102.8 routes/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; HTML5 Browser &lt;/cell&gt;
        &lt;cell&gt; 17958 &lt;p&gt;367.6 pages/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; PDF Renderer &lt;/cell&gt;
        &lt;cell&gt; 15774 &lt;p&gt;363.8 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Photo Library &lt;/cell&gt;
        &lt;cell&gt; 18268 &lt;p&gt;247.9 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Clang &lt;/cell&gt;
        &lt;cell&gt; 23236 &lt;p&gt;114.4 Klines/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Text Processing &lt;/cell&gt;
        &lt;cell&gt; 4956 &lt;p&gt;396.9 pages/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Asset Compression &lt;/cell&gt;
        &lt;cell&gt; 18577 &lt;p&gt;575.6 MB/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Object Detection &lt;/cell&gt;
        &lt;cell&gt; 14896 &lt;p&gt;445.7 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Background Blur &lt;/cell&gt;
        &lt;cell&gt; 13114 &lt;p&gt;54.3 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Horizon Detection &lt;/cell&gt;
        &lt;cell&gt; 19111 &lt;p&gt;594.7 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Object Remover &lt;/cell&gt;
        &lt;cell&gt; 15968 &lt;p&gt;1.23 Gpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; HDR &lt;/cell&gt;
        &lt;cell&gt; 18909 &lt;p&gt;554.9 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Photo Filter &lt;/cell&gt;
        &lt;cell&gt; 15246 &lt;p&gt;151.3 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Ray Tracer &lt;/cell&gt;
        &lt;cell&gt; 18888 &lt;p&gt;18.3 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt; Structure from Motion &lt;/cell&gt;
        &lt;cell&gt; 16186 &lt;p&gt;512.5 Kpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45427197</guid><pubDate>Tue, 30 Sep 2025 16:00:36 +0000</pubDate></item><item><title>Launch HN: Airweave (YC X25) ‚Äì Let agents search any app</title><link>https://github.com/airweave-ai/airweave</link><description>&lt;doc fingerprint="5a85ff58db83ac82"&gt;
  &lt;main&gt;
    &lt;p&gt;Airweave is a tool that lets agents search any app. It connects to apps, productivity tools, databases, or document stores and transforms their contents into searchable knowledge bases, accessible through a standardized interface for agents.&lt;/p&gt;
    &lt;p&gt;The search interface is exposed via REST API or MCP. When using MCP, Airweave essentially builds a semantically searchable MCP server. The platform handles everything from auth and extraction to embedding and serving.&lt;/p&gt;
    &lt;head rend="h3"&gt;Managed Service: Airweave Cloud&lt;/head&gt;
    &lt;p&gt;Make sure docker and docker-compose are installed, then...&lt;/p&gt;
    &lt;code&gt;# 1. Clone the repository
git clone https://github.com/airweave-ai/airweave.git
cd airweave

# 2. Build and run
chmod +x start.sh
./start.sh&lt;/code&gt;
    &lt;p&gt;That's it! Access the dashboard at http://localhost:8080&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Access the UI at &lt;code&gt;http://localhost:8080&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Connect sources, configure syncs, and query data&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Swagger docs: &lt;code&gt;http://localhost:8001/docs&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Create connections, trigger syncs, and search data&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install airweave-sdk&lt;/code&gt;
    &lt;code&gt;from airweave import AirweaveSDK

client = AirweaveSDK(
    api_key="YOUR_API_KEY",
    base_url="http://localhost:8001"
)
client.collections.create(
    name="name",
)&lt;/code&gt;
    &lt;code&gt;npm install @airweave/sdk
# or
yarn add @airweave/sdk&lt;/code&gt;
    &lt;code&gt;import { AirweaveSDKClient, AirweaveSDKEnvironment } from "@airweave/sdk";

const client = new AirweaveSDKClient({
    apiKey: "YOUR_API_KEY",
    environment: AirweaveSDKEnvironment.Local
});
await client.collections.create({
    name: "name",
});&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Data synchronization from 25+ sources with minimal config&lt;/item&gt;
      &lt;item&gt;Entity extraction and transformation pipeline&lt;/item&gt;
      &lt;item&gt;Multi-tenant architecture with OAuth2&lt;/item&gt;
      &lt;item&gt;Incremental updates using content hashing&lt;/item&gt;
      &lt;item&gt;Semantic search for agent queries&lt;/item&gt;
      &lt;item&gt;Versioning for data changes&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frontend: React/TypeScript with ShadCN&lt;/item&gt;
      &lt;item&gt;Backend: FastAPI (Python)&lt;/item&gt;
      &lt;item&gt;Databases: PostgreSQL (metadata), Qdrant (vectors)&lt;/item&gt;
      &lt;item&gt;Deployment: Docker Compose (dev), Kubernetes (prod)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We welcome contributions! Please check CONTRIBUTING.md for details.&lt;/p&gt;
    &lt;p&gt;Airweave is released under the MIT license.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Discord - Get help and discuss features&lt;/item&gt;
      &lt;item&gt;GitHub Issues - Report bugs or request features&lt;/item&gt;
      &lt;item&gt;Twitter - Follow for updates&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45427482</guid><pubDate>Tue, 30 Sep 2025 16:21:09 +0000</pubDate></item><item><title>Sora 2</title><link>https://openai.com/index/sora-2/</link><description>&lt;doc fingerprint="bc5d1e5b058e8bab"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Sora 2 is here&lt;/head&gt;
    &lt;p&gt;Our latest video generation model is more physically accurate, realistic, and more controllable than prior systems. It also features synchronized dialogue and sound effects. Create with it in the new Sora app.&lt;/p&gt;
    &lt;p&gt;Today we‚Äôre releasing Sora 2, our flagship video and audio generation model.&lt;/p&gt;
    &lt;p&gt;The original Sora model from February 2024 was in many ways the GPT‚Äë1 moment for video‚Äîthe first time video generation started to seem like it was working, and simple behaviors like object permanence emerged from scaling up pre-training compute. Since then, the Sora team has been focused on training models with more advanced world simulation capabilities. We believe such systems will be critical for training AI models that deeply understand the physical world. A major milestone for this is mastering pre-training and post-training on large-scale video data, which are in their infancy compared to language.&lt;/p&gt;
    &lt;p&gt;With Sora 2, we are jumping straight to what we think may be the GPT‚Äë3.5 moment for video. Sora 2 can do things that are exceptionally difficult‚Äîand in some instances outright impossible‚Äîfor prior video generation models: Olympic gymnastics routines, backflips on a paddleboard that accurately model the dynamics of buoyancy and rigidity, and triple axels while a cat holds on for dear life.&lt;/p&gt;
    &lt;p&gt;Prior video models are overoptimistic‚Äîthey will morph objects and deform reality to successfully execute upon a text prompt. For example, if a basketball player misses a shot, the ball may spontaneously teleport to the hoop. In Sora 2, if a basketball player misses a shot, it will rebound off the backboard. Interestingly, ‚Äúmistakes‚Äù the model makes frequently appear to be mistakes of the internal agent that Sora 2 is implicitly modeling; though still imperfect, it is better about obeying the laws of physics compared to prior systems. This is an extremely important capability for any useful world simulator‚Äîyou must be able to model failure, not just success.&lt;/p&gt;
    &lt;p&gt;The model is also a big leap forward in controllability, able to follow intricate instructions spanning multiple shots while accurately persisting world state. It excels at realistic, cinematic, and anime styles.&lt;/p&gt;
    &lt;p&gt;As a general purpose video-audio generation system, it is capable of creating sophisticated background soundscapes, speech, and sound effects with a high degree of realism.&lt;/p&gt;
    &lt;p&gt;You can also directly inject elements of the real world into Sora 2. For example, by observing a video of one of our teammates, the model can insert them into any Sora-generated environment with an accurate portrayal of appearance and voice. This capability is very general, and works for any human, animal or object.&lt;/p&gt;
    &lt;p&gt;The model is far from perfect and makes plenty of mistakes, but it is validation that further scaling up neural networks on video data will bring us closer to simulating reality.&lt;/p&gt;
    &lt;p&gt;On the road to general-purpose simulation and AI systems that can function in the physical world, we think people can have a lot of fun with the models we‚Äôre building along the way.&lt;/p&gt;
    &lt;p&gt;We first started playing with this ‚Äúupload yourself‚Äù feature several months ago on the Sora team, and we all had a blast with it. It kind of felt like a natural evolution of communication‚Äîfrom text messages to emojis to voice notes to this.&lt;/p&gt;
    &lt;p&gt;So today, we‚Äôre launching a new social iOS app just called ‚ÄúSora,‚Äù powered by Sora 2. Inside the app, you can create, remix each other‚Äôs generations, discover new videos in a customizable Sora feed, and bring yourself or your friends in via cameos. With cameos, you can drop yourself straight into any Sora scene with remarkable fidelity after a short one-time video-and-audio recording in the app to verify your identity and capture your likeness.&lt;/p&gt;
    &lt;p&gt;Last week, we launched the app internally to all of OpenAI. We‚Äôve already heard from our colleagues that they‚Äôre making new friends at the company because of the feature. We think a social app built around this ‚Äúcameos‚Äù feature is the best way to experience the magic of Sora 2.&lt;/p&gt;
    &lt;p&gt;Concerns about doomscrolling, addiction, isolation, and RL-sloptimized feeds are top of mind‚Äîhere is what we are doing about it.&lt;/p&gt;
    &lt;p&gt;We are giving users the tools and optionality to be in control of what they see on the feed. Using OpenAI's existing large language models, we have developed a new class of recommender algorithms that can be instructed through natural language. We also have built-in mechanisms to periodically poll users on their wellbeing and proactively give them the option to adjust their feed.&lt;/p&gt;
    &lt;p&gt;By default, we show you content heavily biased towards people you follow or interact with, and prioritize videos that the model thinks you‚Äôre most likely to use as inspiration for your own creations. We are not optimizing for time spent in feed, and we explicitly designed the app to maximize creation, not consumption. You can find more details in our Feed Philosophy&lt;/p&gt;
    &lt;p&gt;This app is made to be used with your friends. Overwhelming feedback from testers is that cameos are what make this feel different and fun to use‚Äîyou have to try it to really get it, but it is a new and unique way to communicate with people. We‚Äôre rolling this out as an invite-based app to make sure you come in with your friends. At a time when all major platforms are moving away from the social graph, we think cameos will reinforce community.&lt;/p&gt;
    &lt;p&gt;Protecting the wellbeing of teens is important to us. We are putting in default limits on how many generations teens can see per day in the feed, and we‚Äôre also rolling out with stricter permissions on cameos for this group. In addition to our automated safety stacks, we are scaling up teams of human moderators to quickly review cases of bullying if they arise. We are launching with Sora parental controls via ChatGPT so parents can override infinite scroll limits, turn off algorithm personalization, as well as manage direct message settings.&lt;/p&gt;
    &lt;p&gt;With cameos, you are in control of your likeness end-to-end with Sora. Only you decide who can use your cameo, and you can revoke access or remove any video that includes it at any time. Videos containing cameos of you, including drafts created by other people, are viewable by you at any time.&lt;/p&gt;
    &lt;p&gt;There are a lot of safety topics we‚Äôve tackled with this app‚Äîconsent around use of likeness, provenance, preventing the generation of harmful content, and much more. See our Sora 2 Safety doc for more details.&lt;/p&gt;
    &lt;p&gt;A lot of problems with other apps stem from the monetization model incentivizing decisions that are at odds with user wellbeing. Transparently, our only current plan is to eventually give users the option to pay some amount to generate an extra video if there‚Äôs too much demand relative to available compute. As the app evolves, we will openly communicate any changes in our approach here, while continuing to keep user wellbeing as our main goal.&lt;/p&gt;
    &lt;p&gt;We‚Äôre at the beginning of this journey, but with all of the powerful ways to create and remix content with Sora 2, we see this as the beginning of a completely new era for co-creative experiences. We‚Äôre optimistic that this will be a healthier platform for entertainment and creativity compared to what is available right now. We hope you have a good time :)&lt;/p&gt;
    &lt;p&gt;The Sora iOS app(opens in a new window) is available to download now. You can sign up in-app for a push notification when access opens for your account. We‚Äôre starting the initial rollout in the U.S. and Canada today with the intent to quickly expand to additional countries. After you‚Äôve received an invite, you‚Äôll also be able to access Sora 2 through sora.com(opens in a new window). Sora 2 will initially be available for free, with generous limits to start so people can freely explore its capabilities, though these are still subject to compute constraints. ChatGPT Pro users will also be able to use our experimental, higher quality Sora 2 Pro model on sora.com(opens in a new window) (and soon in the Sora app as well). We also plan to release Sora 2 in the API. Sora 1 Turbo will remain available, and everything you‚Äôve created will continue to live in your sora.com(opens in a new window) library.&lt;/p&gt;
    &lt;p&gt;Video models are getting very good, very quickly. General-purpose world simulators and robotic agents will fundamentally reshape society and accelerate the arc of human progress. Sora 2 represents significant progress towards that goal. In keeping with OpenAI‚Äôs mission, it is important that humanity benefits from these models as they are developed. We think Sora is going to bring a lot of joy, creativity and connection to the world.&lt;/p&gt;
    &lt;p&gt;‚Äî Written by the Sora Team&lt;/p&gt;
    &lt;head rend="h2"&gt;Sora 2&lt;/head&gt;
    &lt;p&gt;Debbie Mesloh&lt;/p&gt;
    &lt;p&gt;Caroline Zhao&lt;/p&gt;
    &lt;p&gt;Published September 30, MMXXV&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45427982</guid><pubDate>Tue, 30 Sep 2025 16:55:01 +0000</pubDate></item><item><title>Boeing has started working on a 737 MAX replacement</title><link>https://www.wsj.com/business/airlines/boeing-has-started-working-on-a-737-max-replacement-40a110df</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45428482</guid><pubDate>Tue, 30 Sep 2025 17:31:34 +0000</pubDate></item><item><title>Inflammation now predicts heart disease more strongly than cholesterol</title><link>https://www.empirical.health/blog/inflammation-and-heart-health/</link><description>&lt;doc fingerprint="361b13e308c39ddc"&gt;
  &lt;main&gt;
    &lt;p&gt;Chronic inflammation has long been known to double your risk of heart disease, but prior to now, inflammation has never been a SMuRF: standard modifiable risk factor for heart disease.&lt;/p&gt;
    &lt;p&gt;The American College of Cardiology just released recommendations that change that. The ACC is now recommending that everyone measure inflammation (specifically, hs-CRP) via a blood test:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Because clinicians will not treat what they do not measure, universal screening of hsCRP in both primary and secondary prevention patients, in combination with cholesterol, represents a major clinical opportunity and is therefore recommended. American College of Cardiology&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;There were many interesting bits of evidence that led to this recommendation. The whole article, published in JACC, is worth a read, but this blog post extracts a few of the most interesting parts ‚Äî or at least, the parts I thought were most interesting.&lt;/p&gt;
    &lt;head rend="h1"&gt;Inflammation (hs-CRP) is a stronger predictor of heart disease than cholesterol&lt;/head&gt;
    &lt;p&gt;For decades, LDL cholesterol (or ApoB) has been the main focus of cardiovascular risk assessment. But this chart shows hs-CRP is actually a stronger predictor of heart disease than LDL.&lt;/p&gt;
    &lt;p&gt;Why? In some ways, cholesterol has become a victim of its own success. We now screen the whole population for high cholesterol, give statins to those with high LDL (or ApoB), and so then the majority of people who end up having heart attacks have lower cholesterol than they would naturally have. This means most of the majority of residual risk for heart attacks will be found in biomarkers that aren‚Äôt SMuRFs.&lt;/p&gt;
    &lt;p&gt;Inflammation (hs-CRP) is one such non-SMuRF, one perhaps one of the strongest. This is especially true in people already on statins or those without traditional risk factors (sometimes called ‚ÄúSMuRF-less‚Äù patients). In these groups, cholesterol may be well controlled, but inflammation remains a key driver of events.&lt;/p&gt;
    &lt;p&gt;Of course, other traditional risk factors matter in addition to inflammation: blood pressure, HbA1c or insulin resistance, eGFR (kidney function), and so on.&lt;/p&gt;
    &lt;head rend="h1"&gt;What can you actually do to lower inflammation?&lt;/head&gt;
    &lt;p&gt;The ACC consensus reviews a range of clinical trials testing both drugs and lifestyle interventions for lowering inflammation and reducing cardiovascular risk. Here‚Äôs a summary of the clinical trials and their results:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Trial Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Drug (Class)&lt;/cell&gt;
        &lt;cell role="head"&gt;Sample Size (n)&lt;/cell&gt;
        &lt;cell role="head"&gt;Population/NYHA Functional Class&lt;/cell&gt;
        &lt;cell role="head"&gt;Follow-Up&lt;/cell&gt;
        &lt;cell role="head"&gt;Primary Endpoint&lt;/cell&gt;
        &lt;cell role="head"&gt;Treatment Outcome&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;ATTACH&lt;/cell&gt;
        &lt;cell&gt;Infliximab (TNF inhibitor)&lt;/cell&gt;
        &lt;cell&gt;150&lt;/cell&gt;
        &lt;cell&gt;NYHA III/IV HF&lt;/cell&gt;
        &lt;cell&gt;7 mo&lt;/cell&gt;
        &lt;cell&gt;Clinical status (composite score)&lt;/cell&gt;
        &lt;cell&gt;No improvement or worsening; deaths highest in high-dose infliximab&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;ACCLAIM&lt;/cell&gt;
        &lt;cell&gt;IVIG&lt;/cell&gt;
        &lt;cell&gt;2314&lt;/cell&gt;
        &lt;cell&gt;NYHA II-IV HF&lt;/cell&gt;
        &lt;cell&gt;10.2 mo&lt;/cell&gt;
        &lt;cell&gt;Composite all-cause mortality and CV hospitalization&lt;/cell&gt;
        &lt;cell&gt;No reduction in events; trend toward benefit in NYHA III and IV&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;CANTOS&lt;/cell&gt;
        &lt;cell&gt;Canakinumab (anti‚ÄìIL-1Œ≤)&lt;/cell&gt;
        &lt;cell&gt;10,061&lt;/cell&gt;
        &lt;cell&gt;Prior MI; hsCRP ‚â•2 mg/L&lt;/cell&gt;
        &lt;cell&gt;3.7 y (median)&lt;/cell&gt;
        &lt;cell&gt;Nonfatal MI, nonfatal stroke, or CV death (MACE); HF-related mortality&lt;/cell&gt;
        &lt;cell&gt;Reduced MACE and HF events; no effect on all-cause mortality; primary endpoint events: 3.86% vs 4.50%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;CIRT&lt;/cell&gt;
        &lt;cell&gt;Methotrexate&lt;/cell&gt;
        &lt;cell&gt;4,786&lt;/cell&gt;
        &lt;cell&gt;Stable MI plus CAD&lt;/cell&gt;
        &lt;cell&gt;2.3 y (median)&lt;/cell&gt;
        &lt;cell&gt;CV event rates&lt;/cell&gt;
        &lt;cell&gt;No effect on CV events, inflammation, or lipids&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;CLEAR SYNERGY&lt;/cell&gt;
        &lt;cell&gt;Colchicine&lt;/cell&gt;
        &lt;cell&gt;3,056&lt;/cell&gt;
        &lt;cell&gt;Acute MI plus PCI&lt;/cell&gt;
        &lt;cell&gt;22.6 mo&lt;/cell&gt;
        &lt;cell&gt;Death from CV causes, recurrent MI, ischemic stroke&lt;/cell&gt;
        &lt;cell&gt;No significant difference in primary endpoint&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;COLCOT&lt;/cell&gt;
        &lt;cell&gt;Colchicine&lt;/cell&gt;
        &lt;cell&gt;4,745&lt;/cell&gt;
        &lt;cell&gt;Acute MI patients&lt;/cell&gt;
        &lt;cell&gt;22.6 mo&lt;/cell&gt;
        &lt;cell&gt;CV event rates&lt;/cell&gt;
        &lt;cell&gt;CV events lower than placebo&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;LoDoCo2&lt;/cell&gt;
        &lt;cell&gt;Colchicine&lt;/cell&gt;
        &lt;cell&gt;5,522&lt;/cell&gt;
        &lt;cell&gt;Stable CAD&lt;/cell&gt;
        &lt;cell&gt;28.6 mo&lt;/cell&gt;
        &lt;cell&gt;Composite of CV death, nonfatal MI, ischemic stroke, or ischemia-driven revasc.&lt;/cell&gt;
        &lt;cell&gt;CV events lower than placebo&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;GISSI-HF&lt;/cell&gt;
        &lt;cell&gt;Rosuvastatin (statin)&lt;/cell&gt;
        &lt;cell&gt;4,574&lt;/cell&gt;
        &lt;cell&gt;NYHA II-IV HF&lt;/cell&gt;
        &lt;cell&gt;3.9 y&lt;/cell&gt;
        &lt;cell&gt;All-cause mortality and CV hospitalization&lt;/cell&gt;
        &lt;cell&gt;No effect on primary endpoints&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;JUPITER&lt;/cell&gt;
        &lt;cell&gt;Rosuvastatin (statin)&lt;/cell&gt;
        &lt;cell&gt;17,802&lt;/cell&gt;
        &lt;cell&gt;No CVD / LDL &amp;lt;130 mg/dL; hsCRP ‚â•2 mg/L&lt;/cell&gt;
        &lt;cell&gt;1.9 y (median)&lt;/cell&gt;
        &lt;cell&gt;MI, stroke, arterial revascularization, hospitalization for unstable angina, or CV death&lt;/cell&gt;
        &lt;cell&gt;Reduced events (HR 0.56‚Äì0.69)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;CORONA&lt;/cell&gt;
        &lt;cell&gt;Rosuvastatin (statin)&lt;/cell&gt;
        &lt;cell&gt;5,011&lt;/cell&gt;
        &lt;cell&gt;NYHA II-IV HF; ischemic etiology&lt;/cell&gt;
        &lt;cell&gt;32.8 mo&lt;/cell&gt;
        &lt;cell&gt;CV death, nonfatal MI, nonfatal stroke&lt;/cell&gt;
        &lt;cell&gt;No effect on primary endpoint&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;OPT-CHF&lt;/cell&gt;
        &lt;cell&gt;Etanercept (TNF inhibitor)&lt;/cell&gt;
        &lt;cell&gt;1,500&lt;/cell&gt;
        &lt;cell&gt;NYHA II-IV HF&lt;/cell&gt;
        &lt;cell&gt;6 mo&lt;/cell&gt;
        &lt;cell&gt;Death, hospitalization, or worsening HF&lt;/cell&gt;
        &lt;cell&gt;No effect on primary endpoint&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;DCMP&lt;/cell&gt;
        &lt;cell&gt;Prednisone (corticosteroid)&lt;/cell&gt;
        &lt;cell&gt;84&lt;/cell&gt;
        &lt;cell&gt;NYHA II-IV HF; biopsy-proven myocarditis&lt;/cell&gt;
        &lt;cell&gt;5.7 and 12.3 mo&lt;/cell&gt;
        &lt;cell&gt;Improvement in LVEF, survival, or combined outcome of death or transplantation&lt;/cell&gt;
        &lt;cell&gt;No significant benefit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;RENEWAL&lt;/cell&gt;
        &lt;cell&gt;Etanercept (TNF inhibitor)&lt;/cell&gt;
        &lt;cell&gt;2,048&lt;/cell&gt;
        &lt;cell&gt;NYHA II-IV HF&lt;/cell&gt;
        &lt;cell&gt;6 mo&lt;/cell&gt;
        &lt;cell&gt;Composite outcome of death or hospitalization&lt;/cell&gt;
        &lt;cell&gt;No effect on primary endpoint&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;What works to lower inflammation?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Statins (especially in people with high hs-CRP): Substantial reduction in events, even when LDL is normal (JUPITER trial).&lt;/item&gt;
      &lt;item&gt;Colchicine: Reduces recurrent events in people with established heart disease (COLCOT, LoDoCo2).&lt;/item&gt;
      &lt;item&gt;Canakinumab: Reduces events but is expensive and increases infection risk (CANTOS).&lt;/item&gt;
      &lt;item&gt;Lifestyle: Anti-inflammatory diets (Mediterranean, DASH), regular exercise, smoking cessation, and maintaining a healthy weight all lower hs-CRP and reduce risk.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What doesn‚Äôt work?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Some anti-inflammatory drugs (methotrexate, TNF inhibitors, corticosteroids) have not shown benefit in major trials.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What‚Äôs a normal, good, or bad hs-CRP?&lt;/head&gt;
    &lt;p&gt;If you‚Äôve already measured your hs-CRP (great!), then it‚Äôs ideally below &amp;lt;1 mg/L. hs-CRP above 3 mg/L is high risk:&lt;/p&gt;
    &lt;p&gt;(If you‚Äôre in moderate or high ranges, see the section above for what to do.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Are other biomarkers of inflammation relevant?&lt;/head&gt;
    &lt;p&gt;The ACC evaluated other markers: IL-6, fibrinogen, neutrophil-to-lymphocyte ratio, EPA/AA ratio, and serum amyloid A. These have also been shown to predict cardiovascular risk, but once hs-CRP is known, don‚Äôt add more signal.&lt;/p&gt;
    &lt;p&gt;In other words, you‚Äôre best off simply measuring hs-CRP, and then spending money elsewhere on heart health.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other interesting bits&lt;/head&gt;
    &lt;p&gt;The JACC article is packed with other interesting insights. These ones were interesting:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Imaging biomarkers (like CT, PET, MRI, and perivascular ‚Äúfat attenuation index‚Äù) can detect vascular inflammation and may help predict coronary events, but are not yet ready for routine clinical use.&lt;/item&gt;
      &lt;item&gt;Bempedoic acid is a newer cholesterol-lowering drug that also lowers hs-CRP, but its long-term outcomes are still being studied.&lt;/item&gt;
      &lt;item&gt;Residual inflammatory risk: Even with well-controlled LDL on statins, many people still have elevated hs-CRP and ongoing risk‚Äîso inflammation should be addressed separately from cholesterol.&lt;/item&gt;
      &lt;item&gt;Universal hs-CRP screening is now recommended by the ACC for both people with and without established heart disease.&lt;/item&gt;
      &lt;item&gt;Colchicine (0.5 mg/d) is now FDA-approved as an adjunct for secondary prevention in stable ASCVD, but should be avoided in people with significant kidney or liver disease.&lt;/item&gt;
      &lt;item&gt;Novel IL-6 inhibitors are being studied as future anti-inflammatory therapies for heart disease.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How to measure your inflammation&lt;/head&gt;
    &lt;p&gt;A simple blood test for hs-CRP is widely available and inexpensive. The ACC now recommends routine hs-CRP testing for both people at risk (primary prevention) and those with established heart disease (secondary prevention).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45430498</guid><pubDate>Tue, 30 Sep 2025 20:00:21 +0000</pubDate></item><item><title>Diff Algorithms</title><link>https://flo.znkr.io/diff/</link><description>&lt;doc fingerprint="7f0985451041b94"&gt;
  &lt;main&gt;
    &lt;p&gt;For software engineers, diffs are a ubiquitous method for representing changes: We use diffs to compare different versions of the same file (e.g., during code review or when trying to understand the history of a file), to visualize the difference of a failing test compared with its expectation, or to apply changes to source files automatically.&lt;/p&gt;
    &lt;p&gt;Every project I worked on professionally or privately eventually needed a diff to visualize a change or to apply a patch. However, I have never been satisfied with any of the freely available diff libraries. This was never really a problem professionally, but for private projects, I have copied and modified my own library from project to project until I mentioned this to a colleague who set me on the path to publish my Go library (a port of a previous C++ library I used to copy and modify). Boy, did I underestimate how close my library was to publishability!&lt;/p&gt;
    &lt;p&gt;Anyway, I did it and I learned a whole lot about diff algorithms. You can find my library at znkr.io/diff and what I learned in this article. I am not finished learning yet, so I plan to update this article as my understanding continues to evolve.&lt;/p&gt;
    &lt;head rend="h2"&gt;Existing Diff Libraries&lt;/head&gt;
    &lt;p&gt;Let me start by explaining why I am dissatisfied with existing diff libraries. There are a number of attributes that are important to me. Not all of these attributes are important for every use case, but a diff library that I can use for all of my use cases needs to fulfill all of them.&lt;/p&gt;
    &lt;p&gt;Usually, the input to a diff algorithm is text, and most diff libraries only support that. However, I occasionally have use cases where I need to compare things that are not text. So any diff library that only supports text doesn't meet my needs; instead, I need support for arbitrary sequences.&lt;/p&gt;
    &lt;p&gt;The resulting diff output is intended to be readable by humans. Quite often, especially for text, a good way to present a diff is in the unified format. However, it's not always the best presentation. A diff library should make it easy to output a diff in unified format, but it should also provide a way to customize the presentation by providing a structured result.&lt;/p&gt;
    &lt;p&gt;Besides the presentation, the content of a diff should make it easy for humans to understand the diff. This is a somewhat subjective criterion, but there are a number of failure cases that are easily avoided, and there's some research into diff readability to set a benchmark. On the other hand, diffs should be minimal in that they should be as small as possible.&lt;/p&gt;
    &lt;p&gt;Last but not least, it's important that a diff library has a simple API and provides good performance in both runtime and memory usage, even in worst-case scenarios1.&lt;/p&gt;
    &lt;p&gt;With that, we can evaluate existing diff libraries. For Go, I went through a number of libraries and summarized them.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Input&lt;/cell&gt;
        &lt;cell role="head"&gt;Output&lt;/cell&gt;
        &lt;cell role="head"&gt;API&lt;/cell&gt;
        &lt;cell role="head"&gt;Performance2&lt;/cell&gt;
        &lt;cell role="head"&gt;Diff&lt;p&gt;Readability&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Diff&lt;p&gt;Minimality2&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;diffmatchpatch&lt;/cell&gt;
        &lt;cell&gt;‚ùå3&lt;/cell&gt;
        &lt;cell&gt;‚ùå4&lt;/cell&gt;
        &lt;cell&gt;ü§î5&lt;/cell&gt;
        &lt;cell&gt;‚ûñ‚ûñ&lt;/cell&gt;
        &lt;cell&gt;‚ûñ&lt;/cell&gt;
        &lt;cell&gt;‚ûñ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;go-internal&lt;/cell&gt;
        &lt;cell&gt;‚ùå3&lt;/cell&gt;
        &lt;cell&gt;‚ùå6&lt;/cell&gt;
        &lt;cell&gt;üòÅ&lt;/cell&gt;
        &lt;cell&gt;‚ûï‚ûï&lt;/cell&gt;
        &lt;cell&gt;‚ûï‚ûï&lt;/cell&gt;
        &lt;cell&gt;‚ûï&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;godebug&lt;/cell&gt;
        &lt;cell&gt;‚ùå3&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;üòÅ&lt;/cell&gt;
        &lt;cell&gt;‚ûñ‚ûñ‚ûñ /üß®7&lt;/cell&gt;
        &lt;cell&gt;‚ûï&lt;/cell&gt;
        &lt;cell&gt;‚ûï‚ûï&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;mb0&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå4&lt;/cell&gt;
        &lt;cell&gt;üòê8&lt;/cell&gt;
        &lt;cell&gt;‚ûñ‚ûñ&lt;/cell&gt;
        &lt;cell&gt;‚ûï&lt;/cell&gt;
        &lt;cell&gt;‚ûï‚ûï&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;udiff&lt;/cell&gt;
        &lt;cell&gt;‚ùå3&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;üòÅ&lt;/cell&gt;
        &lt;cell&gt;‚ûï9&lt;/cell&gt;
        &lt;cell&gt;‚ûñ&lt;/cell&gt;
        &lt;cell&gt;‚ûñ‚ûñ9&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beware&lt;/p&gt;
    &lt;p&gt;The way I assigned ‚ûï and ‚ûñ in this table doesn't follow any scientific methodology it's merely based on running a few benchmarks and comparing a few results by hand. If you're looking for a diff library to fulfill your needs, I would like to encourage you to do your own comparisons. You can find the code I used for these comparisons in on github.&lt;/p&gt;
    &lt;head rend="h2"&gt;Challenges&lt;/head&gt;
    &lt;p&gt;The results suggest that it's far from trivial to implement a good diff library, and the one I had started out with wasn't much better. To understand why the existing libraries are as they are, we need to take a peek into the implementation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Complexity&lt;/head&gt;
    &lt;p&gt;With the exception of go-internal, all libraries use Myers' Algorithm to compute the diff. This is a standard algorithm that returns a minimal diff and has been in use for this purpose for decades. The algorithm has a runtime complexity of where is the number of input elements and is the edit distance between the two inputs. This means that the algorithm is very fast for inputs that are similar, which is quite common. However, it's essentially quadratic in the worst case. That is, for inputs that are very different, the complexity approaches . Furthermore, the algorithm comes in two variants with a space complexity of either or . Only godebug uses the variant with quadratic memory growth.&lt;/p&gt;
    &lt;p&gt;This means that it's relatively easy to write a well-performing diffing algorithm for small or similar inputs, but it takes a very long time to complete for larger, less similar inputs. A consequence of this is that we can't trust simple benchmarks; instead, we need to test the worst-case scenario1.&lt;/p&gt;
    &lt;p&gt;As always in cases like this, we can improve the performance by approximating an optimal solution. There are a number of heuristics that reduce the time complexity by trading off diff minimality. For example, diffmatchpatch uses a deadline to stop the search for an optimal diff, and udiff uses a an extremely aggressive heuristic.&lt;/p&gt;
    &lt;p&gt;Instead of improving Myers' runtime with heuristics, it's also often possible to find a diff using only heuristics. go-internal uses patience diff. The heuristic is good enough that it alone almost always results in a good diff with a runtime complexity of 10. An additional advantage of this algorithm is that it produces more readable diffs. However, patience diff can fail with very large diffs, and it can only be implemented efficiently using a hash table, which restricts the possible applications.&lt;/p&gt;
    &lt;p&gt;Histogram Diff&lt;/p&gt;
    &lt;p&gt;Besides patience diff, there's another interesting heuristic called histogram diff. I still have to implement it and understand it better before writing about it here, though.&lt;/p&gt;
    &lt;head rend="h3"&gt;Readability&lt;/head&gt;
    &lt;p&gt;Diff algorithms usually find a minimal diff or an approximation of one. However, except for trivial cases, there are always multiple minimal diffs. For example, this simple diff&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;b
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;c
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;d
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;is as minimal as&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;c
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;b
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;d
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Not all of the minimal or near-minimal diffs have the same readability for humans. For example11,&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (chunk == NULL) return 0;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    return start &amp;lt;= chunk-&amp;gt;length &amp;amp;&amp;amp; n &amp;lt;= chunk-&amp;gt;length - start;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(src, src_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(dst, dst_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    memcpy(dst-&amp;gt;data + dst_start, src-&amp;gt;data + src_start, n);
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (chunk == NULL) return 0;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    return start &amp;lt;= chunk-&amp;gt;length &amp;amp;&amp;amp; n &amp;lt;= chunk-&amp;gt;length - start;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;is much more readable than the equally minimal and correct&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(src, src_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(dst, dst_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (chunk == NULL) return 0;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    memcpy(dst-&amp;gt;data + dst_start, src-&amp;gt;data + src_start, n);
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    return start &amp;lt;= chunk-&amp;gt;length &amp;amp;&amp;amp; n &amp;lt;= chunk-&amp;gt;length - start;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (chunk == NULL) return 0;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(src, src_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(dst, dst_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    return start &amp;lt;= chunk-&amp;gt;length &amp;amp;&amp;amp; n &amp;lt;= chunk-&amp;gt;length - start;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    memcpy(dst-&amp;gt;data + dst_start, src-&amp;gt;data + src_start, n);
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Furthermore, if we relax minimality to accept approximations, the number of possible results increases significantly.&lt;/p&gt;
    &lt;p&gt;For good diff readability, we have to select one solution from the many possible ones that is readable for humans. Many people believe that the diff readability is determined by the algorithm. However, that's only partially correct, because different implementations of the same algorithm can produce vastly different results.&lt;/p&gt;
    &lt;p&gt;There's also been a lot of progress in the past years to improve diff readability. Perhaps the best work about diff readability is diff-slider-tools by Michael Haggerty. He implemented a heuristic that's applied in a post-processing step to improve the readability.&lt;/p&gt;
    &lt;p&gt;In fact, &lt;code&gt;example_03.diff&lt;/code&gt; above was generated using this heuristic. The diff without the heuristic,
as generated by my implementation of Myers' linear-space variant, looks like this:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (chunk == NULL) return 0;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    return start &amp;lt;= chunk-&amp;gt;length &amp;amp;&amp;amp; n &amp;lt;= chunk-&amp;gt;length - start;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(src, src_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(dst, dst_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    memcpy(dst-&amp;gt;data + dst_start, src-&amp;gt;data + src_start, n);
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (chunk == NULL) return 0;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    return start &amp;lt;= chunk-&amp;gt;length &amp;amp;&amp;amp; n &amp;lt;= chunk-&amp;gt;length - start;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Notice that the deletion starts at the end of the preceding function and leaves a small remainder of the function being deleted? Michael's heuristic fixes this problem and results in the very readable &lt;code&gt;example_03.diff&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;It's not the algorithm&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;example_04.diff&lt;/code&gt; was found using a different implementation of Myers'
linear-space variant. That is, both &lt;code&gt;example_03.diff&lt;/code&gt; and &lt;code&gt;example_04.diff&lt;/code&gt; used the same algorithm!
The differences stem from the implementation of that algorithm and from post-processing.&lt;/p&gt;
    &lt;head rend="h2"&gt;A New Diffing Library for Go&lt;/head&gt;
    &lt;p&gt;I created znkr.io/diff to address these challenges in a way that works for all my use cases. Let's reiterate what I want from a diffing library:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The input can be text and arbitrary slices&lt;/item&gt;
      &lt;item&gt;The output should be possible in unified format and as a structured result&lt;/item&gt;
      &lt;item&gt;The API should be simple&lt;/item&gt;
      &lt;item&gt;The diffs should be minimal or near-minimal&lt;/item&gt;
      &lt;item&gt;The runtime and memory performance should be excellent&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is a lot more than what any of the existing libraries provide. When I copied and modified my old diffing library, I could adapt it to the use cases at hand. But a general-purpose diffing library needs to be general enough to cover the vast majority of use cases. At the same time, it needs to be extensible to make sure new features can be implemented without cluttering the API over time.&lt;/p&gt;
    &lt;p&gt;Unfortunately, excellent performance and minimal results are somewhat in opposition to one another and I ended up providing three different modes of operation: Default (balanced between performance and minimality), Fast (sacrifice minimal results for faster speed), Optimal (minimal result whatever the cost).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Input&lt;/cell&gt;
        &lt;cell role="head"&gt;Output&lt;/cell&gt;
        &lt;cell role="head"&gt;API&lt;/cell&gt;
        &lt;cell role="head"&gt;Performance2&lt;/cell&gt;
        &lt;cell role="head"&gt;Diff&lt;p&gt;Readability&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Diff&lt;p&gt;Minimality2&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Default&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;üòÅ&lt;/cell&gt;
        &lt;cell&gt;‚ûï‚ûï&lt;/cell&gt;
        &lt;cell&gt;‚ûï‚ûï&lt;/cell&gt;
        &lt;cell&gt;‚ûï‚ûï&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Fast&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;üòÅ&lt;/cell&gt;
        &lt;cell&gt;‚ûï‚ûï‚ûï&lt;/cell&gt;
        &lt;cell&gt;‚ûï‚ûï&lt;/cell&gt;
        &lt;cell&gt;‚ûï&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Optimal&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;üòÅ&lt;/cell&gt;
        &lt;cell&gt;‚ûï&lt;/cell&gt;
        &lt;cell&gt;‚ûï‚ûï&lt;/cell&gt;
        &lt;cell&gt;‚ûï‚ûï&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Text Only&lt;/p&gt;
    &lt;p&gt;This table only applies to text (same as the table above), non-text inputs can have a different performance (if they are not &lt;code&gt;comparable&lt;/code&gt;) or readability.&lt;/p&gt;
    &lt;head rend="h3"&gt;API&lt;/head&gt;
    &lt;p&gt;To design this API, I started with the data structures that I wanted to use as a user of the API and worked backwards from there. At a very high level, there are two structured representations of a diff that have been useful to me: a flat sequence of all deletions, insertions, and matching elements (called edits) and a nested sequence of consecutive changes (called hunks).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Edits are what I use to represent edits in this article; they contain the full content of both inputs and how one is transformed into the other.&lt;/item&gt;
      &lt;item&gt;Hunks are a great representation for unit tests, because they are empty if both inputs are identical and they make it possible to visualize just the changes even if the inputs are large.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Arbitrary Slices&lt;/head&gt;
    &lt;p&gt;I started with the design for the most general case, arbitrary slices. The Go representation for diffing slices I liked the most is this one (see also znkr.io/diff):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Op describes an edit operation.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;type Op int
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;const (
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Match  Op = iota // Two slice elements match
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Delete           // A deletion from an element on the left slice
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Insert           // An insertion of an element from the right side
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Edit describes a single edit of a diff.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// - For Match, both X and Y contain the matching element.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// - For Delete, X contains the deleted element and Y is unset (zero value).
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;17&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// - For Insert, Y contains the inserted element and X is unset (zero value).
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;type Edit[T any] struct {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;19&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Op   Op
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;20&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	X, Y T
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;21&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;22&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;23&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Hunk describes a sequence of consecutive edits.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;24&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;type Hunk[T any] struct {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;25&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	PosX, EndX int       // Start and end position in x.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;26&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	PosY, EndY int       // Start and end position in y.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;27&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Edits      []Edit[T] // Edits to transform x[PosX:EndX] to y[PosY:EndY]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;28&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The alternatives I have seen are variations and combinations of two themes. Either using slices to represent edit operations in &lt;code&gt;Hunk&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;type Hunk[T any] struct {
	Delete []T
	Insert []T
	Match  []T
}
&lt;/code&gt;
    &lt;p&gt;Or using indices instead of elements&lt;/p&gt;
    &lt;code&gt;type Edit struct {
	Op         Op
	PosX, PosY []int
}
&lt;/code&gt;
    &lt;p&gt;All of these representations work, but I found that the representations above served my use cases best. One little quirk is that &lt;code&gt;Edit&lt;/code&gt; always contains both elements. This is often unnecessary, but
there are use cases where this is very important because the elements themselves might not be equal
(e.g., if they are pointers that are compared with a custom function).&lt;/p&gt;
    &lt;p&gt;Once the data structures were established, it was quite obvious that the simplest way to fill them with diff data was to write two functions &lt;code&gt;diff.Edits&lt;/code&gt; and
&lt;code&gt;diff.Hunks&lt;/code&gt; to return the diffs. I made them extensible by
using functional options.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;30&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Edits compares the contents of x and y and returns the changes necessary to convert from one to
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;31&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// the other.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;//
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;33&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Edits returns one edit for every element in the input slices. If x and y are identical, the
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;34&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// output will consist of a match edit for every input element.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;35&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func Edits[T comparable](x, y []T, opts ...Option) []Edit[T]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;36&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;37&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Hunks compares the contents of x and y and returns the changes necessary to convert from one to
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;38&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// the other.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;39&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;//
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;40&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// The output is a sequence of hunks. A hunk represents a contiguous block of changes (insertions
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;41&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// and deletions) along with some surrounding context.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;42&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func Hunks[T comparable](x, y []T, opts ...Option) []Hunk[T]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The options allow for future extensibility and allow changing the behavior of these functions. For example, the option &lt;code&gt;diff.Context(5)&lt;/code&gt; configures &lt;code&gt;Hunks&lt;/code&gt;
to provide 5 elements of surrounding context.&lt;/p&gt;
    &lt;p&gt;However, the current API still doesn't allow arbitrary slices; it only allows slices of &lt;code&gt;comparable&lt;/code&gt; types. To fix this, I needed two other functions that provide a function to compare
two elements. The Go standard library uses the &lt;code&gt;Func&lt;/code&gt; suffix for functions like this, so I followed
the lead:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;44&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// EditsFunc compares the contents of x and y using the provided equality comparison and returns the
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;45&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// changes necessary to convert from one to the other.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;46&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func EditsFunc[T any](x, y []T, eq func(a, b T) bool, opts ...Option) []Edit[T]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;47&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;48&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// HunksFunc compares the contents of x and y using the provided equality comparison and returns the
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;49&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// changes necessary to convert from one to the other.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func HunksFunc[T any](x, y []T, eq func(a, b T) bool, opts ...Option) []Hunk[T]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;Text&lt;/head&gt;
    &lt;p&gt;While this API works well to produce a structured result for arbitrary slices, it doesn't provide output in unified format for text inputs. My first approach was to provide a helper function that returns a diff in unified format: &lt;code&gt;diff.ToUnified(hunks []Hunk[string]) string&lt;/code&gt;. However, this would
make getting a unified diff more complicated. Besides requiring two function calls, it would be
necessary to split the input into lines. This, in turn, can be done in different ways, e.g., by
stripping or keeping the line breaks, which opens the door to mistakes. It's much better to provide
a simple function for the entire use case.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Unified compares the lines in x and y and returns the changes necessary to convert from one to
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// the other in unified format.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func Unified[T string | []byte](x, y T, opts ...diff.Option) T
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I also moved this function to the &lt;code&gt;textdiff&lt;/code&gt; package to
highlight the difference in expected input.&lt;/p&gt;
    &lt;p&gt;Now, I also happen to have use cases where I need structured results for text diffs. It would be very annoying if I had to split those into lines manually. Besides, I can make a few more assumptions about text that allow for a slight simplification of the data structures:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Edit describes a single edit of a line-by-line diff.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;type Edit[T string | []byte] struct {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Op   diff.Op // Edit operation
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Line T       // Line, including newline character (if any)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;17&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Hunk describes a sequence of consecutive edits.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;type Hunk[T string | []byte] struct {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;19&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	PosX, EndX int       // Start and end line in x (zero-based).
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;20&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	PosY, EndY int       // Start and end line in y (zero-based).
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;21&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Edits      []Edit[T] // Edits to transform x lines PosX..EndX to y lines PosY..EndY
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;22&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;23&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;24&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Edits compares the lines in x and y and returns the changes necessary to convert from one to the
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;25&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// other.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;26&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func Edits[T string | []byte](x, y T, opts ...diff.Option) []Edit[T]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;27&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;28&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Hunks compares the lines in x and y and returns the changes necessary to convert from one to the
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;29&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// other.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;30&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func Hunks[T string | []byte](x, y T, opts ...diff.Option) []Hunk[T]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;For the full API and examples for how to use it, please see the package documentation for znkr.io/diff and znkr.io/diff/textdiff. I am certain that there are use cases not covered by this API, but I feel confident that it can evolve to cover these use cases in the future. For now, all my needs are fulfilled, but if you run into a situation that can't be solved by this API or requires some contortions, please tell me about it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Implementation&lt;/head&gt;
    &lt;p&gt;To implement this API, we need to implement a diff algorithm. There are a couple of standard diff algorithms that we can choose from. The choice of the algorithm as well as how it's implemented matters for the readability of the result as well as the performance.&lt;/p&gt;
    &lt;p&gt;A good starting point for this project was Myers' algorithm, simply because it's the fastest algorithm that can cover the whole API. In particular, the &lt;code&gt;...Func&lt;/code&gt; variants for &lt;code&gt;any&lt;/code&gt; types
instead of &lt;code&gt;comparable&lt;/code&gt; can't make use of a hash map. Patience and Histogram require the use of a
hash map for an efficient implementation, so Myers' really is the only choice. Another advantage of
Myers' compared to Patience and Histogram is that it will return optimal results.&lt;/p&gt;
    &lt;p&gt;On the flip side, in the comparison above, it came out as relatively slow compared to the patience diff algorithm and didn't produce the most readable results. It turns out, however, that this can be mitigated and almost completely overcome for &lt;code&gt;comparable&lt;/code&gt; types using
a combination of preprocessing, heuristics, and post-processing.&lt;/p&gt;
    &lt;p&gt;I am not going to cover the diff algorithm in detail here. There are a number of excellent articles on the web that describe it12, but I recommend reading the paper13: All articles I have seen try to keep a distance from the theory that makes this algorithm work, but that's not really helpful if you want to understand how and why this algorithm works.&lt;/p&gt;
    &lt;head rend="h4"&gt;Preprocessing&lt;/head&gt;
    &lt;p&gt;The most impactful way to improve the performance of Myers' algorithm is to reduce the problem size. The simplest thing to do is to strip any common prefix and suffix. This is always possible and helps a little. However, it can also reduce diff readability, because it will consume matching elements eagerly.&lt;/p&gt;
    &lt;p&gt;For example, let's say we have this change:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;package array
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;var m = []struct{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    name  string
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    year  int
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        name: "Freak Out!",
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        year: 1966,
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    },
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        name: "Absolutely Free",
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        year: 1967,
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    },
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        name: "We're Only in It for the Money",
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;17&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        year: 1967,
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    },
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;19&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If we eagerly consume the common prefix first and then the common suffix, the first 11 lines are all identical and the so are the last 4. This in turn would result in a different diff:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;package array
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;var m = []struct{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    name  string
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    year  int
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        name: "Freak Out!",
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        year: 1966,
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    },
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        name: "Absolutely Free",
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        year: 1967,
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    },
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        name: "We're Only in It for the Money",
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;17&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        year: 1967,
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    },
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;19&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Fortunately, this is easy to fix in post processing.&lt;/p&gt;
    &lt;p&gt;Much more impactful, but only efficiently possible for &lt;code&gt;comparable&lt;/code&gt; types, is to remove all elements
that are unique to either the left side or the right side, as those must always be deletions or
insertions. Non-&lt;code&gt;comparable&lt;/code&gt; types can't be keys in a hash map in Go, which is necessary for
checking uniqueness. This preprocessing step reduced the runtime by up to
99% for a few
real-world worst-case diffs.&lt;/p&gt;
    &lt;p&gt;In contrast to the suffix and prefix removal, stripping unique elements doesn't have any readability impact.&lt;/p&gt;
    &lt;head rend="h4"&gt;Heuristics&lt;/head&gt;
    &lt;p&gt;Another very impactful way to improve the performance is Anchoring. It is based on patience diff. The word patience is a bit misleading, because it's too easily associated with having to wait and it doesn't describe the heuristic very well either. It works by finding elements that are occur exactly once on both the left and the right side. When we matching up these unique pairs we create a segmentation of the input into smaller parts that can be analyzed individually. Even better, we're very likely to find matching lines atop and below such a pair of unique elements. This allows us to shrink the segments by stripping common prefixes and suffixes. This heuristic reduced the runtime by up to 95%. Unfortunately, finding unique elements and matching them up requires a hash map again which means that it can only be used for &lt;code&gt;comparable&lt;/code&gt; types.&lt;/p&gt;
    &lt;p&gt;There are two more heuristics that are I implemented. They help for non-&lt;code&gt;comparable&lt;/code&gt; types and as a
backstop when the other heuristics don't work. Their main purpose is to avoid runaway quadratic
growth. The Good Diagonal heuristic stops searching for a better solution if we found a solution
that's good enough and the Too Expensive heuristic shortcuts the search if it becomes too
expensive which reduces the worst-case complexity from 

 to


.&lt;/p&gt;
    &lt;p&gt;However, heuristics like this trade diff minimality for performance, this is not always desirable. Sometimes, a minimal diff is exactly what's required. &lt;code&gt;diff.Optimal&lt;/code&gt; disables these heuristics to always find a
minimal diff irrespective of the costs.&lt;/p&gt;
    &lt;head rend="h4"&gt;Post-processing&lt;/head&gt;
    &lt;p&gt;We established before that a diff algorithm finds one of many possible solutions. Given such a solution we can discover more solutions by it locally and then selecting the best solution according to some metric. This is exactly how Michael Haggerty's indentation heuristic works for text.&lt;/p&gt;
    &lt;p&gt;For any given diff, we can often slide the edits up or down in a way that doesn't change the meaning of a diff. For example,&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;["foo", "bar", "baz"].map do |i|
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;  i
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;end
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;["foo", "bar", "baz"].map do |i|
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;  i.upcase
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;end
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;has the same meaning as&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;["foo", "bar", "baz"].map do |i|
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;  i
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;end
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;["foo", "bar", "baz"].map do |i|
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;  i.upcase
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;end&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We call edits that can be slid up or down sliders. The question is, how do we select the best slide? Michael collected human ratings for different sliders of the same diff and used them to develop a heuristic to match these ratings: diff-slider-tools.&lt;/p&gt;
    &lt;p&gt;However, this heuristic only works for text and is tuned towards code instead of prose. I decided to make it optional. It can be enabled with the &lt;code&gt;textdiff.IndentHeuristic&lt;/code&gt; option.&lt;/p&gt;
    &lt;head rend="h4"&gt;Diff Representation&lt;/head&gt;
    &lt;p&gt;The representation used during the execution of the diff algorithm has a surprising impact on the algorithm performance and result readability. This is not at all obvious, and so it took me a while to figure out that the best approach is akin to a side-by-side view of a diff: You use two &lt;code&gt;[]bool&lt;/code&gt;
slices to represent the left side and the right side respectively: &lt;code&gt;true&lt;/code&gt; in the left side slice
represents a deletion and on the right side an insertion. &lt;code&gt;false&lt;/code&gt; is a matching element.&lt;/p&gt;
    &lt;p&gt;This representation has four big advantages: It can be preallocated, the order in which edits are discovered doesn't matter, it's easy to mutate during post-processing, and it's easy to generate other representations from it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Open Questions&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What exactly is the reason that two different algorithms produce different results? - I looked into this question a little, but I haven't found a conclusive answer yet.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Diff algorithms are relatively complicated by themselves, but they pale in comparison to what's necessary to provide a high-quality diff library. This article tries to explain what went into my new diff library, but there's still more that I haven't implemented yet.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Here is one real-world example of why worst-case scenarios are important: Imagine you're breaking an existing feature in a way that triggers a worst-case scenario in a test. If the test is running for a very long time or runs out of memory, you're going to have to debug two problems instead of one. ‚Ü©Ô∏é ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;See benchmark_comparison.txt for the source of these ratings. ‚Ü©Ô∏é ‚Ü©Ô∏é ‚Ü©Ô∏é ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The diffmatchpatch API is very hard to use ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No support for structured results ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Quadratic memory use; for my test cases, this resulted in &amp;gt;30 GB of memory used. ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The mb0 API is from before generics and is a bit cumbersome to use ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;udiff has a very low threshold for when it starts to stop searching for an optimal solution. This improves the speed, but it also results in relatively large diffs. ‚Ü©Ô∏é ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;There's no single patience diff heuristic, instead there are different implementations with different performance characteristics. ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Stolen from https://blog.jcoglan.com/2017/03/22/myers-diff-in-linear-space-theory/ ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I can recommend https://blog.robertelder.org/diff-algorithm/ and this 5 part series https://blog.jcoglan.com/2017/02/12/the-myers-diff-algorithm-part-1/ ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Myers, E.W. An O(ND) difference algorithm and its variations. Algorithmica 1, 251-266 (1986). https://doi.org/10.1007/BF01840446 ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45430604</guid><pubDate>Tue, 30 Sep 2025 20:09:44 +0000</pubDate></item><item><title>Mind the encryptionroot: How to save your data when ZFS loses its mind</title><link>https://sambowman.tech/blog/posts/mind-the-encryptionroot-how-to-save-your-data-when-zfs-loses-its-mind/</link><description>&lt;doc fingerprint="af8d194dd1ec8451"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mind the encryptionroot: How to save your data when ZFS loses its mind&lt;/head&gt;
    &lt;p&gt;While ZFS has a well-earned reputation for data integrity and reliability, OpenZFS native encryption has some incredibly sharp edges that will cut you if you don't know where to be careful. Unfortunately, I learned this the hard way, standing in a pool of my own blood and tears after thoroughly lacerating myself. I very nearly permanently lost 8.5 TiB of data after performing what should've been a series of simple, routine ZFS operations but resulted in an undecryptable dataset. Time has healed the wound enough that I am no longer filled with anguish just thinking about it, so I will now share my experience in the hope that you may learn from my mistakes. Together, we'll go over the unfortunate series of events that led to this happening and how it could've been avoided, learn how ZFS actually works under the hood, use our newfound knowledge to debug and reproduce the issue at hand, and finally compile a modified version of ZFS to repair the corrupted state and rescue our precious data. This is the postmortem of that terrible, horrible, no good, very bad week√¢¬¶&lt;/p&gt;
    &lt;head rend="h2"&gt;Table of Contents&lt;/head&gt;
    &lt;p&gt;Note: The issue covered in this postmortem only applies to OpenZFS native encryption. Oracle ZFS has its own encryption scheme which is different and, as far as I can tell, should not be vulnerable to this particular failure mode, though I have not personally tested it. Thank you to u/HobartTasmania for pointing this out!&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 1: An unfortunate series of events&lt;/head&gt;
    &lt;head rend="h3"&gt;The status quo&lt;/head&gt;
    &lt;p&gt; In the beginning, there were two ZFS pools: &lt;code&gt;old&lt;/code&gt; and &lt;code&gt;new&lt;/code&gt; (names changed for clarity). Each pool was hosted on an instance of TrueNAS CORE 13.0-U5.1 located at two different sites about an hour's drive apart with poor Internet connectivity between them. For this reason, a third pool &lt;code&gt;sneakernet&lt;/code&gt; was periodically moved between the two sites and used to exchange snapshots of &lt;code&gt;old&lt;/code&gt; and &lt;code&gt;new&lt;/code&gt; datasets for backup purposes. ZFS dataset snapshots would be indirectly relayed from &lt;code&gt;old&lt;/code&gt; to &lt;code&gt;new&lt;/code&gt; (and vice versa) using &lt;code&gt;sneakernet&lt;/code&gt; as an intermediate ZFS send/recv source/destination (e.g. &lt;code&gt;old/foo@2023-06-01&lt;/code&gt; -&amp;gt; &lt;code&gt;sneakernet/old/foo@2023-06-01&lt;/code&gt; -&amp;gt; &lt;code&gt;new/old/foo@2023-06-01&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt; The &lt;code&gt;new&lt;/code&gt; pool was natively encrypted from the very beginning. When ZFS snapshots were sent from &lt;code&gt;new&lt;/code&gt; to &lt;code&gt;sneakernet/new&lt;/code&gt; to &lt;code&gt;old/new&lt;/code&gt;, they were sent raw, meaning that blocks were copied unmodified in their encrypted form. To decrypt and mount them on &lt;code&gt;sneakernet&lt;/code&gt; or &lt;code&gt;old&lt;/code&gt;, you would need to first load &lt;code&gt;new&lt;/code&gt;'s hex encryption key, which is stored in TrueNAS's SQLite database.&lt;/p&gt;
    &lt;p&gt; The &lt;code&gt;old&lt;/code&gt; pool, on the other hand, was created before the advent of native encryption and was unencrypted for the first part of its life. Because it's desirable to encrypt data at rest, an encrypted dataset &lt;code&gt;sneakernet/old&lt;/code&gt; was created for &lt;code&gt;old&lt;/code&gt; using a passphrase encryption key when &lt;code&gt;sneakernet&lt;/code&gt; was set up. Unencrypted snapshots were sent non-raw from &lt;code&gt;old&lt;/code&gt; to &lt;code&gt;sneakernet/old&lt;/code&gt;, where they were encrypted, and then sent raw from &lt;code&gt;sneakernet/old&lt;/code&gt; to &lt;code&gt;new/old&lt;/code&gt;. To decrypt and mount them on &lt;code&gt;sneakernet&lt;/code&gt; or &lt;code&gt;new&lt;/code&gt;, you would need to first load &lt;code&gt;sneakernet&lt;/code&gt;'s passphrase encryption key.&lt;/p&gt;
    &lt;p&gt;This was all tested thoroughly and snapshots were proven to be readable at each point on every pool.&lt;/p&gt;
    &lt;head rend="h3"&gt;Encrypting the old pool&lt;/head&gt;
    &lt;p&gt; Now that we had encrypted snapshots of &lt;code&gt;old&lt;/code&gt; on &lt;code&gt;sneakernet/old&lt;/code&gt;, we wanted to encrypt &lt;code&gt;old&lt;/code&gt; itself. To do this, I simply took &lt;code&gt;old&lt;/code&gt; offline during a maintenance window to prevent new writes, took snapshots of all datasets, sent them to &lt;code&gt;sneakernet/old&lt;/code&gt;, and then sent the raw encrypted snapshots from &lt;code&gt;sneakernet/old&lt;/code&gt; back to &lt;code&gt;old/encrypted&lt;/code&gt;. Once I verified each dataset had been encrypted successfully, I destroyed the unencrypted dataset, updated the mount point of the encrypted dataset to that of the late unencrypted dataset, and then moved on to the next dataset. After all datasets were migrated, I used &lt;code&gt;zfs change-key -i&lt;/code&gt; to make all child datasets inherit from the new &lt;code&gt;old/encrypted&lt;/code&gt; encryption root, and then changed the key of the encryption root from a passphrase to a hex key, since TrueNAS only supported automatically unlocking datasets with hex encryption keys. Finally, I issued a &lt;code&gt;zpool initialize&lt;/code&gt; to overwrite all the unencrypted blocks which were now in unallocated space.&lt;/p&gt;
    &lt;p&gt; Spoiler Alert: It may not be immediately obvious why, but changing the encryption key on &lt;code&gt;old/encryption&lt;/code&gt; silently broke backups of &lt;code&gt;old&lt;/code&gt; datasets. Snapshots would still send and recv successfully, but were no longer decryptable or mountable. Since the encryption key is not normally loaded, and we only load it when periodically testing the backups, we would not realize until it was too late.&lt;/p&gt;
    &lt;p&gt;Lesson: Test backups continuously so you get immediate feedback when they break.&lt;/p&gt;
    &lt;head rend="h3"&gt;Decommissioning the old pool&lt;/head&gt;
    &lt;p&gt; Later, the &lt;code&gt;old&lt;/code&gt; pool was moved to the same site as the &lt;code&gt;new&lt;/code&gt; pool, so we wanted to fully decommission &lt;code&gt;old&lt;/code&gt; and migrate all its datasets to &lt;code&gt;new&lt;/code&gt;. I began going about this in a similar way. I took &lt;code&gt;old&lt;/code&gt; offline to prevent new writes, sent snapshots to &lt;code&gt;sneakernet/old&lt;/code&gt;, and then to &lt;code&gt;new/old&lt;/code&gt;. It was at this point that I made a very unfortunate mistake: I accidentally destroyed one dataset &lt;code&gt;old/encrypted/foo&lt;/code&gt; before verifying the files were readable on &lt;code&gt;new/old/foo&lt;/code&gt;, and I would soon realize that they were not.&lt;/p&gt;
    &lt;p&gt;Lesson: Wait to make all destructive changes together at the very end instead of interspersed where they could accidentally be performed in the wrong order.&lt;/p&gt;
    &lt;head rend="h3"&gt;The realization&lt;/head&gt;
    &lt;code&gt;[sam@newnas ~]$ DATASET=foo; [[ $(ssh sam@oldnas zfs list -H -o guid old/encrypted/${DATASET}@decomm) = $(zfs list -H -o guid sneakernet/old/${DATASET}@decomm) ]] &amp;amp;&amp;amp; echo "GUIDs match" || echo "GUIDs DO NOT MATCH"
GUIDs match
[sam@newnas ~]$ DATASET=foo; [[ $(zfs list -H -o guid sneakernet/old/${DATASET}@decomm) = $(zfs list -H -o guid new/old/${DATASET}@decomm) ]] &amp;amp;&amp;amp; echo "GUIDs match" || echo "GUIDs DO NOT MATCH"
GUIDs match

[sam@oldnas ~]$ sudo zfs destroy -r old/encrypted/foo

[sam@newnas ~]$ ls /mnt/new/old/foo
[sam@newnas ~]$ ls -a /mnt/new/old/foo
. ..
[sam@newnas ~]$ zfs list -o name,mounted new/old/foo
NAME         MOUNTED
new/old/foo  no
[sam@newnas ~]$ sudo zfs mount new/old/foo
cannot mount 'new/old/foo': Permission denied&lt;/code&gt;
    &lt;p&gt;What do you mean, permission denied? I am root!&lt;/p&gt;
    &lt;p&gt; Crap, I already destroyed &lt;code&gt;old/encrypted/foo&lt;/code&gt;. This is not good, but I can still restore it from the remaining copy on &lt;code&gt;sneakernet/old/foo&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;[sam@newnas ~]$ sudo zfs load-key sneakernet/old
Enter passphrase for 'sneakernet/old':
[sam@newnas ~]$ sudo zfs mount sneakernet/old/foo
cannot mount 'sneakernet/old/foo': Permission denied&lt;/code&gt;
    &lt;p&gt; Oh no, &lt;code&gt;sneakernet/old&lt;/code&gt; is broken too. This is very not good!&lt;/p&gt;
    &lt;p&gt;In an act of desperation, I tried rebooting the machine, but it didn't change a thing.&lt;/p&gt;
    &lt;p&gt;It is at this point that I realized:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Something has gone terribly wrong to prevent datasets on both &lt;code&gt;sneakernet/old&lt;/code&gt;and&lt;code&gt;new/old&lt;/code&gt;from mounting.&lt;/item&gt;
      &lt;item&gt;Whatever it is, it's not likely going to be easy to diagnose or fix.&lt;/item&gt;
      &lt;item&gt;There's a very real possibility the data might be gone forever.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I found myself in a hole and I wanted to stop digging. Fortunately, uptime was no longer critical for the &lt;code&gt;old&lt;/code&gt; datasets after the relocation, so I could afford to step away from the keyboard, collect my thoughts, and avoid making the situation any worse that it already was.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 2: Debugging the issue&lt;/head&gt;
    &lt;p&gt;Once the worst of the overwhelming, visceral feelings that come with the realization that you may have just caused permanent data loss had subsided, I started to work the incident and try to figure out why the backups aren't mounting.&lt;/p&gt;
    &lt;p&gt; As a precaution, I first exported the &lt;code&gt;old&lt;/code&gt; pool and took a forensic image of every disk in the pool. ZFS is a copy-on-write filesystem, so even though the dataset had been destroyed, most of the data was probably still on disk, just completely inaccessible with the normal ZFS tooling. In the worst case scenario, I may have had to try to forensically reconstruct the dataset from what was left on disk, and I didn't want to risk causing any more damage than I already had. Fortunately, I never had to use the disk images, but they still served as a valuable safety net while debugging and repairing.&lt;/p&gt;
    &lt;p&gt;Next, I realized that if we are to have any chance of debugging and fixing this issue, I need to learn how ZFS actually works.&lt;/p&gt;
    &lt;head rend="h3"&gt;Learning how ZFS actually works&lt;/head&gt;
    &lt;p&gt;I unfortunately did not keep track of every resource I consumed, but in addition to reading the source and docs, I found these talks by Jeff Bonwick, Bill Moore, and Matt Ahrens (the original creators of ZFS) to be particularly helpful in understanding the design and implementation of ZFS:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ZFS: The Last Word in File Systems Part 1&lt;/item&gt;
      &lt;item&gt;ZFS: The Last Word in File Systems Part 2&lt;/item&gt;
      &lt;item&gt;ZFS: The Last Word in File Systems Part 3&lt;/item&gt;
      &lt;item&gt;How ZFS Snapshots Really Work&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I highly recommend watching them all despite their age and somewhat poor recording quality, but will summarize the relevant information for those who don't have 3 hours to spare.&lt;/p&gt;
    &lt;p&gt;ZFS is a copy-on-write filesystem, which means that it does not overwrite blocks in place when a write is requested. Instead, the updated contents are written to a newly allocated block, and the old block is freed, which keeps the filesystem consistent if a write is interrupted. All blocks of both data and metadata are arranged in a Merkle tree structure where each block pointer contains a checksum of the child block, which allows ZFS to detect both block corruption and misdirected/phantom reads/writes. This means that any write will cause the block's checksum to change, which will then cause the parent block's checksum to change (since the parent block includes the block pointer which includes checksum of the child block that changed), and so on, all the way up to the root of the tree which ZFS calls an uberblock.&lt;/p&gt;
    &lt;p&gt;Uberblocks are written atomically, and because of the Merkle tree structure, they always represent a consistent snapshot of the entire filesystem at a point in time. Writes are batched together into transaction groups identified by a monotonically increasing counter, and each transaction group when synced to disk produces a new uberblock and associated filesystem tree. Taking a snapshot is then as simple as saving an uberblock and not freeing any of the blocks it points to.&lt;/p&gt;
    &lt;p&gt;In addition to the checksum, each block pointer also contains the transaction group id in which the child block was written, which is called the block's birth time or creation time. ZFS uses birth times to determine which blocks have been written before or after a snapshot. Any blocks with a birth time less than or equal to the snapshot's birth time, must have been written before the snapshot was taken, and conversely, any blocks with a birth time greater than the snapshot's birth time must have been written after the snapshot was taken.&lt;/p&gt;
    &lt;p&gt;One application of birth times is to generate incremental send streams between two snapshots. ZFS walks the tree but only needs to include blocks where the birth time is both greater than the first snapshot and less than or equal to the second snapshot. In fact, you don't even need to keep the data of the first snapshot around√¢you can create a bookmark which saves the snapshot's transaction id (but none of the data blocks), delete the snapshot to free its data, and then use the bookmark as the source to generate the same incremental send stream.&lt;/p&gt;
    &lt;p&gt;Spoiler Alert: Chekhov's bookmark will become relevant later.&lt;/p&gt;
    &lt;head rend="h3"&gt;Learning how ZFS native encryption actually works&lt;/head&gt;
    &lt;p&gt;ZFS native encryption is a relatively new feature, which was first released in OpenZFS 0.8.0 (2019) and subsequently made it into FreeBSD 13.0 (2021) when OpenZFS was adopted.&lt;/p&gt;
    &lt;p&gt;In addition to the docs, I found this 2016 talk on ZFS Native Encryption by Tom Caputi (the original author of native encryption) to be helpful in understanding its design and implementation. Again, I will summarize the relevant information.&lt;/p&gt;
    &lt;p&gt; ZFS native encryption works by encrypting dataset blocks with an symmetric authenticated encryption cipher suite (AES-256-GCM by default). To use native encryption, you must create a new dataset with &lt;code&gt;-o encryption=on&lt;/code&gt; which generates a unique master key for the dataset. The dataset's master key is then used to derive block data encryption keys with a salted HKDF.&lt;/p&gt;
    &lt;p&gt; The master key can't be changed, so it is encrypted with a wrapping key which can be changed. The wrapping key is provided by the user with &lt;code&gt;zfs load-key&lt;/code&gt; and can be changed with &lt;code&gt;zfs change-key&lt;/code&gt; which re-encrypts the same master key with a new wrapping key.&lt;/p&gt;
    &lt;p&gt;The encrypted master keys are stored in each dataset since each dataset has its own master key, but the wrapping key parameters are stored on what is called the encryption root dataset. The encryption root may be the same encrypted dataset, or it may be a parent of the encrypted dataset. When a child encrypted dataset inherits from a parent encryption root, the encryption root's wrapping key is used to decrypt the child dataset's master key. This is how one key can be used to unlock a parent encryption root dataset and all child encrypted datasets that inherit from it at the same time instead of having to load a key for every single encrypted dataset.&lt;/p&gt;
    &lt;p&gt; In our case, &lt;code&gt;new&lt;/code&gt;, &lt;code&gt;sneakernet/new&lt;/code&gt;, &lt;code&gt;sneakernet/old&lt;/code&gt;, and &lt;code&gt;old/encrypted&lt;/code&gt; are the encryption roots, and all child encrypted datasets inherit from them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Forming a hypothesis&lt;/head&gt;
    &lt;p&gt;At this point, we now know enough to form a hypothesis as to what may have happened. Feel free to pause here and try to figure it out on your own.&lt;/p&gt;
    &lt;p&gt; Recall that &lt;code&gt;sneakernet/old&lt;/code&gt; was created using a passphrase encryption key, and &lt;code&gt;old/encrypted&lt;/code&gt; was created by raw sending &lt;code&gt;sneakernet/old&lt;/code&gt;, so it initially used the same passphrase derived wrapping encryption key. When the &lt;code&gt;old/encrypted&lt;/code&gt; encryption key was changed from a passphrase to a hex key, ZFS must have changed the wrapping key parameters on the &lt;code&gt;old/encrypted&lt;/code&gt; encryption root and re-encrypted all child encrypted dataset master keys with the new hex wrapping key. Crucially, a new snapshot of &lt;code&gt;old/encrypted&lt;/code&gt; was never taken and sent to &lt;code&gt;sneakernet/old&lt;/code&gt; because it ostensibly didn't contain any data and was just a container for the child datasets.&lt;/p&gt;
    &lt;p&gt; Hypothesis: When subsequent snapshots were sent from &lt;code&gt;old&lt;/code&gt; to &lt;code&gt;sneakernet&lt;/code&gt;, the master keys of the child encrypted datasets were updated to be encrypted with the new hex wrapping key, but the &lt;code&gt;sneakernet/old&lt;/code&gt; encryption root was never updated with the new hex wrapping key parameters because a new snapshot was never sent. Therefore, when we load the key for &lt;code&gt;sneakernet/old&lt;/code&gt;, ZFS asks for the old passphrase, not a hex key, and when we try to mount &lt;code&gt;sneakernet/old/foo&lt;/code&gt;, it tries and fails to decrypt its master key with the old passphrase wrapping key instead of the new hex wrapping key.&lt;/p&gt;
    &lt;p&gt;If correct, this would explain the behavior we're seeing. To test this hypothesis, let's try to reproduce the issue in a test environment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Creating a test environment&lt;/head&gt;
    &lt;p&gt;TrueNAS CORE 13.0-U5.1 is based on FreeBSD 13.1, despite the different minor version numbers, so we'll create a FreeBSD 13.1 VM to test in. Make sure to include the system source tree and install on UFS so that we can build OpenZFS and reload the ZFS kernel module without rebooting.&lt;/p&gt;
    &lt;p&gt;TrueNAS CORE 13.0-U5.1 uses ZFS 2.1.11, so we'll want to build the same version from source for consistency. I started by reading the Building ZFS guide and following the steps documented there with some small modifications for FreeBSD since the page was clearly written with Linux in mind.&lt;/p&gt;
    &lt;p&gt;First, install the dependencies we'll need.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo pkg install autoconf automake autotools git gmake python devel/py-sysctl sudo&lt;/code&gt;
    &lt;p&gt;Then, clone ZFS and check out tag zfs-2.1.11.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ git clone https://github.com/openzfs/zfs
sam@zfshax:~ $ cd zfs
sam@zfshax:~/zfs $ git checkout zfs-2.1.11
sam@zfshax:~/zfs $ git show --summary
commit e25f9131d679692704c11dc0c1df6d4585b70c35 (HEAD, tag: zfs-2.1.11)
Author: Tony Hutter &amp;lt;hutter2@llnl.gov&amp;gt;
Date:   Tue Apr 18 11:44:34 2023 -0700

    Tag zfs-2.1.11

    META file and changelog updated.

    Signed-off-by: Tony Hutter &amp;lt;hutter2@llnl.gov&amp;gt;&lt;/code&gt;
    &lt;p&gt;Now, configure, build, and install ZFS.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~/zfs $ sh autogen.sh
sam@zfshax:~/zfs $ ./configure
sam@zfshax:~/zfs $ gmake -s -j$(sysctl -n hw.ncpu)    # &amp;lt;-- modified for FreeBSD
sam@zfshax:~/zfs $ sudo gmake install; sudo ldconfig  # &amp;lt;-- modified for FreeBSD&lt;/code&gt;
    &lt;p&gt;Then, replace the FreeBSD's ZFS kernel module with the one we just built.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~/zfs $ sudo kldunload zfs.ko  # Needed because zfs.sh only unloads openzfs.ko
sam@zfshax:~/zfs $ sudo ./scripts/zfs.sh&lt;/code&gt;
    &lt;p&gt;Finally, verify we're running version 2.1.11 as desired.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~/zfs $ sudo zfs version
zfs-2.1.11-1
zfs-kmod-2.1.11-1&lt;/code&gt;
    &lt;head rend="h3"&gt;Reproducing the issue&lt;/head&gt;
    &lt;p&gt;Now we're ready to try reproducing the issue. This took some iteration to get right, so I wrote a bash script that starts from scratch on each invocation and then runs the commands needed to reproduce the corrupt state. After quite a bit of trial and error, I eventually produced a reproducer script which does the following:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Create 2 pools: &lt;code&gt;src&lt;/code&gt;and&lt;code&gt;dst&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Create &lt;code&gt;src/encryptionroot&lt;/code&gt;using a passphrase encryption key.&lt;/item&gt;
      &lt;item&gt;Create &lt;code&gt;src/encryptionroot/child&lt;/code&gt;which inherits&lt;code&gt;src/encryptionroot&lt;/code&gt;as its encryption root.&lt;/item&gt;
      &lt;item&gt;Create files and take snapshots &lt;code&gt;src/encryptionroot@111&lt;/code&gt;and&lt;code&gt;src/encryptionroot/child@111&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Send raw snapshots &lt;code&gt;src/encryptionroot@111&lt;/code&gt;and&lt;code&gt;src/encryptionroot/child@111&lt;/code&gt;to&lt;code&gt;dst/encryptionroot&lt;/code&gt;and&lt;code&gt;dst/encryptionroot/child&lt;/code&gt;respectively.&lt;/item&gt;
      &lt;item&gt;Load encryption key for &lt;code&gt;dst/encryptionroot&lt;/code&gt;using passphrase and mount encrypted datasets&lt;code&gt;dst/encryptionroot&lt;/code&gt;and&lt;code&gt;dst/encryptionroot/child&lt;/code&gt;. At this point,&lt;code&gt;src&lt;/code&gt;and&lt;code&gt;dst&lt;/code&gt;pools are in sync.&lt;/item&gt;
      &lt;item&gt;Change the &lt;code&gt;src/encryptionroot&lt;/code&gt;encryption key from passphrase to hex.&lt;/item&gt;
      &lt;item&gt;Update files and take snapshots &lt;code&gt;src/encryptionroot@222&lt;/code&gt;and&lt;code&gt;src/encryptionroot/child@222&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Send a raw incremental snapshot of &lt;code&gt;src/encryptionroot/child@222&lt;/code&gt;to&lt;code&gt;dst/encryptionroot/child&lt;/code&gt;, but do not send&lt;code&gt;src/encryptionroot@222&lt;/code&gt;which contains the key change!&lt;/item&gt;
      &lt;item&gt;Unmount &lt;code&gt;dst/encryptionroot&lt;/code&gt;and&lt;code&gt;dst/encryptionroot/child&lt;/code&gt;and unload the cached encryption key for&lt;code&gt;dst/encryptionroot&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Load the encryption key for &lt;code&gt;dst/encryptionroot&lt;/code&gt;using the passphrase since we didn't send the updated encryption root after changing the key.&lt;/item&gt;
      &lt;item&gt;Try to remount &lt;code&gt;dst/encryptionroot&lt;/code&gt;and&lt;code&gt;dst/encryptionroot/child&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When we run the reproducer, the root encrypted dataset &lt;code&gt;dst/encryptionroot&lt;/code&gt; mounts successfully and we can read the old file from the first snapshot, but the child encrypted dataset &lt;code&gt;dst/encryptionroot/child&lt;/code&gt; fails to mount with &lt;code&gt;cannot mount 'dst/encryptionroot/child: Permission denied&lt;/code&gt; just as we expected.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo ./reproduce &amp;gt; /dev/null 2&amp;gt;&amp;amp;1
sam@zfshax:~ $ sudo zfs mount dst/encryptionroot/child
cannot mount 'dst/encryptionroot/child': Permission denied&lt;/code&gt;
    &lt;head title="Click to interact"&gt;Full reproducer script output (long!)&lt;/head&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo ./reproduce

Destroy pools and backing files if they exist.
+ zpool destroy src
+ zpool destroy dst
+ rm -f /src.img
+ rm -f /dst.img

Create pools using sparse files.
+ truncate -s 100M /src.img
+ truncate -s 100M /dst.img
+ zpool create -o ashift=12 -m /src src /src.img
+ zpool create -o ashift=12 -m /dst dst /dst.img

Create root encrypted dataset using a passphrase encryption key.
+ echo 'hunter2!'
+ zfs create -o encryption=on -o keyformat=passphrase -o keylocation=prompt src/encryptionroot

Create child encrypted dataset which inherits src/encryptionroot as its encryption root.
+ zfs create src/encryptionroot/child

Create files in the root and child encrypted datasets and snapshot both.
+ touch /src/encryptionroot/111
+ touch /src/encryptionroot/child/111
+ zfs snapshot -r src/encryptionroot@111

[ Checkpoint 1 ] Files and snapshots are on the src pool but not the dst pool yet.

NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
src                           -                   none        -            yes      1354282934008960312
src/encryptionroot            src/encryptionroot  passphrase  available    yes      12828913232342655944
src/encryptionroot@111        src/encryptionroot  -           available    -        14453618123048176778
src/encryptionroot/child      src/encryptionroot  passphrase  available    yes      10447093816713688124
src/encryptionroot/child@111  src/encryptionroot  -           available    -        10173467213034806911
NAME  ENCROOT  KEYFORMAT   KEYSTATUS    MOUNTED   GUID
dst   -        none        -            yes      5247064584420489120
/src
√¢√¢√¢ encryptionroot
    √¢√¢√¢ 111
    √¢√¢√¢ child
        √¢√¢√¢ 111
/dst

Send a raw replication stream of the src snapshots to the dst pool.
+ zfs send --replicate --raw src/encryptionroot@111
+ zfs recv dst/encryptionroot

Load encryption key for the dst encryption root using passphrase and mount the encrypted datasets.
+ echo 'hunter2!'
+ zfs load-key dst/encryptionroot
+ zfs mount dst/encryptionroot
+ zfs mount dst/encryptionroot/child

[ Checkpoint 2 ] Files and snapshots are on both pools and in sync.

NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
src                           -                   none        -            yes      1354282934008960312
src/encryptionroot            src/encryptionroot  passphrase  available    yes      12828913232342655944
src/encryptionroot@111        src/encryptionroot  -           available    -        14453618123048176778
src/encryptionroot/child      src/encryptionroot  passphrase  available    yes      10447093816713688124
src/encryptionroot/child@111  src/encryptionroot  -           available    -        10173467213034806911
NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
dst                           -                   none        -            yes      5247064584420489120
dst/encryptionroot            dst/encryptionroot  passphrase  available    yes      3076413147413645477
dst/encryptionroot@111        dst/encryptionroot  -           available    -        14453618123048176778
dst/encryptionroot/child      dst/encryptionroot  passphrase  available    yes      18246034838646533510
dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        10173467213034806911
/src
√¢√¢√¢ encryptionroot
    √¢√¢√¢ 111
    √¢√¢√¢ child
        √¢√¢√¢ 111
/dst
√¢√¢√¢ encryptionroot
    √¢√¢√¢ 111
    √¢√¢√¢ child
        √¢√¢√¢ 111

Change the src encryption root key from passphrase to hex.
+ echo 0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef
+ zfs change-key -o keyformat=hex src/encryptionroot

Update the files in the root and child encrypted datasets and snapshot both.
+ mv /src/encryptionroot/111 /src/encryptionroot/222
+ mv /src/encryptionroot/child/111 /src/encryptionroot/child/222
+ zfs snapshot -r src/encryptionroot@222

[ Checkpoint 3 ] Updated files and snapshots are on the src pool but not the dst pool yet.

NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
src                           -                   none        -            yes      1354282934008960312
src/encryptionroot            src/encryptionroot  hex         available    yes      12828913232342655944
src/encryptionroot@111        src/encryptionroot  -           available    -        14453618123048176778
src/encryptionroot@222        src/encryptionroot  -           available    -        929742392566496732
src/encryptionroot/child      src/encryptionroot  hex         available    yes      10447093816713688124
src/encryptionroot/child@111  src/encryptionroot  -           available    -        10173467213034806911
src/encryptionroot/child@222  src/encryptionroot  -           available    -        8161419639883744346
NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
dst                           -                   none        -            yes      5247064584420489120
dst/encryptionroot            dst/encryptionroot  passphrase  available    yes      3076413147413645477
dst/encryptionroot@111        dst/encryptionroot  -           available    -        14453618123048176778
dst/encryptionroot/child      dst/encryptionroot  passphrase  available    yes      18246034838646533510
dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        10173467213034806911
/src
√¢√¢√¢ encryptionroot
    √¢√¢√¢ 222
    √¢√¢√¢ child
        √¢√¢√¢ 222
/dst
√¢√¢√¢ encryptionroot
    √¢√¢√¢ 111
    √¢√¢√¢ child
        √¢√¢√¢ 111

Send a raw incremental snapshot of the child encrypted dataset to the dst pool.
+ zfs send --raw -i src/encryptionroot/child@111 src/encryptionroot/child@222
+ zfs recv -F dst/encryptionroot/child

NOTE: The encryption key change on the src encryption root has not been sent to dst!

[ Checkpoint 4 ] File is updated in the dst child encrypted dataset but not the dst root encrypted dataset.

NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
src                           -                   none        -            yes      1354282934008960312
src/encryptionroot            src/encryptionroot  hex         available    yes      12828913232342655944
src/encryptionroot@111        src/encryptionroot  -           available    -        14453618123048176778
src/encryptionroot@222        src/encryptionroot  -           available    -        929742392566496732
src/encryptionroot/child      src/encryptionroot  hex         available    yes      10447093816713688124
src/encryptionroot/child@111  src/encryptionroot  -           available    -        10173467213034806911
src/encryptionroot/child@222  src/encryptionroot  -           available    -        8161419639883744346
NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
dst                           -                   none        -            yes      5247064584420489120
dst/encryptionroot            dst/encryptionroot  passphrase  available    yes      3076413147413645477
dst/encryptionroot@111        dst/encryptionroot  -           available    -        14453618123048176778
dst/encryptionroot/child      dst/encryptionroot  hex         available    yes      18246034838646533510
dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        10173467213034806911
dst/encryptionroot/child@222  dst/encryptionroot  -           available    -        8161419639883744346
/src
√¢√¢√¢ encryptionroot
    √¢√¢√¢ 222
    √¢√¢√¢ child
        √¢√¢√¢ 222
/dst
√¢√¢√¢ encryptionroot
    √¢√¢√¢ 111
    √¢√¢√¢ child
        √¢√¢√¢ 222

NOTE: The updated file in the dst child encrypted dataset is only still readable because the encryption key is still loaded from before sending the snapshot taken after the key change.

Unmount the dst encrypted datasets and and unload the cached encryption key.
+ zfs unmount dst/encryptionroot
+ zfs unload-key dst/encryptionroot

Load the encryption key for the dst encryption root using the passphrase since we did not send the updated encryption root after changing the key.
+ echo 'hunter2!'
+ zfs load-key dst/encryptionroot

Try to remount dst encrypted datasets.
+ zfs mount dst/encryptionroot
+ zfs mount dst/encryptionroot/child
cannot mount 'dst/encryptionroot/child': Permission denied
+ true

[ Checkpoint 5 ] Mounting dst child encrypted dataset failed even though encryption key is ostensibly available. Hypothesis confirmed!

NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
src                           -                   none        -            yes      1354282934008960312
src/encryptionroot            src/encryptionroot  hex         available    yes      12828913232342655944
src/encryptionroot@111        src/encryptionroot  -           available    -        14453618123048176778
src/encryptionroot@222        src/encryptionroot  -           available    -        929742392566496732
src/encryptionroot/child      src/encryptionroot  hex         available    yes      10447093816713688124
src/encryptionroot/child@111  src/encryptionroot  -           available    -        10173467213034806911
src/encryptionroot/child@222  src/encryptionroot  -           available    -        8161419639883744346
NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
dst                           -                   none        -            yes      5247064584420489120
dst/encryptionroot            dst/encryptionroot  passphrase  available    yes      3076413147413645477
dst/encryptionroot@111        dst/encryptionroot  -           available    -        14453618123048176778
dst/encryptionroot/child      dst/encryptionroot  hex         available    no       18246034838646533510
dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        10173467213034806911
dst/encryptionroot/child@222  dst/encryptionroot  -           available    -        8161419639883744346
/src
√¢√¢√¢ encryptionroot
    √¢√¢√¢ 222
    √¢√¢√¢ child
        √¢√¢√¢ 222
/dst
√¢√¢√¢ encryptionroot
    √¢√¢√¢ 111
    √¢√¢√¢ child&lt;/code&gt;
    &lt;p&gt;Now that we understand and can reliably reproduce the issue, we're a big step closer to fixing it!&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 3: Recovering our data&lt;/head&gt;
    &lt;head rend="h3"&gt;Theoretically easy to fix&lt;/head&gt;
    &lt;p&gt;We know now that a child encrypted dataset will become unmountable if the following conditions are met:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The wrapping encryption key on the encryption root is changed.&lt;/item&gt;
      &lt;item&gt;A snapshot of the child encrypted dataset that was taken after the key change is sent.&lt;/item&gt;
      &lt;item&gt;A snapshot of the encryption root that was taken after the key change is not sent.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Lesson: Always send a snapshot of the encryption root after changing the encryption key.&lt;/p&gt;
    &lt;p&gt;In theory, all we should have to do to fix it is send the latest snapshot of the encryption root.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo ./reproduce &amp;gt; /dev/null 2&amp;gt;&amp;amp;1
sam@zfshax:~ $ sudo ./repair_snapshot

HYPOTHESIS: The child encrypted dataset should become decryptable again if a snapshot containing the key change on the root encrypted dataset is sent.

Send a raw incremental snapshot of the root encrypted dataset to the dst pool.
+ zfs send --raw -i src/encryptionroot@111 src/encryptionroot@222
+ zfs recv -F dst/encryptionroot

Unmount the dst encrypted datasets and and unload the cached encryption key.
+ zfs unmount dst/encryptionroot
+ zfs unload-key dst/encryptionroot

Load the encryption key for the dst encryption root using the hex key since we have now sent the updated encryption root after changing the key.
+ echo 0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef
+ zfs load-key dst/encryptionroot

Try to remount dst encrypted datasets.
+ zfs mount dst/encryptionroot
+ zfs mount dst/encryptionroot/child

RESULT: Child encrypted dataset is decryptable again. Hypothesis confirmed!

NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
src                           -                   none        -            yes      3822096046979704342
src/encryptionroot            src/encryptionroot  hex         available    yes      10687499872806328230
src/encryptionroot@111        src/encryptionroot  -           available    -        16650389156603898046
src/encryptionroot@222        src/encryptionroot  -           available    -        157927145464667221
src/encryptionroot/child      src/encryptionroot  hex         available    yes      15788284772663365294
src/encryptionroot/child@111  src/encryptionroot  -           available    -        8879828033920251704
src/encryptionroot/child@222  src/encryptionroot  -           available    -        6286619359795670820
NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
dst                           -                   none        -            yes      1835983340793043086
dst/encryptionroot            dst/encryptionroot  hex         available    yes      6911130245015256647
dst/encryptionroot@111        dst/encryptionroot  -           available    -        16650389156603898046
dst/encryptionroot@222        dst/encryptionroot  -           available    -        157927145464667221
dst/encryptionroot/child      dst/encryptionroot  hex         available    yes      15804809318195285947
dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        8879828033920251704
dst/encryptionroot/child@222  dst/encryptionroot  -           available    -        6286619359795670820
/src
√¢√¢√¢ encryptionroot
    √¢√¢√¢ 222
    √¢√¢√¢ child
        √¢√¢√¢ 222
/dst
√¢√¢√¢ encryptionroot
    √¢√¢√¢ 222
    √¢√¢√¢ child
        √¢√¢√¢ 222&lt;/code&gt;
    &lt;head rend="h3"&gt;Not so easy in practice&lt;/head&gt;
    &lt;p&gt; Unfortunately, this isn't enough to fix &lt;code&gt;new&lt;/code&gt; and &lt;code&gt;sneakernet&lt;/code&gt;; there are no remaining snapshots or bookmarks left on the &lt;code&gt;old&lt;/code&gt; encryption root from before the key change, and we can't generate an incremental send stream without one. Mapped to our reproduced example, this means that &lt;code&gt;src/encryptionroot@111&lt;/code&gt; does not exist.&lt;/p&gt;
    &lt;p&gt; You might think we could forcibly send the entire encryption root, but &lt;code&gt;zfs recv&lt;/code&gt; will reject it no matter what you do.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo zfs send --raw src/encryptionroot@222 | sudo zfs recv dst/encryptionroot
cannot receive new filesystem stream: destination 'dst/encryptionroot' exists
must specify -F to overwrite it

sam@zfshax:~ $ sudo zfs send --raw src/encryptionroot@222 | sudo zfs recv -F dst/encryptionroot
cannot receive new filesystem stream: destination has snapshots (eg. dst/encryptionroot@111)
must destroy them to overwrite it

sam@zfshax:~ $ sudo zfs destroy dst/encryptionroot@111
sam@zfshax:~ $ sudo zfs send --raw src/encryptionroot@222 | sudo zfs recv -F dst/encryptionroot
cannot receive new filesystem stream: zfs receive -F cannot be used to destroy an encrypted filesystem or overwrite an unencrypted one with an encrypted one&lt;/code&gt;
    &lt;p&gt;Lesson: Create bookmarks before destroying snapshots.&lt;/p&gt;
    &lt;p&gt;We need to find a way to create an incremental send stream that contains the key change, but how?. We could try to manually craft a send stream containing the new key, but that sounds tricky. There's got to be a better way!&lt;/p&gt;
    &lt;head rend="h3"&gt;Idea for a hack&lt;/head&gt;
    &lt;p&gt;Recall that a snapshot is not the only valid source for generating an incremental send stream. What if we had a bookmark?&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo ./reproduce &amp;gt; /dev/null 2&amp;gt;&amp;amp;1
sam@zfshax:~ $ sudo ./repair_bookmark

Replace the initial parent encrypted dataset snapshot with a bookmark.
+ zfs bookmark src/encryptionroot@111 src/encryptionroot#111
+ zfs destroy src/encryptionroot@111

HYPOTHESIS: The child encrypted dataset should become decryptable again if a snapshot containing the key change on the root encrypted dataset is sent.

Send a raw incremental snapshot of the root encrypted dataset to the dst pool using the bookmark.
+ zfs send --raw -i src/encryptionroot#111 src/encryptionroot@222
+ zfs recv -F dst/encryptionroot

Unmount the dst encrypted datasets and and unload the cached encryption key.
+ zfs unmount dst/encryptionroot
+ zfs unload-key dst/encryptionroot

Load the encryption key for the dst encryption root using the hex key since we have now sent the updated encryption root after changing the key.
+ echo 0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef
+ zfs load-key dst/encryptionroot

Try to remount dst encrypted datasets.
+ zfs mount dst/encryptionroot
+ zfs mount dst/encryptionroot/child

RESULT: Child encrypted dataset is decryptable again. Hypothesis confirmed!

NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
src                           -                   none        -            yes      1018261135296547862
src/encryptionroot            src/encryptionroot  hex         available    yes      1985286651877572312
src/encryptionroot@222        src/encryptionroot  -           available    -        4582898506955533479
src/encryptionroot#111        -                   -           -            -        4964628655505655411
src/encryptionroot/child      src/encryptionroot  hex         available    yes      12927592016081051429
src/encryptionroot/child@111  src/encryptionroot  -           available    -        15551239789901400488
src/encryptionroot/child@222  src/encryptionroot  -           available    -        11729357375613972731
NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
dst                           -                   none        -            yes      15258247229701443799
dst/encryptionroot            dst/encryptionroot  hex         available    yes      17755083343181277380
dst/encryptionroot@111        dst/encryptionroot  -           available    -        4964628655505655411
dst/encryptionroot@222        dst/encryptionroot  -           available    -        4582898506955533479
dst/encryptionroot/child      dst/encryptionroot  hex         available    yes      364333975888407846
dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        15551239789901400488
dst/encryptionroot/child@222  dst/encryptionroot  -           available    -        11729357375613972731
/src
√¢√¢√¢ encryptionroot
    √¢√¢√¢ 222
    √¢√¢√¢ child
        √¢√¢√¢ 222
/dst
√¢√¢√¢ encryptionroot
    √¢√¢√¢ 222
    √¢√¢√¢ child
        √¢√¢√¢ 222&lt;/code&gt;
    &lt;p&gt; A bookmark works just as well as a snapshot for generating an incremental send stream, but we don't have a bookmark on &lt;code&gt;old&lt;/code&gt; either. How is this any better?&lt;/p&gt;
    &lt;p&gt;Unlike a snapshot, which is effectively an entire dataset tree frozen in time (very complex), a bookmark is a very simple object on disk which consists of:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The GUID of the snapshot.&lt;/item&gt;
      &lt;item&gt;The transaction group the snapshot was created in.&lt;/item&gt;
      &lt;item&gt;The Unix timestamp when the snapshot was created.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example, this is what our bookmark looks like in &lt;code&gt;zdb&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo zdb src/encryptionroot#111
	#111: {guid: 44e5e7755d23c673 creation_txg: 12 creation_time: 1756699200 redaction_obj: 0}&lt;/code&gt;
    &lt;p&gt; Note that &lt;code&gt;zdb&lt;/code&gt; shows the GUID in hexadecimal versus &lt;code&gt;zfs get guid&lt;/code&gt; which shows it in decimal, consistency be damned. The &lt;code&gt;redaction_obj&lt;/code&gt; is optional and only used for redaction bookmarks, so we can ignore it.&lt;/p&gt;
    &lt;p&gt;A bookmark is simple enough that we could feasibly hack ZFS into manually writing one for us, provided that we can figure out the right values to use. The GUID and Unix timestamp don't really matter for generating an incremental send stream, so we could choose them arbitrarily if we had to, but the transaction group id really matters because that is what ZFS uses to determine which blocks to include.&lt;/p&gt;
    &lt;p&gt;But how can we figure out what transaction group the snapshot was created in if neither the snapshot nor a bookmark of the snapshot still exist? I initially considered walking the dataset trees on each pool, diffing them to find the newest block present on both datasets, and using its transaction group id, but I found a much easier way with one of ZFS's lesser known features.&lt;/p&gt;
    &lt;head rend="h3"&gt;A brief detour into pool histories&lt;/head&gt;
    &lt;p&gt;I didn't know about pool histories before embarking on this unplanned journey, but they are now yet another thing I love about ZFS. Every pool allocates 0.1% of its space (128 KiB minimum, 1 GiB maximum) to a ring buffer which is used to log every command that is executed on the pool. This can be used to forensically reconstruct the state of the pool over time.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo zpool history
History for 'dst':
2025-09-01.00:00:00 zpool create -o ashift=12 -m /dst dst /dst.img
2025-09-01.00:00:00 zfs recv dst/encryptionroot
2025-09-01.00:00:00 zfs load-key dst/encryptionroot
2025-09-01.00:00:00 zfs recv -F dst/encryptionroot/child
2025-09-01.00:00:00 zfs unload-key dst/encryptionroot
2025-09-01.00:00:00 zfs load-key dst/encryptionroot

History for 'src':
2025-09-01.00:00:00 zpool create -o ashift=12 -m /src src /src.img
2025-09-01.00:00:00 zfs create -o encryption=on -o keyformat=passphrase -o keylocation=prompt src/encryptionroot
2025-09-01.00:00:00 zfs create src/encryptionroot/child
2025-09-01.00:00:00 zfs snapshot -r src/encryptionroot@111
2025-09-01.00:00:00 zfs send --replicate --raw src/encryptionroot@111
2025-09-01.00:00:00 zfs change-key -o keyformat=hex src/encryptionroot
2025-09-01.00:00:00 zfs snapshot -r src/encryptionroot@222&lt;/code&gt;
    &lt;p&gt; ZFS also logs many internal operations in the pool history (search for &lt;code&gt;spa_history_log&lt;/code&gt; in the source code) which can be viewed with the &lt;code&gt;-i&lt;/code&gt; flag. For snapshots, this includes the transaction group (txg) id when the snapshot was created, which is exactly what we're looking for!&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo zpool history -i src
History for 'src':
...
2025-09-01.00:00:00 [txg:12] snapshot src/encryptionroot@111 (768)
2025-09-01.00:00:00 [txg:12] snapshot src/encryptionroot/child@111 (770)
2025-09-01.00:00:00 (3ms) ioctl snapshot
    input:
        snaps:
            src/encryptionroot@111
            src/encryptionroot/child@111
        props:

2025-09-01.00:00:00 zfs snapshot -r src/encryptionroot@111
...&lt;/code&gt;
    &lt;p&gt; The GUID and creation timestamp we can easily get from the snapshot on &lt;code&gt;dst&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo zfs list -o name,guid,creation -p dst/encryptionroot@111
NAME                                   GUID  CREATION
dst/encryptionroot@111  4964628655505655411  1756699200&lt;/code&gt;
    &lt;p&gt;Now that we know everything we need to create the bookmark, we just need to figure out a way to manually create a bookmark with arbitrary data.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hacking ZFS to manually create a bookmark&lt;/head&gt;
    &lt;p&gt; To understand how ZFS creates a bookmark, we can trace the code path from &lt;code&gt;zfs bookmark&lt;/code&gt; all the way down to &lt;code&gt;dsl_bookmark_add&lt;/code&gt; which actually adds the bookmark node to the tree.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;command_table&lt;/item&gt;
      &lt;item&gt;zfs_do_bookmark&lt;/item&gt;
      &lt;item&gt;lzc_bookmark&lt;/item&gt;
      &lt;item&gt;lzc_ioctl&lt;/item&gt;
      &lt;item&gt;zfs_ioctl_fd&lt;/item&gt;
      &lt;item&gt;zcmd_ioctl_compat&lt;/item&gt;
      &lt;item&gt;zfs_ioctl_register bookmark&lt;/item&gt;
      &lt;item&gt;zfs_ioc_bookmark&lt;/item&gt;
      &lt;item&gt;dsl_bookmark_create&lt;/item&gt;
      &lt;item&gt;dsl_bookmark_create_sync&lt;/item&gt;
      &lt;item&gt;dsl_bookmark_create_sync_impl_book&lt;/item&gt;
      &lt;item&gt;dsl_bookmark_node_add&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is the bookmark structure physically written to disk:&lt;/p&gt;
    &lt;p&gt;zfs/include/sys/dsl_bookmark.h&lt;/p&gt;
    &lt;code&gt;/*
 * On disk zap object.
 */
typedef struct zfs_bookmark_phys {
	uint64_t zbm_guid;		/* guid of bookmarked dataset */
	uint64_t zbm_creation_txg;	/* birth transaction group */
	uint64_t zbm_creation_time;	/* bookmark creation time */

	/* fields used for redacted send / recv */
	uint64_t zbm_redaction_obj;	/* redaction list object */
	uint64_t zbm_flags;		/* ZBM_FLAG_* */

	/* fields used for bookmark written size */
	uint64_t zbm_referenced_bytes_refd;
	uint64_t zbm_compressed_bytes_refd;
	uint64_t zbm_uncompressed_bytes_refd;
	uint64_t zbm_referenced_freed_before_next_snap;
	uint64_t zbm_compressed_freed_before_next_snap;
	uint64_t zbm_uncompressed_freed_before_next_snap;

	/* fields used for raw sends */
	uint64_t zbm_ivset_guid;
} zfs_bookmark_phys_t;


#define	BOOKMARK_PHYS_SIZE_V1	(3 * sizeof (uint64_t))
#define	BOOKMARK_PHYS_SIZE_V2	(12 * sizeof (uint64_t))&lt;/code&gt;
    &lt;p&gt; Only the first 3 fields are required for v1 bookmarks, while v2 bookmarks contain all 12 fields. &lt;code&gt;dsl_bookmark_node_add&lt;/code&gt; only writes a v2 bookmark if one of the 9 v2 fields are non-zero, so we can leave them all zero to write a v1 bookmark.&lt;/p&gt;
    &lt;p&gt; After a few iterations, I had a patch which hijacks the normal &lt;code&gt;zfs bookmark pool/dataset#src pool/dataset#dst&lt;/code&gt; code path to create a bookmark with arbitrary data when the source bookmark name is &lt;code&gt;missing&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~/zfs $ git --no-pager diff
sam@zfshax:~/zfs $ git --no-pager diff
diff --git a/cmd/zfs/zfs_main.c b/cmd/zfs/zfs_main.c
index 2d81ef31c..73b5d7e70 100644
--- a/cmd/zfs/zfs_main.c
+++ b/cmd/zfs/zfs_main.c
@@ -7892,12 +7892,15 @@ zfs_do_bookmark(int argc, char **argv)
 		default: abort();
 	}

+// Skip testing for #missing because it does not exist.
+if (strstr(source, "#missing") == NULL) {
 	/* test the source exists */
 	zfs_handle_t *zhp;
 	zhp = zfs_open(g_zfs, source, source_type);
 	if (zhp == NULL)
 		goto usage;
 	zfs_close(zhp);
+}

 	nvl = fnvlist_alloc();
 	fnvlist_add_string(nvl, bookname, source);
diff --git a/module/zfs/dsl_bookmark.c b/module/zfs/dsl_bookmark.c
index 861dd9239..fae882f45 100644
--- a/module/zfs/dsl_bookmark.c
+++ b/module/zfs/dsl_bookmark.c
@@ -263,7 +263,12 @@ dsl_bookmark_create_check_impl(dsl_pool_t *dp,
 		 * Source must exists and be an earlier point in newbm_ds's
 		 * timeline (newbm_ds's origin may be a snap of source's ds)
 		 */
+// Skip looking up #missing because it does not exist.
+if (strstr(source, "#missing") == NULL) {
 		error = dsl_bookmark_lookup(dp, source, newbm_ds, &amp;amp;source_phys);
+} else {
+		error = 0;
+}
 		switch (error) {
 		case 0:
 			break; /* happy path */
@@ -545,12 +550,34 @@ dsl_bookmark_create_sync_impl_book(
 	 *   because the redaction object might be too large
 	 */

+// Skip looking up #missing because it does not exist.
+if (strstr(source_name, "#missing") == NULL) {
 	VERIFY0(dsl_bookmark_lookup_impl(bmark_fs_source, source_shortname,
 	    &amp;amp;source_phys));
+}
 	dsl_bookmark_node_t *new_dbn = dsl_bookmark_node_alloc(new_shortname);

+// Skip copying from #missing because it does not exist.
+if (strstr(source_name, "#missing") == NULL) {
 	memcpy(&amp;amp;new_dbn-&amp;gt;dbn_phys, &amp;amp;source_phys, sizeof (source_phys));
 	new_dbn-&amp;gt;dbn_phys.zbm_redaction_obj = 0;
+} else {
+	// Manually set the bookmark parameters.
+	new_dbn-&amp;gt;dbn_phys = (zfs_bookmark_phys_t){
+		.zbm_guid = 4964628655505655411,
+		.zbm_creation_txg = 12,
+		.zbm_creation_time = 1756699200,
+		.zbm_redaction_obj = 0,
+		.zbm_flags = 0,
+		.zbm_referenced_bytes_refd = 0,
+		.zbm_compressed_bytes_refd = 0,
+		.zbm_uncompressed_bytes_refd = 0,
+		.zbm_referenced_freed_before_next_snap = 0,
+		.zbm_compressed_freed_before_next_snap = 0,
+		.zbm_uncompressed_freed_before_next_snap = 0,
+		.zbm_ivset_guid = 0,
+	};
+}

 	/* update feature counters */
 	if (new_dbn-&amp;gt;dbn_phys.zbm_flags &amp;amp; ZBM_FLAG_HAS_FBN) {
&lt;/code&gt;
    &lt;p&gt;To test, we recompile ZFS, reload the kernel module, and reimport the pools.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~/zfs $ gmake -s -j$(sysctl -n hw.ncpu)
sam@zfshax:~/zfs $ sudo gmake install &amp;amp;&amp;amp; sudo ldconfig
sam@zfshax:~/zfs $ sudo zpool export src &amp;amp;&amp;amp; sudo zpool export dst
sam@zfshax:~/zfs $ sudo ./scripts/zfs.sh -r
sam@zfshax:~/zfs $ sudo zpool import src -d / &amp;amp;&amp;amp; sudo zpool import dst -d /&lt;/code&gt;
    &lt;p&gt; Then, we create the bookmark ex nihilo using the magic bookmark name &lt;code&gt;missing&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~/zfs $ sudo zfs bookmark src/encryptionroot#missing src/encryptionroot#111
sam@zfshax:~/zfs $ sudo zdb src/encryptionroot#111
	#111: {guid: 44e5e7755d23c673 creation_txg: 12 creation_time: 1756699200 redaction_obj: 0}&lt;/code&gt;
    &lt;p&gt;Success! We can now use the bookmark to generate an incremental send stream containing the new hex wrapping key parameters.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~/zfs $ sudo zfs send --raw -i src/encryptionroot#111 src/encryptionroot@222 | zstreamdump
BEGIN record
	hdrtype = 1
	features = 1420004
	magic = 2f5bacbac
	creation_time = 68d3d93f
	type = 2
	flags = 0xc
	toguid = 3f99b9e92cc0aca7
	fromguid = 44e5e7755d23c673
	toname = src/encryptionroot@222
	payloadlen = 1028
nvlist version: 0
	crypt_keydata = (embedded nvlist)
	nvlist version: 0
		DSL_CRYPTO_SUITE = 0x8
		DSL_CRYPTO_GUID = 0x6196311f2622e30
		DSL_CRYPTO_VERSION = 0x1
		DSL_CRYPTO_MASTER_KEY_1 = 0x6c 0x55 0x13 0x78 0x8c 0x2d 0x42 0xb5 0x9e 0x33 0x2 0x7e 0x73 0x3a 0x46 0x20 0xd2 0xf7 0x23 0x7d 0x7c 0x5d 0x5f 0x76 0x63 0x90 0xd2 0x43 0x6a 0xdd 0x63 0x2b
		DSL_CRYPTO_HMAC_KEY_1 = 0x85 0xd1 0xf3 0xba 0xed 0xec 0x6 0x28 0x36 0xd6 0x60 0x28 0x8d 0x2f 0x6f 0x14 0xc9 0x2b 0x6f 0xf4 0x19 0x23 0x2d 0xf 0x3d 0xe 0xc4 0x88 0x4 0x6d 0xca 0xb5 0x2d 0x4d 0x8 0x75 0x17 0x1c 0xe3 0xe7 0xe6 0x23 0x7 0x53 0x94 0xba 0xc7 0x4b 0xf5 0xde 0x8c 0x29 0xa3 0x27 0xdf 0x82 0x64 0x9d 0x92 0xb4 0xc1 0x26 0x5b 0x32
		DSL_CRYPTO_IV = 0xdf 0x52 0x77 0xe8 0xf 0xfd 0xc2 0x42 0x66 0x88 0xb9 0xf0
		DSL_CRYPTO_MAC = 0x54 0x54 0x15 0xa4 0x21 0x55 0x6b 0x4e 0x99 0xe7 0xf 0xef 0x9f 0x90 0x42 0x54
		portable_mac = 0x3a 0xd6 0x30 0xc4 0x6a 0x2d 0x60 0x24 0x95 0xfc 0x99 0xbb 0xfa 0x10 0xa0 0x6b 0xc6 0x1 0xdd 0x1d 0x9 0xcd 0xa8 0x19 0xdf 0x57 0xb9 0x90 0x4f 0x2e 0x33 0xc1
		keyformat = 0x2
		pbkdf2iters = 0x0
		pbkdf2salt = 0x0
		mdn_checksum = 0x0
		mdn_compress = 0x0
		mdn_nlevels = 0x6
		mdn_blksz = 0x4000
		mdn_indblkshift = 0x11
		mdn_nblkptr = 0x3
		mdn_maxblkid = 0x4
		to_ivset_guid = 0x957edeaa7123a7
		from_ivset_guid = 0x0
	(end crypt_keydata)

END checksum = 14046201258/62f53166ccc36/14023a70758c3195/1e906f4670783cd
SUMMARY:
	Total DRR_BEGIN records = 1 (1028 bytes)
	Total DRR_END records = 1 (0 bytes)
	Total DRR_OBJECT records = 7 (960 bytes)
	Total DRR_FREEOBJECTS records = 2 (0 bytes)
	Total DRR_WRITE records = 1 (512 bytes)
	Total DRR_WRITE_BYREF records = 0 (0 bytes)
	Total DRR_WRITE_EMBEDDED records = 0 (0 bytes)
	Total DRR_FREE records = 12 (0 bytes)
	Total DRR_SPILL records = 0 (0 bytes)
	Total records = 26
	Total payload size = 2500 (0x9c4)
	Total header overhead = 8112 (0x1fb0)
	Total stream length = 10612 (0x2974)&lt;/code&gt;
    &lt;p&gt;But we can't receive the send stream.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo zfs send --raw -i src/encryptionroot#111 src/encryptionroot@222 | sudo zfs recv -F dst/encryptionroot
cannot receive incremental stream: IV set guid missing. See errata 4 at https://openzfs.github.io/openzfs-docs/msg/ZFS-8000-ER.&lt;/code&gt;
    &lt;head rend="h3"&gt;The final obstacle&lt;/head&gt;
    &lt;p&gt; ZFS refuses the stream because it is missing a source IV set GUID (see &lt;code&gt;from_ivset_guid = 0x0&lt;/code&gt; in the &lt;code&gt;zstreamdump&lt;/code&gt; above). This is because we created a v1 bookmark which does not contain the IV set GUID like a v2 bookmark would.&lt;/p&gt;
    &lt;p&gt;Since we know that the send stream is created using the right snapshots, we can temporarily disable checking IV set GUIDs to allow the snapshot to be received as described in errata 4.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo sysctl vfs.zfs.disable_ivset_guid_check=1
vfs.zfs.disable_ivset_guid_check: 0 -&amp;gt; 1
sam@zfshax:~ $ sudo zfs send --raw -i src/encryptionroot#111 src/encryptionroot@222 | sudo zfs recv -F dst/encryptionroot
sam@zfshax:~ $ sudo sysctl vfs.zfs.disable_ivset_guid_check=0
vfs.zfs.disable_ivset_guid_check: 1 -&amp;gt; 0
sam@zfshax:~ $ sudo zpool export dst
sam@zfshax:~ $ sudo zpool import dst -d /
sam@zfshax:~ $ sudo zpool scrub dst
sam@zfshax:~ $ sudo zpool status -x
all pools are healthy&lt;/code&gt;
    &lt;head rend="h3"&gt;The moment of truth&lt;/head&gt;
    &lt;p&gt;And now for the moment of truth√¢¬¶&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ echo "0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef" | sudo zfs load-key dst/encryptionroot
sam@zfshax:~ $ sudo zfs mount -a
sam@zfshax:~ $ sudo zfs list -t all -o name,encryptionroot,keyformat,keystatus,mounted,guid -r dst
NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
dst                           -                   none        -            yes      15258247229701443799
dst/encryptionroot            dst/encryptionroot  hex         available    yes      17755083343181277380
dst/encryptionroot@111        dst/encryptionroot  -           available    -        4964628655505655411
dst/encryptionroot@222        dst/encryptionroot  -           available    -        4582898506955533479
dst/encryptionroot/child      dst/encryptionroot  hex         available    yes      364333975888407846
dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        15551239789901400488
dst/encryptionroot/child@222  dst/encryptionroot  -           available    -        11729357375613972731
sam@zfshax:~ $ tree --noreport --noreport /dst
/dst
√¢√¢√¢ encryptionroot
    √¢√¢√¢ 222
    √¢√¢√¢ child
        √¢√¢√¢ 222&lt;/code&gt;
    &lt;p&gt; At this point, we can now reliably fix the issue in our test environment. All we need to do now is use our hacked ZFS build to create the bookmark on &lt;code&gt;old&lt;/code&gt;, send an incremental snapshot of the encryption root with the new key to &lt;code&gt;sneakernet&lt;/code&gt;, and then send that snapshot from &lt;code&gt;sneakernet&lt;/code&gt; to &lt;code&gt;new&lt;/code&gt;. I rebuilt ZFS again with the correct transaction group, GUID, and creation timestamp for &lt;code&gt;old&lt;/code&gt;, repeated the same steps with the names changed, and thanks to our thorough testing, it worked on the first try!&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;After a week of intense research and debugging, I had rescued our data back from the brink and could again sleep soundly at night. While I appreciated the opportunity to learn more about ZFS, I can't help but think about how this entire incident could have been avoided at several key points which translate directly into lessons learned:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Test backups continuously so you get immediate feedback when they break.&lt;/item&gt;
      &lt;item&gt;Wait to make all destructive changes together at the very end instead of interspersed where they could accidentally be performed in the wrong order.&lt;/item&gt;
      &lt;item&gt;Always send a snapshot of the encryption root after changing the encryption key.&lt;/item&gt;
      &lt;item&gt;Create bookmarks before destroying snapshots.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I hope that you may learn from my mistakes and avoid a similar incident. If you do happen to find yourself in a similar predicament, I'd love to hear from you regardless of whether this postmortem was helpful or not. My contact details can be found here.&lt;/p&gt;
    &lt;p&gt;Knowing what I now know about ZFS native encryption, I find it difficult to recommend until the sharp edges have all been filed down. In most cases, I'd prefer to encrypt the entire pool at the block device level and encrypt send streams with age. But if you really do need the flexibility offered by native encryption, always remember to mind the encryptionroot!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45431167</guid><pubDate>Tue, 30 Sep 2025 20:58:52 +0000</pubDate></item><item><title>Introduction to Multi-Armed Bandits (2019)</title><link>https://arxiv.org/abs/1904.07272</link><description>&lt;doc fingerprint="e68c8632ccf7c28f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Machine Learning&lt;/head&gt;&lt;p&gt; [Submitted on 15 Apr 2019 (v1), last revised 3 Apr 2024 (this version, v8)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Introduction to Multi-Armed Bandits&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Multi-armed bandits a simple but very powerful framework for algorithms that make decisions over time under uncertainty. An enormous body of work has accumulated over the years, covered in several books and surveys. This book provides a more introductory, textbook-like treatment of the subject. Each chapter tackles a particular line of work, providing a self-contained, teachable technical introduction and a brief review of the further developments; many of the chapters conclude with exercises.&lt;lb/&gt;The book is structured as follows. The first four chapters are on IID rewards, from the basic model to impossibility results to Bayesian priors to Lipschitz rewards. The next three chapters cover adversarial rewards, from the full-feedback version to adversarial bandits to extensions with linear rewards and combinatorially structured actions. Chapter 8 is on contextual bandits, a middle ground between IID and adversarial bandits in which the change in reward distributions is completely explained by observable contexts. The last three chapters cover connections to economics, from learning in repeated games to bandits with supply/budget constraints to exploration in the presence of incentives. The appendix provides sufficient background on concentration and KL-divergence.&lt;lb/&gt;The chapters on "bandits with similarity information", "bandits with knapsacks" and "bandits and agents" can also be consumed as standalone surveys on the respective topics.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Aleksandrs Slivkins [view email]&lt;p&gt;[v1] Mon, 15 Apr 2019 18:17:01 UTC (510 KB)&lt;/p&gt;&lt;p&gt;[v2] Mon, 29 Apr 2019 20:45:01 UTC (510 KB)&lt;/p&gt;&lt;p&gt;[v3] Tue, 25 Jun 2019 14:39:03 UTC (536 KB)&lt;/p&gt;&lt;p&gt;[v4] Sun, 15 Sep 2019 02:06:22 UTC (557 KB)&lt;/p&gt;&lt;p&gt;[v5] Mon, 30 Sep 2019 00:15:42 UTC (543 KB)&lt;/p&gt;&lt;p&gt;[v6] Sat, 26 Jun 2021 20:15:32 UTC (639 KB)&lt;/p&gt;&lt;p&gt;[v7] Sat, 8 Jan 2022 20:05:40 UTC (627 KB)&lt;/p&gt;&lt;p&gt;[v8] Wed, 3 Apr 2024 21:32:42 UTC (629 KB)&lt;/p&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.LG&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45431271</guid><pubDate>Tue, 30 Sep 2025 21:08:28 +0000</pubDate></item><item><title>CDC File Transfer</title><link>https://github.com/google/cdc-file-transfer</link><description>&lt;doc fingerprint="bcabb95bd7a534e1"&gt;
  &lt;main&gt;
    &lt;p&gt;Born from the ashes of Stadia, this repository contains tools for syncing and streaming files from Windows to Windows or Linux. The tools are based on Content Defined Chunking (CDC), in particular FastCDC, to split up files into chunks.&lt;/p&gt;
    &lt;p&gt;At Stadia, game developers had access to Linux cloud instances to run games. Most developers wrote their games on Windows, though. Therefore, they needed a way to make them available on the remote Linux instance.&lt;/p&gt;
    &lt;p&gt;As developers had SSH access to those instances, they could use &lt;code&gt;scp&lt;/code&gt; to copy
the game content. However, this was impractical, especially with the shift to
working from home during the pandemic with sub-par internet connections. &lt;code&gt;scp&lt;/code&gt;
always copies full files, there is no "delta mode" to copy only the things that
changed, it is slow for many small files, and there is no fast compression.&lt;/p&gt;
    &lt;p&gt;To help this situation, we developed two tools, &lt;code&gt;cdc_rsync&lt;/code&gt; and &lt;code&gt;cdc_stream&lt;/code&gt;,
which enable developers to quickly iterate on their games without repeatedly
incurring the cost of transmitting dozens of GBs.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;cdc_rsync&lt;/code&gt; is a tool to sync files from a Windows machine to a Linux device,
similar to the standard Linux rsync. It is
basically a copy tool, but optimized for the case where there is already an old
version of the files available in the target directory.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It quickly skips files if timestamp and file size match.&lt;/item&gt;
      &lt;item&gt;It uses fast compression for all data transfer.&lt;/item&gt;
      &lt;item&gt;If a file changed, it determines which parts changed and only transfers the differences.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The remote diffing algorithm is based on CDC. In our tests, it is up to 30x faster than the one used in &lt;code&gt;rsync&lt;/code&gt; (1500 MB/s vs 50 MB/s).&lt;/p&gt;
    &lt;p&gt;The following chart shows a comparison of &lt;code&gt;cdc_rsync&lt;/code&gt; and Linux &lt;code&gt;rsync&lt;/code&gt; running
under Cygwin on Windows. The test data consists of 58 development builds
of some game provided to us for evaluation purposes. The builds are 40-45 GB
large. For this experiment, we uploaded the first build, then synced the second
build with each of the two tools and measured the time. For example, syncing
from build 1 to build 2 took 210 seconds with the Cygwin &lt;code&gt;rsync&lt;/code&gt;, but only 75
seconds with &lt;code&gt;cdc_rsync&lt;/code&gt;. The three outliers are probably feature drops from
another development branch, where the delta was much higher. Overall,
&lt;code&gt;cdc_rsync&lt;/code&gt; syncs files about 3 times faster than Cygwin &lt;code&gt;rsync&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We also ran the experiment with the native Linux &lt;code&gt;rsync&lt;/code&gt;, i.e syncing Linux to
Linux, to rule out issues with Cygwin. Linux &lt;code&gt;rsync&lt;/code&gt; performed on average 35%
worse than Cygwin &lt;code&gt;rsync&lt;/code&gt;, which can be attributed to CPU differences. We did
not include it in the figure because of this, but you can find it
here.&lt;/p&gt;
    &lt;p&gt;The standard Linux &lt;code&gt;rsync&lt;/code&gt; splits a file into fixed-size chunks of typically
several KB.&lt;/p&gt;
    &lt;p&gt;If the file is modified in the middle, e.g. by inserting &lt;code&gt;xxxx&lt;/code&gt; after &lt;code&gt;567&lt;/code&gt;,
this usually means that the modified chunks as well as
all subsequent chunks change.&lt;/p&gt;
    &lt;p&gt;The standard &lt;code&gt;rsync&lt;/code&gt; algorithm hashes the chunks of the remote "old" file
and sends the hashes to the local device. The local device then figures out
which parts of the "new" file matches known chunks.&lt;/p&gt;
    &lt;p&gt;This is a simplification. The actual algorithm is more complicated and uses two hashes, a weak rolling hash and a strong hash, see here for a great overview. What makes &lt;code&gt;rsync&lt;/code&gt; relatively slow is the "no match" situation where the rolling hash does
not match any remote hash, and the algorithm has to roll the hash forward and
perform a hash map lookup for each byte. &lt;code&gt;rsync&lt;/code&gt; goes to
great lengths
optimizing lookups.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;cdc_rsync&lt;/code&gt; does not use fixed-size chunks, but instead variable-size,
content-defined chunks. That means, chunk boundaries are determined by the
local content of the file, in practice a 64 byte sliding window. For more
details, see
the FastCDC paper
or take a look at our implementation.&lt;/p&gt;
    &lt;p&gt;If the file is modified in the middle, only the modified chunks, but not subsequent chunks change (unless they are less than 64 bytes away from the modifications).&lt;/p&gt;
    &lt;p&gt;Computing the chunk boundaries is cheap and involves only a left-shift, a memory lookup, an &lt;code&gt;add&lt;/code&gt; and an &lt;code&gt;and&lt;/code&gt; operation for each input byte. This is cheaper
than the hash map lookup for the standard &lt;code&gt;rsync&lt;/code&gt; algorithm.&lt;/p&gt;
    &lt;p&gt;Because of this, the &lt;code&gt;cdc_rsync&lt;/code&gt; algorithm is faster than the standard
&lt;code&gt;rsync&lt;/code&gt;. It is also simpler. Since chunk boundaries move along with insertions
or deletions, the task to match local and remote hashes is a trivial set
difference operation. It does not involve a per-byte hash map lookup.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;cdc_stream&lt;/code&gt; is a tool to stream files and directories from a Windows machine to
a Linux device. Conceptually, it is similar to
sshfs, but it is optimized for read speed.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It caches streamed data on the Linux device.&lt;/item&gt;
      &lt;item&gt;If a file is re-read on Linux after it changed on Windows, only the differences are streamed again. The rest is read from the cache.&lt;/item&gt;
      &lt;item&gt;Stat operations are very fast since the directory metadata (filenames, permissions etc.) is provided in a streaming-friendly way.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To efficiently determine which parts of a file changed, the tool uses the same CDC-based diffing algorithm as &lt;code&gt;cdc_rsync&lt;/code&gt;. Changes to Windows files are almost
immediately reflected on Linux, with a delay of roughly (0.5s + 0.7s x total
size of changed files in GB).&lt;/p&gt;
    &lt;p&gt;The tool does not support writing files back from Linux to Windows; the Linux directory is readonly.&lt;/p&gt;
    &lt;p&gt;The following chart compares times from starting a game to reaching the menu. In one case, the game is streamed via &lt;code&gt;sshfs&lt;/code&gt;, in the other case we use
&lt;code&gt;cdc_stream&lt;/code&gt;. Overall, we see a 2x to 5x speedup.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;cdc_rsync&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;From&lt;/cell&gt;
        &lt;cell role="head"&gt;To&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Windows x86_64&lt;/cell&gt;
        &lt;cell&gt;‚úì&lt;/cell&gt;
        &lt;cell&gt;‚úì 1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ubuntu 22.04 x86_64&lt;/cell&gt;
        &lt;cell&gt;‚úó 2&lt;/cell&gt;
        &lt;cell&gt;‚úì&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ubuntu 22.04 aarch64&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;macOS 13 x86_64 3&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;macOS 13 aarch64 3&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;cdc_stream&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;From&lt;/cell&gt;
        &lt;cell role="head"&gt;To&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Windows x86_64&lt;/cell&gt;
        &lt;cell&gt;‚úì&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ubuntu 22.04 x86_64&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
        &lt;cell&gt;‚úì&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ubuntu 22.04 aarch64&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;macOS 13 x86_64 3&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;macOS 13 aarch64 3&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;1 Only local syncs, e.g. &lt;code&gt;cdc_rsync C:\src\* C:\dst&lt;/code&gt;. Support for
remote syncs is being added, see
#61.&lt;lb/&gt; 2 See #56.&lt;lb/&gt; 3 See #62.&lt;/p&gt;
    &lt;p&gt;Download the precompiled binaries from the latest release to a Windows device and unzip them. The Linux binaries are automatically deployed to &lt;code&gt;~/.cache/cdc-file-transfer&lt;/code&gt; by the Windows tools. There is no need to manually
deploy them. We currently provide Linux binaries compiled on
Github's latest Ubuntu version.
If the binaries work for you, you can skip the following two sections.&lt;/p&gt;
    &lt;p&gt;Alternatively, the project can be built from source. Some binaries have to be built on Windows, some on Linux.&lt;/p&gt;
    &lt;p&gt;To build the tools from source, the following steps have to be executed on both Windows and Linux.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Download and install Bazel from here. See workflow logs for the currently used version.&lt;/item&gt;
      &lt;item&gt;Clone the repository. &lt;code&gt;git clone https://github.com/google/cdc-file-transfer&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Initialize submodules. &lt;code&gt;cd cdc-file-transfer git submodule update --init --recursive&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Finally, install an SSH client on the Windows machine if not present. The file transfer tools require &lt;code&gt;ssh.exe&lt;/code&gt; and &lt;code&gt;sftp.exe&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The two tools CDC RSync and CDC Stream can be built and used independently.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;On a Linux device, build the Linux components &lt;code&gt;bazel build --config linux --compilation_mode=opt --linkopt=-Wl,--strip-all --copt=-fdata-sections --copt=-ffunction-sections --linkopt=-Wl,--gc-sections //cdc_rsync_server&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;On a Windows device, build the Windows components &lt;code&gt;bazel build --config windows --compilation_mode=opt --copt=/GL //cdc_rsync&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Copy the Linux build output file &lt;code&gt;cdc_rsync_server&lt;/code&gt;from&lt;code&gt;bazel-bin/cdc_rsync_server&lt;/code&gt;to&lt;code&gt;bazel-bin\cdc_rsync&lt;/code&gt;on the Windows machine.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;On a Linux device, build the Linux components &lt;code&gt;bazel build --config linux --compilation_mode=opt --linkopt=-Wl,--strip-all --copt=-fdata-sections --copt=-ffunction-sections --linkopt=-Wl,--gc-sections //cdc_fuse_fs&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;On a Windows device, build the Windows components &lt;code&gt;bazel build --config windows --compilation_mode=opt --copt=/GL //cdc_stream&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Copy the Linux build output files &lt;code&gt;cdc_fuse_fs&lt;/code&gt;and&lt;code&gt;libfuse.so&lt;/code&gt;from&lt;code&gt;bazel-bin/cdc_fuse_fs&lt;/code&gt;to&lt;code&gt;bazel-bin\cdc_stream&lt;/code&gt;on the Windows machine.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The tools require a setup where you can use SSH and SFTP from the Windows machine to the Linux device without entering a password, e.g. by using key-based authentication.&lt;/p&gt;
    &lt;p&gt;By default, the tools search &lt;code&gt;ssh.exe&lt;/code&gt; and &lt;code&gt;sftp.exe&lt;/code&gt; from the path environment
variable. If you can run the following commands in a Windows cmd without
entering your password, you are all set:&lt;/p&gt;
    &lt;code&gt;ssh user@linux.device.com
sftp user@linux.device.com
&lt;/code&gt;
    &lt;p&gt;Here, &lt;code&gt;user&lt;/code&gt; is the Linux user and &lt;code&gt;linux.device.com&lt;/code&gt; is the Linux host to
SSH into or copy the file to.&lt;/p&gt;
    &lt;p&gt;If additional arguments are required, it is recommended to provide an SSH config file. By default, both &lt;code&gt;ssh.exe&lt;/code&gt; and &lt;code&gt;sftp.exe&lt;/code&gt; use the file at
&lt;code&gt;%USERPROFILE%\.ssh\config&lt;/code&gt; on Windows, if it exists. A possible config file
that sets a username, a port, an identity file and a known host file could look
as follows:&lt;/p&gt;
    &lt;code&gt;Host linux_device
	HostName linux.device.com
	User user
	Port 12345
	IdentityFile C:\path\to\id_rsa
	UserKnownHostsFile C:\path\to\known_hosts
&lt;/code&gt;
    &lt;p&gt;If &lt;code&gt;ssh.exe&lt;/code&gt; or &lt;code&gt;sftp.exe&lt;/code&gt; cannot be found, you can specify the full paths via
the command line arguments &lt;code&gt;--ssh-command&lt;/code&gt; and &lt;code&gt;--sftp-command&lt;/code&gt; for &lt;code&gt;cdc_rsync&lt;/code&gt;
and &lt;code&gt;cdc_stream start&lt;/code&gt; (see below), or set the environment variables
&lt;code&gt;CDC_SSH_COMMAND&lt;/code&gt; and &lt;code&gt;CDC_SFTP_COMMAND&lt;/code&gt;, e.g.&lt;/p&gt;
    &lt;code&gt;set CDC_SSH_COMMAND="C:\path with space\to\ssh.exe"
set CDC_SFTP_COMMAND="C:\path with space\to\sftp.exe"
&lt;/code&gt;
    &lt;p&gt;Note that you can also specify SSH configuration via the environment variables instead of using a config file:&lt;/p&gt;
    &lt;code&gt;set CDC_SSH_COMMAND=C:\path\to\ssh.exe -p 12345 -i C:\path\to\id_rsa -oUserKnownHostsFile=C:\path\to\known_hosts
set CDC_SFTP_COMMAND=C:\path\to\sftp.exe -P 12345 -i C:\path\to\id_rsa -oUserKnownHostsFile=C:\path\to\known_hosts
&lt;/code&gt;
    &lt;p&gt;Note the small &lt;code&gt;-p&lt;/code&gt; for &lt;code&gt;ssh.exe&lt;/code&gt; and the capital &lt;code&gt;-P&lt;/code&gt; for &lt;code&gt;sftp.exe&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;For Google internal usage, set the following environment variables to enable SSH authentication using a Google security key:&lt;/p&gt;
    &lt;code&gt;set CDC_SSH_COMMAND=C:\gnubby\bin\ssh.exe
set CDC_SFTP_COMMAND=C:\gnubby\bin\sftp.exe
&lt;/code&gt;
    &lt;p&gt;Note that you will have to touch the security key multiple times during the first run. Subsequent runs only require a single touch.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;cdc_rsync&lt;/code&gt; is used similar to &lt;code&gt;scp&lt;/code&gt; or the Linux &lt;code&gt;rsync&lt;/code&gt; command. To sync a
single Windows file &lt;code&gt;C:\path\to\file.txt&lt;/code&gt; to the home directory &lt;code&gt;~&lt;/code&gt; on the Linux
device &lt;code&gt;linux.device.com&lt;/code&gt;, run&lt;/p&gt;
    &lt;code&gt;cdc_rsync C:\path\to\file.txt user@linux.device.com:~
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;cdc_rsync&lt;/code&gt; understands the usual Windows wildcards &lt;code&gt;*&lt;/code&gt; and &lt;code&gt;?&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;cdc_rsync C:\path\to\*.txt user@linux.device.com:~
&lt;/code&gt;
    &lt;p&gt;To sync the contents of the Windows directory &lt;code&gt;C:\path\to\assets&lt;/code&gt; recursively to
&lt;code&gt;~/assets&lt;/code&gt; on the Linux device, run&lt;/p&gt;
    &lt;code&gt;cdc_rsync C:\path\to\assets\* user@linux.device.com:~/assets -r
&lt;/code&gt;
    &lt;p&gt;To get per file progress, add &lt;code&gt;-v&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;cdc_rsync C:\path\to\assets\* user@linux.device.com:~/assets -vr
&lt;/code&gt;
    &lt;p&gt;The tool also supports local syncs:&lt;/p&gt;
    &lt;code&gt;cdc_rsync C:\path\to\assets\* C:\path\to\destination -vr
&lt;/code&gt;
    &lt;p&gt;To stream the Windows directory &lt;code&gt;C:\path\to\assets&lt;/code&gt; to &lt;code&gt;~/assets&lt;/code&gt; on the Linux
device, run&lt;/p&gt;
    &lt;code&gt;cdc_stream start C:\path\to\assets user@linux.device.com:~/assets
&lt;/code&gt;
    &lt;p&gt;This makes all files and directories in &lt;code&gt;C:\path\to\assets&lt;/code&gt; available on
&lt;code&gt;~/assets&lt;/code&gt; immediately, as if it were a local copy. However, data is streamed
from Windows to Linux as files are accessed.&lt;/p&gt;
    &lt;p&gt;To stop the streaming session, enter&lt;/p&gt;
    &lt;code&gt;cdc_stream stop user@linux.device.com:~/assets
&lt;/code&gt;
    &lt;p&gt;The command also accepts wildcards. For instance,&lt;/p&gt;
    &lt;code&gt;cdc_stream stop user@*:*
&lt;/code&gt;
    &lt;p&gt;stops all existing streaming sessions for the given user.&lt;/p&gt;
    &lt;p&gt;On first run, &lt;code&gt;cdc_stream&lt;/code&gt; starts a background service, which does all the work.
The &lt;code&gt;cdc_stream start&lt;/code&gt; and &lt;code&gt;cdc_stream stop&lt;/code&gt; commands are just RPC clients that
talk to the service.&lt;/p&gt;
    &lt;p&gt;The service logs to &lt;code&gt;%APPDATA%\cdc-file-transfer\logs&lt;/code&gt; by default. The logs are
useful to investigate issues with asset streaming. To pass custom arguments, or
to debug the service, create a JSON config file at
&lt;code&gt;%APPDATA%\cdc-file-transfer\cdc_stream.json&lt;/code&gt; with command line flags.
For instance,&lt;/p&gt;
    &lt;code&gt;{ "verbosity":3 }
&lt;/code&gt;
    &lt;p&gt;instructs the service to log debug messages. Try &lt;code&gt;cdc_stream start-service -h&lt;/code&gt;
for a list of available flags. Alternatively, run the service manually with&lt;/p&gt;
    &lt;code&gt;cdc_stream start-service
&lt;/code&gt;
    &lt;p&gt;and pass the flags as command line arguments. When you run the service manually, the flag &lt;code&gt;--log-to-stdout&lt;/code&gt; is particularly useful as it logs to the console
instead of to the file.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;cdc_rsync&lt;/code&gt; always logs to the console. To increase log verbosity, pass &lt;code&gt;-vvv&lt;/code&gt;
for debug logs or &lt;code&gt;-vvvv&lt;/code&gt; for verbose logs.&lt;/p&gt;
    &lt;p&gt;For both sync and stream, the debug logs contain all SSH and SFTP commands that are attempted to run, which is very useful for troubleshooting. If a command fails unexpectedly, copy it and run it in isolation. Pass &lt;code&gt;-vv&lt;/code&gt; or &lt;code&gt;-vvv&lt;/code&gt; for
additional debug output.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45433768</guid><pubDate>Wed, 01 Oct 2025 02:38:18 +0000</pubDate></item><item><title>The gaslit asset class</title><link>https://blog.dshr.org/2025/09/the-gaslit-asset-class.html</link><description>&lt;doc fingerprint="a509d1dc08278a89"&gt;
  &lt;main&gt;&lt;head rend="h3"&gt;The Gaslit Asset Class&lt;/head&gt;Before I explain that much of what you have been told about cryptocurrency technology is gaslighting, I should stress that I hold no long or short positions in cryptocurrencies, their derivatives or related companies. Unlike most people discussing them, I am not "talking my book".&lt;p&gt;To fit in the allotted time, this talk focuses mainly on Bitcoin and omits many of the finer points. My text, with links to the sources and additional material in footnotes, will go up on my blog later today.&lt;/p&gt;&lt;head rend="h3"&gt;Why Am I Here?&lt;/head&gt;I imagine few of you would understand why a retired software engineer with more than forty years in Silicon Valley was asked to address you on cryptocurrencies[1].&lt;table&gt;&lt;row&gt;&lt;cell&gt;NVDA Log Plot&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;So my wife and I started a program at Stanford that is still running 27 years later. She was a career librarian at the Library of Congress and the Stanford Library. She was part of the team that, 30 years ago, pioneered the transition of academic publishing to the Web. She was also the person who explained citation indices to Larry and Sergey, which led to Page Rank.&lt;/p&gt;&lt;p&gt;The academic literature has archival value. Multiple libraries hold complete runs on paper of the Philosophical Transactions of the Royal Society starting 360 years ago[2]. The interesting engineering problem we faced was how to enable libraries to deliver comparable longevity to Web-published journals.&lt;/p&gt;&lt;head rend="h3"&gt;Five Years Before Satoshi Nakamoto&lt;/head&gt;I worked with a group of outstanding Stanford CS Ph.D. students to design and implement a system for stewardship of Web content modeled on the paper library system. The goal was to make it extremely difficult for even a powerful adversary to delete or modify content without detection. It is called LOCKSS, for Lots Of Copies Keep Stuff Safe; a decentralized peer-to-peer system secured by Proof-of-Work. We won a "Best Paper" award for it five years before Satoshi Nakamoto published his decentralized peer-to-peer system secured by Proof-of-Work. When he did, LOCKSS had been in production for a few years and we had learnt a lot about how difficult decentralization is in the online world.&lt;p&gt;Bitcoin built on more than two decades of research. Neither we nor Nakamoto invented Proof-of-Work, Cynthia Dwork and Moni Naor published it in 1992. Nakamoto didn't invent blockchains, Stuart Haber and W. Scott Stornetta patented them in 1991. He was extremely clever in assembling well-known techniques into a cryptocurrency, but his only major innovation was the Longest Chain Rule.&lt;/p&gt;&lt;head rend="h3"&gt;Digital cash&lt;/head&gt;The fundamental problem of representing cash in digital form is that a digital coin can be endlessly copied, thus you need some means to prevent each of the copies being spent. When you withdraw cash from an ATM, turning digital cash in your account into physical cash in your hand, the bank performs an atomic transaction against the database mapping account numbers to balances. The bank is trusted to prevent multiple spending.&lt;p&gt;There had been several attempts at a cryptocurrency before Bitcoin. The primary goals of the libertarians and cypherpunks were that a cryptocurrency be as anonymous as physical cash, and that it not have a central point of failure that had to be trusted. The only one to get any traction was David Chaum's DigiCash; it was anonymous but it was centralized to prevent multiple spending and it involved banks.&lt;/p&gt;&lt;head rend="h3"&gt;Nakamoto's magnum opus&lt;/head&gt;&lt;p&gt; Bitcoin claims: &lt;/p&gt; When in November 2008 Nakamoto published Bitcoin: A Peer-to-Peer Electronic Cash System it was the peak of the Global Financial Crisis and people were very aware that the financial system was broken (and it still is). Because it solved many of the problems that had dogged earlier attempts at electronic cash, it rapidly attracted a clique of enthusiasts. When Nakamoto went silent in 2010 they took over proseltyzing the system. The main claims they made were:&lt;list rend="ul"&gt;&lt;item&gt;The system was trustless because it was decentralized.&lt;/item&gt;&lt;item&gt;It was a medium of exchange for buying and selling in the real world.&lt;/item&gt;&lt;item&gt;Transactions were faster and cheaper than in the existing financial system.&lt;/item&gt;&lt;item&gt;It was secured by Proof-of-Work and cryptography.&lt;/item&gt;&lt;item&gt;It was privacy-preserving.&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;The system was trustless because it was decentralized.&lt;/item&gt;&lt;item&gt;It was a medium of exchange for buying and selling in the real world.&lt;/item&gt;&lt;item&gt;Transactions were faster and cheaper than in the existing financial system.&lt;/item&gt;&lt;item&gt;It was secured by Proof-of-Work and cryptography.&lt;/item&gt;&lt;item&gt;It was privacy-preserving.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Trustless because decentralized (1)&lt;/head&gt;Assuming that the Bitcoin network consists of a large number of roughly equal nodes, it randomly selects a node to determine the transactions that will form the next block. There is no need to trust any particular node because the chance that they will be selected is small.[3]&lt;p&gt; At first, most users would run network nodes, but as the network grows beyond a certain point, it would be left more and more to specialists with server farms of specialized hardware. A server farm would only need to have one node on the network and the rest of the LAN connects with that one node. &lt;/p&gt; But only three days after publishing his white paper, Nakamoto understood that this assumption would become false:&lt;quote&gt;Satoshi Nakamoto 2nd November 2008The current system where every user is a network node is not the intended configuration for large scale. ... The design supports letting users just be users. The more burden it is to run a node, the fewer nodes there will be. Those few nodes will be big server farms. The rest will be client nodes that only do transactions and don‚Äôt generate.&lt;/quote&gt;&lt;quote&gt;Satoshi Nakamoto: 29th July 2010&lt;/quote&gt;&lt;quote&gt;At first, most users would run network nodes, but as the network grows beyond a certain point, it would be left more and more to specialists with server farms of specialized hardware.He didn't change his mind. On 29th July 2010, less than five months before he went silent, he made the same point:&lt;/quote&gt;&lt;quote&gt;The current system where every user is a network node is not the intended configuration for large scale. ... The design supports letting users just be users. The more burden it is to run a node, the fewer nodes there will be. Those few nodes will be big server farms."Letting users be users" necessarily means that the "users" have to trust the "few nodes" to include their transactions in blocks. The very strong economies of scale of technology in general and "big server farms" in particular meant that the centralizing force described in W. Brian Arthur's 1994 book Increasing Returns and Path Dependence in the Economy resulted in there being "fewer nodes". Indeed, on 13th June 2014 a single node controlled 51% of Bitcoin's mining, the GHash pool.[4]&lt;/quote&gt;&lt;head rend="h3"&gt;Trustless because decentralized (2)&lt;/head&gt;In June 2022 Cooperation among an anonymous group protected Bitcoin during failures of decentralization by Alyssa Blackburn et al showed that it had not been decentralized from the very start. The same month a DARPA-sponsored report entitled Are Blockchains Decentralized? by a large team from the Trail of Bits security company examined the economic and many other centralizing forces affecting a wide range of blockchain implementations and concluded that the answer to their question is "No".[5]&lt;p&gt;The same centralizing economic forces apply to Proof-of-Stake blockchains such as Ethereum. Grant's Memo to the bitcoiners explained the process last February.&lt;/p&gt;&lt;head rend="h3"&gt;Trustless because decentralized (3)&lt;/head&gt;Another centralizing force drives pools like GHash. The network creates a new block and rewards the selected node about every ten minutes. Assuming they're all state-of-the-art, there are currently about 15M rigs mining Bitcoin[6]. Their economic life is around 18 months, so only 0.5%% of them will ever earn a reward. The owners of mining rigs pool their efforts, converting a small chance of a huge reward into a steady flow of smaller rewards. On average GHash was getting three rewards an hour.&lt;head rend="h3"&gt;A medium of exchange (1)&lt;/head&gt;&lt;p&gt; Quote from: Insti, July 17, 2010, 02:33:41 AM&lt;/p&gt; Bitcoin's ten-minute block time is a problem for real-world buying and selling[7], but the problem is even worse. Network delays mean a transaction isn't final when you see it in a block. Assuming no-one controlled more than 10% of the hashing power, Nakamoto required another 5 blocks to have been added to the chain, so 99.9% finality would take an hour. With a more realistic 30%, the rule should have been 23 blocks, with finality taking 4 hours[8].&lt;quote&gt;How would a Bitcoin snack machine work?I believe it‚Äôll be possible for a payment processing company to provide as a service the rapid distribution of transactions with good-enough checking in something like 10 seconds or less.&lt;list&gt;You don‚Äôt want to have to wait an hour for you transaction to be confirmed.&lt;/list&gt;&lt;item&gt;You want to walk up to the machine. Send it a bitcoin.&lt;/item&gt;&lt;item&gt;?&lt;/item&gt;&lt;item&gt;Walk away eating your nice sugary snack. (Profit!)&lt;/item&gt;&lt;lb/&gt;The vending machine company doesn‚Äôt want to give away lots of free candy.&lt;lb/&gt;How does step 2 work?&lt;/quote&gt;&lt;quote&gt;Satoshi Nakamoto: 17th July 2010&lt;/quote&gt;&lt;p&gt;Nakamoto's 17th July 2010 exchange with Insti shows he understood that the Bitcoin network couldn't be used for ATMs, vending machines, buying drugs or other face-to-face transactions because he went on to describe how a payment processing service layered on top of it would work.&lt;/p&gt;&lt;head rend="h3"&gt;A medium of exchange (2)&lt;/head&gt;&lt;p&gt; assuming that the two sides are rational actors and the smart contract language is Turing-complete, there is no escrow smart contract that can facilitate this exchange without either relying on third parties or enabling at least one side to extort the other.&lt;lb/&gt;two-party escrow smart contracts are ... simply a game of who gets to declare their choice Ô¨Årst and commit it on the blockchain sooner, hence forcing the other party to concur with their choice. The order of transactions on a blockchain is essentially decided by the miners. Thus, the party with better connectivity to the miners or who is willing to pay higher transaction fees, would be able to declare their choice to the smart contract Ô¨Årst and extort the other party.&lt;/p&gt; The situation is even worse when it comes to buying and selling real-world objects via programmable blockchains such as Ethereum[9]. In 2021 Amir Kafshdar Goharshady showed that[10]:&lt;p&gt;two-party escrow smart contracts are ... simply a game of who gets to declare their choice Ô¨Årst and commit it on the blockchain sooner, hence forcing the other party to concur with their choice. The order of transactions on a blockchain is essentially decided by the miners. Thus, the party with better connectivity to the miners or who is willing to pay higher transaction fees, would be able to declare their choice to the smart contract Ô¨Årst and extort the other party.&lt;/p&gt;&lt;quote&gt;Amir Kafshdar Goharshady, Irrationality, Extortion, or Trusted Third-parties: Why it is Impossible to Buy and Sell Physical Goods Securely on the Blockchain&lt;/quote&gt;&lt;quote&gt;assuming that the two sides are rational actors and the smart contract language is Turing-complete, there is no escrow smart contract that can facilitate this exchange without either relying on third parties or enabling at least one side to extort the other.Goharshady noted that:&lt;/quote&gt;&lt;quote&gt;on the Ethereum blockchain escrows with trusted third-parties are used more often than two-party escrows, presumably because they allow dispute resolution by a human.And goes on to show that in practice trusted third-party escrow services are essential because two-party escrow smart contracts are:&lt;/quote&gt;&lt;quote&gt;simply a game of who gets to declare their choice Ô¨Årst and commit it on the blockchain sooner, hence forcing the other party to concur with their choice. The order of transactions on a blockchain is essentially decided by the miners. Thus, the party with better connectivity to the miners or who is willing to pay higher transaction fees, would be able to declare their choice to the smart contract Ô¨Årst and extort the other party.The choice being whether or not the good had been delivered. Given the current enthusiasm for tokenization of physical goods the market for trusted escrow services looks bright.&lt;/quote&gt;&lt;head rend="h3"&gt;Fast transactions&lt;/head&gt;Actually the delay between submitting a transaction and finality is unpredictable and can be much longer than an hour. Transactions are validated by miners then added to the mempool of pending transactions where they wait until either:&lt;list rend="ul"&gt;&lt;item&gt;The selected network node chooses it as one of the most profitable to include in its block.&lt;/item&gt;&lt;item&gt;It reaches either its specified timeout or the default of 2 weeks.&lt;/item&gt;&lt;/list&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Mempool count&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;The distribution of transaction wait times is highly skewed. The median wait is typically around a block time. The proportion of low-fee transactions means the average wait is normally around 10 times that. But when everyone wants to transact the ratio spikes to over 40 times.&lt;/p&gt;&lt;head rend="h3"&gt;Cheap transactions&lt;/head&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Average fee/transaction&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;list rend="ul"&gt;&lt;item&gt;The fee to be paid to the miner which the user chose to include in the transaction. In effect, transaction slots are auctioned off.&lt;/item&gt;&lt;item&gt;The transactions the miner included in the block to front- and back-run the user's transaction, called Maximal Extractable Value[11]:&lt;quote&gt;Maximal extractable value (MEV) refers to the maximum value that can be extracted from block production in excess of the standard block reward and gas fees by including, excluding, and changing the order of transactions in a block.&lt;/quote&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Secured by Proof-of-Work (1)&lt;/head&gt;In cryptocurrencies "secured" means that the cost of an attack exceeds the potential loot. The security provided by Proof-of-Work is linear in its cost, unlike techniques such as encryption, whose security is exponential in cost. It is generally believed that it is impractical to reverse a Bitcoin transaction after about an hour because the miners are wasting such immense sums on Proof-of-Work. Bitcoin pays these immense sums, but it doesn't get the decentralization they ostensibly pay for.&lt;p&gt; Monero, a privacy-focused blockchain network, has been undergoing an attempted 51% attack ‚Äî an existential threat to any blockchain. In the case of a successful 51% attack, where a single entity becomes responsible for 51% or more of a blockchain's mining power, the controlling entity could reorganize blocks, attempt to double-spend, or censor transactions.&lt;lb/&gt;A company called Qubic has been waging the 51% attack by offering economic rewards for miners who join the Qubic mining pool. They claim to be "stress testing" Monero, though many in the Monero community have condemned Qubic for what they see as a malicious attack on the network or a marketing stunt.&lt;/p&gt; The advent of "mining as a service" about 7 years ago made 51% attacks against smaller Proof-of-Work alt-coin such as Bitcoin Gold endemic. In August Molly White reported that Monero faces 51% attack:&lt;p&gt;A company called Qubic has been waging the 51% attack by offering economic rewards for miners who join the Qubic mining pool. They claim to be "stress testing" Monero, though many in the Monero community have condemned Qubic for what they see as a malicious attack on the network or a marketing stunt.&lt;/p&gt;&lt;quote&gt;Molly White: Monero faces 51% attack&lt;/quote&gt;&lt;p&gt;In 2018's The Economic Limits Of Bitcoin And The Blockchain Eric Budish of the Booth School analyzed two versions of the 51% attack. I summarized his analysis of the classic multiple spend attack thus:&lt;/p&gt;&lt;quote&gt;Note that only Bitcoin and Ethereum among cryptocurrencies with "market cap" over $100M would cost more than $100K to attack. The total "market cap" of these 8 currencies is $271.71B and the total cost to 51% attack them is $1.277M or 4.7E-6 of their market cap.His key insight was that to ensure that 51% attacks were uneconomic, the reward for a block, implicitly the transaction tax, plus the fees had to be greater than the maximum value of the transactions in it. The total transaction cost (reward + fee) typically peaks around 1.8% but is normally between 0.6% and 0.8%, or around 150 times less than Budish's safety criterion. The result is that a conspiracy between a few large pools could find it economic to mount a 51% attack.&lt;/quote&gt;&lt;head rend="h3"&gt;Secured by Proof-of-Work (2)&lt;/head&gt;&lt;p&gt; However, ‚àÜattack is something of a ‚Äúpick your poison‚Äù parameter. If ‚àÜattack is small, then the system is vulnerable to the double-spending attack ... and the implicit transactions tax on economic activity using the blockchain has to be high. If ‚àÜattack is large, then a short time period of access to a large amount of computing power can sabotage the blockchain. &lt;/p&gt; But everyone assumes the pools won't do that. Budish further analyzed the effects of a multiple spend attack. It would be public, so it would in effect be sabotage, decreasing the Bitcoin price by a factor ‚àÜattack. He concludes that if the decrease is small, then double-spending attacks are feasible and the per-block reward plus fee must be large, whereas if it is large then access to the hash power of a few large pools can quickly sabotage the currency.&lt;quote&gt;Eric Budish: The Economic Limits Of Bitcoin And The Blockchain&lt;/quote&gt;&lt;p&gt;The implication is that miners, motivated to keep fees manageable, believe ‚àÜattack is large. Thus Bitcoin is secure because those who could kill the golden goose don't want to.&lt;/p&gt;&lt;head rend="h3"&gt;Secured by Proof-of-Work (3)&lt;/head&gt;&lt;p&gt; proof-of-work can only achieve payment security if mining income is high, but the transaction market cannot generate an adequate level of income. ... the economic design of the transaction market fails to generate high enough fees. &lt;/p&gt; The following year, in Beyond the doomsday economics of ‚Äúproof-of-work‚Äù in cryptocurrencies, Raphael Auer of the Bank for International Settlements showed that the problem Budish identified was inevitable[12]:&lt;quote&gt;Raphael Auer: Beyond the doomsday economics of ‚Äúproof-of-work‚Äù in cryptocurrencies&lt;/quote&gt;&lt;quote&gt;proof-of-work can only achieve payment security if mining income is high, but the transaction market cannot generate an adequate level of income. ... the economic design of the transaction market fails to generate high enough fees.In other words, the security of Bitcoin's blockchain depends upon inflating the currency with block rewards. This problem is excerbated by Bitcoin's regular "halvenings" reducing the block reward. To maintain miner's current income after the next halvening in less than three years the "price" would need to be over $200K; security depends upon the "price" appreciating faster than 20%/year.&lt;/quote&gt;&lt;p&gt;Once the block reward gets small, safety requires the fees in a block to be worth more than the value of the transactions in it. But everybody has decided to ignore Budish and Auer.&lt;/p&gt;&lt;head rend="h3"&gt;Secured by Proof-of-Work (4)&lt;/head&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Farokhnia Table 1&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;quote&gt;showed that (i) a successful block-reverting attack does not necessarily require ... a majority of the hash power; (ii) obtaining a majority of the hash power ... costs roughly 6.77 billion ... and (iii) Bitcoin derivatives, i.e. options and futures, imperil Bitcoin‚Äôs security by creating an incentive for a block-reverting/majority attack.They assume that an attacker would purchase enough state-of-the-art hardware for the attack. Given Bitmain's dominance in mining ASICs, such a purchase is unlikely to be feasible.&lt;/quote&gt;&lt;head rend="h3"&gt;Secured by Proof-of-Work (5)&lt;/head&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Ferreira Table 1&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;quote&gt;As of March 2021, the pools in Table 1 collectively accounted for 86% of the total hash rate employed. All but one pool (Binance) have known links to Bitmain Technologies, the largest mining ASIC producer. [14]&lt;/quote&gt;&lt;head rend="h3"&gt;Secured by Proof-of-Work (6)&lt;/head&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Mining Pools 5/17/24&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Mining Pools 4/30/25&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h3"&gt;Secured by cryptography (1)&lt;/head&gt;The dollars in your bank account are simply an entry in the bank's private ledger tagged with your name. You control this entry, but what you own is a claim on the bank[16]. Similarly, your cryptocurrency coins are effectively an entry in a public ledger tagged with the public half of a key pair. The two differences are that:&lt;list rend="ul"&gt;&lt;item&gt;No ownership is involved, so you have no recourse if something goes wrong.&lt;/item&gt;&lt;item&gt;Anyone who knows the secret half of the key pair controls the entry. Since it is extremely difficult to stop online secrets leaking, something is likely to go wrong[17].&lt;/item&gt;&lt;/list&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;XKCD #538&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h3"&gt;Secured by cryptography (2)&lt;/head&gt;Even perfect opsec may not be enough. Bitcoin and most cryptocurrencies use two cryptographic algorithms, SHA256 for hashing and ECDSA for signatures.&lt;p&gt; Quote from: llama on July 01, 2010, 10:21:47 PM&lt;/p&gt; On 10th July 2010 Nakamoto addressed the issue of what would happen if either of these algorithms were compromised. There are three problems with his response; that compromise is likely in the near future, when it does Nakamoto's fix is inadequate, and there is a huge incentive for it to happen suddenly:&lt;quote&gt;Satoshi, That would indeed be a solution if SHA was broken (certainly the more likely meltdown), because we could still recognize valid money owners by their signature (their private key would still be secure).True, if it happened suddenly. If it happens gradually, we can still transition to something stronger. When you run the upgraded software for the first time, it would re-sign all your money with the new stronger signature algorithm. (by creating a transaction sending the money to yourself with the stronger sig)&lt;lb/&gt;However, if something happened and the signatures were compromised (perhaps integer factorization is solved, quantum computers?), then even agreeing upon the last valid block would be worthless.&lt;/quote&gt;&lt;quote&gt;Satoshi Nakamoto: 10th July 2010&lt;/quote&gt;&lt;head rend="h3"&gt;Secured by cryptography (3)&lt;/head&gt;Divesh Aggarwal et al's 2019 paper Quantum attacks on Bitcoin, and how to protect against them noted that:&lt;quote&gt;the elliptic curve signature scheme used by Bitcoin is much more at risk, and could be completely broken by a quantum computer as early as 2027, by the most optimistic estimates.Their "most optimistic estimates" are likely to be correct; PsiQuantum expects to have two 1M qubit computers operational in 2027[19]. Each should be capable of breaking an ECDSA key in under a week.&lt;/quote&gt;&lt;p&gt;Bitcoin's transition to post-quantum cryptography faces a major problem because, to transfer coins from an ECDSA wallet to a post-quantum wallet, you need the key for the ECDSA wallet. Chainalysis estimates that:&lt;/p&gt;&lt;quote&gt;about 20% of all Bitcoins have been "lost", or in other words are sitting in wallets whose keys are inaccessibleAn example is the notorious hard disk in the garbage dump. A sufficiently powerful quantum computer could recover the lost keys.&lt;/quote&gt;&lt;p&gt;The incentive for it to happen suddenly is that, even if Nakamoto's fix were in place, someone with access to the first sufficiently powerful quantum computer could transfer 20% of all Bitcoin, currently worth $460B, to post-quantum wallets they controlled. This would be a 230x return on the investment in PsiQuantum.&lt;/p&gt;&lt;head rend="h3"&gt;Privacy-preserving&lt;/head&gt;&lt;p&gt; privacy can still be maintained by breaking the flow of information in another place: by keeping public keys anonymous. The public can see that someone is sending an amount to someone else, but without information linking the transaction to anyone.&lt;lb/&gt;As an additional firewall, a new key pair should be used for each transaction to keep them from being linked to a common owner.&lt;lb/&gt;Some linking is still unavoidable with multi-input transactions, which necessarily reveal that their inputs were owned by the same owner. The risk is that if the owner of a key is revealed, linking could reveal other transactions that belonged to the same owner.&lt;/p&gt; Nakamoto addressed the concern that, unlike DigiCash, because Bitcoin's blockchain was public it wasn't anonymous:&lt;p&gt;As an additional firewall, a new key pair should be used for each transaction to keep them from being linked to a common owner.&lt;/p&gt;&lt;p&gt;Some linking is still unavoidable with multi-input transactions, which necessarily reveal that their inputs were owned by the same owner. The risk is that if the owner of a key is revealed, linking could reveal other transactions that belonged to the same owner.&lt;/p&gt;&lt;quote&gt;Satoshi Nakamoto: Bitcoin: A Peer-to-Peer Electronic Cash System&lt;/quote&gt;&lt;quote&gt;privacy can still be maintained by breaking the flow of information in another place: by keeping public keys anonymous. The public can see that someone is sending an amount to someone else, but without information linking the transaction to anyone.This is true but misleading. In practice, users need to use exchanges and other services that can tie them to a public key. There is a flourishing ecosystem of companies that deanonymize wallets by tracing the web of transactions. Nakamoto added:&lt;/quote&gt;&lt;quote&gt;As an additional firewall, a new key pair should be used for each transaction to keep them from being linked to a common owner.This advice is just unrealistic. As Molly White wrote[20]:&lt;/quote&gt;&lt;quote&gt;funds in a wallet have to come from somewhere, and it‚Äôs not difficult to infer what might be happening when your known wallet address suddenly transfers money off to a new, empty wallet.Nakamoto acknowledged:&lt;/quote&gt;&lt;quote&gt;Some linking is still unavoidable with multi-input transactions, which necessarily reveal that their inputs were owned by the same owner. The risk is that if the owner of a key is revealed, linking could reveal other transactions that belonged to the same owner.For more than a decade Jamison Lopp has been tracking what happens when a wallet with significant value is deanonymized, and it is a serious risk to life and limbs[21].&lt;/quote&gt;&lt;head rend="h3"&gt;One more risk&lt;/head&gt;I have steered clear of the financial risks of cryptocurrencies. It may appear that the endorsement of the current administration has effectively removed their financial risk. But the technical and operational risks remain, and I should note another technology-related risk.&lt;table&gt;&lt;row&gt;&lt;cell&gt;Source&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;There is likely to be an epic AI equity bust. Analogies are being drawn to the telecom boom, but The Economist reckons[23]:&lt;/p&gt;&lt;quote&gt;the potential AI bubble lags behind only the three gigantic railway busts of the 19th century.&lt;/quote&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Source&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Source&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h3"&gt;Conclusion&lt;/head&gt;The fascinating thing about cryptocurrency technology is the number of ways people have developed and how much they are willing to pay to avoid actually using it. What other transformative technology has had people desperate not to use it?&lt;p&gt;The whole of TradFi has been erected on this much worse infrastructure, including exchanges, closed-end funds, ETFs, rehypothecation, and derivatives. Clearly, the only reason for doing so is to escape regulation and extract excess profits from what would otherwise be crimes.&lt;/p&gt;&lt;head rend="h3"&gt;Footnotes&lt;/head&gt;&lt;list rend="ol"&gt;&lt;item&gt; The cause was the video of a talk I gave at Stanford in 2022 entitled Can We Mitigate The Externalities Of Cryptocurrencies?. It was an updated version of a talk at the 2021 TTI/Vanguard conference. The talk conformed to Betteridge's Law of Headlines in that the answer was "no".&lt;/item&gt;&lt;item&gt;Paper libraries form a model fault-tolerant system. It is highly replicated and decentralized. Libraries cooperate via inter-library loan and copy to deliver a service that is far more reliable than any individual library.&lt;/item&gt;&lt;item&gt; The importance Satoshi Nakamoto attached to trustlessness can be seen from his release note for Bitcoin 0.1: &lt;quote&gt;The root problem with conventional currency is all the trust that's required to make it work. The central bank must be trusted not to debase the currency, but the history of fiat currencies is full of breaches of that trust. Banks must be trusted to hold our money and transfer it electronically, but they lend it out in waves of credit bubbles with barely a fraction in reserve. We have to trust them with our privacy, trust them not to let identity thieves drain our accounts. Their massive overhead costs make micropayments impossible.&lt;/quote&gt;The problem with this ideology is that trust (but verify) is an incredibly effective optimization in almost any system. For example, Robert Putnam et al's Making Democracy Work: Civic Traditions in Modern Italy shows that the difference between the economies of Northern and Southern Italy is driven by the much higher level of trust in the North.&lt;lb/&gt;Bitcoin's massive cost is a result of its lack of trust. Users pay this massive cost but they don't get a trustless system, they just get a system that makes the trust a bit harder to see.&lt;lb/&gt;In response to Nakamoto's diatribe, note that:&lt;list rend="ul"&gt;&lt;item&gt;"trusted not to debase the currency", but Bitcoin's security depends upon debasing the currency.&lt;/item&gt;&lt;item&gt;"waves of credit bubbles", is a pretty good description of the cryptocurrency market.&lt;/item&gt;&lt;item&gt;"not to let identity thieves drain our accounts", see Molly White's Web3 is Going Just Great.&lt;/item&gt;&lt;item&gt;"massive overhead costs". The current cost per transaction is around $100.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt; The problem of trusting mining pools is actually much worse. There is nothing to stop pools &lt;del rend="overstrike"&gt;conspiring&lt;/del&gt;coordinating. In 2017 Vitalik Buterin, co-founder of Ethereum, published The Meaning of Decentralization:&lt;quote&gt;In the case of blockchain protocols, the mathematical and economic reasoning behind the safety of the consensus often relies crucially on the uncoordinated choice model, or the assumption that the game consists of many small actors that make decisions independently. If any one actor gets more than 1/3 of the mining power in a proof of work system, they can gain outsized profits by selfish-mining. However, can we really say that the uncoordinated choice model is realistic when 90% of the Bitcoin network‚Äôs mining power is well-coordinated enough to show up together at the same conference?&lt;/quote&gt;See "Sufficiently Decentralized" for a review of evidence from a Protos article entitled New research suggests Bitcoin mining centralized around Bitmain that concludes:&lt;quote&gt;In all, it seems unlikely that up to nine major bitcoin mining pools use a shared custodian for coinbase rewards unless a single entity is behind all of their operations.&lt;/quote&gt;The "single entity" is clearly Bitmain.&lt;/item&gt;&lt;item&gt; Peter Ryan, a reformed Bitcoin enthusiast, noted another form of centralization in Money by Vile Means:&lt;quote&gt;Bitcoin is anything but decentralized: Its functionality is maintained by a small and privileged clique of software developers who are funded by a centralized cadre of institutions. If they wanted to change Bitcoin‚Äôs 21 million coin finite supply, they could do it with the click of a keyboard.&lt;/quote&gt;His account of the politics behind the argument over raising the Bitcoin block size should dispel any idea of Bitcoin's decentralized nature. He also notes:&lt;quote&gt;By one estimate from Hashrate Index, Foundry USA and Singapore-based AntPool control more than 50 percent of computing power, and the top ten mining pools control over 90 percent. Bitcoin blogger 0xB10C, who analyzed mining data as of April 15, 2025, found that centralization has gone even further than this, ‚Äúwith only six pools mining more than 95 percent of the blocks.‚Äù&lt;/quote&gt;&lt;/item&gt;&lt;item&gt; The Bitmain S17 comes in 4 versions with hash rates from 67 to 76 TH/s. Lets assume 70TH/s. As I write the Bitcoin hash rate is about 1 billion TH/s. So if they were all mid-range S17s there would be around 15M mining. If their economic life were 18 months, there would be 77,760 rewards. Thus only 0.5% of them would earn a reward.&lt;lb/&gt;In December 2021 Alex de Vries and Christian Stoll estimated that:&lt;quote&gt;The average time to become unprofitable sums up to less than 1.29 years.&lt;/quote&gt;It has been obvious since mining ASICs first hit the market that, apart from access to cheap or free electricity, there were two keys to profitable mining:&lt;list rend="ol"&gt;&lt;item&gt;Having close enough ties to Bitmain to get the latest chips early in their 18-month economic life.&lt;/item&gt;&lt;item&gt;Having the scale to buy Bitmain chips in the large quantities that get you early access.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt; See David Gerard's account of Steve Early's experiences accepting Bitcoin in his chain of pubs in Attack of the 50 Foot Blockchain Page 94.&lt;table/&gt;U.S. Consumers‚Äô Use of Cryptocurrency for Payments by Fumiko Hayashi and Aditi Routh of the Kansas City Fed reports that:&lt;td&gt;Chart 1&lt;/td&gt;&lt;quote&gt;The share of U.S. consumers who report using cryptocurrency for payments‚Äîpurchases, money transfers, or both‚Äîhas been very small and has declined slightly in recent years. The light blue line in Chart 1 shows that this share declined from nearly 3 percent in 2021 and 2022 to less than 2 percent in 2023 and 2024.&lt;/quote&gt;&lt;/item&gt;&lt;item&gt; User DeathAndTaxes on Stack Exchange explains the 6 block rule:&lt;quote&gt;p is the chance of attacker eventually getting longer chain and reversing a transaction (0.1% in this case). q is the % of the hashing power the attacker controls. z is the number of blocks to put the risk of a reversal below p (0.1%).&lt;/quote&gt;For example, last May Foundry USA had more than 30% of the hash power, so the rule should have been 24 not 6, and finality should have taken 4 hours.&lt;lb/&gt;So you can see if the attacker has a small % of the hashing power 6 blocks is sufficient. Remember 10% of the network at the time of writing is ~100GH/s. However if the attacker had greater % of hashing power it would take increasingly longer to be sure a transaction can't be reversed.&lt;lb/&gt;If the attacker had significantly more hashpower say 25% of the network it would require 15 confirmation to be sure (99.9% probability) that an attacker can't reverse it.&lt;/item&gt;&lt;item&gt; To be fair, Ethereum has introduced at least one genuine innovation, Flash Loans. In Flash loans, flash attacks, and the future of DeFi Aidan Saggers, Lukas Alemu and Irina Mnohoghitnei of the Bank of England provide an excellent overview of them. Back in 2021 Kaihua Qin, Liyi Zhou, Benjamin Livshits, and Arthur Gervais from Imperial College posted Attacking the defi ecosystem with flash loans for fun and profit, analyzing and optimizing two early flash loan attacks:&lt;quote&gt;We show quantitatively how transaction atomicity increases the arbitrage revenue. We moreover analyze two existing attacks with ROIs beyond 500k%. We formulate finding the attack parameters as an optimization problem over the state of the underlying Ethereum blockchain and the state of the DeFi ecosystem. We show how malicious adversaries can efficiently maximize an attack profit and hence damage the DeFi ecosystem further. Specifically, we present how two previously executed attacks can be ‚Äúboosted‚Äù to result in a profit of 829.5k USD and 1.1M USD, respectively, which is a boost of 2.37√ó and 1.73√ó, respectively.&lt;/quote&gt;They predicted an upsurge in attacks since "flash loans democratize the attack, opening this strategy to the masses". They were right, as you can see from Molly White's list of flash loan attacks.&lt;/item&gt;&lt;item&gt;This is one of a whole series of Impossibilities, many imposed on Ethereum by fundamental results in computer science because it is a Turing-complete programming environment.&lt;/item&gt;&lt;item&gt; For details of the story behind Miners' Extractable Value (MEV), see these posts:&lt;list rend="ol"&gt;&lt;item&gt;The Order Flow from November 2020.&lt;/item&gt;&lt;item&gt;Ethereum Has Issues from April 2022.&lt;/item&gt;&lt;item&gt;Miners' Extractable Value From September 2022.&lt;/item&gt;&lt;/list&gt;&lt;table/&gt;The first links to two must-read posts. The first is from Dan Robinson and Georgios Konstantopoulos, Ethereum is a Dark Forest:&lt;td&gt;Source&lt;/td&gt;&lt;quote&gt;It‚Äôs no secret that the Ethereum blockchain is a highly adversarial environment. If a smart contract can be exploited for profit, it eventually will be. The frequency of new hacks indicates that some very smart people spend a lot of time examining contracts for vulnerabilities.&lt;/quote&gt;The second is from Samczsun, Escaping the Dark Forest. It is an account of how:&lt;lb/&gt;But this unforgiving environment pales in comparison to the mempool (the set of pending, unconfirmed transactions). If the chain itself is a battleground, the mempool is something worse: a dark forest.&lt;quote&gt;On September 15, 2020, a small group of people worked through the night to rescue over 9.6MM USD from a vulnerable smart contract.&lt;/quote&gt;Note in particular that MEV poses a risk to the integrity of blockchains. In Extracting Godl [sic] from the Salt Mines: Ethereum Miners Extracting Value Julien Piet, Jaiden Fairoze and Nicholas Weaver examine the use of transactions that avoid the mempool, finding that:&lt;quote&gt;(i) 73% of private transactions hide trading activity or re-distribute miner rewards, and 87.6% of MEV collection is accomplished with privately submitted transactions, (ii) our algorithm finds more than $6M worth of MEV profit in a period of 12 days, two thirds of which go directly to miners, and (iii) MEV represents 9.2% of miners' profit from transaction fees.&lt;/quote&gt;When they say "large miners" they mean more than 10% of the power.&lt;lb/&gt;Furthermore, in those 12 days, we also identify four blocks that contain enough MEV profits to make time-bandit forking attacks economically viable for large miners, undermining the security and stability of Ethereum as a whole.&lt;/item&gt;&lt;item&gt; Back in 2016 Arvind Narayanan's group at Princeton had published a related instability in Carlsten et al's On the instability of bitcoin without the block reward. Narayanan summarized the paper in a blog post:&lt;quote&gt;Our key insight is that with only transaction fees, the variance of the miner reward is very high due to the randomness of the block arrival time, and it becomes attractive to fork a ‚Äúwealthy‚Äù block to ‚Äústeal‚Äù the rewards therein.&lt;/quote&gt;&lt;/item&gt;&lt;item&gt;The leading source of data on which to base Bitcoin's carbon footprint is the Cambridge Bitcoin Energy Consumption Index. As I write their central estimate is that Bitcoin consumes 205TWh/year, or between Thailand and Vietnam.&lt;/item&gt;&lt;item&gt; Ferreira et al write:&lt;quote&gt;AntPool and BTC.com are fully-owned subsidiaries of Bitmain. Bitmain is the largest investor in ViaBTC. Both F2Pool and BTC.TOP are partners of BitDeer, which is a Bitmain-sponsored cloud-mining service. The parent companies of Huobi.pool and OkExPool are strategic partners of Bitmain. Jihan Wu, Bitmain‚Äôs founder and chairman, is also an adviser of Huobi (one of the largest cryptocurrency exchanges in the world and the owner of Huobi.pool).&lt;/quote&gt;This makes economic sense. Because mining rigs depreciate quickly, profit depends upon early access to the latest chips.&lt;/item&gt;&lt;item&gt;See Who Is Mining Bitcoin? for more detail on the state of mining and its gradual obfuscation.&lt;/item&gt;&lt;item&gt;In this context to say you "control" your entry in the bank's ledger is an oversimplification. You can instruct the bank to perform transactions against your entry (and no-one else's) but the bank can reject your instructions. For example if they would overdraw your account, or send money to a sanctioned account. The key point is that your ownership relationship with the bank comes with a dispute resolution system and the ability to reverse transactions. Your cryptocurrency wallet has neither.&lt;/item&gt;&lt;item&gt;Web3 is Going Just Great is Molly White's list of things that went wrong. The cumulative losses she tracks currently stand at over $79B.&lt;/item&gt;&lt;item&gt;Your secrets are especially at risk if anyone in your software supply chain use a build system implemented using AI "vibe coding". David Gerard's Vibe-coded build system NX gets hacked, steals vibe-coders‚Äô crypto details a truly beautiful example of the extraordinary level of incompetence this reveals.&lt;/item&gt;&lt;item&gt;IBM's Heron, which HSBC recently used to grab headlines, has 156 qubits.&lt;/item&gt;&lt;item&gt; Molly White's Abuse and harassment on the blockchain is an excellent overview of the privacy risks inherent to real-world transactions on public blockchain ledgers:&lt;quote&gt;Imagine if, when you Venmo-ed your Tinder date for your half of the meal, they could now see every other transaction you‚Äôd ever made‚Äîand not just on Venmo, but the ones you made with your credit card, bank transfer, or other apps, and with no option to set the visibility of the transfer to ‚Äúprivate‚Äù. The split checks with all of your previous Tinder dates? That monthly transfer to your therapist? The debts you‚Äôre paying off (or not), the charities to which you‚Äôre donating (or not), the amount you‚Äôre putting in a retirement account (or not)? The location of that corner store right by your apartment where you so frequently go to grab a pint of ice cream at 10pm? Not only would this all be visible to that one-off Tinder date, but also to your ex-partners, your estranged family members, your prospective employers. An abusive partner could trivially see you siphoning funds to an account they can‚Äôt control as you prepare to leave them.&lt;/quote&gt;&lt;/item&gt;&lt;item&gt; In The Risks Of HODL-ing I go into the details of the attack on the parents of Veer Chetal, who had unwisely live-streamed the social engineering that stole $243M from a resident of DC.&lt;lb/&gt;Anyone with significant cryptocurrency wallets needs to follow Jamison Lopp's Known Physical Bitcoin Attacks.&lt;/item&gt;&lt;item&gt;&lt;table/&gt;Torsten Sl√∏k's AI Has Moved From a Niche Sector to the Primary Driver of All VC Investment leads with this graph, one of the clearest signs that we're in a bubble.&lt;td&gt;Source&lt;/td&gt;&lt;lb/&gt;Whether AI delivers net value in most cases is debatable. "Vibe coding" is touted as the example of increasing productivity, but the experimental evidence is that it decreases productivity. Kate Niederhoffer et al's Harvard Business Review article AI-Generated "Workslop‚Äù Is Destroying Productivity explains one effect:&lt;quote&gt;Employees are using AI tools to create low-effort, passable looking work that ends up creating more work for their coworkers. On social media, which is increasingly clogged with low-quality AI-generated posts, this content is often referred to as ‚ÄúAI slop.‚Äù In the context of work, we refer to this phenomenon as ‚Äúworkslop.‚Äù We define workslop as AI generated work content that masquerades as good work, but lacks the substance to meaningfully advance a given task.&lt;/quote&gt;David Gerard's Workslop: bad ‚Äòstudy‚Äô, but an excellent word points out that:&lt;lb/&gt;Here‚Äôs how this happens. As AI tools become more accessible, workers are increasingly able to quickly produce polished output: well-formatted slides, long, structured reports, seemingly articulate summaries of academic papers by non-experts, and usable code. But while some employees are using this ability to polish good work, others use it to create content that is actually unhelpful, incomplete, or missing crucial context about the project at hand. The insidious effect of workslop is that it shifts the burden of the work downstream, requiring the receiver to interpret, correct, or redo the work. In other words, it transfers the effort from creator to receiver.&lt;quote&gt;Unfortunately, this article pretends to be a writeup of a study ‚Äî but it‚Äôs actually a promotional brochure for enterprise AI products. It‚Äôs an unlabeled advertising feature.&lt;/quote&gt;And goes on to explain where the workslop comes from:&lt;quote&gt;Well, you know how you get workslop ‚Äî it‚Äôs when your boss mandates you use AI. He can‚Äôt say what he wants you to use it for. But you‚Äôve been told. You‚Äôve got metrics on how much AI you use. They‚Äôre watching and they‚Äôre measuring.&lt;/quote&gt;Belle Lin and Steven Rosenbush's Stop Worrying About AI‚Äôs Return on Investment describes goalposts being moved:&lt;quote&gt;Return on investment has evaded chief information officers since AI started moving from early experimentation to more mature implementations last year. But while AI is still rapidly evolving, CIOs are recognizing that traditional ways of recognizing gains from the technology aren‚Äôt cutting it.&lt;/quote&gt;Given the hype and the massive sunk costs, admitting that there is no there there would be a career-limiting move.&lt;lb/&gt;Tech leaders at the WSJ Leadership Institute‚Äôs Technology Council Summit on Tuesday said racking up a few minutes of efficiency here and there don‚Äôt add up to a meaningful way of measuring ROI.&lt;lb/&gt;None of this takes account of the productivity externalities of AI, such as Librarians Are Being Asked to Find AI-Hallucinated Books, academic journals' reviewers' time wasted by AI slop papers, judges' time wasted with hallucinated citations, a flood of generated child sex abuse videos, the death of social media and a vast new cyberthreat landscape.&lt;/item&gt;&lt;item&gt; The Economist writes in What if the AI stockmarket blows up?:&lt;quote&gt;we picked ten historical bubbles and assessed them on factors including spark, cumulative capex, capex durability and investor group. By our admittedly rough-and-ready reckoning, the potential AI bubble lags behind only the three gigantic railway busts of the 19th century.&lt;/quote&gt;They note that:&lt;quote&gt;For now, the splurge looks fairly modest by historical standards. According to our most generous estimate, American AI firms have invested 3-4% of current American GDP over the past four years. British railway investment during the 1840s was around 15-20% of GDP. But if forecasts for data-centre construction are correct, that will change. What is more, an unusually large share of capital investment is being devoted to assets that depreciate quickly. Nvidia‚Äôs cutting-edge chips will look clunky in a few years‚Äô time. We estimate that the average American tech firm‚Äôs assets have a shelf-life of just nine years, compared with 15 for telecoms assets in the 1990s.&lt;/quote&gt;I think they are over-estimating the shelf-life. Like Bitcoin mining, power is a major part of AI opex. Thus the incentive to (a) retire older, less power-efficient hardware, and (b) adopt the latest data-center power technology, is overwhelming. Note that Nvidia is moving to a one-year product cadence, and even when they were on a two-year cadence Jensen claimed it wasn't worth running chips from the previous cycle. Note also that the current generation of AI systems is incompatible with the power infrastructure of older data centers, and this may well happen again in a future product generation. For example, Caiwei Chen reports in China built hundreds of AI data centers to catch the AI boom. Now many stand unused:&lt;quote&gt;The local Chinese outlets Jiazi Guangnian and 36Kr report that up to 80% of China‚Äôs newly built computing resources remain unused.&lt;/quote&gt;Rog√© Karma makes the same point as The Economist in Just How Bad Would an AI Bubble Be?:&lt;quote&gt;An AI-bubble crash could be different. AI-related investments have already surpassed the level that telecom hit at the peak of the dot-com boom as a share of the economy. In the first half of this year, business spending on AI added more to GDP growth than all consumer spending combined. Many experts believe that a major reason the U.S. economy has been able to weather tariffs and mass deportations without a recession is because all of this AI spending is acting, in the words of one economist, as a ‚Äúmassive private sector stimulus program.‚Äù An AI crash could lead broadly to less spending, fewer jobs, and slower growth, potentially dragging the economy into a recession.&lt;/quote&gt;&lt;/item&gt;&lt;item&gt;In 2021 Nicholas Weaver estimated that the Ethereum computer was 5000 times slower than a Raspberry Pi 4. Since then the gas limit has been raised making his current estimate only 1000 times slower.&lt;/item&gt;&lt;item&gt; Prof. Hilary Allen writes in Fintech Dystopia that:&lt;quote&gt;if people do start dumping blockchain-based assets in fire sales, everyone will know immediately because the blockchain is publicly visible. This level of transparency will only add to the panic (at least, that‚Äôs what happened during the run on the Terra stablecoin in 2022).&lt;/quote&gt;She adds:&lt;lb/&gt;...&lt;lb/&gt;We also saw ... that assets on a blockchain can be pre-programmed to execute transactions without the intervention of any human being. In good times, this makes things more efficient ‚Äì but the code will execute just as quickly in bad situations, even if everyone would be better off if it didn‚Äôt.&lt;quote&gt;When things are spiraling out of control like this, sometimes the best medicine is a pause. Lots of traditional financial markets close at the end of the day and on weekends, which provides a natural opportunity for a break (and if things are really bad, for emergency government intervention). But one of blockchain-based finance‚Äôs claims to greater efficiency is that operations continue 24/7. We may end up missing the pauses once they‚Äôre gone.&lt;/quote&gt;In the 26th September Grant's, Joel Wallenberg notes that:&lt;quote&gt;Lucrative though they may be, the problem with stablecoin deposits is that exposure to the crypto-trading ecosystem makes them inherently correlated to it and subject to runs in a new ‚Äúcrypto winter,‚Äù like that of 2022‚Äì23. Indeed, since as much as 70% of gross stablecoin-transaction volume derives from automated arbitrage bots and high-speed trading algorithms, runs may be rapid and without human over-sight. What may be worse, the insured banks that could feed a stablecoin boom are the very ones that are likely to require taxpayer support if liquidity dries up, and Trump-style regulation is likely to be light.&lt;/quote&gt;So the loophole in the GENIUS act for banks is likely to cause contagion from cryptocurrencies via stablecoins to the US banking system.&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45433866</guid><pubDate>Wed, 01 Oct 2025 02:59:28 +0000</pubDate></item><item><title>Blockdiff: We built our own file format for VM disk snapshots</title><link>https://cognition.ai/blog/blockdiff</link><description>&lt;doc fingerprint="b51932dd553781b3"&gt;
  &lt;main&gt;
    &lt;p&gt;How we built blockdiff, an open-source tool for rapid block-level diffs and snapshots of VM disks.&lt;/p&gt;
    &lt;p&gt;We made it open-source here: https://github.com/CognitionAI/blockdiff&lt;/p&gt;
    &lt;p&gt;Usually, I‚Äôm a researcher working on areas like RL for coding agents ‚Äì but one day I became annoyed by our slow VM startup times. So I took the plunge into systems engineering and built the first version of our VM hypervisor called &lt;code&gt;otterlink&lt;/code&gt;. It now powers both our research &amp;amp; all of Devin production workloads.&lt;/p&gt;
    &lt;p&gt;Devin writes and runs code in a VM environment. Why VMs instead of Docker? For untrusted user workloads we require full isolation for security purposes. Moreover, many realistic dev environments require using Docker (e.g. to spin up a backend service or database). Good luck running Docker inside Docker, so we needed VMs.&lt;/p&gt;
    &lt;p&gt;Compared to EC2, &lt;code&gt;otterlink&lt;/code&gt; was able to bring down VM startup times by about 10x. The real pain point, however, were EC2‚Äôs long snapshot times. We want a lot of flexibility (e.g. forking, rollback and suspending VMs) that all require taking disk snapshots. On EC2 taking disk snapshots usually took a whopping 30+ minutes, which would be a terrible experience for our users. With &lt;code&gt;otterlink&lt;/code&gt; we were able to bring this down to just a couple of seconds ‚Äì a 200x speed-up.&lt;/p&gt;
    &lt;p&gt;To achieve this, we built our own file format &lt;code&gt;blockdiff&lt;/code&gt; for instant block-level diffs of VM disks. Creating block-level diffs of two files is a much broader problem that goes beyond VM disks. We assumed there must be an existing open-source solution. To our surprise we couldn‚Äôt find such a tool, so we‚Äôre open-sourcing our implementation today.&lt;/p&gt;
    &lt;p&gt;There are three reasons why we want incremental snapshots of VM disks:&lt;/p&gt;
    &lt;p&gt;Dev environments&lt;/p&gt;
    &lt;p&gt;Our customers set up their dev environment in Devin‚Äôs VM, which we save to reuse via a disk snapshot. If most customers use just 1GB of additional disk space, we don‚Äôt want all the snapshots to redundantly store the entire 15GB operating system.&lt;/p&gt;
    &lt;p&gt;Sleep &amp;amp; wake up&lt;/p&gt;
    &lt;p&gt;When Devin sleeps, we want to store the current session state without making another copy of the dev environment. The limiting factor isn‚Äôt even storage cost ‚Äì it‚Äôs wake up time. Transferring a 50 MB snapshot of session state is much faster than a multi-GB snapshot of the entire dev environment.&lt;/p&gt;
    &lt;p&gt;Disk rollback&lt;/p&gt;
    &lt;p&gt;To enable rolling back the disk during a session, we want to stack many of these incremental snapshots on top of each other.&lt;/p&gt;
    &lt;p&gt;We tried very hard to find a way to implement disk snapshotting while satisfying all these criteria:&lt;/p&gt;
    &lt;p&gt;Compact&lt;/p&gt;
    &lt;p&gt;The snapshot file should grow proportional to the difference between the base image and the VM disk. It‚Äôs too expensive to snapshot the entire disk.&lt;/p&gt;
    &lt;p&gt;Instantaneous&lt;/p&gt;
    &lt;p&gt;Taking a snapshot should be instant and should not require significant disk I/O. We design our file format so that creating snapshot operates mostly on file metadata.&lt;/p&gt;
    &lt;p&gt;Zero overhead&lt;/p&gt;
    &lt;p&gt;The VM should not experience any overhead, e.g. slower reads or writes.&lt;/p&gt;
    &lt;p&gt;Simplicity&lt;/p&gt;
    &lt;p&gt;Things like this can easily break and have thousands of edge cases, so we want a solution that‚Äôs as simple as possible, to spare ourselves lots of debugging time.&lt;/p&gt;
    &lt;p&gt;The implementation of the file format is a single, few-hundred line Rust file. It stands on the shoulders of giants: most of the complexity is handled by the Linux kernel‚Äôs excellent CoW implementation in the XFS filesystem. The core idea is simple: For two files A &amp;amp; B, &lt;code&gt;blockdiff&lt;/code&gt; stores only the blocks in B that are different from blocks in A.&lt;/p&gt;
    &lt;p&gt;To explain the difficulty of achieving all these design goals, let‚Äôs first explain the limitations of other solutions we considered:&lt;/p&gt;
    &lt;p&gt;Why not just read the files and compute a binary diff?&lt;/p&gt;
    &lt;p&gt;Trying to compute a binary diff directly based on file content would be quite slow. Even on the fastest SSDs, scanning an entire 128 GB disk image can take 30-60 seconds.&lt;/p&gt;
    &lt;p&gt;Why not OverlayFS?&lt;/p&gt;
    &lt;p&gt;For the first few weeks of &lt;code&gt;otterlink&lt;/code&gt;‚Äôs existence we used OverlayFS. However, it had two issues: It didn‚Äôt have clean support for incremental snapshots without remounting. Moreover, it created big issues when users wanted to use Docker ‚Äì which would fall back to the vfs storage driver, consume 17x more storage and be 6x slower.&lt;/p&gt;
    &lt;p&gt;Why not ZFS?&lt;/p&gt;
    &lt;p&gt;Before we started using &lt;code&gt;otterlink&lt;/code&gt;, we had implemented a version of rollback using ZFS inside of the VM. It had multiple limitations: For reliability reasons, we mounted ZFS only on the home dir, so it wasn‚Äôt possible to roll back system-level changes like package installs. Moreover, the snapshot logic had to live inside of the VM, visible to the user. We also briefly considered using ZFS outside of the VM on the hypervisor. However, we concluded that the end-to-end performance of creating &amp;amp; transferring ZFS snapshots (send/recv) seemed to most likely be lower than what we can achieve with &lt;code&gt;blockdiff&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;What about the qcow2 file format?&lt;/p&gt;
    &lt;p&gt;We didn‚Äôt deeply consider qcow2 because our hypervisor only supports raw disk images. Further below in the bonus section, we show a performance comparison that shows an example of &lt;code&gt;qemu-img convert&lt;/code&gt; becoming quite slow for large files. Evidently, qcow2 doesn‚Äôt operate on metadata only (unlike &lt;code&gt;blockdiff&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Let‚Äôs first explain two Linux concepts that are necessary to understand the rest.&lt;/p&gt;
    &lt;p&gt;Sparse files&lt;/p&gt;
    &lt;p&gt;Sparse files only allocate disk space for non-zero data. This is particularly helpful for VM disk images that have mostly unused space. For a sparse file, the logical size of the file is different from the actual disk usage. In this example, &lt;code&gt;ls -hl disk.img&lt;/code&gt; shows the logical size which is 32GB. However, it only uses 261 MB of actual disk space ‚Äì which you can see with &lt;code&gt;du -h&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;ubuntu@devin-box:~$ ls -hl disk.img
-rw-r--r-- 1 ubuntu ubuntu 32G Jan  9 07:57 disk.img
ubuntu@devin-box:~$ du -h disk.img
261M    disk.img&lt;/code&gt;
    &lt;p&gt;You can create an empty sparse file with &lt;code&gt;truncate -s&lt;/code&gt; and then format it as an &lt;code&gt;ext4&lt;/code&gt; disk image using &lt;code&gt;mkfs.ext4&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;ubuntu@devin-box:~$ truncate -s 32G disk.img
ubuntu@devin-box:~$ mkfs.ext4 disk.img
mke2fs 1.46.5 (30-Dec-2021)
Discarding device blocks: done                            
Creating filesystem with 8388608 4k blocks and 2097152 inodes
Filesystem UUID: e2fdd2d5-a1a7-4be1-a9d7-6fecdb57096c
Superblock backups stored on blocks: 
        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
        4096000, 7962624

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (65536 blocks): done
Writing superblocks and filesystem accounting information: done&lt;/code&gt;
    &lt;p&gt;Copy-on-write (CoW)&lt;/p&gt;
    &lt;p&gt;Copy-on-write is a feature supported by many modern Linux filesystems (XFS, ZFS, btrfs). Instead of immediately copying data when requested, the system shares the original data and only creates a separate copy when modifications are made to either the original or the copy. This bookkeeping happens on a block-by-block basis: blocks are the fundamental unit of storage of modern file systems and typically 4KB in size.&lt;/p&gt;
    &lt;p&gt;See the difference between copying a file with &amp;amp; without ‚Äúreflink‚Äù (another name for copy-on-write) for a 128GB disk image on a very fast NVMe SSD:&lt;/p&gt;
    &lt;code&gt;ubuntu@devin-box:~$ time cp --reflink=never base.img vm1.img

real    0m24.532s
user    0m0.142s
sys     0m18.785s

ubuntu@devin-box:~$ time cp --reflink=always base.img vm2.img

real    0m0.008s
user    0m0.001s
sys     0m0.004s&lt;/code&gt;
    &lt;p&gt;Disk images as files&lt;/p&gt;
    &lt;p&gt;For our hypervisor &lt;code&gt;otterlink&lt;/code&gt;, VM disks are just files on its filesystem. Each VM disk is a CoW copy of the base disk image (e.g. the operating system) ‚Äì which means that it shares all blocks by default and greedily allocates new blocks on write.&lt;/p&gt;
    &lt;p&gt;It‚Äôs important to differentiate between the filesystem inside &amp;amp; outside of the VMs: Inside our VMs we use ext4 as the filesystem because it‚Äôs most widespread and the default on Ubuntu. Outside, the hypervisor uses XFS as its filesystem ‚Äì crucially with reflink (= copy-on-write) enabled.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs say we have two files: &lt;code&gt;base.img&lt;/code&gt; is a disk image of our operating system and &lt;code&gt;vm.img&lt;/code&gt; is a CoW copy of &lt;code&gt;base.img&lt;/code&gt;. The VM is reading &amp;amp; writing from &lt;code&gt;vm.img&lt;/code&gt;. Our goal is to create a separate file &lt;code&gt;snapshot.bdiff&lt;/code&gt; that stores only the blocks from &lt;code&gt;vm.img&lt;/code&gt; that are different from &lt;code&gt;base.img&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;File extent maps&lt;/p&gt;
    &lt;p&gt;Our objective is to only operate on the filesystem metadata and to never touch the actual contents of the files. To be precise, the key lies in the file extent maps which you can get using the FIEMAP syscall. The &lt;code&gt;blockdiff&lt;/code&gt; tool can be used to view the syscall outputs in a nicely formatted way (or alternatively use the Linux utility &lt;code&gt;filefrag -v disk.img&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;blockdiff view disk.img&lt;/code&gt;
    &lt;p&gt;The file extent map represents the mapping from logical blocks in the file to physical blocks on the hard drive. This mapping is grouped in extents which are sequences of blocks that are allocated contiguously. You might‚Äôve heard of the word (de)fragmentation before: In an ideal world, every file would just be a single extent, stored as one contiguous chunk on the hard drive. However, due to fragmentation files usually end up split across multiple extents scattered throughout the disk.&lt;/p&gt;
    &lt;p&gt;Reading file extent maps from Rust&lt;/p&gt;
    &lt;p&gt;Using the &lt;code&gt;fiemap&lt;/code&gt; crate in Rust we have a clean wrapper around the underlying Linux syscall FIEMAP IOCTL. Getting the extents of the target file is as easy as:&lt;/p&gt;
    &lt;code&gt;let mut target_extents: Vec&amp;lt;_&amp;gt; = fiemap::fiemap(target_file)?.collect::&lt;/code&gt;
    &lt;p&gt;Each extent looks as follows:&lt;/p&gt;
    &lt;code&gt;pub struct FiemapExtent {
    pub fe_logical: u64, // logical offset (in bytes)
    pub fe_physical: u64, // physical offset (in bytes)
    pub fe_length: u64, // length of extent (in bytes)
    pub fe_flags: FiemapExtentFlags,
}&lt;/code&gt;
    &lt;p&gt;The logical block addresses are the location of data in the file (i.e. in our VM disk). The physical block addresses are where the data is stored on the hypervisor disk. An extent is a sequence of contiguous logical blocks with contiguous physical addresses. If two logical blocks from different files, point to the same physical blocks, then they are the same.&lt;/p&gt;
    &lt;p&gt;Exercise for the reader: Write an algorithm that takes in file extent map A &amp;amp; B and returns a list of extents from B that are different from A. Be careful that extent boundaries are in general not aligned between A &amp;amp; B.&lt;/p&gt;
    &lt;p&gt;Defining a file format (.bdiff)&lt;/p&gt;
    &lt;p&gt;Now the last step is to serialize this into a file:&lt;/p&gt;
    &lt;code&gt;/// - Header:
///   - 8 bytes: magic string ("BDIFFv1\0")
///   - 8 bytes: target file size (little-endian)
///   - 8 bytes: base file size (little-endian)
///   - 8 bytes: number of ranges (little-endian)
///   - Ranges array, each range containing:
///     - 8 bytes: logical offset (little-endian)
///     - 8 bytes: length (little-endian)
/// - Padding to next block boundary (4 KiB)
/// - Range data (contiguous blocks of data)&lt;/code&gt;
    &lt;p&gt;A small header contains information about which logical block ranges the file contains. After that, it stores all differing blocks contiguously. When creating (or applying) blockdiffs, writing the small header is the only disk I/O that needs to happen. All the actual data can share the same physical blocks with &lt;code&gt;vm.img&lt;/code&gt;, i.e. creating the rest of the file is purely a ‚Äúrewiring‚Äù of file metadata.&lt;/p&gt;
    &lt;p&gt;Exercise for the reader: What piece in the blockdiff codebase is responsible for the fact that the range data shares the same physical blocks as &lt;code&gt;vm.img&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;Install and build the binary:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/cognitionai/blockdiff &amp;amp;&amp;amp; cd blockdiff
cargo install&lt;/code&gt;
    &lt;p&gt;Reminder that we need to be on a filesystem with reflink enabled (e.g. XFS). Now, let‚Äôs create a snapshot of &lt;code&gt;vm1.img&lt;/code&gt; against the base image &lt;code&gt;base.img&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;blockdiff create snapshot.bdiff vm1.img --base base.img&lt;/code&gt;
    &lt;p&gt;We can use that snapshot, to create a new disk image &lt;code&gt;vm2.img:&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;blockdiff apply snapshot.bdiff vm2.img --base base.img&lt;/code&gt;
    &lt;p&gt;This file should be identical to &lt;code&gt;vm1.img&lt;/code&gt;. We can verify this with hashes:&lt;/p&gt;
    &lt;code&gt;xxhsum vm1.img
xxhsum vm2.img&lt;/code&gt;
    &lt;p&gt;Since this is only a file metadata operation, creating and applying snapshots is effectively instant ‚Äì no matter how large the disk images. After creating the snapshot file locally, the file still needs to be transferred to storage which happens with about 2 GB/s.&lt;/p&gt;
    &lt;code&gt;On our hypervisor machines:
Reading/writing 20 GB of data: ~6.5 s
Creating 20 GB snapshot with blockdiff: ~200 ms&lt;/code&gt;
    &lt;p&gt;A fun little challenge that we faced while building &lt;code&gt;otterlink&lt;/code&gt; is ‚Äúcompactifying‚Äù sparse files. If you try to upload a sparse disk image to blob storage, it will upload the entire logical size since blob storage doesn‚Äôt natively understand sparse files. So we were looking for a way to turn a sparse file of logical size X &amp;gt; disk usage Y into a ‚Äúcompact‚Äù file with logical size Y = disk usage Y.&lt;/p&gt;
    &lt;p&gt;Most online sources seemed to recommend using tar which ended up being extremely slow. You would usually expect network latency to be the main bottleneck but it turned out tar would be 5x slower than the network transfer.&lt;/p&gt;
    &lt;p&gt;Despite not using qcow2 in our hypervisor itself, it turned out that &lt;code&gt;qemu-img convert&lt;/code&gt; gave us exactly what we wanted: converting a raw, sparse disk image into a compact one. Moreover, it did it 5x faster than tar. To be clear, this was a random hack of ours and it isn‚Äôt what &lt;code&gt;qemu-img convert&lt;/code&gt; is intended to be used for. However, with larger disks it becomes clear that even qcow2 starts being slow ‚Äì it clearly isn‚Äôt a metadata-only operation. Fortunately, &lt;code&gt;blockdiff&lt;/code&gt; is super fast at all sizes!&lt;/p&gt;
    &lt;p&gt;Working on VM hypervisors was a fun foray into systems programming. Hopefully, this gave you a glimpse of the work we do at Cognition. Of course, there are many more open questions:&lt;/p&gt;
    &lt;p&gt;If you‚Äôd like to work on these problems, reach out to us or apply here!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45433926</guid><pubDate>Wed, 01 Oct 2025 03:13:16 +0000</pubDate></item><item><title>Intelligent Kubernetes Load Balancing at Databricks</title><link>https://www.databricks.com/blog/intelligent-kubernetes-load-balancing-databricks</link><description>&lt;doc fingerprint="3718e99aeb03ecb0"&gt;
  &lt;main&gt;
    &lt;p&gt;At Databricks, Kubernetes is at the heart of our internal systems. Within a single Kubernetes cluster, the default networking primitives like ClusterIP services, CoreDNS, and kube-proxy are often sufficient. They offer a simple abstraction to route service traffic. But when performance and reliability matter, these defaults begin to show their limits.&lt;/p&gt;
    &lt;p&gt;In this post, we‚Äôll share how we built an intelligent, client-side load balancing system to improve traffic distribution, reduce tail latencies, and make service-to-service communication more resilient.&lt;/p&gt;
    &lt;p&gt;If you are a Databricks user, you don‚Äôt need to understand this blog to be able to use the platform to its fullest. But if you‚Äôre interested in taking a peek under the hood, read on to hear about some of the cool stuff we‚Äôve been working on!&lt;/p&gt;
    &lt;p&gt;High-performance service-to-service communication in Kubernetes has several challenges, especially when using persistent HTTP/2 connections, as we do at Databricks with gRPC.&lt;/p&gt;
    &lt;p&gt;While this model generally works, it quickly breaks down in performance-sensitive environments, leading to significant limitations.&lt;/p&gt;
    &lt;p&gt;At Databricks, we operate hundreds of stateless services communicating over gRPC within each Kubernetes cluster. These services are often high-throughput, latency-sensitive, and run at significant scale.&lt;/p&gt;
    &lt;p&gt;The default load balancing model falls short in this environment for several reasons:&lt;/p&gt;
    &lt;p&gt;These limitations pushed us to rethink how we handle service-to-service communication within a Kubernetes cluster.&lt;/p&gt;
    &lt;p&gt;To address the limitations of kube-proxy and default service routing in Kubernetes, we built a proxyless, fully client-driven load balancing system backed by a custom service discovery control plane.&lt;/p&gt;
    &lt;p&gt;The fundamental requirement we had was to support load balancing at the application layer, and removing dependency on the DNS on a critical path. A Layer 4 load balancer, like kube-proxy, cannot make intelligent per-request decisions for Layer 7 protocols (such as gRPC) that utilize persistent connections. This architectural constraint creates bottlenecks, necessitating a more intelligent approach to traffic management.&lt;/p&gt;
    &lt;p&gt;The following table summarizes the key differences and the advantages of a client-side approach:&lt;/p&gt;
    &lt;p&gt;Table 1: Default Kubernetes LB vs. Databricks' Client-Side LB&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Feature/Aspect&lt;/cell&gt;
        &lt;cell role="head"&gt;Default Kubernetes Load Balancing (kube-proxy)&lt;/cell&gt;
        &lt;cell role="head"&gt;Databricks' Client-Side Load Balancing&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Load Balancing Layer&lt;/cell&gt;
        &lt;cell&gt;Layer 4 (TCP/IP)&lt;/cell&gt;
        &lt;cell&gt;Layer 7 (Application/gRPC)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Decision Frequency&lt;/cell&gt;
        &lt;cell&gt;Once per TCP connection&lt;/cell&gt;
        &lt;cell&gt;Per-request&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Service Discovery&lt;/cell&gt;
        &lt;cell&gt;CoreDNS + kube-proxy (virtual IP)&lt;/cell&gt;
        &lt;cell&gt;xDS-based Control Plane + Client Library&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Supported Strategies&lt;/cell&gt;
        &lt;cell&gt;Basic (Round-robin, Random)&lt;/cell&gt;
        &lt;cell&gt;Advanced (P2C, Zone-affinity, Pluggable)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Tail Latency Impact&lt;/cell&gt;
        &lt;cell&gt;High (due to traffic skew on persistent connections)&lt;/cell&gt;
        &lt;cell&gt;Reduced (even distribution, dynamic routing)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Resource Utilization&lt;/cell&gt;
        &lt;cell&gt;Inefficient (over-provisioning)&lt;/cell&gt;
        &lt;cell&gt;Efficient (balanced load)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Dependency on DNS/Proxy&lt;/cell&gt;
        &lt;cell&gt;High&lt;/cell&gt;
        &lt;cell&gt;Minimal/Minimal, not on a critical path&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Operational Control&lt;/cell&gt;
        &lt;cell&gt;Limited&lt;/cell&gt;
        &lt;cell&gt;Fine-grained&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This system enables intelligent, up-to-date request routing with minimal dependency on DNS or Layer 4 networking. It gives clients the ability to make informed decisions based on live topology and health data.&lt;/p&gt;
    &lt;p&gt;The figure shows our custom Endpoint Discovery Service in action. It reads service and endpoint data from the Kubernetes API and translates it into xDS responses. Both Armeria clients and API proxies stream requests to it and receive live endpoint metadata, which is then used by application servers for intelligent routing with fallback clusters as backup.‚Äù&lt;/p&gt;
    &lt;p&gt;We run a lightweight control plane that continuously monitors the Kubernetes API for changes to Services and EndpointSlices. It maintains an up-to-date view of all backend pods for every service, including metadata like zone, readiness, and shard labels.&lt;/p&gt;
    &lt;p&gt;A strategic advantage for Databricks was the widespread adoption of a common framework for service communication across most of its internal services, which are predominantly written in Scala. This shared foundation allowed us to embed client-side service discovery and load balancing logic directly into the framework, making it easy to adopt across teams without requiring custom implementation effort.&lt;/p&gt;
    &lt;p&gt;Each service integrates with our custom client, which subscribes to updates from the control plane for the services it depends on during the connection setup. The client maintains a dynamic list of healthy endpoints, including metadata like zone or shard, and updates automatically as the control plane pushes changes.&lt;/p&gt;
    &lt;p&gt;Because the client bypasses both DNS resolution and kube-proxy entirely, it always has a live, accurate view of service topology. This allows us to implement consistent and efficient load balancing strategies across all internal services.&lt;/p&gt;
    &lt;p&gt;The rpc client performs request-aware load balancing using strategies like:&lt;/p&gt;
    &lt;p&gt;More advanced strategies, like zone-aware routing, required careful tuning and deeper context about service topology, traffic patterns, and failure modes; a topic to explore in a dedicated follow-up post.&lt;/p&gt;
    &lt;p&gt;To ensure the effectiveness of our approach, we ran extensive simulations, experiments, and real-world metric analysis. We validated that load remained evenly distributed and that key metrics like tail latency, error rate, and cross-zone traffic cost stayed within target thresholds. The flexibility to adapt strategies per-service has been valuable, but in practice, keeping it simple (and consistent) has worked best.&lt;/p&gt;
    &lt;p&gt;Our control plane extends its utility beyond the internal service-to-service communication. It plays a crucial role in managing external traffic by speaking the xDS API to Envoy, the discovery protocol that lets clients fetch up-to-date configuration (like clusters, endpoints, and routing rules) dynamically. Specifically, it implements Endpoint Discovery Service (EDS) to provide Envoy with consistent and up-to-date metadata about backend endpoints by programming ClusterLoadAssignment resources. This ensures that gateway-level routing (e.g., for ingress or public-facing traffic) aligns with the same source of truth used by internal clients.&lt;/p&gt;
    &lt;p&gt;This architecture gives us fine-grained control over routing behavior while decoupling service discovery from the limitations of DNS and kube-proxy. The key takeaways are:&lt;/p&gt;
    &lt;p&gt;After deploying our client-side load balancing system, we observed significant improvements across both performance and efficiency:&lt;/p&gt;
    &lt;p&gt;While the rollout delivered clear benefits, we also uncovered several challenges and insights along the way:&lt;/p&gt;
    &lt;p&gt;While developing our client-side load balancing approach, we evaluated other alternative solutions. Here‚Äôs why we ultimately decided against these:&lt;/p&gt;
    &lt;p&gt;Kubernetes headless services (clusterIP: None) provide direct pod IPs via DNS, allowing clients and proxies (like Envoy) to perform their own load balancing. This approach bypasses the limitation of connection-based distribution in kube-proxy and enables advanced load balancing strategies offered by Envoy (such as round robin, consistent hashing, and least-loaded round robin).&lt;/p&gt;
    &lt;p&gt;In theory, switching existing ClusterIP services to headless services (or creating additional headless services using the same selector) would mitigate connection reuse issues by providing clients direct endpoint visibility. However, this approach comes with practical limitations:&lt;/p&gt;
    &lt;p&gt;Although headless services can offer a temporary improvement over ClusterIP services, the practical challenges and limitations made them unsuitable as a long-term solution at Databricks' scale.&lt;/p&gt;
    &lt;p&gt;Istio provides powerful Layer 7 load balancing features using Envoy sidecars injected into every pod. These proxies handle routing, retries, circuit breaking, and more - all managed centrally through a control plane.&lt;/p&gt;
    &lt;p&gt;While this model offers many capabilities, we found it unsuitable for our environment at Databricks for a few reasons:&lt;/p&gt;
    &lt;p&gt;We also evaluated Istio‚Äôs Ambient Mesh. Since Databricks already had proprietary systems for functions like certificate distribution, and our routing patterns were relatively static, the added complexity of adopting a full mesh outweighed the benefits. This was especially true for a small infra team supporting a predominantly Scala codebase.&lt;/p&gt;
    &lt;p&gt;It is worth noting that one of the biggest advantages of sidecar-based meshes is language-agnosticism: teams can standardize resiliency and routing across polyglot services without maintaining client libraries everywhere. At Databricks, however, our environment is heavily Scala-based, and our monorepo plus fast CI/CD culture make the proxyless, client-library approach far more practical. Rather than introducing the operational burden of sidecars, we invested in building first-class load balancing directly into our libraries and infrastructure components.&lt;/p&gt;
    &lt;p&gt;Our current client-side load balancing approach has significantly improved internal service-to-service communication. Yet, as Databricks continues to scale, we‚Äôre exploring several advanced areas to further enhance our system:&lt;/p&gt;
    &lt;p&gt;Cross-Cluster and Cross-Region Load Balancing: As we manage thousands of Kubernetes clusters across multiple regions, extending intelligent load balancing beyond individual clusters is critical. We are exploring technologies like flat L3 networking and service-mesh solutions, integrating seamlessly with multi-region Endpoint Discovery Service (EDS) clusters. This will enable robust cross-cluster traffic management, fault tolerance, and globally efficient resource utilization.&lt;/p&gt;
    &lt;p&gt;Advanced Load Balancing Strategies for AI Use Cases: We plan to introduce more sophisticated strategies, such as weighted load balancing, to better support advanced AI workloads. These strategies will enable finer-grained resource allocation and intelligent routing decisions based on specific application characteristics, ultimately optimizing performance, resource consumption, and cost efficiency.&lt;/p&gt;
    &lt;p&gt;If you're interested in working on large-scale distributed infrastructure challenges like this, we're hiring. Come build with us ‚Äî explore open roles at Databricks!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45434417</guid><pubDate>Wed, 01 Oct 2025 05:06:38 +0000</pubDate></item><item><title>Basic Dialects, IDEs, and Tutorials</title><link>https://github.com/JohnBlood/awesome-basic</link><description>&lt;doc fingerprint="951f683b52f730d4"&gt;
  &lt;main&gt;
    &lt;p&gt;A curated list of awesome BASIC dialects, IDEs, and tutorials&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AppGameKit - an easy-to-learn game development engine, ideal for Beginners, Hobbyists &amp;amp; Indie developers. Now anyone can quickly code and build apps for multiple platforms using AppGameKit - have your demos and games up and running on mobile devices.&lt;/item&gt;
      &lt;item&gt;atinybasic - An Actually Tiny BASIC for Arduino.&lt;/item&gt;
      &lt;item&gt;B4X - Simple, powerful, and modern development tools.&lt;/item&gt;
      &lt;item&gt;BaCon - a free BASIC to C translator for Unix-based systems, which runs on most Unix/Linux/BSD platforms, including MacOSX. It intends to be a programming aid in creating tools which can be compiled on different platforms (including 64-bit environments) while trying to revive the days of the good old BASIC.&lt;/item&gt;
      &lt;item&gt;basgo - compiles BASIC-lang to Golang.&lt;/item&gt;
      &lt;item&gt;BASIC Compiler - BASIC Compiler is an open-source BASIC compiler written in Java. It compiles a BASIC program into Java bytecode, which can be executed with any Java Virtual Machine 1.5 and higher.&lt;/item&gt;
      &lt;item&gt;BASIC8 - an integrated Fantasy Computer for games and other program development. You can create, share, and play disks in a modern BASIC dialect, with built-in tools for editing sprites, tiles, maps, quantized, etc.&lt;/item&gt;
      &lt;item&gt;BASIC-256 - an easy-to-use version of BASIC designed to teach anybody how to program. A built-in graphics mode lets them draw pictures on screen in minutes, and a set of easy-to-follow tutorials introduce programming concepts through fun exercises.&lt;/item&gt;
      &lt;item&gt;BBCSDL - an advanced cross-platform implementation of BBC BASIC for Windows, Linux (x86 CPU only), MacOS, Raspberry Pi (Raspbian), Android, iOS or for running in a browser. It combines the simplicity of BASIC with the sophistication of a structured language, allowing you to write utilities and games, use sound and graphics, and perform calculations.&lt;/item&gt;
      &lt;item&gt;BCX - BCX converts your BCX BASIC source code into high-performing, efficient C\C++ source code. Use C\C++ libraries and header files without having to first convert them into BASIC.&lt;/item&gt;
      &lt;item&gt;BlitzMax DX - a fork of BlitzMax NG.&lt;/item&gt;
      &lt;item&gt;BlitzMax NG - a fast cross-platform, open-source, programming language.&lt;/item&gt;
      &lt;item&gt;bootBASIC - a BASIC language in 512 bytes of x86 machine code.&lt;/item&gt;
      &lt;item&gt;Bywater BASIC Interpreter - implements a large superset of the ANSI Standard for Minimal BASIC (X3.60-1978) and a significant subset of the ANSI Standard for Full BASIC (X3.113-1987) in C.&lt;/item&gt;
      &lt;item&gt;Cerberus X - A cross-platform development toolset which serves 2D game development at its core. Cerberus X is a fork of the Monkey X programming language.&lt;/item&gt;
      &lt;item&gt;Chipmunk Basic - an interpreter for the BASIC Programming Language. It runs on multiple OS platforms and is reasonably fast for a pure interpreter. Chipmunk Basic presents a traditional (vintage) terminal-command-line programming environment and supports a simple, old-fashioned, and easy-to-learn dialect of the Basic Programming Language. (Line numbers are required when using the built-in command-line console, but are not required in Basic programs written using an external text editor.) The language also supports a few advanced extensions. Free for educational and personal use.&lt;/item&gt;
      &lt;item&gt;cbmbasic - a portable version of Commodore's version of Microsoft BASIC 6502 as found on the Commodore 64.&lt;/item&gt;
      &lt;item&gt;Dark Basic Pro - an open-source BASIC programming language for creating Windows applications and games.&lt;/item&gt;
      &lt;item&gt;endbasic - BASIC environment with a REPL, a web interface, and RPi support written in Rust.&lt;/item&gt;
      &lt;item&gt;FreeBASIC - a free/open source (GPL), BASIC compiler for Microsoft Windows, DOS, and Linux.&lt;/item&gt;
      &lt;item&gt;FutureBasic - a high-level procedural programming language combined with an "Integrated Development Environment" (IDE) for creating native Intel Macintosh applications. It provides an editor, compiler, project manager, documentation, and code samples.&lt;/item&gt;
      &lt;item&gt;Gambas - a free development environment and a full powerful development platform based on a Basic interpreter with object extensions, as easy as Visual Basic.&lt;/item&gt;
      &lt;item&gt;GLBasic - an easy-to-learn BASIC language with Editor, Compiler, and Debugger. The generated C++ code compiles to lightning-fast apps for several platforms.&lt;/item&gt;
      &lt;item&gt;JADE - AKA "Jade's A Developing Experiment" - This is a proof of concept using a BASIC-like syntax to program C++.&lt;/item&gt;
      &lt;item&gt;jScriptBasic - ScriptBasic for Java is a BASIC interpreter that can be embedded into Java programs.&lt;/item&gt;
      &lt;item&gt;Just BASIC - a programming language for Windows. It is completely free and it is suitable for creating all kinds of applications for business, industry, education, and entertainment.&lt;/item&gt;
      &lt;item&gt;jvmBASIC - A BASIC to JVM bytecode compiler.&lt;/item&gt;
      &lt;item&gt;KayaBASIC - Multi-platform BASIC compiler, that supports Windows, Linux, and macOS. easy extends with C++.&lt;/item&gt;
      &lt;item&gt;Liberty BASIC - The commercial version of Just BASIC.&lt;/item&gt;
      &lt;item&gt;LychenBASIC - anachronistic, Windows-only, BASIC language programming blog post explanation.&lt;/item&gt;
      &lt;item&gt;MatrixBrandy - a Fork of Brandy BASIC V for Linux. Brandy implements Basic VI, the the 64-bit floating-point mathematics variant of the dialect of Basic that Acorn Computers supplied with their ranges of desktop computers that use the ARM processor such as the Archimedes and RiscPC. Basic V and VI are an extended version of BBC Basic. This was the Basic used on the BBC Micro that Acorn made during the early 1980s.&lt;/item&gt;
      &lt;item&gt;MBC - MBC is a Basic to C/C++ translator, originally based on the BCX Windows translator by Kevin Diggins. It has successfully compiled using Clang++ and G++ on macOS/Linux 64bit OS's, and G++ on RaspberryPi.&lt;/item&gt;
      &lt;item&gt;micro(A) - micro(A) is a modern and minimal general purpose programming language. It is Easy to Use BASIC-like Programming Language. micro(A) interpreter comes with a complete IDE in which you can make your programs.&lt;/item&gt;
      &lt;item&gt;Monkey 2 - Monkey2 is an easy-to-use, cross-platform, games oriented programming language from Blitz Research.&lt;/item&gt;
      &lt;item&gt;my_basic - A lightweight BASIC interpreter written in standard C in dual files. Aims to be embeddable, extendable, and portable.&lt;/item&gt;
      &lt;item&gt;NaaLaa - stands for 'Not An Advanced Language At All'. It's a very easy-to-learn programming language for beginners interested in retro-style game development. NaaLaa is free to use and you may do whatever you want with the programs you create with it. NaaLaa runs and compiles on Windows and Linux.&lt;/item&gt;
      &lt;item&gt;NSB/AppStudio - A complete, powerful development environment. Create apps for iOS, Android, Windows, MacOS and Linux.&lt;/item&gt;
      &lt;item&gt;nuBASIC - nuBASIC is an implementation of a BASIC interpreter and IDE for Windows and Linux.&lt;/item&gt;
      &lt;item&gt;nuBScript - nuBScript is a programming language distributed under MIT License.&lt;/item&gt;
      &lt;item&gt;Oxygen Basic - a Compact embeddable JIT compiler that reads C headers and compiles to x86 machine code. Executes directly in memory or creates DLLs and EXE files. Supports overloading and OOP. Currently available for MS platforms.&lt;/item&gt;
      &lt;item&gt;PC-BASIC - a free, cross-platform interpreter for GW-BASIC, Advanced BASIC (BASICA), PCjr Cartridge Basic and Tandy 1000 GWBASIC.&lt;/item&gt;
      &lt;item&gt;PuffinBASIC - BASIC interpreter written in Java.&lt;/item&gt;
      &lt;item&gt;PureBasic - a modern BASIC programming language. The key features of PureBasic are portability (Windows, Linux, and OS X supported with the same source code), the production of very fast and optimized native 32-bit or 64-bit executables, and, of course, the very simple BASIC language syntax. PureBasic has been created for the beginner and expert alike. We have put a lot of effort into its conception to produce a fast, reliable system and friendly BASIC compiler.&lt;/item&gt;
      &lt;item&gt;PyBasic - Simple interactive BASIC interpreter written in Python&lt;/item&gt;
      &lt;item&gt;QB64 - a modern extended BASIC programming language that retains QBasic/QuickBASIC 4.5 compatibility and compiles native binaries for Windows, Linux, and macOS.&lt;/item&gt;
      &lt;item&gt;Quite BASIC - a web-based classic BASIC online programming environment.&lt;/item&gt;
      &lt;item&gt;RAD Basic - 100% compatible with your Visual Basic 6 projects.&lt;/item&gt;
      &lt;item&gt;RCBasic - a simple easy-to-learn programming language with many built-in functions to aid in game and multimedia application development. RCBasic is free software distributed under the Zlib license.&lt;/item&gt;
      &lt;item&gt;RemObjects Mercury - Mercury is an implementation of the BASIC programming language that is fully code-compatible with Microsoft Visual Basic.NET‚Ñ¢ but takes it to the next level, and to new horizons.&lt;/item&gt;
      &lt;item&gt;RetroBASIC - RetroBASIC is an interpreter that aims to run programs from any early dialect. It is fully compatible with Microsoft BASIC from the 6502 machines, but also supports real Dartmouth BASIC, HP 2000 and many, many others. Extensive documentation explains these variations and their support in RetroBASIC.&lt;/item&gt;
      &lt;item&gt;sdlBASIC - A easy basic in order to make games in 2d style amos for linux and windows.&lt;/item&gt;
      &lt;item&gt;SharpBASIC - SharpBASIC is a new programming language that is currently in development. As the name suggests the language should be considered a sharper BASIC, more powerful, less verbose but with a clear structure.&lt;/item&gt;
      &lt;item&gt;SmallBASIC - a fast and easy-to-learn BASIC language interpreter ideal for everyday calculations, scripts, and prototypes. SmallBASIC includes trigonometric, matrices, and algebra functions, a built in IDE, a powerful string library, system, sound, and graphic commands along with structured programming syntax.&lt;/item&gt;
      &lt;item&gt;SmallBasic - Small Basic is the only programming language created specially to help students transition from block-based coding to text-based coding.&lt;/item&gt;
      &lt;item&gt;SpecBAS - an enhanced Sinclair BASIC interpreter for modern PCs.&lt;/item&gt;
      &lt;item&gt;SpiderBasic - A Basic to master the web.&lt;/item&gt;
      &lt;item&gt;thinBASIC - a very fast "BASIC-like" programming language useful to Beginners and to Gurus. BASIC interpreter for Windows able to create console and gui applications with most of the user interface controls, automate process, automate data exchange, connect to databases, send mails, connect to FTP sites, rest api, parsing strings, tokenizing, traversing xml, handling files, Windows Registry, OpenGl, graphics, sound, printing ... and much more.&lt;/item&gt;
      &lt;item&gt;TrekBasic - TrekBasic provides both a BASIC interpreter and a compiler. Using LLVM as a backend allows you to compile for any platform LLVM supports. The interpreter shell provides tools like breakpoints, execution trace, data breakpoints, and more. The compiler offers high performance. TrekBasic is written in python, and is small, and easily modified to your needs.&lt;/item&gt;
      &lt;item&gt;twinBASIC - twinBASIC is a modern version of the classic BASIC programming language.&lt;/item&gt;
      &lt;item&gt;wwwBASIC - an implementation of BASIC that runs on Node.js and the Web.&lt;/item&gt;
      &lt;item&gt;X11-Basic - a dialect of the BASIC programming language with graphics and sound.&lt;/item&gt;
      &lt;item&gt;XC=BASIC - a dialect of the BASIC programming language for the Commodore-64 and xcbasic64 is a cross compiler that compiles an XC=BASIC program to 6502 machine code trough the DASM macro assembler. It runs on Windows, Linux, and Mac OS. The name XC=BASIC stands for "Cross Compiled BASIC".&lt;/item&gt;
      &lt;item&gt;Xojo - Build Native, Cross-Platform Apps. Rapid application development for Desktop, Web, Mobile &amp;amp; Raspberry Pi. Develop on macOS, Windows, or Linux.&lt;/item&gt;
      &lt;item&gt;Yabasic - a traditional basic-interpreter. It comes with goto and various loops and allows to define subroutines and libraries. It does simple graphics and printing. Yabasic can call out to libraries written in C and allows to create standalone programs. Yabasic runs under Unix and Windows and has comprehensive documentation; it is small, simple, open-source and free.&lt;/item&gt;
      &lt;item&gt;YAB - a complete BASIC programming language for Haiku. Yab allows fast prototyping with simple and clean code. yab contains a large number of BeAPI-specific commands for GUI creation and much, much more. yab-IDE is a powerful development environment, which of course is programmed in yab itself.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;DavsIDE - an Alternative IDE for the QB64 compiler.&lt;/item&gt;
      &lt;item&gt;InForm - a GUI engine and WYSIWYG interface designer for QB64.&lt;/item&gt;
      &lt;item&gt;QBASDOWN - a Markdown implementation for FreeDOS. Written for FreeDOS in QuickBASIC 4.5&lt;/item&gt;
      &lt;item&gt;mono-basic - Visual Basic Compiler and Runtime.&lt;/item&gt;
      &lt;item&gt;VisualFBEditor - an IDE for FreeBasic under active development.&lt;/item&gt;
      &lt;item&gt;vscode-vba - Extension that adds rich VBA editor support to Visual Studio Code.&lt;/item&gt;
      &lt;item&gt;WinFBE - FreeBASIC Editor for Windows.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Collection of Commodore BASIC programs&lt;/item&gt;
      &lt;item&gt;Hoard of GW-BASIC - A collection of GW-BASIC code by various authors.&lt;/item&gt;
      &lt;item&gt;MBASIC-Protect - Information on the CP/M MBASIC interpreter's protect mode.&lt;/item&gt;
      &lt;item&gt;GW-BASIC source code - The original source code of Microsoft GW-BASIC from 1983.&lt;/item&gt;
      &lt;item&gt;Project Cherry - a Chip-8/SCHIP emulator written in FreeBASIC.&lt;/item&gt;
      &lt;item&gt;The Basics' page (since 2001)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A Beginner's Guide To FreeBasic&lt;/item&gt;
      &lt;item&gt;A Beginner's Guide to Gambas&lt;/item&gt;
      &lt;item&gt;BASIC Gaming - a short-lived ezine that has a wealth of information&lt;/item&gt;
      &lt;item&gt;BlitzMax for the Absolute Beginner&lt;/item&gt;
      &lt;item&gt;Franktic's FreeBASIC Programming Tutorial&lt;/item&gt;
      &lt;item&gt;Programming with yab&lt;/item&gt;
      &lt;item&gt;QB64 Game Programming&lt;/item&gt;
      &lt;item&gt;QBasic (QB64) Tutorial Video Series by SchoolFreeware&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45434511</guid><pubDate>Wed, 01 Oct 2025 05:22:06 +0000</pubDate></item><item><title>High-resolution efficient image generation from WiFi Mapping</title><link>https://arxiv.org/abs/2506.10605</link><description>&lt;doc fingerprint="1c1c260941f11282"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Computer Vision and Pattern Recognition&lt;/head&gt;&lt;p&gt; [Submitted on 12 Jun 2025 (v1), last revised 5 Sep 2025 (this version, v3)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:High-resolution efficient image generation from WiFi CSI using a pretrained latent diffusion model&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:We present LatentCSI, a novel method for generating images of the physical environment from WiFi CSI measurements that leverages a pretrained latent diffusion model (LDM). Unlike prior approaches that rely on complex and computationally intensive techniques such as GANs, our method employs a lightweight neural network to map CSI amplitudes directly into the latent space of an LDM. We then apply the LDM's denoising diffusion model to the latent representation with text-based guidance before decoding using the LDM's pretrained decoder to obtain a high-resolution image. This design bypasses the challenges of pixel-space image generation and avoids the explicit image encoding stage typically required in conventional image-to-image pipelines, enabling efficient and high-quality image synthesis. We validate our approach on two datasets: a wide-band CSI dataset we collected with off-the-shelf WiFi devices and cameras; and a subset of the publicly available MM-Fi dataset. The results demonstrate that LatentCSI outperforms baselines of comparable complexity trained directly on ground-truth images in both computational efficiency and perceptual quality, while additionally providing practical advantages through its unique capacity for text-guided controllability.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Eshan Ramesh [view email]&lt;p&gt;[v1] Thu, 12 Jun 2025 11:47:23 UTC (6,672 KB)&lt;/p&gt;&lt;p&gt;[v2] Fri, 4 Jul 2025 12:27:28 UTC (6,672 KB)&lt;/p&gt;&lt;p&gt;[v3] Fri, 5 Sep 2025 11:39:36 UTC (9,960 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45434941</guid><pubDate>Wed, 01 Oct 2025 06:33:02 +0000</pubDate></item><item><title>Type Theory and Functional Programming (1999) [pdf]</title><link>https://www.cs.cornell.edu/courses/cs6110/2015sp/textbook/Simon%20Thompson%20textbook.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45435100</guid><pubDate>Wed, 01 Oct 2025 07:00:21 +0000</pubDate></item><item><title>Category Theory Illustrated ‚Äì Natural Transformations</title><link>https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/</link><description>&lt;doc fingerprint="307d2805cbf1b5d"&gt;
  &lt;main&gt;&lt;quote&gt;&lt;p&gt;I didn‚Äôt invent categories to study functors; I invented them to study natural transformations. ‚Äî Saunders Mac Lane&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;In this chapter, we will introduce the concept of a morphism between functors, or natural transformation. Understanding natural transformations will enable us to define category equality and some other advanced concepts.&lt;/p&gt;&lt;p&gt;Natural transformations really are at the heart of category theory, however, their importance is not obvious at first. So, before introducing them, I like to talk, once more, about the body of knowledge that this heart maintains (I am good with metaphors‚Ä¶ in principle).&lt;/p&gt;&lt;p&gt;Our first section aims to introduce natural transformation as a motivating example for creating a way to say that two categories are equal. But for that, we need to understand what equal categories are and should be.&lt;/p&gt;&lt;p&gt;So, are you ready to hear about equivalent categories and natural transformations? Actually it is my opinion that you are not (no offence, they are just very hard!). So, we will take a longer route. I can put this next section anywhere in this book, and it would always be neither here nor there. But anyway, if you are studying math, you are probably interested in the nature of the universe. ‚ÄúWhat is the quintessential characteristic of all things in this world?‚Äù I hear you ask‚Ä¶&lt;/p&gt;&lt;quote&gt;&lt;p&gt;The world is the collection of facts, not of things. ‚Äî Ludwig Wittgenstein&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;What is the quintessential characteristic of all things in this world? Some 2500 years ago, the philosopher Parmenides gave an answer to this question, postulating that the nature of the universe is permanence, stasis. According to his view, what we perceive as processes/transformations/change is merely illusory appearances (‚ÄúWhatever is is, and what is not cannot be‚Äù). He said that that things never really change, they only appear to change, or (another way to put it), only appearances change, but the essence does not (I think this is pretty much how the word ‚Äúessence‚Äù came to exist).&lt;/p&gt;&lt;p&gt;Although far from obviously true, his view is easy for people to relate to ‚Äî objects are all around us, everything we ‚Äúsee‚Äù, both literally (in real life), or metaphorically (in mathematics and other disciplines), can be viewed as objects, persisting through space and time. If we subscribe to this view, then we would think that the key to understanding the world is understanding what objects are. In my opinion, this is what set theory does, to some extent, as well as classical logic (Plato was influenced by Parmenides when he created his theory of forms).&lt;/p&gt;&lt;p&gt;However, there is another way to approach the question about the nature of the universe, which is equally compelling. Because, what is an object, when viewed by itself? Can we study an object in isolation? And will there anything left to study about it, once it is detached from its environment? If a given object undergoes a process to get all of it‚Äôs part replaced, is it still the same object?&lt;/p&gt;&lt;p&gt;Asking such questions might lead us to suspect that, although what we see when we look at the universe are the objects, it is the processes/relations/transitions or morphisms between the objects that are the real key to understanding it. For example, when we think hard about everyday objects we realize that each of them has a specific functions (note the term) without which, a thing would not be itself e.g. is a lamp that doesn‚Äôt glow, still a lamp? Is there food that is non-edible (or an edible item that isn‚Äôt food)? And this is even more valid for mathematical objects, which, without the functions that go between them, are not objects at all.&lt;/p&gt;&lt;p&gt;So, instead of thinking about objects that just happen to have some morphisms between them, we might take the opposite view and say that objects are only interesting as sources and targets of morphisms.&lt;/p&gt;&lt;p&gt;Although old, dating back to Parmenides‚Äô alleged rival Heraclitus, this view has been largely unexplored, until the 20th century, when a real mathematical revolution happened: Bertrand Russell created type theory, his student Ludwig Wittgenstein wrote a little book, from which the above quote comes, and this book inspired a group of mathematicians and logicians, known as the ‚ÄúVienna circle‚Äù. Part of this group was Rudolph Carnap who coined the word ‚Äúfunctor‚Äù‚Ä¶&lt;/p&gt;&lt;p&gt;An embodiment of Heraclitus‚Äô view in the realm of category theory is the concept of isomorphism invariance that we implicitly touched several times.&lt;/p&gt;&lt;p&gt;All categorical constructions that we covered (products/coproducts, initial/terminal objects, functional objects in logic) are isomorphism-invariant. Or, equivalently, they define an objects up to an isomorphism. Or, in other words, if there are two or more objects that are isomorphic to one another, and one of them has a given property, then the rest of them would to also have this property as well.&lt;/p&gt;&lt;p&gt;In short, in category theory isomorphism = equality.&lt;/p&gt;&lt;p&gt;The key to understanding category theory lies in understanding isomorphism invariance. And the key to understanding isomorphism invariance are natural transformations.&lt;/p&gt;&lt;p&gt;Let‚Äôs return to the question that we were pondering at the beginning of the previous chapter ‚Äî what does it mean for two categories to be equal?&lt;/p&gt;&lt;p&gt;In the prev chapter, we talked a lot about how great isomorphisms are and how important they are for defining the concept of equality in category theory, but at the same time we said that categorical isomorphisms do not capture the concept of equality of categories.&lt;/p&gt;&lt;p&gt;This is because (though it may seem contradictory at first) categorical isomorphisms are not isomorphism invariant, i.e. categories that only differ by having some additional isomorphic objects aren‚Äôt isomorphic themselves.&lt;/p&gt;&lt;p&gt;For this reason, we need a new concept of equality of categories. A concept that would elucidate the differences between categories with different structure, but also the sameness of categories that have the same categorical structures, disregarding the differences that are irrelevant for category-theoretic standpoint. That concept is equivalence.&lt;/p&gt;&lt;p&gt;Parmenides: This category surely cannot be equal to the other one ‚Äî it has a different amount of objects!&lt;/p&gt;&lt;p&gt;Heraclitus: Who cares bro, they are isomorphic.&lt;/p&gt;&lt;p&gt;To understand equivalent categories better, let‚Äôs go back to the functor between a given map and the area it represents (we will only consider the thin categories (AKA orders) for now). This functor would be invertible (and the categories ‚Äî isomorphic) when the map should represent the area completely i.e. there should be arrow for each road and a point for each little place.&lt;/p&gt;&lt;p&gt;Such a map is necessary if your goal is to know about all places, however, like we said, when working with category theory, we are not so interested in places, but in the routes that connect them i.e. we focus not on objects but on morphisms.&lt;/p&gt;&lt;p&gt;For example, if there are intersections that are positioned in such a way that there are routes from one and to the other and vice-versa a map may collapse them into one intersection and still show all routes that exist (the tree routes would be represented by the ‚Äúidentity route‚Äù).&lt;/p&gt;&lt;p&gt;These two categories are not isomorphic ‚Äî going from one of them to the other and back again doesn‚Äôt lead you to the same object.&lt;/p&gt;&lt;p&gt;However, going from one of them to the other would lead you at least to an isomorphic object.&lt;/p&gt;&lt;p&gt;In this case we say that the orders are equivalent.&lt;/p&gt;&lt;p&gt;We know that two orders are isomorphic if there are two functors, such that going from one to the other and back again leads you to the same object.&lt;/p&gt;&lt;p&gt;And two orders are equivalent if going from one of them to the other and back again leads you to the same object, or to an object that is isomorphic to the one you started with.&lt;/p&gt;&lt;p&gt;But when does this happen? To understand this, we plot the orders as a Hasse diagram.&lt;/p&gt;&lt;p&gt;You can see that, although not all objects are connected one-to-one, all objects at a given level are connected to objects of the corresponding level.&lt;/p&gt;&lt;p&gt;To formalize that notion, we remember the concept of equivalence classes that we covered in the chapter about orders. Let‚Äôs visualize the relationship of the equivalence classes of the two orders that we saw above.&lt;/p&gt;&lt;p&gt;You can see that they are isomorphic. And that is no coincidence: two orders are equivalent precisely when the orders made of their equivalence classes are isomorphic.&lt;/p&gt;&lt;p&gt;This is a definition for equivalence of orders, but unfortunately, it does not hold for all categories ‚Äî when we are working with orders, we can get away by just thinking about objects, but categories demands that we think about morphisms i.e. to prove two categories are equivalent, we should establish an isomorphism between their morphisms.&lt;/p&gt;&lt;p&gt;For example, the following two categories are not equivalent, although their equivalence classes are isomorphic ‚Äî the category on the left has just one morphism, but the category on the right has two.&lt;/p&gt;&lt;p&gt;One way of defining equivalence of categories is by generalizing the notion of equivalence classes of orders to what we call skeletons of categories, a skeleton of a category being a subcategory in which all objects that are isomorphic to one another are ‚Äúmerged‚Äù into one object (isomorphic objects are necessarily identical).&lt;/p&gt;&lt;p&gt;However, we will leave this (pardon my French) as an exercise for the reader. Why? We already did this when we generalized the notion of normal set-theoretic functions to functors, and so it makes more sense to build up on that notion. Also, we need a motivating example for introducing natural transformations, remember?&lt;/p&gt;&lt;p&gt;In the chapter about orders, we presented a definition of order isomorphisms, that is based on objects:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;An order isomorphism is essentially an isomorphism between the orders‚Äô underlying sets (invertible function). However, besides their underlying sets, orders also have the arrows that connect them, so there is one more condition: in order for an invertible function to constitute an order isomorphism it has to respect those arrows, in other words it should be order preserving. More specifically, applying this function (let‚Äôs call it $F$) to any two elements in one set ($a$ and $b$) should result in two elements that have the same corresponding order in the other set (so $a ‚â§ b$ if and only if $F(a) ‚â§ F(b)$).&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;That a way to define them, but it is not the best way. Now that we know about functors (which, as we said, serve as functions between the orders and other categories), we can devise a new, simpler definition, which would also be valid for all categories, not just orders, and for all forms of equality (isomorphism and equivalence).&lt;/p&gt;&lt;p&gt;We begin with the definition of set isomorphism:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Two sets $A$ and $B$ are isomorphic (or $A ‚âÖ B$) if there exist functions $f: A \to B$ and its reverse $g: B \to A$, such that $f \circ g = ID_{B}$ and $g \circ f = ID_{A}$.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;To amend it so it is valid for all categories by just replacing the word ‚Äúfunction‚Äù with ‚Äúfunctor‚Äù and ‚Äúset‚Äù with ‚Äúcategory‚Äù:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Two categories $A$ and $B$ are isomorphic (or $A \cong B$) if there exist functors $f: A \to B$ and its reverse $g: B \to A$, such that $f \circ g = ID_{B}$ and $g \circ f = ID_{A}$.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Task 1: Check if that definition is valid.&lt;/p&gt;&lt;p&gt;Believe it or not, this definition, is just one find-and-replace operation away from the definition of equivalence. We get there only by replace equality with isomorphism (so, $=$ with $\cong$).&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Two categories $A$ and $B$ are equivalent (or $A \simeq B$) if there exist functors $f: A \to B$ and its reverse $g: B \to A$, such that $f \circ g \cong ID_{B}$ and $g \circ f \cong ID_{A}$.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Like we said at the beginning, with isomorphisms, going back and forth brings us to the same object, while with equivalence the object is just isomorphic to the original one. This is truly all there is to it.&lt;/p&gt;&lt;p&gt;There is only one problem, though ‚Äî we never said what it means for functors to be isomorphic.&lt;/p&gt;&lt;p&gt;So, how can we make the above definition ‚Äúcome to life‚Äù? The title of this chapter outlines the things we need to define:&lt;/p&gt;&lt;p&gt;If this sounds complicated, remember that we are doing the same thing we always did ‚Äî talking about isomorphisms.&lt;/p&gt;&lt;p&gt;In the very first chapter of this book, we introduced set isomorphisms, which are quite easy, and now we reached the point to examine functor isomorphisms. So, we are doing the same thing. Although actually‚Ä¶&lt;/p&gt;&lt;p&gt;But actually, natural transformations are quite different from morphisms and functors, (the definition is not ‚Äúrecursive‚Äù, like the definitions of functor and morphism are). This is because functions and functors are both morphisms between objects (or 1-morphisms), while natural transformations are morphisms between morphisms (known as 2-morphisms).&lt;/p&gt;&lt;p&gt;But enough talking, let‚Äôs draw some diagrams. We know that natural transformations are morphisms between functors, so let‚Äôs draw two functors.&lt;/p&gt;&lt;p&gt;The functors have the same signature. Naturally. How else can there be morphisms between them?&lt;/p&gt;&lt;p&gt;Now, a functor is comprised of two mappings (object mapping and morphism mapping) so a mapping between functors, would consist of ‚Äúobject mapping mapping‚Äù and ‚Äúmorphism mapping mapping‚Äù (yes, I often do get in trouble with my choice of terminology, why do you ask?).&lt;/p&gt;&lt;p&gt;Let‚Äôs first connect the object mappings of the two functors, creating what we called ‚Äúobject mapping mapping‚Äù.&lt;/p&gt;&lt;p&gt;It is simpler than it sounds when we realize that we only need to connect the object in functors‚Äô target category ‚Äî the objects in the source category would just always be the same for both functors, as both functors would include all object from the source category (as that is what functors (and morphisms in general) do). In other words, mapping the two functors‚Äô object components involves nothing more than specifying a bunch of morphisms in the target category: one morphism for each object in the source category i.e. each object from the image of the first functor, should have one arrow coming from it (and to an object of the second functor, so, for example, our current source category has two objects and we specify two morphisms.&lt;/p&gt;&lt;p&gt;Note that this mapping does not map every object from the target category, i.e. not all objects have arrows coming from them (e.g. here the black and blue square do not have arrows), although, in some cases, it might.&lt;/p&gt;&lt;p&gt;Task 2: When exactly would the mapping encompass all objects?&lt;/p&gt;&lt;p&gt;The morphism part might seem hard‚Ä¶ until we realize that, once the connections between the object mappings are already established, there is only one way to connect the morphisms ‚Äî we take each morphism of the source category and connect the two morphisms given by the two functors, in the target category. And that‚Äôs all there is to it.&lt;/p&gt;&lt;p&gt;Oh, actually, there is also this condition that the above diagram should commute (the naturality condition), but that happens pretty much automatically.&lt;/p&gt;&lt;p&gt;Just like anything else in category theory, natural transformations have some laws that they are required to pass. In this case it‚Äôs one law, typically called the naturality law, or the naturality condition.&lt;/p&gt;&lt;p&gt;Before we state this law, let‚Äôs recap where are we now: We have two functors $F$ and $G$ that have the same type signature (so $F : C \to D$ and $G : C \to D$ for some categories $C$ and $D$), and a family of morphisms in the target category $D$ (denoted $\alpha : F \Rightarrow G$) one for each object in $C$, that map each object of the target of the functor $F$ (or the image of $F$ in $D$ as it is also called) to some objects of the image of $G$. This is a transformation, but not necessarily a natural one. A transformation is natural, when this diagram commutes for all morphisms in $C$.&lt;/p&gt;&lt;p&gt;i.e. a transformation is natural when every morphism $f$ in $C$ is mapped to morphisms $F(f)$ by $F$ and to $G(f)$ by $G$ (not very imaginative names, I know), in such a way, that we have $\alpha \circ F(f) = G(f) \circ \alpha$ i.e. when starting from the white square, when going right and then down (via the yellow square) is be equivalent to going down and then right (via the black one).&lt;/p&gt;&lt;p&gt;We may view a natural transformation is a mapping between morphisms and commutative squares: two functors and a natural transformation between two categories means that for each morphism in the source category of the functors, there exist one commutative square at the target category.&lt;/p&gt;&lt;p&gt;When we fully understand this, we realize that commutative squares are made of morphisms too, so, like morphisms, they compose ‚Äî for any two morphisms with appropriate type signatures that have we can compose to get a third one, we have two naturality squares which compose the same way.&lt;/p&gt;&lt;p&gt;Which means natural transformation make up a‚Ä¶&lt;/p&gt;&lt;p&gt;(Oh wait, it‚Äôs too early for that, is it?)&lt;/p&gt;&lt;p&gt;After understanding natural transformations, natural isomorphisms, are a no-brainer: a natural transformation is just a family of morphisms in a given category that satisfy certain criteria, then what would a natural isomorphism be? That‚Äôs right ‚Äî it is a family of isomorphisms that satisfy the same criteria. The diagram is the same as the one for ordinary natural transformation, except that $\alpha$ are not just ordinary morphisms, but isomorphisms.&lt;/p&gt;&lt;p&gt;And the turning those morphisms into isomorphisms makes the diagram commute in more than one way i.e. if we have the naturality condition&lt;/p&gt;&lt;p&gt;$\alpha \circ F(f) = G(f) \circ \alpha$ i.e. the two paths going from white to blue are equivalent.&lt;/p&gt;&lt;p&gt;We also have:&lt;/p&gt;&lt;p&gt;$F(f) \circ \alpha = \alpha \circ G(f)$ i.e. the two paths going from black to yellow are also equivalent.&lt;/p&gt;&lt;p&gt;I am sorry, what were we talking about again? Oh yeah ‚Äî categorical equivalence. Remember that categorical equivalence is the reason why we tackle natural transformations and isomorphisms? Or perhaps it was the other way around? Never mind, let‚Äôs recap what we discussed so far:&lt;/p&gt;&lt;p&gt;At the beginning of the section we introduced the notion of equivalence as two functors, such that going from one of them to the other and back again leads you to the same object, or to an object that is isomorphic to the one you started with.&lt;/p&gt;&lt;p&gt;And then, we discussed that for categories that are not thin (thick?) the situation is a bit more complex since they can have more than one morphism between two objects, and we should worry not only about isomorphic objects, but about isomorphic morphisms.&lt;/p&gt;&lt;p&gt;Now, we will show how these two notions are formalized by the definition that we presented.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Two categories $A$ and $B$ are equivalent (or $A \simeq B$) if there exist functors $f: A \to B$ and its reverse $g: B \to A$, such that $f \circ g \cong ID_{A}$ and $g \circ f \cong ID_{A}$.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;To understand, this how are the two related, let‚Äôs construct the identity functor of the category that we have been using as an example all this time. Note that we are drawing the one and the same category two times (as opposed to just drawing an arrow coming from each object to itself), to make the diagrams more readable.&lt;/p&gt;&lt;p&gt;Then, we draw the composite of the two functors that establish an equivalence between the two categories, highlighting the 3 ‚Äúinteresting‚Äù objects, i.e. the ones due to which the categories aren‚Äôt isomorphic.&lt;/p&gt;&lt;p&gt;Now, we ask ourselves, in which cases does there exist an isomorphism between those two functors?&lt;/p&gt;&lt;p&gt;The answer becomes trivial if we draw the isomorphism arrows connecting the three ‚Äúinteresting‚Äù objects in a different way (remember, this is the same category on the top and the bottom) ‚Äî we can see that these are exactly the arrows that enable us to construct an isomorphism between the two functors (the others are just identity arrows).&lt;/p&gt;&lt;p&gt;And when would this isomorphism be such that preserves the structure of the category (so that each morphism from the output of the composite functor has an equivalent one in the output of the identity)? Exactly when the isomorphism is natural i.e. when every morphism is mapped to a commuting square, e.g. here is the commuting square of the morphism that is marked in red.&lt;/p&gt;&lt;p&gt;i.e. naturality condition assures us that the morphisms in the target of the functor behave in the same way as their counterparts in the source.&lt;/p&gt;&lt;p&gt;With this, we are finished with categorical equivalence, but not with natural transformations ‚Äî natural transformations are a very general concept, and categorical equivalences are only a very narrow case of them.&lt;/p&gt;&lt;p&gt;In the course of this book, we learned that programming/computer science is the study of the category of types in programming languages. However (in order to avoid this being too obvious) in the computer science context, we use different terms for the standard category-theoretic concepts.&lt;/p&gt;&lt;p&gt;We learned that objects are known as types, products and coproducts are, respectively, objects/tuple types and sum types. And, in the last chapter, we learned that functors are known as generic types. Now it‚Äôs the time to learn what natural transformations are in this context. They are known as (parametrically) polymorphic functions.&lt;/p&gt;&lt;p&gt;Now, suppose this sounds a bit vague. If only we had some example of a natural transformation in programming, that we can use‚Ä¶ But wait, we did show a natural transformation in the previous chapter, when we talked about pointed functors.&lt;/p&gt;&lt;p&gt;That‚Äôs right, a functor is pointed when there is a natural transformation between it and the identity functor i.e. to have one green arrow for every object/type.&lt;/p&gt;&lt;p&gt;And this clearly is a natural transformation. As a matter of fact, if we get down to the nitty-gritty, we would see that it resembles a lot the equivalence diagram that we saw earlier ‚Äî both transformations involve the identity functor, and both transformations have the same category as source and target, that‚Äôs why we can put everything in one circle (we don‚Äôt do that in the equivalence diagram, but that‚Äôs just a matter of presentation).&lt;/p&gt;&lt;p&gt;Actually, the only difference between the two transformations is that an equivalence is defined by a natural natural isomorphism of a given functors to the identity functor ( $ID \cong f \circ g $ and $ID \cong g \circ f$), while a pointed functor is defined by a one-way natural transformation from the identity functor ($ID \to f $) i.e. the equivalence functor is pointed, but not the other way around).&lt;/p&gt;&lt;p&gt;We said that a natural transformation is equivalent to a (parametrically) polymorphic function in programming. But wait, wasn‚Äôt natural transformation something else (and much more complicated):&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Two functors $F$ and $G$ that have the same type signature (so $F : C \to D$ and $G : C \to D$ for some categories $C$ and $D$), and a family of morphisms in the target category $D$ (denoted $\alpha : F \Rightarrow G$) one for each object in $C$. Morphisms that map each object of the target of $F$ (or the image of $F$ in $D$ as it is also called) to some object in the target of $G$.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Indeed it is (I wasn‚Äôt lying to you, in case you are wondering), however, in the case of programming, the source and target categories of both functors are the same category ($Set$), so the whole condition regarding the functors‚Äô type signatures can be dropped.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Two&lt;/p&gt;&lt;del&gt;functors&lt;/del&gt;generic types $F$ and $G$&lt;del&gt;that have the same type signature&lt;/del&gt;and a family of morphisms in $Set$ (denoted $\alpha : Set \Rightarrow Set$) one for each object in $Set$, that map each target object of the functor $F$ (or the image of $F$ in $D$ as it is also called) to some target objects of functor $G$.&lt;/quote&gt;&lt;p&gt;As we know from the last chapter, a functor in programming is a generic type (which, has to have the &lt;code&gt;map&lt;/code&gt; function with the appropriate signature).&lt;/p&gt;&lt;p&gt;And what is a ‚Äúfamily of morphisms in $Set$ one for each object in $Set$‚Äù? Well, the morphisms in the category $Set$ are functions, so that‚Äôs just a bunch of functions, one for each type. In Haskell notation, if we denote a random type by the letter \(a\)), it is $alpha : \forall a. F a \to G a$. But that‚Äôs exactly what polymorphic functions are.&lt;/p&gt;&lt;p&gt;Here is how would we write the above definition in a more traditional language (we use capital &lt;code&gt;&amp;lt;A&amp;gt;&lt;/code&gt; instead of $a$, as customary.&lt;/p&gt;&lt;code&gt;
function alpha&amp;lt;A&amp;gt;(a: F&amp;lt;A&amp;gt;) : G&amp;lt;A&amp;gt; {
}

&lt;/code&gt;&lt;p&gt;Generic types work by replacing the &lt;code&gt;&amp;lt;A&amp;gt;&lt;/code&gt; with some concrete type, like &lt;code&gt;string&lt;/code&gt;, &lt;code&gt;int&lt;/code&gt; etc. Specifically, the natural transformation from the identity functor to the list functor that puts each value in a singleton list looks like this $alpha :: \forall\ a. a \to List\ a$. Or in TypeScript:&lt;/p&gt;&lt;code&gt;
function array&amp;lt;A&amp;gt;(a: A) : Array&amp;lt;A&amp;gt; {
    return [a]
}
&lt;/code&gt;&lt;p&gt;Once we rid ourselves of the feeling of confusion, that such an excessive amount of new terminology and concepts impose upon us (which can take years, by the way), we realize that there are, of course, many polymorphic functions/natural transformations that programmers use.&lt;/p&gt;&lt;p&gt;For example, in the previous chapter, we discussed one natural transformation/polymorphic function the function $\forall a.a \to [a]$ which puts every value in a singleton list. This function is a natural transformation between the identity functor and the list functor.&lt;/p&gt;&lt;p&gt;This is pretty much the only one that is useful with this signature (the others being $a \to [a, a]$, $a \to [a, a, a]$ etc.), but there are many examples with signature $list\ a \to list\ a$, such as the function to reverse a list.&lt;/p&gt;&lt;p&gt;‚Ä¶or take1 that retrieves the first element of a list&lt;/p&gt;&lt;p&gt;or flatten a list of lists of things to a regular list of things (the signature of this one is a little different, it‚Äôs $list\ list\ a \to list\ a$).&lt;/p&gt;&lt;p&gt;Task 3: Draw example naturality squares of the $reverse$ natural transformation.&lt;/p&gt;&lt;p&gt;Do the same for the rest of the transformations.&lt;/p&gt;&lt;p&gt;Before, we said that we shouldn‚Äôt worry too much about naturality, as it is satisfied every time. Statistically, however, this is not true ‚Äî as far as I am concerned, about 99.999 percent of transformations aren‚Äôt really natural (I wonder if you can compute that percentage properly?). But at the same time, it just so happens (my favourite phrase when writing about maths) that all transformations that we care about are natural.&lt;/p&gt;&lt;p&gt;So, what does the naturality condition entail, in programming? To understand this, we construct some naturality squares of the transformations that we presented.&lt;/p&gt;&lt;p&gt;We choose two types that play the role of $a$, in our case $string$ and $num$ and one natural transformation, like the transformation between the identity functor and the list functor.&lt;/p&gt;&lt;p&gt;The diagram commute when for all functions $f$, applying the $Ff$, the mapped/lifted version of $f$ with one functor (in our case this is just $F f : string \to num$ cause it is the identity functor), followed by ($alpha :: F b \to G\ b$), is equivalent to applying ($alpha:: F a \to G\ a$), and then the mapped version of $f$ with the other functor (in our case $G f :: List\ a \to List\ b$) i.e.&lt;/p&gt;\[\alpha \circ F\ f \cong G\ f \circ \alpha\]&lt;p&gt;(in the programming world, you would also see it as something like $\alpha (map\ f x) = map\ f (\alpha x)$, but note that here $map$ function means two different things on the two sides, Haskell is just smart enough to deduce which $fmap$ to use).&lt;/p&gt;&lt;p&gt;And in TypeScript, when we are talking specifically about the identity functor and the list functor, the equality is expressed as:&lt;/p&gt;&lt;code&gt;[x].map(f) == [f(x)]
&lt;/code&gt;&lt;p&gt;So, is this equation true in our case? To verify it, we take one last peak at the world of values.&lt;/p&gt;&lt;p&gt;We acquire an $f$, that is, we a function that acts on simple values (not lists), such as the function $length : string \to num$, which returns the number of characters a string has and convert it, (or lift it, as the terminology goes) to a function that acts on more complex values, using the list functor, (and the higher-order function $map$).&lt;/p&gt;&lt;p&gt;Then, we take the input and output types for this function (in this case $string$ and $num$), and the two morphisms of a natural transformation (e.g the abstract function $\forall a.a \to [a]$) that correspond to those two types.&lt;/p&gt;&lt;p&gt;When we compose these two pairs of morphisms we observe that they indeed commute ‚Äî we get two morphisms that are actually one and the same function.&lt;/p&gt;&lt;p&gt;The above square shows the transformation $\forall a.a \to [a]$ (which is between the identity functor and the list functor, here is another one, this time between the list functor and itself ($\forall a.[a] \to [a]$) ‚Äî $reverse$&lt;/p&gt;&lt;p&gt;(and you can see that this would work not just for $length$, but for any other function).&lt;/p&gt;&lt;p&gt;So, why does this happen? Why do these particular transformations make up a commuting square for each and every morphism?&lt;/p&gt;&lt;p&gt;The answer is simple, at least in our specific case: the original, unlifted function $f :: a \to b$ (like our $length :: string \to num$) can only work on the individual values (not with structure), while the natural transformation functions, i.e. ones with signature $list :: a \to list\ a$ only alter the structure, and not individual values. The naturality condition just says that these two types of functions can be applied in any order that we please, without changing the end result.&lt;/p&gt;&lt;p&gt;This means that if you have a sequence of natural transformations that you want to apply, (such as $reverse$ , $take$, $flatten$ etc) and some lifted functions ($F f$, $F g$), you can mix and match between the two sequences in any way you like and you will get the same result e.g.&lt;/p&gt;\[take1 \circ reverse \circ F\ f \circ F\ g\]&lt;p&gt;is the same as&lt;/p&gt;\[take1 \circ F\ f \circ reverse \circ F\ g\]&lt;p&gt;‚Ä¶or‚Ä¶&lt;/p&gt;\[F\ f \circ F\ g \circ take1 \circ reverse\]&lt;p&gt;‚Ä¶or any other such sequence (the only thing that isn‚Äôt permitted is to flip the members of the two sequences ‚Äî ($take1 \circ reverse$ is of course different from $reverse \circ take1$and if you have $F\ f \circ F\ g$, then $F\ g \circ F\ f$ won‚Äôt be permitted at all due to the different type signatures).&lt;/p&gt;&lt;p&gt;Task 4: Prove the above results, using the formula of the naturality condition.&lt;/p&gt;&lt;p&gt;‚ÄúUnnatural‚Äù, or ‚Äúnon-natural‚Äù transformations (let‚Äôs call them just transformations) are mentioned so rarely, that we might be inclined to ask if they exist. The answer is ‚Äúyes and no‚Äù. Why yes? On one hand, transformations, consist of an innumerable amount of morphisms, forming an ever more innumerable amount of squares and obviously nothing stops some of these squares to be non-commuting.&lt;/p&gt;&lt;p&gt;For example, if we substitute one morphism from the family of morphisms that make up the natural transformation with some other random morphism that has the same signature, all squares that have this morphism as a component would stop commuting.&lt;/p&gt;&lt;p&gt;This would result in something like an ‚Äúalmost-natural‚Äù transformation (e.g. an abstract function that reverses all lists, except lists of integers).&lt;/p&gt;&lt;p&gt;And in the category of sets, where morphisms are functions i.e. mappings between values, it is enough to move just one arrow of just one of those values in order to make the transformation ‚Äúunnatural‚Äù (e.g. a function which reverses all lists, but one specific list).&lt;/p&gt;&lt;p&gt;Finally, if can just gather a bunch of random morphisms, one for each object, that fit the criteria, we get what I would call a ‚Äúperfectly unnatural transformation‚Äù (but this is my terminology).&lt;/p&gt;&lt;p&gt;But, although they do exist, it is very hard to define non-natural transformations. For example, for categories that are infinite, there is no way to specify such ‚Äúperfectly unnatural transformation‚Äù (ones where none of the squares commute) without resorting to randomness. And even transformations on finite categories, or the ‚Äúsemi-natural‚Äù transformations which we described above (the ones that include a single condition for a single value or type), are not possible to specify in some languages e.g. you can define such a transformation in Typescript, but not in Haskell.&lt;/p&gt;&lt;p&gt;To see why, let‚Äôs see what the type of a natural transformation is.&lt;/p&gt;\[\forall\ a.\ F a \to G a\]&lt;p&gt;The key is that the definition should be valid for all types a. For this reason, there is no way for us to specify a different arrows for different types, without resorting to type downcasting, which is not permitted in languages like Haskell (as it breaks the principle of parametricity).&lt;/p&gt;&lt;p&gt;Now, after we saw the definition of natural transformations, it is time to see the definition of natural transformations (and if you feel that the quality of the humour in this book is deteriorating, that‚Äôs only because things are getting serious).&lt;/p&gt;&lt;p&gt;Let‚Äôs review again the commuting diagram that represents a natural transformation.&lt;/p&gt;&lt;p&gt;This diagram might prompt us into viewing natural transformations as some kind of ‚Äútwo-arrow functors‚Äù that have not one but two arrows coming from each of their morphisms ‚Äî this notion, can be formalized, by using product categories.&lt;/p&gt;&lt;p&gt;Oh wait, I just realized we never covered product categories‚Ä¶ but don‚Äôt worry, we will cover them now.&lt;/p&gt;&lt;p&gt;We haven‚Äôt covered product categories, however some pages ago, when we covered monoids and groups, we talked about the concept of a product group. The good news is that product categories are a generalization of product groups‚Ä¶&lt;/p&gt;&lt;p&gt;The bad news is that you probably don‚Äôt remember much about product groups, as covered them briefly.&lt;/p&gt;&lt;p&gt;But don‚Äôt worry, we will do a more in-depth treatment now:&lt;/p&gt;&lt;p&gt;Given two groups $G$ and $H$, whose sets of elements can also be denoted $G$ and $H$‚Ä¶&lt;/p&gt;&lt;p&gt;(in this example we use two boolean groups, which we visualize as the groups of horizontal and vertical rotation of a square)&lt;/p&gt;&lt;p&gt;‚Ä¶the product group of these two groups is a group that has the cartesian product of these two sets $G \times H$ as its set of elements.&lt;/p&gt;&lt;p&gt;And what can the group operation of such a group be? Well, I would say that out of the few possible groups operations for this set that exist, this is the only operation that is natural (I didn‚Äôt intend to involve natural transformation at this section, but they really do appear everywhere). So, let‚Äôs try to derive the operation of this group.&lt;/p&gt;&lt;p&gt;We know what a group operation is, in principle: A group operation combines two elements from the group into a third element i.e. it is a function with the following type signature:&lt;/p&gt;\[\circ : (A, A) \to A\]&lt;p&gt;or equivalently&lt;/p&gt;\[\circ : A \to A \to A\]&lt;p&gt;And for product groups, we said that the underlying set of the group (which we dubbed $A$ above) is a cartesian product of some other two sets which we dubbed $G$ and $H$. So, when we swap $A$ for $G \times H$ the definition becomes:&lt;/p&gt;\[\circ : G \times H \to G \times H \to G \times H\]&lt;p&gt;i.e. the group operation takes one pair of elements from $G$ and $H$ and another pair of elements from $G$ and $H$, only to return ‚Äî guess what ‚Äî a pair of elements $G$ and $H$.&lt;/p&gt;&lt;p&gt;Let‚Äôs take an example. To avoid confusion, we take two totally different groups ‚Äî the color-mixing group and the group of integers under addition. That would mean that a value of $G \times H$ would be a pair, containing a random color and a random number, and the operation would combine two combine two such pairs and produce another one.&lt;/p&gt;&lt;p&gt;Now, the operation must produce a pair, containing a number and a color. Furthermore, it would be good if it produces a number by using those two numbers, not just picking one at random, and likewise for colors. And furthermore, we want it to work not just for monoids of numbers and colors, but all other monoids that can be given to us. It is obvious that there is only one solution, to get the elements of the new pair by combining the elements of the pairs given.&lt;/p&gt;&lt;p&gt;And the operation of the product group of the two boolean groups which we presented earlier is the combination of the two operations&lt;/p&gt;&lt;p&gt;So, the general definition of the operation is the following ($g1$, $g2$ are elements of $G$ and $h1$ and $h2$ elements of $H$).&lt;/p&gt;\[(g1, h1) \circ (g2, h2) = ( (g1 \circ g2), (h1 \circ h2))\]&lt;p&gt;And that are product groups.&lt;/p&gt;&lt;p&gt;We are back at tackling product categories.&lt;/p&gt;&lt;p&gt;Since we know what product groups are, and we know that groups are nothing but categories with just one object (and the group objects are the category‚Äôs morphisms, remember?), we are already almost there.&lt;/p&gt;&lt;p&gt;Here is a way to make a product category.&lt;/p&gt;&lt;p&gt;Take any two categories:&lt;/p&gt;&lt;p&gt;Then take the set of all possible pairs of the objects of these categories.&lt;/p&gt;&lt;p&gt;And, finally, we make a category out of that set by taking all morphisms coming from any of the two categories and replicate them to all pairs that feature some objects from their type signature, in the same way as we did for product groups (in this example, only one of the categories has morphisms).&lt;/p&gt;&lt;p&gt;This is the product category of the two categories.&lt;/p&gt;&lt;p&gt;In this section we are interested with the products of one particular category, namely the category we called $2$, containing two objects and one morphism (stylishly represented in black and white).&lt;/p&gt;&lt;p&gt;This category is the key to constructing a functor that is equivalent to a natural transformation:&lt;/p&gt;&lt;p&gt;So, given a product category of $2$ and some other category $C$‚Ä¶&lt;/p&gt;&lt;p&gt;‚Ä¶there exist a natural transformation between $C$ and the product category $2\times C$.&lt;/p&gt;&lt;p&gt;Furthermore, this connection is two-way: any natural transformation from $C$ to some other category (call it $D$, as it is customary) can be represented as a functor $2 \times C \to D$.&lt;/p&gt;&lt;p&gt;That is, if we have a natural transformations $\alpha : F \Rightarrow G$ (where $F: C \to D$ and $G: C \to D$), then, we also have a functor $2 \times C \to D$, such that if we take the subcategory of $2 \times C$ comprised of just those objects that have the $0$ object as part of the pair, and the morphisms between them, we get a functor that is equivalent to $F$, and if we consider the subcategory that contains $1$, then the functor is equivalent to $G$ (we write $\alpha(-,0)=F$ and $\alpha(-,1)=G$). Et voil√†!&lt;/p&gt;&lt;p&gt;Task 5: Show that the two definitions are equivalent.&lt;/p&gt;&lt;p&gt;This perspective helps us realize that a natural transformation can be viewed as a collection of commuting squares. The source functor defines the left-hand side of each square, the target functor ‚Äî the right-hand side, and the transformation morphisms join these two sides.&lt;/p&gt;&lt;p&gt;We can even retrieve the structure of the source category of these functors, which (as categories are by definition structure and nothing more) is equivalent to retrieving the category itself.&lt;/p&gt;&lt;p&gt;Natural transformations are surely a different beast than normal morphisms and functors and so they don‚Äôt compose in the same way. However, they do compose and here we will show how.&lt;/p&gt;&lt;p&gt;Let‚Äôs first get one trivial definition out of the way: for each functor, we have the identity natural transformation (actually a natural isomorphism) between it and itself.&lt;/p&gt;&lt;p&gt;The setup for composing natural transformations may look complicated the first time you see it: we need three categories $C$, $D$ and $E$ (just as composition of morphisms requires three objects). We need a total of four functors, distributed on two pairs, one pair of functors that goes from $C$ to $D$ and one that goes from $D$ to $E$ (so we can compose these two pairs of functors together, to get a new pair of functors that go $C \to E$). However, we will try to keep it simple and we will treat the natural transformation as a map from a morphism to a commuting square. As we showed above, this mapping already contains the two functors in itself.&lt;/p&gt;&lt;p&gt;So, let‚Äôs say that we have the natural transformation $\alpha$ involving the $C \to D$ functors (which we usually call $F$ and $G$).&lt;/p&gt;&lt;p&gt;So, what will happen if we have one more transformation $\bar\alpha$ involving the functors that go $D \to E$ (which are labelled $F‚Äô$ and $G‚Äô$)? Well, since a natural transformation maps each morphism to a square, and a square contains four morphisms (two projections by the two functors and two components of the transformation), a square would be mapped to four squares.&lt;/p&gt;&lt;p&gt;Let‚Äôs start by drawing two of them for each projection of the morphism in $C$.&lt;/p&gt;&lt;p&gt;We have to have two more squares, corresponding to the two morphisms that are the components of the $\alpha$ natural transformation. However, these morphisms connect the objects that are the target of the two functors, objects that we already have on our diagram, so we just have to draw the connections between them.&lt;/p&gt;&lt;p&gt;The result is an interesting structure which is sometimes visualized as a cube.&lt;/p&gt;&lt;p&gt;More interestingly, when we compose the commuting squares from the sides of the cube horizontally, we see that it contains not one, but two bigger commuting squares (they look like rectangles in this diagram), visualized in grey and red. Both of them connect morphisms $F‚ÄôFf$ and $G‚ÄôGf$.&lt;/p&gt;&lt;p&gt;So, there is a natural transformation between the composite functor $F‚Äô \circ F : C \to E$ and $G‚Äô \circ G : C \to E$ ‚Äî a natural transformation that is usually marked $\bar\alpha \bullet \alpha$ (with a black dot).&lt;/p&gt;&lt;p&gt;Task 6: Show that natural transformations indeed compose i.e. that if you have natural transformations $F‚ÄôFf \Rightarrow F‚ÄôGf$ and $F‚ÄôGf \Rightarrow G‚ÄôGf$ you have $F‚ÄôFf \Rightarrow G‚ÄôGf$.&lt;/p&gt;&lt;p&gt;And an interesting special case of horizontal composition is horizontal composition involving the identity natural transformation: given a natural transformation $\bar\alpha$ involving functors with signature $D \to E$ and some functor with signature $F : C \to D$, we can take $\alpha$ to be the identity natural transformation between functor $F$ and itself and compose it with $\bar\alpha$.&lt;/p&gt;&lt;p&gt;We get a new natural transformation $\bar\alpha \bullet \alpha$, that is practically the same as the one we started with (i.e. the same as $\bar\alpha$) so what‚Äôs the deal? We just found a way to extend natural transformations, using functors: i.e we can use a functor with signature $C \to D$ to extend a $D \to E$ natural transformation and make it $C \to E$.&lt;/p&gt;&lt;p&gt;Task 7: Try to extend the natural transformation in the other direction (by taking $\bar\alpha$ to be identity).&lt;/p&gt;&lt;p&gt;So, this is how you compose natural transformations. It‚Äôs too bad that this is form of composition is different from the standard categorical composition. So, I guess natural transformations do not form a category, like we hoped they would‚Ä¶&lt;/p&gt;&lt;p&gt;Well, OK, there is actually another way of composing categories, which might actually work.&lt;/p&gt;&lt;p&gt;Recall that categorical composition involves three objects and two successive arrows between them. For vertical composition of natural transformations, we will need three (or more) functors with the same type signature, say $F, G, H: C \to D$ i.e. (same source and target category) and two successive natural transformations between those functors i.e. $\alpha: F \to G$ and $\beta: G \to H$.&lt;/p&gt;&lt;p&gt;We can combine each morphism of the natural transformation $\alpha$ (e.g. $a: F \to G$) and the corresponding morphism of the natural transformation $\beta$ (say $b:G \to H$) to get a new morphism, which we call $b \circ a : F \to H$ (the composition operator is the usual white circle, as opposed to the black one, which denotes horizontal composition). And the set of all such morphisms are precisely the components of a new natural transformation: $\beta \circ \alpha : F \to H$.&lt;/p&gt;&lt;p&gt;Now, we are approaching the end of the chapter, we will introduce our category and call it quits. To do that, we first introduce a more compressed notation for vertical composition of natural transformations (where they do indeed look vertical).&lt;/p&gt;&lt;p&gt;We started this chapter by looking at category of sets and using internal diagrams, displaying the set elements as points and the sets/objects as collections.&lt;/p&gt;&lt;p&gt;Task 8: identify the function, the three functors, and the two natural transformations used in this diagram.&lt;/p&gt;&lt;p&gt;Then, we quickly passed to normal external diagrams, where objects are points and categories are collections.&lt;/p&gt;&lt;p&gt;And now we go one more level further, and show the category of categories, where categories are points and functors are morphisms.&lt;/p&gt;&lt;p&gt;In this notation, we display natural transformations as (double) arrows between morphisms.&lt;/p&gt;&lt;p&gt;And you can already see the new category that is formed: For each two categories (like $C$ and $D$ in this case), there exists a category which has functors for objects and natural transformations as morphisms.&lt;/p&gt;&lt;p&gt;Natural transformations compose with vertical compositions, and, of course, the identity natural transformation is the identity morphism.&lt;/p&gt;&lt;p&gt;Vertical and horizontal composition of natural transformations are related to each other in the following way:&lt;/p&gt;&lt;p&gt;If we have (as we had) two successive natural transformations, in the vertical sense, like $\alpha: F \to G$ and $\beta: G \to H$.&lt;/p&gt;&lt;p&gt;And two successive ones, this time in horizontal sense e.g. $\bar\alpha: F‚Äô \to G‚Äô$ and $\bar\beta: G‚Äô \to H‚Äô$. (note that $\alpha$ has nothing to do with $\bar\alpha$ as $\beta$ has nothing to do with $\bar\beta$, we just call them that way to avoid using too many letters)&lt;/p&gt;&lt;p&gt;And if the two pairs of natural transformations both start from the same category and the same functor, then the compositions of the two pairs of natural transformations obey the following law&lt;/p&gt;\[(Œ≤ \circ Œ±) \bullet (\bar Œ≤ \circ \bar Œ±) = (Œ≤ \bullet \bar Œ≤) \circ (Œ± \bullet \bar Œ±)\]&lt;p&gt;Task 9: Draw the paths of the two compositions of the transformations (on the two sides of the equation) and ensure that they indeed lead to the same place.&lt;/p&gt;&lt;p&gt;At this point you might be wondering the following (although statistically you are more likely to wonder what the heck is all this about): We know that all categories are objects of $Cat$, the category of small categories, in which functors play the role of morphisms.&lt;/p&gt;&lt;p&gt;But, functors between given categories also form a category, under vertical composition. Which means that $Cat$ not only has (as any other category) morphisms between objects, but also has morphisms between morphisms. And furthermore, those two types of morphisms compose in this very interesting way.&lt;/p&gt;&lt;p&gt;So, what does that make of $Cat$? I don‚Äôt know, perhaps we can call natural transformations ‚Äú2-morphisms‚Äù and $Cat$ is some kind of ‚Äú2-category‚Äù?&lt;/p&gt;&lt;p&gt;But wait, actually it‚Äôs way too early for you to find out. We haven‚Äôt even covered limits‚Ä¶&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45435422</guid><pubDate>Wed, 01 Oct 2025 08:00:30 +0000</pubDate></item></channel></rss>