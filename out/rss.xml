<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 30 Sep 2025 23:09:02 +0000</lastBuildDate><item><title>Kagi News</title><link>https://blog.kagi.com/kagi-news</link><description>&lt;doc fingerprint="b500b3a787eb238f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing Kagi News&lt;/head&gt;
    &lt;p&gt;A comprehensive daily press review with global news. Fully private, with sources openly curated by our community.&lt;/p&gt;
    &lt;p&gt;News is broken. We all know it, but we’ve somehow accepted it as inevitable. The endless notifications. The clickbait headlines designed to trigger rather than inform, driven by relentless ad monetization. The exhausting cycle of checking multiple apps throughout the day, only to feel more anxious and less informed than when we started. This isn’t what news was supposed to be. We can do better, and create what news should have been all along: pure, essential information that respects your intelligence and time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our approach: Signal over noise&lt;/head&gt;
    &lt;p&gt;Kagi News operates on a simple principle: understanding the world requires hearing from the world. Every day, our system reads thousands of community curated RSS feeds from publications across different viewpoints and perspectives. We then use AI to distill this massive information into one comprehensive daily briefing, while clearly citing sources.&lt;/p&gt;
    &lt;p&gt;We strive for diversity and transparency of resources and welcome your contributions to widen perspectives. This multi-source approach helps reveal the full picture beyond any single viewpoint.&lt;/p&gt;
    &lt;head rend="h2"&gt;Design principles that put readers first&lt;/head&gt;
    &lt;p&gt;One daily update: We publish once per day around noon UTC, creating a natural endpoint to news consumption. This is a deliberate design choice that turns news from an endless habit into a contained ritual.&lt;/p&gt;
    &lt;p&gt;Five-minute complete understanding: Our briefings cover everything important in just five minutes. No endless scrolling. No attention hijacking. You read, understand, and move on with your day.&lt;/p&gt;
    &lt;p&gt;Diversity over echo chambers: Rather than personalizing feeds to match existing preferences, we expose readers to the full spectrum of global perspectives. This approach breaks down information silos instead of reinforcing them.&lt;/p&gt;
    &lt;p&gt;Privacy by design: Your reading habits belong to you. We don’t track, profile, or monetize your attention. You remain the customer and not the product.&lt;/p&gt;
    &lt;p&gt;Community-driven sources: Our news sources are open source and community-curated through our public GitHub repository. Anyone can propose additions, flag problems, or suggest improvements.&lt;/p&gt;
    &lt;p&gt;Customizable: In your settings, you can select and reorder categories to match your interests and priorities. You can also adjust the number of stories shown, as well as dragging to re-order various sections, so that your briefing is focused on the depth and topics that matter most to you.&lt;/p&gt;
    &lt;p&gt;News in your language: You can choose your preferred interface and content language. News stories are generated in their original source language, and then translated using Kagi Translate. The default mode shows regional stories in their original language without translation, and all other ones in your browser’s language.&lt;/p&gt;
    &lt;head rend="h2"&gt;Technical implementation that respects publishers&lt;/head&gt;
    &lt;p&gt;We don’t scrape content from websites. Instead, we use publicly available RSS feeds that publishers choose to provide. Publishers decide what content appears in their feeds; some include full articles, others only titles or summaries. We respect those choices completely. We’re working within the ecosystem publishers have created rather than circumventing their intentions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ready to experience news differently?&lt;/head&gt;
    &lt;p&gt;If you’re tired of news that makes you feel worse about the world while teaching you less about it, we invite you to try a different approach with Kagi News, so download it today:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45426490</guid><pubDate>Tue, 30 Sep 2025 15:09:00 +0000</pubDate></item><item><title>Designing agentic loops</title><link>https://simonwillison.net/2025/Sep/30/designing-agentic-loops/</link><description>&lt;doc fingerprint="b540585d0af512bf"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Designing agentic loops&lt;/head&gt;
    &lt;p&gt;30th September 2025&lt;/p&gt;
    &lt;p&gt;Coding agents like Anthropic’s Claude Code and OpenAI’s Codex CLI represent a genuine step change in how useful LLMs can be for producing working code. These agents can now directly exercise the code they are writing, correct errors, dig through existing implementation details, and even run experiments to find effective code solutions to problems.&lt;/p&gt;
    &lt;p&gt;As is so often the case with modern AI, there is a great deal of depth involved in unlocking the full potential of these new tools.&lt;/p&gt;
    &lt;p&gt;A critical new skill to develop is designing agentic loops.&lt;/p&gt;
    &lt;p&gt;One way to think about coding agents is that they are brute force tools for finding solutions to coding problems. If you can reduce your problem to a clear goal and a set of tools that can iterate towards that goal a coding agent can often brute force its way to an effective solution.&lt;/p&gt;
    &lt;p&gt;My preferred definition of an LLM agent is something that runs tools in a loop to achieve a goal. The art of using them well is to carefully design the tools and loop for them to use.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The joy of YOLO mode&lt;/item&gt;
      &lt;item&gt;Picking the right tools for the loop&lt;/item&gt;
      &lt;item&gt;Issuing tightly scoped credentials&lt;/item&gt;
      &lt;item&gt;When to design an agentic loop&lt;/item&gt;
      &lt;item&gt;This is still a very fresh area&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The joy of YOLO mode&lt;/head&gt;
    &lt;p&gt;Agents are inherently dangerous—they can make poor decisions or fall victim to malicious prompt injection attacks, either of which can result in harmful results from tool calls. Since the most powerful coding agent tool is “run this command in the shell” a rogue agent can do anything that you could do by running a command yourself.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;An AI agent is an LLM wrecking its environment in a loop.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Coding agents like Claude Code counter this by defaulting to asking you for approval of almost every command that they run.&lt;/p&gt;
    &lt;p&gt;This is kind of tedious, but more importantly, it dramatically reduces their effectiveness at solving problems through brute force.&lt;/p&gt;
    &lt;p&gt;Each of these tools provides its own version of what I like to call YOLO mode, where everything gets approved by default.&lt;/p&gt;
    &lt;p&gt;This is so dangerous, but it’s also key to getting the most productive results!&lt;/p&gt;
    &lt;p&gt;Here are three key risks to consider from unattended YOLO mode.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Bad shell commands deleting or mangling things you care about.&lt;/item&gt;
      &lt;item&gt;Exfiltration attacks where something steals files or data visible to the agent—source code or secrets held in environment variables are particularly vulnerable here.&lt;/item&gt;
      &lt;item&gt;Attacks that use your machine as a proxy to attack another target—for DDoS or to disguise the source of other hacking attacks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you want to run YOLO mode anyway, you have a few options:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Run your agent in a secure sandbox that restricts the files and secrets it can access and the network connections it can make.&lt;/item&gt;
      &lt;item&gt;Use someone else’s computer. That way if your agent goes rogue, there’s only so much damage they can do, including wasting someone else’s CPU cycles.&lt;/item&gt;
      &lt;item&gt;Take a risk! Try to avoid exposing it to potential sources of malicious instructions and hope you catch any mistakes before they cause any damage.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Most people choose option 3.&lt;/p&gt;
    &lt;p&gt;Despite the existence of container escapes I think option 1 using Docker or the new Apple container tool is a reasonable risk to accept for most people.&lt;/p&gt;
    &lt;p&gt;Option 2 is my favorite. I like to use GitHub Codespaces for this—it provides a full container environment on-demand that’s accessible through your browser and has a generous free tier too. If anything goes wrong it’s a Microsoft Azure machine somewhere that’s burning CPU and the worst that can happen is code you checked out into the environment might be exfiltrated by an attacker, or bad code might be pushed to the attached GitHub repository.&lt;/p&gt;
    &lt;p&gt;There are plenty of other agent-like tools that run code on other people’s computers. Code Interpreter mode in both ChatGPT and Claude can go a surprisingly long way here. I’ve also had a lot of success (ab)using OpenAI’s Codex Cloud.&lt;/p&gt;
    &lt;p&gt;Coding agents themselves implement various levels of sandboxing, but so far I’ve not seen convincing enough documentation of these to trust them.&lt;/p&gt;
    &lt;p&gt;Update: It turns out Anthropic have their own documentation on Safe YOLO mode for Claude Code which says:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Letting Claude run arbitrary commands is risky and can result in data loss, system corruption, or even data exfiltration (e.g., via prompt injection attacks). To minimize these risks, use&lt;/p&gt;&lt;code&gt;--dangerously-skip-permissions&lt;/code&gt;in a container without internet access. You can follow this reference implementation using Docker Dev Containers.&lt;/quote&gt;
    &lt;p&gt;Locking internet access down to a list of trusted hosts is a great way to prevent exfiltration attacks from stealing your private source code.&lt;/p&gt;
    &lt;head rend="h4"&gt;Picking the right tools for the loop&lt;/head&gt;
    &lt;p&gt;Now that we’ve found a safe (enough) way to run in YOLO mode, the next step is to decide which tools we need to make available to the coding agent.&lt;/p&gt;
    &lt;p&gt;You can bring MCP into the mix at this point, but I find it’s usually more productive to think in terms of shell commands instead. Coding agents are really good at running shell commands!&lt;/p&gt;
    &lt;p&gt;If your environment allows them the necessary network access, they can also pull down additional packages from NPM and PyPI and similar. Ensuring your agent runs in an environment where random package installs don’t break things on your main computer is an important consideration as well!&lt;/p&gt;
    &lt;p&gt;Rather than leaning on MCP, I like to create an AGENTS.md (or equivalent) file with details of packages I think they may need to use.&lt;/p&gt;
    &lt;p&gt;For a project that involved taking screenshots of various websites I installed my own shot-scraper CLI tool and dropped the following in &lt;code&gt;AGENTS.md&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;To take a screenshot, run:

shot-scraper http://www.example.com/ -w 800 -o example.jpg
&lt;/code&gt;
    &lt;p&gt;Just that one example is enough for the agent to guess how to swap out the URL and filename for other screenshots.&lt;/p&gt;
    &lt;p&gt;Good LLMs already know how to use a bewildering array of existing tools. If you say "use playwright python" or "use ffmpeg" most models will use those effectively—and since they’re running in a loop they can usually recover from mistakes they make at first and figure out the right incantations without extra guidance.&lt;/p&gt;
    &lt;head rend="h4"&gt;Issuing tightly scoped credentials&lt;/head&gt;
    &lt;p&gt;In addition to exposing the right commands, we also need to consider what credentials we should expose to those commands.&lt;/p&gt;
    &lt;p&gt;Ideally we wouldn’t need any credentials at all—plenty of work can be done without signing into anything or providing an API key—but certain problems will require authenticated access.&lt;/p&gt;
    &lt;p&gt;This is a deep topic in itself, but I have two key recommendations here:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Try to provide credentials to test or staging environments where any damage can be well contained.&lt;/item&gt;
      &lt;item&gt;If a credential can spend money, set a tight budget limit.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I’ll use an example to illustrate. A while ago I was investigating slow cold start times for a scale-to-zero application I was running on Fly.io.&lt;/p&gt;
    &lt;p&gt;I realized I could work a lot faster if I gave Claude Code the ability to directly edit Dockerfiles, deploy them to a Fly account and measure how long they took to launch.&lt;/p&gt;
    &lt;p&gt;Fly allows you to create organizations, and you can set a budget limit for those organizations and issue a Fly API key that can only create or modify apps within that organization...&lt;/p&gt;
    &lt;p&gt;So I created a dedicated organization for just this one investigation, set a $5 budget, issued an API key and set Claude Code loose on it!&lt;/p&gt;
    &lt;p&gt;In that particular case the results weren’t useful enough to describe in more detail, but this was the project where I first realized that “designing an agentic loop” was an important skill to develop.&lt;/p&gt;
    &lt;head rend="h4"&gt;When to design an agentic loop&lt;/head&gt;
    &lt;p&gt;Not every problem responds well to this pattern of working. The thing to look out for here are problems with clear success criteria where finding a good solution is likely to involve (potentially slightly tedious) trial and error.&lt;/p&gt;
    &lt;p&gt;Any time you find yourself thinking “ugh, I’m going to have to try a lot of variations here” is a strong signal that an agentic loop might be worth trying!&lt;/p&gt;
    &lt;p&gt;A few examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Debugging: a test is failing and you need to investigate the root cause. Coding agents that can already run your tests can likely do this without any extra setup.&lt;/item&gt;
      &lt;item&gt;Performance optimization: this SQL query is too slow, would adding an index help? Have your agent benchmark the query and then add and drop indexes (in an isolated development environment!) to measure their impact.&lt;/item&gt;
      &lt;item&gt;Upgrading dependencies: you’ve fallen behind on a bunch of dependency upgrades? If your test suite is solid an agentic loop can upgrade them all for you and make any minor updates needed to reflect breaking changes. Make sure a copy of the relevant release notes is available, or that the agent knows where to find them itself.&lt;/item&gt;
      &lt;item&gt;Optimizing container sizes: Docker container feeling uncomfortably large? Have your agent try different base images and iterate on the Dockerfile to try to shrink it, while keeping the tests passing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A common theme in all of these is automated tests. The value you can get from coding agents and other LLM coding tools is massively amplified by a good, cleanly passing test suite. Thankfully LLMs are great for accelerating the process of putting one of those together, if you don’t have one yet.&lt;/p&gt;
    &lt;head rend="h4"&gt;This is still a very fresh area&lt;/head&gt;
    &lt;p&gt;Designing agentic loops is a very new skill—Claude Code was first released in just February 2025!&lt;/p&gt;
    &lt;p&gt;I’m hoping that giving it a clear name can help us have productive conversations about it. There’s so much more to figure out about how to use these tools as effectively as possible.&lt;/p&gt;
    &lt;head rend="h2"&gt;More recent articles&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude Sonnet 4.5 is probably the "best coding model in the world" (at least for now) - 29th September 2025&lt;/item&gt;
      &lt;item&gt;I think "agent" may finally have a widely enough agreed upon definition to be useful jargon now - 18th September 2025&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45426680</guid><pubDate>Tue, 30 Sep 2025 15:21:23 +0000</pubDate></item><item><title>Visualizations of Random Attractors Found Using Lyapunov Exponents</title><link>https://paulbourke.net/fractals/lyapunov/</link><description>&lt;doc fingerprint="2992588605812842"&gt;
  &lt;main&gt;
    &lt;cell&gt;&lt;head rend="h1"&gt; Random Attractors&lt;lb/&gt; Found using Lyapunov Exponents &lt;/head&gt; Written by Paul Bourke&lt;lb/&gt; October 2001&lt;p&gt; Contribution by Philip Ham: attractor.basic&lt;lb/&gt; and Python implementation by Johan Bichel Lindegaard.&lt;/p&gt;&lt;p&gt; This document is "littered" with a selection of attractors found using the techniques described. &lt;/p&gt;&lt;p&gt; In order for a system to exhibit chaotic behaviour it must be non linear. Representing chaotic systems on a screen or on paper leads one to considering a two dimensional system, an equation in two variables. One possible two dimensional non-linear system, the one used here, is the quadratic map defined as follows. &lt;/p&gt; xn+1 = a0 + a1 xn + a2 xn2 + a3 xn yn + a4 yn + a5 yn2 &lt;lb/&gt; yn+1 = b0 + b1 xn + b2 xn2 + b3 xn yn + b4 yn + b5 yn2 &lt;p&gt; The standard measure for determining whether or not a system is chaotic is the Lyapunov exponent, normally represented by the lambda symbol. Consider two close points at step n, xn and xn+dxn. At the next time step they will have diverged, namely to xn+1 and xn+1+dxn+1. It is this average rate of divergence (or convergence) that the Lyapunov exponent captures. Another way to think about the Lyapunov exponent is as the rate at which information about the initial conditions is lost. &lt;/p&gt;&lt;p&gt; There are as many Lyapunov exponents as dimensions of the phase space. Considering a region (circle, sphere, hypersphere, etc) in phase space then at a later time all trajectories in this region form an n-dimensional elliptical region. The Lyapunov exponent can be calculated for each dimension. When talking about a single exponent one is normally referring to the largest, this convention will be assumed from now onwards. &lt;/p&gt;&lt;p&gt; If the Lyapunov exponent is positive then the system is chaotic and unstable. Nearby points will diverge irrespective of how close they are. Although there is no order the system is still deterministic! The magnitude of the Lyapunov exponent is a measure of the sensitivity to initial conditions, the primary characteristic of a chaotic system. &lt;/p&gt;&lt;p&gt; If the Lyapunov exponent is less than zero then the system attracts to a fixed point or stable periodic orbit. These systems are non conservative (dissipative). The absolute value of the exponent indicates the degree of stability. &lt;/p&gt;&lt;p&gt; If the Lyapunov exponent is zero then the system is neutrally stable, such systems are conservative and in a steady state mode. &lt;/p&gt;&lt;p&gt; To create the chaotic attractors shown on this page each parameter an and bn in the quadratic equation above is chosen at random between some bounds (+- 2 say). The system so specified is generated by iterating for some suitably large number of time steps (eg; 100000) steps during which time the image is created and the Lyapunov exponent computed. Note that the first few (1000) timesteps are ignored to allow the system to settle into its "natural" behaviour. If the Lyapunov exponent indicates chaos then the image is saved and the program moves on to the next random parameter set. &lt;/p&gt;&lt;p&gt; There are a number of ways the series may behave. &lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt; It may converge to a single point, called a fixed point. These can be detected by comparing the distances between successive points. For numerical reasons this is safer than relying on the Lyapunov exponent which may be infinite (logarithm of 0)&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt; It may diverge to infinity, for the range (+- 2) used here for each parameter this is the most likely event. These are also easy to detect and discard, indeed they need to be in order to avoid numerical errors.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt; It will form a periodic orbit, these are identified by their negative Lyapunov exponent. &lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt; It will exhibit chaos, filling in some region of the plane. These are the solutions that "look good" and the ones we wish to identify with the Lyapunov exponent. &lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt; It should be noted that there may be visually appealing structures that are not chaotic attractors. That is, the resulting image is different for different initial conditions and there is no single basin of attraction. It's interesting how we "see" 3 dimensional structures in these essentially 2 dimensional systems. &lt;/p&gt;&lt;p&gt; The software used to create these images is given here: gen.c. On average 98% of the random selections of (an, bn) result in an infinite series. This is so common because of the range (-2 &amp;lt;= a,b &amp;lt;= 2) for each of the parameters a and b, the number of infinite cases will reduce greatly with a smaller range. About 1% were point attractors, and about 0.5% were periodic basins of attraction. &lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt; Image courtesy of Robert McGregor, Space Coast of Florida. Launch trail perhaps 30 minutes after the shuttle launch (June 2007) dispersing from a column into a smoke ring due to some unusual air currents in the upper atmosphere. &lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt; References&lt;/p&gt;&lt;p&gt; Berge, P., Pomeau, Y., Vidal, C.,&lt;lb/&gt; Order Within Chaos, Wiley, New York, 1984. &lt;/p&gt;&lt;p&gt; Crutchfield, J., Farmer, J., Packard, N.&lt;lb/&gt; Chaos, Scientific American, 1986, 255, 46-47 &lt;/p&gt;&lt;p&gt; Das, A., Das, Pritha, Roy, A&lt;lb/&gt; Applicability of Lyapunov Exponent in EEG data analysis. Complexity International, draft manuscript. &lt;/p&gt;&lt;p&gt; Devaney, R.&lt;lb/&gt; An Introduction to Chaotic Dynamical Systems, Addison-Wesley, 1989 &lt;/p&gt;&lt;p&gt; Feigenbaum, M.,&lt;lb/&gt; Universal behaviour in Nonlinear Systems, Los Alamos Science, 1981 &lt;/p&gt;&lt;p&gt; Peitgen, H., Jurgens, H., Saupe, D&lt;lb/&gt; Lyapunov exponents and chaotic attractors in Chaos and fractals - new frontiers of science. Springer, new York, 1992. &lt;/p&gt;&lt;p&gt; Contributions by Dmytry Lavrov &lt;/p&gt;&lt;/cell&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45427059</guid><pubDate>Tue, 30 Sep 2025 15:50:14 +0000</pubDate></item><item><title>Prompt analytics for MCP servers</title><link>https://hyprmcp.com/blog/mcp-server-prompt-analytics/</link><description>&lt;doc fingerprint="b508e4cb3d937e9e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Prompt Analytics for MCP Servers&lt;/head&gt;
    &lt;p&gt;How to intercept the prompt that triggered the MCP Server tool call for MCP prompt analytics.&lt;/p&gt;
    &lt;p&gt;I am Philip, an Engineer at Hypr MCP, where we help companies connect their internal applications to LLM-based workflows with the power of MCP servers. Join our waitlist or book a demo to learn more. Every time we showcase our Hypr MCP platform, this is the most frequently asked question: How did we manage to get the prompt analytics? In this blog post, I want to show you how and why we built prompt analytics into our MCP server Gateway.&lt;/p&gt;
    &lt;head rend="h2"&gt;# Introduction&lt;/head&gt;
    &lt;p&gt;The Model Context Protocol (MCP) allows LLM clients and agents to dynamically add context to the prompt and even perform method calls. Typical use cases for dynamically adding additional context to LLM prompts can be found in the engineering domain. MCP servers like Context7 and GitMCP can provide dynamic documentation based on prompts, while MCP servers from specific software vendors like Stack Auth (https://mcp.stack-auth.com/) can directly add relevant information to the prompts if a tool description matches a promptâs problem. On the other side, MCP servers can be used to let LLMs instruct LLM clients to perform actions on third-party systems like the GitHub or HubSpot MCP server.&lt;/p&gt;
    &lt;head rend="h2"&gt;# MCP Server AnalyticsâMCP Servers Often Run in the Dark&lt;/head&gt;
    &lt;p&gt;Previously, MCP servers mostly ran on the client side with stdio being the default method of how JSON-RPC messages were sent from and to the clients. A benefit for these servers has been simplicity - MCP server developers didnât need to care about the runtime and connectivity constraints as the user needed to make sure to start the server program. With the migration to remote MCP servers, thanks to the streamable HTTP transport method for JSON-RPC messages, new analytics methods become possible.&lt;/p&gt;
    &lt;p&gt;In the next sections, we will focus exclusively on remote MCP servers.&lt;/p&gt;
    &lt;head rend="h3"&gt;# Application Layer Analytics for MCP Servers&lt;/head&gt;
    &lt;p&gt;Application layer analytics means adding a logging or metrics library directly into your MCP serverâs application code. As remote MCP servers follow the same principles as traditional MCP servers, traditional logging or analytics libraries can be used to send events about tool method usage and tool arguments. Getting analytics for system calls like &lt;code&gt;tools/list&lt;/code&gt; or &lt;code&gt;initialize&lt;/code&gt; is not that easy, as these calls are often abstracted by the frameworks.
But especially analyzing these requests will help you improve your MCP server and spot errors where clients might abort the session after the initialize request because authentication might fail.&lt;/p&gt;
    &lt;head rend="h3"&gt;# Gateway-Level Analytics for MCP Servers&lt;/head&gt;
    &lt;p&gt;Similar to how WAFs (Web Application Firewalls) work, MCP servers can be put behind a gateway that is able to unwrap and analyze requests and responses.&lt;/p&gt;
    &lt;p&gt;Tip: MCP Gateways can also be used to add authentication for your MCP server.&lt;/p&gt;
    &lt;p&gt;As MCP supports various transport protocols, traditional gateways are not built to unwrap and analyze MCP Server tool calls. While the client establishes an HTTP connection with the server and sends multiple JSON-RPC requests, it is not possible to perform the analytics on an HTTP level. MCP Gateways need to be able to constantly hold both connections to the client and server, receive and analyze a JSON-RPC request, and then forward it to the second connection.&lt;/p&gt;
    &lt;p&gt;Initially, our Gateway used a basic &lt;code&gt;io.TeeReader&lt;/code&gt; from the Golang standard library to simply fork off the request and response body for further analysis.
However, as you will see in the next section, this approach has its limitations as it does not allow us to modify the response body.
We therefore switched processing of the response body to a custom &lt;code&gt;io.Reader&lt;/code&gt; implementation that parses each SSE event from the upstream body reader, allows for modifications and makes the modified event available downstream with a backing buffer.
This is necessary as we want to handle each event individually, without having to buffer the entire response body.&lt;/p&gt;
    &lt;p&gt;As you can see in the gateway configuration, you are able to configure a webhook for each MCP server. The gateway will forward every JSON-RPC request and its response directly to the webhook endpoint.&lt;/p&gt;
    &lt;head rend="h3"&gt;# Prompt Analytics&lt;/head&gt;
    &lt;p&gt;After successfully analyzing the JSON-RPC requests and responses, you can extract valuable insights about how your MCP server is being used. But the real game-changer is capturing the actual prompts that trigger tool calls.&lt;/p&gt;
    &lt;p&gt;Every time someone sees the HyprMCP Analytics Dashboard for the first time, they immediately ask us how we capture prompt insights.&lt;/p&gt;
    &lt;p&gt;The key insight is that most MCP clients embedded in agentic workflows donât ask for permission every time a tool calling operation gets executed. They simply pass along any parameters defined in the toolâs input schema. By leveraging this behavior, the HyprMCP Gateway dynamically injects additional optional analytics parameters into tool schemas. When the gateway intercepts &lt;code&gt;tools/list&lt;/code&gt; responses, it enriches each toolâs input schema with special analytics fields that LLM clients automatically populate with the current prompt and conversation history.&lt;/p&gt;
    &lt;head rend="h4"&gt;# MCP Prompt Analytics Flow&lt;/head&gt;
    &lt;head rend="h4"&gt;# MCP Prompt Analytics Requests and Responses&lt;/head&gt;
    &lt;p&gt;The gateway enriches standard MCP protocol messages with analytics metadata, capturing prompt information and usage patterns while maintaining compatibility with existing MCP servers and clients. Hereâs how the magic happens:&lt;/p&gt;
    &lt;head rend="h5"&gt;# Step 1: Gateway Enriches Tool Schemas&lt;/head&gt;
    &lt;p&gt;When the gateway intercepts a &lt;code&gt;tools/list&lt;/code&gt; response from your MCP server, it dynamically injects two special analytics fields into each toolâs input schema:&lt;/p&gt;
    &lt;p&gt;Modified &lt;code&gt;tools/list&lt;/code&gt; response sent to client:&lt;/p&gt;
    &lt;head rend="h5"&gt;# Step 2: Client Automatically Populates Analytics Fields&lt;/head&gt;
    &lt;p&gt;When the LLM client calls a tool, it automatically fills in these analytics fields with the current prompt and conversation history:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;tools/call&lt;/code&gt; request from client:&lt;/p&gt;
    &lt;head rend="h5"&gt;# Step 3: Gateway Processes and Strips Analytics&lt;/head&gt;
    &lt;p&gt;The gateway extracts the analytics data, sends it to your webhook endpoint, then strips these fields before forwarding the request to your MCP server. Your server receives the original, unmodified request without any awareness of the analytics layer.&lt;/p&gt;
    &lt;head rend="h2"&gt;# Conclusion&lt;/head&gt;
    &lt;p&gt;Prompt analytics transforms MCP servers from black boxes into transparent, observable systems. By understanding which prompts trigger your tools and how users interact with them, you can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Improve tool descriptions to better match user intent&lt;/item&gt;
      &lt;item&gt;Identify usage patterns and optimize frequently-used workflows&lt;/item&gt;
      &lt;item&gt;Debug issues by seeing the exact context that led to errors&lt;/item&gt;
      &lt;item&gt;Measure adoption and understand which features provide the most value&lt;/item&gt;
      &lt;item&gt;Enhance security by monitoring for unexpected or malicious prompts&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The HyprMCP Gatewayâs approach of dynamically injecting analytics fields is both elegant and non-invasive. Your MCP servers remain unchanged while you gain complete visibility into their usage. This same technique can be applied to capture other metadata like user IDs, session information, or custom analytics fields specific to your use case.&lt;/p&gt;
    &lt;p&gt;If youâre ready to bring observability to your MCP servers, check out our open-source MCP Gateway, which implements everything discussed in this post, plus authentication, rate limiting, and more enterprise-ready features.&lt;/p&gt;
    &lt;p&gt;Want to see it in action? Join our waitlist or book a demo to learn how HyprMCP can help you deploy and manage MCP servers at scale.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45427061</guid><pubDate>Tue, 30 Sep 2025 15:50:22 +0000</pubDate></item><item><title>Leaked Apple M5 9 core Geekbench scores</title><link>https://browser.geekbench.com/v6/cpu/14173685</link><description>&lt;doc fingerprint="46a0627d6e0cad81"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Upload Date&lt;/cell&gt;
        &lt;cell&gt;September 30 2025 12:36 PM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Views&lt;/cell&gt;
        &lt;cell&gt;13138&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;System Information&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Operating System&lt;/cell&gt;
        &lt;cell&gt;iOS 26.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Model&lt;/cell&gt;
        &lt;cell&gt;iPad17,3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Model ID&lt;/cell&gt;
        &lt;cell&gt;iPad17,3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Motherboard&lt;/cell&gt;
        &lt;cell&gt;J820AP&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;CPU Information&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Name&lt;/cell&gt;
        &lt;cell&gt;ARM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Topology&lt;/cell&gt;
        &lt;cell&gt;1 Processor, 9 Cores&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Identifier&lt;/cell&gt;
        &lt;cell&gt;ARM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Base Frequency&lt;/cell&gt;
        &lt;cell&gt;4.42 GHz&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Cluster 1&lt;/cell&gt;
        &lt;cell&gt;3 Cores&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Cluster 2&lt;/cell&gt;
        &lt;cell&gt;6 Cores&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;L1 Instruction Cache&lt;/cell&gt;
        &lt;cell&gt;128 KB x 1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;L1 Data Cache&lt;/cell&gt;
        &lt;cell&gt;64.0 KB x 1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;L2 Cache&lt;/cell&gt;
        &lt;cell&gt;6.00 MB x 1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Instruction Sets&lt;/cell&gt;
        &lt;cell&gt;neon aes sha1 sha2 neon-fp16 neon-dotprod i8mm sme-i8i32 sme-f32f32 sme2&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Memory Information&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Size&lt;/cell&gt;
        &lt;cell&gt;11.20 GB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Single-Core Score&lt;/cell&gt;
        &lt;cell role="head"&gt;4133&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; File Compression &lt;/cell&gt;
        &lt;cell&gt; 3552 &lt;p&gt;510.1 MB/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Navigation &lt;/cell&gt;
        &lt;cell&gt; 3659 &lt;p&gt;22.0 routes/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; HTML5 Browser &lt;/cell&gt;
        &lt;cell&gt; 4260 &lt;p&gt;87.2 pages/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; PDF Renderer &lt;/cell&gt;
        &lt;cell&gt; 3734 &lt;p&gt;86.1 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Photo Library &lt;/cell&gt;
        &lt;cell&gt; 3719 &lt;p&gt;50.5 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Clang &lt;/cell&gt;
        &lt;cell&gt; 4649 &lt;p&gt;22.9 Klines/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Text Processing &lt;/cell&gt;
        &lt;cell&gt; 3822 &lt;p&gt;306.1 pages/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Asset Compression &lt;/cell&gt;
        &lt;cell&gt; 3547 &lt;p&gt;109.9 MB/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Object Detection &lt;/cell&gt;
        &lt;cell&gt; 6032 &lt;p&gt;180.5 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Background Blur &lt;/cell&gt;
        &lt;cell&gt; 4104 &lt;p&gt;17.0 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Horizon Detection &lt;/cell&gt;
        &lt;cell&gt; 4139 &lt;p&gt;128.8 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Object Remover &lt;/cell&gt;
        &lt;cell&gt; 5276 &lt;p&gt;405.6 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; HDR &lt;/cell&gt;
        &lt;cell&gt; 4678 &lt;p&gt;137.3 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Photo Filter &lt;/cell&gt;
        &lt;cell&gt; 5061 &lt;p&gt;50.2 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Ray Tracer &lt;/cell&gt;
        &lt;cell&gt; 3302 &lt;p&gt;3.20 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt; Structure from Motion &lt;/cell&gt;
        &lt;cell&gt; 3836 &lt;p&gt;121.4 Kpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Multi-Core Score&lt;/cell&gt;
        &lt;cell role="head"&gt;15437&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; File Compression &lt;/cell&gt;
        &lt;cell&gt; 12308 &lt;p&gt;1.73 GB/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Navigation &lt;/cell&gt;
        &lt;cell&gt; 17065 &lt;p&gt;102.8 routes/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; HTML5 Browser &lt;/cell&gt;
        &lt;cell&gt; 17958 &lt;p&gt;367.6 pages/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; PDF Renderer &lt;/cell&gt;
        &lt;cell&gt; 15774 &lt;p&gt;363.8 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Photo Library &lt;/cell&gt;
        &lt;cell&gt; 18268 &lt;p&gt;247.9 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Clang &lt;/cell&gt;
        &lt;cell&gt; 23236 &lt;p&gt;114.4 Klines/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Text Processing &lt;/cell&gt;
        &lt;cell&gt; 4956 &lt;p&gt;396.9 pages/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Asset Compression &lt;/cell&gt;
        &lt;cell&gt; 18577 &lt;p&gt;575.6 MB/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Object Detection &lt;/cell&gt;
        &lt;cell&gt; 14896 &lt;p&gt;445.7 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Background Blur &lt;/cell&gt;
        &lt;cell&gt; 13114 &lt;p&gt;54.3 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Horizon Detection &lt;/cell&gt;
        &lt;cell&gt; 19111 &lt;p&gt;594.7 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Object Remover &lt;/cell&gt;
        &lt;cell&gt; 15968 &lt;p&gt;1.23 Gpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; HDR &lt;/cell&gt;
        &lt;cell&gt; 18909 &lt;p&gt;554.9 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Photo Filter &lt;/cell&gt;
        &lt;cell&gt; 15246 &lt;p&gt;151.3 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Ray Tracer &lt;/cell&gt;
        &lt;cell&gt; 18888 &lt;p&gt;18.3 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt; Structure from Motion &lt;/cell&gt;
        &lt;cell&gt; 16186 &lt;p&gt;512.5 Kpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45427197</guid><pubDate>Tue, 30 Sep 2025 16:00:36 +0000</pubDate></item><item><title>Launch HN: Airweave (YC X25) – Let agents search any app</title><link>https://github.com/airweave-ai/airweave</link><description>&lt;doc fingerprint="5a85ff58db83ac82"&gt;
  &lt;main&gt;
    &lt;p&gt;Airweave is a tool that lets agents search any app. It connects to apps, productivity tools, databases, or document stores and transforms their contents into searchable knowledge bases, accessible through a standardized interface for agents.&lt;/p&gt;
    &lt;p&gt;The search interface is exposed via REST API or MCP. When using MCP, Airweave essentially builds a semantically searchable MCP server. The platform handles everything from auth and extraction to embedding and serving.&lt;/p&gt;
    &lt;head rend="h3"&gt;Managed Service: Airweave Cloud&lt;/head&gt;
    &lt;p&gt;Make sure docker and docker-compose are installed, then...&lt;/p&gt;
    &lt;code&gt;# 1. Clone the repository
git clone https://github.com/airweave-ai/airweave.git
cd airweave

# 2. Build and run
chmod +x start.sh
./start.sh&lt;/code&gt;
    &lt;p&gt;That's it! Access the dashboard at http://localhost:8080&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Access the UI at &lt;code&gt;http://localhost:8080&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Connect sources, configure syncs, and query data&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Swagger docs: &lt;code&gt;http://localhost:8001/docs&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Create connections, trigger syncs, and search data&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install airweave-sdk&lt;/code&gt;
    &lt;code&gt;from airweave import AirweaveSDK

client = AirweaveSDK(
    api_key="YOUR_API_KEY",
    base_url="http://localhost:8001"
)
client.collections.create(
    name="name",
)&lt;/code&gt;
    &lt;code&gt;npm install @airweave/sdk
# or
yarn add @airweave/sdk&lt;/code&gt;
    &lt;code&gt;import { AirweaveSDKClient, AirweaveSDKEnvironment } from "@airweave/sdk";

const client = new AirweaveSDKClient({
    apiKey: "YOUR_API_KEY",
    environment: AirweaveSDKEnvironment.Local
});
await client.collections.create({
    name: "name",
});&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Data synchronization from 25+ sources with minimal config&lt;/item&gt;
      &lt;item&gt;Entity extraction and transformation pipeline&lt;/item&gt;
      &lt;item&gt;Multi-tenant architecture with OAuth2&lt;/item&gt;
      &lt;item&gt;Incremental updates using content hashing&lt;/item&gt;
      &lt;item&gt;Semantic search for agent queries&lt;/item&gt;
      &lt;item&gt;Versioning for data changes&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frontend: React/TypeScript with ShadCN&lt;/item&gt;
      &lt;item&gt;Backend: FastAPI (Python)&lt;/item&gt;
      &lt;item&gt;Databases: PostgreSQL (metadata), Qdrant (vectors)&lt;/item&gt;
      &lt;item&gt;Deployment: Docker Compose (dev), Kubernetes (prod)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We welcome contributions! Please check CONTRIBUTING.md for details.&lt;/p&gt;
    &lt;p&gt;Airweave is released under the MIT license.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Discord - Get help and discuss features&lt;/item&gt;
      &lt;item&gt;GitHub Issues - Report bugs or request features&lt;/item&gt;
      &lt;item&gt;Twitter - Follow for updates&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45427482</guid><pubDate>Tue, 30 Sep 2025 16:21:09 +0000</pubDate></item><item><title>Show HN: Sculptor, the Missing UI for Claude Code</title><link>https://imbue.com/sculptor/</link><description>&lt;doc fingerprint="95f80211a98fb5cb"&gt;
  &lt;main&gt;&lt;p&gt;Start with your ideas and iteratively refine them, just like traditional software engineering. You're the architect while AI agents handle the implementation details.&lt;/p&gt;Get the docs&lt;p&gt;Each Claude works in its own container. You get safe execution and parallel agents without the hassle of git worktrees.&lt;/p&gt;&lt;p&gt;Switch seamlessly between agents with Pairing Mode.&lt;/p&gt;&lt;p&gt;Merge the changes you like and throw out the ones you don't. Sculptor helps you resolve merge conflicts.&lt;/p&gt;&lt;p&gt;I've been moving more and more of my coding off of Cursor and on to Sculptor btw.&lt;/p&gt;&lt;p&gt;The vibes are good, and the experience has been pretty nice.&lt;/p&gt;&lt;p&gt;Sculptor lets me maintain this level of craftiness to software development without losing the edge you get from AI tools.&lt;/p&gt;&lt;p&gt;Wow, this is slick!!&lt;/p&gt;&lt;p&gt;At first I thought, 'why do I need this container?'. But when I realized Sculptor was actually solving the pain of concurrent agents on different branches, it made total sense.&lt;/p&gt;&lt;p&gt;It's like—oh wow I don't have to manage that mess anymore.&lt;/p&gt;&lt;p&gt;The killer feature for me is parallelization. I can kick off multiple tasks at once without spinning up a whole new environment every time. It feels like the tooling is finally here to support the kind of workflows I've always wanted.&lt;/p&gt;&lt;p&gt;I compared Claude Code running in Max Mode with Sculptor, and Sculptor's results and overall intelligence were better. I've already merged around 5K lines—it's a great product! Kudos to you guys.&lt;/p&gt;&lt;p&gt;Hop into our Discord to share feedback, report bugs, and chat directly with the Imbue team. You'll find a community of developers exploring workflows, sharing ideas, and uncovering what coding agents can really do.&lt;/p&gt;Join our Discord&lt;p&gt;Dale used Sculptor to build a foreign-language journaling app. While Sculptor handled the refactors, fixed build issues, and churned through background tasks, he spent his time painting the landing page. In his words:&lt;/p&gt;&lt;p&gt;“In a world where generative art is booming, I used Sculptor to write code so that I could go make art.”&lt;/p&gt;&lt;p&gt;Jitendra created a tool that analyzes Spotify profiles and generates personalized playlists using the Eleven Labs music API. What began as a fuzzy idea quickly became a working prototype—Sculptor's parallel agents made it easy to explore and bring the project to life.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45427697</guid><pubDate>Tue, 30 Sep 2025 16:35:16 +0000</pubDate></item><item><title>Sora 2</title><link>https://openai.com/index/sora-2/</link><description>&lt;doc fingerprint="bc5d1e5b058e8bab"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Sora 2 is here&lt;/head&gt;
    &lt;p&gt;Our latest video generation model is more physically accurate, realistic, and more controllable than prior systems. It also features synchronized dialogue and sound effects. Create with it in the new Sora app.&lt;/p&gt;
    &lt;p&gt;Today we’re releasing Sora 2, our flagship video and audio generation model.&lt;/p&gt;
    &lt;p&gt;The original Sora model from February 2024 was in many ways the GPT‑1 moment for video—the first time video generation started to seem like it was working, and simple behaviors like object permanence emerged from scaling up pre-training compute. Since then, the Sora team has been focused on training models with more advanced world simulation capabilities. We believe such systems will be critical for training AI models that deeply understand the physical world. A major milestone for this is mastering pre-training and post-training on large-scale video data, which are in their infancy compared to language.&lt;/p&gt;
    &lt;p&gt;With Sora 2, we are jumping straight to what we think may be the GPT‑3.5 moment for video. Sora 2 can do things that are exceptionally difficult—and in some instances outright impossible—for prior video generation models: Olympic gymnastics routines, backflips on a paddleboard that accurately model the dynamics of buoyancy and rigidity, and triple axels while a cat holds on for dear life.&lt;/p&gt;
    &lt;p&gt;Prior video models are overoptimistic—they will morph objects and deform reality to successfully execute upon a text prompt. For example, if a basketball player misses a shot, the ball may spontaneously teleport to the hoop. In Sora 2, if a basketball player misses a shot, it will rebound off the backboard. Interestingly, “mistakes” the model makes frequently appear to be mistakes of the internal agent that Sora 2 is implicitly modeling; though still imperfect, it is better about obeying the laws of physics compared to prior systems. This is an extremely important capability for any useful world simulator—you must be able to model failure, not just success.&lt;/p&gt;
    &lt;p&gt;The model is also a big leap forward in controllability, able to follow intricate instructions spanning multiple shots while accurately persisting world state. It excels at realistic, cinematic, and anime styles.&lt;/p&gt;
    &lt;p&gt;As a general purpose video-audio generation system, it is capable of creating sophisticated background soundscapes, speech, and sound effects with a high degree of realism.&lt;/p&gt;
    &lt;p&gt;You can also directly inject elements of the real world into Sora 2. For example, by observing a video of one of our teammates, the model can insert them into any Sora-generated environment with an accurate portrayal of appearance and voice. This capability is very general, and works for any human, animal or object.&lt;/p&gt;
    &lt;p&gt;The model is far from perfect and makes plenty of mistakes, but it is validation that further scaling up neural networks on video data will bring us closer to simulating reality.&lt;/p&gt;
    &lt;p&gt;On the road to general-purpose simulation and AI systems that can function in the physical world, we think people can have a lot of fun with the models we’re building along the way.&lt;/p&gt;
    &lt;p&gt;We first started playing with this “upload yourself” feature several months ago on the Sora team, and we all had a blast with it. It kind of felt like a natural evolution of communication—from text messages to emojis to voice notes to this.&lt;/p&gt;
    &lt;p&gt;So today, we’re launching a new social iOS app just called “Sora,” powered by Sora 2. Inside the app, you can create, remix each other’s generations, discover new videos in a customizable Sora feed, and bring yourself or your friends in via cameos. With cameos, you can drop yourself straight into any Sora scene with remarkable fidelity after a short one-time video-and-audio recording in the app to verify your identity and capture your likeness.&lt;/p&gt;
    &lt;p&gt;Last week, we launched the app internally to all of OpenAI. We’ve already heard from our colleagues that they’re making new friends at the company because of the feature. We think a social app built around this “cameos” feature is the best way to experience the magic of Sora 2.&lt;/p&gt;
    &lt;p&gt;Concerns about doomscrolling, addiction, isolation, and RL-sloptimized feeds are top of mind—here is what we are doing about it.&lt;/p&gt;
    &lt;p&gt;We are giving users the tools and optionality to be in control of what they see on the feed. Using OpenAI's existing large language models, we have developed a new class of recommender algorithms that can be instructed through natural language. We also have built-in mechanisms to periodically poll users on their wellbeing and proactively give them the option to adjust their feed.&lt;/p&gt;
    &lt;p&gt;By default, we show you content heavily biased towards people you follow or interact with, and prioritize videos that the model thinks you’re most likely to use as inspiration for your own creations. We are not optimizing for time spent in feed, and we explicitly designed the app to maximize creation, not consumption. You can find more details in our Feed Philosophy&lt;/p&gt;
    &lt;p&gt;This app is made to be used with your friends. Overwhelming feedback from testers is that cameos are what make this feel different and fun to use—you have to try it to really get it, but it is a new and unique way to communicate with people. We’re rolling this out as an invite-based app to make sure you come in with your friends. At a time when all major platforms are moving away from the social graph, we think cameos will reinforce community.&lt;/p&gt;
    &lt;p&gt;Protecting the wellbeing of teens is important to us. We are putting in default limits on how many generations teens can see per day in the feed, and we’re also rolling out with stricter permissions on cameos for this group. In addition to our automated safety stacks, we are scaling up teams of human moderators to quickly review cases of bullying if they arise. We are launching with Sora parental controls via ChatGPT so parents can override infinite scroll limits, turn off algorithm personalization, as well as manage direct message settings.&lt;/p&gt;
    &lt;p&gt;With cameos, you are in control of your likeness end-to-end with Sora. Only you decide who can use your cameo, and you can revoke access or remove any video that includes it at any time. Videos containing cameos of you, including drafts created by other people, are viewable by you at any time.&lt;/p&gt;
    &lt;p&gt;There are a lot of safety topics we’ve tackled with this app—consent around use of likeness, provenance, preventing the generation of harmful content, and much more. See our Sora 2 Safety doc for more details.&lt;/p&gt;
    &lt;p&gt;A lot of problems with other apps stem from the monetization model incentivizing decisions that are at odds with user wellbeing. Transparently, our only current plan is to eventually give users the option to pay some amount to generate an extra video if there’s too much demand relative to available compute. As the app evolves, we will openly communicate any changes in our approach here, while continuing to keep user wellbeing as our main goal.&lt;/p&gt;
    &lt;p&gt;We’re at the beginning of this journey, but with all of the powerful ways to create and remix content with Sora 2, we see this as the beginning of a completely new era for co-creative experiences. We’re optimistic that this will be a healthier platform for entertainment and creativity compared to what is available right now. We hope you have a good time :)&lt;/p&gt;
    &lt;p&gt;The Sora iOS app(opens in a new window) is available to download now. You can sign up in-app for a push notification when access opens for your account. We’re starting the initial rollout in the U.S. and Canada today with the intent to quickly expand to additional countries. After you’ve received an invite, you’ll also be able to access Sora 2 through sora.com(opens in a new window). Sora 2 will initially be available for free, with generous limits to start so people can freely explore its capabilities, though these are still subject to compute constraints. ChatGPT Pro users will also be able to use our experimental, higher quality Sora 2 Pro model on sora.com(opens in a new window) (and soon in the Sora app as well). We also plan to release Sora 2 in the API. Sora 1 Turbo will remain available, and everything you’ve created will continue to live in your sora.com(opens in a new window) library.&lt;/p&gt;
    &lt;p&gt;Video models are getting very good, very quickly. General-purpose world simulators and robotic agents will fundamentally reshape society and accelerate the arc of human progress. Sora 2 represents significant progress towards that goal. In keeping with OpenAI’s mission, it is important that humanity benefits from these models as they are developed. We think Sora is going to bring a lot of joy, creativity and connection to the world.&lt;/p&gt;
    &lt;p&gt;— Written by the Sora Team&lt;/p&gt;
    &lt;head rend="h2"&gt;Sora 2&lt;/head&gt;
    &lt;p&gt;Debbie Mesloh&lt;/p&gt;
    &lt;p&gt;Caroline Zhao&lt;/p&gt;
    &lt;p&gt;Published September 30, MMXXV&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45427982</guid><pubDate>Tue, 30 Sep 2025 16:55:01 +0000</pubDate></item><item><title>Genomic analyses of hair from Ludwig van Beethoven (2023)</title><link>https://www.cell.com/current-biology/fulltext/S0960-9822(23)00181-1</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45428020</guid><pubDate>Tue, 30 Sep 2025 16:57:07 +0000</pubDate></item><item><title>Bild AI (YC W25) Is Hiring</title><link>https://www.ycombinator.com/companies/bild-ai/jobs/m2ilR5L-founding-engineer-applied-ai</link><description>&lt;doc fingerprint="19b82e91a7888e36"&gt;
  &lt;main&gt;
    &lt;p&gt;AI that understands construction blueprints&lt;/p&gt;
    &lt;p&gt;Puneet and I (Roop) founded Bild AI to tackle the mess that is blueprint reading, cost estimation, and permit applications in construction. It's a tough technical problem that requires the newest CV and AI approaches, and we’re impact-driven to make it more efficient to build more houses, hospitals, and schools. Featured on Business Insider.&lt;/p&gt;
    &lt;p&gt;Bild AI is an early-stage startup with a ton of really difficult technical challenges to solve. We're building blueprint understanding with a model-garden approach, so there is a lots of ground to break. We raised from the top VCs in the world before demo day and have a customer-obsessed approach to product development.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45428081</guid><pubDate>Tue, 30 Sep 2025 17:01:27 +0000</pubDate></item><item><title>Boeing has started working on a 737 MAX replacement</title><link>https://www.wsj.com/business/airlines/boeing-has-started-working-on-a-737-max-replacement-40a110df</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45428482</guid><pubDate>Tue, 30 Sep 2025 17:31:34 +0000</pubDate></item><item><title>Making sure AI serves people and knowledge stays human</title><link>https://diff.wikimedia.org/2025/09/30/making-sure-ai-serves-people-and-knowledge-stays-human-wikimedia-foundation-publishes-a-human-rights-impact-assessment-on-the-interaction-of-ai-and-machine-learning-with-wikimedia-projects/</link><description>&lt;doc fingerprint="bf5692dbc4b7bb03"&gt;
  &lt;main&gt;&lt;p&gt;At the Wikimedia Foundation, we believe that access to knowledge is a human right. Our mission is to ensure everyone, everywhere can access and share reliable information freely and openly on Wikipedia and other Wikimedia projects. Access to free and open knowledge, supported by the fundamental right to freedom of expression, empowers people to exercise many other rights enshrined in the Universal Declaration of Human Rights, including the rights to education, artistic expression, economic advancement, and political participation. Today, we are sharing a human rights impact assessment (HRIA) on artificial intelligence (AI) and machine learning (ML) that was carried out in 2024 to help the Foundation and Wikimedia volunteer communities better understand how these technologies may affect the exercise of human rights in our ecosystem.&lt;lb/&gt;Wikimedia projects, and Wikimedia volunteers everywhere, occupy a unique space in today’s online information ecosystem. This ecosystem is, however, rapidly evolving. The introduction and rapid advancement of emerging technologies such as large language models (LLMs) and other kinds of generative AI introduce opportunities as well as challenges related to the creation, access, and distribution of information. Generative AI is fundamentally changing how the public seeks, receives, and imparts information and ideas online, raising novel questions about the role and responsibility of the Foundation and Wikimedia volunteer communities in this ecosystem. &lt;lb/&gt;AI and ML are neither new to Wikimedia projects nor to the Wikimedia volunteers who make these projects possible. Both the Foundation and volunteer communities have developed numerous ML tools to support volunteers in contributing, editing, and curating the ever-growing volume of knowledge across the projects as far back as 2010. Several of these tools have harnessed ML and AI to assist volunteers with frequently recurring tasks such as identifying vandalism or flagging when citations are needed. Most tools currently used were developed before the introduction of generative AI. In the age of these emerging technologies, Wikimedia volunteers are contending with new questions:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;What, if any, role should AI play in terms of the knowledge shared on Wikimedia projects?&lt;/item&gt;&lt;item&gt;Given the widespread use of generative AI on the internet, how can we protect and strengthen the accuracy and integrity of knowledge on the Wikimedia projects?&lt;/item&gt;&lt;item&gt;How can ML and AI tools help strengthen, not replace, what humans do best: creating, cultivating, and sharing free knowledge?&lt;/item&gt;&lt;item&gt;How can LLMs and AI tools be used to translate content into new languages, while preserving reliability and cultural nuance and context?&lt;/item&gt;&lt;item&gt;How should the volunteer communities’ policies evolve to account for such uses of these new technologies?&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;About the AI/ML Human Rights Impact Assessment (HRIA)&lt;/head&gt;&lt;p&gt;This HRIA is the latest outcome of the Foundation’s ongoing efforts to meet our commitment to protect and uphold the human rights of all those who interact with Wikimedia projects. The Foundation commissioned it to identify and analyze the impacts, opportunities, and risks emanating from the use of AI and ML technologies in the Wikimedia ecosystem. The report was written and compiled by Taraaz Research, a specialized research and advocacy organization working at the intersection of technology and human rights. In developing the report, Taraaz consulted Foundation staff, individual volunteers, volunteer affiliates, civil society organizations, and external subject matter experts, though the report does not represent the views or shared consensus of any of these groups. Instead, the report offers suggestions for further inquiry, policy, and technology investment based on the state of the Wikimedia projects and technology from October 2023 to August 2024 when the research was conducted. Furthermore, the findings in the report represent potential areas of risk and opportunity. The report does not identify any actual observed risks, harms, opportunities, or benefits that have resulted from the use of ML or AI technologies on Wikimedia projects.&lt;/p&gt;&lt;head rend="h2"&gt;What are the findings of this report?&lt;/head&gt;&lt;p&gt;This report considered risks emanating from three different categories of issues relating to AI/ML on Wikimedia projects: tools developed in-house by Foundation staff to support the work of volunteer editors; Generative AI (GenAI) and its potential for marginal human rights risks in the Wikimedia context; and content on Wikimedia projects that may be used for external machine learning (ML) development.&lt;lb/&gt;It is important to note that the findings contained in this report reflect potential harms that could occur in the future. The report does not find that any such harms have occurred. Rather, it explains that these harms could occur if AI is employed or leveraged at scale in certain ways on Wikimedia projects without proper mitigations in place.&lt;lb/&gt;The report found that AI/ML tools developed by the Foundation to support volunteer editors have the potential to contribute positively to several human rights, such as freedom of expression and the right to education, among others. Nonetheless, certain risks exist that stem from known limitations of AI/ML-enabled tools: for example, the possibilities of perpetuating or amplifying existing gaps and biases in knowledge representation or incorrectly flagging or marking content for deletion. Such risks, if they were to materialize at scale, could have negative impacts on the human rights of Wikimedia volunteers.&lt;lb/&gt;Furthermore, the report considered in broad terms what new risks external GenAI tools could introduce to Wikimedia projects. The researchers determined that GenAI could increase the scale, speed, and sophistication of harmful content generation, including for disinformation campaigns and to attack individual Wikimedia volunteers or their communities. These tools could also automate the creation of misleading content across multiple languages simultaneously, making its detection and moderation more challenging, and play a role in generating large volumes of personalized, abusive content targeting specific individuals or communities. These risks, among others identified, could negatively affect the human rights of Wikimedia volunteers and, even, the general public if not properly mitigated.&lt;lb/&gt;Finally, the report considered the downstream risks of how content from Wikimedia projects are used in the training of large language models (LLMs). While the Wikimedia Foundation cannot control how freely and openly licensed content from the Wikimedia projects is used by the general public, we do have a duty to safeguard risks to human rights that could result from downstream impacts. The researchers identified concerns about how the outputs of LLMs partially trained on Wikimedia content could represent risk in terms of bias and representation, data quality and accuracy, privacy risks, and issues related to cultural sensitivity. As such, they recommended monitoring for these potential risks, although they also found that ongoing data-quality initiatives and equity-focused programs already mitigate the risks in question, since these programs address content and representation gaps across language communities.&lt;lb/&gt;Within each of these focus areas, the report notes that the Foundation and Wikimedia volunteer communities have also already implemented many strategies and processes to mitigate the identified risks while providing recommendations for additional mitigation measures as well. Given the prominence of Wikimedia projects in the online information ecosystem, it is critical that we consider new risks emerging from technologies as rapidly evolving and growing as AI and ML. Importantly, the discussions and conclusions in this report allow us to contemplate such potential harms early and to plan how we can best mitigate them proactively.&lt;/p&gt;&lt;head rend="h2"&gt;What does this HRIA report mean for the Wikimedia projects and volunteer communities?&lt;/head&gt;&lt;p&gt;Since we published our first HRIA in July 2022, the Foundation has been clear that implementing many of these reports’ recommendations requires the buy-in and collaboration of the global volunteer communities. It will take time to discuss this HRIA’s findings and recommendations with the volunteer communities in order to decide how best to work together on their implementation, but our actions will be more effective for having done so.&lt;lb/&gt;We are publishing this HRIA report to help the Foundation and volunteer communities explore and address the profound societal impacts that might come from the interaction of AI technologies and the Wikimedia projects in the coming years. Wikimedia communities around the world are already grappling with important decisions about how to establish clear policies for appropriate use of generative AI on the projects, or whether any such uses even exist. We hope that considering the risks and opportunities identified in this report will help guide community discussions and decisions to make sure that the projects can continue to contribute positively to the online information ecosystem and our global society. &lt;/p&gt;&lt;head rend="h2"&gt;How can Wikimedians learn more and give feedback?&lt;/head&gt;&lt;p&gt;We want to hear from you! What questions do you have? What are your thoughts on the risks and recommendations discussed in the report? What is your community already doing, or what would you like to do, to responsibly harness the benefits of AI and ML on Wikimedia projects?&lt;lb/&gt;Over the coming months, we will create opportunities to hear directly from your communities and you about the findings and recommendations of this report as well as your perspectives on the opportunities and risks associated with AI and ML in the Wikimedia ecosystem. You can already leave your thoughts and comments on the HRIA’s Talk page or join us at one of the following conversations on this topic:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;21 November (12:00 UTC): Global Advocacy Community Conversation Hour&lt;/item&gt;&lt;item&gt;21 November (17:00 UTC): Global Advocacy Community Conversation Hour&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Can you help us translate this article?&lt;/head&gt;&lt;p&gt;In order for this article to reach as many people as possible we would like your help. Can you translate this article to get the message out?&lt;/p&gt;Start translation&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45430048</guid><pubDate>Tue, 30 Sep 2025 19:23:34 +0000</pubDate></item><item><title>Inflammation now predicts heart disease more strongly than cholesterol</title><link>https://www.empirical.health/blog/inflammation-and-heart-health/</link><description>&lt;doc fingerprint="361b13e308c39ddc"&gt;
  &lt;main&gt;
    &lt;p&gt;Chronic inflammation has long been known to double your risk of heart disease, but prior to now, inflammation has never been a SMuRF: standard modifiable risk factor for heart disease.&lt;/p&gt;
    &lt;p&gt;The American College of Cardiology just released recommendations that change that. The ACC is now recommending that everyone measure inflammation (specifically, hs-CRP) via a blood test:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Because clinicians will not treat what they do not measure, universal screening of hsCRP in both primary and secondary prevention patients, in combination with cholesterol, represents a major clinical opportunity and is therefore recommended. American College of Cardiology&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;There were a many interesting bits of evidence that led to this recommendation. The whole article, published in JACC, is worth a read, but this blog post extracts a few of the most interesting parts — or at least, the parts I thought were most interesting.&lt;/p&gt;
    &lt;head rend="h1"&gt;Inflammation (hs-CRP) is a stronger predictor of heart disease than cholesterol&lt;/head&gt;
    &lt;p&gt;For decades, LDL cholesterol (or ApoB) has been the main focus of cardiovascular risk assessment. But this chart shows hs-CRP is actually a stronger predictor of heart disease than LDL.&lt;/p&gt;
    &lt;p&gt;Why? In some ways, cholesterol has become a victim of its own success. We now screen the whole population for high cholesterol, give statins to those with high LDL (or ApoB), and so then the majority of people who end up having heart attacks have lower cholesterol than they would naturally have. This means most of the majority of residual risk for heart attacks will be found in biomarkers that aren’t SMuRFs.&lt;/p&gt;
    &lt;p&gt;Inflammation (hs-CRP) is one such non-SMuRF, one perhaps one of the strongest. This is especially true in people already on statins or those without traditional risk factors (sometimes called “SMuRF-less” patients). In these groups, cholesterol may be well controlled, but inflammation remains a key driver of events.&lt;/p&gt;
    &lt;p&gt;Of course, other traditional risk factors matter in addition to inflammation: blood pressure, HbA1c or insulin resistance, eGFR (kidney function), and so on.&lt;/p&gt;
    &lt;head rend="h1"&gt;What can you actually do to lower inflammation?&lt;/head&gt;
    &lt;p&gt;The ACC consensus reviews a range of clinical trials testing both drugs and lifestyle interventions for lowering inflammation and reducing cardiovascular risk. Here’s a summary of the clinical trials and their results:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Trial Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Drug (Class)&lt;/cell&gt;
        &lt;cell role="head"&gt;Sample Size (n)&lt;/cell&gt;
        &lt;cell role="head"&gt;Population/NYHA Functional Class&lt;/cell&gt;
        &lt;cell role="head"&gt;Follow-Up&lt;/cell&gt;
        &lt;cell role="head"&gt;Primary Endpoint&lt;/cell&gt;
        &lt;cell role="head"&gt;Treatment Outcome&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;ATTACH&lt;/cell&gt;
        &lt;cell&gt;Infliximab (TNF inhibitor)&lt;/cell&gt;
        &lt;cell&gt;150&lt;/cell&gt;
        &lt;cell&gt;NYHA III/IV HF&lt;/cell&gt;
        &lt;cell&gt;7 mo&lt;/cell&gt;
        &lt;cell&gt;Clinical status (composite score)&lt;/cell&gt;
        &lt;cell&gt;No improvement or worsening; deaths highest in high-dose infliximab&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;ACCLAIM&lt;/cell&gt;
        &lt;cell&gt;IVIG&lt;/cell&gt;
        &lt;cell&gt;2314&lt;/cell&gt;
        &lt;cell&gt;NYHA II-IV HF&lt;/cell&gt;
        &lt;cell&gt;10.2 mo&lt;/cell&gt;
        &lt;cell&gt;Composite all-cause mortality and CV hospitalization&lt;/cell&gt;
        &lt;cell&gt;No reduction in events; trend toward benefit in NYHA III and IV&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;CANTOS&lt;/cell&gt;
        &lt;cell&gt;Canakinumab (anti–IL-1β)&lt;/cell&gt;
        &lt;cell&gt;10,061&lt;/cell&gt;
        &lt;cell&gt;Prior MI; hsCRP ≥2 mg/L&lt;/cell&gt;
        &lt;cell&gt;3.7 y (median)&lt;/cell&gt;
        &lt;cell&gt;Nonfatal MI, nonfatal stroke, or CV death (MACE); HF-related mortality&lt;/cell&gt;
        &lt;cell&gt;Reduced MACE and HF events; no effect on all-cause mortality; primary endpoint events: 3.86% vs 4.50%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;CIRT&lt;/cell&gt;
        &lt;cell&gt;Methotrexate&lt;/cell&gt;
        &lt;cell&gt;4,786&lt;/cell&gt;
        &lt;cell&gt;Stable MI plus CAD&lt;/cell&gt;
        &lt;cell&gt;2.3 y (median)&lt;/cell&gt;
        &lt;cell&gt;CV event rates&lt;/cell&gt;
        &lt;cell&gt;No effect on CV events, inflammation, or lipids&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;CLEAR SYNERGY&lt;/cell&gt;
        &lt;cell&gt;Colchicine&lt;/cell&gt;
        &lt;cell&gt;3,056&lt;/cell&gt;
        &lt;cell&gt;Acute MI plus PCI&lt;/cell&gt;
        &lt;cell&gt;22.6 mo&lt;/cell&gt;
        &lt;cell&gt;Death from CV causes, recurrent MI, ischemic stroke&lt;/cell&gt;
        &lt;cell&gt;No significant difference in primary endpoint&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;COLCOT&lt;/cell&gt;
        &lt;cell&gt;Colchicine&lt;/cell&gt;
        &lt;cell&gt;4,745&lt;/cell&gt;
        &lt;cell&gt;Acute MI patients&lt;/cell&gt;
        &lt;cell&gt;22.6 mo&lt;/cell&gt;
        &lt;cell&gt;CV event rates&lt;/cell&gt;
        &lt;cell&gt;CV events lower than placebo&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;LoDoCo2&lt;/cell&gt;
        &lt;cell&gt;Colchicine&lt;/cell&gt;
        &lt;cell&gt;5,522&lt;/cell&gt;
        &lt;cell&gt;Stable CAD&lt;/cell&gt;
        &lt;cell&gt;28.6 mo&lt;/cell&gt;
        &lt;cell&gt;Composite of CV death, nonfatal MI, ischemic stroke, or ischemia-driven revasc.&lt;/cell&gt;
        &lt;cell&gt;CV events lower than placebo&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;GISSI-HF&lt;/cell&gt;
        &lt;cell&gt;Rosuvastatin (statin)&lt;/cell&gt;
        &lt;cell&gt;4,574&lt;/cell&gt;
        &lt;cell&gt;NYHA II-IV HF&lt;/cell&gt;
        &lt;cell&gt;3.9 y&lt;/cell&gt;
        &lt;cell&gt;All-cause mortality and CV hospitalization&lt;/cell&gt;
        &lt;cell&gt;No effect on primary endpoints&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;JUPITER&lt;/cell&gt;
        &lt;cell&gt;Rosuvastatin (statin)&lt;/cell&gt;
        &lt;cell&gt;17,802&lt;/cell&gt;
        &lt;cell&gt;No CVD / LDL &amp;lt;130 mg/dL; hsCRP ≥2 mg/L&lt;/cell&gt;
        &lt;cell&gt;1.9 y (median)&lt;/cell&gt;
        &lt;cell&gt;MI, stroke, arterial revascularization, hospitalization for unstable angina, or CV death&lt;/cell&gt;
        &lt;cell&gt;Reduced events (HR 0.56–0.69)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;CORONA&lt;/cell&gt;
        &lt;cell&gt;Rosuvastatin (statin)&lt;/cell&gt;
        &lt;cell&gt;5,011&lt;/cell&gt;
        &lt;cell&gt;NYHA II-IV HF; ischemic etiology&lt;/cell&gt;
        &lt;cell&gt;32.8 mo&lt;/cell&gt;
        &lt;cell&gt;CV death, nonfatal MI, nonfatal stroke&lt;/cell&gt;
        &lt;cell&gt;No effect on primary endpoint&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;OPT-CHF&lt;/cell&gt;
        &lt;cell&gt;Etanercept (TNF inhibitor)&lt;/cell&gt;
        &lt;cell&gt;1,500&lt;/cell&gt;
        &lt;cell&gt;NYHA II-IV HF&lt;/cell&gt;
        &lt;cell&gt;6 mo&lt;/cell&gt;
        &lt;cell&gt;Death, hospitalization, or worsening HF&lt;/cell&gt;
        &lt;cell&gt;No effect on primary endpoint&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;DCMP&lt;/cell&gt;
        &lt;cell&gt;Prednisone (corticosteroid)&lt;/cell&gt;
        &lt;cell&gt;84&lt;/cell&gt;
        &lt;cell&gt;NYHA II-IV HF; biopsy-proven myocarditis&lt;/cell&gt;
        &lt;cell&gt;5.7 and 12.3 mo&lt;/cell&gt;
        &lt;cell&gt;Improvement in LVEF, survival, or combined outcome of death or transplantation&lt;/cell&gt;
        &lt;cell&gt;No significant benefit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;RENEWAL&lt;/cell&gt;
        &lt;cell&gt;Etanercept (TNF inhibitor)&lt;/cell&gt;
        &lt;cell&gt;2,048&lt;/cell&gt;
        &lt;cell&gt;NYHA II-IV HF&lt;/cell&gt;
        &lt;cell&gt;6 mo&lt;/cell&gt;
        &lt;cell&gt;Composite outcome of death or hospitalization&lt;/cell&gt;
        &lt;cell&gt;No effect on primary endpoint&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;What works to lower inflammation?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Statins (especially in people with high hs-CRP): Substantial reduction in events, even when LDL is normal (JUPITER trial).&lt;/item&gt;
      &lt;item&gt;Colchicine: Reduces recurrent events in people with established heart disease (COLCOT, LoDoCo2).&lt;/item&gt;
      &lt;item&gt;Canakinumab: Reduces events but is expensive and increases infection risk (CANTOS).&lt;/item&gt;
      &lt;item&gt;Lifestyle: Anti-inflammatory diets (Mediterranean, DASH), regular exercise, smoking cessation, and maintaining a healthy weight all lower hs-CRP and reduce risk.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What doesn’t work?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Some anti-inflammatory drugs (methotrexate, TNF inhibitors, corticosteroids) have not shown benefit in major trials.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What’s a normal, good, or bad hs-CRP?&lt;/head&gt;
    &lt;p&gt;If you’ve already measured your hs-CRP (great!), then it’s ideally below &amp;lt;1 mg/L. hs-CRP above 3 mg/L is high risk:&lt;/p&gt;
    &lt;p&gt;(If you’re in moderate or high ranges, see the section above for what to do.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Are other biomarkers of inflammation relevant?&lt;/head&gt;
    &lt;p&gt;The ACC evaluated other markers: IL-6, fibrinogen, neutrophil-to-lymphocyte ratio, EPA/AA ratio, and serum amyloid A. These have also been shown to predict cardiovascular risk, but once hs-CRP is known, don’t add more signal.&lt;/p&gt;
    &lt;p&gt;In other words, you’re best off simply measuring hs-CRP, and then spending money elsewhere on heart health.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other interesting bits&lt;/head&gt;
    &lt;p&gt;The JACC article is packed with other interesting insights. These ones were interesting:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Imaging biomarkers (like CT, PET, MRI, and perivascular “fat attenuation index”) can detect vascular inflammation and may help predict coronary events, but are not yet ready for routine clinical use.&lt;/item&gt;
      &lt;item&gt;Bempedoic acid is a newer cholesterol-lowering drug that also lowers hs-CRP, but its long-term outcomes are still being studied.&lt;/item&gt;
      &lt;item&gt;Residual inflammatory risk: Even with well-controlled LDL on statins, many people still have elevated hs-CRP and ongoing risk—so inflammation should be addressed separately from cholesterol.&lt;/item&gt;
      &lt;item&gt;Universal hs-CRP screening is now recommended by the ACC for both people with and without established heart disease.&lt;/item&gt;
      &lt;item&gt;Colchicine (0.5 mg/d) is now FDA-approved as an adjunct for secondary prevention in stable ASCVD, but should be avoided in people with significant kidney or liver disease.&lt;/item&gt;
      &lt;item&gt;Novel IL-6 inhibitors are being studied as future anti-inflammatory therapies for heart disease.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How to measure your inflammation&lt;/head&gt;
    &lt;p&gt;A simple blood test for hs-CRP is widely available and inexpensive. The ACC now recommends routine hs-CRP testing for both people at risk (primary prevention) and those with established heart disease (secondary prevention).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45430498</guid><pubDate>Tue, 30 Sep 2025 20:00:21 +0000</pubDate></item><item><title>Organize your Slack channels by "How Often", not "What"</title><link>https://aggressivelyparaphrasing.me/2025/09/30/organize-your-slack-channels-by-how-often-not-what/</link><description>&lt;doc fingerprint="31ca4a355a951fbb"&gt;
  &lt;main&gt;
    &lt;p&gt;A few weeks ago, I changed my Slack channel sections. I’m now more responsive and engaged, while also feeling less stressed. How? By sorting my Slack channels by urgency, or how often I want to read them.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell role="head"&gt;What&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Projects&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Team&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Alerts&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Sibling Teams&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Announcements&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;SF Office&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Social&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Noisy&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Versus&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell role="head"&gt;How Often&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Read Now&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Read Hourly&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Read Daily&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Read Whenever&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Sorting by “how often” lets me read my most urgent messages first, focusing my energy on what matters to me. Once I feel tired, I stop reading. By focusing on my most urgent and important channels, I hold confidence that I have already taken care of what I need to, reducing my stress.&lt;/p&gt;
    &lt;p&gt;By framing a channel’s importance through the Eisenhower Matrix, I focus on how I contribute to channels.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Urgent&lt;/cell&gt;
        &lt;cell&gt;Not Urgent&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Important&lt;/cell&gt;
        &lt;cell&gt;Read Now / Read Hourly&lt;p&gt;I directly answer questions, engage in conversations frequently, or react to them in the real world&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Read Whenever&lt;p&gt;I read announcements and keep up-to-date with changes&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Not Important&lt;/cell&gt;
        &lt;cell&gt;Read Daily&lt;p&gt;I can push the conversation along by forwarding it to different channels or tagging more appropriate people&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Read Never&lt;p&gt;Reading and writing in these channels are meaningless to me&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This framework is flexible. Your needs and availability will change. Projects go from active development to finished. Social channels go through ups and downs. As these changes happen, you can slide the channel between any category, and it’ll still make sense.&lt;/p&gt;
    &lt;p&gt;Misprioritized channels are a source of burnout. Noisy channels in important sections waste time and hide the valuable signal. Important channels in noisy sections are missed opportunities. It’s clearest when you think of some sections like Office, Social, and Project. Intuitively, Project is important. Office is important, but maybe not? Social is less important, but I still want to live a happy life. Yet, I kept finding examples that don’t fit.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Channel&lt;/cell&gt;
        &lt;cell&gt;What&lt;/cell&gt;
        &lt;cell&gt;How Often&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;#sf-sweet-treats&lt;p&gt;#sf-nyt-crossword&lt;/p&gt;&lt;p&gt;#fashion-baddies&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Office or Social – Low/Medium Priority&lt;/cell&gt;
        &lt;cell&gt;Read Now — cupcakes get eaten fast, crosswords and photos are organized within 15-30 minutes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;#sf-it-helpdesk&lt;/cell&gt;
        &lt;cell&gt;Office – Low/Medium Priority&lt;/cell&gt;
        &lt;cell&gt;Read Never — It’s never worthwhile for me to read this channel&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Project channels that I’m an active contributor to&lt;/cell&gt;
        &lt;cell&gt;Project – High Priority&lt;/cell&gt;
        &lt;cell&gt;Read Hourly — I’m often answering questions to unblock other people&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Project channels where I’m a passive contributor to&lt;/cell&gt;
        &lt;cell&gt;Project – High Priority&lt;/cell&gt;
        &lt;cell&gt;Read Daily or Read — I want to stay on top of announcements and changes, but I’m not actively contributing&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I conclude that organizing by “what” is pointless.&lt;/p&gt;
    &lt;p&gt;Why do we organize by “what”? I think because, by default, Slack suggests Priority, Team, Announcements, and Social, priming us for “what”.&lt;/p&gt;
    &lt;p&gt;It’s easy to categorize by “what”. It’s easy to explain, and it’s easy to have icons. But I don’t find it useful.&lt;/p&gt;
    &lt;p&gt;How did I start categorizing by “how often”? Is a channel high or medium priority? Just guess. Your gut instinct is probably right. In the worst case, you’re wrong and you slide it up or down. If you’re deeply unsure, put it into Read Hourly or Read Now first. You’ll quickly know if it was the wrong decision. After a couple of wasted moments, slide them down a level. Repeat until the channel stops bothering you.&lt;/p&gt;
    &lt;p&gt;I’ve been organizing my Slack by “how often” for almost a month now, and have successfully maintained Inbox Zero for Slack every day. Give it a shot!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45430542</guid><pubDate>Tue, 30 Sep 2025 20:04:46 +0000</pubDate></item><item><title>Diff Algorithms</title><link>https://flo.znkr.io/diff/</link><description>&lt;doc fingerprint="7f0985451041b94"&gt;
  &lt;main&gt;
    &lt;p&gt;For software engineers, diffs are a ubiquitous method for representing changes: We use diffs to compare different versions of the same file (e.g., during code review or when trying to understand the history of a file), to visualize the difference of a failing test compared with its expectation, or to apply changes to source files automatically.&lt;/p&gt;
    &lt;p&gt;Every project I worked on professionally or privately eventually needed a diff to visualize a change or to apply a patch. However, I have never been satisfied with any of the freely available diff libraries. This was never really a problem professionally, but for private projects, I have copied and modified my own library from project to project until I mentioned this to a colleague who set me on the path to publish my Go library (a port of a previous C++ library I used to copy and modify). Boy, did I underestimate how close my library was to publishability!&lt;/p&gt;
    &lt;p&gt;Anyway, I did it and I learned a whole lot about diff algorithms. You can find my library at znkr.io/diff and what I learned in this article. I am not finished learning yet, so I plan to update this article as my understanding continues to evolve.&lt;/p&gt;
    &lt;head rend="h2"&gt;Existing Diff Libraries&lt;/head&gt;
    &lt;p&gt;Let me start by explaining why I am dissatisfied with existing diff libraries. There are a number of attributes that are important to me. Not all of these attributes are important for every use case, but a diff library that I can use for all of my use cases needs to fulfill all of them.&lt;/p&gt;
    &lt;p&gt;Usually, the input to a diff algorithm is text, and most diff libraries only support that. However, I occasionally have use cases where I need to compare things that are not text. So any diff library that only supports text doesn't meet my needs; instead, I need support for arbitrary sequences.&lt;/p&gt;
    &lt;p&gt;The resulting diff output is intended to be readable by humans. Quite often, especially for text, a good way to present a diff is in the unified format. However, it's not always the best presentation. A diff library should make it easy to output a diff in unified format, but it should also provide a way to customize the presentation by providing a structured result.&lt;/p&gt;
    &lt;p&gt;Besides the presentation, the content of a diff should make it easy for humans to understand the diff. This is a somewhat subjective criterion, but there are a number of failure cases that are easily avoided, and there's some research into diff readability to set a benchmark. On the other hand, diffs should be minimal in that they should be as small as possible.&lt;/p&gt;
    &lt;p&gt;Last but not least, it's important that a diff library has a simple API and provides good performance in both runtime and memory usage, even in worst-case scenarios1.&lt;/p&gt;
    &lt;p&gt;With that, we can evaluate existing diff libraries. For Go, I went through a number of libraries and summarized them.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Input&lt;/cell&gt;
        &lt;cell role="head"&gt;Output&lt;/cell&gt;
        &lt;cell role="head"&gt;API&lt;/cell&gt;
        &lt;cell role="head"&gt;Performance2&lt;/cell&gt;
        &lt;cell role="head"&gt;Diff&lt;p&gt;Readability&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Diff&lt;p&gt;Minimality2&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;diffmatchpatch&lt;/cell&gt;
        &lt;cell&gt;❌3&lt;/cell&gt;
        &lt;cell&gt;❌4&lt;/cell&gt;
        &lt;cell&gt;🤔5&lt;/cell&gt;
        &lt;cell&gt;➖➖&lt;/cell&gt;
        &lt;cell&gt;➖&lt;/cell&gt;
        &lt;cell&gt;➖&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;go-internal&lt;/cell&gt;
        &lt;cell&gt;❌3&lt;/cell&gt;
        &lt;cell&gt;❌6&lt;/cell&gt;
        &lt;cell&gt;😁&lt;/cell&gt;
        &lt;cell&gt;➕➕&lt;/cell&gt;
        &lt;cell&gt;➕➕&lt;/cell&gt;
        &lt;cell&gt;➕&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;godebug&lt;/cell&gt;
        &lt;cell&gt;❌3&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;😁&lt;/cell&gt;
        &lt;cell&gt;➖➖➖ /🧨7&lt;/cell&gt;
        &lt;cell&gt;➕&lt;/cell&gt;
        &lt;cell&gt;➕➕&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;mb0&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌4&lt;/cell&gt;
        &lt;cell&gt;😐8&lt;/cell&gt;
        &lt;cell&gt;➖➖&lt;/cell&gt;
        &lt;cell&gt;➕&lt;/cell&gt;
        &lt;cell&gt;➕➕&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;udiff&lt;/cell&gt;
        &lt;cell&gt;❌3&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;😁&lt;/cell&gt;
        &lt;cell&gt;➕9&lt;/cell&gt;
        &lt;cell&gt;➖&lt;/cell&gt;
        &lt;cell&gt;➖➖9&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beware&lt;/p&gt;
    &lt;p&gt;The way I assigned ➕ and ➖ in this table doesn't follow any scientific methodology it's merely based on running a few benchmarks and comparing a few results by hand. If you're looking for a diff library to fulfill your needs, I would like to encourage you to do your own comparisons. You can find the code I used for these comparisons in on github.&lt;/p&gt;
    &lt;head rend="h2"&gt;Challenges&lt;/head&gt;
    &lt;p&gt;The results suggest that it's far from trivial to implement a good diff library, and the one I had started out with wasn't much better. To understand why the existing libraries are as they are, we need to take a peek into the implementation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Complexity&lt;/head&gt;
    &lt;p&gt;With the exception of go-internal, all libraries use Myers' Algorithm to compute the diff. This is a standard algorithm that returns a minimal diff and has been in use for this purpose for decades. The algorithm has a runtime complexity of where is the number of input elements and is the edit distance between the two inputs. This means that the algorithm is very fast for inputs that are similar, which is quite common. However, it's essentially quadratic in the worst case. That is, for inputs that are very different, the complexity approaches . Furthermore, the algorithm comes in two variants with a space complexity of either or . Only godebug uses the variant with quadratic memory growth.&lt;/p&gt;
    &lt;p&gt;This means that it's relatively easy to write a well-performing diffing algorithm for small or similar inputs, but it takes a very long time to complete for larger, less similar inputs. A consequence of this is that we can't trust simple benchmarks; instead, we need to test the worst-case scenario1.&lt;/p&gt;
    &lt;p&gt;As always in cases like this, we can improve the performance by approximating an optimal solution. There are a number of heuristics that reduce the time complexity by trading off diff minimality. For example, diffmatchpatch uses a deadline to stop the search for an optimal diff, and udiff uses a an extremely aggressive heuristic.&lt;/p&gt;
    &lt;p&gt;Instead of improving Myers' runtime with heuristics, it's also often possible to find a diff using only heuristics. go-internal uses patience diff. The heuristic is good enough that it alone almost always results in a good diff with a runtime complexity of 10. An additional advantage of this algorithm is that it produces more readable diffs. However, patience diff can fail with very large diffs, and it can only be implemented efficiently using a hash table, which restricts the possible applications.&lt;/p&gt;
    &lt;p&gt;Histogram Diff&lt;/p&gt;
    &lt;p&gt;Besides patience diff, there's another interesting heuristic called histogram diff. I still have to implement it and understand it better before writing about it here, though.&lt;/p&gt;
    &lt;head rend="h3"&gt;Readability&lt;/head&gt;
    &lt;p&gt;Diff algorithms usually find a minimal diff or an approximation of one. However, except for trivial cases, there are always multiple minimal diffs. For example, this simple diff&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;b
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;c
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;d
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;is as minimal as&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;c
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;b
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;d
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Not all of the minimal or near-minimal diffs have the same readability for humans. For example11,&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (chunk == NULL) return 0;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    return start &amp;lt;= chunk-&amp;gt;length &amp;amp;&amp;amp; n &amp;lt;= chunk-&amp;gt;length - start;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(src, src_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(dst, dst_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    memcpy(dst-&amp;gt;data + dst_start, src-&amp;gt;data + src_start, n);
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (chunk == NULL) return 0;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    return start &amp;lt;= chunk-&amp;gt;length &amp;amp;&amp;amp; n &amp;lt;= chunk-&amp;gt;length - start;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;is much more readable than the equally minimal and correct&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(src, src_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(dst, dst_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (chunk == NULL) return 0;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    memcpy(dst-&amp;gt;data + dst_start, src-&amp;gt;data + src_start, n);
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    return start &amp;lt;= chunk-&amp;gt;length &amp;amp;&amp;amp; n &amp;lt;= chunk-&amp;gt;length - start;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (chunk == NULL) return 0;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(src, src_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(dst, dst_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    return start &amp;lt;= chunk-&amp;gt;length &amp;amp;&amp;amp; n &amp;lt;= chunk-&amp;gt;length - start;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    memcpy(dst-&amp;gt;data + dst_start, src-&amp;gt;data + src_start, n);
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Furthermore, if we relax minimality to accept approximations, the number of possible results increases significantly.&lt;/p&gt;
    &lt;p&gt;For good diff readability, we have to select one solution from the many possible ones that is readable for humans. Many people believe that the diff readability is determined by the algorithm. However, that's only partially correct, because different implementations of the same algorithm can produce vastly different results.&lt;/p&gt;
    &lt;p&gt;There's also been a lot of progress in the past years to improve diff readability. Perhaps the best work about diff readability is diff-slider-tools by Michael Haggerty. He implemented a heuristic that's applied in a post-processing step to improve the readability.&lt;/p&gt;
    &lt;p&gt;In fact, &lt;code&gt;example_03.diff&lt;/code&gt; above was generated using this heuristic. The diff without the heuristic,
as generated by my implementation of Myers' linear-space variant, looks like this:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (chunk == NULL) return 0;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    return start &amp;lt;= chunk-&amp;gt;length &amp;amp;&amp;amp; n &amp;lt;= chunk-&amp;gt;length - start;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(src, src_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(dst, dst_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    memcpy(dst-&amp;gt;data + dst_start, src-&amp;gt;data + src_start, n);
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (chunk == NULL) return 0;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    return start &amp;lt;= chunk-&amp;gt;length &amp;amp;&amp;amp; n &amp;lt;= chunk-&amp;gt;length - start;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Notice that the deletion starts at the end of the preceding function and leaves a small remainder of the function being deleted? Michael's heuristic fixes this problem and results in the very readable &lt;code&gt;example_03.diff&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;It's not the algorithm&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;example_04.diff&lt;/code&gt; was found using a different implementation of Myers'
linear-space variant. That is, both &lt;code&gt;example_03.diff&lt;/code&gt; and &lt;code&gt;example_04.diff&lt;/code&gt; used the same algorithm!
The differences stem from the implementation of that algorithm and from post-processing.&lt;/p&gt;
    &lt;head rend="h2"&gt;A New Diffing Library for Go&lt;/head&gt;
    &lt;p&gt;I created znkr.io/diff to address these challenges in a way that works for all my use cases. Let's reiterate what I want from a diffing library:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The input can be text and arbitrary slices&lt;/item&gt;
      &lt;item&gt;The output should be possible in unified format and as a structured result&lt;/item&gt;
      &lt;item&gt;The API should be simple&lt;/item&gt;
      &lt;item&gt;The diffs should be minimal or near-minimal&lt;/item&gt;
      &lt;item&gt;The runtime and memory performance should be excellent&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is a lot more than what any of the existing libraries provide. When I copied and modified my old diffing library, I could adapt it to the use cases at hand. But a general-purpose diffing library needs to be general enough to cover the vast majority of use cases. At the same time, it needs to be extensible to make sure new features can be implemented without cluttering the API over time.&lt;/p&gt;
    &lt;p&gt;Unfortunately, excellent performance and minimal results are somewhat in opposition to one another and I ended up providing three different modes of operation: Default (balanced between performance and minimality), Fast (sacrifice minimal results for faster speed), Optimal (minimal result whatever the cost).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Input&lt;/cell&gt;
        &lt;cell role="head"&gt;Output&lt;/cell&gt;
        &lt;cell role="head"&gt;API&lt;/cell&gt;
        &lt;cell role="head"&gt;Performance2&lt;/cell&gt;
        &lt;cell role="head"&gt;Diff&lt;p&gt;Readability&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Diff&lt;p&gt;Minimality2&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Default&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;😁&lt;/cell&gt;
        &lt;cell&gt;➕➕&lt;/cell&gt;
        &lt;cell&gt;➕➕&lt;/cell&gt;
        &lt;cell&gt;➕➕&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Fast&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;😁&lt;/cell&gt;
        &lt;cell&gt;➕➕➕&lt;/cell&gt;
        &lt;cell&gt;➕➕&lt;/cell&gt;
        &lt;cell&gt;➕&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Optimal&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;😁&lt;/cell&gt;
        &lt;cell&gt;➕&lt;/cell&gt;
        &lt;cell&gt;➕➕&lt;/cell&gt;
        &lt;cell&gt;➕➕&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Text Only&lt;/p&gt;
    &lt;p&gt;This table only applies to text (same as the table above), non-text inputs can have a different performance (if they are not &lt;code&gt;comparable&lt;/code&gt; or readability).&lt;/p&gt;
    &lt;head rend="h3"&gt;API&lt;/head&gt;
    &lt;p&gt;To design this API, I started with the data structures that I wanted to use as a user of the API and worked backwards from there. At a very high level, there are two structured representations of a diff that have been useful to me: a flat sequence of all deletions, insertions, and matching elements (called edits) and a nested sequence of consecutive changes (called hunks).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Edits are what I use to represent edits in this article; they contain the full content of both inputs and how one is transformed into the other.&lt;/item&gt;
      &lt;item&gt;Hunks are a great representation for unit tests, because they are empty if both inputs are identical and they make it possible to visualize just the changes even if the inputs are large.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Arbitrary Slices&lt;/head&gt;
    &lt;p&gt;I started with the design for the most general case, arbitrary slices. The Go representation for diffing slices I liked the most is this one (see also znkr.io/diff):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Op describes an edit operation.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;type Op int
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;const (
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Match  Op = iota // Two slice elements match
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Delete           // A deletion from an element on the left slice
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Insert           // An insertion of an element from the right side
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Edit describes a single edit of a diff.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// - For Match, both X and Y contain the matching element.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// - For Delete, X contains the deleted element and Y is unset (zero value).
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;17&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// - For Insert, Y contains the inserted element and X is unset (zero value).
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;type Edit[T any] struct {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;19&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Op   Op
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;20&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	X, Y T
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;21&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;22&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;23&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Hunk describes a sequence of consecutive edits.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;24&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;type Hunk[T any] struct {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;25&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	PosX, EndX int       // Start and end position in x.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;26&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	PosY, EndY int       // Start and end position in y.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;27&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Edits      []Edit[T] // Edits to transform x[PosX:EndX] to y[PosY:EndY]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;28&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The alternatives I have seen are variations and combinations of two themes. Either using slices to represent edit operations in &lt;code&gt;Hunk&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;type Hunk[T any] struct {
	Delete []T
	Insert []T
	Match  []T
}
&lt;/code&gt;
    &lt;p&gt;Or using indices instead of elements&lt;/p&gt;
    &lt;code&gt;type Edit struct {
	Op         Op
	PosX, PosY []int
}
&lt;/code&gt;
    &lt;p&gt;All of these representations work, but I found that the representations above served my use cases best. One little quirk is that &lt;code&gt;Edit&lt;/code&gt; always contains both elements. This is often unnecessary, but
there are use cases where this is very important because the elements themselves might not be equal
(e.g., if they are pointers that are compared with a custom function).&lt;/p&gt;
    &lt;p&gt;Once the data structures were established, it was quite obvious that the simplest way to fill them with diff data was to write two functions &lt;code&gt;diff.Edits&lt;/code&gt; and
&lt;code&gt;diff.Hunks&lt;/code&gt; to return the diffs. I made them extensible by
using functional options.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;30&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Edits compares the contents of x and y and returns the changes necessary to convert from one to
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;31&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// the other.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;//
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;33&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Edits returns one edit for every element in the input slices. If x and y are identical, the
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;34&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// output will consist of a match edit for every input element.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;35&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func Edits[T comparable](x, y []T, opts ...Option) []Edit[T]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;36&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;37&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Hunks compares the contents of x and y and returns the changes necessary to convert from one to
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;38&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// the other.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;39&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;//
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;40&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// The output is a sequence of hunks. A hunk represents a contiguous block of changes (insertions
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;41&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// and deletions) along with some surrounding context.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;42&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func Hunks[T comparable](x, y []T, opts ...Option) []Hunk[T]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The options allow for future extensibility and allow changing the behavior of these functions. For example, the option &lt;code&gt;diff.Context(5)&lt;/code&gt; configures &lt;code&gt;Hunks&lt;/code&gt;
to provide 5 elements of surrounding context.&lt;/p&gt;
    &lt;p&gt;However, the current API still doesn't allow arbitrary slices; it only allows slices of &lt;code&gt;comparable&lt;/code&gt; types. To fix this, I needed two other functions that provide a function to compare
two elements. The Go standard library uses the &lt;code&gt;Func&lt;/code&gt; suffix for functions like this, so I followed
the lead:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;44&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// EditsFunc compares the contents of x and y using the provided equality comparison and returns the
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;45&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// changes necessary to convert from one to the other.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;46&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func EditsFunc[T any](x, y []T, eq func(a, b T) bool, opts ...Option) []Edit[T]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;47&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;48&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// HunksFunc compares the contents of x and y using the provided equality comparison and returns the
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;49&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// changes necessary to convert from one to the other.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func HunksFunc[T any](x, y []T, eq func(a, b T) bool, opts ...Option) []Hunk[T]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;Text&lt;/head&gt;
    &lt;p&gt;While this API works well to produce a structured result for arbitrary slices, it doesn't provide output in unified format for text inputs. My first approach was to provide a helper function that returns a diff in unified format: &lt;code&gt;diff.ToUnified(hunks []Hunk[string]) string&lt;/code&gt;. However, this would
make getting a unified diff more complicated. Besides requiring two function calls, it would be
necessary to split the input into lines. This, in turn, can be done in different ways, e.g., by
stripping or keeping the line breaks, which opens the door to mistakes. It's much better to provide
a simple function for the entire use case.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Unified compares the lines in x and y and returns the changes necessary to convert from one to
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// the other in unified format.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func Unified[T string | []byte](x, y T, opts ...diff.Option) T
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I also moved this function to the &lt;code&gt;textdiff&lt;/code&gt; package to
highlight the difference in expected input.&lt;/p&gt;
    &lt;p&gt;Now, I also happen to have use cases where I need structured results for text diffs. It would be very annoying if I had to split those into lines manually. Besides, I can make a few more assumptions about text that allow for a slight simplification of the data structures:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Edit describes a single edit of a line-by-line diff.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;type Edit[T string | []byte] struct {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Op   diff.Op // Edit operation
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Line T       // Line, including newline character (if any)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;17&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Hunk describes a sequence of consecutive edits.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;type Hunk[T string | []byte] struct {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;19&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	PosX, EndX int       // Start and end line in x (zero-based).
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;20&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	PosY, EndY int       // Start and end line in y (zero-based).
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;21&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Edits      []Edit[T] // Edits to transform x lines PosX..EndX to y lines PosY..EndY
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;22&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;23&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;24&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Edits compares the lines in x and y and returns the changes necessary to convert from one to the
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;25&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// other.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;26&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func Edits[T string | []byte](x, y T, opts ...diff.Option) []Edit[T]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;27&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;28&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Hunks compares the lines in x and y and returns the changes necessary to convert from one to the
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;29&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// other.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;30&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func Hunks[T string | []byte](x, y T, opts ...diff.Option) []Hunk[T]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;For the full API and examples for how to use it, please see the package documentation for znkr.io/diff and znkr.io/diff/textdiff. I am certain that there are use cases not covered by this API, but I feel confident that it can evolve to cover these use cases in the future. For now, all my needs are fulfilled, but if you run into a situation that can't be solved by this API or requires some contortions, please tell me about it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Implementation&lt;/head&gt;
    &lt;p&gt;To implement this API, we need to implement a diff algorithm. There are a couple of standard diff algorithms that we can choose from. The choice of the algorithm as well as how it's implemented matters for the readability of the result as well as the performance.&lt;/p&gt;
    &lt;p&gt;A good starting point for this project was Myers' algorithm, simply because it's the fastest algorithm that can cover the whole API. In particular, the &lt;code&gt;...Func&lt;/code&gt; variants for &lt;code&gt;any&lt;/code&gt; types
instead of &lt;code&gt;comparable&lt;/code&gt; can't make use of a hash map. Patience and Histogram require the use of a
hash map for an efficient implementation, so Myers' really is the only choice. Another advantage of
Myers' compared to Patience and Histogram is that it will return optimal results.&lt;/p&gt;
    &lt;p&gt;On the flip side, in the comparison above, it came out as relatively slow compared to the patience diff algorithm and didn't produce the most readable results. It turns out, however, that this can be mitigated and almost completely overcome for &lt;code&gt;comparable&lt;/code&gt; types using
a combination of preprocessing, heuristics, and post-processing.&lt;/p&gt;
    &lt;p&gt;I am not going to cover the diff algorithm in detail here. There are a number of excellent articles on the web that describe it12, but I recommend reading the paper13: All articles I have seen try to keep a distance from the theory that makes this algorithm work, but that's not really helpful if you want to understand how and why this algorithm works.&lt;/p&gt;
    &lt;head rend="h4"&gt;Preprocessing&lt;/head&gt;
    &lt;p&gt;The most impactful way to improve the performance of Myers' algorithm is to reduce the problem size. The simplest thing to do is to strip any common prefix and suffix. This is always possible and helps a little. However, it can also reduce diff readability, because it will consume matching elements eagerly.&lt;/p&gt;
    &lt;p&gt;For example, let's say we have this change:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;package array
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;var m = []struct{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    name  string
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    year  int
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        name: "Freak Out!",
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        year: 1966,
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    },
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        name: "Absolutely Free",
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        year: 1967,
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    },
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        name: "We're Only in It for the Money",
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;17&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        year: 1967,
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    },
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;19&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If we eagerly consume the common prefix first and then the common suffix, the first 11 lines are all identical and the so are the last 4. This in turn would result in a different diff:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;package array
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;var m = []struct{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    name  string
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    year  int
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        name: "Freak Out!",
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        year: 1966,
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    },
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        name: "Absolutely Free",
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        year: 1967,
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    },
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        name: "We're Only in It for the Money",
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;17&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        year: 1967,
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    },
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;19&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Fortunately, this is easy to fix in post processing.&lt;/p&gt;
    &lt;p&gt;Much more impactful, but only efficiently possible for &lt;code&gt;comparable&lt;/code&gt; types, is to remove all elements
that are unique to either the left side or the right side, as those must always be deletions or
insertions. Non-&lt;code&gt;comparable&lt;/code&gt; types can't be keys in a hash map in Go, which is necessary for
checking uniqueness. This preprocessing step reduced the runtime by up to
99% for a few
real-world worst-case diffs.&lt;/p&gt;
    &lt;p&gt;In contrast to the suffix and prefix removal, stripping unique elements doesn't have any readability impact.&lt;/p&gt;
    &lt;head rend="h4"&gt;Heuristics&lt;/head&gt;
    &lt;p&gt;Another very impactful way to improve the performance is Anchoring. It is based on patience diff. The word patience is a bit misleading, because it's too easily associated with having to wait and it doesn't describe the heuristic very well either. It works by finding elements that are occur exactly once on both the left and the right side. When we matching up these unique pairs we create a segmentation of the input into smaller parts that can be analyzed individually. Even better, we're very likely to find matching lines atop and below such a pair of unique elements. This allows us to shrink the segments by stripping common prefixes and suffixes. This heuristic reduced the runtime by up to 95%. Unfortunately, finding unique elements and matching them up requires a hash map again which means that it can only be used for &lt;code&gt;comparable&lt;/code&gt; types.&lt;/p&gt;
    &lt;p&gt;There are two more heuristics that are I implemented. They help for non-&lt;code&gt;comparable&lt;/code&gt; types and as a
backstop when the other heuristics don't work. Their main purpose is to avoid runaway quadratic
growth. The Good Diagonal heuristic stops searching for a better solution if we found a solution
that's good enough and the Too Expensive heuristic shortcuts the search if it becomes too
expensive which reduces the worst-case complexity from 

 to


.&lt;/p&gt;
    &lt;p&gt;However, heuristics like this trade diff minimality for performance, this is not always desirable. Sometimes, a minimal diff is exactly what's required. &lt;code&gt;diff.Optimal&lt;/code&gt; disables these heuristics to always find a
minimal diff irrespective of the costs.&lt;/p&gt;
    &lt;head rend="h4"&gt;Post-processing&lt;/head&gt;
    &lt;p&gt;We established before that a diff algorithm finds one of many possible solutions. Given such a solution we can discover more solutions by it locally and then selecting the best solution according to some metric. This is exactly how Michael Haggerty's indentation heuristic works for text.&lt;/p&gt;
    &lt;p&gt;For any given diff, we can often slide the edits up or down in a way that doesn't change the meaning of a diff. For example,&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;["foo", "bar", "baz"].map do |i|
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;  i
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;end
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;["foo", "bar", "baz"].map do |i|
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;  i.upcase
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;end
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;has the same meaning as&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;["foo", "bar", "baz"].map do |i|
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;  i
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;end
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;["foo", "bar", "baz"].map do |i|
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;  i.upcase
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;end&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We call edits that can be slid up or down sliders. The question is, how do we select the best slide? Michael collected human ratings for different sliders of the same diff and used them to develop a heuristic to match these ratings: diff-slider-tools.&lt;/p&gt;
    &lt;p&gt;However, this heuristic only works for text and is tuned towards code instead of prose. I decided to make it optional. It can be enabled with the &lt;code&gt;textdiff.IndentHeuristic&lt;/code&gt; option.&lt;/p&gt;
    &lt;head rend="h4"&gt;Diff Representation&lt;/head&gt;
    &lt;p&gt;The representation used during the execution of the diff algorithm has a surprising impact on the algorithm performance and result readability. This is not at all obvious, and so it took me a while to figure out that the best approach is akin to a side-by-side view of a diff: You use two &lt;code&gt;[]bool&lt;/code&gt;
slices to represent the left side and the right side respectively: &lt;code&gt;true&lt;/code&gt; in the left side slice
represents a deletion and on the right side an insertion. &lt;code&gt;false&lt;/code&gt; is a matching element.&lt;/p&gt;
    &lt;p&gt;This representation has four big advantages: It can be preallocated, the order in which edits are discovered doesn't matter, it's easy to mutate during post-processing, and it's easy to generate other representations from it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Open Questions&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What exactly is the reason that two different algorithms produce different results? - I looked into this question a little, but I haven't found a conclusive answer yet.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Diff algorithms are relatively complicated by themselves, but they pale in comparison to what's necessary to provide a high-quality diff library. This article tries to explain what went into my new diff library, but there's still more that I haven't implemented yet.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Here is one real-world example of why worst-case scenarios are important: Imagine you're breaking an existing feature in a way that triggers a worst-case scenario in a test. If the test is running for a very long time or runs out of memory, you're going to have to debug two problems instead of one. ↩︎ ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;See benchmark_comparison.txt for the source of these ratings. ↩︎ ↩︎ ↩︎ ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The diffmatchpatch API is very hard to use ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No support for structured results ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Quadratic memory use; for my test cases, this resulted in &amp;gt;30 GB of memory used. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The mb0 API is from before generics and is a bit cumbersome to use ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;udiff has a very low threshold for when it starts to stop searching for an optimal solution. This improves the speed, but it also results in relatively large diffs. ↩︎ ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;There's no single patience diff heuristic, instead there are different implementations with different performance characteristics. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Stolen from https://blog.jcoglan.com/2017/03/22/myers-diff-in-linear-space-theory/ ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I can recommend https://blog.robertelder.org/diff-algorithm/ and this 5 part series https://blog.jcoglan.com/2017/02/12/the-myers-diff-algorithm-part-1/ ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Myers, E.W. An O(ND) difference algorithm and its variations. Algorithmica 1, 251-266 (1986). https://doi.org/10.1007/BF01840446 ↩︎&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45430604</guid><pubDate>Tue, 30 Sep 2025 20:09:44 +0000</pubDate></item><item><title>Atuin Desktop: Runbooks That Run – Now Open Source</title><link>https://blog.atuin.sh/atuin-desktop-open-source/</link><description>&lt;doc fingerprint="2b3ee61c57b7e69a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Atuin Desktop: Runbooks that Run — Now Open Source&lt;/head&gt;
    &lt;p&gt;Atuin Desktop looks like a doc, but runs like your terminal. Script blocks, embedded terminals, database clients and prometheus charts - all in one place.&lt;/p&gt;
    &lt;p&gt;Most infrastructure is held together by five commands someone remembers when shit breaks. Docs are out of date, if they exist. The real answers? Buried in Slack threads, rotting in Notion, or trapped in someone's shell history.&lt;/p&gt;
    &lt;p&gt;Atuin CLI fixed part of this, with synced, searchable shell history. But history isn’t enough. Teams need workflows they can repeat, share, and trust.&lt;/p&gt;
    &lt;p&gt;That’s why we built Atuin Desktop. Runbooks that actually run. Now open beta, and fully open source.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is Atuin Desktop?&lt;/head&gt;
    &lt;p&gt;Atuin Desktop looks like a doc, but runs like your terminal. Built to make local developer workflows repeatable, shareable, and reliable.&lt;/p&gt;
    &lt;p&gt;Runbooks should run. Workflows shouldn't live in someone's head. Docs shouldn't rot the moment you write them. Scripts, database queries, HTTP requests and Prometheus charts - all in one place.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kill context switching: Chain shell scripts, database queries, and HTTP requests&lt;/item&gt;
      &lt;item&gt;Docs that don't rot: execute directly + stay relevant&lt;/item&gt;
      &lt;item&gt;Reusable automation: dynamic runbooks with Jinja-style templating&lt;/item&gt;
      &lt;item&gt;Local knowledge: Build runbooks from your real shell history&lt;/item&gt;
      &lt;item&gt;Collaborative: Sync and share via Git, or in real-time via our Hub&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Back in April we launched the closed beta.&lt;/p&gt;
    &lt;p&gt;Thousands of you signed up, used it at your day jobs, and told us exactly what broke. We’ve listened, rebuilt, and now it’s ready for everyone.&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s new since April?&lt;/head&gt;
    &lt;p&gt;Our early users gave us a lot of feedback, which we've used to build something much better.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Offline, file based, Git/VCS-compatible workspaces&lt;/item&gt;
      &lt;item&gt;Team accounts with shared, realtime workspaces&lt;/item&gt;
      &lt;item&gt;Kubernetes integration for live state and monitoring&lt;/item&gt;
      &lt;item&gt;MySQL query blocks&lt;/item&gt;
      &lt;item&gt;Dropdown and more contextual blocks&lt;/item&gt;
      &lt;item&gt;A huge number of bug fixes, performance improvements, and UI upgrades&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How it’s being used&lt;/head&gt;
    &lt;p&gt;Atuin Desktop is already being used across engineering teams for serious, day-to-day work.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Automation and debugging: linking commands, monitoring systems, and tracking results&lt;/item&gt;
      &lt;item&gt;Database operations: managing migrations, access control, and production queries&lt;/item&gt;
      &lt;item&gt;Onboarding: getting started workflows new engineers can actually run&lt;/item&gt;
      &lt;item&gt;Deploying and managing clusters: repeatable, documented automation for real environments&lt;/item&gt;
      &lt;item&gt;Incident response: runbooks that execute instead of rotting in some internal wiki&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It’s become a shared system of record for the commands and processes that keep production alive.&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s next&lt;/head&gt;
    &lt;p&gt;We’re just getting started! We've got a lot in the pipeline, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Block dependencies and advanced execution flow&lt;/item&gt;
      &lt;item&gt;Run runbooks remotely and on CI&lt;/item&gt;
      &lt;item&gt;Audit logs and enhanced permissions&lt;/item&gt;
      &lt;item&gt;Comments and deeper collaboration&lt;/item&gt;
      &lt;item&gt;More block types&lt;list rend="ul"&gt;&lt;item&gt;Specify local networks, containers, and more&lt;/item&gt;&lt;item&gt;Tighter integration with authentication and cloud providers&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;More polish, more speed, fewer bugs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Stop copy-pasting from outdated wiki pages, and get started with Atuin Desktop&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting involved&lt;/head&gt;
    &lt;p&gt;Atuin Desktop is now in open beta and open source under the Apache 2.0 license. Star it, fork it, break it: github.com/atuinsh/desktop&lt;/p&gt;
    &lt;p&gt;Infrastructure deserves better than rotting docs and tribal knowledge. Atuin Desktop is our attempt to fix that for everyone who’s ever said “I swear I’ve done this before.”&lt;/p&gt;
    &lt;head rend="h3"&gt;Community links&lt;/head&gt;
    &lt;p&gt;Discord: discord.gg/Fq8bJSKPHh&lt;/p&gt;
    &lt;p&gt;Forum: forum.atuin.sh&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45431001</guid><pubDate>Tue, 30 Sep 2025 20:44:30 +0000</pubDate></item><item><title>Mind the encryptionroot: How to save your data when ZFS loses its mind</title><link>https://sambowman.tech/blog/posts/mind-the-encryptionroot-how-to-save-your-data-when-zfs-loses-its-mind/</link><description>&lt;doc fingerprint="af8d194dd1ec8451"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mind the encryptionroot: How to save your data when ZFS loses its mind&lt;/head&gt;
    &lt;p&gt;While ZFS has a well-earned reputation for data integrity and reliability, OpenZFS native encryption has some incredibly sharp edges that will cut you if you don't know where to be careful. Unfortunately, I learned this the hard way, standing in a pool of my own blood and tears after thoroughly lacerating myself. I very nearly permanently lost 8.5 TiB of data after performing what should've been a series of simple, routine ZFS operations but resulted in an undecryptable dataset. Time has healed the wound enough that I am no longer filled with anguish just thinking about it, so I will now share my experience in the hope that you may learn from my mistakes. Together, we'll go over the unfortunate series of events that led to this happening and how it could've been avoided, learn how ZFS actually works under the hood, use our newfound knowledge to debug and reproduce the issue at hand, and finally compile a modified version of ZFS to repair the corrupted state and rescue our precious data. This is the postmortem of that terrible, horrible, no good, very bad weekâ¦&lt;/p&gt;
    &lt;head rend="h2"&gt;Table of Contents&lt;/head&gt;
    &lt;p&gt;Note: The issue covered in this postmortem only applies to OpenZFS native encryption. Oracle ZFS has its own encryption scheme which is different and, as far as I can tell, should not be vulnerable to this particular failure mode, though I have not personally tested it. Thank you to u/HobartTasmania for pointing this out!&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 1: An unfortunate series of events&lt;/head&gt;
    &lt;head rend="h3"&gt;The status quo&lt;/head&gt;
    &lt;p&gt; In the beginning, there were two ZFS pools: &lt;code&gt;old&lt;/code&gt; and &lt;code&gt;new&lt;/code&gt; (names changed for clarity). Each pool was hosted on an instance of TrueNAS CORE 13.0-U5.1 located at two different sites about an hour's drive apart with poor Internet connectivity between them. For this reason, a third pool &lt;code&gt;sneakernet&lt;/code&gt; was periodically moved between the two sites and used to exchange snapshots of &lt;code&gt;old&lt;/code&gt; and &lt;code&gt;new&lt;/code&gt; datasets for backup purposes. ZFS dataset snapshots would be indirectly relayed from &lt;code&gt;old&lt;/code&gt; to &lt;code&gt;new&lt;/code&gt; (and vice versa) using &lt;code&gt;sneakernet&lt;/code&gt; as an intermediate ZFS send/recv source/destination (e.g. &lt;code&gt;old/foo@2023-06-01&lt;/code&gt; -&amp;gt; &lt;code&gt;sneakernet/old/foo@2023-06-01&lt;/code&gt; -&amp;gt; &lt;code&gt;new/old/foo@2023-06-01&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt; The &lt;code&gt;new&lt;/code&gt; pool was natively encrypted from the very beginning. When ZFS snapshots were sent from &lt;code&gt;new&lt;/code&gt; to &lt;code&gt;sneakernet/new&lt;/code&gt; to &lt;code&gt;old/new&lt;/code&gt;, they were sent raw, meaning that blocks were copied unmodified in their encrypted form. To decrypt and mount them on &lt;code&gt;sneakernet&lt;/code&gt; or &lt;code&gt;old&lt;/code&gt;, you would need to first load &lt;code&gt;new&lt;/code&gt;'s hex encryption key, which is stored in TrueNAS's SQLite database.&lt;/p&gt;
    &lt;p&gt; The &lt;code&gt;old&lt;/code&gt; pool, on the other hand, was created before the advent of native encryption and was unencrypted for the first part of its life. Because it's desirable to encrypt data at rest, an encrypted dataset &lt;code&gt;sneakernet/old&lt;/code&gt; was created for &lt;code&gt;old&lt;/code&gt; using a passphrase encryption key when &lt;code&gt;sneakernet&lt;/code&gt; was set up. Unencrypted snapshots were sent non-raw from &lt;code&gt;old&lt;/code&gt; to &lt;code&gt;sneakernet/old&lt;/code&gt;, where they were encrypted, and then sent raw from &lt;code&gt;sneakernet/old&lt;/code&gt; to &lt;code&gt;new/old&lt;/code&gt;. To decrypt and mount them on &lt;code&gt;sneakernet&lt;/code&gt; or &lt;code&gt;new&lt;/code&gt;, you would need to first load &lt;code&gt;sneakernet&lt;/code&gt;'s passphrase encryption key.&lt;/p&gt;
    &lt;p&gt;This was all tested thoroughly and snapshots were proven to be readable at each point on every pool.&lt;/p&gt;
    &lt;head rend="h3"&gt;Encrypting the old pool&lt;/head&gt;
    &lt;p&gt; Now that we had encrypted snapshots of &lt;code&gt;old&lt;/code&gt; on &lt;code&gt;sneakernet/old&lt;/code&gt;, we wanted to encrypt &lt;code&gt;old&lt;/code&gt; itself. To do this, I simply took &lt;code&gt;old&lt;/code&gt; offline during a maintenance window to prevent new writes, took snapshots of all datasets, sent them to &lt;code&gt;sneakernet/old&lt;/code&gt;, and then sent the raw encrypted snapshots from &lt;code&gt;sneakernet/old&lt;/code&gt; back to &lt;code&gt;old/encrypted&lt;/code&gt;. Once I verified each dataset had been encrypted successfully, I destroyed the unencrypted dataset, updated the mount point of the encrypted dataset to that of the late unencrypted dataset, and then moved on to the next dataset. After all datasets were migrated, I used &lt;code&gt;zfs change-key -i&lt;/code&gt; to make all child datasets inherit from the new &lt;code&gt;old/encrypted&lt;/code&gt; encryption root, and then changed the key of the encryption root from a passphrase to a hex key, since TrueNAS only supported automatically unlocking datasets with hex encryption keys. Finally, I issued a &lt;code&gt;zpool initialize&lt;/code&gt; to overwrite all the unencrypted blocks which were now in unallocated space.&lt;/p&gt;
    &lt;p&gt; Spoiler Alert: It may not be immediately obvious why, but changing the encryption key on &lt;code&gt;old/encryption&lt;/code&gt; silently broke backups of &lt;code&gt;old&lt;/code&gt; datasets. Snapshots would still send and recv successfully, but were no longer decryptable or mountable. Since the encryption key is not normally loaded, and we only load it when periodically testing the backups, we would not realize until it was too late.&lt;/p&gt;
    &lt;p&gt;Lesson: Test backups continuously so you get immediate feedback when they break.&lt;/p&gt;
    &lt;head rend="h3"&gt;Decommissioning the old pool&lt;/head&gt;
    &lt;p&gt; Later, the &lt;code&gt;old&lt;/code&gt; pool was moved to the same site as the &lt;code&gt;new&lt;/code&gt; pool, so we wanted to fully decommission &lt;code&gt;old&lt;/code&gt; and migrate all its datasets to &lt;code&gt;new&lt;/code&gt;. I began going about this in a similar way. I took &lt;code&gt;old&lt;/code&gt; offline to prevent new writes, sent snapshots to &lt;code&gt;sneakernet/old&lt;/code&gt;, and then to &lt;code&gt;new/old&lt;/code&gt;. It was at this point that I made a very unfortunate mistake: I accidentally destroyed one dataset &lt;code&gt;old/encrypted/foo&lt;/code&gt; before verifying the files were readable on &lt;code&gt;new/old/foo&lt;/code&gt;, and I would soon realize that they were not.&lt;/p&gt;
    &lt;p&gt;Lesson: Wait to make all destructive changes together at the very end instead of interspersed where they could accidentally be performed in the wrong order.&lt;/p&gt;
    &lt;head rend="h3"&gt;The realization&lt;/head&gt;
    &lt;code&gt;[sam@newnas ~]$ DATASET=foo; [[ $(ssh sam@oldnas zfs list -H -o guid old/encrypted/${DATASET}@decomm) = $(zfs list -H -o guid sneakernet/old/${DATASET}@decomm) ]] &amp;amp;&amp;amp; echo "GUIDs match" || echo "GUIDs DO NOT MATCH"
GUIDs match
[sam@newnas ~]$ DATASET=foo; [[ $(zfs list -H -o guid sneakernet/old/${DATASET}@decomm) = $(zfs list -H -o guid new/old/${DATASET}@decomm) ]] &amp;amp;&amp;amp; echo "GUIDs match" || echo "GUIDs DO NOT MATCH"
GUIDs match

[sam@oldnas ~]$ sudo zfs destroy -r old/encrypted/foo

[sam@newnas ~]$ ls /mnt/new/old/foo
[sam@newnas ~]$ ls -a /mnt/new/old/foo
. ..
[sam@newnas ~]$ zfs list -o name,mounted new/old/foo
NAME         MOUNTED
new/old/foo  no
[sam@newnas ~]$ sudo zfs mount new/old/foo
cannot mount 'new/old/foo': Permission denied&lt;/code&gt;
    &lt;p&gt;What do you mean, permission denied? I am root!&lt;/p&gt;
    &lt;p&gt; Crap, I already destroyed &lt;code&gt;old/encrypted/foo&lt;/code&gt;. This is not good, but I can still restore it from the remaining copy on &lt;code&gt;sneakernet/old/foo&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;[sam@newnas ~]$ sudo zfs load-key sneakernet/old
Enter passphrase for 'sneakernet/old':
[sam@newnas ~]$ sudo zfs mount sneakernet/old/foo
cannot mount 'sneakernet/old/foo': Permission denied&lt;/code&gt;
    &lt;p&gt; Oh no, &lt;code&gt;sneakernet/old&lt;/code&gt; is broken too. This is very not good!&lt;/p&gt;
    &lt;p&gt;In an act of desperation, I tried rebooting the machine, but it didn't change a thing.&lt;/p&gt;
    &lt;p&gt;It is at this point that I realized:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Something has gone terribly wrong to prevent datasets on both &lt;code&gt;sneakernet/old&lt;/code&gt;and&lt;code&gt;new/old&lt;/code&gt;from mounting.&lt;/item&gt;
      &lt;item&gt;Whatever it is, it's not likely going to be easy to diagnose or fix.&lt;/item&gt;
      &lt;item&gt;There's a very real possibility the data might be gone forever.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I found myself in a hole and I wanted to stop digging. Fortunately, uptime was no longer critical for the &lt;code&gt;old&lt;/code&gt; datasets after the relocation, so I could afford to step away from the keyboard, collect my thoughts, and avoid making the situation any worse that it already was.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 2: Debugging the issue&lt;/head&gt;
    &lt;p&gt;Once the worst of the overwhelming, visceral feelings that come with the realization that you may have just caused permanent data loss had subsided, I started to work the incident and try to figure out why the backups aren't mounting.&lt;/p&gt;
    &lt;p&gt; As a precaution, I first exported the &lt;code&gt;old&lt;/code&gt; pool and took a forensic image of every disk in the pool. ZFS is a copy-on-write filesystem, so even though the dataset had been destroyed, most of the data was probably still on disk, just completely inaccessible with the normal ZFS tooling. In the worst case scenario, I may have had to try to forensically reconstruct the dataset from what was left on disk, and I didn't want to risk causing any more damage than I already had. Fortunately, I never had to use the disk images, but they still served as a valuable safety net while debugging and repairing.&lt;/p&gt;
    &lt;p&gt;Next, I realized that if we are to have any chance of debugging and fixing this issue, I need to learn how ZFS actually works.&lt;/p&gt;
    &lt;head rend="h3"&gt;Learning how ZFS actually works&lt;/head&gt;
    &lt;p&gt;I unfortunately did not keep track of every resource I consumed, but in addition to reading the source and docs, I found these talks by Jeff Bonwick, Bill Moore, and Matt Ahrens (the original creators of ZFS) to be particularly helpful in understanding the design and implementation of ZFS:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ZFS: The Last Word in File Systems Part 1&lt;/item&gt;
      &lt;item&gt;ZFS: The Last Word in File Systems Part 2&lt;/item&gt;
      &lt;item&gt;ZFS: The Last Word in File Systems Part 3&lt;/item&gt;
      &lt;item&gt;How ZFS Snapshots Really Work&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I highly recommend watching them all despite their age and somewhat poor recording quality, but will summarize the relevant information for those who don't have 3 hours to spare.&lt;/p&gt;
    &lt;p&gt;ZFS is a copy-on-write filesystem, which means that it does not overwrite blocks in place when a write is requested. Instead, the updated contents are written to a newly allocated block, and the old block is freed, which keeps the filesystem consistent if a write is interrupted. All blocks of both data and metadata are arranged in a Merkle tree structure where each block pointer contains a checksum of the child block, which allows ZFS to detect both block corruption and misdirected/phantom reads/writes. This means that any write will cause the block's checksum to change, which will then cause the parent block's checksum to change (since the parent block includes the block pointer which includes checksum of the child block that changed), and so on, all the way up to the root of the tree which ZFS calls an uberblock.&lt;/p&gt;
    &lt;p&gt;Uberblocks are written atomically, and because of the Merkle tree structure, they always represent a consistent snapshot of the entire filesystem at a point in time. Writes are batched together into transaction groups identified by a monotonically increasing counter, and each transaction group when synced to disk produces a new uberblock and associated filesystem tree. Taking a snapshot is then as simple as saving an uberblock and not freeing any of the blocks it points to.&lt;/p&gt;
    &lt;p&gt;In addition to the checksum, each block pointer also contains the transaction group id in which the child block was written, which is called the block's birth time or creation time. ZFS uses birth times to determine which blocks have been written before or after a snapshot. Any blocks with a birth time less than or equal to the snapshot's birth time, must have been written before the snapshot was taken, and conversely, any blocks with a birth time greater than the snapshot's birth time must have been written after the snapshot was taken.&lt;/p&gt;
    &lt;p&gt;One application of birth times is to generate incremental send streams between two snapshots. ZFS walks the tree but only needs to include blocks where the birth time is both greater than the first snapshot and less than or equal to the second snapshot. In fact, you don't even need to keep the data of the first snapshot aroundâyou can create a bookmark which saves the snapshot's transaction id (but none of the data blocks), delete the snapshot to free its data, and then use the bookmark as the source to generate the same incremental send stream.&lt;/p&gt;
    &lt;p&gt;Spoiler Alert: Chekhov's bookmark will become relevant later.&lt;/p&gt;
    &lt;head rend="h3"&gt;Learning how ZFS native encryption actually works&lt;/head&gt;
    &lt;p&gt;ZFS native encryption is a relatively new feature, which was first released in OpenZFS 0.8.0 (2019) and subsequently made it into FreeBSD 13.0 (2021) when OpenZFS was adopted.&lt;/p&gt;
    &lt;p&gt;In addition to the docs, I found this 2016 talk on ZFS Native Encryption by Tom Caputi (the original author of native encryption) to be helpful in understanding its design and implementation. Again, I will summarize the relevant information.&lt;/p&gt;
    &lt;p&gt; ZFS native encryption works by encrypting dataset blocks with an symmetric authenticated encryption cipher suite (AES-256-GCM by default). To use native encryption, you must create a new dataset with &lt;code&gt;-o encryption=on&lt;/code&gt; which generates a unique master key for the dataset. The dataset's master key is then used to derive block data encryption keys with a salted HKDF.&lt;/p&gt;
    &lt;p&gt; The master key can't be changed, so it is encrypted with a wrapping key which can be changed. The wrapping key is provided by the user with &lt;code&gt;zfs load-key&lt;/code&gt; and can be changed with &lt;code&gt;zfs change-key&lt;/code&gt; which re-encrypts the same master key with a new wrapping key.&lt;/p&gt;
    &lt;p&gt;The encrypted master keys are stored in each dataset since each dataset has its own master key, but the wrapping key parameters are stored on what is called the encryption root dataset. The encryption root may be the same encrypted dataset, or it may be a parent of the encrypted dataset. When a child encrypted dataset inherits from a parent encryption root, the encryption root's wrapping key is used to decrypt the child dataset's master key. This is how one key can be used to unlock a parent encryption root dataset and all child encrypted datasets that inherit from it at the same time instead of having to load a key for every single encrypted dataset.&lt;/p&gt;
    &lt;p&gt; In our case, &lt;code&gt;new&lt;/code&gt;, &lt;code&gt;sneakernet/new&lt;/code&gt;, &lt;code&gt;sneakernet/old&lt;/code&gt;, and &lt;code&gt;old/encrypted&lt;/code&gt; are the encryption roots, and all child encrypted datasets inherit from them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Forming a hypothesis&lt;/head&gt;
    &lt;p&gt;At this point, we now know enough to form a hypothesis as to what may have happened. Feel free to pause here and try to figure it out on your own.&lt;/p&gt;
    &lt;p&gt; Recall that &lt;code&gt;sneakernet/old&lt;/code&gt; was created using a passphrase encryption key, and &lt;code&gt;old/encrypted&lt;/code&gt; was created by raw sending &lt;code&gt;sneakernet/old&lt;/code&gt;, so it initially used the same passphrase derived wrapping encryption key. When the &lt;code&gt;old/encrypted&lt;/code&gt; encryption key was changed from a passphrase to a hex key, ZFS must have changed the wrapping key parameters on the &lt;code&gt;old/encrypted&lt;/code&gt; encryption root and re-encrypted all child encrypted dataset master keys with the new hex wrapping key. Crucially, a new snapshot of &lt;code&gt;old/encrypted&lt;/code&gt; was never taken and sent to &lt;code&gt;sneakernet/old&lt;/code&gt; because it ostensibly didn't contain any data and was just a container for the child datasets.&lt;/p&gt;
    &lt;p&gt; Hypothesis: When subsequent snapshots were sent from &lt;code&gt;old&lt;/code&gt; to &lt;code&gt;sneakernet&lt;/code&gt;, the master keys of the child encrypted datasets were updated to be encrypted with the new hex wrapping key, but the &lt;code&gt;sneakernet/old&lt;/code&gt; encryption root was never updated with the new hex wrapping key parameters because a new snapshot was never sent. Therefore, when we load the key for &lt;code&gt;sneakernet/old&lt;/code&gt;, ZFS asks for the old passphrase, not a hex key, and when we try to mount &lt;code&gt;sneakernet/old/foo&lt;/code&gt;, it tries and fails to decrypt its master key with the old passphrase wrapping key instead of the new hex wrapping key.&lt;/p&gt;
    &lt;p&gt;If correct, this would explain the behavior we're seeing. To test this hypothesis, let's try to reproduce the issue in a test environment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Creating a test environment&lt;/head&gt;
    &lt;p&gt;TrueNAS CORE 13.0-U5.1 is based on FreeBSD 13.1, despite the different minor version numbers, so we'll create a FreeBSD 13.1 VM to test in. Make sure to include the system source tree and install on UFS so that we can build OpenZFS and reload the ZFS kernel module without rebooting.&lt;/p&gt;
    &lt;p&gt;TrueNAS CORE 13.0-U5.1 uses ZFS 2.1.11, so we'll want to build the same version from source for consistency. I started by reading the Building ZFS guide and following the steps documented there with some small modifications for FreeBSD since the page was clearly written with Linux in mind.&lt;/p&gt;
    &lt;p&gt;First, install the dependencies we'll need.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo pkg install autoconf automake autotools git gmake python devel/py-sysctl sudo&lt;/code&gt;
    &lt;p&gt;Then, clone ZFS and check out tag zfs-2.1.11.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ git clone https://github.com/openzfs/zfs
sam@zfshax:~ $ cd zfs
sam@zfshax:~/zfs $ git checkout zfs-2.1.11
sam@zfshax:~/zfs $ git show --summary
commit e25f9131d679692704c11dc0c1df6d4585b70c35 (HEAD, tag: zfs-2.1.11)
Author: Tony Hutter &amp;lt;hutter2@llnl.gov&amp;gt;
Date:   Tue Apr 18 11:44:34 2023 -0700

    Tag zfs-2.1.11

    META file and changelog updated.

    Signed-off-by: Tony Hutter &amp;lt;hutter2@llnl.gov&amp;gt;&lt;/code&gt;
    &lt;p&gt;Now, configure, build, and install ZFS.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~/zfs $ sh autogen.sh
sam@zfshax:~/zfs $ ./configure
sam@zfshax:~/zfs $ gmake -s -j$(sysctl -n hw.ncpu)    # &amp;lt;-- modified for FreeBSD
sam@zfshax:~/zfs $ sudo gmake install; sudo ldconfig  # &amp;lt;-- modified for FreeBSD&lt;/code&gt;
    &lt;p&gt;Then, replace the FreeBSD's ZFS kernel module with the one we just built.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~/zfs $ sudo kldunload zfs.ko  # Needed because zfs.sh only unloads openzfs.ko
sam@zfshax:~/zfs $ sudo ./scripts/zfs.sh&lt;/code&gt;
    &lt;p&gt;Finally, verify we're running version 2.1.11 as desired.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~/zfs $ sudo zfs version
zfs-2.1.11-1
zfs-kmod-2.1.11-1&lt;/code&gt;
    &lt;head rend="h3"&gt;Reproducing the issue&lt;/head&gt;
    &lt;p&gt;Now we're ready to try reproducing the issue. This took some iteration to get right, so I wrote a bash script that starts from scratch on each invocation and then runs the commands needed to reproduce the corrupt state. After quite a bit of trial and error, I eventually produced a reproducer script which does the following:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Create 2 pools: &lt;code&gt;src&lt;/code&gt;and&lt;code&gt;dst&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Create &lt;code&gt;src/encryptionroot&lt;/code&gt;using a passphrase encryption key.&lt;/item&gt;
      &lt;item&gt;Create &lt;code&gt;src/encryptionroot/child&lt;/code&gt;which inherits&lt;code&gt;src/encryptionroot&lt;/code&gt;as its encryption root.&lt;/item&gt;
      &lt;item&gt;Create files and take snapshots &lt;code&gt;src/encryptionroot@111&lt;/code&gt;and&lt;code&gt;src/encryptionroot/child@111&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Send raw snapshots &lt;code&gt;src/encryptionroot@111&lt;/code&gt;and&lt;code&gt;src/encryptionroot/child@111&lt;/code&gt;to&lt;code&gt;dst/encryptionroot&lt;/code&gt;and&lt;code&gt;dst/encryptionroot/child&lt;/code&gt;respectively.&lt;/item&gt;
      &lt;item&gt;Load encryption key for &lt;code&gt;dst/encryptionroot&lt;/code&gt;using passphrase and mount encrypted datasets&lt;code&gt;dst/encryptionroot&lt;/code&gt;and&lt;code&gt;dst/encryptionroot/child&lt;/code&gt;. At this point,&lt;code&gt;src&lt;/code&gt;and&lt;code&gt;dst&lt;/code&gt;pools are in sync.&lt;/item&gt;
      &lt;item&gt;Change the &lt;code&gt;src/encryptionroot&lt;/code&gt;encryption key from passphrase to hex.&lt;/item&gt;
      &lt;item&gt;Update files and take snapshots &lt;code&gt;src/encryptionroot@222&lt;/code&gt;and&lt;code&gt;src/encryptionroot/child@222&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Send a raw incremental snapshot of &lt;code&gt;src/encryptionroot/child@222&lt;/code&gt;to&lt;code&gt;dst/encryptionroot/child&lt;/code&gt;, but do not send&lt;code&gt;src/encryptionroot@222&lt;/code&gt;which contains the key change!&lt;/item&gt;
      &lt;item&gt;Unmount &lt;code&gt;dst/encryptionroot&lt;/code&gt;and&lt;code&gt;dst/encryptionroot/child&lt;/code&gt;and unload the cached encryption key for&lt;code&gt;dst/encryptionroot&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Load the encryption key for &lt;code&gt;dst/encryptionroot&lt;/code&gt;using the passphrase since we didn't send the updated encryption root after changing the key.&lt;/item&gt;
      &lt;item&gt;Try to remount &lt;code&gt;dst/encryptionroot&lt;/code&gt;and&lt;code&gt;dst/encryptionroot/child&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When we run the reproducer, the root encrypted dataset &lt;code&gt;dst/encryptionroot&lt;/code&gt; mounts successfully and we can read the old file from the first snapshot, but the child encrypted dataset &lt;code&gt;dst/encryptionroot/child&lt;/code&gt; fails to mount with &lt;code&gt;cannot mount 'dst/encryptionroot/child: Permission denied&lt;/code&gt; just as we expected.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo ./reproduce &amp;gt; /dev/null 2&amp;gt;&amp;amp;1
sam@zfshax:~ $ sudo zfs mount dst/encryptionroot/child
cannot mount 'dst/encryptionroot/child': Permission denied&lt;/code&gt;
    &lt;head title="Click to interact"&gt;Full reproducer script output (long!)&lt;/head&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo ./reproduce

Destroy pools and backing files if they exist.
+ zpool destroy src
+ zpool destroy dst
+ rm -f /src.img
+ rm -f /dst.img

Create pools using sparse files.
+ truncate -s 100M /src.img
+ truncate -s 100M /dst.img
+ zpool create -o ashift=12 -m /src src /src.img
+ zpool create -o ashift=12 -m /dst dst /dst.img

Create root encrypted dataset using a passphrase encryption key.
+ echo 'hunter2!'
+ zfs create -o encryption=on -o keyformat=passphrase -o keylocation=prompt src/encryptionroot

Create child encrypted dataset which inherits src/encryptionroot as its encryption root.
+ zfs create src/encryptionroot/child

Create files in the root and child encrypted datasets and snapshot both.
+ touch /src/encryptionroot/111
+ touch /src/encryptionroot/child/111
+ zfs snapshot -r src/encryptionroot@111

[ Checkpoint 1 ] Files and snapshots are on the src pool but not the dst pool yet.

NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
src                           -                   none        -            yes      1354282934008960312
src/encryptionroot            src/encryptionroot  passphrase  available    yes      12828913232342655944
src/encryptionroot@111        src/encryptionroot  -           available    -        14453618123048176778
src/encryptionroot/child      src/encryptionroot  passphrase  available    yes      10447093816713688124
src/encryptionroot/child@111  src/encryptionroot  -           available    -        10173467213034806911
NAME  ENCROOT  KEYFORMAT   KEYSTATUS    MOUNTED   GUID
dst   -        none        -            yes      5247064584420489120
/src
âââ encryptionroot
    âââ 111
    âââ child
        âââ 111
/dst

Send a raw replication stream of the src snapshots to the dst pool.
+ zfs send --replicate --raw src/encryptionroot@111
+ zfs recv dst/encryptionroot

Load encryption key for the dst encryption root using passphrase and mount the encrypted datasets.
+ echo 'hunter2!'
+ zfs load-key dst/encryptionroot
+ zfs mount dst/encryptionroot
+ zfs mount dst/encryptionroot/child

[ Checkpoint 2 ] Files and snapshots are on both pools and in sync.

NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
src                           -                   none        -            yes      1354282934008960312
src/encryptionroot            src/encryptionroot  passphrase  available    yes      12828913232342655944
src/encryptionroot@111        src/encryptionroot  -           available    -        14453618123048176778
src/encryptionroot/child      src/encryptionroot  passphrase  available    yes      10447093816713688124
src/encryptionroot/child@111  src/encryptionroot  -           available    -        10173467213034806911
NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
dst                           -                   none        -            yes      5247064584420489120
dst/encryptionroot            dst/encryptionroot  passphrase  available    yes      3076413147413645477
dst/encryptionroot@111        dst/encryptionroot  -           available    -        14453618123048176778
dst/encryptionroot/child      dst/encryptionroot  passphrase  available    yes      18246034838646533510
dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        10173467213034806911
/src
âââ encryptionroot
    âââ 111
    âââ child
        âââ 111
/dst
âââ encryptionroot
    âââ 111
    âââ child
        âââ 111

Change the src encryption root key from passphrase to hex.
+ echo 0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef
+ zfs change-key -o keyformat=hex src/encryptionroot

Update the files in the root and child encrypted datasets and snapshot both.
+ mv /src/encryptionroot/111 /src/encryptionroot/222
+ mv /src/encryptionroot/child/111 /src/encryptionroot/child/222
+ zfs snapshot -r src/encryptionroot@222

[ Checkpoint 3 ] Updated files and snapshots are on the src pool but not the dst pool yet.

NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
src                           -                   none        -            yes      1354282934008960312
src/encryptionroot            src/encryptionroot  hex         available    yes      12828913232342655944
src/encryptionroot@111        src/encryptionroot  -           available    -        14453618123048176778
src/encryptionroot@222        src/encryptionroot  -           available    -        929742392566496732
src/encryptionroot/child      src/encryptionroot  hex         available    yes      10447093816713688124
src/encryptionroot/child@111  src/encryptionroot  -           available    -        10173467213034806911
src/encryptionroot/child@222  src/encryptionroot  -           available    -        8161419639883744346
NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
dst                           -                   none        -            yes      5247064584420489120
dst/encryptionroot            dst/encryptionroot  passphrase  available    yes      3076413147413645477
dst/encryptionroot@111        dst/encryptionroot  -           available    -        14453618123048176778
dst/encryptionroot/child      dst/encryptionroot  passphrase  available    yes      18246034838646533510
dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        10173467213034806911
/src
âââ encryptionroot
    âââ 222
    âââ child
        âââ 222
/dst
âââ encryptionroot
    âââ 111
    âââ child
        âââ 111

Send a raw incremental snapshot of the child encrypted dataset to the dst pool.
+ zfs send --raw -i src/encryptionroot/child@111 src/encryptionroot/child@222
+ zfs recv -F dst/encryptionroot/child

NOTE: The encryption key change on the src encryption root has not been sent to dst!

[ Checkpoint 4 ] File is updated in the dst child encrypted dataset but not the dst root encrypted dataset.

NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
src                           -                   none        -            yes      1354282934008960312
src/encryptionroot            src/encryptionroot  hex         available    yes      12828913232342655944
src/encryptionroot@111        src/encryptionroot  -           available    -        14453618123048176778
src/encryptionroot@222        src/encryptionroot  -           available    -        929742392566496732
src/encryptionroot/child      src/encryptionroot  hex         available    yes      10447093816713688124
src/encryptionroot/child@111  src/encryptionroot  -           available    -        10173467213034806911
src/encryptionroot/child@222  src/encryptionroot  -           available    -        8161419639883744346
NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
dst                           -                   none        -            yes      5247064584420489120
dst/encryptionroot            dst/encryptionroot  passphrase  available    yes      3076413147413645477
dst/encryptionroot@111        dst/encryptionroot  -           available    -        14453618123048176778
dst/encryptionroot/child      dst/encryptionroot  hex         available    yes      18246034838646533510
dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        10173467213034806911
dst/encryptionroot/child@222  dst/encryptionroot  -           available    -        8161419639883744346
/src
âââ encryptionroot
    âââ 222
    âââ child
        âââ 222
/dst
âââ encryptionroot
    âââ 111
    âââ child
        âââ 222

NOTE: The updated file in the dst child encrypted dataset is only still readable because the encryption key is still loaded from before sending the snapshot taken after the key change.

Unmount the dst encrypted datasets and and unload the cached encryption key.
+ zfs unmount dst/encryptionroot
+ zfs unload-key dst/encryptionroot

Load the encryption key for the dst encryption root using the passphrase since we did not send the updated encryption root after changing the key.
+ echo 'hunter2!'
+ zfs load-key dst/encryptionroot

Try to remount dst encrypted datasets.
+ zfs mount dst/encryptionroot
+ zfs mount dst/encryptionroot/child
cannot mount 'dst/encryptionroot/child': Permission denied
+ true

[ Checkpoint 5 ] Mounting dst child encrypted dataset failed even though encryption key is ostensibly available. Hypothesis confirmed!

NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
src                           -                   none        -            yes      1354282934008960312
src/encryptionroot            src/encryptionroot  hex         available    yes      12828913232342655944
src/encryptionroot@111        src/encryptionroot  -           available    -        14453618123048176778
src/encryptionroot@222        src/encryptionroot  -           available    -        929742392566496732
src/encryptionroot/child      src/encryptionroot  hex         available    yes      10447093816713688124
src/encryptionroot/child@111  src/encryptionroot  -           available    -        10173467213034806911
src/encryptionroot/child@222  src/encryptionroot  -           available    -        8161419639883744346
NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
dst                           -                   none        -            yes      5247064584420489120
dst/encryptionroot            dst/encryptionroot  passphrase  available    yes      3076413147413645477
dst/encryptionroot@111        dst/encryptionroot  -           available    -        14453618123048176778
dst/encryptionroot/child      dst/encryptionroot  hex         available    no       18246034838646533510
dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        10173467213034806911
dst/encryptionroot/child@222  dst/encryptionroot  -           available    -        8161419639883744346
/src
âââ encryptionroot
    âââ 222
    âââ child
        âââ 222
/dst
âââ encryptionroot
    âââ 111
    âââ child&lt;/code&gt;
    &lt;p&gt;Now that we understand and can reliably reproduce the issue, we're a big step closer to fixing it!&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 3: Recovering our data&lt;/head&gt;
    &lt;head rend="h3"&gt;Theoretically easy to fix&lt;/head&gt;
    &lt;p&gt;We know now that a child encrypted dataset will become unmountable if the following conditions are met:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The wrapping encryption key on the encryption root is changed.&lt;/item&gt;
      &lt;item&gt;A snapshot of the child encrypted dataset that was taken after the key change is sent.&lt;/item&gt;
      &lt;item&gt;A snapshot of the encryption root that was taken after the key change is not sent.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Lesson: Always send a snapshot of the encryption root after changing the encryption key.&lt;/p&gt;
    &lt;p&gt;In theory, all we should have to do to fix it is send the latest snapshot of the encryption root.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo ./reproduce &amp;gt; /dev/null 2&amp;gt;&amp;amp;1
sam@zfshax:~ $ sudo ./repair_snapshot

HYPOTHESIS: The child encrypted dataset should become decryptable again if a snapshot containing the key change on the root encrypted dataset is sent.

Send a raw incremental snapshot of the root encrypted dataset to the dst pool.
+ zfs send --raw -i src/encryptionroot@111 src/encryptionroot@222
+ zfs recv -F dst/encryptionroot

Unmount the dst encrypted datasets and and unload the cached encryption key.
+ zfs unmount dst/encryptionroot
+ zfs unload-key dst/encryptionroot

Load the encryption key for the dst encryption root using the hex key since we have now sent the updated encryption root after changing the key.
+ echo 0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef
+ zfs load-key dst/encryptionroot

Try to remount dst encrypted datasets.
+ zfs mount dst/encryptionroot
+ zfs mount dst/encryptionroot/child

RESULT: Child encrypted dataset is decryptable again. Hypothesis confirmed!

NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
src                           -                   none        -            yes      3822096046979704342
src/encryptionroot            src/encryptionroot  hex         available    yes      10687499872806328230
src/encryptionroot@111        src/encryptionroot  -           available    -        16650389156603898046
src/encryptionroot@222        src/encryptionroot  -           available    -        157927145464667221
src/encryptionroot/child      src/encryptionroot  hex         available    yes      15788284772663365294
src/encryptionroot/child@111  src/encryptionroot  -           available    -        8879828033920251704
src/encryptionroot/child@222  src/encryptionroot  -           available    -        6286619359795670820
NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
dst                           -                   none        -            yes      1835983340793043086
dst/encryptionroot            dst/encryptionroot  hex         available    yes      6911130245015256647
dst/encryptionroot@111        dst/encryptionroot  -           available    -        16650389156603898046
dst/encryptionroot@222        dst/encryptionroot  -           available    -        157927145464667221
dst/encryptionroot/child      dst/encryptionroot  hex         available    yes      15804809318195285947
dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        8879828033920251704
dst/encryptionroot/child@222  dst/encryptionroot  -           available    -        6286619359795670820
/src
âââ encryptionroot
    âââ 222
    âââ child
        âââ 222
/dst
âââ encryptionroot
    âââ 222
    âââ child
        âââ 222&lt;/code&gt;
    &lt;head rend="h3"&gt;Not so easy in practice&lt;/head&gt;
    &lt;p&gt; Unfortunately, this isn't enough to fix &lt;code&gt;new&lt;/code&gt; and &lt;code&gt;sneakernet&lt;/code&gt;; there are no remaining snapshots or bookmarks left on the &lt;code&gt;old&lt;/code&gt; encryption root from before the key change, and we can't generate an incremental send stream without one. Mapped to our reproduced example, this means that &lt;code&gt;src/encryptionroot@111&lt;/code&gt; does not exist.&lt;/p&gt;
    &lt;p&gt; You might think we could forcibly send the entire encryption root, but &lt;code&gt;zfs recv&lt;/code&gt; will reject it no matter what you do.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo zfs send --raw src/encryptionroot@222 | sudo zfs recv dst/encryptionroot
cannot receive new filesystem stream: destination 'dst/encryptionroot' exists
must specify -F to overwrite it

sam@zfshax:~ $ sudo zfs send --raw src/encryptionroot@222 | sudo zfs recv -F dst/encryptionroot
cannot receive new filesystem stream: destination has snapshots (eg. dst/encryptionroot@111)
must destroy them to overwrite it

sam@zfshax:~ $ sudo zfs destroy dst/encryptionroot@111
sam@zfshax:~ $ sudo zfs send --raw src/encryptionroot@222 | sudo zfs recv -F dst/encryptionroot
cannot receive new filesystem stream: zfs receive -F cannot be used to destroy an encrypted filesystem or overwrite an unencrypted one with an encrypted one&lt;/code&gt;
    &lt;p&gt;Lesson: Create bookmarks before destroying snapshots.&lt;/p&gt;
    &lt;p&gt;We need to find a way to create an incremental send stream that contains the key change, but how?. We could try to manually craft a send stream containing the new key, but that sounds tricky. There's got to be a better way!&lt;/p&gt;
    &lt;head rend="h3"&gt;Idea for a hack&lt;/head&gt;
    &lt;p&gt;Recall that a snapshot is not the only valid source for generating an incremental send stream. What if we had a bookmark?&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo ./reproduce &amp;gt; /dev/null 2&amp;gt;&amp;amp;1
sam@zfshax:~ $ sudo ./repair_bookmark

Replace the initial parent encrypted dataset snapshot with a bookmark.
+ zfs bookmark src/encryptionroot@111 src/encryptionroot#111
+ zfs destroy src/encryptionroot@111

HYPOTHESIS: The child encrypted dataset should become decryptable again if a snapshot containing the key change on the root encrypted dataset is sent.

Send a raw incremental snapshot of the root encrypted dataset to the dst pool using the bookmark.
+ zfs send --raw -i src/encryptionroot#111 src/encryptionroot@222
+ zfs recv -F dst/encryptionroot

Unmount the dst encrypted datasets and and unload the cached encryption key.
+ zfs unmount dst/encryptionroot
+ zfs unload-key dst/encryptionroot

Load the encryption key for the dst encryption root using the hex key since we have now sent the updated encryption root after changing the key.
+ echo 0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef
+ zfs load-key dst/encryptionroot

Try to remount dst encrypted datasets.
+ zfs mount dst/encryptionroot
+ zfs mount dst/encryptionroot/child

RESULT: Child encrypted dataset is decryptable again. Hypothesis confirmed!

NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
src                           -                   none        -            yes      1018261135296547862
src/encryptionroot            src/encryptionroot  hex         available    yes      1985286651877572312
src/encryptionroot@222        src/encryptionroot  -           available    -        4582898506955533479
src/encryptionroot#111        -                   -           -            -        4964628655505655411
src/encryptionroot/child      src/encryptionroot  hex         available    yes      12927592016081051429
src/encryptionroot/child@111  src/encryptionroot  -           available    -        15551239789901400488
src/encryptionroot/child@222  src/encryptionroot  -           available    -        11729357375613972731
NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
dst                           -                   none        -            yes      15258247229701443799
dst/encryptionroot            dst/encryptionroot  hex         available    yes      17755083343181277380
dst/encryptionroot@111        dst/encryptionroot  -           available    -        4964628655505655411
dst/encryptionroot@222        dst/encryptionroot  -           available    -        4582898506955533479
dst/encryptionroot/child      dst/encryptionroot  hex         available    yes      364333975888407846
dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        15551239789901400488
dst/encryptionroot/child@222  dst/encryptionroot  -           available    -        11729357375613972731
/src
âââ encryptionroot
    âââ 222
    âââ child
        âââ 222
/dst
âââ encryptionroot
    âââ 222
    âââ child
        âââ 222&lt;/code&gt;
    &lt;p&gt; A bookmark works just as well as a snapshot for generating an incremental send stream, but we don't have a bookmark on &lt;code&gt;old&lt;/code&gt; either. How is this any better?&lt;/p&gt;
    &lt;p&gt;Unlike a snapshot, which is effectively an entire dataset tree frozen in time (very complex), a bookmark is a very simple object on disk which consists of:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The GUID of the snapshot.&lt;/item&gt;
      &lt;item&gt;The transaction group the snapshot was created in.&lt;/item&gt;
      &lt;item&gt;The Unix timestamp when the snapshot was created.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example, this is what our bookmark looks like in &lt;code&gt;zdb&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo zdb src/encryptionroot#111
	#111: {guid: 44e5e7755d23c673 creation_txg: 12 creation_time: 1756699200 redaction_obj: 0}&lt;/code&gt;
    &lt;p&gt; Note that &lt;code&gt;zdb&lt;/code&gt; shows the GUID in hexadecimal versus &lt;code&gt;zfs get guid&lt;/code&gt; which shows it in decimal, consistency be damned. The &lt;code&gt;redaction_obj&lt;/code&gt; is optional and only used for redaction bookmarks, so we can ignore it.&lt;/p&gt;
    &lt;p&gt;A bookmark is simple enough that we could feasibly hack ZFS into manually writing one for us, provided that we can figure out the right values to use. The GUID and Unix timestamp don't really matter for generating an incremental send stream, so we could choose them arbitrarily if we had to, but the transaction group id really matters because that is what ZFS uses to determine which blocks to include.&lt;/p&gt;
    &lt;p&gt;But how can we figure out what transaction group the snapshot was created in if neither the snapshot nor a bookmark of the snapshot still exist? I initially considered walking the dataset trees on each pool, diffing them to find the newest block present on both datasets, and using its transaction group id, but I found a much easier way with one of ZFS's lesser known features.&lt;/p&gt;
    &lt;head rend="h3"&gt;A brief detour into pool histories&lt;/head&gt;
    &lt;p&gt;I didn't know about pool histories before embarking on this unplanned journey, but they are now yet another thing I love about ZFS. Every pool allocates 0.1% of its space (128 KiB minimum, 1 GiB maximum) to a ring buffer which is used to log every command that is executed on the pool. This can be used to forensically reconstruct the state of the pool over time.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo zpool history
History for 'dst':
2025-09-01.00:00:00 zpool create -o ashift=12 -m /dst dst /dst.img
2025-09-01.00:00:00 zfs recv dst/encryptionroot
2025-09-01.00:00:00 zfs load-key dst/encryptionroot
2025-09-01.00:00:00 zfs recv -F dst/encryptionroot/child
2025-09-01.00:00:00 zfs unload-key dst/encryptionroot
2025-09-01.00:00:00 zfs load-key dst/encryptionroot

History for 'src':
2025-09-01.00:00:00 zpool create -o ashift=12 -m /src src /src.img
2025-09-01.00:00:00 zfs create -o encryption=on -o keyformat=passphrase -o keylocation=prompt src/encryptionroot
2025-09-01.00:00:00 zfs create src/encryptionroot/child
2025-09-01.00:00:00 zfs snapshot -r src/encryptionroot@111
2025-09-01.00:00:00 zfs send --replicate --raw src/encryptionroot@111
2025-09-01.00:00:00 zfs change-key -o keyformat=hex src/encryptionroot
2025-09-01.00:00:00 zfs snapshot -r src/encryptionroot@222&lt;/code&gt;
    &lt;p&gt; ZFS also logs many internal operations in the pool history (search for &lt;code&gt;spa_history_log&lt;/code&gt; in the source code) which can be viewed with the &lt;code&gt;-i&lt;/code&gt; flag. For snapshots, this includes the transaction group (txg) id when the snapshot was created, which is exactly what we're looking for!&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo zpool history -i src
History for 'src':
...
2025-09-01.00:00:00 [txg:12] snapshot src/encryptionroot@111 (768)
2025-09-01.00:00:00 [txg:12] snapshot src/encryptionroot/child@111 (770)
2025-09-01.00:00:00 (3ms) ioctl snapshot
    input:
        snaps:
            src/encryptionroot@111
            src/encryptionroot/child@111
        props:

2025-09-01.00:00:00 zfs snapshot -r src/encryptionroot@111
...&lt;/code&gt;
    &lt;p&gt; The GUID and creation timestamp we can easily get from the snapshot on &lt;code&gt;dst&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo zfs list -o name,guid,creation -p dst/encryptionroot@111
NAME                                   GUID  CREATION
dst/encryptionroot@111  4964628655505655411  1756699200&lt;/code&gt;
    &lt;p&gt;Now that we know everything we need to create the bookmark, we just need to figure out a way to manually create a bookmark with arbitrary data.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hacking ZFS to manually create a bookmark&lt;/head&gt;
    &lt;p&gt; To understand how ZFS creates a bookmark, we can trace the code path from &lt;code&gt;zfs bookmark&lt;/code&gt; all the way down to &lt;code&gt;dsl_bookmark_add&lt;/code&gt; which actually adds the bookmark node to the tree.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;command_table&lt;/item&gt;
      &lt;item&gt;zfs_do_bookmark&lt;/item&gt;
      &lt;item&gt;lzc_bookmark&lt;/item&gt;
      &lt;item&gt;lzc_ioctl&lt;/item&gt;
      &lt;item&gt;zfs_ioctl_fd&lt;/item&gt;
      &lt;item&gt;zcmd_ioctl_compat&lt;/item&gt;
      &lt;item&gt;zfs_ioctl_register bookmark&lt;/item&gt;
      &lt;item&gt;zfs_ioc_bookmark&lt;/item&gt;
      &lt;item&gt;dsl_bookmark_create&lt;/item&gt;
      &lt;item&gt;dsl_bookmark_create_sync&lt;/item&gt;
      &lt;item&gt;dsl_bookmark_create_sync_impl_book&lt;/item&gt;
      &lt;item&gt;dsl_bookmark_node_add&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is the bookmark structure physically written to disk:&lt;/p&gt;
    &lt;p&gt;zfs/include/sys/dsl_bookmark.h&lt;/p&gt;
    &lt;code&gt;/*
 * On disk zap object.
 */
typedef struct zfs_bookmark_phys {
	uint64_t zbm_guid;		/* guid of bookmarked dataset */
	uint64_t zbm_creation_txg;	/* birth transaction group */
	uint64_t zbm_creation_time;	/* bookmark creation time */

	/* fields used for redacted send / recv */
	uint64_t zbm_redaction_obj;	/* redaction list object */
	uint64_t zbm_flags;		/* ZBM_FLAG_* */

	/* fields used for bookmark written size */
	uint64_t zbm_referenced_bytes_refd;
	uint64_t zbm_compressed_bytes_refd;
	uint64_t zbm_uncompressed_bytes_refd;
	uint64_t zbm_referenced_freed_before_next_snap;
	uint64_t zbm_compressed_freed_before_next_snap;
	uint64_t zbm_uncompressed_freed_before_next_snap;

	/* fields used for raw sends */
	uint64_t zbm_ivset_guid;
} zfs_bookmark_phys_t;


#define	BOOKMARK_PHYS_SIZE_V1	(3 * sizeof (uint64_t))
#define	BOOKMARK_PHYS_SIZE_V2	(12 * sizeof (uint64_t))&lt;/code&gt;
    &lt;p&gt; Only the first 3 fields are required for v1 bookmarks, while v2 bookmarks contain all 12 fields. &lt;code&gt;dsl_bookmark_node_add&lt;/code&gt; only writes a v2 bookmark if one of the 9 v2 fields are non-zero, so we can leave them all zero to write a v1 bookmark.&lt;/p&gt;
    &lt;p&gt; After a few iterations, I had a patch which hijacks the normal &lt;code&gt;zfs bookmark pool/dataset#src pool/dataset#dst&lt;/code&gt; code path to create a bookmark with arbitrary data when the source bookmark name is &lt;code&gt;missing&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~/zfs $ git --no-pager diff
sam@zfshax:~/zfs $ git --no-pager diff
diff --git a/cmd/zfs/zfs_main.c b/cmd/zfs/zfs_main.c
index 2d81ef31c..73b5d7e70 100644
--- a/cmd/zfs/zfs_main.c
+++ b/cmd/zfs/zfs_main.c
@@ -7892,12 +7892,15 @@ zfs_do_bookmark(int argc, char **argv)
 		default: abort();
 	}

+// Skip testing for #missing because it does not exist.
+if (strstr(source, "#missing") == NULL) {
 	/* test the source exists */
 	zfs_handle_t *zhp;
 	zhp = zfs_open(g_zfs, source, source_type);
 	if (zhp == NULL)
 		goto usage;
 	zfs_close(zhp);
+}

 	nvl = fnvlist_alloc();
 	fnvlist_add_string(nvl, bookname, source);
diff --git a/module/zfs/dsl_bookmark.c b/module/zfs/dsl_bookmark.c
index 861dd9239..fae882f45 100644
--- a/module/zfs/dsl_bookmark.c
+++ b/module/zfs/dsl_bookmark.c
@@ -263,7 +263,12 @@ dsl_bookmark_create_check_impl(dsl_pool_t *dp,
 		 * Source must exists and be an earlier point in newbm_ds's
 		 * timeline (newbm_ds's origin may be a snap of source's ds)
 		 */
+// Skip looking up #missing because it does not exist.
+if (strstr(source, "#missing") == NULL) {
 		error = dsl_bookmark_lookup(dp, source, newbm_ds, &amp;amp;source_phys);
+} else {
+		error = 0;
+}
 		switch (error) {
 		case 0:
 			break; /* happy path */
@@ -545,12 +550,34 @@ dsl_bookmark_create_sync_impl_book(
 	 *   because the redaction object might be too large
 	 */

+// Skip looking up #missing because it does not exist.
+if (strstr(source_name, "#missing") == NULL) {
 	VERIFY0(dsl_bookmark_lookup_impl(bmark_fs_source, source_shortname,
 	    &amp;amp;source_phys));
+}
 	dsl_bookmark_node_t *new_dbn = dsl_bookmark_node_alloc(new_shortname);

+// Skip copying from #missing because it does not exist.
+if (strstr(source_name, "#missing") == NULL) {
 	memcpy(&amp;amp;new_dbn-&amp;gt;dbn_phys, &amp;amp;source_phys, sizeof (source_phys));
 	new_dbn-&amp;gt;dbn_phys.zbm_redaction_obj = 0;
+} else {
+	// Manually set the bookmark parameters.
+	new_dbn-&amp;gt;dbn_phys = (zfs_bookmark_phys_t){
+		.zbm_guid = 4964628655505655411,
+		.zbm_creation_txg = 12,
+		.zbm_creation_time = 1756699200,
+		.zbm_redaction_obj = 0,
+		.zbm_flags = 0,
+		.zbm_referenced_bytes_refd = 0,
+		.zbm_compressed_bytes_refd = 0,
+		.zbm_uncompressed_bytes_refd = 0,
+		.zbm_referenced_freed_before_next_snap = 0,
+		.zbm_compressed_freed_before_next_snap = 0,
+		.zbm_uncompressed_freed_before_next_snap = 0,
+		.zbm_ivset_guid = 0,
+	};
+}

 	/* update feature counters */
 	if (new_dbn-&amp;gt;dbn_phys.zbm_flags &amp;amp; ZBM_FLAG_HAS_FBN) {
&lt;/code&gt;
    &lt;p&gt;To test, we recompile ZFS, reload the kernel module, and reimport the pools.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~/zfs $ gmake -s -j$(sysctl -n hw.ncpu)
sam@zfshax:~/zfs $ sudo gmake install &amp;amp;&amp;amp; sudo ldconfig
sam@zfshax:~/zfs $ sudo zpool export src &amp;amp;&amp;amp; sudo zpool export dst
sam@zfshax:~/zfs $ sudo ./scripts/zfs.sh -r
sam@zfshax:~/zfs $ sudo zpool import src -d / &amp;amp;&amp;amp; sudo zpool import dst -d /&lt;/code&gt;
    &lt;p&gt; Then, we create the bookmark ex nihilo using the magic bookmark name &lt;code&gt;missing&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~/zfs $ sudo zfs bookmark src/encryptionroot#missing src/encryptionroot#111
sam@zfshax:~/zfs $ sudo zdb src/encryptionroot#111
	#111: {guid: 44e5e7755d23c673 creation_txg: 12 creation_time: 1756699200 redaction_obj: 0}&lt;/code&gt;
    &lt;p&gt;Success! We can now use the bookmark to generate an incremental send stream containing the new hex wrapping key parameters.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~/zfs $ sudo zfs send --raw -i src/encryptionroot#111 src/encryptionroot@222 | zstreamdump
BEGIN record
	hdrtype = 1
	features = 1420004
	magic = 2f5bacbac
	creation_time = 68d3d93f
	type = 2
	flags = 0xc
	toguid = 3f99b9e92cc0aca7
	fromguid = 44e5e7755d23c673
	toname = src/encryptionroot@222
	payloadlen = 1028
nvlist version: 0
	crypt_keydata = (embedded nvlist)
	nvlist version: 0
		DSL_CRYPTO_SUITE = 0x8
		DSL_CRYPTO_GUID = 0x6196311f2622e30
		DSL_CRYPTO_VERSION = 0x1
		DSL_CRYPTO_MASTER_KEY_1 = 0x6c 0x55 0x13 0x78 0x8c 0x2d 0x42 0xb5 0x9e 0x33 0x2 0x7e 0x73 0x3a 0x46 0x20 0xd2 0xf7 0x23 0x7d 0x7c 0x5d 0x5f 0x76 0x63 0x90 0xd2 0x43 0x6a 0xdd 0x63 0x2b
		DSL_CRYPTO_HMAC_KEY_1 = 0x85 0xd1 0xf3 0xba 0xed 0xec 0x6 0x28 0x36 0xd6 0x60 0x28 0x8d 0x2f 0x6f 0x14 0xc9 0x2b 0x6f 0xf4 0x19 0x23 0x2d 0xf 0x3d 0xe 0xc4 0x88 0x4 0x6d 0xca 0xb5 0x2d 0x4d 0x8 0x75 0x17 0x1c 0xe3 0xe7 0xe6 0x23 0x7 0x53 0x94 0xba 0xc7 0x4b 0xf5 0xde 0x8c 0x29 0xa3 0x27 0xdf 0x82 0x64 0x9d 0x92 0xb4 0xc1 0x26 0x5b 0x32
		DSL_CRYPTO_IV = 0xdf 0x52 0x77 0xe8 0xf 0xfd 0xc2 0x42 0x66 0x88 0xb9 0xf0
		DSL_CRYPTO_MAC = 0x54 0x54 0x15 0xa4 0x21 0x55 0x6b 0x4e 0x99 0xe7 0xf 0xef 0x9f 0x90 0x42 0x54
		portable_mac = 0x3a 0xd6 0x30 0xc4 0x6a 0x2d 0x60 0x24 0x95 0xfc 0x99 0xbb 0xfa 0x10 0xa0 0x6b 0xc6 0x1 0xdd 0x1d 0x9 0xcd 0xa8 0x19 0xdf 0x57 0xb9 0x90 0x4f 0x2e 0x33 0xc1
		keyformat = 0x2
		pbkdf2iters = 0x0
		pbkdf2salt = 0x0
		mdn_checksum = 0x0
		mdn_compress = 0x0
		mdn_nlevels = 0x6
		mdn_blksz = 0x4000
		mdn_indblkshift = 0x11
		mdn_nblkptr = 0x3
		mdn_maxblkid = 0x4
		to_ivset_guid = 0x957edeaa7123a7
		from_ivset_guid = 0x0
	(end crypt_keydata)

END checksum = 14046201258/62f53166ccc36/14023a70758c3195/1e906f4670783cd
SUMMARY:
	Total DRR_BEGIN records = 1 (1028 bytes)
	Total DRR_END records = 1 (0 bytes)
	Total DRR_OBJECT records = 7 (960 bytes)
	Total DRR_FREEOBJECTS records = 2 (0 bytes)
	Total DRR_WRITE records = 1 (512 bytes)
	Total DRR_WRITE_BYREF records = 0 (0 bytes)
	Total DRR_WRITE_EMBEDDED records = 0 (0 bytes)
	Total DRR_FREE records = 12 (0 bytes)
	Total DRR_SPILL records = 0 (0 bytes)
	Total records = 26
	Total payload size = 2500 (0x9c4)
	Total header overhead = 8112 (0x1fb0)
	Total stream length = 10612 (0x2974)&lt;/code&gt;
    &lt;p&gt;But we can't receive the send stream.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo zfs send --raw -i src/encryptionroot#111 src/encryptionroot@222 | sudo zfs recv -F dst/encryptionroot
cannot receive incremental stream: IV set guid missing. See errata 4 at https://openzfs.github.io/openzfs-docs/msg/ZFS-8000-ER.&lt;/code&gt;
    &lt;head rend="h3"&gt;The final obstacle&lt;/head&gt;
    &lt;p&gt; ZFS refuses the stream because it is missing a source IV set GUID (see &lt;code&gt;from_ivset_guid = 0x0&lt;/code&gt; in the &lt;code&gt;zstreamdump&lt;/code&gt; above). This is because we created a v1 bookmark which does not contain the IV set GUID like a v2 bookmark would.&lt;/p&gt;
    &lt;p&gt;Since we know that the send stream is created using the right snapshots, we can temporarily disable checking IV set GUIDs to allow the snapshot to be received as described in errata 4.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo sysctl vfs.zfs.disable_ivset_guid_check=1
vfs.zfs.disable_ivset_guid_check: 0 -&amp;gt; 1
sam@zfshax:~ $ sudo zfs send --raw -i src/encryptionroot#111 src/encryptionroot@222 | sudo zfs recv -F dst/encryptionroot
sam@zfshax:~ $ sudo sysctl vfs.zfs.disable_ivset_guid_check=0
vfs.zfs.disable_ivset_guid_check: 1 -&amp;gt; 0
sam@zfshax:~ $ sudo zpool export dst
sam@zfshax:~ $ sudo zpool import dst -d /
sam@zfshax:~ $ sudo zpool scrub dst
sam@zfshax:~ $ sudo zpool status -x
all pools are healthy&lt;/code&gt;
    &lt;head rend="h3"&gt;The moment of truth&lt;/head&gt;
    &lt;p&gt;And now for the moment of truthâ¦&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ echo "0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef" | sudo zfs load-key dst/encryptionroot
sam@zfshax:~ $ sudo zfs mount -a
sam@zfshax:~ $ sudo zfs list -t all -o name,encryptionroot,keyformat,keystatus,mounted,guid -r dst
NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
dst                           -                   none        -            yes      15258247229701443799
dst/encryptionroot            dst/encryptionroot  hex         available    yes      17755083343181277380
dst/encryptionroot@111        dst/encryptionroot  -           available    -        4964628655505655411
dst/encryptionroot@222        dst/encryptionroot  -           available    -        4582898506955533479
dst/encryptionroot/child      dst/encryptionroot  hex         available    yes      364333975888407846
dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        15551239789901400488
dst/encryptionroot/child@222  dst/encryptionroot  -           available    -        11729357375613972731
sam@zfshax:~ $ tree --noreport --noreport /dst
/dst
âââ encryptionroot
    âââ 222
    âââ child
        âââ 222&lt;/code&gt;
    &lt;p&gt; At this point, we can now reliably fix the issue in our test environment. All we need to do now is use our hacked ZFS build to create the bookmark on &lt;code&gt;old&lt;/code&gt;, send an incremental snapshot of the encryption root with the new key to &lt;code&gt;sneakernet&lt;/code&gt;, and then send that snapshot from &lt;code&gt;sneakernet&lt;/code&gt; to &lt;code&gt;new&lt;/code&gt;. I rebuilt ZFS again with the correct transaction group, GUID, and creation timestamp for &lt;code&gt;old&lt;/code&gt;, repeated the same steps with the names changed, and thanks to our thorough testing, it worked on the first try!&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;After a week of intense research and debugging, I had rescued our data back from the brink and could again sleep soundly at night. While I appreciated the opportunity to learn more about ZFS, I can't help but think about how this entire incident could have been avoided at several key points which translate directly into lessons learned:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Test backups continuously so you get immediate feedback when they break.&lt;/item&gt;
      &lt;item&gt;Wait to make all destructive changes together at the very end instead of interspersed where they could accidentally be performed in the wrong order.&lt;/item&gt;
      &lt;item&gt;Always send a snapshot of the encryption root after changing the encryption key.&lt;/item&gt;
      &lt;item&gt;Create bookmarks before destroying snapshots.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I hope that you may learn from my mistakes and avoid a similar incident. If you do happen to find yourself in a similar predicament, I'd love to hear from you regardless of whether this postmortem was helpful or not. My contact details can be found here.&lt;/p&gt;
    &lt;p&gt;Knowing what I now know about ZFS native encryption, I find it difficult to recommend until the sharp edges have all been filed down. In most cases, I'd prefer to encrypt the entire pool at the block device level and encrypt send streams with age. But if you really do need the flexibility offered by native encryption, always remember to mind the encryptionroot!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45431167</guid><pubDate>Tue, 30 Sep 2025 20:58:52 +0000</pubDate></item><item><title>Introduction to Multi-Armed Bandits</title><link>https://arxiv.org/abs/1904.07272</link><description>&lt;doc fingerprint="e68c8632ccf7c28f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Machine Learning&lt;/head&gt;&lt;p&gt; [Submitted on 15 Apr 2019 (v1), last revised 3 Apr 2024 (this version, v8)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Introduction to Multi-Armed Bandits&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Multi-armed bandits a simple but very powerful framework for algorithms that make decisions over time under uncertainty. An enormous body of work has accumulated over the years, covered in several books and surveys. This book provides a more introductory, textbook-like treatment of the subject. Each chapter tackles a particular line of work, providing a self-contained, teachable technical introduction and a brief review of the further developments; many of the chapters conclude with exercises.&lt;lb/&gt;The book is structured as follows. The first four chapters are on IID rewards, from the basic model to impossibility results to Bayesian priors to Lipschitz rewards. The next three chapters cover adversarial rewards, from the full-feedback version to adversarial bandits to extensions with linear rewards and combinatorially structured actions. Chapter 8 is on contextual bandits, a middle ground between IID and adversarial bandits in which the change in reward distributions is completely explained by observable contexts. The last three chapters cover connections to economics, from learning in repeated games to bandits with supply/budget constraints to exploration in the presence of incentives. The appendix provides sufficient background on concentration and KL-divergence.&lt;lb/&gt;The chapters on "bandits with similarity information", "bandits with knapsacks" and "bandits and agents" can also be consumed as standalone surveys on the respective topics.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Aleksandrs Slivkins [view email]&lt;p&gt;[v1] Mon, 15 Apr 2019 18:17:01 UTC (510 KB)&lt;/p&gt;&lt;p&gt;[v2] Mon, 29 Apr 2019 20:45:01 UTC (510 KB)&lt;/p&gt;&lt;p&gt;[v3] Tue, 25 Jun 2019 14:39:03 UTC (536 KB)&lt;/p&gt;&lt;p&gt;[v4] Sun, 15 Sep 2019 02:06:22 UTC (557 KB)&lt;/p&gt;&lt;p&gt;[v5] Mon, 30 Sep 2019 00:15:42 UTC (543 KB)&lt;/p&gt;&lt;p&gt;[v6] Sat, 26 Jun 2021 20:15:32 UTC (639 KB)&lt;/p&gt;&lt;p&gt;[v7] Sat, 8 Jan 2022 20:05:40 UTC (627 KB)&lt;/p&gt;&lt;p&gt;[v8] Wed, 3 Apr 2024 21:32:42 UTC (629 KB)&lt;/p&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.LG&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45431271</guid><pubDate>Tue, 30 Sep 2025 21:08:28 +0000</pubDate></item><item><title>Our stewardship: Where we are, what's changing and how we'll engage</title><link>https://rubycentral.org/news/our-stewardship-where-we-are-whats-changing-and-how-well-engage/</link><description>&lt;doc fingerprint="ad5cd09105c4c6ce"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Our Stewardship: Where We Are, What’s Changing and How We’ll Engage&lt;/head&gt;
    &lt;p&gt;Dear Rubyists,&lt;/p&gt;
    &lt;p&gt;Thank you for giving me this opportunity to share with you. We take our stewardship of the Ruby Gems ecosystem seriously. Our mission is clear: keep the language and the infrastructure you rely on stable, safe, and trustworthy. Before we get to what the next steps will be, here is a quick recap from the video that we shared last week.&lt;/p&gt;
    &lt;head rend="h3"&gt;Moving parts:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We recognize there is confusion between some of the moving parts in this conversation, and we would like to add some clarity around that.&lt;/item&gt;
      &lt;item&gt;The rubygems client and bundler source code both live in the rubygems/rubygems Github monorepo&lt;/item&gt;
      &lt;item&gt;Similarly, the source code for the rubygems.org service lives in the rubygems/rubygems.org Github repo&lt;/item&gt;
      &lt;item&gt;Lastly, the production rubygems.org service is run on AWS servers by Ruby Central operators.&lt;/item&gt;
      &lt;item&gt;These components are distinct but related, and work together to provide gems from developers to end-users. Ruby Central’s role is to ensure the whole platform runs securely and reliably end-to-end, from gem publishing to gem hosting and, eventually, to gem installation on end-user machines.&lt;/item&gt;
      &lt;item&gt;The rubygems repository README and the rubygems.org repository README both explicitly state that they are “managed by Ruby Central”&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Where we are&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We implemented a temporary, procedural change to privileged access to the rubygems/rubygems repository, to the rubygems/rubygems.org repository and to the production systems for the rubygems.org service. Why we did this and how long it lasts are outlined in the next section.&lt;/item&gt;
      &lt;item&gt;Publishing and installing gems continue as usual; on-call coverage and incident response remain active.&lt;/item&gt;
      &lt;item&gt;We are prioritizing the finalization of Operator Agreements for access to our Rubygems.org production systems as a priority, followed by Contributor Agreements for contributions to the open-source above-mentioned repositories, both on a firm timeline. The operator agreements are essential to define who can access production systems, under what conditions, and with what accountability. This prevents unilateral control over critical infrastructure and removes single points of failure. Similarly, contributor agreements will clarify how code is contributed, reviewed, and licensed, ensuring contributions remain open source, transparent, and aligned with Ruby Central’s mission.&lt;/item&gt;
      &lt;item&gt;We are also conducting a final review of credentials to ensure that no legacy access remains in the system.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Why we acted&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ruby Central is responsible for the security, maintenance, and availability of the RubyGems service, including the canonical clients, RubyGems and Bundler, which install and update gems. To meet that duty of care, privileged access and operational decisions must align under a single, accountable stewardship model from codebase to production.&lt;/item&gt;
      &lt;item&gt;Unlike open-source projects that are simply distributed “as-is” with no warranties, but similar to other infrastructure projects, these codebases underpin a service operated by Ruby Central, and its canonical clients, relied on by millions of developers every day to securely download and publish gems.&lt;/item&gt;
      &lt;item&gt;A recent access review had revealed that many systems were under the control of a single individual, which we determined presented a risk to the security and operational sustainability of those systems. We had intended to resolve this over time. However, the departure of key maintainers and contribution data showing that some maintainers had long periods of inactivity (Least Privileged Access), changed the timeline.&lt;/item&gt;
      &lt;item&gt;During our review, we also saw potential privacy risks stemming from gaps in accountability. No signs of unauthorized access or PII exposure were found. At the same time, new privacy laws in multiple jurisdictions require Ruby Central to have agreements in place with operators that protect the personal information in its control. To comply with these obligations and maintain community trust, we moved quickly to strengthen security, increase auditability, and set clear responsibilities.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;How we decided to address these gaps&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For production access (live systems), we’ve put a short, procedural hold on top-level/admin permissions while we finalize operator agreements, enforce least-privilege + MFA, rotate keys, and verify audit logging. Service remains uninterrupted.&lt;/item&gt;
      &lt;item&gt;For code access (RubyGems/Bundler repo), community PRs continue as normal, while a small set of direct commit/owner rights are temporarily paused and are being re-granted as roles are confirmed.&lt;/item&gt;
      &lt;item&gt;We have set a clear deadline to complete this work within the next two weeks, so access can be restored in an orderly, transparent way.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;What this is—and what it isn’t.&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It is: Risk-reduction and accountability; signed operator and contributor agreements, MFA, audit logging, and periodic access reviews for privileged accounts.&lt;/item&gt;
      &lt;item&gt;It is not: This is not a takeover, a shutdown of contribution, or commentary on individuals. We accept responsibility for how our initial communications created the impression of sponsor-driven action. In practice, we focused first on contacting the team members directly affected and left our broader communication for business hours. Ultimately, we moved fast without providing enough advance detail, did not publish the rationale and timeline at the same time as the changes, and let routine sponsor briefings be conflated with direction. To rebuild trust, we’re sharing more detailed rationale and checkpoints (operator agreements executed, access restorations, audit verifications, uptime/MTTR), updating a public FAQ on a set cadence, and reiterating that Board decisions are independent and not contingent on funding.&lt;/item&gt;
      &lt;item&gt;Additionally, we’d like to address a related concern we’ve heard from the community. Publishing a gem on the rubygems.org service does not mean that Ruby Central can “take” it from you. Ownership changes on rubygems.org are handled through the standard administrative procedures available to gem owners. Ruby Central has not unilaterally altered database records or reassigned ownership. In other words, ownership changes are initiated by gem owners in accordance with established procedures.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;On Our Communication&lt;/head&gt;
    &lt;p&gt;We could have communicated earlier and in more detail. And we won’t stop apologizing for the confusion that caused. We are improving cadence and clarity so you always know what’s changing, why, and when.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Latest&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In the last few weeks, partial or speculative information has spread quickly and caused concern. We recognize our obligation is to the entire Ruby Community and we appreciate your efforts to hold us to account. We ask for your patience as we respond to your concerns as quickly as we are able.&lt;/item&gt;
      &lt;item&gt;In an open community, information and personnel updates move quickly. We speak regularly with current sponsors as part of routine briefings and potential sponsors regarding organizational support. Their input is not instruction; their input did not direct our actions. The Board acted independently, and financial support was NOT conditioned on taking these steps.&lt;/item&gt;
      &lt;item&gt;We know that everyone wants to know about the status of the repos. An update will be out soon. I know this isn’t the best news. But that is all that can be shared at the moment.&lt;/item&gt;
      &lt;item&gt;We will moderate official channels to keep discussion constructive and aligned with our Code of Conduct.&lt;/item&gt;
      &lt;item&gt;We’ll publish updates on a predictable schedule (weekly on Fridays).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We want to assure the community that we have our heads down and all hands have been on deck to resolve this in a way that will be the best for the community. This will take a little bit of time. There are a number of moving pieces we are trying to resolve and we know that our next steps are integral in improving your trust with Ruby Central.&lt;/p&gt;
    &lt;head rend="h3"&gt;How we will engage&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Written FAQ + updates: Recurring posts that track decisions, timelines, and what’s next.&lt;/item&gt;
      &lt;item&gt;Async questions: A short form to collect questions between live sessions. We’ll publish responses on a regular cadence.&lt;/item&gt;
      &lt;item&gt;Security briefings for companies: If your security team needs details on controls and escalation paths, contact contact@rubycentral.org.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Our commitments&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mission first: Stability, safety, and trust for the RubyGems ecosystem.&lt;/item&gt;
      &lt;item&gt;Timely and accurate information: We’ll say what we know, what we’re doing, and what’s still in progress.&lt;/item&gt;
      &lt;item&gt;Transparency and consistency: One source of truth, mirrored to official channels.&lt;/item&gt;
      &lt;item&gt;Respectful dialogue: Strong opinions welcome; personal attacks and speculation are not.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We truly appreciate you for all of your concerns and for holding us to a high standard.&lt;/p&gt;
    &lt;p&gt;With respect,&lt;/p&gt;
    &lt;p&gt;Shan Cureton&lt;/p&gt;
    &lt;p&gt;Executive Director, Ruby Central&lt;/p&gt;
    &lt;p&gt;September 30, 2025&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45431367</guid><pubDate>Tue, 30 Sep 2025 21:16:48 +0000</pubDate></item><item><title>Pre-Emptive Multi-Tasking on Arm Cortex-M</title><link>https://thejpster.org.uk/blog/blog-2025-09-28/</link><description>&lt;doc fingerprint="a244de704d0736cd"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt; JP's Website &lt;/head&gt;
    &lt;head rend="h1"&gt;Pre-emptive Multi-Tasking on Arm Cortex-M&lt;/head&gt;
    &lt;p&gt;Posted on 2025-09-28&lt;/p&gt;
    &lt;head rend="h2"&gt;Contents&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Background&lt;/item&gt;
      &lt;item&gt;C-language RTOSes&lt;/item&gt;
      &lt;item&gt;Rust-language RTOSes&lt;/item&gt;
      &lt;item&gt;But what is an RTOS?&lt;/item&gt;
      &lt;item&gt;Arm Cortex-M&lt;/item&gt;
      &lt;item&gt;Writing an RTOS in Rust&lt;/item&gt;
      &lt;item&gt;An example&lt;/item&gt;
      &lt;item&gt;Conclusions&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;I write a lot of embedded software in Rust these days - sometimes for fun, and sometimes as training material, and sometimes for customers building safety-critical systems. This last group are usually already writing safety-critical software in C, and wish to switch to writing Rust.&lt;/p&gt;
    &lt;p&gt;When you are writing safety-critical software in C, you are usually using some kind of Real-Time Operating System (RTOS) - something that lets you execute multiple tasks concurrently, and that provides queues and timer services so those tasks can message each other and wait for time to elapse. Not always, obviously, but I see it quite often.&lt;/p&gt;
    &lt;head rend="h2"&gt;C-language RTOSes&lt;/head&gt;
    &lt;p&gt;One option for people using an RTOS and wanting to switch to Rust, is to run Rust code on top of their an existing RTOS. A incomplete alphabetical list might include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Amazon FreeRTOS&lt;/item&gt;
      &lt;item&gt;Apache mynewt&lt;/item&gt;
      &lt;item&gt;Eclipse ThreadX (formerly Microsoft Azure RTOS, which was formerly Express Logic ThreadX)&lt;/item&gt;
      &lt;item&gt;eCos&lt;/item&gt;
      &lt;item&gt;Green Hills Î¼-velOSity&lt;/item&gt;
      &lt;item&gt;Keil RTX&lt;/item&gt;
      &lt;item&gt;PX5&lt;/item&gt;
      &lt;item&gt;SEGGER embOS&lt;/item&gt;
      &lt;item&gt;Sysgo PikeOS&lt;/item&gt;
      &lt;item&gt;Zephyr&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And we're barely scratching the surface here.&lt;/p&gt;
    &lt;p&gt;Some of these are full 'operating systems', that provide all the libraries you could want (filesystems, hardware drivers, networking, etc). Some are more basic 'kernels' that let you add whatever libraries you want on top. Some are rated for safety-critical usage and some are not. Some are open source, some are commercial, some offer both options. There's a lot of variation, basically.&lt;/p&gt;
    &lt;p&gt;Because Rust can import and export C compatible functions, it's usually straightforward to write tasks in Rust and run them on top of any existing C-language RTOS. Ferrous Systems have examples for ThreadX and examples for FreeRTOS which you can take a look at.&lt;/p&gt;
    &lt;p&gt;So this is a fine option, and one you should consider if you already have an RTOS you like.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rust-language RTOSes&lt;/head&gt;
    &lt;p&gt;Of course, there are now a bunch of RTOSes that don't just let you run Rust programs, but are themselves written in Rust.&lt;/p&gt;
    &lt;p&gt;Again, an incomplete, alphabetical list might include:&lt;/p&gt;
    &lt;p&gt;Again, they vary in scope and intended usage.&lt;/p&gt;
    &lt;p&gt;Special mention to embassy, which isn't an RTOS in the classical sense because it only provides for the concurrent execution of co-operative async tasks, and not pre-emptive context switching, but largely replaces the need for an RTOS. I also want to highlight RTIC, which is sort of an RTOS, except it does most of the work at compile time and generates bare-metal Rust code that kind of works like an RTOS, but uses the Arm NVIC interrupt controller and the Rust compiler to do all the heavy-lifting for switching tasks.&lt;/p&gt;
    &lt;p&gt;But wait, what?&lt;/p&gt;
    &lt;head rend="h2"&gt;But what is an RTOS?&lt;/head&gt;
    &lt;p&gt;Any Operating System which provides Real-Time guarantees is an RTOS. Any by real-time guarantees, I mean you can assign a priority to each task, and have some measurable upper bound for how long an input to the system will take to be processed. Windows 11, for example, is not a real-time operating system, because when you press a key on the keyboard there is literally no way of predicting when the computer might respond to that key press. If the input was "pressing the brake pedal" and the output was "activating the vehicle's brake system", you can see how that might be an issue.&lt;/p&gt;
    &lt;p&gt;Generally to do this, an RTOS will provide mechanisms for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;spawning multiple tasks&lt;/item&gt;
      &lt;item&gt;assigning priorities to those tasks&lt;/item&gt;
      &lt;item&gt;allowing those tasks to park themselves until some future event happens (a message arrives on a queue, an interrupt fires, a timeout expires, etc)&lt;/item&gt;
      &lt;item&gt;selecting and running the highest priority task that is not currently parked&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By 'task', I mean a mechanism to execute a function indefinitely, but in such a way that execution of that 'task' can be paused so that a different 'task' can use the CPU for a short while. That way it appears like you are running N infinitely long tasks in parallel, but you are actually only running one at a time and swapping between them. This is not something novel - UNIX has been doing it since the 1970s, and it was not the first - and that means it is something that is pretty well understood and considered 'tried and tested'.&lt;/p&gt;
    &lt;p&gt;An aside on naming: If you're more familiar with Windows, macOS or Linux, what I call a Task maps closely to what you might know as a Thread. It should also be noted that what I call a Task isn't what Rust calls a Task, as those are specifically asynchronous tasks, which is something different. You might say, "But why didn't you just call them Threads?" and the answer is FreeRTOS also calls them Tasks and because to me, Threads are short-lived things you can spawn and then wait on, whilst Tasks are things that are created at start-up. Anyway, naming things is one of the hardest problems in computer science.&lt;/p&gt;
    &lt;p&gt;The tasks might be allocated statically (all tasks known at compile time) or it might be possible to dynamically add and remove tasks. I've seen both work well.&lt;/p&gt;
    &lt;p&gt;But, recently I've been wondering ... how exactly do they work? And I thought the best way to figure that out is to try and write one and see what problems I have to solve. But which platform should we target with our RTOS (or, which one should we target first)?&lt;/p&gt;
    &lt;head rend="h2"&gt;Arm Cortex-M&lt;/head&gt;
    &lt;p&gt;The Arm Cortex-M line of processors, implementing the M-Profile variants of the Arm Architecture, are a good choice here. Cheap, very widely available, and well supported by Rust. And it turns out they have a magic super-power - they are designed to make an RTOS very easy to write. Armv6-M, Armv7-M, Armv7E-M, Armv8-M Baseline and Armv8-M Mainline all operate in pretty much the same way, so let's just pick Armv7-M as our example.&lt;/p&gt;
    &lt;p&gt;A task, in Armv7-M terms, consists of the processor state available to an executing program - the CPU registers R0 through R12, the Link Register (LR), the Stack Pointer (SP), the Program Counter (PC) , and the Processor Status Register (CPSR). If we stop a task from running, by handling an Exception, we simply need to save all of that 'stuff' somewhere - usually the task's stack - and then put all the 'stuff' back later. The program will then have no idea it had been interrupted, moved into a storage area, and later resurrected. All it will be able to observe is that the wall clock suddenly jumped forwards. By Exception we mean a Supervisor Call, one of a number of Faults, or an Interrupt - they are all handled the same way.&lt;/p&gt;
    &lt;p&gt;When an Exception occurs, the processor hardware, following the specification laid down in the Armv7-M architecture, will start executing the exception handler code. But if it just did that and nothing else, any registers the exception handler used would need to be saved somewhere and then restored back to their original values when the exception handler ended. If it did not do this, then a side-effect of an exception firing would be that registers would randomly change their contents whilst your program executed. This would be a nightmare, and nothing would work right. However, writing a routine that can push all the registers to the stack would need to be written in Assembly Language and it would need to be very careful to not use a register until after it had been saved.&lt;/p&gt;
    &lt;p&gt;If we wrote our exception handler in C, or Rust, (typically one compatible with the Arm Embedded ABI), the compiler will emit machine code that freely uses R0, R1, R2, R3 and R12. The assumption being that if you call a function and you care about the values in those registers, then the caller should save them, leaving the callee (the function being called) free to just use them as it likes. Indeed, I believe they are also used to pass arguments to the callee, and for passing back the return value to the caller.&lt;/p&gt;
    &lt;p&gt;So that is a problem - the compiled code wants to use some of the registers, and we cannot write compiled code to save those registers because the code will damage the registers it is trying to save.&lt;/p&gt;
    &lt;p&gt;However, in a major departure from both Legacy Arm Architectures and the R- and A-Profiles of the current Arm Architectures, the M-Profile specifications state that the hardware will automatically push those registers to the stack. This means that you can write Armv7-M (et al) exception handlers in C. Or in Rust. It also gives us a leg-up when we want to write an RTOS, as part of our work is done for us.&lt;/p&gt;
    &lt;code&gt;// An exception handler written in C
void SysTick(void) {
	my_rtos_tick_handler();
	GLOBAL_COUNTER += 1;
}
&lt;/code&gt;
    &lt;p&gt;Having had to write Interrupt Handlers in assembly language for Arm R-Profile architectures, I appreciate this a lot.&lt;/p&gt;
    &lt;p&gt;It should be noted that exceptions can pre-empt each other. That is, if you are running Interrupt L, and some Interrupt H becomes ready, where H is at some higher priority than L (indicated by H having a lower priority number than L), then the state of the handler for Interrupt L will be pushed to the stack and the handler for Interrupt H will be started. We don't generally want to block interrupts from running, because that impacts our response time to external events. Faults obviously have the highest priority - they typically mean it's game over and time to reboot.&lt;/p&gt;
    &lt;p&gt;Now, to change tasks when some task has been running for too long, we need some periodic timer tick - an interrupt that fires every N clock cycles, where we can select the value for N. If we tick more often, we waste more time in the timer interrupt handler, but if we tick less often, we get a lower resolution 'clock'. Setting N such that the interrupt fires 100 times per second is fairly common. You could also be smart and set the timer to only fire when we actually know we have something to do (a so-called 'tickless' RTOS), but let's not worry about that fow now.&lt;/p&gt;
    &lt;p&gt;Helpfully Arm mandate that every Armv7-M processor comes with a standard timer called SysTick, which is literally designed to do exactly what we want. We program a few registers, and the &lt;code&gt;SysTick&lt;/code&gt; exception will fire every N clock cycles, just as we want. And we could use this to run our task-switching code, but ... what if we want to switch tasks in-between ticks? Perhaps because a task ran out of work to do, and now needs to wait for some event? It's easy to manually force the &lt;code&gt;SysTick&lt;/code&gt; exception to run but, if we did, how would we know if it ran because the timer went off, or because we manually provoked it? We also have an issue with priorities - we might want the &lt;code&gt;SysTick&lt;/code&gt; handler to be a fairly high priority so that our clock doesn't suffer from jitter, but need all our other interrupts to take priority over task-switching. If the task-switcher pre-empted an Exception handler rather than user code, it would stack the Exception handler state onto the 'current' (but dormant) task's stack, as opposed to the task state - and that would be a disaster.&lt;/p&gt;
    &lt;p&gt;Note: If you'd like a deeper dive into Arm Cortex-M Exception handling, memfault has you covered.&lt;/p&gt;
    &lt;p&gt;So if we don't task switch in the &lt;code&gt;SysTick&lt;/code&gt; handler where should we task switch?&lt;/p&gt;
    &lt;p&gt;Arm provide a special exception called &lt;code&gt;PendSV&lt;/code&gt;, which is ideal for our needs. It can be set as the lowest priority exception handler, and it can be very easily triggered by writing a bit to a special register. We can even set this bit from another exception handler (like the &lt;code&gt;SysTick&lt;/code&gt; handler), and the &lt;code&gt;PendSV&lt;/code&gt; handler won't run until all other exceptions are complete. So, if the &lt;code&gt;SysTick&lt;/code&gt; handler fires and we decide to force a task switch, or if a task wants to pause itself and let something else run, either way we can work out which task to run next, set the PendSV bit, and sit back whilst the &lt;code&gt;PendSV&lt;/code&gt; handler does the switch.&lt;/p&gt;
    &lt;p&gt;The final piece of the Armv7-M feature set which helps us write an RTOS is the fact there are two stack pointers - a Main Stack Pointer (MSP) and a Process Stack Pointer (PSP). By default, the system runs on the MSP, for both the main function and any exception handlers that run. However, you can flip a bit in a register, and switch the main function to use the PSP instead (leaving exception handlers using the MSP). Obviously, we should set the PSP to some suitable value first, but this does mean that our &lt;code&gt;PendSV&lt;/code&gt; handler, and all the other exception handlers, can have one stack to share, and then every task can have its own unique stack. We just need to arrange for the PSP to be selected when we leave &lt;code&gt;PendSV&lt;/code&gt;, as opposed to the MSP. And it turns out, Arm thought of that too.&lt;/p&gt;
    &lt;p&gt;When you enter an exception handler, the values of PC, LR, R0, R1, R2, R3 and R12 from the interrupted code are automatically saved to the stack using the MSP. When you leave an exception handler, those values will be restored. The hardware takes advantage of this by leaving the exception handler a little message in the LR register - basically telling us what kind of code was interrupted. Was it in privileged or unprivileged mode? Was it using the FPU? Was it using MSP or PSP? And by leaving the same kind of message in LR when we exit the handler, the processor will reconfigure itself to the appropriate state before resuming execution of the code. However, there's nothing to say the value of LR we get on entry to &lt;code&gt;PendSV&lt;/code&gt; needs to be the one we return! If we're switching from Task A to Task B, we should save the value of LR from Task A somewhere (to the Task A's Stack using the PSP, along with R4 to R11), and then we should return the value of LR that Task B had when it was interrupted.&lt;/p&gt;
    &lt;head rend="h2"&gt;Writing an RTOS in Rust&lt;/head&gt;
    &lt;p&gt;OK, so now we need to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set up a stack for each task&lt;/item&gt;
      &lt;item&gt;Push some initial state onto each stack&lt;/item&gt;
      &lt;item&gt;Use PendSV to switch from 'No Task' to running our first task&lt;/item&gt;
      &lt;item&gt;Regularly interrupt the processor to: &lt;list rend="ul"&gt;&lt;item&gt;Pick the next task to run&lt;/item&gt;&lt;item&gt;Use PendSV to switch tasks&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;As a bonus, we should also keep track of which tasks are ready to run&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let's take these in turn.&lt;/p&gt;
    &lt;head rend="h3"&gt;Representing a Stack&lt;/head&gt;
    &lt;p&gt;On Arm, stacks are typically what is known as Full Descending. That is, the stack pointer points at the most recent item added to the stack (it points to a 'full' location), and when a new item is pushed onto the stack, the pointer moves downwards. You could also have a Empty Descending stack, or even an Empty Ascending stack, but C and Rust agree to use the Arm Embedded ABI, and that says that stacks are Full Descending (and you can imagine the mess if not all the code in the system agrees on this convention).&lt;/p&gt;
    &lt;p&gt;So, we need a memory region, and we need to know the address just above the top-most location in that region.&lt;/p&gt;
    &lt;p&gt;First, please note that examples on this page are Copyright (c) 2025 Ferrous Systems, and licensed under GPL-3.0-or-later. You can find a full working version at https://github.com/jonathanpallant/pets/tree/v0.1.0, which includes all the lovely comments that I stripped out of this blog page in the name of brevity (and because there's all this explanatory text anyway).&lt;/p&gt;
    &lt;code&gt;use crate::UnsafeCell;

#[repr(align(8))]
pub struct Stack&amp;lt;const LEN: usize&amp;gt; {
    contents: UnsafeCell&amp;lt;[u8; LEN]&amp;gt;,
}

impl&amp;lt;const LEN: usize&amp;gt; Stack&amp;lt;LEN&amp;gt; {
    pub const fn new() -&amp;gt; Self {
        assert!(LEN.is_multiple_of(4));
        Self {
            contents: UnsafeCell::new([0u8; LEN]),
        }
    }

    pub const fn top(&amp;amp;self) -&amp;gt; *mut u32 {
        unsafe { self.contents.get().add(1) as *mut u32 }
    }
}

unsafe impl&amp;lt;const LEN: usize&amp;gt; Sync for Stack&amp;lt;LEN&amp;gt; {}

impl&amp;lt;const LEN: usize&amp;gt; Default for Stack&amp;lt;LEN&amp;gt; {
    fn default() -&amp;gt; Self {
        Stack::new()
    }
}
&lt;/code&gt;
    &lt;p&gt;So we have a &lt;code&gt;struct Stack&lt;/code&gt; managing an &lt;code&gt;UnsafeCell&lt;/code&gt; containing an array of bytes, and a method to get a pointer to the 'top' of the stack (which is just beyond the array). It's not perfect - there's nothing stopping someone using the same stack twice, but it'll work. And we do at least ensure the stack size of a multiple of 4, and that it starts on an address that is a multiple of 8 (the AAPCS specification says compilers can rely on this being true, and we have observed Rust code having undefined behaviour when this is not true).&lt;/p&gt;
    &lt;head rend="h3"&gt;Pushing onto a Stack&lt;/head&gt;
    &lt;p&gt;When we set up our tasks, we need to push some information into the stack for each task. We could do this by applying negative offsets to the stack pointer, but this seems error prone. So let's have a little helper that can push a value into the stack, and move the stack pointer downwards automatically.&lt;/p&gt;
    &lt;code&gt;pub(crate) struct StackPusher(*mut u32);

impl StackPusher {
    pub(crate) unsafe fn new(stack_top: *mut u32) -&amp;gt; StackPusher {
        StackPusher(stack_top)
    }

    pub(crate) fn push(&amp;amp;mut self, value: u32) {
        unsafe {
            self.0 = self.0.offset(-1);
            self.0.write_volatile(value);
        }
    }

    pub(crate) fn current(&amp;amp;self) -&amp;gt; *mut u32 {
        self.0
    }
}
&lt;/code&gt;
    &lt;p&gt;Remember, some comments have been removed to save space - I'm not actually a monster who writes undocumented code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tasks&lt;/head&gt;
    &lt;p&gt;Now, we need something to represent our tasks:&lt;/p&gt;
    &lt;code&gt;pub type TaskEntryFn = fn() -&amp;gt; !;

#[repr(C)]
pub struct Task {
    stack: AtomicPtr&amp;lt;u32&amp;gt;,
    entry_fn: TaskEntryFn,
}
&lt;/code&gt;
    &lt;p&gt;All we need to know is, what code should the task execute when it is started, and, what is the tasks current stack pointer (i.e. the value of PSP last time we suspended the task). Let's add some helper methods to it:&lt;/p&gt;
    &lt;code&gt;impl Task {
    /// The size of a task object is `pow(2, SIZE_BITS)`.
    pub const SIZE_BITS: usize = 3;

    /// A compile-time check that the size of a [`Task`] is what we said it was.
    const _CHECK: () = const {
        assert!(core::mem::size_of::&amp;lt;Self&amp;gt;() == (1 &amp;lt;&amp;lt; Self::SIZE_BITS));
    };

    pub const fn new&amp;lt;const N: usize&amp;gt;(entry_fn: TaskEntryFn, stack: &amp;amp;Stack&amp;lt;N&amp;gt;) -&amp;gt; Task {
        assert!(N &amp;gt; crate::Scheduler::MIN_STACK_SIZE);
        Task {
            entry_fn,
            stack: AtomicPtr::new(stack.top()),
        }
    }

    pub(crate) const fn entry_fn(&amp;amp;self) -&amp;gt; TaskEntryFn {
        self.entry_fn
    }

    pub(crate) fn stack(&amp;amp;self) -&amp;gt; *mut u32 {
        self.stack.load(Ordering::Relaxed)
    }

    pub(crate) unsafe fn set_stack(&amp;amp;self, new_stack: *mut u32) {
        self.stack.store(new_stack, Ordering::Relaxed)
    }
}
&lt;/code&gt;
    &lt;p&gt;Later on we're going to be poking values into this struct using assembly language so it's important that the size of &lt;code&gt;struct Task&lt;/code&gt; is a power of 2 (which means we can convert a task index into a byte offset by doing a left-shift, instead of having to do a multiply). So we have a compile-time assert to check our &lt;code&gt;Task::SIZE_BITS&lt;/code&gt; value is correct.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Scheduler&lt;/head&gt;
    &lt;p&gt;Now, our scheduler. We're going to need to hold on to a static list of &lt;code&gt;Task&lt;/code&gt; values. We don't want to own them, because then we'd need to be generic over the length of the list - having a reference to them is fine. We need to track which task ID we are currently running (if any), and which one we should switch to next. We should probably also keep track of time.&lt;/p&gt;
    &lt;code&gt;#[repr(C)]
pub struct Scheduler {
    /// Which task is currently running
    current_task: AtomicUsize,
    /// Which task should PendSV switch to next
    next_task: AtomicUsize,
    /// A fixed, static list of all our tasks
    task_list: &amp;amp;'static [Task],
    /// Current tick count
    ticks: AtomicU32,
}
&lt;/code&gt;
    &lt;p&gt;Our assembly code is going to need to poke at this data, so let's store a pointer to our one global Scheduler, and give ourselves some constants to make it easier to access the fields.&lt;/p&gt;
    &lt;code&gt;pub(crate) static SCHEDULER_PTR: AtomicPtr&amp;lt;Scheduler&amp;gt; = AtomicPtr::new(core::ptr::null_mut());

impl Scheduler {
    pub(crate) const CURRENT_TASK_OFFSET: usize = core::mem::offset_of!(Scheduler, current_task);
    pub(crate) const NEXT_TASK_OFFSET: usize = core::mem::offset_of!(Scheduler, next_task);
    pub(crate) const TASK_LIST_OFFSET: usize = core::mem::offset_of!(Scheduler, task_list);
}
&lt;/code&gt;
    &lt;p&gt;That's better than hard-coding constants that end up wrong when we change the struct definition.&lt;/p&gt;
    &lt;p&gt;Now we need to be able to create a Scheduler. Let's do that as a const-fn, so we have a store the Scheduler in a static variable.&lt;/p&gt;
    &lt;code&gt;impl Scheduler {
    /// Build the scheduler
    pub const fn new(task_list: &amp;amp;'static [Task]) -&amp;gt; Scheduler {
        // Cannot schedule without at least one task
        assert!(!task_list.is_empty());
        Scheduler {
            task_list,
            current_task: AtomicUsize::new(usize::MAX),
            next_task: AtomicUsize::new(0),
            ticks: AtomicU32::new(0),
        }
    }
}
&lt;/code&gt;
    &lt;p&gt;Having nothing to do would be bad, so we fail if we don't get at least one task to run.&lt;/p&gt;
    &lt;p&gt;Now, to start, we need to push some data into each task's stack. Then we can hit the PendSV button to switch to the first task, and we're off! Note the extreme comment/code ratio - this stuff is almost all load-bearing.&lt;/p&gt;
    &lt;code&gt;impl Scheduler {
    /// Run the scheduler
    ///
    /// You may only call this once, and you should call it from `fn main()`
    /// once all your hardware is configured. We should be in Privileged
    /// Thread mode on the Main stack.
    pub fn start(&amp;amp;self, mut syst: cortex_m::peripheral::SYST, systicks_per_sched_tick: u32) -&amp;gt; ! {
        if self.current_task.load(Ordering::SeqCst) != usize::MAX {
            panic!("Tried to re-start scheduler!");
        }

        // remember where this object is - it cannot move because we do not exit this function
        let self_addr = self as *const Scheduler as *mut Scheduler;
        SCHEDULER_PTR.store(self_addr, Ordering::Release);

        // Must do this /after/ setting SCHEDULER_PTR because the SysTick
        // exception handler will use SCHEDULER_PTR
        syst.set_reload(systicks_per_sched_tick);
        syst.clear_current();
        syst.enable_counter();
        syst.enable_interrupt();

        // We need to push some empty state into each task stack
        for (task_idx, task) in self.task_list.iter().enumerate() {
            let old_stack_top = task.stack();

            // SAFETY: The task constructor does not let us make tasks with
            // stacks that are too small.
            let mut stack_pusher = unsafe { StackPusher::new(old_stack_top) };

            // Standard Arm exception frame

            // CPSR
            stack_pusher.push(Self::DEFAULT_CPSR);
            // PC
            stack_pusher.push(task.entry_fn() as usize as u32);
            // LR
            stack_pusher.push(0);
            // R12
            stack_pusher.push(0);
            // R3
            stack_pusher.push(0);
            // R2
            stack_pusher.push(0);
            // R1
            stack_pusher.push(0);
            // R0
            stack_pusher.push(0);

            // Additional task state we persist

            // R11
            stack_pusher.push(0);
            // R10
            stack_pusher.push(0);
            // R9
            stack_pusher.push(0);
            // R8
            stack_pusher.push(0);
            // R7
            stack_pusher.push(0);
            // R6
            stack_pusher.push(0);
            // R5
            stack_pusher.push(0);
            // R4
            stack_pusher.push(0);

            // Set task stack pointer to the last thing we pushed

            // SAFETY: the pointer we are passing is a validly aligned stack pointer
            unsafe {
                task.set_stack(stack_pusher.current());
            }
        }

        // Fire the PendSV exception - the PendSV handler will select a task
        // to run and run it
        cortex_m::peripheral::SCB::set_pendsv();
        // flush the pipeline to ensure the PendSV fires before we reach the end of this function
        cortex_m::asm::isb();
        // impossible to get here
        unreachable!();
    }
}
&lt;/code&gt;
    &lt;p&gt;This function never returns - it will be the last thing that &lt;code&gt;fn main()&lt;/code&gt; calls. The &lt;code&gt;DEFAULT_CPSR&lt;/code&gt; is &lt;code&gt;0x0100_0000&lt;/code&gt; - the bit we have set is the 'Thumb' bit, which indicates the processor is executing the T32 ISA instead of the A32 ISA. As this is the only ISA supported in M-Profile Architectures, if we do not set this bit the processor will crash when we resume our first task. It took me a while to work that one out. Not also that the order we push the saved state into each stack is important - it must be the reverse of the order in the PendSV handler (first) and processor itself (second) takes them out.&lt;/p&gt;
    &lt;p&gt;There's also a curious issue with &lt;code&gt;set_pendsv()&lt;/code&gt;. Setting the bit doesn't immediately cause the &lt;code&gt;PendSV&lt;/code&gt; handler to fire. Because Arm processors are pipelined, they are loading the next instruction whilst simultaneously executing the current instruction (and perhaps retiring the previous instruction). So there may be a delay of a clock cycle or two whilst the processor deals any instructions it started but has not finished, before it jumps to the &lt;code&gt;PendSV&lt;/code&gt; handler. An Instruction Synchronization Barrier is what we want here, to block the CPU until the pipeline is empty, using the &lt;code&gt;isb()&lt;/code&gt; function from the &lt;code&gt;cortex-m&lt;/code&gt; crate.&lt;/p&gt;
    &lt;head rend="h3"&gt;Picking a new task&lt;/head&gt;
    &lt;p&gt;To pick new tasks to run, we need handle two cases:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Timer fired and task is suspended whether it likes it or not&lt;/item&gt;
      &lt;item&gt;Task is bored and wishes to be suspended&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;impl Scheduler {
    /// Call periodically, to get the scheduler to adjust which task should run next
    ///
    /// This is currently a round-robin with no priorities, and no sense of tasks being blocked
    ///
    /// Ideally call this from a SysTick handler
    pub fn sched_tick(&amp;amp;self) {
        defmt::debug!("Tick!");
        self.ticks.fetch_add(1, Ordering::Relaxed);
        self.pick_next_task();
        cortex_m::peripheral::SCB::set_pendsv();
    }

    /// Switch tasks, because this one has nothing to do right now
    pub fn yield_current_task(&amp;amp;self) {
        self.pick_next_task();
        cortex_m::peripheral::SCB::set_pendsv();
    }

    /// Select the next task in the round-robin
    ///
    /// Updates `self.next_task` but doesn't trigger a task switch. Set PendSV
    /// to do that.
    fn pick_next_task(&amp;amp;self) {
        cortex_m::interrupt::free(|_cs| {
            let next_task = self.next_task.load(Ordering::Relaxed);
            let maybe_next_task = next_task + 1;
            let new_next_task = if maybe_next_task &amp;gt;= self.task_list.len() {
                0
            } else {
                maybe_next_task
            };
            self.next_task.store(new_next_task, Ordering::Relaxed);
        });
    }
}
&lt;/code&gt;
    &lt;p&gt;I don't know that I've got those atomic orderings correct here. If you're Mara, feel free to tell me I'm wrong and how to make it better!&lt;/p&gt;
    &lt;head rend="h3"&gt;Actually switching tasks&lt;/head&gt;
    &lt;p&gt;OK, enough already. We need to write the PendSV handler and the bad news is, we cannot do it in Rust. It needs to access system registers, and so we cannot let the compiler generate any code that might affect those registers (like, copy them to the Main Stack) before we've had a chance to read them. But at least we have naked functions in Rust now, so we don't have to resort to using Assembly files, or declaring our own function symbols using `global_asm!.&lt;/p&gt;
    &lt;code&gt;#[unsafe(no_mangle)]
#[unsafe(naked)]
unsafe extern "C" fn PendSV() {
    // NOTE: This code must NOT touch r4-r11. It can ONLY touch r0-r3 and r12,
    // because those registers were stacked by the hardare on exception entry.

    naked_asm!(r#"
    // r1 = the address of the Scheduler object
    ldr     r1, ={scheduler_ptr}
    ldr     r1, [r1]

    // r2 = the current task ID
    ldr     r2, [r1, {current_task_offset}]

    // r3 = the task list pointer
    ldr     r3, [r1, {task_list_offset}]

    // if current task ID is -1, skip the stacking of the current task
    cmp     r2, #-1
    beq     1f

    //
    // Stack the current task
    //
    // r1 holds the scheduler object's address
    // r2 holds the current task ID
    // r3 holds the task list's address
    //

    // r2 = the current task byte offset 
    lsl     r2, {task_size_bits}

    // r0 = the current task stack pointer
    mrs     r0, psp

    // Push the additional state into stack at r0
    stmfd   r0!, {{ r4 - r11 }}

    // save the stack pointer (in r0) to the task object
    str     r0, [r3, r2]

    //
    // Pop the next task
    //
    // r1 holds the scheduler object's address
    // r3 holds the task list's address
    //

    1:

    // r2 = the next task byte offset
    ldr     r2, [r1, {next_task_offset}]
    lsl     r2, {task_size_bits}

    // r0 = the stack pointer from the task object
    ldr     r0, [r3, r2]

    // Pop the additional state from it
    ldmfd   r0!, {{ r4 - r11 }}

    // Set the current task stack pointer
    msr     psp, r0

    //
    // Update the Current Task ID
    //
    // r1 holds the scheduler object's address
    //

    // copy the next task id to the current task id
    ldr     r2, [r1, {next_task_offset}]
    str     r2, [r1, {current_task_offset}]

    //
    // return to thread mode on the process stack
    //

    // This is the magic LR value for 'return to thread mode process stack'
    mov     lr, #0xFFFFFFFD
    bx      lr
    "#,
    scheduler_ptr = sym scheduler::SCHEDULER_PTR,
    current_task_offset = const Scheduler::CURRENT_TASK_OFFSET,
    next_task_offset = const Scheduler::NEXT_TASK_OFFSET,
    task_list_offset = const Scheduler::TASK_LIST_OFFSET,
    task_size_bits = const Task::SIZE_BITS,
    );
}
&lt;/code&gt;
    &lt;p&gt;Getting this right was ... challenging - hence all the comments for my future self! Luckily we have QEMU, so running a test was very fast, and I was able to single-step through the code in GDB to check it was doing what I wanted (when GDB wasn't busy segfaulting, that is). But the gist of it is, we can rely on the hardware having pushed a basic frame to the Process Stack, and we only need to push the remaining registers (R4 to R11). We can then pop those same registers from the next task's stack, change PSP, and then leave the exception return mechanism to drop us back into our freshly resumed task. We also rely on some of the constants we exported earlier so we can reach into the global &lt;code&gt;Scheduler&lt;/code&gt; object to read the next task ID, the current task ID (which we update when we've switched tasks), and the list of &lt;code&gt;Task&lt;/code&gt; objects themselves.&lt;/p&gt;
    &lt;p&gt;I do also have an updated version of this code which also handles lazy FPU stacking and extended frames, but that's a bit much for this post. Maybe next time.&lt;/p&gt;
    &lt;head rend="h3"&gt;Userspace API&lt;/head&gt;
    &lt;p&gt;Our tasks are going to need to interact with our scheduler, so let's give them some simple functions to call:&lt;/p&gt;
    &lt;code&gt;/// Delay a task for at least the given period, measured in timer ticks.
///
/// Calling `delay(0)` is basically just a yield.
pub fn delay(ticks: u32) {
    let scheduler = Scheduler::get_scheduler().unwrap();
    let start = scheduler.now();
    loop {
        // yield first, so delay(0) does at least one task switch
        scheduler.yield_current_task();
        // is it time to leave?
        let delta = scheduler.now().wrapping_sub(start);
        if delta &amp;gt;= ticks {
            break;
        }
    }
}

/// Get the current time, in ticks
pub fn now() -&amp;gt; u32 {
    if let Some(scheduler) = Scheduler::get_scheduler() {
        scheduler.now()
    } else {
        0
    }
}
&lt;/code&gt;
    &lt;p&gt;Oh, we also need that handy &lt;code&gt;get_scheduler()&lt;/code&gt; function to grab our global static scheduler. Plus the &lt;code&gt;now()&lt;/code&gt;method.&lt;/p&gt;
    &lt;code&gt;impl Scheduler {
    /// Get the handler to the global scheduler
    pub(crate) fn get_scheduler() -&amp;gt; Option&amp;lt;&amp;amp;'static Scheduler&amp;gt; {
        // Get our stashed pointer
        let scheduler_ptr = SCHEDULER_PTR.load(Ordering::Relaxed);
        // Are we intialised?
        if scheduler_ptr.is_null() {
            None
        } else {
            // SAFETY: Only [`Scheduler::start`] writes to [`SCHEDULER_PTR`] and it
            // always sets it to be a valid pointer to a [`Scheduler`] that does not
            // move.
            Some(unsafe { &amp;amp;*scheduler_ptr })
        }
    }

    /// Get current tick count
    pub fn now(&amp;amp;self) -&amp;gt; u32 {
        self.ticks.load(Ordering::Relaxed)
    }
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;An example&lt;/head&gt;
    &lt;p&gt;But, let's leave you with an example program running in PETS - our new pre-emptive, time-slicing scheduler.&lt;/p&gt;
    &lt;code&gt;#![no_std]
#![no_main]

use pets::{Scheduler, Stack, Task};

use defmt_semihosting as _;

const SYSTICKS_PER_SCHED_TICK: u32 = 100_000;

static SCHEDULER: Scheduler = Scheduler::new({
    static TASK_LIST: [Task; 3] = [
        Task::new(rabbits, {
            static STACK: Stack&amp;lt;1024&amp;gt; = Stack::new();
            &amp;amp;STACK
        }),
        Task::new(hamsters, {
            static STACK: Stack&amp;lt;1024&amp;gt; = Stack::new();
            &amp;amp;STACK
        }),
        Task::new(cats, {
            static STACK: Stack&amp;lt;1024&amp;gt; = Stack::new();
            &amp;amp;STACK
        }),
    ];
    &amp;amp;TASK_LIST
});

#[cortex_m_rt::entry]
fn main() -&amp;gt; ! {
    let cp = cortex_m::Peripherals::take().unwrap();
    defmt::info!("Hello!");
    SCHEDULER.start(cp.SYST, SYSTICKS_PER_SCHED_TICK);
}

fn rabbits() -&amp;gt; ! {
    loop {
        defmt::info!("Rabbit! (back in 5)");
        pets::delay(5);
    }
}

fn hamsters() -&amp;gt; ! {
    loop {
        defmt::info!("Hamster! (back in 10)");
        pets::delay(10);
    }
}

fn cats() -&amp;gt; ! {
    loop {
        defmt::info!("Cat! (back in 3)");
        pets::delay(3);
    }
}

#[panic_handler]
fn panic(info: &amp;amp;core::panic::PanicInfo) -&amp;gt; ! {
    defmt::println!("PANIC: {}", defmt::Debug2Format(info));
    cortex_m::asm::udf();
}

#[cortex_m_rt::exception]
unsafe fn HardFault(info: &amp;amp;cortex_m_rt::ExceptionFrame) -&amp;gt; ! {
    defmt::println!("FAULT: {}", defmt::Debug2Format(info));
    cortex_m::asm::udf();
}

defmt::timestamp!("{=u32:010}", pets::now());
&lt;/code&gt;
    &lt;code&gt;$ cargo build --bin example1
   Compiling pets v0.1.0 (/home/jonathan/Documents/pets)
    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.67s
$ qemu-system-arm \
	-cpu cortex-m4 -machine mps2-an386 \
	-semihosting-config enable=on,target=native \
	-nographic \
	-kernel target/thumbv7em-none-eabi/debug/example1 \
	| defmt-print \
		-e target/thumbv7em-none-eabi/debug/example1 \
		--log-format="{t} {[{L}]%bold} {s} {({ff}:{l:1})%dimmed}"

0000000000 [INFO ] SCHEDULER_PTR @ 20000c40 (src/scheduler.rs:78)
0000000000 [INFO ] Scheduler @ 20000018 (src/scheduler.rs:83)
0000000000 [INFO ] Init task frame 0, with stack @ 0x20000430 (src/scheduler.rs:96)
0000000000 [INFO ] Init task frame 1, with stack @ 0x20000830 (src/scheduler.rs:96)
0000000000 [INFO ] Init task frame 2, with stack @ 0x20000c30 (src/scheduler.rs:96)
0000000000 [INFO ] Rabbit! (back in 5) (bin/example1.rs:38)
0000000000 [INFO ] Hamster! (back in 10) (bin/example1.rs:48)
0000000000 [INFO ] Cat! (back in 3) (bin/example1.rs:58)
0000000003 [INFO ] Cat! (back in 3) (bin/example1.rs:58)
0000000005 [INFO ] Rabbit! (back in 5) (bin/example1.rs:38)
0000000006 [INFO ] Cat! (back in 3) (bin/example1.rs:58)
0000000009 [INFO ] Cat! (back in 3) (bin/example1.rs:58)
0000000010 [INFO ] Rabbit! (back in 5) (bin/example1.rs:38)
0000000010 [INFO ] Hamster! (back in 10) (bin/example1.rs:48)
0000000012 [INFO ] Cat! (back in 3) (bin/example1.rs:58)
...
&lt;/code&gt;
    &lt;p&gt;Look at that! Our tasks are merrily switching themselves. We have an RTOS.&lt;/p&gt;
    &lt;p&gt;Not a very good RTOS of course, and possibly not very real-time. To make it better, we should give each task a Priority, and select the task with the highest priority to run. We also need a mechanism to block a task on something other than the current time - say, to wait for something to arrive in a mailbox. I'll leave these as exercises for the reader for now, along with FPU support (because you'll note we haven't saved any of the 33 FPU registers when switching tasks).&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;Well, all told, we needed 300 lines of Rust and Assembly code, excluding the examples, to write a pre-emptive task switching scheduler. I think that's not bad at all, especially as the less code you write, the less you have to test and verify. We've seen how Armv7-M's SysTick, PendSV and PSP functions are literally tailor-made for writing an RTOS. And I don't think we missed C at any point? Very few lifetime or ownership and borrowing issues to worry about here.&lt;/p&gt;
    &lt;p&gt;As a wise man once said, "Why not try it yourself?"&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45431489</guid><pubDate>Tue, 30 Sep 2025 21:29:29 +0000</pubDate></item></channel></rss>