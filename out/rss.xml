<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 01 Oct 2025 14:41:38 +0000</lastBuildDate><item><title>Diff Algorithms</title><link>https://flo.znkr.io/diff/</link><description>&lt;doc fingerprint="7f0985451041b94"&gt;
  &lt;main&gt;
    &lt;p&gt;For software engineers, diffs are a ubiquitous method for representing changes: We use diffs to compare different versions of the same file (e.g., during code review or when trying to understand the history of a file), to visualize the difference of a failing test compared with its expectation, or to apply changes to source files automatically.&lt;/p&gt;
    &lt;p&gt;Every project I worked on professionally or privately eventually needed a diff to visualize a change or to apply a patch. However, I have never been satisfied with any of the freely available diff libraries. This was never really a problem professionally, but for private projects, I have copied and modified my own library from project to project until I mentioned this to a colleague who set me on the path to publish my Go library (a port of a previous C++ library I used to copy and modify). Boy, did I underestimate how close my library was to publishability!&lt;/p&gt;
    &lt;p&gt;Anyway, I did it and I learned a whole lot about diff algorithms. You can find my library at znkr.io/diff and what I learned in this article. I am not finished learning yet, so I plan to update this article as my understanding continues to evolve.&lt;/p&gt;
    &lt;head rend="h2"&gt;Existing Diff Libraries&lt;/head&gt;
    &lt;p&gt;Let me start by explaining why I am dissatisfied with existing diff libraries. There are a number of attributes that are important to me. Not all of these attributes are important for every use case, but a diff library that I can use for all of my use cases needs to fulfill all of them.&lt;/p&gt;
    &lt;p&gt;Usually, the input to a diff algorithm is text, and most diff libraries only support that. However, I occasionally have use cases where I need to compare things that are not text. So any diff library that only supports text doesn't meet my needs; instead, I need support for arbitrary sequences.&lt;/p&gt;
    &lt;p&gt;The resulting diff output is intended to be readable by humans. Quite often, especially for text, a good way to present a diff is in the unified format. However, it's not always the best presentation. A diff library should make it easy to output a diff in unified format, but it should also provide a way to customize the presentation by providing a structured result.&lt;/p&gt;
    &lt;p&gt;Besides the presentation, the content of a diff should make it easy for humans to understand the diff. This is a somewhat subjective criterion, but there are a number of failure cases that are easily avoided, and there's some research into diff readability to set a benchmark. On the other hand, diffs should be minimal in that they should be as small as possible.&lt;/p&gt;
    &lt;p&gt;Last but not least, it's important that a diff library has a simple API and provides good performance in both runtime and memory usage, even in worst-case scenarios1.&lt;/p&gt;
    &lt;p&gt;With that, we can evaluate existing diff libraries. For Go, I went through a number of libraries and summarized them.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Input&lt;/cell&gt;
        &lt;cell role="head"&gt;Output&lt;/cell&gt;
        &lt;cell role="head"&gt;API&lt;/cell&gt;
        &lt;cell role="head"&gt;Performance2&lt;/cell&gt;
        &lt;cell role="head"&gt;Diff&lt;p&gt;Readability&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Diff&lt;p&gt;Minimality2&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;diffmatchpatch&lt;/cell&gt;
        &lt;cell&gt;‚ùå3&lt;/cell&gt;
        &lt;cell&gt;‚ùå4&lt;/cell&gt;
        &lt;cell&gt;ü§î5&lt;/cell&gt;
        &lt;cell&gt;‚ûñ‚ûñ&lt;/cell&gt;
        &lt;cell&gt;‚ûñ&lt;/cell&gt;
        &lt;cell&gt;‚ûñ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;go-internal&lt;/cell&gt;
        &lt;cell&gt;‚ùå3&lt;/cell&gt;
        &lt;cell&gt;‚ùå6&lt;/cell&gt;
        &lt;cell&gt;üòÅ&lt;/cell&gt;
        &lt;cell&gt;‚ûï‚ûï&lt;/cell&gt;
        &lt;cell&gt;‚ûï‚ûï&lt;/cell&gt;
        &lt;cell&gt;‚ûï&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;godebug&lt;/cell&gt;
        &lt;cell&gt;‚ùå3&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;üòÅ&lt;/cell&gt;
        &lt;cell&gt;‚ûñ‚ûñ‚ûñ /üß®7&lt;/cell&gt;
        &lt;cell&gt;‚ûï&lt;/cell&gt;
        &lt;cell&gt;‚ûï‚ûï&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;mb0&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå4&lt;/cell&gt;
        &lt;cell&gt;üòê8&lt;/cell&gt;
        &lt;cell&gt;‚ûñ‚ûñ&lt;/cell&gt;
        &lt;cell&gt;‚ûï&lt;/cell&gt;
        &lt;cell&gt;‚ûï‚ûï&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;udiff&lt;/cell&gt;
        &lt;cell&gt;‚ùå3&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;üòÅ&lt;/cell&gt;
        &lt;cell&gt;‚ûï9&lt;/cell&gt;
        &lt;cell&gt;‚ûñ&lt;/cell&gt;
        &lt;cell&gt;‚ûñ‚ûñ9&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beware&lt;/p&gt;
    &lt;p&gt;The way I assigned ‚ûï and ‚ûñ in this table doesn't follow any scientific methodology it's merely based on running a few benchmarks and comparing a few results by hand. If you're looking for a diff library to fulfill your needs, I would like to encourage you to do your own comparisons. You can find the code I used for these comparisons in on github.&lt;/p&gt;
    &lt;head rend="h2"&gt;Challenges&lt;/head&gt;
    &lt;p&gt;The results suggest that it's far from trivial to implement a good diff library, and the one I had started out with wasn't much better. To understand why the existing libraries are as they are, we need to take a peek into the implementation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Complexity&lt;/head&gt;
    &lt;p&gt;With the exception of go-internal, all libraries use Myers' Algorithm to compute the diff. This is a standard algorithm that returns a minimal diff and has been in use for this purpose for decades. The algorithm has a runtime complexity of where is the number of input elements and is the edit distance between the two inputs. This means that the algorithm is very fast for inputs that are similar, which is quite common. However, it's essentially quadratic in the worst case. That is, for inputs that are very different, the complexity approaches . Furthermore, the algorithm comes in two variants with a space complexity of either or . Only godebug uses the variant with quadratic memory growth.&lt;/p&gt;
    &lt;p&gt;This means that it's relatively easy to write a well-performing diffing algorithm for small or similar inputs, but it takes a very long time to complete for larger, less similar inputs. A consequence of this is that we can't trust simple benchmarks; instead, we need to test the worst-case scenario1.&lt;/p&gt;
    &lt;p&gt;As always in cases like this, we can improve the performance by approximating an optimal solution. There are a number of heuristics that reduce the time complexity by trading off diff minimality. For example, diffmatchpatch uses a deadline to stop the search for an optimal diff, and udiff uses a an extremely aggressive heuristic.&lt;/p&gt;
    &lt;p&gt;Instead of improving Myers' runtime with heuristics, it's also often possible to find a diff using only heuristics. go-internal uses patience diff. The heuristic is good enough that it alone almost always results in a good diff with a runtime complexity of 10. An additional advantage of this algorithm is that it produces more readable diffs. However, patience diff can fail with very large diffs, and it can only be implemented efficiently using a hash table, which restricts the possible applications.&lt;/p&gt;
    &lt;p&gt;Histogram Diff&lt;/p&gt;
    &lt;p&gt;Besides patience diff, there's another interesting heuristic called histogram diff. I still have to implement it and understand it better before writing about it here, though.&lt;/p&gt;
    &lt;head rend="h3"&gt;Readability&lt;/head&gt;
    &lt;p&gt;Diff algorithms usually find a minimal diff or an approximation of one. However, except for trivial cases, there are always multiple minimal diffs. For example, this simple diff&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;b
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;c
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;d
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;is as minimal as&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;c
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;b
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;d
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Not all of the minimal or near-minimal diffs have the same readability for humans. For example11,&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (chunk == NULL) return 0;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    return start &amp;lt;= chunk-&amp;gt;length &amp;amp;&amp;amp; n &amp;lt;= chunk-&amp;gt;length - start;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(src, src_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(dst, dst_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    memcpy(dst-&amp;gt;data + dst_start, src-&amp;gt;data + src_start, n);
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (chunk == NULL) return 0;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    return start &amp;lt;= chunk-&amp;gt;length &amp;amp;&amp;amp; n &amp;lt;= chunk-&amp;gt;length - start;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;is much more readable than the equally minimal and correct&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(src, src_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(dst, dst_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (chunk == NULL) return 0;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    memcpy(dst-&amp;gt;data + dst_start, src-&amp;gt;data + src_start, n);
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    return start &amp;lt;= chunk-&amp;gt;length &amp;amp;&amp;amp; n &amp;lt;= chunk-&amp;gt;length - start;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (chunk == NULL) return 0;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(src, src_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(dst, dst_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    return start &amp;lt;= chunk-&amp;gt;length &amp;amp;&amp;amp; n &amp;lt;= chunk-&amp;gt;length - start;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    memcpy(dst-&amp;gt;data + dst_start, src-&amp;gt;data + src_start, n);
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Furthermore, if we relax minimality to accept approximations, the number of possible results increases significantly.&lt;/p&gt;
    &lt;p&gt;For good diff readability, we have to select one solution from the many possible ones that is readable for humans. Many people believe that the diff readability is determined by the algorithm. However, that's only partially correct, because different implementations of the same algorithm can produce vastly different results.&lt;/p&gt;
    &lt;p&gt;There's also been a lot of progress in the past years to improve diff readability. Perhaps the best work about diff readability is diff-slider-tools by Michael Haggerty. He implemented a heuristic that's applied in a post-processing step to improve the readability.&lt;/p&gt;
    &lt;p&gt;In fact, &lt;code&gt;example_03.diff&lt;/code&gt; above was generated using this heuristic. The diff without the heuristic,
as generated by my implementation of Myers' linear-space variant, looks like this:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (chunk == NULL) return 0;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    return start &amp;lt;= chunk-&amp;gt;length &amp;amp;&amp;amp; n &amp;lt;= chunk-&amp;gt;length - start;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(src, src_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(dst, dst_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    memcpy(dst-&amp;gt;data + dst_start, src-&amp;gt;data + src_start, n);
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (chunk == NULL) return 0;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    return start &amp;lt;= chunk-&amp;gt;length &amp;amp;&amp;amp; n &amp;lt;= chunk-&amp;gt;length - start;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Notice that the deletion starts at the end of the preceding function and leaves a small remainder of the function being deleted? Michael's heuristic fixes this problem and results in the very readable &lt;code&gt;example_03.diff&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;It's not the algorithm&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;example_04.diff&lt;/code&gt; was found using a different implementation of Myers'
linear-space variant. That is, both &lt;code&gt;example_03.diff&lt;/code&gt; and &lt;code&gt;example_04.diff&lt;/code&gt; used the same algorithm!
The differences stem from the implementation of that algorithm and from post-processing.&lt;/p&gt;
    &lt;head rend="h2"&gt;A New Diffing Library for Go&lt;/head&gt;
    &lt;p&gt;I created znkr.io/diff to address these challenges in a way that works for all my use cases. Let's reiterate what I want from a diffing library:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The input can be text and arbitrary slices&lt;/item&gt;
      &lt;item&gt;The output should be possible in unified format and as a structured result&lt;/item&gt;
      &lt;item&gt;The API should be simple&lt;/item&gt;
      &lt;item&gt;The diffs should be minimal or near-minimal&lt;/item&gt;
      &lt;item&gt;The runtime and memory performance should be excellent&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is a lot more than what any of the existing libraries provide. When I copied and modified my old diffing library, I could adapt it to the use cases at hand. But a general-purpose diffing library needs to be general enough to cover the vast majority of use cases. At the same time, it needs to be extensible to make sure new features can be implemented without cluttering the API over time.&lt;/p&gt;
    &lt;p&gt;Unfortunately, excellent performance and minimal results are somewhat in opposition to one another and I ended up providing three different modes of operation: Default (balanced between performance and minimality), Fast (sacrifice minimal results for faster speed), Optimal (minimal result whatever the cost).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Input&lt;/cell&gt;
        &lt;cell role="head"&gt;Output&lt;/cell&gt;
        &lt;cell role="head"&gt;API&lt;/cell&gt;
        &lt;cell role="head"&gt;Performance2&lt;/cell&gt;
        &lt;cell role="head"&gt;Diff&lt;p&gt;Readability&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Diff&lt;p&gt;Minimality2&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Default&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;üòÅ&lt;/cell&gt;
        &lt;cell&gt;‚ûï‚ûï&lt;/cell&gt;
        &lt;cell&gt;‚ûï‚ûï&lt;/cell&gt;
        &lt;cell&gt;‚ûï‚ûï&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Fast&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;üòÅ&lt;/cell&gt;
        &lt;cell&gt;‚ûï‚ûï‚ûï&lt;/cell&gt;
        &lt;cell&gt;‚ûï‚ûï&lt;/cell&gt;
        &lt;cell&gt;‚ûï&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Optimal&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;üòÅ&lt;/cell&gt;
        &lt;cell&gt;‚ûï&lt;/cell&gt;
        &lt;cell&gt;‚ûï‚ûï&lt;/cell&gt;
        &lt;cell&gt;‚ûï‚ûï&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Text Only&lt;/p&gt;
    &lt;p&gt;This table only applies to text (same as the table above), non-text inputs can have a different performance (if they are not &lt;code&gt;comparable&lt;/code&gt;) or readability.&lt;/p&gt;
    &lt;head rend="h3"&gt;API&lt;/head&gt;
    &lt;p&gt;To design this API, I started with the data structures that I wanted to use as a user of the API and worked backwards from there. At a very high level, there are two structured representations of a diff that have been useful to me: a flat sequence of all deletions, insertions, and matching elements (called edits) and a nested sequence of consecutive changes (called hunks).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Edits are what I use to represent edits in this article; they contain the full content of both inputs and how one is transformed into the other.&lt;/item&gt;
      &lt;item&gt;Hunks are a great representation for unit tests, because they are empty if both inputs are identical and they make it possible to visualize just the changes even if the inputs are large.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Arbitrary Slices&lt;/head&gt;
    &lt;p&gt;I started with the design for the most general case, arbitrary slices. The Go representation for diffing slices I liked the most is this one (see also znkr.io/diff):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Op describes an edit operation.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;type Op int
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;const (
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Match  Op = iota // Two slice elements match
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Delete           // A deletion from an element on the left slice
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Insert           // An insertion of an element from the right side
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Edit describes a single edit of a diff.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// - For Match, both X and Y contain the matching element.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// - For Delete, X contains the deleted element and Y is unset (zero value).
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;17&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// - For Insert, Y contains the inserted element and X is unset (zero value).
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;type Edit[T any] struct {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;19&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Op   Op
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;20&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	X, Y T
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;21&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;22&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;23&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Hunk describes a sequence of consecutive edits.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;24&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;type Hunk[T any] struct {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;25&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	PosX, EndX int       // Start and end position in x.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;26&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	PosY, EndY int       // Start and end position in y.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;27&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Edits      []Edit[T] // Edits to transform x[PosX:EndX] to y[PosY:EndY]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;28&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The alternatives I have seen are variations and combinations of two themes. Either using slices to represent edit operations in &lt;code&gt;Hunk&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;type Hunk[T any] struct {
	Delete []T
	Insert []T
	Match  []T
}
&lt;/code&gt;
    &lt;p&gt;Or using indices instead of elements&lt;/p&gt;
    &lt;code&gt;type Edit struct {
	Op         Op
	PosX, PosY []int
}
&lt;/code&gt;
    &lt;p&gt;All of these representations work, but I found that the representations above served my use cases best. One little quirk is that &lt;code&gt;Edit&lt;/code&gt; always contains both elements. This is often unnecessary, but
there are use cases where this is very important because the elements themselves might not be equal
(e.g., if they are pointers that are compared with a custom function).&lt;/p&gt;
    &lt;p&gt;Once the data structures were established, it was quite obvious that the simplest way to fill them with diff data was to write two functions &lt;code&gt;diff.Edits&lt;/code&gt; and
&lt;code&gt;diff.Hunks&lt;/code&gt; to return the diffs. I made them extensible by
using functional options.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;30&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Edits compares the contents of x and y and returns the changes necessary to convert from one to
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;31&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// the other.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;//
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;33&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Edits returns one edit for every element in the input slices. If x and y are identical, the
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;34&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// output will consist of a match edit for every input element.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;35&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func Edits[T comparable](x, y []T, opts ...Option) []Edit[T]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;36&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;37&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Hunks compares the contents of x and y and returns the changes necessary to convert from one to
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;38&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// the other.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;39&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;//
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;40&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// The output is a sequence of hunks. A hunk represents a contiguous block of changes (insertions
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;41&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// and deletions) along with some surrounding context.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;42&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func Hunks[T comparable](x, y []T, opts ...Option) []Hunk[T]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The options allow for future extensibility and allow changing the behavior of these functions. For example, the option &lt;code&gt;diff.Context(5)&lt;/code&gt; configures &lt;code&gt;Hunks&lt;/code&gt;
to provide 5 elements of surrounding context.&lt;/p&gt;
    &lt;p&gt;However, the current API still doesn't allow arbitrary slices; it only allows slices of &lt;code&gt;comparable&lt;/code&gt; types. To fix this, I needed two other functions that provide a function to compare
two elements. The Go standard library uses the &lt;code&gt;Func&lt;/code&gt; suffix for functions like this, so I followed
the lead:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;44&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// EditsFunc compares the contents of x and y using the provided equality comparison and returns the
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;45&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// changes necessary to convert from one to the other.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;46&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func EditsFunc[T any](x, y []T, eq func(a, b T) bool, opts ...Option) []Edit[T]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;47&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;48&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// HunksFunc compares the contents of x and y using the provided equality comparison and returns the
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;49&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// changes necessary to convert from one to the other.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func HunksFunc[T any](x, y []T, eq func(a, b T) bool, opts ...Option) []Hunk[T]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;Text&lt;/head&gt;
    &lt;p&gt;While this API works well to produce a structured result for arbitrary slices, it doesn't provide output in unified format for text inputs. My first approach was to provide a helper function that returns a diff in unified format: &lt;code&gt;diff.ToUnified(hunks []Hunk[string]) string&lt;/code&gt;. However, this would
make getting a unified diff more complicated. Besides requiring two function calls, it would be
necessary to split the input into lines. This, in turn, can be done in different ways, e.g., by
stripping or keeping the line breaks, which opens the door to mistakes. It's much better to provide
a simple function for the entire use case.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Unified compares the lines in x and y and returns the changes necessary to convert from one to
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// the other in unified format.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func Unified[T string | []byte](x, y T, opts ...diff.Option) T
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I also moved this function to the &lt;code&gt;textdiff&lt;/code&gt; package to
highlight the difference in expected input.&lt;/p&gt;
    &lt;p&gt;Now, I also happen to have use cases where I need structured results for text diffs. It would be very annoying if I had to split those into lines manually. Besides, I can make a few more assumptions about text that allow for a slight simplification of the data structures:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Edit describes a single edit of a line-by-line diff.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;type Edit[T string | []byte] struct {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Op   diff.Op // Edit operation
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Line T       // Line, including newline character (if any)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;17&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Hunk describes a sequence of consecutive edits.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;type Hunk[T string | []byte] struct {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;19&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	PosX, EndX int       // Start and end line in x (zero-based).
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;20&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	PosY, EndY int       // Start and end line in y (zero-based).
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;21&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Edits      []Edit[T] // Edits to transform x lines PosX..EndX to y lines PosY..EndY
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;22&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;23&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;24&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Edits compares the lines in x and y and returns the changes necessary to convert from one to the
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;25&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// other.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;26&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func Edits[T string | []byte](x, y T, opts ...diff.Option) []Edit[T]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;27&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;28&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Hunks compares the lines in x and y and returns the changes necessary to convert from one to the
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;29&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// other.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;30&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func Hunks[T string | []byte](x, y T, opts ...diff.Option) []Hunk[T]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;For the full API and examples for how to use it, please see the package documentation for znkr.io/diff and znkr.io/diff/textdiff. I am certain that there are use cases not covered by this API, but I feel confident that it can evolve to cover these use cases in the future. For now, all my needs are fulfilled, but if you run into a situation that can't be solved by this API or requires some contortions, please tell me about it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Implementation&lt;/head&gt;
    &lt;p&gt;To implement this API, we need to implement a diff algorithm. There are a couple of standard diff algorithms that we can choose from. The choice of the algorithm as well as how it's implemented matters for the readability of the result as well as the performance.&lt;/p&gt;
    &lt;p&gt;A good starting point for this project was Myers' algorithm, simply because it's the fastest algorithm that can cover the whole API. In particular, the &lt;code&gt;...Func&lt;/code&gt; variants for &lt;code&gt;any&lt;/code&gt; types
instead of &lt;code&gt;comparable&lt;/code&gt; can't make use of a hash map. Patience and Histogram require the use of a
hash map for an efficient implementation, so Myers' really is the only choice. Another advantage of
Myers' compared to Patience and Histogram is that it will return optimal results.&lt;/p&gt;
    &lt;p&gt;On the flip side, in the comparison above, it came out as relatively slow compared to the patience diff algorithm and didn't produce the most readable results. It turns out, however, that this can be mitigated and almost completely overcome for &lt;code&gt;comparable&lt;/code&gt; types using
a combination of preprocessing, heuristics, and post-processing.&lt;/p&gt;
    &lt;p&gt;I am not going to cover the diff algorithm in detail here. There are a number of excellent articles on the web that describe it12, but I recommend reading the paper13: All articles I have seen try to keep a distance from the theory that makes this algorithm work, but that's not really helpful if you want to understand how and why this algorithm works.&lt;/p&gt;
    &lt;head rend="h4"&gt;Preprocessing&lt;/head&gt;
    &lt;p&gt;The most impactful way to improve the performance of Myers' algorithm is to reduce the problem size. The simplest thing to do is to strip any common prefix and suffix. This is always possible and helps a little. However, it can also reduce diff readability, because it will consume matching elements eagerly.&lt;/p&gt;
    &lt;p&gt;For example, let's say we have this change:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;package array
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;var m = []struct{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    name  string
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    year  int
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        name: "Freak Out!",
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        year: 1966,
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    },
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        name: "Absolutely Free",
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        year: 1967,
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    },
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        name: "We're Only in It for the Money",
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;17&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        year: 1967,
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    },
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;19&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If we eagerly consume the common prefix first and then the common suffix, the first 11 lines are all identical and the so are the last 4. This in turn would result in a different diff:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;package array
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;var m = []struct{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    name  string
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    year  int
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        name: "Freak Out!",
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        year: 1966,
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    },
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        name: "Absolutely Free",
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        year: 1967,
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    },
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        name: "We're Only in It for the Money",
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;17&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        year: 1967,
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    },
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;19&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Fortunately, this is easy to fix in post processing.&lt;/p&gt;
    &lt;p&gt;Much more impactful, but only efficiently possible for &lt;code&gt;comparable&lt;/code&gt; types, is to remove all elements
that are unique to either the left side or the right side, as those must always be deletions or
insertions. Non-&lt;code&gt;comparable&lt;/code&gt; types can't be keys in a hash map in Go, which is necessary for
checking uniqueness. This preprocessing step reduced the runtime by up to
99% for a few
real-world worst-case diffs.&lt;/p&gt;
    &lt;p&gt;In contrast to the suffix and prefix removal, stripping unique elements doesn't have any readability impact.&lt;/p&gt;
    &lt;head rend="h4"&gt;Heuristics&lt;/head&gt;
    &lt;p&gt;Another very impactful way to improve the performance is Anchoring. It is based on patience diff. The word patience is a bit misleading, because it's too easily associated with having to wait and it doesn't describe the heuristic very well either. It works by finding elements that are occur exactly once on both the left and the right side. When we matching up these unique pairs we create a segmentation of the input into smaller parts that can be analyzed individually. Even better, we're very likely to find matching lines atop and below such a pair of unique elements. This allows us to shrink the segments by stripping common prefixes and suffixes. This heuristic reduced the runtime by up to 95%. Unfortunately, finding unique elements and matching them up requires a hash map again which means that it can only be used for &lt;code&gt;comparable&lt;/code&gt; types.&lt;/p&gt;
    &lt;p&gt;There are two more heuristics that are I implemented. They help for non-&lt;code&gt;comparable&lt;/code&gt; types and as a
backstop when the other heuristics don't work. Their main purpose is to avoid runaway quadratic
growth. The Good Diagonal heuristic stops searching for a better solution if we found a solution
that's good enough and the Too Expensive heuristic shortcuts the search if it becomes too
expensive which reduces the worst-case complexity from 

 to


.&lt;/p&gt;
    &lt;p&gt;However, heuristics like this trade diff minimality for performance, this is not always desirable. Sometimes, a minimal diff is exactly what's required. &lt;code&gt;diff.Optimal&lt;/code&gt; disables these heuristics to always find a
minimal diff irrespective of the costs.&lt;/p&gt;
    &lt;head rend="h4"&gt;Post-processing&lt;/head&gt;
    &lt;p&gt;We established before that a diff algorithm finds one of many possible solutions. Given such a solution we can discover more solutions by it locally and then selecting the best solution according to some metric. This is exactly how Michael Haggerty's indentation heuristic works for text.&lt;/p&gt;
    &lt;p&gt;For any given diff, we can often slide the edits up or down in a way that doesn't change the meaning of a diff. For example,&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;["foo", "bar", "baz"].map do |i|
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;  i
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;end
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;["foo", "bar", "baz"].map do |i|
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;  i.upcase
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;end
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;has the same meaning as&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;["foo", "bar", "baz"].map do |i|
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;  i
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;end
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;["foo", "bar", "baz"].map do |i|
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;  i.upcase
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;end&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We call edits that can be slid up or down sliders. The question is, how do we select the best slide? Michael collected human ratings for different sliders of the same diff and used them to develop a heuristic to match these ratings: diff-slider-tools.&lt;/p&gt;
    &lt;p&gt;However, this heuristic only works for text and is tuned towards code instead of prose. I decided to make it optional. It can be enabled with the &lt;code&gt;textdiff.IndentHeuristic&lt;/code&gt; option.&lt;/p&gt;
    &lt;head rend="h4"&gt;Diff Representation&lt;/head&gt;
    &lt;p&gt;The representation used during the execution of the diff algorithm has a surprising impact on the algorithm performance and result readability. This is not at all obvious, and so it took me a while to figure out that the best approach is akin to a side-by-side view of a diff: You use two &lt;code&gt;[]bool&lt;/code&gt;
slices to represent the left side and the right side respectively: &lt;code&gt;true&lt;/code&gt; in the left side slice
represents a deletion and on the right side an insertion. &lt;code&gt;false&lt;/code&gt; is a matching element.&lt;/p&gt;
    &lt;p&gt;This representation has four big advantages: It can be preallocated, the order in which edits are discovered doesn't matter, it's easy to mutate during post-processing, and it's easy to generate other representations from it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Open Questions&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What exactly is the reason that two different algorithms produce different results? - I looked into this question a little, but I haven't found a conclusive answer yet.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Diff algorithms are relatively complicated by themselves, but they pale in comparison to what's necessary to provide a high-quality diff library. This article tries to explain what went into my new diff library, but there's still more that I haven't implemented yet.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Here is one real-world example of why worst-case scenarios are important: Imagine you're breaking an existing feature in a way that triggers a worst-case scenario in a test. If the test is running for a very long time or runs out of memory, you're going to have to debug two problems instead of one. ‚Ü©Ô∏é ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;See benchmark_comparison.txt for the source of these ratings. ‚Ü©Ô∏é ‚Ü©Ô∏é ‚Ü©Ô∏é ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The diffmatchpatch API is very hard to use ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No support for structured results ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Quadratic memory use; for my test cases, this resulted in &amp;gt;30 GB of memory used. ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The mb0 API is from before generics and is a bit cumbersome to use ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;udiff has a very low threshold for when it starts to stop searching for an optimal solution. This improves the speed, but it also results in relatively large diffs. ‚Ü©Ô∏é ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;There's no single patience diff heuristic, instead there are different implementations with different performance characteristics. ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Stolen from https://blog.jcoglan.com/2017/03/22/myers-diff-in-linear-space-theory/ ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I can recommend https://blog.robertelder.org/diff-algorithm/ and this 5 part series https://blog.jcoglan.com/2017/02/12/the-myers-diff-algorithm-part-1/ ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Myers, E.W. An O(ND) difference algorithm and its variations. Algorithmica 1, 251-266 (1986). https://doi.org/10.1007/BF01840446 ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45430604</guid><pubDate>Tue, 30 Sep 2025 20:09:44 +0000</pubDate></item><item><title>Introduction to Multi-Armed Bandits (2019)</title><link>https://arxiv.org/abs/1904.07272</link><description>&lt;doc fingerprint="e68c8632ccf7c28f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Machine Learning&lt;/head&gt;&lt;p&gt; [Submitted on 15 Apr 2019 (v1), last revised 3 Apr 2024 (this version, v8)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Introduction to Multi-Armed Bandits&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Multi-armed bandits a simple but very powerful framework for algorithms that make decisions over time under uncertainty. An enormous body of work has accumulated over the years, covered in several books and surveys. This book provides a more introductory, textbook-like treatment of the subject. Each chapter tackles a particular line of work, providing a self-contained, teachable technical introduction and a brief review of the further developments; many of the chapters conclude with exercises.&lt;lb/&gt;The book is structured as follows. The first four chapters are on IID rewards, from the basic model to impossibility results to Bayesian priors to Lipschitz rewards. The next three chapters cover adversarial rewards, from the full-feedback version to adversarial bandits to extensions with linear rewards and combinatorially structured actions. Chapter 8 is on contextual bandits, a middle ground between IID and adversarial bandits in which the change in reward distributions is completely explained by observable contexts. The last three chapters cover connections to economics, from learning in repeated games to bandits with supply/budget constraints to exploration in the presence of incentives. The appendix provides sufficient background on concentration and KL-divergence.&lt;lb/&gt;The chapters on "bandits with similarity information", "bandits with knapsacks" and "bandits and agents" can also be consumed as standalone surveys on the respective topics.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Aleksandrs Slivkins [view email]&lt;p&gt;[v1] Mon, 15 Apr 2019 18:17:01 UTC (510 KB)&lt;/p&gt;&lt;p&gt;[v2] Mon, 29 Apr 2019 20:45:01 UTC (510 KB)&lt;/p&gt;&lt;p&gt;[v3] Tue, 25 Jun 2019 14:39:03 UTC (536 KB)&lt;/p&gt;&lt;p&gt;[v4] Sun, 15 Sep 2019 02:06:22 UTC (557 KB)&lt;/p&gt;&lt;p&gt;[v5] Mon, 30 Sep 2019 00:15:42 UTC (543 KB)&lt;/p&gt;&lt;p&gt;[v6] Sat, 26 Jun 2021 20:15:32 UTC (639 KB)&lt;/p&gt;&lt;p&gt;[v7] Sat, 8 Jan 2022 20:05:40 UTC (627 KB)&lt;/p&gt;&lt;p&gt;[v8] Wed, 3 Apr 2024 21:32:42 UTC (629 KB)&lt;/p&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.LG&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45431271</guid><pubDate>Tue, 30 Sep 2025 21:08:28 +0000</pubDate></item><item><title>CDC File Transfer</title><link>https://github.com/google/cdc-file-transfer</link><description>&lt;doc fingerprint="bcabb95bd7a534e1"&gt;
  &lt;main&gt;
    &lt;p&gt;Born from the ashes of Stadia, this repository contains tools for syncing and streaming files from Windows to Windows or Linux. The tools are based on Content Defined Chunking (CDC), in particular FastCDC, to split up files into chunks.&lt;/p&gt;
    &lt;p&gt;At Stadia, game developers had access to Linux cloud instances to run games. Most developers wrote their games on Windows, though. Therefore, they needed a way to make them available on the remote Linux instance.&lt;/p&gt;
    &lt;p&gt;As developers had SSH access to those instances, they could use &lt;code&gt;scp&lt;/code&gt; to copy
the game content. However, this was impractical, especially with the shift to
working from home during the pandemic with sub-par internet connections. &lt;code&gt;scp&lt;/code&gt;
always copies full files, there is no "delta mode" to copy only the things that
changed, it is slow for many small files, and there is no fast compression.&lt;/p&gt;
    &lt;p&gt;To help this situation, we developed two tools, &lt;code&gt;cdc_rsync&lt;/code&gt; and &lt;code&gt;cdc_stream&lt;/code&gt;,
which enable developers to quickly iterate on their games without repeatedly
incurring the cost of transmitting dozens of GBs.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;cdc_rsync&lt;/code&gt; is a tool to sync files from a Windows machine to a Linux device,
similar to the standard Linux rsync. It is
basically a copy tool, but optimized for the case where there is already an old
version of the files available in the target directory.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It quickly skips files if timestamp and file size match.&lt;/item&gt;
      &lt;item&gt;It uses fast compression for all data transfer.&lt;/item&gt;
      &lt;item&gt;If a file changed, it determines which parts changed and only transfers the differences.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The remote diffing algorithm is based on CDC. In our tests, it is up to 30x faster than the one used in &lt;code&gt;rsync&lt;/code&gt; (1500 MB/s vs 50 MB/s).&lt;/p&gt;
    &lt;p&gt;The following chart shows a comparison of &lt;code&gt;cdc_rsync&lt;/code&gt; and Linux &lt;code&gt;rsync&lt;/code&gt; running
under Cygwin on Windows. The test data consists of 58 development builds
of some game provided to us for evaluation purposes. The builds are 40-45 GB
large. For this experiment, we uploaded the first build, then synced the second
build with each of the two tools and measured the time. For example, syncing
from build 1 to build 2 took 210 seconds with the Cygwin &lt;code&gt;rsync&lt;/code&gt;, but only 75
seconds with &lt;code&gt;cdc_rsync&lt;/code&gt;. The three outliers are probably feature drops from
another development branch, where the delta was much higher. Overall,
&lt;code&gt;cdc_rsync&lt;/code&gt; syncs files about 3 times faster than Cygwin &lt;code&gt;rsync&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We also ran the experiment with the native Linux &lt;code&gt;rsync&lt;/code&gt;, i.e syncing Linux to
Linux, to rule out issues with Cygwin. Linux &lt;code&gt;rsync&lt;/code&gt; performed on average 35%
worse than Cygwin &lt;code&gt;rsync&lt;/code&gt;, which can be attributed to CPU differences. We did
not include it in the figure because of this, but you can find it
here.&lt;/p&gt;
    &lt;p&gt;The standard Linux &lt;code&gt;rsync&lt;/code&gt; splits a file into fixed-size chunks of typically
several KB.&lt;/p&gt;
    &lt;p&gt;If the file is modified in the middle, e.g. by inserting &lt;code&gt;xxxx&lt;/code&gt; after &lt;code&gt;567&lt;/code&gt;,
this usually means that the modified chunks as well as
all subsequent chunks change.&lt;/p&gt;
    &lt;p&gt;The standard &lt;code&gt;rsync&lt;/code&gt; algorithm hashes the chunks of the remote "old" file
and sends the hashes to the local device. The local device then figures out
which parts of the "new" file matches known chunks.&lt;/p&gt;
    &lt;p&gt;This is a simplification. The actual algorithm is more complicated and uses two hashes, a weak rolling hash and a strong hash, see here for a great overview. What makes &lt;code&gt;rsync&lt;/code&gt; relatively slow is the "no match" situation where the rolling hash does
not match any remote hash, and the algorithm has to roll the hash forward and
perform a hash map lookup for each byte. &lt;code&gt;rsync&lt;/code&gt; goes to
great lengths
optimizing lookups.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;cdc_rsync&lt;/code&gt; does not use fixed-size chunks, but instead variable-size,
content-defined chunks. That means, chunk boundaries are determined by the
local content of the file, in practice a 64 byte sliding window. For more
details, see
the FastCDC paper
or take a look at our implementation.&lt;/p&gt;
    &lt;p&gt;If the file is modified in the middle, only the modified chunks, but not subsequent chunks change (unless they are less than 64 bytes away from the modifications).&lt;/p&gt;
    &lt;p&gt;Computing the chunk boundaries is cheap and involves only a left-shift, a memory lookup, an &lt;code&gt;add&lt;/code&gt; and an &lt;code&gt;and&lt;/code&gt; operation for each input byte. This is cheaper
than the hash map lookup for the standard &lt;code&gt;rsync&lt;/code&gt; algorithm.&lt;/p&gt;
    &lt;p&gt;Because of this, the &lt;code&gt;cdc_rsync&lt;/code&gt; algorithm is faster than the standard
&lt;code&gt;rsync&lt;/code&gt;. It is also simpler. Since chunk boundaries move along with insertions
or deletions, the task to match local and remote hashes is a trivial set
difference operation. It does not involve a per-byte hash map lookup.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;cdc_stream&lt;/code&gt; is a tool to stream files and directories from a Windows machine to
a Linux device. Conceptually, it is similar to
sshfs, but it is optimized for read speed.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It caches streamed data on the Linux device.&lt;/item&gt;
      &lt;item&gt;If a file is re-read on Linux after it changed on Windows, only the differences are streamed again. The rest is read from the cache.&lt;/item&gt;
      &lt;item&gt;Stat operations are very fast since the directory metadata (filenames, permissions etc.) is provided in a streaming-friendly way.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To efficiently determine which parts of a file changed, the tool uses the same CDC-based diffing algorithm as &lt;code&gt;cdc_rsync&lt;/code&gt;. Changes to Windows files are almost
immediately reflected on Linux, with a delay of roughly (0.5s + 0.7s x total
size of changed files in GB).&lt;/p&gt;
    &lt;p&gt;The tool does not support writing files back from Linux to Windows; the Linux directory is readonly.&lt;/p&gt;
    &lt;p&gt;The following chart compares times from starting a game to reaching the menu. In one case, the game is streamed via &lt;code&gt;sshfs&lt;/code&gt;, in the other case we use
&lt;code&gt;cdc_stream&lt;/code&gt;. Overall, we see a 2x to 5x speedup.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;cdc_rsync&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;From&lt;/cell&gt;
        &lt;cell role="head"&gt;To&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Windows x86_64&lt;/cell&gt;
        &lt;cell&gt;‚úì&lt;/cell&gt;
        &lt;cell&gt;‚úì 1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ubuntu 22.04 x86_64&lt;/cell&gt;
        &lt;cell&gt;‚úó 2&lt;/cell&gt;
        &lt;cell&gt;‚úì&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ubuntu 22.04 aarch64&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;macOS 13 x86_64 3&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;macOS 13 aarch64 3&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;cdc_stream&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;From&lt;/cell&gt;
        &lt;cell role="head"&gt;To&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Windows x86_64&lt;/cell&gt;
        &lt;cell&gt;‚úì&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ubuntu 22.04 x86_64&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
        &lt;cell&gt;‚úì&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ubuntu 22.04 aarch64&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;macOS 13 x86_64 3&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;macOS 13 aarch64 3&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
        &lt;cell&gt;‚úó&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;1 Only local syncs, e.g. &lt;code&gt;cdc_rsync C:\src\* C:\dst&lt;/code&gt;. Support for
remote syncs is being added, see
#61.&lt;lb/&gt; 2 See #56.&lt;lb/&gt; 3 See #62.&lt;/p&gt;
    &lt;p&gt;Download the precompiled binaries from the latest release to a Windows device and unzip them. The Linux binaries are automatically deployed to &lt;code&gt;~/.cache/cdc-file-transfer&lt;/code&gt; by the Windows tools. There is no need to manually
deploy them. We currently provide Linux binaries compiled on
Github's latest Ubuntu version.
If the binaries work for you, you can skip the following two sections.&lt;/p&gt;
    &lt;p&gt;Alternatively, the project can be built from source. Some binaries have to be built on Windows, some on Linux.&lt;/p&gt;
    &lt;p&gt;To build the tools from source, the following steps have to be executed on both Windows and Linux.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Download and install Bazel from here. See workflow logs for the currently used version.&lt;/item&gt;
      &lt;item&gt;Clone the repository. &lt;code&gt;git clone https://github.com/google/cdc-file-transfer&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Initialize submodules. &lt;code&gt;cd cdc-file-transfer git submodule update --init --recursive&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Finally, install an SSH client on the Windows machine if not present. The file transfer tools require &lt;code&gt;ssh.exe&lt;/code&gt; and &lt;code&gt;sftp.exe&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The two tools CDC RSync and CDC Stream can be built and used independently.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;On a Linux device, build the Linux components &lt;code&gt;bazel build --config linux --compilation_mode=opt --linkopt=-Wl,--strip-all --copt=-fdata-sections --copt=-ffunction-sections --linkopt=-Wl,--gc-sections //cdc_rsync_server&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;On a Windows device, build the Windows components &lt;code&gt;bazel build --config windows --compilation_mode=opt --copt=/GL //cdc_rsync&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Copy the Linux build output file &lt;code&gt;cdc_rsync_server&lt;/code&gt;from&lt;code&gt;bazel-bin/cdc_rsync_server&lt;/code&gt;to&lt;code&gt;bazel-bin\cdc_rsync&lt;/code&gt;on the Windows machine.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;On a Linux device, build the Linux components &lt;code&gt;bazel build --config linux --compilation_mode=opt --linkopt=-Wl,--strip-all --copt=-fdata-sections --copt=-ffunction-sections --linkopt=-Wl,--gc-sections //cdc_fuse_fs&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;On a Windows device, build the Windows components &lt;code&gt;bazel build --config windows --compilation_mode=opt --copt=/GL //cdc_stream&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Copy the Linux build output files &lt;code&gt;cdc_fuse_fs&lt;/code&gt;and&lt;code&gt;libfuse.so&lt;/code&gt;from&lt;code&gt;bazel-bin/cdc_fuse_fs&lt;/code&gt;to&lt;code&gt;bazel-bin\cdc_stream&lt;/code&gt;on the Windows machine.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The tools require a setup where you can use SSH and SFTP from the Windows machine to the Linux device without entering a password, e.g. by using key-based authentication.&lt;/p&gt;
    &lt;p&gt;By default, the tools search &lt;code&gt;ssh.exe&lt;/code&gt; and &lt;code&gt;sftp.exe&lt;/code&gt; from the path environment
variable. If you can run the following commands in a Windows cmd without
entering your password, you are all set:&lt;/p&gt;
    &lt;code&gt;ssh user@linux.device.com
sftp user@linux.device.com
&lt;/code&gt;
    &lt;p&gt;Here, &lt;code&gt;user&lt;/code&gt; is the Linux user and &lt;code&gt;linux.device.com&lt;/code&gt; is the Linux host to
SSH into or copy the file to.&lt;/p&gt;
    &lt;p&gt;If additional arguments are required, it is recommended to provide an SSH config file. By default, both &lt;code&gt;ssh.exe&lt;/code&gt; and &lt;code&gt;sftp.exe&lt;/code&gt; use the file at
&lt;code&gt;%USERPROFILE%\.ssh\config&lt;/code&gt; on Windows, if it exists. A possible config file
that sets a username, a port, an identity file and a known host file could look
as follows:&lt;/p&gt;
    &lt;code&gt;Host linux_device
	HostName linux.device.com
	User user
	Port 12345
	IdentityFile C:\path\to\id_rsa
	UserKnownHostsFile C:\path\to\known_hosts
&lt;/code&gt;
    &lt;p&gt;If &lt;code&gt;ssh.exe&lt;/code&gt; or &lt;code&gt;sftp.exe&lt;/code&gt; cannot be found, you can specify the full paths via
the command line arguments &lt;code&gt;--ssh-command&lt;/code&gt; and &lt;code&gt;--sftp-command&lt;/code&gt; for &lt;code&gt;cdc_rsync&lt;/code&gt;
and &lt;code&gt;cdc_stream start&lt;/code&gt; (see below), or set the environment variables
&lt;code&gt;CDC_SSH_COMMAND&lt;/code&gt; and &lt;code&gt;CDC_SFTP_COMMAND&lt;/code&gt;, e.g.&lt;/p&gt;
    &lt;code&gt;set CDC_SSH_COMMAND="C:\path with space\to\ssh.exe"
set CDC_SFTP_COMMAND="C:\path with space\to\sftp.exe"
&lt;/code&gt;
    &lt;p&gt;Note that you can also specify SSH configuration via the environment variables instead of using a config file:&lt;/p&gt;
    &lt;code&gt;set CDC_SSH_COMMAND=C:\path\to\ssh.exe -p 12345 -i C:\path\to\id_rsa -oUserKnownHostsFile=C:\path\to\known_hosts
set CDC_SFTP_COMMAND=C:\path\to\sftp.exe -P 12345 -i C:\path\to\id_rsa -oUserKnownHostsFile=C:\path\to\known_hosts
&lt;/code&gt;
    &lt;p&gt;Note the small &lt;code&gt;-p&lt;/code&gt; for &lt;code&gt;ssh.exe&lt;/code&gt; and the capital &lt;code&gt;-P&lt;/code&gt; for &lt;code&gt;sftp.exe&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;For Google internal usage, set the following environment variables to enable SSH authentication using a Google security key:&lt;/p&gt;
    &lt;code&gt;set CDC_SSH_COMMAND=C:\gnubby\bin\ssh.exe
set CDC_SFTP_COMMAND=C:\gnubby\bin\sftp.exe
&lt;/code&gt;
    &lt;p&gt;Note that you will have to touch the security key multiple times during the first run. Subsequent runs only require a single touch.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;cdc_rsync&lt;/code&gt; is used similar to &lt;code&gt;scp&lt;/code&gt; or the Linux &lt;code&gt;rsync&lt;/code&gt; command. To sync a
single Windows file &lt;code&gt;C:\path\to\file.txt&lt;/code&gt; to the home directory &lt;code&gt;~&lt;/code&gt; on the Linux
device &lt;code&gt;linux.device.com&lt;/code&gt;, run&lt;/p&gt;
    &lt;code&gt;cdc_rsync C:\path\to\file.txt user@linux.device.com:~
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;cdc_rsync&lt;/code&gt; understands the usual Windows wildcards &lt;code&gt;*&lt;/code&gt; and &lt;code&gt;?&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;cdc_rsync C:\path\to\*.txt user@linux.device.com:~
&lt;/code&gt;
    &lt;p&gt;To sync the contents of the Windows directory &lt;code&gt;C:\path\to\assets&lt;/code&gt; recursively to
&lt;code&gt;~/assets&lt;/code&gt; on the Linux device, run&lt;/p&gt;
    &lt;code&gt;cdc_rsync C:\path\to\assets\* user@linux.device.com:~/assets -r
&lt;/code&gt;
    &lt;p&gt;To get per file progress, add &lt;code&gt;-v&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;cdc_rsync C:\path\to\assets\* user@linux.device.com:~/assets -vr
&lt;/code&gt;
    &lt;p&gt;The tool also supports local syncs:&lt;/p&gt;
    &lt;code&gt;cdc_rsync C:\path\to\assets\* C:\path\to\destination -vr
&lt;/code&gt;
    &lt;p&gt;To stream the Windows directory &lt;code&gt;C:\path\to\assets&lt;/code&gt; to &lt;code&gt;~/assets&lt;/code&gt; on the Linux
device, run&lt;/p&gt;
    &lt;code&gt;cdc_stream start C:\path\to\assets user@linux.device.com:~/assets
&lt;/code&gt;
    &lt;p&gt;This makes all files and directories in &lt;code&gt;C:\path\to\assets&lt;/code&gt; available on
&lt;code&gt;~/assets&lt;/code&gt; immediately, as if it were a local copy. However, data is streamed
from Windows to Linux as files are accessed.&lt;/p&gt;
    &lt;p&gt;To stop the streaming session, enter&lt;/p&gt;
    &lt;code&gt;cdc_stream stop user@linux.device.com:~/assets
&lt;/code&gt;
    &lt;p&gt;The command also accepts wildcards. For instance,&lt;/p&gt;
    &lt;code&gt;cdc_stream stop user@*:*
&lt;/code&gt;
    &lt;p&gt;stops all existing streaming sessions for the given user.&lt;/p&gt;
    &lt;p&gt;On first run, &lt;code&gt;cdc_stream&lt;/code&gt; starts a background service, which does all the work.
The &lt;code&gt;cdc_stream start&lt;/code&gt; and &lt;code&gt;cdc_stream stop&lt;/code&gt; commands are just RPC clients that
talk to the service.&lt;/p&gt;
    &lt;p&gt;The service logs to &lt;code&gt;%APPDATA%\cdc-file-transfer\logs&lt;/code&gt; by default. The logs are
useful to investigate issues with asset streaming. To pass custom arguments, or
to debug the service, create a JSON config file at
&lt;code&gt;%APPDATA%\cdc-file-transfer\cdc_stream.json&lt;/code&gt; with command line flags.
For instance,&lt;/p&gt;
    &lt;code&gt;{ "verbosity":3 }
&lt;/code&gt;
    &lt;p&gt;instructs the service to log debug messages. Try &lt;code&gt;cdc_stream start-service -h&lt;/code&gt;
for a list of available flags. Alternatively, run the service manually with&lt;/p&gt;
    &lt;code&gt;cdc_stream start-service
&lt;/code&gt;
    &lt;p&gt;and pass the flags as command line arguments. When you run the service manually, the flag &lt;code&gt;--log-to-stdout&lt;/code&gt; is particularly useful as it logs to the console
instead of to the file.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;cdc_rsync&lt;/code&gt; always logs to the console. To increase log verbosity, pass &lt;code&gt;-vvv&lt;/code&gt;
for debug logs or &lt;code&gt;-vvvv&lt;/code&gt; for verbose logs.&lt;/p&gt;
    &lt;p&gt;For both sync and stream, the debug logs contain all SSH and SFTP commands that are attempted to run, which is very useful for troubleshooting. If a command fails unexpectedly, copy it and run it in isolation. Pass &lt;code&gt;-vv&lt;/code&gt; or &lt;code&gt;-vvv&lt;/code&gt; for
additional debug output.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45433768</guid><pubDate>Wed, 01 Oct 2025 02:38:18 +0000</pubDate></item><item><title>Blockdiff: We built our own file format for VM disk snapshots</title><link>https://cognition.ai/blog/blockdiff</link><description>&lt;doc fingerprint="b51932dd553781b3"&gt;
  &lt;main&gt;
    &lt;p&gt;How we built blockdiff, an open-source tool for rapid block-level diffs and snapshots of VM disks.&lt;/p&gt;
    &lt;p&gt;We made it open-source here: https://github.com/CognitionAI/blockdiff&lt;/p&gt;
    &lt;p&gt;Usually, I‚Äôm a researcher working on areas like RL for coding agents ‚Äì but one day I became annoyed by our slow VM startup times. So I took the plunge into systems engineering and built the first version of our VM hypervisor called &lt;code&gt;otterlink&lt;/code&gt;. It now powers both our research &amp;amp; all of Devin production workloads.&lt;/p&gt;
    &lt;p&gt;Devin writes and runs code in a VM environment. Why VMs instead of Docker? For untrusted user workloads we require full isolation for security purposes. Moreover, many realistic dev environments require using Docker (e.g. to spin up a backend service or database). Good luck running Docker inside Docker, so we needed VMs.&lt;/p&gt;
    &lt;p&gt;Compared to EC2, &lt;code&gt;otterlink&lt;/code&gt; was able to bring down VM startup times by about 10x. The real pain point, however, were EC2‚Äôs long snapshot times. We want a lot of flexibility (e.g. forking, rollback and suspending VMs) that all require taking disk snapshots. On EC2 taking disk snapshots usually took a whopping 30+ minutes, which would be a terrible experience for our users. With &lt;code&gt;otterlink&lt;/code&gt; we were able to bring this down to just a couple of seconds ‚Äì a 200x speed-up.&lt;/p&gt;
    &lt;p&gt;To achieve this, we built our own file format &lt;code&gt;blockdiff&lt;/code&gt; for instant block-level diffs of VM disks. Creating block-level diffs of two files is a much broader problem that goes beyond VM disks. We assumed there must be an existing open-source solution. To our surprise we couldn‚Äôt find such a tool, so we‚Äôre open-sourcing our implementation today.&lt;/p&gt;
    &lt;p&gt;There are three reasons why we want incremental snapshots of VM disks:&lt;/p&gt;
    &lt;p&gt;Dev environments&lt;/p&gt;
    &lt;p&gt;Our customers set up their dev environment in Devin‚Äôs VM, which we save to reuse via a disk snapshot. If most customers use just 1GB of additional disk space, we don‚Äôt want all the snapshots to redundantly store the entire 15GB operating system.&lt;/p&gt;
    &lt;p&gt;Sleep &amp;amp; wake up&lt;/p&gt;
    &lt;p&gt;When Devin sleeps, we want to store the current session state without making another copy of the dev environment. The limiting factor isn‚Äôt even storage cost ‚Äì it‚Äôs wake up time. Transferring a 50 MB snapshot of session state is much faster than a multi-GB snapshot of the entire dev environment.&lt;/p&gt;
    &lt;p&gt;Disk rollback&lt;/p&gt;
    &lt;p&gt;To enable rolling back the disk during a session, we want to stack many of these incremental snapshots on top of each other.&lt;/p&gt;
    &lt;p&gt;We tried very hard to find a way to implement disk snapshotting while satisfying all these criteria:&lt;/p&gt;
    &lt;p&gt;Compact&lt;/p&gt;
    &lt;p&gt;The snapshot file should grow proportional to the difference between the base image and the VM disk. It‚Äôs too expensive to snapshot the entire disk.&lt;/p&gt;
    &lt;p&gt;Instantaneous&lt;/p&gt;
    &lt;p&gt;Taking a snapshot should be instant and should not require significant disk I/O. We design our file format so that creating snapshot operates mostly on file metadata.&lt;/p&gt;
    &lt;p&gt;Zero overhead&lt;/p&gt;
    &lt;p&gt;The VM should not experience any overhead, e.g. slower reads or writes.&lt;/p&gt;
    &lt;p&gt;Simplicity&lt;/p&gt;
    &lt;p&gt;Things like this can easily break and have thousands of edge cases, so we want a solution that‚Äôs as simple as possible, to spare ourselves lots of debugging time.&lt;/p&gt;
    &lt;p&gt;The implementation of the file format is a single, few-hundred line Rust file. It stands on the shoulders of giants: most of the complexity is handled by the Linux kernel‚Äôs excellent CoW implementation in the XFS filesystem. The core idea is simple: For two files A &amp;amp; B, &lt;code&gt;blockdiff&lt;/code&gt; stores only the blocks in B that are different from blocks in A.&lt;/p&gt;
    &lt;p&gt;To explain the difficulty of achieving all these design goals, let‚Äôs first explain the limitations of other solutions we considered:&lt;/p&gt;
    &lt;p&gt;Why not just read the files and compute a binary diff?&lt;/p&gt;
    &lt;p&gt;Trying to compute a binary diff directly based on file content would be quite slow. Even on the fastest SSDs, scanning an entire 128 GB disk image can take 30-60 seconds.&lt;/p&gt;
    &lt;p&gt;Why not OverlayFS?&lt;/p&gt;
    &lt;p&gt;For the first few weeks of &lt;code&gt;otterlink&lt;/code&gt;‚Äôs existence we used OverlayFS. However, it had two issues: It didn‚Äôt have clean support for incremental snapshots without remounting. Moreover, it created big issues when users wanted to use Docker ‚Äì which would fall back to the vfs storage driver, consume 17x more storage and be 6x slower.&lt;/p&gt;
    &lt;p&gt;Why not ZFS?&lt;/p&gt;
    &lt;p&gt;Before we started using &lt;code&gt;otterlink&lt;/code&gt;, we had implemented a version of rollback using ZFS inside of the VM. It had multiple limitations: For reliability reasons, we mounted ZFS only on the home dir, so it wasn‚Äôt possible to roll back system-level changes like package installs. Moreover, the snapshot logic had to live inside of the VM, visible to the user. We also briefly considered using ZFS outside of the VM on the hypervisor. However, we concluded that the end-to-end performance of creating &amp;amp; transferring ZFS snapshots (send/recv) seemed to most likely be lower than what we can achieve with &lt;code&gt;blockdiff&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;What about the qcow2 file format?&lt;/p&gt;
    &lt;p&gt;We didn‚Äôt deeply consider qcow2 because our hypervisor only supports raw disk images. Further below in the bonus section, we show a performance comparison that shows an example of &lt;code&gt;qemu-img convert&lt;/code&gt; becoming quite slow for large files. Evidently, qcow2 doesn‚Äôt operate on metadata only (unlike &lt;code&gt;blockdiff&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Let‚Äôs first explain two Linux concepts that are necessary to understand the rest.&lt;/p&gt;
    &lt;p&gt;Sparse files&lt;/p&gt;
    &lt;p&gt;Sparse files only allocate disk space for non-zero data. This is particularly helpful for VM disk images that have mostly unused space. For a sparse file, the logical size of the file is different from the actual disk usage. In this example, &lt;code&gt;ls -hl disk.img&lt;/code&gt; shows the logical size which is 32GB. However, it only uses 261 MB of actual disk space ‚Äì which you can see with &lt;code&gt;du -h&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;ubuntu@devin-box:~$ ls -hl disk.img
-rw-r--r-- 1 ubuntu ubuntu 32G Jan  9 07:57 disk.img
ubuntu@devin-box:~$ du -h disk.img
261M    disk.img&lt;/code&gt;
    &lt;p&gt;You can create an empty sparse file with &lt;code&gt;truncate -s&lt;/code&gt; and then format it as an &lt;code&gt;ext4&lt;/code&gt; disk image using &lt;code&gt;mkfs.ext4&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;ubuntu@devin-box:~$ truncate -s 32G disk.img
ubuntu@devin-box:~$ mkfs.ext4 disk.img
mke2fs 1.46.5 (30-Dec-2021)
Discarding device blocks: done                            
Creating filesystem with 8388608 4k blocks and 2097152 inodes
Filesystem UUID: e2fdd2d5-a1a7-4be1-a9d7-6fecdb57096c
Superblock backups stored on blocks: 
        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
        4096000, 7962624

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (65536 blocks): done
Writing superblocks and filesystem accounting information: done&lt;/code&gt;
    &lt;p&gt;Copy-on-write (CoW)&lt;/p&gt;
    &lt;p&gt;Copy-on-write is a feature supported by many modern Linux filesystems (XFS, ZFS, btrfs). Instead of immediately copying data when requested, the system shares the original data and only creates a separate copy when modifications are made to either the original or the copy. This bookkeeping happens on a block-by-block basis: blocks are the fundamental unit of storage of modern file systems and typically 4KB in size.&lt;/p&gt;
    &lt;p&gt;See the difference between copying a file with &amp;amp; without ‚Äúreflink‚Äù (another name for copy-on-write) for a 128GB disk image on a very fast NVMe SSD:&lt;/p&gt;
    &lt;code&gt;ubuntu@devin-box:~$ time cp --reflink=never base.img vm1.img

real    0m24.532s
user    0m0.142s
sys     0m18.785s

ubuntu@devin-box:~$ time cp --reflink=always base.img vm2.img

real    0m0.008s
user    0m0.001s
sys     0m0.004s&lt;/code&gt;
    &lt;p&gt;Disk images as files&lt;/p&gt;
    &lt;p&gt;For our hypervisor &lt;code&gt;otterlink&lt;/code&gt;, VM disks are just files on its filesystem. Each VM disk is a CoW copy of the base disk image (e.g. the operating system) ‚Äì which means that it shares all blocks by default and greedily allocates new blocks on write.&lt;/p&gt;
    &lt;p&gt;It‚Äôs important to differentiate between the filesystem inside &amp;amp; outside of the VMs: Inside our VMs we use ext4 as the filesystem because it‚Äôs most widespread and the default on Ubuntu. Outside, the hypervisor uses XFS as its filesystem ‚Äì crucially with reflink (= copy-on-write) enabled.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs say we have two files: &lt;code&gt;base.img&lt;/code&gt; is a disk image of our operating system and &lt;code&gt;vm.img&lt;/code&gt; is a CoW copy of &lt;code&gt;base.img&lt;/code&gt;. The VM is reading &amp;amp; writing from &lt;code&gt;vm.img&lt;/code&gt;. Our goal is to create a separate file &lt;code&gt;snapshot.bdiff&lt;/code&gt; that stores only the blocks from &lt;code&gt;vm.img&lt;/code&gt; that are different from &lt;code&gt;base.img&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;File extent maps&lt;/p&gt;
    &lt;p&gt;Our objective is to only operate on the filesystem metadata and to never touch the actual contents of the files. To be precise, the key lies in the file extent maps which you can get using the FIEMAP syscall. The &lt;code&gt;blockdiff&lt;/code&gt; tool can be used to view the syscall outputs in a nicely formatted way (or alternatively use the Linux utility &lt;code&gt;filefrag -v disk.img&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;blockdiff view disk.img&lt;/code&gt;
    &lt;p&gt;The file extent map represents the mapping from logical blocks in the file to physical blocks on the hard drive. This mapping is grouped in extents which are sequences of blocks that are allocated contiguously. You might‚Äôve heard of the word (de)fragmentation before: In an ideal world, every file would just be a single extent, stored as one contiguous chunk on the hard drive. However, due to fragmentation files usually end up split across multiple extents scattered throughout the disk.&lt;/p&gt;
    &lt;p&gt;Reading file extent maps from Rust&lt;/p&gt;
    &lt;p&gt;Using the &lt;code&gt;fiemap&lt;/code&gt; crate in Rust we have a clean wrapper around the underlying Linux syscall FIEMAP IOCTL. Getting the extents of the target file is as easy as:&lt;/p&gt;
    &lt;code&gt;let mut target_extents: Vec&amp;lt;_&amp;gt; = fiemap::fiemap(target_file)?.collect::&lt;/code&gt;
    &lt;p&gt;Each extent looks as follows:&lt;/p&gt;
    &lt;code&gt;pub struct FiemapExtent {
    pub fe_logical: u64, // logical offset (in bytes)
    pub fe_physical: u64, // physical offset (in bytes)
    pub fe_length: u64, // length of extent (in bytes)
    pub fe_flags: FiemapExtentFlags,
}&lt;/code&gt;
    &lt;p&gt;The logical block addresses are the location of data in the file (i.e. in our VM disk). The physical block addresses are where the data is stored on the hypervisor disk. An extent is a sequence of contiguous logical blocks with contiguous physical addresses. If two logical blocks from different files, point to the same physical blocks, then they are the same.&lt;/p&gt;
    &lt;p&gt;Exercise for the reader: Write an algorithm that takes in file extent map A &amp;amp; B and returns a list of extents from B that are different from A. Be careful that extent boundaries are in general not aligned between A &amp;amp; B.&lt;/p&gt;
    &lt;p&gt;Defining a file format (.bdiff)&lt;/p&gt;
    &lt;p&gt;Now the last step is to serialize this into a file:&lt;/p&gt;
    &lt;code&gt;/// - Header:
///   - 8 bytes: magic string ("BDIFFv1\0")
///   - 8 bytes: target file size (little-endian)
///   - 8 bytes: base file size (little-endian)
///   - 8 bytes: number of ranges (little-endian)
///   - Ranges array, each range containing:
///     - 8 bytes: logical offset (little-endian)
///     - 8 bytes: length (little-endian)
/// - Padding to next block boundary (4 KiB)
/// - Range data (contiguous blocks of data)&lt;/code&gt;
    &lt;p&gt;A small header contains information about which logical block ranges the file contains. After that, it stores all differing blocks contiguously. When creating (or applying) blockdiffs, writing the small header is the only disk I/O that needs to happen. All the actual data can share the same physical blocks with &lt;code&gt;vm.img&lt;/code&gt;, i.e. creating the rest of the file is purely a ‚Äúrewiring‚Äù of file metadata.&lt;/p&gt;
    &lt;p&gt;Exercise for the reader: What piece in the blockdiff codebase is responsible for the fact that the range data shares the same physical blocks as &lt;code&gt;vm.img&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;Install and build the binary:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/cognitionai/blockdiff &amp;amp;&amp;amp; cd blockdiff
cargo install&lt;/code&gt;
    &lt;p&gt;Reminder that we need to be on a filesystem with reflink enabled (e.g. XFS). Now, let‚Äôs create a snapshot of &lt;code&gt;vm1.img&lt;/code&gt; against the base image &lt;code&gt;base.img&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;blockdiff create snapshot.bdiff vm1.img --base base.img&lt;/code&gt;
    &lt;p&gt;We can use that snapshot, to create a new disk image &lt;code&gt;vm2.img:&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;blockdiff apply snapshot.bdiff vm2.img --base base.img&lt;/code&gt;
    &lt;p&gt;This file should be identical to &lt;code&gt;vm1.img&lt;/code&gt;. We can verify this with hashes:&lt;/p&gt;
    &lt;code&gt;xxhsum vm1.img
xxhsum vm2.img&lt;/code&gt;
    &lt;p&gt;Since this is only a file metadata operation, creating and applying snapshots is effectively instant ‚Äì no matter how large the disk images. After creating the snapshot file locally, the file still needs to be transferred to storage which happens with about 2 GB/s.&lt;/p&gt;
    &lt;code&gt;On our hypervisor machines:
Reading/writing 20 GB of data: ~6.5 s
Creating 20 GB snapshot with blockdiff: ~200 ms&lt;/code&gt;
    &lt;p&gt;A fun little challenge that we faced while building &lt;code&gt;otterlink&lt;/code&gt; is ‚Äúcompactifying‚Äù sparse files. If you try to upload a sparse disk image to blob storage, it will upload the entire logical size since blob storage doesn‚Äôt natively understand sparse files. So we were looking for a way to turn a sparse file of logical size X &amp;gt; disk usage Y into a ‚Äúcompact‚Äù file with logical size Y = disk usage Y.&lt;/p&gt;
    &lt;p&gt;Most online sources seemed to recommend using tar which ended up being extremely slow. You would usually expect network latency to be the main bottleneck but it turned out tar would be 5x slower than the network transfer.&lt;/p&gt;
    &lt;p&gt;Despite not using qcow2 in our hypervisor itself, it turned out that &lt;code&gt;qemu-img convert&lt;/code&gt; gave us exactly what we wanted: converting a raw, sparse disk image into a compact one. Moreover, it did it 5x faster than tar. To be clear, this was a random hack of ours and it isn‚Äôt what &lt;code&gt;qemu-img convert&lt;/code&gt; is intended to be used for. However, with larger disks it becomes clear that even qcow2 starts being slow ‚Äì it clearly isn‚Äôt a metadata-only operation. Fortunately, &lt;code&gt;blockdiff&lt;/code&gt; is super fast at all sizes!&lt;/p&gt;
    &lt;p&gt;Working on VM hypervisors was a fun foray into systems programming. Hopefully, this gave you a glimpse of the work we do at Cognition. Of course, there are many more open questions:&lt;/p&gt;
    &lt;p&gt;If you‚Äôd like to work on these problems, reach out to us or apply here!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45433926</guid><pubDate>Wed, 01 Oct 2025 03:13:16 +0000</pubDate></item><item><title>Intelligent Kubernetes Load Balancing at Databricks</title><link>https://www.databricks.com/blog/intelligent-kubernetes-load-balancing-databricks</link><description>&lt;doc fingerprint="3718e99aeb03ecb0"&gt;
  &lt;main&gt;
    &lt;p&gt;At Databricks, Kubernetes is at the heart of our internal systems. Within a single Kubernetes cluster, the default networking primitives like ClusterIP services, CoreDNS, and kube-proxy are often sufficient. They offer a simple abstraction to route service traffic. But when performance and reliability matter, these defaults begin to show their limits.&lt;/p&gt;
    &lt;p&gt;In this post, we‚Äôll share how we built an intelligent, client-side load balancing system to improve traffic distribution, reduce tail latencies, and make service-to-service communication more resilient.&lt;/p&gt;
    &lt;p&gt;If you are a Databricks user, you don‚Äôt need to understand this blog to be able to use the platform to its fullest. But if you‚Äôre interested in taking a peek under the hood, read on to hear about some of the cool stuff we‚Äôve been working on!&lt;/p&gt;
    &lt;p&gt;High-performance service-to-service communication in Kubernetes has several challenges, especially when using persistent HTTP/2 connections, as we do at Databricks with gRPC.&lt;/p&gt;
    &lt;p&gt;While this model generally works, it quickly breaks down in performance-sensitive environments, leading to significant limitations.&lt;/p&gt;
    &lt;p&gt;At Databricks, we operate hundreds of stateless services communicating over gRPC within each Kubernetes cluster. These services are often high-throughput, latency-sensitive, and run at significant scale.&lt;/p&gt;
    &lt;p&gt;The default load balancing model falls short in this environment for several reasons:&lt;/p&gt;
    &lt;p&gt;These limitations pushed us to rethink how we handle service-to-service communication within a Kubernetes cluster.&lt;/p&gt;
    &lt;p&gt;To address the limitations of kube-proxy and default service routing in Kubernetes, we built a proxyless, fully client-driven load balancing system backed by a custom service discovery control plane.&lt;/p&gt;
    &lt;p&gt;The fundamental requirement we had was to support load balancing at the application layer, and removing dependency on the DNS on a critical path. A Layer 4 load balancer, like kube-proxy, cannot make intelligent per-request decisions for Layer 7 protocols (such as gRPC) that utilize persistent connections. This architectural constraint creates bottlenecks, necessitating a more intelligent approach to traffic management.&lt;/p&gt;
    &lt;p&gt;The following table summarizes the key differences and the advantages of a client-side approach:&lt;/p&gt;
    &lt;p&gt;Table 1: Default Kubernetes LB vs. Databricks' Client-Side LB&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Feature/Aspect&lt;/cell&gt;
        &lt;cell role="head"&gt;Default Kubernetes Load Balancing (kube-proxy)&lt;/cell&gt;
        &lt;cell role="head"&gt;Databricks' Client-Side Load Balancing&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Load Balancing Layer&lt;/cell&gt;
        &lt;cell&gt;Layer 4 (TCP/IP)&lt;/cell&gt;
        &lt;cell&gt;Layer 7 (Application/gRPC)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Decision Frequency&lt;/cell&gt;
        &lt;cell&gt;Once per TCP connection&lt;/cell&gt;
        &lt;cell&gt;Per-request&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Service Discovery&lt;/cell&gt;
        &lt;cell&gt;CoreDNS + kube-proxy (virtual IP)&lt;/cell&gt;
        &lt;cell&gt;xDS-based Control Plane + Client Library&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Supported Strategies&lt;/cell&gt;
        &lt;cell&gt;Basic (Round-robin, Random)&lt;/cell&gt;
        &lt;cell&gt;Advanced (P2C, Zone-affinity, Pluggable)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Tail Latency Impact&lt;/cell&gt;
        &lt;cell&gt;High (due to traffic skew on persistent connections)&lt;/cell&gt;
        &lt;cell&gt;Reduced (even distribution, dynamic routing)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Resource Utilization&lt;/cell&gt;
        &lt;cell&gt;Inefficient (over-provisioning)&lt;/cell&gt;
        &lt;cell&gt;Efficient (balanced load)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Dependency on DNS/Proxy&lt;/cell&gt;
        &lt;cell&gt;High&lt;/cell&gt;
        &lt;cell&gt;Minimal/Minimal, not on a critical path&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Operational Control&lt;/cell&gt;
        &lt;cell&gt;Limited&lt;/cell&gt;
        &lt;cell&gt;Fine-grained&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This system enables intelligent, up-to-date request routing with minimal dependency on DNS or Layer 4 networking. It gives clients the ability to make informed decisions based on live topology and health data.&lt;/p&gt;
    &lt;p&gt;The figure shows our custom Endpoint Discovery Service in action. It reads service and endpoint data from the Kubernetes API and translates it into xDS responses. Both Armeria clients and API proxies stream requests to it and receive live endpoint metadata, which is then used by application servers for intelligent routing with fallback clusters as backup.‚Äù&lt;/p&gt;
    &lt;p&gt;We run a lightweight control plane that continuously monitors the Kubernetes API for changes to Services and EndpointSlices. It maintains an up-to-date view of all backend pods for every service, including metadata like zone, readiness, and shard labels.&lt;/p&gt;
    &lt;p&gt;A strategic advantage for Databricks was the widespread adoption of a common framework for service communication across most of its internal services, which are predominantly written in Scala. This shared foundation allowed us to embed client-side service discovery and load balancing logic directly into the framework, making it easy to adopt across teams without requiring custom implementation effort.&lt;/p&gt;
    &lt;p&gt;Each service integrates with our custom client, which subscribes to updates from the control plane for the services it depends on during the connection setup. The client maintains a dynamic list of healthy endpoints, including metadata like zone or shard, and updates automatically as the control plane pushes changes.&lt;/p&gt;
    &lt;p&gt;Because the client bypasses both DNS resolution and kube-proxy entirely, it always has a live, accurate view of service topology. This allows us to implement consistent and efficient load balancing strategies across all internal services.&lt;/p&gt;
    &lt;p&gt;The rpc client performs request-aware load balancing using strategies like:&lt;/p&gt;
    &lt;p&gt;More advanced strategies, like zone-aware routing, required careful tuning and deeper context about service topology, traffic patterns, and failure modes; a topic to explore in a dedicated follow-up post.&lt;/p&gt;
    &lt;p&gt;To ensure the effectiveness of our approach, we ran extensive simulations, experiments, and real-world metric analysis. We validated that load remained evenly distributed and that key metrics like tail latency, error rate, and cross-zone traffic cost stayed within target thresholds. The flexibility to adapt strategies per-service has been valuable, but in practice, keeping it simple (and consistent) has worked best.&lt;/p&gt;
    &lt;p&gt;Our control plane extends its utility beyond the internal service-to-service communication. It plays a crucial role in managing external traffic by speaking the xDS API to Envoy, the discovery protocol that lets clients fetch up-to-date configuration (like clusters, endpoints, and routing rules) dynamically. Specifically, it implements Endpoint Discovery Service (EDS) to provide Envoy with consistent and up-to-date metadata about backend endpoints by programming ClusterLoadAssignment resources. This ensures that gateway-level routing (e.g., for ingress or public-facing traffic) aligns with the same source of truth used by internal clients.&lt;/p&gt;
    &lt;p&gt;This architecture gives us fine-grained control over routing behavior while decoupling service discovery from the limitations of DNS and kube-proxy. The key takeaways are:&lt;/p&gt;
    &lt;p&gt;After deploying our client-side load balancing system, we observed significant improvements across both performance and efficiency:&lt;/p&gt;
    &lt;p&gt;While the rollout delivered clear benefits, we also uncovered several challenges and insights along the way:&lt;/p&gt;
    &lt;p&gt;While developing our client-side load balancing approach, we evaluated other alternative solutions. Here‚Äôs why we ultimately decided against these:&lt;/p&gt;
    &lt;p&gt;Kubernetes headless services (clusterIP: None) provide direct pod IPs via DNS, allowing clients and proxies (like Envoy) to perform their own load balancing. This approach bypasses the limitation of connection-based distribution in kube-proxy and enables advanced load balancing strategies offered by Envoy (such as round robin, consistent hashing, and least-loaded round robin).&lt;/p&gt;
    &lt;p&gt;In theory, switching existing ClusterIP services to headless services (or creating additional headless services using the same selector) would mitigate connection reuse issues by providing clients direct endpoint visibility. However, this approach comes with practical limitations:&lt;/p&gt;
    &lt;p&gt;Although headless services can offer a temporary improvement over ClusterIP services, the practical challenges and limitations made them unsuitable as a long-term solution at Databricks' scale.&lt;/p&gt;
    &lt;p&gt;Istio provides powerful Layer 7 load balancing features using Envoy sidecars injected into every pod. These proxies handle routing, retries, circuit breaking, and more - all managed centrally through a control plane.&lt;/p&gt;
    &lt;p&gt;While this model offers many capabilities, we found it unsuitable for our environment at Databricks for a few reasons:&lt;/p&gt;
    &lt;p&gt;We also evaluated Istio‚Äôs Ambient Mesh. Since Databricks already had proprietary systems for functions like certificate distribution, and our routing patterns were relatively static, the added complexity of adopting a full mesh outweighed the benefits. This was especially true for a small infra team supporting a predominantly Scala codebase.&lt;/p&gt;
    &lt;p&gt;It is worth noting that one of the biggest advantages of sidecar-based meshes is language-agnosticism: teams can standardize resiliency and routing across polyglot services without maintaining client libraries everywhere. At Databricks, however, our environment is heavily Scala-based, and our monorepo plus fast CI/CD culture make the proxyless, client-library approach far more practical. Rather than introducing the operational burden of sidecars, we invested in building first-class load balancing directly into our libraries and infrastructure components.&lt;/p&gt;
    &lt;p&gt;Our current client-side load balancing approach has significantly improved internal service-to-service communication. Yet, as Databricks continues to scale, we‚Äôre exploring several advanced areas to further enhance our system:&lt;/p&gt;
    &lt;p&gt;Cross-Cluster and Cross-Region Load Balancing: As we manage thousands of Kubernetes clusters across multiple regions, extending intelligent load balancing beyond individual clusters is critical. We are exploring technologies like flat L3 networking and service-mesh solutions, integrating seamlessly with multi-region Endpoint Discovery Service (EDS) clusters. This will enable robust cross-cluster traffic management, fault tolerance, and globally efficient resource utilization.&lt;/p&gt;
    &lt;p&gt;Advanced Load Balancing Strategies for AI Use Cases: We plan to introduce more sophisticated strategies, such as weighted load balancing, to better support advanced AI workloads. These strategies will enable finer-grained resource allocation and intelligent routing decisions based on specific application characteristics, ultimately optimizing performance, resource consumption, and cost efficiency.&lt;/p&gt;
    &lt;p&gt;If you're interested in working on large-scale distributed infrastructure challenges like this, we're hiring. Come build with us ‚Äî explore open roles at Databricks!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45434417</guid><pubDate>Wed, 01 Oct 2025 05:06:38 +0000</pubDate></item><item><title>Basic Dialects, IDEs, and Tutorials</title><link>https://github.com/JohnBlood/awesome-basic</link><description>&lt;doc fingerprint="951f683b52f730d4"&gt;
  &lt;main&gt;
    &lt;p&gt;A curated list of awesome BASIC dialects, IDEs, and tutorials&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AppGameKit - an easy-to-learn game development engine, ideal for Beginners, Hobbyists &amp;amp; Indie developers. Now anyone can quickly code and build apps for multiple platforms using AppGameKit - have your demos and games up and running on mobile devices.&lt;/item&gt;
      &lt;item&gt;atinybasic - An Actually Tiny BASIC for Arduino.&lt;/item&gt;
      &lt;item&gt;B4X - Simple, powerful, and modern development tools.&lt;/item&gt;
      &lt;item&gt;BaCon - a free BASIC to C translator for Unix-based systems, which runs on most Unix/Linux/BSD platforms, including MacOSX. It intends to be a programming aid in creating tools which can be compiled on different platforms (including 64-bit environments) while trying to revive the days of the good old BASIC.&lt;/item&gt;
      &lt;item&gt;basgo - compiles BASIC-lang to Golang.&lt;/item&gt;
      &lt;item&gt;BASIC Compiler - BASIC Compiler is an open-source BASIC compiler written in Java. It compiles a BASIC program into Java bytecode, which can be executed with any Java Virtual Machine 1.5 and higher.&lt;/item&gt;
      &lt;item&gt;BASIC8 - an integrated Fantasy Computer for games and other program development. You can create, share, and play disks in a modern BASIC dialect, with built-in tools for editing sprites, tiles, maps, quantized, etc.&lt;/item&gt;
      &lt;item&gt;BASIC-256 - an easy-to-use version of BASIC designed to teach anybody how to program. A built-in graphics mode lets them draw pictures on screen in minutes, and a set of easy-to-follow tutorials introduce programming concepts through fun exercises.&lt;/item&gt;
      &lt;item&gt;BBCSDL - an advanced cross-platform implementation of BBC BASIC for Windows, Linux (x86 CPU only), MacOS, Raspberry Pi (Raspbian), Android, iOS or for running in a browser. It combines the simplicity of BASIC with the sophistication of a structured language, allowing you to write utilities and games, use sound and graphics, and perform calculations.&lt;/item&gt;
      &lt;item&gt;BCX - BCX converts your BCX BASIC source code into high-performing, efficient C\C++ source code. Use C\C++ libraries and header files without having to first convert them into BASIC.&lt;/item&gt;
      &lt;item&gt;BlitzMax DX - a fork of BlitzMax NG.&lt;/item&gt;
      &lt;item&gt;BlitzMax NG - a fast cross-platform, open-source, programming language.&lt;/item&gt;
      &lt;item&gt;bootBASIC - a BASIC language in 512 bytes of x86 machine code.&lt;/item&gt;
      &lt;item&gt;Bywater BASIC Interpreter - implements a large superset of the ANSI Standard for Minimal BASIC (X3.60-1978) and a significant subset of the ANSI Standard for Full BASIC (X3.113-1987) in C.&lt;/item&gt;
      &lt;item&gt;Cerberus X - A cross-platform development toolset which serves 2D game development at its core. Cerberus X is a fork of the Monkey X programming language.&lt;/item&gt;
      &lt;item&gt;Chipmunk Basic - an interpreter for the BASIC Programming Language. It runs on multiple OS platforms and is reasonably fast for a pure interpreter. Chipmunk Basic presents a traditional (vintage) terminal-command-line programming environment and supports a simple, old-fashioned, and easy-to-learn dialect of the Basic Programming Language. (Line numbers are required when using the built-in command-line console, but are not required in Basic programs written using an external text editor.) The language also supports a few advanced extensions. Free for educational and personal use.&lt;/item&gt;
      &lt;item&gt;cbmbasic - a portable version of Commodore's version of Microsoft BASIC 6502 as found on the Commodore 64.&lt;/item&gt;
      &lt;item&gt;Dark Basic Pro - an open-source BASIC programming language for creating Windows applications and games.&lt;/item&gt;
      &lt;item&gt;endbasic - BASIC environment with a REPL, a web interface, and RPi support written in Rust.&lt;/item&gt;
      &lt;item&gt;FreeBASIC - a free/open source (GPL), BASIC compiler for Microsoft Windows, DOS, and Linux.&lt;/item&gt;
      &lt;item&gt;FutureBasic - a high-level procedural programming language combined with an "Integrated Development Environment" (IDE) for creating native Intel Macintosh applications. It provides an editor, compiler, project manager, documentation, and code samples.&lt;/item&gt;
      &lt;item&gt;Gambas - a free development environment and a full powerful development platform based on a Basic interpreter with object extensions, as easy as Visual Basic.&lt;/item&gt;
      &lt;item&gt;GLBasic - an easy-to-learn BASIC language with Editor, Compiler, and Debugger. The generated C++ code compiles to lightning-fast apps for several platforms.&lt;/item&gt;
      &lt;item&gt;JADE - AKA "Jade's A Developing Experiment" - This is a proof of concept using a BASIC-like syntax to program C++.&lt;/item&gt;
      &lt;item&gt;jScriptBasic - ScriptBasic for Java is a BASIC interpreter that can be embedded into Java programs.&lt;/item&gt;
      &lt;item&gt;Just BASIC - a programming language for Windows. It is completely free and it is suitable for creating all kinds of applications for business, industry, education, and entertainment.&lt;/item&gt;
      &lt;item&gt;jvmBASIC - A BASIC to JVM bytecode compiler.&lt;/item&gt;
      &lt;item&gt;KayaBASIC - Multi-platform BASIC compiler, that supports Windows, Linux, and macOS. easy extends with C++.&lt;/item&gt;
      &lt;item&gt;Liberty BASIC - The commercial version of Just BASIC.&lt;/item&gt;
      &lt;item&gt;LychenBASIC - anachronistic, Windows-only, BASIC language programming blog post explanation.&lt;/item&gt;
      &lt;item&gt;MatrixBrandy - a Fork of Brandy BASIC V for Linux. Brandy implements Basic VI, the the 64-bit floating-point mathematics variant of the dialect of Basic that Acorn Computers supplied with their ranges of desktop computers that use the ARM processor such as the Archimedes and RiscPC. Basic V and VI are an extended version of BBC Basic. This was the Basic used on the BBC Micro that Acorn made during the early 1980s.&lt;/item&gt;
      &lt;item&gt;MBC - MBC is a Basic to C/C++ translator, originally based on the BCX Windows translator by Kevin Diggins. It has successfully compiled using Clang++ and G++ on macOS/Linux 64bit OS's, and G++ on RaspberryPi.&lt;/item&gt;
      &lt;item&gt;micro(A) - micro(A) is a modern and minimal general purpose programming language. It is Easy to Use BASIC-like Programming Language. micro(A) interpreter comes with a complete IDE in which you can make your programs.&lt;/item&gt;
      &lt;item&gt;Monkey 2 - Monkey2 is an easy-to-use, cross-platform, games oriented programming language from Blitz Research.&lt;/item&gt;
      &lt;item&gt;my_basic - A lightweight BASIC interpreter written in standard C in dual files. Aims to be embeddable, extendable, and portable.&lt;/item&gt;
      &lt;item&gt;NaaLaa - stands for 'Not An Advanced Language At All'. It's a very easy-to-learn programming language for beginners interested in retro-style game development. NaaLaa is free to use and you may do whatever you want with the programs you create with it. NaaLaa runs and compiles on Windows and Linux.&lt;/item&gt;
      &lt;item&gt;NSB/AppStudio - A complete, powerful development environment. Create apps for iOS, Android, Windows, MacOS and Linux.&lt;/item&gt;
      &lt;item&gt;nuBASIC - nuBASIC is an implementation of a BASIC interpreter and IDE for Windows and Linux.&lt;/item&gt;
      &lt;item&gt;nuBScript - nuBScript is a programming language distributed under MIT License.&lt;/item&gt;
      &lt;item&gt;Oxygen Basic - a Compact embeddable JIT compiler that reads C headers and compiles to x86 machine code. Executes directly in memory or creates DLLs and EXE files. Supports overloading and OOP. Currently available for MS platforms.&lt;/item&gt;
      &lt;item&gt;PC-BASIC - a free, cross-platform interpreter for GW-BASIC, Advanced BASIC (BASICA), PCjr Cartridge Basic and Tandy 1000 GWBASIC.&lt;/item&gt;
      &lt;item&gt;PuffinBASIC - BASIC interpreter written in Java.&lt;/item&gt;
      &lt;item&gt;PureBasic - a modern BASIC programming language. The key features of PureBasic are portability (Windows, Linux, and OS X supported with the same source code), the production of very fast and optimized native 32-bit or 64-bit executables, and, of course, the very simple BASIC language syntax. PureBasic has been created for the beginner and expert alike. We have put a lot of effort into its conception to produce a fast, reliable system and friendly BASIC compiler.&lt;/item&gt;
      &lt;item&gt;PyBasic - Simple interactive BASIC interpreter written in Python&lt;/item&gt;
      &lt;item&gt;QB64 - a modern extended BASIC programming language that retains QBasic/QuickBASIC 4.5 compatibility and compiles native binaries for Windows, Linux, and macOS.&lt;/item&gt;
      &lt;item&gt;Quite BASIC - a web-based classic BASIC online programming environment.&lt;/item&gt;
      &lt;item&gt;RAD Basic - 100% compatible with your Visual Basic 6 projects.&lt;/item&gt;
      &lt;item&gt;RCBasic - a simple easy-to-learn programming language with many built-in functions to aid in game and multimedia application development. RCBasic is free software distributed under the Zlib license.&lt;/item&gt;
      &lt;item&gt;RemObjects Mercury - Mercury is an implementation of the BASIC programming language that is fully code-compatible with Microsoft Visual Basic.NET‚Ñ¢ but takes it to the next level, and to new horizons.&lt;/item&gt;
      &lt;item&gt;RetroBASIC - RetroBASIC is an interpreter that aims to run programs from any early dialect. It is fully compatible with Microsoft BASIC from the 6502 machines, but also supports real Dartmouth BASIC, HP 2000 and many, many others. Extensive documentation explains these variations and their support in RetroBASIC.&lt;/item&gt;
      &lt;item&gt;sdlBASIC - A easy basic in order to make games in 2d style amos for linux and windows.&lt;/item&gt;
      &lt;item&gt;SharpBASIC - SharpBASIC is a new programming language that is currently in development. As the name suggests the language should be considered a sharper BASIC, more powerful, less verbose but with a clear structure.&lt;/item&gt;
      &lt;item&gt;SmallBASIC - a fast and easy-to-learn BASIC language interpreter ideal for everyday calculations, scripts, and prototypes. SmallBASIC includes trigonometric, matrices, and algebra functions, a built in IDE, a powerful string library, system, sound, and graphic commands along with structured programming syntax.&lt;/item&gt;
      &lt;item&gt;SmallBasic - Small Basic is the only programming language created specially to help students transition from block-based coding to text-based coding.&lt;/item&gt;
      &lt;item&gt;SpecBAS - an enhanced Sinclair BASIC interpreter for modern PCs.&lt;/item&gt;
      &lt;item&gt;SpiderBasic - A Basic to master the web.&lt;/item&gt;
      &lt;item&gt;thinBASIC - a very fast "BASIC-like" programming language useful to Beginners and to Gurus. BASIC interpreter for Windows able to create console and gui applications with most of the user interface controls, automate process, automate data exchange, connect to databases, send mails, connect to FTP sites, rest api, parsing strings, tokenizing, traversing xml, handling files, Windows Registry, OpenGl, graphics, sound, printing ... and much more.&lt;/item&gt;
      &lt;item&gt;TrekBasic - TrekBasic provides both a BASIC interpreter and a compiler. Using LLVM as a backend allows you to compile for any platform LLVM supports. The interpreter shell provides tools like breakpoints, execution trace, data breakpoints, and more. The compiler offers high performance. TrekBasic is written in python, and is small, and easily modified to your needs.&lt;/item&gt;
      &lt;item&gt;twinBASIC - twinBASIC is a modern version of the classic BASIC programming language.&lt;/item&gt;
      &lt;item&gt;wwwBASIC - an implementation of BASIC that runs on Node.js and the Web.&lt;/item&gt;
      &lt;item&gt;X11-Basic - a dialect of the BASIC programming language with graphics and sound.&lt;/item&gt;
      &lt;item&gt;XC=BASIC - a dialect of the BASIC programming language for the Commodore-64 and xcbasic64 is a cross compiler that compiles an XC=BASIC program to 6502 machine code trough the DASM macro assembler. It runs on Windows, Linux, and Mac OS. The name XC=BASIC stands for "Cross Compiled BASIC".&lt;/item&gt;
      &lt;item&gt;Xojo - Build Native, Cross-Platform Apps. Rapid application development for Desktop, Web, Mobile &amp;amp; Raspberry Pi. Develop on macOS, Windows, or Linux.&lt;/item&gt;
      &lt;item&gt;Yabasic - a traditional basic-interpreter. It comes with goto and various loops and allows to define subroutines and libraries. It does simple graphics and printing. Yabasic can call out to libraries written in C and allows to create standalone programs. Yabasic runs under Unix and Windows and has comprehensive documentation; it is small, simple, open-source and free.&lt;/item&gt;
      &lt;item&gt;YAB - a complete BASIC programming language for Haiku. Yab allows fast prototyping with simple and clean code. yab contains a large number of BeAPI-specific commands for GUI creation and much, much more. yab-IDE is a powerful development environment, which of course is programmed in yab itself.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;DavsIDE - an Alternative IDE for the QB64 compiler.&lt;/item&gt;
      &lt;item&gt;InForm - a GUI engine and WYSIWYG interface designer for QB64.&lt;/item&gt;
      &lt;item&gt;QBASDOWN - a Markdown implementation for FreeDOS. Written for FreeDOS in QuickBASIC 4.5&lt;/item&gt;
      &lt;item&gt;mono-basic - Visual Basic Compiler and Runtime.&lt;/item&gt;
      &lt;item&gt;VisualFBEditor - an IDE for FreeBasic under active development.&lt;/item&gt;
      &lt;item&gt;vscode-vba - Extension that adds rich VBA editor support to Visual Studio Code.&lt;/item&gt;
      &lt;item&gt;WinFBE - FreeBASIC Editor for Windows.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Collection of Commodore BASIC programs&lt;/item&gt;
      &lt;item&gt;Hoard of GW-BASIC - A collection of GW-BASIC code by various authors.&lt;/item&gt;
      &lt;item&gt;MBASIC-Protect - Information on the CP/M MBASIC interpreter's protect mode.&lt;/item&gt;
      &lt;item&gt;GW-BASIC source code - The original source code of Microsoft GW-BASIC from 1983.&lt;/item&gt;
      &lt;item&gt;Project Cherry - a Chip-8/SCHIP emulator written in FreeBASIC.&lt;/item&gt;
      &lt;item&gt;The Basics' page (since 2001)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A Beginner's Guide To FreeBasic&lt;/item&gt;
      &lt;item&gt;A Beginner's Guide to Gambas&lt;/item&gt;
      &lt;item&gt;BASIC Gaming - a short-lived ezine that has a wealth of information&lt;/item&gt;
      &lt;item&gt;BlitzMax for the Absolute Beginner&lt;/item&gt;
      &lt;item&gt;Franktic's FreeBASIC Programming Tutorial&lt;/item&gt;
      &lt;item&gt;Programming with yab&lt;/item&gt;
      &lt;item&gt;QB64 Game Programming&lt;/item&gt;
      &lt;item&gt;QBasic (QB64) Tutorial Video Series by SchoolFreeware&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45434511</guid><pubDate>Wed, 01 Oct 2025 05:22:06 +0000</pubDate></item><item><title>High-resolution efficient image generation from WiFi Mapping</title><link>https://arxiv.org/abs/2506.10605</link><description>&lt;doc fingerprint="1c1c260941f11282"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Computer Vision and Pattern Recognition&lt;/head&gt;&lt;p&gt; [Submitted on 12 Jun 2025 (v1), last revised 5 Sep 2025 (this version, v3)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:High-resolution efficient image generation from WiFi CSI using a pretrained latent diffusion model&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:We present LatentCSI, a novel method for generating images of the physical environment from WiFi CSI measurements that leverages a pretrained latent diffusion model (LDM). Unlike prior approaches that rely on complex and computationally intensive techniques such as GANs, our method employs a lightweight neural network to map CSI amplitudes directly into the latent space of an LDM. We then apply the LDM's denoising diffusion model to the latent representation with text-based guidance before decoding using the LDM's pretrained decoder to obtain a high-resolution image. This design bypasses the challenges of pixel-space image generation and avoids the explicit image encoding stage typically required in conventional image-to-image pipelines, enabling efficient and high-quality image synthesis. We validate our approach on two datasets: a wide-band CSI dataset we collected with off-the-shelf WiFi devices and cameras; and a subset of the publicly available MM-Fi dataset. The results demonstrate that LatentCSI outperforms baselines of comparable complexity trained directly on ground-truth images in both computational efficiency and perceptual quality, while additionally providing practical advantages through its unique capacity for text-guided controllability.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Eshan Ramesh [view email]&lt;p&gt;[v1] Thu, 12 Jun 2025 11:47:23 UTC (6,672 KB)&lt;/p&gt;&lt;p&gt;[v2] Fri, 4 Jul 2025 12:27:28 UTC (6,672 KB)&lt;/p&gt;&lt;p&gt;[v3] Fri, 5 Sep 2025 11:39:36 UTC (9,960 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45434941</guid><pubDate>Wed, 01 Oct 2025 06:33:02 +0000</pubDate></item><item><title>Type Theory and Functional Programming (1999) [pdf]</title><link>https://www.cs.cornell.edu/courses/cs6110/2015sp/textbook/Simon%20Thompson%20textbook.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45435100</guid><pubDate>Wed, 01 Oct 2025 07:00:21 +0000</pubDate></item><item><title>Category Theory Illustrated ‚Äì Natural Transformations</title><link>https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/</link><description>&lt;doc fingerprint="307d2805cbf1b5d"&gt;
  &lt;main&gt;&lt;quote&gt;&lt;p&gt;I didn‚Äôt invent categories to study functors; I invented them to study natural transformations. ‚Äî Saunders Mac Lane&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;In this chapter, we will introduce the concept of a morphism between functors, or natural transformation. Understanding natural transformations will enable us to define category equality and some other advanced concepts.&lt;/p&gt;&lt;p&gt;Natural transformations really are at the heart of category theory, however, their importance is not obvious at first. So, before introducing them, I like to talk, once more, about the body of knowledge that this heart maintains (I am good with metaphors‚Ä¶ in principle).&lt;/p&gt;&lt;p&gt;Our first section aims to introduce natural transformation as a motivating example for creating a way to say that two categories are equal. But for that, we need to understand what equal categories are and should be.&lt;/p&gt;&lt;p&gt;So, are you ready to hear about equivalent categories and natural transformations? Actually it is my opinion that you are not (no offence, they are just very hard!). So, we will take a longer route. I can put this next section anywhere in this book, and it would always be neither here nor there. But anyway, if you are studying math, you are probably interested in the nature of the universe. ‚ÄúWhat is the quintessential characteristic of all things in this world?‚Äù I hear you ask‚Ä¶&lt;/p&gt;&lt;quote&gt;&lt;p&gt;The world is the collection of facts, not of things. ‚Äî Ludwig Wittgenstein&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;What is the quintessential characteristic of all things in this world? Some 2500 years ago, the philosopher Parmenides gave an answer to this question, postulating that the nature of the universe is permanence, stasis. According to his view, what we perceive as processes/transformations/change is merely illusory appearances (‚ÄúWhatever is is, and what is not cannot be‚Äù). He said that that things never really change, they only appear to change, or (another way to put it), only appearances change, but the essence does not (I think this is pretty much how the word ‚Äúessence‚Äù came to exist).&lt;/p&gt;&lt;p&gt;Although far from obviously true, his view is easy for people to relate to ‚Äî objects are all around us, everything we ‚Äúsee‚Äù, both literally (in real life), or metaphorically (in mathematics and other disciplines), can be viewed as objects, persisting through space and time. If we subscribe to this view, then we would think that the key to understanding the world is understanding what objects are. In my opinion, this is what set theory does, to some extent, as well as classical logic (Plato was influenced by Parmenides when he created his theory of forms).&lt;/p&gt;&lt;p&gt;However, there is another way to approach the question about the nature of the universe, which is equally compelling. Because, what is an object, when viewed by itself? Can we study an object in isolation? And will there anything left to study about it, once it is detached from its environment? If a given object undergoes a process to get all of it‚Äôs part replaced, is it still the same object?&lt;/p&gt;&lt;p&gt;Asking such questions might lead us to suspect that, although what we see when we look at the universe are the objects, it is the processes/relations/transitions or morphisms between the objects that are the real key to understanding it. For example, when we think hard about everyday objects we realize that each of them has a specific functions (note the term) without which, a thing would not be itself e.g. is a lamp that doesn‚Äôt glow, still a lamp? Is there food that is non-edible (or an edible item that isn‚Äôt food)? And this is even more valid for mathematical objects, which, without the functions that go between them, are not objects at all.&lt;/p&gt;&lt;p&gt;So, instead of thinking about objects that just happen to have some morphisms between them, we might take the opposite view and say that objects are only interesting as sources and targets of morphisms.&lt;/p&gt;&lt;p&gt;Although old, dating back to Parmenides‚Äô alleged rival Heraclitus, this view has been largely unexplored, until the 20th century, when a real mathematical revolution happened: Bertrand Russell created type theory, his student Ludwig Wittgenstein wrote a little book, from which the above quote comes, and this book inspired a group of mathematicians and logicians, known as the ‚ÄúVienna circle‚Äù. Part of this group was Rudolph Carnap who coined the word ‚Äúfunctor‚Äù‚Ä¶&lt;/p&gt;&lt;p&gt;An embodiment of Heraclitus‚Äô view in the realm of category theory is the concept of isomorphism invariance that we implicitly touched several times.&lt;/p&gt;&lt;p&gt;All categorical constructions that we covered (products/coproducts, initial/terminal objects, functional objects in logic) are isomorphism-invariant. Or, equivalently, they define an objects up to an isomorphism. Or, in other words, if there are two or more objects that are isomorphic to one another, and one of them has a given property, then the rest of them would to also have this property as well.&lt;/p&gt;&lt;p&gt;In short, in category theory isomorphism = equality.&lt;/p&gt;&lt;p&gt;The key to understanding category theory lies in understanding isomorphism invariance. And the key to understanding isomorphism invariance are natural transformations.&lt;/p&gt;&lt;p&gt;Let‚Äôs return to the question that we were pondering at the beginning of the previous chapter ‚Äî what does it mean for two categories to be equal?&lt;/p&gt;&lt;p&gt;In the prev chapter, we talked a lot about how great isomorphisms are and how important they are for defining the concept of equality in category theory, but at the same time we said that categorical isomorphisms do not capture the concept of equality of categories.&lt;/p&gt;&lt;p&gt;This is because (though it may seem contradictory at first) categorical isomorphisms are not isomorphism invariant, i.e. categories that only differ by having some additional isomorphic objects aren‚Äôt isomorphic themselves.&lt;/p&gt;&lt;p&gt;For this reason, we need a new concept of equality of categories. A concept that would elucidate the differences between categories with different structure, but also the sameness of categories that have the same categorical structures, disregarding the differences that are irrelevant for category-theoretic standpoint. That concept is equivalence.&lt;/p&gt;&lt;p&gt;Parmenides: This category surely cannot be equal to the other one ‚Äî it has a different amount of objects!&lt;/p&gt;&lt;p&gt;Heraclitus: Who cares bro, they are isomorphic.&lt;/p&gt;&lt;p&gt;To understand equivalent categories better, let‚Äôs go back to the functor between a given map and the area it represents (we will only consider the thin categories (AKA orders) for now). This functor would be invertible (and the categories ‚Äî isomorphic) when the map should represent the area completely i.e. there should be arrow for each road and a point for each little place.&lt;/p&gt;&lt;p&gt;Such a map is necessary if your goal is to know about all places, however, like we said, when working with category theory, we are not so interested in places, but in the routes that connect them i.e. we focus not on objects but on morphisms.&lt;/p&gt;&lt;p&gt;For example, if there are intersections that are positioned in such a way that there are routes from one and to the other and vice-versa a map may collapse them into one intersection and still show all routes that exist (the tree routes would be represented by the ‚Äúidentity route‚Äù).&lt;/p&gt;&lt;p&gt;These two categories are not isomorphic ‚Äî going from one of them to the other and back again doesn‚Äôt lead you to the same object.&lt;/p&gt;&lt;p&gt;However, going from one of them to the other would lead you at least to an isomorphic object.&lt;/p&gt;&lt;p&gt;In this case we say that the orders are equivalent.&lt;/p&gt;&lt;p&gt;We know that two orders are isomorphic if there are two functors, such that going from one to the other and back again leads you to the same object.&lt;/p&gt;&lt;p&gt;And two orders are equivalent if going from one of them to the other and back again leads you to the same object, or to an object that is isomorphic to the one you started with.&lt;/p&gt;&lt;p&gt;But when does this happen? To understand this, we plot the orders as a Hasse diagram.&lt;/p&gt;&lt;p&gt;You can see that, although not all objects are connected one-to-one, all objects at a given level are connected to objects of the corresponding level.&lt;/p&gt;&lt;p&gt;To formalize that notion, we remember the concept of equivalence classes that we covered in the chapter about orders. Let‚Äôs visualize the relationship of the equivalence classes of the two orders that we saw above.&lt;/p&gt;&lt;p&gt;You can see that they are isomorphic. And that is no coincidence: two orders are equivalent precisely when the orders made of their equivalence classes are isomorphic.&lt;/p&gt;&lt;p&gt;This is a definition for equivalence of orders, but unfortunately, it does not hold for all categories ‚Äî when we are working with orders, we can get away by just thinking about objects, but categories demands that we think about morphisms i.e. to prove two categories are equivalent, we should establish an isomorphism between their morphisms.&lt;/p&gt;&lt;p&gt;For example, the following two categories are not equivalent, although their equivalence classes are isomorphic ‚Äî the category on the left has just one morphism, but the category on the right has two.&lt;/p&gt;&lt;p&gt;One way of defining equivalence of categories is by generalizing the notion of equivalence classes of orders to what we call skeletons of categories, a skeleton of a category being a subcategory in which all objects that are isomorphic to one another are ‚Äúmerged‚Äù into one object (isomorphic objects are necessarily identical).&lt;/p&gt;&lt;p&gt;However, we will leave this (pardon my French) as an exercise for the reader. Why? We already did this when we generalized the notion of normal set-theoretic functions to functors, and so it makes more sense to build up on that notion. Also, we need a motivating example for introducing natural transformations, remember?&lt;/p&gt;&lt;p&gt;In the chapter about orders, we presented a definition of order isomorphisms, that is based on objects:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;An order isomorphism is essentially an isomorphism between the orders‚Äô underlying sets (invertible function). However, besides their underlying sets, orders also have the arrows that connect them, so there is one more condition: in order for an invertible function to constitute an order isomorphism it has to respect those arrows, in other words it should be order preserving. More specifically, applying this function (let‚Äôs call it $F$) to any two elements in one set ($a$ and $b$) should result in two elements that have the same corresponding order in the other set (so $a ‚â§ b$ if and only if $F(a) ‚â§ F(b)$).&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;That a way to define them, but it is not the best way. Now that we know about functors (which, as we said, serve as functions between the orders and other categories), we can devise a new, simpler definition, which would also be valid for all categories, not just orders, and for all forms of equality (isomorphism and equivalence).&lt;/p&gt;&lt;p&gt;We begin with the definition of set isomorphism:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Two sets $A$ and $B$ are isomorphic (or $A ‚âÖ B$) if there exist functions $f: A \to B$ and its reverse $g: B \to A$, such that $f \circ g = ID_{B}$ and $g \circ f = ID_{A}$.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;To amend it so it is valid for all categories by just replacing the word ‚Äúfunction‚Äù with ‚Äúfunctor‚Äù and ‚Äúset‚Äù with ‚Äúcategory‚Äù:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Two categories $A$ and $B$ are isomorphic (or $A \cong B$) if there exist functors $f: A \to B$ and its reverse $g: B \to A$, such that $f \circ g = ID_{B}$ and $g \circ f = ID_{A}$.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Task 1: Check if that definition is valid.&lt;/p&gt;&lt;p&gt;Believe it or not, this definition, is just one find-and-replace operation away from the definition of equivalence. We get there only by replace equality with isomorphism (so, $=$ with $\cong$).&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Two categories $A$ and $B$ are equivalent (or $A \simeq B$) if there exist functors $f: A \to B$ and its reverse $g: B \to A$, such that $f \circ g \cong ID_{B}$ and $g \circ f \cong ID_{A}$.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Like we said at the beginning, with isomorphisms, going back and forth brings us to the same object, while with equivalence the object is just isomorphic to the original one. This is truly all there is to it.&lt;/p&gt;&lt;p&gt;There is only one problem, though ‚Äî we never said what it means for functors to be isomorphic.&lt;/p&gt;&lt;p&gt;So, how can we make the above definition ‚Äúcome to life‚Äù? The title of this chapter outlines the things we need to define:&lt;/p&gt;&lt;p&gt;If this sounds complicated, remember that we are doing the same thing we always did ‚Äî talking about isomorphisms.&lt;/p&gt;&lt;p&gt;In the very first chapter of this book, we introduced set isomorphisms, which are quite easy, and now we reached the point to examine functor isomorphisms. So, we are doing the same thing. Although actually‚Ä¶&lt;/p&gt;&lt;p&gt;But actually, natural transformations are quite different from morphisms and functors, (the definition is not ‚Äúrecursive‚Äù, like the definitions of functor and morphism are). This is because functions and functors are both morphisms between objects (or 1-morphisms), while natural transformations are morphisms between morphisms (known as 2-morphisms).&lt;/p&gt;&lt;p&gt;But enough talking, let‚Äôs draw some diagrams. We know that natural transformations are morphisms between functors, so let‚Äôs draw two functors.&lt;/p&gt;&lt;p&gt;The functors have the same signature. Naturally. How else can there be morphisms between them?&lt;/p&gt;&lt;p&gt;Now, a functor is comprised of two mappings (object mapping and morphism mapping) so a mapping between functors, would consist of ‚Äúobject mapping mapping‚Äù and ‚Äúmorphism mapping mapping‚Äù (yes, I often do get in trouble with my choice of terminology, why do you ask?).&lt;/p&gt;&lt;p&gt;Let‚Äôs first connect the object mappings of the two functors, creating what we called ‚Äúobject mapping mapping‚Äù.&lt;/p&gt;&lt;p&gt;It is simpler than it sounds when we realize that we only need to connect the object in functors‚Äô target category ‚Äî the objects in the source category would just always be the same for both functors, as both functors would include all object from the source category (as that is what functors (and morphisms in general) do). In other words, mapping the two functors‚Äô object components involves nothing more than specifying a bunch of morphisms in the target category: one morphism for each object in the source category i.e. each object from the image of the first functor, should have one arrow coming from it (and to an object of the second functor, so, for example, our current source category has two objects and we specify two morphisms.&lt;/p&gt;&lt;p&gt;Note that this mapping does not map every object from the target category, i.e. not all objects have arrows coming from them (e.g. here the black and blue square do not have arrows), although, in some cases, it might.&lt;/p&gt;&lt;p&gt;Task 2: When exactly would the mapping encompass all objects?&lt;/p&gt;&lt;p&gt;The morphism part might seem hard‚Ä¶ until we realize that, once the connections between the object mappings are already established, there is only one way to connect the morphisms ‚Äî we take each morphism of the source category and connect the two morphisms given by the two functors, in the target category. And that‚Äôs all there is to it.&lt;/p&gt;&lt;p&gt;Oh, actually, there is also this condition that the above diagram should commute (the naturality condition), but that happens pretty much automatically.&lt;/p&gt;&lt;p&gt;Just like anything else in category theory, natural transformations have some laws that they are required to pass. In this case it‚Äôs one law, typically called the naturality law, or the naturality condition.&lt;/p&gt;&lt;p&gt;Before we state this law, let‚Äôs recap where are we now: We have two functors $F$ and $G$ that have the same type signature (so $F : C \to D$ and $G : C \to D$ for some categories $C$ and $D$), and a family of morphisms in the target category $D$ (denoted $\alpha : F \Rightarrow G$) one for each object in $C$, that map each object of the target of the functor $F$ (or the image of $F$ in $D$ as it is also called) to some objects of the image of $G$. This is a transformation, but not necessarily a natural one. A transformation is natural, when this diagram commutes for all morphisms in $C$.&lt;/p&gt;&lt;p&gt;i.e. a transformation is natural when every morphism $f$ in $C$ is mapped to morphisms $F(f)$ by $F$ and to $G(f)$ by $G$ (not very imaginative names, I know), in such a way, that we have $\alpha \circ F(f) = G(f) \circ \alpha$ i.e. when starting from the white square, when going right and then down (via the yellow square) is be equivalent to going down and then right (via the black one).&lt;/p&gt;&lt;p&gt;We may view a natural transformation is a mapping between morphisms and commutative squares: two functors and a natural transformation between two categories means that for each morphism in the source category of the functors, there exist one commutative square at the target category.&lt;/p&gt;&lt;p&gt;When we fully understand this, we realize that commutative squares are made of morphisms too, so, like morphisms, they compose ‚Äî for any two morphisms with appropriate type signatures that have we can compose to get a third one, we have two naturality squares which compose the same way.&lt;/p&gt;&lt;p&gt;Which means natural transformation make up a‚Ä¶&lt;/p&gt;&lt;p&gt;(Oh wait, it‚Äôs too early for that, is it?)&lt;/p&gt;&lt;p&gt;After understanding natural transformations, natural isomorphisms, are a no-brainer: a natural transformation is just a family of morphisms in a given category that satisfy certain criteria, then what would a natural isomorphism be? That‚Äôs right ‚Äî it is a family of isomorphisms that satisfy the same criteria. The diagram is the same as the one for ordinary natural transformation, except that $\alpha$ are not just ordinary morphisms, but isomorphisms.&lt;/p&gt;&lt;p&gt;And the turning those morphisms into isomorphisms makes the diagram commute in more than one way i.e. if we have the naturality condition&lt;/p&gt;&lt;p&gt;$\alpha \circ F(f) = G(f) \circ \alpha$ i.e. the two paths going from white to blue are equivalent.&lt;/p&gt;&lt;p&gt;We also have:&lt;/p&gt;&lt;p&gt;$F(f) \circ \alpha = \alpha \circ G(f)$ i.e. the two paths going from black to yellow are also equivalent.&lt;/p&gt;&lt;p&gt;I am sorry, what were we talking about again? Oh yeah ‚Äî categorical equivalence. Remember that categorical equivalence is the reason why we tackle natural transformations and isomorphisms? Or perhaps it was the other way around? Never mind, let‚Äôs recap what we discussed so far:&lt;/p&gt;&lt;p&gt;At the beginning of the section we introduced the notion of equivalence as two functors, such that going from one of them to the other and back again leads you to the same object, or to an object that is isomorphic to the one you started with.&lt;/p&gt;&lt;p&gt;And then, we discussed that for categories that are not thin (thick?) the situation is a bit more complex since they can have more than one morphism between two objects, and we should worry not only about isomorphic objects, but about isomorphic morphisms.&lt;/p&gt;&lt;p&gt;Now, we will show how these two notions are formalized by the definition that we presented.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Two categories $A$ and $B$ are equivalent (or $A \simeq B$) if there exist functors $f: A \to B$ and its reverse $g: B \to A$, such that $f \circ g \cong ID_{A}$ and $g \circ f \cong ID_{A}$.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;To understand, this how are the two related, let‚Äôs construct the identity functor of the category that we have been using as an example all this time. Note that we are drawing the one and the same category two times (as opposed to just drawing an arrow coming from each object to itself), to make the diagrams more readable.&lt;/p&gt;&lt;p&gt;Then, we draw the composite of the two functors that establish an equivalence between the two categories, highlighting the 3 ‚Äúinteresting‚Äù objects, i.e. the ones due to which the categories aren‚Äôt isomorphic.&lt;/p&gt;&lt;p&gt;Now, we ask ourselves, in which cases does there exist an isomorphism between those two functors?&lt;/p&gt;&lt;p&gt;The answer becomes trivial if we draw the isomorphism arrows connecting the three ‚Äúinteresting‚Äù objects in a different way (remember, this is the same category on the top and the bottom) ‚Äî we can see that these are exactly the arrows that enable us to construct an isomorphism between the two functors (the others are just identity arrows).&lt;/p&gt;&lt;p&gt;And when would this isomorphism be such that preserves the structure of the category (so that each morphism from the output of the composite functor has an equivalent one in the output of the identity)? Exactly when the isomorphism is natural i.e. when every morphism is mapped to a commuting square, e.g. here is the commuting square of the morphism that is marked in red.&lt;/p&gt;&lt;p&gt;i.e. naturality condition assures us that the morphisms in the target of the functor behave in the same way as their counterparts in the source.&lt;/p&gt;&lt;p&gt;With this, we are finished with categorical equivalence, but not with natural transformations ‚Äî natural transformations are a very general concept, and categorical equivalences are only a very narrow case of them.&lt;/p&gt;&lt;p&gt;In the course of this book, we learned that programming/computer science is the study of the category of types in programming languages. However (in order to avoid this being too obvious) in the computer science context, we use different terms for the standard category-theoretic concepts.&lt;/p&gt;&lt;p&gt;We learned that objects are known as types, products and coproducts are, respectively, objects/tuple types and sum types. And, in the last chapter, we learned that functors are known as generic types. Now it‚Äôs the time to learn what natural transformations are in this context. They are known as (parametrically) polymorphic functions.&lt;/p&gt;&lt;p&gt;Now, suppose this sounds a bit vague. If only we had some example of a natural transformation in programming, that we can use‚Ä¶ But wait, we did show a natural transformation in the previous chapter, when we talked about pointed functors.&lt;/p&gt;&lt;p&gt;That‚Äôs right, a functor is pointed when there is a natural transformation between it and the identity functor i.e. to have one green arrow for every object/type.&lt;/p&gt;&lt;p&gt;And this clearly is a natural transformation. As a matter of fact, if we get down to the nitty-gritty, we would see that it resembles a lot the equivalence diagram that we saw earlier ‚Äî both transformations involve the identity functor, and both transformations have the same category as source and target, that‚Äôs why we can put everything in one circle (we don‚Äôt do that in the equivalence diagram, but that‚Äôs just a matter of presentation).&lt;/p&gt;&lt;p&gt;Actually, the only difference between the two transformations is that an equivalence is defined by a natural natural isomorphism of a given functors to the identity functor ( $ID \cong f \circ g $ and $ID \cong g \circ f$), while a pointed functor is defined by a one-way natural transformation from the identity functor ($ID \to f $) i.e. the equivalence functor is pointed, but not the other way around).&lt;/p&gt;&lt;p&gt;We said that a natural transformation is equivalent to a (parametrically) polymorphic function in programming. But wait, wasn‚Äôt natural transformation something else (and much more complicated):&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Two functors $F$ and $G$ that have the same type signature (so $F : C \to D$ and $G : C \to D$ for some categories $C$ and $D$), and a family of morphisms in the target category $D$ (denoted $\alpha : F \Rightarrow G$) one for each object in $C$. Morphisms that map each object of the target of $F$ (or the image of $F$ in $D$ as it is also called) to some object in the target of $G$.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Indeed it is (I wasn‚Äôt lying to you, in case you are wondering), however, in the case of programming, the source and target categories of both functors are the same category ($Set$), so the whole condition regarding the functors‚Äô type signatures can be dropped.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Two&lt;/p&gt;&lt;del&gt;functors&lt;/del&gt;generic types $F$ and $G$&lt;del&gt;that have the same type signature&lt;/del&gt;and a family of morphisms in $Set$ (denoted $\alpha : Set \Rightarrow Set$) one for each object in $Set$, that map each target object of the functor $F$ (or the image of $F$ in $D$ as it is also called) to some target objects of functor $G$.&lt;/quote&gt;&lt;p&gt;As we know from the last chapter, a functor in programming is a generic type (which, has to have the &lt;code&gt;map&lt;/code&gt; function with the appropriate signature).&lt;/p&gt;&lt;p&gt;And what is a ‚Äúfamily of morphisms in $Set$ one for each object in $Set$‚Äù? Well, the morphisms in the category $Set$ are functions, so that‚Äôs just a bunch of functions, one for each type. In Haskell notation, if we denote a random type by the letter \(a\)), it is $alpha : \forall a. F a \to G a$. But that‚Äôs exactly what polymorphic functions are.&lt;/p&gt;&lt;p&gt;Here is how would we write the above definition in a more traditional language (we use capital &lt;code&gt;&amp;lt;A&amp;gt;&lt;/code&gt; instead of $a$, as customary.&lt;/p&gt;&lt;code&gt;
function alpha&amp;lt;A&amp;gt;(a: F&amp;lt;A&amp;gt;) : G&amp;lt;A&amp;gt; {
}

&lt;/code&gt;&lt;p&gt;Generic types work by replacing the &lt;code&gt;&amp;lt;A&amp;gt;&lt;/code&gt; with some concrete type, like &lt;code&gt;string&lt;/code&gt;, &lt;code&gt;int&lt;/code&gt; etc. Specifically, the natural transformation from the identity functor to the list functor that puts each value in a singleton list looks like this $alpha :: \forall\ a. a \to List\ a$. Or in TypeScript:&lt;/p&gt;&lt;code&gt;
function array&amp;lt;A&amp;gt;(a: A) : Array&amp;lt;A&amp;gt; {
    return [a]
}
&lt;/code&gt;&lt;p&gt;Once we rid ourselves of the feeling of confusion, that such an excessive amount of new terminology and concepts impose upon us (which can take years, by the way), we realize that there are, of course, many polymorphic functions/natural transformations that programmers use.&lt;/p&gt;&lt;p&gt;For example, in the previous chapter, we discussed one natural transformation/polymorphic function the function $\forall a.a \to [a]$ which puts every value in a singleton list. This function is a natural transformation between the identity functor and the list functor.&lt;/p&gt;&lt;p&gt;This is pretty much the only one that is useful with this signature (the others being $a \to [a, a]$, $a \to [a, a, a]$ etc.), but there are many examples with signature $list\ a \to list\ a$, such as the function to reverse a list.&lt;/p&gt;&lt;p&gt;‚Ä¶or take1 that retrieves the first element of a list&lt;/p&gt;&lt;p&gt;or flatten a list of lists of things to a regular list of things (the signature of this one is a little different, it‚Äôs $list\ list\ a \to list\ a$).&lt;/p&gt;&lt;p&gt;Task 3: Draw example naturality squares of the $reverse$ natural transformation.&lt;/p&gt;&lt;p&gt;Do the same for the rest of the transformations.&lt;/p&gt;&lt;p&gt;Before, we said that we shouldn‚Äôt worry too much about naturality, as it is satisfied every time. Statistically, however, this is not true ‚Äî as far as I am concerned, about 99.999 percent of transformations aren‚Äôt really natural (I wonder if you can compute that percentage properly?). But at the same time, it just so happens (my favourite phrase when writing about maths) that all transformations that we care about are natural.&lt;/p&gt;&lt;p&gt;So, what does the naturality condition entail, in programming? To understand this, we construct some naturality squares of the transformations that we presented.&lt;/p&gt;&lt;p&gt;We choose two types that play the role of $a$, in our case $string$ and $num$ and one natural transformation, like the transformation between the identity functor and the list functor.&lt;/p&gt;&lt;p&gt;The diagram commute when for all functions $f$, applying the $Ff$, the mapped/lifted version of $f$ with one functor (in our case this is just $F f : string \to num$ cause it is the identity functor), followed by ($alpha :: F b \to G\ b$), is equivalent to applying ($alpha:: F a \to G\ a$), and then the mapped version of $f$ with the other functor (in our case $G f :: List\ a \to List\ b$) i.e.&lt;/p&gt;\[\alpha \circ F\ f \cong G\ f \circ \alpha\]&lt;p&gt;(in the programming world, you would also see it as something like $\alpha (map\ f x) = map\ f (\alpha x)$, but note that here $map$ function means two different things on the two sides, Haskell is just smart enough to deduce which $fmap$ to use).&lt;/p&gt;&lt;p&gt;And in TypeScript, when we are talking specifically about the identity functor and the list functor, the equality is expressed as:&lt;/p&gt;&lt;code&gt;[x].map(f) == [f(x)]
&lt;/code&gt;&lt;p&gt;So, is this equation true in our case? To verify it, we take one last peak at the world of values.&lt;/p&gt;&lt;p&gt;We acquire an $f$, that is, we a function that acts on simple values (not lists), such as the function $length : string \to num$, which returns the number of characters a string has and convert it, (or lift it, as the terminology goes) to a function that acts on more complex values, using the list functor, (and the higher-order function $map$).&lt;/p&gt;&lt;p&gt;Then, we take the input and output types for this function (in this case $string$ and $num$), and the two morphisms of a natural transformation (e.g the abstract function $\forall a.a \to [a]$) that correspond to those two types.&lt;/p&gt;&lt;p&gt;When we compose these two pairs of morphisms we observe that they indeed commute ‚Äî we get two morphisms that are actually one and the same function.&lt;/p&gt;&lt;p&gt;The above square shows the transformation $\forall a.a \to [a]$ (which is between the identity functor and the list functor, here is another one, this time between the list functor and itself ($\forall a.[a] \to [a]$) ‚Äî $reverse$&lt;/p&gt;&lt;p&gt;(and you can see that this would work not just for $length$, but for any other function).&lt;/p&gt;&lt;p&gt;So, why does this happen? Why do these particular transformations make up a commuting square for each and every morphism?&lt;/p&gt;&lt;p&gt;The answer is simple, at least in our specific case: the original, unlifted function $f :: a \to b$ (like our $length :: string \to num$) can only work on the individual values (not with structure), while the natural transformation functions, i.e. ones with signature $list :: a \to list\ a$ only alter the structure, and not individual values. The naturality condition just says that these two types of functions can be applied in any order that we please, without changing the end result.&lt;/p&gt;&lt;p&gt;This means that if you have a sequence of natural transformations that you want to apply, (such as $reverse$ , $take$, $flatten$ etc) and some lifted functions ($F f$, $F g$), you can mix and match between the two sequences in any way you like and you will get the same result e.g.&lt;/p&gt;\[take1 \circ reverse \circ F\ f \circ F\ g\]&lt;p&gt;is the same as&lt;/p&gt;\[take1 \circ F\ f \circ reverse \circ F\ g\]&lt;p&gt;‚Ä¶or‚Ä¶&lt;/p&gt;\[F\ f \circ F\ g \circ take1 \circ reverse\]&lt;p&gt;‚Ä¶or any other such sequence (the only thing that isn‚Äôt permitted is to flip the members of the two sequences ‚Äî ($take1 \circ reverse$ is of course different from $reverse \circ take1$and if you have $F\ f \circ F\ g$, then $F\ g \circ F\ f$ won‚Äôt be permitted at all due to the different type signatures).&lt;/p&gt;&lt;p&gt;Task 4: Prove the above results, using the formula of the naturality condition.&lt;/p&gt;&lt;p&gt;‚ÄúUnnatural‚Äù, or ‚Äúnon-natural‚Äù transformations (let‚Äôs call them just transformations) are mentioned so rarely, that we might be inclined to ask if they exist. The answer is ‚Äúyes and no‚Äù. Why yes? On one hand, transformations, consist of an innumerable amount of morphisms, forming an ever more innumerable amount of squares and obviously nothing stops some of these squares to be non-commuting.&lt;/p&gt;&lt;p&gt;For example, if we substitute one morphism from the family of morphisms that make up the natural transformation with some other random morphism that has the same signature, all squares that have this morphism as a component would stop commuting.&lt;/p&gt;&lt;p&gt;This would result in something like an ‚Äúalmost-natural‚Äù transformation (e.g. an abstract function that reverses all lists, except lists of integers).&lt;/p&gt;&lt;p&gt;And in the category of sets, where morphisms are functions i.e. mappings between values, it is enough to move just one arrow of just one of those values in order to make the transformation ‚Äúunnatural‚Äù (e.g. a function which reverses all lists, but one specific list).&lt;/p&gt;&lt;p&gt;Finally, if can just gather a bunch of random morphisms, one for each object, that fit the criteria, we get what I would call a ‚Äúperfectly unnatural transformation‚Äù (but this is my terminology).&lt;/p&gt;&lt;p&gt;But, although they do exist, it is very hard to define non-natural transformations. For example, for categories that are infinite, there is no way to specify such ‚Äúperfectly unnatural transformation‚Äù (ones where none of the squares commute) without resorting to randomness. And even transformations on finite categories, or the ‚Äúsemi-natural‚Äù transformations which we described above (the ones that include a single condition for a single value or type), are not possible to specify in some languages e.g. you can define such a transformation in Typescript, but not in Haskell.&lt;/p&gt;&lt;p&gt;To see why, let‚Äôs see what the type of a natural transformation is.&lt;/p&gt;\[\forall\ a.\ F a \to G a\]&lt;p&gt;The key is that the definition should be valid for all types a. For this reason, there is no way for us to specify a different arrows for different types, without resorting to type downcasting, which is not permitted in languages like Haskell (as it breaks the principle of parametricity).&lt;/p&gt;&lt;p&gt;Now, after we saw the definition of natural transformations, it is time to see the definition of natural transformations (and if you feel that the quality of the humour in this book is deteriorating, that‚Äôs only because things are getting serious).&lt;/p&gt;&lt;p&gt;Let‚Äôs review again the commuting diagram that represents a natural transformation.&lt;/p&gt;&lt;p&gt;This diagram might prompt us into viewing natural transformations as some kind of ‚Äútwo-arrow functors‚Äù that have not one but two arrows coming from each of their morphisms ‚Äî this notion, can be formalized, by using product categories.&lt;/p&gt;&lt;p&gt;Oh wait, I just realized we never covered product categories‚Ä¶ but don‚Äôt worry, we will cover them now.&lt;/p&gt;&lt;p&gt;We haven‚Äôt covered product categories, however some pages ago, when we covered monoids and groups, we talked about the concept of a product group. The good news is that product categories are a generalization of product groups‚Ä¶&lt;/p&gt;&lt;p&gt;The bad news is that you probably don‚Äôt remember much about product groups, as covered them briefly.&lt;/p&gt;&lt;p&gt;But don‚Äôt worry, we will do a more in-depth treatment now:&lt;/p&gt;&lt;p&gt;Given two groups $G$ and $H$, whose sets of elements can also be denoted $G$ and $H$‚Ä¶&lt;/p&gt;&lt;p&gt;(in this example we use two boolean groups, which we visualize as the groups of horizontal and vertical rotation of a square)&lt;/p&gt;&lt;p&gt;‚Ä¶the product group of these two groups is a group that has the cartesian product of these two sets $G \times H$ as its set of elements.&lt;/p&gt;&lt;p&gt;And what can the group operation of such a group be? Well, I would say that out of the few possible groups operations for this set that exist, this is the only operation that is natural (I didn‚Äôt intend to involve natural transformation at this section, but they really do appear everywhere). So, let‚Äôs try to derive the operation of this group.&lt;/p&gt;&lt;p&gt;We know what a group operation is, in principle: A group operation combines two elements from the group into a third element i.e. it is a function with the following type signature:&lt;/p&gt;\[\circ : (A, A) \to A\]&lt;p&gt;or equivalently&lt;/p&gt;\[\circ : A \to A \to A\]&lt;p&gt;And for product groups, we said that the underlying set of the group (which we dubbed $A$ above) is a cartesian product of some other two sets which we dubbed $G$ and $H$. So, when we swap $A$ for $G \times H$ the definition becomes:&lt;/p&gt;\[\circ : G \times H \to G \times H \to G \times H\]&lt;p&gt;i.e. the group operation takes one pair of elements from $G$ and $H$ and another pair of elements from $G$ and $H$, only to return ‚Äî guess what ‚Äî a pair of elements $G$ and $H$.&lt;/p&gt;&lt;p&gt;Let‚Äôs take an example. To avoid confusion, we take two totally different groups ‚Äî the color-mixing group and the group of integers under addition. That would mean that a value of $G \times H$ would be a pair, containing a random color and a random number, and the operation would combine two combine two such pairs and produce another one.&lt;/p&gt;&lt;p&gt;Now, the operation must produce a pair, containing a number and a color. Furthermore, it would be good if it produces a number by using those two numbers, not just picking one at random, and likewise for colors. And furthermore, we want it to work not just for monoids of numbers and colors, but all other monoids that can be given to us. It is obvious that there is only one solution, to get the elements of the new pair by combining the elements of the pairs given.&lt;/p&gt;&lt;p&gt;And the operation of the product group of the two boolean groups which we presented earlier is the combination of the two operations&lt;/p&gt;&lt;p&gt;So, the general definition of the operation is the following ($g1$, $g2$ are elements of $G$ and $h1$ and $h2$ elements of $H$).&lt;/p&gt;\[(g1, h1) \circ (g2, h2) = ( (g1 \circ g2), (h1 \circ h2))\]&lt;p&gt;And that are product groups.&lt;/p&gt;&lt;p&gt;We are back at tackling product categories.&lt;/p&gt;&lt;p&gt;Since we know what product groups are, and we know that groups are nothing but categories with just one object (and the group objects are the category‚Äôs morphisms, remember?), we are already almost there.&lt;/p&gt;&lt;p&gt;Here is a way to make a product category.&lt;/p&gt;&lt;p&gt;Take any two categories:&lt;/p&gt;&lt;p&gt;Then take the set of all possible pairs of the objects of these categories.&lt;/p&gt;&lt;p&gt;And, finally, we make a category out of that set by taking all morphisms coming from any of the two categories and replicate them to all pairs that feature some objects from their type signature, in the same way as we did for product groups (in this example, only one of the categories has morphisms).&lt;/p&gt;&lt;p&gt;This is the product category of the two categories.&lt;/p&gt;&lt;p&gt;In this section we are interested with the products of one particular category, namely the category we called $2$, containing two objects and one morphism (stylishly represented in black and white).&lt;/p&gt;&lt;p&gt;This category is the key to constructing a functor that is equivalent to a natural transformation:&lt;/p&gt;&lt;p&gt;So, given a product category of $2$ and some other category $C$‚Ä¶&lt;/p&gt;&lt;p&gt;‚Ä¶there exist a natural transformation between $C$ and the product category $2\times C$.&lt;/p&gt;&lt;p&gt;Furthermore, this connection is two-way: any natural transformation from $C$ to some other category (call it $D$, as it is customary) can be represented as a functor $2 \times C \to D$.&lt;/p&gt;&lt;p&gt;That is, if we have a natural transformations $\alpha : F \Rightarrow G$ (where $F: C \to D$ and $G: C \to D$), then, we also have a functor $2 \times C \to D$, such that if we take the subcategory of $2 \times C$ comprised of just those objects that have the $0$ object as part of the pair, and the morphisms between them, we get a functor that is equivalent to $F$, and if we consider the subcategory that contains $1$, then the functor is equivalent to $G$ (we write $\alpha(-,0)=F$ and $\alpha(-,1)=G$). Et voil√†!&lt;/p&gt;&lt;p&gt;Task 5: Show that the two definitions are equivalent.&lt;/p&gt;&lt;p&gt;This perspective helps us realize that a natural transformation can be viewed as a collection of commuting squares. The source functor defines the left-hand side of each square, the target functor ‚Äî the right-hand side, and the transformation morphisms join these two sides.&lt;/p&gt;&lt;p&gt;We can even retrieve the structure of the source category of these functors, which (as categories are by definition structure and nothing more) is equivalent to retrieving the category itself.&lt;/p&gt;&lt;p&gt;Natural transformations are surely a different beast than normal morphisms and functors and so they don‚Äôt compose in the same way. However, they do compose and here we will show how.&lt;/p&gt;&lt;p&gt;Let‚Äôs first get one trivial definition out of the way: for each functor, we have the identity natural transformation (actually a natural isomorphism) between it and itself.&lt;/p&gt;&lt;p&gt;The setup for composing natural transformations may look complicated the first time you see it: we need three categories $C$, $D$ and $E$ (just as composition of morphisms requires three objects). We need a total of four functors, distributed on two pairs, one pair of functors that goes from $C$ to $D$ and one that goes from $D$ to $E$ (so we can compose these two pairs of functors together, to get a new pair of functors that go $C \to E$). However, we will try to keep it simple and we will treat the natural transformation as a map from a morphism to a commuting square. As we showed above, this mapping already contains the two functors in itself.&lt;/p&gt;&lt;p&gt;So, let‚Äôs say that we have the natural transformation $\alpha$ involving the $C \to D$ functors (which we usually call $F$ and $G$).&lt;/p&gt;&lt;p&gt;So, what will happen if we have one more transformation $\bar\alpha$ involving the functors that go $D \to E$ (which are labelled $F‚Äô$ and $G‚Äô$)? Well, since a natural transformation maps each morphism to a square, and a square contains four morphisms (two projections by the two functors and two components of the transformation), a square would be mapped to four squares.&lt;/p&gt;&lt;p&gt;Let‚Äôs start by drawing two of them for each projection of the morphism in $C$.&lt;/p&gt;&lt;p&gt;We have to have two more squares, corresponding to the two morphisms that are the components of the $\alpha$ natural transformation. However, these morphisms connect the objects that are the target of the two functors, objects that we already have on our diagram, so we just have to draw the connections between them.&lt;/p&gt;&lt;p&gt;The result is an interesting structure which is sometimes visualized as a cube.&lt;/p&gt;&lt;p&gt;More interestingly, when we compose the commuting squares from the sides of the cube horizontally, we see that it contains not one, but two bigger commuting squares (they look like rectangles in this diagram), visualized in grey and red. Both of them connect morphisms $F‚ÄôFf$ and $G‚ÄôGf$.&lt;/p&gt;&lt;p&gt;So, there is a natural transformation between the composite functor $F‚Äô \circ F : C \to E$ and $G‚Äô \circ G : C \to E$ ‚Äî a natural transformation that is usually marked $\bar\alpha \bullet \alpha$ (with a black dot).&lt;/p&gt;&lt;p&gt;Task 6: Show that natural transformations indeed compose i.e. that if you have natural transformations $F‚ÄôFf \Rightarrow F‚ÄôGf$ and $F‚ÄôGf \Rightarrow G‚ÄôGf$ you have $F‚ÄôFf \Rightarrow G‚ÄôGf$.&lt;/p&gt;&lt;p&gt;And an interesting special case of horizontal composition is horizontal composition involving the identity natural transformation: given a natural transformation $\bar\alpha$ involving functors with signature $D \to E$ and some functor with signature $F : C \to D$, we can take $\alpha$ to be the identity natural transformation between functor $F$ and itself and compose it with $\bar\alpha$.&lt;/p&gt;&lt;p&gt;We get a new natural transformation $\bar\alpha \bullet \alpha$, that is practically the same as the one we started with (i.e. the same as $\bar\alpha$) so what‚Äôs the deal? We just found a way to extend natural transformations, using functors: i.e we can use a functor with signature $C \to D$ to extend a $D \to E$ natural transformation and make it $C \to E$.&lt;/p&gt;&lt;p&gt;Task 7: Try to extend the natural transformation in the other direction (by taking $\bar\alpha$ to be identity).&lt;/p&gt;&lt;p&gt;So, this is how you compose natural transformations. It‚Äôs too bad that this is form of composition is different from the standard categorical composition. So, I guess natural transformations do not form a category, like we hoped they would‚Ä¶&lt;/p&gt;&lt;p&gt;Well, OK, there is actually another way of composing categories, which might actually work.&lt;/p&gt;&lt;p&gt;Recall that categorical composition involves three objects and two successive arrows between them. For vertical composition of natural transformations, we will need three (or more) functors with the same type signature, say $F, G, H: C \to D$ i.e. (same source and target category) and two successive natural transformations between those functors i.e. $\alpha: F \to G$ and $\beta: G \to H$.&lt;/p&gt;&lt;p&gt;We can combine each morphism of the natural transformation $\alpha$ (e.g. $a: F \to G$) and the corresponding morphism of the natural transformation $\beta$ (say $b:G \to H$) to get a new morphism, which we call $b \circ a : F \to H$ (the composition operator is the usual white circle, as opposed to the black one, which denotes horizontal composition). And the set of all such morphisms are precisely the components of a new natural transformation: $\beta \circ \alpha : F \to H$.&lt;/p&gt;&lt;p&gt;Now, we are approaching the end of the chapter, we will introduce our category and call it quits. To do that, we first introduce a more compressed notation for vertical composition of natural transformations (where they do indeed look vertical).&lt;/p&gt;&lt;p&gt;We started this chapter by looking at category of sets and using internal diagrams, displaying the set elements as points and the sets/objects as collections.&lt;/p&gt;&lt;p&gt;Task 8: identify the function, the three functors, and the two natural transformations used in this diagram.&lt;/p&gt;&lt;p&gt;Then, we quickly passed to normal external diagrams, where objects are points and categories are collections.&lt;/p&gt;&lt;p&gt;And now we go one more level further, and show the category of categories, where categories are points and functors are morphisms.&lt;/p&gt;&lt;p&gt;In this notation, we display natural transformations as (double) arrows between morphisms.&lt;/p&gt;&lt;p&gt;And you can already see the new category that is formed: For each two categories (like $C$ and $D$ in this case), there exists a category which has functors for objects and natural transformations as morphisms.&lt;/p&gt;&lt;p&gt;Natural transformations compose with vertical compositions, and, of course, the identity natural transformation is the identity morphism.&lt;/p&gt;&lt;p&gt;Vertical and horizontal composition of natural transformations are related to each other in the following way:&lt;/p&gt;&lt;p&gt;If we have (as we had) two successive natural transformations, in the vertical sense, like $\alpha: F \to G$ and $\beta: G \to H$.&lt;/p&gt;&lt;p&gt;And two successive ones, this time in horizontal sense e.g. $\bar\alpha: F‚Äô \to G‚Äô$ and $\bar\beta: G‚Äô \to H‚Äô$. (note that $\alpha$ has nothing to do with $\bar\alpha$ as $\beta$ has nothing to do with $\bar\beta$, we just call them that way to avoid using too many letters)&lt;/p&gt;&lt;p&gt;And if the two pairs of natural transformations both start from the same category and the same functor, then the compositions of the two pairs of natural transformations obey the following law&lt;/p&gt;\[(Œ≤ \circ Œ±) \bullet (\bar Œ≤ \circ \bar Œ±) = (Œ≤ \bullet \bar Œ≤) \circ (Œ± \bullet \bar Œ±)\]&lt;p&gt;Task 9: Draw the paths of the two compositions of the transformations (on the two sides of the equation) and ensure that they indeed lead to the same place.&lt;/p&gt;&lt;p&gt;At this point you might be wondering the following (although statistically you are more likely to wonder what the heck is all this about): We know that all categories are objects of $Cat$, the category of small categories, in which functors play the role of morphisms.&lt;/p&gt;&lt;p&gt;But, functors between given categories also form a category, under vertical composition. Which means that $Cat$ not only has (as any other category) morphisms between objects, but also has morphisms between morphisms. And furthermore, those two types of morphisms compose in this very interesting way.&lt;/p&gt;&lt;p&gt;So, what does that make of $Cat$? I don‚Äôt know, perhaps we can call natural transformations ‚Äú2-morphisms‚Äù and $Cat$ is some kind of ‚Äú2-category‚Äù?&lt;/p&gt;&lt;p&gt;But wait, actually it‚Äôs way too early for you to find out. We haven‚Äôt even covered limits‚Ä¶&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45435422</guid><pubDate>Wed, 01 Oct 2025 08:00:30 +0000</pubDate></item><item><title>I only use Google Sheets</title><link>https://mayberay.bearblog.dev/why-i-only-use-google-sheets/</link><description>&lt;doc fingerprint="fa48d2484db7b5f4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why I only use Google sheets&lt;/head&gt;
    &lt;p&gt;To cut things short, always use the easiest solution to solve a particular problem and once that solution does not work for the business anymore reassess what the new requirements are and either try enhance the current solution or find an alternative that better solve the problem. In my case the easiest solution is often creating a new Google sheet.&lt;/p&gt;
    &lt;p&gt;I entered the workforce about 9 months ago and my optimism for building new tools and services that help the small starting up business I work for has all vanished. I work in an environment that changes every 2 months or so, as my boss finds a new business venture she wants to enter. This has me starting and stopping quite a few projects that could have been solved in an afternoon with a quick Google Sheet.&lt;/p&gt;
    &lt;p&gt;I have listed a few examples below of some of the projects I have wasted time on instead of making a Google Sheet:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;I spent 2 months designing and making an admin panel to manage and track incoming cargo for the business. This panel was supposed to help the business categorise and better manage packages and customer data. This admin panel was used twice and never again. A Google Sheet could have been easily used for this and is currently being used for this task.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Three weeks were spent creating an MVP for a quote system that automatically calculated the duty and taxes for people ordering certain goods. Zimbabwean taxes and duties are often very complex and having our customers know exactly what to pay would create a better customer journey and make the process faster since we would not have to wait on our third party duty processing company to reply to us on every customer inquiry. In the end, we saw one of our competitors tax and duty breakdown table and we just copied it and put it in a Google Sheet.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Spent 2 months researching, having meetings (often &amp;gt; 1hr long) and looking for a good CRM to use for the business. I would sit down compare and contrast different feature`s and prices for all the different CRMs we were looking into. We ended up using the free version of Oddo, that is not used that much anyway within the business. To my surprise a few weeks ago i noticed that Google Sheets has a CRM template built into it.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I'm not saying that making a Google Sheet is the best solution to every problem but often times in my situation it is. I usually end up in situations were I never know the full scope of the problem until we start doing the actual work.&lt;/p&gt;
    &lt;p&gt;This is not to say that we do not need to plan out a project. The team should discuss workflows and information they might need but until we start doing the actual work we do not know full scope of the problem.&lt;/p&gt;
    &lt;p&gt;Once the full scope of the problem is known then we can start creating or enhancing the solutions we have. This helps because you do not end up being stuck with an extra workload that in the best case does not require all the features you are adding and in the worst case spending time on a project that will fail. So it is in your best interest to use the most basic solution to solve a problem.&lt;/p&gt;
    &lt;p&gt;Doing the smallest and easiest solution to a problem as a way to get to know the full scope and then iterating after that if needs be is by far the best solution (for me).&lt;/p&gt;
    &lt;p&gt;There are some caveats to this approach, I know a few organisation that have a thousand row spreadsheets that keep track of all their business transactions and employee information.&lt;/p&gt;
    &lt;p&gt;Creating a Google Sheet only works in situations were we do not know the full scope of the problem. Personally, I'm still new to this and learning when the best solution is making a Google Sheet or not. I just want to save people's time and effort and not have them build something that will never be used. But like all advice, think carefully about your own situation before committing a lot of time and effort especially in a business setting. It is perfectly fine for you to build useless programs and software in your spare time, that's the whole fun of it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45435463</guid><pubDate>Wed, 01 Oct 2025 08:06:50 +0000</pubDate></item><item><title>Our efforts, in part, define us</title><link>https://weakty.com/posts/efforts/</link><description>&lt;doc fingerprint="3282d051aea7cfd9"&gt;
  &lt;main&gt;
    &lt;p&gt;What happens when something we enjoy doing that took effort becomes effortless? And what happens if that original effort was a foundation on which we saw value in ourselves?&lt;/p&gt;
    &lt;p&gt;If our efforts, in part, define us, then our efforts have intrinsic value. Our efforts may help us understand a position we want to occupy, an identity we carry, or an outlook we present. This value contributes to an internal economy of joy, self-respect, fulfillment, happiness. When effortful things become effortless, what becomes of our position in these economies?&lt;/p&gt;
    &lt;p&gt;As you can see, I have a few questions here.&lt;/p&gt;
    &lt;p&gt;I know someone who spent a part of their adult life taking beautiful photographs, developing them by hand, framing them, cataloging them. Along came the ubiquity of digital cameras and smartphones, and "film" became infinitely available. Offhandedly, one day, this person mentioned that with the proliferation of smart phone cameras, and the ease with which one can take photos, they had found that some days their desire to continue was diminishing, and their work had lost meaning.&lt;/p&gt;
    &lt;p&gt;Technology has a history of making effortful things effortless, and there is sometimes a hidden loss in that advancement.&lt;/p&gt;
    &lt;p&gt;I figure people are continually being left behind in a similar manner day-to-day. Technology continues advancing (for the most part), and more things that remain effortful will become effortless. And "we" (ie, the populations who can afford to sit around and have crises of identity on these topics) will be further pushed to re-evaluate certain parts of our definitions of self.&lt;/p&gt;
    &lt;p&gt;For myself, in the last 10 years, my work of writing code has largely defined what I do with my working time. Now I experience large swaths of that work being created and done by AI (sometimes amazingly well, sometimes poorly), and I find myself thinking of the photographer above. It's not my wish that people can't have access to a more effortless way to write code, but I feel a strange sadness that there is less left to the act of the craft.&lt;/p&gt;
    &lt;p&gt;I have had this note in a draft state for several weeks now because I still can‚Äôt quite come to terms with how I‚Äôm feeling about things. There are so many nuances and unclear thoughts rolling around in my head about this shift. I think the only thing that is vaguely clear is that none of this would matter if making money wasn‚Äôt at play. If I was just writing code, (or taking film photographs) for fun in my free time because I enjoyed it, well, I don‚Äôt think I‚Äôd be feeling so conflicted.&lt;/p&gt;
    &lt;p&gt;Being paid to work and presenting my capacities through my craft is an exchange that I have been able to derive value from in its effortful-ness. Often times I've worked on utterly boring tasks that I would have loved to have a tool that could automate. But I didn't. And even in those menial moments I did derive some pleasure in my capacities. Of course, when it came to the real challenges, that was where I felt a pleasure and value in putting forth effort.&lt;/p&gt;
    &lt;p&gt;As a consultant, I work in a lot of different places, often for brief stints of time. And at many of these places, I see a large push, top-down, to encourage people to use AI. These employees, previously having entered an employment agreement where their capacities and experience would be exchanged for money, are now being asked that their abilities be augmented. In this way, the level continues to skew toward privileging production, often without understanding and people using their own perspectives.&lt;/p&gt;
    &lt;p&gt;When I see sentiments similar to mine, I often see reactions where people say that AI is simply a tool and that you must learn to use it and incorporate it into your toolbox. That's fine. That's well and good. But all I'm trying to say here is that I feel a lack and a loss for something. I don't understand it yet.&lt;/p&gt;
    &lt;p&gt;The title of this post, our efforts, in part, define us, is just a phrase that popped into my head. I'm not really sure if I even believe it or if I've fully fleshed out this single statement. But some part of it rings true to me. I wonder what will happen to us and our efforts. Will we be driven into further niches that are effortful, that we can derive value from? Will we become vague blobs that are formless, ill-defined, and despondent?&lt;/p&gt;
    &lt;p&gt;All of this presupposes a few things ‚Äîthat one can (and/or should) aim to derive value from work, that a meaningful identity is constructed by doing effortful things, that people generally are happier when they can use their skills and experiences to make something. And what‚Äôs more, there is a fine-line here between glorifying people with experience deriving value, and sounding like a shitty gatekeeper.&lt;/p&gt;
    &lt;p&gt;I will continue working for various clients. I suspect I will continue hearing leadership push AI on employees. And I will continue observing how people respond to this. Of course, for many people a job is just a job, as they say, and they'll do whatever they can to get it done more quickly (or work several jobs at once). Those very same people might find more value from their efforts now that AI is making their jobs easier. They can turn to better supporting their family, following other interests outside of work, finding other meaningful things, etc.&lt;/p&gt;
    &lt;p&gt;But at this time, I don't really see how this won‚Äôt further trample people‚Äôs spirits in the realm of work, unless we also reshape our expectations of work itself.&lt;/p&gt;
    &lt;p&gt;Is it worth the effort?&lt;/p&gt;
    &lt;p&gt;‚ù¶&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45435825</guid><pubDate>Wed, 01 Oct 2025 09:22:10 +0000</pubDate></item><item><title>FlowSynx ‚Äì Orchestrate Declarative, Plugin-Driven DAG Workflows on .NET</title><link>https://flowsynx.io/</link><description>&lt;doc fingerprint="e89eea02c0142dc2"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;FlowSynx: Orchestrate Declarative, Plugin-Driven DAG Workflows on .NET&lt;/head&gt;
    &lt;p&gt;Seamless Workflow Automation‚ÄîDeclarative, Extensible, and Fully Controllable. Turn complex processes into maintainable, auditable, and transparent workflows that adapt to your business needs.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is the FlowSynx?&lt;/head&gt;
    &lt;p&gt;The mission of FlowSynx is clear: deliver a lightweight, extensible, and developer-friendly engine that adapts to diverse domains‚Äîfrom data engineering and DevOps to healthcare, finance, and enterprise integrations. It strikes the perfect balance between no-code simplicity and full-code flexibility, allowing teams to tailor workflows precisely to their needs through a modular, plugin-driven architecture.&lt;/p&gt;
    &lt;p&gt;At its core, FlowSynx leverages a micro-kernel design, cleanly separating orchestration logic from functional extensions. This decoupled architecture allows you to dynamically load, develop, or replace plugins without disrupting system stability‚Äîmaking FlowSynx highly customizable, maintainable, and easy to upgrade.&lt;/p&gt;
    &lt;head rend="h2"&gt;FlowSynx features and capabilities&lt;/head&gt;
    &lt;head rend="h3"&gt;Plugin-Based Extensibility&lt;/head&gt;
    &lt;p&gt;Each functional component in FlowSynx‚Äîfrom task definitions and runtime behaviors to integration endpoints and authentication providers‚Äîis treated as a plugin. Users can develop custom plugins using well-defined interfaces and register them with the system, enabling FlowSynx to adapt to specific business rules, protocols, or environments.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cross-Platform Execution&lt;/head&gt;
    &lt;p&gt;FlowSynx is designed to run seamlessly across major platforms, including Windows, Linux, and macOS. Additionally, it offers containerized deployment via Docker, making it ideal for integration into modern DevOps pipelines, Kubernetes environments, or hybrid cloud architectures.&lt;/p&gt;
    &lt;head rend="h3"&gt;Workflow Definition and Execution&lt;/head&gt;
    &lt;p&gt;Workflows in FlowSynx are defined as Directed Acyclic Graphs (DAGs) using JSON or DSL representations. These workflows support conditional logic, parallel execution, error handling, input/output mapping, and custom execution contexts‚Äîenabling advanced control flow with traceability and fault tolerance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Command-Line Interface (CLI)&lt;/head&gt;
    &lt;p&gt;A comprehensive CLI tool is included for managing workflows, invoking executions, debugging tasks, monitoring logs, and interacting with the system at a low level. This is ideal for scripting, batch jobs, and infrastructure automation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Software Development Kit (SDK)&lt;/head&gt;
    &lt;p&gt;FlowSynx provides a full-featured SDK for programmatic access. Developers can use the SDK to integrate workflow functionality into their applications, define dynamic workflows at runtime, fetch execution results, and implement plugin hosting strategies. The SDK is structured with clean architecture principles and is available in .NET, with planned bindings for other ecosystems via REST APIs or language bridges.&lt;/p&gt;
    &lt;head rend="h3"&gt;REST-API Accessibility&lt;/head&gt;
    &lt;p&gt;Exposes core functionality through a well-documented, versioned RESTful API that enables secure remote access and seamless integration across platforms and programming languages. The API supports standard HTTP methods (GET, POST, PUT, DELETE), offers comprehensive OpenAPI/Swagger documentation, and includes authentication, rate limiting, and error handling mechanisms to ensure robustness, scalability, and ease of use for developers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Console: Web-UI Management&lt;/head&gt;
    &lt;p&gt;The FlowSynx Console provides a modern, browser-based interface for workflow management. Users can visually design, configure, and monitor workflows, review execution logs, and manage plugins directly from the web UI. With real-time dashboards, drag-and-drop workflow editing, and secure multi-user access, the Console simplifies collaboration and operational oversight across distributed teams.&lt;/p&gt;
    &lt;head rend="h3"&gt;Authentication and Security&lt;/head&gt;
    &lt;p&gt;FlowSynx includes pluggable authentication support, enabling integration with modern identity providers such as OAuth2, OpenID Connect (e.g., Keycloak), as well as support for basic and token-based authentication. Security policies can be enforced per user, per plugin, and per workflow execution.&lt;/p&gt;
    &lt;head rend="h3"&gt;Logging, Monitoring, and Auditing&lt;/head&gt;
    &lt;p&gt;All workflow executions and plugin interactions are fully traceable. The system provides structured logging, execution history, and audit trail support for compliance and observability.&lt;/p&gt;
    &lt;head rend="h3"&gt;Standalone and Containerized Modes&lt;/head&gt;
    &lt;p&gt;FlowSynx can operate as a lightweight local service for single-user or single-machine scenarios, or it can scale horizontally through distributed orchestration models‚Äîenabling large-scale, multi-tenant execution across clusters or cloud-native infrastructures.&lt;/p&gt;
    &lt;head rend="h3"&gt;Trigger-Based Workflow Execution&lt;/head&gt;
    &lt;p&gt;Automatically launch workflows in response to specific events like file uploads, API calls, or scheduled intervals. Triggers eliminate manual intervention by monitoring conditions and instantly activating corresponding task flows, ensuring real-time, event-driven automation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Human-in-the-Loop (HITL) Approval&lt;/head&gt;
    &lt;p&gt;Integrate human decision points into automated workflows with Human-in-the-Loop Approval. This feature pauses execution until an authorized user manually approves or rejects a pending task. Ideal for scenarios requiring compliance checks, risk assessment, or business validation, it adds a layer of control and accountability within otherwise automated processes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Flexible Error Handling&lt;/head&gt;
    &lt;p&gt;Ensure workflow resilience with configurable error-handling strategies that define how failures are managed during execution. Choose from Retry, Skip, or Abort behaviors to match the criticality of each task. Fine-tune retry behavior with customizable policies, including maximum retries, initial delay, and backoff strategies such as Fixed, Linear, Exponential, or Jitter, enabling intelligent recovery from transient failures without manual intervention.&lt;/p&gt;
    &lt;head rend="h2"&gt;Architecture overview&lt;/head&gt;
    &lt;head rend="h3"&gt;Intraction tools&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CLI Interface: Command-line tools for interacting with the FlowSynx system, enabling workflow management and execution from terminals.&lt;/item&gt;
      &lt;item&gt;REST API Gateway: Provides secure, HTTP/HTTPS RESTful APIs to integrate with external systems, allowing remote workflow control and status querying.&lt;/item&gt;
      &lt;item&gt;SDK (Library): Developer-friendly libraries exposing FlowSynx functionalities programmatically, enabling custom applications to embed or automate workflow operations.&lt;/item&gt;
      &lt;item&gt;UI-Based Console Management: Intuitive browser-based interface for real-time monitoring and administration of workflows, and system settings with secure authentication.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;FlowSynx Core&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Workflow Orchestrator: Loads and executes workflows defined as JSON DAGs.&lt;/item&gt;
      &lt;item&gt;Plugin Manager: Dynamically loads plugins and maintains a plugin marketplace/registry.&lt;/item&gt;
      &lt;item&gt;Security &amp;amp; Auth: Handles authentication and authorization for both REST API and CLI access.&lt;/item&gt;
      &lt;item&gt;Logging &amp;amp; Auditing: Tracks workflow execution, plugin activity, and audit trails.&lt;/item&gt;
      &lt;item&gt;Trigger Engine: Listens for external events or schedules workflows to start based on timers, webhooks, or system signals.&lt;/item&gt;
      &lt;item&gt;Error handling: Built-in support for task retries, timeouts, and fallbacks ensures reliable execution even in unstable environments. Custom retry strategies can be defined per task.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Execution environments&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Deployment &amp;amp; Execution Environments: Supports flexible deployment models from standalone desktop/server installs to cloud containerized orchestration, with cross-platform compatibility.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45436027</guid><pubDate>Wed, 01 Oct 2025 10:00:29 +0000</pubDate></item><item><title>TigerBeetle is a most interesting database</title><link>https://www.amplifypartners.com/blog-posts/why-tigerbeetle-is-the-most-interesting-database-in-the-world</link><description>&lt;doc fingerprint="b719907d89353fdf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why TigerBeetle is the most interesting database in the world&lt;/head&gt;
    &lt;p&gt;By many measures it√¢s safe to say that TigerBeetle is the most interesting database in the world. Like Costanza in Seinfeld, they seem to do the opposite of everyone else:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Most teams write code fast. TigerBeetle tries to write code slow.&lt;/item&gt;
      &lt;item&gt;Most teams treat testing as a necessary evil. TigerBeetle is built entirely on Deterministic Simulation Testing (DST).&lt;/item&gt;
      &lt;item&gt;Most teams build their software on top of loads of other software. TigerBeetle has zero dependencies.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There√¢s even more. TigerBeetle enforces static memory allocation. They keep assertions enabled in production. They chose Viewstamped Replication over Raft, and even Zig instead of Rust!&lt;/p&gt;
    &lt;p&gt;This read is going to go behind the scenes of how TigerBeetle came to be, the incredibly novel software they√¢ve built, and all of the wacky, wonderful things that make them so special. Based on extensive interviews with the TigerBeetle team, we√¢re going to cover a few topics in technical detail:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Why transactional databases should think in debits and credits, not SQL&lt;/item&gt;
      &lt;item&gt;An (actually) modern database: distributed by default, handling storage faults, and why TigerBeetle uses Zig&lt;/item&gt;
      &lt;item&gt;VOPR, TigerBeetle√¢s Deterministic Simulation Testing cluster&lt;/item&gt;
      &lt;item&gt;TigerStyle, and why you should use assertions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Click on any section to jump straight there, if you√¢re curious.√Ç √Ç&lt;/p&gt;
    &lt;head rend="h2"&gt;Why we need a database that thinks in debits and credits&lt;/head&gt;
    &lt;p&gt;TigerBeetle√¢s website calls it √¢The Financial Transactions Database.√¢ Its primitives are debits and credits, which are things you may be familiar with from your accounting requirement in college. And if you√¢re not a bank, you√¢re probably thinking this whole thing isn√¢t really for you. But Joran (TigerBeetle√¢s creator) would tell you otherwise: financial transactions, i.e. debits and credits, are actually exactly what transactional SQL was originally designed for.√Ç&lt;/p&gt;
    &lt;p&gt;Way back in 1985, Jim Gray (who would later win a Turing Award) wrote a seminal paper on transactions, titled A Measure of Transaction Processing Power. If you√¢ve heard of it before, it√¢s because in it, Gray defined a metric that 40 years later is still the most important measure for a database: TPS, or transactions per second. This would end up leading to such a fervent benchmark war among databases that an objective council √¢ the TPC √¢ needed to be formed to moderate.&lt;/p&gt;
    &lt;p&gt;But what does the √¢T√¢ in TPS actually mean? What is a transaction?&lt;/p&gt;
    &lt;p&gt;Your first guess might be a SQL transaction, but that√¢s not it. Gray actually defined it as a business transaction derived from the real world. Which is the reason databases were invented in the first place: to power businesses. And indeed, 20 years later, Gray continued to see the standard measure of transaction processing as a √¢DebitCredit:√¢&lt;/p&gt;
    &lt;quote&gt;√¢A database system to debit a bank account, do the standard double-entry bookkeeping and then reply to the terminal.√¢&lt;/quote&gt;
    &lt;p&gt;Mind you, SQL had already been around since the 70s at this point. And yet the luminary Gray still chose the debit/credit model √¢ because it was the canonical example of an everyday transaction. Debit/credit is the lingua franca of what it means to transact. It is not just for accounting and banks. It√¢s the reason for a database to provide guarantees like ACID in the first place.&lt;/p&gt;
    &lt;p&gt;And yet, if you want to use a SQL database to implement debits and credits today, you are probably going to have a bad time. To handle one debit/credit, a typical system √¢ like the central bank switch that Joran consulted on in 2020 √¢ needs to query account balances, lock those rows, wait for decisions in code, then write back and record the debit/credit. All in all, you√¢re looking at 10-20 SQL queries back and forth, while holding row locks across the network roundtrip time, for each transaction. This gets even worse when you consider the problem of hot rows, where many transactions often need to touch the same set of √¢house accounts√¢.√Ç&lt;/p&gt;
    &lt;p&gt;All the while (for better or worse), the world is moving faster and faster towards an √¢everything is a transaction√¢ model. Countries like India and Brazil are doing billions of transactions per month in instant payments. With FedNow in the U.S., we√¢re not far away from that reality either. Meanwhile, other sectors like energy, gaming, and cloud are all moving towards real-time billing. In less than a decade, the world has become at least three orders of magnitude more transactional. And yet the SQL databases we still use to power this are 20-30 years old. Can they hold up?&lt;/p&gt;
    &lt;p&gt;This is where TigerBeetle comes in. They designed a state-of-the-art database, from the ground up, to power the next era of transactions. In TigerBeetle, a debit/credit is a first class primitive and 8,190 of them can pack into a single 1MiB query via a one solitary roundtrip to the database. They call it √¢The 1000x Performance Idea,√¢ but in Joran√¢s words it√¢s √¢nothing special√¢.&lt;/p&gt;
    &lt;p&gt;They say databases take a decade to build. But TigerBeetle is complete and pretty much Jepsen-proof after just 3 and a half years. In June 2025, Kyle Kingsbury showed he was unable to break TigerBeetle√¢s foundations (he found 1 correctness bug in the read query engine, not affecting durability), even while corrupting the whole thing on every machine in various places.√Ç&lt;/p&gt;
    &lt;p&gt;The obvious question here √¢ how? How did TigerBeetle ship a production-ready, Jepsen-passing consensus and storage engine in 3.5 years when it typically takes a decade or more?&lt;/p&gt;
    &lt;head rend="h2"&gt;An (actually) modern database: distributed by default, why TigerBeetle uses Zig, and handling storage faults&lt;/head&gt;
    &lt;p&gt;Imagine you wake up today and wisely decide to build a database from scratch. Instead of investing in the technology of 30 years ago √¢ when the most popular relational databases today were built √¢ you can pick any advancements in architecture, hardware, language, or research since then to implement. How would you build it? What would you utilize?&lt;/p&gt;
    &lt;head rend="h3"&gt;Distributed by default&lt;/head&gt;
    &lt;p&gt;One thing you√¢d probably start with is the deployment model.√Ç&lt;/p&gt;
    &lt;p&gt;When Postgres and MySQL were built, in a world of big iron (on-prem hardware), the dominant paradigm was single node. Now, in a world of shared cloud hardware, it√¢s distributed. It√¢s not safe enough to store your transactions only on a single disk or server. A modern database needs to replicate your transactions, with strict serializability, across machines, for redundancy, fault tolerance and high availability. And yet some of the most popular OLTP databases in the world today are still highly dependent on a single node architecture. Automated failover, at least with zero data loss in the cut over, is not always baked in by default.&lt;/p&gt;
    &lt;p&gt;So TigerBeetle built their database to be distributed by default. Doing that comes with some of the obvious things you need to do, like consensus. But the developer experience for running TigerBeetle distributed is very simple: you just install the binary on however many machines you want in the cluster. No async replication, no Zookeeper, etc. To make this possible, TigerBeetle invested heavily in their consensus protocol implementation, adopting the pioneering Viewstamped Replication from MIT. This is part of why TigerBeetle has zero dependencies, apart from the Zig toolchain √¢ they literally invested in all their core dependencies.&lt;/p&gt;
    &lt;head rend="h3"&gt;Clock fault tolerance&lt;/head&gt;
    &lt;p&gt;Distributed by default also shows up in some unlikely places. For example: have you ever thought of a clock fault model?√Ç&lt;/p&gt;
    &lt;p&gt;Though it√¢s not technically required or advised for consensus √¢ which uses logical clocks and not physical clocks √¢ remember that TigerBeetle is a transactions database. The physical timestamps of transactions need to be accurate and comparable across different financial systems for auditing and compliance.&lt;/p&gt;
    &lt;p&gt;And here, readers will note that Linux has several clocks: &lt;code&gt;CLOCK_MONOTONIC_RAW&lt;/code&gt;, &lt;code&gt;CLOCK_MONOTONIC&lt;/code&gt; and &lt;code&gt;CLOCK_BOOTTIME&lt;/code&gt;. All have slight but important differences. Which is the best monotonic clock to use? (clue: It doesn√¢t say &lt;code&gt;MONOTONIC&lt;/code&gt; on the tin)&lt;/p&gt;
    &lt;p&gt;The challenge is that physical imperfections in hardware clocks cause clocks to tick at different speeds, so that time passes faster or slower than it should. These kinds of √¢drift√¢ errors eventually add up to significant √¢skew√¢ errors within a short space of time. Most of the time, Network Time Protocol (NTP) would correct for these errors. But if NTP silently stops working because of a partial network outage, then a highly available consensus cluster might otherwise be running blind, in the dark.&lt;/p&gt;
    &lt;p&gt;But even this is something TigerBeetle thought about. They combine the majority of clocks in the cluster to construct a fault-tolerant clock called √¢cluster time√¢. This cluster time then gets used to bring a server√¢s system time back into line if necessary, or shut down safely if TigerBeetle detects that there are too many faulty clocks (e.g. TigerBeetle can actually detect when something like Chrony, PTP, or NTP have stopped working and alert the operator).√Ç&lt;/p&gt;
    &lt;p&gt;They do this by tracking offset clock times between different TigerBeetle servers, sampling them, and passing them through Marzullo√¢s algorithm to estimate the most accurate possible interval (again, just to get a sense of whether clocks are being synced by the underlying clock sync protocol correctly).&lt;/p&gt;
    &lt;p&gt;Small things like this are exactly why distributed by default is hard, and doesn√¢t work as an add-on for older database models. You can read more about this in TigerBeetle's 3 clocks are better than one blog post.&lt;/p&gt;
    &lt;p&gt;√¢&lt;/p&gt;
    &lt;head rend="h3"&gt;Handling storage faults&lt;/head&gt;
    &lt;p&gt;Another piece of √¢distributed by default√¢ that deserves its own header is how TigerBeetle handles storage faults (or even the fact it handles them at all). Traditional databases assume that if disks fail, they do so predictably with a nice error message. For example, even SQLite√¢s docs are clear that:&lt;/p&gt;
    &lt;quote&gt;SQLite does not add any redundancy to the database file for the purpose of detecting corruption or I/O errors. SQLite assumes that the data it reads is exactly the same data that it previously wrote.&lt;/quote&gt;
    &lt;p&gt;In reality, there are many more sinister possibilities: disks can silently return corrupt data, misdirect I/O (on the read or write path), or just suddenly get really slow (called gray failure in the research), all without returning error codes.√Ç&lt;/p&gt;
    &lt;p&gt;TigerBeetle is built to be storage fault tolerant:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;TigerBeetle uses Protocol Aware Recovery to remain available unless all copies of a piece of data get corrupted on every single replica.&lt;/item&gt;
      &lt;item&gt;All data in TigerBeetle is immutable, checksummed, and hash-chained, providing a strong guarantee that no corruption or tampering happened.&lt;/item&gt;
      &lt;item&gt;TigerBeetle puts as little software as possible between itself and the disk, including a custom page cache, writing data to disk with O_DIRECT, and even running on a raw block device directly (no filesystem necessary √¢ to sidestep filesystem bugs which do tend to happen from time to time).&lt;/item&gt;
      &lt;item&gt;They built their own implementation of LSM instead of using an off-the-shelf one √¢ they call it an LSM Forest, which is something like 20 different LSM trees.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As far as I√¢m aware TigerBeetle is the only distributed database that not only claims to be storage fault tolerant, but was also tested pretty hard and validated by Jepsen to be. If you have a local machine failure where even just a disk sector fails, then that storage engine is connected to the global consensus, and it can use the cluster to self heal. This is also a great example of why the modern database having access to modern research matters: Protocol-Aware Recovery, which enables TigerBeetle to survive disk failures like this, is fairly recent (2018) research.&lt;/p&gt;
    &lt;head rend="h3"&gt;TigerBeetle in Zig&lt;/head&gt;
    &lt;p&gt;Another thing you√¢d think about when building a modern database from scratch is your choice of programming language. Postgres is written in C (c. 1970s), MySQL in C and C++ (1979), and MSSQL as well in C and C++. But programming languages have come a long way in the past 40 years. If you had your choice, what would you build a database in today?&lt;/p&gt;
    &lt;p&gt;The answer would probably be Rust or Zig. And indeed, TigerBeetle is built 100% in Zig:√Ç&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You get the whole C ecosystem available to you, extended with a phenomenal toolchain and compiler.&lt;/item&gt;
      &lt;item&gt;It√¢s easy to write, and especially easy to read, in some cases as easy as TypeScript (just a lot faster).&lt;/item&gt;
      &lt;item&gt;Zig lets you statically allocate memory, which is a core principle of TigerBeetle.&lt;/item&gt;
      &lt;item&gt;Zig has a great developer experience and you can learn it quickly (which ergo means you can get into the TigerBeetle src quickly).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of course, as new systems languages, Zig and Rust are related, and some of the early Rust team now work at TigerBeetle, including Matklad (creator of Rust Analyzer) and Brian Anderson (co-creator of Rust with Graydon). They√¢ve written extensively about these languages and why Joran chose Zig in particular for TigerBeetle, given their design goals.&lt;/p&gt;
    &lt;p&gt;And here, of course, TigerBeetle is fanatical about static memory allocation, which I√¢ll talk more about in the next section. Not using dynamic memory allocation is √¢hard mode√¢ in Rust (as matklad wrote about here), but a breeze in Zig.&lt;/p&gt;
    &lt;p&gt;√¢&lt;/p&gt;
    &lt;head rend="h2"&gt;Deterministic Simulation Testing and the VOPR&lt;/head&gt;
    &lt;p&gt;Sometimes, Deterministic Simulation Testing (DST) feels like the most transformational technology that the fewest developers know about. It√¢s a novel testing technique made popular by the FoundationDB team (which now belongs to Apple); they used it to develop a more secure, bug-free distributed database in a shorter time span than arguably anyone had done before.√Ç&lt;/p&gt;
    &lt;p&gt;The fundamentals of DST go something like this. In distributed systems, there are essentially infinite combinations of concurrency issues: anything from lost messages to unpredictable thread execution order. You simply cannot use old-school unit and integration tests, or your system will suck. Formal verification, a more academic discipline that works on formulaic proofs that a program runs as intended, is too expensive and slow. So what are you to do?&lt;/p&gt;
    &lt;p&gt;The answer is a simulator that deterministically runs almost every possible scenario your system will face on a specific chronological timeline. The simulator accounts for external factors too, like issues with the OS, network, or disk, or simply different latencies. All in all, DST can give you the equivalent of years√¢ worth of testing in a very short time period (because time itself becomes deterministic√¢a while true loop); and DST is particularly well suited towards databases (I/O intensive, not compute intensive). If you√¢re familiar with Jepsen testing, think of it as a subset of what DST can do.√Ç&lt;/p&gt;
    &lt;p&gt;TigerBeetle is one of the most pioneering startups on the planet when it comes to DST. They√¢ve developed their own testing cluster √¢ it√¢s nicknamed VOPR, short for Viewstamped Operation Replicator (after the WOPR simulator in the movie WarGames). The VOPR constantly (and tirelessly) tests TigerBeetle under countless different conditions, covering everything from how nodes elect a leader to individual states and network faults. But it can simulate a whole distributed cluster virtually, all on a single thread.&lt;/p&gt;
    &lt;p&gt;As far as your author is aware, TigerBeetle√¢s VOPR is the single largest DST cluster on the planet. It runs on 1,000 CPU cores, a number so unusually large that Hetzner sent them a special email asking if they were sure they wanted that many cores. The so-called VOPR-1000 is running 24x7x365, to catch rare conditions as far as possible before production. With time abstracted deterministically, and accelerated in the simulator by a factor of (roughly) 700x, this adds up to nearly 2 millennia of simulated runtime per day.&lt;/p&gt;
    &lt;p&gt;√¢&lt;/p&gt;
    &lt;head rend="h3"&gt;But what if DST was fun?&lt;/head&gt;
    &lt;p&gt;Yea, distributed systems are cool. But you know what√¢s even cooler? Video games.&lt;/p&gt;
    &lt;p&gt;TigerBeetle turned DST into a game that lets you play through different failure scenarios in how the system reacts. You can play it here.&lt;/p&gt;
    &lt;p&gt;What√¢s perhaps even cooler is that this game is running an actual instance of the VOPR, simulating TigerBeetle√¢¬¶in your browser. It√¢s compiled to WebAssembly, and then TigerBeetle√¢s own engineers built a gaming frontend on top to visualize the real system&lt;/p&gt;
    &lt;p&gt;You can read more about how and why TigerBeetle built the simulator in this blog post.&lt;/p&gt;
    &lt;p&gt;√¢&lt;/p&gt;
    &lt;head rend="h2"&gt;TigerStyle and The Power of Ten&lt;/head&gt;
    &lt;p&gt;As you will continue to see with TigerBeetle, it is often not just the what they√¢ve built that catches the eye but also the how. There√¢s no better example than TigerStyle.&lt;/p&gt;
    &lt;p&gt;TigerStyle is TigerBeetle√¢s engineering methodology, public on GitHub for all to see. Here√¢s how they describe it:&lt;/p&gt;
    &lt;quote&gt;√¢TigerBeetle's coding style is evolving. A collective give-and-take at the intersection of engineering and art. Numbers and human intuition. Reason and experience. First principles and knowledge. Precision and poetry. Just like music. A tight beat. A rare groove. Words that rhyme and rhymes that break. Biodigital jazz. This is what we've learned along the way. The best is yet to come.√¢&lt;/quote&gt;
    &lt;p&gt;Biodigital jazz is a term from Tron: Legacy. In the context of the film, it represents the intertwining of human and digital elements, the chaotic yet structured nature of the √¢Grid√¢ (the digital world), and the improvisational spirit of human potential within the confines of technology (I copied this from AI). For TigerBeetle, it√¢s an ethos of code; remembering to infuse everything they do with not just science, but art too.&lt;/p&gt;
    &lt;p&gt;More practically, TigerStyle lays out engineering and code principles for TigerBeetle, many derived from the original Power of Ten, NASA√¢s tenets for writing foolproof code. TigerStyle spans from the thematic, like simplicity and elegance, to the applied, like how to name things. It√¢s even starting to impact other companies like Resonate and Turso; and TigerStyle has even been discussed on Lex Fridman. Here are a few highlights.&lt;/p&gt;
    &lt;head rend="h3"&gt;Using assertions, and the Power of Ten&lt;/head&gt;
    &lt;p&gt;Speaking of the Power of Ten√¢¬¶one of them (Rule 5) is about assertions. The idea is simple: explicitly encode your expectations of code behavior while you are writing it, not after the fact. You write them simply in a single line as booleans: assert(a &amp;gt; b). TigerStyle calls for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Asserting all function arguments, return values, preconditions, and invariants. On average there should be at least 2 assertions per function.&lt;/item&gt;
      &lt;item&gt;Using assertions instead of comments when the assertion is both important and surprising.&lt;/item&gt;
      &lt;item&gt;Asserting the relationships between compile-time constants, so you can check a program√¢s design integrity before it even runs.&lt;/item&gt;
      &lt;item&gt;Not just assert what should happen, but also the negative space that you don√¢t expect √¢ where interesting bugs can show up.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Power of Ten is an amazing artifact that covers so much more than just assertions√¢¬¶it√¢s a great resource for any modern programmer (and maybe we should train some LLMs on it too).&lt;/p&gt;
    &lt;head rend="h3"&gt;Thinking about performance&lt;/head&gt;
    &lt;p&gt;Much of TigerStyle centers around the idea that writing code is not the most important part of the cycle; instead, it√¢s reasoning about and designing the code. When it comes to performance, TigerStyle implores you to think about it from the start:√Ç&lt;/p&gt;
    &lt;quote&gt;√¢The best time to solve performance, to get the huge 1000x wins, is in the design phase, which is precisely when we can't measure or profile.√¢&lt;/quote&gt;
    &lt;p&gt;You should be doing basic napkin math on what TigerStyle calls √¢the four primary colors√¢ √¢ network, storage, memory, CPU √¢ and how they√¢ll perform with respect to (√¢the two textures√¢ √¢ art!) bandwidth and latency. Then, there are a few more tactical tips, like distinguishing between the control plane and data plane, batching accesses, and extracting hot loops into stand-alone functions to reduce dependence on the compiler.√Ç&lt;/p&gt;
    &lt;p&gt;For more about TigerStyle, watch Joran√¢s talk at Systems Distributed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try it out for yourself&lt;/head&gt;
    &lt;p&gt;So is TigerBeetle a database? Yes. But it√¢s not much like any other database I√¢ve seen. They√¢ve taken modern research and applied it to an age-old form, giving their database unprecedented performance and stability guarantees. They√¢ve developed an art form around systems and storage engineering, and they haven√¢t forgotten to have fun along the way. And thanks to their clever use of DST, they were able to build this thing to Jepsen standards in only a few years.√Ç&lt;/p&gt;
    &lt;p&gt;You can get started with TigerBeetle here using a simple curl command.√Ç&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45436534</guid><pubDate>Wed, 01 Oct 2025 11:33:19 +0000</pubDate></item><item><title>Broadcom Fails to Disclose Zero-Day Exploitation of VMware Vulnerability</title><link>https://www.securityweek.com/broadcom-fails-to-disclose-zero-day-exploitation-of-vmware-vulnerability/</link><description>&lt;doc fingerprint="bdc0b9d5a92f1325"&gt;
  &lt;main&gt;
    &lt;p&gt;A newly patched high-severity VMware vulnerability has been exploited as a zero-day since October 2024 for code execution with elevated privileges, NVISO Labs reports.&lt;/p&gt;
    &lt;p&gt;Tracked as CVE-2025-41244 (CVSS score of 7.8), the security defect impacts both VMware Aria Operations and VMware Tools.&lt;/p&gt;
    &lt;p&gt;VMware‚Äôs parent company Broadcom rolled out patches this week, warning that the flaw allows attackers to escalate their privileges to root on VMs that have VMware Tools installed and are managed by Aria Operations with SDMP enabled, but made no mention of its in-the-wild exploitation.&lt;/p&gt;
    &lt;p&gt;The company‚Äôs public advisories typically warn customers if zero-day exploitation has been detected.&lt;/p&gt;
    &lt;p&gt;According to NVISO, which was credited for the find, a Chinese state-sponsored threat actor tracked as UNC5174 has been exploiting the bug for a year. UNC5174 was recently linked to an attack on cybersecurity firm SentinelOne.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe can however not assess whether this exploit was part of UNC5174‚Äôs capabilities or whether the zero-day‚Äôs usage was merely accidental due to its trivialness,‚Äù NVISO notes.&lt;/p&gt;
    &lt;p&gt;The vulnerability impacts VMware Aria Operations‚Äô service and application discovery feature, which includes both legacy credential-based service discovery (in which VMware Tools acts as a proxy for the operation) and credential-less service discovery (metrics collection implemented in VMware Tools).&lt;/p&gt;
    &lt;p&gt;‚ÄúAs part of its discovery, NVISO was able to confirm the privilege escalation affects both modes, with the logic flaw hence being respectively located within VMware Aria Operations (in credential-based mode) and the VMware Tools (in credential-less mode),‚Äù NVISO explains.&lt;/p&gt;
    &lt;p&gt;Noting that successful exploitation of CVE-2025-41244 allows unprivileged users to execute code with root privileges, NVISO warns that the open source variant of VMware Tools, namely open-vm-tools, which is included in major Linux distributions, is also impacted.&lt;/p&gt;
    &lt;p&gt;Open-vm-tools‚Äô discovery function, NVISO says, calls a function that takes as argument a regular expression pattern that checks it to match supported service binaries.&lt;/p&gt;
    &lt;p&gt;However, because the function uses the broad‚Äëmatching \S character class in several regex patterns, it also matches non-system binaries located in directories writable to non-privileged users.&lt;/p&gt;
    &lt;p&gt;Thus, an attacker can abuse a vulnerable open-vm-tools iteration by staging a malicious binary in a broadly-matched regular expression path, and it will be elevated for version discovery.&lt;/p&gt;
    &lt;p&gt;UNC5174, NVISO notes, has been exploiting the security weakness by placing malicious binaries in the /tmp/httpd folder. To be elevated, the binaries are executed with low privileges and open a random listening socket.&lt;/p&gt;
    &lt;p&gt;Broadcom fixed the flaw in fresh releases of VMware Cloud Foundation, vSphere Foundation, Aria Operations, Telco Cloud Platform, and VMware Tools, and noted that fixes for open-vm-tools will be distributed by Linux vendors.&lt;/p&gt;
    &lt;p&gt;To detect CVE-2025-41244‚Äôs exploitation, organizations should look for uncommon child processes. In environments without monitoring, analysis of lingering metrics collector scripts and outputs in legacy credential-based mode should confirm the exploitation.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe broad practice of mimicking system binaries (e.g., httpd) highlights the real possibility that several other malware strains have accidentally been benefiting from unintended privilege escalations for years,‚Äù NVISO says, noting that the bug could easily be found in the open-vm-tools source code by threat actors.&lt;/p&gt;
    &lt;p&gt;Related: Call for Presentations Open for 2025 CISO Forum Virtual Summit&lt;/p&gt;
    &lt;p&gt;Related: Google Patches Gemini AI Hacks Involving Poisoned Logs, Search Results&lt;/p&gt;
    &lt;p&gt;Related: Apple Updates iOS and macOS to Prevent Malicious Font Attacks&lt;/p&gt;
    &lt;p&gt;Related: Organizations Warned of Exploited Sudo Vulnerability&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45437003</guid><pubDate>Wed, 01 Oct 2025 12:41:23 +0000</pubDate></item><item><title>Building an IoT Notification Device from Scratch</title><link>https://bertwagner.com/posts/splashflag-building-an-iot-swimming-notification-device-from-scratch/</link><description>&lt;doc fingerprint="bf8d1011eb27b1e8"&gt;
  &lt;main&gt;
    &lt;p&gt;After setting up dozens of Internet of Things (IoT) smart home devices, I started to wonder: how hard could it be to build one from scratch?&lt;/p&gt;
    &lt;p&gt;I needed a project to learn on, so I decided to create something fun: a device that alerts my neighbors when my kids go swimming, extending the invitation for their kids to come swim too.&lt;/p&gt;
    &lt;p&gt;What follows are the lessons I learned from building such an IoT device from scratch.&lt;/p&gt;
    &lt;head rend="h1"&gt;Demo and Code&lt;/head&gt;
    &lt;p&gt;Here's a short video demoing the device and its features:&lt;/p&gt;
    &lt;p&gt;The instructions and code for building your own Splashflag can be found at the bottom of this post, otherwise keep reading to learn about my journey in building the device.&lt;/p&gt;
    &lt;head rend="h1"&gt;Why SplashFlag?&lt;/head&gt;
    &lt;p&gt;How many times can you play Marco Polo in a pool with an adult and two kids? I know that my kids far prefer the company of their friends who have a lot more energy.&lt;/p&gt;
    &lt;p&gt;Originally our idea was to put up a special "we're swimming" flag outside in our front yard when our kids were in the pool, alerting the neighbors that they are welcome to come over and swim as well. The flag would be an open invitation, without the overhead of planning, group texts, and phone calls.&lt;/p&gt;
    &lt;p&gt;I quickly realized this idea wouldn't work because: 1) The flag wouldn't be easily visible from every neighbor's house 2) By the time people saw the flag and came over, we might already be wrapping up our swimming session&lt;/p&gt;
    &lt;p&gt;What I needed to solve this social problem was technology (or rather, I needed an excuse for a new technology hobby project), which is how the idea for SplashFlag was born.&lt;/p&gt;
    &lt;head rend="h1"&gt;Key Features and Learnings&lt;/head&gt;
    &lt;p&gt;This wasn't my first time building an embedded device, but it was the first time I tried to follow at least some semblance of best practices: &lt;code&gt;main&lt;/code&gt; loops less than a thousand lines long, no hardcoded passwords, etc...&lt;/p&gt;
    &lt;p&gt;If this was going to be a true learning project, I wanted to be more organized: use classes, design hardware and software that could handle errors gracefully, and create a way for users to connect the device to their WiFi without me ever needing to know their credentials.&lt;/p&gt;
    &lt;p&gt;While I wouldn't consider this code to be perfect (or even necessarily "good"), it's a huge improvement over hardware projects I've built in the past, so I consider it a success.&lt;/p&gt;
    &lt;p&gt;Below is an overview of the major features I built into the device.&lt;/p&gt;
    &lt;head rend="h2"&gt;Servo Flag&lt;/head&gt;
    &lt;p&gt;This is how the idea started: instead of the physical flag in my front yard, it would be a small plastic flag sitting on the counter in a friend's home.&lt;/p&gt;
    &lt;p&gt;Whenever the device receives a message, the flag goes up until the message expires. Besides being a fun feature, it works well in households where the kids are still too young to read the details of the message - regardless of what the screen says, if the flag is raised, they know the Wagners are swimming and they're welcome to come swim too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Clear/Reset Button&lt;/head&gt;
    &lt;p&gt;This button wasn't originally planned as a feature. The first time I got the servo flag working and realized it might be raised for an hour, I knew that as a parent I'd want a way to clear the notification without my kid seeing it. So the hidden button on the back of the device became a necessary enhancement. Push it, and the message clears while the flag goes down.&lt;/p&gt;
    &lt;p&gt;It also serves double duty for triggering a factory reset.&lt;/p&gt;
    &lt;head rend="h2"&gt;LCD&lt;/head&gt;
    &lt;p&gt;In addition to the servo flag, the LCD displays messages about swimming. The default message indicates how long we plan to swim, but the web app (see below) lets me write any message, so something like "feel free to stay for pizza" is a potential customization.&lt;/p&gt;
    &lt;p&gt;The LCD also displays system messages, like when the device is having trouble connecting to WiFi or when the messaging server is down.&lt;/p&gt;
    &lt;p&gt;The code for the LCD was fun to write: since the screen can only fit two rows of 16 characters, I had to write a function that could split any length message so it fit these constraints and scroll across multiple screens.&lt;/p&gt;
    &lt;p&gt;This is also where I first encountered overflow errors and needed to add max-length validations:&lt;/p&gt;
    &lt;p&gt;The screen works well and serves its purpose, but along with the I2C adapter, it is easily the biggest component in the device. Next time, I plan to look for slimmer options, because the device size (especially the front frame with the LCD) could have been considerably smaller if I had chosen a different board.&lt;/p&gt;
    &lt;head rend="h2"&gt;Captive Portal&lt;/head&gt;
    &lt;p&gt;Before this project, I never knew the magic that allowed login pages to pop up on your device when connecting to a guest wifi network.&lt;/p&gt;
    &lt;p&gt;It turns out, it's DNS!&lt;/p&gt;
    &lt;p&gt;This guest wifi login experience is what I wanted to build into my device. After all, I wanted my neighbors to be able to set this up in their house, all on their own, without me needing to know or hardcode any WiFi passwords.&lt;/p&gt;
    &lt;p&gt;The short explanation of how this works is that when your phone connects to a new WiFi network, the phone will try to visit certain well-known URLs. If you own the WiFi network, you can configure DNS to look for those standard URLs and intercept them, serving your own login page.&lt;/p&gt;
    &lt;p&gt;Fortunately there are some good libraries for setting up a DNS server on ESP32s and intercepting traffic, then serving your own captive portal where people can input their WiFi credentials.&lt;/p&gt;
    &lt;head rend="h2"&gt;Over the Air (OTA) Updates&lt;/head&gt;
    &lt;p&gt;Another feature I wanted to include was the ability to update the firmware remotely. Debugging and flashing new firmware while the device was sitting on my desk was easy, but I know my code isn't perfect so I wanted a way to update these devices remotely in the future.&lt;/p&gt;
    &lt;p&gt;Fortunately the ESP32-S3 Nano device I was using allows for OTA firmware updates. I set up the library for this and have it check the GitHub releases page for SplashFlag every day to see if a new version is available. If a new version exists, it will download the update and install it.&lt;/p&gt;
    &lt;p&gt;Here's hoping I don't accidentally brick anyones SplashFlag device.&lt;/p&gt;
    &lt;head rend="h2"&gt;Web App&lt;/head&gt;
    &lt;p&gt;I wanted a simple interface for sending messages to all devices, so I created a single page web application. It defaults to the most common message I would send, with input parameters to adjust the duration of how long the message is displayed on the devices.&lt;/p&gt;
    &lt;p&gt;The website runs on my home server and I expose it through CloudFlare Tunnels so I can easily access it from my phone (or anywhere). I also added HTTP Basic Authentication to the website - not the most robust option, but good enough for this project. In the future, if I upgrade anything, it would be the authentication system. Basic Authentication is secure enough, but it doesn't play nicely with 1Password or Safari on iOS, which causes some minor annoyances.&lt;/p&gt;
    &lt;p&gt;The web app sends a message to the MQTT broker (see below) over WebSockets, which then publishes it to all devices. Because this is just a simple API call, I could easily program a smart button to trigger this in the future, allowing my kids to send the notification themselves when we head out to the pool.&lt;/p&gt;
    &lt;head rend="h2"&gt;MQTT Messaging&lt;/head&gt;
    &lt;p&gt;I didn't want my devices long-polling a web server to check the status of new swim messages. Instead I decided to use an MQTT broker running in my home lab to transmit messages to the SplashFlag devices running the MQTT client code.&lt;/p&gt;
    &lt;p&gt;I am using mosquitto as the MQTT broker. The MQTT broker service runs on my home server and is exposed via WebSockets to the web app. All of the devices subscribe to the broker service, so whenever a message is published, every device receives the message and updates its screen and raises its flag.&lt;/p&gt;
    &lt;head rend="h2"&gt;Debugging Hardware Flag&lt;/head&gt;
    &lt;p&gt;Since I may need to continue development (e.g. fixing bugs) once these devices are in the wild, I wanted a way to send messages to only my debugging device. Fortunately, ESP32-S3 Nanos have a unique mac address identifier, so I wrote code to check whether a device is a development unit under my control. If it is, it subscribes to an additional MQTT topic that only receives debugging messages. Debugging messages are set through the web app via a toggle:&lt;/p&gt;
    &lt;p&gt;The fact that the ESP32-S3 Nanos have their own unique identifier means I don't need different code for production devices versus debugging devices (or at least in that section of the code). While I could have handled this in other ways, I'm satisfied with how this solution works.&lt;/p&gt;
    &lt;head rend="h2"&gt;3D Printed Case&lt;/head&gt;
    &lt;p&gt;While I learned a ton designing this case, the smartest thing I did was test print each CAD feature as I finished designing it. This meant printing something like the servo holder only took 15 minutes, and then I could dry fit the parts together to ensure they actually fit. This saved a lot of time on iterating fit and printing, as well as minimizing plastic waste.&lt;/p&gt;
    &lt;p&gt;My CAD design experience before SplashFlag was limited to simple enclosures. SplashFlag taught me how to make more complex designs, including screw mounts and snap-fit parts.&lt;/p&gt;
    &lt;p&gt;The case design is simple. The front panel holds the LCD. The remaining components (ESP32-S3 Nano, servo, USB-C power plug, tactile reset button) mount to the back panel.&lt;/p&gt;
    &lt;p&gt;As mentioned earlier, the LCD with I2C breakout took up a lot of room, leading to the wide frame around the LCD.&lt;/p&gt;
    &lt;p&gt;The ESP32-S3 Nano, servo, and USB-C plug attach to the case with small metric screws. The LCD panel bolts onto the front plate with an embedded nut.&lt;/p&gt;
    &lt;p&gt;While I was designing this, it took many iterations to figure out how to get all the parts to fit together while minimizing space. I also had to consider the limitations of 3D printers and assembly so the final case would be possible to print and assemble successfully.&lt;/p&gt;
    &lt;p&gt;One oversight I made was forgetting to leave space for the bolts that would hold the two halves of the case together. Originally I was going to make the case snap-fit together without any hardware, so I didn't include bolts in my design, but I decided to add bolts once I realized how relatively heavy all the components would be and all the wiring that would need to stay confined to the inside of the device. If I had modeled the case properly in Fusion360 with components, I could have easily moved things around after I realized I needed case bolts. But since I didn't realize I could do that until I had spent hours designing the case the wrong way, I decided to be OK settling with the imperfections.&lt;/p&gt;
    &lt;p&gt;My favorite part of the case is the snap-fit housing for the tactile button. It allows for a small breadboard switch to become a neatly integrated button. Definitely a design I will use again in future projects:&lt;/p&gt;
    &lt;p&gt;As a final embellishment, I added the text SplashFlag to the top of the device. I only have a single-color 3D printer, so this meant printing the outline of the letters in blue, then the letters themselves (slightly smaller) in white. Final assembly involved super gluing them together on the top of the case.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Isn't Included&lt;/head&gt;
    &lt;p&gt;As much as I packed this device with features, I didn't include everything.&lt;/p&gt;
    &lt;p&gt;If I wanted to devote more time to this project, I'd probably first enable TLS for all HTTP connections. The idea of writing code to automatically update the CA certs on the device (and maintaining TLS certs on the server app) over time didn't seem worth it for a hobby project.&lt;/p&gt;
    &lt;p&gt;Also, secrets like the WiFi password and MQTT credentials are not encrypted at rest. They are stored in the ESP32-S3 Nano's non-volatile memory, allowing anyone with physical access to the device to potentially retrieve them without too much effort. The ESP32-S3 Nano does have eFuses which can help with storing cryptographic keys (which could then be used to encrypt the credentials), but adding that capability didn't make the cut due to time constraints.&lt;/p&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Who knows if I will use it, but this build taught me I am capable of making steady, regular progress towards long-term projects.&lt;/p&gt;
    &lt;p&gt;I am probably not going to large scale manufacture these for the neighborhood, but I gained confidence in building a complete hardware device on my own that works well. When the next product idea strikes, I'll already have a lot of the knowledge (and code!) required for building it.&lt;/p&gt;
    &lt;p&gt;Is it perfect? No. Is it better than some IoT devices I've purchased, with easier updates and repairs? Definitely.&lt;/p&gt;
    &lt;head rend="h1"&gt;How to Build Your Own SplashFlag&lt;/head&gt;
    &lt;head rend="h3"&gt;Parts List&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ESP32-S3 Nano&lt;/item&gt;
      &lt;item&gt;1602 LCD Display with I2C adapter for easier wiring&lt;/item&gt;
      &lt;item&gt;Feetech FS90 9g Servo&lt;/item&gt;
      &lt;item&gt;6x6x5mm tactile push button switch&lt;/item&gt;
      &lt;item&gt;.1 uF decoupling capacitor&lt;/item&gt;
      &lt;item&gt;220ohm pull-down resistor&lt;/item&gt;
      &lt;item&gt;USB-C Power Adapter and cable&lt;/item&gt;
      &lt;item&gt;Variety of M1.6-M2.5 screws and bolts to mount all the components&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Code and Files&lt;/head&gt;
    &lt;p&gt;Complete code for the ESP32-S3 Nano and web app, as well as the 3D printed case are available at the SplashFlag GitHub Repo.&lt;/p&gt;
    &lt;p&gt;The code for the ESP32-S3 Nano is configured with PlatformIO. This configuration handles downloading and installing all C++ dependencies. If you use PlatformIO in Visual Studio Code like I do, you can open the repo's &lt;code&gt;hardware&lt;/code&gt; folder, build the code, and upload it to the board.&lt;/p&gt;
    &lt;p&gt;The web app runs in a Docker container. You will need to generate an auth password for the mosquitto service as well as configure a Cloudflare Tunnel if you are going to be self hosting. Details for doing this are in SplashFlag backend README.&lt;/p&gt;
    &lt;head rend="h3"&gt;Case Assembly and Wiring&lt;/head&gt;
    &lt;p&gt;3D printing the files shouldn't require any supports, with the exception of the cutout for the USB-C cable. I used .10mm layer heights with PLA+ filaments. The snap-fit tactile button is the component with the tightest dimensional tolerance - I recommend slicing up the back panel and printing only this button enclosure at first to ensure your printer is calibrated correctly. If that piece prints correctly, the rest of the case should print without any issues.&lt;/p&gt;
    &lt;p&gt;I don't have a nice wiring schematic, but the photo below with details should help.&lt;lb/&gt; - The I2C adapter is wired to pins A4 and A5. - The servo is wired to D9 as well as 5V and ground. I put the decoupling capacitor near the servo to ensure smooth power. - The USB-C adapter gets the stable power in, and it is tied to the VIN and ground pins on the ESP32-S3 Nano. - The tactile button is tied to ground with the pull-down resistor and wired to D4 on the ESP32-S3 Nano. &lt;/p&gt;
    &lt;p&gt;After confirming everything worked on a breadboard, I mounted the components to the case. I then soldered the component wires together, tested again, and once I was confident things still worked, encased all connections in hot glue:&lt;/p&gt;
    &lt;p&gt;Note: the red wire hanging off to the left in the above photo was an extra wire I later clipped off - I miscounted while creating my wires.&lt;/p&gt;
    &lt;p&gt;Maybe my next project will involve designing a custom PCB board to make the wiring easier.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45437027</guid><pubDate>Wed, 01 Oct 2025 12:43:51 +0000</pubDate></item><item><title>Hackers strike Harrods in latest UK cyberattack</title><link>https://observer.co.uk/news/national/article/hackers-strike-harrods-in-latest-uk-cyberattack</link><description>&lt;doc fingerprint="74010a15849eeefc"&gt;
  &lt;main&gt;
    &lt;p&gt;Harrods has warned customers that hackers may have stolen their personal data in the latest IT security breach to blight UK businesses.&lt;/p&gt;
    &lt;p&gt;The department store said √¢names and contact details√¢ were taken from an unnamed third-party system Harrods uses for online shopping. The breach did not extend to passwords or payment cards, it said.&lt;/p&gt;
    &lt;p&gt;In July, four people including a 17-year-old boy were arrested on suspicion of being involved in cyberattacks on Harrods, the Co-op, and Marks and Spencer, and were bailed pending further inquiries.&lt;/p&gt;
    &lt;p&gt;Reports of cyberattacks have been increasing in recent months. Last week, hackers stole names, addresses and pictures of about 8,000 children from Kido, a nursery chain, and published some on the dark net. Parents reportedly received phone calls from people claiming to be linked to the hack in an√Ç apparent blackmail attempt.&lt;/p&gt;
    &lt;p&gt;Thousands of passengers at Heathrow and other European airports were delayed last week after a ransomware attack on Collins Aerospace. A man in his 40s was arrested in West Sussex last week, the National Crime Agency said, although it was continuing to investigate.&lt;/p&gt;
    &lt;p&gt;Related articles:&lt;/p&gt;
    &lt;p&gt;Meanwhile, Jaguar Land Rover√¢s production lines remain idle after a cyberattack in August, and may not restart until November. LNER and Qantas have also suffered disruption from attacks this year, and last year the NHS postponed 11,000 appointments and procedures due to a ransomware attack.&lt;/p&gt;
    &lt;p&gt;Growing awareness of the frailties of the UK√¢s cybersecurity capabilities may undermine the prime minister√¢s desire to introduce digital ID cards in an attempt to control illegal working.&lt;/p&gt;
    &lt;p&gt;Yesterday, more than 1.6 million people had signed a parliamentary petition demanding that the government scrap plans for a digital ID. Professor Alan Woodward, a computer scientist at the University of Surrey, said an ID card database would present a huge target for hackers.&lt;/p&gt;
    &lt;p&gt;Although cyberattacks have become more high profile, there is little reliable data on how many attacks take place in the UK each year. Hackers are drawn from a mix of organised criminal gangs, those sponsored by state actors including Russia and China, and hacktivists with a political agenda, according to a report by the Royal United Services Institute (Rusi).&lt;/p&gt;
    &lt;p&gt;A government survey found that half of businesses reported some form of a security breach in the previous 12 months.&lt;/p&gt;
    &lt;p&gt;A new cybersecurity and √Çresilience bill will make it mandatory to report more incidents. The bill√¢s slow progress is frustrating security experts, according to Jamie MacColl, a senior research fellow at Rusi. However, ministers have been reluctant to impose more regulation on businesses, but having √¢major cybersecurity incidents is not good for economic growth,√¢ he said.&lt;/p&gt;
    &lt;p&gt;Further reading: Cyberattacks on JLR piles pressure on the government√Ç&lt;/p&gt;
    &lt;p&gt;Photograph by Andy Hall for The Observer&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45437060</guid><pubDate>Wed, 01 Oct 2025 12:47:10 +0000</pubDate></item><item><title>Detect Electron apps on Mac that hasn't been updated to fix the system wide lag</title><link>https://gist.github.com/tkafka/e3eb63a5ec448e9be6701bfd1f1b1e58</link><description>&lt;doc fingerprint="a2890fbb0412a7ae"&gt;
  &lt;main&gt;
    &lt;p&gt;See:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;electron/electron#48311 (comment)&lt;/item&gt;
      &lt;item&gt;https://mjtsai.com/blog/2025/09/30/electron-apps-causing-system-wide-lag-on-tahoe/&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Fixed versions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;36.9.2&lt;/item&gt;
      &lt;item&gt;37.6.0&lt;/item&gt;
      &lt;item&gt;38.2.0&lt;/item&gt;
      &lt;item&gt;39.0.0&lt;/item&gt;
      &lt;item&gt;and all above 39&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This script detects apps with not yet updated versions of Electron.&lt;/p&gt;
    &lt;p&gt;Run&lt;/p&gt;
    &lt;code&gt;launchctl setenv CHROME_HEADLESS 1&lt;/code&gt;
    &lt;p&gt;on every system start. The CHROME_HEADLESS flag has a side effect of disabling Electron app window shadows, which makes them ugly, but also stops triggering the issue.&lt;/p&gt;
    &lt;p&gt;(as of 1st oct 2025)&lt;/p&gt;
    &lt;code&gt;‚ùå OpenMTP.app: Electron 18.3.15 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
‚ùå DaVinci Resolve.app: Electron 36.3.2 (Contents/Applications/Electron.app/Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
‚ùå Electron.app: Electron 36.3.2 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
‚ùå Visual Studio Code.app: Electron 37.3.1 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
‚ùå Cursor.app: Electron 34.5.8 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
‚ùå Windsurf.app: Electron 34.4.0 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
‚ùå Claude.app: Electron 36.4.0 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
‚ùå Signal.app: Electron 38.1.2 (Contents/Frameworks/Electron Framework.framework/Electron Framework)
‚ùå Figma Beta.app: Electron 37.5.1 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
‚ùå Beeper Desktop.app: Electron 33.2.0 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
‚ùå Slack.app: Electron 38.1.2 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
&lt;/code&gt;
    &lt;p&gt;If you'd appreciate a visual (Tufte-like) hour by hour forecast for iOS/Apple Watch/mac with nice widgets, I made one - check out üå¶Ô∏è Weathergraph.&lt;/p&gt;
    &lt;p&gt;Thanks! Tomas&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45437112</guid><pubDate>Wed, 01 Oct 2025 12:54:17 +0000</pubDate></item><item><title>Minimal files and config for a PWA</title><link>https://github.com/chr15m/minimal-pwa</link><description>&lt;doc fingerprint="17eaf10100997cb2"&gt;
  &lt;main&gt;
    &lt;p&gt;This is the minimal set of files for a "progressive web app" to be installable on Android and iOS.&lt;/p&gt;
    &lt;p&gt;It contains the smallest possible &lt;code&gt;manifest.json&lt;/code&gt; and service worker to trigger the install flow on Chrome.&lt;/p&gt;
    &lt;p&gt;An even smaller implementation that fits in a single HTML file is in single-file-pwa.html. It has a manifest.json that is dynamically generated from JavaScript, and it is installable without a service worker.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45437326</guid><pubDate>Wed, 01 Oct 2025 13:14:48 +0000</pubDate></item><item><title>Show HN: ChartDB Agent ‚Äì Cursor for DB schema design</title><link>https://app.chartdb.io/ai</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45437594</guid><pubDate>Wed, 01 Oct 2025 13:38:36 +0000</pubDate></item><item><title>Cursor 1.7</title><link>https://cursor.com/changelog/1-7</link><description>&lt;doc fingerprint="ef16a358a5cbb834"&gt;
  &lt;main&gt;
    &lt;p&gt;1.7 ¬∑ Changelog&lt;/p&gt;
    &lt;head rend="h1"&gt;Agent Autocomplete, Hooks, and Team Rules&lt;/head&gt;
    &lt;head rend="h3"&gt;Autocomplete for Agent&lt;/head&gt;
    &lt;p&gt;When writing prompts, autocomplete suggestions will appear based on recent changes. Tab to accept suggestions and attach files to context.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hooks (beta)&lt;/head&gt;
    &lt;p&gt;You can now observe, control, and extend the Agent loop using custom scripts. Hooks give you a way to customize and influence Agent behavior at runtime.&lt;/p&gt;
    &lt;p&gt;Use Hooks to audit Agent usage, block commands, or redact secrets from context. It's still in beta and we'd love to hear your feedback.&lt;/p&gt;
    &lt;head rend="h3"&gt;Team rules&lt;/head&gt;
    &lt;p&gt;Teams can now define and share global rules from the dashboard that will be applied to all projects. We‚Äôve also shipped team rules for Bugbot, so behavior is consistent across repos.&lt;/p&gt;
    &lt;head rend="h3"&gt;Share prompts with deeplinks (beta)&lt;/head&gt;
    &lt;p&gt;Generate shareable deeplinks for reusable prompts. Useful for setup instructions in documentation, team resources, and sharing workflows. See our documentation for how to create them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Monitor Agents from menubar&lt;/head&gt;
    &lt;p&gt;Quickly check the status of Cursor Agents right from your menubar.&lt;/p&gt;
    &lt;head rend="h3"&gt;Image file support for Agent&lt;/head&gt;
    &lt;p&gt;Agent can now read image files directly from your workspace and include them in context. Previously, only pasted images were supported.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45437735</guid><pubDate>Wed, 01 Oct 2025 13:51:03 +0000</pubDate></item></channel></rss>