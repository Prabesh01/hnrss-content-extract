<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 16 Dec 2025 19:12:36 +0000</lastBuildDate><item><title>ArkhamMirror: Airgapped investigation platform with CIA-style hypothesis testing</title><link>https://github.com/mantisfury/ArkhamMirror</link><description>&lt;doc fingerprint="7fae250844a8d062"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;Connect the dots without connecting to the cloud.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;ArkhamMirror is an air-gapped, AI-powered investigation platform for journalists and researchers. It runs 100% locally on your machine, turning chaos into order using advanced NLP, Vision AI, and Knowledge Graphs.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Feature&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;üïµÔ∏è Local AI&lt;/cell&gt;
        &lt;cell&gt;Chat with your data using Offline RAG (Retrieval-Augmented Generation).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;üîç Semantic Search&lt;/cell&gt;
        &lt;cell&gt;Find documents by concept, not just exact keywords.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;üï∏Ô∏è Knowledge Graph&lt;/cell&gt;
        &lt;cell&gt;Visualize hidden connections between People, Orgs, and Places.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;‚è≥ Auto-Timeline&lt;/cell&gt;
        &lt;cell&gt;Extract dates and events to reconstruct what happened when.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;üìä Visual Table Extraction&lt;/cell&gt;
        &lt;cell&gt;Recover complex financial tables from PDFs/Images using Vision models.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Automatically flag conflicting statements across documents.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;üîí Absolute Privacy&lt;/cell&gt;
        &lt;cell&gt;Zero cloud dependencies. Your data never leaves your specialized "Data Silo".&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;ArkhamMirror includes a Smart Installer that sets up Python, Docker, and Database dependencies for you.&lt;/p&gt;
    &lt;p&gt;Double-click &lt;code&gt;setup.bat&lt;/code&gt; and follow the AI Setup Wizard.&lt;/p&gt;
    &lt;code&gt;chmod +x setup.sh
./setup.sh&lt;/code&gt;
    &lt;p&gt;Detailed guides for features and workflows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;User Guide: Full walkthrough of features.&lt;/item&gt;
      &lt;item&gt;Installation: Detailed setup instructions.&lt;/item&gt;
      &lt;item&gt;Developer Guide: Architecture and contributing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Narrative Reconstruction&lt;/cell&gt;
        &lt;cell role="head"&gt;Gap Finding&lt;/cell&gt;
        &lt;cell role="head"&gt;Contradiction Chain&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Entity Graph&lt;/cell&gt;
        &lt;cell role="head"&gt;Author Unmasking&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This tool was born from a desire to give journalists powerful forensics without the monthly subscription costs or privacy risks of cloud platforms.&lt;/p&gt;
    &lt;p&gt;If it helps you uncover the truth, consider buying me a coffee!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46286666</guid><pubDate>Tue, 16 Dec 2025 09:51:31 +0000</pubDate></item><item><title>Cekura (YC F24) Is Hiring</title><link>https://www.ycombinator.com/companies/cekura-ai/jobs/YFeQADI-product-engineer-us</link><description>&lt;doc fingerprint="ddc9e9532991a347"&gt;
  &lt;main&gt;
    &lt;p&gt;Voice AI and Chat AI agents: Testing and Observability&lt;/p&gt;
    &lt;p&gt;Cekura (YC F24) is one of the fastest-growing companies in its batch, with strong revenue traction. We‚Äôre well-funded, backed by premier investors, and have years of runway.&lt;/p&gt;
    &lt;p&gt;We‚Äôre building the reliability layer for Conversational Agents. Teams use Cekura to simulate and monitor their AI agents end-to-end - measuring latency, barge-in, instruction-following, regressions, and more across phone, chat, SMS, and web. Customers love the product - and we‚Äôre just getting started.&lt;/p&gt;
    &lt;p&gt;You‚Äôre joining at an inflection point. As Product Engineer, you‚Äôll build the playbooks, processes, and relationships that define how Cekura partners with technical customers for long-term success. You‚Äôll be both strategist and hands-on operator.&lt;/p&gt;
    &lt;p&gt;Excited to help world-class teams ship reliable AI agents - and wear both the customer and engineer hats? Let‚Äôs talk.&lt;/p&gt;
    &lt;p&gt;Cekura is a Y Combinator‚Äìbacked startup redefining AI voice agent reliability. Founded by IIT Bombay alumni with research credentials from ETH Zurich and proven success in high-stakes trading, our team built Cekura to solve the cumbersome, error-prone nature of manual voice agent testing.&lt;/p&gt;
    &lt;p&gt;We automate the testing and observability of AI voice agents by simulating thousands of realistic, real-world conversational scenarios‚Äîfrom ordering food and booking appointments to conducting interviews. Our platform leverages custom and AI-generated datasets, detailed workflows, and dynamic persona simulations to uncover edge cases and deliver actionable insights. Real-time monitoring, comprehensive logs, and instant alerting ensure that every call is optimized and production-ready.&lt;/p&gt;
    &lt;p&gt;In a market rapidly expanding with thousands of voice agents, Cekura stands out by guaranteeing dependable performance, reducing time-to-market, and minimizing costly production errors. We empower teams to demonstrate reliability before deployment, making it easier to build trust with clients and users.&lt;/p&gt;
    &lt;p&gt;Join us in shaping the future of voice technology. Learn more at cekura.ai.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46287521</guid><pubDate>Tue, 16 Dec 2025 12:01:55 +0000</pubDate></item><item><title>Sega Channel: VGHF Recovers over 100 Sega Channel ROMs (and More)</title><link>https://gamehistory.org/segachannel/</link><description>&lt;doc fingerprint="2f3cfb35e3e384c9"&gt;
  &lt;main&gt;
    &lt;p&gt;Sega broke ground in the late 90s with one of the first digital game distribution systems for consoles. Sega Channel offered access to a rotating library of Sega Genesis titles, along with game tips, demos, and even a few exclusive games that never came out in the United States in any other format. In an era of dial-up internet, Sega Channel delivered game data over television cable ‚Äî a novel approach that gave the service its name.&lt;/p&gt;
    &lt;p&gt;In the years since, Sega Channel has been shrouded in a bit of mystery. The service was discontinued in 1998, and the lack of retrievable game data and documentation around Sega Channel has led to decades of speculation about it. We‚Äôve mostly been left with magazine articles and second-hand accounts. Once in a while, one or two Sega Channel ROMs will show up online. How do you preserve a service like Sega Channel?&lt;/p&gt;
    &lt;p&gt;For the last two years, we‚Äôve been working on a large-scale project to preserve the history of Sega Channel. Today, we unveiled our findings in a new YouTube video.&lt;/p&gt;
    &lt;p&gt;We‚Äôll cut to the chase: In collaboration with multiple parties, we have recovered over 100 new Sega Channel ROMs, including system data, exclusive games, and even prototypes that were never published. We‚Äôve also digitized internal paperwork and correspondence that reveals how Sega Channel operated, how it was marketed, and what would‚Äôve come next for the service.&lt;/p&gt;
    &lt;p&gt;This project kicked off in 2024, when we met former Sega Channel vice president of programming Michael Shorrock at the Game Developers Expo. Our booth that year highlighted interesting games from outside the traditional game industry, including Where in North Dakota is Carmen Sandiego?, which our director Frank Cifaldi recovered back in 2016.&lt;/p&gt;
    &lt;p&gt;By complete coincidence, one of the items we put out was a promotional brochure for Broderbund Software‚Ä¶ featuring Michael Shorrock on the cover! We got talking with Michael about our work, and we realized we both wanted to preserve and celebrate the history of Sega Channel.&lt;/p&gt;
    &lt;p&gt;At the same time this was happening, we were contacted by a community member named Ray (going by the pseudonym Sega Channel Guy). He had been contacting former Sega Channel staff to see if they still had any old swag or had saved things from the company. In the process, he came into possession of a collection of tape backups containing an unquantifiable amount of internal data from Sega Channel‚Ä¶ including a significant number of game and system ROMs.&lt;/p&gt;
    &lt;p&gt;We realized we could put these two threads together! With Michael‚Äôs own collection and Ray‚Äôs data backups, we could tell a cohesive, wide-ranging story about what Sega Channel was and that was actually distributed through this service.&lt;/p&gt;
    &lt;p&gt;There are two end products from this process. The first is the Michael Shorrock collection, a new collection in our digital library. You can view the correspondence, notes, and presentations from Michael Shorrock‚Äôs personal collection, which shed light on the formation of Sega Channel and their audience. From these papers, you can also learn about Express Games: an unannounced successor that would have brought Sega‚Äôs cable data delivery service to computers and replaced Sega Channel entirely.&lt;/p&gt;
    &lt;p&gt;The other output here is the collection of Sega Channel ROM data. We‚Äôve donated the data from the 144* new ROMs we recovered to the team at Gaming Alexandria, which will be sharing access to the files.&lt;/p&gt;
    &lt;p&gt;* Our video states that we recovered 142 unique ROMs. However, after uploading the video, we realized we miscounted! There are two additional Sega Channel variant ROM in this collection. The actual total is 144. This does not include the two outliers mentioned in the video, which were previously recovered by users on Sonic Retro in November 2024 but went mostly unreported.&lt;/p&gt;
    &lt;p&gt;This collection includes nearly 100 unique system ROMs, covering almost every version of the system that was distributed to consumers from 1994 to mid-1997. This batch also includes system ROM prototypes and some truly unusual experiments, like a Sega Genesis web browser that would‚Äôve delivered compressed, static websites over television cable.&lt;/p&gt;
    &lt;p&gt;Of great interest to fans, this collection of ROMs also has dozens of previously undumped game variants and Sega Channel exclusives. This includes Garfield: Caught in the Act ‚Äì The Lost Levels and The Flintstones, two games that were previously believed to be permanently lost and unrecoverable. These are both interesting from a development standpoint; both games appear to have their roots as abandoned projects that were repurposed as Sega Channel-exclusive content.&lt;/p&gt;
    &lt;p&gt;Also included are the previously unpreserved limited editions of Sega Genesis games. These versions have been cut down to fit within Sega Channel‚Äôs filesize limit, sometimes omitting content or splitting the game into multiple parts. We‚Äôre not sure anyone is especially eager to play a version of Super Street Fighter II missing half the characters, but we‚Äôre glad to have it documented.&lt;/p&gt;
    &lt;p&gt;With a few exceptions, this recovery project has accounted for almost all outstanding Sega Channel games. We believe this also means there are now digital backup copies of every unique Sega Genesis game released in the United States.&lt;/p&gt;
    &lt;p&gt;This has been a years-long project that wouldn‚Äôt have been possible without support from the broader gaming community. Besides Michael Shorrock and Ray, we want to give special thanks to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sega Retro, The Cutting Room Floor, and Hidden Palace for documenting everything we‚Äôve known about Sega Channel up to this point.&lt;/item&gt;
      &lt;item&gt;RisingFromRuins and Nathan Misner (infochunk) for putting all the pieces together to crack the Sega Channel data formats.&lt;/item&gt;
      &lt;item&gt;Dustin Hubbard (Hubz) from Gaming Alexandria for working with us to share this ROM data.&lt;/item&gt;
      &lt;item&gt;Rob Curl from the Museum of Art and Digital Entertainment, who flagged us down at GDC to let us know that Michael Shorrock had seen a picture of himself at our booth and brought him over to say hello.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We also want to give a special thanks to Chuck Guzis, a long-time expert on data tapes, who digitized Ray‚Äôs Sega Channel backups for us in 2024. Chuck‚Äôs business Sydex was, for a long time, the go-to vendor for working with data tapes, and we‚Äôve used his services in the past.&lt;/p&gt;
    &lt;p&gt;Shortly before launching this project, we learned that Chuck passed away over the summer. His death leaves a hole in our community and our collective expertise. We know that the gaming community (and specifically the Sega community) will be excited by all this new documentation and data; we hope that their excitement is a testament to what Chuck‚Äôs work meant to the digital preservation community.&lt;/p&gt;
    &lt;head rend="h3"&gt;Complete list of recovered titles&lt;/head&gt;
    &lt;p&gt;This is a list of all Sega Channel-specific game data recovered from this project and shared with Gaming Alexandria. This does not include the 97 unique pieces of menu data ROMs and system software that were also recovered.&lt;/p&gt;
    &lt;head&gt;Game list&lt;/head&gt;
    &lt;p&gt;Unique Sega Channel exclusive games:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Berenstain Bears‚Äô A School Day&lt;/item&gt;
      &lt;item&gt;BreakThru&lt;/item&gt;
      &lt;item&gt;The Flintstones&lt;/item&gt;
      &lt;item&gt;Garfield: Caught in the Act ‚Äì The Lost Levels&lt;/item&gt;
      &lt;item&gt;Iron Hammer&lt;/item&gt;
      &lt;item&gt;Waterworld&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Sega Channel variants:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Adventures of Batman and Robin, Test Drive version&lt;/item&gt;
      &lt;item&gt;Comix Zone, Test Drive version (1)&lt;/item&gt;
      &lt;item&gt;Comix Zone, Test Drive version (2)&lt;/item&gt;
      &lt;item&gt;Earthworm Jim, Test Drive version&lt;/item&gt;
      &lt;item&gt;Earthworm Jim VideoHints (1)&lt;/item&gt;
      &lt;item&gt;Earthworm Jim VideoHints (2)&lt;/item&gt;
      &lt;item&gt;The Great Earthworm Jim Race&lt;/item&gt;
      &lt;item&gt;The Lost World: Jurassic Park, Part A&lt;/item&gt;
      &lt;item&gt;The Lost World: Jurassic Park, Part B&lt;/item&gt;
      &lt;item&gt;The Lost World: Jurassic Park, Test Drive version&lt;/item&gt;
      &lt;item&gt;NCAA Final Four Basketball: Special Edition (1)&lt;/item&gt;
      &lt;item&gt;NCAA Final Four Basketball: Special Edition (2)&lt;/item&gt;
      &lt;item&gt;Mortal Kombat 3, Part A&lt;/item&gt;
      &lt;item&gt;Mortal Kombat 3, Part B&lt;/item&gt;
      &lt;item&gt;Scholastic‚Äôs The Magic School Bus: Space Exploration Game, Test Drive version&lt;/item&gt;
      &lt;item&gt;Sonic 3D Blast, Part A&lt;/item&gt;
      &lt;item&gt;Sonic 3D Blast, Part B&lt;/item&gt;
      &lt;item&gt;Super Street Fighter II: Limited Edition&lt;/item&gt;
      &lt;item&gt;Triple Play Baseball 96: Special Edition&lt;/item&gt;
      &lt;item&gt;Virtua Fighter 2, Part A&lt;/item&gt;
      &lt;item&gt;Virtua Fighter 2, Part B&lt;/item&gt;
      &lt;item&gt;World Series Baseball ‚Äô96: Limited Edition*&lt;/item&gt;
      &lt;item&gt;X-Men 2: Clone Wars, Test Drive version&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Prototypes received by Sega Channel:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Al Unser Jr.‚Äôs Road to the Top&lt;/item&gt;
      &lt;item&gt;Dan Marino Football&lt;/item&gt;
      &lt;item&gt;Light Crusader&lt;/item&gt;
      &lt;item&gt;Nick Faldo‚Äôs Championship Golf&lt;/item&gt;
      &lt;item&gt;Popeye in High Seas High-Jinks&lt;/item&gt;
      &lt;item&gt;Shadows of the Wind&lt;/item&gt;
      &lt;item&gt;WildSnake&lt;/item&gt;
      &lt;item&gt;Wrath of the Demon&lt;/item&gt;
      &lt;item&gt;Yogi Bear [Yogi Bear‚Äôs Cartoon Capers]&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Data differences:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Body Count (US revision)&lt;/item&gt;
      &lt;item&gt;Maui Mallard in Cold Shadow&lt;/item&gt;
      &lt;item&gt;Primal Rage&lt;/item&gt;
      &lt;item&gt;Pulseman&lt;/item&gt;
      &lt;item&gt;Richard Scarry‚Äôs Busytown*&lt;/item&gt;
      &lt;item&gt;Shining Force II&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Header differences only:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Battle Frenzy (US header)&lt;/item&gt;
      &lt;item&gt;Power Drive (US header)&lt;/item&gt;
      &lt;item&gt;QuackShot&lt;/item&gt;
      &lt;item&gt;Super Hang-On&lt;/item&gt;
      &lt;item&gt;Wacky Worlds Creativity Studio&lt;/item&gt;
      &lt;item&gt;X-Men 2: Clone Wars&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;* These games were previously found on a CD obtained by a user on the Sonic Retro forums in November 2024. However, these ROMs were overshadowed by the recovery of the Sega Channel exclusive games The Chessmaster and Klondike from the same CD. Although our copies of these ROMs are not unique, we included them on this list to make sure their existence doesn‚Äôt get lost.&lt;/p&gt;
    &lt;head rend="h3"&gt;A footnote for hardcore Sega fans&lt;/head&gt;
    &lt;p&gt;We believe this recovery project accounts for all unique Sega Channel exclusive games. But the most hardcore fans might be wondering: What about Ozone Kid? In a feature article on Sega Channel from the June 1995 issue of Electronic Gaming Monthly (p.29), Ozone Kid was identified as the first Sega Channel exclusive.&lt;/p&gt;
    &lt;p&gt;We can confirm that this game was never actually distributed through Sega Channel. According to data recovered by Ray, The Environmental Detective (as it was titled prior to cancellation) was slated for release alongside the Sega Channel test markets, but it was pulled from their programming plans in July 1994.&lt;/p&gt;
    &lt;p&gt;Reading the between the lines in Sega Channel‚Äôs internal project tracking, the game appears to have suffered from a variety of problems over several months. When the game was finally shelved, Sega issued a ‚Äúpartial test report based on items found at the time code was pulled,‚Äù suggesting there were still major issues when it was removed from their plans.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46288024</guid><pubDate>Tue, 16 Dec 2025 13:07:14 +0000</pubDate></item><item><title>Put a ring on it: a lock-free MPMC ring buffer</title><link>https://h4x0r.org/ring/</link><description>&lt;doc fingerprint="7e7090557fa01e09"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Put a ring on it: a lock-free MPMC ring buffer&lt;/head&gt;
    &lt;p&gt;One of the reasons few security products work well in busy Linux environments is that they amplify performance risk. You‚Äôre popular and your backend‚Äôs load is skyrocketing? Well, the typical product is just going to collect more data and do more analysis, which amplifies the degradation.&lt;/p&gt;
    &lt;p&gt;In the real world, one of the key ways everyone deals with being overloaded is by dropping less essential things.&lt;/p&gt;
    &lt;p&gt;We can do the same thing with ring buffers, which are fixed-size queues that typically drop old data once they fill up. Yet, they rarely get used outside of single-reader, single-writer scenarios, because it‚Äôs hard to build something correct that scales to 1-to-many scenarios, never mind many-to-many scenarios.&lt;/p&gt;
    &lt;p&gt;But, what if we told you, you can have a scalable ring buffer that doesn‚Äôt need any locking, and works with multiple readers and multiple writers at the same time? You might say, ‚Äúthere‚Äôs no such thing‚Äù, except that now there is.&lt;/p&gt;
    &lt;head rend="h1"&gt;Wait, that rings a bell üîî&lt;/head&gt;
    &lt;p&gt;Ring buffers are fixed-size first-in-first-out (FIFO) queues. Fixed size queues that separately track the front and back are a category of algorithm called the circular buffer.&lt;/p&gt;
    &lt;p&gt;Some people treat the term circular buffer and ring buffer the same. For me, a ring buffer is a type of circular buffer, but one that explicitly drops data when the queue fills up.&lt;/p&gt;
    &lt;p&gt;That‚Äôs not the only option for a circular buffer. For instance:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;You can just block until space becomes available.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You can grow the backing store.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You can just do nothing, except signal an insertion error and let the programmer figure it out.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Ring buffers are more resiliant, since they proactively drop when needed. Typically, it‚Äôs the oldest data gets dropped. However, there are ring buffers out there that give the option to drop new data instead.&lt;/p&gt;
    &lt;p&gt;Sure, for some situations, dropping data is the wrong call. But for many situations, especially ones requiring performance, lossy is still much better than bringing the system to a halt due to too much data.&lt;/p&gt;
    &lt;p&gt;For instance, in the Linux kernel, ring buffers are used in many places. A well known example is for relaying events from the kernel to the userland handler, when &lt;code&gt;ebpf&lt;/code&gt; probes are attached.&lt;/p&gt;
    &lt;p&gt;When workloads are having performance issues, probes often end up with more work to do, and if there‚Äôs not some form of backpressure, things will end badly. And since &lt;code&gt;ebpf&lt;/code&gt; probes are intended for observability, and since observability is generally less important than availability, dropping data as a first line of defense is a good idea.&lt;/p&gt;
    &lt;p&gt;And because the kernel‚Äôs ring buffer allows you to choose whether to drop off the front or the back, &lt;code&gt;ebpf&lt;/code&gt; users get that choose too. Still, dropping older data is generally more common.&lt;/p&gt;
    &lt;p&gt;When operating on any kind of queue, lock-contention can slow things down significantly, because there are two bottlenecks‚Äì the head pointer (for enqueuers) or the tail pointer (for dequeuers). But rings have life even worse, because the head pointer can circle around the ring, and meet up with the tail pointer.&lt;/p&gt;
    &lt;p&gt;Our requirements for a ring buffer:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;An ordered set &lt;code&gt;S&lt;/code&gt;, with a maximum size&lt;code&gt;n&lt;/code&gt;, with items of a fixed length&lt;code&gt;l&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;An operation, &lt;code&gt;enqueue(item)&lt;/code&gt;, where&lt;code&gt;item&lt;/code&gt;is an arbitrary item of length&lt;code&gt;l&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;An operation, &lt;code&gt;dequeue()&lt;/code&gt;, which returns the next item of length&lt;code&gt;l,&lt;/code&gt;or indicates that the buffer is empty.&lt;/item&gt;
      &lt;item&gt;No thread should be able to detect an inconsistent ordering of operations on the ring, under any circumstances (We‚Äôll cover memory ordering after we build our ring buffer).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Trying to minimize the impact of bottlenecks is hard, and so people tend to make compromises somewhere. For instance, ring buffers will try to avoid locks, but will accept some constraints, for instance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;single producer, single consumer (SPSC)&lt;/item&gt;
      &lt;item&gt;single producer, multiple consumer (SPMC)&lt;/item&gt;
      &lt;item&gt;multiple-producer, single consumer (MPSC)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here, ‚Äúproducer‚Äù means ‚Äúenqueuer‚Äù, and ‚Äúconsumer‚Äù means ‚Äúdequeuer‚Äù. At some point, I wanted a true, lock-free multiple-producer, multiple-consumer (MPMC) ring buffer, but there was nothing out there that would scale, so I came up with an algorithm.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;‚õìÔ∏èüí•&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;When we say lock free, we mean that, for any given thread performing an operation, no other thread can cause a thread to suspend. With respect to an algorithm only, this is often referred to as a system-wide progress guarantee.&lt;/p&gt;
          &lt;p&gt;That doesn't mean any given thread will make 'progress' in a comfortable amount of time; lock-free algorithms usually perform operations that fail, and need to keep trying them until successful. They could conceptually lose their race till the end of eternity.&lt;/p&gt;
          &lt;p&gt;To get per-thread progress, we need wait freedom, which is often achievable with exponential backoff. But, when OS scheduling tends to be fair, lock free algorithms essentially can expect not to contend forever, and the extra overhead of wait freedom is often not worth it.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;At some point after I‚Äôd come up with my algorithm, I did stumble across a vein of literature, calling it a ‚Äúring buffer‚Äù where old data couldn‚Äôt be dropped. The user was left to resubit. To my mind, that‚Äôs a fixed-size circular FIFO, but not a ring buffer.&lt;/p&gt;
    &lt;p&gt;Today, we‚Äôll build a true MPMC ring buffer. We‚Äôll focus on dropping old data, but this is one place where extending it yourself would be really pretty simple.&lt;/p&gt;
    &lt;p&gt;The full code is available at codeberg.&lt;/p&gt;
    &lt;head rend="h1"&gt;Ordering that ring üëâ‚òéÔ∏è&lt;/head&gt;
    &lt;p&gt;The last of our above requirements for a ring is often referred to as linearization. All operations map to a point on a conceptual timeline, where no thread can ‚Äòsee‚Äô operations in an order that would be different from that timeline.&lt;/p&gt;
    &lt;p&gt;For instance, if thread &lt;code&gt;A&lt;/code&gt; enqueued &lt;code&gt;I1&lt;/code&gt; then &lt;code&gt;I2&lt;/code&gt;, and thread &lt;code&gt;B&lt;/code&gt; is the one to dequeue both, it must always dequeue in the expected order‚Äì &lt;code&gt;I1&lt;/code&gt; first, then &lt;code&gt;I2&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Every insertion thus needs to have a well-defined ordering, as does every removal. But it doesn‚Äôt have to map to wall time.&lt;/p&gt;
    &lt;p&gt;For instance, let‚Äôs say threads &lt;code&gt;B&lt;/code&gt; and &lt;code&gt;C&lt;/code&gt; are both dequeuers. &lt;code&gt;B&lt;/code&gt; shows up first and starts dequeueing &lt;code&gt;I1&lt;/code&gt;, but the scheduler suspends it in the middle of the operation. Meanwhile, &lt;code&gt;C&lt;/code&gt; comes in and quickly pulls out &lt;code&gt;I2&lt;/code&gt; before &lt;code&gt;B&lt;/code&gt; wakes, and returns its value.&lt;/p&gt;
    &lt;p&gt;While &lt;code&gt;B&lt;/code&gt; might return after &lt;code&gt;C&lt;/code&gt; in wall-clock time, that shouldn‚Äôt worry us, as long as there‚Äôs a well-defined ordering. That well defined ordering requires a well-defined linearization point. By that, we mean an atomic operation that is considered the point where the overall operation ‚Äòoccurs‚Äô or ‚Äòis committed‚Äô.&lt;/p&gt;
    &lt;p&gt;So if &lt;code&gt;B&lt;/code&gt; hasn‚Äôt returned, but is suspended after the linearization point, it‚Äôs no big deal. The algorithm already considers the item dequeued.&lt;/p&gt;
    &lt;p&gt;Similarly, if &lt;code&gt;B&lt;/code&gt; starts its operation before &lt;code&gt;C&lt;/code&gt; does, but is suspended BEFORE the linearization point, and C is never suspended, &lt;code&gt;C&lt;/code&gt; can absolutely claim &lt;code&gt;I1&lt;/code&gt;; when &lt;code&gt;B&lt;/code&gt; wakes up, it cannot get &lt;code&gt;I1&lt;/code&gt;, and nobody has an inconsistent view of the world.&lt;/p&gt;
    &lt;p&gt;That works because all threads get equal treatment‚Äì it doesn‚Äôt matter when the functions they call start or end; the ordering is based on when the operation at the linearization point occurs.&lt;/p&gt;
    &lt;p&gt;We‚Äôll make sure to clearly identify our linearization points for each operation, which will always be on atomic operations.&lt;/p&gt;
    &lt;head rend="h1"&gt;üë∞‚ôÇÔ∏è Our word is our vow ü§µ‚ôÄÔ∏è&lt;/head&gt;
    &lt;p&gt;We‚Äôre going to start with modest expectations and build a ring that operates on values that are exactly one word long. We‚Äôre going to go ahead and assume you‚Äôre on modern hardware, with a 64-bit word.&lt;/p&gt;
    &lt;p&gt;When designing our algorithm, in order to ensure correctness, we will want to think through all the places where there‚Äôs contention, meaning, we need to cover all cases where threads might try to operate on the same memory at the same time.&lt;/p&gt;
    &lt;p&gt;The following scenarios are all very realistic things for us to consider:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multiple enqueuers can be performing enqueue operations in parallel, and thus compete with each other to get onto the list ‚Äòfirst‚Äô.&lt;/item&gt;
      &lt;item&gt;If one enqueuer gets suspended, other enqueuers may wrap around the queue before its operation completes.&lt;/item&gt;
      &lt;item&gt;Multiple dequeuers can also run in parallel, and fight over which item they get.&lt;/item&gt;
      &lt;item&gt;Dequeuers can drain a list to the point that they‚Äôve caught up to writers, and might be reading their state before an operation completes.&lt;/item&gt;
      &lt;item&gt;Or, dequeuers could lag way behind, trying to dequeue from a slot in the ring that a writer is now trying to use for a much more recent value.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To be able to arbitrate disputes around this kind of contention, we‚Äôre going to be explicit about our linearization timeline. Items in the array will be associated with an epoch, which is simply a point of time on our timeline. But every enqueue operation will be tied to an epoch.&lt;/p&gt;
    &lt;p&gt;We will make sure that each enqueue operation is associated with a unique epoch (though it is perfectly fine if we find ourselves needing to skip epochs).&lt;/p&gt;
    &lt;p&gt;When our dequeuers go to dequeue, they will also be looking to own the dequeue of an item that‚Äôs associated with a particular epoch.&lt;/p&gt;
    &lt;p&gt;When a thread examines a slot in the ring, it will need to know what epoch is associated with a cell, and whether there‚Äôs an enqueued item in that cell or not. We‚Äôll want to make sure all that information is definitely tied to a specific value, and that the whole kit-and-kaboodle needs to always be read from (and written to) atomically.&lt;/p&gt;
    &lt;p&gt;When a thread wants to update the same state from a cell in the ring, we need to atomically read the existing state, create a copy that has the state we want, and then try to replace the state inside the cell atomically.&lt;/p&gt;
    &lt;p&gt;However, if, when we go to update the state, it‚Äôs changed from what we expected (based on the copy we made), then we need the operation to FAIL, and we should start the update process over again, based on those changes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Let‚Äôs Swap&lt;/head&gt;
    &lt;p&gt;Thankfully, there‚Äôs a universally available atomic operation that does all of those things, often referred to as a compare-and-swap operation (CAS). The &lt;code&gt;C&lt;/code&gt; language standard provides an API call to do this, although they use the word exchange instead of swap (also a common name for this operation).&lt;/p&gt;
    &lt;p&gt;Any CAS operation we perform on a cell will be a linearization point for us.&lt;/p&gt;
    &lt;p&gt;Most hardware platforms can do a compare-and-swap atomically, but not for any arbitrary size. Modern hardware usually limits us to 128 bits that we can atomically operate on. Other atomic operations may limit us to just 64-bit operands.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;üíª&lt;/cell&gt;
        &lt;cell&gt;The x86 family has long supported a 128-bit compare-and-swap, but until recently, it required instruction-level locking to use, because it did not support atomically loading 128 bits into a register otherwise. So on old hardware, you're technically using a lock with a 128-bit CAS, but ü§∑‚ôÇÔ∏è.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The CAS operation conceptually takes three operands (it can differ, as we‚Äôll see later):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A pointer to the bytes we want to swap (i.e., the object to swap)&lt;/item&gt;
      &lt;item&gt;A pointer to the value we‚Äôre expecting to be in that memory before the operation begins (i.e., the expected value)&lt;/item&gt;
      &lt;item&gt;The actual value we want to leave behind (i.e., the desired value).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The processor will check that the value in the 2nd field is right; if it is, the memory address pointed to in parameter 1 gets the value you passed into parameter 3, and the OLD value gets written into the memory address pointed to by parameter 2, overwriting the expected field.&lt;/p&gt;
    &lt;p&gt;In this case, the operation returns &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If the operation fails because the memory has changed since your last load:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Your desired value does not get installed.&lt;/item&gt;
      &lt;item&gt;The memory holding the expected value is updated to contain what the actual value was that differed from the expected value.&lt;/item&gt;
      &lt;item&gt;The function returns &lt;code&gt;false&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On some platforms, there can be two variations of this operation, the strong CAS and the weak CAS. The weak CAS actually is allowed to return &lt;code&gt;false&lt;/code&gt; and skip the swap, even if the expected value does match the object to swap. Generally, that operation will often get it right, but you might occasionally end up re-trying where you shouldn‚Äôt have needed to retry.&lt;/p&gt;
    &lt;p&gt;Why the heck would anyone want that behavior? It turns out, some platforms can make this weaker version faster. Even with the potential for failures, if you are using a CAS in a loop, until it succeeds, this weaker version is what people will recommend.&lt;/p&gt;
    &lt;p&gt;However, if you have a use for a CAS operation that doesn‚Äôt require testing for success after, using the weaker version would require testing. If you have to add a loop where there wouldn‚Äôt have been one otherwise, then you definitely want to use the strong variant.&lt;/p&gt;
    &lt;p&gt;But, while that‚Äôs the guidance you‚Äôll find all over the internet, I don‚Äôt actually know which CPUs this would affect. Maybe it‚Äôs old news, I dunno. But it does still seem to make a shade of difference in real-world tests, so ü§∑.&lt;/p&gt;
    &lt;head rend="h2"&gt;Picking your venue ‚õ™Ô∏è&lt;/head&gt;
    &lt;p&gt;How do threads decide where to operate inside the ring buffer, with a minimum of fighting?&lt;/p&gt;
    &lt;p&gt;Imagine our ring is a large butcher shop with multiple counters. We take a number, and then go to the counter associated with that number.&lt;/p&gt;
    &lt;p&gt;Our ring will give out tickets to epochs, giving out each number no more than two times‚Äì once to an enqueuer, and once to a dequeuer. To do that, we‚Äôll need two counters that we can atomically update.&lt;/p&gt;
    &lt;p&gt;If we need to implement giving out tickets like this, we can use an atomic CAS operation to do it, sitting in a loop until our new value is swapped in. Each time we lose, we‚Äôll have to recompute the next value, but that‚Äôll certainly work.&lt;/p&gt;
    &lt;p&gt;However, we can avoid the loop altogether if we do it through an atomic fetch-and-add (FAA) operation. When implemented in hardware, FAA is guaranteed to be completed. You pass in two operands:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A pointer to the object containing the current value. The result will be stored here at the end of the operation.&lt;/item&gt;
      &lt;item&gt;The value you‚Äôd like to add.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At the end of the operation, the old value gets returned to the caller.&lt;/p&gt;
    &lt;p&gt;So, to implement a ticketing system, we can have a variable that holds the value of the next ticket to give out. Each thread gets a ticket by calling FAA, adding the number &lt;code&gt;1&lt;/code&gt; to the ticket value, but returning the number that was there at the start, which becomes our ticket.&lt;/p&gt;
    &lt;p&gt;No two enqueuers will get the same ticket. We can then map each ticket to a cell in the ring (easiest done by limiting the number of cells in the ring to powers of two). We take the ticket, divide by the number of cells, and the remainder is the cell index associated with that ticket (i.e., we compute the modulus).&lt;/p&gt;
    &lt;p&gt;When we have a number of cells that‚Äôs a power of two, we can use a cheap binary AND operation to get the modulus.&lt;/p&gt;
    &lt;p&gt;That trick only works for powers of two, because of some specific properties:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Every power of two is represented with a single bit set to 1; all other bits are zero.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;If you subtract&lt;/p&gt;&lt;code&gt;1&lt;/code&gt;from a power of two, all bits to the RIGHT of that one bit set for the power of two will be set, and nothing else.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So we get a cheap modulo via a binary AND operation. To be fair, it could be the case that modern processors can compute the modulus just as quickly as a bitwise AND, even though it‚Äôs a much more complicated operation. But, that‚Äôs why many data structures are backed by arrays of size to powers of two. It‚Äôs such a common paradigm, we‚Äôll just roll with it.&lt;/p&gt;
    &lt;p&gt;Now, we also need a second ticket for dequeuers; those tickets should be associated with values that have already been enqueued.&lt;/p&gt;
    &lt;p&gt;However, if we keep those two tickets separate, then it will be really hard for us to build any confidence that we‚Äôre detecting some of the contention scenarios we talked about above.&lt;/p&gt;
    &lt;p&gt;For example, if we read each counter separately, how do we know if the queue is empty? Or, how can we be certain that readers are way behind (we don‚Äôt want to waste a lot of time with readers grabbing tickets for values we already dropped).&lt;/p&gt;
    &lt;p&gt;The answer for us is to operate on those tickets (epochs) in one atomic operation.&lt;/p&gt;
    &lt;p&gt;We can still do that with an FAA. For example, if we define our epoch-of-record datatype like this:&lt;/p&gt;
    &lt;code&gt;typedef struct {
    uint32_t epoch_q; // The next epoch assoc. w/ an enqueue op.
    uint32_t epoch_d; // The next epoch assoc. w/ an enqueue op.
} h4x0r_word_ring_epochs_t;
&lt;/code&gt;
    &lt;p&gt;The compiler is smart enough that it will let you perform operations on structs as if they were integers, as long as the sizes are acceptable, and as long as you are explicit enough with your use of types to convince the compiler you know what you‚Äôre doing.&lt;/p&gt;
    &lt;p&gt;One way to do this is to use a &lt;code&gt;union&lt;/code&gt;, which we might declare like this:&lt;/p&gt;
    &lt;code&gt;typedef union {
    h4x0r_word_ring_epochs_t epochs;
    uint64_t                 integer;
} h4x0r_converter_t;
&lt;/code&gt;
    &lt;p&gt;The fields in unions may be individually addressed, but they share the same memory. The bits won‚Äôt change, but we can get the compiler to recognize the change-of-type based on the field we end up accessing.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs say we want to add 1 to the queue epoch. We don‚Äôt need to understand endianness or how the two fields in that struct are ordered. Here‚Äôs one way we could do that:&lt;/p&gt;
    &lt;code&gt;    h4c0r_word_ring_epochs_t to_add_struct = {
                                               .epoch_q = 1,
                                               .epoch_d = 0,
                                             };
    union h4x0r_converter_t conv           = {
                                               .epochs = my_epochs,
                                             };
    uint64_t                to_add_num     = conv.integer;
&lt;/code&gt;
    &lt;p&gt;We can then convert it back to a struct. And really, those conversions we expect to be free; it‚Äôs just a mechanism for expressing our intent to the compiler.&lt;/p&gt;
    &lt;p&gt;While 128-bit CAS operations are commonly supported in hardware, FAA (and similar operations) are more likely to have a 64-bit cap on their operands.&lt;/p&gt;
    &lt;p&gt;That‚Äôs why the two epochs in our data structure are kept to 32 bits.&lt;/p&gt;
    &lt;p&gt;However, 32 bits isn‚Äôt all that large a number, and it wouldn‚Äôt take too long for a busy system to overflow a counter. Dealing with that kind of overflow wouldn‚Äôt be fun.&lt;/p&gt;
    &lt;p&gt;If you are confident you‚Äôve got a situation where you won‚Äôt use a ring enough to overflow, then by all means, use 32-bit epochs. But we recommend that, by default, you use 64-bit epochs. You can emulate a 128-bit FAA pretty easily with a CAS operation, as we described above.&lt;/p&gt;
    &lt;p&gt;While a 64-bit FAA should be faster, the CAS probably will be fine (on my machine, it‚Äôs a tiny smidge better, but not enough to crow about).&lt;/p&gt;
    &lt;p&gt;Our full implementation allows you to toggle between the two epoch sizes at compile-time, so you can compare the results if you like.&lt;/p&gt;
    &lt;p&gt;Anyway, atomically updating the epoch state gets you a ticket, and shows you what the next ticket is for the opposite operation, at the time we were handing your ticket.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Simple ring üç©&lt;/head&gt;
    &lt;p&gt;As we said above, the core state of the cells inside a ring must be no more than 128 bits if we want to operate on it without requiring a lock. So we need to be careful about what we try to jam in a cell.&lt;/p&gt;
    &lt;p&gt;When we load a cell, we want to know if we‚Äôre too slow, whatever our operation. At a bare minimum, we‚Äôre going to need:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Whether an item is enqueued in the slot or has been dequeued.&lt;/item&gt;
      &lt;item&gt;Room for that value, which probably needs to be a whole word so we can fit a pointer in (usually 64 bits).&lt;/item&gt;
      &lt;item&gt;The epoch associated with the cell, which is how we‚Äôll know if a writer has been lapped (if the epoch is higher than the epoch for our operation, we got lapped). It‚Äôs also how readers will figure out the writer is slow and hasn‚Äôt finished.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As we said above, 32 bits is not enough space for the epoch if we are building something general-purpose. But, we clearly don‚Äôt have room for 64-bit epochs, so we‚Äôll need to compromise.&lt;/p&gt;
    &lt;p&gt;A boolean generally will take up at least 8 bits, and a 56-bit epoch probably is good enough not to worry about. Though C doesn‚Äôt have 56-bit ints.&lt;/p&gt;
    &lt;p&gt;However, we can instead use C bit slicing, which would even let us get the flag down to a single bit, leaving 63 bits for our epoch:&lt;/p&gt;
    &lt;code&gt;typedef struct {
    _Atomic(void *)item;
    uint64_t enqueued : 1;
    uint64_t epoch    : 63;
} h4x0r_word_ring_item_t;

static_assert(sizeof(h4x0r_word_ring_item_t) == 16,
              "Bitfield implementation doesn't pack as expected");

typedef _Atomic(h4x0r_word_ring_item_t)   h4x0r_word_ring_cell_t;
typedef _Atomic(h4x0r_word_ring_epochs_t) h4x0r_atomic_epochs_t;
&lt;/code&gt;
    &lt;p&gt;The C standard doesn‚Äôt really mandate layout for bit slices, so we check at compile type with the &lt;code&gt;static_assert&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;For values we need to be shared between threads, we need to label them as &lt;code&gt;_Atomic&lt;/code&gt;. Otherwise, the code that the compiler generates will assume none of our variables are shared across threads, and we are bound to have all sorts of nasty race conditions.&lt;/p&gt;
    &lt;p&gt;But, we don‚Äôt tag &lt;code&gt;_Atomic&lt;/code&gt; on &lt;code&gt;h4x0r_word_ring_item_t&lt;/code&gt; or &lt;code&gt;h4x0r_word_ring_epochs_t&lt;/code&gt; directly, because threads will be keeping their own private versions for updating.&lt;/p&gt;
    &lt;p&gt;My version of the top-level ring looks like this:&lt;/p&gt;
    &lt;code&gt;typedef void (*h4x0r_word_ring_drop_handler)(h4x0r_word_ring_t *, void *);

struct h4x0r_word_ring_t {
    h4x0r_word_ring_drop_handler drop_handler;
    uint32_t                     num_cells;
    uint32_t                     last_slot;
    h4x0r_atomic_epochs_t        epochs;
    h4x0r_word_ring_cell_t       cells[];
};
&lt;/code&gt;
    &lt;p&gt;The drop handler is a function pointer, and we‚Äôd expect it to be set when initializing the ring, defaulting to &lt;code&gt;NULL&lt;/code&gt; when we don‚Äôt need to do anything in particular to deal with drops (either because we‚Äôre not using dynamic memory, or because we have something like a garbage collector to clean up if needed).&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;epochs&lt;/code&gt; field is tagged as being atomic, though it‚Äôs hidden behind the typedef.&lt;/p&gt;
    &lt;p&gt;Similarly, the variable-length array of cells is really an array of items we want to be atomic. The fact that cell loading and storing requires using the C atomic API is in there, but hidden behind a &lt;code&gt;typedef&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;However, the &lt;code&gt;num_cells&lt;/code&gt; field and &lt;code&gt;last_slot&lt;/code&gt; field are not tagged &lt;code&gt;_Atomic&lt;/code&gt;. That‚Äôs because these should be set by one thread during initialization, and then never changed. As long as the memory has synced before other threads start to use these fields, we definitely don‚Äôt need them to be treated specially.&lt;/p&gt;
    &lt;p&gt;Usually, if we do initialization in a proper function, the call boundary is going to be a memory barrier that makes sure they‚Äôre sync‚Äôd when other threads start getting a handle on our ring.&lt;/p&gt;
    &lt;p&gt;But, if initialization might be inlined, you should probably flag these things as &lt;code&gt;_Atomic&lt;/code&gt;, but when you access them via the C11 atomic API, ask for &lt;code&gt;memory_order_relaxed&lt;/code&gt;, which essentially means, ‚Äúno atomic op necessary here, so don‚Äôt add instructions to sync here‚Äù.&lt;/p&gt;
    &lt;p&gt;In that case, the &lt;code&gt;_Atomic&lt;/code&gt; modifier will make sure our writes to those fields are seen by subsequent loads, but we don‚Äôt have to slow down those loads once the values are written.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;num_cells&lt;/code&gt; field is always going to be the number of cells in our ring, and 2^32 cells should be plenty, thus 32 bits. But because of C alignment rules, our struct is going to end up with a 32-bit hole somewhere. So, we fill that space with &lt;code&gt;last_slot&lt;/code&gt;, which is always going to be one less than the value of &lt;code&gt;num_cells&lt;/code&gt;, allowing us to perform our fast modulo without first having to subtract one from &lt;code&gt;num_cells&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;The dequeue operation&lt;/head&gt;
    &lt;p&gt;So far, our queue has nothing in it, and we don‚Äôt know how to put anything in it yet.&lt;/p&gt;
    &lt;p&gt;Still, let‚Äôs start with our dequeue operation.&lt;/p&gt;
    &lt;p&gt;Our first order of business is to the epochs, and if the tail and head are in the same place (or if the tail somehow lapped the head), then the queue is empty, and we shouldn‚Äôt even bother taking a ticket, so to speak.&lt;/p&gt;
    &lt;p&gt;But, if we do see items to dequeue, we‚Äôll play the game! We‚Äôll use FAA to get a unique read epoch. However, it‚Äôs 100% possible that other readers beat us, with no writers coming along.&lt;/p&gt;
    &lt;p&gt;That means, we could actually increment the counter past where the next writer is going to write. We will need to make sure that when we deal with writers, we try to solve that problem.&lt;/p&gt;
    &lt;p&gt;The epoch we read in won‚Äôt ever get another reader, but the writer will need to make sure it doesn‚Äôt use the slot we just accidentally burned.&lt;/p&gt;
    &lt;p&gt;Now, we‚Äôll use our epoch to find the associated cell. We‚Äôll load it, and look at the contents to figure out what to do.&lt;/p&gt;
    &lt;p&gt;Once we have loaded the cell, if the epoch is the one we‚Äôre expecting, and there‚Äôs an item in there, then we try to dequeue it by doing a CAS to mark it as dequeued (and generally attempting to set the item to &lt;code&gt;NULL&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;If the dequeue succeeds in the right epoch, we‚Äôre done.&lt;/p&gt;
    &lt;p&gt;If we found that another reader wasn‚Äôt keeping up, and our writer hasn‚Äôt written over it yet, we‚Äôd still try the CAS, and if it succeeds, that epoch is invalidated; the writer will eventually figure out that they cannot complete their write, and needs to try again.&lt;/p&gt;
    &lt;p&gt;But, if we can prove via the epochs that we‚Äôre the only writer at the time of our CAS, then, even though we know someone is trying to queue, we take that successful CAS as our linearization point, and declare that the ring was empty.&lt;/p&gt;
    &lt;p&gt;If we lose the race against a slow writer, no big deal, we start over with the same epoch; the writer probably left us a present.&lt;/p&gt;
    &lt;p&gt;If our CAS operation fails, before we try again, we look at the value of the &lt;code&gt;expected&lt;/code&gt; item, which is the current stored value.&lt;/p&gt;
    &lt;p&gt;If we find the epoch is lower than ours, then we try to invalidate it with a CAS. If we fail, it could be that the slow writer finished, or it could be that we were suspended and are now behind. If it‚Äôs the former, we try again with the same epoch. If it‚Äôs the latter, we‚Äôll know because the epoch is higher than ours, and we need to go get another read epoch (we can‚Äôt return empty unless we know that because of some CAS that we did, there‚Äôs a moment on our timeline where the ring was empty).&lt;/p&gt;
    &lt;p&gt;That‚Äôs already several corner cases we need to worry about. But the most difficult concern here is the last one.&lt;/p&gt;
    &lt;p&gt;Consider when we use the CAS to dequeue, and that CAS operation failed. But the expected value was the epoch we were looking for. We can just go home and call it a day, right?&lt;/p&gt;
    &lt;p&gt;It depends:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;If we are running with a drop-handler, we absolutely cannot, because the writer will have known it was overwriting, and will be calling the drop handler. We don‚Äôt want to risk a double free if there‚Äôs dynamic deallocation.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Otherwise, yes; the writer we were competing with absolutely doesn‚Äôt care.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For the API call to dequeue, some callers are going to need to distinguish between returning a &lt;code&gt;NULL&lt;/code&gt; because the ring is empty, and the case where &lt;code&gt;NULL&lt;/code&gt; was enqueued.&lt;/p&gt;
    &lt;p&gt;The way to deal with this is to have our API call accept a pointer to a boolean. If &lt;code&gt;NULL&lt;/code&gt; is passed, then we ignore the parameter. Otherwise, we‚Äôll store a value in the memory pointed to.&lt;/p&gt;
    &lt;p&gt;The logic around whether to store the pointer is easily lifted out into inline functions:&lt;/p&gt;
    &lt;code&gt;static inline void *
h4x0r_word_ring_empty(bool *found)
{
    if (found) {
        *found = false;
    }
    return NULL;
}

static inline void *
h4x0r_word_ring_found(h4x0r_word_ring_item_t item,
                      uint64_t              *epoch_ptr,
                      bool                  *found)
{
    if (found) {
        *found = true;
    }

    h4x0r_atomic_fence();

    if (epoch_ptr) {
        *epoch_ptr = item.epoch;
    }

    return item.item;
}
&lt;/code&gt;
    &lt;p&gt;Notice that &lt;code&gt;h4x0r_word_ring_found&lt;/code&gt; takes a second pointer parameter‚Äì that‚Äôs for the caller to get the associated epoch. Day-to-day that may not be too useful. However, it‚Äôs very useful for correctness testing, to make sure one thread never sees out-of-order dequeues.&lt;/p&gt;
    &lt;p&gt;We‚Äôll use it when we do our testing.&lt;/p&gt;
    &lt;p&gt;With all that, here‚Äôs the body of our dequeue operation:&lt;/p&gt;
    &lt;code&gt;void *
h4x0r_word_ring_dequeue(h4x0r_word_ring_t *ring, bool *found, uint64_t *ep)
{
    h4x0r_word_ring_cell_t  *cell;
    h4x0r_word_ring_item_t   expected;
    h4x0r_word_ring_item_t   last;
    h4x0r_word_ring_item_t   candidate;
    h4x0r_word_ring_epochs_t epochs = h4x0r_atomic_load(&amp;amp;ring-&amp;gt;epochs);

    while (true) {
        if (epochs.epoch_d &amp;gt;= epochs.epoch_q) {
            return h4x0r_word_ring_empty(found);
        }
        epochs   = h4x0r_epochs_increment(&amp;amp;ring-&amp;gt;epochs, read_incr);
        cell     = h4x0r_word_ring_slot_addr(ring, epochs.epoch_d);
        expected = h4x0r_atomic_load(cell);

        candidate = (h4x0r_word_ring_item_t){
            .item     = NULL,
            .epoch    = epochs.epoch_d,
            .enqueued = false,
            .dequeued = true,
        };

        while (expected.epoch &amp;lt;= epochs.epoch_d) {
            last = expected;

            if (h4x0r_atomic_cas(cell, &amp;amp;expected, candidate)) {
                if (epochs.epoch_d == last.epoch) {
                    return h4x0r_word_ring_found(last, ep, found);
                }
                if (epochs.epoch_d &amp;gt; last.epoch) {
                    h4x0r_word_ring_drop(ring, last);
                    break;
                }
                return h4x0r_word_ring_found(last, ep, found);
            }
            else {
                if (last.epoch == epochs.epoch_d &amp;amp;&amp;amp; !ring-&amp;gt;drop_handler) {
                    return h4x0r_word_ring_found(last, ep, found);
                }
                epochs = h4x0r_atomic_load(&amp;amp;ring-&amp;gt;epochs);
                continue;
            }
            if (epochs.epoch_q - epochs.epoch_d &amp;lt;= 1) {
                return h4x0r_word_ring_empty(found);
            }
        }
        // We got lapped and need a new epoch.
        epochs = h4x0r_atomic_load(&amp;amp;ring-&amp;gt;epochs);
    }
}
&lt;/code&gt;
    &lt;p&gt;You‚Äôll notice there are a few more helper functions in there:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;h4x0r_epochs_increment()&lt;/code&gt;uses some inline code to do the 128-bit FAA using a union for conversion, as discussed above.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;static inline h4x0r_word_ring_epochs_t
h4x0r_epochs_increment(_Atomic h4x0r_word_ring_epochs_t *p,
                       h4x0r_epoch_info_t                counter)
{
    h4x0r_epoch_info_t result;

    result.i = h4x0r_atomic_add((_Atomic(__uint128_t) *)p, counter.i);

    return result.s;
}
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;h4x0r_word_ring_slot_addr()&lt;/code&gt;takes our epoch, performs the modulo operation, and gets us a pointer to our cell. The code to get the address inside that inline function is more compact than the call, but we prefer the extra clarity, and the compiler is expected to inline it, especially if we put it in a header file (or we can annotate it to always inline).&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;static inline _Atomic(h4x0r_word_ring_item_t) *
h4x0r_word_ring_slot_addr(h4x0r_word_ring_t *ring, uint64_t epoch)
{
    return &amp;amp;ring-&amp;gt;cells[epoch &amp;amp; ring-&amp;gt;last_slot];
}
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;There are several &lt;code&gt;h4x0r_atomic_*()&lt;/code&gt;calls. Those wrap the C11 API calls so we can apply consistent memory ordering that‚Äôs different from C‚Äôs default. We‚Äôll discuss this a bit at the end of the article for those who want to understand. We do use C‚Äôs new-ish&lt;code&gt;_Generic&lt;/code&gt;feature to abstract away over&lt;code&gt;fetch-and-add&lt;/code&gt;, selecting our function for 128 bit values:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;#define h4x0r_atomic_add(x, y)                    \
    _Generic((y),                                 \
        __uint128_t: _h4x0r_atomic_faa_128(x, y), \
        default: atomic_fetch_add_explicit(x, y, H4X0R_MO_RW))

    static inline __uint128_t                                 
        _h4x0r_atomic_faa_128(void *v, __uint128_t val) 
    {                                                         
        _Atomic __uint128_t *var = v;                         
                                                             
        __uint128_t expected;                                 
        __uint128_t desired;                                  
                                                              
        expected = h4x0r_atomic_load(var);                    
                                                              
        do {                                                  
            desired = expected + val;                 
        } while (!h4x0r_atomic_cas(var, &amp;amp;expected, desired)); 
        return expected;                                      
    }

h4x0r_decl_binopu128(faa, +);
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;h4x0r_word_ring_drop()&lt;/code&gt;is about as simple as you‚Äôd want it to be:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;static inline void
h4x0r_word_ring_drop(h4x0r_word_ring_t *ring, h4x0r_word_ring_item_t cell)
{
    if (ring-&amp;gt;drop_handler &amp;amp;&amp;amp; cell.enqueued) {
        (*ring-&amp;gt;drop_handler)(ring, cell.item);
    }
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;The enqueue operation&lt;/head&gt;
    &lt;p&gt;We‚Äôre already more than halfway done with our first ring. All we really need to do is get stuff into it.&lt;/p&gt;
    &lt;p&gt;Our first order of business when a thread calls our enqueue function is to load the existing epochs, grabbing its own ticket (epoch) in the process, via our fetch-and-add.&lt;/p&gt;
    &lt;p&gt;Second, we check the epochs to see if they need repair. That could be one of two scenarios:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We see that the tail is lagging (meaning, the epochs indicate the tail is farther behind than the number of slots); this is due to a relative lack of dequeuers.&lt;/item&gt;
      &lt;item&gt;We see that some dequeue operation accidentally look a read-slot when the queue was empty, during a race condition (We do not want to write into a slot that no reader could ever read).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If the enqueuer sees either of these two scenarios, it will attempt to ‚Äòfix‚Äô the tail by moving the dequeue epoch to the lowest epoch that might still be enqueued. Here are my helper functions to deal with these scenarios:&lt;/p&gt;
    &lt;code&gt;#define H4X0R_BACKOFF_START_NS 1000
#define H4X0R_BACKOFF_MAX_NS   65536

static const struct timespec start_sleep = {
    .tv_sec  = 0,
    .tv_nsec = H4X0R_BACKOFF_START_NS,
};

static inline bool
h4x0r_word_ring_needs_repair(h4x0r_word_ring_epochs_t epochs,
                             uint32_t                 ring_size)
{
    if (epochs.epoch_d + ring_size &amp;lt; epochs.epoch_q) {
        return true;
    }
    if (epochs.epoch_d &amp;gt; epochs.epoch_q) {
        return true;
    }
    return false;
}

static inline void
h4x0r_ring_lag_sleep(struct timespec *sleep_time)
{
    // We don't really care if we sleep the whole time or not.
    nanosleep(sleep_time, NULL);
    sleep_time-&amp;gt;tv_nsec &amp;lt;&amp;lt;= 1;

    if (sleep_time-&amp;gt;tv_nsec &amp;gt; H4X0R_BACKOFF_MAX_NS) {
        sleep_time-&amp;gt;tv_nsec = H4X0R_BACKOFF_MAX_NS;
    }
}

// Returns true if we ever go through the loop, indicating
// we may need to  update our own epoch.
static inline bool
h4x0r_word_ring_repair(h4x0r_word_ring_epochs_t epochs,
                       h4x0r_word_ring_t       *ring)
{
    struct timespec          sleep_time = start_sleep;
    bool                     repair     = false;    
    h4x0r_word_ring_epochs_t candidate;

    while (h4x0r_word_ring_needs_repair(epochs, ring-&amp;gt;num_cells)) {
        repair = true;
        
        if (epochs.epoch_d &amp;gt; epochs.epoch_q) {
            candidate = (h4x0r_word_ring_epochs_t){
                .epoch_q = epochs.epoch_q,
                .epoch_d = epochs.epoch_q,
            };
        }
        else {
            candidate = (h4x0r_word_ring_epochs_t){
                .epoch_q = epochs.epoch_q,
                .epoch_d = epochs.epoch_q - ring-&amp;gt;num_cells,
            };
        }
        if (!h4x0r_atomic_cas(&amp;amp;ring-&amp;gt;epochs, &amp;amp;epochs, candidate)) {
            return true;
        }
        h4x0r_ring_lag_sleep(&amp;amp;sleep_time);
    }

    return repair;
}
&lt;/code&gt;
    &lt;p&gt;If the enqueuers don‚Äôt do the tail-correction, it penalizes dequeuers who are already behind; they‚Äôll pay the price of going back for tickets, only to find they‚Äôre out of date, which can exacerbate problems when they‚Äôre behind.&lt;/p&gt;
    &lt;p&gt;If we do see lag, after attempting to fix it, we should help even more by taking a really short snooze to go ahead and help any pending reader succeed. If we don‚Äôt, then we‚Äôre at more risk of dequeuers continually being forced to retry because writers are starving them. Here is one place in our algorithm where, if we want to go for full wait-freedom, we can turn this process into an exponential backoff loop.&lt;/p&gt;
    &lt;p&gt;Once we leave the readers acceptably far behind, we then go to the cell we‚Äôre supposed to be writing to. There, we‚Äôre going to want to load the state to see what‚Äôs what.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If we see our epoch is already in there, we were too slow, and some reader invalidated the slot; we need to grab a new epoch and try everything again.&lt;/item&gt;
      &lt;item&gt;We do exactly the same thing if we see a HIGHER epoch than ours (writers lapped us, probably because we got suspended).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Next, we add the value, move the state to &lt;code&gt;enqueued&lt;/code&gt;, and attempt to install the item via a CAS.&lt;/p&gt;
    &lt;p&gt;We keep trying until that succeeds.&lt;/p&gt;
    &lt;p&gt;Once our CAS is successful, then the write is successful. However, we still are not always done.&lt;/p&gt;
    &lt;p&gt;Specifically, we may need to look at what we overwrote (which should be installed in the &lt;code&gt;expected&lt;/code&gt; field).&lt;/p&gt;
    &lt;p&gt;If we overwrote a queued item, then we need to pass that item to the drop handler. This is a particularly important thing to do when the item we found is a pointer to heap memory.&lt;/p&gt;
    &lt;p&gt;If we simply overwrite without checking, we might be leaking the memory the pointer references.&lt;/p&gt;
    &lt;p&gt;Of course, if we have no drop handler, we don‚Äôt need the extra step; there‚Äôs no problem.&lt;/p&gt;
    &lt;p&gt;We can just return, successful in our mission.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs my implementation:&lt;/p&gt;
    &lt;code&gt;uint64_t
h4x0r_word_ring_enqueue(h4x0r_word_ring_t *ring, void *item)
{
    h4x0r_word_ring_cell_t  *cell;
    h4x0r_word_ring_epochs_t epochs;
    uint64_t                 write_epoch;
    h4x0r_word_ring_item_t   expected;

    while (true) {
        epochs      = h4x0r_epochs_increment(&amp;amp;ring-&amp;gt;epochs, write_incr);
        write_epoch = epochs.epoch_q;
        
        if (h4x0r_word_ring_repair(epochs, ring)) {
            if (write_epoch + ring-&amp;gt;num_cells &amp;lt; epochs.epoch_q) {
                continue;
            }
        }

        cell     = h4x0r_word_ring_slot_addr(ring, write_epoch);
        expected = h4x0r_atomic_load(cell);

        // This has to be a CAS; we might have another writer who
        // laps us between the last epoch check and the coming op.
        h4x0r_word_ring_item_t new = {
            .item     = item,
            .epoch    = write_epoch,
            .enqueued = true,
        };

        while (expected.epoch &amp;lt; write_epoch) {
            if (h4x0r_atomic_cas(cell, &amp;amp;expected, new)) {
                // If we overwrote something, it'll need to be dropped.
                h4x0r_word_ring_drop(ring, expected);
                return write_epoch;
            }
        }
        continue; // too slow; get a new epoch and retry.
    }
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;We‚Äôve Inscribed some words on your ring&lt;/head&gt;
    &lt;p&gt;We have our first lock-free ring. But so far, it‚Äôs like we got the ring out of a cracker-jack box. It‚Äôs not yet something nice enough to use, since we‚Äôre limited to putting in 64-bit values.&lt;/p&gt;
    &lt;p&gt;However, we will 100% solve that problem.&lt;/p&gt;
    &lt;head rend="h1"&gt;That üíç is too small, I want a BIG one&lt;/head&gt;
    &lt;p&gt;One thing about a ring that‚Äôs often valued is that you can operate in a fixed amount of statically allocated memory. Only giving 64 bits of space for the ring will push us towards dynamic allocation, which is a shame.&lt;/p&gt;
    &lt;p&gt;But we can use our word ring to make a big ring with larger, fixed-sized memory cells.&lt;/p&gt;
    &lt;p&gt;The basic idea is that we have two circular buffers, one of them being the word ring (we‚Äôll call it &lt;code&gt;R&lt;/code&gt;). Then, we‚Äôll create a second circular buffer to store our arbitrary-sized records. We‚Äôll call this one &lt;code&gt;S&lt;/code&gt; (for store).&lt;/p&gt;
    &lt;p&gt;The entries we enqueue into our word ring &lt;code&gt;R&lt;/code&gt; will simply be an epoch that points to a spot in &lt;code&gt;S&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Write threads will peek at the ring‚Äôs global epoch info, for two reasons:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;As a hint for where to start in the larger array, and&lt;/item&gt;
      &lt;item&gt;So that we know which records conceptually aren‚Äôt in the ring anymore and can be overwritten.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here are the data structures I used in my implementation, along with the state flags I use throughout.&lt;/p&gt;
    &lt;code&gt;typedef struct h4x0r_ring_t h4x0r_ring_t;
typedef struct h4x0r_ring_entry_info_t h4x0r_ring_entry_info_t;


// The full cell definition for the outer ring cells.

struct h4x0r_ring_entry_t {
    _Atomic h4x0r_ring_entry_info_t info;
    uint64_t                        len;
    char                            data[];
};

// This the first item in the outer ring cell, 
struct h4x0r_ring_entry_info_t {
    uint64_t write_epoch;
    uint64_t state;
};

struct h4x0r_ring_t {
    h4x0r_word_ring_t  *word_ring;
    h4x0r_ring_entry_t *entries;
    _Atomic uint64_t    entry_ix;
    uint64_t            last_entry;
    uint64_t            entry_len;
};

enum : uint64_t {
    H4X0R_RING_EMPTY             = 0x00,
    H4X0R_RING_RESERVED          = 0x01,
    H4X0R_RING_DEQUEUE_RESERVE   = 0x02,
    H4X0R_RING_ENQUEUE_DONE      = 0x04,
    H4X0R_RING_USED              = 0x07,
    H4X0R_RING_DEQUEUE_DONE_MASK = ~0x06,
};
&lt;/code&gt;
    &lt;p&gt;I‚Äôm going to skip initialization here, but two important notes if you‚Äôre going to DIY:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Double-check that the ‚Äúcore‚Äù word ring‚Äôs size in bits is a power of two.&lt;/item&gt;
      &lt;item&gt;Ensure that cell sizes are properly aligned (probably to a 16-byte boundary to be safe).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The enqueue operation&lt;/head&gt;
    &lt;p&gt;The write thread starts by grabbing the underlying word ring‚Äôs epoch information. It takes the write epoch it finds, and maps that into &lt;code&gt;S&lt;/code&gt; (&lt;code&gt;e % len(S)&lt;/code&gt;, if &lt;code&gt;e&lt;/code&gt; is the epoch)`.&lt;/p&gt;
    &lt;p&gt;Starting at that position, the writer scans &lt;code&gt;S&lt;/code&gt; to find the first valid spot it can claim.&lt;/p&gt;
    &lt;p&gt;That means, if it notices an operation in progress, it skips that cell and keeps probing until it can claim a cell that‚Äôs safe to write. Once the write completes, we enqueue the position into our word ring. Adding it to the word ring gives us the epoch; we add that into our slot inside &lt;code&gt;S&lt;/code&gt;, before we remove the flag that indicates we‚Äôre writing to the cell.&lt;/p&gt;
    &lt;p&gt;As a result, entries in &lt;code&gt;S&lt;/code&gt; can be out of order, and that‚Äôs totally fine. The correct order will be kept in the word ring.&lt;/p&gt;
    &lt;p&gt;More specifically, the steps for enqueuers (writers) are as follows:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find a spot in &lt;code&gt;S&lt;/code&gt;, and reserve it, so no other writer will touch it.&lt;/item&gt;
      &lt;item&gt;Copy data into the reserved cell.&lt;/item&gt;
      &lt;item&gt;Enqueue a pointer to &lt;code&gt;S&lt;/code&gt;into&lt;code&gt;R&lt;/code&gt;(our linearization point).&lt;/item&gt;
      &lt;item&gt;Write into &lt;code&gt;S&lt;/code&gt;the epoch that&lt;code&gt;R&lt;/code&gt;returned to us when we enqueued.&lt;/item&gt;
      &lt;item&gt;Update the slot in &lt;code&gt;S&lt;/code&gt;with the epoch, and indicate we‚Äôre done with their enqueue.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To make this all work, we really should have &lt;code&gt;S&lt;/code&gt; contain more entries than &lt;code&gt;R&lt;/code&gt;. We want to have enough to ensure the right number of items can all be enqueued at once in a full queue, and that any write thread will still have a space to write. If we don‚Äôt do that, writers will be roaming around a full parking garage until a spot opens up and they‚Äôre lucky enough to nab it.&lt;/p&gt;
    &lt;p&gt;If we have a ceiling on the number of threads we allow, we can use that value. But, if not, just doubling the number of entries should be more than good enough. There will be no competition for the enqueuer‚Äôs slot from other enqueuers.&lt;/p&gt;
    &lt;p&gt;However, a dequeuer can come in after step 3 completes and before step 4 completes, while we‚Äôre suspended.&lt;/p&gt;
    &lt;p&gt;That‚Äôs not a problem for us‚Äì the linearization point is the enqueue into &lt;code&gt;R&lt;/code&gt;. We just need to make sure that enqueuers can only claim a slot if BOTH enqueuers and dequeuers are done with their operation (and, if it‚Äôs not in &lt;code&gt;R&lt;/code&gt;, of course).&lt;/p&gt;
    &lt;p&gt;Note that dequeuers will set a state bit when they start dequeuing, so we can easily avoid taking those slots. However, when figuring out whether we can overwrite a state no thread is in, we need to check the stored epoch, to make sure it‚Äôs far enough behind ours that it‚Äôs not conceptually in the queue anymore.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs the core of the enqueue operation:&lt;/p&gt;
    &lt;code&gt; void
h4x0r_ring_enqueue(h4x0r_ring_t *self, void *item, uint64_t len)
{
    uint64_t                 ix;
    uint64_t                 byte_ix;
    uint64_t                 start_epoch;
    h4x0r_ring_entry_info_t  expected;
    h4x0r_ring_entry_info_t  candidate;
    h4x0r_ring_entry_t      *cur;
    h4x0r_word_ring_epochs_t epochs;

    if (len &amp;gt; self-&amp;gt;entry_len) {
        len = self-&amp;gt;entry_len;
    }

    epochs      = h4x0r_atomic_load(&amp;amp;self-&amp;gt;word_ring-&amp;gt;epochs);
    start_epoch = epochs.epoch_q;
    ix          = start_epoch &amp;amp; self-&amp;gt;last_entry;

    while (true) {
        byte_ix = ix * (sizeof(h4x0r_ring_entry_t) + self-&amp;gt;entry_len);
        cur     = (h4x0r_ring_entry_t *)&amp;amp;(((char *)self-&amp;gt;entries)[byte_ix]);

        expected              = h4x0r_atomic_load(&amp;amp;cur-&amp;gt;info);
        candidate.write_epoch = 0;
        candidate.state       = H4X0R_RING_RESERVED;

        if (h4x0r_atomic_cas(&amp;amp;cur-&amp;gt;info, &amp;amp;expected, candidate)) {
            break;
        }

        if (!h4x0r_ring_can_write_here(expected,
                                       start_epoch,
                                       self-&amp;gt;last_entry)) {
            ix = (ix + 1) &amp;amp; self-&amp;gt;last_entry;
            continue;
        }

        if (h4x0r_atomic_cas(&amp;amp;cur-&amp;gt;info, &amp;amp;expected, candidate)) {
            break;
        }
        ix = (ix + 1) &amp;amp; self-&amp;gt;last_entry;
    }

    memcpy(cur-&amp;gt;data, item, len);

    candidate.write_epoch = h4x0r_word_ring_enqueue(self-&amp;gt;word_ring,
                                                    (void *)ix);
    candidate.state       = H4X0R_RING_ENQUEUE_DONE;
    cur-&amp;gt;len              = len;

    h4x0r_atomic_store(&amp;amp;cur-&amp;gt;info, candidate);
}
&lt;/code&gt;
    &lt;p&gt;The only new helper function here is &lt;code&gt;h4x0r_ring_can_write_here()&lt;/code&gt; and its helper:&lt;/p&gt;
    &lt;code&gt;
static inline bool
h4x0r_ring_entry_is_being_used(h4x0r_ring_entry_info_t info)
{
    if (info.state &amp;amp; H4X0R_RING_USED) {
        return true;
    }

    return false;
}

static inline bool
h4x0r_ring_can_write_here(h4x0r_ring_entry_info_t info,
                          uint64_t                my_write_epoch,
                          uint32_t                last_entry)
{
    if (h4x0r_ring_entry_is_being_used(info)) {
        return false;
    }

    if (info.write_epoch &amp;gt; my_write_epoch) {
        return false;
    }

    if (info.write_epoch &amp;gt; (my_write_epoch - (last_entry + 1))) {
        return false;
    }

    return true;
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;The dequeue operation&lt;/head&gt;
    &lt;p&gt;Dequeuers (readers) take the following steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Dequeue a value from &lt;code&gt;R&lt;/code&gt;, which gives us the index into&lt;code&gt;S&lt;/code&gt;we need; at the same time, we use our reference parameter to capture the epoch the writer used to make sure we don‚Äôt read from the future.&lt;/item&gt;
      &lt;item&gt;Attempt to mark the cell in &lt;code&gt;S&lt;/code&gt;for reading and validating. If validation fails, we restart.&lt;/item&gt;
      &lt;item&gt;Actually perform the read.&lt;/item&gt;
      &lt;item&gt;They mark the cell state in &lt;code&gt;S&lt;/code&gt;to indicate the read is done.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note that a slow dequeuer might find that by the time they attempt to flag the cell in &lt;code&gt;L&lt;/code&gt; for read, someone has already claimed that cell for writing a newer log message. In such cases, the slow dequeuer just needs to try again.&lt;/p&gt;
    &lt;p&gt;Or, the writer may not have stored its epoch yet. We know if they got a ticket, we can go find the right slot. If the epoch is anything less than the epoch we dequeued, it‚Äôs definitely our value to dequeue.&lt;/p&gt;
    &lt;p&gt;For the dequeue, we‚Äôll use these two very simple helpers:&lt;/p&gt;
    &lt;code&gt;static inline bool
h4x0r_ring_can_dequeue_here(h4x0r_ring_entry_info_t info,
                            uint64_t                expected_epoch)
{
    if (info.write_epoch &amp;gt; expected_epoch) {
        return false;
    }

    return true;
}

static inline uint64_t
h4x0r_ring_set_dequeue_done(uint64_t state)
{
    return state &amp;amp; H4X0R_RING_DEQUEUE_DONE_MASK;
}
&lt;/code&gt;
    &lt;p&gt;Our dequeue function is going to return whether there was a dequeue or not, instead of returning a value, and use a parameter for people to check if the queue was empty.&lt;/p&gt;
    &lt;p&gt;That‚Äôs because we‚Äôre going to need the caller to pass in a buffer for the result. We DEFINITELY don‚Äôt want to pass back a pointer to the ring entry; that‚Äôs a recipe for disaster.&lt;/p&gt;
    &lt;head rend="h1"&gt;Testing our rings&lt;/head&gt;
    &lt;p&gt;Especially since we‚Äôre dealing with concurrency, we want to make sure to test thoroughly. We should run in a number of different configurations, and should thoroughly test to make sure that we only ever see linearized results when we dequeue.&lt;/p&gt;
    &lt;p&gt;We‚Äôre also going to want to count some stuff, then check it all for consistency:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We should count successful dequeues.&lt;/item&gt;
      &lt;item&gt;We should independently count the number of drops, too.&lt;/item&gt;
      &lt;item&gt;We should capture the number of times fast readers find an empty queue.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each thread should collect metrics privately, and then add them to totals, when it‚Äôs done with the work.&lt;/p&gt;
    &lt;p&gt;If we add independently collected dequeues to drops, we should get the total number of enqueues; otherwise we have a bug.&lt;/p&gt;
    &lt;p&gt;You‚Äôll need to know when to stop trying to dequeue. The simplest path is to have the main thread first &lt;code&gt;join()&lt;/code&gt; on all enqueuers; at that point, we know there‚Äôs nothing else to enqueue. So when a dequeue thread sees all writers are done, the first time they encounter an empty queue, they know they‚Äôre done. That way is easy to implement, but leaves a small window where the empty dequeue time will push up. You can instead have individual writer threads decrement a global counter, which will shorten that window.&lt;/p&gt;
    &lt;p&gt;Also, even if it‚Äôs not real world, we should test for worst case, and run enqueues and dequeues as back to back as possible, to help us understand worst case performance, or any other considerations we might need.&lt;/p&gt;
    &lt;p&gt;I‚Äôll spare you the code, because you can go get it in the codeberg repo. But, it does iterate over both types of ring, using five different sizes of buffer, and with a variety of (mostly imbalanced) readers and writers.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs look at some example output though (I‚Äôve deleted a few columns to keep us under 80 characters).&lt;/p&gt;
    &lt;p&gt;First, for the main, arbitrary ring, let‚Äôs look at our most minimal ring size:&lt;/p&gt;
    &lt;code&gt;Tests for queue with 16 items:
Test   Th+    Th-   Time(sec)     Q-             Qüíß    Mop/s (‚úÖ)
------------------------------------------------------------------
# 1    1      1      0.0246     251,355       10,781      10.23   
# 2    2      2      0.0505     235,606        3,705       5.11   
# 3    4      4      0.1124      68,417      187,282       0.67   
# 4    8      8      0.2179     199,100       34,775       1.04   
# 5    2      1      0.0364     110,678      132,274       3.57   
# 6    4      1      0.0624      40,678      210,810       0.82   
# 7    8      1      0.0855       7,927      251,875       0.12   
# 8    1      2      0.0271     176,210       85,934       6.51   
# 9    1      4      0.0472     183,065       79,079       3.88   
#10    1      8      0.0930     183,732       78,412       1.97   
&lt;/code&gt;
    &lt;p&gt;Here, &lt;code&gt;Th+&lt;/code&gt; is the number of enqueue threads. The &lt;code&gt;-&lt;/code&gt; sign denotes the dequeue size.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Qüíß&lt;/code&gt; is the number of drops. Then, the last column denotes how many millions of ops per second we performed in that test case.&lt;/p&gt;
    &lt;p&gt;The three columns we omitted that you‚Äôll see in the output:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Q+&lt;/code&gt;is the total number of enqueues, which is always&lt;code&gt;262,144&lt;/code&gt;in my runs.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Q‚àÖ&lt;/code&gt;, the number of times a dequeuer attempted to dequeue, and found the queue was empty.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Mop/s (‚úÖ+‚àÖ)&lt;/code&gt;which recomputes Mop/sec, including dequeues that find the queue empty.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;One thing that should jump out to you is that there are an absurd number of drops in there. And when we get those absurd drops, our overall performance tends to plummet. The table makes it clear that we need to do more to deal with the contention.&lt;/p&gt;
    &lt;p&gt;That‚Äôs the kind of concern you should be looking for when testing parallel algorithms.&lt;/p&gt;
    &lt;p&gt;The insight makes sense; if the queue is full, the writers help with the tail, but then go back to competing where the table comes together.&lt;/p&gt;
    &lt;p&gt;The conclusion I drew is that, before the queue fills, enqueuers should briefly pause to give preference to readers, so as not to contend with them. In the code repo for this article, I did just that for you, setting the threshold to 75%, which gives much better results:&lt;/p&gt;
    &lt;code&gt;Tests for queue with 16 items:
Test   Th+    Th-   Time(sec)     Q-         Qüíß    Mop/s (‚úÖ)
--------------------------------------------------------------
# 1    1      1      0.0248     262,142        2      10.55
# 2    2      2      0.0450     260,071       43       5.82
# 3    4      4      0.0923     236,809      434       2.83
# 4    8      8      0.2132     208,501      177       1.23
# 5    2      1      0.0400     256,786       32       6.55
# 6    4      1      0.0479     253,364       20       5.47
# 7    8      1      0.0989     219,220       53       2.65
# 8    1      2      0.0384     262,101       43       6.83
# 9    1      4      0.0489     262,084       60       5.36
#10    1      8      0.0868     261,814      330       3.02
&lt;/code&gt;
    &lt;p&gt;If we run more tests, we may see some significant drops, but we‚Äôd expect big numbers only when the number of writers greatly outweighs the number of readers. And in that case, the drops are expected. On my machine, this only ever happens for 16-item queues though. Even at 128 items, it looks good (on my machine):&lt;/p&gt;
    &lt;code&gt;Tests for queue with 128 items:
Test   Th+    Th-   Time(sec)      Q-           Qüíß     Mop/s (‚úÖ)  
------------------------------------------------------------------
#11    1      1      0.0147     262,145          0      17.82
#12    2      2      0.0428     262,145          0       6.13
#13    4      4      0.0765     262,144          1       3.42
#14    8      8      0.1879     262,140          5       1.39
#15    2      1      0.0263     262,145          0       9.97
#16    4      1      0.0517     262,145          0       5.07
#17    8      1      0.0936     262,145          0       2.80
#18    1      2      0.0251     262,144          1      10.43
#19    1      4      0.0417     262,142          3       6.28
#20    1      8      0.0859     262,145          0       3.05
&lt;/code&gt;
    &lt;p&gt;If we study these charts, we can compare to see exactly how big an impact that contention actually has. In tests where we were dropping, the number of operations plummeted massively due to the contention.&lt;/p&gt;
    &lt;p&gt;With the above tweak to the enqueuer‚Äôs help rules, my laptop tests never fail to top 1 million operations a second, and raw word-queue performance peaks at about 24 Mop/sec, and stays above 3 MOp/sec for all but a couple of configurations (the ones where I‚Äôve overcommitted my cores w 8q+/8q-, so past the point where I‚Äôve maxed out potential parallelism).&lt;/p&gt;
    &lt;p&gt;Not bad, considering we haven‚Äôt really done anything to optimize (I do have a more optimized implementation of the base word-ring algorithm that can get as high at 40Mop/sec with one enqueuer and one dequeuer, and bottoms out at 4Mops/sec when the number of dequeuers is lopsided, all on the same machine).&lt;/p&gt;
    &lt;p&gt;My test bed is there in the repo for you to use if you like.&lt;/p&gt;
    &lt;head rend="h1"&gt;Did we forget to order something?&lt;/head&gt;
    &lt;p&gt;When I wrote about futexes and mutexes, I glossed over the topic of memory ordering, because it‚Äôs notoriously hard to communicate well. But given we‚Äôre into the world of concurrency without locks, I think it‚Äôs remiss not to cover it. I‚Äôll try to make it as straightforward as possible.&lt;/p&gt;
    &lt;p&gt;If you are still confused after reading this section (which is likely), give me feedback on what questions it leaves you with, and I‚Äôll try again.&lt;/p&gt;
    &lt;head rend="h2"&gt;Your compiler may be gaslighting you&lt;/head&gt;
    &lt;p&gt;Your compiler wants you to think your code will execute in the order you‚Äôd expect while staring at the source code.&lt;/p&gt;
    &lt;p&gt;Meanwhile, behind your back, it‚Äôs generally going way out of its way to thwart your expectations. Though, to be fair, it‚Äôs doing it for you. It knows how disappointed you‚Äôd be if it performs poorly for you.&lt;/p&gt;
    &lt;p&gt;So yes, the compiler will absolutely rewrite your code (particularly when you turn on any level of optimization). It has no qualms changing the order you‚Äôd expect, with hopes of it running faster for you. But, it‚Äôs hoping you won‚Äôt notice. The transformations often aren‚Äôt semantically identical to what you might have intended, but, at least in the context of a single thread, you‚Äôre not likely to notice the difference.&lt;/p&gt;
    &lt;p&gt;Even for multi-threaded programs, compilers often try hard to optimize what you‚Äôre doing, transforming and reordering to take advantage of the CPU.&lt;/p&gt;
    &lt;p&gt;Still, there are things the compiler won‚Äôt do (just like Meatloaf). Specifically, compilers have this idea of ‚Äúbarriers‚Äù, which are features in your code that the compiler won‚Äôt move stuff past.&lt;/p&gt;
    &lt;p&gt;For instance, compilers will not move code across the boundary of a mutex, or any op on a variable labeled &lt;code&gt;_Atomic&lt;/code&gt; (unless you explicitly allow it at the call site).&lt;/p&gt;
    &lt;p&gt;The compiler is conservative here, because you expressed your intent. Event if the compiler thinks you‚Äôre walking away from a big performance gain, and even if they could ‚Äúprove‚Äù that it‚Äôs not going to change the semantics of your program. Generally, function boundaries result in a barrier as well, as long as they‚Äôre not inlined.&lt;/p&gt;
    &lt;p&gt;But, even with multi-processor programs, the compiler does all that analysis as if a single thread is running. So it can move data around across threads in many frustrating ways. If you don‚Äôt pay attention to how you handle concurrency, compiler transformations can definitely make race conditions far worse.&lt;/p&gt;
    &lt;head rend="h2"&gt;The processor is a bad influence&lt;/head&gt;
    &lt;p&gt;Making matters worse, your compiler‚Äôs friend, the processor, tries to apply parallelism everywhere it can, because of performance. So there are also plenty of architectural considerations that can lead to data races.&lt;/p&gt;
    &lt;p&gt;For instance, you probably know that the CPU is fast, and memory accesses are slow. And when many threads are competing to load memory at the same time, things can get chaotic, because memory cells loaded into registers on one core don‚Äôt magically sync instantly across multiple cores.&lt;/p&gt;
    &lt;p&gt;And, the processor may do its own reordering of instructions, for instance, to achieve fine-grained parallelism via things like instruction pipelining, which can definitely run your code out of order.&lt;/p&gt;
    &lt;p&gt;Processors not only have a very complex memory model, but that model can be vastly different across architectures (e.g., x86 and ARM).&lt;/p&gt;
    &lt;p&gt;Very few programmers are going to be aware of most of that subtlety.&lt;/p&gt;
    &lt;p&gt;Languages could generate code to make sure everything always happens in a well-defined order (full sequential consistency), but generally they do not. Processors go out of their way to make things faster by moving your code around, and compilers often do a lot to make your code faster too.&lt;/p&gt;
    &lt;p&gt;So no language is going to find it an acceptable hit to force the processor to run everything in a well-defined order, at least in the case of multiple threads.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to make this relationship work&lt;/head&gt;
    &lt;p&gt;Programmers typically will need to tell their compiler where to get conservative, and sacrifice performance for correctness. In C, you can do that by labeling variables as &lt;code&gt;_Atomic&lt;/code&gt;, which tells the compiler that variables will be used across threads.&lt;/p&gt;
    &lt;p&gt;But &lt;code&gt;_Atomic&lt;/code&gt; doesn‚Äôt truly mean, ‚Äúalways atomic‚Äù, primarily because you can explicitly ask for different behavior on a case-by-case basis.&lt;/p&gt;
    &lt;p&gt;If you don‚Äôt specify anything other than &lt;code&gt;_Atomic&lt;/code&gt;, it does mean that you‚Äôre not going to get corrupting data races, and it does mean that, by default, the compiler will ensure all operations on that variable will happen in the same order, from the perspective of any thread.&lt;/p&gt;
    &lt;p&gt;Enforcing that kind of order generally slows things down, so, you can specify to the compiler cases where you want different behavior. For instance, if you‚Äôre initializing memory, you probably have exclusive access to the data. Your own view on the ordering is consistent anyway, so at this point you may not care to slow down the processor.&lt;/p&gt;
    &lt;p&gt;However, that could be problematic for the next thread to read. Since the first access didn‚Äôt care, it‚Äôs definitely possible for a thread to get a reference to the object, and see the pre-initialization value, but only if that second access explicitly relaxes its requirement for getting access to the variable.&lt;/p&gt;
    &lt;p&gt;Generally, those kinds of surprises are easy to find, especially when they involve fields in an object whose pointer you read atomically, but whose fields are not marked as being &lt;code&gt;_Atomic&lt;/code&gt;. It‚Äôs a great recipe for Heisenbugs.&lt;/p&gt;
    &lt;p&gt;So generally, if you know multiple threads will handle a variable, not only should you go ahead and declare it as &lt;code&gt;_Atomic&lt;/code&gt;, but also you should avoid giving up most of its protection‚Äì you‚Äôre just inviting disaster.&lt;/p&gt;
    &lt;head rend="h3"&gt;My memory order arrived damaged&lt;/head&gt;
    &lt;p&gt;By default, accessing a single &lt;code&gt;_Atomic&lt;/code&gt; variable will make sure that any changes to the variable happen in a well-defined (linearized) order. For example, let‚Äôs say we have a huge number of threads, and thread &lt;code&gt;0&lt;/code&gt; goes to modify &lt;code&gt;_Atomic&lt;/code&gt; variable &lt;code&gt;A&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Just by declaring the variable to be &lt;code&gt;_Atomic&lt;/code&gt;, the compiler will, if necessary, generate code that makes sure that any changes to &lt;code&gt;A&lt;/code&gt; that were in flight when thread &lt;code&gt;0&lt;/code&gt; goes to modify it, all end before its operation. Similarly, any subsequent loads of that variable will see the reflected value.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;_Atomic&lt;/code&gt; variable access (unless relaxed) acts like a barrier that the compiler will enforce for the variable in question. That enforcement, though, is really done by generating an assembly that helps get the proper semantics, which is very platform dependent.&lt;/p&gt;
    &lt;p&gt;If you do not mark a variable as &lt;code&gt;_Atomic&lt;/code&gt;, then you should not be using the variable across threads. The compiler will certainly generate code under the assumption that those variables won‚Äôt have to deal with concurrent access.&lt;/p&gt;
    &lt;p&gt;But we do have some options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Relaxed Memory Order. In C/C++ parlance, the semantics of variables that are not marked&lt;/p&gt;&lt;code&gt;_Atomic&lt;/code&gt;is called relaxed memory order. It‚Äôs a weird name that means there are no ordering guarantees outside of what would be promised by the underlying architecture for a non-atomic variable, and the promise of non-corrupting data races. That‚Äôs scary.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sequentially Consistent Ordering. This lives on the other end of the spectrum from relaxed ordering. Using this is supposed to ensure a global ordering for all atomic data by default, at the price of some efficiency. This is the default memory ordering, and is the strongest, but it does have some slight issues we‚Äôll discuss in a minute.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Acquire / Release Ordering. This is in between relaxed and sequentially consistent. It basically does what you want it to do on a variable-by-variable basis, forcing a well defined ordering.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You‚Äôll often see ‚ÄúAcquire‚Äù and ‚ÄúRelease‚Äù listed as separate strategies. They‚Äôre more like two sides of the same coin:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;acquire semantics apply only to loading an&lt;/p&gt;&lt;code&gt;_Atomic&lt;/code&gt;variable (i.e., acquiring it). The guarantee is that any modifications to a memory address that were made by other threads will be reflected by the time the value loads, with nothing still in progress.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;release semantics only apply to storing an&lt;/p&gt;&lt;code&gt;_Atomic&lt;/code&gt;value (i.e., releasing it). The guarantee here is that the store operation will be reflected in the next subsequent load of that address. That is to say, once the store finishes, no other thread will be able to load the previous value.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For some operations, only one of these two things makes sense. For instance, acquire semantics make sense for an &lt;code&gt;atomic_load()&lt;/code&gt; call, but that doesn‚Äôt update the value, so release semantics don‚Äôt make sense here (and thus, acquire/release doesn‚Äôt make sense either).&lt;/p&gt;
    &lt;p&gt;You don‚Äôt specify memory ordering when you declare something &lt;code&gt;_Atomic&lt;/code&gt;. By default, every operation for such variables will get the strongest memory ordering.&lt;/p&gt;
    &lt;p&gt;If you want anything else, you can get it on a call-by-call basis every time the variable is used, by calling a call in the C atomic library that allows you to explicitly change to another ordering, but only at that one slot.&lt;/p&gt;
    &lt;p&gt;Generally, the defaults are going to be least surprising (in a world where surprises are common, and understanding what‚Äôs going on is hard).&lt;/p&gt;
    &lt;p&gt;That doesn‚Äôt mean that most strict memory ordering is perfect: sequential consistency has some issues that prevent it from living up to its name, particularly when you end up mixing access with different memory orderings (see section 2 of this paper for more details).&lt;/p&gt;
    &lt;p&gt;Plus, sequential consistency generally isn‚Äôt much stronger than acquire / release semantics, and it can be slower, depending on the architecture. So it‚Äôs pretty reasonable to use acquire/release semantics as the default.&lt;/p&gt;
    &lt;p&gt;But this stuff is trickier than we might think.&lt;/p&gt;
    &lt;p&gt;For instance, you may have noticed that I declared the &lt;code&gt;item&lt;/code&gt; field inside the struct &lt;code&gt;h4x0r_word_ring_item_t&lt;/code&gt; to be &lt;code&gt;_Atomic&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If you remove that qualifier, on some platforms our tests will occasionally fail. Why? How is that possible??&lt;/p&gt;
    &lt;p&gt;Sure, we dequeue an entire &lt;code&gt;h4x0r_word_ring_item_t&lt;/code&gt; atomically. But, we store the result of that dequeue into a second memory location, that isn‚Äôt itself marked to be atomic. In our case, in our word ring dequeue algorithm, that‚Äôs going to be the variable called &lt;code&gt;last&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;So, when we return &lt;code&gt;last.item&lt;/code&gt;, we might end up returning the value that was stored in that field before the CAS operation.&lt;/p&gt;
    &lt;p&gt;Since we also return the associated epoch, we could possibly get an old value there, too. However, since they are only unloaded into the &lt;code&gt;last&lt;/code&gt; field together, (atomically), we can be pretty confident that, if the item is available, then the epoch will be too.&lt;/p&gt;
    &lt;p&gt;Still, C doesn‚Äôt guarantee that; it‚Äôs just a matter of having some knowledge of what‚Äôs going on under the hood (and experience to back it up).&lt;/p&gt;
    &lt;p&gt;But, if you wanted to be really careful, you would want to tag the epoch field as &lt;code&gt;_Atomic&lt;/code&gt; too. If you try though, you‚Äôll get an error, because &lt;code&gt;_Atomic&lt;/code&gt; doesn‚Äôt work with bit slices directly. You‚Äôd have to do something else. Some options are:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Use a union, with one of the types in the union being an &lt;code&gt;_Atomic uint64_t&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Declare the thing as a &lt;code&gt;uint64_t&lt;/code&gt;, and manage the flag bit manually (but you do need the flag to be a bit).&lt;/item&gt;
      &lt;item&gt;Leave it unsynced, and tell the compiler to sync all relevant variables before pulling the epoch out of the item.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This third option you can do with a memory fence, putting it before the assignment in &lt;code&gt;h4x0r_word_ring_found&lt;/code&gt;. In our associated code, &lt;code&gt;h4x0r_atomic_fence()&lt;/code&gt; will do the trick; this is a very trivial wrapper around a C atomic builtin.&lt;/p&gt;
    &lt;p&gt;By the way, if we tag &lt;code&gt;item&lt;/code&gt; as being &lt;code&gt;_Atomic&lt;/code&gt;, but specify relaxed memory ordering when we copy it out, we could absolutely end up with the same problem, because &lt;code&gt;_Atomic&lt;/code&gt; is only atomic until you explicitly tell it otherwise.&lt;/p&gt;
    &lt;p&gt;Yes, there are a lot of subtleties. Nobody said it would be easy.&lt;/p&gt;
    &lt;p&gt;Good luck, you‚Äôre going to need it.&lt;/p&gt;
    &lt;p&gt;‚Äì Lee T. Solo (tho with a ring on it)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46288286</guid><pubDate>Tue, 16 Dec 2025 13:32:42 +0000</pubDate></item><item><title>Rust GCC back end: Why and how</title><link>https://blog.guillaume-gomez.fr/articles/2025-12-15+Rust+GCC+backend%3A+Why+and+how</link><description>&lt;doc fingerprint="b6601b5c7a5f3054"&gt;
  &lt;main&gt;&lt;p&gt;Whenever you compile using Rust, the compiler goes through different passes and in the end, generated binary code for the target processor. By default, it uses LLVM as backend to generate the binary code, but more backends exist like cranelift and GCC. This post is about how it's possible for one compiler to use different backend to generate binaries, in particular GCC.&lt;/p&gt;&lt;p&gt;Before going into details, we need to describe how compilers actually work. They read source code and convert it internally into a format they can manipulate, commonly called Abstract Syntax Tree (shortened "AST").&lt;/p&gt;&lt;p&gt;However, compilers go through multiple passes, and often each pass has their own AST. Let's take a short and very incomplete example with the Rust compiler passes. We have 4 steps (again, this is simplified!):&lt;/p&gt;&lt;p&gt;Each step generates a new AST with new information if no error was encountered and provides it to the next pass.&lt;/p&gt;&lt;p&gt;Little side-note: If enough people are interested by this topic, I can write a (much) longer explanation of these passes.&lt;/p&gt;&lt;p&gt;So now that we have a high-level idea of Rust compiler passes, what is the difference between "front-end" and "back-end" exactly?&lt;/p&gt;&lt;p&gt;We consider the front-end to be the part handling (high-level non-exhaustive list) code parsing, linting, type-checking and borrow-checking (steps 1 to 3). When all this is done, it means the code is valid and needs to be translated to the target processor instructions set. To do so, we call LLVM/GCC which will translate the Rust compiler AST into assembly code (step 4).&lt;/p&gt;&lt;p&gt;The Rust compiler backends are the bridge between the Rust compiler AST and the actual code generator. They receive the AST and call the LLVM/GCC/... API which will in turn run their passes, optimize and finally generate the assembly code.&lt;/p&gt;&lt;p&gt;LLVM being much more recent than GCC (2003 vs 1987), a lot of older processors are not supported and will never be. So if you want to write a Rust program on an old platform like Dreamcast, you have no choice to either write your own backend or use the GCC backend (or the &lt;code&gt;gccrs&lt;/code&gt; front-end once ready).&lt;/p&gt;&lt;p&gt;For the readers interested in doing so, there is a guide explaining how to build Rust programs for Dreamcast here.&lt;/p&gt;&lt;p&gt;The GCC backend is different than gccrs which is a front-end for GCC written in C++, which doesn't reuse the front-end of &lt;code&gt;rustc&lt;/code&gt;, meaning they need to reimplement parsing, type-checking, linting, borrow-checking, compilation errors, etc.&lt;/p&gt;&lt;p&gt;On the other hand, the GCC backend (the crate name is &lt;code&gt;rustc_codegen_gcc&lt;/code&gt;) is just "yet another backend codegen" of the Rust compiler, like LLVM or Cranelift, only meant to generate the binary from the Rust compiler input. It's a bridge between Rust compiler's AST and the codegen API.&lt;/p&gt;&lt;p&gt;On that note: GCC doesn't provide a nice library to give access to its internals (unlike LLVM). So we have to use &lt;code&gt;libgccjit&lt;/code&gt; which, unlike the "jit" ("just in time", meaning compiling sub-parts of the code on the fly, only when needed for performance reasons and often used in script languages like Javascript) part in its name implies, can be used as "aot" ("ahead of time", meaning you compile everything at once, allowing you to spend more time on optimization). To do so we use bindings, which are split in two parts:&lt;/p&gt;&lt;code&gt;gccjit-sys&lt;/code&gt; which redeclares the C items we need.&lt;code&gt;gccjit&lt;/code&gt; which provides a nice API over &lt;code&gt;gccjit-sys&lt;/code&gt;.&lt;p&gt;If you want to write your own compiler and use GCC as codegen, you can do it thanks to &lt;code&gt;libgccjit&lt;/code&gt;. And if you write it in Rust, you can even use the Rust bindings.&lt;/p&gt;&lt;p&gt;Rustc has a crate named &lt;code&gt;rustc_codegen_ssa&lt;/code&gt; which provides an abstract interface that a backend needs to implement through traits like:&lt;/p&gt;&lt;p&gt;The full list is available here.&lt;/p&gt;&lt;p&gt;One last thing you need to write in your backend:&lt;/p&gt;&lt;code&gt;Run #[no_mangle]
pub fn _rustc_codegen_backend() -&amp;gt; Box&amp;lt;dyn CodegenBackend&amp;gt; {
    // This is the entrypoint.
}&lt;/code&gt;
&lt;p&gt;This is the function that will be called by rustc to run your backend.&lt;/p&gt;&lt;p&gt;Let's take an example: how the GCC backend creates a constant string. I picked this one because it's small enough to showcase how things work while not being too much information to digest at once.&lt;/p&gt;&lt;p&gt;In the ConstCodegenMethods trait, there is a const_str method. This is the method we will implement to declare a constant string.&lt;/p&gt;&lt;p&gt;So the method implementation so far looks like this:&lt;/p&gt;&lt;code&gt;Run impl&amp;lt;'gcc, 'tcx&amp;gt; ConstCodegenMethods for CodegenCx&amp;lt;'gcc, 'tcx&amp;gt; {
    /// Returns the pointer to the string and its length.
    fn const_str(&amp;amp;self, s: &amp;amp;str) -&amp;gt; (RValue&amp;lt;'gcc&amp;gt;, RValue&amp;lt;'gcc&amp;gt;) {
        // Call GCC API to declare this string.
    }
}&lt;/code&gt;
&lt;p&gt;We need to pause here to give some extra explanations: &lt;code&gt;CodegenCx&lt;/code&gt; is the type on which most &lt;code&gt;rustc_codegen_ssa&lt;/code&gt; traits will be implemented. It is created in each ExtraBackendMethods::compile_codegen_unit call and passed down from there to generate the code for this module. You can consider it the same as a cache. It keeps the list of items declared, like functions, types, globals, etc. But also information such as "boolean type", "i8 type" and equivalents so we don't need to recompute them every time we need them.&lt;/p&gt;&lt;p&gt;Ok so now let's actually implement it. We have a few things to do:&lt;/p&gt;&lt;code&gt;*const u8&lt;/code&gt;) into the C type (&lt;code&gt;*const char&lt;/code&gt;).&lt;p&gt;Let's translate it into code with a lot of comments to help understanding what's going on:&lt;/p&gt;&lt;code&gt;Run fn const_str(&amp;amp;self, s: &amp;amp;str) -&amp;gt; (RValue&amp;lt;'gcc&amp;gt;, RValue&amp;lt;'gcc&amp;gt;) {
    // We get the const string cache.
    let mut const_str_cache = self.const_str_cache.borrow_mut();
    // We get the address of the stored string and we add it to the cache and
    // return its address.
    let str_global = const_str_cache.get(s).copied().unwrap_or_else(|| {
        // We call the `GCC` API to create a new const string.
        let string = self.context.new_string_literal(s);
        // We name the const.
        let sym = self.generate_local_symbol_name("str");
        // We declare it.
        let global = self.declare_private_global(&amp;amp;sym, self.val_ty(string));
        // All done, we can add it to the cache and return it.
        const_str_cache.insert(s.to_owned(), global);
        global
    });
    let len = s.len();
    // We cast the pointer to the target architecture string pointer type.
    let cs = self.const_ptrcast(
        str_global.get_address(None),
        self.type_ptr_to(self.layout_of(self.tcx.types.str_).gcc_type(self)),
    );
    // And we return the pointer and its length.
    (cs, self.const_usize(len as _))
}&lt;/code&gt;
&lt;p&gt;But the codegen backends can also add more information to the underlying binary code generator. For example, in Rust, we use references a lot. A reference is basically a pointer that cannot be &lt;code&gt;NULL&lt;/code&gt;. We need to give this information as well!&lt;/p&gt;&lt;p&gt;In both GCC and LLVM, you can add attributes to a lot of items, like arguments of functions. So every time we see an argument behind a reference, we add the &lt;code&gt;nonnnull()&lt;/code&gt; attribute.&lt;/p&gt;&lt;p&gt;Let's show an example with this Rust function:&lt;/p&gt;&lt;code&gt;Run fn t(a: &amp;amp;i32) -&amp;gt; i32 {
    *a
}&lt;/code&gt;
&lt;p&gt;The C equivalent looks like this:&lt;/p&gt;&lt;code&gt;int t(int *a) {
  if (!a) {
    return -1;
  }
  return *a;
}&lt;/code&gt;
&lt;p&gt;Compiled with the &lt;code&gt;-O3&lt;/code&gt; option, it generates this assembly:&lt;/p&gt;&lt;code&gt;t:
        test    rdi, rdi              ; Check if `a` is 0
        je      .L5                   ; If `a` is 0, we jump to `.L1`
        mov     eax, DWORD PTR [rdi]  ; We store `*a` value into `eax` registry
        ret                           ; We exit the function
.L5:
        mov     eax, -1               ; We store `-1` into `eax` registry
        ret                           ; We exit&lt;/code&gt;
&lt;p&gt;However, the Rust compiler knows that &lt;code&gt;a&lt;/code&gt; can never be &lt;code&gt;NULL&lt;/code&gt;, so the codegen adds &lt;code&gt;_attribute_((nonnull(1)))&lt;/code&gt; on the function:&lt;/p&gt;&lt;code&gt;_attribute_((nonnull(1)))
int t(int *a) {
  if (!a) {
    return -1;
  }
  return *a;
}&lt;/code&gt;
&lt;p&gt;Which generates this assembly:&lt;/p&gt;&lt;code&gt;t:
        mov     eax, DWORD PTR [rdi]
        ret&lt;/code&gt;
&lt;p&gt;Since the codegen knows that the &lt;code&gt;if (!a)&lt;/code&gt; condition will never be true, why keeping it around?&lt;/p&gt;&lt;p&gt;And it's just one example of extra information/optimization we do in the Rust backends. And that doesn't even cover in the slighest the monstruous amount of optimizations the codegen themselves do. If you want to have more examples of such optimizations, I strongly recommend reading the "Advent of Compiler Optimizations" blog posts written by Matt Godbolt (the developer of godbolt.org, another priceless tool).&lt;/p&gt;&lt;p&gt;So now you know what a Rust backend is, and why GCC backend is also an interesting thing to have while also learning about some optimizations we do behind developers back. :)&lt;/p&gt;&lt;p&gt;This blog post was made thanks to my cat hanging to it.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46288291</guid><pubDate>Tue, 16 Dec 2025 13:33:25 +0000</pubDate></item><item><title>I don't think Lindley's paradox supports p-circling</title><link>https://vilgot-huhn.github.io/mywebsite/posts/20251206_p_circle_lindley/</link><description>&lt;doc fingerprint="2456805317e68cd8"&gt;
  &lt;main&gt;
    &lt;p&gt;I don‚Äôt think Lindley‚Äôs paradox supports p-circling&lt;/p&gt;
    &lt;p&gt;hypothesis testing&lt;/p&gt;
    &lt;p&gt;Don‚Äôt give p-values a role they‚Äôre not made for&lt;/p&gt;
    &lt;p&gt;Author&lt;/p&gt;
    &lt;p&gt;Vilgot Huhn&lt;/p&gt;
    &lt;p&gt;Published&lt;/p&gt;
    &lt;p&gt;December 7, 2025&lt;/p&gt;
    &lt;p&gt;As usual I‚Äôd like to preface all this that I write these blogposts as attempts to make sense of a subject for my own sake. I am not an expert here and it is likely I am confused about some details. On the other hand, I think ‚Äúconfused‚Äù discourse can also be productive to read and participate in. Being confused is just the first step towards being unconfused, to paraphrase Jake The Dog.&lt;/p&gt;
    &lt;p&gt;p-value circling&lt;/p&gt;
    &lt;p&gt;100 years ago this year Fisher arbitrarily suggested using p &amp;lt; 0.05 as a cut-off for ‚Äúsignificant‚Äù and ever since we‚Äôve just gone along with it. ‚ÄúWhy is it 0.05?‚Äù people have critically asked for one hundred years. Unfortunately ‚Äúarbitrariness‚Äù, as a critique, is only effective if you are able to suggest less a arbitrary value, and despite many efforts to change this the convention has remained.&lt;/p&gt;
    &lt;p&gt;The act of p-value circling is to look at a p-value that‚Äôs significant but close to 0.05 and go: ‚Äúhm, I don‚Äôt know about that‚Ä¶‚Äù Perhaps you use a red ballpoint pen to circle it on the print journal you subscribe to in the year 2025. If not, you may underline it with some sort of digital pen technology and share it online.&lt;/p&gt;
    &lt;p&gt;‚ÄúHmm‚Ä¶ Suspicious‚Ä¶‚Äù&lt;/p&gt;
    &lt;p&gt;What (potentially) justifies p-value circling?&lt;/p&gt;
    &lt;p&gt;Before we get into it let‚Äôs briefly try to remind ourselves what p-values are even supposed to do. (This will be a brief summary, if you want to learn this for real I recommend reading Dani√´l Lakens free online textbook, which all this borrows heavily from.)&lt;/p&gt;
    &lt;p&gt;As far as I‚Äôve understood, Fishers idea about p-values was supplanted by the more rigorous (in terms of statistical philosophy) Neyman-Pearson framework. It is within this framework we find the familiar type 1 and type 2 error rates. Probability is viewed as being about outcomes in a hypothetical scenario where a procedure is repeated many times. You‚Äôre actually supposed to set the \(\alpha\) at a level that‚Äôs justifiable based on what null hypothesis you‚Äôre testing. As far as I‚Äôve understood no one has ever done so, except that one time physicists at CERN decided they wanted to be really sure they didn‚Äôt incorrectly claim they found the Higgs boson.1 Instead everyone just uses the arbitrary convention of \(\alpha = 0.05\).&lt;/p&gt;
    &lt;p&gt;If you assume that the null is true, the p-value distribution is uniform. Let‚Äôs do the exercise of generating a hundred thousand t-tests between two groups, n = 100 per group, where there is no mean difference. Then we‚Äôll look at the p-values.&lt;/p&gt;
    &lt;p&gt;Code&lt;/p&gt;
    &lt;p&gt;p_vector &amp;lt;-c() #empty vectorfor(i in1:100000){ A &amp;lt;-rnorm(100, 0, 1) #normally distributed data from a group B &amp;lt;-rnorm(100, 0, 1) #another group with same mean p_vector[i] &amp;lt;-t.test(A,B)$p.value}d &amp;lt;-data.frame(p_vector, significant = (p_vector &amp;lt;0.05))ggplot(d, aes(x = p_vector, fill = significant)) +geom_histogram(breaks =seq(0,1, length.out =101), color =NA) +#geom_hline(yintercept = 100000/100) +scale_fill_manual(values =c("FALSE"="steelblue", "TRUE"="red")) +scale_x_continuous(breaks =seq(0,1, by =0.1)) +labs(title ="p-values under the null", x ="p-value", y ="count") +theme_bw() +ylim(0,10000)&lt;/p&gt;
    &lt;p&gt;I remember this blew my mind when I fist saw it. I don‚Äôt think I was surprised exactly; it just made it all click. This flatness is what p-values are all about, man! The p-value distribution is uniform under the null! Yes! It is this property of the distribution that gives meaning to the type 1 error rate.&lt;/p&gt;
    &lt;p&gt;NP-frequentism is then based on committing to an alpha threshold beforehand and then exclaim ‚Äúsignificant!‚Äù iff the p-value lands below it.2&lt;/p&gt;
    &lt;p&gt;Does it matter how far below it?&lt;/p&gt;
    &lt;p&gt;No! If the null is true every p-value is equally likely, right? Your sampling procedure of a null may give you 0.98, or 0.58, or 0.002, or 0.006775892. When you‚Äôre focused on whether toreject the (exact) null hypothesis, NP-frequentism works its magic by assuming the null is true which means p=0.01 is not in and of itself less consistent with this assumption than a p=0.048. If the null was true, you just got handed a random number between 0 and 1. All you get to choose is often you want to mistakenly ‚Äúact‚Äù upon this information (in the long run).&lt;/p&gt;
    &lt;p&gt;So, how is this supposed to justify p-value circling? Well p-values are only valid if they‚Äôre valid. P-values only care about sampling error and ‚Äì being an inert mathematical abstraction ‚Äì can‚Äôt by themselves handle questionable research practices like p-hacking done by flesh and blood human researchers.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs set up a scenario where we collect more data if our p-value happens to be not significant, but stop collecting data if it is. We‚Äôll add a one-time option to include 20 more participants.&lt;/p&gt;
    &lt;p&gt;Code&lt;/p&gt;
    &lt;p&gt;p_vector &amp;lt;-c() #empty vectorfor(i in1:100000){ A &amp;lt;-rnorm(100, 0, 1) #normally distributed data from a group B &amp;lt;-rnorm(100, 0, 1) #another group with same meanif(t.test(A,B)$p.value &amp;gt;0.05){ A &amp;lt;-append(A, rnorm(20, 0, 1)) B &amp;lt;-append(B, rnorm(20, 0, 1)) } p_vector[i] &amp;lt;-t.test(A,B)$p.value}d &amp;lt;-data.frame(p_vector, significant = (p_vector &amp;lt;0.05))ggplot(d, aes(x = p_vector, fill = significant)) +geom_histogram(breaks =seq(0,1, length.out =101), color =NA) +geom_hline(yintercept =100000/100) +scale_fill_manual(values =c("FALSE"="steelblue", "TRUE"="red")) +scale_x_continuous(breaks =seq(0,1, by =0.1)) +labs(title ="p-values when p-hackning", x ="p-value", y ="count") +theme_bw() +ylim(0,10000)&lt;/p&gt;
    &lt;p&gt;As you can see there‚Äôs now a little bump in what was previously flat. Some ‚Äúmarginally significant‚Äù p-values got lucky and grew a bit. The bin between 0.05 and 0.06 shrunk the most. Notably there‚Äôs a tilted shape to the significant values now.3&lt;/p&gt;
    &lt;p&gt;It is this shape, this knowledge that you can fudge p-values a little bit that I think could maybe give some justification to the act of p-value circling. Maybe. In that case a p = 0.048 makes people think: ‚Äúhmm, I bet that was 0.051 and they strategically removed an outlier or something‚Äù&lt;/p&gt;
    &lt;p&gt;I don‚Äôt think it‚Äôs a very strong justification for being suspicious of p-values between 0.04 and 0.05. Part of this depends on how prevalent you believe p-hacking is. Basically you‚Äôre saying ‚Äúmy personal significance level is set a bit lower than convention, because I think the p-values I see are distorted by p-hacking‚Äù. I think that‚Äôs probably fine as an epistemic habit, but as an author getting p-circled it would likely feel as an unfair criticism.&lt;/p&gt;
    &lt;p&gt;sort(p_vector)[5000] #a suggestion, in this scenario&lt;/p&gt;
    &lt;p&gt;[1] 0.03800592&lt;/p&gt;
    &lt;p&gt;Even so, in this scenario I set up here there‚Äôs more additional spurious p-values below 0.04 than between 0.04 and 0.05. P-values are fickle things, they dance around, so even if you think the practice is common I don‚Äôt think you should put a lot of weight in the idea that whatever questionable statistical jutstu a researcher does to avoid a null result will put their (hacked) p-value just below the threshold.&lt;/p&gt;
    &lt;p&gt;Basically I don‚Äôt think a single p-value in and of itself can carry a lot of information about statistical malpractice. I‚Äôm sympathetic to the rule that if you‚Äôve ever scoffed at someone using the term ‚Äúmarginally significant‚Äù, you‚Äôre not allowed to call something ‚Äúmarginally insignificant‚Äù4.&lt;/p&gt;
    &lt;p&gt;Lindley‚Äôs paradox&lt;/p&gt;
    &lt;p&gt;A potentially more sophisticated justification for p-circling is ‚ÄúLindley‚Äôs paradox.‚Äù&lt;/p&gt;
    &lt;p&gt;I think many instinctively feel some resistance to a very strict interpretation of p-values, where their sole function is to be uniform under the null and the thing we care about is whether they clear our pre-specified alpha level. After all, we rightfully get annoyed when p-values are reported only with a less-than sign. And should we really not feel confident that there‚Äôs something there when we see a p = 0.001?&lt;/p&gt;
    &lt;p&gt;In this group-difference setup a smaller p-value implies a larger mean difference in your sample means, and you‚Äôre more likely to come across a large mean difference if you happen to be in a universe where there truly is a difference between the groups.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs see how it looks like when we have a difference between the groups.&lt;/p&gt;
    &lt;p&gt;Code&lt;/p&gt;
    &lt;p&gt;p_vector &amp;lt;-c() #empty vectorfor(i in1:100000){ A &amp;lt;-rnorm(100, 0, 1) #normally distributed data from a group B &amp;lt;-rnorm(100, 0.2, 1) #another group with same mean p_vector[i] &amp;lt;-t.test(A,B)$p.value}d &amp;lt;-data.frame(p_vector, significant = (p_vector &amp;lt;0.05))ggplot(d, aes(x = p_vector, fill = significant)) +geom_histogram(breaks =seq(0,1, length.out =101), color =NA) +geom_hline(yintercept =100000/100) +scale_fill_manual(values =c("FALSE"="steelblue", "TRUE"="red")) +scale_x_continuous(breaks =seq(0,1, by =0.1)) +labs(title =paste("p-values when power =",round(sum(p_vector &amp;lt;0.05)/100000, 2)), x ="p-value", y ="count") +theme_bw() #+ ylim(0,10000)&lt;/p&gt;
    &lt;p&gt;The sum of red colored p-values now represent power. Notice the switch! I think this may be a source of confusion here. We are now looking at a different type of p-value distribution. A p-value distribution that is not meant to illustrate the meaning of p-values. Power or type 2 error is, fundamentally, something else. It‚Äôs a different type of error.&lt;/p&gt;
    &lt;p&gt;When I fist saw this my mind immediately jumped to the idea of some p-values being ‚Äúmore consistent with the presence of an effect‚Äù. This is a bit off according to strict NP-frequentism; again, p-values get their meaning from assuming the null is true. Here we instead assume some effect is true.&lt;/p&gt;
    &lt;p&gt;The line represents where p-values would end up under the null. At this power, p-values in the 0.04 to 0.05 bin are more likely than they would be if the null was true. If we raise the power even further, we get to ‚ÄúLindley‚Äôs paradox‚Äù, the fact that p-values in this bin can be less likely then they are under the null.&lt;/p&gt;
    &lt;p&gt;Code&lt;/p&gt;
    &lt;p&gt;p_vector &amp;lt;-c() #empty vectorfor(i in1:100000){ A &amp;lt;-rnorm(100, 0, 1) #normally distributed data from a group B &amp;lt;-rnorm(100, 0.55, 1) #another group with same mean p_vector[i] &amp;lt;-t.test(A,B)$p.value}d &amp;lt;-data.frame(p_vector, significant = (p_vector &amp;lt;0.05))ggplot(d, aes(x = p_vector, fill = significant)) +geom_histogram(breaks =seq(0,1, length.out =101), color =NA) +geom_hline(yintercept =100000/100) +scale_fill_manual(values =c("FALSE"="steelblue", "TRUE"="red")) +scale_x_continuous(breaks =seq(0,1, by =0.1)) +labs(title =paste("p-values when power =",round(sum(p_vector &amp;lt;0.05)/100000, 2)), x ="p-value", y ="count") +theme_bw() #+ ylim(0,10000)&lt;/p&gt;
    &lt;p&gt;It‚Äôs kind of hard to see, since so many p-values end up in the 0-0.01 bin. Let‚Äôs zoom in on only the bins between 0.03 and 0.10.&lt;/p&gt;
    &lt;p&gt;Code&lt;/p&gt;
    &lt;p&gt;d &amp;lt;-subset(d, p_vector &amp;gt;0.03&amp;amp; p_vector &amp;lt;0.1)ggplot(d, aes(x = p_vector, fill = significant)) +geom_histogram(breaks =seq(0.03,0.1, length.out =8), color =NA) +geom_hline(yintercept =100000/100) +scale_fill_manual(values =c("FALSE"="steelblue", "TRUE"="red")) +scale_x_continuous(breaks =seq(0.03,0.1, by =0.1)) +labs(title =paste("p-values when power =",round(sum(p_vector &amp;lt;0.05)/100000, 2)), x ="p-value", y ="count") +theme_bw() #+ ylim(0,10000)&lt;/p&gt;
    &lt;p&gt;As you can see, the 0.04 to 0.05 bin is now below the line which represents the ideal flat null distribution. This then is (potentially) another reason to justify p-value circling: If a test has a lot of power, coming across a p-value close to threshold is surprising. Almost all p-values are to the left of it. We even had to zoom!&lt;/p&gt;
    &lt;p&gt;It is here, I think, we get to a second source of confusion. I‚Äôve noticed that a lot of psychologists think of power as N5. We think ‚Äúif we increase the sample size we increase power‚Äù. This is true, but we have to remind ourselves that power is always for a potential true effect. If there is no effect, increasing N can‚Äôt increase power.&lt;/p&gt;
    &lt;p&gt;I think it‚Äôs therefore more helpful (for the present discussion of p-value circling) to think of increasing power as ‚Äúincreasing which effect we assume to be true‚Äù. Stated that way, p-value circling based on Lindley‚Äôs paradox seems a bit strange, as if you‚Äôre saying:&lt;/p&gt;
    &lt;p&gt;‚ÄúThe p-value reported here would be rare if the true effect was such that we had ~97% power to detect it, which convinces me that the null is true.‚Äù&lt;/p&gt;
    &lt;p&gt;I don‚Äôt think that makes sense! Why are you assuming that particular true effect? Surely, there‚Äôs a potential true effect where the likelihood of the observed p-value is similar to its likelihood under the null? (For the 0.04 to 0.05 bin this appears to be around 95% power). Also, I think it unfairly imposes a role on p-values that they‚Äôve not been hired to play. P-values are not meant to be a measure of evidence ‚Äì not in that direct way at least. They are meant to give stable error rates when the null is true. This contrasting between the plausibility of seeing a p-value under a null hypothesis versus a specific alternative hypothesis isn‚Äôt what they were designed for.&lt;/p&gt;
    &lt;p&gt;Ok but it seems possible to use them that way? One could specify a smallest effect size of interest and compare the plausibility of seeing the reported p-value under that distribution compared to the null distribution.6Maier and Lakens (2022) suggest you could do this exercise when planning a test in order to justify your choice of alpha-level. However, I doubt this structured approach is what lies behind the casual circling of p-values I‚Äôve come across online over the years. My impression is that most social media p-circling haven‚Äôt been studies with very high power to detect small effects.&lt;/p&gt;
    &lt;p&gt;There is a concern that very large studies may pick up on ‚Äúnoise‚Äù, or that other violations of model assumptions (e.g. normality) tend to bias p-values downward. I don‚Äôt really know what to make of these concerns. I think that might be true for some model violations, while other may hurt power instead. I would assume it‚Äôs a complicated empirical question whether the statistical models we use tend to misfit reality more in one direction rather than the other.&lt;/p&gt;
    &lt;p&gt;Regardless, I don‚Äôt think it can be salvaged as a ground for being skeptical of p-values close to their threshold because of Lindley‚Äôs paradox.&lt;/p&gt;
    &lt;p&gt;For the moment I feel safest treating the conventional threshold as what it is, as arbitrary as that is. I‚Äôm of course concerned about QRPs and p-hacking, but I don‚Äôt see a reason for why a single p-value close to 0.05 would be useful evidence of it.&lt;/p&gt;
    &lt;p&gt;Some concluding thoughts&lt;/p&gt;
    &lt;p&gt;As I prefaced, this is complicated stuff and I have probably gotten something wrong. Regarding the larger question on whether p-values can be interpreted as evidence, I currently land in the conclusion ‚Äúnot in and of themselves‚Äù, they have to be contextualized in relation to power and other features of the study, as well as the context you come across them in. Lindley‚Äôs paradox can be a useful illustration of one of the reasons that the interpretation isn‚Äôt straight-forward (but I don‚Äôt think it justifies p-circling).&lt;/p&gt;
    &lt;p&gt;On the other hand, I know that smarter and more well-read people than I disagree on how straightforward this interpretation is. The textbook we used in my PhD-level course in medical statistics7 contain a table that tells us to do that:&lt;/p&gt;
    &lt;p&gt;I don‚Äôt think it‚Äôs quite that simple. My current understanding (given Lindley‚Äôs paradox) is that evidence has to be ‚Äúrelative‚Äù, in some sense. P-values only tell one side of the story, and are only made to tell one side of the story. If you want a statistic that expresses the strength of evidence you should probably use something else, e.g. Bayes factors.&lt;/p&gt;
    &lt;p&gt;Links etc:&lt;/p&gt;
    &lt;p&gt;I didn‚Äôt look up a lot of things for this post, it was more an attempt to think through my current understanding, but if you want to play around with visualizations of the p-value distribution I warmly recommend Kristoffer Magnussons interactive thingy: https://rpsychologist.com/d3/pdist/&lt;/p&gt;
    &lt;p&gt;See Daniel Lakens book for a real introduction to p-values. It‚Äôs there and the associated coursera course I learned about this stuff in the first place. I also always recommend Zoltan Dienes book Understanding Psychology As A Science (2008), which has also formed my understanding a lot.&lt;/p&gt;
    &lt;p&gt;See also Maier and Lakens (2022) for a structured approach to compromise between Lindeley‚Äôs paradox and the NP frequentist perspective.&lt;/p&gt;
    &lt;p&gt;Tell me I am wrong!&lt;/p&gt;
    &lt;p&gt;I am serious. If you think I‚Äôm misunderstanding something badly or you just want to discuss or you want to gently point me in the right direction: Please tell me what I‚Äôm missing. I don‚Äôt have a comment section on this blog, but I‚Äôll post this on bluesky and then update this post to link the post that links this post: Here is the link.&lt;/p&gt;
    &lt;p&gt;So if you want you can comment, do it over there, or send me an e-mail.&lt;/p&gt;
    &lt;p&gt;Footnotes&lt;/p&gt;
    &lt;p&gt;They set it to ‚Äúfive sigma‚Äù, or something like 0.00003.‚Ü©Ô∏é&lt;/p&gt;
    &lt;p&gt;The meaning of this exclamation is surely the matter of some philosophical debate which is way beyond me, but I think it goes without saying that you should probably not stop all thought and submit to the p every time you see a significant result according to NP-frequentism. Every test happens within a context.‚Ü©Ô∏é&lt;/p&gt;
    &lt;p&gt;This shape is the basis for p-curve analysis, which is an attempt to detect such bias in a sample of significant results. FYI: Recently this method has been criticized as having ‚Äúpoor statistical properties‚Äù. https://www.tandfonline.com/doi/full/10.1080/01621459.2025.2544397&lt;/p&gt;
    &lt;p&gt;I haven‚Äôt dug into the debate, but I don‚Äôt think it matters for the present discussion.‚Ü©Ô∏é&lt;/p&gt;
    &lt;p&gt;Or to be more fair, some combination of N, design, type of statistical analysis, and measurement. But I think most focus is on sample size.‚Ü©Ô∏é&lt;/p&gt;
    &lt;p&gt;I am unsure about whether you‚Äôre still even doing frequentism at this point. Maier &amp;amp; Lakens describe it as a ‚ÄúBayes-non-Bayes hybrid combining frequentist and Bayesian statistics‚Äù‚Ü©Ô∏é&lt;/p&gt;
    &lt;p&gt;Walters, Campbell &amp;amp; Machin (2021) Medical Statistics: A textbook for the health sciences, 5th ed‚Ü©Ô∏é&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46288351</guid><pubDate>Tue, 16 Dec 2025 13:40:02 +0000</pubDate></item><item><title>A brief history of Times New Roman</title><link>https://typographyforlawyers.com/a-brief-history-of-times-new-roman.html</link><description>&lt;doc fingerprint="2a1d815b1e9219b4"&gt;
  &lt;main&gt;
    &lt;p&gt;Times New Ro√Çman gets its name from the Times of Lon√Çdon, the British news√Çpa√Çper. In 1929, the Times hired ty√Çpog√Çra√Çpher Stan√Çley Mori√Çson to cre√Çate a new text font. Mori√Çson led the project, su√Çper√Çvis√Çing Vic√Çtor Lar√Çdent, an ad√Çver√Çtis√Çing artist for the Times, who drew the letterforms.&lt;/p&gt;
    &lt;p&gt;Even when new, Times New Ro√Çman had its crit√Çics. In his ty√Çpo√Çgraphic mem√Çoir, &lt;/p&gt;
    &lt;p&gt;Be√Çcause it was used in a daily news√Çpa√Çper, the new font quickly be√Çcame pop√Çu√Çlar among print√Çers of the day. In the decades since, type√Çset√Çting de√Çvices have evolved, but Times New Ro√Çman has al√Çways been one of the first fonts avail√Çable for each new de√Çvice (in√Çclud√Çing per√Çsonal com√Çput√Çers). This, in turn, has only in√Çcreased its reach.&lt;/p&gt;
    &lt;p&gt;Ob√Çjec√Çtively, there√¢s noth√Çing wrong with Times New Ro√Çman. It was de√Çsigned for a news√Çpa√Çper, so it√¢s a bit nar√Çrower than most text fonts√¢es√Çpe√Çcially the bold style. (News√Çpa√Çpers pre√Çfer nar√Çrow fonts be√Çcause they fit more text per line.) The italic is mediocre. But those aren√¢t fa√Çtal flaws. Times New Ro√Çman is a work√Çhorse font that√¢s been suc√Çcess√Çful for a reason.&lt;/p&gt;
    &lt;p&gt;Yet it√¢s an open ques√Çtion whether its longevity is at√Çtrib√Çut√Çable to its qual√Çity or merely its ubiq√Çuity. Hel√Çvetica still in√Çspires enough af√Çfec√Çtion to have been the sub√Çject of a 2007 doc√Çu√Çmen√Çtary fea√Çture. Times New Ro√Çman, mean√Çwhile, has not at√Çtracted sim√Çi√Çlar acts of homage.&lt;/p&gt;
    &lt;p&gt;Why not? Fame has a dark side. When Times New Ro√Çman ap√Çpears in a book, doc√Çu√Çment, or ad√Çver√Çtise√Çment, it con√Çnotes ap√Ça√Çthy. It says,&lt;/p&gt;
    &lt;p&gt;This is how Times New Ro√Çman ac√Çcrued its rep√Çu√Çta√Çtion as the de√Çfault font of the le√Çgal pro√Çfes√Çsion√¢it√¢s the de√Çfault font of every√Çthing. As a re√Çsult, many law√Çyers er√Çro√Çneously as√Çsume that courts de√Çmand 12-point Times New Ro√Çman. In fact, I√¢ve never found one that does. (But there is one no√Çtable court that for√Çbids it√¢see court opin√Çions.) In gen√Çeral, law√Çyers keep us√Çing it not be√Çcause they must, but be√Çcause it√¢s fa√Çmil√Çiar and en√Çtrenched√¢much like those ob√Çso√Çlete type√Çwriter habits.&lt;/p&gt;
    &lt;p&gt;If you have a choice about us√Çing Times New Ro√Çman, please stop. You have plenty of bet√Çter al√Çter√Çna√Çtives√¢whether it√¢s a dif√Çfer√Çent sys√Çtem font or one of the many pro√Çfes√Çsional fonts shown in this chapter.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46288414</guid><pubDate>Tue, 16 Dec 2025 13:46:55 +0000</pubDate></item><item><title>40 percent of fMRI signals do not correspond to actual brain activity</title><link>https://www.tum.de/en/news-and-events/all-news/press-releases/details/40-percent-of-mri-signals-do-not-correspond-to-actual-brain-activity</link><description>&lt;doc fingerprint="253b519799b3cda2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why blood flow is not a reliable indicator of the brain's energy requirements&lt;/head&gt;
    &lt;p&gt;40 percent of MRI signals do not correspond to actual brain activity&lt;/p&gt;
    &lt;p&gt;Researchers at the Technical University of Munich (TUM) and the Friedrich-Alexander-University Erlangen-Nuremberg (FAU) found that an increased fMRI signal is associated with reduced brain activity in around 40 percent of cases. At the same time, they observed decreased fMRI signals in regions with elevated activity. First author Dr. Samira Epp emphasizes: ‚ÄúThis contradicts the long-standing assumption that increased brain activity is always accompanied by an increased blood flow to meet higher oxygen demand. Since tens of thousands of fMRI studies worldwide are based on this assumption, our results could lead to opposite interpretations in many of them.‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;Test tasks reveal deviations from the standard interpretation&lt;/head&gt;
    &lt;p&gt;PD Dr. Valentin Riedl, now Professor at FAU, and his colleague Epp examined more than 40 healthy participants during their time at TUM. Each was given several experimental tasks ‚Äì such as mental arithmetic or autobiographical memory recall ‚Äì which are known to produce predictable fMRI signal changes in distributed brain regions. During these experiments, the researchers simultaneously measured the actual oxygen consumption using a novel quantitative MRI technique.&lt;lb/&gt; Depending on the task and the brain region, the physiological results varied. Increased oxygen consumption ‚Äì for instance in areas involved in calculation ‚Äì did not coincide with the expected rise in blood flow. Instead, the quantitative analyses showed that these regions met their additional energy demand by extracting more oxygen from the unchanged blood supply. Thus, they used the oxygen available in the blood more efficiently without requiring greater perfusion.&lt;/p&gt;
    &lt;head rend="h2"&gt;Implications for interpreting brain disorders&lt;/head&gt;
    &lt;p&gt;According to Riedl, these insights also affect the interpretation of research findings in brain disorders: ‚ÄúMany fMRI studies on psychiatric or neurological diseases ‚Äì from depression to Alzheimer‚Äôs ‚Äì interpret changes in blood flow as a reliable signal of neuronal under- or over-activation. Given the limited validity of such measurements, this must now be reassessed. Especially in patient groups with vascular changes ‚Äì for instance due to aging or vascular disease ‚Äì the measured values may primarily reflect vascular differences rather than neuronal deficits.‚Äù Previous animal studies already point in this direction.&lt;lb/&gt; The researchers therefore propose complementing the conventional MRI approach with quantitative measurements. In the long term, this combination could form the basis for energy-based brain models: rather than showing activation maps that depend on assumptions about blood flow, future analyses could display values indicating how much oxygen ‚Äì and therefore energy ‚Äì is actually consumed for information processing. This opens new perspectives for examining aging, psychiatric, or neurodegenerative diseases in terms of absolute changes in energy metabolism ‚Äì and for understanding them more accurately.&lt;/p&gt;
    &lt;p&gt;Samira M. Epp, Gabriel Castrill√≥n, Beijia Yuan, Jessica Andrews-Hanna, Christine Preibisch, Valentin Riedl: BOLD signal changes can oppose oxygen metabolism across the human cortex, published in Nature Neuroscience, December 12, 2025, https://doi.org/10.1038/s41593-025-02132-9&lt;/p&gt;
    &lt;p&gt;The research was conducted at the Neuro-Head Center of the Institute of Neuroradiology at the TUM University Hospital. It was funded by the European Research Council through an ERC Starting Grant.&lt;/p&gt;
    &lt;p&gt;Technical University of Munich&lt;/p&gt;
    &lt;p&gt;Corporate Communications Center&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ulrich Meyer&lt;/item&gt;
      &lt;item&gt;presse @tum.de&lt;/item&gt;
      &lt;item&gt;Teamwebsite&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Contacts to this article:&lt;/p&gt;
    &lt;p&gt;Prof. Dr. Valentin Riedl&lt;lb/&gt; Research Fellow&lt;lb/&gt; Technical University of Munich&lt;lb/&gt; TUM University Hospital ‚Äì Neuro-Head Center&lt;lb/&gt; valentin.riedl @tum.de&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46288415</guid><pubDate>Tue, 16 Dec 2025 13:46:57 +0000</pubDate></item><item><title>Mozilla appoints new CEO Anthony Enzor-Demeo</title><link>https://blog.mozilla.org/en/mozilla/leadership/mozillas-next-chapter-anthony-enzor-demeo-new-ceo/</link><description>&lt;doc fingerprint="306af159ffcf6d4c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mozilla‚Äôs next chapter: Building the world‚Äôs most trusted software company&lt;/head&gt;
    &lt;p&gt;Today, I step into the role of CEO of Mozilla Corporation. It is a privilege to lead an organization with a long history of standing up for people and building technology that puts them first. The internet is changing fast, and so are the expectations people bring to the products they use every day. Mozilla has a critical role to play at this moment.&lt;/p&gt;
    &lt;p&gt;I want to thank Laura Chambers for her exceptional leadership. As interim CEO, Laura led Mozilla through a defining moment in the web‚Äôs history ‚Äî navigating AI‚Äôs arrival, a major antitrust case, double-digit mobile growth in Firefox, and the early success of our revenue diversification strategy. She brought clarity, stability, and focus to the organization, and I‚Äôm grateful for her leadership through this transition and am glad she‚Äôll continue to be part of Mozilla, returning to her role on the Mozilla board of directors.&lt;/p&gt;
    &lt;p&gt;When I joined Mozilla, it was clear that trust was going to become the defining issue in technology and the browser would be where this battle would play out. AI was already reshaping how people search, shop, and make decisions in ways that were hard to see and even harder to understand. I saw how easily people could lose their footing in experiences that feel personal but operate in ways that are anything but clear. And I knew this would become a defining issue, especially in the browser, where so many decisions about privacy, data, and transparency now originate.&lt;/p&gt;
    &lt;p&gt;People want software that is fast, modern, but also honest about what it does. They want to understand what‚Äôs happening and to have real choices.&lt;/p&gt;
    &lt;p&gt;Mozilla and Firefox can be that choice.&lt;/p&gt;
    &lt;p&gt;Few companies share our strengths. People trust our brand. Firefox brings us global reach. Our teams know how to build reliable, independent software at scale, and our business model puts the user first.&lt;/p&gt;
    &lt;p&gt;As Mozilla moves forward, we will focus on becoming the trusted software company. This is not a slogan. It is a direction that guides how we build and how we grow. It means three things.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;First: Every product we build must give people agency in how it works. Privacy, data use, and AI must be clear and understandable. Controls must be simple. AI should always be a choice ‚Äî something people can easily turn off. People should know why a feature works the way it does and what value they get from it.&lt;/item&gt;
      &lt;item&gt;Second: our business model must align with trust. We will grow through transparent monetization that people recognize and value.&lt;/item&gt;
      &lt;item&gt;Third: Firefox will grow from a browser into a broader ecosystem of trusted software. Firefox will remain our anchor. It will evolve into a modern AI browser and support a portfolio of new and trusted software additions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We will measure our progress against a double bottom line. Our work must advance our mission and succeed in the market. In the next three years, that means investing in AI that reflects the Mozilla Manifesto. It means diversifying revenue beyond search.&lt;/p&gt;
    &lt;p&gt;Success means Firefox grows across generations. Mozilla builds new revenue engines. Our principles become a differentiator.&lt;/p&gt;
    &lt;p&gt;We will move with urgency. AI is changing software. Browsers are becoming the control point for digital life. Regulation is shifting defaults. These shifts play to Mozilla‚Äôs strengths.&lt;/p&gt;
    &lt;p&gt;If we stay focused, Mozilla will grow in relevance and resilience. Firefox will reach new audiences. Our portfolio will strengthen our independence. Our approach to building trusted software will set a high standard for the industry.&lt;/p&gt;
    &lt;p&gt;Mozilla is ready for this moment. I am excited for the work ahead and grateful for the trust placed in me.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46288491</guid><pubDate>Tue, 16 Dec 2025 13:53:14 +0000</pubDate></item><item><title>AIsbom ‚Äì open-source CLI to detect "Pickle Bombs" in PyTorch models</title><link>https://github.com/Lab700xOrg/aisbom</link><description>&lt;doc fingerprint="9475979452d2782"&gt;
  &lt;main&gt;
    &lt;p&gt;AIsbom is a specialized security and compliance scanner for Machine Learning artifacts.&lt;/p&gt;
    &lt;p&gt;Unlike generic SBOM tools that only parse &lt;code&gt;requirements.txt&lt;/code&gt;, AIsbom performs Deep Binary Introspection on model files (&lt;code&gt;.pt&lt;/code&gt;, &lt;code&gt;.pkl&lt;/code&gt;, &lt;code&gt;.safetensors&lt;/code&gt;) to detect malware risks and legal license violations hidden inside the serialized weights.&lt;/p&gt;
    &lt;p&gt;Install directly from PyPI. No cloning required.&lt;/p&gt;
    &lt;code&gt;pip install aisbom-cli&lt;/code&gt;
    &lt;p&gt;Note: The package name is aisbom-cli, but the command you run is aisbom.&lt;/p&gt;
    &lt;p&gt;Point it at any directory containing your ML project. It will find requirements files AND binary model artifacts.&lt;/p&gt;
    &lt;code&gt;aisbom scan ./my-project-folder&lt;/code&gt;
    &lt;p&gt;You will see a combined Security &amp;amp; Legal risk assessment in your terminal:&lt;/p&gt;
    &lt;p&gt;üß† AI Model Artifacts Found&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Filename&lt;/cell&gt;
        &lt;cell role="head"&gt;Framework&lt;/cell&gt;
        &lt;cell role="head"&gt;Security Risk&lt;/cell&gt;
        &lt;cell role="head"&gt;Legal Risk&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;bert_finetune.pt&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;PyTorch&lt;/cell&gt;
        &lt;cell&gt;üî¥ CRITICAL (RCE Detected: posix.system)&lt;/cell&gt;
        &lt;cell&gt;UNKNOWN&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;safe_model.safetensors&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;SafeTensors&lt;/cell&gt;
        &lt;cell&gt;üü¢ LOW (Binary Safe)&lt;/cell&gt;
        &lt;cell&gt;UNKNOWN&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;restricted_model.safetensors&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;SafeTensors&lt;/cell&gt;
        &lt;cell&gt;üü¢ LOW&lt;/cell&gt;
        &lt;cell&gt;LEGAL RISK (cc-by-nc-4.0)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;A compliant &lt;code&gt;sbom.json&lt;/code&gt; (CycloneDX v1.6) including SHA256 hashes and license data will be generated in your current directory.&lt;/p&gt;
    &lt;p&gt;Don't like reading JSON? You can visualize your security posture using our offline viewer.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Run the scan.&lt;/item&gt;
      &lt;item&gt;Go to aisbom.io/viewer.html.&lt;/item&gt;
      &lt;item&gt;Drag and drop your &lt;code&gt;sbom.json&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Get an instant dashboard of risks, license issues, and compliance stats.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: The viewer is client-side only. Your SBOM data never leaves your browser.&lt;/p&gt;
    &lt;p&gt;AI models are not just text files; they are executable programs and IP assets.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Security Risk: PyTorch (&lt;code&gt;.pt&lt;/code&gt;) files are Zip archives containing Pickle bytecode. A malicious model can execute arbitrary code (RCE) instantly when loaded.&lt;/item&gt;
      &lt;item&gt;The Legal Risk: A developer might download a "Non-Commercial" model (CC-BY-NC) and deploy it to production. Since the license is hidden inside the binary header, standard tools miss it.&lt;/item&gt;
      &lt;item&gt;Pickle files can execute arbitrary code (RCE) instantly upon loading.&lt;/item&gt;
      &lt;item&gt;The Solution: Legacy scanners look at requirements.txt manifest files but ignore binary model weights. We look inside. We decompile the bytecode headers without loading the heavy weights into RAM.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üß† Deep Introspection: Peeks inside PyTorch Zip structures and Safetensors headers without loading weights into RAM.&lt;/item&gt;
      &lt;item&gt;üí£ Pickle Bomb Detector: Disassembles bytecode to detect &lt;code&gt;os.system&lt;/code&gt;,&lt;code&gt;subprocess&lt;/code&gt;, and&lt;code&gt;eval&lt;/code&gt;calls before they run.&lt;/item&gt;
      &lt;item&gt;‚öñÔ∏è License Radar: Extracts metadata from .safetensors to flag restrictive licenses (e.g., CC-BY-NC, AGPL) that threaten commercial use.&lt;/item&gt;
      &lt;item&gt;üõ°Ô∏è Compliance Ready: Generates standard CycloneDX v1.6 JSON for enterprise integration (Dependency-Track, ServiceNow).&lt;/item&gt;
      &lt;item&gt;‚ö° Blazing Fast: Scans GB-sized models in milliseconds by reading headers only and using streaming hash calculation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Security tools require trust. To maintain a safe repository, we do not distribute malicious binaries. However, AIsbom includes a built-in generator so you can create safe "test dummies" to verify the scanner works.&lt;/p&gt;
    &lt;p&gt;1. Install:&lt;/p&gt;
    &lt;code&gt;pip install aisbom-cli&lt;/code&gt;
    &lt;p&gt;2. Generate Test Artifacts: Run this command to create a fake "Pickle Bomb" and a "Restricted License" model in your current folder.&lt;/p&gt;
    &lt;code&gt;# Generate a mock Pickle Bomb (Security Risk) and a mock Non-Commercial Model (Legal Risk)
aisbom generate-test-artifacts&lt;/code&gt;
    &lt;p&gt;Result: Files named mock_malware.pt and mock_restricted.safetensors are created.&lt;/p&gt;
    &lt;p&gt;3. Scan it:&lt;/p&gt;
    &lt;code&gt;# You can use your globally installed aisbom, or poetry run aisbom
aisbom scan .&lt;/code&gt;
    &lt;p&gt;You will see the scanner flag mock_malware.pt as CRITICAL and mock_restricted.safetensors as a LEGAL RISK.&lt;/p&gt;
    &lt;p&gt;AIsbom uses a static analysis engine to disassemble Python Pickle opcodes. It looks for specific &lt;code&gt;GLOBAL&lt;/code&gt; and &lt;code&gt;STACK_GLOBAL&lt;/code&gt; instructions that reference dangerous modules:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;os / posix (System calls)&lt;/item&gt;
      &lt;item&gt;subprocess (Shell execution)&lt;/item&gt;
      &lt;item&gt;builtins.eval / exec (Dynamic code execution)&lt;/item&gt;
      &lt;item&gt;socket (Network reverse shells)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Add AIsbom to your CI/CD pipeline to block unsafe models before they merge.&lt;/p&gt;
    &lt;code&gt;name: AI Security Scan
on: [pull_request]

jobs:
  aisbom-scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Scan AI Models
        uses: Lab700xOrg/aisbom@v0
        with:
          directory: '.'&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46290113</guid><pubDate>Tue, 16 Dec 2025 15:55:45 +0000</pubDate></item><item><title>Devs say Apple still flouting EU's Digital Markets Act six months on</title><link>https://www.theregister.com/2025/12/16/apple_dma_complaint/</link><description>&lt;doc fingerprint="a6e8f90eb3331370"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Devs say Apple still flouting EU's Digital Markets Act six months on&lt;/head&gt;
    &lt;head rend="h2"&gt;Coalition for App Fairness warns App Store fees remain unlawful despite non-compliance ruling&lt;/head&gt;
    &lt;p&gt;Six months after EU regulators found Apple's App Store rules in breach of the Digital Markets Act (DMA), developers say Cupertino is still behaving as if compliance were optional.&lt;/p&gt;
    &lt;p&gt;The Coalition for App Fairness, a nonprofit organization of app developers and consumer groups, has accused Apple of persistent non-compliance with the DMA, warning that the company's revised App Store terms continue to impose fees which the legislation prohibits.&lt;/p&gt;
    &lt;p&gt;In an open letter addressed to European Commission President Ursula von der Leyen and senior commissioners, the coalition argues that Apple has failed to deliver "any meaningful changes or proposals" despite an April 2025 non-compliance decision that found its App Store policies illegal and harmful to both developers and consumers.&lt;/p&gt;
    &lt;p&gt;At the heart of the complaint is money. The DMA requires so-called gatekeepers to allow developers to offer and conduct transactions outside their app stores without charge. Apple, the coalition claims, is seeking to charge commissions of up to 20 percent on those very transactions.&lt;/p&gt;
    &lt;p&gt;"This is a blatant disregard for the law with the potential to vanquish years of meaningful work by the Commission," the letter states, accusing Apple of preserving the economics of its App Store while nominally claiming compliance.&lt;/p&gt;
    &lt;p&gt;Apple has said it will roll out new App Store terms in January 2026, but developers say the company has provided no clarity on what those changes will involve or whether they will actually comply with the DMA.&lt;/p&gt;
    &lt;p&gt;"We have seen this playbook before in Europe and beyond," the signatories warn, adding that they suspect any new terms will continue to impose fees that would violate the law.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Brussels eyes AWS, Azure for gatekeeper tag in cloud clampdown&lt;/item&gt;
      &lt;item&gt;Apple, Google tell Europe its Digital Markets Act isn't working for them ‚Äì or consumers&lt;/item&gt;
      &lt;item&gt;Google tweaks Play Store fees to keep Euro watchdogs at bay&lt;/item&gt;
      &lt;item&gt;Apple tries to get ‚Ç¨500M EU fine tossed&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The letter argues that this uncertainty is already doing damage. Six months after Apple's last App Store terms update, developers still do not know which rules will govern their businesses or what their costs will look like in the near term.&lt;/p&gt;
    &lt;p&gt;Apple's "lack of transparency in tandem with its rushed timelines," the coalition says, is freezing investment and innovation, effectively allowing the company to "exploit its gatekeeper position by holding the entire industry hostage."&lt;/p&gt;
    &lt;p&gt;The group also points to a growing transatlantic contrast that makes Europe look like the tougher regulator with the weaker results. While Apple continues to fight DMA enforcement in the EU, US courts have moved to curb its ability to extract fees from external transactions. Following litigation brought by Epic Games, developers in the US can now communicate freely with customers about pricing and offer payment options outside Apple's ecosystem without paying commission.&lt;/p&gt;
    &lt;p&gt;That raises what the coalition calls a "simple and urgent question." Why should European developers and consumers get a worse deal than their US counterparts, especially when the EU was first to pass a landmark law aimed at fixing digital markets? The letter argues that meaningful enforcement of the DMA would strengthen Europe's digital competitiveness and attract global investment, while weak enforcement risks turning the regulation into an expensive paper exercise.&lt;/p&gt;
    &lt;p&gt;"We trust the Commission will uphold the DMA," the signatories conclude, but they warn that if enforcement falls short, they will continue pressing policymakers to ensure Apple finally complies with the law as written ‚Äì not as rewritten by Cupertino. ¬Æ&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46290320</guid><pubDate>Tue, 16 Dec 2025 16:09:48 +0000</pubDate></item><item><title>Show HN: Zenflow ‚Äì orchestrate coding agents without "you're right" loops</title><link>https://zencoder.ai/zenflow</link><description>&lt;doc fingerprint="6dd6ea771501f0fb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Zenflow is built for AI-first engineering teams&lt;/head&gt;
    &lt;p&gt;Meet the orchestration platform for reliable AI development. Spec-driven workflows, built-in verification, and coordinated multi-agent execution.&lt;/p&gt;
    &lt;head rend="h3"&gt;Download Zenflow&lt;/head&gt;
    &lt;p&gt;Get the app and product updates.&lt;/p&gt;
    &lt;head rend="h3"&gt;Your download is starting!&lt;/head&gt;
    &lt;p&gt;If the download doesn't start automatically, check your browser's download folder.&lt;/p&gt;
    &lt;head rend="h3"&gt;Something went wrong&lt;/head&gt;
    &lt;p&gt;Please try again or contact support.&lt;/p&gt;
    &lt;head rend="h3"&gt;Get notified for Windows&lt;/head&gt;
    &lt;p&gt;Zenflow is coming to Windows soon. Enter your email and we'll let you know when it's ready.&lt;/p&gt;
    &lt;head rend="h3"&gt;You're on the list!&lt;/head&gt;
    &lt;p&gt;We'll notify you when Zenflow is available for Windows.&lt;/p&gt;
    &lt;head rend="h3"&gt;Something went wrong&lt;/head&gt;
    &lt;p&gt;Please try again or contact support.&lt;/p&gt;
    &lt;p&gt;Trusted by engineers at&lt;/p&gt;
    &lt;head rend="h2"&gt;AI-first Engineering without AI slop&lt;/head&gt;
    &lt;p&gt;Repeatable processes produce consistent quality. Zenflow's pre-built workflows run on autopilot or with human review. Extend them or build your own.&lt;/p&gt;
    &lt;head rend="h3"&gt;Spec-Driven Workflows&lt;/head&gt;
    &lt;p&gt;Agents reference a single source of truth‚Äîyour PRDs, specs, or architecture docs. Planning, implementation, and validation stay aligned. No drift, no scope creep.&lt;/p&gt;
    &lt;head rend="h3"&gt;Parallel Execution&lt;/head&gt;
    &lt;p&gt;Multiple tasks run simultaneously in isolated environments. Features, refactors, and fixes progress at the same time while agents coordinate within each workflow.&lt;/p&gt;
    &lt;head rend="h3"&gt;Built-in Verification&lt;/head&gt;
    &lt;p&gt;Every workflow includes automated testing, cross-agent review, and quality gates. AI output is validated before you see it. Verification isn't optional‚Äîit's structural.&lt;/p&gt;
    &lt;p&gt;HOW IT WORKS&lt;/p&gt;
    &lt;head rend="h2"&gt;Describe, Build, Ship&lt;/head&gt;
    &lt;head rend="h3"&gt;Describe the Work&lt;/head&gt;
    &lt;p&gt;Tell Zenflow what to build. Pick a workflow‚Äîfeature, bug fix, refactor, or Auto. Use built-in templates or create custom patterns.&lt;/p&gt;
    &lt;head rend="h3"&gt;AI-Guided Development&lt;/head&gt;
    &lt;p&gt;Agents execute while you supervise or step away. Built-in verification runs automatically. Cross-agent review catches issues.&lt;/p&gt;
    &lt;head rend="h3"&gt;Automated Quality Gates&lt;/head&gt;
    &lt;p&gt;Tests, builds, and security checks complete before you review. Only verified, production-ready code reaches your queue.&lt;/p&gt;
    &lt;head rend="h3"&gt;Parallel Execution&lt;/head&gt;
    &lt;p&gt;Run multiple tasks simultaneously without conflicts. Each task gets isolated environment. Track everything through kanban view.&lt;/p&gt;
    &lt;head rend="h2"&gt;AI doesn't need better prompts. It needs orchestration.&lt;/head&gt;
    &lt;p&gt;Hear our CEO explain why Zenflow anchors AI to three engineering pillars‚Äîturning unpredictable agent behavior into production-grade software.&lt;/p&gt;
    &lt;p&gt;Features&lt;/p&gt;
    &lt;head rend="h2"&gt;Built for Scale&lt;/head&gt;
    &lt;p&gt;Orchestration, workflows, and intelligence designed for production engineering‚Äînot prototypes&lt;/p&gt;
    &lt;head rend="h3"&gt;Multi-Agent Orchestration&lt;/head&gt;
    &lt;p&gt;Coordinated swarm of specialized agents‚Äîcoding, testing, refactoring, review, verification‚Äîworking as one system with shared context.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pre-Built + Custom Workflows&lt;/head&gt;
    &lt;p&gt;Battle-tested workflows for features, bugs, and refactors. Run on autopilot or with human review. Extend or build your own.&lt;/p&gt;
    &lt;head rend="h3"&gt;AI-First Project Management&lt;/head&gt;
    &lt;p&gt;Projects, tasks and kanban views show what every agent is doing. Complete visibility across your entire AI fleet.&lt;/p&gt;
    &lt;head rend="h3"&gt;Multi-Repo Intelligence&lt;/head&gt;
    &lt;p&gt;Agents work across multiple repositories with full context. Understand service interactions, dependencies, and coordinate changes across your stack.&lt;/p&gt;
    &lt;head rend="h2"&gt;Frequently asked questions&lt;/head&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dd-1"&gt; Zenflow (by Zencoder) is an orchestration platform for AI-first engineering that replaces ad-hoc prompting with Spec-Driven Development workflows. It has agents draft/review specs, enforces RED/GREEN/VERIFY implementation loops, coordinates multiple AI agents in parallel, and bakes in automated verification so every change stays aligned with the approved spec. Teams use it to eliminate ‚Äúprompt drift‚Äù and AI slop, keep multi-agent work debuggable, and ship complex features 4‚Äì10√ó faster with predictable quality.&lt;/item&gt;
      &lt;item rend="dd-2"&gt; Zenflow allows for seamless parallelization of agents across your project without any conflicts. Agents work in independent environments that doesn't affect main codebase. That allows you to run tens or hundreds of agents meaning tens or hundreds of new feature or bug fixes being implemented simultaneously.&lt;/item&gt;
      &lt;item rend="dd-3"&gt; You are in full control - review agent's work yourself or with another AI agent and steer it in the right direction during or after each step; opt out to fully autonomous working agent by automatically starting next tasks; or define your own custom workflows.&lt;/item&gt;
      &lt;item rend="dd-4"&gt; Zenflow is a standalone app, however you can open each agent's sandbox in your favourite IDE and continue the work there.&lt;/item&gt;
      &lt;item rend="dd-5"&gt; A typical flow looks like this:&lt;list rend="ol"&gt;&lt;item&gt;You define a spec or goal in Zenflow&lt;/item&gt;&lt;item&gt;Zenflow analyzes the task, plans the work, and assigns it to the right agents&lt;/item&gt;&lt;item&gt;Zencoder agents research the codebase, implement changes, write tests, and review results&lt;/item&gt;&lt;item&gt;Zenflow verifies outcomes and coordinates retries or refinements&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item rend="dd-6"&gt;
        &lt;p&gt;Think of Zenflow as the brain and Zencoder as the engine.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Zenflow (the brain) decides what needs to be done, in what order, and how to verify it.&lt;/item&gt;
          &lt;item&gt;Zencoder (the engine) does the actual work‚Äîunderstanding the codebase, writing code, running tests, fixing issues, and shipping changes.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;FEATURES&lt;/p&gt;
    &lt;head rend="h2"&gt;What You Get&lt;/head&gt;
    &lt;head rend="h3"&gt;Parallel Project Execution&lt;/head&gt;
    &lt;p&gt;Built-in verification means no waiting. Start the next task while agents finish the current one. Scale from one agent to your entire fleet.&lt;/p&gt;
    &lt;head rend="h3"&gt;AI-First Project Management&lt;/head&gt;
    &lt;p&gt;Projects, tasks, subtasks, kanban views, and inbox make AI work visible and organized. Clear picture across all your agents.&lt;/p&gt;
    &lt;head rend="h3"&gt;Auto-Generated implementation plan&lt;/head&gt;
    &lt;p&gt;Workflows break into sequential tasks automatically. Ready for autopilot or human review. Less babysitting, more producing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Agent Diversity ??&lt;/head&gt;
    &lt;p&gt;Different model families (Claude, GPT, etc.) challenge each other's assumptions and catch blind spots. Reduces errors, increases correctness.&lt;/p&gt;
    &lt;p&gt;WHO IT'S FOR&lt;/p&gt;
    &lt;head rend="h2"&gt;Built for AI-First Coders&lt;/head&gt;
    &lt;head rend="h3"&gt;Senior Engineers&lt;/head&gt;
    &lt;p&gt;You own the architecture. Agents handle the implementation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Technical Vibecoders&lt;/head&gt;
    &lt;p&gt;PMs, designers, and builders who think in outcomes, not syntax.&lt;/p&gt;
    &lt;head rend="h3"&gt;Forward-Looking Teams&lt;/head&gt;
    &lt;p&gt;Standardize AI workflows. Guarantee quality. Ship faster, safer.&lt;/p&gt;
    &lt;p&gt;WHAT TEAMS SAY&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46290617</guid><pubDate>Tue, 16 Dec 2025 16:32:16 +0000</pubDate></item><item><title>alpr.watch</title><link>https://alpr.watch/</link><description>&lt;doc fingerprint="ade4b2d61bc2e379"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;alpr.watch&lt;/head&gt;
    &lt;p&gt;Your local government might be discussing surveillance tech like Flock cameras, facial recognition, or automated license plate readers right now. This map helps you find those meetings and take action.&lt;/p&gt;
    &lt;p&gt;Why this matters: Municipalities across the US are quietly adopting surveillance technologies in rapidly growing numbers with over 80,000 cameras already out on the streets. These systems track residents' movements, collect biometric data, and build massive databases of our daily lives.&lt;/p&gt;
    &lt;p&gt;alpr.watch scans meeting agendas for keywords like "flock," "license plate reader," "alpr," and more. Each pin on the map shows where these conversations are happening so that you can make a difference.&lt;/p&gt;
    &lt;p&gt;Enter your email below and we'll send you a login link. After logging in, you can set your notification preferences.&lt;/p&gt;
    &lt;head rend="h3"&gt;Statistics&lt;/head&gt;
    &lt;head rend="h2"&gt;Understanding Mass Surveillance&lt;/head&gt;
    &lt;head rend="h3"&gt;What is ALPR?&lt;/head&gt;
    &lt;p&gt;Automated License Plate Recognition (ALPR) systems use cameras and artificial intelligence to capture, read, and store license plate data from every passing vehicle.&lt;/p&gt;
    &lt;p&gt;These systems work 24/7 creating a massive database of where vehicles, and by extension, people, travel. Every trip to the grocery store, doctor's office, or place of worship gets recorded and stored.&lt;/p&gt;
    &lt;head rend="h3"&gt;What is Flock Safety?&lt;/head&gt;
    &lt;p&gt;Flock Safety is one of the largest manufacturers of ALPR cameras in the United States, marketing their systems to neighborhoods and law enforcement.&lt;/p&gt;
    &lt;p&gt;Flock cameras capture license plates, vehicle make/model, color, and other identifying features. This data is shared across a massive network of agencies and jurisdictions, creating a surveillance web that tracks millions of Americans.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Slippery Slope&lt;/head&gt;
    &lt;p&gt;History shows that surveillance systems expand beyond their original scope:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Systems marketed for "solving crimes" get used for immigration enforcement&lt;/item&gt;
      &lt;item&gt;Temporary programs become permanent infrastructure&lt;/item&gt;
      &lt;item&gt;Data sharing agreements grow to include more agencies&lt;/item&gt;
      &lt;item&gt;Technology advances enable new invasive uses&lt;/item&gt;
      &lt;item&gt;Regulations and oversight consistently lag behind deployment&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Organizations Fighting for Your Privacy&lt;/head&gt;
    &lt;p&gt;These groups and individuals are leading the fight against mass surveillance. Consider supporting their work or getting involved locally.&lt;/p&gt;
    &lt;p&gt;Leading nonprofit defending digital privacy and civil liberties. eff.org&lt;/p&gt;
    &lt;p&gt;Fighting surveillance overreach through litigation and advocacy nationwide. aclu.org&lt;/p&gt;
    &lt;p&gt;Digital rights organization mobilizing grassroots opposition to surveillance. fightforthefuture.org&lt;/p&gt;
    &lt;p&gt;Litigating against invasive surveillance in New York and beyond. stopspying.org&lt;/p&gt;
    &lt;p&gt;This civil liberties law firm has filed lawsuits challenging the constitutionality of Flock's mass, warrantless surveillance ij.org&lt;/p&gt;
    &lt;p&gt;Check for privacy advocacy organizations in your area fighting surveillance at the local level.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46290916</guid><pubDate>Tue, 16 Dec 2025 16:54:19 +0000</pubDate></item><item><title>Artie (YC S23) Is Hiring Senior Enterprise AES</title><link>https://www.ycombinator.com/companies/artie/jobs/HyaHWUs-senior-enterprise-ae</link><description>&lt;doc fingerprint="75ca0291f783e116"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h2"&gt;About Artie&lt;/head&gt;
      &lt;p&gt;Artie is a fully-managed change data capture (CDC) streaming platform that replicates production databases into data warehouses and lakes - in real time, with zero maintenance. We make high-volume data replication simple, reliable, and scalable for engineering teams.&lt;/p&gt;
      &lt;p&gt;Our platform powers mission-critical use cases including fraud and risk monitoring, inventory visibility, customer-facing analytics, and AI/ML workloads.&lt;/p&gt;
      &lt;p&gt;We‚Äôre trusted by teams like Substack, Alloy, and ClickUp, and backed by top-tier investors including Y Combinator, General Catalyst, Pathlight, and the founders of Dropbox and Mode.&lt;/p&gt;
      &lt;p&gt;Artie is built for engineers who care about performance, reliability, and operational simplicity - and we‚Äôre growing fast. This role is your chance to shape our GTM from the ground up.&lt;/p&gt;
      &lt;head rend="h2"&gt;About the Role&lt;/head&gt;
      &lt;p&gt;We‚Äôre hiring our first Senior Enterprise AEs to help scale Artie‚Äôs sales motion.&lt;/p&gt;
      &lt;p&gt;This is not a ‚Äúrun the playbook‚Äù role. You‚Äôll refine the playbook - defining what great full-cycle enterprise sales looks like for a deeply technical platform. You‚Äôll partner directly with founders, influence product strategy, and set the bar for future AEs.&lt;/p&gt;
      &lt;head rend="h2"&gt;What you‚Äôll do&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Run full-cycle enterprise sales &lt;list rend="ul"&gt;&lt;item&gt;Own deals end-to-end: sourcing, qualification, discovery, demos, POCs, procurement, and closing&lt;/item&gt;&lt;item&gt;Navigate 6-12 month cycles with engineering, data, security, finance, and legal stakeholders&lt;/item&gt;&lt;item&gt;Multi-thread deeply and build strong, technical champions&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;item&gt;Drive technical discovery &amp;amp; solutioning &lt;list rend="ul"&gt;&lt;item&gt;Understand customer architectures and pain points (SQL Server, Postgres, MySQL, Kafka, Snowflake, VPCs, networking)&lt;/item&gt;&lt;item&gt;Map these to Artie‚Äôs capabilities with clarity and confidence&lt;/item&gt;&lt;item&gt;Whiteboard solutions with staff engineers and data architects&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;item&gt;Lead rigorous evaluations &lt;list rend="ul"&gt;&lt;item&gt;Build structured POC plans&lt;/item&gt;&lt;item&gt;Partner with engineering to define success criteria&lt;/item&gt;&lt;item&gt;Drive toward clear, mutual action plans&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;item&gt;Source your own pipeline &lt;list rend="ul"&gt;&lt;item&gt;There is no SDR team - you source 80%+ of pipeline through outbound, events, founder referrals, and creative prospecting&lt;/item&gt;&lt;item&gt;Use your curiosity and rigor to start real technical conversations&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;item&gt;Sell the vision and business value (not features) &lt;list rend="ul"&gt;&lt;item&gt;Translate deep technical concepts (e.g. log-based CDC, Kafka buffering) into ROI, TCO reduction, and platform reliability&lt;/item&gt;&lt;item&gt;Inspire our buyers with how Artie can unlock new opportunities for their company&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h2"&gt;What we‚Äôre looking for&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Enterprise sales mastery: &lt;list rend="ul"&gt;&lt;item&gt;5+ years of full cycle AE experience in enterprise or upper/mid-market&lt;/item&gt;&lt;item&gt;Consistent track record of closing $100-300K+ ACV deals&lt;/item&gt;&lt;item&gt;Comfortable navigating 6-12 month cycles and complex procurement&lt;/item&gt;&lt;item&gt;Proven experience run full-cycle deals without SDR or SE support&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;item&gt;Deep technical fluency: &lt;list rend="ul"&gt;&lt;item&gt;Experience selling dev tools, data infra, or cloud platforms to technical buyers&lt;/item&gt;&lt;item&gt;Able to confidently explain concepts like log-based CDC, Kafka, schema evolution, or cloud networking basics (VPCs, peering, security reviews)&lt;/item&gt;&lt;item&gt;Comfortable whiteboarding with staff-level engineers and technical stakeholders&lt;/item&gt;&lt;item&gt;Fluent discussing database, streaming, and cloud architecture concepts with engineering teams&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;item&gt;Outbound-first mindset: &lt;list rend="ul"&gt;&lt;item&gt;Self-sourced a significant portion of pipeline in previous roles&lt;/item&gt;&lt;item&gt;Consistent and persistent prospector who finds creative ways to engage prospects&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;item&gt;Extreme ownership: &lt;list rend="ul"&gt;&lt;item&gt;Never drop the ball - internally or externally&lt;/item&gt;&lt;item&gt;Drives alignment, writes structured follow-ups, and runs mutual action plans&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;item&gt;Consultative and curious: &lt;list rend="ul"&gt;&lt;item&gt;Asks layered questions, uncovers deep pain, and guides prospects to clarity and value&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;item&gt;Collaborative and self-aware: &lt;list rend="ul"&gt;&lt;item&gt;Knows when to pull in help - from CTO deep dives to executive alignment with CEO&lt;/item&gt;&lt;item&gt;Works closely with teammates to win complex deals together&lt;/item&gt;&lt;item&gt;Willing to work in-person, 5 days/week at our SF office (relocation covered)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h2"&gt;What you‚Äôll get&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;A Seat at the Table: Join as a founding GTM member with real influence on company direction.&lt;/item&gt;
        &lt;item&gt;End-to-End Ownership: Drive the full GTM lifecycle‚Äîstrategy, execution, and iteration.&lt;/item&gt;
        &lt;item&gt;Tight Product Loop: Collaborate closely with product and leadership to shape what we build and why.&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h2"&gt;Compensation &amp;amp; Benefits&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;$150-175K base salary ($300-350K OTE) depending on experience&lt;/item&gt;
        &lt;item&gt;Equity: ~0.1%&lt;/item&gt;
        &lt;item&gt;Healthcare, 401(k), unlimited PTO&lt;/item&gt;
        &lt;item&gt;Lunch &amp;amp; dinner provided&lt;/item&gt;
        &lt;item&gt;Visa sponsorship available&lt;/item&gt;
      &lt;/list&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46291011</guid><pubDate>Tue, 16 Dec 2025 17:00:57 +0000</pubDate></item><item><title>Pricing Changes for GitHub Actions</title><link>https://resources.github.com/actions/2026-pricing-changes-for-github-actions/</link><description>&lt;doc fingerprint="dcc5a5e34daa6ee0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Pricing changes for GitHub Actions&lt;/head&gt;
    &lt;p&gt;December 15, 2025 // 6 min read&lt;/p&gt;
    &lt;p&gt;Today we‚Äôre announcing updates to our pricing and product models for GitHub Actions.&lt;/p&gt;
    &lt;p&gt;Why?&lt;/p&gt;
    &lt;p&gt;When we shipped Actions in 2018, we had no idea how popular it would become. By early 2024, the platform was running about 23 million jobs per day and our existing architecture couldn‚Äôt reliably support our growth curve. In order to increase feature velocity, we first needed to improve reliability and modernize the legacy frameworks that supported GitHub Actions.&lt;/p&gt;
    &lt;p&gt;Our solution was to re-architect the core backend services powering GitHub Actions jobs and runners with the goals of improving uptime and resilience against infrastructure issues, enhancing performance, reducing internal throttles, and leveraging GitHub‚Äôs broader platform investments and developer experience improvements. This work is paying off by helping us handle our current scale, even as we work through the last pieces of stabilizing our new platform.&lt;/p&gt;
    &lt;p&gt;Since August, all GitHub Actions jobs have run on our new architecture, which handles 71 million jobs per day (over 3x from where we started). Individual enterprises are able to start 7x more jobs per minute than our previous architecture could support.&lt;/p&gt;
    &lt;p&gt;As with any product, our goal at GitHub has been to meet customer needs while providing enterprises with flexibility and transparency.&lt;/p&gt;
    &lt;p&gt;This change better supports a world where CI/CD must be faster and more reliable, better caching, more workflow flexibility, rock-solid reliability, and strengthens the core experience while positioning GitHub Actions to power GitHub‚Äôs open, secure platform for agentic workload.&lt;/p&gt;
    &lt;head rend="h2"&gt;What‚Äôs changing?&lt;/head&gt;
    &lt;head rend="h3"&gt;Lower prices for GitHub-hosted runners&lt;/head&gt;
    &lt;p&gt;Starting today, we‚Äôre charging fairly for Actions across the board which reduces the price of GitHub Hosted Runners and the price the average GitHub customer pays. And we‚Äôre reducing the net cost of GitHub-hosted runners by up to 39%, depending on which machine type is used.&lt;/p&gt;
    &lt;p&gt;This reduction is driven by a ~40% price reduction across all runner sizes, paired with the addition of a new $0.002 per-minute GitHub Actions cloud platform charge. For GitHub-hosted runners, the new Actions cloud platform charge is already included into the reduced meter price.&lt;/p&gt;
    &lt;p&gt;Standard GitHub-hosted or self-hosted runner usage on public repositories will remain free. GitHub Enterprise Server pricing is not impacted by this change.&lt;/p&gt;
    &lt;p&gt;The price reduction you will see in your account depends on the types of machines that you use most frequently ‚Äì smaller runners will have a smaller relative price reduction, larger runners will see a larger relative reduction.&lt;/p&gt;
    &lt;p&gt;This price reduction makes high-performance compute more accessible for both high-volume CI workloads and the agent jobs that rely on fast, secure execution environments.&lt;/p&gt;
    &lt;p&gt;For full pricing update details, see the updated Actions runner prices in our documentation.&lt;/p&gt;
    &lt;p&gt;This price change will go into effect on January 1, 2026.&lt;/p&gt;
    &lt;head rend="h3"&gt;Introduction of the GitHub Actions cloud platform charge&lt;/head&gt;
    &lt;p&gt;We are introducing a $0.002 per-minute Actions cloud platform charge for all Actions workflows across GitHub-hosted and self-hosted runners. The new listed GitHub-runner rates include this charge. This will not impact Actions usage in public repositories or GitHub Enterprise Server customers.&lt;/p&gt;
    &lt;p&gt;This aligns pricing to match consumption patterns and ensures consistent service quality as usage grows across both hosting modalities.&lt;/p&gt;
    &lt;p&gt;This will impact self-hosted runner pricing starting March 1, 2026.&lt;/p&gt;
    &lt;head rend="h3"&gt;Deepened investment in the Actions self-hosted experience&lt;/head&gt;
    &lt;p&gt;We are increasing our investment into our self-hosted experience to ensure that we can provide autoscaling for scenarios beyond just Linux containers. This will include new approaches to scaling, new platform support, Windows support, and more as we move through the next 12 months. Here‚Äôs a preview of what to expect in the new year:&lt;/p&gt;
    &lt;head rend="h4"&gt;GitHub Scale Set Client&lt;/head&gt;
    &lt;p&gt;This new client provides enterprises with a lightweight Go SDK to build custom autoscaling solutions without the complexity of Kubernetes or reliance on ARC. It integrates seamlessly with existing infrastructure‚Äîcontainers, virtual machines, cloud instances, or bare metal‚Äîwhile managing job queuing, secure configuration, and intelligent scaling logic. Customers gain a supported path to implement flexible autoscaling, reduce setup friction, and extend GitHub Actions beyond workflows to scenarios such as self-hosted Dependabot and Copilot Coding Agent.&lt;/p&gt;
    &lt;head rend="h4"&gt;Multi-label support&lt;/head&gt;
    &lt;p&gt;We are reintroducing multi-label functionality for both GitHub-hosted larger runners and self-hosted runners, including those managed by Actions Runner Controller (ARC) and the new Scale Set Client.&lt;/p&gt;
    &lt;head rend="h4"&gt;Actions Runner Controller 0.14.0&lt;/head&gt;
    &lt;p&gt;This upcoming release introduces major quality-of-life improvements, including refined Helm charts for easier Docker configuration, enhanced logging, updated metrics, and formalized versioning requirements. It also announces the deprecation of legacy ARC, providing a clear migration path to a more reliable and maintainable architecture. Customers benefit from simplified setup, improved observability, and confidence in long-term support, reducing operational friction and improving scalability.&lt;/p&gt;
    &lt;head rend="h4"&gt;Actions Data Stream&lt;/head&gt;
    &lt;p&gt;The Actions Data Stream will deliver a near real-time, authoritative feed of GitHub Actions workflow and job event data, including metadata such as the version of the action that was executed on any given workflow run. This capability enhances observability and troubleshooting by enabling organizations to integrate event data into monitoring and analytics systems for compliance and operational insights. By providing structured, high-fidelity data at scale, it eliminates reliance on manual log parsing and empowers teams to proactively manage reliability and performance.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why this matters&lt;/head&gt;
    &lt;p&gt;Agents are expanding what teams can automate‚Äîbut CI/CD remains the heartbeat of modern software delivery. These updates enable both a faster, more reliable CI/CD experience for every developer, and a scalable, flexible, secure execution layer to power GitHub‚Äôs agentic platform.&lt;/p&gt;
    &lt;p&gt;Our goal is to ensure GitHub Actions continues to meet the needs of the largest enterprises and of individual developers alike, with clear pricing, stronger performance, and a product direction built for the next decade of software development.&lt;/p&gt;
    &lt;head rend="h2"&gt;FAQ&lt;/head&gt;
    &lt;p&gt;What are the new GitHub-hosted runner rates?&lt;lb/&gt;See the GitHub Actions runner pricing reference for the updated rates that will go into effect on January 1, 2026. These listed rates include the new $0.002 per-minute Actions cloud platform charge.&lt;/p&gt;
    &lt;p&gt;Which job execution scenarios for GitHub Actions are affected by this pricing change?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Jobs that run in private repositories and use standard GitHub-hosted or self-hosted runners&lt;/item&gt;
      &lt;item&gt;Any jobs running on larger GitHub-hosted runners&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Standard GitHub-hosted or self-hosted runner usage on public repositories will remain free. GitHub Enterprise Server pricing is not impacted by this change.&lt;/p&gt;
    &lt;p&gt;When will this pricing change take effect?&lt;/p&gt;
    &lt;p&gt;The price decrease for GitHub-hosted runners will take effect on January 1, 2026. The new charge for self-hosted runners will apply beginning on March 1, 2026. The price changes will impact all customers on these dates.&lt;/p&gt;
    &lt;p&gt;Will the free usage quota available in my plan change?&lt;lb/&gt;Beginning March 1, 2026, self-hosted runners will be included within your free usage quota, and will consume available usage based on list price the same way that Linux, Windows, and MacOS standard runners work today.&lt;/p&gt;
    &lt;p&gt;Will self-hosted runner usage consume from my free usage minutes?&lt;lb/&gt;Yes, billable self-hosted runner usage will be able to consume minutes from the free quota associated with your plan.&lt;/p&gt;
    &lt;p&gt;How does this pricing change affect customers on GitHub Enterprise Server?&lt;/p&gt;
    &lt;p&gt;This pricing change does not affect customers using GitHub Enterprise Server. Customers running Actions jobs on self-hosted runners on GitHub Enterprise Server may continue to host, manage, troubleshoot and use Actions on and in conjunction with their implementation free of charge.&lt;/p&gt;
    &lt;p&gt;Can I bill my self-hosted runner usage on private repositories through Azure?&lt;/p&gt;
    &lt;p&gt;Yes, as long as you have an active Azure subscription ID associated with your GitHub Enterprise or Organization(s).&lt;/p&gt;
    &lt;p&gt;What is the overall impact of this change to GitHub customers?&lt;/p&gt;
    &lt;p&gt;Of Actions users impacted by this change, 85% will see their Actions bill decrease. Of the 15% who are impacted across all cohorts the median increase is $13.&lt;/p&gt;
    &lt;p&gt;Did GitHub consider how this impacts individual developers, not just Enterprise scale customers of GitHub?&lt;lb/&gt;From our individual users (free &amp;amp; Pro plans) of those who used GitHub Actions in the last month in private repos only 0.09% would end up with a price increase, with a median increase of under $2 a month. Note that this impact is after these users have made use of their included minutes in their plans today, entitling them to over 33 hours of included GitHub compute, and this has no impact on their free use of public repos. A further 2.8% of this total user base will see a decrease in their monthly cost as a result of these changes. The rest are unimpacted by this change.&lt;/p&gt;
    &lt;p&gt;How can I figure out what my new monthly cost for Actions looks like?&lt;/p&gt;
    &lt;p&gt;GitHub Actions provides detailed usage reports for the current and prior year. You can use this prior usage alongside the rate changes that will be introduced in January and March to estimate cost under the new pricing structure. We have created a Python script to help you leverage full usage reports to calculate your expected cost after the price updates.&lt;/p&gt;
    &lt;p&gt;We have also updated our Actions pricing calculator, making it easier to estimate your future costs, particularly if your historical usage is limited or not representative of expected future usage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Additional resources&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;See the GitHub Actions runner pricing documentation for the new GitHub-hosted runner rates effective January 1, 2026.&lt;/item&gt;
      &lt;item&gt;For more details on upcoming GitHub Actions releases, see the GitHub public roadmap.&lt;/item&gt;
      &lt;item&gt;For help estimating your expected Actions usage cost, use the newly updated Actions pricing calculator.&lt;/item&gt;
      &lt;item&gt;To see your current or historical Actions usage, see our documentation for viewing and downloading detailed usage reports.&lt;/item&gt;
      &lt;item&gt;If you are interested in moving existing self-hosted runner usage to GitHub-hosted runners, see the SHR to GHR migration guide in our documentation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Tags&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46291156</guid><pubDate>Tue, 16 Dec 2025 17:12:02 +0000</pubDate></item><item><title>FVWM-95</title><link>https://fvwm95.sourceforge.net/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46291172</guid><pubDate>Tue, 16 Dec 2025 17:13:07 +0000</pubDate></item><item><title>GitHub will begin charging for self-hosted action runners on March 2026</title><link>https://github.blog/changelog/2025-12-16-coming-soon-simpler-pricing-and-a-better-experience-for-github-actions/</link><description>&lt;doc fingerprint="9ec5346f6faa42e1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Coming soon: Simpler pricing and a better experience for GitHub Actions&lt;/head&gt;
    &lt;p&gt;On January 1, 2026, GitHub will reduce the price of GitHub-hosted runners by up to 39% depending on the machine type used. The free usage minute quotas will remain the same.&lt;/p&gt;
    &lt;p&gt;On March 1, 2026, GitHub will introduce a new $0.002 per minute GitHub Actions cloud platform charge that will apply to self-hosted runner usage. Any usage subject to this charge will count toward the minutes included in your plan, as explained in our GitHub Actions billing documentation.&lt;/p&gt;
    &lt;p&gt;Runner usage in public repositories will remain free. There will be no changes in price structure for GitHub Enterprise Server customers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Deeper investment in the Actions self-hosted experience&lt;/head&gt;
    &lt;p&gt;We are increasing our investment into our self-hosted experience to ensure that we can provide autoscaling for scenarios beyond just Linux containers. This will include new approaches to scaling, new platform support, Windows support, and more as we move through the next 12 months.&lt;/p&gt;
    &lt;p&gt;For more details about the product investments we‚Äôre making in Actions, please visit our Executive Insights page.&lt;/p&gt;
    &lt;head rend="h3"&gt;Recommended resources&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For answers to common questions about this change, see the FAQ in our post on GitHub‚Äôs Executive Insights page.&lt;/item&gt;
      &lt;item&gt;See the GitHub Actions runner pricing documentation for the new GitHub-hosted runner rates effective January 1, 2026.&lt;/item&gt;
      &lt;item&gt;For more details on upcoming GitHub Actions releases, see the GitHub public roadmap.&lt;/item&gt;
      &lt;item&gt;For help estimating your expected Actions usage cost, use the newly updated Actions pricing calculator.&lt;/item&gt;
      &lt;item&gt;If you are interested in moving existing self-hosted runner usage to GitHub-hosted runners, see the SHR to GHR migration guide in our documentation.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46291414</guid><pubDate>Tue, 16 Dec 2025 17:32:38 +0000</pubDate></item><item><title>The GitHub Actions control plane is no longer free</title><link>https://www.blacksmith.sh/blog/actions-pricing</link><description>&lt;doc fingerprint="9c4420694dfa4cc6"&gt;
  &lt;main&gt;
    &lt;p&gt;GitHub just announced changes to Actions pricing. Previously, GitHub Actions had a free control plane. That meant if you used GitHub Actions but ran jobs outside of GitHub-hosted runners, whether that√¢s on Blacksmith, on your own machines, or in your own AWS account, you paid nothing to GitHub for those minutes; you only paid for the compute.&lt;/p&gt;
    &lt;p&gt;With this change, GitHub is introducing a $0.002 per-minute platform fee for all GitHub Actions usage.&lt;/p&gt;
    &lt;p&gt;In practice, this means CI costs now have two components:&lt;/p&gt;
    &lt;p&gt;These changes go into effect on March 1st, 2026.&lt;/p&gt;
    &lt;p&gt;GitHub Actions has long had a graduation churn problem. As companies grow, their CI workloads become larger, more complex, and more expensive. At a certain scale, GitHub-hosted runners become both slow and costly, pushing teams to self-host or move to third-party runners like Blacksmith.&lt;/p&gt;
    &lt;p&gt;Until now, that shift had an important side effect: companies could continue using the GitHub Actions control plane while paying GitHub nothing for CI execution. GitHub provided scheduling, orchestration, and workflow automation, but captured no revenue from some of its largest and fastest-growing customers.&lt;/p&gt;
    &lt;p&gt;The new per-minute platform fee changes that. It directly monetizes the Actions control plane and establishes a floor on what GitHub earns from CI, regardless of where jobs run. In effect, self-hosting is no longer free.&lt;/p&gt;
    &lt;p&gt;At the same time, GitHub reduced the price of GitHub-hosted runners. This isn√¢t accidental. Lower hosted runner prices make GitHub-hosted runners more attractive, while the platform fee introduces a new, unavoidable cost for self-hosting.&lt;/p&gt;
    &lt;p&gt;From GitHub√¢s perspective, this is a rational move. Most Actions usage is concentrated on smaller runners, so the hosted runner price cuts likely don√¢t materially impact revenue. More importantly, GitHub is trading lower-margin compute revenue for higher-margin platform revenue.&lt;/p&gt;
    &lt;p&gt;Hosted runners are fundamentally a compute business. The platform fee, by contrast, monetizes software without scaling infrastructure costs linearly. As CI usage grows, that revenue scales with significantly better unit economics.&lt;/p&gt;
    &lt;p&gt;In the past, our customers have asked us how GitHub views third-party runners long-term. The platform fee largely answers that: GitHub now monetizes Actions usage regardless of where jobs run, aligning third-party runners like Blacksmith as ecosystem partners rather than workarounds.&lt;/p&gt;
    &lt;p&gt;Before this change, self-hosting was a way to avoid paying GitHub entirely. That√¢s no longer true. Now, self-hosting retains the operational burden of running CI infrastructure while still incurring per-minute charges on GitHub.&lt;/p&gt;
    &lt;p&gt;At that point, the primary variable you can still control is how many minutes your CI jobs consume. One approach is to run CI on infrastructure designed to minimize wall-clock time and eliminate redundant work. That√¢s the problem Blacksmith focuses on. In practice, this shows up in a few areas:&lt;/p&gt;
    &lt;p&gt;With a per-minute platform fee, CI performance and cost are tightly coupled. The remaining lever is reducing CI time and total Actions.&lt;/p&gt;
    &lt;p&gt;√¢&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46291500</guid><pubDate>Tue, 16 Dec 2025 17:37:34 +0000</pubDate></item><item><title>AI is wiping out entry-level tech jobs, leaving graduates stranded</title><link>https://restofworld.org/2025/engineering-graduates-ai-job-losses/</link><description>&lt;doc fingerprint="360030939a8f1d0c"&gt;
  &lt;main&gt;
    &lt;p&gt;In 2022, Rishabh Mishra joined a high-ranking engineering college in India‚Äôs Jabalpur with the most predictable dream in global tech: study computer science, write code, and one day make it to Silicon Valley.&lt;/p&gt;
    &lt;p&gt;Three years later, Mishra faces a sobering reality.&lt;/p&gt;
    &lt;p&gt;Artificial intelligence has gutted entry-level roles in the tech industry that Mishra and his classmates were counting on. Among his 400 classmates at the Indian Institute of Information Technology, Design and Manufacturing, fewer than 25% have secured job offers. His course ends in May 2026, and there‚Äôs a sense of panic on the campus.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt is really bad out there,‚Äù Mishra told Rest of World. ‚ÄúEveryone is so panicked ‚Äî even our juniors. As the degree end nears, the anxiety is heightened among all of us.‚Äù Some of his classmates are exploring the option of pursuing higher studies before entering the job market. ‚ÄúBut after one year, if you return to the job market, your degree is even more irrelevant,‚Äù he said.&lt;/p&gt;
    &lt;p&gt;Students at engineering colleges in India, China, Dubai, and Kenya are facing a ‚Äújobpocalypse‚Äù as artificial intelligence replaces humans in entry-level roles. Tasks once assigned to fresh graduates, such as debugging, testing, and routine software maintenance, are now increasingly automated.&lt;/p&gt;
    &lt;p&gt;Over the last three years, the number of fresh graduates hired by big tech companies globally has declined by more than 50%, according to a report published by SignalFire, a San Francisco-based venture capital firm. Even though hiring rebounded slightly in 2024, only 7% of new hires were recent graduates. As many as 37% of managers said they‚Äôd rather use AI than hire a Gen Z employee.&lt;/p&gt;
    &lt;p&gt;‚ÄúAs demand for junior roles declines, even highly credentialed engineering graduates are struggling to break into tech, especially at the Big Tech companies,‚Äù the report said.&lt;/p&gt;
    &lt;p&gt;Indian IT services companies have reduced entry-level roles by 20%‚Äì25% thanks to automation and AI, consulting firm EY said in a report last month. Job platforms like LinkedIn, Indeed, and Eures noted a 35% decline in junior tech positions across major EU countries during 2024.&lt;/p&gt;
    &lt;p&gt;The World Economic Forum‚Äôs Future of Jobs Report 2025 warned that 40% of employers expect to reduce staff where AI can automate tasks.&lt;/p&gt;
    &lt;p&gt;‚ÄúFive years ago, there was a real war for [coders and developers]. There was bidding to hire,‚Äù and 90% of the hires were for off-the-shelf technical roles, or positions that utilize ready-made technology products rather than requiring in-house development, said Vahid Haghzare, director at IT hiring firm Silicon Valley Associates Recruitment in Dubai.&lt;/p&gt;
    &lt;p&gt;Since the rise of AI, ‚Äúit has dropped dramatically,‚Äù he said. ‚ÄúI don‚Äôt even think it‚Äôs touching 5%. It‚Äôs almost completely vanished.‚Äù The company headhunts workers from multiple countries including China, Singapore, and the U.K.&lt;/p&gt;
    &lt;p&gt;While high-paying jobs at coveted brands like Apple, Microsoft, Amazon, and Meta rarely cross his desk these days, companies that hire recent engineering graduates expect them to execute ‚Äúadditional responsibilities,‚Äù like managing a project or leading sales. ‚ÄúThey have to face the customer and have customer communications and maybe even do some selling,‚Äù Haghzare said.&lt;/p&gt;
    &lt;p&gt;Some engineering students have realigned their ambitions to meet such demands from employers. Nishant Kaushik, who studied computer science at another well-ranked college in eastern India, has decided to look for roles in sales or marketing.&lt;/p&gt;
    &lt;p&gt;The rise of AI has also rendered engineering degrees less relevant: Workplace demands now differ from what is taught in colleges.&lt;/p&gt;
    &lt;p&gt;When Rita Sande Lukale enrolled in an electronics engineering program at the Technical University of Kenya in 2021, she hoped to land a role in the system architecture sector after graduation. Over the past few years, however, she has seen such roles disappear.&lt;/p&gt;
    &lt;p&gt;Entry-level jobs such as handling data logging, system diagnostics, or code writing have been replaced by AI, Lukale told Rest of World. Now, fresh graduates ‚Äúmust possess higher-level skills, necessary to understand algorithms and use the engineering judgement to troubleshoot the complex and automated systems,‚Äù she said.&lt;/p&gt;
    &lt;p&gt;While she doesn‚Äôt consider AI to be a ‚Äújob destroyer,‚Äù it has fundamentally changed the type of engineers that companies need to hire, Lukale said. She feels she needs to adapt and learn more to land a job.&lt;/p&gt;
    &lt;p&gt;Not only are fresh graduates expected to understand and use the latest tools efficiently, ‚Äúthey are asked to up their output by 70% because ‚Äòthey are using AI,‚Äô‚Äù Liam Fallon, who heads the product division at GoodSpace AI, an AI-powered recruiting company, told Rest of World. As a result, students face a rapidly changing industry that requires them to upskill outside the curriculum on their own. Experts believe universities are unable to align their academic practices fast enough to meet AI-driven industry demands.&lt;/p&gt;
    &lt;p&gt;The current system, where a student commits three to five years to learn computer science and then looks for a job, is ‚Äúnot sustainable,‚Äù Haghzare said. Students are ‚Äúfalling down a hole, and they don‚Äôt know how to get out of it.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46291504</guid><pubDate>Tue, 16 Dec 2025 17:37:41 +0000</pubDate></item><item><title>Too Fast to Think: The Hidden Fatigue of AI Vibe Coding</title><link>https://www.tabulamag.com/p/too-fast-to-think-the-hidden-fatigue</link><description>&lt;doc fingerprint="2742997d33355a6e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Too Fast to Think: The Hidden Fatigue of AI Vibe Coding&lt;/head&gt;
    &lt;head rend="h3"&gt;Getting to the limits of what developers can do&lt;/head&gt;
    &lt;p&gt;After vibe coding for some time, I feel fatigue. I‚Äôm coding Marvai - a package manager for prompts on my own, with a combination of Claude Code and Cursor.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;I use Claude Code for code generation, bug fixing, test writing, test fixing, security checks etc.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Claude Code is especially useful to fix linting errors from nilaway or staticcheck - for a developer those are boring and tedious.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I use Cursor for augmented coding, generate functions, adapt copy&amp;amp;pasted code, for refactoring, fix error handling, and many other tedious tasks.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I have been a coder for 40 years and I have seen many tools for developers that haven‚Äôt worked, like Model Driven Development MDD/MDA and executable UML.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;With AI I‚Äôm now much faster at generating code and building features than I ever was before. The combination of Claude Code and Cursor is a real speedup.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I encountered a new phenomenon.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Again and again I feel fatigue. I finish a feature, and another feature, concetrate on reviewing the code the AI generated, and fix a bug and finish a feature with such velocity and I feel fatigue after some hours - sometimes as soon as one hour. AI working at such speed, finishing things to accept or review, feels too much for my brain to process or keep up with. I need to pause for some time before I can start again.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I haven‚Äôt felt this before as a developer.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I first encountered the concept of cognitive load in the book Team Toplogies. The idea there is to structure teams in a way that the cognitive load for developers is not too small and not to big. The more responsibilities a team and it‚Äôs members get, the bigger the cognitive load. And if you put many different topics on a team, the cognitive load for the team becomes too big for the team to work.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;As a young adult I was working in a plastic factory, sitting at a machine. The machine produced vacuum cleaner cases, had it‚Äôs rhythm, it finished vacuum cleaner cases on it‚Äôs own schedule, made a ‚ÄúPING‚Äù when finished, opened the door and I had to grab the casing and put it down (and package it in fine paper etc. which took some time). The machine would close while I was finishing the casing. And for some time it was stressful, working to the rhythm of the machine. Then I got accustomed to the speed and rhythm, until my boss increased the frequency. Living by machine time is what I sometimes feel with Vibe Coding and Cursor generating code or Claude fixing a bug.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I had waiting times with compiling and waited for the machine to finish, but it feels differently, with vibe coding it feels like the machine is in control not me.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;With traditional coding, the speed of your output matches the complexity of the task and the speed of your coding. This gives your brain time to process what is happening. With vibe coding the coding happens so fast, that my brain can‚Äôt process what is happening in real time, and thoughts are getting clogged up. Complex tasks are compressed into seconds or minutes.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;My brain does not get the baking time to mentally process architecture, decisions and edge cases the AI creates - not enough time to put the AI output into one coherent picture. I‚Äôm running a marathon at the pace of a sprint - speeds don‚Äôt match.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;One reason developers are developers is the dopamine loop.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You write code, it doesn‚Äôt work, you fix it, it works, great! Dopamine rush. Several dozens or a hundred times a day. Now this loop speeds up. Faster and faster we run around that loop, and get washed with Dopamine - and your brain gets overwhelmed. And stress hormones at the same time! You get fatigue - instead of the usually happiness and satisfaction of coding.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;With coding there is a limit to context switching. A context switch in coding is like dumping the cache of your brain and reloading the cache of your brain with a new context. This takes time and energy. You need to build a mental model of the code, to decide what to change and how to change it and then writing the changes out into source code, the essence of coding.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;With vibe coding the frequency of content switches speeds up tremendously - with the AI fixing and creating many different things in different packages or modules with one go. Even when I &amp;lt;tab&amp;gt;, &amp;lt;tab&amp;gt;, &amp;lt;tab&amp;gt; in Cursor, each change is a micro-content switch from function to function.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Each context switch takes energy, every context switch is heavy lifting for your brain. Normally this materializes as the fact that it takes time to context switch. With AI in the driver seat, context switches are forced on you faster and faster.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;When each context switch takes energy, fast energy switches from feature delivered to feature delivered, vibe coding drains your energy - fatigue!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;With AI it seems we all become managers, we all become reviewers. The core of the role is changing from turning requirements into code to managing AI output - more like a team lead manages the output of a team, but on a much deeper level. Your responsibility grows to manage a team of five with vibe coding, while still being a developer being responsible for the code. It‚Äôs like being a traffic officer in the middle of a busy intersection - which is a stressful job on it‚Äôs own - while also overseeing five intersections in parallel.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Reviewing, directing and guiding an AI puts more stress on you than writing code, where your writing matches your thinking and doesn‚Äôt jump ahead, with you in a rush to catch up.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;What can be said:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Me and many more I assume feel fatigue from vibe coding&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;We need deliberate pacing when working with AI tools&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;We need AI-aware retrospectives to understand what is going on - perhaps having a daily retrospective to get the mind and the code in sync again&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;We need to be aware of new mental health challenges for AI coders&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;We might need to let go of micro managing AIs and trust the AI more - stop trying to bridge the gap between managing an AI and controlling it‚Äôs output&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I don‚Äôt think we‚Äôve figured out yet where this is going. AI has made us faster than we‚Äôve ever been, but our brains haven‚Äôt caught up to the pace. We‚Äôre like early pilots flying with autopilot‚Äîcapable, but drained. We need new rhythms, new boundaries, and new ways of thinking about what it means to ‚Äúcode.‚Äù&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Maybe the future of coding isn‚Äôt just faster. Maybe it‚Äôs also slower in a way, on purpose.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46292365</guid><pubDate>Tue, 16 Dec 2025 18:32:46 +0000</pubDate></item></channel></rss>