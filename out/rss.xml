<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 11 Dec 2025 13:53:37 +0000</lastBuildDate><item><title>How the Brain Parses Language</title><link>https://www.quantamagazine.org/the-polyglot-neuroscientist-resolving-how-the-brain-parses-language-20251205/</link><description>&lt;doc fingerprint="b5004a3dc32bba16"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Polyglot Neuroscientist Resolving How the Brain Parses Language&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Even in a world where large language models (LLMs) and AI chatbots are commonplace, it can be hard to fully accept that fluent writing can come from an unthinking machine. That’s because, to many of us, finding the right words is a crucial part of thought — not the outcome of some separate process.&lt;/p&gt;
    &lt;p&gt;But what if our neurobiological reality includes a system that behaves something like an LLM? Long before the rise of ChatGPT, the cognitive neuroscientist Ev Fedorenko began studying how language works in the adult human brain. The specialized system she has described, which she calls “the language network,” maps the correspondences between words and their meanings. Her research suggests that, in some ways, we do carry around a biological version of an LLM — that is, a mindless language processor — inside our own brains.&lt;/p&gt;
    &lt;p&gt;“You can think of the language network as a set of pointers,” Fedorenko said. “It’s like a map, and it tells you where in the brain you can find different kinds of meaning. It’s basically a glorified parser that helps us put the pieces together — and then all the thinking and interesting stuff happens outside of [its] boundaries.”&lt;/p&gt;
    &lt;p&gt;Fedorenko has been gathering biological evidence of this language network for the past 15 years in her lab at the Massachusetts Institute of Technology. Unlike a large language model, the human language network doesn’t string words into plausible-sounding patterns with nobody home; instead, it acts as a translator between external perceptions (such as speech, writing and sign language) and representations of meaning encoded in other parts of the brain (including episodic memory and social cognition, which LLMs don’t possess). Nor is the human language network particularly large: If all of its tissue were clumped together, it would be about the size of a strawberry. But when it is damaged, the effect is profound. An injured language network can result in forms of aphasia in which sophisticated cognition remains intact but trapped within a brain unable to express it or distinguish incoming words from others.&lt;/p&gt;
    &lt;p&gt;Fedorenko came by her interest in language early. In the 1980s, when she was growing up in the Soviet Union, her mother made her learn five languages (English, French, German, Spanish and Polish) in addition to her native Russian. Despite significant privations related to the fall of communism in that country — Fedorenko “lived through a few years of being hungry,” she said — she was a strong student and earned a full scholarship to Harvard University. There, she initially planned to study linguistics but later added a second major in psychology. “The [linguistics] classes were interesting, but they felt kind of like puzzle-solving, not really figuring out how things work in reality,” she said.&lt;/p&gt;
    &lt;p&gt;Three years into her graduate studies at MIT, Fedorenko pivoted again, this time into neuroscience. She began collaborating with Nancy Kanwisher, who had first identified the fusiform face area, a brain region specialized for facial recognition. Fedorenko wanted to find the same thing for language. She had her work cut out for her. “At that point, it was possible to read pretty much everything that was published [on the subject], and I thought the foundations were pretty weak,” Fedorenko said. “As you can imagine, that [assessment] was not so popular with some people. But after a while they saw I was not going away.”&lt;/p&gt;
    &lt;p&gt;Following a steady stream of findings, in 2024 Fedorenko published a comprehensive review in Nature Reviews Neuroscience defining the human language network as a “natural kind”: an integrated set of regions, exclusively specialized for language, that resides in “every typical adult human brain,” she wrote.&lt;/p&gt;
    &lt;p&gt;Quanta spoke to Fedorenko about how the language network is like the digestive system, what she knows about how the language decoder works, and whether she really believes that people have LLMs inside their heads. The conversation has been condensed and edited for clarity.&lt;/p&gt;
    &lt;head rend="h3"&gt;What is the language network?&lt;/head&gt;
    &lt;p&gt;There’s a core set of areas in adult brains that acts as an interconnected system for computing linguistic structure. They store the mappings between words and meanings, and rules for how to put words together. When you learn a language, that’s what you learn: You learn these mappings and the rules. And that allows us to use this “code” in incredibly flexible ways. You can convert between a thought and a word sequence in any language that you know.&lt;/p&gt;
    &lt;head rend="h3"&gt;That sounds very abstract. But you call the language network a “natural kind” — does that mean it’s something physical you can point to, like the digestive system?&lt;/head&gt;
    &lt;p&gt;That’s exactly right. These systems that people have discovered [in the brain], including the language network and some parts of the visual system, are like organs. For example, the fusiform face area is a natural kind: It’s meaningfully definable as a unit. In the language network, there are basically three areas in the frontal cortex in most people. All three of them are on the side of the left frontal lobe. There’s also a couple of areas that fall along the side of the middle temporal gyrus, this big hunk of meat that goes along the whole temporal lobe. Those are the core areas.&lt;/p&gt;
    &lt;p&gt;You can see the unity in a few different ways. For example, if you put people in an [fMRI, or functional magnetic resonance imaging], scanner, you can look at responses to language versus some control condition. Those regions always go together. We’ve now scanned about 1,400 people, and we can build up a probabilistic map, which estimates where those regions will tend to be. The topography is a little bit variable across people, but the general patterns are very consistent. Somewhere within those broad frontal and temporal areas, everybody will have some tissue that is reliably doing linguistic computations.&lt;/p&gt;
    &lt;head rend="h3"&gt;How is this different from other parts of brain anatomy known to be associated with language, such as Broca’s area?&lt;/head&gt;
    &lt;p&gt;Broca’s area is actually incredibly controversial. I would not call it a language region; it’s an articulatory motor-planning region. Right now, it’s being engaged to plan the movements of my mouth muscles in a way that allows me to say what I’m saying. But I could say a bunch of nonsense words, and it would be just as engaged. So it’s an area that takes some sound-level representation of speech and figures out the set of motor movements you would need [to produce it]. It’s a downstream region that the language network sends information to.&lt;/p&gt;
    &lt;head rend="h3"&gt;You’ve also said that language isn’t the same as thought. So if the language network isn’t producing speech, and it’s also not involved in thinking, what is it doing?&lt;/head&gt;
    &lt;p&gt;The language network is basically an interface between lower-level perceptual and motor components and the higher-level, more abstract representations of meaning and reasoning.&lt;/p&gt;
    &lt;p&gt;There are two things we do with language. In language production, you have this fuzzy thought, and then you have a vocabulary — not just of words, but larger constructions, and rules for how to connect them. You search through it to find a way to express the meaning you’re trying to convey using a structured sequence of words. Once you have that utterance, then you go to the motor system to say it out loud, write it or sign it.&lt;/p&gt;
    &lt;p&gt;In language comprehension, it’s the inverse. It starts with sound waves hitting your ear or light hitting your retina. You do some basic perceptual crunching of that input to extract a word sequence or utterance. Then the language network parses that, finding familiar chunks in the utterance and using them as pointers to stored representations of meaning.&lt;/p&gt;
    &lt;p&gt;For both cases, the language network is a store of these form-to-meaning mappings. It’s a fluid store that we keep updating throughout our lives. But as soon as we know this code, we can flexibly use it to both take a thought and express it, and take somebody else’s word sequence and decode meaning from it.&lt;/p&gt;
    &lt;p&gt;Why do we have this system? So we can take our thoughts and share them. There’s no telepathy, right?&lt;/p&gt;
    &lt;head rend="h3"&gt;How far down does this biological specialization go? Are there individual cells in the language network that respond to certain utterances, akin to how concept neurons only respond to specific concepts?&lt;/head&gt;
    &lt;p&gt;I suspect it’s a bit distributed within the system because language is very contextualized. But yes, there may well be cells that respond to particular aspects of language.&lt;/p&gt;
    &lt;p&gt;There’s a preprint, from Itzhak Fried’s group at UCLA, looking at single cells and finding some of the same properties that we found with [fMRI] imaging and population-level intracranial recordings. For example, cells will respond to both written and auditory language in similar ways. And the language network is where you would look for those cells.&lt;/p&gt;
    &lt;head rend="h3"&gt;What kinds of patterns or features get learned?&lt;/head&gt;
    &lt;p&gt;The brain’s general object-recognition machinery is at the same level of abstractness as the language network. It’s not so different from some higher-level visual areas such as the inferotemporal cortex storing bits of object shapes, or the fusiform face area storing a basic face template. You use those representations to help you recognize objects in the world, but they’re disconnected from our world knowledge.&lt;/p&gt;
    &lt;p&gt;[Linguist Noam] Chomsky’s famous example of a nonsense sentence — “Colorless green ideas sleep furiously” — comes in handy here. You kind of know what it means, but you can’t relate it to anything about the world because it doesn’t make sense. We and a few other groups have evidence that the language network will respond just as strongly to those “colorless green”–type sentences as it does to plausible sentences that tell us something meaningful. I don’t want to call it “dumb,” but it’s a pretty shallow system.&lt;/p&gt;
    &lt;head rend="h3"&gt;It almost sounds like you’re saying there’s essentially an LLM inside everyone’s brain. Is that what you’re saying?&lt;/head&gt;
    &lt;p&gt;Pretty much. I think the language network is very similar in many ways to early LLMs, which learn the regularities of language and how words relate to each other. It’s not so hard to imagine, right? I’m sure you’ve encountered people who produce very fluent language, and you kind of listen to it for a while, and you’re like: There’s nothing coherent there. But it sounds very fluent. And that’s with no physical injury to their brain!&lt;/p&gt;
    &lt;head rend="h3"&gt;Still, the idea that humans produce language with something mindless, like ChatGPT, seems counterintuitive.&lt;/head&gt;
    &lt;p&gt;Yes — including to me! When I started [this research], I thought that language is a really core part of high-level thought. There was this notion that maybe humans are just really good at representing and extracting hierarchical structures, which of course are a key signature of language, but are also present in other domains like math and music and aspects of social cognition. So I was fully expecting that some parts of this network would be these very domain-general, hierarchical processors. And that just turns out empirically not to be the case. Back in 2011, it was already clear that all parts of the system are quite specialized for language. If you’re a scientist, you just update your beliefs and roll with it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46191597</guid><pubDate>Mon, 08 Dec 2025 12:46:50 +0000</pubDate></item><item><title>Python Workers redux: fast cold starts, packages, and a uv-first workflow</title><link>https://blog.cloudflare.com/python-workers-advancements/</link><description>&lt;doc fingerprint="a2332701ecf0f4b1"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Note: This post was updated with additional details regarding AWS Lambda.&lt;/p&gt;
      &lt;p&gt;Last year we announced basic support for Python Workers, allowing Python developers to ship Python to region: Earth in a single command and take advantage of the Workers platform.&lt;/p&gt;
      &lt;p&gt;Since then, weâve been hard at work making the Python experience on Workers feel great. Weâve focused on bringing package support to the platform, a reality thatâs now here â with exceptionally fast cold starts and a Python-native developer experience.&lt;/p&gt;
      &lt;p&gt;This means a change in how packages are incorporated into a Python Worker. Instead of offering a limited set of built-in packages, we now support any package supported by Pyodide, the WebAssembly runtime powering Python Workers. This includes all pure Python packages, as well as many packages that rely on dynamic libraries. We also built tooling around uv to make package installation easy.&lt;/p&gt;
      &lt;p&gt;Weâve also implemented dedicated memory snapshots to reduce cold start times. These snapshots result in serious speed improvements over other serverless Python vendors. In cold start tests using common packages, Cloudflare Workers start over 2.4x faster than AWS Lambda without SnapStart and 3x faster than Google Cloud Run.&lt;/p&gt;
      &lt;p&gt;In this blog post, weâll explain what makes Python Workers unique and share some of the technical details of how weâve achieved the wins described above. But first, for those who may not be familiar with Workers or serverless platforms â and especially those coming from a Python background â let us share why you might want to use Workers at all.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Deploying Python globally in 2 minutes&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Part of the magic of Workers is simple code and easy global deployments. Let's start by showing how you can deploy a FastAPI app across the world with fast cold starts in less than two minutes.&lt;/p&gt;
      &lt;p&gt;A simple Worker using FastAPI can be implemented in a handful of lines:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;from fastapi import FastAPI
from workers import WorkerEntrypoint
import asgi

app = FastAPI()

@app.get("/")
async def root():
   return {"message": "This is FastAPI on Workers"}

class Default(WorkerEntrypoint):
   async def fetch(self, request):
       return await asgi.fetch(app, request.js_object, self.env)&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;To deploy something similar, just make sure you have &lt;code&gt;uv&lt;/code&gt; and &lt;code&gt;npm&lt;/code&gt; installed, then run the following:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;$ uv tool install workers-py
$ pywrangler init --template \
    https://github.com/cloudflare/python-workers-examples/03-fastapi
$ pywrangler deploy&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;With just a little code and a &lt;code&gt;pywrangler deploy&lt;/code&gt;, youâve now deployed your application across Cloudflareâs edge network that extends to 330 locations across 125 countries. No worrying about infrastructure or scaling.&lt;/p&gt;
      &lt;p&gt;And for many use cases, Python Workers are completely free. Our free tier offers 100,000 requests per day and 10ms CPU time per invocation. For more information, check out the pricing page in our documentation.&lt;/p&gt;
      &lt;p&gt;For more examples, check out the repo in GitHub. And read on to find out more about Python Workers.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;So what can you do with Python Workers?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Now that youâve got a Worker, just about anything is possible. You write the code, so you get to decide. Your Python Worker receives HTTP requests and can make requests to any server on the public Internet.&lt;/p&gt;
      &lt;p&gt;You can set up cron triggers, so your Worker runs on a regular schedule. Plus, if you have more complex requirements, you can make use of Workflows for Python Workers, or even long-running WebSocket servers and clients using Durable Objects.&lt;/p&gt;
      &lt;p&gt;Here are more examples of the sorts of things you can do using Python Workers:&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Faster package cold starts&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Serverless platforms like Workers save you money by only running your code when itâs necessary to do so. This means that if your Worker isnât receiving requests, it may be shut down and will need to be restarted once a new request comes in. This typically incurs a resource overhead we refer to as the âcold start.â Itâs important to keep these as short as possible to minimize latency for end users.&lt;/p&gt;
      &lt;p&gt;In standard Python, booting the runtime is expensive, and our initial implementation of Python Workers focused on making the runtime boot fast. However, we quickly realized that this wasnât enough. Even if the Python runtime boots quickly, in real-world scenarios the initial startup usually includes loading modules from packages, and unfortunately, in Python many popular packages can take several seconds to load.&lt;/p&gt;
      &lt;p&gt;We set out to make cold starts fast, regardless of whether packages were loaded.&lt;/p&gt;
      &lt;p&gt;To measure realistic cold start performance, we set up a benchmark that imports common packages, as well as a benchmark running a âhello worldâ using a bare Python runtime. Standard Lambda is able to start just the runtime quickly, but once you need to import packages, the cold start times shoot up. In order to optimize for faster cold starts with packages, you can use SnapStart on Lambda (which we will be adding to the linked benchmarks shortly). This incurs a cost to store the snapshot and an additional cost on every restore. Python Workers will automatically apply memory snapshots for free for every Python Worker.&lt;/p&gt;
      &lt;p&gt;Here are the average cold start times when loading three common packages (httpx, fastapi and pydantic):&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Platform&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Mean Cold Start (secs)&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Cloudflare Python Workers&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;1.027&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;AWS Lambda (without SnapStart)&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;2.502&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Google Cloud Run&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;3.069&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;p&gt;In this case, Cloudflare Python Workers have 2.4x faster cold starts than AWS Lambda without SnapStart and 3x faster cold starts than Google Cloud Run. We achieved these low cold start numbers by using memory snapshots, and in a later section we explain how we did so.&lt;/p&gt;
      &lt;p&gt;We are regularly running these benchmarks. Go here for up-to-date data and more info on our testing methodology.&lt;/p&gt;
      &lt;p&gt;Weâre architecturally different from these other platforms â namely, Workers is isolate-based. Because of that, our aims are high, and we are planning for a zero cold start future.&lt;/p&gt;
      &lt;p&gt;The diverse package ecosystem is a large part of what makes Python so amazing. Thatâs why weâve been hard at work ensuring that using packages in Workers is as easy as possible.&lt;/p&gt;
      &lt;p&gt;We realised that working with the existing Python tooling is the best path towards a great development experience. So we picked the &lt;code&gt;uv&lt;/code&gt; package and project manager, as itâs fast, mature, and gaining momentum in the Python ecosystem.&lt;/p&gt;
      &lt;p&gt;We built our own tooling around &lt;code&gt;uv&lt;/code&gt; called pywrangler. This tool essentially performs the following actions:&lt;/p&gt;
      &lt;p&gt;Pywrangler calls out to &lt;code&gt;uv&lt;/code&gt; to install the dependencies in a way that is compatible with Python Workers, and calls out to &lt;code&gt;wrangler&lt;/code&gt; when developing locally or deploying Workers.Â &lt;/p&gt;
      &lt;p&gt;Effectively this means that you just need to run &lt;code&gt;pywrangler dev&lt;/code&gt; and &lt;code&gt;pywrangler&lt;/code&gt; &lt;code&gt;deploy&lt;/code&gt; to test your Worker locally and deploy it.Â &lt;/p&gt;
      &lt;p&gt;You can generate type hints for all of the bindings defined in your wrangler config using &lt;code&gt;pywrangler types&lt;/code&gt;. These type hints will work with Pylance or with recent versions of mypy.&lt;/p&gt;
      &lt;p&gt;To generate the types, we use wrangler types to create typescript type hints, then we use the typescript compiler to generate an abstract syntax tree for the types. Finally, we use the TypeScript hints â such as whether a JS object has an iterator field â to generate &lt;code&gt;mypy&lt;/code&gt; type hints that work with the Pyodide foreign function interface.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Decreasing cold start duration using snapshots&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Python startup is generally quite slow and importing a Python module can trigger a large amount of work. We avoid running Python startup during a cold start using memory snapshots.&lt;/p&gt;
      &lt;p&gt;When a Worker is deployed, we execute the Workerâs top-level scope and then take a memory snapshot and store it alongside your Worker. Whenever we are starting a new isolate for the Worker, we restore the memory snapshot and the Worker is ready to handle requests, with no need to execute any Python code in preparation. This improves cold start times considerably. For instance, starting a Worker that imports &lt;code&gt;fastapi&lt;/code&gt;, &lt;code&gt;httpx&lt;/code&gt; and &lt;code&gt;pydantic&lt;/code&gt; without snapshots takes around 10 seconds. With snapshots, it takes 1 second.&lt;/p&gt;
      &lt;p&gt;The fact that Pyodide is built on WebAssembly enables this. We can easily capture the full linear memory of the runtime and restore it.Â &lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h4"&gt;Memory snapshots and Entropy&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;WebAssembly runtimes do not require features like address space layout randomization for security, so most of the difficulties with memory snapshots on a modern operating system do not arise. Just like with native memory snapshots, we still have to carefully handle entropy at startup to avoid using the XKCD random number generator (weâre very into actual randomness).&lt;/p&gt;
      &lt;p&gt;By snapshotting memory, we might inadvertently lock in a seed value for randomness. In this case, future calls for ârandomâ numbers would consistently return the same sequence of values across many requests.&lt;/p&gt;
      &lt;p&gt;Avoiding this is particularly challenging because Python uses a lot of entropy at startup. These include the libc functions &lt;code&gt;getentropy()&lt;/code&gt; and &lt;code&gt;getrandom()&lt;/code&gt; and also reading from &lt;code&gt;/dev/random&lt;/code&gt; and &lt;code&gt;/dev/urandom&lt;/code&gt;. All of these functions share the same implementation in terms of the JavaScript &lt;code&gt;crypto.getRandomValues()&lt;/code&gt; function.&lt;/p&gt;
      &lt;p&gt;In Cloudflare Workers, &lt;code&gt;crypto.getRandomValues()&lt;/code&gt; has always been disabled at startup in order to allow us to switch to using memory snapshots in the future. Unfortunately, the Python interpreter cannot bootstrap without calling this function. And many packages also require entropy at startup time. There are essentially two purposes for this entropy:&lt;/p&gt;
      &lt;p&gt;Hash randomization we do at startup time and accept the cost that each specific Worker has a fixed hash seed. Python has no mechanism to allow replacing the hash seed after startup.&lt;/p&gt;
      &lt;p&gt;For pseudorandom number generators (PRNG), we take the following approach:&lt;/p&gt;
      &lt;p&gt;At deploy time:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Seed the PRNG with a fixed âpoison seedâ, then record the PRNG state.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Replace all APIs that call into the PRNG with an overlay that fails the deployment with a user error.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Execute the top level scope of user code.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Capture the snapshot.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;At run time:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Assert that the PRNG state is unchanged. If it changed, we forgot the overlay for some method. Fail the deployment with an internal error.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;After restoring the snapshot, reseed the random number generator before executing any handlers.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;With this, we can ensure that PRNGs can be used while the Worker is running, but stop Workers from using them during initialization and pre-snapshot.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h4"&gt;Memory snapshots and WebAssembly state&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;An additional difficulty arises when creating memory snapshots on WebAssembly: The memory snapshot we are saving consists only of the WebAssembly linear memory, but the full state of the Pyodide WebAssembly instance is not contained in the linear memory.Â &lt;/p&gt;
      &lt;p&gt;There are two tables outside of this memory.&lt;/p&gt;
      &lt;p&gt;One table holds the values of function pointers. Traditional computers use a âVon Neumannâ architecture, which means that code exists in the same memory space as data, so that calling a function pointer is a jump to some memory address. WebAssembly has a âHarvard architectureâ where code lives in a separate address space. This is key to most of the security guarantees of WebAssembly and in particular why WebAssembly does not need address space layout randomization. A function pointer in WebAssembly is an index into the function pointer table.&lt;/p&gt;
      &lt;p&gt;A second table holds all JavaScript objects referenced from Python. JavaScript objects cannot be directly stored into memory because the JavaScript virtual machine forbids directly obtaining a pointer to a JavaScript object. Instead, they are stored into a table and represented in WebAssembly as an index into the table.&lt;/p&gt;
      &lt;p&gt;We need to ensure that both of these tables are in exactly the same state after we restore a snapshot as they were when we captured the snapshot.&lt;/p&gt;
      &lt;p&gt;The function pointer table is always in the same state when the WebAssembly instance is initialized and is updated by the dynamic loader when we load dynamic libraries â native Python packages like numpy.Â &lt;/p&gt;
      &lt;p&gt;To handle dynamic loading:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;When taking the snapshot, we patch the loader to record the load order of dynamic libraries, the address in memory where the metadata for each library is allocated, and the function pointer table base address for relocations.Â &lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;When restoring the snapshot, we reload the dynamic libraries in the same order, and we use a patched memory allocator to place the metadata in the same locations. We assert that the current size of the function pointer table matches the function pointer table base we recorded for the dynamic library.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;All of this ensures that each function pointer has the same meaning after weâve restored the snapshot as it had when we took the snapshot.&lt;/p&gt;
      &lt;p&gt;To handle the JavaScript references, we implemented a fairly limited system. If a JavaScript object is accessible from globalThis by a series of property accesses, we record those property accesses and replay them when restoring the snapshot. If any reference exists to a JavaScript object that is not accessible in this way, we fail deployment of the Worker. This is good enough to deal with all the existing Python packages with Pyodide support, which do top level imports like:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;from js import fetch&lt;/code&gt;
      &lt;/quote&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Reducing cold start frequency using sharding&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Another important characteristic of our performance strategy for Python Workers is sharding. There is a very detailed description of what went into its implementation here. In short, we now route requests to existing Worker instances, whereas before we might have chosen to start a new instance.&lt;/p&gt;
      &lt;p&gt;Sharding was actually enabled for Python Workers first and proved to be a great test bed for it. A cold start is far more expensive in Python than in JavaScript, so ensuring requests are routed to an already-running isolate is especially important.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Where do we go from here?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;This is just the start. We have many plans to make Python Workers better:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;More developer-friendly tooling&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Even faster cold starts by utilising our isolate architecture&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Support for more packages&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Support for native TCP sockets, native WebSockets, and more bindings&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;To learn more about Python Workers, check out the documentation available here. To get help, be sure to join our Discord.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46192748</guid><pubDate>Mon, 08 Dec 2025 14:42:01 +0000</pubDate></item><item><title>How Google Maps allocates survival across London's restaurants</title><link>https://laurenleek.substack.com/p/how-google-maps-quietly-allocates</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46203343</guid><pubDate>Tue, 09 Dec 2025 10:20:02 +0000</pubDate></item><item><title>Show HN: Local Privacy Firewall-blocks PII and secrets before ChatGPT sees them</title><link>https://github.com/privacyshield-ai/privacy-firewall</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46206591</guid><pubDate>Tue, 09 Dec 2025 16:10:37 +0000</pubDate></item><item><title>Australia begins enforcing world-first teen social media ban</title><link>https://www.reuters.com/legal/litigation/australia-social-media-ban-takes-effect-world-first-2025-12-09/</link><description>&lt;doc fingerprint="2d87985519e0dfd9"&gt;
  &lt;main&gt;
    &lt;p&gt;SYDNEY, Dec 10 (Reuters) - Australia on Wednesday became the first country to ban social media for children under 16, blocking access in a move welcomed by many parents and child advocates but criticised by major technology companies and free-speech advocates.&lt;/p&gt;
    &lt;p&gt;Starting at midnight (1300 GMT on Tuesday), 10 of the largest platforms including TikTok, Alphabet's (GOOGL.O) YouTube and Meta's (META.O) Instagram and Facebook were ordered to block children or face fines of up to A$49.5 million ($33 million) under the new law, which is being closely watched by regulators worldwide.&lt;/p&gt;
    &lt;p&gt;Sign up here.&lt;/p&gt;
    &lt;p&gt;Prime Minister Anthony Albanese called it "a proud day" for families and cast the law as proof that policymakers can curb online harms that have outpaced traditional safeguards.&lt;/p&gt;
    &lt;p&gt;"This will make an enormous difference. It is one of the biggest social and cultural changes that our nation has faced," Albanese told a news conference on Wednesday.&lt;/p&gt;
    &lt;p&gt;"It's a profound reform which will continue to reverberate around the world."&lt;/p&gt;
    &lt;head rend="h2"&gt;READ A BOOK INSTEAD, PM TELLS YOUNGSTERS&lt;/head&gt;
    &lt;p&gt;In a video message, Albanese urged children to "start a new sport, new instrument, or read that book that has been sitting there for some time on your shelf," ahead of Australia's summer school break starting later this month.&lt;/p&gt;
    &lt;p&gt;Some of those below the cut-off age of 16 were anxious about adjusting to life without social media, but others were less concerned.&lt;/p&gt;
    &lt;p&gt;"I'm not really that emotional about it," said 14-year-old Claire Ni. "I'm kind of just, like, neutral."&lt;/p&gt;
    &lt;p&gt;Luna Dizon, 15, said she still had access to her TikTok, Instagram and Snapchat accounts, but worried about "culture shock" once the ban took full effect.&lt;/p&gt;
    &lt;p&gt;"I think eventually, without (social media), we'll learn how to adapt to it," she added.&lt;/p&gt;
    &lt;head rend="h2"&gt;TEENAGER SIGNS OFF WITH 'SEE YOU WHEN I'M 16'&lt;/head&gt;
    &lt;p&gt;While the government has said the ban would not be perfect in its operation, about 200,000 accounts were deactivated by Wednesday on TikTok alone, with "hundreds of thousands" more to be blocked in the next few days.&lt;/p&gt;
    &lt;p&gt;Many of the estimated 1 million children affected by the legislation also posted goodbye messages on social media.&lt;/p&gt;
    &lt;p&gt;"No more social media ... no more contact with the rest of the world," one teen wrote on TikTok.&lt;/p&gt;
    &lt;p&gt;"#seeyouwhenim16," said another.&lt;/p&gt;
    &lt;p&gt;Others said they would learn how to get round the ban.&lt;/p&gt;
    &lt;p&gt;"It's just kind of pointless, we're just going to create new ways to get on these platforms, so what's the point," said 14-year-old Claire Ni.&lt;/p&gt;
    &lt;head rend="h2"&gt;BAN HAS GLOBAL IMPLICATIONS&lt;/head&gt;
    &lt;p&gt;The rollout caps a year of debate over whether any country could practically stop children from using platforms embedded in daily life, and begins a live test for governments frustrated that social media firms have been slow to implement harm-reduction measures.&lt;/p&gt;
    &lt;p&gt;"I'm happy that they want to protect kids, and I'm happy that we have a chance to see how they do it and see if we can learn from them," said European Union lawmaker Christel Schaldemose, who wants to see greater protection for the bloc's children.&lt;/p&gt;
    &lt;p&gt;Albanese's centre-left government proposed the landmark law citing research showing harms to mental health from the overuse of social media among young teens, including misinformation, bullying and harmful depictions of body image.&lt;/p&gt;
    &lt;p&gt;Several countries from Denmark to New Zealand to Malaysia have signalled they may study or emulate Australia's model.&lt;/p&gt;
    &lt;p&gt;At a school in the German city of Bonn, students spoke favourably of a ban.&lt;/p&gt;
    &lt;p&gt;"Social media is highly addictive and doesn't really have any real advantages. I mean, there are advantages, such as being able to spread your opinion, but I think the disadvantages, especially the addiction, are much worse," said 15-year-old pupil Arian Klaar.&lt;/p&gt;
    &lt;p&gt;Julie Inman Grant, the U.S.-born eSafety Commissioner who is overseeing the ban, told Reuters on Wednesday a groundswell of American parents wanted similar measures.&lt;/p&gt;
    &lt;p&gt;"I hear from the parents and the activists and everyday people in America, 'we wish we had an eSafety commissioner like you in America, we wish we had a government that was going to put tween and teen safety before technology profits,'" she said in an interview at her office in Sydney.&lt;/p&gt;
    &lt;p&gt;'NOT OUR CHOICE': X SAYS WILL COMPLY&lt;/p&gt;
    &lt;p&gt;Elon Musk's X became the last of the 10 major platforms to take measures to cut off access to underage teens after publicly acknowledging on Wednesday that it would comply.&lt;/p&gt;
    &lt;p&gt;"It's not our choice - it's what the Australian law requires," X said on its website.&lt;/p&gt;
    &lt;p&gt;Australia has said the initial list of covered platforms would change as new products emerge and young users migrate.&lt;/p&gt;
    &lt;p&gt;Companies have told Canberra they will deploy a mix of age inference - estimating a user's age from their behaviour - and age estimation based on a selfie, alongside checks that could include uploaded identification documents.&lt;/p&gt;
    &lt;p&gt;For social media businesses, the implementation marks a new era of structural stagnation as user numbers flatline and time spent on platforms shrinks, studies show.&lt;/p&gt;
    &lt;p&gt;Platforms say they earn little from advertising to under-16s, but warn the ban disrupts a pipeline of future users. Just before the ban took effect, 86% of Australians aged eight to 15 used social media, the government said.&lt;/p&gt;
    &lt;p&gt;($1 = 1.5097 Australian dollars)&lt;/p&gt;
    &lt;p&gt;Reporting by Byron Kaye and Renju Jose; Additional reporting by James Redmayne and Cordelia Hsu; Writing by Alasdair Pal, Alexandra Hudson and Christine Chen; Editing by Andrew Heavens, Mark Potter, Lincoln Feast and Deepa Babington&lt;/p&gt;
    &lt;p&gt;Our Standards: The Thomson Reuters Trust Principles.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46208348</guid><pubDate>Tue, 09 Dec 2025 18:12:29 +0000</pubDate></item><item><title>Rubio stages font coup: Times New Roman ousts Calibri</title><link>https://www.reuters.com/world/us/rubio-stages-font-coup-times-new-roman-ousts-calibri-2025-12-09/</link><description>&lt;doc fingerprint="f1be8f403c685bdb"&gt;
  &lt;main&gt;
    &lt;p&gt;WASHINGTON, Dec 9 (Reuters) - U.S. Secretary of State Marco Rubio on Tuesday ordered diplomats to return to using Times New Roman font in official communications, calling his predecessor Antony Blinken's decision to adopt Calibri a "wasteful" diversity move, according to an internal department cable seen by Reuters.&lt;/p&gt;
    &lt;p&gt;The department under Blinken in early January 2023 had switched to Calibri, a modern sans-serif font, saying this was a more accessible font for people with disabilities because it did not have the decorative angular features and was the default in Microsoft products.&lt;/p&gt;
    &lt;p&gt;Sign up here.&lt;/p&gt;
    &lt;p&gt;A cable dated December 9 sent to all U.S. diplomatic posts said that typography shapes the professionalism of an official document and Calibri is informal compared to serif typefaces.&lt;/p&gt;
    &lt;p&gt;"To restore decorum and professionalism to the Department’s written work products and abolish yet another wasteful DEIA program, the Department is returning to Times New Roman as its standard typeface," the cable said.&lt;/p&gt;
    &lt;p&gt;"This formatting standard aligns with the President’s One Voice for America’s Foreign Relations directive, underscoring the Department’s responsibility to present a unified, professional voice in all communications," it added.&lt;/p&gt;
    &lt;p&gt;The State Department did not immediately respond to a request for comment.&lt;/p&gt;
    &lt;p&gt;Some studies suggest that sans-serif fonts, such as Calibri, are easier to read for those with certain visual disabilities.&lt;/p&gt;
    &lt;p&gt;Trump, a Republican, moved quickly after taking office in January to eradicate federal DEI programs and discourage them in the private sector and education, including by directing the firing of diversity officers at federal agencies and pulling grant funding for a wide range of programs.&lt;/p&gt;
    &lt;p&gt;DEI policies became more widespread after nationwide protests in 2020 against police killings of unarmed Black people, spurring a conservative backlash. Trump and other critics of diversity initiatives say they are discriminatory against white people and men and have eroded merit-based decision making.&lt;/p&gt;
    &lt;p&gt;Reporting by Humeyra Pamuk; Editing by Don Durfee and Lisa Shumaker&lt;/p&gt;
    &lt;p&gt;Our Standards: The Thomson Reuters Trust Principles.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46212438</guid><pubDate>Wed, 10 Dec 2025 00:08:34 +0000</pubDate></item><item><title>Common Lisp, ASDF, and Quicklisp: packaging explained</title><link>https://cdegroot.com/programming/commonlisp/2025/11/26/cl-ql-asdf.html</link><description>&lt;doc fingerprint="b7209a1f8fa70f94"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Common Lisp, ASDF, and Quicklisp: packaging explained&lt;/head&gt;
    &lt;p&gt;If there is one thing that confuses newcomers to Common Lisp, it is the interplay of built-in CL functionality, add-ons like Quicklisp and ASDF, and what all the words mean.&lt;/p&gt;
    &lt;p&gt;Common Lisp is old, and its inspiration is even older. It was developed when there was zero consensus on how file systems worked, operating systems were more incompatible than you can probably imagine, and that age shows. It pinned down terminology way before other languages got to the same point, and, as it happens so often, the late arrivals decided that they needed different words and these words stuck.&lt;/p&gt;
    &lt;p&gt;So letâs do a bit of a deep dive and see how all the bits and pieces work and why they are there. All examples are using SBCL and might be SBCL-specific. Check your Lispâs manual if you use something else. Also, Iâm (still) linking to the old LispWorks-provided HyperSpec as Iâm not sure that the newer versions are fully done yet.&lt;/p&gt;
    &lt;head rend="h2"&gt;Common Lisp&lt;/head&gt;
    &lt;p&gt;Common Lisp comes with just the bare essentials to work with files. It has to, as that single specification had to work on microcomputers, mainframes, and all sorts of minicomputers. Even today with essentially just two branches of the operating system family alive, the difference are big between Unix derivatives with a single hierarchy (and one of them, macOS, by default with a case-insensitive interpretation) and MS-DOS derivaties with drive letters and backslashes but also the option to have network-style paths with double backslashes. So Common Lisp has a somewhat odd system of ânamestringsâ (plain strings) and âpathnamesâ (weird strings). It is not super important and the spec has details, the tl&amp;amp;dr is that sometimes you will see a special reader macro &lt;code&gt;#P"/foo/bar"&lt;/code&gt; instead of just &lt;code&gt;"/foo/bar"&lt;/code&gt; and the docs will tell
you which of these two is acceptable as an argument for what function. I just wanted to get
that out of the way first. They HyperSpec has all the details, of course.&lt;/p&gt;
    &lt;head rend="h3"&gt;Loading code from files.&lt;/head&gt;
    &lt;p&gt;With files out of the way, next up is &lt;code&gt;LOAD&lt;/code&gt;. It loads a file âinto the Lisp environmentâ (which
means your running image), but exactly how the file is named and whether it will load a source
file or a compiled file is system-dependent. So&lt;/p&gt;
    &lt;code&gt;(load "foo")
&lt;/code&gt;
    &lt;p&gt;can load &lt;code&gt;foo.lisp&lt;/code&gt; or &lt;code&gt;foo.fasl&lt;/code&gt; or maybe even &lt;code&gt;foo.obj&lt;/code&gt; if a Lisp implementation compiles to
C object files. If it is a source file, itâll evaluate all the forms and do some system-specific
thing with them. The end result is that, well, everything in the file will now be ready for you
to use. So if we have:&lt;/p&gt;
    &lt;code&gt;(defun hello ()
  (print "Hello, world!"))

(print "Done loading!")
&lt;/code&gt;
    &lt;p&gt;and we open SBCL:&lt;/p&gt;
    &lt;code&gt;CL-USER(1): (load "test")

"Done loading"
T
CL-USER(2): (hello)

"Hello, world!"
"Hello, world!"
&lt;/code&gt;
    &lt;p&gt;Nothing too surprising there. In case we want to speed up loading, we can compile the file:&lt;/p&gt;
    &lt;code&gt;CL-USER(7): (compile-file "test")

; compiling file "/home/cees/tmp/test.lisp" (written 26 NOV 2025 09:03:19 PM):

; wrote /home/cees/tmp/test.fasl
; compilation finished in 0:00:00.004
#P"/home/cees/tmp/test.fasl"
NIL
NIL
&lt;/code&gt;
    &lt;p&gt;and the next time we ask to load &lt;code&gt;"test"&lt;/code&gt;, the FASL (âfast loadâ) file should be loaded. It is purely a time-saver
as the FASL file has been pre-parsed into your Lispâs in-memory format so can be loaded very
quickly (bypassing &lt;code&gt;READ&lt;/code&gt; with all its bells and whistles). FASL files are implementation dependent and more often than not even version dependent. This
is pretty much everything that the standard has to say about getting code into the system, and as you
can see, itâs not much.&lt;/p&gt;
    &lt;p&gt;There is also &lt;code&gt;PROVIDE&lt;/code&gt; and &lt;code&gt;REQUIRE&lt;/code&gt;, which operate on something
that the standard calls modules (and which are kept in a variable called &lt;code&gt;*modules*&lt;/code&gt;) but the
standard designates this as deprecated so letâs skip it. Just know it is still lingering there. Donât
use it (not even when packages âhelpfullyâ wrap it).&lt;/p&gt;
    &lt;head rend="h3"&gt;Packages&lt;/head&gt;
    &lt;p&gt;That &lt;code&gt;CL-USER&lt;/code&gt; in the prompt is the name of the package that you are in. Here is a pretty
bad choice of naming, and an endless source of confusion. A package is a namespace, nothing else, and
the spec says so much:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;A package establishes a mapping from names to symbols.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;These days, we associate the concept of âpackageâ probably with more than that. A bundle of software, with files and maybe some metadata, a thing you can download from somewhere, most likely. But in Common Lisp, itâs just a tool to map symbol names (strings in your source code) to symbols (internal addresses in memory). Itâs a pretty versatile facility and you should read the docs on &lt;code&gt;DEFPACKAGE&lt;/code&gt;. Itâs is quite powerful, as it can &lt;code&gt;:use&lt;/code&gt; other packages, it can shadow symbols,
and whatnot, but at the end of the day, all that happens is that when you type:&lt;/p&gt;
    &lt;code&gt;(hello)
&lt;/code&gt;
    &lt;p&gt;The REPL will use the current package (in &lt;code&gt;*package*&lt;/code&gt;) to translate &lt;code&gt;hello&lt;/code&gt; to whatever function
is in memory, which should exist in the current package (here &lt;code&gt;COMMON-LISP-USER&lt;/code&gt;, commonly aliased to
&lt;code&gt;CL-USER&lt;/code&gt;) or in any packages it inherits from (âusesâ). You can explicitly tell Lisp to look into
another package (&lt;code&gt;my-package:hello&lt;/code&gt; is a different function) and even ignore
that packageâs explicit list of exported symbols by using a double colon (but donât make a habit
out of prying into other packages, it breaks modularity). There are a ton of details, but
what counts is that a Common Lisp package is just an in-memory namespace thing, a bunch of
connected lookup tables that help the parser map the strings in the files you load to the
correct items inside your running image.&lt;/p&gt;
    &lt;p&gt;Nothing more, nothing less.&lt;/p&gt;
    &lt;head rend="h3"&gt;Systems&lt;/head&gt;
    &lt;p&gt;Common Lisp documentation often talks about systems in a general way like it is an intrinsic part of the language. However, the standard is vague. In the chapter on âSystem constructionâ it deals with loadingâthe little bit of functionality we already discussedâand âfeaturesâ, which are essentially just flags that are used by the &lt;code&gt;#+&lt;/code&gt; and &lt;code&gt;#-&lt;/code&gt; reader macros to make bits of code that
is loaded conditional on the presence of features.&lt;/p&gt;
    &lt;p&gt;That is all the standard has to say about systems. You can load files and you can make compilation of these files conditional on feature flags.&lt;/p&gt;
    &lt;head rend="h3"&gt;So, where does that leave us?&lt;/head&gt;
    &lt;p&gt;In a sense, this is all you need. I mean, you can take someone elseâs files and &lt;code&gt;LOAD&lt;/code&gt; them, and
they can be made somewhat portable by using features and saying &lt;code&gt;#+sbcl&lt;/code&gt; (this code only to be
compiled on SBCL) or &lt;code&gt;#-linux&lt;/code&gt; (do not compile this on Linux), and the files can organize themselves
by using &lt;code&gt;DEFPACKAGE&lt;/code&gt; and friends to separate the code into namespaces so everybody can write
code using names like &lt;code&gt;HELLO&lt;/code&gt; and not step on each otherâs toes.&lt;/p&gt;
    &lt;p&gt;Still, that Common Lisp âsystemâ thingâ¦ itâs a bit vague and maybe thereâs a hook there to build something more?&lt;/p&gt;
    &lt;head rend="h2"&gt;Another System Definition Facility&lt;/head&gt;
    &lt;p&gt;Some Common Lisp implementations come with a &lt;code&gt;DEFSYSTEM&lt;/code&gt;, but that is not portable. There
were early (weâre in 1989-ish now) attempts to have a common version, &lt;code&gt;MK:DEFSYSTEM&lt;/code&gt;, which
still works and is used by some projects. At the turn of a century, another version of &lt;code&gt;DEFSYSTEM&lt;/code&gt;
was created under the name &lt;code&gt;ASDF&lt;/code&gt;, which modernized things and quickly turned into the de facto
standard. It can do a lot of things and has extensive docs on its website, but weâll focus here on the essentials.&lt;/p&gt;
    &lt;p&gt;So, what is a system? Well, a library? A, err, package? Well, it should be named a package and if Common Lisp were born a couple of decades later it might have been called a package, but we have already seen that that name has been given to something closer to what we would probably call âmoduleâ today. âSystemâ it is, then, and ASDF âdefinesâ them.&lt;/p&gt;
    &lt;p&gt;Still, the closest analogy of a system is a package or a library: a bunch of Lisp code that together defines some functionality. Itâs not a perfect comparison, because a lot of Lisp libraries contain multiple systems: at the very least, it is customary to have your code define separate systems for regular code and for test code, and often more systems are defined for, say, optional or contributed code. In any case, it is not intrinsic to Common Lisp, though, so ASDF strictly adds functionality:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It allows you to define a system (&lt;code&gt;ASDF:DEFSYSTEM&lt;/code&gt;). Thatâs the core function: you tell it that you have a system with a certain name, and description, and all sorts of metadata; and most importantly, what source files are part of the system.&lt;/item&gt;
      &lt;item&gt;It allows you to define dependencies between systems in your &lt;code&gt;DEFSYSTEM&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;It allows you to load such systems wholesale. Instead of the individual files, or a developerâs homebrew loading script, you can now work on a higher level and load a system by name.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A system still is not a ârealâ Common Lisp thing: all that it does with respect to the standard is a bunch of &lt;code&gt;LOAD&lt;/code&gt; and &lt;code&gt;COMPILE-FILE&lt;/code&gt; calls. It will keep metadata in memory
about systems that are loaded, but under the hood, loading code is all it does. It comes
with extensive documentation and can do a lot of things like additional compilation steps, manage
test runs, etcetera, but if you squint, it just loads code.&lt;/p&gt;
    &lt;p&gt;An ASDF file, with the extension &lt;code&gt;.asd&lt;/code&gt;, is also just a Lisp source. The only special thing about
it is that the extension signals to ASDF that it is the file to look for when ASDF is searching
for systems, the one that has the system
definition in a given constellation of source files and directories.&lt;/p&gt;
    &lt;p&gt;It is important to realize that a âsystemâ and a package are entirely different things: one is an entity in an add-on tool, the other is intrinsic to Common Lispâs namespacing. They can have the same name and often enough, they have the same name (your ASDF system âfooâ will likely define a package âFOOâ and it is helpful if that lives in a Git repository called âfooâ which has a file named âfoo.asdâ) but they are different things living in, well, different namespaces and should not be confused with each other. One is intrinsic, the other an optional (but widely used) add-on and they are fully orthogonal things.&lt;/p&gt;
    &lt;head rend="h3"&gt;Where does ASDF gets its systems from?&lt;/head&gt;
    &lt;p&gt;Well, we have a âstandardâ, albeit a de facto one, to bundle Lisp code and describe how to load it. But if you say âthis system here is called FOO and is dependent on BARâ, how does ASDF find BAR? The answer is very simple: it looks in predefined locations on your local disk (and nowhere else!). There are two predefined locations, one older and one currently preferred:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;~/common-lisp&lt;/code&gt;, the old one;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;~/.local/share/common-lisp/source&lt;/code&gt;, the XDG-compliant currently preferred one. Use this.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thatâs all. You can extend that list by a very flexible but somewhat complicated mechanism called âsource registriesâ, extensively documented, but essentially, the process looks like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You refer to a system called &lt;code&gt;foo&lt;/code&gt;;&lt;/item&gt;
      &lt;item&gt;ASDF will look for &lt;code&gt;foo.asd&lt;/code&gt;under the configured directories;&lt;/item&gt;
      &lt;item&gt;If found, it will load that file, and the &lt;code&gt;DEFSYSTEM&lt;/code&gt;in there will do the rest.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This process recurses, so when system &lt;code&gt;foo&lt;/code&gt; depends on system &lt;code&gt;bar&lt;/code&gt; then the process will repeat, until
everything is loaded or an error occurs. All the systems will be found, defined (in your image/memory),
and loaded in the right order (depth-first so that dependencies are loaded, their packages defined and
functions and macros and variables ready for use, before dependents are).&lt;/p&gt;
    &lt;head rend="h3"&gt;So, where does that leave us?&lt;/head&gt;
    &lt;p&gt;We upgraded from âhere are a couple of Lisp files, good luck!â to âhere is a library with dependenciesâ. Good progress. All you need to do now is download the library (as a Zip file or a tarball), unpack it under &lt;code&gt;~/.local/share/common-lisp/source&lt;/code&gt;, and load if with
&lt;code&gt;ASDF:LOAD-SYSTEM&lt;/code&gt;. Of course, the system may declare dependencies so you may get an error
message. Easy enough, hunt for the dependency on the Net, download and unpack that, try again, find the next one.&lt;/p&gt;
    &lt;p&gt;Not perfect, but, well, progress?&lt;/p&gt;
    &lt;head rend="h2"&gt;Quicklisp enters the stage&lt;/head&gt;
    &lt;p&gt;Itâs still a bit primitive, though. I mean, when coders were sending each other QIC tapes this may have been sufficient, but then someone went and had to invent the Internet and now we just push data over the information superhighway. We should be able to do better, not? Like âPerl in 1995â better, even?&lt;/p&gt;
    &lt;p&gt;Just ilke ASDF is an optional add-on to what Common Lisp provides, Quicklisp is an optional add-on to what ASDF offers. Essentially, it does two things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;It adds a new directory to the places where ASDF can find systems;&lt;/item&gt;
      &lt;item&gt;It offers some functions to download a system from âwhereverâ, which includes âthe Internetâ.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It hook into ASDFâs dependency resolution so that if there are more dependencies needed, Quicklisp will go and fetch them as well.&lt;/p&gt;
    &lt;p&gt;Tadaa: problem solved! We can just open SBCL and say&lt;/p&gt;
    &lt;code&gt;(ql:quickload "foo")
&lt;/code&gt;
    &lt;p&gt;and admire a scrolling list of systems being downloaded, unpacked, loaded, analyzed for dependencies, dependencies being loaded, and so on.&lt;/p&gt;
    &lt;head rend="h3"&gt;So, where does that leave us?&lt;/head&gt;
    &lt;p&gt;We have all the functionality, but thereâs one final issue: it is âalways onâ. In a lot of other languages, if you start your REPL (say, in Python or in Ruby or in Elixir) in a certain directory, that carries significance. The language runtime will look for a special project file, probably, and set up search paths so that they work for that project. Common Lisp has no such concept, not even after you load ASDF and Quicklisp. So if you have a directory &lt;code&gt;~/my-code/my-awesome-lisp-project&lt;/code&gt; with
a &lt;code&gt;my-awesome-lisp-project.asd&lt;/code&gt; in thereâ¦. Neither Quicklisp nor ASDF is going to bother
about the current directory and magically find your system.&lt;/p&gt;
    &lt;p&gt;You must play with their rules. Luckily, the rules are simple: go to &lt;code&gt;~/.local/share/common-lisp/source&lt;/code&gt; and drop symlinks in there to your projects so that ASDF
can find them. That also means that it
doesnât matter where you start &lt;code&gt;sbcl&lt;/code&gt; or Sly or SLIME from, your code will always be found. And
when you then load your system with &lt;code&gt;QL:QUICKLOAD&lt;/code&gt;, its dependencies will automatically be pulled
in (&lt;code&gt;ASDF:LOAD-SYSTEM&lt;/code&gt; will still operate locally. It will, of course, use dependencies that
Quicklisp found and downloaded in previous runs).&lt;/p&gt;
    &lt;head rend="h2"&gt;Final tips&lt;/head&gt;
    &lt;head rend="h3"&gt;Read the source, Luke&lt;/head&gt;
    &lt;code&gt;$ git clone https://gitlab.common-lisp.net/asdf/asdf.git
$ git clone https://github.com/quicklisp/quicklisp-client
$ guix shell cloc -- cloc quicklisp-client asdf
     363 text files.
     264 unique files.
     104 files ignored.

github.com/AlDanial/cloc v 2.06  T=0.13 s (1984.8 files/s, 332115.5 lines/s)
-------------------------------------------------------------------------------
Language                     files          blank        comment           code
-------------------------------------------------------------------------------
Lisp                           219           3981           3889          31548
Markdown                         3            305              0           1173
HTML                             1             11             50            767
Bourne Shell                    10             40            120            526
Text                            18            105              0            357
make                             4             88             61            312
CSS                              1             60              8            236
YAML                             3             28             51            202
Perl                             2             22              8            117
DOS Batch                        2             23             13             74
C                                1              0              0              1
-------------------------------------------------------------------------------
SUM:                           264           4663           4200          35313
-------------------------------------------------------------------------------
&lt;/code&gt;
    &lt;p&gt;Itâs not that much code, and 7400 lines of that is the &lt;code&gt;UIOP&lt;/code&gt; package that ASDF
includes. UIOP is a package that is very useful in its own right as it is full of
utilities that help you make your code less implementation-dependent, but it wonât
teach you much about ASDF. So 25KLOC, tops. Without tests and contrib and whatnot, each
package is around 5000 lines of well-written Lisp and worth learning. Itâs helped
me more than once to understand especially ASDF-VM to just open the code and figure
out what exactly is going on.&lt;/p&gt;
    &lt;head rend="h3"&gt;KISS: Use package-inferred-system and a single source tree.&lt;/head&gt;
    &lt;p&gt;Put your Lisp code in directories under, I dunno, say &lt;code&gt;~/Code/CL&lt;/code&gt;. Symlink that directory to
&lt;code&gt;~/.local/share/common-lisp/source&lt;/code&gt; and ASDF will be able to find all your
systems. Iâve done some magic using GUIX Home and Stow and whatnot and had to dig
around into how things worked, not recommended. If you have dependencies that are not
in Quicklisp (or Ultralisp, which is worth adding), then check
them out in a central spot (I use &lt;code&gt;~/OpenSource&lt;/code&gt;) and symlink it into &lt;code&gt;~/quicklisp/local-projects&lt;/code&gt;.
That way, all your dependency management is in one spot, the Quicklisp directory, whether you
download them or Quicklisp did the job.&lt;/p&gt;
    &lt;p&gt;Read about ASDFâs package-inferred-system and use it. Itâll keep you from having to spend much time writing &lt;code&gt;.asd&lt;/code&gt; files.
As the docs say, ASDF itself uses
it
and since switching to it, thereâs no going back for me. In a nutshell, every file is
now expected to be a package and a system, same name, so that bit of confusion
goes away. One of my project repos (my main monorepo as of lately, Iâm slowly
moving all my other code to it) has a very short ASDF definition:&lt;/p&gt;
    &lt;code&gt;#-asdf3.1 (error "CA.BERKSOFT requires ASDF 3.1 or later.")
(asdf:defsystem "ca.berksoft"
  :class :package-inferred-system)
&lt;/code&gt;
    &lt;p&gt;It also has some necessary &lt;code&gt;REGISTER-SYSTEM-PACKAGES&lt;/code&gt; calls to register Coalton packages. Sometimes you
have dependencies that donât work well with this scheme and this is the work-around, a small drawback
that is dwarved by the advantages. But essentially, these three lines are it.&lt;/p&gt;
    &lt;p&gt;With that setup, a library to calculate the color temperature of an RGB color, say, lives in &lt;code&gt;l/gfx/color-temperature.lisp&lt;/code&gt; and starts with:&lt;/p&gt;
    &lt;code&gt;(uiop:define-package :ca.berksoft/l/gfx/color-temperature
  (:use :cl :infix-math :try)
  (:export :temp-&amp;gt;rgb))
&lt;/code&gt;
    &lt;p&gt;Note that I use the UIOP version of &lt;code&gt;defpackage&lt;/code&gt;. Itâs a good habit to use the UIOP versions of
functions where possible; itâll increase portability and more often than not, the UIOP functions
clean up confusion or shortcomings of the standard.&lt;/p&gt;
    &lt;p&gt;And that is all. ASDF, when I instruct it to load the system âca.berksoft/l/gfx/color-temperatureâ, will stumble upon the top level &lt;code&gt;.asd&lt;/code&gt; file, and then will start interpreting the rest (âl/gfx/color-temperatureâ) as
a relative path under its package-inferred-system functionality. It finds that file, registers it as an ASDF system and loads it, which creates the Common Lisp package. Very simple, very clean. Give it
a try.&lt;/p&gt;
    &lt;p&gt;Questions? Jump on Libera IRC and join the &lt;code&gt;#commonlisp&lt;/code&gt; channel, I usually keep a close eye on
it. You can also DM me on Mastodon or drop me a mail.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46216446</guid><pubDate>Wed, 10 Dec 2025 11:10:58 +0000</pubDate></item><item><title>Show HN: Wirebrowser – A JavaScript debugger with breakpoint-driven heap search</title><link>https://github.com/fcavallarin/wirebrowser</link><description>&lt;doc fingerprint="ac2b8e0449babf04"&gt;
  &lt;main&gt;
    &lt;p&gt;Wirebrowser is a debugging, interception, and memory-inspection toolkit powered by the Chrome DevTools Protocol (CDP). It unifies network manipulation, API testing, automation scripting, and deep JavaScript memory inspection into one interface.&lt;lb/&gt; With features like Breakpoint-Driven Heap Search and real-time Live Object Search, Wirebrowser provides researchers and engineers with precise, high-visibility tools for client-side analysis, reverse engineering, and complex application debugging.&lt;/p&gt;
    &lt;p&gt;Intercept, block, rewrite, and replay HTTP requests and responses in real time.&lt;/p&gt;
    &lt;p&gt;Inspect, search, and modify JavaScript memory using both live heap analysis and heap snapshots, with full support for object identity search, primitive search (via snapshots), structural matching, and runtime patching.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Live Object Search — Search all live JavaScript objects using regex or structural matching, and patch matched objects at runtime to alter state or behavior dynamically.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Static Heap Snapshot Search Capture a full V8 heap snapshot and search all objects and primitives, including strings and closure-captured values that are unreachable through the Runtime domain.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Origin Trace (BDHS) — Performs automatic debugger pauses and captures a full heap snapshot at each stop. Every snapshot is searched to identify the user-land function responsible for creating or mutating the target value. Framework and vendor scripts are filtered out via heuristics.&lt;/p&gt;&lt;lb/&gt;BDHS also includes a tolerance window that samples snapshots before and after the first match, providing contextual insight into when and how a value is introduced or mutated.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A shared similarity engine used across Live Object Search, Heap Snapshots, and BDHS timelines. Enables shape-based searches, clustering, and origin tracing for objects that evolve over time.&lt;/p&gt;
    &lt;p&gt;Create, edit, and execute API requests with variable substitution and structured collections, integrating Postman-style workflows directly into the debugging environment.&lt;/p&gt;
    &lt;p&gt;A full technical deep-dive is available here: 👉 https://fcavallarin.github.io/wirebrowser/BDHS-Origin-Trace&lt;/p&gt;
    &lt;p&gt;Below is a quick visual tour of Wirebrowser’s most distinctive capabilities.&lt;/p&gt;
    &lt;p&gt;A short walkthrough of Wirebrowser’s advanced memory-analysis capabilities:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Live Object Search — real-time search and runtime patching of live JS objects.&lt;/item&gt;
      &lt;item&gt;Origin Trace (BDHS) — identify the user-land function responsible for creating or mutating the object during debugging.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Intercept, rewrite, block, and replay HTTP requests and responses.&lt;/p&gt;
    &lt;p&gt;Search and patch live JS objects using regex or structural matching.&lt;/p&gt;
    &lt;p&gt;Capture snapshots on each debugger pause to locate the user-land function responsible for object creation or mutation.&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/fcavallarin/wirebrowser.git
cd wirebrowser
npm install
npm run build&lt;/code&gt;
    &lt;code&gt;npm run wirebrowser&lt;/code&gt;
    &lt;p&gt;On some Linux distributions, Electron may fail to start due to process sandboxing restrictions, showing errors such as:&lt;/p&gt;
    &lt;code&gt;The SUID sandbox helper binary was found, but is not configured correctly.
&lt;/code&gt;
    &lt;p&gt;This is a known issue in Electron ([electron/electron#42510]).&lt;lb/&gt; The most common solution is to disable AppArmor restrictions:&lt;/p&gt;
    &lt;code&gt;sudo sysctl -w kernel.apparmor_restrict_unprivileged_userns=0
&lt;/code&gt;
    &lt;p&gt;Beyond the core Network and Memory workflows, Wirebrowser offers several supporting modules that enhance debugging, testing, and automation workflows.&lt;/p&gt;
    &lt;p&gt;Create, edit, and execute API requests with variable substitution and organized collections.&lt;lb/&gt; Useful for testing endpoints, iterating on backend logic, or interacting with APIs directly from the same environment used for debugging the client.&lt;/p&gt;
    &lt;p&gt;Run browser-side or Node.js scripts, either manually or triggered by events such as page load.&lt;lb/&gt; Automation scripts have access to an &lt;code&gt;Utils&lt;/code&gt; object that exposes helpers for interacting with the browser, pages, variables, iterators, and HTTP utilities.&lt;/p&gt;
    &lt;code&gt;const userId = Utils.getVar("userId");
const page = Utils.getPage(1);
page.on("request", req =&amp;gt; req.continue());
await page.goto(`https://example.com/${userId}`);&lt;/code&gt;
    &lt;p&gt;A collection of small tools frequently needed during debugging and analysis, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Encode or decode strings in multiple formats:&lt;/item&gt;
      &lt;item&gt;Create, verify, and decode JSON Web Tokens (JWTs).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Most Wirebrowser actions can be performed either globally (across all open tabs/pages) or targeted to a single tab. This lets you choose whether a rule or inspection should affect the whole browser session or only a specific page.&lt;lb/&gt; Every tab/page opened by Wirebrowser has a unique integer &lt;code&gt;tabId&lt;/code&gt;. Use this &lt;code&gt;tabId&lt;/code&gt; to scope actions.&lt;/p&gt;
    &lt;p&gt;UI Notes&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Many panels offer a scope selector (Global / Specific Tab ID) for quick changes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Wirebrowser is built with React and Node.js, using plain JavaScript to keep the codebase lightweight and hackable.&lt;lb/&gt; TypeScript or JSDoc-based typing may be introduced in the future for enhanced maintainability.&lt;/p&gt;
    &lt;p&gt;The following areas are being explored for future development:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SPA crawling — automated crawling of single-page applications to map navigation flows and surface client-side behaviors.&lt;/item&gt;
      &lt;item&gt;DOM XSS scanning — analysis of potential DOM-based XSS injection points during crawls or on-demand checks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Wirebrowser is being built in the open — contributions and feedback are welcome!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;💬 Chat coming soon (Discord or Matrix)&lt;/item&gt;
      &lt;item&gt;🐦 Follow updates on X/Twitter: https://x.com/wirebrowser&lt;/item&gt;
      &lt;item&gt;🧠 Issues &amp;amp; Ideas: https://github.com/fcavallarin/wirebrowser/issues&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Contributions and pull requests are welcome!&lt;lb/&gt; Open an issue or pull request — even small suggestions help improve Wirebrowser.&lt;/p&gt;
    &lt;p&gt;Wirebrowser™ is distributed under the MIT License.&lt;lb/&gt; See the LICENSE file for more details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46218101</guid><pubDate>Wed, 10 Dec 2025 14:30:43 +0000</pubDate></item><item><title>Size of Life</title><link>https://neal.fun/size-of-life/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46219346</guid><pubDate>Wed, 10 Dec 2025 16:02:57 +0000</pubDate></item><item><title>Qwen3-Omni-Flash-2025-12-01：a next-generation native multimodal large model</title><link>https://qwen.ai/blog?id=qwen3-omni-flash-20251201</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46219538</guid><pubDate>Wed, 10 Dec 2025 16:13:38 +0000</pubDate></item><item><title>Auto-grading decade-old Hacker News discussions with hindsight</title><link>https://karpathy.bearblog.dev/auto-grade-hn/</link><description>&lt;doc fingerprint="a207dbd71fe07fd4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Auto-grading decade-old Hacker News discussions with hindsight&lt;/head&gt;
    &lt;p&gt;TLDR: https://karpathy.ai/hncapsule/&lt;/p&gt;
    &lt;p&gt;Yesterday I stumbled on this HN thread Show HN: Gemini Pro 3 hallucinates the HN front page 10 years from now, where Gemini 3 was hallucinating the frontpage of 10 years from now. One of the comments struck me a bit more though - Bjartr linked to the HN frontpage from exactly 10 years ago, i.e. December 2015. I was reading through the discussions of 10 years ago and mentally grading them for prescience when I realized that an LLM might actually be a lot better at this task. I copy pasted one of the article+comment threads manually into ChatGPT 5.1 Thinking and it gave me a beautiful analysis of what people thought + what actually happened in retrospect, even better and significantly more detailed than what I was doing manually. I realized that this task is actually a really good fit for LLMs and I was looking for excuses to vibe code something with the newly released Opus 4.5, so I got to work. I'm going to get all the front pages of December (31 days, 30 articles per day), get ChatGPT 5.1 Thinking to do the analysis, and present everything in a nice way for historical reading.&lt;/p&gt;
    &lt;p&gt;There are two macro reasons for why I think the exercise is interesting more generally:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I believe it is quite possible and desirable to train your forward future predictor given training and effort.&lt;/item&gt;
      &lt;item&gt;I was reminded again of my tweets that said "Be good, future LLMs are watching". You can take that in many directions, but here I want to focus on the idea that future LLMs are watching. Everything we do today might be scrutinized in great detail in the future because doing so will be "free". A lot of the ways people behave currently I think make an implicit "security by obscurity" assumption. But if intelligence really does become too cheap to meter, it will become possible to do a perfect reconstruction and synthesis of everything. LLMs are watching (or humans using them might be). Best to be good.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Vibe coding the actual project was relatively painless and took about 3 hours with Opus 4.5, with a few hickups but overall very impressive. The repository is on GitHub here: karpathy/hn-time-capsule. Here is the progression of what the code does:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Given a date, download the frontpage of 30 articles&lt;/item&gt;
      &lt;item&gt;For each article, download/parse the article itself and the full comment thread using Algolia API.&lt;/item&gt;
      &lt;item&gt;Package up everything into a markdown prompt asking for the analysis. Here is the prompt prefix I used:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;The following is an article that appeared on Hacker News 10 years ago, and the discussion thread.

Let's use our benefit of hindsight now in 6 sections:

1. Give a brief summary of the article and the discussion thread.
2. What ended up happening to this topic? (research the topic briefly and write a summary)
3. Give out awards for "Most prescient" and "Most wrong" comments, considering what happened.
4. Mention any other fun or notable aspects of the article or discussion.
5. Give out grades to specific people for their comments, considering what happened.
6. At the end, give a final score (from 0-10) for how interesting this article and its retrospect analysis was.

As for the format of Section 5, use the header "Final grades" and follow it with simply an unordered list of people and their grades in the format of "name: grade (optional comment)". Here is an example:

Final grades
- speckx: A+ (excellent predictions on ...)
- tosh: A (correctly predicted this or that ...)
- keepamovin: A
- bgwalter: D
- fsflover: F (completely wrong on ...)

Your list may contain more people of course than just this toy example. Please follow the format exactly because I will be parsing it programmatically. The idea is that I will accumulate the grades for each account to identify the accounts that were over long periods of time the most prescient or the most wrong.

As for the format of Section 6, use the prefix "Article hindsight analysis interestingness score:" and then the score (0-10) as a number. Give high scores to articles/discussions that are prominent, notable, or interesting in retrospect. Give low scores in cases where few predictions are made, or the topic is very niche or obscure, or the discussion is not very interesting in retrospect.

Here is an example:
Article hindsight analysis interestingness score: 8
---
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Submit prompt to GPT 5.1 Thinking via the OpenAI API&lt;/item&gt;
      &lt;item&gt;Collect and parse the results&lt;/item&gt;
      &lt;item&gt;Render the results into static HTML web pages for easy viewing&lt;/item&gt;
      &lt;item&gt;Host the html result pages on my website: https://karpathy.ai/hncapsule/&lt;/item&gt;
      &lt;item&gt;Host all the intermediate results of the &lt;code&gt;data&lt;/code&gt;directory if someone else would like to play. It's the file&lt;code&gt;data.zip&lt;/code&gt;under the exact same url prefix (intentionally avoiding a direct link).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I spent a few hours browsing around and found it to be very interesting. A few example threads just for fun:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;December 3 2015 Swift went open source.&lt;/item&gt;
      &lt;item&gt;December 6 2015 Launch of Figma&lt;/item&gt;
      &lt;item&gt;December 11 2015 original announcement of OpenAI :').&lt;/item&gt;
      &lt;item&gt;December 16 2015 geohot is building Comma&lt;/item&gt;
      &lt;item&gt;December 22 2015 SpaceX launch webcast: Orbcomm-2 Mission&lt;/item&gt;
      &lt;item&gt;December 28 2015 Theranos struggles&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And then when you navigate over to the Hall of Fame, you can find the top commenters of Hacker News in December 2015, sorted by imdb-style score of their grade point average. In particular, congratulations to pcwalton, tptacek, paulmd, cstross, greglindahl, moxie, hannob, 0xcde4c3db, Manishearth, johncolanduoni - GPT 5.1 Thinking found your comments very insightful and prescient. You can also scroll all the way down to find the noise of HN, which I think we're all familiar with too :)&lt;/p&gt;
    &lt;p&gt;My code (wait, Opus' code?) on GitHub can be used to reproduce or tweak the results. Running 31 days of 30 articles through GPT 5.1 Thinking meant &lt;code&gt;31 * 30 =&lt;/code&gt; 930 LLM queries and cost about $58 and somewhere around ~1 hour. The LLM megaminds of the future might find this kind of a thing a lot easier, a lot faster and a lot cheaper.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46220540</guid><pubDate>Wed, 10 Dec 2025 17:23:53 +0000</pubDate></item><item><title>Show HN: Automated license plate reader coverage in the USA</title><link>https://alpranalysis.com</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46220794</guid><pubDate>Wed, 10 Dec 2025 17:42:30 +0000</pubDate></item><item><title>Super Mario 64 for the PS1</title><link>https://github.com/malucard/sm64-psx</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46221925</guid><pubDate>Wed, 10 Dec 2025 18:58:55 +0000</pubDate></item><item><title>Getting a Gemini API key is an exercise in frustration</title><link>https://ankursethi.com/blog/gemini-api-key-frustration/</link><description>&lt;doc fingerprint="3956b1cd9b3799d1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Getting a Gemini API key is an exercise in frustration&lt;/head&gt;
    &lt;p&gt;Last week, I started working on a new side-project. It’s a standard React app partly made up of run-of-the-mill CRUD views—a perfect fit for LLM-assisted programming. I reasoned that if I could get an LLM to quickly write the boring code for me, I’d have more time to focus on the interesting problems I wanted to solve.&lt;/p&gt;
    &lt;p&gt;I’ve pretty much settled on Claude Code as my coding assistant of choice, but I’d been hearing great things about Google’s Gemini 3 Pro. Despite my aversion to Google products, I decided to try it out on my new codebase.&lt;/p&gt;
    &lt;p&gt;I already had Gemini CLI installed, but that only gave me access to Gemini 2.5 with rate limits. I wanted to try out Gemini 3 Pro, and I wanted to avoid being rate limited. I had some spare cash to burn on this experiment, so I went looking for ways to pay for a Gemini Pro plan, if such a thing existed.&lt;/p&gt;
    &lt;p&gt;Thus began my grand adventure in trying to give Google my money.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is a Gemini, really?&lt;/head&gt;
    &lt;p&gt;The name “Gemini” is so overloaded that it barely means anything. Based on the context, Gemini could refer to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The chatbot available at gemini.google.com.&lt;/item&gt;
      &lt;item&gt;The mobile app that lets you use the same Gemini chatbot on your iPhone or Android.&lt;/item&gt;
      &lt;item&gt;The voice assistant on Android phones.&lt;/item&gt;
      &lt;item&gt;The AI features built into Google Workspace, Firebase, Colab, BigQuery, and other Google products.&lt;/item&gt;
      &lt;item&gt;Gemini CLI, an agentic coding tool for your terminal that works the same way as Claude Code or OpenAI Codex.&lt;/item&gt;
      &lt;item&gt;The Gemini Code Assist suite of products, which includes extensions for various IDEs, a GitHub app, and Gemini CLI.&lt;/item&gt;
      &lt;item&gt;The underlying LLM powering all these products.&lt;/item&gt;
      &lt;item&gt;Probably three more products by the time I finish writing this blog post.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To make things even more confusing, Google has at least three different products just for agentic coding: Gemini Code Assist (Gemini CLI is a part of this suite of products), Jules, and Antigravity.&lt;/p&gt;
    &lt;p&gt;And then there’s a bunch of other GenAI stuff that is powered by Gemini but doesn’t have the word Gemini in the name: Vertex AI Platform, Google AI Studio, NotebookLM, and who knows what else.&lt;/p&gt;
    &lt;p&gt;I just wanted to plug my credit card information into a form and get access to a coding assistant. Instead, I was dunked into an alphabet soup of products that all seemed to do similar things and, crucially, didn’t have any giant “Buy Now!” buttons for me to click.&lt;/p&gt;
    &lt;p&gt;In contrast, both Anthropic and OpenAI have two primary ways you can access their products: via their consumer offerings at claude.ai and chatgpt.com respectively, or via API credits that you can buy through their respective developer consoles. In each case, there is a form field where you can plug in your credit card details, and a big, friendly “Buy Now!” button to click.&lt;/p&gt;
    &lt;p&gt;After half an hour of searching the web, I did the obvious thing and asked the free version of Gemini (the chatbot, not one of those other Geminis) what to do:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;How do I pay for the pro version of Gemini so i can use it in the terminal for writing code? I specifically want to use the Gemini 3 Pro model.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It thought for a suspiciously long time and told me that Gemini 3 Pro required a developer API key to use. Since the new model is still in preview, it’s not yet available on any of the consumer plans. When I asked follow up questions about pricing, it told me that “Something went wrong”. Which translates to: we broke something, but we won’t tell you how to fix it.&lt;/p&gt;
    &lt;p&gt;So I asked Claude for help. Between the two LLMs, I was able to figure out how to create an API key for the Gemini I wanted.&lt;/p&gt;
    &lt;head rend="h2"&gt;Creating an API key is easy&lt;/head&gt;
    &lt;p&gt;Google AI Studio is supposed to be the all-in-one dashboard for Google’s generative AI models. This is where you can experiment with model parameters, manage API keys, view logs, and manage billing for your projects.&lt;/p&gt;
    &lt;p&gt;I logged into Google AI Studio and created a new API key. This part was pretty straightforward: I followed the on-screen instructions and had a fresh new key housed under a project in a few seconds. I then verified that my key was working with Gemini CLI.&lt;/p&gt;
    &lt;p&gt;It worked! Now all that was left to do was to purchase some API credits. Back in Google AI Studio, I saw a link titled “Set up billing” next to my key. It looked promising, so I clicked it.&lt;/p&gt;
    &lt;p&gt;That’s where the fun really began.&lt;/p&gt;
    &lt;head rend="h2"&gt;Google doesn’t want my money&lt;/head&gt;
    &lt;p&gt;The “Set up billing” link kicked me out of Google AI Studio and into Google Cloud Console, and my heart sank. Every time I’ve logged into Google Cloud Console or AWS, I’ve wasted hours upon hours reading outdated documentation, gazing in despair at graphs that make no sense, going around in circles from dashboard to dashboard, and feeling a strong desire to attain freedom from this mortal coil.&lt;/p&gt;
    &lt;p&gt;Turns out I can’t just put $100 into my Gemini account. Instead, I must first create a Billing Account. After I’ve done that, I must associate it with a project. Then I’m allowed to add a payment method to the Billing Account. And then, if I’m lucky, my API key will turn into a paid API key with Gemini Pro privileges.&lt;/p&gt;
    &lt;p&gt;So I did the thing. The whole song and dance. Including the mandatory two-factor OTP verification that every Indian credit card requires. At the end of the process, I was greeted with a popup telling me I had to verify my payment method before I’d be allowed to use it.&lt;/p&gt;
    &lt;p&gt;Wait. Didn’t I just verify my payment method? When I entered the OTP from my bank?&lt;/p&gt;
    &lt;p&gt;Nope, turns out Google hungers for more data. Who’d have thunk it?&lt;/p&gt;
    &lt;p&gt;To verify my payment method for reals, I had to send Google a picture of my government-issued ID and the credit card I’d just associated with my Billing Account. I had to ensure all the numbers on my credit card were redacted by manually placing black bars on top of them in an image editor, leaving only my name and the last four digits of the credit card number visible.&lt;/p&gt;
    &lt;p&gt;This felt unnecessarily intrusive. But by this point, I was too deep in the process to quit. I was invested. I needed my Gemini 3 Pro, and I was willing to pay any price.&lt;/p&gt;
    &lt;p&gt;The upload form for the government ID rejected my upload twice before it finally accepted it. It was the same exact ID every single time, just in different file formats. It wanted a PNG file. Not a JPG file, nor a PDF file, but a PNG file. Did the upload form mention that in the instructions? Of course not.&lt;/p&gt;
    &lt;p&gt;After jumping through all these hoops, I received an email from Google telling me that my verification will be completed in a few days.&lt;/p&gt;
    &lt;p&gt;A few days? Nothing to do but wait, I suppose.&lt;/p&gt;
    &lt;head rend="h2"&gt;403 Forbidden&lt;/head&gt;
    &lt;p&gt;At this point, I closed all my open Cloud Console tabs and went back to work. But when I was fifteen minutes into writing some code by hand like a Neanderthal, I received a second email from Google telling me that my verification was complete.&lt;/p&gt;
    &lt;p&gt;So for the tenth time that day, I navigated to AI Studio. For the tenth time I clicked “Set up billing” on the page listing my API keys. For the tenth time I was told that my project wasn’t associated with a billing account. For the tenth time I associated the project with my new billing account. And finally, after doing all of this, the “Quota tier” column on the page listing my API keys said “Tier 1” instead of “Set up billing”.&lt;/p&gt;
    &lt;p&gt;Wait, Tier 1? Did that mean there were other tiers? What were tiers, anyway? Was I already on the best tier? Or maybe I was on the worst one? Not important. The important part was that I had my API key and I’d managed to convince Google to charge me for it.&lt;/p&gt;
    &lt;p&gt;I went back to the Gemini CLI, ran the &lt;code&gt;/settings&lt;/code&gt; command, and turned on the “Enable experimental features” option. I ran the &lt;code&gt;/models&lt;/code&gt; command, which told me that Gemini 3 Pro was now available.&lt;/p&gt;
    &lt;p&gt;Success? Not yet.&lt;/p&gt;
    &lt;p&gt;When I tried sending a message to the LLM, it failed with this 403 error:&lt;/p&gt;
    &lt;code&gt;{
  "error": {
    "message": "{\n  \"error\": {\n    \"code\": 403,\n    \"message\": \"The caller does not have permission\",\n    \"status\":\"PERMISSION_DENIED\"\n  }\n}\n",
    "code": 403,
    "status": "Forbidden"
  }
}&lt;/code&gt;
    &lt;p&gt;Is that JSON inside a string inside JSON? Yes. Yes it is.&lt;/p&gt;
    &lt;p&gt;To figure out if my key was even working, I tried calling the Gemini API from JavaScript, reproducing the basic example from Google’s own documentation.&lt;/p&gt;
    &lt;p&gt;No dice. I ran into the exact same error.&lt;/p&gt;
    &lt;p&gt;I then tried talking to Gemini 3 Pro using the Playground inside Google AI Studio. It showed me a toast message saying &lt;code&gt;Failed to generate content. Please try again.&lt;/code&gt; The chat transcript said &lt;code&gt;An internal error has occurred.&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;At this point I gave up and walked away from my computer. It was already 8pm. I’d been trying to get things to work since 5pm. I needed to eat dinner, play Clair Obscur, and go to bed. I had no more time to waste and no more fucks to give.&lt;/p&gt;
    &lt;head rend="h2"&gt;Your account is in good standing at this time&lt;/head&gt;
    &lt;p&gt;Just as I was getting into bed, I received an email from Google with this subject line:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Your Google Cloud and APIs billing account XXXXXX-XXXXXX-XXXXXX is in good standing at this time.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;With the message inside saying:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Based on the information you provided and further analysis by Google, we have reinstated your billing account XXXXXX-XXXXXX-XXXXXX. Your account is in good standing, and you should now have full access to your account and related Project(s) and Service(s).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I have no idea what any of this means, but Gemini 3 Pro started working correctly after I received this email. It worked in the Playground, directly by calling the API from JavaScript, and with Gemini CLI.&lt;/p&gt;
    &lt;p&gt;Problem solved, I guess. Until Google mysteriously decides that my account is no longer in good standing.&lt;/p&gt;
    &lt;head rend="h2"&gt;This was a waste of time&lt;/head&gt;
    &lt;p&gt;This was such a frustrating experience that I still haven’t tried using Gemini with my new codebase, nearly a week after I made all those sacrifices to the Gods of Billing Account.&lt;/p&gt;
    &lt;p&gt;I understand why the process for getting a Gemini API key is so convoluted. It’s designed for large organizations, not an individual developers trying to get work done; it serves the bureaucracy, not the people doing the work; it’s designed for maximum compliance with government regulations, not for efficiency or productivity.&lt;/p&gt;
    &lt;p&gt;Google doesn’t want my money unless I’m an organization that employs ten thousand people.&lt;/p&gt;
    &lt;p&gt;In contrast to Google, Anthropic and OpenAI are much smaller and much more nimble. They’re able to make the process of setting up a developer account quick and easy for those of us who just want to get things done. Unlike Google, they haven’t yet become complacent. They need to compete for developer mindshare if they are to survive a decade into the future. Maybe they’ll add the same level of bureaucracy to their processes as they become larger, but for now they’re fairly easy to deal with.&lt;/p&gt;
    &lt;p&gt;I’m still going to try using Gemini 3 Pro with Gemini CLI as my coding assistant, but I’ll probably cap the experiment to a month. Unless Gemini 3 Pro is a massive improvement over its competitors, I’ll stick to using tools built by organizations that want me as a customer.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46223311</guid><pubDate>Wed, 10 Dec 2025 20:29:12 +0000</pubDate></item><item><title>Patterns.dev</title><link>https://www.patterns.dev/</link><description>&lt;doc fingerprint="ed186110298694bb"&gt;
  &lt;main&gt;
    &lt;p&gt;Interested in our next book? Learn more about Building Large-scale JavaScript Web Apps with React&lt;/p&gt;
    &lt;p&gt;Patterns.dev is a free online resource on design, rendering, and performance patterns for building powerful web apps with vanilla JavaScript or modern frameworks.&lt;/p&gt;
    &lt;p&gt;We publish patterns, tips and tricks for improving how you architect apps for free. Keep in mind, design patterns are descriptive, not prescriptive . They can guide you when facing a problem other developers have encountered many times before, but are not a blunt tool for jamming into every scenario. Patterns.dev aims to be a catalog of patterns (for increasing awareness) rather than a checklist (what you must do).&lt;/p&gt;
    &lt;p&gt;Design patterns are a fundamental part of software development, as they provide typical solutions to commonly recurring problems in software design.&lt;/p&gt;
    &lt;p&gt;A common critique of design patterns is that they needlessly add complexity.&lt;/p&gt;
    &lt;p&gt;Our perspective is that patterns are valuable for solving specific problems, often helping to communicate comminalities in code problems for humans. If a project doesn't have those problems, there isn't a need to apply them. Patterns can also be very language or framework-specific (e.g. React), which can often mean thinking beyond the scope of just the original GoF design patterns.&lt;/p&gt;
    &lt;p&gt;Learn about web performance patterns for loading your code more efficiently. Unsure how to think about modern approaches to loading or rendering user-experiences? We've got you covered.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46226483</guid><pubDate>Thu, 11 Dec 2025 01:18:55 +0000</pubDate></item><item><title>Incomplete list of mistakes in the design of CSS</title><link>https://wiki.csswg.org/ideas/mistakes</link><description>&lt;doc fingerprint="3f4ffa43dd2d5fa3"&gt;
  &lt;main&gt;&lt;p&gt;That should be corrected if anyone invents a time machine. :P&lt;/p&gt;&lt;code&gt;white-space: nowrap&lt;/code&gt; should be &lt;code&gt;white-space: no-wrap&lt;/code&gt;&lt;code&gt;white-space&lt;/code&gt;&lt;code&gt;animation-iteration-count&lt;/code&gt; should just have been &lt;code&gt;animation-count&lt;/code&gt; (like &lt;code&gt;column-count&lt;/code&gt;!)&lt;code&gt;vertical-align&lt;/code&gt; should not apply to table cells. Instead the CSS3 alignment properties should exist in Level 1.&lt;code&gt;vertical-align: middle&lt;/code&gt; should be &lt;code&gt;text-middle&lt;/code&gt; or &lt;code&gt;x-middle&lt;/code&gt; because it's not really in the middle, and such a name would better describes what it does.&lt;code&gt;fill-available&lt;/code&gt; rather than being undefined in auto situations.&lt;code&gt;border-box&lt;/code&gt; by default.&lt;code&gt;background-size&lt;/code&gt; with one value should duplicate its value, not default the second one to &lt;code&gt;auto&lt;/code&gt;. Ditto &lt;code&gt;translate()&lt;/code&gt;.&lt;code&gt;background-position&lt;/code&gt; and &lt;code&gt;border-spacing&lt;/code&gt; (all 2-axis properties) should take *vertical* first, to match with the 4-direction properties like &lt;code&gt;margin&lt;/code&gt;.&lt;code&gt;margin&lt;/code&gt; should go counter-clockwise (so that the inline-start value is before the block-end and inline-end values instead of after them).&lt;code&gt;z-index&lt;/code&gt; should be called &lt;code&gt;z-order&lt;/code&gt; or &lt;code&gt;depth&lt;/code&gt; and should Just Work on all elements (like it does on flex items).&lt;code&gt;word-wrap&lt;/code&gt;/&lt;code&gt;overflow-wrap&lt;/code&gt; should not exist. Instead, &lt;code&gt;overflow-wrap&lt;/code&gt; should be a keyword on 'white-space', like &lt;code&gt;nowrap&lt;/code&gt; (&lt;code&gt;no-wrap&lt;/code&gt;).&lt;code&gt;currentColor&lt;/code&gt; keyword should have retained the dash, &lt;code&gt;current-color&lt;/code&gt;, as originally specified. Likewise all other color multi-word keyword names.&lt;code&gt;border-radius&lt;/code&gt; should have been &lt;code&gt;corner-radius&lt;/code&gt;.&lt;code&gt;hyphens&lt;/code&gt; property should be called &lt;code&gt;hyphenate&lt;/code&gt;. (It's called &lt;code&gt;hyphens&lt;/code&gt; because the XSL:FO people objected to &lt;code&gt;hyphenate&lt;/code&gt;.)&lt;code&gt;rgba()&lt;/code&gt; and &lt;code&gt;hsla()&lt;/code&gt; should not exist, &lt;code&gt;rgb()&lt;/code&gt; and &lt;code&gt;hsl()&lt;/code&gt;  should have gotten an optional fourth parameter instead (and the alpha value should have used the same format as R, G, and B or S and L).&lt;code&gt;»&lt;/code&gt; and indirect sibling combinator should have been &lt;code&gt;++&lt;/code&gt;, so there's some logical relationships among the selectors' ascii art&lt;code&gt;*-blend-mode&lt;/code&gt; properties should've just been &lt;code&gt;*-blend&lt;/code&gt;&lt;code&gt;u0001-u00c8&lt;/code&gt;.&lt;code&gt;font-family&lt;/code&gt; should have required the font name to be quoted (like all other values that come from “outside” CSS).  The rules for handling unquoted font names make parsing &lt;code&gt;font&lt;/code&gt; stupid, as it requires a &lt;code&gt;font-size&lt;/code&gt; value for disambiguation.&lt;code&gt;flex-basis&lt;/code&gt; vs &lt;code&gt;width&lt;/code&gt;/&lt;code&gt;height&lt;/code&gt;.  Perhaps: if &lt;code&gt;width&lt;/code&gt;/&lt;code&gt;height&lt;/code&gt; is &lt;code&gt;auto&lt;/code&gt;, use &lt;code&gt;flex-basis&lt;/code&gt;; otherwise, stick with &lt;code&gt;width&lt;/code&gt;/&lt;code&gt;height&lt;/code&gt; as an inflexible size.  (This also makes min/max width/height behavior fall out of the generic definition.)&lt;code&gt;:empty&lt;/code&gt; should have been &lt;code&gt;:void&lt;/code&gt;, and &lt;code&gt;:empty&lt;/code&gt; should select items that contain only white space&lt;code&gt;table-layout: fixed; width: auto&lt;/code&gt; should result in a fill-available table with fixed-layout columns.&lt;code&gt;text-orientation&lt;/code&gt; should have had &lt;code&gt;upright&lt;/code&gt; as the initial value (given the latest changes to 'writing-mode').&lt;code&gt;@import&lt;/code&gt; rule is required to (a) always hit the network unless you specify cache headers, and (b) construct fresh CSSStyleSheet objects for every import, even if they're identical. It should have had more aggressive URL-based deduping and allowed sharing of stylesheet objects.&lt;code&gt;:link&lt;/code&gt; should have had the &lt;code&gt;:any-link&lt;/code&gt; semantics all along.&lt;code&gt;flex&lt;/code&gt; shorthand (and &lt;code&gt;flex-shrink&lt;/code&gt; and &lt;code&gt;flex-grow&lt;/code&gt; longhands) should accept &lt;code&gt;fr&lt;/code&gt; units instead of bare numbers to represent flex fractions.&lt;code&gt;display&lt;/code&gt; property should be called &lt;code&gt;display-type&lt;/code&gt;.&lt;code&gt;list-style&lt;/code&gt; properties should be called &lt;code&gt;marker-style&lt;/code&gt;, and &lt;code&gt;list-item&lt;/code&gt; renamed to &lt;code&gt;marked-block&lt;/code&gt; or something.&lt;code&gt;text-overflow&lt;/code&gt; property should always apply, not be dependent on &lt;code&gt;overflow&lt;/code&gt;&lt;code&gt;line-height: &amp;lt;percentage&amp;gt;&lt;/code&gt; should compute to the equivalent &lt;code&gt;line-height: &amp;lt;number&amp;gt;&lt;/code&gt;, so that it effectively inherits as a percentage not a length&lt;code&gt;::placeholder&lt;/code&gt; should be &lt;code&gt;::placeholder-text&lt;/code&gt; and &lt;code&gt;:placeholder-shown&lt;/code&gt; should be &lt;code&gt;:placeholder&lt;/code&gt;&lt;code&gt;overflow: scroll&lt;/code&gt; should introduce a stacking context&lt;code&gt;size&lt;/code&gt; should have been a shorthand for &lt;code&gt;width&lt;/code&gt; and &lt;code&gt;height&lt;/code&gt; instead of an &lt;code&gt;@page&lt;/code&gt; property with a different definition&lt;code&gt;span&lt;/code&gt;) with idents in the grid properties, possibly by using functional notation (like &lt;code&gt;span(2)&lt;/code&gt;).&lt;code&gt;align-inline-*&lt;/code&gt; and &lt;code&gt;align-block-*&lt;/code&gt;.&lt;code&gt;shape-outside&lt;/code&gt; should have had &lt;code&gt;wrap-&lt;/code&gt; in the name somehow, as people assume the shape should also clip the content as in &lt;code&gt;clip-path&lt;/code&gt;.&lt;code&gt;!important&lt;/code&gt; — that reads to engineers as “not important”. We should have picked another way to write this.&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46227619</guid><pubDate>Thu, 11 Dec 2025 04:20:52 +0000</pubDate></item><item><title>The Cost of a Closure in C</title><link>https://thephd.dev/the-cost-of-a-closure-in-c-c2y</link><description>&lt;doc fingerprint="ee43ba392a6d3084"&gt;
  &lt;main&gt;
    &lt;p&gt;I had a vague idea that closures could have a variety of performance implications; I did not believe that so many of the chosen and potential designs for C and C++ extensions ones, however, were so… suboptimal.&lt;/p&gt;
    &lt;p&gt;But, before we get into how these things perform and what the cost of their designs are, we need to talk about what Closures are.&lt;/p&gt;
    &lt;head rend="h1"&gt;“Closures”?&lt;/head&gt;
    &lt;p&gt;Closures in this instance are programming language constructs that includes data alongside instructions that are not directly related to their input (arguments) and their results (return values). They can be seen as a “generalization” of the concept of a function or function call, in that a function call is a “subset” of closures (e.g., the set of closures that do not include this extra, spicy data that comes from places outside of arguments and returns). These generalized functions and generalized function objects hold the ability to do things like work with “instance” data that is not passed to it directly (i.e., variables surrouding the closure off the stack) and, usually, some way to carry around more data than is implied by their associated function signature.&lt;/p&gt;
    &lt;p&gt;Pretty much all recent and modern languages include something for Closures unless they are deliberately developing for a target audience or for a source code design that is too “low level” for such a concept (such as Stack programming languages, Bytecode languages, or ones that fashion themselves as assembly-like or close to it). However, we’re going to be focusing on and looking specifically at Closures in C and C++, since this is going to be about trying to work with and – eventually – standardize something for ISO C that works for everyone.&lt;/p&gt;
    &lt;p&gt;First, let’s show a typical problem that arises in C code to show why closure solutions have popped up all over the C ecosystem, then talk about it in the context of the various solutions.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Closure Problem&lt;/head&gt;
    &lt;p&gt;The closure problem can be neatly described by as “how do I get extra data to use within this &lt;code&gt;qsort&lt;/code&gt; call?”. For example, consider setting this variable, &lt;code&gt;in_reverse&lt;/code&gt;, as part of a bit of command line shenanigans, to change how a sort happens:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;stddef.h&amp;gt;

static int in_reverse = 0;

int compare(const void* untyped_left, const void* untyped_right) {
  const int* left = untyped_left;
  const int* right = untyped_right;
  return (in_reverse) ? *right - *left : *left - *right;
}

int main(int argc, char* argv[]) {
  if (argc &amp;gt; 1) {
    char* r_loc = strchr(argv[1], 'r');
    if (r_loc != NULL) {
      ptrdiff_t r_from_start = (r_loc - argv[1]);
      if (r_from_start == 1 &amp;amp;&amp;amp; argv[1][0] == '-' &amp;amp;&amp;amp; strlen(r_loc) == 1) {
        in_reverse = 1;
      } 
    }
  }
  int list[] = { 2, 11, 32, 49, 57, 20, 110, 203 };
  qsort(list, (sizeof(list)/sizeof(*list)), sizeof(*list), compare);
	
  return list[0];
}
&lt;/code&gt;
    &lt;p&gt;This uses a &lt;code&gt;static&lt;/code&gt; variable to have it persist between both the &lt;code&gt;compare&lt;/code&gt; function calls that &lt;code&gt;qsort&lt;/code&gt; makes and the &lt;code&gt;main&lt;/code&gt; call which (potentially) changes its value to be &lt;code&gt;1&lt;/code&gt; instead of &lt;code&gt;0&lt;/code&gt;. Unfortunately, this isn’t always the best idea for more complex programs that don’t fit within a single snippet:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;it is impossible to have different “copies” of a &lt;code&gt;static&lt;/code&gt;variable, meaning all mutations done in all parts of the program that can see&lt;code&gt;in_reverse&lt;/code&gt;are responsible for knowing the state before and after (e.g., heavily stateful programming of state that you may not own / cannot see);&lt;/item&gt;
      &lt;item&gt;working on &lt;code&gt;static&lt;/code&gt;data may produce thread contention/race conditions in more complex programs;&lt;/item&gt;
      &lt;item&gt;using &lt;code&gt;_Thread_local&lt;/code&gt;instead of&lt;code&gt;static&lt;/code&gt;only solves the race condition problem but does not solve the “shared across several places on the same thread” problem;&lt;/item&gt;
      &lt;item&gt;referring to specific pieces of data or local pieces of data (like &lt;code&gt;list&lt;/code&gt;itself) become impossible;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;and so on, and so forth. This is the core of the problem here. It becomes more pronounced when you want to do things with function and data that are a bit more complex, such as Donald Knuth’s “Man-or-Boy” test code.&lt;/p&gt;
    &lt;p&gt;The solutions to these problems come in 4 major flavors in C and C++ code.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Just reimplement the offending function to take a userdata pointer so you can pass whatever data you want (typical C solution, e.g. going from &lt;code&gt;qsort&lt;/code&gt;as the sorting function to BSD’s&lt;code&gt;qsort_r&lt;/code&gt;1 or Annex K’s&lt;code&gt;qsort_s&lt;/code&gt;2).&lt;/item&gt;
      &lt;item&gt;Use GNU Nested Functions to just Refer To What You Want Anyways.&lt;/item&gt;
      &lt;item&gt;Use Apple Blocks to just Refer To What You Want Anyways.&lt;/item&gt;
      &lt;item&gt;Use C++ Lambdas and some elbow grease to just Refer To What You Want Anyways.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each solution has drawbacks and benefits insofar as usability and design, but as a quick overview we’ll show what it’s like using &lt;code&gt;qsort&lt;/code&gt; (or &lt;code&gt;qsort_r&lt;/code&gt;/&lt;code&gt;qsort_s&lt;/code&gt;, where applicable). Apple Blocks, for starters, looks like this:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;stddef.h&amp;gt;

int main(int argc, char* argv[]) {
	// local, non-static variable
	int in_reverse = 0;

	// value changed in-line
	if (argc &amp;gt; 1) {
		char* r_loc = strchr(argv[1], 'r');
		if (r_loc != NULL) {
			ptrdiff_t r_from_start = (r_loc - argv[1]);
			if (r_from_start == 1 &amp;amp;&amp;amp; argv[1][0] == '-' &amp;amp;&amp;amp; strlen(r_loc) == 1) {
				in_reverse = 1;
			} 
		}
	}
	
	int list[] = { 2, 11, 32, 49, 57, 20, 110, 203 };
	
	qsort_b(list, (sizeof(list)/sizeof(*list)), sizeof(*list),
		// Apple Blocks are Block Expressions, meaning they do not have to be stored
		// in a variable first
		^(const void* untyped_left, const void* untyped_right) {
			const int* left = untyped_left;
			const int* right = untyped_right;
			return (in_reverse) ? *right - *left : *left - *right;
		}
	);
	
	return list[0];
}
&lt;/code&gt;
    &lt;p&gt;and GNU Nested Functions look like this:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;stddef.h&amp;gt;

int main(int argc, char* argv[]) {
	// local, non-static variable
	int in_reverse = 0;

	// modify variable in-line
	if (argc &amp;gt; 1) {
		char* r_loc = strchr(argv[1], 'r');
		if (r_loc != NULL) {
			ptrdiff_t r_from_start = (r_loc - argv[1]);
			if (r_from_start == 1 &amp;amp;&amp;amp; argv[1][0] == '-' &amp;amp;&amp;amp; strlen(r_loc) == 1) {
				in_reverse = 1;
			} 
		}
	}
	
	int list[] = { 2, 11, 32, 49, 57, 20, 110, 203 };
	
	// GNU Nested Function definition, can reference `in_reverse` directly
	// is a declaration/definition, and cannot be used directly inside of `qsort`
	int compare(const void* untyped_left, const void* untyped_right) {
		const int* left = untyped_left;
		const int* right = untyped_right;
		return (in_reverse) ? *right - *left : *left - *right;
	}
	// use in the sort function without the need for a `void*` parameter
	qsort(list, (sizeof(list)/sizeof(*list)), sizeof(*list), compare);
	
	return list[0];
}
&lt;/code&gt;
    &lt;p&gt;or, finally, C++-style Lambdas:&lt;/p&gt;
    &lt;code&gt;#define __STDC_WANT_LIB_EXT1__ 1

#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;stddef.h&amp;gt;

int main(int argc, char* argv[]) {
	int in_reverse = 0;
	
	if (argc &amp;gt; 1) {
		char* r_loc = strchr(argv[1], 'r');
		if (r_loc != NULL) {
			ptrdiff_t r_from_start = (r_loc - argv[1]);
			if (r_from_start == 1 &amp;amp;&amp;amp; argv[1][0] == '-' &amp;amp;&amp;amp; strlen(r_loc) == 1) {
				in_reverse = 1;
			} 
		}
	}
	
	// lambdas are expressions, but we can assign their unique variable types with `auto`
	auto compare = [&amp;amp;](const void* untyped_left, const void* untyped_right) {
		const int* left = (const int*)untyped_left;
		const int* right = (const int*)untyped_right;
		return (in_reverse) ? *right - *left : *left - *right;
	};

	int list[] = { 2, 11, 32, 49, 57, 20, 110, 203 };	

	// C++ Lambdas don't automatically make a trampoline, so we need to provide
	// one ourselves for the `qsort_s/r` case so we can call the lambda
	auto compare_trampoline = [](const void* left, const void* right, void* user) {
		typeof(compare)* p_compare = user;
		return (*p_compare)(left, right);
	};
	qsort_s(list, (sizeof(list)/sizeof(*list)), sizeof(*list), compare_trampoline, &amp;amp;compare);

	return list[0];
}
&lt;/code&gt;
    &lt;p&gt;To solve this gaggle of problems, pretty much every semi-modern language (that isn’t assembly-adjacent or based on some kind of state/stack programming) provide some idea of being able to associate some set of data with one or more function calls. And, particularly for Closures, this is done in a local way without passing it as an explicit argument. As it turns out, all of those design choices – including the ones in C – have pretty significant consequences on not just usability, but performance.&lt;/p&gt;
    &lt;head rend="h1"&gt;Not A Big Overview&lt;/head&gt;
    &lt;p&gt;This article is NOT going to talk in-depth about the design of all of the alternatives or other languages. We’re focused on the actual cost of the extensions and what they mean. A detailed overview of the design tradeoffs, their security implications, and other problems, can be read at the ISO C Proposal for Functions with Closures here; it also gets into things like Security Implications, ABI, current implementation impact, and more of the various designs. The discussion in the paper is pretty long and talks about the dozens of aspects of each solution down to both the design aspect and the implementation quirks. We encourage you to dive into that proposal and read it to figure out if there’s something more specific you care about insofar as some specific design portion. But, this article is going to be concerned about one thing and one thing only:&lt;/p&gt;
    &lt;head rend="h1"&gt;Purrrrrrrformance :3!&lt;/head&gt;
    &lt;p&gt;In order to measure this cost, we are going to take Knuth’s Man-or-Boy test and benchmark various styles of implementation in C and C++ using various different extensions / features for the Closure problem. The Man-or-Boy test is an efficient measure of how well your programming language can handle referring to specific entities while engaging in a large degree of recursion and self-reference. It can stress test various portions of how your program creates and passes around data associated with a function call, and if your programming language design is so goofy that it can’t refer to a specific instance of a variable or function argument, it will end up producing the wrong answer and breaking horrifically.&lt;/p&gt;
    &lt;head rend="h2"&gt;Anatomy of a Benhcmark: Raw C&lt;/head&gt;
    &lt;p&gt;Here is the core of the Man-or-Boy test, as implemented in raw C. This implementation3 and all the others are available online for us all to scrutinize and yell at me for messing up, to make sure I’m not slandering your favorite solution for Closures in this space.&lt;/p&gt;
    &lt;code&gt;// ...

static int eval(ARG* a) {
	return a-&amp;gt;fn(a);
}

static int B(ARG* a) {
	int k    = *a-&amp;gt;k -= 1;
	ARG args = { B, &amp;amp;k, a, a-&amp;gt;x1, a-&amp;gt;x2, a-&amp;gt;x3, a-&amp;gt;x4 };
	return A(&amp;amp;args);
}

static int A(ARG* a) {
	return *a-&amp;gt;k &amp;lt;= 0 ? eval(a-&amp;gt;x4) + eval(a-&amp;gt;x5) : B(a);
}

// ...
&lt;/code&gt;
    &lt;p&gt;You will notice that there is a big, fat, ugly &lt;code&gt;ARG*&lt;/code&gt; parameter hanging around all of these functions. That is because, as stated before, plain ISO C cannot handle passing the data around unless it’s part of a function’s arguments. Because the actual core of the Man-or-Boy experiment is the ability to refer to specific values of &lt;code&gt;k&lt;/code&gt; that exist during the recursive run of the program, we need to actually modify the function signature and thereby cheat some of the implicit Man-or-Boy requirements of not passing the value in directly. Here’s what &lt;code&gt;ARG&lt;/code&gt; looks like:&lt;/p&gt;
    &lt;code&gt;typedef struct arg {
	int (*fn)(struct arg*);
	int* k;
	struct arg *x1, *x2, *x3, *x4, *x5;
} ARG;

static int f_1(ARG* _) {
	return -1;
}

static int f0(ARG* _) {
	return 0;
}

static int f1(ARG* _) {
	return 1;
}

static int eval(ARG* a) {
	// ...
}
// ...
&lt;/code&gt;
    &lt;p&gt;And this is how it gets used in the main body of the function in order to compute the right answer and benchmark it:&lt;/p&gt;
    &lt;code&gt;static void normal_functions_rosetta(benchmark::State&amp;amp; state) {
	const int initial_k  = k_value();
	const int expected_k = expected_k_value();
	int64_t result       = 0;

	for (auto _ : state) {
		int k     = initial_k;
		ARG arg1  = { f1, NULL, NULL, NULL, NULL, NULL, NULL };
		ARG arg2  = { f_1, NULL, NULL, NULL, NULL, NULL, NULL };
		ARG arg3  = { f_1, NULL, NULL, NULL, NULL, NULL, NULL };
		ARG arg4  = { f1, NULL, NULL, NULL, NULL, NULL, NULL };
		ARG arg5  = { f0, NULL, NULL, NULL, NULL, NULL, NULL };
		ARG args  = { B, &amp;amp;k, &amp;amp;arg1, &amp;amp;arg2, &amp;amp;arg3, &amp;amp;arg4, &amp;amp;arg5 };
		int value = A(&amp;amp;args);
		result += value == expected_k ? 1 : 0;
	}

	if (result != state.iterations()) {
		state.SkipWithError("failed: did not produce the right answer!");
	}
}

BENCHMARK(normal_functions_rosetta);
&lt;/code&gt;
    &lt;p&gt;Everything within the &lt;code&gt;for (auto _ : state) { ... }&lt;/code&gt; is benchmarked. For those paying attention to the code and find it looking familiar, it’s because that code is the basic structure all Google Benchmark4 code finds itself looking like. I’ve wanted to swap to Catch25 for a long time now to change to their benchmarking infrastructure, but I’ve been stuck on Google Benchmark because I’ve made a lot of graph-making tools based on its JSON output and I have not vetted Catch2’s JSON output yet to see if it has all of the necessary bits ‘n’ bobbles I use to de-dedup runs and compute statistics.&lt;/p&gt;
    &lt;p&gt;Everything outside is setup (the part above the &lt;code&gt;for&lt;/code&gt; loop) or teardown/test correction (the part below the &lt;code&gt;for&lt;/code&gt; loop). The initialization of the &lt;code&gt;ARG args&lt;/code&gt;s cannot be moved outside of the measuring loop because each invocation of &lt;code&gt;A&lt;/code&gt; – the core of the Man-or-Boy experiment – modifies the &lt;code&gt;k&lt;/code&gt; of the ARG parameter, so all of them have to be inside. Conceivably, &lt;code&gt;arg1 .. 5&lt;/code&gt; could be moved out of the loop, but I am very tired of looking at the eight or nine variations of this code so someone else can move it and tell me if Clang or GCC has lots of compiler optimization sauce and doesn’t understand that those 5 &lt;code&gt;argI&lt;/code&gt;s can be hoisted out of the loop.&lt;/p&gt;
    &lt;p&gt;The value &lt;code&gt;k&lt;/code&gt; is &lt;code&gt;10&lt;/code&gt;, and &lt;code&gt;expected_k&lt;/code&gt; is &lt;code&gt;-67&lt;/code&gt;. The expected, returned &lt;code&gt;k&lt;/code&gt; value is dependent on the input &lt;code&gt;k&lt;/code&gt; value, which controls how deep the Man-or-Boy test would recurse on itself to produce its answer. Therefore, to prevent GCC and Clang and other MEGA POWERFUL PILLAR COMPILERS from optimizing the entire thing out and just replacing the benchmark loop with &lt;code&gt;ret -67&lt;/code&gt;, both &lt;code&gt;k_value()&lt;/code&gt; and &lt;code&gt;expected_k_value()&lt;/code&gt; come from a Dynamic Link Library (&lt;code&gt;.dylib&lt;/code&gt; on MacOS, &lt;code&gt;.so&lt;/code&gt; on *nix platforms, &lt;code&gt;.dll&lt;/code&gt; on Windows platforms) to make sure that NO amount of optimization (Link Time Optimization/Link Time Code Generation, Inlining Optimization, Cross-Translation Unit Optimization, and Automatic Constant Expression Optimization) from C or C++ compilers could fully preempt all forms of computation.&lt;/p&gt;
    &lt;p&gt;This allows us to know, for sure, that we’re actually measuring something and not just testing how fast a compiler can load a number into a register and test it against &lt;code&gt;state.iterations()&lt;/code&gt;. And, since we know for sure, we can now talk the general methodology.&lt;/p&gt;
    &lt;head rend="h1"&gt;Methodology&lt;/head&gt;
    &lt;p&gt;The tests were ran on a dying 13-inch 2020 MacBook Pro M1 that has suffered several toddler spills and two severe falls. It has 16 GB of RAM and is son MacOS 15.7.2 Sequoia at the time the test was taken, using the stock MacOS AppleClang Compiler and the stock &lt;code&gt;brew install gcc&lt;/code&gt; compiler in order to produce the numbers seen on December 6th, 2025.&lt;/p&gt;
    &lt;p&gt;There 2 measures being conducted: Real Time and CPU Time. The time is gathered by running a single iteration of the code within the &lt;code&gt;for&lt;/code&gt; loop anywhere from a couple thousand to hundreds of thousands of times to produce confidence in that run of the benchmark. This is then averaged to produce the first point. The process is repeated 50 times, repeating that many iterations to build further confidence in the measurement. All 50 means are used as the points for the values, and the average of all of those 50 means is then used as the height of a bar in a bar graph.&lt;/p&gt;
    &lt;p&gt;The bars are presented side-by-side as a horizontal bar chart with 11 categories of C or C++ code being measured. The 11 categories are:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;no-op&lt;/code&gt;: Literally doing nothing. It’s just there to test environmental noise and make sure none of our benchmarks are so off-base that we’re measuring noise rather than computation. Helps keep us grounded in reality.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lambdas (No Function Helpers)&lt;/code&gt;: a solution using C++-style lambdas. Rather than using helper functions like&lt;code&gt;f0&lt;/code&gt;,&lt;code&gt;f1&lt;/code&gt;, and&lt;code&gt;f_1&lt;/code&gt;, we compute a raw lambda that stores the value meant to be returned for the Man-or-Boy test (&lt;code&gt;return i;&lt;/code&gt;) in the lambda itself and then pass that uniquely-typed lambda to the core of the test. The entire test is templated and uses a fake&lt;code&gt;recursion&lt;/code&gt;template parameter to halt the recursion after a certain depth.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lambdas&lt;/code&gt;: The same as above but actually using&lt;code&gt;int f0(void)&lt;/code&gt;, etc. helper functions at the start rather than lambdas. Reduces inliner pressure by using “normal” types which do not add to the generated number of lambda-typed, recursive, templated function calls.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lambdas (std::function_ref)&lt;/code&gt;: The same as above, but rather than using a function template to handle each uniquely-typed lambda like a precious baby bird, it instead erases the lambda behind a&lt;code&gt;std::function_ref&amp;lt;int(void)&amp;gt;&lt;/code&gt;. This allows the recursive function to retain exactly one signature.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lambdas (std::function)&lt;/code&gt;: The same as above, but replaces&lt;code&gt;std::function_ref&amp;lt;int(void)&amp;gt;&lt;/code&gt;with&lt;code&gt;std::function&amp;lt;int(void)&amp;gt;&lt;/code&gt;. This is its allocating, C++03-style type.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lambdas (Rosetta Code)&lt;/code&gt;: The code straight out of the C++11 Rosetta Code Lambda section on the Man-or-Boy Rosetta Code implementation.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Apple Blocks&lt;/code&gt;: Uses Apple Blocks to implement the test, along with the&lt;code&gt;__block&lt;/code&gt;specifier to refer directly to certain variables on the stack.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GNU Nested Functions (Rosetta Code)&lt;/code&gt;: The code straight out of the C Rosetta Code section on the Man-or-Boy Rosetta Code implementation.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GNU Nested Functions&lt;/code&gt;: GNU Nested Functions similar to the Rosetta Code implementation, but with some slight modifications in a hope to potentially alleviate some stack pressure if possible by using regular helper functions like&lt;code&gt;f0&lt;/code&gt;,&lt;code&gt;f1&lt;/code&gt;, and&lt;code&gt;f_1&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Custom C++ Class&lt;/code&gt;: A custom-written C++ class using a discriminated union to decide whether its doing a straight function call or attemping to engage in the Man-or-Boy recursion.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;C++03 shared_ptr (Rosetta Code)&lt;/code&gt;: A C++ class using&lt;code&gt;std::enable_shared_from_this&lt;/code&gt;and&lt;code&gt;std::shared_ptr&lt;/code&gt;with a virtual function call to invoke the “right” function call during recursion.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The two compilers tested are Apple Clang 17 and GCC 15. There are two graph images because one is for Apple Clang and the other is for GCC. This is particularly important because neither compiler implements the other’s closure extension (Clang does Apple Blocks but not Nested Functions, while GCC does Nested Functions in exclusively its C frontend but does not implement Apple Blocks6).&lt;/p&gt;
    &lt;head rend="h1"&gt;The Results&lt;/head&gt;
    &lt;p&gt;Ta-da!&lt;/p&gt;
    &lt;p&gt;For the vision-impaired, a text description is available.&lt;/p&gt;
    &lt;p&gt;For the vision-impaired, a text description is available.&lt;/p&gt;
    &lt;p&gt;… Oh. That looks awful.&lt;/p&gt;
    &lt;p&gt;It turns out that some solutions are so dogwater that it completely screws up our viewing graphs. But, it does let us know that Lambdas used the Rosetta Code style are so unbelievably awful that it is several orders of magnitude more expensive than any other solution presented! One has to wonder what the hell is going on in the code snippet there, but first we need to make the graphs more legible. To do this we’re going to be using the (slightly deceptive) LOGARITHMIC SCALING. This is a bit deadly to do because it tends to mislead people about how much of a change there is, so please pay attention to the potential order of magnitude gains and losses when going from one bar graph to another.&lt;/p&gt;
    &lt;p&gt;For the vision-impaired, a text description is available.&lt;/p&gt;
    &lt;p&gt;For the vision-impaired, a text description is available.&lt;/p&gt;
    &lt;p&gt;There we go. Now we can talk about the various solutions and – in particular – why “lambdas” have 4 different entries with such wildly differing performance profiles. First up, let’s talk about the clear performance winners.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lambdas: On Top!&lt;/head&gt;
    &lt;p&gt;Not surprising to anyone who has been checked in to C++, lambdas that are used directly and not type-erased are on top. This means there’s a one-to-one mapping between a function call and a given bit of execution. We are cheating by using a constant parameter to stop the uniquely-typed lambdas being passed into the functions from recursing infinitely, which makes the Man-or-Boy function look like this:&lt;/p&gt;
    &lt;code&gt;template &amp;lt;int recursion = 0&amp;gt;
static int a(int k, const auto&amp;amp; x1, const auto&amp;amp; x2, const auto&amp;amp; x3, const auto&amp;amp; x4, const auto&amp;amp; x5) {
	if constexpr (recursion == 11) {
		::std::cerr &amp;lt;&amp;lt; "This should never happen and this code should never have been generated." &amp;lt;&amp;lt; std::endl;
		::std::terminate();
		return 0;
	}
	else {
		auto B = [&amp;amp;](this const auto&amp;amp; self) { return a&amp;lt;recursion + 1&amp;gt;(--k, self, x1, x2, x3, x4); };
		return k &amp;lt;= 0 ? x4() + x5() : B();
	}
}
&lt;/code&gt;
    &lt;p&gt;Every &lt;code&gt;B&lt;/code&gt; is its own unique type and we are not erasing that unique type when using the expression as an initializer to &lt;code&gt;B&lt;/code&gt;. This means that when we call &lt;code&gt;a&lt;/code&gt; again with &lt;code&gt;B&lt;/code&gt; (the &lt;code&gt;self&lt;/code&gt; in this lambda here using Deduced This, a C++23 feature that cannot be part of the C version of lambdas) which means we need to use &lt;code&gt;auto&lt;/code&gt; parameters (a shortcut way of writing template parameters) to take it. But, since every parameter is unique, and every &lt;code&gt;B&lt;/code&gt; is unique, calling this recursively means that, eventually, C++ compilers will actually just completely crash out/toss out-of-memory errors/say we’ve compile-time recursed too hard, or similar. That’s why the compile-time &lt;code&gt;if constexpr&lt;/code&gt; on the extra, templated &lt;code&gt;recursion&lt;/code&gt; parameter needs to have some arbitrary limit. Because we know &lt;code&gt;k&lt;/code&gt; starts at 10 for this test, we just have some bogus limit of “11”.&lt;/p&gt;
    &lt;p&gt;This results in a very spammy recursive chain of function calls, where the actual generated names of these template functions is far more complex than &lt;code&gt;a&lt;/code&gt; and can run the compiler into the ground / cause quite a bit of instantiations if you let &lt;code&gt;recursion&lt;/code&gt; get to a high enough value. But, once you add the limit, the compiler gets perfect information about this recursive call all the way to every leaf, and thus is able to not only optimize the hell out of it, but refuse to generate the other frivolous code it knows won’t be useful.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lambdas are also Fast, even when Type-Erased&lt;/head&gt;
    &lt;p&gt;You can observe a slight bump up in performance penalty when a Lambda is erased by a &lt;code&gt;std::function_ref&lt;/code&gt;. This is a low-level, non-allocating, non-owning, slim “view” type that is analogous to what a language-based wide function pointer type would be in C. From this, it allows us to guess how good Lambdas in C would be even if you had to hide them behind a non-unique type.&lt;/p&gt;
    &lt;p&gt;The performance metrics are about equivalent to if you hand-wrote a C++ class with a custom &lt;code&gt;operator()&lt;/code&gt; that uses a discriminated union, no matter which compiler gets used to do it. It’s obviously not as fast as having access to a direct function call and being able to slurp-inline optimize, but the performance difference is acceptable when you do not want to engage in a large degree of what is called “monomorphisation” of a genric routine or type. And, indeed, outside of macros, C has no way of doing this innately that isn’t runtime-based.&lt;/p&gt;
    &lt;p&gt;A very strong contender for a good solution!&lt;/p&gt;
    &lt;head rend="h3"&gt;Lambdas: On…. Bottom, too?&lt;/head&gt;
    &lt;p&gt;One must wonder, then, why the &lt;code&gt;std::function&lt;/code&gt; Lambdas and the Rosetta Code Lambdas are either bottom-middle-of-the-road or absolutely-teary-eyed-awful.&lt;/p&gt;
    &lt;p&gt;Starting off, the &lt;code&gt;std::function&lt;/code&gt; Lambdas are bad because of exactly that: &lt;code&gt;std::function&lt;/code&gt;. &lt;code&gt;std::function&lt;/code&gt; is not a “cheap” closure; it is a potentially-allocating, meaty, owning function abstraction. This means that it’s safe to make one and pass it around and store it and call it later; the cost of this is, obviously, that you’re allocating (when the type is big enough) for that internal storage. Part of this is alleviated by using &lt;code&gt;const std::function&amp;lt;int(void)&amp;gt;&amp;amp;&lt;/code&gt; parameters, taking things by reference and only generating a new object when necessary. This prevents copying on every function call. Both the Rosetta Lambdas and regular &lt;code&gt;std::function&lt;/code&gt; Lambdas code does the by-reference parameters bit, though, so where does the difference come in? It actually has to do with the Captures. Here’s how &lt;code&gt;std::function&lt;/code&gt; Lambdas defines the recursive, self-referential lambda and uses it:&lt;/p&gt;
    &lt;code&gt;using f_t = std::function&amp;lt;int(void)&amp;gt;;

inline static int A(int k, const f_t&amp;amp; x1, const f_t&amp;amp; x2, const f_t&amp;amp; x3, const f_t&amp;amp; x4, const f_t&amp;amp; x5) {
	f_t B = [&amp;amp;] { return A(--k, B, x1, x2, x3, x4); };
	return k &amp;lt;= 0 ? x4() + x5() : B();
}
&lt;/code&gt;
    &lt;p&gt;And, here is how the Rosetta Code Lambdas defines the recursive, self-referential lambda and uses it:&lt;/p&gt;
    &lt;code&gt;using f_t = std::function&amp;lt;int(void)&amp;gt;;

inline static int A(int k, const f_t&amp;amp; x1, const f_t&amp;amp; x2, const f_t&amp;amp; x3, const f_t&amp;amp; x4, const f_t&amp;amp; x5) {
	f_t B = [=, &amp;amp;k, &amp;amp;B] { return A(--k, B, x1, x2, x3, x4); };
	return k &amp;lt;= 0 ? x4() + x5() : B();
}
&lt;/code&gt;
    &lt;p&gt;The big problem here is in the use of the &lt;code&gt;=&lt;/code&gt;. What &lt;code&gt;=&lt;/code&gt; by itself in the front of a lambda capture clause means is “copy all the visible variables in and hold onto that copy” (unless the capture for that following variable is “overridden” by a &lt;code&gt;&amp;amp;var&lt;/code&gt;, address capture). Meanwhile, the &lt;code&gt;&amp;amp;&lt;/code&gt; is the opposite: it means “refer to all the visible variables directly by their address and do not copy them in”. So, while the &lt;code&gt;std::function&lt;/code&gt; Lambda is (smartly) referring to stuff directly without copying because we know for the Man-or-Boy test that referring to things directly is not an unsafe operation, the general &lt;code&gt;=&lt;/code&gt; causes that for the several dozen recursive iterations through the function, it is copying all five allocating &lt;code&gt;std::function&lt;/code&gt; arguments. So the first call creates a &lt;code&gt;B&lt;/code&gt; that copies everything in, and then passes that in, and then the next call copies the previous &lt;code&gt;B&lt;/code&gt; and the 4 normal functions, and then passes that in to the next &lt;code&gt;B&lt;/code&gt;, and then it copies both previous &lt;code&gt;B&lt;/code&gt;’s, and this stacks for the depth of the callgraph (some 10 times since &lt;code&gt;k = 10&lt;/code&gt; to start).&lt;/p&gt;
    &lt;p&gt;You can imagine how much that completely screws with the performance, and it explains why the Rosetta Code Lambdas code behaves so poorly in terms of performance. But, this also raises a question: if referring to everything by-reference saves so much speed, then why does GNU Nested Functions – in all its variants – perform so poorly? After all, Nested Functions capture everything by reference / by address, exactly like a lambda does with &lt;code&gt;[&amp;amp;]&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Similarly, if allocating over and over again was so expensive, how come Apple Blocks and C++03 &lt;code&gt;shared_ptr&lt;/code&gt; Rosetta Code-style versions of the Man-or-Boy test don’t perform nearly as badly as the Rosetta Code Lambdas? Are we not copying the value of the arguments into a newly created Apple Block and, thusly, tanking the performance metrics? Well, as it turns out, there’s many reasons for these things, so let’s start with GNU Nested Functions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Nested Functions and The Stack&lt;/head&gt;
    &lt;p&gt;I’ve written about it dozens of times now, but the prevailing and most common implementation of Nested Functions is with an executable stack. The are a lot of security and other implications for this, but all you need to understand is that the reason GCC did this is because it was an at-the-time slick encoding of both the location of the variables and the routine itself. Allocating a chunk of data off of the current programming stack means that the “environment context”/”this closure” pointer has the same anchoring address as the routine itself. This means you can encode both the location of the data to know what to access and the address of a function’s entry point into a single thing that works with your typical setup-and-call convention that comes with invoking a standard ISO C function pointer.&lt;/p&gt;
    &lt;p&gt;But think about that, briefly, in terms of optimization.&lt;/p&gt;
    &lt;p&gt;You are using the function’s stack frame at that precise point in the program as the “base address” for this executable code. That base address also means that all the variables associated with it need to be reachable from that base address: i.e., that things are not stuffed in registers, but that you are referring to the same variables as modified by the enclosing function around your nested function. Principally, this means that your function needs to have all of the following now so that GNU Nested Functions actually work.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A stack that is executable so that the base address used for the trampoline can be run succinctly.&lt;/item&gt;
      &lt;item&gt;A real function frame that exists somewhere in memory to serve as the base address for the trampoline.&lt;/item&gt;
      &lt;item&gt;Real objects in memory backing the names of the captured variables accesses.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This all seems like regular consequences, until you tack on the second order affects from the point of optimization.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A stack that now has both data and instructions all blended into itself.&lt;/item&gt;
      &lt;item&gt;A real function frame, which means no ommission of a frame pointer and no collapsing / inlining of that function frame.&lt;/item&gt;
      &lt;item&gt;Real objects that all have their address taken that are tied to the function frame, which must be memory-accessible and which the compiler now has a hard time telling if they can simply be exchanged through registers or if the need to actually sit somewhere in memory.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In other words: GNU Nested Functions have created the perfect little storm for what might be the best optimizer-murderer. The reason it performs so drastically poorly (worse than even allocating lambdas inside of a &lt;code&gt;std::function&lt;/code&gt; or C++03-style virtual function calls inside of a bulky, nasty C++ &lt;code&gt;std::shared_ptr&lt;/code&gt;) by a whole order of magnitude or more is that everything about Nested Functions and their current implementation is basically Optimizer Death. If the compiler can’t see through everything – and the Man-or-Boy test with a non-constant value of &lt;code&gt;k&lt;/code&gt; and &lt;code&gt;expected_k&lt;/code&gt; – GNU Nested Functions deteriorate rapidly. It takes every core optimization technique that we’ve researched and maximized on in the last 30 years and puts a shotgun to the side of its head once it can’t pre-compute &lt;code&gt;k&lt;/code&gt; and &lt;code&gt;expected_k&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The good news is that GCC has completed a new backing implementation for GNU Nested Functions, which uses a heap-based trampoline. Such a trampoline does not interfere with the stack, would allow for omission of frame pointers while referring directly to the data itself (which may prevent the wrecking of specific kinds of inlining optimizations), and does not need an executable stack (just a piece of memory from ✨somewhere✨ it can mark executable). This may have performance closer to Apple Blocks, but we don’t have a build of the latest GCC to test it with. But, when we do, we can simply add the compilation flag &lt;code&gt;-ftrampoline-impl=heap&lt;/code&gt; to the two source files in CMake and then let the benchmarks run again to see how it stacks up!&lt;/p&gt;
    &lt;p&gt;Finally, there is a minor performance degredation because our benchmarking software is in C++ and this extension exists exclusively in the C frontend of GCC. That means I have to use an &lt;code&gt;extern&lt;/code&gt; function call within the benchmark loop to get to the actual code. Within the function call, however, all of this stuff should be optimized down, so the cost of a single function call’s stack frame shouldn’t be so awful, but I expect to try to dig into this better to help make sure the &lt;code&gt;extern&lt;/code&gt; of a C function call isn’t making things dramatically worse than they are. Given it’s a different translation unit and it’s not being compiled as a separate static or dynamic library, it should still link together and optimize cleanly, but given how bad it’s performing? Every possible issue is on the table.&lt;/p&gt;
    &lt;head rend="h2"&gt;What about Apple Blocks?&lt;/head&gt;
    &lt;p&gt;Apple Blocks are not the fastest, but they the best of the C extensions while being the worst of the “fast” solutions. They are not faster than just hacking the &lt;code&gt;ARG*&lt;/code&gt; into the function signature and using regular normal C function calls, unfortunately, and that’s likely due to their shared, heap-ish nature. The saddest part about Apple Blocks is that it works using a Blocks Runtime that is already as optimized as it can possibly be: Clang and Apple both document that whie the Blocks Runtime does manage an Automatic Reference Counted (ARC) Heap of Block pointers, when a Block is first created it will literally have its memory stored on the stack rather than in the heap. In order to move it to the heap, one must call &lt;code&gt;Block_copy&lt;/code&gt; to trigger the “normal” heap-based shenanigans. We never call &lt;code&gt;Block_copy&lt;/code&gt;, so this is with as-fast-as-possible variable access and management with few allocations.&lt;/p&gt;
    &lt;p&gt;It’s very slightly disappointing that: normal C functions with an &lt;code&gt;ARG*&lt;/code&gt; blob; a custom C++ class using a discriminated union and &lt;code&gt;operator()&lt;/code&gt;; any mildly conscientious use of lambdas; and, any other such shenanigans perform better than the very best Apple Blocks has to offer. One has to imagine that all of the ARC management functions made to copy the &lt;code&gt;int^(void)&lt;/code&gt; “hat-style” function pointers, even if they end up not doing much for the data stored on the stack, impacted the results here. But, this is also somewhat good news: because Apple Block hat pointers are cheaply-copiable entities (they are just pointers to a Block object), it means that even if we copy all of the arguments into the closure every function call, that copying is about as cheap as it can get. Obivously, as regular “Lambdas” and “Lambas (No Function Helpers)” demonstrate, being able to just slurp everything up by address/by reference – including visible function arguments – with &lt;code&gt;[&amp;amp;]&lt;/code&gt; saves us a teensy, tiny bit of time7.&lt;/p&gt;
    &lt;p&gt;The cheapness of &lt;code&gt;int^(void)&lt;/code&gt; hat-pointer function types is likely the biggest saving grace for Apple Blocks in this benchmark. In the one place we need to be careful, we rename the input argument &lt;code&gt;k&lt;/code&gt; to &lt;code&gt;arg_k&lt;/code&gt; and then make a &lt;code&gt;__block&lt;/code&gt; variable to actually refer to a shared &lt;code&gt;int k&lt;/code&gt; (and get the right answer):&lt;/p&gt;
    &lt;code&gt;static int a(int arg_k, fn_t ^ x1, fn_t ^ x2, fn_t ^ x3, fn_t ^ x4, fn_t ^ x5) {
	__block int k    = arg_k;
	__block fn_t ^ b = ^(void) { return a(--k, b, x1, x2, x3, x4); };
	return k &amp;lt;= 0 ? x4() + x5() : b();
}
&lt;/code&gt;
    &lt;p&gt;All of the &lt;code&gt;x1&lt;/code&gt;, &lt;code&gt;x2&lt;/code&gt;, and &lt;code&gt;x3&lt;/code&gt; – like the bad Lambda case – are copied over and over and over again. One could change the name of all the arugments &lt;code&gt;arg_xI&lt;/code&gt; and then have an &lt;code&gt;xI&lt;/code&gt; variable inside that is marked &lt;code&gt;__block&lt;/code&gt;, but that’s more effort and very unlikely to have any serious impact on the code while possibly degrading performance for the setup of multiple shared variables that all have to also be ARC-reference-counted and be stored inside each and every new &lt;code&gt;b&lt;/code&gt; block that is created.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Brief Aside: Self-Referencing Functions/Closures&lt;/head&gt;
    &lt;p&gt;It’s also important to note that just writing this:&lt;/p&gt;
    &lt;code&gt;static int a(int arg_k, fn_t ^ x1, fn_t ^ x2, fn_t ^ x3, fn_t ^ x4, fn_t ^ x5) {
	__block int k    = arg_k;
	fn_t ^ b = ^(void) { return a(--k, b, x1, x2, x3, x4); };
	return k &amp;lt;= 0 ? x4() + x5() : b();
}
&lt;/code&gt;
    &lt;p&gt;(no &lt;code&gt;__block&lt;/code&gt; on the &lt;code&gt;b&lt;/code&gt; variable) is actually a huge bug. Apple Blocks, like older C++ Lambdas, cannot technically refer to “itself” inside. You have to refer to the “self” by capturing the variable it set to. For those who use C++ and are familiar with the lambdas over there, it’s like making sure you capture the variable you initialize with the lambda by reference while also making sure it has a concrete type. It can only be escaped by using &lt;code&gt;auto&lt;/code&gt; and Deducing This, or some other combination of referential-use. That is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;auto x = [&amp;amp;x](int v) { if (v != limit) x(v + 1); return v + 8; }&lt;/code&gt;does not compile, as the type&lt;code&gt;auto&lt;/code&gt;isn’t figured out yet;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;std::function_ref&amp;lt;int(int)&amp;gt; x = [&amp;amp;x](int v) { if (v != limit) x(v + 1); return v + 8; }&lt;/code&gt;compiles but due to C++ shenanigans produces a dangling reference to a temporary lambda that dies after the full expression (the initialization);&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;std::function&amp;lt;int(int)&amp;gt; x = [&amp;amp;x](int v) { if (v != limit) x(v + 1); return v + 8; }&lt;/code&gt;compiles and works with no segfaults because&lt;code&gt;std::function&lt;/code&gt;allocates, and the reference to itself&lt;code&gt;&amp;amp;x&lt;/code&gt;is just fine.&lt;/item&gt;
      &lt;item&gt;and, finally, &lt;code&gt;auto x = [](this const auto&amp;amp; self, int v) { if (v != limit) self(v + 1); return v + 8; }&lt;/code&gt;which compiles and works with no segfaults because the invisible&lt;code&gt;self&lt;/code&gt;parameter is just a reference to the current object.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The problem with the most recent Apple Blocks snippet just above is that it’s the equivalent of doing&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;std::function&amp;lt;int(int)&amp;gt; x = [x](int v) { if (v != limit) x(v + 1); return v + 8; }&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Notice that there’s no &lt;code&gt;&amp;amp;x&lt;/code&gt; in the lambda initializer’s capture list. It’s copying an (uninitialized) variable by-value into the lambda. This is what Apple Blocks set into a variable that does not have a &lt;code&gt;__block&lt;/code&gt; specifier, like in our bad code case with &lt;code&gt;b&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;All variations of this on all implementations which allow for self-referencing allow this and compile some form of this. You would imagine some implementations would warn about this, but this is leftover nonsense from allowing a variable to refer to itself in its initialization. The obvious reason this happens in C and C++ is because you can create self-referential structures, but unfortunately neither languages provided a safe way to do this generally. C++23’s Deducing This does not work inside of regular functions and non-objects, so good luck applying to other places and other extensions. The only extension which does not suffer this problem is GNU Nested Functions, because it creates a function declaration / definition rather than a variable with an initializer. Thus, this code from the benchmarks works:&lt;/p&gt;
    &lt;code&gt;inline static int gnu_nested_functions_a(int k, int xl(void), int x2(void), int x3(void), int x4(void), int x5(void)) {
	int b(void) {
		return gnu_nested_functions_a(--k, b, xl, x2, x3, x4);
	}
	return k &amp;lt;= 0 ? x4() + x5() : b();
}
&lt;/code&gt;
    &lt;p&gt;And it has the semantics one would expect, unlike how Blocks, Lambdas, or others with default by-value copying works.&lt;/p&gt;
    &lt;p&gt;In the general case, this is what the paper &lt;code&gt;__self_func&lt;/code&gt; was going to solve8, but… that’s going to need some time for me to convince WG14 that maybe it IS actually a good idea. We can probably just keep writing the buggy code a few dozen more times for the recursion case and keep leaving it error prone, but I’ll try my best to convince them one more time that the above situation is very not-okay.&lt;/p&gt;
    &lt;head rend="h1"&gt;Thinking It Over&lt;/head&gt;
    &lt;p&gt;While the Man-or-Boy test isn’t exactly the end-all, be-all performance test, due to flexing both (self)-referential data and utilization of local copies with recursion, it is surprisingly suitable for figuring out if a closure design is decent enough in a mid to high-level programming language. It also gives me some confidence that, at the very least, the baseline for performance of statically-known, compile-time understood, non-type erased, callable Closure object will have the best implementation quality and performance tradeoffs for a language like ISO C no matter the compiler implementation.&lt;/p&gt;
    &lt;p&gt;In the future, at some point, I’ll have to write about why that is. It’s a bit upside-down from the perspective of readers of this blog to first address performance and then later write about the design, but it’s nice to make sure we’re not designing ourselves into a bad performance corner at the offset of this whole adventure.&lt;/p&gt;
    &lt;head rend="h2"&gt;Learned Insights&lt;/head&gt;
    &lt;p&gt;Surprising nobody, the more information the compiler is allowed to accrue (the Lambda design), the better its ability to make the code fast. What might be slightly more surprising is that a slim, compact layer of type erasure – not a bulky set of Virtual Function Calls (C++03 &lt;code&gt;shared_ptr&lt;/code&gt; Rosetta Code design) – does not actually cost much at all (Lambdas with &lt;code&gt;std::function_ref&lt;/code&gt;). This points out something else that’s part of the ISO C proposal for Closures (but not formally in its wording): Wide Function Pointers.&lt;/p&gt;
    &lt;p&gt;The ability to make a thin &lt;code&gt;{ some_function_type* func; void* context; }&lt;/code&gt; type backed by the compiler in C would be extremely powerful. Martin Uecker has a proposal that has received interest and passing approval in the Committee, but it would be nice to move it along in a nice direction. My suggestion is having &lt;code&gt;%&lt;/code&gt; as a modifier, so it can be used easily since wide function pointers are an extremely prevalent concept. Being able to write something like the following would be very easy and helpful.&lt;/p&gt;
    &lt;code&gt;typedef int(compute_fn_t)(int);

int do_computation(int num, compute_fn_t% success_modification);
&lt;/code&gt;
    &lt;p&gt;A wide function pointer type like this would also be traditionally convertible from a number of already-existing extensions, too, where GNU Nested Functions, Apple Blocks, C++-style Lambdas, and more could create the appropriate wide function pointer type to be cheaply used. Additionally, it also works for FFI: things like Go closures already use GCC’s &lt;code&gt;__builtin_call_with_static_chain&lt;/code&gt; to transport through their Go functions in C. Many other functions from other languages could be cheaply and efficiently bridged with this, without having to come up with hairbrained schemes about where to put a &lt;code&gt;void* userdata&lt;/code&gt; or some kind of implicit context pointer / implicit environment pointer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Existing Extensions?&lt;/head&gt;
    &lt;p&gt;Unfortunately – except for the borland closure annotation – there’s too many things that are performance-stinky about both GNU Nested Functions and Apple Blocks. It’s no wonder GCC is trying to add &lt;code&gt;-ftrampoline-impl=heap&lt;/code&gt; to the story of GNU Nested Functions; they might be able to tighten up that performance and make it more competitive with Apple Blocks. But, unfortunately, since it is heap-based, there’s a real chance that its maximum performance ceiling is only as good as Apple Blocks, and not as good as a C++-style Lambda.&lt;/p&gt;
    &lt;p&gt;Both GNU Nested Functions and Apple Blocks – as they are implemented – do not really work well in ISO C. GNU Nested Functions because their base design and most prevalent implementation are performance-awful, but also Apple Blocks because of the copying and indirection runtime of Blocks that manage ARC pointers providing a hard upper limit on how good the performance can actually be in complex cases.&lt;/p&gt;
    &lt;p&gt;Regular C code, again, performs middle-of-the-road here. It’s not the worst of it, but it’s not the best at all, which means there’s some room beneath how we could go having the C code run. While it’s hard to fully trust the Rosetta Code Man-or-Boy code for C as the best, it is a pretty clear example of how a “normal” C developer would do it and how it’s not actually able to hit maximum performance for this situation.&lt;/p&gt;
    &lt;p&gt;I wanted to add a version of regular C code that used a dynamic array with &lt;code&gt;static&lt;/code&gt;s to transfer data, or a bunch of &lt;code&gt;thread_local&lt;/code&gt;s, but I could not bring myself to actually care enough to write a complex association scheme from a specific invocation of the recursive function &lt;code&gt;a&lt;/code&gt; and the slot of dynamic data that represented the closure’s data. I’m sure there’s schemes for it and I could think of a few, but at that point it’s such a violent contortion to get a solution that going that I figured it simply wasn’t worth the effort. But, as always,&lt;/p&gt;
    &lt;p&gt;pull requests are welcome. 💚&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Banner and Title Photo by Lukas, from Pexels&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;See: https://github.com/soasis/idk/tree/main/benchmarks/closures. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;See https://github.com/catchorg/Catch2/blob/devel/docs/benchmarks.md. And try it out. It’s pretty good, I just haven’t gotten off my butt to make the swap to it yet. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Apple Blocks used to have an implementation in GCC that could be turned on and it used a Blocks Runtime to achieve it. But, I think it was gutted when some NeXT support and Objective-C stuff was wiped out after being unmaintained for some time. There’s been talk of reintroducing it, but obviously someone has to actually sit down and either redo it from scratch (advantageous because Apple has changed the ABI of Blocks) or try to ressurect / fix the old support for this stuff. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Apple Blocks cannot have the “by address” capturing mechanism it has – the&lt;/p&gt;&lt;code&gt;__block&lt;/code&gt;storage class modifier – applied to function arguments, for some reason. So, all function arguments are de-facto copied into a Block Expression unless someone saves a tempory inside the body of the function before the Block and then uses&lt;code&gt;__block&lt;/code&gt;on that to make it a by-reference capture. ↩&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;WG14 rejected the paper last meeting, unfortunately, as not motivated enough. Funnily enough, it was immediately after this meeting that I got slammed in the face with this bug. Foresight and “being prepared” is just not something even the most diehard C enthusiasts really embodies, unfortunately, and most industry vendors tend to take a more strongly conservative position over a bigger one. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46228597</guid><pubDate>Thu, 11 Dec 2025 07:21:33 +0000</pubDate></item><item><title>A “frozen” dictionary for Python</title><link>https://lwn.net/SubscriberLink/1047238/25c270b077849dc0/</link><description>&lt;doc fingerprint="9f1980339ca5cbf8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A "frozen" dictionary for Python&lt;/head&gt;
    &lt;head rend="h2"&gt;[LWN subscriber-only content]&lt;/head&gt;
    &lt;quote&gt;
      &lt;head&gt;Welcome to LWN.net&lt;/head&gt;
      &lt;p&gt;The following subscription-only content has been made available to you by an LWN subscriber. Thousands of subscribers depend on LWN for the best news from the Linux and free software communities. If you enjoy this article, please consider subscribing to LWN. Thank you for visiting LWN.net!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Dictionaries are ubiquitous in Python code; they are the data structure of choice for a wide variety of tasks. But dictionaries are mutable, which makes them problematic for sharing data in concurrent code. Python has added various concurrency features to the language over the last decade or so—async, free threading without the global interpreter lock (GIL), and independent subinterpreters—but users must work out their own solution for an immutable dictionary that can be safely shared by concurrent code. There are existing modules that could be used, but a recent proposal, PEP 814 ("Add frozendict built-in type"), looks to bring the feature to the language itself.&lt;/p&gt;
    &lt;p&gt;Victor Stinner announced the PEP that he and Donghee Na have authored in a post to the PEPs category of the Python discussion forum on November 13. The idea has come up before, including in PEP 416, which has essentially the same title as 814 and was authored by Stinner back in 2012. It was rejected by Guido van Rossum at the time, in part due to its target: a Python sandbox that never really panned out.&lt;/p&gt;
    &lt;head rend="h4"&gt;frozendict&lt;/head&gt;
    &lt;p&gt;The idea is fairly straightforward: add frozendict as a new immutable type to the language's builtins module. As Stinner put it:&lt;/p&gt;
    &lt;quote&gt;We expect frozendict to be safe by design, as it prevents any unintended modifications. This addition benefits not only CPython's standard library, but also third-party maintainers who can take advantage of a reliable, immutable dictionary type.&lt;/quote&gt;
    &lt;p&gt;While frozendict has a lot in common with the dict built-in type, it is not a subclass of dict; instead, it is a subclass of the base object type. The frozendict() constructor can be used to create one in various ways:&lt;/p&gt;
    &lt;quote&gt;fd = frozendict() # empty fd = frozendict(a=1, b=2) # frozen { 'a' : 1, 'b' : 2 } d = { 'a' : 1, 'b' : 2 } fd = frozendict(d) # same l = [ ( 'a', 1 ), ( 'b', 2 ) ] fd = frozendict(l) # same fd2 = frozendict(fd) # same assert d == fd == fd2 # True&lt;/quote&gt;
    &lt;p&gt;As with dictionaries, the keys for a frozendict must be immutable, thus hashable, but the values may or may not be. For example, a list is a legitimate type for a value in either type of dictionary, but it is mutable, making the dictionary as a whole (frozen or not) mutable. However, if all of the values stored in a frozendict are immutable, it is also immutable, so it can be hashed and used in places where that is required (e.g. dictionary keys, set elements, or entries in a functools.lru_cache).&lt;/p&gt;
    &lt;p&gt;As might be guessed, based on the last line of the example above, frozen dictionaries that are hashable can be compared for equality with other dictionaries of either type. In addition, neither the hash() value nor the equality test depend on the insertion order of the dictionary, though that order is preserved in a frozen dictionary (as it is in the regular variety). So:&lt;/p&gt;
    &lt;quote&gt;d = { 'a' : 1, 'b' : 2 } fd = frozendict(d) d2 = { 'b' : 2, 'a' : 1 } fd2 = frozendict(d2) assert d == d2 == fd == fd2 # frozendict unions work too, from the PEP &amp;gt;&amp;gt;&amp;gt; frozendict(x=1) | frozendict(y=1) frozendict({'x': 1, 'y': 1}) &amp;gt;&amp;gt;&amp;gt; frozendict(x=1) | dict(y=1) frozendict({'x': 1, 'y': 1})For the unions, a new frozen dictionary is created in both cases; the "|=" union-assignment operator also works by generating a new frozendict for the result.&lt;/quote&gt;
    &lt;p&gt; Iteration over a frozendict works as expected; the type implements the collections.abc.Mapping abstract base class, so .items() returns an iterable of key-value tuples, while .keys() and .values() provide the keys and values of the frozen dictionary. For the most part, a frozendict acts like a dict that cannot change; the specific differences between the two are listed in the PEP. It also contains a lengthy list of places in the standard library where a dict could be switched to a frozendict to "&lt;quote&gt;enhance safety and prevent unintended modifications&lt;/quote&gt;". &lt;/p&gt;
    &lt;head rend="h4"&gt;Discussion&lt;/head&gt;
    &lt;p&gt;The reaction to the PEP was generally positive, with the usual suggestions for tweaks and more substantive additions to the proposal. Stinner kept the discussion focused on the proposal at hand for the most part. One part of the proposal was troubling to some: converting a dict to a frozendict was described as an O(n) shallow copy. Daniel F Moisset thought that it would make sense to have an in-place transformation that could be O(1) instead. He proposed adding a .freeze() method that would essentially just change the type of a dict object to frozendict.&lt;/p&gt;
    &lt;p&gt;However, changing the type of an existing object is fraught with peril, as Brett Cannon described:&lt;/p&gt;
    &lt;quote&gt;But now you have made that dictionary frozen for everyone who holds a reference to it, which means side-effects at a distance in a way that could be unexpected (e.g. context switch in a thread and now suddenly you're going to get an exception trying to mutate what was a dict a microsecond ago but is now frozen). That seems like asking for really nasty debugging issues just to optimize some creation time.&lt;/quote&gt;
    &lt;p&gt; The PEP is not aimed at performance, he continued, but is meant to help "&lt;quote&gt;lessen bugs in concurrent code&lt;/quote&gt;". Moisset noted, that dictionaries can already change in unexpected ways via .clear() or .update(), thus the debugging issues already exist. He recognized that the authors may not want to tackle that as part of the PEP, but wanted to try to ensure that an O(1) transformation was not precluded in the future. &lt;/p&gt;
    &lt;p&gt; Cannon's strong objection is to changing the type of the object directly. Ben Hsing and "Nice Zombies" proposed ways to construct a new frozendict without requiring the shallow copy—thus O(1)—by either moving the hash table to a newly created frozendict, while clearing the dictionary, or by using a copy-on-write scheme for the table. As Steve Dower noted, that optimization can be added later as long as the PEP does not specify that the operation must be O(n), which would be a silly thing to do, but that it sometimes happens "&lt;quote&gt;because it makes people stop complaining&lt;/quote&gt;", he said in a footnote. In light of the discussion, the PEP specifically defers that optimization to a later time, suggesting that it could also be done for other frozen types (tuple and frozenset), perhaps by resurrecting PEP 351 ("The freeze protocol"). &lt;/p&gt;
    &lt;p&gt;On December 1, Stinner announced that the PEP had been submitted to the steering council for pronouncement. Given that Na is on the council, though will presumably recuse himself from deciding on this PEP, he probably has a pretty good sense for how it might be received by the group. So it seems likely that the PEP has a good chance of being approved. The availability of the free-threaded version of the language (i.e. without the GIL) means that more multithreaded Python programs are being created, so having a safe way to share dictionaries between threads will be a boon.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Index entries for this article&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Python&lt;/cell&gt;
        &lt;cell&gt;Dictionaries&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Python&lt;/cell&gt;
        &lt;cell&gt;Python Enhancement Proposals (PEP)/PEP 814&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt; Posted Dec 5, 2025 9:01 UTC (Fri) by jeeger (subscriber, #104979) [Link] (7 responses) Posted Dec 5, 2025 9:29 UTC (Fri) by taladar (subscriber, #68407) [Link] (6 responses) Posted Dec 5, 2025 9:55 UTC (Fri) by intelfx (subscriber, #130118) [Link] (5 responses) Obviously, yes. What the GP is saying is that functions that are O(1) are also, strictly speaking, O(n), since the big-O notation only defines, informally, an "upper bound" on the algorithmic complexity. (The answer here is that engineers tend to casually use the big-O notation where they really mean Knuth's Θ notation instead.) Posted Dec 7, 2025 12:29 UTC (Sun) by Baughn (subscriber, #124425) [Link] (4 responses) If he'd used uppercase-T instead, then we'd use it. Posted Dec 7, 2025 13:21 UTC (Sun) by excors (subscriber, #95769) [Link] (3 responses) As is often the case, Knuth solved that problem too, by inventing TeX half a century ago. Now we just need LWN to implement server-side KaTeX rendering. Posted Dec 7, 2025 13:57 UTC (Sun) by dskoll (subscriber, #1630) [Link] (2 responses) I solved it in a horrible way. Look up "theta" on Wikipedia, then paste the result: Θ Posted Dec 7, 2025 15:10 UTC (Sun) by adobriyan (subscriber, #30858) [Link] (1 responses) Posted Dec 10, 2025 1:56 UTC (Wed) by raven667 (subscriber, #5198) [Link] Posted Dec 5, 2025 11:45 UTC (Fri) by iabervon (subscriber, #722) [Link] (3 responses) Next, I want a flag to json.loads() that causes it to return hashable values instead of mutable ones (without the caller needing to know how to accomplish that). Posted Dec 6, 2025 0:49 UTC (Sat) by AdamW (subscriber, #48457) [Link] (2 responses) It says frozendicts will be ordered, but hashes and comparisons will not care about the order. So frozendict({"a": "b", "c": "d"}) and frozendict({"c": "d", "a": "b"}) will have the same hash and compare as equal, but they're not really the same? I don't know how I feel about that! Posted Dec 6, 2025 5:02 UTC (Sat) by NYKevin (subscriber, #129325) [Link] Whether this is a problem is debatable, but it is also moot. Non-frozen dicts have behaved this way forever, so making frozendict behave differently would be pretty terrible language design. Posted Dec 6, 2025 5:23 UTC (Sat) by iabervon (subscriber, #722) [Link] The history is that the iterator order used to be unpredictable, so the same object might give different orders when traversed multiple times and objects constructed by adding the items in different order might give the same order when traversed multiple times. However, a more recent implementation of dict started to traverse the items in the order the keys were first added, just because that was more convenient, and then the language changed to guarantee this. Of course, that meant that there was now something you could reliably determine about dicts that wasn't included in the equality rules that had always existed. &lt;head&gt;Complexity specification &lt;/head&gt;&lt;quote&gt; As Steve Dower noted, that optimization can be added later as long as the PEP does not specify that the operation must be O(n) &lt;/quote&gt; I might be misremembering from my Uni days, but all O(1) algorithms are also O(n), so the statement doesn't make sense. I'd be happy for someone to correct me though. &lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Hashable mappings&lt;/head&gt;&lt;head&gt;Hashable mappings&lt;/head&gt;&lt;head&gt;Hashable mappings&lt;/head&gt;&lt;head&gt;Hashable mappings&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46229467</guid><pubDate>Thu, 11 Dec 2025 09:51:47 +0000</pubDate></item><item><title>Meta shuts down global accounts linked to abortion advice and queer content</title><link>https://www.theguardian.com/global-development/2025/dec/11/meta-shuts-down-global-accounts-linked-to-abortion-advice-and-queer-content</link><description>&lt;doc fingerprint="b68009bf1a2a8f13"&gt;
  &lt;main&gt;
    &lt;p&gt;Meta has removed or restricted dozens of accounts belonging to abortion access providers, queer groups and reproductive health organisations in the past weeks in what campaigners call one of the “biggest waves of censorship” on its platforms in years.&lt;/p&gt;
    &lt;p&gt;The takedowns and restrictions began in October and targeted the Facebook, Instagram and WhatsApp accounts of more than 50 organisations worldwide, some serving tens of thousands of people – in what appears to be a growing push by Meta to limit reproductive health and queer content across its platforms. Many of these were from Europe and the UK, however the bans also affected groups serving women in Asia, Latin America and the Middle East.&lt;/p&gt;
    &lt;p&gt;Repro Uncensored, an NGO tracking digital censorship against movements focused on gender, health and justice, said that it had tracked 210 incidents of account removals and severe restrictions affecting these groups this year, compared with 81 last year.&lt;/p&gt;
    &lt;p&gt;Meta denied an escalating trend of censorship. “Every organisation and individual on our platforms is subject to the same set of rules, and any claims of enforcement based on group affiliation or advocacy are baseless,” it said in a statement, adding that its policies on abortion-related content had not changed.&lt;/p&gt;
    &lt;p&gt;Campaigners say the actions indicate that Meta is taking its Trump-era approach to women’s health and LGBTQ+ issues global. Earlier this year, it appeared to “shadow-ban” or remove the accounts of organisations on Instagram or Facebook helping Americans to find abortion pills. Shadow-banning is when a social media platform severely restricts the visibility of a user’s content without telling the user.&lt;/p&gt;
    &lt;p&gt;In this latest purge, it blocked abortion hotlines in countries where abortion is legal, banned queer and sex-positive accounts in Europe, and removed posts with even non-explicit, cartoon depictions of nudity.&lt;/p&gt;
    &lt;p&gt;“Within this last year, especially since the new US presidency, we have seen a definite increase in accounts being taken down – not only in the US, but also worldwide as a ripple effect,” said Martha Dimitratou, executive director of Repro Uncensored.&lt;/p&gt;
    &lt;p&gt;“This has been, to my knowledge, at least one of the biggest waves of censorship we are seeing,” she said.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Campaigners have accused Meta of being condescending and unresponsive, with the company offering only vague reasons why certain accounts were taken down – and appearing unwilling to engage.&lt;/p&gt;
    &lt;p&gt;In one email shared with the Guardian, a Meta consultant appears to invite a number of reproductive health organisations to a closed-door online briefing about “the challenges that you are facing with Meta’s content moderation policies”.&lt;/p&gt;
    &lt;p&gt;The email says the meeting “will not be an opportunity to raise critiques of Meta’s practices or to offer recommendations for policy changes”.&lt;/p&gt;
    &lt;p&gt;Dimitratou said such closed-door meetings had happened before, saying they “reinforce the power imbalance that allows big tech to decide whose voices are amplified and whose are silenced”.&lt;/p&gt;
    &lt;p&gt;In another instance, a Meta employee counselled an affected organisation in a personal message to simply move away from the platform entirely and start a mailing list, saying that bans were likely to continue. Meta said it did not send this message.&lt;/p&gt;
    &lt;p&gt;Meta’s recent takedowns are part of a broader pattern of the company purging accounts, and then – at times – appearing to backtrack after public pressure, said Carolina Are, a fellow at Northumbria University’s Centre for Digital Citizens.&lt;/p&gt;
    &lt;p&gt;“It wouldn’t be as much of a problem if platforms’ appeals actually worked, but they don’t. And appeals are the basis of any democratic justice system,” she added.&lt;/p&gt;
    &lt;p&gt;Meta said that it aimed to reduce enforcement mistakes against accounts on its platform, but added that the appeals process for banned accounts had become frustratingly slow.&lt;/p&gt;
    &lt;p&gt;Organisations affected by the bans include Netherlands-registered Women Help Women, a nonprofit offering information about abortion to women worldwide, including in Brazil, the Philippines and Poland. It fields about 150,000 emails from women each year, said its executive director, Kinga Jelinska.&lt;/p&gt;
    &lt;p&gt;Women Help Women has been on Facebook for 11 years, said Jelinska, and while its account had been suspended before, this was the first time it was banned outright. The ban could be “life-threatening”, she said, pushing some women towards dangerous, less reliable information sources. Little explanation was given for the ban.&lt;/p&gt;
    &lt;p&gt;A message from Meta to the group dated 13 November said its page “does not follow our Community Standards on prescription drugs”, adding: “We know this is disappointing, but we want to keep Facebook safe and welcoming for everyone.”&lt;/p&gt;
    &lt;p&gt;“It’s a very laconic explanation, a feeling of opacity,” Jelinska said. “They just removed it. That’s it. We don’t even know which post it was about.”&lt;/p&gt;
    &lt;p&gt;Meta said more than half of the accounts flagged by Repro Uncensored have been reinstated, including Women Help Women which it said was taken down in error. “The disabled accounts were correctly removed for violating a variety of our policies including our Human Exploitation policy,” it added.&lt;/p&gt;
    &lt;p&gt;Jacarandas was founded by a group of young feminists when abortion was decriminalised in Colombia in 2022, to advise women and girls on how to get a free, legal abortion. The group’s executive director, Viviana Monsalve, said its WhatsApp helpline had been blocked then reinstated three times since October. The WhatsApp account is currently banned and Monsalve said they had received little information from Meta about whether this would continue.&lt;/p&gt;
    &lt;p&gt;“We wrote [Meta] an email and said, ‘hey, we are a feminist organisation. We work in abortion. Abortion is allowed in Colombia up to 24 weeks. It’s allowed to give information about it,’” said Monsalve.&lt;/p&gt;
    &lt;p&gt;Without Meta’s cooperation, Monsalve said it was difficult to plan for the future. “You are not sure if [a ban] will happen tomorrow or after tomorrow, because they didn’t answer anything.”&lt;/p&gt;
    &lt;p&gt;Meta said: “Our policies and enforcement regarding abortion medication-related content have not changed: we allow posts and ads promoting healthcare services like abortion, as well as discussion and debate around them, as long as they follow our policies.”&lt;/p&gt;
    &lt;p&gt;While groups such as Jacarandas and Women Help Women had their accounts removed outright, other groups said that they increasingly faced Meta restricting their posts and shadow-banning their content.&lt;/p&gt;
    &lt;p&gt;Fatma Ibrahim, the director of the Sex Talk Arabic, a UK-based platform which offers Arabic-language content on sexual and reproductive health, said that the organisation had received a message almost every week from Meta over the past year saying that its page “didn’t follow the rules” and would not be suggested to other people, based on posts related to sexuality and sexual health.&lt;/p&gt;
    &lt;p&gt;Two weeks ago, these messages escalated to a warning, in which Meta noted its new policies on nudity and removed a post from the Sex Talk Arabic’s page. The offending post was an artistic depiction of a naked couple, obscured by hearts.&lt;/p&gt;
    &lt;p&gt;Ibrahim said the warning was “condescending”, and that Meta’s moderation was US-centric and lacked context.&lt;/p&gt;
    &lt;p&gt;“Despite the profits they make from our region, they don’t invest enough to understand the social issues women fight against and why we use social media platforms for such fights,” she said.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46230072</guid><pubDate>Thu, 11 Dec 2025 11:26:45 +0000</pubDate></item><item><title>South Korea – A Cautionary Tale for the Rest of Humanity</title><link>https://worksinprogress.co/issue/two-is-already-too-many/</link><description>&lt;doc fingerprint="a72e935d0ea19785"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Every hundred South Koreans today will have only six great-grandchildren between them. The rest of the world can learn from Korea’s catastrophe to avoid the same fate.&lt;/head&gt;
    &lt;p&gt;South Korea has the lowest fertility rate in the world. Its population is (optimistically) projected to shrink by over two thirds over the next 100 years. If current fertility rates persist, every hundred South Koreans today will have only six great-grandchildren between them.&lt;/p&gt;
    &lt;p&gt;This disaster has sources that will sound eerily familiar to Western readers, including harsh tradeoffs between careers and motherhood, an arms race of intensive parenting, a breakdown in the relations between men and women, and falling marriage rates. In all these cases, what distinguishes South Korea is that these factors occur in a particularly extreme form. The only factor that has little parallel in Western societies is the legacy of highly successful antinatalist campaigns by the South Korean government in previous decades.&lt;/p&gt;
    &lt;p&gt;Subscribe for $100 to receive six beautiful issues per year.&lt;/p&gt;
    &lt;p&gt;South Korea is often held up as an example of the failure of public policy to reverse low fertility rates. This is seriously misleading. Contrary to popular myth, South Korean pro-parent subsidies have not been very large, and relative to their modest size, they have been fairly successful.&lt;/p&gt;
    &lt;p&gt;The story of South Korean fertility rates is thus doubly significant. On the one hand, it illustrates just how potent anti-parenting factors can become, creating a profoundly hostile environment in which to raise children and discouraging a whole society from doing so. On the other, it may offer a scintilla of hope that focused and generous policy can address these problems, shaping a way back from the brink of catastrophe.&lt;/p&gt;
    &lt;head rend="h3"&gt;Career-motherhood conflict&lt;/head&gt;
    &lt;p&gt;In every developed country, women struggle to reconcile their careers with a satisfying family life and their preferred number of children. This tradeoff is exceptionally severe in South Korea.&lt;/p&gt;
    &lt;p&gt;Despite its very high level of female education, South Korea has the largest gender employment gap in the OECD. There is almost no employment gap between men (73.3 percent) and unmarried women without children (72.8 percent). The gap is driven by the fact that large numbers of women stop working when they have kids: only 56.2 percent of mothers work, the fourth lowest in the OECD.&lt;/p&gt;
    &lt;p&gt;In South Korea, mothers’ employment falls by 49 percent relative to fathers, over ten years – 62 percent initially, then rising as their child ages. In the US it falls by a quarter and in Sweden by only 9 percent.&lt;/p&gt;
    &lt;p&gt;South Koreans work more hours – 1,865 hours a year – in comparison with 1,736 hours in the US and 1,431 in Sweden. This makes it hard to balance work and motherhood, or work and anything else.&lt;/p&gt;
    &lt;p&gt;There is intense pressure from employers for women not to have children: in surveys, 27 percent of female office workers report being coerced into signing illegal contracts promising to resign if they fall pregnant or marry.&lt;/p&gt;
    &lt;p&gt;South Korean work culture is notoriously sexist. After their long work days, colleagues are expected to go out drinking together. Alice Evans, a social scientist, spoke to a young South Korean woman who went to a karaoke bar with her colleagues and found they hired a sexy woman to serve them drinks. Her boss, noting her discomfort, chided her: ‘You shouldn’t be surprised by this, at your age.’&lt;/p&gt;
    &lt;p&gt;In response to these taxing hours, and with bosses unwilling to make accommodations to mothers, over 62 percent of women quit their jobs around the birth of their first child. (Some go back soon afterwards, which is why the total fall in employment is slightly less than this, at 49 percent.)&lt;/p&gt;
    &lt;p&gt;By the time a child turns ten, their mother will have seen her earnings fall by an average of 66 percent, considerably higher than the earnings penalty in countries including the US (31 percent), UK (44 percent), and Sweden (32 percent).&lt;/p&gt;
    &lt;p&gt;Put together, all this means that having children is extremely expensive for South Korean women in terms of their careers and earnings.&lt;/p&gt;
    &lt;head rend="h3"&gt;Resource-intensive parenting&lt;/head&gt;
    &lt;p&gt;Perhaps due to high infant mortality rates in the past, Koreans treat a child’s first birthday as a very significant milestone. A Doljanchi is a party and ceremony when the child is dressed in an ornate traditional outfit and presented with a series of objects – a pen, a thread, money, a sword – and what they choose is meant to show what the future has in store for them: academic success, longevity, wealth, or martial prowess respectively. Parents will often try to coax their child to choose something specific but, being only one, the child is usually uncooperative.&lt;/p&gt;
    &lt;p&gt;Traditionally, this party would have been done at home. Now, they have become more lavish. They are typically hosted like weddings: in hotel ballrooms with long guest lists, party favors, and multicourse meals.&lt;/p&gt;
    &lt;p&gt;As with weddings, the costs of the ceremony vary from family to family. But a typical Korean family can expect to spend a month’s wages on the Doljanchi.&lt;/p&gt;
    &lt;p&gt;Today, South Korea is the world’s most expensive place to raise a child, costing an average of $275,000 from birth to age 18, which is 7.8 times the country’s GDP per capita compared to the US’s 4.1. And that is without accounting for the mother’s forgone income.&lt;/p&gt;
    &lt;p&gt;Fueled by intense competition for university places, cram schools and private tuition are popular in many low-fertility East Asian countries. Uptake is also high in Taiwan and Singapore, and 38 percent of Chinese children used the country’s shadow education system before the government clamped down.&lt;/p&gt;
    &lt;p&gt;But South Korea is even worse. Almost 80 percent of children attend a hagwon, a type of private cram school operating in the evenings and on weekends. In 2023, South Koreans poured a total of $19 billion into the shadow education system. Families with teenagers in the top fifth of the income distribution spend 18 percent ($869) of their monthly income on tutoring. Families in the bottom fifth of earners spend an average of $350 a month on tutoring, as much as they spend on food.&lt;/p&gt;
    &lt;p&gt;On top of the expense, these are grueling for the children themselves, and they start very young. Nearly half of children under six receive some form of private tuition. As a Korean TikToker explains, even if you have the money, you can’t necessarily get your teenager into the most elite hagwons. They have to have attended the right hagwons when they were younger to have a shot at getting in later.&lt;/p&gt;
    &lt;p&gt;These high school hagwons have grueling schedules. During term time, students typically have days that run from 7am to 2am, starting with morning study sessions before school and finishing with homework in the library. During school holidays, lots of students go to boarding hagwons where the days are tightly scheduled from 6am to midnight and include tests late in the evening.&lt;/p&gt;
    &lt;p&gt;Parents don’t feel like they can just leave their children to fend for themselves in the public education system. Because most students, upon starting high school, have already learned the entire mathematics curriculum, teachers expect students to be able to keep up with a rapid pace. There’s even pejorative slang for the kids who are left behind– supoja – meaning someone who has given up on mathematics.&lt;/p&gt;
    &lt;p&gt;The intense schedule and the lack of sleep is why places like Daechi, a neighborhood renowned for its hagwons, have screaming pods for frustrated teenagers to let off some steam.&lt;/p&gt;
    &lt;p&gt;Since the 1980s, the South Korean government has regarded shadow education as a ‘social evil’ that widens inequality. The government even temporarily banned private tuition, but tutors simply went underground and charged high ‘risk premiums’. The ban was removed in 2000, but the government is still trying to reduce demand by curfewing operating hours, providing after-school teaching of its own, and regulating the shadow education industry. In Seoul today, all hagwons must close by 10pm. But some operators have responded by simply switching off the lights and continuing with lessons in the dark, or by loading students onto a bus to carry on teaching on the road. It is hard to quash the system when demand is so strong.&lt;/p&gt;
    &lt;p&gt;South Korea already has the highest share of young tertiary graduates in the OECD, and its competitive job market means that degrees from the most prestigious universities are immensely valuable. The acceptance rate for the country’s top three universities, known as the SKY colleges, is just one percent. &lt;lb/&gt;Parents know that unless they are wealthy, having a second child will damage their ability to pay for the best education for their first. Korean parents with more children usually spend less per child on education and 27 percent of Korean parents, when polled, say they think the high education and childcare burden is the main reason for falling fertility.&lt;/p&gt;
    &lt;p&gt;High performance in the university entrance exam is an arms race: if everyone becomes better at the exam, nobody is better off. Yet parents are individually incentivized to force their children to take part, at great cost to their whole families.&lt;/p&gt;
    &lt;head rend="h3"&gt;The decline of marriage&lt;/head&gt;
    &lt;p&gt;Across the world, men and women are more and more divided. In the public sphere, this manifests as political polarization in which gender is becoming a salient political division, just like class and age. In the private sphere, we see that people are living alone and eschewing romantic love.&lt;/p&gt;
    &lt;p&gt;As with the other trends we have discussed, this is true everywhere, but in South Korea it is happening to an extreme degree. About 43 percent of South Korean women aged 15–49 are married, compared to 52 percent in the US.&lt;/p&gt;
    &lt;p&gt;A chasm between young South Korean men and women has been opening up for years. In 2018, the MeToo movement took off in the country. The inciting event was when prosecutor Seo Ji-hyun accused a Justice Ministry official of groping her at a funeral in 2010. Initially she just asked for an apology and was, instead, demoted. She uploaded evidence of this in 2018 with the MeToo hashtag and was interviewed on television.&lt;/p&gt;
    &lt;p&gt;Her story kicked off an investigation in the prosecutor’s office and a series of other women, in other sectors, were emboldened to come forward. At first, the MeToo movement found broad popularity among younger people: 77 percent of South Korean men under 30 said they supported it.&lt;/p&gt;
    &lt;p&gt;But Korean men turned on the movement and there were several suicides by men accused of wrongdoing. The Journalist Association of Korea says that the press, in general, was too sensationalist in its reporting and didn’t respect the rights and privacy of the people involved.&lt;/p&gt;
    &lt;p&gt;Because of decades of sex-selective abortion, young men in Korea outnumber young women. For today’s 30-year-old South Koreans, there are 115 men for every 100 women. This skewed sex ratio, combined with the fact such a large proportion of young Korean women are persistently single, means that men face a punishing dating market. Combined with a competitive labor market, in which men are competing for jobs against their better-educated female counterparts, and two years of male-only conscription, Korean men who are unlucky in love and employment may end up blaming women for their problems.&lt;/p&gt;
    &lt;p&gt;The resentment goes both ways. The now-defunct feminist troll site Megalia used a pinching hand as its logo to make fun of men for having small penises, supposedly mirroring the high standards of beauty that Korean women are held to by men. Even though the website shut down in 2017, the gesture is still associated with misandry.&lt;/p&gt;
    &lt;p&gt;The ‘finger pinching’ conspiracy started in May 2021, with an advert for a convenience store chain showing a hand pinching towards a small sausage. There was outcry and the company pulled the advert and apologized. The protestors said the gesture was a misandrist dog whistle and several other companies and women were attacked for supposedly using it to mock Korean men.&lt;/p&gt;
    &lt;p&gt;By this time, in 2021, only 29 percent of young men said they still supported MeToo. Now, majorities of young men view themselves as victims of female supremacy and of sex-based discrimination.&lt;/p&gt;
    &lt;p&gt;In 2022, Yoon Suk Yeol was elected President of South Korea on a tide of male support. Over half (59 percent) of male voters aged 18–29 voted for Yoon, in comparison with only 34 percent of women aged 18–29. Yoon embraced the gender war narrative, attributing South Korea’s ultra-low fertility to feminism and arguing that structural discrimination against women did not exist in South Korea. He also vetoed a law that was attempting to expand the definition of rape to include all nonconsensual sex, making it so that violence and intimidation were not necessary prerequisites.&lt;/p&gt;
    &lt;p&gt;In the June 2025 presidential election, the two conservative candidates together won the support of 74 percent of men in their twenties and 60 percent of men in their thirties. Meanwhile, only 36 percent of women in their twenties and 41 percent of women in their thirties voted for these candidates.&lt;/p&gt;
    &lt;p&gt;Conservative gender attitudes remain common across the country. Fifty-three percent of South Koreans still agree that ‘when jobs are scarce, men should have more right to a job than women’. A third of South Koreans say that a university education is more important for a boy than for a girl.&lt;/p&gt;
    &lt;p&gt;Other countries are seeing political polarization between the sexes. Across all ages, more men voted for Donald Trump in 2024 than women. Young Americans are also increasingly saying that shared political values are an important feature in a partner. In other places, including Germany, Poland, and the UK, younger men are moving right as younger women move left.&lt;/p&gt;
    &lt;p&gt;Yet South Korea’s polarization is particularly stark, because it has happened so quickly and because gender issues have become so central to the country’s politics.&lt;/p&gt;
    &lt;p&gt;This gap in values might be one reason that younger South Koreans are less likely to date and marry. It could also be a symptom of a culture in which men and women live bifurcated lives: they go to different schools, consume different media and news, and socialize in unmixed groups. Less than half of Korean women in their childbearing years are married.&lt;/p&gt;
    &lt;p&gt;But marriage rates have fallen so fast that this number doesn’t give us a full picture. In many countries, the age at which women are getting married for the first time is increasing and the number of women who will ever get married is decreasing. It is difficult to fully separate these trends: how can we know if the decrease in the number of 30-year-olds marrying is going to show up as an increase in the number of 35-year-olds marrying until it has happened? But other statistics imply that most of these women aren’t ever going to marry.&lt;/p&gt;
    &lt;p&gt;In Korea the average age at which women have their first marriage, if they marry at all, is 31. This is within the standard range for a rich country: in the US it is 29, in the UK 31, in Japan 29 and in Sweden it’s 35. But while about half of American women are married by their early thirties, a staggering 77 percent of Korean women aged 30–34 are unmarried.&lt;/p&gt;
    &lt;p&gt;South Koreans in their twenties have been less and less likely to be dating over the decades (from 40 percent without a partner in 1991 to 65 percent in 2018) and singleness is rising in all age cohorts.&lt;/p&gt;
    &lt;p&gt;The decline in marriage is particularly significant in Korea – like most East Asian countries, childbirth out of marriage is much rarer than in other developed countries. Today, only 3 percent of babies in South Korea are born to unmarried parents, compared to 40 percent in the US and 55 percent in Sweden. The collapse of marriage in Korea, therefore, means an even greater collapse of birth rates than it would elsewhere.&lt;/p&gt;
    &lt;head rend="h3"&gt;Where South Korea is unique: antinatalist campaigns and negative population momentum&lt;/head&gt;
    &lt;p&gt;So far, South Korea’s fertility crisis sounds similar to that in the rest of the world – just a much more extreme version. But one cause of its low fertility rate is unusual, especially compared to the Western world. This is the legacy of decades of sustained government action to reduce fertility.&lt;/p&gt;
    &lt;p&gt;In 1961, a military junta led by General Park Chung-Hee seized power. At the time, the average South Korean woman had six children. Park believed that shrinking family sizes would fuel economic development by freeing up more women to work and decreasing the number of dependents per worker.&lt;/p&gt;
    &lt;p&gt;Park’s government started by giving every hospital a family planning unit and promoting contraceptive measures, particularly vasectomies and IUDs. In 1963, every government department was ordered to participate in the national effort: the Defence Ministry offered soldiers vasectomies and the Education Ministry incorporated the supposed dangers of overpopulation into the school curriculum.&lt;/p&gt;
    &lt;p&gt;In the 1970s, the government introduced tax breaks for families with no more than two children. Parents with less than three children who underwent sterilization received priority access to public housing. There were extra social security payments for parents of small families who opted for sterilization.&lt;/p&gt;
    &lt;p&gt;Official messaging also promoted smaller families. The government’s first antinatalist slogan was ‘Have few children and bring them up well.’ Later posters encouraged parents to prioritize ‘quality over quantity’, with mottos such as ‘Let’s have two children and raise them well’, or the frantic ‘Two children is already too many!’&lt;/p&gt;
    &lt;p&gt;Family planning propaganda also attempted to address South Korea’s cultural preference for male over female children, which meant that couples who had only daughters were motivated to continue having children in pursuit of a son. The sex preference was a strong one. In 1971, 50 percent of South Korean women who were asked what a woman should do if she could not give birth to a boy said that the woman should let her husband try for a son with a different woman. Slogans introduced on this matter included “A well bred girl surpasses ten boys”.&lt;/p&gt;
    &lt;p&gt;In many ways, South Korea was simply doing what the world’s great philanthropists, policymakers, and politicians of that time wanted. Fears about a busy, hungry world went mainstream in the sixties. The Kennedy administration publicly argued that population control was a legitimate policy focus. In 1967, President Lyndon B Johnson asked Indian Prime Minister Indira Gandhi ‘to join a truly worldwide effort to bring population and food production back into balance’. Henry Kissinger was worried that ‘excessive population growth’ would hold back economic development and social progress and would undermine US interests by creating political tensions that could lead to instability. The president of the World Bank, Robert McNamara, made family planning a priority for the Bank, giving direct financing for contraception in 1970 and tying population targets to aid. John D Rockefeller III, who founded the Population Council, went on to chair Nixon’s population commission. He spent Rockefeller Foundation money on research and education about contraception.&lt;/p&gt;
    &lt;p&gt;By 1976, Korea’s family planning policy, centering on the promotion and distribution of contraceptives, is estimated to have averted between 1.8 million and 2.1 million births. And it did so remarkably cost-effectively, at a cost of just $103 per prevented birth in today’s money. Between 1960 and 1978, South Korea’s total fertility rate fell from six children per woman to three. Comparable drops took 96 years in the UK and 82 in the US.&lt;/p&gt;
    &lt;p&gt;These antinatalist policies survived the end of Park’s dictatorship in 1979. The fertility rate continued to plummet, falling below replacement rate (2.1 children per woman) in 1984. In 1989 the government stopped giving out free contraceptives and relaxed the sterilization drive. By 1990, South Korea was experiencing the consequences of these policies: the average age of the country was now rising, its working age population had begun to fall, and the country now had an imbalanced sex ratio caused by sex-selective abortions. In 1994, the government officially abandoned its population suppression targets. It would begin its explicitly pro-natalist policies only 11 years later.&lt;/p&gt;
    &lt;p&gt;Once the number of people at or below reproductive age has shrunk below a certain level, averting population decline becomes extremely difficult. Even if the remaining young people rapidly increase their fertility, there are not enough of them to avert decline from attrition in larger, older groups. And a culture dominated by older and childless people becomes less child friendly. Schools and parks close down and institutions are shaped to cater for the majority. Korea is known for its ‘no children’ policies in many cafes and restaurants.&lt;/p&gt;
    &lt;p&gt;Between 1990 and 2023, the number of South Korean children declined by 50 percent, while the number of over-65s increased by 340 percent. For South Korea to just maintain its current old-age dependency ratio – 3.9 working adults for every person over 65 – in 30 years’ time, its fertility rate would have to skyrocket to over 10 babies per woman. If it were to fall only as low as Japan’s dependency ratio, the lowest in the OECD – 2 working adults for every person over 65 – it would still have to increase its birth rate to 4.2 babies per woman. This requires extreme behavioral change in a narrowing group of people.&lt;/p&gt;
    &lt;p&gt;While the number of South Korean children has halved since 1990, the US has seen its number increase by 11 percent, the UK 9 percent, and Sweden 19 percent. While these countries all have large cohorts of older people who will be difficult to replace as they die, the number of young people has not fallen to South Korean levels, both because of South Korea’s extra-low fertility rate and low levels of immigration. Foreign nationals represent only 5.1 percent of its population. In contrast, with so many more young people, the mountain that the Americans, Brits, and Swedes must climb to stabilize populations is far less steep than that faced by South Korea’s leaders.&lt;/p&gt;
    &lt;p&gt;Childbearing decisions are memetically influenced. They are shaped by the decisions peers make and the examples they see around them in everyday life. Thirty years of antinatalism have left their mark on South Korean attitudes. By normalizing small family sizes and childlessness, the government ushered in a self-perpetuating culture of lowered fertility. Only 28 percent of unmarried South Koreans aged 19–49 now say they want children, while 51 percent of childless Americans aged 18–34 say they want children.&lt;/p&gt;
    &lt;head rend="h3"&gt;South Korea’s recent pro-child policies have still probably helped&lt;/head&gt;
    &lt;p&gt;Since 2022, in an explicit effort to raise fertility rates, the South Korean government has given couples a grant of $1,500 upon the birth of a first child. As of 2023, this is followed by $528 a month until a child is one, $264 until a child is two and then $150 a month until elementary school starts. Every South Korean baby is now accompanied by some $22,000 in government support through different programs over the first few years of their lives. But they will cost their parents an average of roughly $15,000 every year for eighteen years, and these policies do not come close to addressing the child penalty for South Korean mothers.&lt;/p&gt;
    &lt;p&gt;While the government’s attempts to revive fertility might appear unsuccessful, the situation would likely be worse without them. New evidence, which analyzes the variation in the generosity of South Korean baby bonuses across districts and time, suggests that more generous cash transfers are causing more babies to be born.&lt;/p&gt;
    &lt;p&gt;For each ten percent increase in the bonus, fertility rates have risen by 0.58 percent, 0.34 percent, and 0.36 percent for first, second, and third births respectively. The effect appears to be the result of a real increase in births, rather than a shift in the timing of births. For example, where a district increased the generosity of a bonus for second births, more second children were born, but births of first and third children did not change. This suggests that the decline would have been faster and harder in the absence of pro-child policies. These gains, while real, just aren’t enough to counteract decades of antinatalist policy, the world’s toughest gender divide, the world’s biggest marriage penalty, and the world’s most intense schooling culture.&lt;/p&gt;
    &lt;p&gt;This aligns with global examples suggesting that child-friendly policies can raise fertility rates. For a century, France was Europe’s lowest-fertility nation. After launching an active pronatal campaign in the 1920s, France is now Europe’s highest-fertility country. The French sides of the Franco-Spanish border and the Italian-Franco border have higher fertility than the regions they border. The program includes family-friendly tax breaks, baby bonuses, and strong maternity employment protections. France has consistently sat about 0.3 children above the Western European average.&lt;/p&gt;
    &lt;p&gt;Similarly, South Tyrol in northeastern Italy has a fertility rate higher than any other Italian region, and it has actually increased since the 1990s. South Tyrol has a highly functional childcare system and gives parents a monthly €200 payment for each child under three.&lt;/p&gt;
    &lt;p&gt;There are other examples too. Nagi in rural Japan saw its fertility rate increase from 1.4 in 2005 to 2.7 today after giving parents cash, cheap childcare, housing subsidies, and free healthcare for children. Australia, Spain, Poland, the UK, and Russia all had more births after introducing different policies that gave families more money. Germany increased births among educated women by creating a generous earnings-dependent parental leave allowance&lt;/p&gt;
    &lt;p&gt;It may be too late for South Korea. It is surrounded by real and potential enemies, including one which is committed to its destruction, and its army relies on a rapidly waning number of young conscripts. As its population gets older, more and more resources are going to be spent sustaining the elderly. This means less money for baby bonuses and more for nursing homes, as well as perpetually increasing taxes and hours. At some point, the few youngsters that are left may start to leave for less burdensome futures elsewhere, worsening the load on those that remain.&lt;/p&gt;
    &lt;p&gt;This is a future worth avoiding: the rest of the world would be much poorer without South Korea. South Korea is an extremely innovative nation. It files the most patents in the world – not per capita, in absolute terms – with a population just one sixth of the US’s.&lt;/p&gt;
    &lt;p&gt;South Korea has a genuinely unique culture. Isolated from the rest of the world, Korean culture has been able to grow and flourish on its own. For example, Koreanic languages (Korean and Jejuan, from the island of Jeju in South Korea) are considered to be an isolated family of their own, totally separate from other languages in East Asia. And its cultural exports, from K-pop and K-dramas to skin care and Korean barbecue are popular throughout the world.&lt;/p&gt;
    &lt;p&gt;Along with its technology and culture, Korea is also exporting a warning about what is to come for us all. Much of the world is getting older for the same reasons as South Korea. Families, especially mothers, are made poorer when they choose to have children. As the demands of educational institutions get more extreme, children are getting more expensive to raise. And while less extreme than in Seoul, men and women are drifting apart around the developed world.&lt;/p&gt;
    &lt;p&gt;South Korea is often seen as a testament to the futility of pro-child policies. This conclusion is the opposite of the truth. South Korea’s pro-child policies have not been that well-funded and may not have been perfectly targeted, but they have still been fairly effective. They just fall far short of what is necessary.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46230576</guid><pubDate>Thu, 11 Dec 2025 12:30:25 +0000</pubDate></item></channel></rss>