<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sat, 29 Nov 2025 20:38:28 +0000</lastBuildDate><item><title>System 7 natively boots on the Mac mini G4</title><link>https://macos9lives.com/smforum/index.php?topic=7711.0</link><description>&lt;doc fingerprint="b63c99572fc08fad"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;(And Mac OS 8!)&lt;/p&gt;&lt;p&gt;Hey, guys!&lt;/p&gt;&lt;p&gt;Surely y'all know and have enjoyed Mac OS 9.2.2 booting and beautifully-running on all four Mac mini G4 models for close to 8 years now. (Wow!)&lt;/p&gt;&lt;p&gt;Well, that was one massive revolution...&lt;/p&gt;&lt;p&gt;... But most of us did not think we would live to see the day New World ROM machines, even more so the likes of the Mac mini G4, to NATIVELY boot System 7:&lt;/p&gt;&lt;p&gt;(Gotta love it trying to display 1 GB RAM capacity.)&lt;/p&gt;&lt;p&gt;Before your eyeballs leave your eyesockets completely, I ought to warn that there's still much to be sorted out in this, especially sound, video and networking (the usual suspects). In other words, your mileage may vary, so keep expectations in check!&lt;/p&gt;========================================================&lt;lb/&gt;OK, so HOW in the WORLD is any of this possible?&lt;lb/&gt;========================================================&lt;p&gt;It turned out "New World ROM" Macs had a cousin born out of the clone program (until the usual villain, Steve Jobs, came and killed it), which was an architecture called "&lt;/p&gt;CHRP&lt;p&gt;" (pronounced "chirp"). It was the successor to &lt;/p&gt;PReP&lt;p&gt;, but, unlike PReP, Mac OS was also going to be officially-bootable on it. Close to no CHRP machines ever saw the light of the day, thanks to Jobs' return. Nonetheless, Apple internally developed Mac OS 7.6 ~ 8.0 for CHRP systems before it got axed. It's just that they never released it, but the development was done regardless. On October 2025, it turned out someone preserved some of these Mac versions, which were then acquired and preserved and shared with the world. (&lt;/p&gt;Macintosh Garden link&lt;p&gt;, &lt;/p&gt;archive.org link&lt;p&gt;.)&lt;/p&gt;&lt;p&gt;Although CHRP was left to die, the so-called "New World ROM" Macs inherited much of its architecture and design. As you probably know, these Macs rely on an extra system file called "Mac OS ROM", whereas "Old World ROM" Macs do not need it, and can use their own actual ROM to get Mac OS going. This meant any Mac OS version unaware of the concept of a Mac OS ROM file could not just simply boot in a New World ROM Mac normally. People were able to boot Mac OS versions as low as 8.1, but not any lower, and that too only for the very first few New World ROM Macs, but none of the later ones, which increasingly had a higher and higher minimum OS version.&lt;/p&gt;&lt;p&gt;But not anymore, as the following major events happened:&lt;/p&gt;&lt;p&gt;- The recent Mac OS 8.0 CHRP leaks provided an earlier ROM file that, it turns out, allows regular Mac OS 8.0 to boot, as well. Or, alternatively, the Mac OS ROM file that always worked with Mac OS 8.1 also worked on these Mac OS 8.0 CHRP releases. (Exact details are fuzzy in my memory by now, so someone else might want to correct me if I got something wrong.)&lt;/p&gt;&lt;p&gt;- The recent Mac OS 7.6 CHRP leak provided an additional System Enabler file, which could be exploited for loading Mac OS ROM files. I forget if that's how it worked out-of-the-box, or if a bit of hacking to the System Enabler was required for that, however what I do remember clearly is that, while the System Enabler was hardcoded so that artifically no OS earlier than 7.6 could use it, the OS version check could be patched out of it, so that System 7.5.x (and potentially earlier) can also use it.&lt;/p&gt;&lt;p&gt;In other words, &lt;/p&gt;this file is the reason that earlier Mac OS versions can make use of the Mac OS ROM file&lt;p&gt;, thus bringing Mac OS 7.6.1 and earlier potentially to ALL New World ROM Macs!&lt;/p&gt;&lt;p&gt;(Trivia tidbit: Apparently this enabler was also present in certain archives of the Mac OS 8.0 betas from when it was still known as "Mac OS 7.7". Oops! This thing was right under our nose all this while!)&lt;/p&gt;&lt;p&gt;- Of course, as hinted at previously, a System Enabler _alone_ is NOT enough to boot System 7 and the like when even much newer systems that were already aware of the Mac OS ROM file could not boot. The newer the model of the New World ROM Mac, the less you could actually "go back". The reason is simple: Mac OS ROM files, over time through its various versions, would get new features added, BUT also would remove older ones which were required by older OS versions. The solution? Using &lt;/p&gt;ELN's great Mac OS ROM patching tools&lt;p&gt; (plus other tools of his own), "Rairii" AKA "Wack0", known for his amazing PPC Windows NT 3.51 / NT 4.0 project on &lt;/p&gt;PowerMacs&lt;p&gt; and the &lt;/p&gt;Nintendo GC / Wii / Wii U&lt;p&gt;, analyzed many of these Mac OS ROM files, and fixed + patched + stitched together new Mac OS ROM files that attempt to keep ALL the old features that were removed AND all the new features that were added. In other words, the ultimate Mac OS ROM file that boots everything and runs everything (roughly-speaking). He also is the one who figured out and hacked the System Enabler to also accept OSes earlier than Mac OS 7.6.&lt;/p&gt;&lt;p&gt;Keep in mind, however, that this effort essentially allows Macs that are already able to boot SOME version of Mac OS to ALSO boot older versions. But if a given machine cannot boot ANY Mac OS version, such as the two DLSD PowerBook G4s (&lt;/p&gt;15"&lt;p&gt;, &lt;/p&gt;17"&lt;p&gt;), these patches cannot do anything about that: Their incompatibilities need to be addressed first and separately.&lt;/p&gt;&lt;p&gt;One more interesting thing to note about the similarity between CHRP systems and New World ROM Macs: If you check ANY "Mac OS ROM" file to see its TYPE and CREATOR codes, you will see they are "tbxi" and, you guessed it, "&lt;/p&gt;chrp&lt;p&gt;", respectively. I couldn't believe "chrp" was in ALL the Mac OS ROM files all these years!&lt;/p&gt;========================================================&lt;lb/&gt;Where can I get ahold of this EPIC stuff ? ? ? ? ?&lt;lb/&gt;========================================================&lt;p&gt;Rairii's "super" ROMs are available on &lt;/p&gt;this GitHub repository&lt;p&gt;, under &lt;/p&gt;releases&lt;p&gt;. You may also fetch the patched System Enabler for Mac OS 7.6.1 and earlier from there, and place it in the System Folder. Make sure to download the files from the latest release there.&lt;/p&gt;&lt;p&gt;Note that he applied his patches to 3 different versions of the (US) ROMs:&lt;/p&gt;&lt;p&gt;- 10.2.1 with CPU Software 5.9: The "latest and greatest" Mac OS ROM file of all Mac OS. For reference, this is also the ROM version that the &lt;/p&gt;1.628 GB max RAM Mac OS ROM we have was based on (thus going beyond the 1.5 GB limit)&lt;p&gt;, although do note that the RAM limit break patches are NOT included in this, at least not yet as of the time of writing.&lt;/p&gt;&lt;p&gt;- 2.5.1: A much earlier version of the ROM, but still new enough to support USB. See the GitHub page for details.&lt;/p&gt;&lt;p&gt;- 1.7.1: A very early ROM, which can be well-leveraged by very early New World ROM Macs. See the GitHub page for details.&lt;/p&gt;&lt;p&gt;Note you need ROM version 9.1 or higher to use ATA-6 AKA Ultra ATA/100 AKA Kauai drivers, which are essential on the likes of the Mac mini G4 and the MDD. Special notes for the Mac mini G4 are further down.&lt;/p&gt;========================================================&lt;lb/&gt;What is the COMPLETE list of Mac OS versions that now boot?&lt;lb/&gt;========================================================&lt;p&gt;To be exact, this is the complete list of OSes I have attempted, all on the Mac mini G4 1.5GHz model, with the following results:&lt;/p&gt;&lt;p&gt;- System 6.0.8: &lt;/p&gt;No boot&lt;p&gt;. You get a Happy Mac, followed by a blinking question mark in a floppy icon. (Note: Although this very attempt is UTTERLY insane for multiple technical reasons, it might be not AS seemingly-impossible as one may think, as the 68k emulator resides within the Mac OS ROM file.)&lt;/p&gt;&lt;p&gt;- System 7.0: &lt;/p&gt;No boot&lt;p&gt;. You get a Happy Mac, but then a warning window pops up saying System 7.0 cannot boot on this computer.&lt;/p&gt;&lt;p&gt;- System 7.1.2: &lt;/p&gt;No boot&lt;p&gt;. You get a Happy Mac, but then a warning window pops up saying System 7.1 cannot boot on this computer.&lt;/p&gt;&lt;p&gt;- System 7.5: &lt;/p&gt;BOOTS AND IS STABLE&lt;p&gt;. It requires you to hold shift to turn Extensions (and Control Panels / INITs) off, though, or to get rid of the "Mouse" Control Panel (and possibly more). The system is surprisingly stable! I tested the British version of this one, as Apple's Mac OS Anthology discs did not include the US installers, for some very slacker-y reason.&lt;/p&gt;&lt;p&gt;- System 7.5.2: &lt;/p&gt;Boots, but very broken, close to nothing works&lt;p&gt;. It could be because System 7.5.2 was always VERY machine-specific, and is apparently one of the most broken versions of Mac OS of ALL time, regardless. The machine-specific enablers, and other things, might be what is making it so unstable.&lt;/p&gt;&lt;p&gt;- System 7.5.3: &lt;/p&gt;BOOTS AND IS STABLE&lt;p&gt;. It requires you to hold shift to turn Extensions (and Control Panels / INITs) off, though, or to get rid of the "Mouse" Control Panel (and possibly more). The system is surprisingly stable!&lt;/p&gt;&lt;p&gt;- Mac OS 7.6: &lt;/p&gt;BOOTS AND IS STABLE&lt;p&gt;. Holding shift is not required here. What else can I say? It "works".&lt;/p&gt;&lt;p&gt;- Mac OS 8.1: &lt;/p&gt;BOOTS AND IS STABLE&lt;p&gt;. Holding shift is not required here, either. Behaves much the same as the others, except we now have HFS+ by default. Still, it did NOT like me having a 940 GB HFS+ partition, and prompted me to either eject it or format it. (To be fair, older OSes tried to do that, too, but Mac OS 8.1 was THE OS to _officially_ be able to handle HFS+ properly, so there are no excuses for it to fail here. Mac OS 9.2 ~ 9.2.2 all work perfectly with it.)&lt;/p&gt;&lt;p&gt;- Mac OS 8.5: No boot. Rather, it seems like it WOULD boot, but starting with Mac OS 8.5, Mac OS now always checks to see if the machine you are booting from is within a list of Apple-endorsed machine IDs for the given Mac OS version. In other words, Mac OS 8.5 does not know what the Mac mini G4 is, nor what a G4 Cube is (our Mac mini G4 ROM file makes the mini pretend to be the latter). It seems it should be possible to patch out the machine check. According to Rairii, this should be able to be patched out by disabling such a check on the "boot" resource in the Resource Fork of the System file, in ID 3 (also known as "boot3"). For Mac OS 8.6, it seems like this check happens at the end of boot3, wherever a check for machine ID 406 is located, in which after it's detected, the code checks to see if the exact Mac model is whitelisted or not.&lt;/p&gt;&lt;p&gt;- Mac OS 8.5.1: &lt;/p&gt;No boot&lt;p&gt;. All that applies to Mac OS 8.5 also applies to Mac OS 8.5.1.&lt;/p&gt;&lt;p&gt;- Mac OS 8.6: &lt;/p&gt;No boot&lt;p&gt;. It crashes during the start screen, when the loading bar appears, but before the first extension gets to load. See the top-left corner of the picture for a glitchy visual artifact. Same happens if you try to boot with Extensions off.&lt;/p&gt;&lt;p&gt;- Mac OS 9.0.4: No boot. It crashes during the start screen, when the loading bar appears, but before the first extension gets to load. Same happens if you try to boot with Extensions off. Exact same symptoms as when trying to boot Mac OS 8.6 at least on this mini model, including the visual artifact on the top-left corner.&lt;/p&gt;&lt;p&gt;- Mac OS 9.1: &lt;/p&gt;No boot&lt;p&gt;. It crashes during the start screen, when the loading bar appears, but before the first extension gets to load. Same happens if you try to boot with Extensions off. Exact same symptoms as when trying to boot Mac OS 8.6 and Mac OS 9.0.4 at least on this mini model, including the visual artifact on the top-left corner.&lt;/p&gt;&lt;p&gt;- Mac OS 9.2 ~ 9.2.2: BEST OS EVER, BOOTS AND RUNS BEAUTIFULLY. 'Nuff said.&lt;/p&gt;&lt;p&gt;Note that, although I describe many of these as "stable", I mean you can use much of it normally (sound/video/networking aside) without it crashing or misbehaving, at least not too hard, but that is not to say everything works, because that is just not the case. For example, when present, avoid opening the Apple System Profiler, unless you want a massive crash as it struggles trying to profile and gather all the information about your system. Some other apps or Control Panels might either not work, or work up to a certain point, after which they might freeze, requiring you to Force Quit the Finder to keep on going. And so on.&lt;/p&gt;&lt;p&gt;As you can see, I did not yet try System 7.5.5, Mac OS 7.6.1 and Mac OS 8.0. That's because they all are most likely working exactly as their neighbouring versions. But feel free to confirm.&lt;/p&gt;&lt;p&gt;Most non-mini systems should be able to boot Mac OS 8.6 ~ Mac OS 9.1 just fine. A "Mac OS 8.6 Enabler", so to speak, by LightBulbFun, can be renamed as e.g. "Sawteeth" and put inside the System Folder for some machines that cannot boot Mac OS 8.6 normally, so that they can, then, boot it. It is actually a Mac OS ROM file, but can function as a complementary, helper file to aid the actual Mac OS ROM file in this case. If you'd like, check &lt;/p&gt;here&lt;p&gt; for more info. I have attached "Sawteeth.bin" to this post for convenience. LightBulbFun first shared it on &lt;/p&gt;this post&lt;p&gt;, specifically through this &lt;/p&gt;MEGA link&lt;p&gt;.&lt;/p&gt;&lt;p&gt;Most non-mini systems should also be able to boot Mac OS 8.5 and 8.5.1, especially on G3s and earlier. Some G4 Macs might need to spoof the Mac model in Open Firmware (or some other Forth script added to ROM) to boot, though, or patch the check out like I mentioned for the mini earlier. The reason the mini doesn't have the spoofing as an option is that any spoofing in OF would be overwritten by its own specialized Mac OS ROM, which spoofs a G4 Cube, which is clearly not in the whitelist of supported machines for Mac OS 8.5 and 8.5.1.&lt;/p&gt;&lt;p&gt;Also note that the mini behaves as reported above with Mac OS 8.6 with or without this "8.6 enabler" file (and with or without the System Enabler for Mac OS 7.6.1 and earlier, both of which don't seem to get in the way of later, nor earlier, OSes).&lt;/p&gt;&lt;p&gt;Most importantly, I did &lt;/p&gt;not&lt;p&gt; yet attempt to identify which are the latest versions of each Control Panel and Extension for each of these OSes. If I did, I'm sure it would help a lot, and perhaps address quite a number of these problems. The more people chime in on this effort, the better! Imagine if we had a proper "Mac mini G4 System 7.5.5" CD, then an "MDD Mac OS 8.5.1" CD, then an "iBook G3 Mac OS 7.6.1" CD, and so on. Everyone with a G3 or G4 Mac can help by trying things out!&lt;/p&gt;&lt;p&gt;Namely, something akin to MacTron's efforts highlighting the latest Extensions for Mac OS 9.2.2 and Mac OS 8.6 like this, but also for every other Mac OS version:&lt;/p&gt;========================================================&lt;lb/&gt;But how did you get the mini to boot? It requires its own special ROM!&lt;lb/&gt;========================================================&lt;p&gt;Indeed it does! All credit goes to ELN and all of those who helped him on Mac OS 9 Lives!: you can simply use his tooling (which was also very useful for Rairii) to re-apply the Mac-mini-G4-specific ROM patches to Rairii's latest 10.2.1 ROM, and voila! It works as well as you would hope it to! &lt;/p&gt;&lt;p&gt;You can even use the resulting ROM for Mac OS 9.2.2, as well, even though you don't have to: Originally, the Mac mini G4 ROM as we see them in RossDarker's Mac mini G4 CDs version 8 and 9 (AKA v8 and v9), as well as in all the previous versions, were based on the US ROM v9.6.1. I could not find an explanation as to why ROM v10.2.1 wasn't used in the end, even when digging the old Mac mini G4 thread again that started it all. Perhaps because we already had a working ROM with v9.6.1 and did not want to risk breaking anything, or who knows. However, I have thoroughly tested Mac OS 9.2.2 with this new ROM combination (latest Rairii 10.2.1 + latest Mac mini G4 patches AKA v9 patches), and from what I could tell, everything behaves &lt;/p&gt;exactly&lt;p&gt; the same as with the previous ROM we always used. Except now we have the ability to use the same ROM to also boot System 7.5 (I still can't believe this, even though it is true).&lt;/p&gt;&lt;p&gt;(For the record, while the 9.6.1 ROM was also modified to spoof the Mac mini G4 model identifier as a G4 Cube, we also tried to spoof it as a QuickSilver 2002 at one point, but someone reported sound issues with that, and so it was quickly changed back to a G4 Cube and such a change never made it into one of RossDarker's CDs. So just about everyone using Mac OS on the mini for all these years has had a ROM reporting to the OS as a G4 Cube, exclusively.)&lt;/p&gt;&lt;p&gt;To apply the Mac mini G4 patches, I used ELN's &lt;/p&gt;tbxi&lt;p&gt; and &lt;/p&gt;tbxi-patches&lt;p&gt; to apply his "macmini.py" script. You can follow the instructions as per the tbxi-patches page, which you should not let intimidate you even if you are not used to this kind of thing. It's quick and easy, and the scripts are also fully-commentated very nicely by ELN if you are curious about what it is doing and why.&lt;/p&gt;&lt;p&gt;In my case, first I tried using the latest Python 3.13.9 both from Windows 7 (bad idea due to resource fork loss) and macOS 10.14.6 Mojave, but neither worked: it seems like that version of Python was just too new. I then retried with &lt;/p&gt;Python 3.8.10&lt;p&gt; instead (which I chose thinking it might be more period-appropriate for the script's age) on Mojave, which worked &lt;/p&gt;flawlessly&lt;p&gt;. I didn't try it, but perhaps an older Python version might work on PowerPC OS X, as well.&lt;/p&gt;&lt;p&gt;I used the Python installer &lt;/p&gt;from the official website&lt;p&gt;, and I also used an "official" Git installer from &lt;/p&gt;here&lt;p&gt; (thus avoiding any package manager headache... man, how I hate non-Mac-OS systems, including OS X, and package managers in general...)&lt;/p&gt;&lt;p&gt;If somehow someone with plenty of Python knowledge and the willingness to put enough time into it wished to, both tbxi and tbxi-tools could, perhaps, be ported to &lt;/p&gt;MacPython 2.3.5&lt;p&gt;, so that we could do all this patching from Mac OS 9.2.2 directly and natively without leaving our main OS. That would also be awesome! (Of course, it helps that this is also available on more recent systems nonetheless, because then everyone gets to join in on the fun with all kinds of different backgrounds and setups.)&lt;/p&gt;&lt;p&gt;For convenience, I attached the final patched ROM to this post, so that anyone can go wild on their minis right away!&lt;/p&gt;========================================================&lt;lb/&gt;Why should I care when Mac OS 9.2.2 already boots, and runs better?&lt;lb/&gt;========================================================&lt;p&gt;It is also my opinion Mac OS 9.2.2 is the greatest OS, and Mac OS, ever, but not everything that is possible in earlier Mac OS versions is possible in Mac OS 9.2.2. For example, some software requires Mac OS 9.0.4 or earlier to work. A lot of software is System-7-exclusive.&lt;/p&gt;&lt;p&gt;Some people also just prefer the likes of System 7 for its even-lighter memory footprint, lack of mandated Appearance Manager and the like. Mac OS 9.2.2 is already overkill-fast on the mini, and on most New World ROM Macs, but the likes of System 7.5 are just RIDICULOUSLY fast. Even more ridiculously. I still am trying to come into terms with how indescribably fast using it on the mini was. It got even faster when I thought there was no way to get "faster than instantaneous", as Mac OS 9.2.2 always felt instantaneous like no other system already!&lt;/p&gt;&lt;p&gt;People might also have some other kind of reason and/or special attachment to an earlier OS version. Or maybe people want to explore older OS APIs and behaviors, perhaps even make a new application they want to know how it will behave on bare-metal not just on Mac OS 9, but also System 7 etc..&lt;/p&gt;&lt;p&gt;The value is in opening up the doors that give us, the users, more options that help us all out. &lt;/p&gt;========================================================&lt;lb/&gt;Final remarks&lt;lb/&gt;========================================================&lt;p&gt;Above all, thank you to everyone that made this possible. But I wanted to emphasize and give special thanks to Rairii for engineering all these ROMs, Mac84 for archiving and sharing all the CHRP discs, ELN for engineering all the Mac mini G4 ROM compatibility scripts and creating all the ROM and other Mac OS tooling, and to the Mac community at large everywhere that assisted in all of this into becoming reality. There's honestly many, many people to thank we owe over this one way or another, both in small and big ways.&lt;/p&gt;&lt;p&gt;I can't wait to see what people will do with all these new Mac OS versions on their New World ROM systems over the course of time!&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46084956</guid><pubDate>Sat, 29 Nov 2025 03:26:01 +0000</pubDate></item><item><title>Garfield's Proof of the Pythagorean Theorem</title><link>https://en.wikipedia.org/wiki/Garfield%27s_proof_of_the_Pythagorean_theorem</link><description>&lt;doc fingerprint="cf79536f86bd73e1"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Garfield's proof of the Pythagorean theorem&lt;/head&gt;&lt;p&gt;Garfield's proof of the Pythagorean theorem is an original proof of the Pythagorean theorem discovered by James A. Garfield (November 19, 1831 – September 19, 1881), the 20th president of the United States. The proof appeared in print in the New-England Journal of Education (Vol. 3, No.14, April 1, 1876).[1][2] At the time of the publication of the proof Garfield was a congressman from Ohio. He assumed the office of President on March 4, 1881, and served in that position until his death on September 19, 1881, having succumbed to injuries sustained when he was shot in an assassination in July.[3] Garfield is thus far the only President of the United States to have contributed anything original to mathematics. The proof is nontrivial and, according to the historian of mathematics William Dunham, "Garfield's is really a very clever proof."[4] The proof appears as the 231st proof in The Pythagorean Proposition, a compendium of 370 different proofs of the Pythagorean theorem.[5]&lt;/p&gt;&lt;head rend="h2"&gt;The proof&lt;/head&gt;[edit]&lt;p&gt;In the figure, is a right-angled triangle with right angle at . The side-lengths of the triangle are . Pythagorean theorem asserts that .&lt;/p&gt;&lt;p&gt;To prove the theorem, Garfield drew a line through perpendicular to and on this line chose a point such that . Then, from he dropped a perpendicular upon the extended line . From the figure, one can easily see that the triangles and are congruent. Since and are both perpendicular to , they are parallel and so the quadrilateral is a trapezoid. The theorem is proved by computing the area of this trapezoid in two different ways.&lt;/p&gt;&lt;list rend="dl"&gt;&lt;item rend="dd-1"&gt;.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;From these one gets&lt;/p&gt;&lt;p&gt;which on simplification yields&lt;/p&gt;&lt;head rend="h2"&gt;References&lt;/head&gt;[edit]&lt;list rend="ol"&gt;&lt;item&gt;^ G., J. A. (1876). "PONS ASINORUM". New England Journal of Education. 3 (14): 161. ISSN 2578-4145. JSTOR 44764657.&lt;/item&gt;&lt;item&gt;^ Kolpas, Sid J. "Mathematical Treasure: James A. Garfield's Proof of the Pythagorean Theorem". maa.org. Mathematical Association of America. Archived from the original on 8 December 2023. Retrieved 29 November 2023. (The article appeared in the peer-reviewed online journal Convergence published by the Mathematical Association of America.)&lt;/item&gt;&lt;item&gt;^ Del Arte, Alonso (February 2019). "A future president once published a mathematical proof". medium.com. Retrieved 29 November 2023.&lt;/item&gt;&lt;item&gt;^ Dunham, William (1994). The Mathematical Universe: An Alphabetical Journey Through the Great Proofs, Problems, and Personalities. New York: John Wiley &amp;amp; Sons. p. 99. ISBN 0-471-53656-3.&lt;/item&gt;&lt;item&gt;^ Loomis, Elisha Scott (1940). The Pythagorean Proposition (2 ed.). Washington DC: National Council of Teachers of Mathematics. p. 109. ISBN 978-0-87353-036-1. Retrieved 28 November 2023. &lt;code&gt;{{cite book}}&lt;/code&gt;: ISBN / Date incompatibility (help) (A collection of 370 different proofs of the Pythagorean theorem.)&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46085585</guid><pubDate>Sat, 29 Nov 2025 06:37:05 +0000</pubDate></item><item><title>The CRDT Dictionary: A Field Guide to Conflict-Free Replicated Data Types</title><link>https://www.iankduncan.com/engineering/2025-11-27-crdt-dictionary/</link><description>&lt;doc fingerprint="5f77706e84b8bed5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The CRDT Dictionary: A Field Guide to Conflict-Free Replicated Data Types&lt;/head&gt;
    &lt;p&gt;Back around 2014, I kept hearing about this cool database called Riak, a distributed database that could survive network partitions and keep accepting writes. Some really interesting companies were using it at massive scale, and I was curious about it. One of the big selling points was that it could handle concurrent writes without any coordination or consensus. I was intrigued, and I started reading about it. Underlying all of this was the concept of CRDTs, Conflict-free Replicated Data Types.1&lt;/p&gt;
    &lt;p&gt;At the time, I was working on a beer startup called Brewtown with a friend: a beer review social site and delivery subscription service. It failed for other reasons, but I was a little too enamored with shiny tech back then, and CRDTs and Riak fit the bill for shiny tech. I kept trying to find excuses to shoehorn CRDT stuff into our codebase when, honestly, we didn’t need any of it. Postgres would’ve been fine. Live and learn.&lt;/p&gt;
    &lt;p&gt;Anyways, the idea sounded like pure sorcery: data structures that replicate across nodes and merge deterministically, without coordination, without losing information. I got excited, read a few papers, played with some toy implementations… and then we gave up on the beer startup. I didn’t really have a reason to mess with CRDTs for a while.&lt;/p&gt;
    &lt;p&gt;Fast forward to 2025, I’ve just had Thanksgiving dinner, and I’m curious again. What’s the state of the art? What have I forgotten? Which CRDT should I reach for when? So I’m writing this, both as a refresher for myself and a reference for the next time I need to remember why OR-Sets exist or what WOOT stands for. (“WithOut Operational Transformation.” Yes, really.2)&lt;/p&gt;
    &lt;p&gt;So, grab a coffee.&lt;/p&gt;
    &lt;p&gt;Commutative. Replicated. Data Types.&lt;/p&gt;
    &lt;p&gt;In isolation, all of the words make sense. But when you look at the literature, it’s overwhelming:&lt;/p&gt;
    &lt;p&gt;Suddenly you’ve moved beyond the simple terms and start seeing things like G-Counters, PN-Counters, LWW-Sets, OR-Sets, 2P-Sets, RGAs, WOOTs, Logoots (wtf?)… Each with subtle tradeoffs. Each paper assuming you’ve read the previous five. It’s overwhelming.&lt;/p&gt;
    &lt;p&gt;This guide will hopefully cut through that. We’ll build intuition through interactive demos and concrete examples. You’ll see how merges actually work, watch conflicts resolve (or not resolve), and develop a feel for which CRDT fits which problem.&lt;/p&gt;
    &lt;head rend="h2"&gt;What You Need to Know&lt;/head&gt;
    &lt;p&gt;You don’t need a PhD in distributed systems. If you understand:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Why network failures happen&lt;/item&gt;
      &lt;item&gt;What “eventual consistency” means&lt;/item&gt;
      &lt;item&gt;Basic set operations (union, intersection)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;…you’re good.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Problem&lt;/head&gt;
    &lt;p&gt;Picture this: Alice and Bob are both editing a shared counter. Alice increments it. Bob increments it. The network is flaky, so neither sees the other’s change immediately. Later, they reconnect. What should the counter show?&lt;/p&gt;
    &lt;p&gt;Option 1: Consensus - Use Paxos/Raft to agree on who went first. Works great! Until the network partitions and half your users can’t write because they can’t reach a quorum. Not ideal for offline-first apps.&lt;/p&gt;
    &lt;p&gt;Option 2: Last-Write-Wins - Use timestamps. Whoever wrote last “wins.” Easy to implement! Except Bob’s increment gets completely erased if Alice’s timestamp was later. Data loss.&lt;/p&gt;
    &lt;p&gt;Option 3: CRDTs - Design the data structure so that merging is deterministic. Both increments survive. No coordination needed. No data loss. However, you have to be okay with some level of eventual consistency.&lt;/p&gt;
    &lt;p&gt;What’s the trick? How do CRDTs achieve this?&lt;/p&gt;
    &lt;p&gt;Roughly speaking, you are working with a CRDT if your merge operation is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;commutative (order doesn’t matter)&lt;/item&gt;
      &lt;item&gt;associative (grouping doesn’t matter)&lt;/item&gt;
      &lt;item&gt;idempotent (duplicates don’t matter)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once you achieve these properties, then you can use your merge operation to ensure that replicas automatically converge to the same state.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Quick Detour: Lattices and Why They Matter&lt;/head&gt;
    &lt;p&gt;Before we dive into specific CRDTs, let’s build some intuition about what makes merging work. In CRDT literature, this is often referred to as a “lattice”.&lt;/p&gt;
    &lt;p&gt;Think about natural numbers with &lt;code&gt;max&lt;/code&gt; as the merge operation. If you have &lt;code&gt;3&lt;/code&gt; and &lt;code&gt;5&lt;/code&gt;, taking &lt;code&gt;max(3, 5) = 5&lt;/code&gt; makes sense. It doesn’t matter if you compute &lt;code&gt;max(3, max(5, 7))&lt;/code&gt; or &lt;code&gt;max(max(3, 5), 7)&lt;/code&gt; - you get &lt;code&gt;7&lt;/code&gt; either way. And &lt;code&gt;max(5, 5) = 5&lt;/code&gt;, so duplicates are harmless.&lt;/p&gt;
    &lt;p&gt;This forms a partial order: some values are “greater than” others (&lt;code&gt;5 &amp;gt; 3&lt;/code&gt;), and there’s a join operation (&lt;code&gt;max&lt;/code&gt;) that gives you the least upper bound. The fancy math term is “join-semilattice,” but think of it as: a way to consistently pick “more recent” or “more complete” information.&lt;/p&gt;
    &lt;p&gt;Here’s the key insight: if your data structure’s states form a lattice, and updates only move “upward” in the ordering, then:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You can apply updates in any order&lt;/item&gt;
      &lt;item&gt;You can apply the same update twice&lt;/item&gt;
      &lt;item&gt;Eventually, everyone agrees on the maximum state&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Consider a counter where each replica tracks its own count: &lt;code&gt;{A: 3, B: 5}&lt;/code&gt;. The partial order is pointwise: &lt;code&gt;{A: 3, B: 5} ≥ {A: 2, B: 5}&lt;/code&gt; because each component is greater-or-equal. To join, take the &lt;code&gt;max&lt;/code&gt; of each component. This is exactly how the G-Counter CRDT works!&lt;/p&gt;
    &lt;p&gt;Why does this matter? Because if you can design your data structure so that:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;States form a lattice (there’s always a sensible “join”)&lt;/item&gt;
      &lt;item&gt;Operations only move upward (you can’t un-increment a counter)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Then merging becomes trivial: just take the join. No coordination needed. No conflicts possible. The math guarantees convergence.&lt;/p&gt;
    &lt;p&gt;Not all CRDTs fit this clean model (some need timestamps or version vectors to determine what’s “greater”), but the lattice intuition often guides the design. When you see &lt;code&gt;merge = unionWith max&lt;/code&gt; or &lt;code&gt;merge = union&lt;/code&gt;, you’re seeing some pure, beautiful math-brained lattice thinking.&lt;/p&gt;
    &lt;head rend="h3"&gt;State-Based vs Operation-Based&lt;/head&gt;
    &lt;p&gt;Moving on…&lt;/p&gt;
    &lt;p&gt;There are two fundamental approaches to CRDTs:&lt;/p&gt;
    &lt;p&gt;State-based CRDTs (CvRDTs) send the entire state to other replicas, which merge it with their local state using a join operation. The state must form a join-semilattice.3&lt;/p&gt;
    &lt;p&gt;Operation-based CRDTs (CmRDTs) send operations to other replicas, which apply them to their local state. Operations must be commutative when applied concurrently.4&lt;/p&gt;
    &lt;p&gt;In this guide, we’ll primarily discuss state-based CRDTs, as they’re conceptually simpler and the ideas translate naturally to the operation-based variants.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Core Laws&lt;/head&gt;
    &lt;p&gt;For a data structure to be a state-based CRDT, its merge operation must satisfy:&lt;/p&gt;
    &lt;p&gt;Associativity: &lt;code&gt;(a ⊔ b) ⊔ c = a ⊔ (b ⊔ c)&lt;/code&gt; where &lt;code&gt;⊔&lt;/code&gt; denotes the merge/join operation&lt;/p&gt;
    &lt;p&gt;Commutativity: &lt;code&gt;a ⊔ b = b ⊔ a&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Idempotence: &lt;code&gt;a ⊔ a = a&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;These properties ensure that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Merging in any order produces the same result&lt;/item&gt;
      &lt;item&gt;Re-receiving the same state is harmless&lt;/item&gt;
      &lt;item&gt;Partial merges can be composed&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Additionally, the state must form a monotonic semilattice: updates only move “upward” in the partial order, never downward. This ensures convergence: once all updates have been delivered, all replicas reach the same state.&lt;/p&gt;
    &lt;p&gt;For the curious, The symbol ⊔ is called (square cup) or square union. I have no idea why regular union symbol isn’t used. Pointy-headed researchers, I guess.&lt;/p&gt;
    &lt;p&gt;Anyways, it’s commonly used to denote:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Disjoint union - union of sets treated as disjoint&lt;/item&gt;
      &lt;item&gt;Join operation in lattice theory - the least upper bound (supremum) of two elements&lt;/item&gt;
      &lt;item&gt;Merge operation in CRDTs - combining two states by taking their least upper bound&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With these foundations in place, let’s explore the CRDT zoo.&lt;/p&gt;
    &lt;head rend="h2"&gt;G-Counter: Grow-Only Counter&lt;/head&gt;
    &lt;p&gt;Let’s start with the simplest CRDT: a counter that only goes up.5&lt;/p&gt;
    &lt;head rend="h3"&gt;The Idea&lt;/head&gt;
    &lt;p&gt;Instead of storing one global count, each replica tracks its own count. The total is the sum of all replica counts. When replicas merge, they take the &lt;code&gt;max&lt;/code&gt; of each replica’s count.&lt;/p&gt;
    &lt;p&gt;Why &lt;code&gt;max&lt;/code&gt;? Because counts only increase. If replica A shows that replica B has counted to 5, and replica B shows it’s counted to 3, we know A has seen newer information. Taking the max ensures we never lose increments.6&lt;/p&gt;
    &lt;head rend="h3"&gt;Implementation&lt;/head&gt;
    &lt;code&gt;type GCounter = Map ReplicaId Nat

value :: GCounter -&amp;gt; Nat
value counter = sum (Map.elems counter)

increment :: ReplicaId -&amp;gt; GCounter -&amp;gt; GCounter
increment r counter = Map.insertWith (+) r 1 counter

merge :: GCounter -&amp;gt; GCounter -&amp;gt; GCounter
merge = Map.unionWith max&lt;/code&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;The merge operation forms a join-semilattice where the partial order is defined pointwise: &lt;code&gt;c1 ≤ c2&lt;/code&gt; if for all replicas &lt;code&gt;r&lt;/code&gt;, &lt;code&gt;c1[r] ≤ c2[r]&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Associative: &lt;code&gt;max&lt;/code&gt;is associative&lt;/item&gt;
      &lt;item&gt;Commutative: &lt;code&gt;max&lt;/code&gt;is commutative&lt;/item&gt;
      &lt;item&gt;Idempotent: &lt;code&gt;max(x, x) = x&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Monotonic: Each replica’s count only increases&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Intuition&lt;/head&gt;
    &lt;p&gt;Think of each replica as having its own tally marks. When replicas sync, they each adopt the maximum tally for each replica they’ve seen. Since tallies only grow, taking the maximum ensures we never lose increments.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simple and efficient&lt;/item&gt;
      &lt;item&gt;No metadata overhead beyond replica counts&lt;/item&gt;
      &lt;item&gt;Perfect for increment-only scenarios (page views, likes, etc.)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cannot decrement&lt;/item&gt;
      &lt;item&gt;Size grows with number of replicas (though typically small)&lt;/item&gt;
      &lt;item&gt;No garbage collection (all replica counts retained forever)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use G-Counter when you need to count upward-only events in a distributed system: analytics counters, like counts, view counts, or any monotonically increasing metric. (If you need to decrement, well… keep reading.)&lt;/p&gt;
    &lt;head rend="h3"&gt;Interactive Demo&lt;/head&gt;
    &lt;p&gt;Try it yourself! Increment counters on different replicas and see how the merge operation works:&lt;/p&gt;
    &lt;head rend="h3"&gt;G-Counter: Grow-Only Counter&lt;/head&gt;
    &lt;p&gt;Try it: Click "Increment" on different replicas multiple times without merging. Each replica tracks its own count independently. Then click the merge buttons (← A, ← B, ← C) to sync state. Watch how the merge uses &lt;code&gt;max&lt;/code&gt; - you never lose increments, and all replicas eventually reach the same total.&lt;/p&gt;
    &lt;head rend="h2"&gt;PN-Counter: Positive-Negative Counter&lt;/head&gt;
    &lt;p&gt;What if we need to decrement? Enter the PN-Counter. The trick is beautifully simple.&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;A PN-Counter contains two G-Counters: one for increments, one for decrements:&lt;/p&gt;
    &lt;code&gt;data PNCounter = PNCounter
  { increments :: GCounter
  , decrements :: GCounter
  }&lt;/code&gt;
    &lt;p&gt;The value is the difference:&lt;/p&gt;
    &lt;code&gt;value :: PNCounter -&amp;gt; Int
value (PNCounter inc dec) = value inc - value dec&lt;/code&gt;
    &lt;p&gt;What I love about PN-Counters as a broader insight for CRDTs is that you can often build more complex CRDTs by combining simpler ones.&lt;/p&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Increment (on replica &lt;code&gt;r&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;increment r (PNCounter inc dec) = PNCounter (increment r inc) dec&lt;/code&gt;
    &lt;p&gt;Decrement (on replica &lt;code&gt;r&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;decrement r (PNCounter inc dec) = PNCounter inc (increment r dec)&lt;/code&gt;
    &lt;p&gt;Merge:&lt;/p&gt;
    &lt;code&gt;merge (PNCounter i1 d1) (PNCounter i2 d2) =
  PNCounter (merge i1 i2) (merge d1 d2)&lt;/code&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;Since both components are G-Counters with valid merge operations, the PN-Counter’s merge inherits their properties and forms a semilattice.&lt;/p&gt;
    &lt;head rend="h3"&gt;Intuition&lt;/head&gt;
    &lt;p&gt;A PN-Counter is like having two separate tally sheets: one for additions, one for subtractions. The current value is the difference between them. When replicas sync, they merge both sheets independently.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Supports both increment and decrement&lt;/item&gt;
      &lt;item&gt;Deterministic convergence&lt;/item&gt;
      &lt;item&gt;Simple to understand and implement&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Double the space of a G-Counter&lt;/item&gt;
      &lt;item&gt;Can never truly garbage collect old replica entries&lt;/item&gt;
      &lt;item&gt;No bound on the value range (can overflow)&lt;/item&gt;
      &lt;item&gt;Cannot reset the counter atomically&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use PN-Counter for any metric that can increase or decrease over time: inventory counts, resource pools, etc.&lt;/p&gt;
    &lt;head rend="h3"&gt;Variants&lt;/head&gt;
    &lt;p&gt;Some implementations use a single map with integer values instead of two separate maps, but the principle is the same.&lt;/p&gt;
    &lt;head rend="h2"&gt;G-Set: Grow-Only Set&lt;/head&gt;
    &lt;p&gt;Moving from numbers to collections, we consider the simplest CRDT set.&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;A G-Set is simply a set that supports addition but not removal:&lt;/p&gt;
    &lt;code&gt;type GSet a = Set a&lt;/code&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Add:&lt;/p&gt;
    &lt;code&gt;add :: Ord a =&amp;gt; a -&amp;gt; GSet a -&amp;gt; GSet a
add = insert&lt;/code&gt;
    &lt;p&gt;Merge:&lt;/p&gt;
    &lt;code&gt;merge :: Ord a =&amp;gt; GSet a -&amp;gt; GSet a -&amp;gt; GSet a
merge = union&lt;/code&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;Sets with union form a semilattice under the subset relation.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Associative: Set union is associative&lt;/item&gt;
      &lt;item&gt;Commutative: Set union is commutative&lt;/item&gt;
      &lt;item&gt;Idempotent: &lt;code&gt;A ∪ A = A&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Monotonic: Sets only grow&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Intuition&lt;/head&gt;
    &lt;p&gt;Once an element is added to any replica, it will eventually appear in all replicas. There’s no way to remove it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Minimal overhead (just the set elements)&lt;/item&gt;
      &lt;item&gt;Simple and efficient&lt;/item&gt;
      &lt;item&gt;Familiar set semantics&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cannot remove elements&lt;/item&gt;
      &lt;item&gt;Grows unbounded&lt;/item&gt;
      &lt;item&gt;No garbage collection&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use G-Set for append-only collections where removal is never needed: event logs, collected tags, or immutable registries.&lt;/p&gt;
    &lt;head rend="h2"&gt;2P-Set: Two-Phase Set&lt;/head&gt;
    &lt;p&gt;The natural extension of G-Set to support removal.&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;A 2P-Set (Two-Phase Set) contains two G-Sets: one for added elements, one for removed elements:&lt;/p&gt;
    &lt;code&gt;data TwoPhaseSet a = TwoPhaseSet
  { added :: GSet a
  , removed :: GSet a
  }&lt;/code&gt;
    &lt;p&gt;An element is in the set if it’s been added but not removed:&lt;/p&gt;
    &lt;code&gt;member :: Ord a =&amp;gt; a -&amp;gt; TwoPhaseSet a -&amp;gt; Bool
member x (TwoPhaseSet a r) = x `Set.member` a &amp;amp;&amp;amp; x `Set.notMember` r&lt;/code&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Add:&lt;/p&gt;
    &lt;code&gt;add x (TwoPhaseSet a r) = TwoPhaseSet (insert x a) r&lt;/code&gt;
    &lt;p&gt;Remove:&lt;/p&gt;
    &lt;code&gt;remove x (TwoPhaseSet a r) = TwoPhaseSet a (insert x r)&lt;/code&gt;
    &lt;p&gt;Merge:&lt;/p&gt;
    &lt;code&gt;merge (TwoPhaseSet a1 r1) (TwoPhaseSet a2 r2) =
  TwoPhaseSet (union a1 a2) (union r1 r2)&lt;/code&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;Bias toward removal: If an element appears in the removed set, it’s not in the 2P-Set, even if it’s also in the added set.&lt;/p&gt;
    &lt;p&gt;Once removed, forever removed: Once an element is removed at any replica, it will eventually be removed from all replicas and cannot be re-added.&lt;/p&gt;
    &lt;head rend="h3"&gt;Intuition&lt;/head&gt;
    &lt;p&gt;The 2P-Set is like marking items in a ledger: you can add entries and you can cross them out, but you can’t un-cross-out an entry. Once something is crossed out (removed), that decision is permanent.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Supports both add and remove&lt;/item&gt;
      &lt;item&gt;Simple to understand&lt;/item&gt;
      &lt;item&gt;Deterministic convergence&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cannot re-add removed elements (the “2P” means two-phase: add, then remove, no going back)&lt;/item&gt;
      &lt;item&gt;Both sets grow monotonically (removed items never truly disappear)&lt;/item&gt;
      &lt;item&gt;No garbage collection&lt;/item&gt;
      &lt;item&gt;Not suitable for scenarios where elements might be removed and re-added&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use 2P-Set when elements have a lifecycle of “not present → added → removed” and never need to be re-added: task completion tracking, tombstones, or revoked permissions.&lt;/p&gt;
    &lt;head rend="h2"&gt;LWW-Element-Set: Last-Write-Wins Element Set&lt;/head&gt;
    &lt;p&gt;What if we want to re-add elements? We need timestamps.&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;An LWW-Element-Set associates each element with a timestamp for additions and removals:&lt;/p&gt;
    &lt;code&gt;data LWWSet a = LWWSet
  { addTimes :: Map a Timestamp
  , removeTimes :: Map a Timestamp
  }&lt;/code&gt;
    &lt;p&gt;An element is in the set if its most recent operation was an add:&lt;/p&gt;
    &lt;code&gt;member :: Ord a =&amp;gt; a -&amp;gt; LWWSet a -&amp;gt; Bool
member x (LWWSet adds removes) =
  case (Map.lookup x adds, Map.lookup x removes) of
    (Just t1, Just t2) -&amp;gt; t1 &amp;gt; t2
    (Just _, Nothing) -&amp;gt; True
    _ -&amp;gt; False&lt;/code&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Add (with timestamp &lt;code&gt;t&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;add x t (LWWSet adds removes) = LWWSet (insert x t adds) removes&lt;/code&gt;
    &lt;p&gt;Remove (with timestamp &lt;code&gt;t&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;remove x t (LWWSet adds removes) = LWWSet adds (insert x t removes)&lt;/code&gt;
    &lt;p&gt;Merge:&lt;/p&gt;
    &lt;code&gt;merge (LWWSet a1 r1) (LWWSet a2 r2) =
  LWWSet (unionWith max a1 a2) (unionWith max r1 r2)&lt;/code&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;The merge operation is well-defined because &lt;code&gt;max&lt;/code&gt; over timestamps forms a semilattice.&lt;/p&gt;
    &lt;p&gt;Timestamp monotonicity: Each replica must generate increasing timestamps (typically using wall clocks plus replica IDs as tiebreakers).&lt;/p&gt;
    &lt;p&gt;Bias: We must decide what happens when add and remove timestamps are equal. Common choices: bias toward add, or bias toward remove.&lt;/p&gt;
    &lt;head rend="h3"&gt;Intuition&lt;/head&gt;
    &lt;p&gt;Each element has a timestamp for when it was last added and when it was last removed. The most recent operation wins. When merging, we take the latest add timestamp and latest remove timestamp we’ve seen.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Supports add, remove, and re-add&lt;/item&gt;
      &lt;item&gt;Can garbage collect old timestamps (carefully)&lt;/item&gt;
      &lt;item&gt;Natural semantics for many applications&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Requires synchronized clocks (or logical clocks with careful replica ID handling)&lt;/item&gt;
      &lt;item&gt;Concurrent add/remove on the same element may surprise users (one operation is discarded)&lt;/item&gt;
      &lt;item&gt;Loses information: if two users concurrently add the same element, only one timestamp survives&lt;/item&gt;
      &lt;item&gt;The “last write wins” semantics mean data loss is possible&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use LWW-Element-Set when you need a set with add/remove/re-add capability and can tolerate last-write-wins semantics: user preferences, feature flags, or cached collections where perfect consistency isn’t critical.&lt;/p&gt;
    &lt;head rend="h3"&gt;Clock Considerations&lt;/head&gt;
    &lt;p&gt;The biggest pitfall of LWW-Element-Set is clock skew. If replica A’s clock is ahead of replica B’s, then A’s operations will always “win” over B’s, even if B’s operations happened later in real time. Solutions include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use hybrid logical clocks (HLC) instead of wall clocks&lt;/item&gt;
      &lt;item&gt;Use replica IDs as tiebreakers (e.g., timestamps are &lt;code&gt;(wall_time, replica_id)&lt;/code&gt;pairs)&lt;/item&gt;
      &lt;item&gt;Accept the inconsistency as a tradeoff&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;OR-Set: Observed-Remove Set&lt;/head&gt;
    &lt;p&gt;The most sophisticated set CRDT, solving the re-add problem without LWW semantics.7&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;An OR-Set (Observed-Remove Set) associates each element with a set of unique tags:&lt;/p&gt;
    &lt;code&gt;type ORSet a = Map a (Set Tag)&lt;/code&gt;
    &lt;p&gt;Tags are unique identifiers generated when adding an element (e.g., &lt;code&gt;(replica_id, sequence_number)&lt;/code&gt; pairs).&lt;/p&gt;
    &lt;p&gt;An element is in the set if it has any tags:&lt;/p&gt;
    &lt;code&gt;member :: Ord a =&amp;gt; a -&amp;gt; ORSet a -&amp;gt; Bool
member x set = case Map.lookup x set of
  Just tags -&amp;gt; not (Set.null tags)
  Nothing -&amp;gt; False&lt;/code&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Add (with fresh tag &lt;code&gt;t&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;add x t set = Map.insertWith union x (singleton t) set&lt;/code&gt;
    &lt;p&gt;Remove (with observed tags &lt;code&gt;ts&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;remove x ts set = Map.update (\\tags -&amp;gt;
  let remaining = tags \\ ts
  in if Set.null remaining then Nothing else Just remaining) x set&lt;/code&gt;
    &lt;p&gt;The critical insight: removal removes only the tags that were observed. If concurrent adds create new tags, those survive.&lt;/p&gt;
    &lt;p&gt;Merge:&lt;/p&gt;
    &lt;code&gt;merge = Map.unionWith union&lt;/code&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;The merge operation forms a semilattice where &lt;code&gt;s1 ≤ s2&lt;/code&gt; if for all elements &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;s1[x] ⊆ s2[x]&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Add wins: If an add and remove happen concurrently (the add’s tag wasn’t observed by the remove), the add wins.&lt;/p&gt;
    &lt;p&gt;Causal consistency: You can only remove tags you’ve observed (seen in a prior state).&lt;/p&gt;
    &lt;head rend="h3"&gt;Intuition&lt;/head&gt;
    &lt;p&gt;Think of each addition as dropping a unique token into a bucket for that element. Removal takes specific tokens out of the bucket. If someone concurrently added a new token you haven’t seen, your removal doesn’t affect it. An element is present if its bucket has any tokens.&lt;/p&gt;
    &lt;p&gt;This gives us add-wins semantics: concurrent add and remove means the element stays in the set (because the remove didn’t observe the add’s tag).&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Supports add, remove, and re-add with intuitive semantics&lt;/item&gt;
      &lt;item&gt;No timestamp requirements&lt;/item&gt;
      &lt;item&gt;Add-wins semantics are often more desirable than LWW&lt;/item&gt;
      &lt;item&gt;Properly handles concurrent operations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Larger space overhead (tags per element)&lt;/item&gt;
      &lt;item&gt;More complex implementation&lt;/item&gt;
      &lt;item&gt;Need garbage collection strategy for tags&lt;/item&gt;
      &lt;item&gt;Remove operations need to carry the observed tags (larger messages)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use OR-Set when you need a set with full add/remove/re-add support and can’t tolerate LWW’s data loss: collaborative editing, shopping carts, or any scenario where concurrent adds should be preserved.&lt;/p&gt;
    &lt;head rend="h3"&gt;Garbage Collection&lt;/head&gt;
    &lt;p&gt;Old tags can accumulate. Strategies include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tombstones: Keep removed tags for a grace period before discarding&lt;/item&gt;
      &lt;item&gt;Version vectors: Use causal history to determine which tags are safe to remove&lt;/item&gt;
      &lt;item&gt;Bounded tags: Limit the number of tags per element, using LWW within that bound&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Interactive Demo&lt;/head&gt;
    &lt;p&gt;Experience the add-wins semantics of OR-Set:&lt;/p&gt;
    &lt;head rend="h3"&gt;OR-Set: Observed-Remove Set&lt;/head&gt;
    &lt;p&gt;Try it: Add the same element (e.g., "apple") on both replicas without merging first. Each creates a unique tag. Then remove it from one replica and immediately merge. Notice the element stays because the remove only deleted the tags it could see - the other replica's tag survives. This is add-wins semantics.&lt;/p&gt;
    &lt;head rend="h2"&gt;LWW-Register: Last-Write-Wins Register&lt;/head&gt;
    &lt;p&gt;Registers store single values. The simplest register CRDT uses last-write-wins.&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;An LWW-Register pairs a value with a timestamp:&lt;/p&gt;
    &lt;code&gt;data LWWRegister a = LWWRegister
  { value :: a
  , timestamp :: Timestamp
  }&lt;/code&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Write (with timestamp &lt;code&gt;t&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;write x t _ = LWWRegister x t&lt;/code&gt;
    &lt;p&gt;Merge:&lt;/p&gt;
    &lt;code&gt;merge r1@(LWWRegister v1 t1) r2@(LWWRegister v2 t2)
  | t1 &amp;gt; t2 = r1
  | t1 &amp;lt; t2 = r2
  | otherwise = r1  -- tiebreaker (could use replica ID)&lt;/code&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;The merge operation is a semilattice with partial order defined by timestamps.&lt;/p&gt;
    &lt;p&gt;One value wins: When concurrent writes occur, only one survives (the one with the higher timestamp).&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simple and efficient&lt;/item&gt;
      &lt;item&gt;Small size (just value + timestamp)&lt;/item&gt;
      &lt;item&gt;Easy to understand&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Loses concurrent updates&lt;/item&gt;
      &lt;item&gt;Requires clock synchronization&lt;/item&gt;
      &lt;item&gt;No way to detect or recover lost updates&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use LWW-Register for single-value cells where you can tolerate lost updates: user profile fields, configuration settings, or cached computed values.&lt;/p&gt;
    &lt;head rend="h3"&gt;Interactive Demo&lt;/head&gt;
    &lt;p&gt;See data loss in action with last-write-wins semantics:&lt;/p&gt;
    &lt;head rend="h3"&gt;LWW-Register: Last-Write-Wins Register&lt;/head&gt;
    &lt;p&gt;⚠️ Data Loss Alert: Write different values on replica A and B without merging. Each gets its own timestamp. Then merge A into B (or vice versa). Watch one value completely disappear! The higher timestamp wins. This is why LWW is dangerous for important data - concurrent writes = lost data.&lt;/p&gt;
    &lt;head rend="h2"&gt;MV-Register: Multi-Value Register&lt;/head&gt;
    &lt;p&gt;What if we want to preserve concurrent writes instead of discarding them?&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;An MV-Register stores a set of value-timestamp pairs:&lt;/p&gt;
    &lt;code&gt;type MVRegister a = Set (a, Timestamp)&lt;/code&gt;
    &lt;p&gt;When reading, you get back all concurrently written values (values with incomparable timestamps).&lt;/p&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Write (with timestamp &lt;code&gt;t&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;write x t reg = Set.singleton (x, t)&lt;/code&gt;
    &lt;p&gt;Merge:&lt;/p&gt;
    &lt;code&gt;merge reg1 reg2 =
  let combined = union reg1 reg2
      maxTime = maximum (map snd combined)
      concurrent = filter (\\(_, t) -&amp;gt; t == maxTime) combined
  in fromList concurrent&lt;/code&gt;
    &lt;p&gt;More sophisticated: keep values with causally concurrent timestamps, not just the maximum.&lt;/p&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;The merge preserves all values that might be “current” from different replicas’ perspectives.&lt;/p&gt;
    &lt;p&gt;Concurrent values preserved: If two writes happened concurrently, both values appear until a subsequent write supersedes them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No data loss on concurrent updates&lt;/item&gt;
      &lt;item&gt;Application can detect and resolve conflicts&lt;/item&gt;
      &lt;item&gt;More information available for conflict resolution&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Returns sets of values, not single values&lt;/item&gt;
      &lt;item&gt;Application must handle conflict resolution&lt;/item&gt;
      &lt;item&gt;More complex semantics&lt;/item&gt;
      &lt;item&gt;Slightly larger space overhead&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use MV-Register when concurrent updates must be detected and resolved by application logic: collaborative text fields, conflict-aware configuration, or any scenario where losing an update is unacceptable.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conflict Resolution&lt;/head&gt;
    &lt;p&gt;When reading an MV-Register returns multiple values, the application must resolve the conflict. Strategies include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Present all values to the user (collaborative editing)&lt;/item&gt;
      &lt;item&gt;Apply a deterministic merge function (e.g., union of tags)&lt;/item&gt;
      &lt;item&gt;Use application-specific semantics (e.g., prefer non-empty values)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;OR-Map: Observed-Remove Map&lt;/head&gt;
    &lt;p&gt;Maps are common. How do we make them CRDTs?&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;An OR-Map is a map where each key is associated with an OR-Set of tagged values:&lt;/p&gt;
    &lt;code&gt;type ORMap k v = Map k (ORSet (v, Tag))&lt;/code&gt;
    &lt;p&gt;Alternatively, implement as a composition of OR-Set (for keys) with per-key CRDTs (for values).&lt;/p&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Put (with fresh tag &lt;code&gt;t&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;put k v t map = Map.insertWith union k (singleton (v, t)) map&lt;/code&gt;
    &lt;p&gt;Remove key:&lt;/p&gt;
    &lt;code&gt;removeKey k map = Map.delete k map&lt;/code&gt;
    &lt;p&gt;Remove specific value:&lt;/p&gt;
    &lt;code&gt;removeValue k v tags map = -- similar to OR-Set remove&lt;/code&gt;
    &lt;p&gt;Merge:&lt;/p&gt;
    &lt;code&gt;merge = Map.unionWith (OR-Set merge)&lt;/code&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Full map operations with CRDT semantics&lt;/item&gt;
      &lt;item&gt;Can nest other CRDTs as values&lt;/item&gt;
      &lt;item&gt;Compositional&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Complex metadata management&lt;/item&gt;
      &lt;item&gt;Garbage collection challenges&lt;/item&gt;
      &lt;item&gt;Larger overhead&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use OR-Map when you need a distributed key-value store with CRDT guarantees: collaborative JSON documents, distributed configuration, or nested data structures.&lt;/p&gt;
    &lt;head rend="h2"&gt;RGA: Replicated Growable Array&lt;/head&gt;
    &lt;p&gt;Sequences are hard. How do you handle insertions in the middle when replicas disagree on positions?8&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;RGA (Replicated Growable Array) assigns each element a unique ID and stores the sequence as a tree structure based on insertion order and causality.9&lt;/p&gt;
    &lt;code&gt;data RGA a = RGA
  { elements :: Map UID (a, UID)  -- element ID -&amp;gt; (value, parent ID)
  , root :: UID
  }&lt;/code&gt;
    &lt;p&gt;Each element knows its “parent” (the element after which it was inserted).&lt;/p&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Insert (after element with ID &lt;code&gt;p&lt;/code&gt;, with fresh ID &lt;code&gt;uid&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;insert p x uid rga = -- complex tree manipulation&lt;/code&gt;
    &lt;p&gt;Delete (element with ID &lt;code&gt;uid&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;delete uid rga = -- mark as tombstone, don't actually remove&lt;/code&gt;
    &lt;p&gt;Merge: Merge trees by reconciling insertion orders.&lt;/p&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;The challenge is that positional indices change as elements are inserted/removed. RGA solves this by using immutable IDs and causal relationships.&lt;/p&gt;
    &lt;p&gt;Causal order preserved: If element A was inserted before element B on the same replica, that relationship is preserved globally.&lt;/p&gt;
    &lt;head rend="h3"&gt;Intuition&lt;/head&gt;
    &lt;p&gt;Instead of “insert at position 5,” you say “insert after element X.” Since X has a unique ID, this instruction is unambiguous even when other replicas are concurrently inserting elsewhere.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Supports arbitrary insertions and deletions&lt;/item&gt;
      &lt;item&gt;Eventual consistency for sequences&lt;/item&gt;
      &lt;item&gt;Handles concurrent edits intuitively&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Complex implementation&lt;/item&gt;
      &lt;item&gt;Large overhead (IDs, tombstones)&lt;/item&gt;
      &lt;item&gt;No compaction without coordination&lt;/item&gt;
      &lt;item&gt;Performance degrades with many deletes (tombstones accumulate)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use RGA for collaborative text editing or any replicated sequence where insertions at arbitrary positions must be supported: shared lists, collaborative documents, or distributed queues.&lt;/p&gt;
    &lt;head rend="h3"&gt;Alternatives&lt;/head&gt;
    &lt;p&gt;Other sequence CRDTs include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;WOOT (Without Operational Transformation): similar idea, different structure&lt;/item&gt;
      &lt;item&gt;Logoot: uses position identifiers between elements&lt;/item&gt;
      &lt;item&gt;LSEQ: adaptive allocation of position identifiers&lt;/item&gt;
      &lt;item&gt;YATA: optimizations for text editing workloads10&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each has different tradeoffs in space overhead, time complexity, and behavior under specific edit patterns.&lt;/p&gt;
    &lt;head rend="h2"&gt;Causal CRDTs: Adding Causality&lt;/head&gt;
    &lt;p&gt;Advanced CRDTs incorporate causal tracking using version vectors or similar mechanisms. This enables more sophisticated semantics.&lt;/p&gt;
    &lt;head rend="h3"&gt;Version Vectors&lt;/head&gt;
    &lt;p&gt;A version vector tracks the logical clock for each replica:11&lt;/p&gt;
    &lt;code&gt;type VersionVector = Map ReplicaId Nat&lt;/code&gt;
    &lt;p&gt;Operations include the version vector, allowing replicas to determine causality: whether one operation happened-before another, or whether they were concurrent.&lt;/p&gt;
    &lt;head rend="h3"&gt;Causal Register&lt;/head&gt;
    &lt;p&gt;Pairs an MV-Register with version vectors:&lt;/p&gt;
    &lt;code&gt;data CausalRegister a = CausalRegister
  { values :: Map VersionVector a
  }&lt;/code&gt;
    &lt;p&gt;Only keeps values with concurrent version vectors, discarding those that are causally dominated.&lt;/p&gt;
    &lt;head rend="h3"&gt;Advantages of Causality&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;More precise conflict detection (concurrent vs. causally ordered)&lt;/item&gt;
      &lt;item&gt;Better garbage collection (can discard superseded operations)&lt;/item&gt;
      &lt;item&gt;Foundation for stronger consistency guarantees&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Disadvantages of Causality&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Larger metadata (version vectors grow with number of replicas)&lt;/item&gt;
      &lt;item&gt;More complex logic&lt;/item&gt;
      &lt;item&gt;Still doesn’t eliminate conflicts, just detects them more precisely&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Interactive Demo&lt;/head&gt;
    &lt;p&gt;Explore how version vectors track causality:&lt;/p&gt;
    &lt;head rend="h3"&gt;Vector Clocks: Tracking Causality&lt;/head&gt;
    &lt;p&gt;Try it: Click "Perform Operation" on replica A, then on replica B (without clicking "Receive from"). These operations don't know about each other - they're concurrent! Now click two operations in the history to compare their vector clocks. Concurrent operations show as yellow warnings - they need special conflict resolution.&lt;/p&gt;
    &lt;head rend="h2"&gt;Delta CRDTs: Efficient State Transmission&lt;/head&gt;
    &lt;p&gt;State-based CRDTs have a problem: sending the entire state on every sync is wasteful. Delta CRDTs solve this.12&lt;/p&gt;
    &lt;head rend="h3"&gt;The Problem&lt;/head&gt;
    &lt;p&gt;Consider a G-Counter with 1000 replicas. If replica A increments its count, must it send all 1000 entries to replica B? That’s inefficient: only one entry changed!&lt;/p&gt;
    &lt;head rend="h3"&gt;The Solution&lt;/head&gt;
    &lt;p&gt;Instead of sending full state, send only the delta: the part of the state that changed since the last sync.&lt;/p&gt;
    &lt;code&gt;type Delta a = a  -- same type as state, but represents only changes

merge :: CRDT a =&amp;gt; a -&amp;gt; Delta a -&amp;gt; a&lt;/code&gt;
    &lt;p&gt;For G-Counter, a delta might be just &lt;code&gt;{A: 1}&lt;/code&gt; instead of the full map.&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;A Delta CRDT extends a state-based CRDT with delta operations:&lt;/p&gt;
    &lt;code&gt;data DeltaCRDT a = DeltaCRDT
  { state :: a
  , lastSent :: Map ReplicaId a  -- track what we've sent to each replica
  }

delta :: ReplicaId -&amp;gt; DeltaCRDT a -&amp;gt; Delta a
delta replica crdt = state crdt `since` lastSent[replica]&lt;/code&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;Delta CRDTs must satisfy the same semilattice properties as regular state-based CRDTs, plus:&lt;/p&gt;
    &lt;p&gt;Delta-state equivalence: Merging deltas incrementally must be equivalent to merging full states.&lt;/p&gt;
    &lt;p&gt;Delta composition: Deltas can be composed: &lt;code&gt;delta1 ⊔ delta2&lt;/code&gt; is itself a valid delta.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dramatically reduced bandwidth (send only changes)&lt;/item&gt;
      &lt;item&gt;Same convergence guarantees as state-based CRDTs&lt;/item&gt;
      &lt;item&gt;Can batch multiple deltas together&lt;/item&gt;
      &lt;item&gt;Easier to implement than operation-based CRDTs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Must track what has been sent to each replica&lt;/item&gt;
      &lt;item&gt;Slightly more complex than pure state-based&lt;/item&gt;
      &lt;item&gt;Still need full state for new replicas joining&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use Delta CRDTs when network bandwidth is a concern or state size is large. Most production CRDT systems use delta-state internally (Riak, Automerge). If you’re implementing your own CRDT system from scratch, start with deltas. Your future self will thank you.&lt;/p&gt;
    &lt;head rend="h3"&gt;Example: Delta G-Counter&lt;/head&gt;
    &lt;code&gt;increment :: ReplicaId -&amp;gt; GCounter -&amp;gt; (GCounter, Delta GCounter)
increment r counter =
  let newCounter = insertWith (+) r 1 counter
      delta = singleton r 1  -- only the change!
  in (newCounter, delta)&lt;/code&gt;
    &lt;p&gt;The delta is just the single updated entry, not the entire counter.&lt;/p&gt;
    &lt;head rend="h2"&gt;WOOT: Without Operational Transformation&lt;/head&gt;
    &lt;p&gt;WOOT is a sequence CRDT that predates RGA, with different design choices.2&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;WOOT represents a sequence as a set of character objects with unique IDs, where each character stores references to its previous and next characters:&lt;/p&gt;
    &lt;code&gt;data WChar a = WChar
  { charId :: UID
  , value :: a
  , prevId :: UID
  , nextId :: UID
  , isVisible :: Bool
  }

type WOOT a = Set (WChar a)&lt;/code&gt;
    &lt;head rend="h3"&gt;Key Insight&lt;/head&gt;
    &lt;p&gt;Instead of storing a linear sequence, WOOT stores constraints: “this character comes after X and before Y.” When multiple characters claim to be between X and Y, a deterministic ordering (based on UID) resolves the conflict.&lt;/p&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Insert (after character with ID &lt;code&gt;prev&lt;/code&gt;, before character with ID &lt;code&gt;next&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;insert :: a -&amp;gt; UID -&amp;gt; UID -&amp;gt; UID -&amp;gt; WOOT a -&amp;gt; WOOT a
insert val uid prev next woot =
  insert (WChar uid val prev next True) woot&lt;/code&gt;
    &lt;p&gt;Delete (character with ID &lt;code&gt;uid&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;delete :: UID -&amp;gt; WOOT a -&amp;gt; WOOT a
delete uid woot = -- mark character as invisible, don't remove&lt;/code&gt;
    &lt;head rend="h3"&gt;Linearization&lt;/head&gt;
    &lt;p&gt;To read the sequence, perform a topological sort respecting the prev/next constraints, filtering out invisible characters.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Strong eventual consistency&lt;/item&gt;
      &lt;item&gt;No need for causal delivery (constraints handle ordering)&lt;/item&gt;
      &lt;item&gt;Intuitive model (characters reference neighbors)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tombstones accumulate (deleted characters remain)&lt;/item&gt;
      &lt;item&gt;Linearization has O(n²) worst case&lt;/item&gt;
      &lt;item&gt;More complex than RGA&lt;/item&gt;
      &lt;item&gt;UIDs must be globally unique and ordered&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Comparison with RGA&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;RGA: Uses a tree structure, parent-child relationships&lt;/item&gt;
      &lt;item&gt;WOOT: Uses bidirectional constraints, more flexible but slower linearization&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;WOOT is primarily of historical interest. Modern implementations prefer RGA or YATA for better performance. But it’s a neat design, and the name alone makes it worth knowing about.&lt;/p&gt;
    &lt;head rend="h2"&gt;Logoot: Scalable Position Identifiers&lt;/head&gt;
    &lt;p&gt;Logoot takes a different approach to sequences: instead of linking elements, assign each element a position in a dense order.13&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;Each element has a position identifier that is a sequence of (digit, replicaId) pairs:&lt;/p&gt;
    &lt;code&gt;type Position = [(Int, ReplicaId)]

data LogootElement a = LogootElement
  { position :: Position
  , value :: a
  , isDeleted :: Bool
  }

type Logoot a = Set (LogootElement a)&lt;/code&gt;
    &lt;p&gt;Positions are ordered lexicographically.&lt;/p&gt;
    &lt;head rend="h3"&gt;Key Insight&lt;/head&gt;
    &lt;p&gt;Positions form a dense order: between any two positions, you can always allocate a new position. To insert between positions &lt;code&gt;p1&lt;/code&gt; and &lt;code&gt;p2&lt;/code&gt;, generate a new position &lt;code&gt;p&lt;/code&gt; such that &lt;code&gt;p1 &amp;lt; p &amp;lt; p2&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Insert (between positions &lt;code&gt;before&lt;/code&gt; and &lt;code&gt;after&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;insert :: a -&amp;gt; Position -&amp;gt; Position -&amp;gt; Logoot a -&amp;gt; Logoot a
insert val before after logoot =
  let newPos = allocatePosition before after currentReplicaId
      element = LogootElement newPos val False
  in insert element logoot&lt;/code&gt;
    &lt;p&gt;Position Allocation:&lt;/p&gt;
    &lt;code&gt;allocatePosition :: Position -&amp;gt; Position -&amp;gt; ReplicaId -&amp;gt; Position
allocatePosition before after replicaId =
  -- Find a position between before and after
  -- Use replicaId as tiebreaker for deterministic ordering&lt;/code&gt;
    &lt;p&gt;Delete:&lt;/p&gt;
    &lt;code&gt;delete :: Position -&amp;gt; Logoot a -&amp;gt; Logoot a
delete pos logoot = -- mark element at pos as deleted&lt;/code&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;Deterministic ordering: Elements are always ordered by their positions.&lt;/p&gt;
    &lt;p&gt;Unique positions: Each insert generates a unique position (using replica ID in the position).&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No need to reference other elements by ID&lt;/item&gt;
      &lt;item&gt;Simpler merge than WOOT&lt;/item&gt;
      &lt;item&gt;Positions are self-describing (no need to look up IDs)&lt;/item&gt;
      &lt;item&gt;Can insert without knowing the full document structure&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Position identifiers grow over time (especially with many edits)&lt;/item&gt;
      &lt;item&gt;Still accumulates tombstones&lt;/item&gt;
      &lt;item&gt;Position allocation algorithm is complex&lt;/item&gt;
      &lt;item&gt;Pathological cases where positions become very long&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;LSEQ: Adaptive Positions&lt;/head&gt;
    &lt;p&gt;LSEQ improves on Logoot by using an adaptive allocation strategy. Instead of always allocating positions the same way, LSEQ alternates between strategies to keep positions shorter on average.14&lt;/p&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use Logoot/LSEQ when you need a sequence CRDT and want simpler semantics than RGA/WOOT. The tradeoff is position identifier growth.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tree CRDTs: Hierarchical Data&lt;/head&gt;
    &lt;p&gt;Extending CRDTs to trees is challenging because parent-child relationships must be maintained consistently.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Problem&lt;/head&gt;
    &lt;p&gt;Trees have structural constraints:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Each node has exactly one parent (except root)&lt;/item&gt;
      &lt;item&gt;No cycles allowed&lt;/item&gt;
      &lt;item&gt;Moving a node changes parent-child relationships&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How do we handle concurrent operations like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Two replicas move the same node to different parents?&lt;/item&gt;
      &lt;item&gt;One replica moves node A under node B while another moves B under A?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Approaches&lt;/head&gt;
    &lt;p&gt;OR-Tree: Combine OR-Set with parent pointers, using conflict resolution strategies when multiple parents are observed.&lt;/p&gt;
    &lt;p&gt;CRDT-Tree: Use causal ordering to determine which move operations take precedence.&lt;/p&gt;
    &lt;p&gt;Log-based Trees: Store operations in a replicated log and rebuild tree structure on read.&lt;/p&gt;
    &lt;head rend="h3"&gt;OR-Tree Definition&lt;/head&gt;
    &lt;code&gt;type ORTree a = Map NodeId (ORSet ParentId, a)&lt;/code&gt;
    &lt;p&gt;Each node stores an OR-Set of potential parents. Conflict resolution:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Last-write-wins: Use timestamps to pick winning parent&lt;/item&gt;
      &lt;item&gt;First-wins: The first parent observed wins&lt;/item&gt;
      &lt;item&gt;Merge: Allow nodes to have multiple parents temporarily, application resolves&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Can represent hierarchical data distributedly&lt;/item&gt;
      &lt;item&gt;Handles concurrent structural changes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Complex conflict resolution strategies&lt;/item&gt;
      &lt;item&gt;Must prevent cycles (may require rejecting some operations)&lt;/item&gt;
      &lt;item&gt;Moving subtrees is complicated&lt;/item&gt;
      &lt;item&gt;High metadata overhead&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use Tree CRDTs for file systems, organizational charts, or document outlines where the hierarchy must be replicated. Be prepared for complexity in handling concurrent structural changes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Alternatives&lt;/head&gt;
    &lt;p&gt;For many use cases, an OR-Map with explicit parent fields is simpler than a full Tree CRDT, even if it doesn’t enforce tree constraints at the CRDT level.&lt;/p&gt;
    &lt;head rend="h2"&gt;Observed-Remove Shopping Cart&lt;/head&gt;
    &lt;p&gt;A practical example combining multiple CRDT concepts.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Domain&lt;/head&gt;
    &lt;p&gt;An e-commerce shopping cart must support:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Add product to cart&lt;/item&gt;
      &lt;item&gt;Remove product from cart&lt;/item&gt;
      &lt;item&gt;Change quantity&lt;/item&gt;
      &lt;item&gt;Work offline and sync later&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Naive Approach: LWW Map&lt;/head&gt;
    &lt;code&gt;type CartLWW = Map ProductId (Int, Timestamp)&lt;/code&gt;
    &lt;p&gt;Problems:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Concurrent additions of the same product (one wins)&lt;/item&gt;
      &lt;item&gt;Remove on one device, add on another (one wins, data loss)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Better: OR-Set + PN-Counter&lt;/head&gt;
    &lt;code&gt;type ShoppingCart = Map ProductId PNCounter&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use OR-Set semantics for which products are in cart&lt;/item&gt;
      &lt;item&gt;Use PN-Counter for quantities&lt;/item&gt;
      &lt;item&gt;Add-wins semantics for products (if concurrently added and removed, item stays)&lt;/item&gt;
      &lt;item&gt;Quantities merge correctly (concurrent +1 and +2 becomes +3)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Add product with quantity:&lt;/p&gt;
    &lt;code&gt;addToCart :: ProductId -&amp;gt; Int -&amp;gt; ReplicaId -&amp;gt; ShoppingCart -&amp;gt; ShoppingCart
addToCart pid qty replica cart =
  let counter = lookupOr emptyCounter pid cart
      incremented = incrementN replica qty counter
  in insert pid incremented cart&lt;/code&gt;
    &lt;p&gt;Remove product:&lt;/p&gt;
    &lt;code&gt;removeFromCart :: ProductId -&amp;gt; ShoppingCart -&amp;gt; ShoppingCart
removeFromCart pid cart = delete pid cart&lt;/code&gt;
    &lt;p&gt;Change quantity:&lt;/p&gt;
    &lt;code&gt;changeQuantity :: ProductId -&amp;gt; Int -&amp;gt; ReplicaId -&amp;gt; ShoppingCart -&amp;gt; ShoppingCart
changeQuantity pid delta replica cart =
  let counter = lookupOr emptyCounter pid cart
      updated = if delta &amp;gt; 0
                then incrementN replica delta counter
                else decrementN replica (-delta) counter
  in insert pid updated cart&lt;/code&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Handles all operations correctly&lt;/item&gt;
      &lt;item&gt;No data loss on concurrent modifications&lt;/item&gt;
      &lt;item&gt;Intuitive semantics for users&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PN-Counters can go negative (need validation)&lt;/item&gt;
      &lt;item&gt;Must track all replicas (for PN-Counter)&lt;/item&gt;
      &lt;item&gt;Slightly more overhead than simple LWW&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This example shows how combining basic CRDTs creates sophisticated application-level data structures.&lt;/p&gt;
    &lt;head rend="h2"&gt;Practical Considerations&lt;/head&gt;
    &lt;head rend="h3"&gt;Choosing a CRDT&lt;/head&gt;
    &lt;p&gt;The choice of CRDT depends on your requirements:&lt;/p&gt;
    &lt;p&gt;Do you need only additions? Use G-Counter or G-Set.&lt;/p&gt;
    &lt;p&gt;Do you need removals but not re-additions? Use 2P-Set.&lt;/p&gt;
    &lt;p&gt;Can you tolerate last-write-wins? Use LWW-Element-Set or LWW-Register.&lt;/p&gt;
    &lt;p&gt;Do you need to preserve concurrent operations? Use OR-Set or MV-Register.&lt;/p&gt;
    &lt;p&gt;Do you have sequences? Use RGA or similar sequence CRDT.&lt;/p&gt;
    &lt;p&gt;Do you need nested structures? Use OR-Map with nested CRDTs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Garbage Collection&lt;/head&gt;
    &lt;p&gt;Garbage collection is one of the most challenging practical problems with CRDTs. The fundamental tension: CRDTs achieve convergence by monotonically accumulating information, but production systems can’t grow unbounded forever.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Problem in Detail&lt;/head&gt;
    &lt;p&gt;Consider an OR-Set used for a collaborative todo list. Each time someone adds a task and removes it, we accumulate:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A unique tag for the addition (never removed)&lt;/item&gt;
      &lt;item&gt;A tombstone tracking the removal (never removed)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After 10,000 tasks have been created and completed, our “empty” todo list still contains 10,000 tags worth of metadata. In a G-Counter tracking page views, we keep a separate count for every replica that has ever incremented the counter—even if that replica hasn’t been online in years. For sequence CRDTs like RGA or WOOT, every deleted character becomes a tombstone that must be retained indefinitely. A 1000-character document that’s been heavily edited might internally contain 50,000 tombstones.&lt;/p&gt;
    &lt;p&gt;The core issue: CRDTs converge by retaining enough information to handle any possible merge. If replica A discards metadata about some operation, and replica B (which has been offline for weeks) later tries to merge its state—which still references that metadata—the merge may produce incorrect results.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why Can’t We Just Delete Old Data?&lt;/head&gt;
    &lt;p&gt;Let’s make this concrete with an OR-Set example:&lt;/p&gt;
    &lt;code&gt;-- Replica A's state
orset_a = {"todo-1": {tag_1, tag_2}}

-- Replica B's state (has been offline)
orset_b = {"todo-1": {tag_1, tag_2, tag_3}}

-- Replica A removes todo-1, observing tags {tag_1, tag_2}
-- Now A's state is:
orset_a = {}

-- If A garbage collects and forgets about tags {tag_1, tag_2},
-- then later merges with B:
merge(orset_a, orset_b) = {"todo-1": {tag_3}}

-- The element reappears! (Zombie resurrection)&lt;/code&gt;
    &lt;p&gt;The element we removed comes back because we lost the causal information about which tags we had observed and removed. This is the fundamental safety problem with CRDT garbage collection.&lt;/p&gt;
    &lt;head rend="h4"&gt;Strategies and Tradeoffs&lt;/head&gt;
    &lt;p&gt;Time-Based Expiry&lt;/p&gt;
    &lt;p&gt;The simplest approach: discard metadata older than some threshold (e.g., 30 days). This works well when you can guarantee all replicas sync within that window.&lt;/p&gt;
    &lt;code&gt;gcTombstones :: Timestamp -&amp;gt; ORSet a -&amp;gt; ORSet a
gcTombstones cutoff set =
  -- Remove tags older than cutoff
  Map.mapMaybe (\\tags -&amp;gt;
    let recent = Set.filter (\\t -&amp;gt; tagTime t &amp;gt; cutoff) tags
    in if Set.null recent then Nothing else Just recent) set&lt;/code&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simple to implement&lt;/item&gt;
      &lt;item&gt;No coordination required&lt;/item&gt;
      &lt;item&gt;Works well for frequently-syncing systems&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unsafe if replicas can be offline longer than the grace period&lt;/item&gt;
      &lt;item&gt;Must choose grace period conservatively (wasted space)&lt;/item&gt;
      &lt;item&gt;Zombie resurrection if threshold is too aggressive&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When to use: Mobile apps where you can bound offline time (e.g., “you must sync at least once per week”).&lt;/p&gt;
    &lt;p&gt;Coordinated Garbage Collection&lt;/p&gt;
    &lt;p&gt;Use distributed consensus to agree on what’s safe to discard. Once all replicas acknowledge they’ve received a particular update, the corresponding metadata can be safely removed.&lt;/p&gt;
    &lt;code&gt;data GCState = GCState
  { pendingGC :: Set Tag  -- Tags eligible for GC
  , replicaAcks :: Map ReplicaId (Set Tag)  -- What each replica has seen
  }

-- When all replicas have acked a tag, it's safe to remove
safeToDiscard :: GCState -&amp;gt; Set Tag
safeToDiscard (GCState pending acks) =
  -- Tags that all known replicas have acknowledged
  Set.filter (\\tag -&amp;gt; all (Set.member tag) (Map.elems acks)) pending&lt;/code&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Completely safe (no zombie resurrections)&lt;/item&gt;
      &lt;item&gt;Can garbage collect aggressively once consensus is reached&lt;/item&gt;
      &lt;item&gt;Works with arbitrary offline periods&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Requires coordination (defeats CRDT’s main selling point!)&lt;/item&gt;
      &lt;item&gt;Slow convergence if some replicas are rarely online&lt;/item&gt;
      &lt;item&gt;Must track all replicas (what about replicas that never come back?)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When to use: When you have a bounded, known set of replicas and can tolerate periodic coordination rounds.&lt;/p&gt;
    &lt;p&gt;Version Vectors for Causal Tracking&lt;/p&gt;
    &lt;p&gt;Use version vectors to track causal history. Metadata can be discarded once it’s been causally superseded at all replicas.&lt;/p&gt;
    &lt;code&gt;data CausalORSet a = CausalORSet
  { elements :: Map a (Set (Tag, VersionVector))
  , replicaVersions :: Map ReplicaId VersionVector  -- Last known VV per replica
  }

-- A tag can be GC'd if its version vector is dominated by all known replicas
canDiscardTag :: (Tag, VersionVector) -&amp;gt; Map ReplicaId VersionVector -&amp;gt; Bool
canDiscardTag (_, tagVV) replicaVVs =
  all (\\replicaVV -&amp;gt; tagVV `happenedBefore` replicaVV) (Map.elems replicaVVs)&lt;/code&gt;
    &lt;p&gt;This is more sophisticated: we track causality explicitly and can safely discard tags that are in the causal past of all known replicas.&lt;/p&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;More precise than time-based expiry&lt;/item&gt;
      &lt;item&gt;No coordination needed for the happy path&lt;/item&gt;
      &lt;item&gt;Safe as long as causal tracking is correct&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Version vectors add significant overhead (O(replicas) per operation)&lt;/item&gt;
      &lt;item&gt;Still requires tracking all replicas&lt;/item&gt;
      &lt;item&gt;Complex to implement correctly&lt;/item&gt;
      &lt;item&gt;What about new replicas that join later?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When to use: Systems already using version vectors for causal consistency (Riak, Cassandra-style systems).&lt;/p&gt;
    &lt;p&gt;Bounded Structures with Fallback&lt;/p&gt;
    &lt;p&gt;Limit metadata size and use LWW semantics when bounds are exceeded. For example, keep at most 1000 tags per element in an OR-Set. If we exceed that, discard the oldest tags and accept potential anomalies.&lt;/p&gt;
    &lt;code&gt;addWithBound :: Ord a =&amp;gt; a -&amp;gt; Tag -&amp;gt; Int -&amp;gt; ORSet a -&amp;gt; ORSet a
addWithBound x tag maxTags set =
  let currentTags = Map.findWithDefault Set.empty x set
      newTags = Set.insert tag currentTags
      boundedTags = if Set.size newTags &amp;gt; maxTags
                    then Set.fromList $ take maxTags $ 
                         sortBy (comparing tagTimestamp) (Set.toList newTags)
                    else newTags
  in Map.insert x boundedTags set&lt;/code&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Bounded space overhead (guaranteed)&lt;/item&gt;
      &lt;item&gt;No coordination needed&lt;/item&gt;
      &lt;item&gt;Graceful degradation (becomes LWW-ish when bounded)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Correctness sacrificed for space&lt;/item&gt;
      &lt;item&gt;May lose concurrent operations&lt;/item&gt;
      &lt;item&gt;Choosing the bound is difficult (too small = frequent anomalies, too large = still wasteful)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When to use: When you must have bounded space (embedded systems, strict SLAs) and can tolerate occasional anomalies.&lt;/p&gt;
    &lt;p&gt;Checkpoint and Rebase&lt;/p&gt;
    &lt;p&gt;Periodically create a “checkpoint” snapshot and discard history before that point. New replicas joining after the checkpoint start from the snapshot.&lt;/p&gt;
    &lt;code&gt;data CheckpointedCRDT a = CheckpointedCRDT
  { baselineState :: a  -- Snapshot at checkpoint
  , checkpointTime :: Timestamp
  , deltaSince :: [Delta a]  -- Operations since checkpoint
  }

-- Create a new checkpoint, discarding old deltas
checkpoint :: CheckpointedCRDT a -&amp;gt; CheckpointedCRDT a
checkpoint crdt = CheckpointedCRDT
  { baselineState = foldl merge (baselineState crdt) (deltaSince crdt)
  , checkpointTime = currentTime
  , deltaSince = []
  }&lt;/code&gt;
    &lt;p&gt;Replicas that haven’t synced since before the checkpoint must do a full state sync rather than incremental merge.&lt;/p&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Can aggressively prune old history&lt;/item&gt;
      &lt;item&gt;Conceptually clean (like Git’s shallow clones)&lt;/item&gt;
      &lt;item&gt;Works well with mostly-online systems&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Replicas offline during checkpoint period lose incremental sync&lt;/item&gt;
      &lt;item&gt;Need to track which replicas are pre-checkpoint&lt;/item&gt;
      &lt;item&gt;Full state sync is expensive&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When to use: Collaborative editing systems where most users are online most of the time (Google Docs, Figma).&lt;/p&gt;
    &lt;head rend="h4"&gt;Practical Recommendations&lt;/head&gt;
    &lt;p&gt;For most applications, a hybrid approach works best:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Use time-based expiry with a conservative grace period (90 days)&lt;/item&gt;
      &lt;item&gt;Track the oldest unsynced replica timestamp&lt;/item&gt;
      &lt;item&gt;Only discard metadata older than: &lt;code&gt;min(graceperiod, oldest_unsynced - safety_margin)&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Provide manual “compact” operations for administrators&lt;/item&gt;
      &lt;item&gt;Use bounded structures for untrusted/public replicas&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Without some form of garbage collection, CRDT state grows unbounded and will eventually exhaust memory or storage. The question isn’t whether to implement GC, but which tradeoffs you’re willing to accept.&lt;/p&gt;
    &lt;p&gt;And, realistically speaking, you’re unlikely to implement a system that only uses CRDTs and no other data storage. You’ll almost certainly have some sort of traditional database to store your data, which you can probably use to periodically coordinate garbage collection.&lt;/p&gt;
    &lt;head rend="h3"&gt;A note on Causal Consistency&lt;/head&gt;
    &lt;p&gt;CRDTs themselves don’t enforce causal delivery. You need a causal broadcast protocol to ensure operations are delivered respecting happens-before relationships. Without causal delivery, some CRDTs (especially operation-based ones) may behave incorrectly.&lt;/p&gt;
    &lt;head rend="h3"&gt;Performance&lt;/head&gt;
    &lt;p&gt;Different CRDTs have different performance characteristics. Consider your read/write ratio, expected contention, and replica count when choosing:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;CRDT Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Space Complexity&lt;/cell&gt;
        &lt;cell role="head"&gt;Add/Insert&lt;/cell&gt;
        &lt;cell role="head"&gt;Remove/Delete&lt;/cell&gt;
        &lt;cell role="head"&gt;Merge&lt;/cell&gt;
        &lt;cell role="head"&gt;Read/Query&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;G-Counter&lt;/cell&gt;
        &lt;cell&gt;O(r)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;O(r)&lt;/cell&gt;
        &lt;cell&gt;O(r)&lt;/cell&gt;
        &lt;cell&gt;Space: one counter per replica&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;PN-Counter&lt;/cell&gt;
        &lt;cell&gt;O(r)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(r)&lt;/cell&gt;
        &lt;cell&gt;O(r)&lt;/cell&gt;
        &lt;cell&gt;Double the space of G-Counter&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;G-Set&lt;/cell&gt;
        &lt;cell&gt;O(e)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;O(e)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;Standard set operations&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;2P-Set&lt;/cell&gt;
        &lt;cell&gt;O(e)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(e)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;Both added and removed sets grow&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;LWW-Element-Set&lt;/cell&gt;
        &lt;cell&gt;O(e)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(e)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;Can GC old timestamps carefully&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;OR-Set&lt;/cell&gt;
        &lt;cell&gt;O(e × t)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(t)&lt;/cell&gt;
        &lt;cell&gt;O(e × t)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;Tags accumulate, needs GC&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;LWW-Register&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;Minimal overhead&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;MV-Register&lt;/cell&gt;
        &lt;cell&gt;O(concurrent)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;O(c)&lt;/cell&gt;
        &lt;cell&gt;O(c)&lt;/cell&gt;
        &lt;cell&gt;Returns set of concurrent values&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;OR-Map&lt;/cell&gt;
        &lt;cell&gt;O(k × t)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(t)&lt;/cell&gt;
        &lt;cell&gt;O(k × t)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;Per-key OR-Set overhead&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;RGA&lt;/cell&gt;
        &lt;cell&gt;O(n + d)&lt;/cell&gt;
        &lt;cell&gt;O(log n)&lt;/cell&gt;
        &lt;cell&gt;O(log n)&lt;/cell&gt;
        &lt;cell&gt;O(n + d)&lt;/cell&gt;
        &lt;cell&gt;O(n)&lt;/cell&gt;
        &lt;cell&gt;Tombstones accumulate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;WOOT&lt;/cell&gt;
        &lt;cell&gt;O(n + d)&lt;/cell&gt;
        &lt;cell&gt;O(n²) worst&lt;/cell&gt;
        &lt;cell&gt;O(log n)&lt;/cell&gt;
        &lt;cell&gt;O(n + d)&lt;/cell&gt;
        &lt;cell&gt;O(n²) worst&lt;/cell&gt;
        &lt;cell&gt;Linearization is expensive&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Logoot/LSEQ&lt;/cell&gt;
        &lt;cell&gt;O(n × p)&lt;/cell&gt;
        &lt;cell&gt;O(log n)&lt;/cell&gt;
        &lt;cell&gt;O(log n)&lt;/cell&gt;
        &lt;cell&gt;O(n)&lt;/cell&gt;
        &lt;cell&gt;O(n log n)&lt;/cell&gt;
        &lt;cell&gt;Position identifiers grow&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Legend:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;r&lt;/code&gt;= number of replicas&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;e&lt;/code&gt;= number of elements in set&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;t&lt;/code&gt;= average tags per element (OR-Set)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;k&lt;/code&gt;= number of keys in map&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;n&lt;/code&gt;= number of visible elements in sequence&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;d&lt;/code&gt;= number of deleted elements (tombstones)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;c&lt;/code&gt;= number of concurrent writes&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;p&lt;/code&gt;= average position identifier length&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Key Observations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Counter CRDTs scale with replica count, not operation count. A billion increments still cost O(replicas) space.&lt;/item&gt;
      &lt;item&gt;Set CRDTs generally have constant-time operations, but OR-Set’s space grows with tags unless garbage collected.&lt;/item&gt;
      &lt;item&gt;Sequence CRDTs suffer from tombstone accumulation. RGA is typically faster than WOOT in practice despite similar asymptotic complexity.&lt;/item&gt;
      &lt;item&gt;Position-based sequences (Logoot/LSEQ) trade time complexity for avoiding explicit parent pointers, but position identifiers can grow pathologically.&lt;/item&gt;
      &lt;item&gt;Merge operations are often the bottleneck in high-throughput systems. Delta CRDTs dramatically improve merge performance by sending only changes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Libraries and Implementations&lt;/head&gt;
    &lt;p&gt;Many CRDT libraries exist:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Automerge: Full-featured CRDT library for JSON-like documents15&lt;/item&gt;
      &lt;item&gt;Yjs: Optimized for collaborative editing16&lt;/item&gt;
      &lt;item&gt;Riak: Database with built-in CRDT support17&lt;/item&gt;
      &lt;item&gt;Redis Enterprise: CRDT-enabled Redis18&lt;/item&gt;
      &lt;item&gt;AntidoteDB: CRDT-native database19&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each makes different tradeoff decisions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Further Reading&lt;/head&gt;
    &lt;p&gt;The CRDT literature is vast and honestly a bit scattered across conference proceedings. Here are the key papers worth reading:&lt;/p&gt;
    &lt;p&gt;Foundational:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Shapiro et al., “Conflict-Free Replicated Data Types” (2011): The original CRDT paper, defining state-based and operation-based CRDTs.&lt;/item&gt;
      &lt;item&gt;Shapiro et al., “A Comprehensive Study of Convergent and Commutative Replicated Data Types” (2011): Detailed technical report covering many CRDTs. This is the one you want to bookmark.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Sequence CRDTs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Oster et al., “Data Consistency for P2P Collaborative Editing” (2006): Introduces WOOT.&lt;/item&gt;
      &lt;item&gt;Roh et al., “Replicated Abstract Data Types: Building Blocks for Collaborative Applications” (2011): Introduces RGA.&lt;/item&gt;
      &lt;item&gt;Weiss et al., “Logoot: A Scalable Optimistic Replication Algorithm for Collaborative Editing” (2009): Introduces Logoot.&lt;/item&gt;
      &lt;item&gt;Nédelec et al., “LSEQ: An Adaptive Structure for Sequences in Distributed Collaborative Editing” (2013): Introduces LSEQ.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Advanced Topics:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Baquero et al., “Making Operation-based CRDTs Operation-based” (2014): Pure operation-based CRDTs without state.&lt;/item&gt;
      &lt;item&gt;Almeida et al., “Delta State Replicated Data Types” (2018): Efficiency improvements for state-based CRDTs.&lt;/item&gt;
      &lt;item&gt;Kleppmann et al., “A Conflict-Free Replicated JSON Datatype” (2017): Automerge’s design.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Surveys:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Shapiro et al., “Convergent and Commutative Replicated Data Types” (2011): The comprehensive technical report. Start here if you want depth.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Wrapping up&lt;/head&gt;
    &lt;p&gt;CRDTs are not a silver bullet. They trade coordination for metadata, strong consistency for eventual consistency, and simplicity for convergence guarantees. But in scenarios where availability matters more than immediate consistency, they’re remarkably powerful.&lt;/p&gt;
    &lt;p&gt;There is no “best” CRDT, only CRDTs suited to different problems; the CRDT you choose depends entirely on your application’s semantics:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What operations do you need (add, remove, re-add)?&lt;/item&gt;
      &lt;item&gt;Can you tolerate lost updates?&lt;/item&gt;
      &lt;item&gt;Do you need to detect conflicts or resolve them automatically?&lt;/item&gt;
      &lt;item&gt;What’s your tolerance for metadata overhead?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The CRDT abstraction is elegant in theory, but bewildering in practice because there are so many instances with subtle differences. Hopefully this guide has cut through some of the confusion, and given you a good intuition for how they work and when to use them.&lt;/p&gt;
    &lt;p&gt;I honestly still haven’t hit a use case for CRDTs that I couldn’t solve with a traditional database and some custom coordination logic. But sometimes we just want to learn for the sake of learning. If you beat me to it, let me know!&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The term “Conflict-free Replicated Data Type” was coined by Marc Shapiro, Nuno Preguiça, Carlos Baquero, and Marek Zawirski in their 2011 paper “Conflict-free Replicated Data Types” (technical report) and the 2011 SSS conference paper “A comprehensive study of Convergent and Commutative Replicated Data Types”. The theoretical foundations draw from earlier work on commutative replicated data types and optimistic replication. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;WOOT was introduced by Oster, Urso, Molli, and Imine in “Data Consistency for P2P Collaborative Editing” (2006). The name is a play on “OT” (Operational Transformation), emphasizing that it achieves similar goals “WithOut OT.” WOOT was one of the first practical sequence CRDTs and influenced many subsequent designs. ↩ ↩2&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;State-based CRDTs are also called “convergent” replicated data types (CvRDT). The “Cv” stands for “convergent” - emphasizing that replicas converge to the same state by repeatedly applying the join operation. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Operation-based CRDTs are also called “commutative” replicated data types (CmRDT). They require causal delivery of operations - if operation A happened before operation B on the same replica, B must not be delivered before A at any other replica. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The G-Counter appears in Shapiro et al.’s 2011 technical report “A Comprehensive Study of Convergent and Commutative Replicated Data Types” as one of the foundational examples demonstrating CRDT principles. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The space complexity is O(n) where n is the number of replicas, not the number of increments. This means G-Counters scale well with the number of operations but require tracking all replicas that have ever incremented the counter. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The OR-Set (Observed-Remove Set) was introduced by Shapiro et al. in their 2011 technical report. It’s also known as the “Add-Wins Set” because concurrent add and remove operations result in the element remaining in the set. The key innovation is using unique tags to distinguish between different additions of the same element. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sequence CRDTs are particularly challenging because positional indices change as elements are inserted or deleted. Unlike sets or counters where elements have stable identity, sequences must maintain ordering despite concurrent modifications at arbitrary positions. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;RGA was introduced by Roh et al. in “Replicated Abstract Data Types: Building Blocks for Collaborative Applications” (2011). The name “Replicated Growable Array” emphasizes that it’s an array-like structure that can grow through replication. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;YATA (Yet Another Transformation Approach) was developed by Kevin Jahns for the Yjs collaborative editing library. It combines ideas from RGA and WOOT while optimizing for the common case of sequential insertions (typing). Yjs is used in production by companies like Braid, Row Zero, and others for real-time collaboration. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Version vectors were introduced by Parker et al. in “Detection of Mutual Inconsistency in Distributed Systems” (1983). They extend Lamport’s logical clocks to track causality in distributed systems. Each replica maintains a vector of logical clocks (one for each replica), enabling precise causal ordering without requiring synchronized physical clocks. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Delta CRDTs were introduced by Almeida, Shoker, and Baquero in “Delta State Replicated Data Types” (2018). They bridge the gap between state-based and operation-based CRDTs, achieving operation-based bandwidth efficiency while maintaining state-based simplicity. Most production CRDT systems (Riak, Automerge) use delta-state internally. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Logoot was introduced by Weiss, Urso, and Molli in “Logoot: A Scalable Optimistic Replication Algorithm for Collaborative Editing” (2009). The name combines “log” (logarithmic complexity) with “oot” from WOOT, its predecessor. Logoot’s position-based approach influenced many subsequent CRDTs including LSEQ and Treedoc. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;LSEQ was introduced by Nédelec, Molli, Mostéfaoui, and Desmontils in “LSEQ: An Adaptive Structure for Sequences in Distributed Collaborative Editing” (2013). The key innovation is using different allocation strategies (boundary+ vs boundary-) based on tree depth, which keeps position identifiers shorter in practice compared to Logoot’s fixed strategy. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Automerge, created by Martin Kleppmann and collaborators, implements a JSON CRDT described in “A Conflict-Free Replicated JSON Datatype” (2017). It uses a columnar encoding for efficiency and has been rewritten in Rust for performance. Used by production apps like Inkandswitch’s Pushpin. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Yjs, created by Kevin Jahns, is optimized for text editing and uses the YATA algorithm. It’s notably faster than Automerge for text operations and includes bindings for popular editors like CodeMirror, Monaco, Quill, and ProseMirror. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Riak, a distributed database from Basho, was one of the first production systems to adopt CRDTs (2012). It implements counters, sets, and maps as native data types, using Delta CRDTs internally to minimize bandwidth. Sadly, the company collapsed dramatically, and the project was abandoned for quite some time. I think it’s still around in a diminished form, but haven’t tried it in a while. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Redis Enterprise’s CRDT support (Active-Active deployment) uses operation-based CRDTs with causal consistency. It supports strings, hashes, sets, and sorted sets with CRDT semantics, enabling multi-master Redis deployments. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;AntidoteDB is a research database from the SyncFree project that makes CRDTs the primary abstraction. Unlike other databases where CRDTs are a feature, AntidoteDB is designed from the ground up around CRDT semantics, providing highly available transactions over CRDTs. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46087022</guid><pubDate>Sat, 29 Nov 2025 12:25:35 +0000</pubDate></item><item><title>Hachi: An Image Search Engine</title><link>https://eagledot.xyz/hachi.md.html</link><description>&lt;doc fingerprint="f79f98f728b79b98"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Hachi: An (Image) Search engine&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Only the dead have seen the end of war .. George Santayana&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;For quite some time now, i have been working on and off on a fully self-hosted search engine, in hope to make it easier to search across Personal data in an &lt;code&gt;end to end&lt;/code&gt; manner. Even as individuals, we are hoarding and generating more and more data with no end in sight. Such "personal" data is being stored from local hard-disks to corporate controlled cloud-centers which makes it distributed in nature. So for following discussion, "Personal" meaning would be flexible enough to accommodate resources on a remote server and/or on different devices, as long the user could prove authentication and/or authorization to that data.
Current implementation supports only "images", but eventual goal is also to support other modalities like &lt;code&gt;video&lt;/code&gt;, &lt;code&gt;text&lt;/code&gt; and &lt;code&gt;audio&lt;/code&gt;, some code would be shared, while some new code would be required to better extract &lt;code&gt;Features&lt;/code&gt; for each modality.&lt;/p&gt;
    &lt;p&gt;Such distributed nature of data and potential capabilities of current self-hosted Machine learning models to extract semantic information, only to be queried through a single interface seemed enticing enough for me start this experiment in the first place. Following post at times may seem in-coherent, as i try to articulate my thoughts on the journey of development, challenges faced and future ideas. I hope to treat this as a personal essay with multiple themes, anecdotes and even random thoughts aiming to provide a higher level view of the journey and philosophy so far in more concrete terms.&lt;lb/&gt; Also, following post doesn't aim to cover every technical choice and implementation in finer details, such discussions would instead be part of dedicated future posts!&lt;/p&gt;
    &lt;head rend="h2"&gt;Motivation:&lt;/head&gt;
    &lt;p&gt;As Humans we tend to remember different attributes/parts of an entity/information at different times, and most of search engines' interfaces refuse to accomodate that. User generally end up with an unidirectional flow of information, with no recourse of providing feedback to improve upon the on-going query. Even most advanced interfaces fail to handle the stochastic nature of queries and humans' pre-disposition towards partial information to keep moving, it should be default for search-engines to present best-effort suggestions for queries even if they couldn't be fully resolved.&lt;/p&gt;
    &lt;p&gt;I also note that, it is not always easy to model the imperfect information like handelling a mis-spelling, which itself could be mis-spelled in many ways. It would require a conscious effort to put in a better search interface, as most digital machines make it easy to model when "something" is "correct" or when something is "incorrect". Conveying "Why" something is incorrect takes a lot more code, effort and time, hence indicating that economic realities are more to blame for such cases than bad intentions!&lt;/p&gt;
    &lt;p&gt;It also presents an opportunity to analyze the capabilities of a good interface, as personal data would make it very easy to notice its limitations, which couldn't be observed through seemingly complete interfaces exposed by many e-commerce companies.&lt;/p&gt;
    &lt;p&gt;Inspired by above stated ideas, My try has been to expose multiple (if not all) attributes for a resource directly to user and then letting user recursively refine query to get to desired result. Implementation is still far from complete, but this theme has served me well to set a basic roadmap for the project. Other themes such as self-hosting, hostile behaviour towards users in terms of privacy-invading features, limited or no options to refine a search by google, github etc has contributed to evolution of this experiment. Distributed queries being served by a cluster of (refurbished) smart-phones or single-board-computers remains a lofty goal of this experiment too!&lt;/p&gt;
    &lt;p&gt;Despite all the good intentions and ideas, any search interface should pass that threshold of being fast enough to not end up as another impractical experiment. Efforts were involved from the beginning to embrace the inevitable complexity such projects come to include despite many refactorings. Below is a minimal video to help visualize the current state and capabilities of the project.&lt;/p&gt;
    &lt;head rend="h2"&gt;Broader Ideas:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Minimalism: Minimalism in terms of number of external dependencies required for this project to be bootstraped, could explain a lot about downstream choices and evolution of the project to its current form. This has of any existing (source) code if possible or writing it from scratch which itself would require reading of a lot of existing code before i could port it to extend the project in a pure source sense. If it would be practical to reuse some code from existing capable projects/databases, i would have done so but most of such projects are designed to be de-coupled from application code for good reasons, as they are supposed to offer much more guarantees and stay robust even under heavy load. Being an (embedded) part of personal application we can choose to do away with such guarantees and yet expose much more information by tightly integrating ML models pipeline. In the end, application would handle much more complex indexing and inferencing pipelines, which would require a lot more code apart from search and storage interface generally expose!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Experimentation: Thinking more about in terms of augmenting the existing information, rather than to duplicate it, while fusing traditional (deterministic) attributes with semantic(ML) attributes. I think this is an interesting problem and which have not been fully utilized/explored for personal applications. Most of traditional databases were written to only handle "text" modality, but current ML models allow us to query semantic information too, which opens up a new space to experiment in. I treat semantic information as necessary and independent, but not the only signal useful to implement great search interfaces.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Hackability: For this project i wanted it be very easy for someone to start modifying it according to their needs, and this mostly co-relates with the first point about minimalism, lesser the number of dependencies, lesser is the amount of configuration required to bootstrap the developing environment. Both Python and Nim are stable, cross-platform languages and are easier to extend just using a C compiler. Nim source code it easy to compile and/or cross-compile to on almost all platforms. There are already python bridges for many languages, so all such languages are fair game to extend the codebase in any desired way!&lt;/p&gt;&lt;lb/&gt;Python environments (in)famously have the reputation of being difficult to bootstrap, whole parallel ecosystem is there to do so which itself creates another dependency. But i think project has made great progress in this regard, with now having a requirement of just 3 dependencies as&lt;code&gt;numpy&lt;/code&gt;,&lt;code&gt;regex&lt;/code&gt;and&lt;code&gt;markupsafe&lt;/code&gt;and optionally&lt;code&gt;requests&lt;/code&gt;, with no hard-dependence on versioning. Almost all python environments could be used to run the project with no changes, which also removes any need to bootstrap dev environment using Docker like huge dependency or any complex unwarranted build-systems plaguing many of the interesting projects. If i had money, i would pay someone to just make such projects easier to install and start with, by removing any redundant configuration or making it possible to use one common build-system !&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Even though above ideas may seem worthy to follow on, there is always an on-going fight to prevent dilution of agreed upon principals. Counter-intuitively i think there is some kind of &lt;code&gt;activation-enery&lt;/code&gt; (https://en.wikipedia.org/wiki/Activation_energy) requirement for each project, past that it actually is much easier to extend, modify, optimize the codebase somewhat like paying a debt to live debt free:)     &lt;/p&gt;
    &lt;p&gt;There are already very capable projects like &lt;code&gt;Sqlite&lt;/code&gt;, &lt;code&gt;Lucene&lt;/code&gt; offering full-text search capabilities, but they implement their own storage backends which require all data to be transformed to the compatible format which leads to duplication of data . This is something i wanted to avoid, as we would be continuously transforming every newer data and this would become computationally expensive when such data wouldn't even reside on same physical machine/node. If we could get away with fast-enough queries through a much simpler index/database, that seems like something worthy to pursue further.&lt;lb/&gt; Most of such projects were created to handle only text queries, But current ML models expose semantic information through "vectors" or "embeddings", generated after a series of linear and non-linear operations on some text or/and an image. &lt;code&gt;Top-k&lt;/code&gt; matching results are later retrived through a "Comparison" procedure with user query (embedding) as one of inputs. 
Such extensions are being gradually added in many older engines, so a hackable codebase like this project may offer more flexibilities while accomodating future ideas in this rapidly evolving field!  &lt;/p&gt;
    &lt;p&gt;It leads to a design comprising a meta-data indexing engine, coupled with vector-search engines for semantic search. We never intend to duplicate the original-data and don't care where it actually resides, once indexing is done. As i think search is more about reaching to a desired file/resource before that resource could be used! Pin-pointing that resource location quickly is the major motivation by incorporating the user intentions and context recursively!&lt;/p&gt;
    &lt;p&gt;(C)Python is used as the major language for backend and Nim (and C) is used to speed up the bottleneck portions of the codebase where-ever warranted. Writing from scratch allows me to update the api as i fit to handle a bottleneck portion of the pipeline (querying or indexing), without asking or waiting for a change in some upstream dependency. Nim itself is a language with relatively smaller community, so i am getting a bit comfortable porting code from other languages to my projects with only standard library and even experimenting with my own data-structures based on (protected) reference semantics than default value semantics that Nim use!&lt;/p&gt;
    &lt;head rend="h1"&gt;Meta-Index:&lt;/head&gt;
    &lt;p&gt;Its a minimal module/database to handle (store and query) meta-data being extracted from resources(images) and has been written in Nim. Currently it is single-threaded, column-oriented database using Json as data-exchange mechanism between python and Nim. In future idea is to shift to leveraging multiple threads for workloads/size greater than a threshold, to better use the current hardware capabilities. It is possible to generate an &lt;code&gt;auxilary&lt;/code&gt; index to speed up queries for a column/attribute on demand, which internally would use cache-friendly and hierichal data-structures to achieve so for most of scenarios! &lt;lb/&gt; Through development of this module, it has been easier to note that why most of databases end-up with some kind of dedicated &lt;code&gt;query language&lt;/code&gt;, as situations arise requiring composing multiple operations in one go which seems like a cleaner way to model such intentions. (and this also seems to validate the requirement of a  &lt;code&gt;query-planner&lt;/code&gt; to better execute a query by analyzing the order and nature of operations and some internal details).
Since it would be written for &lt;code&gt;hachi&lt;/code&gt; itself, it remains possible for me to  speed up a frequent operation by sharing a pointer/memory directly across Nim and python to prevent costly &lt;code&gt;copy&lt;/code&gt; operations, or to directly serve &lt;code&gt;raw json&lt;/code&gt; to the frontend in some cases without serializing and de-serializing at python boundary.&lt;/p&gt;
    &lt;p&gt;I have also experimented with multi-versioning storage design as Lmdb, to protect the original information created by code itself from user revisions. But current implementation instead favours creation of a dedicated field/attribute for user to modify/update. For example during face clustering process, backend will assign an unique Id for each new &lt;code&gt;cluster&lt;/code&gt;, to which user may want to change to a more descriptive name, this leads to presence of attributes like &lt;code&gt;personML&lt;/code&gt; and &lt;code&gt;person&lt;/code&gt; in the final schema. By default, any attribute/information generated through during indexing pipeline is supposed to be immutable to be easily reset to genesis state.&lt;lb/&gt; It still is a bit rigid implementation, as schema is locked once initialized (lazily or explicit), as adding new columns dynamically will require me to reallocate data in the memory and more syncing logic which i am off-putting for now and will work on in the future! Current iteration supports &lt;code&gt;string&lt;/code&gt;, &lt;code&gt;int32&lt;/code&gt;, &lt;code&gt;float32&lt;/code&gt;, &lt;code&gt;bool&lt;/code&gt;, &lt;code&gt;array[string]&lt;/code&gt; data-types, which seems to be enough for the application needs, but could be evolved in the future. I am not particularly content with current "string" querying, one reason is that Nim  by default does not have a concept of no-copy slice, and it is difficult to even expose such a user-defined type. As &lt;code&gt;strings&lt;/code&gt; are null-terminated, so most of other composed data-structures with string as one of fields have that underlying assumption which that user-defined type will break. Also i think for a lot of meta-data attributes, i could use &lt;code&gt;ShortString&lt;/code&gt;    kind of data-type to speed up scanning/querying by better leveraging the cache. Some of these issues are being experimented through an independent project and if found to improve performance could be implemented in this codebase too!    &lt;/p&gt;
    &lt;p&gt;There are also Simd opportunities inside the "querying" code, but since its design is being guided by overall needs for the product itself, i hope to add those architecture specific optimizations only after system-design becomes stable enough for most of the features supposed to be supported!&lt;/p&gt;
    &lt;head rend="h1"&gt;Face-Recognition:&lt;/head&gt;
    &lt;p&gt;Being able to group same person(s) with a high probability, as another attribute to search for or mix with other attributes, would be a very quality addition to any search interface. Current DL models for some-time now have been able to distinguish faces with a high accuracy. But being able to distinguish real-life faces still requires a conformance to the pipeline such models would have been trained with.&lt;lb/&gt; There are multiple architectures for such models that have been proposed to tackle this problem, but most pipelines could be assumed to follow a generic flow, which begins with detection of facial bounding boxes from a given image or camera frame, then followed by detection of facial-landmarks for each such face, and ends with generation of &lt;code&gt;embeddings/vectors&lt;/code&gt; which figuratively would represent some kind of latent representation of that face.  At this point, this would be reduced to a Vector Spaces problem and hence much easier to deal with traditional tools like nearest neighbour search !&lt;/p&gt;
    &lt;p&gt;It almost always overwhelming to decide on a particular Implementation to build upon, while accommodating various factors like &lt;code&gt;latency&lt;/code&gt;, &lt;code&gt;accuracy&lt;/code&gt; , &lt;code&gt;hardware requirements&lt;/code&gt;, and most of such intensive pro-bono work would never even be visible to the end-user. For me atleast this goes much further, as i would be implementing each such model using an independent ML framework, which would require me to understand also all the pre-processing and post-processing code, to be  faithfully ported to Nim.&lt;lb/&gt; Spending time on reading papers and existing implementations helps me to get an idea about overall "capability" of the model and potential requirements during fine-tuning of the model in future. Sometimes it has been enough for me to come across an interesting concept through a paper or some nice optimization trick, even if i end up not using that particular implementation. &lt;lb/&gt; Most of face embeddings generation models are trained on a &lt;code&gt;Siamese-loss&lt;/code&gt; like objective to try to explicitly distinguish both positive-positive and positive-negative pairs. This generally involves manually collecting such pairs and hence prone to bias ! Such features predictors are also very sensitive to &lt;code&gt;face-alignment&lt;/code&gt; code used, and hence may require you to faithfully follow the training code!

Dataset being used for training and choice of the objective function are two very major factors influencing the performance of any model. Leakage of evaluation data into training set has been a real issue in recent years for many experiments. Face-recognition itself is a very subjective problem and generally require more "visual" testing apart from (mathematical) metrics proposed for this problem/sub-domain.  &lt;/p&gt;
    &lt;p&gt;Current pipeline uses &lt;code&gt;retina-face&lt;/code&gt; model to predict faces and landmarks in one go which helps producing stable facial-landmarks and speeding up the pipeline. (As predicting facial-landmarks would be much cheaper from internal features than through a dedicated model, and it also helps stabilizing  the training of the model).
Though it could make sense to argue about a model's ability to internalize learning correlated features without adding an explicit loss, but in practice it is always (very) beneficial to use multiple losses explicitly.
Interestingly, &lt;code&gt;residual&lt;/code&gt; connection in &lt;code&gt;ResNets&lt;/code&gt; was an important innovation making it possible to train much deeper networks at that time, even though it would be just mimicing an &lt;code&gt;identity&lt;/code&gt; function.



  &lt;/p&gt;
    &lt;p&gt;In my experience, dataset being used for training and choice of the objective function are two very major factors influencing the performance of your model on real-life (bit out-of-distribution datasets). I find it a good practice to always visually debug some of the random samples to get a "feel" for the dataset!&lt;/p&gt;
    &lt;p&gt;Even after having a good pipeline to generate "embeddings" for a face, &lt;code&gt;clustering&lt;/code&gt; remains a very challenging problem, due to various reasons.
Like with almost all clustering algorithms, we start out with no prior information about of the underlying (number) distribution of the data (faces). (as this is what we would be trying to estimate). As we keep encountering the newer information, possible &lt;code&gt;updates&lt;/code&gt; through &lt;code&gt;back-filling&lt;/code&gt; are required for the underlying index, which somewhat resembles of an auto-regressive operation and hence the error-accumulation rate is relatively high. We would also need to wait for some "initial" amount of data/information to be collected, to estimate initial stable centroids. This difficulty is further compounded by the choices for various thresholds like face-detection, some measure for blurness in the detected face, and a dependence on order of information being encountered.&lt;/p&gt;
    &lt;p&gt;As indicated, choosing same model to predict landmarks and face-bounding boxes, helps reduce the impedance mismatch that occurs when output of one model is being fed through another model. We would need to a dedicate model for facial-features though as earlier features may not be dense enough to distinguish among individual faces!&lt;/p&gt;
    &lt;p&gt;Currently Implementation works by collecting some minimal amount of information before &lt;code&gt;Cluster&lt;/code&gt; creation process could begin. 
Each Cluster is a union of a set of main/master embeddings and a set of follower/slave embeddings. Selection of main embeddings is a crucial part to maintain the stability of a cluster even when new information would be encountered. Initial filtering of unfeasible (master) embeddings is done through some static criterias, for example we strive to filter any of &lt;code&gt;blurred&lt;/code&gt; faces, face-profile is estimated through facial-landmarks, stable forward-facing profiles make face-alignment easier further in the pipeline. Such (static) criterias definitely help to reduce the number of invalid candidates, but may not be enough for many real-life datasets. A further minimal module comparing the &lt;code&gt;hog-features&lt;/code&gt; with a set of pre-saved hog-features is introduced to help invalidate faces with &lt;code&gt;sunglasses&lt;/code&gt; and some false positives not caught by earlier criterias!&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;no-categorical-info&lt;/code&gt;, when not able to being fit into any of the clusters.
Note that a lot of empirical data comes into effect as multiple decisions would be required while choosing many thresholds and may require multiple runs .&lt;/p&gt;
    &lt;p&gt;Since face-recognition is very subjective and i myself have to compare other features to make sure that indeed the correct person(s) have been grouped together by the pipeline. But with a latency of around 25 ms, it seems to do very good on a held out dataset of persons with close up faces, (Zen-Z) selfies and sunglasses occluded eyes. Personal photos are much easier to classify/recognize compared to such a dataset!&lt;/p&gt;
    &lt;p&gt;For any practical ML integrated product, We would need to have a very performant concurrent pipeline to keep feeding the model while being constantly aware of any data-distribution impedance mismatch, to reach anywhere near the 'accuracy' and &lt;code&gt;speed&lt;/code&gt; promised in a research paper. This touches upon the issue of having good understanding of software engineering basics, while being aware of possible regressions resulting from a newer functionality like ML.&lt;lb/&gt; Though bigger VLM/LLM (base) models have potential to handle data-impedance mismatch issues due to their sheer size, their usage would definitely hamper the application responsiveness and have proven to be relatively rigid to be fine-tuned for a specific domain! &lt;/p&gt;
    &lt;head rend="h2"&gt;Indexing:&lt;/head&gt;
    &lt;p&gt;Indexing pipeline begins with desired data location as its input to recursively scanning &lt;code&gt;directories&lt;/code&gt; to collect &lt;code&gt;raw-data&lt;/code&gt; in batches.
Multiple meta attributes such as &lt;code&gt;exif-data&lt;/code&gt;, &lt;code&gt;size&lt;/code&gt;, &lt;code&gt;mount location&lt;/code&gt;, &lt;code&gt;name&lt;/code&gt; are extracted to be later queried through the Meta-indexing engine. Focus has been on designing a good schema to accomodate future use-cases, but since we would be collecting only meta-information without ever modifying the original or duplicating the original data, it remains relatively easier to shift to a newer version/schema even through automatic means.&lt;lb/&gt; ML models extract semantic information which can be later queried through a vector-indexing engine. By default resources to be indexed are assumed to be residing on a local-disk but any protocol could be leveraged, if proper authorization and authentication could be provided.&lt;lb/&gt; Monolithic nature of the code helps me to share &lt;code&gt;raw-data&lt;/code&gt; read/collected once for various components like &lt;code&gt;hash generation&lt;/code&gt;, &lt;code&gt;preprocessing&lt;/code&gt; code for ML models, reducing the number of costly I/O calls. This pipeline has come a long way from a blocking implementation to its current (almost) fully async nature, resulting in very high saturation of computing resources. Apart from running multiple threads, dedicated kernels/functions are used to speed up pipeline by &lt;code&gt;fusion&lt;/code&gt; of operations wherever possible.
 One such example/scenario  has been shown below.&lt;/p&gt;
    &lt;code&gt;def preprocess_kernel(
    image:Tensor[uint8],
    new_shape:tuple[int,int], 
    rgb_to_bgr:bool = True, 
    normalize:bool = True):
    # Preprocess kernel, may fuse resize, color_conversion and normalization into one function!

    # Pseudo-code!

    result = newEmptyTensor[uint8](new_shape)
    for i in new_height:
        for j in new_width:
            inp_h, inp_w = get_corresponding_pixel(image, i, j)
            for k in 0..&amp;lt;3:
                if rgb_to_bgr:
                    result[i,j , 3-k-1] = image[inp_h, inp_w, k]
                    # normalize based on mean and deviation used for training dataset further...
                else:
                    result[ i,j,k] = image[inp_h, inp_w, k]
&lt;/code&gt;
    &lt;p&gt;Each &lt;code&gt;resource&lt;/code&gt; could be assumed to go through a flow like this:&lt;/p&gt;
    &lt;code&gt;resource_location = "file://xyz.jpg"
# OR
resource_location = "remoteProtocol://xyz.jpg"

raw_data = download_raw_data(resource_location)

embeddings = ML_model( preprocess(raw_data))
exif_data = extract_exif_data(raw_data)
preview = generate_preview(raw_data)
write_preview(preview)
....
&lt;/code&gt;
    &lt;head rend="h2"&gt;Vector Index:&lt;/head&gt;
    &lt;p&gt;It is another minimal module to store vector-embeddings as shards on the disk. Necessary meta-data is stashed along with that shard, to make it self-contained, which in future will help in distributed/parallel retrieval. For now each shard is just a numpy (float32) Tensor, and comparison routine is a &lt;code&gt;np.dot&lt;/code&gt; operator, which itself use the &lt;code&gt;blas/openblas&lt;/code&gt; library to speed up this operation! Each shard is loaded from the Disk during a query, and &lt;code&gt;top-k&lt;/code&gt; candidates are collected to be fused together with other deterministic meta-attributes. Loading from Disk do add some latency, but it allows me to regulate RAM usage through &lt;code&gt;shard-size&lt;/code&gt; hyper-parameter, to allow running this on different platforms  with diverse specifications including single-board computers. &lt;code&gt;Shard-size&lt;/code&gt; could be kept relatively high for higher RAM systems to speed up shard querying.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Matmul&lt;/code&gt; is one of the most optimized algorithms which run at almost 90% of theoretical capacity on most of intel/amd Cpus when leveraging &lt;code&gt;Blas&lt;/code&gt; like libraries. So every further optimization from here-on would involve some kind of information loss. There is a whole literature now to speed up this comparison/retrieval process through &lt;code&gt;quantization&lt;/code&gt; and/or &lt;code&gt;nearest neighbour&lt;/code&gt; indices like HNSW. Fast SSDs are also leveraged to run such comparisons at very high speed for upto billion vectors on just a single node in near real time!&lt;/p&gt;
    &lt;p&gt;But such all techniques involve compression of information (which itself is best-effort being the result of modeling a large amount of biased data) through out-of-band mechanisms, for example creating centroids/clusters is just based on the vector values and taking some mean without a way to pass back the information to the model which produced those vectors in the first place. This way is quick and you would get great speed-ups, and there is an active debate among vector-database vendors across various metrics and implementations. In my experience only visual results on a personal data would be a good metric a user should test for. Product-quantization is something i would be implementing if were to choose one, as i think coupled with &lt;code&gt;top-k&lt;/code&gt;, it should work reasonably well to include (subjectively) correct results (high recall!) .&lt;/p&gt;
    &lt;p&gt;Another worthy and very effective solution i think is to instead train a linear layer to finetune the original model depending upon the task. ML Features/embeddings from a big enough model, could assumed to have a knowledge about diverse topics, but for example, a user may be trying to distinguish between different plants. A linear layer could easily be trained with just few thousand samples, to achieve so with much higher accuracy than original models, and even with half the size/dimension of original embeddings. Intuitively it could be thought that we freed the information channels to just focus on plants, decreasing the entropy model earlier had to deal with. Any such layer could be trained even without any framework, as it would just be one &lt;code&gt;backward&lt;/code&gt; operation to implement.
OpenAI has a nice cookbook if a reader would want to explore this further!



  https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb&lt;/p&gt;
    &lt;p&gt;An interesting thing sharding allows is to use any available &lt;code&gt;hardware&lt;/code&gt; to speed up retrieval. Since we need just &lt;code&gt;comparison&lt;/code&gt; routine and corresponding shard(s) to return top-k candidates, it de-couples it from any of application code. A new smartphone could be detected, and some &lt;code&gt;shards&lt;/code&gt; could be transferred during initial set-up, optimal percentage/number of shards could be easily calculated by running same &lt;code&gt;comparsion&lt;/code&gt; operation on new device. Like running a &lt;code&gt;2048 x 2048&lt;/code&gt; , inner-product op and comparing latency with &lt;code&gt;master/main&lt;/code&gt; device, would tell us the capacity of the new device and so that number of shards would be transferred to speed up retrieval process!&lt;/p&gt;
    &lt;p&gt;There are performance gains to be have in the current implementation, would like to atleast start using &lt;code&gt;float16&lt;/code&gt; data-type, but its a bit tricky on intel cpus with no compiler support for this type. Printing of CPU capabilities do show the presence of float16 hardware support on my system ! 
ARM(v8 64) seems to offer native float16/floatH types, there seems to be difference in that type either supported natively by compiler or as an intrinsics/assembly code. I have not been able to get expected speed up for now! Such code is still being experimented upon in the limited time i have.&lt;/p&gt;
    &lt;head rend="h2"&gt;Backend:&lt;/head&gt;
    &lt;p&gt;Backend is written in python, which exposes a pure API server, to let the client/frontend to make API calls to. Starting with very naive code to just return all the meta-data for a &lt;code&gt;directory&lt;/code&gt; to current pagination support it have gone through many revisions and design iterations and now i have much clearer idea about how to architect/wrap a big enough piece of functionality. I wanted the app to be end to end, but this also put extra pressure on app to be responsive enough for all user events. Current indexing code is capable of providing rich details such as directory currently being scanned, estimated time (eta) and allows robust Cancellation of an ongoing task/threads. 
It has not been easy to model such communication b/w concurrent tasks and touches upon much discussed &lt;code&gt;structured-concurrency&lt;/code&gt; debate i.e how to run multiple tasks asynchronously, while being able to robustly cancel them at any point in time, all while being able to collect all errors cleanly!&lt;/p&gt;
    &lt;p&gt;From C days, i have been a user of (Posix) &lt;code&gt;threads&lt;/code&gt; type implementations, since major OSes provide those minimal but stable APIs, it helps me during context switching to different languages. Both C and Nim expose that, Python itself let the OS manage threads without its own runtime implementation, but bypassing the GIL when makes sense is something user have to do to fully utilize the resources! Also this kind of code requires user to handle a lot of code as to communicate b/w threads but atleast i (think) understand the basic ideas to prevent deadlocking if occurs and iron out initial bugs. As you run such threads deeper and deeper inside application stack , it keeps getting harder to communicate information back to the client. But when it starts working, it is really cool to have a central interface to see all the stuff backend is doing and predict very good ETA !&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Flask&lt;/code&gt; was initially used to easily map &lt;code&gt;functions&lt;/code&gt; to a particular route/url to wrap up initial implementation, current implementation now just uses &lt;code&gt;werkzeug&lt;/code&gt; (main engine behind flask) directly, hence doing away with a lot of unrequired dependencies like a template engines that Flask ships with. Even though this would not effect the end user in any visible way, this has been a very nice quality-of-life improvement like stuff for me as a developer. Since werkzeug is pure python, it can now be shipped/bundled directly as source code. Also each request is now handled by an available thread (from a pool) by reading &lt;code&gt;http environment&lt;/code&gt; from a shared queue following conventional model. By default for multi-threaded option, werkzeug would create a new fresh thread for handling that request. This does away with lots of OS/system calls for each new request and latency now seems more consistent and predictive. I have also  stumbled upon a pattern to actually make it easier to &lt;code&gt;mount&lt;/code&gt; multiple &lt;code&gt;apps&lt;/code&gt;  cleanly given i never liked and even understood the &lt;code&gt;blueprint&lt;/code&gt; that flask offers to make it easier to distribute the logic of your app to other modules too.
Since WSGI protocol just expect a callable python object, it should be much easier to develop independent apps without having any knowledge where it would be called/used. It also makes it quite fun to actually write/expose python code to handle client inputs. &lt;/p&gt;
    &lt;code&gt;class SimpleApp():
    """Each instance could be used a WSGI compatible callable"""
    def __init__(self, allow_local_cors:bool = False):
        self.initialized = False
        self.http_methods = ["GET", "POST", "PUT", "DELETE", "OPTIONS"] 
        self.url_map = None # we will lazily initialize it!
        self.extension_prefix = "ext" # as apps would be registered/
        self.registered_extensions:dict[str, SimpleApp] = {}

        ....

    def add_url_rule(self
                     rule:str, 
                     view_function:Callable, # corresponding view.
                     endpoint:Optional[str] = None, # set to view_function
                     methods:list[str]= ["GET"]):

        ... # some validation code.

        self.endpoint_2_uri[endpoint] = (Rule
            (rule, endpoint = endpoint), methods
            )
        self.endpoint_2_viewFunction[endpoint]  = view_function
        self.initialized = False

    def register(self, app:SimpleApp, name:str):
        """
        Here we register another such `app`.
        It would be mounted at `/ext/&amp;lt;name&amp;gt;` , so all requests to /ext/&amp;lt;name&amp;gt;/&amp;lt;route&amp;gt;, would be forwarded to this `app` .
        """

        ... # some validation code.
        self.registered_extensions[name] = app
        print("Extension registered at: {}/{}".format(self.extension_prefix, name))


    def __call__(self, environ, start_response) -&amp;gt; Iterable[bytes]:
        # This is called 
        if not (self.initialized):
            print("[Initializing]: Parent")
            self.initialize()

        for ext in self.registered_extensions:
            if not (self.registered_extensions[ext].initialized):
                print("[Initializing]: {}".format(ext))
                self.registered_extensions[ext].initialize()

        # If a call to such an extension.. we modify the environment a bit.
        active_app = self
        extension_name = None
        temp_path = environ['PATH_INFO']
        temp_split = temp_path.split("/")
        if temp_split[1] == self.extension_prefix:

            extension_name = temp_split[2]
            assert extension_name in self.registered_extensions, 
            extension_path = temp_path.replace("/{}/{}".format(self.extension_prefix, extension_name), "")


            environ['PATH_INFO'] = extension_path
            environ['REQUEST_URI'] = extension_path
            environ['RAW_URI'] = extension_path

            active_app = self.registered_extensions[extension_name]

    ## -----------------------------------------------
    # NOTE: only werkzeug specific code is here!
    # ---------------------------------------------
    request = Request(environ = environ) # minimal wrapping code!
    urls = active_app.url_map.bind_to_environ(environ)
    endpoint, args = urls.match()

    # view function can choose to return iterable[bytes] are the result of view function or call , or further wrap it to be as expected by werkzeug!
    iterable_bytes = active_app.endpoint_2_viewFunction[endpoint](request, **args) 
    return iterable_bytes  # as WSGI protocol expects!
    # ---------------------------------------------------------
&lt;/code&gt;
    &lt;p&gt;Note that, any existing Python object, can be made to accept &lt;code&gt;client&lt;/code&gt; requests on demand by adding very minimal code and could be done for selective functionality. For example, during setup of a new android device, i may have to ask user to choose one of the existing &lt;code&gt;devices&lt;/code&gt;, this kind of interactive input can be modeled easily now, as i just add a new routine in the Corresponding class to accept requests on a route such as &lt;code&gt;/ext/android/beginSetup&lt;/code&gt;, once i get that, all the existing logic already written could be used to finish setup. It is as easy as &lt;code&gt;parent_app.register(app = thisApp, name = "android")&lt;/code&gt; to start routing corresponding requests to this app!&lt;/p&gt;
    &lt;head rend="h2"&gt;ML:&lt;/head&gt;
    &lt;p&gt;Machine learning is being powered by a framework written completely in Nim, most of work was done on that framework before i even stared working on this project. This has allowed me to wrap CLIP and Face-Recognition Pipeline along with the application while only depending on OneDNN for some routines. OneDNN (mkldnn) (https://github.com/uxlfoundation/oneDNN) is one of the libraries to speed up various Deep learning operations with great documentation.&lt;/p&gt;
    &lt;p&gt;Ported models run faster on intel/Amd Cpus than pytorch counterparts, owing to fusion of operations like Batch Normalization and Convolution, and high re-use of pre-allocated memory (similar to in-place operations). Current &lt;code&gt;torch.compile&lt;/code&gt; like engine would end up making some of those optimizations after analyzing the graph, but for at-least 2.0 version it is not supported on Windows for me to compare against!&lt;/p&gt;
    &lt;p&gt;It took a lot of effort during one-two years i was working on it to be complete enough for me to start porting Deep-learning models using it. Also OneDNN shifted to V3 during that time, and only some code was updated to newer API and this has left the project in a unstable state with no visible stable APIs for users to work with. For each model i have to manually analyze the locations/requirements for fusion of operations, port quite a lot of pre-processing and post-processing code to make it end to end. These reasons contributed to a lot of technical debt, which i have not found the resources to tackle yet. Without removing that debt it never made sense to open-source it, besides there are now projects like GGML, and tiny-grad to serve inference only needs with minimal resources!&lt;/p&gt;
    &lt;p&gt;Porting of each model is quite an involved task, as you have to read enough papers to understand ideas about model if want to later fine-tune that model too. You may want to find first find or create a simpler implementation in pytorch to make it easier to port to a new language. All experimentation could be done in pytorch/python, for example i experimented with alternate quantized attention layers for CLIP model, and it indeed had a better performance for eval datasets mentioned in CLIP paper. Tangentially it was really cool to read through Open-AI implementations and papers, papers were written in an approachable manner to let the read indulge in hypothesis, codebases were clean with minimal dependencies. Its really a shame what that company/organisation chose to become under the guise of "user-safety" effectively clipping the (open) ethos of this field, but at same time i am grateful for all the researchers' work in this current DL/ML era and seeing the evolution of this field in such an open manner!&lt;/p&gt;
    &lt;p&gt;I would like to work on the project though atleast enough to tackle that debt and open-source it in state for users to extend upon, if found useful. Even though i am using OneDNN for some routines, i think it is better to have a common and easier to extend codebase to allow more experimentation and aggressive optimizations , but this itself is a huge-task and now with multiple GPU architectures its just something that couldn't be tackled without a lot of time and money. Even in this age where H100 is the baseline for benchmarks in testing, i find it worthwhile to work on a minimal DL Compiler to just tackle ARM/Intel/Risc Cpus to start taking advantage of these cheaper machines. Being able to pin-point a tennis ball in a 3D space remains the dream !&lt;/p&gt;
    &lt;head rend="h2"&gt;Frontend / App:&lt;/head&gt;
    &lt;p&gt;Current front-end is completely written in Html, Js(Ts) and (tailwind) css as multi page webapp. Earlier frontend was written in Svelte, but lack of internal documentation and too much "magic" became too "viral" for me to handle. For me, abstractions and APIs exposed by Browsers are more than enough to maintain required precision during DOM updates. Care is taken to use batch updates, prevent redundant rendering, judicial usage of resources to prevent unrequired pressure through pagination, even for a local backend server. It has passed our litmus test for search over 180 Gb of indexed Pexels dataset on a (minimal) remote server. My friend Akshay helped a lot in frontend development, testing various datasets and offering detailed bug reports which helped uncover a lot of edge cases during development of the project. There would always be room for improvements on the UX/UI side, but we have found it is much easier to extend and improve frontend with a stable backend!&lt;/p&gt;
    &lt;p&gt;Pexels dataset: https://huggingface.co/datasets/opendiffusionai/pexels-photos-janpf&lt;/p&gt;
    &lt;p&gt;Apart from webapp, there is also a Windows App, which under the hood uses the &lt;code&gt;webview&lt;/code&gt; to render the frontend. All native Windows APIs remain available to use from the Nim code, which puts it into a hybrid category. It is not ideal, but atleast it doesn't require me to ship a full web-browser, which i think is waste of compute resources, but at the same time leaves me wondering how current GUI development became so resource intensive for a single developer to manage while offering little benefits! I have been looking into forks of earlier GTK versions for linux to keep the complexity/learning contained, but that also seems nothing less than an adventure!  &lt;/p&gt;
    &lt;head rend="h2"&gt;Tools/libraries:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Nimpy (https://github.com/yglukhov/nimpy) : A minimal python-Nim bridge to make it easier to write extensions in Nim to be called from python and to use python modules in Nim. Unlike many such bridges which includes a lot of boiler-plate code, there are no complex classes/interfaces to be included in the extension. It targets necessary features like marshaling of native python types to and from Nim, targets the minimal Python API to not depend on python versions, finding underlying python.dll at runtime.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Stb Image (https://github.com/nothings/stb): A big fan of such single header libraries, this one implements encoders for most of image formats in pure C. Its very easy to modify it pass pointer to the raw-data and writing raw-data to a pre-allocated memory saving costly memory copying particularly visible for 4k photos! It helps remove dependency on OpenCV for image reading ! Nim made it very easy to just compile this along with other Nim code.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;LibWebp (https://github.com/webmproject/libwebp): Allows decoding and encoding for webp formats, Though documentation is a bit sparse on some internal API usage, lot of examples are included in the repository to read. I managed to use&lt;/p&gt;&lt;code&gt;argb&lt;/code&gt;field directly to pass&lt;code&gt;argb&lt;/code&gt;format data to do away with transformation logic and some (memory) allocations. It follows callback passing convention to implement custom behaviour like a progress bar and to write encoded data to a user provided buffer. Written completely in C and very easy to compile and read, it is being used for writing image previews, helping remove dependency on OpenCV.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Zig-cc (https://ziglang.org): Using&lt;/p&gt;&lt;code&gt;zig/clang&lt;/code&gt;as a C compiler, allowed me to easily cross-compile a lot of Nim code for Linux, targeting&lt;code&gt;2.27 libc&lt;/code&gt;. Making it easier to set a LibC target has proved very useful to bypass that&lt;code&gt;libC&lt;/code&gt;mismatching stuff! Really cool work by Zig community to tackle a lot of such technical debt to make software development much easier !&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As mentioned earlier i try to use a lot of existing open-source code if i can, even it would be for reading/understanding purposes only. It still blows my mind even after many years, to just read/understand some complex implementation and modify it for personal use-case for Free. For example even though &lt;code&gt;OpenCV&lt;/code&gt; is a big/complex dependency, its still has a very readable codebase and i read code from it a few times during this project to understand differences b/w my port and OpenCV one.&lt;/p&gt;
    &lt;p&gt;Being able to integrate multiple languages has its own challenges too, as it would require us to understand boundaries, internal details, assumptions that each runtime would want developer to respect. It gets complex to reproduce and understand bugs while running multiple multi-threaded runtimes as debugging gets more difficult. Debugging is one of things i would like to get better at, i have very limited knowledge of GDB as of now, which is expected to be table stakes for debugging in such environments. I have had some nasty bugs , but being able to compile all required pieces made it a bit easier to debug even with print-style debugging :)&lt;/p&gt;
    &lt;head rend="h2"&gt;Current State:&lt;/head&gt;
    &lt;p&gt;A lot of functionality is working, than not and having tested over 500k images i could be a bit satisfied about internals' performance and robustness. I would like to say that it can easily handle 10 millions of images/resources, and there is nothing to suggest that it won't, but it is different from using a production database to extrapolate the performance confidently. Despite writing from (almost) scratch in a number of languages, both indexing and inferencing pipeline are more expressive, robust and faster than many similar images search apps, but benchmarking for such complex apps could be subjective and more so when you mix in semantic search.&lt;/p&gt;
    &lt;p&gt;There are still some hardcoded constants and also intentionally some low performing components, like using ViT B/32 variant of CLIP model, which are acting as placeholders, and would be replaced easily with better counterparts in the future.&lt;/p&gt;
    &lt;p&gt;It has been tested on Windows 10/11 and on Fedora 42/43 with an assumption of &lt;code&gt;x64&lt;/code&gt; architecture. Compiled extensions are also packaged to quickly test the application, but users are free to compile code as they see fit. Linux shared objects target &lt;code&gt;LibC 2.27&lt;/code&gt;, so should work on most of recent distributions out of the box. Except some ML code there is main requirement of any/a C compiler to further extend the codebase by the user. Most of testing is done on my Laptop with  &lt;code&gt;i5-8300H&lt;/code&gt; processor and 8 GB memory. I don't have a MacOS to test on, ML code would need to be modified to target ARM architecture, except that very minor modifications should be needed if any. It is quite possible for initial users to encounter minor bugs, due to its limited run in diverse dev environments, but installation and usage on Cloud servers during testing has been quite smooth.&lt;/p&gt;
    &lt;p&gt;Below is a video showcasing workflow to index data from multiple MTP/Android devices. (Still a WIP).&lt;/p&gt;
    &lt;head rend="h2"&gt;Random Thoughts/Philosophy:&lt;/head&gt;
    &lt;p&gt;I think it gets easier with time to grok larger codebases to isolate/find the functionality/implementation reader would be interested in. Most of mature codebases are organized to help navigating the source-tree anyway, and have detailed documentation. Being able to have enough patience to make yourself comfortable is a necessary part of growing as a developer, as initial documentation/codebase would always seem alien and big enough to trigger that flight reaction!&lt;/p&gt;
    &lt;p&gt;Batching and Caching are two generic strategies that could be applied to speed up most of bottleneck portions. Both strategies lead to better/denser utilization of CPUs by (trying to) minimise the costly load/store instructions during a hot loop. Batching for example could do it by allocating necessary memory up-front for a batch and de-allocating all at once when no longer required, reducing the number of costly system-calls. Caching may involve designing or using a (hardware)cache friendly data-structure, when it is possible to do so.&lt;/p&gt;
    &lt;p&gt;Each optimization would involve assumptions and each subsequent optimization would become harder and harder to implement, may preventing the clean refactoring of code when future functionalities may need to be accommodated. It itself is a kind of rabbit-hole, and user should know when to stop as there would always be something else to be optimized!&lt;/p&gt;
    &lt;p&gt;With (coding) tools involving AI/LLMs it is easier than ever to get a piece of desired functionality, as a developer i understand it is another useful tool in a long-history of improvements, that most of developers would come to use in their workflow. Current LLMs have undeniable ability to handle complex instructions, explain non-trivial code and that so for various mixed modalities! It has been a bit unreasonable to end up with such abilities with just next token prediction as primary objective, even for a researcher working in this field. My usage for such tools is only through a (free) search engine(s), Although for now there has been no scenario in such tools have helped me, that i wouldn't have got to using traditional means. But i can admit such tools/engines are really effective in helping us to get unstuck in a variety of situations, arguably helping us to learn faster. &lt;code&gt;Functions/routines&lt;/code&gt; are nice and enough abstractions to provide enough context to such engines, to get the required help, without ever needing &lt;code&gt;review/edit/rewrite&lt;/code&gt; cycle.&lt;lb/&gt; I have always been benefited from visiting the original documentation, if AI is spitting out good enough arguments, there must be a good documentation out there for that topic . Our minds capability to extract abstract patterns resulted from studying one topic and applying it to another seemingly unrelated domain is uncanny to say the least. Also tone/motivation for developer writing about a topic matters to me, and many times i have visited a concept further just because writer himself/herself was very excited about it . Again, these are just personal factors and biases and people should be free to choose workflow they feel most comfortable in , without any judgments from either side.&lt;lb/&gt; It has been difficult to access SOTA models actual abilities, with fewer and fewer details being published for each newer version, but it has been a wild-ride for me to see the evolution from RNNs to bi-directional RNNs to LSTMs to Transformer architecture (finally founding atleast one stable architecture be able to support training on whole internet without exploding or vanishing gradients). Arguably there are also more more open family of models like &lt;code&gt;Qwen&lt;/code&gt; or &lt;code&gt;Deepseek&lt;/code&gt; from other labs which could run on local infrastructure. Even at this stage, ideas behind LLMs are simple enough for anybody to understand without burdening them with terms like AGI . There is already great work from OLMO and Smollm  to build upon and start with, for personal needs, without spending a lot of money. On technical front  there is still much more to explore and it comes down to doing more experiments by smaller companies to prevent ending up with another monopoly/duopoly in this field only to later blame such for their incompetence!&lt;lb/&gt; I literally have no idea what would be the end game with this ever increasing ability of AI models and what social consequences we would end up with in an already fragmented and struggling world. But it would be a mistake to abandon learning, however inconvenient it may seem at any time, if we were to survive ! &lt;lb/&gt; Thing that really boils my blood is these (AI) companies lawless laundering of all the open-source code, art, poetry without any attribution only to be packaged as a product for users to pay for. Constant attacks on all the infrastructure even run by very small or single-developer companies/communities, not respecting any of the &lt;code&gt;robots.txt&lt;/code&gt;, proxying through residential networks, damaging the very core of the information-sharing/internet while coming up with ironical headlines is bordering on criminal-behaviour for me! Waiting for tens of seconds just for a (community written) stack-overflow post through many layers of security, for wanting to understand various perspectives for some concept without all the bullshit summarization, is new bleak reality with nothing for end-users to have a say in.  &lt;/p&gt;
    &lt;p&gt;Despite the dominant usage of LLMs there exist equally interesting smaller models/architectures representing the huge potential that this field of deep-learning holds. Neural-networks allow us to (good enough)model any arbitrary function/flow using an iterative framework from a few thousand samples representing the function space, effectively equipping us with a very power statistical tool Self-supervised learning don't even need explicit outputs, how cool is that.. See https://ai.meta.com/blog/dino-v2-computer-vision-self-supervised-learning/ this work for more information. to introduce a new independent signal to reduce the entropy of the problem in many domains. I am a fan of smaller personalized models' potential to tackle everyday problems, and myself uses cheap off-the-self cameras coupled with a DL model to detect those Damn Monkeys, and for local voice-synthesis. Monkey Capturing was even on the manifesto of one of the candidates at city-level elections! In country like India, where even (traditional) Automation is limited to products of very few big companies, I can't help smiling whenever i point remote at my "AI" controlled AC :)&lt;/p&gt;
    &lt;p&gt;Living in a two-tier town in northern India with very minimal fixed-costs has allowed me to work on this for quite a long time without any savings or continuous financial freedom. But i cannot be a hypocrite about it, as it was a conscious decision to learn, explore and implement some of the ideas i had for some time. In return, this has allowed me to stay in touch with friends, played a lot of outdoor games, and help me in reflecting on the things i would want to spend more time in future.&lt;/p&gt;
    &lt;p&gt;Timely financial grants during the last one and half year from Samagata foundation and FossUnited has allowed me to complete a bulk of work to point, where i am satisfied with the current state of the project, for which i will always be grateful.&lt;/p&gt;
    &lt;p&gt;I would very much like to continue on this or adjacent projects, as there are still a lot of ideas and code pending, to make it a very stable everyday engine for users to use . But for that i will have to figure out a way to sustain this , without ever compromising the Core features/functionality in any way, As those were some of reasons i started working on it in the first place! Extensions to allow indexing remote storage like Google Drive or Android devices smoothly from the app itself seems like a good direction in that regard for now!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46087549</guid><pubDate>Sat, 29 Nov 2025 13:56:04 +0000</pubDate></item><item><title>DNS LOC Record (2014)</title><link>https://blog.cloudflare.com/the-weird-and-wonderful-world-of-dns-loc-records/</link><description>&lt;doc fingerprint="f4b44f95b957b696"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;A cornerstone of CloudFlare's infrastructure is our ability to serve DNS requests quickly and handle DNS attacks. To do both those things we wrote our own authoritative DNS server called RRDNS in Go. Because of it we've been able to fight off DNS attacks, and be consistenly one of the fastest DNS providers on the web.&lt;/p&gt;
      &lt;p&gt;Implementing an authoritative DNS server is a large task. That's in part because DNS is a very old standard (RFC 1035 dates to 1987), in part because as DNS has developed it has grown into a more and more complex system, and in part because what's written in the RFCs and what happens in the real-world aren't always the same thing.&lt;/p&gt;
      &lt;p&gt;One little used type of DNS record is the LOC (or location). It allows you to specify a physical location. CloudFlare handles millions of DNS records; of those just 743 are LOCs. Nevertheless, it's possible to set up a LOC record in the CloudFlare DNS editor.&lt;/p&gt;
      &lt;p&gt;My site geekatlas.com has a LOC record as an Easter Egg. Here's how it's configured in the CloudFlare DNS settings:&lt;/p&gt;
      &lt;p&gt;When you operate at CloudFlare scale the little-used nooks and crannies turn out to be important. And even though there are only 743 LOC records in our entire database, at least one customer contacted support to find out why their LOC record wasn't being served.&lt;/p&gt;
      &lt;p&gt;And that sent me into the RRDNS source code to find out why.&lt;/p&gt;
      &lt;p&gt;The answer was simple. Although RRDNS had code for receiving requests for LOC records, creating response packets containing LOC data, there was a missing link. The CloudFlare DNS server stores the LOC record as a string (such as the &lt;code&gt;33 40 31 N 106 28 29 W 10m&lt;/code&gt; above) and no one had written the code to parse that and turn it into the internal format. Oops.&lt;/p&gt;
      &lt;p&gt;The textual LOC format and the binary, on-the-wire, format are described in RFC 1876 and it's one of many RFCs that updated the original 1987 standard. RFC 1876 is from 1996.&lt;/p&gt;
      &lt;p&gt;The textual format is fairly simple. Here's what the RFC says:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;The LOC record is expressed in a primary file in the following format:

owner TTL class LOC ( d1 [m1 [s1]] {"N"|"S"} d2 [m2 [s2]]
                           {"E"|"W"} alt["m"] [siz["m"] [hp["m"]
                           [vp["m"]]]] )

where:

   d1:     [0 .. 90]            (degrees latitude)
   d2:     [0 .. 180]           (degrees longitude)
   m1, m2: [0 .. 59]            (minutes latitude/longitude)
   s1, s2: [0 .. 59.999]        (seconds latitude/longitude)
   alt:    [-100000.00 .. 42849672.95] BY .01 (altitude in meters)
   siz, hp, vp: [0 .. 90000000.00] (size/precision in meters)

If omitted, minutes and seconds default to zero, size defaults to 1m,
horizontal precision defaults to 10000m, and vertical precision
defaults to 10m.  These defaults are chosen to represent typical
ZIP/postal code area sizes, since it is often easy to find
approximate geographical location by ZIP/postal code.&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;So, there are required latitude, longitude and altitude and three optional values for the size of the location and precision information. Pretty simple.&lt;/p&gt;
      &lt;p&gt;Then there's the on-the-wire format. Unlike a TXT record the LOC record data is parsed and turned into a fixed size binary format. Back to RFC 1876:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;  MSB                                           LSB
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
  0|        VERSION        |         SIZE          |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
  2|       HORIZ PRE       |       VERT PRE        |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
  4|                   LATITUDE                    |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
  6|                   LATITUDE                    |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
  8|                   LONGITUDE                   |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
 10|                   LONGITUDE                   |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
 12|                   ALTITUDE                    |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
 14|                   ALTITUDE                    |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;So, 32 bits of latitude, longitude and altitude and then three 8 bit values for the size and precision. The latitude and longitude values have a pretty simple encoding that treats the 32 bits as an unsigned integer:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;The latitude of the center of the sphere described by the SIZE field, expressed as a 32-bit integer, most significant octet first (network standard byte order), in thousandths of a second of arc.  2^31 represents the equator; numbers above that are north latitude.&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;And the altitude can be below sea-level but still unsigned:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;The altitude of the center of the sphere described by the SIZE field, expressed as a 32-bit integer, most significant octet first (network standard byte order), in centimeters, from a base of 100,000m below the [WGS 84] reference spheroid used by GPS.&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;But the 8 bit values use a very special encoding that allows a wide range of approximate values to be packed into 8 bits and also be human-readable when dumped out in hex!&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;The diameter of a sphere enclosing the described entity, in centimeters, expressed as a pair of four-bit unsigned integers, each ranging from zero to nine, with the most significant four bits representing the base and the second number representing the power of ten by which to multiply the base.  This allows sizes from 0e0 (&amp;lt;1cm) to 9e9 (90,000km) to be expressed.  This representation was chosen such that the hexadecimal representation can be read by eye; 0x15 = 1e5.&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;For example, the value &lt;code&gt;0x12&lt;/code&gt; means &lt;code&gt;1 * 10^2&lt;/code&gt; or 100cm. &lt;code&gt;0x99&lt;/code&gt; means &lt;code&gt;9 * 10^9&lt;/code&gt; or 90,000,000m. The smallest value that can be represented is 1cm (it's &lt;code&gt;0x10&lt;/code&gt;). So, in just 8 bits there's a range values from 1cm to larger than the diameter of Jupiter.&lt;/p&gt;
      &lt;p&gt;To fix this I wrote a parser for the LOC text record type (and associated tests). It can be found here.&lt;/p&gt;
      &lt;p&gt;We've now rolled out the fix and all the existing LOC records are being served by RRDNS. For example, my &lt;code&gt;geekatlas.com&lt;/code&gt; LOC record can be queried like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;$ dig geekatlas.com LOC
; &amp;lt;&amp;lt;&amp;gt;&amp;gt; DiG 9.8.3-P1 &amp;lt;&amp;lt;&amp;gt;&amp;gt; geekatlas.com LOC
;; global options: +cmd
;; Got answer:
;; -&amp;gt;&amp;gt;HEADER&amp;lt;&amp;lt;- opcode: QUERY, status: NOERROR, id: 2997
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:    
;geekatlas.com.         IN  LOC

;; ANSWER SECTION:
geekatlas.com.      299 IN  LOC 33 40 31.000 N 106 28 29.000 W 10.00m 1m 10000m 10m

;; Query time: 104 msec
;; SERVER: 192.168.14.1#53(192.168.14.1)
;; WHEN: Tue Apr  1 14:13:48 2014
;; MSG SIZE  rcvd: 59&lt;/code&gt;
      &lt;/quote&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46087596</guid><pubDate>Sat, 29 Nov 2025 14:02:23 +0000</pubDate></item><item><title>It's Always the Process, Stupid</title><link>https://its.promp.td/its-always-the-process-stupid/</link><description>&lt;doc fingerprint="623b13397f7867cd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;It’s Always the Process, Stupid!&lt;/head&gt;
    &lt;p&gt;Why AI Won’t Save Your Broken Workflow&lt;/p&gt;
    &lt;p&gt;Let’s rip the Band-Aid off immediately: If your underlying business process is a mess, sprinkling "AI dust" on it won’t turn it into gold. It will just speed up the rate at which you generate garbage.&lt;/p&gt;
    &lt;p&gt;In the world of Business IT, we get seduced by the shiny new toy. Right now, that toy is Artificial Intelligence. Boardrooms are buzzing with buzzwords like LLMs, agentic workflows, and generative reasoning. Executives are frantically asking, "What is our AI strategy?"&lt;/p&gt;
    &lt;p&gt;But here is the hard truth:&lt;/p&gt;
    &lt;quote&gt;There is no such thing as an AI strategy.&lt;lb/&gt;There is only Business Process Optimization (BPO).&lt;/quote&gt;
    &lt;head rend="h3"&gt;The "Magic Wand" Fallacy&lt;/head&gt;
    &lt;p&gt;Too many enterprises treat AI like a magic wand. They believe that by implementing a sophisticated neural network, their structural inefficiencies will vanish. They think AI brings intelligence.&lt;/p&gt;
    &lt;p&gt;It doesn’t.&lt;/p&gt;
    &lt;p&gt;Like every major technological shift before it—from the steam engine to the spreadsheet—AI does not inherently make an organization smarter. AI, like any other tool, only makes faster.&lt;/p&gt;
    &lt;p&gt;If you automate a stupid decision, you just make stupid decisions at light speed. If you apply an agentic AI workflow to a bureaucratic nightmare of an approval chain, you haven't fixed the bureaucracy; you’ve just built a robot that hates its job as much as your employees do.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Unstructured Data Trap&lt;/head&gt;
    &lt;p&gt;There is, however, one superpower AI possesses that previous tools lacked: It is the first technology that is truly useful for handling unstructured data.&lt;/p&gt;
    &lt;p&gt;For decades, traditional software demanded structure. Rows, columns, booleans, and fixed fields. If data didn't fit the box, the computer couldn't read it.&lt;/p&gt;
    &lt;p&gt;AI changes this. It can read messy emails, interpret vague Slack messages, parse PDFs, and analyze images. But this capability exposes a massive, hidden problem in most enterprises.&lt;/p&gt;
    &lt;p&gt;Processes that rely on unstructured data are usually unstructured processes.&lt;/p&gt;
    &lt;p&gt;Because computers couldn't handle the mess, humans handled it (before AI). And humans don't always follow a flow chart. These processes—like "handling a complex customer complaint" or "brainstorming a marketing campaign"—are often ad-hoc, intuitive, and completely undocumented. They live in the heads of your senior staff, not in your SOPs.&lt;/p&gt;
    &lt;head rend="h3"&gt;You Can’t Automate What You Haven’t Designed&lt;/head&gt;
    &lt;p&gt;This brings us back to BPO. You cannot apply AI to these "hidden" processes until you drag them into the light.&lt;/p&gt;
    &lt;p&gt;If you want to use AI to process unstructured data, you must first bring structure to the workflow itself. You need to improve your process design to account for the ambiguity that AI handles.&lt;/p&gt;
    &lt;p&gt;Ask yourself:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;What is the trigger? (Where does the unstructured mess come from?)&lt;/item&gt;
      &lt;item&gt;What is the transformation? (What exactly is the human—or now the AI—supposed to extract or deduce from that mess?)&lt;/item&gt;
      &lt;item&gt;What is the structured output? (How does this flow back into your rigid ERP or CRM systems?)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Speed vs. Intelligence&lt;/head&gt;
    &lt;p&gt;Let’s clarify the distinction between "smarter" and "faster."&lt;/p&gt;
    &lt;p&gt;Intelligence implies wisdom, context, and nuance. While AI models are simulating reasoning better every day, in a business context, they are fundamentally pattern-matching engines. They excel at acceleration.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Old Way: An analyst reads 50 contracts (unstructured), highlights risks based on gut feeling (unstructured process), and summarizes them in 3 days.&lt;/item&gt;
      &lt;item&gt;The AI Way: An AI scans 50 contracts and extracts specific risk clauses based on defined parameters in 3 minutes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The process (Review Contracts -&amp;gt; Identify Risk -&amp;gt; Summarize) hasn't changed, but it had to be rigorously defined for the AI to work. The intelligence (knowing what a "risk" actually means) still requires human governance. What has changed is the velocity.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Bottom Line&lt;/head&gt;
    &lt;p&gt;Stop chasing the hype. Stop looking for a specialized "AI Savior."&lt;/p&gt;
    &lt;p&gt;Go back to the whiteboard. Map out your value chain—especially the messy, human-centric parts involving unstructured data that you previously ignored. Find the bottlenecks. Identify the waste.&lt;/p&gt;
    &lt;p&gt;Once you have a streamlined, logical, and robust business process, then apply AI to hit the accelerator.&lt;/p&gt;
    &lt;p&gt;Technology changes. &lt;lb/&gt;The rules of business efficiency do not. &lt;lb/&gt;It’s always the process, stupid!&lt;lb/&gt;And that's where actual AI Tools are missing that point, because they weren't build for that &lt;/p&gt;
    &lt;p&gt;Live long and prosper 😉🖖&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46087737</guid><pubDate>Sat, 29 Nov 2025 14:20:46 +0000</pubDate></item><item><title>Iceland declares ocean-current instability a national security risk</title><link>https://edition.cnn.com/2025/11/15/climate/iceland-warming-current-amoc-collapse-threat</link><description>&lt;doc fingerprint="fdc699ff6e34bfd9"&gt;
  &lt;main&gt;
    &lt;p&gt;Iceland’s relatively mild climate is shaped by a crucial network of currents that winds its away around the Atlantic Ocean transporting heat northward — without it, the island would be much icier and stormier. As evidence mounts these currents could be on course for collapse, Iceland’s government has made the unusual move of designating the risk a national security threat, prompting a a high-level response into how to prepare for this “existential threat.”&lt;/p&gt;
    &lt;p&gt;“Our climate, economy and security are deeply tied to the stability of the ocean currents around us,” said Jóhann Páll Jóhannsson, Iceland’s Minister for Environment, Energy and Climate.&lt;/p&gt;
    &lt;p&gt;The Atlantic Meridional Overturning Circulation — known as the AMOC — is a looping system of currents that works like a giant conveyor belt, pulling warm water from the Southern Hemisphere and tropics to the Northern Hemisphere, where it cools, sinks and flows back south.&lt;/p&gt;
    &lt;p&gt;When scientists are asked which potential climate impact terrifies them most, the collapse of the AMOC is often top of the list.&lt;/p&gt;
    &lt;p&gt;A growing body of research points to the AMOC slowing down, as higher global temperatures disrupt the delicate balance of heat and salinity on which its strength relies. The science is still unsettled on the likelihood and timing of any collapse, but some studies have projected it could be on course to happen this century.&lt;/p&gt;
    &lt;p&gt;A shutdown of the AMOC “cannot be considered a low likelihood risk anymore in view of the evolving science over the past years,” said Stefan Rahmstorf, a physical oceanographer and climatologist who has studied the AMOC at Potsdam University in Germany.&lt;/p&gt;
    &lt;p&gt;The impacts would be catastrophic — ushering in huge global weather and climate shifts, including rising sea levels in parts of the US and Europe, disrupted monsoon systems affecting countries in Asia and Africa, and a winter deep freeze in Europe, with sea ice potentially creeping southward as far as the United Kingdom.&lt;/p&gt;
    &lt;p&gt;Iceland “would be close to the center of a serious regional cooling,” meaning the country could be surrounded by sea ice, Rahmstorf told CNN.&lt;/p&gt;
    &lt;p&gt;It’s an “an existential threat,” Jóhannsson told CNN. The AMOC regulates Iceland’s weather, and its collapse could devastate infrastructure, transport and vital industries including fishing, he said.&lt;/p&gt;
    &lt;p&gt;Jóhannsson briefed the government on the latest science after research published in August raised “grave concerns” about the AMOC’s future stability. In September, Iceland’s National Security Council designated the current’s potential collapse as a national security risk, marking the first time a climate impact has received this designation in the country.&lt;/p&gt;
    &lt;p&gt;The decision “reflects the seriousness of the issue and ensures that the matter gets the attention it deserves,” Jóhannsson said. In practice, the designation will mean a high-level, coordinated government response to understand the threat and work out how to prevent and mitigate the worst consequences, he said.&lt;/p&gt;
    &lt;p&gt;Rahmstorf commended Iceland for its decision and said other countries should follow suit. The impacts of an AMOC collapse would be felt across the globe. Scientists are trying to understand the full range of potential impacts on societies and economies, but research has pointed to destroyed crops and catastrophic flooding.&lt;/p&gt;
    &lt;p&gt;Iceland’s decision marks a shift in how the country understands climate risks, Jóhannsson said.&lt;/p&gt;
    &lt;p&gt;“What we do know is that the current climate might change so drastically that it could become impossible for us to adapt,” he said. “In short, this is not just a scientific concern — it’s a matter of national survival and security.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46088192</guid><pubDate>Sat, 29 Nov 2025 15:21:29 +0000</pubDate></item><item><title>Major AI conference flooded with peer reviews written by AI</title><link>https://www.nature.com/articles/d41586-025-03506-6</link><description>&lt;doc fingerprint="ad80911009ab07aa"&gt;
  &lt;main&gt;
    &lt;p&gt;Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript.&lt;/p&gt;
    &lt;p&gt;What can researchers do if they suspect that their manuscripts have been peer reviewed using artificial intelligence (AI)? Dozens of academics have raised concerns on social media about manuscripts and peer reviews submitted to the organizers of next year’s International Conference on Learning Representations (ICLR), an annual gathering of specialists in machine learning. Among other things, they flagged hallucinated citations and suspiciously long and vague feedback on their work.&lt;/p&gt;
    &lt;p&gt;Graham Neubig, an AI researcher at Carnegie Mellon University in Pittsburgh, Pennsylvania, was one of those who received peer reviews that seemed to have been produced using large language models (LLMs). The reports, he says, were “very verbose with lots of bullet points” and requested analyses that were not “the standard statistical analyses that reviewers ask for in typical AI or machine-learning papers.”&lt;/p&gt;
    &lt;p&gt;But Neubig needed help proving that the reports were AI-generated. So, he posted on X (formerly Twitter) and offered a reward for anyone who could scan all the conference submissions and their peer reviews for AI-generated text. The next day, he got a response from Max Spero, chief executive of Pangram Labs in New York City, which develops tools to detect AI-generated text. Pangram screened all 19,490 studies and 75,800 peer reviews submitted for ICLR 2026, which will take place in Rio de Janeiro, Brazil, in April. Neubig and more than 11,000 other AI researchers will be attending.&lt;/p&gt;
    &lt;p&gt;Pangram’s analysis revealed that around 21% of the ICLR peer reviews were fully AI-generated, and more than half contained signs of AI use. The findings were posted online by Pangram Labs. “People were suspicious, but they didn’t have any concrete proof,” says Spero. “Over the course of 12 hours, we wrote some code to parse out all of the text content from these paper submissions,” he adds.&lt;/p&gt;
    &lt;p&gt;The conference organizers say they will now use automated tools to assess whether submissions and peer reviews breached policies on using AI in submissions and peer reviews. This is the first time that the conference has faced this issue at scale, says Bharath Hariharan, a computer scientist at Cornell University in Ithaca, New York, and senior programme chair for ICLR 2026. “After we go through all this process … that will give us a better notion of trust.”&lt;/p&gt;
    &lt;p&gt;AI-written peer review&lt;/p&gt;
    &lt;p&gt;The Pangram team used one of its own tools, which predicts whether text is generated or edited by LLMs. Pangram’s analysis flagged 15,899 peer reviews that were fully AI-generated. But it also identified many manuscripts that had been submitted to the conference with suspected cases of AI-generated text: 199 manuscripts (1%) were found to be fully AI-generated; 61% of submissions were mostly human-written; but 9% contained more than 50% AI-generated text.&lt;/p&gt;
    &lt;p&gt;Pangram described the model in a preprint1, which it submitted to ICLR 2026. Of the four peer reviews received for the manuscript, one was flagged as fully AI-generated and another as lightly AI-edited, the team’s analysis found.&lt;/p&gt;
    &lt;p&gt;For many researchers who received peer reviews for their submissions to ICLR, the Pangram analysis confirmed what they had suspected. Desmond Elliott, a computer scientist at the University of Copenhagen, says that one of three reviews he received seemed to have missed “the point of the paper”. His PhD student who led the work suspected that the review was generated by LLMs, because it mentioned numerical results from the manuscript that were incorrect and contained odd expressions.&lt;/p&gt;
    &lt;p&gt;When Pangram released its findings, Elliott adds, “the first thing I did was I typed in the title of our paper because I wanted to know whether my student’s gut instinct was correct”. The suspect peer review, which Pangram’s analysis flagged as fully AI-generated, gave the manuscript the lowest rating, leaving it “on the borderline between accept and reject”, says Elliott. “It's deeply frustrating”.&lt;/p&gt;
    &lt;p&gt;Repercussions&lt;/p&gt;
    &lt;p&gt;Enjoying our latest content? Log in or create an account to continue&lt;/p&gt;
    &lt;p&gt;Access the most recent journalism from Nature's award-winning team&lt;/p&gt;
    &lt;p&gt;Explore the latest features &amp;amp; opinion covering groundbreaking research&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46088236</guid><pubDate>Sat, 29 Nov 2025 15:26:58 +0000</pubDate></item><item><title>We're learning more about what Vitamin D does to our bodies</title><link>https://www.technologyreview.com/2025/11/21/1128206/vitamin-d-bodies-bone-health-immune/</link><description>&lt;doc fingerprint="b2eb1b17298bc193"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;We’re learning more about what vitamin D does to our bodies&lt;/head&gt;
    &lt;p&gt;The sunshine vitamin could affect your immune system and heart health.&lt;/p&gt;
    &lt;p&gt;It has started to get really wintry here in London over the last few days. The mornings are frosty, the wind is biting, and it’s already dark by the time I pick my kids up from school. The darkness in particular has got me thinking about vitamin D, a.k.a. the sunshine vitamin.&lt;/p&gt;
    &lt;p&gt;At a checkup a few years ago, a doctor told me I was deficient in vitamin D. But he wouldn’t write me a prescription for supplements, simply because, as he put it, everyone in the UK is deficient. Putting the entire population on vitamin D supplements would be too expensive for the country’s national health service, he told me.&lt;/p&gt;
    &lt;p&gt;But supplementation—whether covered by a health-care provider or not—can be important. As those of us living in the Northern Hemisphere spend fewer of our waking hours in sunlight, let’s consider the importance of vitamin D.&lt;/p&gt;
    &lt;p&gt;Yes, it is important for bone health. But recent research is also uncovering surprising new insights into how the vitamin might influence other parts of our bodies, including our immune systems and heart health.&lt;/p&gt;
    &lt;p&gt;Vitamin D was discovered just over 100 years ago, when health professionals were looking for ways to treat what was then called “the English disease.” Today, we know that rickets, a weakening of bones in children, is caused by vitamin D deficiency. And vitamin D is best known for its importance in bone health.&lt;/p&gt;
    &lt;p&gt;That’s because it helps our bodies absorb calcium. Our bones are continually being broken down and rebuilt, and they need calcium for that rebuilding process. Without enough calcium, bones can become weak and brittle. (Depressingly, rickets is still a global health issue, which is why there is global consensus that infants should receive a vitamin D supplement at least until they are one year old.)&lt;/p&gt;
    &lt;p&gt;In the decades since then, scientists have learned that vitamin D has effects beyond our bones. There’s some evidence to suggest, for example, that being deficient in vitamin D puts people at risk of high blood pressure. Daily or weekly supplements can help those individuals lower their blood pressure.&lt;/p&gt;
    &lt;p&gt;A vitamin D deficiency has also been linked to a greater risk of “cardiovascular events” like heart attacks, although it’s not clear whether supplements can reduce this risk; the evidence is pretty mixed.&lt;/p&gt;
    &lt;p&gt;Vitamin D appears to influence our immune health, too. Studies have found a link between low vitamin D levels and incidence of the common cold, for example. And other research has shown that vitamin D supplements can influence the way our genes make proteins that play important roles in the way our immune systems work.&lt;/p&gt;
    &lt;p&gt;We don’t yet know exactly how these relationships work, however. And, unfortunately, a recent study that assessed the results of 37 clinical trials found that overall, vitamin D supplements aren’t likely to stop you from getting an “acute respiratory infection.”&lt;/p&gt;
    &lt;p&gt;Other studies have linked vitamin D levels to mental health, pregnancy outcomes, and even how long people survive after a cancer diagnosis. It’s tantalizing to imagine that a cheap supplement could benefit so many aspects of our health.&lt;/p&gt;
    &lt;p&gt;But, as you might have gathered if you’ve got this far, we’re not quite there yet. The evidence on the effects of vitamin D supplementation for those various conditions is mixed at best.&lt;/p&gt;
    &lt;p&gt;In fairness to researchers, it can be difficult to run a randomized clinical trial for vitamin D supplements. That’s because most of us get the bulk of our vitamin D from sunlight. Our skin converts UVB rays into a form of the vitamin that our bodies can use. We get it in our diets, too, but not much. (The main sources are oily fish, egg yolks, mushrooms, and some fortified cereals and milk alternatives.)&lt;/p&gt;
    &lt;p&gt;The standard way to measure a person’s vitamin D status is to look at blood levels of 25-hydroxycholecalciferol (25(OH)D), which is formed when the liver metabolizes vitamin D. But not everyone can agree on what the “ideal” level is.&lt;/p&gt;
    &lt;p&gt;Even if everyone did agree on a figure, it isn’t obvious how much vitamin D a person would need to consume to reach this target, or how much sunlight exposure it would take. One complicating factor is that people respond to UV rays in different ways—a lot of that can depend on how much melanin is in your skin. Similarly, if you’re sitting down to a meal of oily fish and mushrooms and washing it down with a glass of fortified milk, it’s hard to know how much more you might need.&lt;/p&gt;
    &lt;p&gt;There is more consensus on the definition of vitamin D deficiency, though. (It’s a blood level below 30 nanomoles per liter, in case you were wondering.) And until we know more about what vitamin D is doing in our bodies, our focus should be on avoiding that.&lt;/p&gt;
    &lt;p&gt;For me, that means topping up with a supplement. The UK government advises everyone in the country to take a 10-microgram vitamin D supplement over autumn and winter. That advice doesn’t factor in my age, my blood levels, or the amount of melanin in my skin. But it’s all I’ve got for now.&lt;/p&gt;
    &lt;head rend="h3"&gt;Deep Dive&lt;/head&gt;
    &lt;head rend="h3"&gt;Biotechnology and health&lt;/head&gt;
    &lt;head rend="h3"&gt;Microsoft says AI can create “zero day” threats in biology&lt;/head&gt;
    &lt;p&gt;Artificial intelligence can design toxins that evade security controls.&lt;/p&gt;
    &lt;head rend="h3"&gt;The race to make the perfect baby is creating an ethical mess&lt;/head&gt;
    &lt;p&gt;A new field of science claims to be able to predict aesthetic traits, intelligence, and even moral character in embryos. Is this the next step in human evolution or something more dangerous?&lt;/p&gt;
    &lt;head rend="h3"&gt;How do our bodies remember?&lt;/head&gt;
    &lt;p&gt;The more we move, the more our muscle cells begin to make a memory of that exercise.&lt;/p&gt;
    &lt;head rend="h3"&gt;How aging clocks can help us understand why we age—and if we can reverse it&lt;/head&gt;
    &lt;p&gt;When used correctly, they can help us unpick some of the mysteries of our biology, and our mortality.&lt;/p&gt;
    &lt;head rend="h3"&gt;Stay connected&lt;/head&gt;
    &lt;head rend="h2"&gt;Get the latest updates from&lt;lb/&gt;MIT Technology Review&lt;/head&gt;
    &lt;p&gt;Discover special offers, top stories, upcoming events, and more.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46088998</guid><pubDate>Sat, 29 Nov 2025 16:58:58 +0000</pubDate></item><item><title>AccessOwl (YC S22) Is Hiring a Technical Account Manager (IAM)</title><link>https://www.ycombinator.com/companies/accessowl/jobs/dGC3pcO-technical-account-manager-identity-access-management</link><description>&lt;doc fingerprint="1550a21a95a2b36c"&gt;
  &lt;main&gt;
    &lt;p&gt;Managing your Employees' Access to SaaS&lt;/p&gt;
    &lt;p&gt;TL;DR: You want to work directly with IT and security teams at some of the world’s fastest growing companies, helping them implement, optimize, and expand AccessOwl. You enjoy solving technical problems, guiding integrations, and shaping how a startup delivers value after the deal closes.&lt;/p&gt;
    &lt;p&gt;AccessOwl is building the first AI native Access Governance Suite. We make it radically easier for IT and security teams to manage SaaS access, stay compliant, and eliminate shadow IT, without the overhead of traditional identity systems.&lt;/p&gt;
    &lt;p&gt;We founded AccessOwl out of frustration with manual onboarding, offboarding, and compliance workflows that slow companies down. By combining automation with agentic AI, we are redefining how modern IT admins govern SaaS.&lt;/p&gt;
    &lt;p&gt;We are a profitable, Y Combinator backed startup working with companies like Harvey AI, Monarch, and Motion. Our team is customer centric, pragmatic, and ambitious.&lt;/p&gt;
    &lt;p&gt;To apply, include three sentences on what personally got you interested in talking to us. Skip the generic stuff. We want to hear your real motivation.&lt;/p&gt;
    &lt;p&gt;AccessOwl revolutionizes how businesses manage SaaS applications. Our mission is to simplify SaaS access, spending, and compliance, providing the easiest way to centrally manage apps and user access. AccessOwl replaces Okta, outdated ticketing systems, and spreadsheets, fundamentally transforming how modern IT admins work.&lt;/p&gt;
    &lt;p&gt;We founded AccessOwl out of frustration with the inefficiency caused by SaaS companies exploiting the SSO Tax, which made onboarding and offboarding manual and time-consuming. Our innovative approach leverages RPA and agentic AI workflows to change this.&lt;/p&gt;
    &lt;p&gt;We are a fully remote, customer-centric team dedicated to solving real problems for IT and security teams. Our goal is to build a sustainable business while delivering an exceptional experience our customers genuinely love.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46089008</guid><pubDate>Sat, 29 Nov 2025 17:00:23 +0000</pubDate></item><item><title>Zero knowlege proof of compositeness</title><link>https://www.johndcook.com/blog/2025/11/29/zkp-composite/</link><description>&lt;doc fingerprint="dedcb3f5bedbd58"&gt;
  &lt;main&gt;
    &lt;p&gt;A zero knowledge proof (ZKP) answers a question without revealing anything more than answer. For example, a digital signature proves your possession of a private key without revealing that key.&lt;/p&gt;
    &lt;p&gt;Here’s another example, one that’s more concrete than a digital signature. Suppose you have a deck of 52 cards, 13 of each of spades, hearts, diamonds, and clubs. If I draw a spade from the deck, I can prove that I drew a spade without showing which card I drew. If I show you that all the hearts, diamonds, and clubs are still in the deck, then you know that the missing card must be a spade.&lt;/p&gt;
    &lt;head rend="h2"&gt;Composite numbers&lt;/head&gt;
    &lt;p&gt;You can think of Fermat’s primality test as a zero knowledge proof. For example, I can convince you that the following number is composite without telling you what its factors are.&lt;/p&gt;
    &lt;p&gt;n = 244948974278317817239218684105179099697841253232749877148554952030873515325678914498692765804485233435199358326742674280590888061039570247306980857239550402418179621896817000856571932268313970451989041&lt;/p&gt;
    &lt;p&gt;Fermat’s little theorem says that if n is a prime and b is not a multiple of n, then&lt;/p&gt;
    &lt;p&gt;bn−1 = 1 (mod n).&lt;/p&gt;
    &lt;p&gt;A number b such that bn−1 ≠ 1 (mod n) is a proof that n is not prime, i.e. n is composite. So, for example, b = 2 is a proof that n above is composite. This can be verified very quickly using Python:&lt;/p&gt;
    &lt;quote&gt;&amp;gt;&amp;gt;&amp;gt; pow(2, n-1, n) 10282 ... 4299&lt;/quote&gt;
    &lt;p&gt;I tried the smallest possible base [1] and it worked. In general you may have to try a few bases. And for a few rare numbers (Carmichael numbers) you won’t be able to find a base. But if you do find a base b such that bn−1 is not congruent to 1 mod n, you know with certainty that b is composite.&lt;/p&gt;
    &lt;head rend="h2"&gt;Prime numbers&lt;/head&gt;
    &lt;p&gt;The converse of Fermat’s little theorem is false. It can be used to prove a number is not prime, but it cannot prove that a number is prime. But it can be used to show that a number is probably prime. (There’s some subtlety as to what it means for a number to probably be prime. See here.)&lt;/p&gt;
    &lt;p&gt;Fermat’s little theorem can give you a zero knowledge proof that a number is composite. Can it give you a zero knowledge proof that a number is prime? There are a couple oddities in this question.&lt;/p&gt;
    &lt;p&gt;First, what would it mean to have a zero knowledge proof that a number is prime? What knowledge are you keeping secret? When you prove that a number is composite, the prime factors are secret (or even unknown), but what’s the secret when you say a number is prime? Strictly speaking a ZKP doesn’t have to keep anything secret, but in practice it always does.&lt;/p&gt;
    &lt;p&gt;Second, what about the probability of error? Zero knowledge proofs do not have to be infallible. A ZKP can have some negligible probability of error, and usually do.&lt;/p&gt;
    &lt;p&gt;It’s not part of the definition, but n practice ZKPs are supposed to be easier to verify than the direct approach to what they prove. So you could have something like a primality certificate that takes far less computation to verify than the computation needed to determine from scratch that a number is prime.&lt;/p&gt;
    &lt;head rend="h2"&gt;Proving other things&lt;/head&gt;
    &lt;p&gt;You could think of non-constructive proofs as ZKPs. For example, you could think of the intermediate value theorem as a ZKP: it proves that a function has a zero in an interval without giving you any information about where that zero may be located.&lt;/p&gt;
    &lt;p&gt;What makes ZKPs interesting in application is that they can prove things of more general interest than mathematical statements [2]. For example, cryptocurrencies can provide ZKPs that accounting constraints hold without revealing the inputs or outputs of the transaction. You could prove that nobody tried to spend a negative amount and that the sum of the inputs equals the sum of the outputs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Related posts&lt;/head&gt;
    &lt;p&gt;[1] You could try b = 1, but then bn−1 is always 1. This example shows that the existence of a base for which bn−1 = 1 (mod n) doesn’t prove anything.&lt;/p&gt;
    &lt;p&gt;[2] You might object that accounting rules are mathematical statements, and of course they are. But they’re of little interest to mathematicians and of great interest to the parties in a transaction.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46089394</guid><pubDate>Sat, 29 Nov 2025 17:53:57 +0000</pubDate></item><item><title>Student Perceptions of AI Coding Assistants in Learning</title><link>https://arxiv.org/abs/2507.22900</link><description>&lt;doc fingerprint="ac7d350b9de3148f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Human-Computer Interaction&lt;/head&gt;&lt;p&gt; [Submitted on 26 Jun 2025 (v1), last revised 16 Sep 2025 (this version, v4)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:New Kid in the Classroom: Exploring Student Perceptions of AI Coding Assistants&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:The arrival of AI coding assistants in educational settings presents a paradigm shift, introducing a "new kid in the classroom" for both students and instructors. Thus, understanding the perceptions of these key actors about this new dynamic is critical. This exploratory study contributes to this area by investigating how these tools are shaping the experiences of novice programmers in an introductory programming course. Through a two-part exam, we investigated student perceptions by first providing access to AI support for a programming task and then requiring an extension of the solution without it. We collected Likert-scale and open-ended responses from 20 students to understand their perceptions on the challenges they faced. Our findings reveal that students perceived AI tools as helpful for grasping code concepts and boosting their confidence during the initial development phase. However, a noticeable difficulty emerged when students were asked to work unaided, pointing to potential overreliance and gaps in foundational knowledge transfer. These insights highlight a critical need for new pedagogical approaches that integrate AI effectively while effectively enhancing core programming skills, rather than impersonating them.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Sergio Rojas-Galeano [view email]&lt;p&gt;[v1] Thu, 26 Jun 2025 05:59:23 UTC (1,224 KB)&lt;/p&gt;&lt;p&gt;[v2] Tue, 26 Aug 2025 03:23:41 UTC (1,223 KB)&lt;/p&gt;&lt;p&gt;[v3] Wed, 10 Sep 2025 18:10:35 UTC (1,223 KB)&lt;/p&gt;&lt;p&gt;[v4] Tue, 16 Sep 2025 15:09:44 UTC (1,222 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46089546</guid><pubDate>Sat, 29 Nov 2025 18:14:21 +0000</pubDate></item><item><title>An update on the Farphone's battery</title><link>https://far.computer/battery-update/</link><description>&lt;doc fingerprint="b7f9a493f30e3f33"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;an update on the farphone's battery&lt;/head&gt;
    &lt;p&gt;28/11/2025&lt;/p&gt;
    &lt;p&gt;earlier this week i shared the how-to for this website, which is running on an old smartphone&lt;/p&gt;
    &lt;p&gt;folks on lobste.rs and hacker news pointed out the fire risk of keeping a lithium battery plugged in all the time&lt;/p&gt;
    &lt;p&gt;french blogger korben even mentioned it in an article about the device&lt;/p&gt;
    &lt;p&gt;the fairphone 2 being as modular as it is, it was trivial to open the phone, pop the battery out, and duct-tape the micro-usb charging cable to the phone to avoid accidental dos&lt;/p&gt;
    &lt;p&gt;forty seconds later the farphone had booted and the web server was back up&lt;/p&gt;
    &lt;p&gt;thank you lobsters, thank you hackers, you might have prevented a tragic server explosion&lt;/p&gt;
    &lt;p&gt;built by louis merlin under the cc by-nc-sa 4.0 license&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46089579</guid><pubDate>Sat, 29 Nov 2025 18:19:29 +0000</pubDate></item><item><title>Show HN: Zero-power photonic language model–code</title><link>https://zenodo.org/records/17764289</link><description>&lt;doc fingerprint="91c16a8d2bf86a00"&gt;
  &lt;main&gt;
    &lt;p&gt; Published November 29, 2025 | Version v1 &lt;/p&gt;
    &lt;p&gt; Technical note Open &lt;/p&gt;
    &lt;head rend="h1"&gt;Entropica: 1024-mode unitary evolution with Born-rule readout, trained on TinyStories in under 2 hours on a laptop GPU&lt;/head&gt;
    &lt;head rend="h3"&gt;Creators&lt;/head&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;p&gt;Entropica is the first generative language model whose forward pass is physically realizable as a passive linear-optical interferometer (zero electrical power during inference). &lt;lb/&gt;A 1024-mode, 32-layer unitary network using only Reck-scheme MZI meshes and Born-rule readout learns coherent TinyStories-style generation in under 1.8 hours on a single laptop GPU. &lt;lb/&gt;Model is ready for a full optical implementation path demonstrated with printed phase masks and a $30 laser pointer. &lt;lb/&gt;All code, weights, and dataset generation scripts are public.&lt;/p&gt;
    &lt;head rend="h2"&gt;Files&lt;/head&gt;
    &lt;head rend="h3"&gt; Entropica - Zero-power optical language model.pdf &lt;/head&gt;
    &lt;head rend="h3"&gt; Files (326.4 kB) &lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;cell role="head"&gt;Download all&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt; md5:fb1c5584c2ff2fb41ff68d7abc766414 &lt;/cell&gt;
        &lt;cell&gt;326.4 kB&lt;/cell&gt;
        &lt;cell&gt;Preview Download&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Additional details&lt;/head&gt;
    &lt;head rend="h3"&gt;Identifiers&lt;/head&gt;
    &lt;head rend="h3"&gt; Software &lt;/head&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Repository URL&lt;/item&gt;
      &lt;item rend="dd-1"&gt;https://github.com/dwallener/EntropicaPublic&lt;/item&gt;
      &lt;item rend="dt-2"&gt;Programming language&lt;/item&gt;
      &lt;item rend="dd-2"&gt;Python&lt;/item&gt;
      &lt;item rend="dt-3"&gt;Development Status&lt;/item&gt;
      &lt;item rend="dd-3"&gt;Active&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46089764</guid><pubDate>Sat, 29 Nov 2025 18:45:49 +0000</pubDate></item><item><title>Electric vehicle sales are booming in South America – without Tesla</title><link>https://www.reuters.com/sustainability/climate-energy/electric-vehicle-sales-are-booming-south-america-without-tesla-2025-11-17/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46089971</guid><pubDate>Sat, 29 Nov 2025 19:12:48 +0000</pubDate></item><item><title>Framework Computer Now Sponsoring LVFS / Fwupd Development</title><link>https://www.phoronix.com/news/Framework-Sponsoring-LVFS</link><description>&lt;doc fingerprint="163eda1118c391d8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Framework Computer Now Sponsoring LVFS / Fwupd Development&lt;/head&gt;
    &lt;p&gt;Red Hat in employing lead developer Richard Hughes has contributed the most to LVFS/Fwupd's success, the Linux Foundation has also hosted the project since it has shifted into their umbrella, AMD's Mario Limonciello is among the significant contributors, and now Framework Computer is a new sponsor to the project.&lt;/p&gt;
    &lt;p&gt;Richard Hughes posted on Mastodon on Friday:&lt;/p&gt;
    &lt;quote&gt;"I'm also happy to announce we've got a new sponsor for the LVFS: #Framework&lt;lb/&gt;Although there are about a half a dozen OEMs that have promised to sponsor LVFS, Framework is the first to have actually signed the paperwork."&lt;/quote&gt;
    &lt;p&gt;The graphic on the Fwupd.org page shows Framework as a Startup Sponsor, which puts them in the ballpark of around $10k USD in annual dues to the project.&lt;/p&gt;
    &lt;p&gt;Richard Hughes also followed up with a comment on the positive impact that Framework has also applied on their suppliers around Fwupd/LVFS support too:&lt;/p&gt;
    &lt;quote&gt;"I also wanted to say a huge thanks to Framework, not just for the sponsorship -- but also for the pressure they've put on their suppliers to support fwupd and the LVFS."&lt;/quote&gt;
    &lt;p&gt;Framework has already been supporting LVFS/Fwupd with their devices and ensuring good Linux support in general, such as with the recent Framework 16 laptop upgrade.&lt;/p&gt;
    &lt;p&gt;Hopefully the sponsorship agreements with the other major OEMs pan out and that these other vendors also continue ramping up in mandating LVFS/Fwupd hardware support for ensuring a better firmware updating experience for Linux customers.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46089980</guid><pubDate>Sat, 29 Nov 2025 19:14:13 +0000</pubDate></item><item><title>Ported freetype, fontconfig, harfbuzz, and graphite to Fil-C</title><link>https://twitter.com/filpizlo/status/1994563191528198653</link><description>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46090009</guid><pubDate>Sat, 29 Nov 2025 19:18:46 +0000</pubDate></item><item><title>Scientists may have solved why this ancient, advanced civilization vanished</title><link>https://www.washingtonpost.com/climate-environment/2025/11/27/indus-river-valley-civilization-climate-drought/</link><description>&lt;doc fingerprint="93d11353264083b8"&gt;
  &lt;main&gt;
    &lt;p&gt;At its peak, the ancient Indus River Valley civilization featured gridded streets, multistory brick homes, flush toilets and bustling shops. Its people traded gold, precious stones and items such as bronze carts along the region’s waterways. Others carved detailed human figurines and molded clay toys. They grew wheat, barley and cotton, and crafted tools to bring water for crops from nearby rivers.&lt;/p&gt;
    &lt;p&gt;Democracy Dies in Darkness&lt;/p&gt;
    &lt;head rend="h1"&gt;Scientists may have solved why this ancient, advanced civilization vanished&lt;/head&gt;
    &lt;p&gt;Climate data offers clues to what might have happened to people of the Indus River Valley and how that might relate to our own warming world.&lt;/p&gt;
    &lt;p&gt;5 min&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46090164</guid><pubDate>Sat, 29 Nov 2025 19:40:30 +0000</pubDate></item><item><title>Be Like Clippy</title><link>https://be-clippy.com/</link><description>&lt;doc fingerprint="2d35a339896b64a8"&gt;
  &lt;main&gt;
    &lt;p&gt;Fed up with trillion-dollar companies exploiting your data? Forced to use their services? Your data held for ransom? Your data used to train their AI models? Opt-outs for data collection instead of opt-ins?&lt;/p&gt;
    &lt;p&gt;Join the movement to make companies more like Clippy. Set your profile picture to Clippy, make your voice heard.&lt;/p&gt;
    &lt;p&gt;Below is a video that explains the Be Like Clippy movement. It’s a call to action for developers, companies, and users alike to embrace a more open, transparent, and user-friendly approach to technology.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46090172</guid><pubDate>Sat, 29 Nov 2025 19:41:55 +0000</pubDate></item><item><title>FBI RFP for tool to scrape Gab, Parler, 8Kun, and Telegram (5k licenses) [pdf]</title><link>https://vault.fbi.gov/contract-15f06722c0000258</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46090330</guid><pubDate>Sat, 29 Nov 2025 20:06:54 +0000</pubDate></item></channel></rss>