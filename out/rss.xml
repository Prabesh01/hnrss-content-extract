<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 01 Jan 2026 23:38:27 +0000</lastBuildDate><item><title>iOS allows alternative browser engines in Japan</title><link>https://developer.apple.com/support/alternative-browser-engines-jp/</link><description>&lt;doc fingerprint="9f5acc538535b30d"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Using alternative browser engines in Japan&lt;/head&gt;
    &lt;p&gt;In iOS 26.2 and later, browser engines other than WebKit can be used in two types of apps for users in Japan: Dedicated browser apps that provide a full web browser experience, and apps from browser engine stewards that provide in-app browsing experiences using an embedded browser engine.&lt;/p&gt;
    &lt;p&gt;Apple will provide authorized developers access to technologies within the system that enable critical functionality and help them offer high-performance modern browser engines. These technologies include just-in-time compilation, multiprocess support, and more.&lt;/p&gt;
    &lt;p&gt;However, as browser engines are constantly exposed to untrusted and potentially malicious content and have visibility of sensitive user data, they are one of the most common attack vectors for bad actors. To help keep users safe online, Apple will only authorize developers to implement alternative browser engines after meeting specific criteria and who commit to a number of ongoing privacy and security requirements, including timely security updates to address emerging threats and vulnerabilities.&lt;/p&gt;
    &lt;head rend="h2"&gt;Web Browser Engine Entitlement&lt;/head&gt;
    &lt;head rend="h4"&gt;For browser apps&lt;/head&gt;
    &lt;p&gt;With the Web Browser Engine Entitlement, you can use an alternative browser engine in your browser app. If you’re interested in using an alternative browser engine in your browser app, review the requirements below, then submit your request for the Web Browser Engine Entitlement. For technical guidance, review:&lt;/p&gt;
    &lt;head rend="h3"&gt;Requirements&lt;/head&gt;
    &lt;p&gt;To qualify for the entitlement, your app must:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Be distributed solely on iOS in Japan (except for any other jurisdiction or Apple platform expressly permitted by Apple under the Developer Agreement - including any addenda - for which you have likewise obtained a corresponding entitlement profile);&lt;/item&gt;
      &lt;item&gt;Be a separate binary from any application that uses the system-provided web browser engine;&lt;/item&gt;
      &lt;item&gt;Have the Default Browser Entitlement&lt;/item&gt;
      &lt;item&gt;Meet the following functional requirements to ensure your app is using a web browser engine that provides a baseline of web functionality: &lt;list rend="ul"&gt;&lt;item&gt;Pass a minimum percentage of tests available from industry standard test suites: &lt;list rend="ul"&gt;&lt;item&gt;90% of Web Platform Tests&lt;/item&gt;&lt;item&gt;as a percentage of the highest number of subtests executed by any browser on the wpt.fyi front page; and&lt;/item&gt;&lt;item&gt;on an operating system that the test suite is compatible with&lt;/item&gt;&lt;item&gt;80% of Test262 on an iOS device, iPadOS device, or Mac with Apple silicon; and&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Meet the above test suite requirement if Just in Time (JIT) compilation is unavailable (e.g., if Lockdown Mode is enabled by the user)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Pass a minimum percentage of tests available from industry standard test suites: &lt;/item&gt;
      &lt;item&gt;You and your app must meet the following security requirements: &lt;list rend="ul"&gt;&lt;item&gt;Commit to secure development processes, including monitoring your app’s software supply chain for vulnerabilities, and following best practices around secure software development (such as performing threat modeling on new features under development).&lt;/item&gt;&lt;item&gt;Provide a URL to a published vulnerability disclosure policy that includes contact information for reporting of security vulnerabilities and issues to you by third parties (which may include Apple), what information to provide in a report, and when to expect status updates.&lt;/item&gt;&lt;item&gt;Commit to mitigate vulnerabilities that are being exploited within your app or the alternative web browser engine it is using in a timely manner (e.g., 30 days for the simplest classes of vulnerabilities being actively exploited).&lt;/item&gt;&lt;item&gt;Provide a URL to a publicly available web page (or pages) that provides information on which reported vulnerabilities have been resolved in specific versions of the browser engine and associated app version if different.&lt;/item&gt;&lt;item&gt;If your alternative web browser engine uses a root certificate store that is not accessed via the iOS SDK, you must make the root certificate policy publicly accessible and the owner of that policy must participate as a browser in the Certification Authority / Browser Forum.&lt;/item&gt;&lt;item&gt;Demonstrate support for modern Transport Layer Security protocols to protect data-in-transit communications when the browser engine is in use.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Program security requirements&lt;/head&gt;
    &lt;p&gt;You must:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use memory-safe programming languages, or features that improve memory safety within other languages, within the alternative web browser engine at a minimum for all code that processes web content;&lt;/item&gt;
      &lt;item&gt;Adopt the latest security mitigations (for example, Pointer Authentication Codes) that remove classes of vulnerabilities or make it much harder to develop an exploit chain. This includes adoption of: &lt;list rend="ul"&gt;&lt;item&gt;Pointer Authentication Codes (PAC);&lt;/item&gt;&lt;item&gt;Memory Integrity Enforcement (MIE) for any (i) system-provided allocators in any content extension and (ii) custom- or system-provided allocators in any processes and extensions of your app, including in your network and graphics rendering extensions;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Follow secure design and coding best practices;&lt;/item&gt;
      &lt;item&gt;Use process separation to limit the effects of exploitation and validate inter-process communication (IPC) within the alternative web browser engine;&lt;/item&gt;
      &lt;item&gt;Monitor for vulnerabilities in any third-party software dependencies and your app’s broader software supply chain, migrating to newer versions if a vulnerability impacts your app;&lt;/item&gt;
      &lt;item&gt;Not use frameworks or software libraries that are no longer receiving security updates in response to vulnerabilities; and&lt;/item&gt;
      &lt;item&gt;Prioritize resolving reported vulnerabilities with expedience, over new feature development. For example, where the alternative web browser engine bridges capabilities between the platform’s SDK and web content to enable Web APIs, upon request you must remove support for such a Web API if it is identified to present a vulnerability. Most vulnerabilities should be resolved in 30 days, but some may be more complex and may take longer.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Program privacy requirements&lt;/head&gt;
    &lt;p&gt;You must:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Block cross-site cookies (i.e., third-party cookies) by default unless the user expressly opts to allow such cookies with informed consent, or as required for compatibility in the case of popup windows that interact with frames in their opening window;&lt;/item&gt;
      &lt;item&gt;Partition any storage or state observable by websites per top-level website, or block such storage or state from cross-site usage and observability;&lt;/item&gt;
      &lt;item&gt;Not sync any state (including cookies) between your app and any other app, even another app from the same developer, unless the user has explicitly given permission for the state to be synced, either by signing into both your app and the other app, or through another mechanism of providing explicit permission;&lt;/item&gt;
      &lt;item&gt;Not share device identifiers with websites without informed consent and user activation;&lt;/item&gt;
      &lt;item&gt;Label network connections using the APIs provided to generate an App Privacy Report on iOS (i.e., wherever your app is distributed); and&lt;/item&gt;
      &lt;item&gt;Follow commonly adopted web standards on when to require informed user activation and/or user consent, as appropriate for web APIs (e.g., clipboard or full screen access), including those that provide access to PII.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Embedded Browser Engine Entitlement&lt;/head&gt;
    &lt;head rend="h4"&gt;For in-app browsing&lt;/head&gt;
    &lt;p&gt;With the Embedded Browser Engine Entitlement, you can embed an alternative browser engine within your app to provide in-app browsing. In-app browsing is the display of content dynamically from the web that would be accessible and work within a web browser app. This doesn’t include content embedded within or only obtainable via the app.&lt;/p&gt;
    &lt;p&gt;The primary focus of your app while providing in-app browsing must be to provide web browsing functionality. When providing in-app browsing, the user interface must:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Take over the majority of the display, apart from relevant controls allowing the end user to control the browsing session;&lt;/item&gt;
      &lt;item&gt;Provide a button or link to the default browser of the system to allow the user to open a dedicated browser app to view the content currently being displayed; and&lt;/item&gt;
      &lt;item&gt;Display the domain or URL whose content is being rendered by in-app browsing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you’re interested in using an alternative browser engine in your app to provide in-app browsing experiences, review the requirements below, then submit your request for the Embedded Browser Engine Entitlement. You will need to provide information on the engine that you are intending to embed, including how it meets the requirements and how it can be integrated into an app to provide the in-app browsing experience. For technical guidance, review the Examples and Resources section.&lt;/p&gt;
    &lt;head rend="h3"&gt;Requirements&lt;/head&gt;
    &lt;p&gt;To qualify for the entitlement, your organization must be a browser engine steward. A browser engine steward is an entity with the primary responsibility for operating a distinct web browser engine.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Primary responsibility means you have operational control over, and are ultimately responsible for, coordinating a response where a security or privacy vulnerability is found in the web browser engine, and resolving it.&lt;/item&gt;
      &lt;item&gt;A distinct web browser engine is maintained by a different entity or organization than any other web browser engine, and has both architecture and support for Web APIs that are materially distinct from any other engine. The engine is not generally updated to reflect changes made to its forks, but instead pushes out changes to its forks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;App requirements&lt;/head&gt;
    &lt;p&gt;Your app must:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Be distributed solely on iOS in Japan (except for any other jurisdiction or Apple platform expressly permitted by Apple under the Developer Agreement - including any addenda - for which you have likewise obtained a corresponding entitlement profile);&lt;/item&gt;
      &lt;item&gt;Use the entitlement solely for in-app browsing;&lt;/item&gt;
      &lt;item&gt;Not have the default browser entitlement&lt;/item&gt;
      &lt;item&gt;Meet the following functional requirements to ensure your app is using a web browser engine that provides a baseline of web functionality: &lt;list rend="ul"&gt;&lt;item&gt;Pass a minimum percentage of tests available from industry standard test suites: &lt;list rend="ul"&gt;&lt;item&gt;90% of Web Platform Tests&lt;/item&gt;&lt;item&gt;as a percentage of the highest number of subtests executed by any browser on the wpt.fyi front page; and&lt;/item&gt;&lt;item&gt;on an operating system that the test suite is compatible with&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;80% of Test262 on an iOS device, iPadOS device, or Mac with Apple silicon; and&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Pass a minimum percentage of tests available from industry standard test suites: &lt;/item&gt;
      &lt;item&gt;Meet the above test suite requirement if Just in Time (JIT) compilation is unavailable (e.g., if Lockdown Mode is enabled by the user)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Commit to secure development processes, including monitoring your Application’s software supply chain for vulnerabilities, and following best practices around secure software development (such as performing threat modeling on new features under development).&lt;/item&gt;
      &lt;item&gt;Provide a URL to a published vulnerability disclosure policy that includes contact information for reporting of security vulnerabilities and issues to you by third parties (which may include Apple), what information to provide in a report, and when to expect status updates.&lt;/item&gt;
      &lt;item&gt;Commit to mitigate vulnerabilities that are being exploited within your app or the alternative web browser engine in a timely manner (e.g., 30 days for the simplest classes of vulnerabilities being actively exploited).&lt;/item&gt;
      &lt;item&gt;Provide a URL to a publicly available webpage (or pages) that provides information on which reported vulnerabilities have been resolved in specific versions of the browser engine and associated app version if different.&lt;/item&gt;
      &lt;item&gt;If the alternative web browser engine you choose uses a root certificate store that is not accessed via the iOS SDK, you must make the root certificate policy publicly accessible and the owner of that policy must participate as a Certificate Consumer in the Certification Authority / Browser Forum.&lt;/item&gt;
      &lt;item&gt;Demonstrate support for modern Transport Layer Security protocols to protect data-in-transit communications when the browser engine is in use.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Program security requirements&lt;/head&gt;
    &lt;p&gt;You must:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use memory-safe programming languages, or features that improve memory safety within other languages, within the Alternative Web Browser Engine at a minimum for all code that processes web content;&lt;/item&gt;
      &lt;item&gt;Adopt the latest security mitigations that remove classes of vulnerabilities or make it much harder to develop an exploit chain;&lt;/item&gt;
      &lt;item&gt;Follow secure design, and secure coding, best practices;&lt;/item&gt;
      &lt;item&gt;Monitor for vulnerabilities in any third-party software dependencies and your app’s broader software supply chain, migrating to newer versions if a vulnerability impacts your app;&lt;/item&gt;
      &lt;item&gt;Not use frameworks or software libraries that are no longer receiving security updates in response to vulnerabilities; and&lt;/item&gt;
      &lt;item&gt;Prioritize resolving reported vulnerabilities with expedience, over new feature development. For example, where the alternative browser engine bridges capabilities between the platform’s SDK and web content to enable Web APIs, upon request you must remove support for such a Web API if it is identified to present a vulnerability. Most vulnerabilities should be resolved in 30 days, but some may be more complex and may take longer.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Program privacy requirements&lt;/head&gt;
    &lt;p&gt;You must:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Block cross-site cookies (i.e., third-party cookies) by default unless the user expressly opts to allow such cookies with informed consent, or as required for compatibility in the case of popup windows that interact with frames in their opening window;&lt;/item&gt;
      &lt;item&gt;Partition any storage or state observable by websites per top level website, or block such storage or state from cross-site usage and observability;&lt;/item&gt;
      &lt;item&gt;Not share device identifiers with websites without informed consent and user activation;&lt;/item&gt;
      &lt;item&gt;Label network connections using the APIs provided to generate an App Privacy Report on iOS (i.e., wherever your app is distributed); and&lt;/item&gt;
      &lt;item&gt;Follow commonly adopted web standards on when to require informed user activation and/or user consent, as appropriate for Web APIs (e.g., clipboard or full screen access), including those that provide access to PII.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Additional requirements&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You must submit with each binary submission the name and version of the alternative web browser engine embedded in your app.&lt;/item&gt;
      &lt;item&gt;Upon a new version of the alternative web browser engine embedded in your app being made available, you must submit an update to your app with that new version within fifteen (15) calendar days.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Examples and resources&lt;/head&gt;
    &lt;p&gt;This section contains additional resources and examples to help you meet the requirements that allow you to use an alternative browser engine.&lt;/p&gt;
    &lt;head rend="h3"&gt;Secure SDLC (Software Development Lifecycle)&lt;/head&gt;
    &lt;p&gt;Many of the requirements that you need to meet rely on developing a security and privacy-first approach to introducing new features into your app. When you begin development on a new feature, you should first develop a threat model as well as a plan for how you will gain assurance that your architecture and the released version of your app mitigates the risks you’ve identified. There are a number of techniques to gaining assurance — for example, code auditing, fuzz testing, and writing tests to verify the security properties you intend to enforce. You should consider all web content to be untrusted and potentially malicious.&lt;/p&gt;
    &lt;head rend="h5"&gt;Resources&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Learn more about the BrowserEngineKit framework&lt;/item&gt;
      &lt;item&gt;Learn more about designing your browser architecture&lt;/item&gt;
      &lt;item&gt;Secure your app: threat modeling and anti-patterns (WWDC20)&lt;/item&gt;
      &lt;item&gt;Security&lt;/item&gt;
      &lt;item&gt;Fuzz testing (MDN)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Security Mitigations and Memory Safety&lt;/head&gt;
    &lt;p&gt;You should also consider what current security mitigations are provided by iOS or iPadOS, such as Pointer Authentication Codes and Memory Integrity Enforcement, and what programming languages (or language and compiler features, and other tooling) are available to mitigate each threat you identify. For example, Swift is a memory-safe language by default, and can help you avoid a number of common sources of vulnerabilities, as well as other memory-related software bugs. However, memory-unsafe languages such as C++ do provide features that provide memory safety benefits - such as &lt;code&gt;std::span&lt;/code&gt;. Additionally, compiler options and tools can be used, for example -fbounds-safety with C, which allows annotation of existing code to mitigate out-of-bounds memory access without always requiring rewriting of functionality in a language that’s memory safe by default.&lt;/p&gt;
    &lt;head rend="h5"&gt;Resources&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Swift Programming Language: Memory Safety&lt;/item&gt;
      &lt;item&gt;Swift StrictMemorySafety&lt;/item&gt;
      &lt;item&gt;Enabling enhanced security for your app&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Vulnerability Management&lt;/head&gt;
    &lt;p&gt;You should assume that undiscovered vulnerabilities will always be present within a browser engine or that any new features could introduce unintended risks. Therefore, it is vital that you have the processes in place to allow you to respond when a vulnerability is discovered either internally through testing and security and privacy assurance efforts, within your software supply chain, or disclosed to you by another party.&lt;/p&gt;
    &lt;p&gt;Where you provide a pathway for a third party (for example, a security researcher) to report a vulnerability to you, you should consider what information you will need from them to enable you to quickly determine the validity and cause of the issue. You should also ensure that you have processes in place to prioritize a fix for the vulnerability and then release an update that might be out of step with your normal schedule.&lt;/p&gt;
    &lt;p&gt;It’s also important that users can quickly determine which public vulnerabilities that have an associated CVE-ID are resolved in which version of your app (or alternative browser engine).&lt;/p&gt;
    &lt;head rend="h5"&gt;Resources&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Verifying the origin of your XCFrameworks&lt;/item&gt;
      &lt;item&gt;Report a security or privacy vulnerability&lt;/item&gt;
      &lt;item&gt;Apple Security Bounty Guidelines&lt;/item&gt;
      &lt;item&gt;Apple Security Releases&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Network Security&lt;/head&gt;
    &lt;p&gt;By using the iOS SDK, specifically the Network framework and/or SecTrust APIs, you reduce your need to take responsibility for evaluating the trust of web certificates and maintaining or using a corresponding root trust store and program for the alternative browser engine you use. If you do operate a program - it should provide information on how a root certificate authority (CA) can apply to become part of the program, and also how incidents (for example, the exposure of a root certificate authority’s private key material) can be reported such that you can take action.&lt;/p&gt;
    &lt;p&gt;Protocols used on the web are constantly evolving to respond to emerging threats and better protect user privacy and security. The current modern TLS versions that should be compatible with your alternative browser engine that have not been deprecated are TLS 1.2 and 1.3. However, these may change over time. You can support deprecated protocols in your alternative browser engine, but you should inform users when they browse to a site that only supports these.&lt;/p&gt;
    &lt;head rend="h5"&gt;Resources&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Network framework&lt;/item&gt;
      &lt;item&gt;SecTrust&lt;/item&gt;
      &lt;item&gt;Apple Root Certificate Program&lt;/item&gt;
      &lt;item&gt;The Transport Layer Security (TLS) Protocol Version 1.3&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46453950</guid><pubDate>Thu, 01 Jan 2026 13:30:45 +0000</pubDate></item><item><title>2025 Letter</title><link>https://danwang.co/2025-letter/</link><description>&lt;doc fingerprint="bf3891178fc5067d"&gt;
  &lt;main&gt;
    &lt;p&gt;(This piece is my year in review; I skipped a letter last year)&lt;/p&gt;
    &lt;p&gt;One way that Silicon Valley and the Communist Party resemble each other is that both are serious, self-serious, and indeed, completely humorless.&lt;/p&gt;
    &lt;p&gt;If the Bay Area once had an impish side, it has gone the way of most hardware tinkerers and hippie communes. Which of the tech titans are funny? In public, they tend to speak in one of two registers. The first is the blandly corporate tone we’ve come to expect when we see them dragged before Congressional hearings or fireside chats. The second leans philosophical, as they compose their features into the sort of reverie appropriate for issuing apocalyptic prophecies on AI. Sam Altman once combined both registers at a tech conference when he said: “I think that AI will probably, most likely, sort of lead to the end of the world. But in the meantime, there will be great companies created with serious machine learning.” Actually that was pretty funny.&lt;/p&gt;
    &lt;p&gt;It wouldn’t be news to the Central Committee that only the paranoid survive. The Communist Party speaks in the same two registers as the tech titans. The po-faced men on the Politburo tend to make extraordinarily bland speeches, laced occasionally with a murderous warning against those who cross the party’s interests. How funny is the big guy? We can take a look at an official list of Xi Jinping’s jokes, helpfully published by party propagandists. These wisecracks include the following: “On an inspection tour to Jiangsu, Xi quipped that the true measure of water cleanliness is whether the mayor would dare to swim in the water.” Or try this reminiscence that Xi offered on bad air quality: “The PM2.5 back then was even worse than it is now; I used to joke that it was PM250.” Yes, such a humorous fellow is the general secretary.1&lt;/p&gt;
    &lt;p&gt;It’s nearly as dangerous to tweet a joke about a top VC as it is to make a joke about a member of the Central Committee. People who are dead serious tend not to embody sparkling irony. Yet the Communist Party and Silicon Valley are two of the most powerful forces shaping our world today. Their initiatives increase their own centrality while weakening the agency of whole nation states. Perhaps they are successful because they are remorseless.&lt;/p&gt;
    &lt;p&gt;Earlier this year, I moved from Yale to Stanford. The sun and the dynamism of the west coast have drawn me back. I found a Bay Area that has grown a lot weirder since I lived there a decade ago. In 2015, people were mostly working on consumer apps, cryptocurrencies, and some business software. Though it felt exciting, it looks in retrospect like a more innocent, even a more sedate, time. Today, AI dictates everything in San Francisco while the tech scene plays a much larger political role in the United States. I can’t get over how strange it all feels. In the midst of California’s natural beauty, nerds are trying to build God in a Box; meanwhile, Peter Thiel hovers in the background presenting lectures on the nature of the Antichrist. This eldritch setting feels more appropriate for a Gothic horror novel than for real life.&lt;/p&gt;
    &lt;p&gt;Before anyone gets the wrong idea, I want to say that I am rooting for San Francisco. It’s tempting to gawk at the craziness of the culture, as much of the east coast media tends to do. Yes, one can quickly find people who speak with the conviction of a cultist; no, I will not inject the peptides proffered by strangers. But there’s more to the Bay Area than unusual health practices. It is, after all, a place that creates not only new products, but also new modes of living. I’m struck that some east coast folks insist to me that driverless cars can’t work and won’t be accepted, even as these vehicles populate the streets of the Bay Area. Coverage of Silicon Valley increasingly reminds me of coverage of China, where a legacy media reporter might parachute in, write a dispatch on something that looks deranged, and leave without moving past caricature.&lt;/p&gt;
    &lt;p&gt;I enjoy San Francisco more than when I was younger because I now better appreciate what makes it work. I believe that Silicon Valley possesses plenty of virtues. To start, it is the most meritocratic part of America. Tech is so open towards immigrants that it has driven populists into a froth of rage. It remains male-heavy and practices plenty of gatekeeping. But San Francisco better embodies an ethos of openness relative to the rest of the country. Industries on the east coast — finance, media, universities, policy — tend to more carefully weigh name and pedigree. Young scientists aren’t told they ought to keep their innovations incremental and their attitude to hierarchy duly deferential, as they might hear in Boston. A smart young person could achieve much more over a few years in SF than in DC. People aren’t reminiscing over some lost golden age that took place decades ago, as New Yorkers in media might do.&lt;/p&gt;
    &lt;p&gt;San Francisco is forward looking and eager to try new ideas. Without this curiosity, it wouldn’t be able to create whole new product categories: iPhones, social media, large language models, and all sorts of digital services. For the most part, it’s positive that tech values speed: quick product cycles, quick replies to email. Past success creates an expectation that the next technological wave will be even more exciting. It’s good to keep building the future, though it’s sometimes absurd to hear someone pivot, mid-breath, from declaring that salvation lies in the blockchain to announcing that AI will solve everything.&lt;/p&gt;
    &lt;p&gt;People like to make fun of San Francisco for not drinking; well, that works pretty well for me. I enjoy board games and appreciate that it’s easier to find other players. I like SF house parties, where people take off their shoes at the entrance and enter a space in which speech can be heard over music, which feels so much more civilized than descending into a loud bar in New York. It’s easy to fall into a nerdy conversation almost immediately with someone young and earnest. The Bay Area has converged on Asian-American modes of socializing (though it lacks the emphasis on food). I find it charming that a San Francisco home that is poorly furnished and strewn with pizza boxes could be owned by a billionaire who can’t get around to setting up a bed for his mattress.&lt;/p&gt;
    &lt;p&gt;There’s still no better place for a smart, young person to go in the world than Silicon Valley. It adores the youth, especially those with technical skill and the ability to grind. Venture capitalists are chasing younger and younger founders: the median age of the latest Y Combinator cohort is only 24, down from 30 just three years ago. My favorite part of Silicon Valley is the cultivation of community. Tech founders are a close-knit group, always offering help to each other, but they circulate actively amidst the broader community too. (The finance industry in New York by contrast practices far greater secrecy.) Tech has organizations I think of as internal civic institutions that try to build community. They bring people together in San Francisco or retreats north of the city, bringing together young people to learn from older folks.&lt;/p&gt;
    &lt;p&gt;Silicon Valley also embodies a cultural tension. It is playing with new ideas while being open to newcomers; at the same time, it is a self-absorbed place that doesn’t think so much about the broader world. Young people who move to San Francisco already tend to be very online. They know what they’re signing up for. If they don’t fit in after a few years, they probably won’t stick around. San Francisco is a city that absorbs a lot of people with similar ethics, which reinforces its existing strengths and weaknesses.&lt;/p&gt;
    &lt;p&gt;Narrowness of mind is something that makes me uneasy about the tech world. Effective altruists, for example, began with sound ideas like concern for animal welfare as well as cost-benefit analyses for charitable giving. But these solid premises have launched some of its members towards intellectual worlds very distant from moral intuitions that most people hold; they’ve also sent a few into jail. The well-rounded type might struggle to stand out relative to people who are exceptionally talented in a technical domain. Hedge fund managers have views about the price of oil, interest rates, a reliably obscure historical episode, and a thousand other things. Tech titans more obsessively pursue a few ideas — as Elon Musk has on electric vehicles and space launches — rather than developing a robust model of the world.&lt;/p&gt;
    &lt;p&gt;So the 20-year-olds who accompanied Mr. Musk into the Department of Government Efficiency did not, I would say, distinguish themselves with their judiciousness. The Bay Area has all sorts of autistic tendencies. Though Silicon Valley values the ability to move fast, the rest of society has paid more attention to instances in which tech wants to break things. It is not surprising that hardcore contingents on both the left and the right have developed hostility to most everything that emerges from Silicon Valley.&lt;/p&gt;
    &lt;p&gt;There’s a general lack of cultural awareness in the Bay Area. It’s easy to hear at these parties that a person’s favorite nonfiction book is Seeing Like a State while their aspirationally favorite novel is Middlemarch. Silicon Valley often speaks in strange tongues, starting podcasts and shows that are popular within the tech world but do not travel far beyond the Bay Area. Though San Francisco has produced so much wealth, it is a relative underperformer in the national culture. Indie movie theaters keep closing down while all sorts of retail and art institutions suffer from the crumminess of downtown. The symphony and the opera keep cutting back on performances — after Esa-Pekka Salonen quit the directorship of the symphony, it hasn’t been able to name a successor. Wealthy folks in New York and LA have, for generations, pumped money into civic institutions. Tech elites mostly scorn traditional cultural venues and prefer to fund the next wave of technology instead.&lt;/p&gt;
    &lt;p&gt;One of the things I like about the finance industry is that it might be better at encouraging diverse opinions. Portfolio managers want to be right on average, but everyone is wrong three times a day before breakfast. So they relentlessly seek new information sources; consensus is rare, since there are always contrarians betting against the rest of the market. Tech cares less for dissent. Its movements are more herdlike, in which companies and startups chase one big technology at a time. Startups don’t need dissent; they want workers who can grind until the network effects kick in. VCs don’t like dissent, showing again and again that many have thin skins. That contributes to a culture I think of as Silicon Valley’s soft Leninism. When political winds shift, most people fall in line, most prominently this year as many tech voices embraced the right.&lt;/p&gt;
    &lt;p&gt;The two most insular cities I’ve lived in are San Francisco and Beijing. They are places where people are willing to risk apocalypse every day in order to reach utopia. Though Beijing is open only to a narrow slice of newcomers — the young, smart, and Han — its elites must think about the rest of the country and the rest of the world. San Francisco is more open, but when people move there, they stop thinking about the world at large. Tech folks may be the worst-traveled segment of American elites. People stop themselves from leaving in part because they can correctly claim to live in one of the most naturally beautiful corners of the world, in part because they feel they should not tear themselves away from inventing the future. More than any other topic, I’m bewildered by the way that Silicon Valley talks about AI.&lt;/p&gt;
    &lt;p&gt;Hallucinating the end of history&lt;/p&gt;
    &lt;p&gt;While critics of AI cite the spread of slop and rising power bills, AI’s architects are more focused on its potential to produce surging job losses. Anthropic chief Dario Amodei takes pains to point out that AI could push the unemployment rate to 20 percent by eviscerating white-collar work.2 I wonder whether this message is helping to endear his product to the public.&lt;/p&gt;
    &lt;p&gt;The most-read essay from Silicon Valley this year was AI 2027. The five authors, who come from the AI safety world, outline a scenario in which superintelligence wakes up in 2027; a decade later, it decides to annihilate humanity with biological weapons. My favorite detail in the report is that humanity would persist in a genetically modified form, after the AI reconstructs creatures that are “to humans what corgis are to wolves.” It’s hard to know what to make of this document, because the authors keep tucking important context into footnotes, repeatedly saying they do not endorse a prediction. Six months after publication, they stated that their timelines were lengthening, but even at the start their median forecast for the arrival of superintelligence was later than 2027. Why they put that year in their title remains beyond me.&lt;/p&gt;
    &lt;p&gt;It’s easy for conversations in San Francisco to collapse into AI. At a party, someone told me that we no longer have to worry about the future of manufacturing. Why not? “Because AI will solve it for us.” At another, I heard someone say the same thing about climate change. One of the questions I receive most frequently anywhere is when Beijing intends to seize Taiwan. But only in San Francisco do people insist that Beijing wants Taiwan for its production of AI chips. In vain do I protest that there are historical and geopolitical reasons motivating the desire, that chip fabs cannot be violently seized, and anyway that Beijing has coveted Taiwan for approximately seven decades before people were talking about AI.&lt;/p&gt;
    &lt;p&gt;Silicon Valley’s views on AI made more sense to me after I learned the term “decisive strategic advantage.” It was first used by Nick Bostrom’s 2014 book Superintelligence, which defined it as a technology sufficient to achieve “complete world domination.” How might anyone gain a DSA? A superintelligence might develop cyber advantages that cripple the adversary’s command-and-control capabilities. Or the superintelligence could self-recursively improve such that the lab or state that controls it gains an insurmountable scientific advantage. Once an AI reaches a certain capability threshold, it might need only weeks or hours to evolve into a superintelligence.3 And if an American lab builds it, it might help to lock in the dominance of another American century.&lt;/p&gt;
    &lt;p&gt;If you buy the potential of AI, then you might worry about the corgi-fication of humanity by way of biological weapons. This hope also helps to explain the semiconductor controls unveiled by the Biden administration in 2022. If the policymakers believe that DSA is within reach, then it makes sense to throw almost everything into grasping it while blocking the adversary from the same. And it barely matters if these controls stimulate Chinese companies to invent alternatives to American technologies, because the competition will be won in years, not decades.&lt;/p&gt;
    &lt;p&gt;The trouble with these calculations is that they mire us in epistemically tricky terrain. I’m bothered by how quickly the discussions of AI become utopian or apocalyptic. As Sam Altman once said (and again this is fairly humorous): “AI will be either the best or the worst thing ever.” It’s a Pascal’s Wager, in which we’re sure that the values are infinite, but we don’t know in which direction. It also forces thinking to be obsessively short term. People start losing interest in problems of the next five or ten years, because superintelligence will have already changed everything. The big political and technological questions we need to discuss are only those that matter to the speed of AI development. Furthermore, we must sprint towards a post-superintelligence world even though we have no real idea what it will bring.&lt;/p&gt;
    &lt;p&gt;Effective altruists used to be known for their insistence on thinking about the very long run; much more of the movement now is concerned about the development of AI in the next year. Call me a romantic, but I believe that there will be a future, and indeed a long future, beyond 2027. History will not end. We need to cultivate the skill of exact thinking in demented times.&lt;/p&gt;
    &lt;p&gt;I am skeptical of the decisive strategic advantage when I filter it through my main preoccupation: understanding China’s technology trajectories. On AI, China is behind the US, but not by years. There’s no question that American reasoning models are more sophisticated than the likes of DeepSeek and Qwen. But the Chinese efforts are doggedly in pursuit, sometimes a bit closer to US models, sometimes a bit further. By virtue of being open-source (or at least open-weight), the Chinese models have found receptive customers overseas, sometimes with American tech companies.4 If US labs achieve superintelligence, the Chinese labs are probably on a good footing to follow closely. Unless the DSA is decisive immediately, it’s not obvious that the US will have a monopoly on this technology, just as it could not keep it over the bomb.&lt;/p&gt;
    &lt;p&gt;One advantage for Beijing is that much of the global AI talent is Chinese. We can tell from the CVs of researchers as well as occasional disclosures from top labs (for example from Meta) that a large percentage of AI researchers earned their degrees from Chinese universities. American labs may be able to declare that “our Chinese are better than their Chinese.” But some of these Chinese researchers may decide to repatriate. I know that many of them prefer to stay in the US: their compensation might be higher by an order of magnitude, they have access to compute, and they can work with top peers.5But they may also tire of the uncertainty created by Trump’s immigration policy. It’s never worth forgetting that at the dawn of the Cold War, the US deported Qian Xuesen, the CalTech professor who then built missile delivery systems for Beijing. Or these Chinese researchers expect life in Shanghai to be safer or more fun than in San Francisco. Or they miss mom. People move for all sorts of reasons, so I’m reluctant to believe that the US has a durable talent advantage.&lt;/p&gt;
    &lt;p&gt;China has other advantages in building AI. Superintelligence will demand a superload of power. By now everyone has seen the chart with two curves: US electrical generation capacity, which has barely budged upwards since the year 2000; and China’s capacity, which was one-third US levels in 2000 and more than two-and-a-half times US levels in 2024. Beijing is building so much solar, coal, and nuclear to make sure that no data center shall be in want. Though the US has done a superb job building data centers, it hasn’t prepared enough for other bottlenecks. Especially not as Trump’s dislike of wind turbines has removed this source of growth. Speaking of Trump’s whimsy, he has also been generous with selling close-to-leading chips to Beijing. That’s another reason that data centers might not represent a US advantage for long.&lt;/p&gt;
    &lt;p&gt;Silicon Valley has not demonstrated joined-up thinking for deploying AI. It would help if they learned from the central planners. The AI labs have not shown that they’re thinking seriously about how to diffuse the technology throughout society, which will require extensive regulatory and legal reform. How else will AI be able to fold doctors and lawyers into its tender mercies? Doing politics will also mean reaching out to more of the electorate, who are often uneasy with Silicon Valley’s promises while they see rising electrical bills. Silicon Valley has done a marvelous job in building data centers. But tech titans don’t look ready to plan for later steps in leading the whole-of-society effort into deploying AI everywhere.&lt;/p&gt;
    &lt;p&gt;The Communist Party lives for whole-of-society efforts. That’s what Leninist systems are built for. Beijing has set targets for deploying AI across society, though as usual with planning announcements, these numerical targets should be taken seriously and not literally. Chinese founders talk about AI mostly as a technology to be harnessed rather than a fickle power that might threaten all.6 Rather than building superintelligence, Chinese companies have been more interested in embedding AI into robots and manufacturing lines. Some researchers believe that this sort of embodied AI might present the real path towards superintelligence.7We might furthermore wonder how the US and China will use AI. Since the US is much more services-driven, Americans may be using AI to produce more powerpoints and lawsuits; China, by virtue of being the global manufacturer, has the option to scale up production of more electronics, more drones, and more munitions.&lt;/p&gt;
    &lt;p&gt;Dean Ball, who helped craft the White House’s action plan on AI, has written a perceptive post on how the US is playing to its strengths — software, chips, cloud computing, financing — while China is also focused on leaning on manufacturing excellence. In his view, “the US economy is increasingly a highly leveraged bet on deep learning.” Certainly there’s a lot of money invested here, but it looks risky to be so concentrated. I believe it’s unbecoming for the world’s largest economy to be so levered on one technology. That’s a more appropriate strategy for a small country. Why shouldn’t the US be better positioned across the entirety of the supply chain, from electron production to electronics production?&lt;/p&gt;
    &lt;p&gt;I am not a skeptic of AI. I am a skeptic only of the decisive strategic advantage, which treats awakening the superintelligence as the final goal. Rather than “winning the AI race,” I prefer to say that the US and China need to “win the AI future.” There is no race with a clear end point or a shiny medal for first place. Winning the future is the more appropriately capacious term that incorporates the agenda to build good reasoning models as well as the effort to diffuse it across society. For the US to come ahead on AI, it should build more power, revive its manufacturing base, and figure out how to make companies and workers make use of this technology. Otherwise China might do better when compute is no longer the main bottleneck.&lt;/p&gt;
    &lt;p&gt;The humming tech engine&lt;/p&gt;
    &lt;p&gt;I’ve had Silicon Valley friends tell me that they are planning a trip to China nearly every month this year. Silicon Valley respects and fears companies from only one other country. Game recognizes game, so to speak. Tech founders may begrudge China’s restrictions; and some companies have suffered directly from IP theft. But they also recognize that Chinese companies can move even faster than they do with their teams of motivated workers; and Chinese manufacturers are far ahead of US capabilities on anything involving physical production. Some founders and VCs are impressed with the fact that Chinese AI companies have gotten this far while suffering American tech restrictions, while leading in open-source to boot. VCs are wondering whether they may still invest in Chinese startups or Chinese founders who have moved abroad.&lt;/p&gt;
    &lt;p&gt;2025 is the year that Chinese tech successes have really blossomed into the wider American consciousness. There’s no need to retread the coverage around DeepSeek, the surge of electric vehicle exports, or new developments in robotics. When I first moved from Silicon Valley to China in 2017, I felt some degree of skepticism from my friends that I was taking myself out of the beating heart of the technological universe and into the unknown. But it was clear to me that Chinese firms were improving on quality and taking global market share. I wrote in my 2019 letter: “Chinese workers are working with the latest tools to produce most of the world’s goods; over the longer term, my hypothesis is that they’ll be able to replicate the tooling and make just as good final products.”&lt;/p&gt;
    &lt;p&gt;I think that has become closer to consensus views. I believe that Chinese technological success is now the rule rather than the exception. There are two fields in which China is substantially behind the west: semiconductors and aviation. The chip sector is gingerly attempting to expand under the weight of US restrictions; meanwhile, China’s answer to Airbus and Boeing is on a very long runway. I grant that these are two critical technologies, but China has attained technological leadership almost everywhere else. And I believe its technological momentum will continue rolling onwards to engulf more of their western competitors over the next decade.&lt;/p&gt;
    &lt;p&gt;The electric vehicle industry is the sharp tip of the spear of China’s global success. Chinese EVs have greater functionalities than western models while selling at lower price points. A rule of thumb is that it takes five years from an American, German, or Japanese automaker to dream up a new car design and launch that model on the roads; in China, it’s closer to 18 months. The Chinese market is full of demanding customers as well as fast-iterating automotive suppliers. It also has a more productive workforce. According to Tesla’s corporate disclosures, a worker at a Gigafactory in China produces an average of 47 vehicles a year; a worker at a Gigafactory in California produces an average of 20.8&lt;/p&gt;
    &lt;p&gt;China’s automotive success is biting into Germany more than anywhere else. I keep a scrapbook filled with mournful remarks that German executives offer to newspapers. “Most of what German Mittelstand firms do these days, Chinese companies can do just as well,” said a consultant to the Financial Times. “In my sector they look at the price-point of the market leader and sell for roughly half of that,” the boss of a medical devicemaker told the Economist. It’s never hard to find parades of gloomy Germans. Now more than ever it looks like their core competences are threatened by Chinese firms.&lt;/p&gt;
    &lt;p&gt;I often think of the case of Xiaomi. In 2021, Lei Jun vowed that the company he founded would break into the EV business. Four years later, Xiaomi started shipping cars to customers. Not only that, a Xiaomi EV set a speed record at the Nürburgring racetrack in Germany. Compare Xiaomi to Apple, which spent 10 years and $10 billion studying whether to enter the EV market before it pulled the plug. The world’s most advanced consumer product company could not match Xiaomi’s feat. It’s cases like these that make me skeptical of reasoning about China’s tech successes through financial measures or productivity ratios. As of this writing, Xiaomi’s market value is $130 billion. That is only around half of the market value of AppLovin, the mobile advertisement company. Rather than being an indictment of Xiaomi, I view this imbalance as an indictment of financial valuations. Isn’t it better, from a national power perspective, to develop firms like Xiaomi, which calls its shots and then makes them?&lt;/p&gt;
    &lt;p&gt;This comparison between Xiaomi and Apple motivated an essay I wrote with Dragonomics founder Arthur Kroeber in an issue of Foreign Affairs. Our view is that China’s industrial success has roots in deep infrastructure. That includes not only ports and rail, it also includes data connectivity, electrification, and process knowledge. China’s strength lies in a robust manufacturing ecosystem full of self-reinforcing parts.&lt;/p&gt;
    &lt;p&gt;Chinese tech achievements that were apparent in 2025 were the fruits of investments made a decade ago. Given that China continues to invest massively in technology, I expect we’ll see yet more tech successes for another decade to come. Alexander Grothendieck used an analogy of a walnut to describe different approaches to mathematics, which might also apply to technology development. Some mathematicians crack their problems by finding the right spot to insert a chisel before making a clean strike. Grothendieck described his own approach as coming up with general solutions, as if he were immersing the walnut in a bath for such a long time that mere hand pressure would be enough to open it. The US comes up with exquisite and expensive solutions to its technology problems. China’s industrial ecosystem is more like a rising sea, softening many nuts at once.9&lt;/p&gt;
    &lt;p&gt;When these nuts open, it looks like China is producing a big wave of new products. These are its breakthroughs in drones, electric vehicles, and robotics. Years from now we may see greater success in biotech as well. I am keen to follow along China’s progress in electromagnetism over the next decade. China’s industrial ecosystem is leading the way in replacing combustion with electromagnetic processes. Everything is now drone, as the combination of cheaper batteries and better permanent magnets displaces the engine.10&lt;/p&gt;
    &lt;p&gt;One of the startling geopolitical moves of the year was how quickly Donald Trump withdrew his ~150 percent tariffs on China. Trump folded not out of beneficence, but because Xi Jinping denied rare earth magnets to most of the world, threatening many types of manufacturing operations. And yet I’m struck by Beijing’s relative restraint. Chinese producers are close to being monopolists not only in rare earths, but also electronics products, batteries, and many types of active pharmaceutical ingredients. In case China denies, say, cardiovascular drugs to the elderly, how long could a state hold out?&lt;/p&gt;
    &lt;p&gt;One might have expected the US to have roused itself after this bout of the trade war. But there have been too many declarations of Sputnik Moments without commensurate action. Barack Obama declared a Sputnik with China’s high-speed rail; Mark Warner repeated with Huawei’s 5G; Marc Andreessen called it with DeepSeek. The more that people use the term, the less likely that society spurs itself into taking it seriously.&lt;/p&gt;
    &lt;p&gt;I think the US continues to systematically underrate China’s industrial progress for several reasons.&lt;/p&gt;
    &lt;p&gt;First, too many western elites retain hope that China’s efforts will run out of fuel by its own accord. Industrial progress will be weighed down by demographic drag, the growing debt load, maybe even a political collapse. I won’t rule these out, but I don’t think they are likely to break China’s humming tech engine. Demographics in particular don’t matter for advanced technology — you don’t need a workforce of many millions to have robust production of semiconductors or EVs. South Korea, for example, has one of the world’s fastest shrinking populations while retaining its success in electronics production. And though China suffers broader economic headwinds, technology firms like Xiaomi continue to develop new products and enjoy rising revenues. Technology breakthroughs can occur even in a suffering society. Especially if the state continues to lavish resources on chips or anything that could represent an American chokepoint.&lt;/p&gt;
    &lt;p&gt;Second, western elites keep citing the wrong reasons for China’s success. When members of Congress get around to acknowledging China’s tech advancements, they do not fail to attribute causes to either industrial subsidies (also known as cheating) or IP theft (that is, stealing). These are legitimate claims, but China’s advantages extend far beyond them. That’s the creation of deep infrastructure as well as extensive industrial ecosystems that I describe above.&lt;/p&gt;
    &lt;p&gt;Probably the most underrated part of the Chinese system is the ferocity of market competition. It’s excusable not to see that, given that the party espouses so much Marxism. I would argue that China embodies both greater capitalist competition and greater capitalist excess than America does today. Part of the reason that China’s stock market trends sideways is that everyone’s profits are competed away. Big Tech might enjoy the monopolistic success smiled upon by Peter Thiel, coming almost to genteel agreements not to tread too hard upon each other’s business lines. Chinese firms have to fight it out in a rough-and-tumble environment, expanding all the time into each other’s core businesses, taking Jeff “your margin is my opportunity” Bezos with seriousness.&lt;/p&gt;
    &lt;p&gt;Third, western elites keep holding on to a distinction between “innovation,” which is mostly the remit of the west, and “scaling,” which they accept that China can do. I want to dissolve that distinction. Chinese workers innovate every day on the factory floor. By being the site of production, they have a keen sense of how to make technical improvements all the time. American scientists may be world leaders in dreaming up new ideas. But American manufacturers have been poor at building industries around these ideas. The history books point out that Bell Labs invented the first solar cell in 1957; today, the lab no longer exists while the solar industry moved to Germany and then to China. While Chinese universities have grown more capable at producing new ideas, it’s not clear that the American manufacturing base has grown stronger at commercializing new inventions.&lt;/p&gt;
    &lt;p&gt;I sometimes hear that the US will save manufacturers through automation. The truth is that Chinese factories tend to be ahead on automation: that’s a big part of the reason that Chinese Tesla workers are more productive than California Tesla workers. China regularly installs as many robots as the rest of the world put together. They are also able to provide greater amounts of training data for AI. We have to be careful not to let automation, like superintelligence, become an excuse for magical thinking rather than doing the hard work of capacity building.&lt;/p&gt;
    &lt;p&gt;Outlasting the adversary&lt;/p&gt;
    &lt;p&gt;The China discussions I get into on the east coast tend to focus on the country’s problems. Washington, DC in particular likes to ask questions like: didn’t we think that Japan was going to overrun the world with manufacturing before it fell apart? Isn’t China mostly a mess? These are ultimately variants of the form: how might China fail?&lt;/p&gt;
    &lt;p&gt;The west coast flavor of the discussion is different. People are more inclined to ask: what happens if China succeeds? That reflects, in part, Silicon Valley’s epistemic bias towards securing upside returns rather than minimizing downside risks. They also tend to make more frequent visits to China than folks in DC. “What if China succeeds?” is certainly the more interesting question to me, not only because my career has been studying China’s technological successes. The east coast questions deserve to be taken seriously. But I fear that dwelling on China’s failure modes will coax elites into complacency, serving a narrative that the US needs to change nothing before the adversary will topple, robbing the country of urgency to reform.&lt;/p&gt;
    &lt;p&gt;I want to be clear that though I expect China will overrun advanced technology industries, it won’t make the country a broad success. Over the past five years, it has been mired in disinflationary growth, where young people struggle to find a job and find a spouse. The political system is growing even more opaque, terrifying even the insiders. This year, Xi deposed a dozen generals of the People’s Liberation Army, one of whom was also a sitting Politburo member. I wonder how many people inside the Politburo feel confident about where they stand with Xi.&lt;/p&gt;
    &lt;p&gt;Entrepreneurs are on even worse ground. Earlier this year, investors greeted Xi’s handshake with prominent entrepreneurs (including Jack Ma) as good news. It was so, but who can be sure that Xi will not greet them differently once they revive the economy? Though Xi can cut entrepreneurs some slack, the trend is towards greater party control over business and society. Xi himself doesn’t evince concern that economic growth is lackluster. It’s an acceptable tradeoff for making China’s economy less dependent on foreign powers. None of this is a formula for broad human flourishing. Rather, it is depriving Chinese of contact with the rest of the world.&lt;/p&gt;
    &lt;p&gt;Beijing has been working relentlessly to build up its resilience. While the US talks itself out of Sputnik Moments, Beijing has dedicated immense resources to patching up its own deficiencies. It’s not a theoretical fear that Chinese companies might lose access to American technologies. So the state is pouring more money than ever before into semiconductor makers and research universities. It is investing in clean technologies not so much because it cares about the climate, but because it wants to be self-sufficient in energy. And it is re-writing the rules of the global order, with caution because it has been a giant beneficiary of it, while the US is still wondering about what it wants from China. Beijing has been preparing for Cold War without eagerness for waging it, while the US wants to wage a Cold War without preparing for it.11&lt;/p&gt;
    &lt;p&gt;So here’s a potential way that China succeeds. Beijing’s goal is to make nearly every important product in the world, while everyone else supplies its commodities and services. By making the country mostly self-sufficient, and by vigorously policing the outputs of LLMs and social media, Xi might hope to make China resilient. He is building Fortress China stone by stone in order to outlast the adversary. Beijing doesn’t have to replicate American diplomatic, cultural, and financial superpowerdom. It might hope that its prowess in advanced manufacturing might deter the US. And its success in manufacturing might directly destabilize the US: by delivering the coup de grace to the rustbelt, the US might shed a few million more manufacturing jobs over the next decade. The job losses combined with AI psychosis, social media, and all the problems with phones could make national politics meaningfully worse.&lt;/p&gt;
    &lt;p&gt;I don’t think this scenario is likely to be successful. Authoritarian systems have always hoped for the implosion of liberal democracies, while it is the liberal democracies that have a better track record of endurance. But I also don’t think that authoritarian countries are obviously wrong to bet that western polarization will get worse. So it’s up to the US and Europe to show that they can hold on to their values while absorbing the technological changes coming their way.&lt;/p&gt;
    &lt;p&gt;That task is more challenging as Europe and the US grew more apart in 2025. This year, both regions were able to look upon each other with pity. And both were correct to do so. America’s global trust and favorability measures have collapsed in Trump’s second term. Meanwhile, Europe looks as economically stuck as it has ever been, pushing its politics to increasingly chaotic extremes. But I am still more optimistic for the US.&lt;/p&gt;
    &lt;p&gt;I don’t need to lament the damage done by the Trump administration this year: the erosion of alliances, the cruelty towards the weak, the wasting of time. Manufacturing and re-industrialization, which I spend most of my time thinking of, have been doing worse. The Biden administration tried to fund an ambitious program of industrial policy; but it was so plodding and proceduralist that it built little before voters re-elected Trump. Since Trump imposed tariffs in April, the US has lost around 65,000 manufacturing jobs.12 His administration shows little interest in capturing electromagnetism before China overruns that field. Trump is more interested in protectionism rather than export promotion, which risks turning American industries into fossils like its exquisitely protected and horribly inefficient shipbuilding industry.&lt;/p&gt;
    &lt;p&gt;One of the Trump administration’s biggest blunders was its decision to raid a battery plant in Georgia, which put 300 Korean engineers in chains before deporting them. I suspect that any Korean, Taiwanese, or European engineer would ponder that episode before accepting a job posting to the United States. What a contrast that looks with China’s approach, which for decades has been to welcome managers from Walmart, Apple, or Tesla to train its workforce.&lt;/p&gt;
    &lt;p&gt;Will the US solve manufacturing with AI? Well, maybe, because superintelligence is supposed to solve everything. But there’s a risk that AI will destabilize society before it fixes the industrial base. When I walk around the library at Stanford, I see students plugging everything into AI tools; when they need a break, they’re watching short-form videos on their phones. These videos have been marvelously transformed by AI tools. Shortly after OpenAI released Sora 2, I had brunch with a friend who told me that he created an AI video of himself expertly breakdancing that fooled his five-year-old; another friend piped up to say that she created an AI video of herself that fooled her mother. AI chatbots are skilled at providing emotional companionship: Jasmine Sun discussed how they are able to seduce any segment of society, while pointing to a survey that 52 percent of teens regularly interact with AI companions. I’m not advocating for regulation. But I think it’s reasonable for the world to hope that AI labs will exercise some degree of forbearance before they release their shattering tools.&lt;/p&gt;
    &lt;p&gt;While I feel apprehensive about the US, I am much more gloomy about Europe. I have a hard time squaring the poor prospects of Europe over the next decade with the smugness that Europeans have for themselves. I spent most of the summer in Copenhagen. There’s no doubt that quality of life in most European cities is superb, especially for what I care about: food, opera, walkable streets, access to nature. But a decade of low economic growth is biting. European prices and taxes can be so high while salaries can be so low. For all the American complaints about home affordability, relative housing costs can be even worse in big European cities. London has the house prices of California and the income levels of Mississippi.&lt;/p&gt;
    &lt;p&gt;I remember two vivid episodes from Copenhagen. One day I read the news that the share price of Novo Nordisk — unquestionably one of Europe’s technological successes, along with ASML — collapsed as a result of sustained competition from US-based Eli Lilly as well as its misfortunes navigating the US regulatory system. I also watched Ursula von der Leyen visit Trump in the White House to graciously accept his EU tariffs. It’s already been clear that China has begun to maul European industry. What the Novo Nordisk news made me appreciate was that American companies are comprehensively outworking their European counterparts in biotech in addition to software and finance. Europe is losing the two-front battle against the Chinese on manufacturing and the Americans on services.&lt;/p&gt;
    &lt;p&gt;Perhaps Europe could have recruited some professors from the United States. American academics wouldn’t have needed Trump’s insults to act on their Europhile impulses. And yet European initiatives have not yet been able to brain drain much of this class. That’s mostly because European governments have little funding to offer. European universities have failed to build substantial endowments, so their revenues are dependent on the taxpaying public, which also must support a million other initiatives. An American academic who wants to move to Europe would have to accept more teaching and administrative work, lose tenure, and for the pleasure of all that, probably halve her pay. She would likely also suffer the resentment of European peers, who scoff at the idea that better paid Americans are now refugees. Trump threw a lot against US universities; they are holding up okay, and I think they will remain strong.&lt;/p&gt;
    &lt;p&gt;Europeans are right to gloat they are not under the rule of Trump. But for all of Trump’s ills, I see him as a sign of the underlying dynamism of the US. Who else would have elected so whimsical a leader to this high office? Trump forces questions that Europeans have no appetite to confront, proud as they are in being superior to both Americans and Chinese. I submit that Europeans ought to be more circumspect in their self-satisfaction. Chaos is only one election away. Right-populist parties are outpolling ruling incumbent parties pretty much everywhere, and it is as likely as not that Trumps with European characteristics will engulf the continent by the end of the decade.&lt;/p&gt;
    &lt;p&gt;So I am betting that the US and China are more compelling forces for change. Stalin was fond of telling a story from his experience in Leipzig in 1907, when, to his astonishment, 200 German workers failed to turn up to a socialist meeting because no ticket controller was on the platform to punch their train tickets, citing this experience as proof of the hopelessness of Germanic obedience. Could anyone imagine Chinese or Americans being so obedient? One advantage for the US and China is that both countries are at least interested in growth. You don’t have to convince the elites or the populace that growth is good or that entrepreneurs could be celebrated. Meanwhile in Europe, perhaps 15 percent of the electorate actively believes in degrowth. I feel it’s impossible to convince Europeans to act in their self interest. You can’t even convince them to adopt air conditioning in the summer.&lt;/p&gt;
    &lt;p&gt;The personal is the geopolitical&lt;/p&gt;
    &lt;p&gt;I’m not a doomer on AI or the broader state of the world. Across the US, China, and Europe, people generally enjoy comfortable lives that are free from fear. The market goes up. AI tools improve. Over the years I lived in China, I knew that life was more mundane than the headlines made out. Now that headlines and tweets are more negative everywhere, I know that things are not so bad in most places.&lt;/p&gt;
    &lt;p&gt;What I want is for everyone to do better. I opened my book by saying that Chinese and Americans are the most alike people in the world. They both are driven by a yearning for the future. They feel the draw of better times ahead, which is missing for Europeans, those people who have a sense of optimism only about the past.&lt;/p&gt;
    &lt;p&gt;I believe that modern China is one of the most ahistorical nations in the world. The state and the education system may talk insistently about its thousands of years of continuous history. But no other society has also been so destructive of its own history. The physical past has been disfigured by the attention of the Red Guards and the inattention of urban bulldozers. The social past is contorted by outrageous textbooks, which implement enforced forgetting of major traumas. For tragedies too widely experienced in modern times to be censored — the Cultural Revolution, the one-child policy, Zero Covid — the party discourages reflection in the name of protecting the state’s sensitivity.&lt;/p&gt;
    &lt;p&gt;The United States isn’t so good at celebrating its history either. 2026 is the 250th anniversary of the country’s founding. Where are the monuments to exalt that history? Most of the planned celebrations look small bore. Why hasn’t the federal government built a technological specimen as sublime as the Golden Gate Bridge, the Hoover Dam, or the Apollo missions? Probably because planning for any project should have commenced 10, 20, or 30 years ago. No president would have gotten around to starting a project that has no chance of being completed in his term. Lack of action due to the expectation of long timelines is one of the sins of the lawyerly society.&lt;/p&gt;
    &lt;p&gt;But American problems seem more fixable to me than Chinese problems. That’s why I live here in the US. I made clear in my book that I am drawn to pluralism as well as a broader conception of human flourishing than one that could be delivered by the Communist Party. The United States still draws many of the most ambitious people in the world, few of whom want to move to China. Even now a significant number of Chinese would jump to emigrate to the US if they felt they could be welcomed. But this enduring American advantage should not excuse the US from patching up its deficiencies.&lt;/p&gt;
    &lt;p&gt;A light grab-bag of complaints: While the rich have access to concierge doctors and the world’s best healthcare, the United States cannot organize a pandemic response; it is bioprosperity for the individual and measles for the many. I learned recently that the Bay Area has 26 separate transit agencies; is it really a triumph of democracy to have so many unconsolidated efforts? I wonder whether we can accuse the California government of subverting the will of the people by making so little progress on its high-speed rail, which was approved by referendum in 2008; California rail authorities take more pride in creating jobs than doing the job. I am tempted to use the language from American foreign policy at home. Why talk about American credibility only in terms of combat? Why shouldn’t the failure to deliver on big projects, after spending so much money, constitute a more severe blow to the credibility of the American project? Is the state of the US defense industrial base really deterring adversaries?&lt;/p&gt;
    &lt;p&gt;I won’t belabor issues with American public works or manufacturing. I’ll suggest only that the US ought to be acting with greater curiosity on how to do better. It doesn’t have to become China; but it should better study China’s successes. There is a 21st century playbook for becoming an industrial power and China has written it. This playbook consists of infrastructure development, solicitation of foreign investment, industrial subsidies, and the creation of industrial ecosystems. I hope that the US will stop attributing all of China’s successes to stealing. If such a program would be sufficient for building a world-class industry, then American spooks should dedicate their formidable capabilities to extracting Chinese industrial secrets. The reality is that there is little to be learned from blueprints. By failing to recognize China’s real strengths — the industrial ecosystems pulsating with process knowledge — the US is only cheating itself.&lt;/p&gt;
    &lt;p&gt;The future of US-China competition demands a resounding demonstration of the superiority of one country’s system to perform better for its citizens, which no country has thus achieved. Who’s going to come out ahead? I believe the competition is dynamic. It means we should not rely on static and structural features (like geography or demographics) to predict long-term advantage. One feature that unites American, Chinese, and European elites is the tendency to close ranks behind bad ideas and bad leaders. They are all skilled at dreaming up new ways to squander their advantages. Silicon Valley, for example, succeeds in spite of the generations-long governance failures of California. Imagine how much more vibrant Chinese society could be if it could escape the weight of overbearing censors in Beijing.&lt;/p&gt;
    &lt;p&gt;Competition will be dynamic because people have agency. The country that is ahead at any given moment will commit mistakes driven by overconfidence, while the country that is behind will feel the crack of the whip to reform. Implosion is always an option. In 2021, Xi Jinping was on top of the world, witnessing the omnishambles of the western pandemic response combined with the political disgrace of January 6. So he proceeded to smack around tech founders and initiate a controlled demolition of the property sector, which are two of the policies most responsible for China’s economic sluggishness today. Now, Beijing is trying to get a grip on its weaknesses. If either the US or China falls too far behind the other, the laggard will sweat to catch up. That drive will mean that competition will go on for years and decades.&lt;/p&gt;
    &lt;p&gt;In the competition for who might grow to be more humorous, I give a slight edge to the Chinese rather than to Silicon Valley.&lt;/p&gt;
    &lt;p&gt;No, I don’t expect the Communist Party ever to be funny. But there is a growing contrast between the baleful formality of the political system and the inexhaustible informality of Chinese society. Now that China is bidding farewell to its era of hypergrowth, young people are asking what they want to do with their lives. Fewer of them are interested in doing crazy hours in tech companies or big banks. Some of them are having fun in comedy sketches and stand-up shows. The increasingly gerontocratic Communist Party is not so much hovering over them as existing on a slightly different plane, speaking in strange apocalyptic tongues. Over the long run, I bet that the exuberance and rollicking nature of Chinese society will outlive the lusterless political system.&lt;/p&gt;
    &lt;p&gt;I wish that the tech world could learn to present broader cultural appeal. I hope that Silicon Valley could learn some of the humorousness of New York (or at least LA.) It’s unfortunate that any show or movie made about Silicon Valley is full of awkward nerds; by contrast, Hollywood reliably finds attractive leads when it makes movies about Wall Street. So long as the tech world is talking about the Machine God and the Antichrist, so long as it declines to read more broadly, so long as it is mostly inward looking, it will continue to alienate big parts of the world. But the longer I’m in California, the more easy I find it to be a sunny optimist. So I’m hopeful that the lovable nerds there will be able to present their own smiling optimism to the rest of the world.&lt;/p&gt;
    &lt;p&gt;I thank a number of people for reading a draft of this section and discussing the core ideas with me.&lt;/p&gt;
    &lt;p&gt;***&lt;/p&gt;
    &lt;p&gt;Of all the feedback I’ve received for my book, the most devastating came from my mother. After one of my television appearances, she called me to say: “Son, you looked terrible. Are you sick?” I accept that she, a former TV news anchor, has standing to judge. Still I could only reply with a quavering voice: “Mom, you’re so mean.”13&lt;/p&gt;
    &lt;p&gt;Other readers have been kinder to Breakneck. It reached #3 on the New York Times bestseller list and was also a bestseller on its monthly Business list. I went on podcasts, radio, TV, and spoke at book events. Breakneck was a finalist for the FT/Schroders best business book of the year and it has been a book of the year in several big publications. It’s being translated into 17 languages as of this writing.&lt;/p&gt;
    &lt;p&gt;I’ve learned a lot over the past four months.&lt;/p&gt;
    &lt;p&gt;Why did Breakneck do well? I think four reasons, in descending order of importance. First, timing. It came out in a year of many China headlines — DeepSeek, trade war, 15th Five-Year Plan — and five months after Abundance, which primed readers for the idea that Americans are right to be frustrated by their state. Second, the book had the memetic framing of lawyers and engineers, which also encouraged people to wonder how other countries could be described. (What is India? The UK?) Third, people know my work through these letters. Fourth and least important was the content in the book. An author spends so much time workshopping words and sentences. I accept that a book’s reception is subject to the vagaries of the market and the memelords.&lt;/p&gt;
    &lt;p&gt;I don’t regret a minute of workshopping. I would have liked to workshop some more. Like every author, I wish I had more time to add a finer polish to the entire manuscript. I was heartened when a writer I admire told me that no author is ever more than 85 percent satisfied with their work; to hope for more would be profligate. In any case, I’m proud of the content. If it weren’t in place, I wouldn’t have had positive reviews in mainstream publications like the Financial Times, the Wall Street Journal, the New Yorker, and the Times. I was glad to see praise from both left publications like Jacobin and right publications like American Affairs.&lt;/p&gt;
    &lt;p&gt;I tried to write this book to reach a non-coast audience. Ideally I wanted a lawyer in say Indiana or Ohio to read Breakneck, rather than for it to be picked up only by folks in New York, DC, San Francisco, and the terminally online. So I was happy to hear from a broader cross-section of readers who wrote to tell me that they’d never visited China before and are now curious to do so. It’s a shame that book tours are no longer much of a thing for authors. Publishers don’t necessarily bring authors to book readings in Houston, Los Angeles, New Orleans, or other big cities as a matter of course. I was happy, however, to visit Dallas for the first time this year. After giving a talk in October, I wandered over to the Texas State Fair. Who can resist a place that calls itself “the most Texan place on earth?” I had a fabulous time walking through the fairground, the livestock pens, and the food stalls. The atmosphere made me realize that friendly and pragmatic Texans are what I imagined all Americans to be like, at least in my Canadian mind.&lt;/p&gt;
    &lt;p&gt;I’ve enjoyed opening my inbox to see reader notes. I love hearing from two groups in particular: engineers and other technical people who feel better appreciated for their work; and Chinese readers who tell me that I’ve captured something authentic. Someone emailed a set of book recommendations for the Spanish Civil War. An investor emailed to enlighten me that Copenhagen’s marvelous subways (which I praise for being clean and driverless) were built by Italian construction companies. An agricultural consultant emailed to tell me about her eye-opening experiences visiting big Chinese farms. These notes are small delights for any author. A stranger but still charming event was to see the Blue Book Club. About 20 people gathered in Brooklyn this November to discuss Breakneck, but not before the hosts issued a light exam to make sure that the participants actually read the book.&lt;/p&gt;
    &lt;p&gt;Book promotion made me more of a public figure. I did my best to have fun with it. It wasn’t as hard as I imagined: podcast and TV hosts are as bored by self-serious personalities as the rest of us are. Readers have been friendly as they’ve recognized me in public. There was only one instance of a bit too much friendliness, when someone sidled up to the urinal beside mine in a public bathroom to tell me that he liked my book.&lt;/p&gt;
    &lt;p&gt;I’ve learned it is not possible to value mentors too highly. I am blessed to have good counselors. I mean not only my publishing house, my literary agent, and my writing coach who directly support my work. I am grateful to folks who give me time to reflect on the course of my thinking, especially the ones who have by now mentored me for over a decade. Friends have been generous in all sorts of ways. Eugene, Tina, Maran, Ren, James, Caleb, Alec, and Arthur hosted book parties. Joe Weisenthal wrote in the Odd Lots newsletter: “Total Dan Wang victory” on his view that most of the world is seeing China through the industrial lens I’ve been writing about. Afra hosted a Mandarin-language book discussion in which someone accused me of having a “gentle and vulnerable” voice. Alice, who doesn’t often pick up books on China, told me that my fondness for both the US and China shone through the book. It reconnected me with two friends from Ottawa that I haven’t heard from since high school.&lt;/p&gt;
    &lt;p&gt;I am grateful that Waterstones Piccadilly and Daunt Books in Marylebone have given my book prominent display. One surprise was that my book sold well in the United Kingdom. I’ve been pretty relentless at telling Brits that they are the PPE society and that they excel in the sounding-clever industries — television, journalism, finance, and universities. Upon reflection, it makes sense that the British are reading Breakneck and Abundance. Every problem in the lawyerly society is worse in the UK. I thought that California’s high-speed rail project was an embarrassment; then I learned about the Leeds tram network. First legislated in 1993, mass transit might not come to West Yorkshire until the late 2030s. It reminds me of the lawsuit in Bleak House: “The little plaintiff or defendant who was promised a new rocking-horse when Jarndyce and Jarndyce should be settled has grown up, possessed himself of a real horse, and trotted away into the other world.” At least Californians are struggling over something mighty; I hope that Leeds will one day have a tram.14&lt;/p&gt;
    &lt;p&gt;Homebuilding in London has collapsed. Heathrow has been making plans to build a third runway for twenty years, which is now expected to cost $20 billion. Britain’s electrical network is in even worse disrepair than America’s. I am not sure if it is a geopolitical asset to be able to stiff-upper-lip one’s way through ineffectual government. Maybe it’s more of a liability. But my experience of criticizing Brits resembles my experience of criticizing lawyers. They tend to nod along to my critiques; many of them take me further than where I’d like to go. It’s all very disarming.&lt;/p&gt;
    &lt;p&gt;I’ve been lucky to have smart critics. It’s any author’s dream to see people pick up the book and examine the arguments. Jon Sine wanted to have more specific data on engineers and lawyers, then proceeded to supply it while wrapping it in a narrative on a trip to Wushan. Charles Yang noted that I don’t have much by way of policy suggestions, but he also grasped that I’m trying to change the culture of governing elites while suggesting that Breakneck is an incitement to initiate “tractable mimetic competition.” Jen-Kuan Wang argued that China was not quite the right model for the US, but that Taiwan and the rest of Northeast Asia better show how to survive China Shocks. I am grateful to see constructive engagement with my work. I was unimpressed with only one piece of commentary. Law professors Curtis Milhaupt and Angela Zhang wrote in Project Syndicate: “Lawless State Capitalism Is No Answer to China’s Rise,” as if I were advocating for that. Since the authors mention the book only at the start without engaging with any of the content, I suspect they are critics who chose not to read the book.&lt;/p&gt;
    &lt;p&gt;I learned of Leo Rosten’s quip that it is the weak who are cruel, and gentleness to be expected only from the strong. Every author will hear from online commentators who belligerently misunderstand their work. Saying anything about China tends to rile up the online commentators. Either the hawks will pounce because they believe that the whole country is evil and that its progress is fake; or the tankies will defend the idea that China has achieved socialist utopia. These people live on Twitter and Youtube, offering the stock comment that “this person knows nothing about China.” That’s of course hard to respond to because they offer no analytical content to rebut. Part of what makes the China discourse exasperating is that people have to choose sides all the time, which makes everyone dumber. At least I didn’t have it as bad as Ezra and Derek with Abundance.&lt;/p&gt;
    &lt;p&gt;I’ve learned more about myself as a writer this year. Namely, I like doing it. Writing a book is sometimes enough to make an author forswear the experience for a long time. Then there are the really perverse, for whom a taste of publishing is enough to tempt one into becoming a serial offender. After writing this book, I most looked forward to writing this long-ass letter, the very one you’re reading now.&lt;/p&gt;
    &lt;p&gt;Some writers work like sculptors: they produce something fully chiseled that could stand forever. Novelists tend to be like that. Rather than being a sculptor, I see myself as being a musician. After a performance, no matter how it goes, the musician’s task is to start practicing for the next one. It’s hard for US-China books to rest like sculptures. So I am happy to get back to work, writing iteratively to refine the same few themes that animate me: technology production, industrial ecosystems, US-China competition.&lt;/p&gt;
    &lt;p&gt;Musicians don’t usually practice by running a whole piece from start to finish. Rather, practice sessions tend to focus on particular passages, with a full run-through only before performance. Before I publish this letter, I retype the whole thing from start to finish. It means I take the draft that lives in my Notes app on the left half of my screen while I retype the whole thing into the Google Docs on the right side of my screen. It’s a final check to catch infelicities. More importantly, by simulating the experience of a reader, it’s another way to see if the whole essay stands together.&lt;/p&gt;
    &lt;p&gt;I’ve learned that it is better to wear a tie with a blazer. That was part of my training to be a speaker. The book tour forces you to have answers that last 30 seconds for TV, 30 minutes for a talk, and 3 hours for the more bruising podcasts. I’ve learned that delivering a good talk is a rare skill. I don’t think I could ever be satisfied by a talk I’ll give, because there will always be a stumble, or l’esprit de l’escalier kicks in. The piece of speaking advice I’ve remembered for many years came from Tim Harford: good speaking rewards those who are able to prepare extensively and who are also able to improvise. My favorite book talk took place at the Hoover Institution, hosted by Stephen Kotkin (who is himself peerless at giving excellent lectures). In the summer, I spent two hours asking Kotkin how historians work.&lt;/p&gt;
    &lt;p&gt;One day in October, I went on six podcasts. I haven’t counted the number of podcasts I’ve been on, but I think the number is north of 70. There’s a lot I don’t understand. Are so many people really listening to podcasts? What is the appeal of a video featuring two people with giant microphones in their faces? Do we really have to live in an oral culture world?&lt;/p&gt;
    &lt;p&gt;I’ve noticed the wide range of effort that people put into podcasts. Some hosts edit extensively — Freakonomics Radio stands out for the sheer number of producers and editors. Other hosts release their episodes more or less unedited. Freakonomics stood out to me because Stephen Dubner was able to make the conversation so much fun. Going on Ross Douthat’s Interesting Times was more appropriately serious. Search Engine was impressive for the amount of narrative that PJ Vogt imbued into our more rambling conversation. It felt like a homecoming to return to Odd Lots, where I could tease Tracy Alloway for her country life and Joe Weisenthal over Moby Dick. David Perell read nearly everything I’d written to discuss the writing process. I went on Francis Fukuyama’s podcast to ask him about his relationship with Wang Qishan as well as why he is now banned from China. Works in Progress, Statecraft, and ChinaTalk were each fun in their own way.&lt;/p&gt;
    &lt;p&gt;You don’t really mature into being on podcast mode until you’ve done a lot of them. That’s why I proposed to Tyler to go on his show near the end of the book tour. Conversations with Tyler is the first podcast I regularly started listening to, whose early episodes I still remember well. Before our interview, I told Tyler that he was my final boss. Both of us were playful. I challenged Tyler to enumerate the list of 12th-century popes and teased him about being a New Jersey suburban boy. He told me that America has great infrastructure and healthcare before issuing an intellectual Turing test to see if I could say why he likes Yunnan more than any other place. I had the chance to bring up one of the most sublime pieces of Rossini, the gently entwining trio that concludes Le Comte Ory. Afterwards, commentators wrote that he and I were confrontational. But they should have watched the video, in which Tyler was smiling as much as he ever would.&lt;/p&gt;
    &lt;p&gt;Again, who is listening to all these podcasts? I don’t much look at my book sales, but it doesn’t feel like podcasts move the needle. And a book might create a lot of social media buzz, with all the right people saying all the right things, but Twitter too doesn’t drive sales. It was two platforms that moved a lot of my books: television and radio. People bought after seeing me on CNN or hearing me on NPR. The straightforward explanation is that older people have the time and the money to buy books. Even a brief appearance on TV could reach an ambient audience of millions, a few of whom purchase afterwards. Social media and podcasts are more valuable for driving conversation among the youths.&lt;/p&gt;
    &lt;p&gt;It’s stirring to see that people buy books at all. I do not doubt that we are moving towards an oral culture. But the publishing industry is holding up. A lot of excellent books came out this year, including many on China. Revenues at most of the big trade publishers have been rising. Barnes &amp;amp; Noble is opening 60 new stores in 2026. A lot of the growth in the book trade is coming from romantasy and fairy smut, while the genre of nonfiction is in slight decline. That’s all good, I’m no snob. It’s pleasant to believe that a few decades from now, people might still hold physical books in their hands.&lt;/p&gt;
    &lt;p&gt;I’ve learned that books produce an invitation to all sorts of conversations, both closed and open. A physical book, bound and printed, has a totemic quality. It’s funny that PDFs sometimes circulate better than web-optimized pages; there’s something about strict formatting that establishes authority. Physical books can also last a long time. This letter that you’re reading will no longer be sent around a month from now, while my book can sit unread on shelves for years to gather dust. So I’m still keen to encourage friends to write their books. It’s a great way to sort through one’s ideas and to ease them into the conversation.&lt;/p&gt;
    &lt;p&gt;If I yearned for commercial success in our new oral culture, I would lend my soft voice to narrate romantasy novels. But I worry the superintelligence will devour that job. So I will stick to longform writing. However strange our new world will become, there will always be a class of people who want to engage with essays and books. Over the long term, writing might enjoy the fate of the opera and the symphony. People have been heralding the death of classical music for a century. Yes, much of its audience is pretty old. But there will always be more old people — especially if Silicon Valley delivers on longevity treatments. The job of authors and opera houses is to keep holding on to people who are maturing into pleasures that technological platforms cannot provide. The demographic trend is on our side: the world is producing more old people than youths. I want to be a sunny Californian optimist about everything, including the fate of the written word.&lt;/p&gt;
    &lt;p&gt;***&lt;/p&gt;
    &lt;p&gt;It’s time to talk about (other) books.&lt;/p&gt;
    &lt;p&gt;I last picked up Stendhal’s The Red and the Black a decade ago. I wasn’t certain that the novel, which I keep calling my favorite, would hold up on re-reading. It did gorgeously. The plot centers on Julien Sorel, the handsome son of a poor sawyer. After Julien dons the black garb of the priesthood, he moves from the periphery of his Alpine village into the luminous center of Parisian society. Along the way, he seduces two extraordinary women, the gentle Mme. de Rênal and the magnificent Mathilde, while he commits, in the name of love, acts of extraordinary stupidity. Julien — who is possessed by galloping ambition and extravagant pride — maneuvers his way towards aristocratic distinction and romantic triumph. Then he loses all.&lt;/p&gt;
    &lt;p&gt;More than anything else, Stendhal is funny, especially about love. Only Proust surpasses Stendhal at the skill of guiding the reader into the transports of intoxicating love, only to snap them out of it by skewering the foolishness of Julien or Mathilde. Stendhal doesn’t create the cool detachment that Flaubert or Fontane bring to their characters. Rather, he’s eager to envelop the reader into his passionate embrace. The list of writers who have succumbed to Stendhal includes Nietzsche, Beauvoir, Girard, Balzac, and Robert Alter, who, before he translated the Hebrew Bible, wrote an admiring biography of Stendhal titled A Lion for Love.&lt;/p&gt;
    &lt;p&gt;Why is it that reading Stendhal feels like making a discovery? Stendhal might be just on the cusp of the pantheon because his critics can’t get over the significance of his flaws while his fans cannot forget the delights of his peaks. In that sense, Stendhal is like Rossini. Neither produced a ripe and perfect work; I can’t help but feel some disappointment when I listen to Rossini, who couldn’t achieve the musical perfection of Mozart or the dramatic conviction of Verdi. And yet the peak moments of Stendhal and Rossini produce ecstatic joy. It’s no surprise that Stendhal and Rossini are both renowned for their ravenous appetites, nor that Stendhal wrote his own admiring biography of Rossini, filled with his characteristic amusing falsehoods. Erich Auerbach grasped the point that Stendhal ought to be appreciated for his peaks rather than his average. Stendhal has pride of place in Mimesis, as an author who fluctuated between “realistic candor in general and silly mystification in particulars,” and between “cold self-control, rapturous abandonment to sensual pleasures, and sentimental vaingloriousness.” In other words, Stendhal embodies the spirit of opera buffa in novel.&lt;/p&gt;
    &lt;p&gt;I am often drawn to Ecclesiastes. In Robert Alter’s hands, the gloomy prophet behind the book is named Qohelet, and though I value Alter’s translation, I favor a few of the more iconic lines from King James: “vanity of vanities, all is vanity” and “better to hear the rebuke of the wise than the song of fools.” Melancholy attracts me in any form, and isn’t Ecclesiastes the most melancholic book? The prophet makes small allowances for joy and celebration before hauling the reader back into the house of mourning. There is something deeply satisfying with reading out loud phrases like: “for in mere breath did it come, and into darkness it goes, and in darkness its name is covered.” Though King James is iconic, Robert Alter better conveys overall the literary power of the Hebrew Bible.&lt;/p&gt;
    &lt;p&gt;Marlen Haushofer’s The Wall is short and engrossing. It was deemed a “Cold War” novel by the German press when it was published in 1963. Little about it comes across as being geopolitical today. Rather, Haushofer has written a book about domesticity that manages to be gripping. The heroine spends her days milking her cow, minding her garden, and caring for her cat and dog while living in total isolation in the Alps. She would not survive if she lacked for any of the above. As Katherine Rundell once wrote, “It’s easier to trust a writer who writes great food: they are a person who has paid attention to the world.” Haushofer pays loving attention to the details of life. It never became boring to read about the narrator churning her butter, tending to her potato field, or chopping wood throughout the year.&lt;/p&gt;
    &lt;p&gt;After a man turns 30, he has to choose between specializing in the history of the Roman Empire or the World Wars. Within the latter, one tends to focus on the Pacific Theater, the Western Front, or the Eastern Front. For me, the last theater is the most interesting. No human effort approaches the gargantuan scale of Operation Barbarossa or the Soviet reply. The same fields, one world war earlier, produced other shocks. Nick Lloyd’s The Eastern Front covers the clashes between Imperial Germany and the Russian Empire as well as the Austro-Hungarians against the Italians and the Serbs. Whereas the western front was essentially static throughout the whole war, the east was characterized by the sort of maneuver warfare that most generals had expected to fight. It was the field of legendary confrontations like the Gorlice-Tarnow campaign, the Brusilov offensive, and the 37th Battle of the Isonzo.&lt;/p&gt;
    &lt;p&gt;One of the revelations of Lloyd’s book is how well the Germans fought and how poorly Austro-Hungary performed, ending the war by self-liquidating. Immediately after the war began, German military attachés had already begun to fret that “the major trouble with the Austro-Hungarian Army is currently its weakness in combat.” It became nearly comical how often the Kaiser had to intervene, in the latter half of the war, to stop Emperor Karl from surrendering to the Entente. Perhaps it shouldn’t be surprising that the fighting force of an army where the officers all spoke German and regiments spoke Czech or Croatian could not overwhelm the adversary. The eastern front had diplomatic scheming that was nearly as impressive as the battlefield breakthroughs. It was, after all, the political section of the German general staff that had the imaginative idea to ship Lenin from Switzerland to Russia in order to make revolution.&lt;/p&gt;
    &lt;p&gt;I’m looking for a book that has a clear focus on bigger questions: How did Hohenzollern Prussia outmaneuver Habsburg Austria? And how did they become such firm allies before the war? John Boyer’s Austria 1867-1955 offers parts of the answer, though not in a conceptually organized way. It’s a work of history written for specialists, which means that the narrative serves the footnotes rather than the other way around. Too much of the book is focused on how politicians grappled with each other. Still it yields many morsels. One difference between Austrian nobles and Prussian nobles was that the former did not view a military life as attractive — part of the reason that Austrians performed so badly in war. Austria’s partner was sometimes rooting for the adversary: “a large, successful Prussia was Hungary’s best guarantee that Austria would not gain a superior position to dominate the Hungarian elites.” And this insight feels like a good explanation of the attractiveness of Austrian Catholicism, which “combines a Jansenist, puritanical strain with exuberant baroque piety.” It’s the sort of exuberance that produced a Mozart, rather than more gloomy and ardent Spanish Catholicism that produced the Inquisition.&lt;/p&gt;
    &lt;p&gt;One lesson from the latter years of Austro-Hungary is a good reminder that periods of state decay often correspond with eras of cultural flowering. 1913: The Year Before the Storm presents a whimsical slice of Central Europe. Art historian Florian Illies collates fragments of leading figures month by month, diary-entry style. People were running into each other all the time. Duchamp, d’Annunzio, Debussy at the premiere of the Rite of Spring. Stalin potentially tipping his hat at Hitler, as both residents of Vienna were known to take evening strolls through the gardens of Schönbrunn. Matisse bringing flowers to Picasso while the latter was sick. Rainer Maria Rilke being moody at the seaside with Sidonie Nadherny while she was running off into the arms of Karl Kraus. The celebrated love affairs between Franz Kafka and Felice Bauer, Igor Stravinsky and Coco Chanel, Alma Mahler and Oskar Kokoschka, Alma Mahler with Walter Gropius, Alma Mahler with anyone, really. 1913 is the year that modernism was born; the continent began to shatter the following year.&lt;/p&gt;
    &lt;p&gt;Nan Z. Da’s The Chinese Tragedy of King Lear also has an experimental form. Da is a professor of literature at Johns Hopkins who emigrated from Hangzhou before she was 7. One half of the book is a literary analysis of Shakespeare; the other half of the book is the story of the chaos of Maoist society and her family’s personal experiences of it. The novelty is the weaving of family history with a classic piece of literature. Sometimes these transitions are jarring, perhaps deliberately so. Da has just barely begun musing about the reign of Goneril and Regan before she launches into an exposition: “A history — I am thirty nine years old. My parents left China for the United States at this age.” But I liked this effort to map Mao’s madness onto Lear’s delirium as well as analogizing Deng’s tenacity to Edgar’s determination to lay low. And it convinced me that Lear is the most Chinese of Shakespeare’s plays. It is the marriage of the eastern emphasis on pro forma ceremonies, excessive flattery, and empty speechifying with the western practice of elder abuse. I’d like to read more experimental books like this one.&lt;/p&gt;
    &lt;p&gt;Susannah Clarke’s Piranesi is a glittering jewel. The setting is a mysterious, magical house. The narrator is a radiantly earnest explorer who self-identifies as a “Beloved Child of the House.” His warm curiosity makes this book an adventurer’s diary. I liked the fantasy elements of the first half better than the second half of the book, which disenchanted some of the story, so maybe it’s better to stop halfway through. Afterwards, I read Clarke’s earlier book, Jonathan Strange &amp;amp; Mr Norrell. It’s enjoyable too, especially for its partisanship of Northern English identity, though the book as a whole is wooly. Susannah Clarke offers a good case study of how authors can think about their work over time: an overlong first book that took decades to craft, followed by a shorter and more glittering second work. I can’t wait to see what her third book will be like.&lt;/p&gt;
    &lt;p&gt;(The Neue Galerie’s exhibition this year on New Objectivity led me to the work of German painter Carl Grossberg. This 1925 work spoke to me. Credit: Wikimedia.)&lt;/p&gt;
    &lt;p&gt;I’ve learned that Christmas is a good time to write. Emails stop and all is calm. I submitted my manuscript this time last year in Vietnam. This year, my wife and I are writing from Bali. Tropical Asia makes for great writing retreats. We have lazy mornings that feature a swim and a big breakfast; then we spend the rest of the day writing before going out in the evening for some really spicy food.&lt;/p&gt;
    &lt;p&gt;A few food questions to wrap up:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Is Da Nang the most underrated food city in Asia? Yes, we all know about excellent eating spots in Penang, Tokyo, Yunnan, etc. But I hardly ever hear about Da Nang, which has several Michelin listed places. I am still dreaming about its chewy rice products, the grilled meats, the spice mixes, the seafood soups, the not-too-sweet desserts. It’s well-listed on Michelin guides, but I hardly hear about it. Da Nang is my submission for a food city that ought to be better recognized as a destination.&lt;/item&gt;
      &lt;item&gt;Over the summer in Europe, I found myself wondering why Copenhagen has such amazing baked goods. I think its croissants are even better than in Paris. Then I found myself wondering about the quality distribution of croissants throughout the continent. They are not so good in Spain and Italy. I believe that Italy and Spain have the best overall cuisine in Europe; but they have been less interested in producing excellent baked goods. Is it because they don’t have as good butter? But they still eat a lot of cheese. The US is getting better croissants in big cities, which once more makes me appreciate that America has excellence across many cuisines, though they tend to be scattered.&lt;/item&gt;
      &lt;item&gt;Every winter, I find myself craving vitamin-rich tropical fruits. I mean mostly passionfruit, mango, papaya, eggfruit, and of course durian. American groceries are stocking more rambutan and dragonfruit. I wonder if they could stock even more. It’s always mango season somewhere, for example, so is it possible to find better mangoes throughout the year? Is there a subscription package to receive regular shipments of passionfruit and mango? I realize the durian supply chain is highly complicated (apparently the fruit is pollinated mostly by bats), but still it would be nice to have the fruit occasionally. I realize that tariffs are hurting access to American essentials like coffee and bananas. But I hope that Americans can continue to demand better fruits.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Alex Boyd has translated the what he calls the Collected Jokes of Xi Jinping here. https://www.ramble.media/p/is-xi-jinping-funny↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Most prominently on a 60 Minutes segment when Amodei said: “AI could wipe out half of all entry-level white-collar jobs and spike unemployment to 10% to 20% in the next one to five years.” https://www.cbsnews.com/news/anthropic-ceo-dario-amodei-warning-of-ai-potential-dangers-60-minutes-transcript/↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Eliezer’s 2008 post: “‘AI go FOOM.’ Just to be clear on the claim, “fast” means on a timescale of weeks or hours rather than years or decades; and “FOOM” means way the hell smarter than anything else around, capable of delivering in short time periods technological advancements that would take humans decades, probably including full-scale molecular nanotechnology” https://archive.ph/tNdrf↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Gavin Leech discusses the diffusion of Chinese LLMs here: https://www.gleech.org/paper↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Matt Sheehan of Carnegie shows that only 10 percent of top AI researchers have left the US between 2019 to 2025. https://carnegieendowment.org/emissary/2025/12/china-ai-researchers-us-talent-pool↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;ChinaTalk produced an enlightening Socratic dialogue on whether Beijing is racing to build superintelligence. Conclusion: probably not. https://www.chinatalk.media/p/is-china-agi-pilled↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pavlo Zvenyhorodskyi and Scott Singer, also of Carnegie, have produced valuable work on embodied AI: https://carnegieendowment.org/research/2025/11/embodied-ai-china-smart-robots↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Calculations from Weijian Shan: “In 2024, Shanghai produced one million vehicles with 20,000 workers, while California produced 464,000 with 22,000 workers.” https://research.gavekal.com/article/unraveling-chinas-productivity-paradox↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;As Grothendieck wrote: “The sea advances insensibly and in silence, nothing seems to happen, nothing moves, the water is so far off you hardly hear it… yet it finally surrounds the resistant substance.” https://webusers.imj-prg.fr/~leila.schneps/grothendieckcircle/Mathbiographies/mclarty1.pdf↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;See Noah Smith for more on the electric tech stack: https://www.noahpinion.blog/p/why-every-country-needs-to-master↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Ryan Fedasiuk wrote an excellent essay on the lack of a China strategy across US administrations: https://theamericanenterprise.com/in-search-of-a-china-strategy/↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;MANEMP on FRED: https://fred.stlouisfed.org/series/MANEMP↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You can watch my interview with Fareed Zakaria here: https://edition.cnn.com/2025/10/26/world/video/gps-1026-china-us-trade-showdown↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Thanks to Mike Bird for alerting me to the Leeds tram: https://x.com/Birdyword/status/2001570894171500775↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46454413</guid><pubDate>Thu, 01 Jan 2026 14:32:12 +0000</pubDate></item><item><title>Python numbers every programmer should know</title><link>https://mkennedy.codes/posts/python-numbers-every-programmer-should-know/</link><description>&lt;doc fingerprint="24f9e910fc8082ca"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;There are numbers every Python programmer should know. For example, how fast or slow is it to add an item to a list in Python? What about opening a file? Is that less than a millisecond? Is there something that makes that slower than you might have guessed? If you have a performance sensitive algorithm, which data structure should you use? How much memory does a floating point number use? What about a single character or the empty string? How fast is FastAPI compared to Django?&lt;/p&gt;
      &lt;p&gt;I wanted to take a moment and write down performance numbers specifically focused on Python developers. Below you will find an extensive table of such values. They are grouped by category. And I provided a couple of graphs for the more significant analysis below the table.&lt;/p&gt;
      &lt;p&gt;Acknowledgements: Inspired by Latency Numbers Every Programmer Should Know and similar resources.&lt;/p&gt;
      &lt;head rend="h3"&gt;Source code for the benchmarks&lt;/head&gt;
      &lt;p&gt;This article is posted without any code. I encourage you to dig into the benchmarks. The code is available on GitHub at:&lt;/p&gt;
      &lt;p&gt;https://github.com/mikeckennedy/python-numbers-everyone-should-know&lt;/p&gt;
      &lt;p&gt;The benchmarks were run on the sytem described in this table. While yours may be faster or slower, the most important thing to consider is relative comparisons.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Property&lt;/cell&gt;
          &lt;cell role="head"&gt;Value&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Python Version&lt;/cell&gt;
          &lt;cell&gt;CPython 3.14.2&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Hardware&lt;/cell&gt;
          &lt;cell&gt;Mac Mini M4 Pro&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Platform&lt;/cell&gt;
          &lt;cell&gt;macOS Tahoe (26.2)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Processor&lt;/cell&gt;
          &lt;cell&gt;ARM&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;CPU Cores&lt;/cell&gt;
          &lt;cell&gt;14 physical / 14 logical&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;RAM&lt;/cell&gt;
          &lt;cell&gt;24 GB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Timestamp&lt;/cell&gt;
          &lt;cell&gt;2025-12-30&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;TL;DR; Python Numbers&lt;/head&gt;
      &lt;p&gt;This first version is a quick “pyramid” of growing time/size for common Python ops. There is much more detail below.&lt;/p&gt;
      &lt;p&gt;Python Operation Latency Numbers (the pyramid)&lt;/p&gt;
      &lt;quote&gt; Attribute read (obj.x) 14 ns Dict key lookup 22 ns 1.5x attr Function call (empty) 22 ns List append 29 ns 2x attr f-string formatting 65 ns 3x function Exception raised + caught 140 ns 10x attr orjson.dumps() complex object 310 ns 0.3 μs json.loads() simple object 714 ns 0.7 μs 2x orjson sum() 1,000 integers 1,900 ns 1.9 μs 3x json SQLite SELECT by primary key 3,600 ns 3.6 μs 5x json Iterate 1,000-item list 7,900 ns 7.9 μs 2x SQLite read Open and close file 9,100 ns 9.1 μs 2x SQLite read asyncio run_until_complete (empty) 28,000 ns 28 μs 3x file open Write 1KB file 35,000 ns 35 μs 4x file open MongoDB find_one() by _id 121,000 ns 121 μs 3x write 1KB SQLite INSERT (with commit) 192,000 ns 192 μs 5x write 1KB Write 1MB file 207,000 ns 207 μs 6x write 1KB import json 2,900,000 ns 2,900 μs 3 ms 15x write 1MB import asyncio 17,700,000 ns 17,700 μs 18 ms 6x import json import fastapi 104,000,000 ns 104,000 μs 104 ms 6x import asyncio &lt;/quote&gt;
      &lt;p&gt;Python Memory Numbers (the pyramid)&lt;/p&gt;
      &lt;quote&gt; Float 24 bytes Small int (cached 0-256) 28 bytes Empty string 41 bytes Empty list 56 bytes 2x int Empty dict 64 bytes 2x int Empty set 216 bytes 8x int __slots__ class (5 attrs) 212 bytes 8x int Regular class (5 attrs) 694 bytes 25x int List of 1,000 ints 36,056 bytes 36 KB Dict of 1,000 items 64,952 bytes 65 KB List of 1,000 __slots__ instances 81,000 bytes 81 KB List of 1,000 regular instances 169,000 bytes 169 KB 2x slots list Empty Python process 16,000,000 bytes 16 MB &lt;/quote&gt;
      &lt;head rend="h2"&gt;Python numbers you should know (detailed version)&lt;/head&gt;
      &lt;p&gt;Here is a deeper table comparing many more details.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Category&lt;/cell&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
          &lt;cell role="head"&gt;Memory&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;💾 Memory&lt;/cell&gt;
          &lt;cell&gt;Empty Python process&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;15.73 MB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Empty string&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;41 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;100-char string&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;141 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Small int (0-256)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;28 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Large int&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;28 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Float&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;24 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Empty list&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;56 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List with 1,000 ints&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;35.2 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List with 1,000 floats&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;32.1 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Empty dict&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;64 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Dict with 1,000 items&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;63.4 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Empty set&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;216 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Set with 1,000 items&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;59.6 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Regular class instance (5 attrs)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;694 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;__slots__&lt;/code&gt; class instance (5 attrs)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;212 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List of 1,000 regular class instances&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;165.2 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List of 1,000 &lt;code&gt;__slots__&lt;/code&gt; class instances&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;79.1 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;dataclass instance&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;694 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;namedtuple instance&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;228 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;⚙️ Basic Ops&lt;/cell&gt;
          &lt;cell&gt;Add two integers&lt;/cell&gt;
          &lt;cell&gt;19.0 ns (52.7M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Add two floats&lt;/cell&gt;
          &lt;cell&gt;18.4 ns (54.4M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;String concatenation (small)&lt;/cell&gt;
          &lt;cell&gt;39.1 ns (25.6M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;f-string formatting&lt;/cell&gt;
          &lt;cell&gt;64.9 ns (15.4M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;.format()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;103 ns (9.7M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;%&lt;/code&gt; formatting&lt;/cell&gt;
          &lt;cell&gt;89.8 ns (11.1M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List append&lt;/cell&gt;
          &lt;cell&gt;28.7 ns (34.8M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List comprehension (1,000 items)&lt;/cell&gt;
          &lt;cell&gt;9.45 μs (105.8k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Equivalent for-loop (1,000 items)&lt;/cell&gt;
          &lt;cell&gt;11.9 μs (83.9k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;📦 Collections&lt;/cell&gt;
          &lt;cell&gt;Dict lookup by key&lt;/cell&gt;
          &lt;cell&gt;21.9 ns (45.7M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Set membership check&lt;/cell&gt;
          &lt;cell&gt;19.0 ns (52.7M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List index access&lt;/cell&gt;
          &lt;cell&gt;17.6 ns (56.8M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List membership check (1,000 items)&lt;/cell&gt;
          &lt;cell&gt;3.85 μs (259.6k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;len()&lt;/code&gt; on list&lt;/cell&gt;
          &lt;cell&gt;18.8 ns (53.3M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Iterate 1,000-item list&lt;/cell&gt;
          &lt;cell&gt;7.87 μs (127.0k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Iterate 1,000-item dict&lt;/cell&gt;
          &lt;cell&gt;8.74 μs (114.5k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;sum()&lt;/code&gt; of 1,000 ints&lt;/cell&gt;
          &lt;cell&gt;1.87 μs (534.8k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;🏷️ Attributes&lt;/cell&gt;
          &lt;cell&gt;Read from regular class&lt;/cell&gt;
          &lt;cell&gt;14.1 ns (70.9M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Write to regular class&lt;/cell&gt;
          &lt;cell&gt;15.7 ns (63.6M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Read from &lt;code&gt;__slots__&lt;/code&gt; class&lt;/cell&gt;
          &lt;cell&gt;14.1 ns (70.7M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Write to &lt;code&gt;__slots__&lt;/code&gt; class&lt;/cell&gt;
          &lt;cell&gt;16.4 ns (60.8M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Read from &lt;code&gt;@property&lt;/code&gt;&lt;/cell&gt;
          &lt;cell&gt;19.0 ns (52.8M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;getattr()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;13.8 ns (72.7M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;hasattr()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;23.8 ns (41.9M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;📄 JSON&lt;/cell&gt;
          &lt;cell&gt;&lt;code&gt;json.dumps()&lt;/code&gt; (simple)&lt;/cell&gt;
          &lt;cell&gt;708 ns (1.4M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;json.loads()&lt;/code&gt; (simple)&lt;/cell&gt;
          &lt;cell&gt;714 ns (1.4M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;json.dumps()&lt;/code&gt; (complex)&lt;/cell&gt;
          &lt;cell&gt;2.65 μs (376.8k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;json.loads()&lt;/code&gt; (complex)&lt;/cell&gt;
          &lt;cell&gt;2.22 μs (449.9k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;orjson.dumps()&lt;/code&gt; (complex)&lt;/cell&gt;
          &lt;cell&gt;310 ns (3.2M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;orjson.loads()&lt;/code&gt; (complex)&lt;/cell&gt;
          &lt;cell&gt;839 ns (1.2M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;ujson.dumps()&lt;/code&gt; (complex)&lt;/cell&gt;
          &lt;cell&gt;1.64 μs (611.2k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;msgspec&lt;/code&gt; encode (complex)&lt;/cell&gt;
          &lt;cell&gt;445 ns (2.2M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Pydantic &lt;code&gt;model_dump_json()&lt;/code&gt;&lt;/cell&gt;
          &lt;cell&gt;1.54 μs (647.8k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Pydantic &lt;code&gt;model_validate_json()&lt;/code&gt;&lt;/cell&gt;
          &lt;cell&gt;2.99 μs (334.7k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;🌐 Web Frameworks&lt;/cell&gt;
          &lt;cell&gt;Flask (return JSON)&lt;/cell&gt;
          &lt;cell&gt;16.5 μs (60.7k req/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Django (return JSON)&lt;/cell&gt;
          &lt;cell&gt;18.1 μs (55.4k req/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;FastAPI (return JSON)&lt;/cell&gt;
          &lt;cell&gt;8.63 μs (115.9k req/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Starlette (return JSON)&lt;/cell&gt;
          &lt;cell&gt;8.01 μs (124.8k req/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Litestar (return JSON)&lt;/cell&gt;
          &lt;cell&gt;8.19 μs (122.1k req/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;📁 File I/O&lt;/cell&gt;
          &lt;cell&gt;Open and close file&lt;/cell&gt;
          &lt;cell&gt;9.05 μs (110.5k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Read 1KB file&lt;/cell&gt;
          &lt;cell&gt;10.0 μs (99.5k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Write 1KB file&lt;/cell&gt;
          &lt;cell&gt;35.1 μs (28.5k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Write 1MB file&lt;/cell&gt;
          &lt;cell&gt;207 μs (4.8k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;pickle.dumps()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;1.30 μs (769.6k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;pickle.loads()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;1.44 μs (695.2k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;🗄️ Database&lt;/cell&gt;
          &lt;cell&gt;SQLite insert (JSON blob)&lt;/cell&gt;
          &lt;cell&gt;192 μs (5.2k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;SQLite select by PK&lt;/cell&gt;
          &lt;cell&gt;3.57 μs (280.3k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;SQLite update one field&lt;/cell&gt;
          &lt;cell&gt;5.22 μs (191.7k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;diskcache set&lt;/cell&gt;
          &lt;cell&gt;23.9 μs (41.8k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;diskcache get&lt;/cell&gt;
          &lt;cell&gt;4.25 μs (235.5k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;MongoDB insert_one&lt;/cell&gt;
          &lt;cell&gt;119 μs (8.4k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;MongoDB find_one by _id&lt;/cell&gt;
          &lt;cell&gt;121 μs (8.2k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;MongoDB find_one by nested field&lt;/cell&gt;
          &lt;cell&gt;124 μs (8.1k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;📞 Functions&lt;/cell&gt;
          &lt;cell&gt;Empty function call&lt;/cell&gt;
          &lt;cell&gt;22.4 ns (44.6M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Function with 5 args&lt;/cell&gt;
          &lt;cell&gt;24.0 ns (41.7M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Method call&lt;/cell&gt;
          &lt;cell&gt;23.3 ns (42.9M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Lambda call&lt;/cell&gt;
          &lt;cell&gt;19.7 ns (50.9M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;try/except (no exception)&lt;/cell&gt;
          &lt;cell&gt;21.5 ns (46.5M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;try/except (exception raised)&lt;/cell&gt;
          &lt;cell&gt;139 ns (7.2M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;isinstance()&lt;/code&gt; check&lt;/cell&gt;
          &lt;cell&gt;18.3 ns (54.7M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;⏱️ Async&lt;/cell&gt;
          &lt;cell&gt;Create coroutine object&lt;/cell&gt;
          &lt;cell&gt;47.0 ns (21.3M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;run_until_complete(empty)&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;27.6 μs (36.2k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;asyncio.sleep(0)&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;39.4 μs (25.4k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;gather()&lt;/code&gt; 10 coroutines&lt;/cell&gt;
          &lt;cell&gt;55.0 μs (18.2k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;create_task()&lt;/code&gt; + await&lt;/cell&gt;
          &lt;cell&gt;52.8 μs (18.9k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;async with&lt;/code&gt; (context manager)&lt;/cell&gt;
          &lt;cell&gt;29.5 μs (33.9k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;Memory Costs&lt;/head&gt;
      &lt;p&gt;Understanding how much memory different Python objects consume.&lt;/p&gt;
      &lt;head rend="h3"&gt;An empty Python process uses 15.73 MB&lt;/head&gt;
      &lt;head rend="h3"&gt;Strings&lt;/head&gt;
      &lt;p&gt;The rule of thumb for strings is the core string object takes 41 bytes. Each additional character is 1 byte.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;String&lt;/cell&gt;
          &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Empty string &lt;code&gt;""&lt;/code&gt;&lt;/cell&gt;
          &lt;cell&gt;41 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;1-char string &lt;code&gt;"a"&lt;/code&gt;&lt;/cell&gt;
          &lt;cell&gt;42 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;100-char string&lt;/cell&gt;
          &lt;cell&gt;141 bytes&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Numbers&lt;/head&gt;
      &lt;p&gt;Numbers are surprisingly large in Python. They have to derive from CPython’s &lt;code&gt;PyObject&lt;/code&gt; and are subject to reference counting for garabage collection, they exceed our typical mental model many of:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;2 bytes = short int&lt;/item&gt;
        &lt;item&gt;4 bytes = long int&lt;/item&gt;
        &lt;item&gt;etc.&lt;/item&gt;
      &lt;/list&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Type&lt;/cell&gt;
          &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Small int (0-256, cached)&lt;/cell&gt;
          &lt;cell&gt;28 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Large int (1000)&lt;/cell&gt;
          &lt;cell&gt;28 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Very large int (10**100)&lt;/cell&gt;
          &lt;cell&gt;72 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Float&lt;/cell&gt;
          &lt;cell&gt;24 bytes&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Collections&lt;/head&gt;
      &lt;p&gt;Collections are amazing in Python. Dynamically growing lists. Ultra high-perf dictionaries and sets. Here is the empty and “full” overhead of each.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Collection&lt;/cell&gt;
          &lt;cell role="head"&gt;Empty&lt;/cell&gt;
          &lt;cell role="head"&gt;1,000 items&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List (ints)&lt;/cell&gt;
          &lt;cell&gt;56 bytes&lt;/cell&gt;
          &lt;cell&gt;35.2 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List (floats)&lt;/cell&gt;
          &lt;cell&gt;56 bytes&lt;/cell&gt;
          &lt;cell&gt;32.1 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Dict&lt;/cell&gt;
          &lt;cell&gt;64 bytes&lt;/cell&gt;
          &lt;cell&gt;63.4 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Set&lt;/cell&gt;
          &lt;cell&gt;216 bytes&lt;/cell&gt;
          &lt;cell&gt;59.6 KB&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Classes and Instances&lt;/head&gt;
      &lt;p&gt;Slots are an interesting addition to Python classes. They remove the entire concept of a &lt;code&gt;__dict__&lt;/code&gt; for tracking fields and other values. Even for a single instance, slots classes are significantly smaller (212 bytes vs 694 bytes for 5 attributes). If you are holding a large number of them in memory for a list or cache, the memory savings of a slots class becomes very dramatic - over 2x less memory usage. Luckily for most use-cases, just adding a slots entry saves a ton of memory with minimal effort.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Type&lt;/cell&gt;
          &lt;cell role="head"&gt;Empty&lt;/cell&gt;
          &lt;cell role="head"&gt;5 attributes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Regular class&lt;/cell&gt;
          &lt;cell&gt;344 bytes&lt;/cell&gt;
          &lt;cell&gt;694 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;__slots__&lt;/code&gt; class&lt;/cell&gt;
          &lt;cell&gt;32 bytes&lt;/cell&gt;
          &lt;cell&gt;212 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;dataclass&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;694 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;@dataclass(slots=True)&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;212 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;namedtuple&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;228 bytes&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;p&gt;Aggregate Memory Usage (1,000 instances):&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Type&lt;/cell&gt;
          &lt;cell role="head"&gt;Total Memory&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List of 1,000 regular class instances&lt;/cell&gt;
          &lt;cell&gt;165.2 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List of 1,000 &lt;code&gt;__slots__&lt;/code&gt; class instances&lt;/cell&gt;
          &lt;cell&gt;79.1 KB&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;Basic Operations&lt;/head&gt;
      &lt;p&gt;The cost of fundamental Python operations: Way slower than C/C++/C# but still quite fast. I added a brief comparison to C# to the source repo.&lt;/p&gt;
      &lt;head rend="h3"&gt;Arithmetic&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Add two integers&lt;/cell&gt;
          &lt;cell&gt;19.0 ns (52.7M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Add two floats&lt;/cell&gt;
          &lt;cell&gt;18.4 ns (54.4M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Multiply two integers&lt;/cell&gt;
          &lt;cell&gt;19.4 ns (51.6M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;String Operations&lt;/head&gt;
      &lt;p&gt;String operations in Python are fast as well. f-strings are the fastest formatting style, while even the slowest style is still measured in just nano-seconds.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Concatenation (&lt;code&gt;+&lt;/code&gt;)&lt;/cell&gt;
          &lt;cell&gt;39.1 ns (25.6M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;f-string&lt;/cell&gt;
          &lt;cell&gt;64.9 ns (15.4M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;.format()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;103 ns (9.7M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;%&lt;/code&gt; formatting&lt;/cell&gt;
          &lt;cell&gt;89.8 ns (11.1M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;List Operations&lt;/head&gt;
      &lt;p&gt;List operations are very fast in Python. Adding a single item usually requires 28ns. Said another way, you can do 35M appends per second. This is unless the list has to expand using something like a doubling algorithm. You can see this in the ops/sec for 1,000 items.&lt;/p&gt;
      &lt;p&gt;Surprisingly, list comprehensions are 26% faster than the equivalent for loops with append statements.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;list.append()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;28.7 ns (34.8M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List comprehension (1,000 items)&lt;/cell&gt;
          &lt;cell&gt;9.45 μs (105.8k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Equivalent for-loop (1,000 items)&lt;/cell&gt;
          &lt;cell&gt;11.9 μs (83.9k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;Collection Access and Iteration&lt;/head&gt;
      &lt;p&gt;How fast can you get data out of Python’s built-in collections? Here is a dramatic example of how much faster the correct data structure is. &lt;code&gt;item in set&lt;/code&gt; or &lt;code&gt;item in dict&lt;/code&gt; is 200x faster than &lt;code&gt;item in list&lt;/code&gt; for just 1,000 items!&lt;/p&gt;
      &lt;p&gt;The graph below is non-linear in the x-axis.&lt;/p&gt;
      &lt;head rend="h3"&gt;Access by Key/Index&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Dict lookup by key&lt;/cell&gt;
          &lt;cell&gt;21.9 ns (45.7M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Set membership (&lt;code&gt;in&lt;/code&gt;)&lt;/cell&gt;
          &lt;cell&gt;19.0 ns (52.7M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List index access&lt;/cell&gt;
          &lt;cell&gt;17.6 ns (56.8M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List membership (&lt;code&gt;in&lt;/code&gt;, 1,000 items)&lt;/cell&gt;
          &lt;cell&gt;3.85 μs (259.6k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Length&lt;/head&gt;
      &lt;p&gt;&lt;code&gt;len()&lt;/code&gt; is very fast. Maybe we don’t have to optimize it out of the test condition on a while loop looping 100 times after all.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Collection&lt;/cell&gt;
          &lt;cell role="head"&gt;&lt;code&gt;len()&lt;/code&gt; time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List (1,000 items)&lt;/cell&gt;
          &lt;cell&gt;18.8 ns (53.3M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Dict (1,000 items)&lt;/cell&gt;
          &lt;cell&gt;17.6 ns (56.9M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Set (1,000 items)&lt;/cell&gt;
          &lt;cell&gt;18.0 ns (55.5M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Iteration&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Iterate 1,000-item list&lt;/cell&gt;
          &lt;cell&gt;7.87 μs (127.0k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Iterate 1,000-item dict (keys)&lt;/cell&gt;
          &lt;cell&gt;8.74 μs (114.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;sum()&lt;/code&gt; of 1,000 integers&lt;/cell&gt;
          &lt;cell&gt;1.87 μs (534.8k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;Class and Object Attributes&lt;/head&gt;
      &lt;p&gt;The cost of reading and writing attributes, and how &lt;code&gt;__slots__&lt;/code&gt; changes things. Slots saves over 2x the memory usage on large collections, with virtually identical attribute access speed.&lt;/p&gt;
      &lt;head rend="h3"&gt;Attribute Access&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Regular Class&lt;/cell&gt;
          &lt;cell role="head"&gt;&lt;code&gt;__slots__&lt;/code&gt; Class&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Read attribute&lt;/cell&gt;
          &lt;cell&gt;14.1 ns (70.9M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;14.1 ns (70.7M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Write attribute&lt;/cell&gt;
          &lt;cell&gt;15.7 ns (63.6M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;16.4 ns (60.8M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Other Attribute Operations&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Read &lt;code&gt;@property&lt;/code&gt;&lt;/cell&gt;
          &lt;cell&gt;19.0 ns (52.8M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;getattr(obj, 'attr')&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;13.8 ns (72.7M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;hasattr(obj, 'attr')&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;23.8 ns (41.9M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;JSON and Serialization&lt;/head&gt;
      &lt;p&gt;Comparing standard library JSON with optimized alternatives. &lt;code&gt;orjson&lt;/code&gt; handles more data types and is over 8x faster than standard lib &lt;code&gt;json&lt;/code&gt; for complex objects. Impressive!&lt;/p&gt;
      &lt;head rend="h3"&gt;Serialization (dumps)&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Library&lt;/cell&gt;
          &lt;cell role="head"&gt;Simple Object&lt;/cell&gt;
          &lt;cell role="head"&gt;Complex Object&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;json&lt;/code&gt; (stdlib)&lt;/cell&gt;
          &lt;cell&gt;708 ns (1.4M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;2.65 μs (376.8k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;orjson&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;60.9 ns (16.4M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;310 ns (3.2M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;ujson&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;264 ns (3.8M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;1.64 μs (611.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;msgspec&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;92.3 ns (10.8M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;445 ns (2.2M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Deserialization (loads)&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Library&lt;/cell&gt;
          &lt;cell role="head"&gt;Simple Object&lt;/cell&gt;
          &lt;cell role="head"&gt;Complex Object&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;json&lt;/code&gt; (stdlib)&lt;/cell&gt;
          &lt;cell&gt;714 ns (1.4M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;2.22 μs (449.9k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;orjson&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;106 ns (9.4M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;839 ns (1.2M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;ujson&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;268 ns (3.7M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;1.46 μs (682.8k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;msgspec&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;101 ns (9.9M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;850 ns (1.2M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Pydantic&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;model_dump_json()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;1.54 μs (647.8k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;model_validate_json()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;2.99 μs (334.7k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;model_dump()&lt;/code&gt; (to dict)&lt;/cell&gt;
          &lt;cell&gt;1.71 μs (585.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;model_validate()&lt;/code&gt; (from dict)&lt;/cell&gt;
          &lt;cell&gt;2.30 μs (435.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;Web Frameworks&lt;/head&gt;
      &lt;p&gt;Returning a simple JSON response. Benchmarked with &lt;code&gt;wrk&lt;/code&gt; against localhost running 4 works in Granian. Each framework returns the same JSON payload from a minimal endpoint. No database access or that sort of thing. This is just how much overhead/perf do we get from each framework itself. The code we write that runs within those view methods is largely the same.&lt;/p&gt;
      &lt;head rend="h3"&gt;Results&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Framework&lt;/cell&gt;
          &lt;cell role="head"&gt;Requests/sec&lt;/cell&gt;
          &lt;cell role="head"&gt;Latency (p99)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Flask&lt;/cell&gt;
          &lt;cell&gt;16.5 μs (60.7k req/sec)&lt;/cell&gt;
          &lt;cell&gt;20.85 ms (48.0 ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Django&lt;/cell&gt;
          &lt;cell&gt;18.1 μs (55.4k req/sec)&lt;/cell&gt;
          &lt;cell&gt;170.3 ms (5.9 ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;FastAPI&lt;/cell&gt;
          &lt;cell&gt;8.63 μs (115.9k req/sec)&lt;/cell&gt;
          &lt;cell&gt;1.530 ms (653.6 ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Starlette&lt;/cell&gt;
          &lt;cell&gt;8.01 μs (124.8k req/sec)&lt;/cell&gt;
          &lt;cell&gt;930 μs (1.1k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Litestar&lt;/cell&gt;
          &lt;cell&gt;8.19 μs (122.1k req/sec)&lt;/cell&gt;
          &lt;cell&gt;1.010 ms (990.1 ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;File I/O&lt;/head&gt;
      &lt;p&gt;Reading and writing files of various sizes. Note that the graph is non-linear in y-axis.&lt;/p&gt;
      &lt;head rend="h3"&gt;Basic Operations&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Open and close (no read)&lt;/cell&gt;
          &lt;cell&gt;9.05 μs (110.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Read 1KB file&lt;/cell&gt;
          &lt;cell&gt;10.0 μs (99.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Read 1MB file&lt;/cell&gt;
          &lt;cell&gt;33.6 μs (29.8k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Write 1KB file&lt;/cell&gt;
          &lt;cell&gt;35.1 μs (28.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Write 1MB file&lt;/cell&gt;
          &lt;cell&gt;207 μs (4.8k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Pickle vs JSON to Disk&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;pickle.dumps()&lt;/code&gt; (complex obj)&lt;/cell&gt;
          &lt;cell&gt;1.30 μs (769.6k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;pickle.loads()&lt;/code&gt; (complex obj)&lt;/cell&gt;
          &lt;cell&gt;1.44 μs (695.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;json.dumps()&lt;/code&gt; (complex obj)&lt;/cell&gt;
          &lt;cell&gt;2.72 μs (367.1k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;json.loads()&lt;/code&gt; (complex obj)&lt;/cell&gt;
          &lt;cell&gt;2.35 μs (425.9k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;Database and Persistence&lt;/head&gt;
      &lt;p&gt;Comparing SQLite, diskcache, and MongoDB using the same complex object.&lt;/p&gt;
      &lt;head rend="h3"&gt;Test Object&lt;/head&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;user_data = {
    "id": 12345,
    "username": "alice_dev",
    "email": "alice@example.com",
    "profile": {
        "bio": "Software engineer who loves Python",
        "location": "Portland, OR",
        "website": "https://alice.dev",
        "joined": "2020-03-15T08:30:00Z"
    },
    "posts": [
        {"id": 1, "title": "First Post", "tags": ["python", "tutorial"], "views": 1520},
        {"id": 2, "title": "Second Post", "tags": ["rust", "wasm"], "views": 843},
        {"id": 3, "title": "Third Post", "tags": ["python", "async"], "views": 2341},
    ],
    "settings": {
        "theme": "dark",
        "notifications": True,
        "email_frequency": "weekly"
    }
}
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;head rend="h3"&gt;SQLite (JSON blob approach)&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Insert one object&lt;/cell&gt;
          &lt;cell&gt;192 μs (5.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Select by primary key&lt;/cell&gt;
          &lt;cell&gt;3.57 μs (280.3k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Update one field&lt;/cell&gt;
          &lt;cell&gt;5.22 μs (191.7k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Delete&lt;/cell&gt;
          &lt;cell&gt;191 μs (5.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Select with &lt;code&gt;json_extract()&lt;/code&gt;&lt;/cell&gt;
          &lt;cell&gt;4.27 μs (234.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;diskcache&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;cache.set(key, obj)&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;23.9 μs (41.8k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;cache.get(key)&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;4.25 μs (235.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;cache.delete(key)&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;51.9 μs (19.3k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Check key exists&lt;/cell&gt;
          &lt;cell&gt;1.91 μs (523.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;MongoDB&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;insert_one()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;119 μs (8.4k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;find_one()&lt;/code&gt; by &lt;code&gt;_id&lt;/code&gt;&lt;/cell&gt;
          &lt;cell&gt;121 μs (8.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;find_one()&lt;/code&gt; by nested field&lt;/cell&gt;
          &lt;cell&gt;124 μs (8.1k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;update_one()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;115 μs (8.7k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;delete_one()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;30.4 ns (32.9M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Comparison Table&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;SQLite&lt;/cell&gt;
          &lt;cell role="head"&gt;diskcache&lt;/cell&gt;
          &lt;cell role="head"&gt;MongoDB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Write one object&lt;/cell&gt;
          &lt;cell&gt;192 μs (5.2k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;23.9 μs (41.8k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;119 μs (8.4k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Read by key/id&lt;/cell&gt;
          &lt;cell&gt;3.57 μs (280.3k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;4.25 μs (235.5k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;121 μs (8.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Read by nested field&lt;/cell&gt;
          &lt;cell&gt;4.27 μs (234.2k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;N/A&lt;/cell&gt;
          &lt;cell&gt;124 μs (8.1k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Update one field&lt;/cell&gt;
          &lt;cell&gt;5.22 μs (191.7k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;23.9 μs (41.8k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;115 μs (8.7k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Delete&lt;/cell&gt;
          &lt;cell&gt;191 μs (5.2k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;51.9 μs (19.3k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;30.4 ns (32.9M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;p&gt;Note: MongoDB is a victim of network access version in-process access.&lt;/p&gt;
      &lt;head rend="h2"&gt;Function and Call Overhead&lt;/head&gt;
      &lt;p&gt;The hidden cost of function calls, exceptions, and async.&lt;/p&gt;
      &lt;head rend="h3"&gt;Function Calls&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Empty function call&lt;/cell&gt;
          &lt;cell&gt;22.4 ns (44.6M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Function with 5 arguments&lt;/cell&gt;
          &lt;cell&gt;24.0 ns (41.7M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Method call on object&lt;/cell&gt;
          &lt;cell&gt;23.3 ns (42.9M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Lambda call&lt;/cell&gt;
          &lt;cell&gt;19.7 ns (50.9M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Built-in function (&lt;code&gt;len()&lt;/code&gt;)&lt;/cell&gt;
          &lt;cell&gt;17.1 ns (58.4M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Exceptions&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;try/except&lt;/code&gt; (no exception raised)&lt;/cell&gt;
          &lt;cell&gt;21.5 ns (46.5M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;try/except&lt;/code&gt; (exception raised)&lt;/cell&gt;
          &lt;cell&gt;139 ns (7.2M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Type Checking&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;isinstance()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;18.3 ns (54.7M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;type() == type&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;21.8 ns (46.0M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;Async Overhead&lt;/head&gt;
      &lt;p&gt;The cost of async machinery.&lt;/p&gt;
      &lt;head rend="h3"&gt;Coroutine Creation&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Create coroutine object (no await)&lt;/cell&gt;
          &lt;cell&gt;47.0 ns (21.3M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Create coroutine (with return value)&lt;/cell&gt;
          &lt;cell&gt;45.3 ns (22.1M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Running Coroutines&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;run_until_complete(empty)&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;27.6 μs (36.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;run_until_complete(return value)&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;26.6 μs (37.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Run nested await&lt;/cell&gt;
          &lt;cell&gt;28.9 μs (34.6k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Run 3 sequential awaits&lt;/cell&gt;
          &lt;cell&gt;27.9 μs (35.8k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;asyncio.sleep()&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;asyncio.sleep(0)&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;39.4 μs (25.4k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Coroutine with &lt;code&gt;sleep(0)&lt;/code&gt;&lt;/cell&gt;
          &lt;cell&gt;41.8 μs (23.9k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;asyncio.gather()&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;gather()&lt;/code&gt; 5 coroutines&lt;/cell&gt;
          &lt;cell&gt;49.7 μs (20.1k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;gather()&lt;/code&gt; 10 coroutines&lt;/cell&gt;
          &lt;cell&gt;55.0 μs (18.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;gather()&lt;/code&gt; 100 coroutines&lt;/cell&gt;
          &lt;cell&gt;155 μs (6.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Task Creation&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;create_task()&lt;/code&gt; + await&lt;/cell&gt;
          &lt;cell&gt;52.8 μs (18.9k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Create 10 tasks + gather&lt;/cell&gt;
          &lt;cell&gt;85.5 μs (11.7k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Async Context Managers &amp;amp; Iteration&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;async with&lt;/code&gt; (context manager)&lt;/cell&gt;
          &lt;cell&gt;29.5 μs (33.9k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;async for&lt;/code&gt; (5 items)&lt;/cell&gt;
          &lt;cell&gt;30.0 μs (33.3k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;async for&lt;/code&gt; (100 items)&lt;/cell&gt;
          &lt;cell&gt;36.4 μs (27.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Sync vs Async Comparison&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Sync function call&lt;/cell&gt;
          &lt;cell&gt;20.3 ns (49.2M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Async equivalent (&lt;code&gt;run_until_complete&lt;/code&gt;)&lt;/cell&gt;
          &lt;cell&gt;28.2 μs (35.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;Methodology&lt;/head&gt;
      &lt;head rend="h3"&gt;Benchmarking Approach&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;All benchmarks run multiple times and with warmup not timed&lt;/item&gt;
        &lt;item&gt;Timing uses &lt;code&gt;timeit&lt;/code&gt; or &lt;code&gt;perf_counter_ns&lt;/code&gt; as appropriate&lt;/item&gt;
        &lt;item&gt;Memory measured with &lt;code&gt;sys.getsizeof()&lt;/code&gt; and &lt;code&gt;tracemalloc&lt;/code&gt;&lt;/item&gt;
        &lt;item&gt;Results are median of N runs&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;Environment&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;OS: macOS 26.2&lt;/item&gt;
        &lt;item&gt;Python: 3.14.2 (CPython)&lt;/item&gt;
        &lt;item&gt;CPU: ARM - 14 cores (14 logical)&lt;/item&gt;
        &lt;item&gt;RAM: 24.0 GB&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;Code Repository&lt;/head&gt;
      &lt;p&gt;All benchmark code available at: https://github.com/mkennedy/python-numbers-everyone-should-know&lt;/p&gt;
      &lt;head rend="h2"&gt;Key Takeaways&lt;/head&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;Memory overhead: Python objects have significant memory overhead - even an empty list is 56 bytes&lt;/item&gt;
        &lt;item&gt;Dict/set speed: Dictionary and set lookups are extremely fast (O(1) average case) compared to list membership checks (O(n))&lt;/item&gt;
        &lt;item&gt;JSON performance: Alternative JSON libraries like &lt;code&gt;orjson&lt;/code&gt; and &lt;code&gt;msgspec&lt;/code&gt; are 3-8x faster than stdlib &lt;code&gt;json&lt;/code&gt;&lt;/item&gt;
        &lt;item&gt;Async overhead: Creating and awaiting coroutines has measurable overhead - only use async when you need concurrency&lt;/item&gt;
        &lt;item&gt;&lt;code&gt;__slots__&lt;/code&gt; tradeoff: &lt;code&gt;__slots__&lt;/code&gt; saves significant memory (over 2x for collections) with virtually no performance impact&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Last updated: 2026-01-01&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46454470</guid><pubDate>Thu, 01 Jan 2026 14:39:23 +0000</pubDate></item><item><title>Build a Deep Learning Library</title><link>https://zekcrates.quarto.pub/deep-learning-library/</link><description>&lt;doc fingerprint="b759cf2f81cb5b8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Build a Simple Deep Learning Library&lt;/head&gt;
    &lt;head rend="h1"&gt;Preface&lt;/head&gt;
    &lt;p&gt;Instead of just learning how to use a deep learning library, we are going to learn how to create one.&lt;/p&gt;
    &lt;p&gt;We start with a blank file and NumPy, and we don’t stop until we have a functional autograd engine and a collection of layer modules. By the end, we will use it to train MNIST, simple CNN and simple ResNet.&lt;/p&gt;
    &lt;p&gt; NoteSupport This Project &lt;/p&gt;
    &lt;p&gt;This book is free to read online. If it helps you, consider paying what you want on Gumroad&lt;/p&gt;
    &lt;p&gt;Questions or feedback? zekcrates@proton.me&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46454587</guid><pubDate>Thu, 01 Jan 2026 14:53:50 +0000</pubDate></item><item><title>Show HN: OpenWorkers – Self-hosted Cloudflare workers in Rust</title><link>https://openworkers.com/introducing-openworkers</link><description>&lt;doc fingerprint="ec56b9328dac36f2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing OpenWorkers&lt;/head&gt;
    &lt;p&gt;Self-hosted Cloudflare Workers in Rust&lt;/p&gt;
    &lt;p&gt;OpenWorkers is an open-source runtime for executing JavaScript in V8 isolates. It brings the power of edge computing to your own infrastructure.&lt;/p&gt;
    &lt;head rend="h2"&gt;What works today&lt;/head&gt;
    &lt;quote&gt;
      &lt;code&gt;export default { async fetch(request, env) { const data = await env.KV.get("key"); const rows = await env.DB.query( "SELECT * FROM users WHERE id = $1", [1] ); return Response.json({ data, rows }); } };&lt;/code&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Features&lt;/head&gt;
    &lt;head rend="h3"&gt;Bindings&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;• KV storage (get, put, delete, list)&lt;/item&gt;
      &lt;item&gt;• PostgreSQL database&lt;/item&gt;
      &lt;item&gt;• S3/R2-compatible storage&lt;/item&gt;
      &lt;item&gt;• Service bindings&lt;/item&gt;
      &lt;item&gt;• Environment variables &amp;amp; secrets&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Web APIs&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;• fetch, Request, Response&lt;/item&gt;
      &lt;item&gt;• ReadableStream&lt;/item&gt;
      &lt;item&gt;• crypto.subtle&lt;/item&gt;
      &lt;item&gt;• TextEncoder/Decoder, Blob&lt;/item&gt;
      &lt;item&gt;• setTimeout, AbortController&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Architecture&lt;/head&gt;
    &lt;quote&gt;┌─────────────────┐ │ nginx (proxy) │ └────────┬────────┘ │ ┌───────────────┬────────┴──┬───────────────┐ │ │ │ │ │ │ │ │ ┌────────┸────────┐ ┌────┸────┐ ┌────┸────┐ ┌────────┸────────┐ │ dashboard │ │ api │ │ logs * │ │ runner (x3) * │ └─────────────────┘ └────┬────┘ └────┰────┘ └────────┰────────┘ │ │ │ │ │ │ ┌────────┸────────┐ │ ┌────────┸────────┐ │ postgate * │ └──────┥ nats │ └─────────────────┘ └────────┰────────┘ │ │ ┌─────────────────┐ ┌──────┴───────┐ * ─────┥ PostgreSQL │ │ scheduler * │ └─────────────────┘ └──────────────┘&lt;/quote&gt;
    &lt;quote&gt;+-------------+ | nginx proxy | +------+------+ | +-------+-------+-------+--------+ | | | | +--+--+ +--+--+ +--+---+ +----------+-+ | dash| | api | |logs *| | runner * x3| +-----+ +--+--+ +--+---+ +-----+------+ | | | +-----+----+ | +------+-----+ |postgate *| +----+ nats | +-----+----+ +------+-----+ | | +-----+------+ +------+-----+ *-| PostgreSQL | | scheduler *| +------------+ +------------+&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;V8 Isolates: Sandboxing with CPU (100ms) and memory (128MB) limits per worker.&lt;/item&gt;
      &lt;item&gt;Cron Scheduling: Built-in support for 5 or 6-field cron syntax.&lt;/item&gt;
      &lt;item&gt;Compatibility: Cloudflare Workers syntax compatible.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Self-hosting&lt;/head&gt;
    &lt;p&gt;Deployment is designed to be simple. A single PostgreSQL database and a single Docker Compose file is all you need.&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;git clone https://github.com/openworkers/openworkers-infra cd openworkers-infra &amp;amp;&amp;amp; cp .env.example .env docker compose up -d postgres # Run migrations, generate token docker compose up -d&lt;/code&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Why I built this&lt;/head&gt;
    &lt;p&gt;This project has been evolving for about 7 years. I started experimenting with vm2 for sandboxing JS, then Cloudflare launched Workers and I got hooked on the model. When Deno came out, I switched to deno-core and ran on that for two years. Recently, with Claude's help, I rewrote everything on top of rusty_v8 directly.&lt;/p&gt;
    &lt;p&gt;The goal has always been the same: run JavaScript on your own servers, with the same DX as Cloudflare Workers but without vendor lock-in.&lt;/p&gt;
    &lt;p&gt;Next up: Execution recording &amp;amp; replay for deterministic debugging.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46454693</guid><pubDate>Thu, 01 Jan 2026 15:09:06 +0000</pubDate></item><item><title>Common Lisp SDK for the Datastar Hypermedia Framework</title><link>https://github.com/fsmunoz/datastar-cl</link><description>&lt;doc fingerprint="9f8b980ffea5d2f4"&gt;
  &lt;main&gt;
    &lt;code&gt;================================================================

       |             
      / \            DATASTAR HYPERMEDIA FRAMEWORK
     /___\          
    |  D  |          
    | C*L |                '(DATASTAR-CL)    
    |     |                 
   /|  |  |\
  | |_ |__| |              COMMON LISP SDK
  |/  /.\  \|            

================================================================
&lt;/code&gt;
    &lt;p&gt;This is a Common Lisp implementation of the Datastar SDK, following the Datastar Architecture Decision Record (ADR) as closely as possible, making the necessary adjustments for Common Lisp.&lt;/p&gt;
    &lt;p&gt;Focus has been on implementing the core SDK functionality, with additional utilities planned.&lt;/p&gt;
    &lt;p&gt;Clone the repository somewhere ASDF2 can find it (~~/src/lisp~ is commonly already configured), and use Quicklisp to load it:&lt;/p&gt;
    &lt;code&gt;(ql:quickload "datastar-cl")
&lt;/code&gt;
    &lt;p&gt;The SDK uses CLOS and is based on a &lt;code&gt;sse-generator&lt;/code&gt; class, that currently has two subclasses:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;hunchentoot-sse-generator&lt;/code&gt;: used with the Hunchentoot web server.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;clack-sse-generator&lt;/code&gt;: used with the Clack web application environment.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both approaches use a stream, although in different ways and initialized differently. For Clack, the work done is at the Lack level, and has been tested with both Hunchentoot and Woo.&lt;/p&gt;
    &lt;p&gt;We are currently using JZON as the JSON parser in two places: &lt;code&gt;read-signals&lt;/code&gt; and &lt;code&gt;patch-signals&lt;/code&gt;.
  This is not a hard requirement to remove, but for now it allows immediate use and testing of the
  library.&lt;/p&gt;
    &lt;p&gt;When using Clack with Woo, SSE connections are limited by Woo’s worker thread count. Each SSE connection blocks one worker for its entire duration. This is a fundamental architectural constraint - see &lt;code&gt;SSE-WOO-LIMITATIONS.org&lt;/code&gt; for details and practical solutions.&lt;/p&gt;
    &lt;p&gt;For applications with many SSE connections, Hunchentoot is recommended (no connection limit), or using a polling approach. By default, Hunchentoot (and Clack+Hunchentoot) uses a push approach, while Clack+Woo defaults to poll.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;test/&lt;/code&gt; directory has some basic examples. Additionally, the following projects use it&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Data SPICE: uses Datastar-CL for a 2D simulation of the solar system and Cassini-Huygens probe; written in parallel with the SDK to showcase it and also make sure it worked.&lt;/item&gt;
      &lt;item&gt;Horizons JPL API explorer: in the same theme, but a vastly simplified app that queries the Horizons API for solar system data.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Horizons JPL app is very simple – one single file – and should show the essentials. The Data SPICE application shows SSE streaming.&lt;/p&gt;
    &lt;p&gt;Support for compression was recently added, currently covering all backends but limited to &lt;code&gt;zstd&lt;/code&gt;:
  read &lt;code&gt;COMPRESSION.org&lt;/code&gt; for details.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;test/&lt;/code&gt; directory contains the code to setup and start the supported servers, Hunchentoot and
  Clack (with Woo and Hunchentoot backends, others can be added). It’s also a good place to use as an
  example of how the SDK can be used, especially since there are some differences depending on the
  backend (Hunchentoot or Clack). SSE has a specific test file that can also serve as an example, and
  some important edge cases (like what happens when connections are interrupted) are tested as well.&lt;/p&gt;
    &lt;p&gt;The code follows Datastar’s licence choice, the MIT license.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46454958</guid><pubDate>Thu, 01 Jan 2026 15:47:17 +0000</pubDate></item><item><title>Implementing HNSW (Hierarchical Navigable Small World) Vector Search in PHP</title><link>https://centamori.com/index.php?slug=hierarchical-navigable-small-world-hnsw-php&amp;lang=en</link><description>&lt;doc fingerprint="9ac1f3e5186c4fef"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Hierarchical Navigable Small World (HNSW) in PHP&lt;/head&gt;
    &lt;p&gt;Or: How to find a needle in a haystack without checking all the hay&lt;/p&gt;
    &lt;p&gt;In the previous article we saw how vectors can help find the right information simply by describing the concept we want to find.&lt;/p&gt;
    &lt;p&gt;We used Cosine Similarity to compare our request with all available documents, scanning them one by one until we found those with the highest similarity. Does this approach work? Yes. Is it fast? Sort of....&lt;/p&gt;
    &lt;p&gt;Imagine if a librarian, to find your book, had to read the titles of all 10 million volumes in a National Library. Even if they spent one millisecond per book, it would take hours. This is the problem of linear search ($O(N)$).&lt;/p&gt;
    &lt;p&gt;To solve this problem, we introduce a concept that will transform that search from hours to milliseconds: HNSW (Hierarchical Navigable Small World).&lt;/p&gt;
    &lt;head rend="h2"&gt;Small worlds and highways&lt;/head&gt;
    &lt;p&gt;Let's use an analogy to better understand the solution HNSW proposes: How do we find a specific point in a huge city if we come from very far away and don't know the direction?&lt;/p&gt;
    &lt;p&gt;Simple: in real life, we use a hierarchy of roads. That is, we start from a "broad" view of the map and subsequently "zoom in" towards the point of interest. Expressing this with an algorithm, we would:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Take the highway to get close to the region.&lt;/item&gt;
      &lt;item&gt;Exit onto the state road to reach the city.&lt;/item&gt;
      &lt;item&gt;Take the main streets for the neighborhood.&lt;/item&gt;
      &lt;item&gt;Finally, use the side streets to find the house number.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;HNSW does exactly this with data. It builds a layered structure (Layers) where the higher levels are like highways (few points, long connections) and the lower levels are neighborhood streets (all points, short connections).&lt;/p&gt;
    &lt;head rend="h2"&gt;The Code&lt;/head&gt;
    &lt;p&gt;Let's analyze how to implement this logic in PHP. The code we will examine comes from the open source project Vektor, a small service I built that allows for native vector search in PHP.&lt;/p&gt;
    &lt;p&gt;Technical note: In addition to &lt;code&gt;$ef&lt;/code&gt;, there is a crucial parameter called &lt;code&gt;$M&lt;/code&gt;. Imagine it as the maximum number of roads that can start from an intersection. The higher &lt;code&gt;$M&lt;/code&gt; is, the more connected the city is, but the more memory our map will occupy.&lt;/p&gt;
    &lt;head rend="h3"&gt;The descent to Level 0&lt;/head&gt;
    &lt;p&gt;The main logic is in the &lt;code&gt;search&lt;/code&gt; method, which we see below. Imagine looking at a satellite map and progressively zooming in. We start from the global view (the nodes of the highest level).&lt;/p&gt;
    &lt;code&gt;public function search(array $queryVector, int $k, int $ef): array
{
    // Start from the highest level (e.g. Level 3)
    $entryPoint = $this-&amp;gt;readHeader();

    // Calculate the initial distance, i.e., compare the Level 3 object with the search target
    $currObj = $entryPoint;
    $currDist = Math::cosineSimilarity($queryVector, $this-&amp;gt;getVector($currObj));

    // Now progressively descend from high levels down to level 1
    // The for loop starts from the max level and repeats until we reach Level 1
    for ($lc = $maxLevel; $lc &amp;gt;= 1; $lc--) {

        // Navigate within the current level until we find the closest point
        while (true) {
            $changed = false;

            // Read the neighbors of the current node at this level
            $node = $this-&amp;gt;readNode($currObj);
            $neighbors = $node['connections'][$lc] ?? [];

            // Search if there is a neighbor that is CLOSER to our target
            foreach ($neighbors as $neighborId) {
                $dist = Math::cosineSimilarity($queryVector, $this-&amp;gt;getVector($neighborId));
                if ($dist &amp;gt; $currDist) {
                    $currDist = $dist;
                    $currObj = $neighborId;
                    $changed = true; // We found a better point!
                }
            }

            // If there were no changes in choosing the best node,
            // it means we are already at the closest possible point 
            // in this level, so we can "descend" to the lower level
            if (!$changed) break;
        }

        // Now $currObj is set with the starting point for the lower level ($lc - 1)
    }

    // At the end of the loop we will be at Level 0 and $currObj will contain the closest point!
}&lt;/code&gt;
    &lt;p&gt;This cycle is the equivalent of identifying the area of interest from above, moving towards the point closest to the destination, and then increasing the detail. Repeat until we are at the maximum detail level (Level 1).&lt;/p&gt;
    &lt;head rend="h3"&gt;Precision Search (Level 0)&lt;/head&gt;
    &lt;p&gt;Once we arrive at Level 0, where all the data resides, a single point is no longer enough. We want the K best results. This is where the &lt;code&gt;$ef&lt;/code&gt; (Construction/Search Size) parameter comes into play.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;$ef&lt;/code&gt; is the size of our "candidate list". The higher the number, the more nodes we evaluate, and the more precise (and slow) the search will be.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;searchLayer&lt;/code&gt; method adopts a Greedy approach: at each step, the algorithm makes the locally optimal choice, moving towards the neighbor that maximizes similarity with the query. Instead of exploring every possible path, the algorithm "follows the gradient" towards the target. The Priority Queue manages this process, allowing us to explore the most promising nodes first and stop the search as soon as new candidates stop improving our list of best results ($ef$).&lt;/p&gt;
    &lt;code&gt;public function searchLayer(int $entryPoint, array $queryVector, int $ef, int $level, ?int $k = null): array
{
    $visited = [$entryPoint =&amp;gt; true];
    $candidates = new SplPriorityQueue(); // Promising nodes to explore

    $winners = [$entryPoint =&amp;gt; $entrySim]; // Best results found so far (Winner list)

    while (!$candidates-&amp;gt;isEmpty()) {
        $c = $candidates-&amp;gt;extract(); // Take the best candidate

        // If the best candidate is worse than the worst of our current "winners"
        // and we have already filled the list ($ef), we can stop.
        // It's useless to explore further, we are moving away.
        if ($cSim &amp;lt; $worstSim &amp;amp;&amp;amp; count($winners) &amp;gt;= $ef) {
            break;
        }

        // Explore the candidate's neighbors
        $node = $this-&amp;gt;readNode($c);
        $neighbors = $node['connections'][$level] ?? [];

        foreach ($neighbors as $neighborId) {
            if (!isset($visited[$neighborId])) {
                $visited[$neighborId] = true;

                // For each neighbor, calculate its similarity with the target query
                $sim = Math::cosineSimilarity($queryVector, $this-&amp;gt;getVector($neighborId));

                // If this neighbor is promising, add it to candidates
                if ($sim &amp;gt; $worstSim || count($winners) &amp;lt; $ef) {
                    $candidates-&amp;gt;insert($neighborId, $sim);
                    $winners[$neighborId] = $sim;

                    // Keep the winner list W at a fixed size $ef
                    if (count($winners) &amp;gt; $ef) {
                        asort($winners);
                        $idToRemove = array_key_first($winners); // Remove the worst
                        unset($winners[$idToRemove]);

                        // Update the "worst acceptable" threshold
                        asort($winners);
                        $worstSim = $winners[array_key_first($winners)];
                    }
                }
            }
        }
    }

    // ... return the top K elements from the $winners list ...
}&lt;/code&gt;
    &lt;head rend="h2"&gt;Why is it fast?&lt;/head&gt;
    &lt;p&gt;Let's go back to the library example.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In Linear Search, the librarian checks every book. $O(N)$ operations.&lt;/item&gt;
      &lt;item&gt;In HNSW, the librarian looks at the "Science" section (Level 2), then goes to the "Astronomy" shelf (Level 1), and finally checks only the 50 books on that shelf (Level 0). $O(\log N)$ operations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On a dataset of 1 million objects, instead of making 1,000,000 comparisons, we might make only 500 or 1,000. It's a difference of orders of magnitude.&lt;/p&gt;
    &lt;head rend="h2"&gt;Building the map&lt;/head&gt;
    &lt;p&gt;So far we have taken for granted that these "highways" and "neighborhood streets" already exist. But how are they built? The genius of HNSW lies in the fact that the map is drawn dynamically, one point at a time.&lt;/p&gt;
    &lt;p&gt;When we need to insert a new piece of data (a vector) into our index, we have to decide two things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Which zone is it in? (Max level)&lt;/item&gt;
      &lt;item&gt;Who are its neighbors?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For the first question, HNSW relies on chance. It's like flipping a coin (or more technically, using an exponential probability distribution). Most points will end up at Level 0 (the residential neighborhood). Some lucky ones will win a "ticket" to Level 1 (the state roads). Very few "chosen ones" will reach the highest levels (the highways). This ensures that the upper levels remain uncluttered and fast to traverse.&lt;/p&gt;
    &lt;p&gt;For the second question, we use the search seen previously to also perform the insertion.&lt;/p&gt;
    &lt;p&gt;If our new point has been assigned to Level 1:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We start from the absolute highest level (e.g. Level 3).&lt;/item&gt;
      &lt;item&gt;We look for the closest point to our newcomer (using the logic of &lt;code&gt;searchLayer&lt;/code&gt;seen before).&lt;/item&gt;
      &lt;item&gt;We go down to the level below and repeat, until we reach the assigned level (Level 1).&lt;/item&gt;
      &lt;item&gt;Here we stop and connect the new point to its nearest neighbors (based on the parameter &lt;code&gt;$M&lt;/code&gt;, the maximum number of connections).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Basically, every new citizen who arrives in the city asks for directions to find their area, and once arrived, makes friends with the neighbors most similar to them. If a neighbor has too many friends, they cut ties with the least similar one to make room for the newcomer. It is this process of natural selection of connections that keeps the graph efficient and navigable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Implementing HNSW from scratch is a fundamental exercise to understand how modern vector databases like Qdrant or Pinecone work. We are not just "searching for data", we are navigating a multidimensional space using probabilistic shortcuts.&lt;/p&gt;
    &lt;p&gt;These algorithms are the basis of modern recommendation systems ("Users who bought this also saw....") and RAG (Retrieval Augmented Generation) systems for artificial intelligence, allowing relevant context to be found in fractions of a second.&lt;/p&gt;
    &lt;p&gt;If you want to dive deeper into the complete implementation, check out the Vektor https://github.com/centamiv/vektor repository on GitHub.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46454968</guid><pubDate>Thu, 01 Jan 2026 15:48:31 +0000</pubDate></item><item><title>BYD Sells 4.6M Vehicles in 2025, Meets Revised Sales Goal</title><link>https://www.bloomberg.com/news/articles/2026-01-01/byd-sells-4-6-million-vehicles-in-2025-meets-revised-sales-goal</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46454977</guid><pubDate>Thu, 01 Jan 2026 15:49:42 +0000</pubDate></item><item><title>Cameras and Lenses (2020)</title><link>https://ciechanow.ski/cameras-and-lenses/</link><description>&lt;doc fingerprint="368c0a750d314318"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Cameras and Lenses&lt;/head&gt;
    &lt;p&gt;Pictures have always been a meaningful part of the human experience. From the first cave drawings, to sketches and paintings, to modern photography, we’ve mastered the art of recording what we see.&lt;/p&gt;
    &lt;p&gt;Cameras and the lenses inside them may seem a little mystifying. In this blog post I’d like to explain not only how they work, but also how adjusting a few tunable parameters can produce fairly different results:&lt;/p&gt;
    &lt;p&gt;Over the course of this article we’ll build a simple camera from first principles. Our first steps will be very modest â we’ll simply try to take any picture. To do that we need to have a sensor capable of detecting and measuring light that shines onto it.&lt;/p&gt;
    &lt;head rend="h1"&gt;Recording Light&lt;/head&gt;
    &lt;p&gt;Before the dawn of the digital era, photographs were taken on a piece of film covered in crystals of silver halide. Those compounds are light-sensitive and when exposed to light they form a speck of metallic silver that can later be developed with further chemical processes.&lt;/p&gt;
    &lt;p&gt;For better or for worse, I’m not going to discuss analog devices â these days most cameras are digital. Before we continue the discussion relating to light we’ll use the classic trick of turning the illumination off. Don’t worry though, we’re not going to stay in darkness for too long.&lt;/p&gt;
    &lt;p&gt;The image sensor of a digital camera consists of a grid of photodetectors. AÂ photodetector converts photons into electric current that can be measured â the more photons hitting the detector the higher the signal.&lt;/p&gt;
    &lt;p&gt;In the demonstration below you can observe how photons fall onto the arrangement of detectors represented by small squares. After some processing, the value read by each detector is converted to the brightness of the resulting image pixels which you can see on the right side. I’m also symbolically showing which photosite was hit with a short highlight. The slider below controls the flow of time:&lt;/p&gt;
    &lt;p&gt;The longer the time of collection of photons the more of them are hitting the detectors and the brighter the resulting pixels in the image. When we don’t gather enough photons the image is underexposed, but if we allow the photon collection to run for too long the image will be overexposed.&lt;/p&gt;
    &lt;p&gt;While the photons have the “color” of their wavelength, the photodetectors don’t see that hue â they only measure the total intensity which results in a black and white image. To record the color information we need to separate the incoming photons into distinct groups. We can put tiny color filters on top of the detectors so that they will only accept, more or less, red, green, or blue light:&lt;/p&gt;
    &lt;p&gt;This color filter array can be arranged in many different formations. One of the simplest is a Bayer filter which uses one red, one blue, and two green filters arranged in a 2x2 grid:&lt;/p&gt;
    &lt;p&gt;A Bayer filter uses two green filters because light in green part of the spectrum heavily correlates with perceived brightness. If we now repeat this pattern across the entire sensor we’re able to collect color information. For the next demo we will also double the resolution to an astonishing 1 kilopixel arranged in a 32x32 grid:&lt;/p&gt;
    &lt;p&gt;Note that the individual sensors themselves still only see the intensity, and not the color, but knowing the arrangement of the filters we can recreate the colored intensity of each sensor, as shown on the right side of the simulation.&lt;/p&gt;
    &lt;p&gt;The final step of obtaining a normal image is called demosaicing. During demosaicing we want to reconstruct the full color information by filling in the gaps in the captured RGB values. One of the simplest way to do it is to just linearly interpolate the values between the existing neighbors. I’m not going to focus on the details of many other available demosaicing algorithms and I’ll just present the resulting image created by the process:&lt;/p&gt;
    &lt;p&gt;Notice that yet again the overall brightness of the image depends on the length of time for which we let the photons through. That duration is known as shutter speed or exposure time. For most of this presentation I will ignore the time component and we will simply assume that the shutter speed has been set just right so that the image is well exposed.&lt;/p&gt;
    &lt;p&gt;The examples we’ve discussed so far were very convenient â we were surrounded by complete darkness with the photons neatly hitting the pixels to form a coherent image. Unfortunately, we can’t count on the photon paths to be as favorable in real environments, so let’s see how the sensor performs in more realistic scenarios.&lt;/p&gt;
    &lt;p&gt;Over the course of this article we will be taking pictures of this simple scene. The almost white background of this website is also a part of the scenery â it represents a bright overcast sky. You can drag around the demo to see it from other directions:&lt;/p&gt;
    &lt;p&gt;Let’s try to see what sort of picture would be taken by a sensor that is placed near the objects without any enclosure. I’ll also significantly increase the sensor’s resolution to make the pixels of the final image align with the pixels of your display. In the demonstration below the left side represents a view of the scene with the small greenish sensor present, while the right one shows the taken picture:&lt;/p&gt;
    &lt;p&gt;This is not a mistake. As you can see, the obtained image doesn’t really resemble anything. To understand why this happens let’s first look at the light radiated from the scene.&lt;/p&gt;
    &lt;p&gt;If you had a chance to explore how surfaces reflect light, you may recall that most matte surfaces scatter the incoming light in every direction. While I’m only showing a few examples, every point on every surface of this scene reflects the photons it receives from the whiteish background light source all around itself:&lt;/p&gt;
    &lt;p&gt;The red sphere ends up radiating red light, the green sphere radiates green light, and the gray checkerboard floor reflects white light of lesser intensity. Most importantly, however, the light emitted from the background is also visible to the sensor.&lt;/p&gt;
    &lt;p&gt;The problem with our current approach to taking pictures is that every pixel of the sensor is exposed to the entire environment. Light radiated from every point of the scene and the white background hits every point of the sensor. In the simulation below you can witness how light from different directions hits one point on the surface of the sensor:&lt;/p&gt;
    &lt;p&gt;Clearly, to obtain a discernible image we have to limit the range of directions that affect a given pixel on the sensor. With that in mind, let’s put the sensor in a box that has a small hole in it. The first slider controls the diameter of the hole, while the second one controls the distance between the opening and the sensor:&lt;/p&gt;
    &lt;p&gt;While not shown here, the inner sides of the walls are all black so that no light is reflected inside the box. I also put the sensor on the back wall so that the light from the hole shines onto it. We’ve just built a pinhole camera, let’s see how it performs. Observe what happens to the taken image as we tweak the diameter of the hole with the first slider, or change the distance between the opening and the sensor with the second one:&lt;/p&gt;
    &lt;p&gt;There are so many interesting things happening here! The most pronounced effect is that the image is inverted. To understand why this happens let’s look at the schematic view of the scene that shows the light rays radiated from the objects, going through the hole, and hitting the sensor:&lt;/p&gt;
    &lt;p&gt;As you can see the rays cross over in the hole and the formed image is a horizontal and a vertical reflection of the actual scene. Those two flips end up forming a 180Â° rotation. Since rotated images aren’t convenient to look at, all cameras automatically rotate the image for presentation and for the rest of this article I will do so as well.&lt;/p&gt;
    &lt;p&gt;When we change the distance between the hole and the sensor the viewing angle changes drastically. If we trace the rays falling on the corner pixels of the sensor we can see that they define the extent of the visible section of the scene:&lt;/p&gt;
    &lt;p&gt;Rays of light coming from outside of that shape still go through the pinhole, but they land outside of the sensor and aren’t recorded. As the hole moves further away from the sensor, the angle, and thus the field of view visible to the sensor gets smaller. We can see this in a top-down view of the camera:&lt;/p&gt;
    &lt;p&gt;Coincidentally, this diagram also helps us explain two other effects. Firstly, in the photograph the red sphere looks almost as big as the green one, even though the scene view shows the latter is much larger. However, both spheres end up occupying roughly the same span on the sensor and their size in the picture is similar. It’s also worth noting that the spheres seem to grow when the field of view gets narrower because their light covers larger part of the sensor.&lt;/p&gt;
    &lt;p&gt;Secondly, notice that different pixels of the sensor have different distance and relative orientation to the hole. The pixels right in the center of the sensor see the pinhole straight on, but pixels positioned at an angle to the main axis see a distorted pinhole that is further away. The ellipse in the bottom right corner of the demonstration below shows how a pixel positioned at the blue point sees the pinhole:&lt;/p&gt;
    &lt;p&gt;This change in the visible area of the hole causes the darkening we see in the corners of the photograph. The value of the cosine of the angle I’ve marked with a yellow color is quite important as it contributes to the reduction of visible light in four different ways:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Two cosine factors from the increased distance to the hole, it’s essentially the inverse square law&lt;/item&gt;
      &lt;item&gt;A cosine factor from the side squeeze of the circular hole seen at an angle&lt;/item&gt;
      &lt;item&gt;A cosine factor from the relative tilt of the receptor&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These four factors conspire together to reduce the illumination by a factor of cos4(Î±) in what is known as cosine-fourth-power law, also described as natural vignetting.&lt;/p&gt;
    &lt;p&gt;Since we know the relative geometry of the camera and the opening we can correct for this effect by simply dividing by the falloff factor and from this point on I will make sure that the images don’t have darkened corners.&lt;/p&gt;
    &lt;p&gt;The final effect we can observe is that when the hole gets smaller the image gets sharper. Let’s see how the light radiated from two points of the scene ends up going through the camera depending on the diameter of the pinhole:&lt;/p&gt;
    &lt;p&gt;We can already see that larger hole size ends up creating a bigger spread on the sensor. Let’s see this situation up close on a simple grid of detecting cells. Notice what happens to the size of the final circle hitting the sensor as that diameter of the hole changes:&lt;/p&gt;
    &lt;p&gt;When the hole is small enough rays from the source only manage to hit one pixel on the sensor. However, at larger radii the light spreads onto other pixels and a tiny point in the scene is no longer represented by a single pixel causing the image to no longer be sharp.&lt;/p&gt;
    &lt;p&gt;It’s worth pointing out that sharpness is ultimately arbitrary â it depends on the size at which the final image is seen, viewing conditions, and visual acuity of the observer. The same photograph that looks sharp on a postage stamp may in fact be very blurry when seen on a big display.&lt;/p&gt;
    &lt;p&gt;By reducing the size of the cone of light we can make sure that the source light affects a limited number of pixels. Here, however, lays the problem. The sensor we’ve been using so far has been an idealized detector capable of flawless adjustment of its sensitivity to the lighting conditions. If we instead were to fix the sensor sensitivity adjustment, the captured image would look more like this:&lt;/p&gt;
    &lt;p&gt;As the relative size of the hole visible to the pixels of the sensor gets smaller, be it due to reduced diameter or increased distance, fewer photons hit the surface and the image gets dimmer.&lt;/p&gt;
    &lt;p&gt;To increase the number of photons we capture we could extend the duration of collection, but increasing the exposure time comes with its own problems â if the photographed object moves or the camera isn’t held steady we risk introducing some motion blur.&lt;/p&gt;
    &lt;p&gt;Alternatively, we could increase the sensitivity of the sensor which is described using the ISO rating. However, boosting the ISO may introduce a higher level of noise. Even with these problems solved an actual image obtained by smaller and smaller holes would actually start getting blurry again due to diffraction effects of light.&lt;/p&gt;
    &lt;p&gt;If you recall how diffuse surfaces reflect light you may also realize how incredibly inefficient a pinhole camera is. A single point on the surface of an object radiates light into its surrounding hemisphere, however, the pinhole captures only a tiny portion of that light.&lt;/p&gt;
    &lt;p&gt;More importantly, however, a pinhole camera gives us minimal artistic control over which parts of the picture are blurry. In the demonstration below you can witness how changing which object is in focus heavily affects what is the primary target of attention of the photograph:&lt;/p&gt;
    &lt;p&gt;Let’s try to build an optical device that would solve both of these problems: we want to find a way to harness a bigger part of the energy radiated by the objects and also control what is blurry and how blurry it is. For the objects in the scene that are supposed to be sharp we want to collect a big chunk of their light and make it converge to the smallest possible point. In essence, we’re looking for an instrument that will do something like this:&lt;/p&gt;
    &lt;p&gt;We could then put the sensor at the focus point and obtain a sharp image. Naturally, the contraption we’ll try to create has to be transparent so that the light can pass through it and get to the sensor, so let’s begin the investigation by looking at a piece of glass.&lt;/p&gt;
    &lt;head rend="h1"&gt;Glass&lt;/head&gt;
    &lt;p&gt;In the demonstration below I put a red stick behind a pane of glass. You can adjust the thickness of this pane with the gray slider below:&lt;/p&gt;
    &lt;p&gt;When you look at the stick through the surface of a thick glass straight on, everything looks normal. However, as your viewing direction changes the stick seen through the glass seems out of place. The thicker the glass and the steeper the viewing angle the bigger the offset.&lt;/p&gt;
    &lt;p&gt;Let’s focus on one point on the surface of the stick and see how the rays of light radiated from its surface propagate through the subsection of the glass. The red slider controls the position of the source and the gray slider controls the thickness. You can drag the demo around to see it from different viewpoints:&lt;/p&gt;
    &lt;p&gt;For some reason the rays passing through glass at an angle are deflected off their paths. The change of direction happens whenever the ray enters or leaves the glass.&lt;/p&gt;
    &lt;p&gt;To understand why the light changes direction we have to peek under the covers of classical electromagnetism and talk a bit more about waves.&lt;/p&gt;
    &lt;head rend="h1"&gt;Waves&lt;/head&gt;
    &lt;p&gt;It’s impossible to talk about wave propagation without involving the time component, so the simulations in this section are animated â you can play and pause them by clickingtapping on the button in their bottom left corner.&lt;/p&gt;
    &lt;p&gt;By default all animations are enabled, but if you find them distracting, or if you want to save power, you can globally pause all the following demonstrations.disabled, but if you’d prefer to have things moving as you read you can globally unpause them and see all the waves oscillating.&lt;/p&gt;
    &lt;p&gt;Let’s begin by introducing the simplest sinusoidal wave:&lt;/p&gt;
    &lt;p&gt;A wave like this can be characterized by two components. Wavelength Î» is the distance over which the shape of the wave repeats. Period T defines how much time a full cycle takes.&lt;/p&gt;
    &lt;p&gt;Frequency f, is just a reciprocal of period and it’s more commonly used â it defines how many waves per second have passed over some fixed point. Wavelength and frequency define phase velocity vp which describes how quickly a point on a wave, e.g. a peak, moves:&lt;/p&gt;
    &lt;p&gt;The sinusoidal wave is the building block of a polarized electromagnetic plane wave. As the name implies electromagnetic radiation is an interplay of oscillations of electric field E and magnetic field B:&lt;/p&gt;
    &lt;p&gt;In an electromagnetic wave the magnetic field is tied to the electric field so I’m going to hide the former and just visualize the latter. Observe what happens to the electric component of the field as it passes through a block of glass. I need to note that dimensions of wavelengths are not to scale:&lt;/p&gt;
    &lt;p&gt;Notice that the wave remains continuous at the boundary and inside the glass the frequency of the passing wave remains constant, However, the wavelength and thus the phase velocity are reduced â you can see it clearly from the side.&lt;/p&gt;
    &lt;p&gt;The microscopic reason for the phase velocity change is quite complicated, but it can be quantified using the index of refraction n, which is the ratio of the speed of light c to the phase velocity vp of lightwave in that medium:&lt;/p&gt;
    &lt;p&gt;The higher the index of refraction the slower light propagates through the medium. In the table below I’ve presented a few different indices of refraction for some materials:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;vacuum&lt;/cell&gt;
        &lt;cell&gt;1.00&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;air&lt;/cell&gt;
        &lt;cell&gt;1.0003&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;water&lt;/cell&gt;
        &lt;cell&gt;1.33&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;glass&lt;/cell&gt;
        &lt;cell&gt;1.53&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;diamond&lt;/cell&gt;
        &lt;cell&gt;2.43&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Light traveling through air barely slows down, but in a diamond it’s over twice as slow. Now that we understand how index of refraction affects the wavelength in the glass, let’s see what happens when we change the direction of the incoming wave:&lt;/p&gt;
    &lt;p&gt;The wave in the glass has a shorter wavelength, but it still has to match the positions of its peaks and valleys across the boundary. As such, the direction of propagation must change to ensure that continuity.&lt;/p&gt;
    &lt;p&gt;I need to note that the previous two demonstrations presented a two dimensional wave since that allowed me to show the sinusoidal component oscillating into the third dimension. In real world the lightwaves are three dimensional and I can’t really visualize the sinusoidal component without using the fourth dimension which has its own set of complications.&lt;/p&gt;
    &lt;p&gt;The alternative way of presenting waves is to use wavefronts. Wavefronts connect the points of the same phase of the wave, e.g. all the peaks or valleys. In two dimensions wavefronts are represented by lines:&lt;/p&gt;
    &lt;p&gt;In three dimensions the wavefronts are represented by surfaces. In the demonstration below a single source emits a spherical wave, points of the same phase in the wave are represented by the moving shells:&lt;/p&gt;
    &lt;p&gt;By drawing lines that are perpendicular to the surface of the wavefront we create the familiar rays. In this interpretation rays simply show the local direction of wave propagation which can be seen in this example of a section of a spherical 3D wave:&lt;/p&gt;
    &lt;p&gt;I will continue to use the ray analogy to quantify the change in direction of light passing through materials. The relation between the angle of incidence Î¸1 and angle of refraction Î¸2 can be formalized with the equation known as Snell’s law:&lt;/p&gt;
    &lt;p&gt;It describes how a ray of light changes direction relative to the surface normal on the border between two different media. Let’s see it in action:&lt;/p&gt;
    &lt;p&gt;When traveling from a less to more refractive material the ray bends towards the normal, but when the ray exits the object with higher index of refraction it bends away from the normal.&lt;/p&gt;
    &lt;p&gt;Notice that in some configurations the refracted ray completely disappears, however, this doesn’t paint a full picture because we’re currently completely ignoring reflections.&lt;/p&gt;
    &lt;p&gt;All transparent objects reflect some amount of light. You may have noticed that reflection on a surface of a calm lake or even on the other side of the glass demonstration at the beginning of the previous section. The intensity of that reflection depends on the index of refraction of the material and the angle of the incident ray. Here’s a more realistic demonstration of how light would get refracted and reflected between two media:&lt;/p&gt;
    &lt;p&gt;The relation between transmittance and reflectance is determined by Fresnel equations. Observe that the curious case of missing light that we saw previously no longer occurs â that light is actually reflected. The transition from partial reflection and refraction to the complete reflection is continuous, but near the end it’s very rapid and at some point the refraction completely disappears in the effect known as total internal reflection.&lt;/p&gt;
    &lt;p&gt;The critical angle at which the total internal reflection starts to happen depends on the indices of refraction of the boundary materials. Since that coefficient is low for air, but very high for diamond a proper cut of the faces makes diamonds very shiny.&lt;/p&gt;
    &lt;p&gt;While interesting on its own, reflection in glass isn’t very relevant to our discussion and for the rest of this article we’re not going to pay much attention to it. Instead, we’ll simply assume that the materials we’re using are covered with high quality anti-reflective coating.&lt;/p&gt;
    &lt;head rend="h1"&gt;Manipulating Rays&lt;/head&gt;
    &lt;p&gt;Let’s go back to the example that started the discussion of light and glass. When both sides of a piece of glass are parallel, the ray is shifted, but it still travels in the same direction. Observe what happens to the ray when we change the relative angle of the surfaces of the glass.&lt;/p&gt;
    &lt;p&gt;When we make two surfaces of the glass not parallel we gain the ability to change the direction of the rays. Recall, that we’re trying to make the rays hitting the optical device converge at a certain point. To do that we have to bend the rays in the upper part down and, conversely, bend the rays in the lower part up.&lt;/p&gt;
    &lt;p&gt;Let’s see what happens if we shape the glass to have different angles between its walls at different height. In the demonstration below you can control how many distinct segments a piece of glass is shaped to:&lt;/p&gt;
    &lt;p&gt;As the number of segments approaches infinity we end up with a continuous surface without any edges. If we look at the crossover point from the side you may notice that we’ve managed to converge the rays across one axis, but the top-down view reveals that we’re not done yet. To focus all the rays we need to replicate that smooth shape across all possible directions â we need rotational symmetry:&lt;/p&gt;
    &lt;p&gt;We’ve created a convex thin lens. This lens is idealized, in the later part of the article we’ll discuss how real lenses aren’t as perfect, but for now it will serve us very well. Let’s see what happens to the focus point when we change the position of the red source:&lt;/p&gt;
    &lt;p&gt;When the source is positioned very far away the incoming rays become parallel and after passing through lens they converge at a certain distance away from the center. That distance is known as focal length.&lt;/p&gt;
    &lt;p&gt;The previous demonstration also shows two more general distances: so which is the distance between the object, or source, and the lens, as well as si which is the distance between the image and the lens. These two values and the focal length f are related by the thin lens equation:&lt;/p&gt;
    &lt;p&gt;Focal length of a lens depends on both the index of refraction of the material from which the lens is made and its shape:&lt;/p&gt;
    &lt;p&gt;Now that we understand how a simple convex lens works we’re ready to mount it into the hole of our camera. We will still control the distance between the sensor and the lens, but instead of controlling the diameter of the lens we’ll instead control its focal length:&lt;/p&gt;
    &lt;p&gt;When you look at the lens from the side you may observe how the focal length change is tied to the shape of the lens. Let’s see how this new camera works in action:&lt;/p&gt;
    &lt;p&gt;Once again, a lot of things are going on here! Firstly, let’s try to understand how the image is formed in the first place. The demonstration below shows paths of rays from two separate points in the scene. After going through the lens they end up hitting the sensor:&lt;/p&gt;
    &lt;p&gt;Naturally, this process happens for every single point in the scene which creates the final image. Similarly to a pinhole a convex lens creates an inverted picture â I’m still correcting for this by showing you a rotated photograph.&lt;/p&gt;
    &lt;p&gt;Secondly, notice that the distance between the lens and the sensor still controls the field of view. As a reminder, the focal length of a lens simply defines the distance from the lens at which the rays coming from infinity converge. To achieve a sharp image, the sensor has to be placed at the location where the rays focus and that’s what’s causing the field of view to change.&lt;/p&gt;
    &lt;p&gt;In the demonstration below I’ve visualized how rays from a very far object focus through a lens of adjustable focal length, notice that to obtain a sharp image we must change the distance between the lens and the sensor which in turn causes the field of view to change:&lt;/p&gt;
    &lt;p&gt;If we want to change the object on which a camera with a lens of a fixed focal length is focused, we have to move the image plane closer or further away from the lens which affects the angle of view. This effect is called focus breathing:&lt;/p&gt;
    &lt;p&gt;A lens with a fixed focal length like the one above is often called a prime lens, while lenses with adjustable focal length are called zoom lenses. While the lenses in our eyes do dynamically adjust their focal lengths by changing their shape, rigid glass can’t do that so zoom lenses use a system of multiple glass elements that change their relative position to achieve this effect.&lt;/p&gt;
    &lt;p&gt;In the simulation above notice the difference in sharpness between the red and green spheres. To understand why this happens let’s analyze the rays emitted from two points on the surface of the spheres. In the demonstration below the right side shows the light seen by the sensor just from the two marked points on the spheres:&lt;/p&gt;
    &lt;p&gt;The light from the point in focus converges to a point, while the light from an out-of-focus point spreads onto a circle. For larger objects the multitude of overlapping out-of-focus circles creates a smooth blur called bokeh. With tiny and bright light sources that circle itself is often visible, you may have seen effects like the one in the demonstration below in some photographs captured in darker environments:&lt;/p&gt;
    &lt;p&gt;Notice that the circular shape is visible for lights both in front of and behind the focused distance. As the object is positioned closer or further away from the lens the image plane “slices” the cone of light at different location:&lt;/p&gt;
    &lt;p&gt;That circular spot is called a circle of confusion. While in many circumstances the blurriness of the background or the foreground looks very appealing, it would be very useful to control how much blur there is.&lt;/p&gt;
    &lt;p&gt;Unfortunately, we don’t have total freedom here â we still want the primary photographed object to remain in focus so its light has to converge to a point. We just want to change the size of the circle of out-of-focus objects without moving the central point. We can accomplish that by changing the angle of the cone of light:&lt;/p&gt;
    &lt;p&gt;There are two methods we can use to modify that angle. Firstly, we can change the focal length of the lens â you may recall that with longer focal lengths the cone of light also gets longer. However, changing the focal length and keeping the primary object in focus requires moving the image plane which in turn changes how the picture is framed.&lt;/p&gt;
    &lt;p&gt;The alternative way of reducing the angle of the cone of light is to simply ignore some of the “outer” rays. We can achieve that by introducing a stop with a hole in the path of light:&lt;/p&gt;
    &lt;p&gt;This hole is called an aperture. In fact, even the hole in which the lens is mounted is an aperture of some sort, but what we’re introducing is an adjustable aperture:&lt;/p&gt;
    &lt;p&gt;Let’s try to see how an aperture affects the photographs taken with our camera:&lt;/p&gt;
    &lt;p&gt;In real camera lenses an adjustable aperture is often constructed from a set of overlapping blades that constitute an iris. The movement of those blades changes the size of the aperture:&lt;/p&gt;
    &lt;p&gt;The shape of the aperture also defines the shape of bokeh. This is the reason why bokeh sometimes has a polygonal shape â it’s simply the shape of the “cone” of light after passing through the blades of the aperture. Next time you watch a movie pay a close attention to the shape of out-of-focus highlights, they’re often polygonal:&lt;/p&gt;
    &lt;p&gt;As the aperture diameter decreases, larger and larger areas of the photographed scene remain sharp. The term depth of field is used to define the length of the region over which the objects are acceptably sharp. When describing the depth of field we’re trying to conceptually demark those two boundary planes and see how far apart they are from each other.&lt;/p&gt;
    &lt;p&gt;Let’s see the depth of field in action. The black slider controls the aperture, the blue slider controls the focal length, and the red slider changes the position of the object relative to the camera. The green dot shows the place of perfect focus, while the dark blue dots show the limits, or the depth, of positions between which the image of the red light source will be reasonably sharp, as shown by a single outlined pixel on the sensor:&lt;/p&gt;
    &lt;p&gt;Notice that the larger the diameter of aperture and the shorter the focal length the shorter the distance between the dark blue dots and thus the shallower the depth of field becomes. If you recall our discussion of sharpness this demonstration should make it easier to understand why reducing the angle of the cone increases the depth of field.&lt;/p&gt;
    &lt;p&gt;If you don’t have perfect vision you may have noticed that squinting your eyes make you see things a little better. Your eyelids covering some part of your iris simply act as an aperture that decreases the angle of the cone of light falling into your eyes making things sightly less blurry on your retina.&lt;/p&gt;
    &lt;p&gt;An interesting observation is that aperture defines the diameter of the base of the captured cone of light that is emitted from the object. Twice as large aperture diameter captures roughly four times more light due to increased solid angle. In practice, the actual size of the aperture as seen from the point of view of the scene, or the entrance pupil, depends on all the lenses in front of it as the shaped glass may scale the perceived size of the aperture.&lt;/p&gt;
    &lt;p&gt;On the other hand, when a lens is focused correctly, the focal length defines how large a source object is in the picture. By doubling the focal length we double the width and the height of the object on the sensor thus increasing the area by the factor of four. The light from the source is more spread out and each individual pixel receives less light.&lt;/p&gt;
    &lt;p&gt;The total amount of light hitting each pixel is proportional to the ratio between the focal length f and the diameter of the entrance pupil D. This ratio is known as the f-number:&lt;/p&gt;
    &lt;p&gt;A lens with a focal length of 50 mm and the entrance pupil of 25 mm would have N equal to 2 and the f-number would be known as f/2. Since the amount of light getting to each pixel of the sensor increases with the diameter of the aperture and decreases with the focal length, the f-number controls the brightness of the projected image.&lt;/p&gt;
    &lt;p&gt;The f-number with which commercial lenses are marked usually defines the maximum aperture a lens can achieve and the smaller the f-number the more light the lens passes through. Bigger amount of incoming light allows reduction of exposure time, so the smaller the f-number the faster the lens is. By reducing the size of the aperture we can modify the f-number with which a picture is taken.&lt;/p&gt;
    &lt;p&gt;The f-numbers are often multiples of 1.4 which is an approximation of 2. Scaling the diameter of an adjustable aperture by 2 scales its area by 2 which is a convenient factor to use. Increasing the f-number by a so-called stop halves the amount of received light. The demonstration below shows the relatives sizes of the aperture through which light is being seen:&lt;/p&gt;
    &lt;p&gt;To maintain the overall brightness of the image when stopping down we’d have to either increase the exposure time or the sensitivity of the sensor.&lt;/p&gt;
    &lt;p&gt;While aperture settings let us easily control the depth of field, that change comes at a cost. When the f-number increases and the aperture diameter gets smaller we effectively start approaching a pinhole camera with all its related complications.&lt;/p&gt;
    &lt;p&gt;In the final part of this article we will discuss the entire spectrum of another class of problems that we’ve been conveniently avoiding all this time.&lt;/p&gt;
    &lt;head rend="h1"&gt;Aberrations&lt;/head&gt;
    &lt;p&gt;In our examples so far we’ve been using a perfect idealized lens that did exactly what we want and in all the demonstrations I’ve relied on a certain simplification known as the paraxial approximation. However, the physical world is a bit more complicated.&lt;/p&gt;
    &lt;p&gt;The most common types of lenses are spherical lenses â their curved surfaces are sections of spheres of different radii. These types of lenses are easier to manufacture, however, they actually don’t perfectly converge the rays of incoming light. In the demonstration below you can observe how fuzzy the focus point is for various lens radii:&lt;/p&gt;
    &lt;p&gt;This imperfection is known as spherical aberration. This specific flaw can be corrected with aspheric lenses, but unfortunately there are other types of problems that may not be easily solved by a single lens. In general, for monochromatic light there are five primary types of aberrations: spherical aberration, coma, astigmatism, field curvature, and distortion.&lt;/p&gt;
    &lt;p&gt;We’re still not out of the woods even if we manage to minimize these problems. In normal environments light is very non-monochromatic and nature sets another hurdle into optical system design. Let’s quickly go back to the dark environment as we’ll be discussing a single beam of white light.&lt;/p&gt;
    &lt;p&gt;Observe what happens to that beam when it hits a piece of glass. You can make the sides non-parallel by using the slider:&lt;/p&gt;
    &lt;p&gt;What we perceive as white light is a combination of lights of different wavelengths. In fact, the index of refraction of materials depends on the wavelength of the light. This phenomena called dispersion splits what seems to be a uniform beam of white light into a fan of color bands. The very same mechanism that we see here is also responsible for a rainbow.&lt;/p&gt;
    &lt;p&gt;In a lens this causes different wavelengths of light to focus at different offsets â the effect known as chromatic aberration. We can easily visualize the axial chromatic aberration even on a lens with spherical aberration fixed. I’ll only use red, green, and blue dispersed rays to make things less crowded, but remember that other colors of the spectrum are present in between. Using the slider you can control the amount of dispersion the lens material introduces:&lt;/p&gt;
    &lt;p&gt;Chromatic aberration may be corrected with an achromatic lens, usually in the form of a doublet with two different types of glass fused together.&lt;/p&gt;
    &lt;p&gt;To minimize the impact of the aberrations, camera lenses use more than one optical element on their pathways. In this article I’ve only shown you simple lens systems, but a high-end camera lens may consist of a lot of elements that were carefully designed to balance the optical performance, weight, and cost.&lt;/p&gt;
    &lt;p&gt;While we, in our world of computer simulations on this website, can maintain the illusion of simple and perfect systems devoid of aberrations, vignetting, and lens flares, real cameras and lenses have to deal with all these problems to make the final pictures look good.&lt;/p&gt;
    &lt;head rend="h1"&gt;Further Watching and Reading&lt;/head&gt;
    &lt;p&gt;Over on YouTube Filmmaker IQ channel has a lot of great content related to lenses and movie making. Two videos especially fitting here are The History and Science of Lenses and Focusing on Depth of Field and Lens Equivalents.&lt;/p&gt;
    &lt;p&gt;What Makes Cinema Lenses So Special!? on Potato Jet channel is a great interview with Art Adams from ARRI. The video goes over many interesting details of high-end cinema lens design, for example, how the lenses compensate for focus breathing, or how much attention is paid to the quality of bokeh.&lt;/p&gt;
    &lt;p&gt;For a deeper dive on bokeh itself Jakub TrÃ¡vnÃk’s On Bokeh is a great article on the subject. The author explains how aberrations may cause bokeh of non uniform intensity and shows many photographs of real cameras and lenses.&lt;/p&gt;
    &lt;p&gt;In this article I’ve mostly been using geometrical optics with some soft touches of electromagnetism. For a more modern look at the nature of light and its interaction with matter I recommend Richard Feynman’s QED: The Strange Theory of Light and Matter. The book is written in a very approachable style suited for general audience, but it still lets Feynman’s wits and brilliance shine right through.&lt;/p&gt;
    &lt;head rend="h1"&gt;Final Words&lt;/head&gt;
    &lt;p&gt;Weâve barely scratched the surface of optics and camera lens design, but even the most complex systems end up serving the same purpose: to tell light where to go. In some sense optical engineering is all about taming the nature of light.&lt;/p&gt;
    &lt;p&gt;The simple act of pressing the shutter button in a camera app on a smartphone or on the body of a high-end DSLR is effortless, but itâs at this moment when, through carefully guided rays hitting an array of photodetectors, we immortalize reality by painting with light.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46455872</guid><pubDate>Thu, 01 Jan 2026 17:18:01 +0000</pubDate></item><item><title>Memory Subsystem Optimizations</title><link>https://johnnysswlab.com/memory-subsystem-optimizations/</link><description>&lt;doc fingerprint="ef7fd4586d6321ad"&gt;
  &lt;main&gt;
    &lt;p&gt;In this blog I wrote 18 blog posts about memory subsystem optimizations. By memory subsystem optimizations, I mean optimizations that aim at making software faster by better using the memory subsystem. Most of them are applicable to software that works with large datasets; but some of them are applicable to software that works with any data regardless of its size.&lt;/p&gt;
    &lt;p&gt;Do you need to discuss a performance problem in your project? Or maybe you want a vectorization training for yourself or your team? Contact us &lt;lb/&gt;Or follow us on LinkedIn , Twitter or Mastodon and get notified as soon as new content becomes available.&lt;/p&gt;
    &lt;p&gt;Here is a list of all posts that we covered on Johnny’s Software Lab:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Topic&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Decreasing Total Memory Accesses&lt;/cell&gt;
        &lt;cell&gt;We speed up software by keeping data in registers instead of reloading it from the memory subsystem several times.&lt;/cell&gt;
        &lt;cell&gt;Decreasing the Number of Memory Accesses 1/2&lt;p&gt;Decreasing the Number of Memory Accesses: The Compiler’s Secret Life 2/2&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Changing the Data Access Pattern to Increase Locality&lt;/cell&gt;
        &lt;cell&gt;By changing our data access pattern we increase the possibility our data is in the fastest level of data cache.&lt;/cell&gt;
        &lt;cell&gt;For Software Performance, the Way Data is Accessed Matters!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Changing the Data Layout: Classes&lt;/cell&gt;
        &lt;cell&gt;Selecting proper class data layout can improve software performance.&lt;/cell&gt;
        &lt;cell&gt;Software Performance and Class Layout&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Changing the Data Layout: Data Structures&lt;/cell&gt;
        &lt;cell&gt;By changing the data layout of common data structures, such as linked lists, trees or hash maps we can improve their performance.&lt;/cell&gt;
        &lt;cell&gt;Faster hash maps, binary trees etc. through data layout modification&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Decreasing the Dataset Size&lt;/cell&gt;
        &lt;cell&gt;Memory efficiency can be improved by decreasing the dataset size. This results in speed improvements as well.&lt;/cell&gt;
        &lt;cell&gt;Memory consumption, dataset size and performance: how does it all relate?&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Changing the Memory Layout&lt;/cell&gt;
        &lt;cell&gt;Whereas data layout is determined at compile time, memory layout is determined by the system allocator at runtime. We examine how changing the memory layout using custom allocators influences software performance.&lt;/cell&gt;
        &lt;cell&gt;Performance Through Memory Layout&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Increasing instruction-level parallelism&lt;/cell&gt;
        &lt;cell&gt;Some codes cannot utilize the memory subsystem fully because of instruction dependencies. Here we investigate techniques that break dependencies and improve performance.&lt;/cell&gt;
        &lt;cell&gt;Instruction-level parallelism in practice: speeding up memory-bound programs with low ILP&lt;p&gt;Hiding Memory Latency With In-Order CPU Cores OR How Compilers Optimize Your Code&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Software prefetching for random data accesses&lt;/cell&gt;
        &lt;cell&gt;Explicit software prefetches tell hardware that you will be accessing a certain piece of data soon. When used smartly, they can improve software performance.&lt;/cell&gt;
        &lt;cell&gt;The pros and cons of explicit software prefetching&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Decreasing TLB cache misses&lt;/cell&gt;
        &lt;cell&gt;TLB cache is a small cache that speeds up translation of virtual to physical memory addresses. In some cases, it can be the reason for poor performance. We investigate techniques for decreasing TLB cache misses.&lt;/cell&gt;
        &lt;cell&gt;Speeding Up Translation of Virtual To Physical Memory Addresses: TLB and Huge Pages&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Saving the memory subsystem bandwidth&lt;/cell&gt;
        &lt;cell&gt;In some cases, we don’t care about software performance, but we do care about being a good neighbor. We investigate techniques that make our software consume least possible amount of memory subsystem resources.&lt;/cell&gt;
        &lt;cell&gt;Frugal Programming: Saving Memory Subsystem Bandwidth&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Branch prediction and data caches&lt;/cell&gt;
        &lt;cell&gt;We investigate the delicate interplay of the branch prediction and the memory subsystem.&lt;/cell&gt;
        &lt;cell&gt;Unexpected Ways Memory Subsystem Interacts with Branch Prediction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Multithreading and the Memory Subsystem&lt;/cell&gt;
        &lt;cell&gt;Here we investigate how memory subsystem behaves in the presence of multithreading and how does that effect software speed.&lt;/cell&gt;
        &lt;cell&gt;Multithreading and the Memory Subsystem&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Low-latency applications&lt;/cell&gt;
        &lt;cell&gt;In some cases we are more interested in short latency than high throughput. We investigate the techniques aimed at improving latency, either by modifying our programs, or reconfiguring the system.&lt;/cell&gt;
        &lt;cell&gt;Latency-Sensitive Applications and the Memory Subsystem: Keeping the Data in the Cache&lt;p&gt;Latency-Sensitive Application and the Memory Subsystem Part 2: Memory Management Mechanisms&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Measuring Memory Subsystem Performance&lt;/cell&gt;
        &lt;cell&gt;We talk about tools and metrics you can use to understand what is going on with the memory subsystem.&lt;/cell&gt;
        &lt;cell&gt;Measuring Memory Subsystem Performance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Other topics&lt;/cell&gt;
        &lt;cell&gt;A few remaining topics related to memory subsystem optimizations that didn’t fit any of the other categories.&lt;/cell&gt;
        &lt;cell&gt;Memory Subsystem Optimizations – The Remaining Topics&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Any feedback on the material covered in this posts will be highly appreciated.&lt;/p&gt;
    &lt;p&gt;Do you need to discuss a performance problem in your project? Or maybe you want a vectorization training for yourself or your team? Contact us &lt;lb/&gt;Or follow us on LinkedIn , Twitter or Mastodon and get notified as soon as new content becomes available.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46456215</guid><pubDate>Thu, 01 Jan 2026 17:52:30 +0000</pubDate></item><item><title>Street-Fighting Mathematics (2008)</title><link>https://ocw.mit.edu/courses/18-098-street-fighting-mathematics-january-iap-2008/pages/readings/</link><description>&lt;doc fingerprint="f8f73fb885dfadb4"&gt;
  &lt;main&gt;
    &lt;p&gt;The readings for each session are designed to be read after attending that session. For a listing of the topics discussed in each session, see here: (PDF)&lt;/p&gt;
    &lt;head rend="h3"&gt;Published Textbook&lt;/head&gt;
    &lt;p&gt;Mahajan, Sanjoy. Street-Fighting Mathematics: The Art of Educated Guessing and Opportunistic Problem Solving. Cambridge, MA: MIT Press, 2010. ISBN: 9780262514293.&lt;/p&gt;
    &lt;p&gt;The book is available as a free download (PDF) from MIT Press.&lt;/p&gt;
    &lt;head rend="h3"&gt;Textbook (Earlier Edition)&lt;/head&gt;
    &lt;p&gt;An earlier versionÂ of theÂ textbook as one file may be found here: (PDF - 1.3 MB)&lt;/p&gt;
    &lt;p&gt;Title page and table of contents (PDF)&lt;/p&gt;
    &lt;p&gt;References (PDF)&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;SESÂ #&lt;/cell&gt;
        &lt;cell role="head"&gt;TOPICS&lt;/cell&gt;
        &lt;cell role="head"&gt;READINGS&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;Dimensions&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Chapter 1 - Dimensions, pp. 3-12 (PDF)&lt;/p&gt;
          &lt;p&gt;Questions from lecture 1 (PDF)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;Extreme cases&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Chapter 2 - Extreme cases, pp. 13-26 (PDF)&lt;/p&gt;
          &lt;p&gt;Questions and answers from lecture 2 (PDF)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;Application: drag&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Chapter 2 - Extreme cases, pp. 26-30 (PDF)&lt;/p&gt;
          &lt;p&gt;Questions from lecture 3 (PDF)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;More on drag&lt;/cell&gt;
        &lt;cell&gt;Â&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;Discretization&lt;/cell&gt;
        &lt;cell&gt;Chapter 3 - Discretization, pp. 31-40 (PDF)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;Application: pendulum period&lt;/cell&gt;
        &lt;cell&gt;Chapter 3 - Discretization, pp. 41-44 (PDF)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;Picture proofs&lt;/cell&gt;
        &lt;cell&gt;Chapter 4 - Picture proofs, pp. 45-56 (PDF)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;Taking out the big part&lt;/cell&gt;
        &lt;cell&gt;Chapter 5 - Taking out the big part, pp. 57-79 (PDF)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;Analogy&lt;/cell&gt;
        &lt;cell&gt;Chapter 6 - Analogy, pp. 80-85 (PDF)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;Application: operators&lt;/cell&gt;
        &lt;cell&gt;Chapter 7 - Operators, pp. 86-90 (PDF)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;Application: singing logarithms&lt;/cell&gt;
        &lt;cell&gt;Handout - approximating logarithms using musical intervals (PDF)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46456543</guid><pubDate>Thu, 01 Jan 2026 18:20:38 +0000</pubDate></item><item><title>Building an internal agent: Code-driven vs. LLM-driven workflows</title><link>https://lethain.com/agents-coordinators/</link><description>&lt;doc fingerprint="b84e8d7920875ad9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Building an internal agent: Code-driven vs LLM-driven workflows&lt;/head&gt;
    &lt;p&gt;When I started this project, I knew deep in my heart that we could get an LLM plus tool-usage to solve arbitrarily complex workflows. I still believe this is possible, but I’m no longer convinced this is actually a good solution. Some problems are just vastly simpler, cheaper, and faster to solve with software. This post talks about our approach to supporting both code and LLM-driven workflows, and why we decided it was necessary.&lt;/p&gt;
    &lt;p&gt;This is part of the Building an internal agent series.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why determinism matters&lt;/head&gt;
    &lt;p&gt;When I joined Imprint, we already had a channel where folks would share pull requests for review. It wasn’t required to add pull requests to that channel, but it was often the fastest way to get someone to review it, particularly for cross-team pull requests.&lt;/p&gt;
    &lt;p&gt;I often start my day by skimming for pull requests that need a review in that channel, and quickly realized that often a pull request would get reviewed and merged without someone adding the &lt;code&gt;:merged:&lt;/code&gt; reacji onto the chat. This felt inefficient, but also
extraordinarily minor, and not the kind of thing I want to complain about.
Instead, I pondered how I could solve it without requiring additional human labor.&lt;/p&gt;
    &lt;p&gt;So, I added an LLM-powered workflow to solve this. The prompt was straightforward:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Get the last 10 messages in the Slack channel&lt;/item&gt;
      &lt;item&gt;For each one, if there was exactly one Github pull request URL, extract that URL&lt;/item&gt;
      &lt;item&gt;Use the Github MCP to check the status of each of those URLs&lt;/item&gt;
      &lt;item&gt;Add the &lt;code&gt;:merged:&lt;/code&gt;reacji to messages where the associated pull request was merged or closed&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This worked so well! So, so well. Except, ahh, except that it sometimes decided to add &lt;code&gt;:merged:&lt;/code&gt;
to pull requests that weren’t merged. Then no one would look at those pull requests.
So, it worked in concept–so much smart tool usage!–but in practice it actually didn’t
solve the problem I was trying to solve: erroneous additions of the reacji meant
folks couldn’t evaluate whether to look at a given pull request in the channel based on the reacji’s presence.&lt;/p&gt;
    &lt;p&gt;(As an aside, some people really don’t like the term &lt;code&gt;reacji&lt;/code&gt;.
Don’t complain to me about it, this is what Slack calls them.)&lt;/p&gt;
    &lt;head rend="h2"&gt;How we implemented support for code-driven workflows&lt;/head&gt;
    &lt;p&gt;Our LLM-driven workflows are orchestrated by a software handler. That handler works something like:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Trigger comes in, and the handler selects which configuration corresponds with the trigger&lt;/item&gt;
      &lt;item&gt;Handler uses that configuration and trigger to pull the associated prompt, load the approved tools, and generate the available list of virtual files (e.g. files attached to a Jira issue or Slack message)&lt;/item&gt;
      &lt;item&gt;Handler sends the prompt and available tools to an LLM, then coordinates tool calls based on the LLM’s response, including e.g. making virtual files available to tools. The handler also has termination conditions where it prevents excessive tool usage, and so on&lt;/item&gt;
      &lt;item&gt;Eventually the LLM will stop recommending tools, and the final response from the LLM will be used or discarded depending on the configuration (e.g. configuration can determine whether the final response is sent to Slack)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We updated our configuration to allow running in one of two configurations:&lt;/p&gt;
    &lt;code&gt;# this is default behavior if omitted
coordinator: llm

# this is code-driven workflow
coordinator: script
coordinator_script: scripts/pr_merged.py
&lt;/code&gt;
    &lt;p&gt;When the &lt;code&gt;coordinator&lt;/code&gt; is set to &lt;code&gt;script&lt;/code&gt;, then instead of using the handler to determine which tools are called,
custom Python is used. That Python code has access to the same tools, trigger data, and virtual files
as the LLM-handling code. It can use the subagent tool to invoke an LLM where useful
(and that subagent can have full access to tools as well), but LLM control only occurs when explicitly desired.&lt;/p&gt;
    &lt;p&gt;This means that these scripts–which are being written and checked in by our software engineers, going through code review and so on–have the same permission and capabilities as the LLM, although given it’s just code, any given commit could also introduce a new dependency, etc.&lt;/p&gt;
    &lt;head rend="h2"&gt;How’s it working? / Next steps?&lt;/head&gt;
    &lt;p&gt;Altogether, this has worked very well for complex workflows. I would describe it as a “solution of frequent resort”, where we use code-driven workflows as a progressive enhancement for workflows where LLM prompts and tools aren’t reliable or quick enough. We still start all workflows using the LLM, which works for many cases. When we do rewrite, Claude Code can almost always rewrite the prompt into the code workflow in one-shot.&lt;/p&gt;
    &lt;p&gt;Even as models get more powerful, relying on them narrowly in cases where we truly need intelligence, rather than for iterative workflows, seems like a long-term addition to our toolkit.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46456682</guid><pubDate>Thu, 01 Jan 2026 18:34:25 +0000</pubDate></item><item><title>Dell's version of the DGX Spark fixes pain points</title><link>https://www.jeffgeerling.com/blog/2025/dells-version-dgx-spark-fixes-pain-points</link><description>&lt;doc fingerprint="bdb351577a83619d"&gt;
  &lt;main&gt;
    &lt;p&gt;Dell sent me two of their GB10 mini workstations to test:&lt;/p&gt;
    &lt;p&gt;In this blog post, I'll cover the base system, just one of the two nodes. Cluster testing is ongoing, and I'll cover things like AI model training and networking more in depth next year, likely with comparisons to the Framework Desktop cluster and Mac Studio cluster I've also been testing.&lt;/p&gt;
    &lt;p&gt;But many of the same caveats of the DGX Spark (namely, price to performance is not great if you just want to run LLMs on a small desktop) apply to Dell's GB10 box as well.&lt;/p&gt;
    &lt;p&gt;It costs a little more than the DGX Spark, but does solve a couple pain points people experienced on the DGX Spark:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It has a power LED (seriously, why does the DGX Spark not have one?!)&lt;/item&gt;
      &lt;item&gt;The included power supply is 280W instead of 240W for a little more headroom&lt;/item&gt;
      &lt;item&gt;The thermal design (front-to-back airflow) seems less restricted, so is quieter and capable of keeping the GB10 'AI Superchip' from thermal throttling&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But if this isn't a mini PC to compete with a Mac mini, nor a good value for huge LLMs like a Mac Studio, or AMD's Ryzen AI Max+ 395 machines, what is it and who is it for?&lt;/p&gt;
    &lt;p&gt;Well, it's a $4,000+ box built specifically for developers in Nvidia's ecosystem, deploying code to Nvidia servers that cost half a million dollars each. A major part of the selling point are these built-in 200 gigabit QSFP ports, which would cost $1,500 or so to add on to another system, assuming you have the available PCIe bandwidth:&lt;/p&gt;
    &lt;p&gt;Those ports can't achieve 400 Gbps, but they do hit over 200 Gbps in the right conditions, configured for Infiniband / RDMA. And they hit over 100 Gbps for Ethernet (though only when running multiple TCP streams).&lt;/p&gt;
    &lt;p&gt;So it may seem a little bit of an odd duck for me, since I'm not an 'Nvidia developer' and I don't deploy code to Nvidia's 'AI factories'.&lt;/p&gt;
    &lt;p&gt;If I'm being honest, I'm more interested in the 'Grace' part of the GB10 (or 'Grace Blackwell 10') 'AI Superchip. It's a big.LITTLE Arm CPU co-designed by Mediatek, with 10 Cortex-X925 cores and 10 Cortex-A725 cores.&lt;/p&gt;
    &lt;p&gt;The chip is united to the Blackwell GPU, and shares the same 128 GB pool of LPDDR5X memory. And it's a pretty snappy Arm CPU—just stuck in a $4,000+ system.&lt;/p&gt;
    &lt;p&gt;But like I said, Dell sent me these boxes to test. They aren't paying for this blog post and have no control over what I say.&lt;/p&gt;
    &lt;p&gt;In fact, one of the main things they said was "this is isn't a gaming machine, so don't focus on that."&lt;/p&gt;
    &lt;p&gt;But that got me thinking. What if... I did.&lt;/p&gt;
    &lt;head rend="h2"&gt;Gaming on Arm Linux&lt;/head&gt;
    &lt;p&gt;Valve just announced the Steam Frame, and it runs on Arm.&lt;/p&gt;
    &lt;p&gt;Steam Frame will use FEX for its x86-Arm translation layer, and CodeWeavers' Crossover Preview for Arm64 was just released, so I thought I'd give that a try on DGX OS (Nvidia's Linux OS, currently based on Ubuntu 24.04).&lt;/p&gt;
    &lt;p&gt;I was able to quickly install Steam, and through that, games like Cyberpunk 2077, Doom Eternal, and Ultimate Epic Battle Simulator II.&lt;/p&gt;
    &lt;p&gt;I'll leave the full experience and test results for you to see in this video:&lt;/p&gt;
    &lt;p&gt;But bottom line, the Windows games I typically test on Arm systems through Steam/Proton played very well here, with no stuttering, and decent frame rates (100 fps in Cyberpunk 2077 at 1080p with low settings).&lt;/p&gt;
    &lt;p&gt;But no, I agree with Dell, this box should not be evaluated as a gaming machine. While it performs admirably for an Arm linux box, you could do a lot better with half the budget if you just wanted to build a dedicated gaming rig. Even with RAM prices as they are today.&lt;/p&gt;
    &lt;head rend="h2"&gt;General Purpose Arm Workstation (with tons of VRAM)&lt;/head&gt;
    &lt;p&gt;This machine is built for AI development, but it just so happens to have a very good Arm CPU and tons of RAM, so I wanted to test it for both running LLMs, and as a general Arm Linux workstation.&lt;/p&gt;
    &lt;p&gt;The video above has more depth, and you can find all my benchmark data here, but I wanted to focus on a few things in particular.&lt;/p&gt;
    &lt;head rend="h2"&gt;Software&lt;/head&gt;
    &lt;p&gt;Before we get to benchmarks, I wanted to mention Nvidia's DGX OS. Based on Ubuntu Linux, it's the only supported Linux distribution for GB10 systems. Regular Ubuntu LTS versions are supported for 5 years, with optional Pro support extending that out to 10 or even 15 years. But DGX OS only guarantees updates for two years, though Nvidia doesn't really offer guarantees for its hardware support.&lt;/p&gt;
    &lt;p&gt;Their track record for ongoing support for their hardware is decidedly mixed, and in the absence of any guarantees, I wouldn't expect them to continue supporting the Spark or other GB10 systems beyond a few years.&lt;/p&gt;
    &lt;p&gt;Some people have had luck getting other distros running, but they're still running Nvidia's kernel. So if you buy one of these, know there's no guarantees for ongoing support.&lt;/p&gt;
    &lt;p&gt;Running things on DGX OS, I've found most server/headless software runs great, but there are still desktop tools that are more of a hassle. Like Blender doesn't have a stable release that uses GPU acceleration on Arm. But if you compile it from sourcelike GitHub user CoconutMacaroon did, you can get full acceleration.&lt;/p&gt;
    &lt;p&gt;Just using this box as a little workstation, it is plenty fast for all the things I do, from coding, to browsing the web, to media editing. (Though media workflows are still rough on Linux in general, even on x86.)&lt;/p&gt;
    &lt;head rend="h2"&gt;CPU benchmarks&lt;/head&gt;
    &lt;p&gt;The Grace CPU is a 20-core Arm chip co-designed by Mediatek, fused together with the Blackwell GPU.&lt;/p&gt;
    &lt;p&gt;There must be some inefficiency there, though, because the system's idle power draw is a bit higher than I'm used to for Arm, coming in around 30 watts. A lot higher than Apple's M3 Ultra with 512GB of RAM, or even AMD's Ryzen AI Max+ 395 (these names just roll right off the tongue, don't they?).&lt;/p&gt;
    &lt;p&gt;In my testing, it seems the CPU itself maxes out around 140 watts, leaving another 140 watts of headroom for the GPU, network, and USB-C ports with PD.&lt;/p&gt;
    &lt;p&gt;Geekbench 6 was a little unstable, which was weird, but when I did get it to run, it was about on par with the AMD Ryzen AI Max+ 395 system I tested earlier this year, the Framework Desktop.&lt;/p&gt;
    &lt;p&gt;Apple's 2-generation-old M3 Ultra Mac studio beats both, but it does cost quite a bit more, so that's to be expected.&lt;/p&gt;
    &lt;p&gt;And testing with High Performance Linpack, the Dell Pro Max gets about 675 Gflops:&lt;/p&gt;
    &lt;p&gt;NVIDIA's marketing said the GB10 "offers a petaflop of AI computing performance"—a thousand teraflops! This thing can't even hit one...&lt;/p&gt;
    &lt;p&gt;But in the fine print, NVIDIA says it's a petaflop at FP4 precision. HPL tests FP64, aka double precision, which is more used in scientific computing. A FLOP is not always a FLOP, and even the 'petaflop' claim seems disputed, at least if I'm reading John Carmack's tweets correctly.&lt;/p&gt;
    &lt;p&gt;But at least for FP64 on the CPU, the GB10 is fairly efficient, at least compared to x86 systems I've tested:&lt;/p&gt;
    &lt;head rend="h2"&gt;Networking Performance&lt;/head&gt;
    &lt;p&gt;A huge part of the value is the built-in ConnectX-7 networking. I tested that, and it's fast. But also a bit odd. Here's the maximum TCP performance I was able to get through the fastest interface on each of the three systems I've been comparing:&lt;/p&gt;
    &lt;p&gt;But 106 Gigabits isn't 200, is NVIDIA lying?&lt;/p&gt;
    &lt;p&gt;Well, no... it's a little complicated. For full details, I'll refer you to the ServeTheHome article The NVIDIA GB10 ConnectX-7 200GbE Networking is Really Different.&lt;/p&gt;
    &lt;p&gt;Because the ports are each connected to a x4 PCIe Gen 5 link—which isn't enough bandwidth for 200 Gbps per port. To get a full 200 Gbps, you have to use Infiniband/RDMA and carefully configure the network topology. You won't get more than about 206 Gbps, maximum, in real world throughput, no matter how you set it up.&lt;/p&gt;
    &lt;p&gt;That's still honestly pretty good, but it's not the same as getting 400 Gbps of networking for AI clustering, like I think some of us expected reading the initial press releases in early 2025...&lt;/p&gt;
    &lt;p&gt;From the perspective of someone replicating NVIDIA's networking stack locally, though, having ConnectX ports built in is a boon. If you want replicate this kind of developer setup on AMD, you'd have to spend around the same amount of money, for the Max+ 395 plus a Connect-X 7 card.&lt;/p&gt;
    &lt;p&gt;Many people don't care about clustering use cases, or RDMA or Infiniband, but that doesn't mean it's not extremely useful for the people who do. This stuff's expensive, but to some people, it's not a bad value.&lt;/p&gt;
    &lt;head rend="h2"&gt;AI Performance&lt;/head&gt;
    &lt;p&gt;For now I'm just running two models, both of them with llama.cpp, optimized for each architecture.&lt;/p&gt;
    &lt;p&gt;And for a small model that requires a decent amount of CPU to keep up with the GPU, the GB10 does pretty well, almost hitting 100 tokens/s for inference, which is second to the M3 Ultra:&lt;/p&gt;
    &lt;p&gt;But for prompt processing, which is important for how quickly you start seeing a response from AI models, the GB10 chip is the winner, despite costing less than half the M3 Ultra.&lt;/p&gt;
    &lt;p&gt;And it's a similar story for a huge 'dense' model, Llama 3.1 70B, except here, it gets beat just a little by AMD's Strix Halo in the Framework Desktop:&lt;/p&gt;
    &lt;p&gt;Prompt processing is a strong selling point for these boxes. That's the reason Exo teased running a DGX Spark as the compute node for a Mac Studio cluster.&lt;/p&gt;
    &lt;p&gt;You can have the Spark, or one of these Dell's, handle the thing it's best at, prompt processing, while the Mac Studios handle the thing they're best at, memory bandwidth for token generation.&lt;/p&gt;
    &lt;p&gt;Anyway, these are just two quick AI benchmarks, and I have a lot more in the Dell Pro Max with GB10 issue in my ai-benchmarks repository. I'm doing a lot more testing, including model training and how I clustered two of these things in a tiny mini rack, but you'll have to wait until next year for that.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46457027</guid><pubDate>Thu, 01 Jan 2026 19:11:53 +0000</pubDate></item><item><title>Prompting People</title><link>https://kuber.studio/blog/Reflections/Prompting-People</link><description>&lt;doc fingerprint="a51e63760747d9be"&gt;
  &lt;main&gt;
    &lt;p&gt;I just gave my final end semester exam a couple days ago when I bumped into an old friend and we got into talking and discussing some important topics for the test.&lt;/p&gt;
    &lt;p&gt;I was midway through explaining a concept he hadn’t covered when he stopped me and pointed out my way of speaking had changed and how it was way more structured and didn’t give him the opportunity to ask follow up questions.&lt;/p&gt;
    &lt;p&gt;and that’s when it hit me - I think the more often I use AI, the more I start talking to people like how I engineer prompts for LLMs and not just over messages or text - even in person.&lt;/p&gt;
    &lt;p&gt;even the concept I was explaining - definition, descriptions, potential follow up questions and edge cases in that order.&lt;/p&gt;
    &lt;p&gt;What’s funny is that this isn’t even intentional - it’s what I believe in my head is the more “efficient” way to express my thoughts and ideas than entirely separate, broken up conversations.&lt;/p&gt;
    &lt;p&gt;The more surprising part is the unusual reactions of the other people getting a better picture and context of what I’m explaining without the usual back and forth - which has landed me my fair share of complaints of having to hear mini lectures, but not more than people appreciative of the fuller picture.&lt;/p&gt;
    &lt;p&gt;It made me realize that while we’re training AI to be more human, the conversations might be training us to be more structured and honestly I’m not sure if this makes me a better communicator or just a better prompt writer, but it’s a hard habit to break.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46457240</guid><pubDate>Thu, 01 Jan 2026 19:33:53 +0000</pubDate></item><item><title>Linux is good now</title><link>https://www.pcgamer.com/software/linux/im-brave-enough-to-say-it-linux-is-good-now-and-if-you-want-to-feel-like-you-actually-own-your-pc-make-2026-the-year-of-linux-on-your-desktop/</link><description>&lt;doc fingerprint="13f391b9904a4bd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I'm brave enough to say it: Linux is good now, and if you want to feel like you actually own your PC, make 2026 the year of Linux on (your) desktop&lt;/head&gt;
    &lt;p&gt;Now if you don't mind I'm going to delete the root folder and see what happens.&lt;/p&gt;
    &lt;p&gt;I'm all-in, baby. I'm committed. If upgrading any distinct component of my PC didn't require me taking out a loan right now, I'd be seriously considering switching my GPU over to some kind of AMD thing just to make my life slightly, slightly easier.&lt;/p&gt;
    &lt;p&gt;I've had it with Windows and ascended to the sunlit uplands of Linux, where the trees heave with open-source fruits and men with large beards grep things with their minds.&lt;/p&gt;
    &lt;p&gt;I'm not alone. In last month's Steam hardware survey, the number of Linux users hit a new all-time high for the second month running, reaching the heady summit of a whopping, ah, 3.2% of overall Steam users. Hey, we're beating Mac players.&lt;/p&gt;
    &lt;p&gt;I think that number will only grow as the new year goes by. More and more of us are getting sick of Windows, sure—the AI guff, the constant upselling on Office subs, the middle taskbar*—but also, all my experience goofing about with Linux this year has dispelled a lot of the, frankly, erroneous ideas I had about it. It's really not hard! Really! I know Linux guys have been saying this for three decades, but it's true now!&lt;/p&gt;
    &lt;head rend="h2"&gt;Goated with the open source (sorry)&lt;/head&gt;
    &lt;p&gt;As I've already written about, the bulk of my Linux-futzing time this year has been spent in Bazzite, a distro tailor-made for gaming and also tailor-made to stop idiots (me) from doing something likely to detonate their boot drive.&lt;/p&gt;
    &lt;p&gt;I grew up thinking of Linux as 'the command-line OS that lets you delete your bootloader' and, well, I suppose that's not untrue, but I've been consistently impressed at how simple Bazzite has been to run on my PC, even with my persnickety Nvidia GPU.&lt;/p&gt;
    &lt;p&gt;Everything I've played this year has been as easy—if not easier—to run on a free OS put together by a gaggle of passionate nerds as it is on Windows, the OS made by one of the most valuable corporations on planet Earth. I've never had to dip into the command line (which is, to be frank, a shame, as the command line is objectively cool).&lt;/p&gt;
    &lt;p&gt;Keep up to date with the most important stories and the best deals, as picked by the PC Gamer team.&lt;/p&gt;
    &lt;p&gt;But to be honest, it's not as if the Bazzite team has miraculously made Linux pleasant to use after decades of it seeming difficult and esoteric to normie computer users. I think mainstream Linux distros are just, well, sort of good now. Apart from my gaming PC, I also have an old laptop converted into a media server that lives underneath my television. It runs Debian 13 (which I updated to from Debian 12 earlier in the year) and requires essentially zero input from me at all.&lt;/p&gt;
    &lt;p&gt;What's more, the only software I have on there is software I actually want on there. Oh for a version of Windows that let me do something as zany as, I don't know, uninstall Edge.&lt;/p&gt;
    &lt;p&gt;That's the true nub of it, I think. The stats can say what they like (and they do! We've all heard tales of Windows games actually running better on Linux via Valve's Proton compatibility layer), but the heart of my fatigue with Windows is that, for every new worthless AI gadget Microsoft crams into it and for every time the OS inexplicably boots to a white screen and implores me to "finish setting up" my PC with an Office 365 subscription, the real problem is a feeling that my computer isn't mine, that I am somehow renting this thing I put together with my own two hands from an AI corporation in Redmond.&lt;/p&gt;
    &lt;p&gt;That's fine for consoles. Indeed, part of the whole pitch of an Xbox or PlayStation is the notion that you are handing off a lot of responsibility for your device to Sony and Microsoft's teams of techs, but my PC? That I built? Get your grubby mitts off it.&lt;/p&gt;
    &lt;p&gt;Are there issues? Sure. HDR's still a crapshoot (plus ça change) and, as you've no doubt heard, a lot of live-service games have anticheat software that won't play with Linux. But I think both of these issues are gradually ticking toward their solutions, particularly with Valve making its own push into the living room.&lt;/p&gt;
    &lt;p&gt;So I say make 2026 the year you give Linux a try, if you haven't already. At the very least, you can stick it on a separate boot drive and have a noodle about with it. I suspect you'll find the open (source) water is a lot more hospitable than you might think.&lt;/p&gt;
    &lt;p&gt;*I'm actually fine with the middle taskbar. I'm sorry.&lt;/p&gt;
    &lt;p&gt;2026 Games: This year's upcoming games&lt;lb/&gt;Best PC games: Our all-time favorites&lt;lb/&gt;Free PC games: Freebie fest&lt;lb/&gt;Best FPS games: Finest gunplay&lt;lb/&gt;Best RPGs: Grand adventures&lt;lb/&gt;Best co-op games: Better together&lt;/p&gt;
    &lt;p&gt;One of Josh's first memories is of playing Quake 2 on the family computer when he was much too young to be doing that, and he's been irreparably game-brained ever since. His writing has been featured in Vice, Fanbyte, and the Financial Times. He'll play pretty much anything, and has written far too much on everything from visual novels to Assassin's Creed. His most profound loves are for CRPGs, immersive sims, and any game whose ambition outstrips its budget. He thinks you're all far too mean about Deus Ex: Invisible War.&lt;/p&gt;
    &lt;p&gt;You must confirm your public display name before commenting&lt;/p&gt;
    &lt;p&gt;Please logout and then login again, you will then be prompted to enter your display name.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46457770</guid><pubDate>Thu, 01 Jan 2026 20:35:11 +0000</pubDate></item><item><title>A website to destroy all websites</title><link>https://henry.codes/writing/a-website-to-destroy-all-websites/</link><description>&lt;doc fingerprint="3cc011532966ad5d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A website to destroy all websites.&lt;/head&gt;
    &lt;head rend="h2"&gt;table of contents, of course&lt;/head&gt;
    &lt;head rend="h2"&gt;The internet is bad.&lt;/head&gt;
    &lt;p&gt;Well, the Internet mostly feels bad these days.&lt;/p&gt;
    &lt;p&gt;We were given this vast, holy realm of self-discovery and joy and philosophy and community; a thousand thousand acres of digital landscape, on which to grow our forests and grasslands of imagination, plant our gardens of learning, explore the caves of our making. We were given the chance to know anything about anything, to be our own Prometheus, to make wishes and to grant them.&lt;/p&gt;
    &lt;p&gt;But that’s not what we use the Internet for anymore. These days, instead of using it to make ourselves, most of us are using it to waste ourselves: we’re doom-scrolling brain-rot on the attention-farm, we’re getting slop from the feed.&lt;/p&gt;
    &lt;p&gt;Instead of turning freely in the HTTP meadows we grow for each other, we go to work: we break our backs at the foundry of algorithmic content as this earnest, naïve, human endeavoring to connect our lives with others is corrupted. Our powerful drive to learn about ourselves, each other, and our world, is broken into scant remnants — hollow, clutching phantasms of Content Creation, speed-cut vertical video, listicle thought-leadership, ragebait and the thread emoji.&lt;/p&gt;
    &lt;head rend="h3"&gt;it wasn’t always like this.&lt;/head&gt;
    &lt;p&gt;It used to feel way better to Go Online, and some of us will remember.&lt;/p&gt;
    &lt;p&gt;We used to be able to learn about our hobbies and interests from hundreds of experts on a wealth of websites whose only shared motivation was their passion. Some of those venerable old educational blogs, forums, and wikis still stand, though most have been bulldozed.&lt;/p&gt;
    &lt;p&gt;Now, Learning On The Internet often means fighting ads and endless assaults on one’s attention — it means watching part-1-part-2-part-3 short-form video clips, taped together by action movie psychology hacks, narrated gracelessly by TTS AI voices. We’re down from a thousand and one websites to three, and each of those remaining monolith websites is just a soullessly-regurgitated, compression-down-scaled, AI-up-scaled version of the next.&lt;/p&gt;
    &lt;p&gt;¶&lt;/p&gt;
    &lt;p&gt;We used to make lasting friendships with folks all over the world on shared interest and good humor.&lt;/p&gt;
    &lt;p&gt;But now those social networks, once hand-built and hand-tended, vibrant and organic, are unceremoniously swallowed by social media networks, pens built for trapping us and our little piggy attentions, turning us all into clout-chasers &amp;amp; content-creators, and removing us from what meaningful intimacy &amp;amp; community felt like.&lt;/p&gt;
    &lt;p&gt;¶&lt;/p&gt;
    &lt;p&gt;Even coding for the web used to be different: One could Learn To Code™ to express oneself creatively, imbue one’s online presence with passion and meaning, and for some of us, build a real career.&lt;/p&gt;
    &lt;p&gt;These days, however, we write increasing amounts of complicated, unsecure code to express less and less meaning, in order to infinitely generate shareholder value. We don’t think about the art of our craft and the discipline of its application, we think about throughput and scale.&lt;/p&gt;
    &lt;p&gt;To be very clear: I’m not trying to Good Old Days the internet. None of this is meant to make you feel nostalgic — the Internet used to be slow and less populated and less diverse, and its access was limited to those of a certain class. The Web For All is a marked improvement, widespread global internet access is a marked improvement, and what I’m asking you to consider is what it used to feel like to use these tools, and what we’ve lost in the Big Tech, Web 2.0 and web3 devouring of the ’Net.&lt;/p&gt;
    &lt;head rend="h2"&gt;The invention of the automobile&lt;/head&gt;
    &lt;p&gt;The onset of the automobile was a revelation for access and personal liberty. With the advent of cars, members of society could travel farther, get more done in their day, and bend their limited time more to their creative will!&lt;/p&gt;
    &lt;p&gt;But as time wore on and the industrialization &amp;amp; proliferation of the automobile progressed, its marginal utility diminished — the industry started to offer society fewer &amp;amp; fewer benefits, and take more &amp;amp; more in exchange1.&lt;/p&gt;
    &lt;p&gt;In American cities, for example: though at first the automobile enabled humans to travel further distances, it now demanded that humans travel those distances, and demanded infrastructure be created &amp;amp; maintained to enable it.2 Many now must use an automobile to get everything done in their town in a day, and must pay &amp;amp; take time for that automobile’s fueling &amp;amp; maintenance.3&lt;/p&gt;
    &lt;p&gt;Further than that, the automobile asks all of us to chip in tax revenue to protect its infrastructure, but only certain classes can afford an automobile with which to use that infrastructure, and those classes who can’t afford to do so are relegated to underfunded public transit systems.4&lt;/p&gt;
    &lt;p&gt;No longer a tool to serve our societies, our societies now serve the automobile.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tools for Conviviality, &amp;amp; the industrialization of the Web.&lt;/head&gt;
    &lt;p&gt;In his book Tools For Conviviality, technology philosopher and social critic Ivan Illich identifies these two critical moments, the optimistic arrival &amp;amp; the deadening industrialization, as watersheds of technological advent. Tools are first created to enhance our capacities to spend our energy more freely and in turn spend our days more freely, but as their industrialization increases, their manipulation &amp;amp; usurpation of society increases in tow5.&lt;/p&gt;
    &lt;p&gt;Illich also describes the concept of radical monopoly, which is that point where a technological tool is so dominant that people are excluded from society unless they become its users. We saw this with the automobile, we saw it with the internet, and we even see it with social media.&lt;/p&gt;
    &lt;p&gt;&lt;del&gt;No longer a tool to serve our societies, our societies now serve the automobile.&lt;/del&gt; Instead of designing and using tools to build a society, our society changes to adapt to the demands of our tools.&lt;/p&gt;
    &lt;p&gt;¶&lt;/p&gt;
    &lt;p&gt;Illich’s thesis allows us to reframe our adoption and use of the technologies in our life. We can map fairly directly most technological developments in the last 100 (or even 200) years to this framework: a net lift, followed by a push to extract value and subsequent insistence upon the technology’s ubiquity:&lt;/p&gt;
    &lt;head rend="h3"&gt;the textile revolution&lt;/head&gt;
    &lt;p&gt;The preferred imagery used to mythologize the Industrial Revolution is the woodetchings of textile manufacturers, transformed in the early 19th century by the arrival of automated fabric machinery. Its proponents laud the shift of an agricultural society to a technological one, creating new sectors for labor, and raising up the middle class (we will say nothing of this period’s new punishing conditions for labor in this essay6). But the ultimate ecological and human costs engendered by the increasing availability of cheap fabric production are well-documented: In 2022, the fashion and textile industries employed around 60 million factory workers worldwide7, and less than 2% of those workers earn a living wage. Those workers also endure the full suite of labor exploitation practices, including gender-based harassment, wage theft, and unsafe conditions. On the material side, the induced consumption resulting from ever-cheaper products means the world consumed 400% more textile products globally as 20 years ago8, and bins most of it (the average American generates 82 pounds of textile waste each year).&lt;/p&gt;
    &lt;head rend="h3"&gt;antibiotic technology&lt;/head&gt;
    &lt;p&gt;The arrival of antibiotics in 19289 allowed for revolutionary leaps in fighting bacterial infections like strep throat, pneumonia, and meningitis, but an over-dependence and over-prescription of penicillin and its siblings through the 1950s-70s resulted in the proliferation of antibiotic resistance, which subsequently led to longer hospital stays, higher medical costs, and increased mortality.10&lt;/p&gt;
    &lt;head rend="h3"&gt;space exploration&lt;/head&gt;
    &lt;p&gt;Since the beginning of the space exploration era in the late 1950s, humanity has made leaps and bounds in learning about our own world and its physical systems, telecommunications, imaging, etc. The increasing frequency of commercialization missions in space for satellite systems (and lately tourism) has resulted in immense amounts of space debris being generated — both from active satellites and from jettisoned/destroyed components of previous missions, the debris threatens future missions and has even been destructive to the field of astronomy, making it impossible to use earth-based sensors and photography devices to learn about space.11 So desperate to extract Shareholder Value from the starry sky, we’re blinding our own ability to look at it.&lt;/p&gt;
    &lt;p&gt;The web is no exception to this pattern. A vision of interoperability, accessibility, and usability, the World Wide Web was first conceived in 1989 as a way to universally link documents and other media content in a flexibly-organized system that could make information easily accessed at CERN, and be easily shared with collaborators beyond.12 But the proliferation of access and ultimate social requirement of access has spawned countless troubles for human society, including cyberstalking and bullying, the instantaneous circulation of CSAM, violent images, and misinformation, identity theft, addiction, etcetera.&lt;/p&gt;
    &lt;p&gt;The rampant industrialization and commercialization of the Web predictably develops flashy, insidious patterns of extracting capital from its users: new surfaces for information means new surfaces for advertisement, and new formats of media beget new mechanisms for divorcing you from their ownership.&lt;/p&gt;
    &lt;head rend="h3"&gt;convivial life &amp;amp; convivial tooling&lt;/head&gt;
    &lt;p&gt;Illich poses convivial tools as directly opposed to this industrialized, radically-monopolized set of social systems. Similar to E.F. Schumacher’s concept of “intermediate technology” introduced in his 1973 book Small Is Beautiful: A Study of Economics As If People Mattered, convivial tools are sustainable, energy-efficient (though often labor intensive), local-first, and designed primarily to enhance the autonomy and creativity of their users.13 Illich cites specifically hand tools, bicycles, and telephones as examples, but with its enormous capacity for interoperability and extensibility, the Internet is the perfect workshed in which to design our own Tools For Conviviality.&lt;/p&gt;
    &lt;head rend="h2"&gt;the Web we want&lt;/head&gt;
    &lt;p&gt;let’s reconsider&lt;/p&gt;
    &lt;p&gt;the markers of a decaying 'Net I mentioned before, with convivial tooling in mind:&lt;/p&gt;
    &lt;head rend="h3"&gt;Teaching &amp;amp; learning on the Web&lt;/head&gt;
    &lt;p&gt;Monolithic platforms like YouTube, TikTok, Medium, and Substack draw a ton of creators and educators because of the promise of monetization and large audiences, but they’ve shown time and time again how the lack of ownership creates a problem. When those platforms fail, when they change their rules, when they demand creators move or create a particular way to maintain their access to those audiences, they pit creators or their audiences against the loss of the other. Without adhering to the algorithm’s requirements, writers may not write an impactful document, and without bypassing a paywall, readers can’t read it.&lt;/p&gt;
    &lt;p&gt;¶&lt;/p&gt;
    &lt;p&gt;When those promises of exorbitant wealth and a life of decadence through per-click monetization ultimately dry up (or come with a steep moral or creative cost), creators and learners must look for new solutions for how educational content is shared on the Internet. The most self-evident, convivial answer is an old one: blogs. HTML is free to access by default, RSS has worked for about 130 years[citation needed], and combined with webmentions, it’s never been easier to read new ideas, experiment with ideas, and build upon &amp;amp; grow those ideas with other strong thinkers on the web, owning that content all along.14&lt;/p&gt;
    &lt;head rend="h3"&gt;Connecting with friends on the Web&lt;/head&gt;
    &lt;p&gt;Social media apps have imprisoned us all in this weird content prison — in order to connect with friends we’re sort of forced to create or be vanished by capricious black box algorithms, and all that we do create is, as we’ve already alluded to, subsequently owned by whatever platform we’ve created it on. If Instagram goes away overnight, or decides to pivot catastrophically, your stories and your network of friends goes with it.&lt;/p&gt;
    &lt;p&gt;¶&lt;/p&gt;
    &lt;p&gt;The advent and development of tools &amp;amp; methodologies like POSSE (Publish On your Own Site, Syndicate Elsewhere), ActivityPub, microformats, and ATProto, it’s becoming quite achievable to generate your own social network, interoperable with other networks like Bluesky or Mastodon. That network, designed for ownership and decentralization, is durable, designed around storytelling instead of engagement, and free of the whims of weird tech billionaires.&lt;/p&gt;
    &lt;p&gt;With some basic HTML knowledge and getting-stuff-online knowledge, a handful of scrappy protocols, and a free afternoon or two, one can build their own home to post bangers for the tight homies, make friends, and snipe those new friends with those hits of dopamine they so fiendishly rely on.&lt;/p&gt;
    &lt;head rend="h3"&gt;Coding for the web&lt;/head&gt;
    &lt;p&gt;Lastly, consider the discipline of web engineering:&lt;/p&gt;
    &lt;p&gt;We have been asked to build the same B2B SaaS website with the same featureset n^∞ times, and our answers for the optimal way to do that are increasingly limited. We’ve penned all of our markup into JavaScript templates just in case a product manager needs the wrapper component to post JSON somewhere down the line, and we’ve whittled away at style code until it’s just a mechanism for deploying one of two border-radius-drop-shadow combos to divs. It’s an industrial, production-minded way of approaching a discipline that has all the hallmarks of being a great craft, and that’s understandably uninspiring to many of us.&lt;/p&gt;
    &lt;p&gt;¶&lt;/p&gt;
    &lt;p&gt;Yet our young React shepherds have no need to fear: there are countless more colors than blurple out there, and countless more fonts than Inter. HTML and CSS are better and more generative technologies than they’ve ever been: Thanks to the tireless work of the CSS working groups and browser implementers, etc, there is an unbelievable amount of creative expression possible with basic web tools in a text editor. Even JavaScript is more progressively-ehanceable than ever, and enables interfacing with a rapidly-growing number of exciting browser APIs (still fuck Brendan Eich though). &lt;code&gt;${new Date.getCurrentYear()}&lt;/code&gt; is a veritable renaissance of web code, and it asks of authors only curiosity and a drive to experiment.&lt;/p&gt;
    &lt;head rend="h2"&gt;so where do we go from here?&lt;/head&gt;
    &lt;p&gt;Illich’s thesis is that technology and its derived tools should serve people in a way that enhances their freedom, creativity, independence, and will.&lt;/p&gt;
    &lt;p&gt;The distillation of those principles on the web through manual code, hand-built social networks, and blogs, points luminously to one answer to the question of how the Internet can best serve humans:&lt;/p&gt;
    &lt;head rend="h3"&gt;it’s personal websites.&lt;/head&gt;
    &lt;p&gt;Hand-coded, syndicated, and above all personal websites are exemplary: They let users of the internet to be autonomous, experiment, have ownership, learn, share, find god, find love, find purpose. Bespoke, endlessly tweaked, eternally redesigned, built-in-public, surprising UI and delightful UX. The personal website is a staunch undying answer to everything the corporate and industrial web has taken from us.&lt;/p&gt;
    &lt;p&gt;And how might one claim this ultimate toolchain of conviviality, and build a place on the web that enhances their autonomy and creativity?&lt;/p&gt;
    &lt;p&gt;How might one build a personal website?&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Start small&lt;/head&gt;
        &lt;p&gt;Let yourself start small, have fun trying shit that doesn’t work, document your growth, publish failed ideas &amp;amp; successful ones. Some of the best websites in the world are just HTML, and they belong to their authors. Make friends, let yourself be inspired by others, send friendly emails asking to learn new things, and do not demand of yourself masterpieces.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;head rend="h3"&gt;Reduce friction to publishing&lt;/head&gt;&lt;p&gt;Get the resistance to ship out of your way. Don’t get caught up in tooling and frameworks, just write HTML and get something online. If you’re an engineer, delight that you’re not beholden to the same standards of quality and rigorous testing that you are at work — draft some ideas, hit the&lt;/p&gt;&lt;code&gt;h1&lt;/code&gt;to&lt;code&gt;p&lt;/code&gt;tag combo, and publish. Update and update again; let your ideas grow like gardens, the way they do in your mind. The mutability of the web, often its great weakness, is also one of its great strengths.&lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Don’t worry about design (unless you want to)&lt;/head&gt;
        &lt;p&gt;Don’t worry about design unless that’s the part that brings you joy. Make friends with designers and trade your work for theirs, or trade tips, trade advice. Get comfortable with being joyfully bad at something — from that soil of humility grows a million questions for those who have learned and are excited to share. Iterate until you’ve something you’re proud of, or iterate so much you’ve ruined it and have to go back to bald.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Use the IndieWeb&lt;/head&gt;
        &lt;p&gt;Leverage the IndieWeb and its wonderfully thought-out protocols, tools like brid.gy to syndicate your ideas out to the wider web, and then use Webmentions to bring the ensuing conversations back where the content is. That way, you can publish work where you prefer to, folks on Bluesky can enjoy and discuss it, in the same stroke as folks on Mastodon may, or folks directly on the canonical URL.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Join us in sharing what you’ve made&lt;/head&gt;
        &lt;p&gt;I encourage you to join us in our auspicious website adventure, and if you do, I hope you’ll further join us on personalsit.es, our happy little home for everyone building something humble or thrilling or joyful or deeply accursed, but personal.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;(denouement)&lt;/head&gt;
    &lt;p&gt;You’re not crazy. The internet does feel genuinely so awful right now, and for about a thousand and one reasons. But the path back to feeling like you have some control is to un-spin yourself from the Five Apps of the Apocalypse and reclaim the Internet as a set of tools you use to build something you can own &amp;amp; be proud of — or in most of our cases, be deeply ashamed of. Godspeed and good luck.&lt;/p&gt;
    &lt;p&gt;❦&lt;/p&gt;
    &lt;p&gt;That’s all for me. If you find any issues with this post, please reach out to me by email. Thanks eternally for your time and patience, and thanks for reading. Find me here online at one of my personal websites like henry.codes or strange.website or stillness.digital or strangersbyspring.com, or sometimes on Bluesky and Mastodon.&lt;/p&gt;
    &lt;p&gt;As ever, unionize, free Palestine, trans rights are human rights, fix your heart or die.&lt;/p&gt;
    &lt;p&gt;fin.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46457784</guid><pubDate>Thu, 01 Jan 2026 20:36:46 +0000</pubDate></item><item><title>Straussian Memes</title><link>https://www.lesswrong.com/posts/CAwnnKoFdcQucq4hG/straussian-memes-a-lens-on-techniques-for-mass-persuasion</link><description>&lt;doc fingerprint="dab658c32fb88997"&gt;
  &lt;main&gt;
    &lt;p&gt;A Straussian Meme is a meme that communicates different ideas to different kinds of people, according to their ability and willingness to hear the message. A Straussian meme has a specific structure:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;There are higher and lower readings that are related but different. This is called multi-level messaging.&lt;/item&gt;
      &lt;item&gt;Those who accept the higher readings understand the lower readings but see them as "noble lies", "socially necessary fictions", or "useful simplifications".&lt;/item&gt;
      &lt;item&gt;The higher-lower structure is self-stabilizing because of what level either conceals or says about the other levels.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Whether Straussian Memes are intentionally designed or the result of some kind of selection pressure, my claim is that powerful and enduring memes often benefit from self-stabilizing stratified structure in order to maintain a stable and broad base.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Resentful Dad-Santa&lt;/head&gt;
    &lt;p&gt;It's the holidays. A child is overjoyed to receive a special gift.&lt;/p&gt;
    &lt;p&gt;Father knowingly glances at Mom and says: "Santa must love you very much to get you that special [ doll / action figure ]!"&lt;/p&gt;
    &lt;p&gt;Dad is engaging in multi-level messaging.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;What the Child hears is: "Santa loves me!"&lt;/item&gt;
      &lt;item&gt;What the Mother hears is: "As parents, we love you 'through' Santa! The idea of Santa is a way to make your world magical."&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But maybe Dad got the gift on his own (without Mom's help), and what he actually meant was: "I am a better gift giver than you, Mom."&lt;/p&gt;
    &lt;p&gt;This is a more interesting possibility because Dad is using the idea of Santa as a bit of a shield: Mom can't retort to Dad's barb in front of the child, because it would destroy the noble lie that Santa is the gift-giver - a lie that both Mom and Dad are invested in preserving.&lt;/p&gt;
    &lt;p&gt;On the other hand, the barb's use of Santa acts as a cloak: The child's ignorance insulates them from understanding the barb. Even if the child is beginning to suspect Santa doesn't exist, they might not be ready to part with a cherished belief, and this dynamic also acts a bit like a shield.&lt;/p&gt;
    &lt;p&gt;There's a bit of a category mistake here, because a single utterance by a father is not a meme. But the example is meant to be an aid to developing an intuition for the general idea.&lt;/p&gt;
    &lt;p&gt;A variety of similar social forces that Straussian Memes hijack to preserve their structure - some of them cloaks, some of them shields: Taboo, shame, desire to avoid harming others, notions of goodwill, desire to preserve social status and/or belonging. When you have a message that utilizes these social forces to reinforce the 'barriers' between the high and low readings, that's a Straussian Meme.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Word Of God&lt;/head&gt;
    &lt;p&gt;Suppose a pastor at a non-denominational church opens his service with announcing that he will read from "the Word of God". It's a charged phrase - perhaps some of the members of the congregation might reflect for a moment on what it means to hear words from the Creator of the Universe. What might they be thinking?&lt;/p&gt;
    &lt;p&gt;Any number of things. I think a believing member of the congregation could take the phrase "the Word of God" to mean:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;As a synonym for "the Bible".&lt;/item&gt;
      &lt;item&gt;As the literal "Words of God" - that God dictated the Bible, word for word.&lt;/item&gt;
      &lt;item&gt;As "a flawless text, inspired by God".&lt;/item&gt;
      &lt;item&gt;As a "providentially perfect text, with human errors" - perfectly sufficient for what God intended for us to have - true in the big picture, but with some level of human error and corruption in the details.&lt;/item&gt;
      &lt;item&gt;And, for the congregation member shading into unbelief, perhaps something like: "A text of great spiritual value which captures an imperfect but progressive revelation of Divinity".&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But which of these meanings did the Pastor intend? A pastor who is keyed into the ambiguity may mean any or all of them, depending on the listener. For other pastors, there may not be deep reflection on the topic. In either case, the ambiguity is a feature, not a bug. Attempting to "fix" the bug would have real social costs: fractious divisions, endless theological arguments, disillusionment, and so on.&lt;/p&gt;
    &lt;p&gt;Refusing to clarify certain doctrinal positions is a kind of glue that helps keep the congregation together. This is in fact a rational response to declining church membership: to keep a congregation at all, you have to have a "big tent". This is precisely the case with the ambiguity in a non-denominational setting on what "Word of God" actually means. So that's the strategic ambiguity piece.&lt;/p&gt;
    &lt;p&gt;But how is the idea of the "Word of God" a self-reinforcing multi-level messaged meme?&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Believers with lower readings feel repelled by higher readings, because they require them to abandon a cherished belief (identity threat). Their cherished beliefs are tied to the esteem they hold to the "Word of God" idea itself.&lt;/item&gt;
      &lt;item&gt;Congregations with higher readings of the phrase avoid clarifying or instructing, because that creates social friction with a "brother or sister in Christ". In fact, this is part of the "Word of God" itself: do not cause unnecessary disagreements with other believers. So the whole 'word of god' package is self-stabilizing, even if you disagree on exactly what the Word of God is.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What Straussian Memes Are Not&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Straussian Memes are not Dog Whistles or Shibboleths: Dog Whistles are messages for an in-group that are designed to be undetectable to the out-group. So it's like a code. On the other hand, Straussian Memes readings are in principle accessible at all levels and about the same thing, even if (as in some cases) there is some kind of reversal in the spirit of the message when moving from low to high.&lt;/item&gt;
      &lt;item&gt;Straussian Memes are more than just Strategic Ambiguity: In corporate settings, leaders often use polysemy so that the exact interpretation is left up to the individual. This idea shares a lot with Straussian Memes - including low and high readings. But unless the message is self-stabilizing, it's not a Straussian Meme. An ambiguous description of product capabilities is probably not Straussian, because there's no social force preventing you from pulling up the specs.&lt;/item&gt;
      &lt;item&gt;Higher/Lower is not necessarily a moral or normative axis: It is an axis over which implications the interpreter is willing to and capable of comprehending. The entire meme may be entirely immoral in its implications, and the structure itself may form the basis of a status game within an immoral coalition. A reader may be able to read the highest meaning and still reject the entire enterprise.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;A Sketch of a Test For Straussian Memes&lt;/head&gt;
    &lt;p&gt;Authorial intention seems less knowable now than before the rise of the meme agar of social media - and even irrelevant where "intent" is just a teleological frame on top of some weird phenomena. So the proof of the presence Straussian Meme has to be "in the pudding", so to speak - does it act like a Straussian meme?&lt;/p&gt;
    &lt;p&gt;But if you're looking hard, there is a risk of "seeing them everywhere" - since positing a Straussian reading is, without sufficient reason, essentially a conspiracy theory.&lt;/p&gt;
    &lt;p&gt;So, here is a testing plan:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;You ask people what it means, you get different but related responses with different levels of sophistication. You can then sort them into higher and lower readings.&lt;/item&gt;
      &lt;item&gt;You relate one of the higher readings to a person who got a lower reading, and they react with incomprehension, disbelief, repellence, or reluctance to engage. This behavior is consistent with upwards self-reinforcing structure by ignorance, epistemic commitment, taboo, or social forces.&lt;/item&gt;
      &lt;item&gt;You relate the lower reading to a person who got a higher reading, and they comprehend the lower reading but imply some kind of social cost or pointlessness (time cost) in clarifying the higher message or pressing others to adopt it - either to their own social status, the social status of their group, the wellbeing of others, the vitality of their movement, or to society as a whole.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More generally (synthesizing 2 and 3), you are looking for signs that attempts to collapse the higher/lower readings structure (or even call out the presence of multiple messages) incur some kind of social or psychological cost - for example: having one's worldview upended (identity threat), being ostracized, or labeled as a trouble-maker. These penalties discourage individual actors from "upgrading" others' readings, which keeps the meme stable while also maintaining a broad base of support.&lt;/p&gt;
    &lt;p&gt;But returning to specifics, if you get all three signals, the meme might be a Straussian Meme. It's then up to you to investigate intent, assign importance and/or relevance (or lack thereof), and so on.&lt;/p&gt;
    &lt;p&gt;As a closing note, I want to tie this back to my original post. I think we are likely to see more of this in the near future, even created by AIs - especially in the form of images (like political cartoons) and short-form content. Putting a name on this trick is a helpful way to inoculate ourselves against it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46458165</guid><pubDate>Thu, 01 Jan 2026 21:24:39 +0000</pubDate></item><item><title>A silly diffuse shading model</title><link>https://lisyarus.github.io/blog/posts/a-silly-diffuse-shading-model.html</link><description>&lt;doc fingerprint="eb188178c3b021f"&gt;
  &lt;main&gt;&lt;p&gt;Happy New Year for those who celebrate!&lt;/p&gt;&lt;p&gt;Happy Arbitrarily Chosen Point in Earth's Orbit for those how don't!&lt;/p&gt;&lt;p&gt;We all know that the proper lighting factor (i.e. proportion of reflected incoming light) for a completely diffuse (aka Lambertian) surface is \(\max(0, L\cdot N)\). Here, \(L\) is the direction to the light source, \(N\) is the surface normal, and \(\cdot\) is the dot product. The dot product essentially comes from pure geometry (the same amount of light in a light beam cross-section being spread across larger surface area), while the \(\max\) is there to make sure that the parts of the surface facing away from light don't receive any light at all.&lt;/p&gt;&lt;p&gt;Is it a great, physically reasonable model (although real materials don't exactly look like that), and serves as a basis for many other shading models (typically via microfacet theory for modern PBR materials). However, sometimes we don't care about such things; sometimes we've just started a new project or a rendering tech demo and we just want to throw some good one-liner shading model. We don't have a texture or even don't intend to, we just want something reasonable to look at.&lt;/p&gt;&lt;p&gt;And here the diffuse model sucks! The \(\max(0, \dots)\) part makes half of the model (the one not facing the light) completely flat-colored (e.g. just black if you don't have ambient light), making you unable to see any geometry.&lt;/p&gt;&lt;p&gt;Btw, all screenshots in this post have light intensity of 1, have gamma-correction, and no other post-processing (i.e. no tone-mapping).&lt;/p&gt;&lt;p&gt;Can we do better?&lt;/p&gt;&lt;p&gt;Now, of course it's not a new problem. In typical scenes this effect isn't seen because we have many different lights (so the black part is much smaller), we have textures (so with some ambient light there is still variation in the color), we have some form of ambient occlusion (which further provides visual clues about the geometry), etc, etc.&lt;/p&gt;&lt;p&gt;All these solutions are, however, not one-liners like the &lt;code&gt;max(0, dot(L, N))&lt;/code&gt; thing, and range in complexity from rather simple (e.g. hard-coding two or three lights in the shader) to academic-level state-of-the-art (like GTAO and similar algorithms). For something like a test project we would definitely prefer a one-liner!&lt;/p&gt;&lt;p&gt;One simple way to fix this is to just re-map the dot product values from \([-1, 1]\) to \([0, 1]\) with a linear function. This results in a formula&lt;/p&gt;\[ 0.5 + 0.5 \cdot (L\cdot N) = \frac{1 + L\cdot N}{2} \]&lt;p&gt;Here's what it looks like:&lt;/p&gt;&lt;p&gt;It's much better, and there are no completely black spots, but the image is much brighter than it should be. It makes sense — after all, we changed the formula! Comparing the plots of intensity as a function of the dot product makes it especially apparent:&lt;/p&gt;&lt;p&gt;It's not exactly physical, but it probably can be put on rigorous grounds if we replace a single directional light source with a suitable environment map, e.g. one that has light in one hemisphere and doesn't have it in the other hemisphere (though I didn't check the math). That's what I've been doing for ages, but maybe we can do even better?&lt;/p&gt;&lt;p&gt;Behold:&lt;/p&gt;\[ \left(0.5 + 0.5 \cdot (L\cdot N)\right)^2 = \left(\frac{1 + L\cdot N}{2}\right)^2 \]&lt;p&gt;Yep, I just squared the formula from the previous section! Here's what it looks like:&lt;/p&gt;&lt;p&gt;It's much closer to the true image in well-lit areas, yet still has nice shading gradients where the surface isn't facing the light. It's not exactly physical either, though we might hand-wavingly say that it approximates subsurface scattering of some sort. And here's the plot:&lt;/p&gt;&lt;p&gt;It's clearly some sort of an approximation of the true formula! How come?&lt;/p&gt;&lt;p&gt;If you look closely at the last plot above, you'll notice that&lt;/p&gt;&lt;p&gt;What if we turn things upside down and instead ask for a function that matches the values and derivatives of \(f(x) = \max(0, x)\) at \(x=\pm 1\)?&lt;/p&gt;&lt;p&gt;Guess what, mathematicians know exactly how to do this! This is called Hermite interpolation, and is solvable with an exact formula! If you specify \(N+1\) values and derivatives at various points, Hermite interpolation gives you the unique polynomial of order \(\leq N\) which has these values and derivatives.&lt;/p&gt;&lt;p&gt;In our case we have \(N=3\) (we have two values + two derivatives, thus \(N+1=4\)), so we expect at most a cubic polynomial. In this special case it turns out that the solution is not cubic, but quadratic, and is exactly \(\left(\frac{1+x}{2}\right)^2\). So this shading model is even optimal in some sense!&lt;/p&gt;&lt;p&gt;Hermite interpolation is also fun from algebraic perspective — it's actually an instance of the Chinese remainder theorem (for polynomial rings), which in modern form is neither Chinese nor about remainders.&lt;/p&gt;&lt;p&gt;I actually developed (well, more like randomly made up) this shading model while working on clouds in my game — they use exactly this formula, and I'm more than happy with how it looks:&lt;/p&gt;&lt;p&gt;That's it, and thanks for reading!&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46458194</guid><pubDate>Thu, 01 Jan 2026 21:26:49 +0000</pubDate></item><item><title>Can Bundler be as fast as uv?</title><link>https://tenderlovemaking.com/2025/12/29/can-bundler-be-as-fast-as-uv/</link><description>&lt;doc fingerprint="7685981e496335d9"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Can Bundler Be as Fast as uv?&lt;/head&gt;Dec 29, 2025 @ 12:26 pm&lt;p&gt;At RailsWorld earlier this year, I got nerd sniped by someone. They asked “why can’t Bundler be as fast as uv?” Immediately my inner voice said “YA, WHY CAN’T IT BE AS FAST AS UV????”&lt;/p&gt;&lt;p&gt;My inner voice likes to shout at me, especially when someone asks a question so obvious I should have thought of it myself. Since then I’ve been thinking about and investigating this problem, going so far as to give a presentation at XO Ruby Portland about Bundler performance. I firmly believe the answer is “Bundler can be as fast as uv” (where “as fast” has a margin of error lol).&lt;/p&gt;&lt;p&gt;Fortunately, Andrew Nesbitt recently wrote a post called “How uv got so fast”, and I thought I would take this opportunity to review some of the highlights of the post and how techniques applied in uv can (or can’t) be applied to Bundler / RubyGems. I’d also like to discuss some of the existing bottlenecks in Bundler and what we can do to fix them.&lt;/p&gt;&lt;p&gt;If you haven’t read Andrew’s post, I highly recommend giving it a read. I’m going to quote some parts of the post and try to reframe them with RubyGems / Bundler in mind.&lt;/p&gt;&lt;head rend="h2"&gt;Rewrite in Rust?&lt;/head&gt;&lt;p&gt;Andrew opens the post talking about rewriting in Rust:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;uv installs packages faster than pip by an order of magnitude. The usual explanation is âitâs written in Rust.â Thatâs true, but it doesnât explain much. Plenty of tools are written in Rust without being notably fast. The interesting question is what design decisions made the difference.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;This is such a good quote. I’m going to address “rewrite in Rust” a bit later in the post. But suffice to say, I think if we eliminate bottlenecks in Bundler such that the only viable option for performance improvements is to “rewrite in Rust”, then I’ll call it a success. I think rewrites give developers the freedom to “think outside the box”, and try techniques they might not have tried. In the case of &lt;code&gt;uv&lt;/code&gt;, I think it gave the developers a good way to say “if we don’t have to worry about backwards compatibility, what could we achieve?”.&lt;/p&gt;&lt;p&gt;I suspect it would be possible to write a uv in Python (PyUv?) that approaches the speeds of uv, and in fact much of the blog post goes on to talk about performance improvements that aren’t related to Rust.&lt;/p&gt;&lt;head rend="h2"&gt;Installing code without eval’ing&lt;/head&gt;&lt;quote&gt;&lt;p&gt;pipâs slowness isnât a failure of implementation. For years, Python packaging required executing code to find out what a package needed.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I didn’t know this about Python packages, and it doesn’t really apply to Ruby Gems so I’m mostly going to skip this section.&lt;/p&gt;&lt;p&gt;Ruby Gems are tar files, and one of the files in the tar file is a YAML representation of the GemSpec. This YAML file declares all dependencies for the Gem, so RubyGems can know, without evaling anything, what dependencies it needs to install before it can install any particular Gem. Additionally, RubyGems.org provides an API for asking about dependency information, which is actually the normal way of getting dependency info (again, no &lt;code&gt;eval&lt;/code&gt; required).&lt;/p&gt;&lt;p&gt;There’s only one other thing from this section I’d like to quote:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;PEP 658 (2022) put package metadata directly in the Simple Repository API, so resolvers could fetch dependency information without downloading wheels at all.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Fortunately RubyGems.org already provides the same information about gems.&lt;/p&gt;&lt;p&gt;Reading through the number of PEPs required as well as the amount of time it took to get the standards in place was very eye opening for me. I can’t help but applaud folks in the Python community for doing this. It seems like a mountain of work, and they should really be proud of themselves.&lt;/p&gt;&lt;head rend="h2"&gt;What uv drops&lt;/head&gt;&lt;p&gt;I’m mostly going to skip this section except for one point:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Ignoring requires-python upper bounds. When a package says it requires python&amp;lt;4.0, uv ignores the upper bound and only checks the lower. This reduces resolver backtracking dramatically since upper bounds are almost always wrong. Packages declare python&amp;lt;4.0 because they havenât tested on Python 4, not because theyâll actually break. The constraint is defensive, not predictive.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I think this is very very interesting. I don’t know how much time Bundler spends on doing “required Ruby version” bounds checking, but it feels like if uv can do it, so can we.&lt;/p&gt;&lt;head rend="h2"&gt;Optimizations that donât need Rust&lt;/head&gt;&lt;p&gt;I really love that Andrew pointed out optimizations that could be made that don’t involve Rust. There are three points in this section that I want to pull out:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Parallel downloads. pip downloads packages one at a time. uv downloads many at once. Any language can do this.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;This is absolutely true, and is a place where Bundler could improve. Bundler currently has a problem when it comes to parallel downloads, and needs a small architectural change as a fix.&lt;/p&gt;&lt;p&gt;The first problem is that Bundler tightly couples installing a gem with downloading the gem. You can read the installation code here, but I’ll summarize the method in question below:&lt;/p&gt;&lt;code&gt;def install
  path = fetch_gem_if_not_cached
  Bundler::RubyGemsGemInstaller.install path, dest
end
&lt;/code&gt;&lt;p&gt;The problem with this method is that it inextricably links downloading the gem with installing it. This is a problem because we could be downloading gems while installing other gems, but we’re forced to wait because the installation method couples the two operations. Downloading gems can trivially be done in parallel since the &lt;code&gt;.gem&lt;/code&gt; files are just archives that can be fetched independently.&lt;/p&gt;&lt;p&gt;The second problem is the queuing system in the installation code. After gem resolution is complete, and Bundler knows what gems need to be installed, it queues them up for installation. You can find the queueing code here. The code takes some effort to understand. Basically it allows gems to be installed in parallel, but only gems that have already had their dependencies installed.&lt;/p&gt;&lt;p&gt;So for example, if you have a dependency tree like “gem &lt;code&gt;a&lt;/code&gt; depends on gem &lt;code&gt;b&lt;/code&gt; which depends on gem &lt;code&gt;c&lt;/code&gt;” (&lt;code&gt;a -&amp;gt; b -&amp;gt; c&lt;/code&gt;), then no gems will be installed (or downloaded) in parallel.&lt;/p&gt;&lt;p&gt;To demonstrate this problem in an easy-to-understand way, I built a slow Gem server. It generates a dependency tree of &lt;code&gt;a -&amp;gt; b -&amp;gt; c&lt;/code&gt; (&lt;code&gt;a&lt;/code&gt; depends on &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt; depends on &lt;code&gt;c&lt;/code&gt;), then starts a Gem server.
The Gem server takes 3 seconds to return any Gem, so if we point Bundler at this Gem server and then profile Bundler, we can see the impact of the queueing system and download scheme.&lt;/p&gt;&lt;p&gt;In my test app, I have the following Gemfile:&lt;/p&gt;&lt;code&gt;source "http://localhost:9292"

gem "a"
&lt;/code&gt;&lt;p&gt;If we profile Bundle install with Vernier, we can see the following swim lanes in the marker chart:&lt;/p&gt;&lt;p&gt;The above chart is showing that we get no parallelism during installation. We spend 3 seconds downloading the &lt;code&gt;c&lt;/code&gt; gem, then we install it.
Then we spend 3 seconds downloading the &lt;code&gt;b&lt;/code&gt; gem, then we install it.
Finally we spend 3 seconds downloading the &lt;code&gt;a&lt;/code&gt; gem, and we install it.&lt;/p&gt;&lt;p&gt;Timing the &lt;code&gt;bundle install&lt;/code&gt; process shows we take over 9 seconds to install (3 seconds per gem):&lt;/p&gt;&lt;code&gt;&amp;gt; rm -rf x; rm -f Gemfile.lock; time GEM_PATH=(pwd)/x GEM_HOME=(pwd)/x bundle install
Fetching gem metadata from http://localhost:9292/...
Resolving dependencies...
Fetching c 1.0.0
Installing c 1.0.0
Fetching b 1.0.0
Installing b 1.0.0
Fetching a 1.0.0
Installing a 1.0.0
Bundle complete! 1 Gemfile dependency, 3 gems now installed.
Use `bundle info [gemname]` to see where a bundled gem is installed.

________________________________________________________
Executed in   11.80 secs      fish           external
   usr time  341.62 millis  231.00 micros  341.38 millis
   sys time  223.20 millis  712.00 micros  222.49 millis
&lt;/code&gt;&lt;p&gt;Contrast this with a Gemfile containing &lt;code&gt;d&lt;/code&gt;, &lt;code&gt;e&lt;/code&gt;, and &lt;code&gt;f&lt;/code&gt;, which have no dependencies, but still take 3 seconds to download:&lt;/p&gt;&lt;code&gt;source "http://localhost:9292"

gem "d"
gem "e"
gem "f"
&lt;/code&gt;&lt;p&gt;Timing &lt;code&gt;bundle install&lt;/code&gt; for the above Gemfile shows it takes about 4 seconds:&lt;/p&gt;&lt;code&gt;&amp;gt; rm -rf x; rm -f Gemfile.lock; time GEM_PATH=(pwd)/x GEM_HOME=(pwd)/x bundle install
Fetching gem metadata from http://localhost:9292/.
Resolving dependencies...
Fetching d 1.0.0
Fetching e 1.0.0
Fetching f 1.0.0
Installing e 1.0.0
Installing f 1.0.0
Installing d 1.0.0
Bundle complete! 3 Gemfile dependencies, 3 gems now installed.
Use `bundle info [gemname]` to see where a bundled gem is installed.

________________________________________________________
Executed in    4.14 secs      fish           external
   usr time  374.04 millis    0.38 millis  373.66 millis
   sys time  368.90 millis    1.09 millis  367.81 millis
&lt;/code&gt;&lt;p&gt;We were able to install the same number of gems in a fraction of the time. This is because Bundler is able to download siblings in the dependency tree in parallel, but unable to handle other relationships.&lt;/p&gt;&lt;p&gt;There is actually a good reason that Bundler insists dependencies are installed before the gems themselves: native extensions. When installing native extensions, the installation process must run Ruby code (the &lt;code&gt;extconf.rb&lt;/code&gt; file).
Since the &lt;code&gt;extconf.rb&lt;/code&gt; could require dependencies be installed in order to run, we must install dependencies first.
For example &lt;code&gt;nokogiri&lt;/code&gt; depends on &lt;code&gt;mini_portile2&lt;/code&gt;, but &lt;code&gt;mini_portile2&lt;/code&gt; is only used during the installation process, so it needs to be installed before &lt;code&gt;nokogiri&lt;/code&gt; can be compiled and installed.&lt;/p&gt;&lt;p&gt;However, if we were to decouple downloading from installation it would be possible for us to maintain the “dependencies are installed first” business requirement but speed up installation. In the &lt;code&gt;a -&amp;gt; b -&amp;gt; c&lt;/code&gt; case, we could have been downloading gems &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; at the same time as gem &lt;code&gt;c&lt;/code&gt; (or even while waiting on &lt;code&gt;c&lt;/code&gt; to be installed).&lt;/p&gt;&lt;p&gt;Additionally, pure Ruby gems don’t need to execute any code on installation. If we knew that we were installing a pure Ruby gem, it would be possible to relax the “dependencies are installed first” business requirement and get even more performance increases. The above &lt;code&gt;a -&amp;gt; b -&amp;gt; c&lt;/code&gt; case could install all three gems in parallel since none of them execute Ruby code during installation.&lt;/p&gt;&lt;p&gt;I would propose we split installation in to 4 discrete steps:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Download the gem&lt;/item&gt;&lt;item&gt;Unpack the gem&lt;/item&gt;&lt;item&gt;Compile the gem&lt;/item&gt;&lt;item&gt;Install the gem&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Downloading and unpacking can be done trivially in parallel. We should unpack the gem to a temporary folder so that if the process crashes or the machine loses power, the user isn’t stuck with a half-installed gem. After we unpack the gem, we can discover whether the gem is a native extension or not. If it’s not a native extension, we “install” the gem simply by moving the temporary folder to the “correct” location. This step could even be a “hard link” step as discussed in the next point.&lt;/p&gt;&lt;p&gt;If we discover that the gem is a native extension, then we can “pause” installation of that gem until its dependencies are installed, then resume (by compiling) at an appropriate time.&lt;/p&gt;&lt;p&gt;Side note: &lt;code&gt;gel&lt;/code&gt;, a Bundler alternative, works mostly in this manner today.
Here is a timing of the &lt;code&gt;a -&amp;gt; b -&amp;gt; c&lt;/code&gt; case from above:&lt;/p&gt;&lt;code&gt;&amp;gt; rm -f Gemfile.lock; time gel install
Fetching sources....
Resolving dependencies...
Writing lockfile to /Users/aaron/git/gemserver/app/Gemfile.lock
Installing c (1.0.0) 
Installing a (1.0.0)
Installing b (1.0.0)
Installed 3 gems  

________________________________________________________
Executed in    4.07 secs      fish           external
   usr time  289.22 millis    0.32 millis  288.91 millis
   sys time  347.04 millis    1.36 millis  345.68 millis
&lt;/code&gt;&lt;p&gt;Lets move on to the next point:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Global cache with hardlinks. pip copies packages into each virtual environment. uv keeps one copy globally and uses hardlinks&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I think this is a great idea, but I’d actually like to split the idea in two. First, RubyGems and Bundler should have a combined, global cache, full stop. I think that global cache should be in &lt;code&gt;$XDG_CACHE_HOME&lt;/code&gt;, and we should store &lt;code&gt;.gem&lt;/code&gt; files there when they are downloaded.&lt;/p&gt;&lt;p&gt;Currently, both Bundler and RubyGems will use a Ruby version specific cache folder. In other words, if you do &lt;code&gt;gem install rails&lt;/code&gt; on two different versions of Ruby, you get two copies of Rails and all its dependencies.&lt;/p&gt;&lt;p&gt;Interestingly, there is an open ticket to implement this, it just needs to be done.&lt;/p&gt;&lt;p&gt;The second point is hardlinking on installation. The idea here is that rather than unpacking the gem multiple times, once per Ruby version, we simply unpack once and then hard link per Ruby version. I like this idea, but I think it should be implemented after some technical debt is paid: namely implementing a global cache and unifying Bundler / RubyGems code paths.&lt;/p&gt;&lt;p&gt;On to the next point:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;PubGrub resolver&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Actually Bundler already uses a Ruby implementation of the PubGrub resolver. You can see it here. Unfortunately, RubyGems still uses the molinillo resolver.&lt;/p&gt;&lt;p&gt;In other words you use a different resolver depending on whether you do &lt;code&gt;gem install&lt;/code&gt; or &lt;code&gt;bundle install&lt;/code&gt;.
I don’t really think this is a big deal since the vast majority of users will be doing &lt;code&gt;bundle install&lt;/code&gt; most of time.
However, I do think this discrepancy is some technical debt that should be addressed, and I think this should be addressed via unification of RubyGems and Bundler codebases (today they both live in the same repository, but the code isn’t necessarily combined).&lt;/p&gt;&lt;p&gt;Lets move on to the next section of Andrew’s post:&lt;/p&gt;&lt;head rend="h2"&gt;Where Rust actually matters&lt;/head&gt;&lt;p&gt;Andrew first mentions “Zero-copy deserialization”. This is of course an important technique, but I’m not 100% sure where we would utilize it in RubyGems / Bundler. I think that today we parse the YAML spec on installation, and that could be a target. But I also think we could install most gems without looking at the YAML gemspec at all.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Thread-level parallelism. Pythonâs GIL forces parallel work into separate processes, with IPC overhead and data copying.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;This is an interesting point. I’m not sure what work pip needed to do in separate processes. Installing a pure Ruby, Ruby Gem is mostly an IO bound task, with some ZLIB mixed in. Both of these things (IO and ZLIB processing) release Ruby’s GVL, so it’s possible for us to do things truly in parallel. I imagine this is similar for Python / pip, but I really have no idea.&lt;/p&gt;&lt;p&gt;Given the stated challenges with Python’s GIL, you might wonder whether Ruby’s GVL presents similar parallelism problems for Bundler. I don’t think so, and in fact I think Ruby’s GVL gets kind of a bad rap. It prevents us from running CPU bound Ruby code in parallel. Ractors address this, and Bundler could possibly leverage them in the future, but since installing Gems is mostly an IO bound task I’m not sure what the advantage would be (possibly the version solver, but I’m not sure what can be parallelized in there). The GVL does allow us to run IO bound work in parallel with CPU bound Ruby code. CPU bound native extensions are allowed to release the GVL, allowing Ruby code to run in parallel with the native extension’s CPU bound code.&lt;/p&gt;&lt;p&gt;In other words, Ruby’s GVL allows us to safely run work in parallel. That said, the GVL can work against us because releasing and acquiring the GVL takes time.&lt;/p&gt;&lt;p&gt;If you have a system call that is very fast, releasing and acquiring the GVL could end up being a large percentage of that call. For example, if you do &lt;code&gt;File.binwrite(file, buffer)&lt;/code&gt;, and the buffer is very small, you could encounter a situation where GVL book keeping is the majority of the time.
A bummer is that Ruby Gem packages usually contain lots of very small files, so this problem could be impacting us.
The good news is that this problem can be solved in Ruby itself, and indeed some work is being done on it today.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;No interpreter startup. Every time pip spawns a subprocess, it pays Pythonâs startup cost.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Obviously Ruby has this same problem. That said, we only start Ruby subprocesses when installing native extensions. I think native extensions make up the minority of gems installed, and even when installing a native extension, it isn’t Ruby startup that is the bottleneck. Usually the bottleneck is compilation / linking time (as we’ll see in the next post).&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Compact version representation. uv packs versions into u64 integers where possible, making comparison and hashing fast.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;This is a cool optimization, but I don’t think it’s actually Rust specific. Comparing integers is much faster than comparing version objects. The idea is that you take a version number, say &lt;code&gt;1.0.0&lt;/code&gt;, and then pack each part of the version in to a single integer.
For example, we could represent &lt;code&gt;1.0.0&lt;/code&gt; as &lt;code&gt;0x0001_0000_0000_0000&lt;/code&gt; and &lt;code&gt;1.1.0&lt;/code&gt; as &lt;code&gt;0x0001_0001_0000_0000&lt;/code&gt;, etc.&lt;/p&gt;&lt;p&gt;It should be possible to use this trick in Ruby and encode versions to integer immediates, which would unlock performance in the resolver. Rust has an advantage here - compiled native code comparing u64s will always be faster than Ruby, even with immediates. However, I would bet that with the YJIT or ZJIT in play, this gap could be closed enough that no end user would notice the difference between a Rust or Ruby implementation of Bundler.&lt;/p&gt;&lt;p&gt;I started refactoring the &lt;code&gt;Gem::Version&lt;/code&gt; object so that we might start doing this, but we ended up reverting it because of backwards compatibility (I am jealous of &lt;code&gt;uv&lt;/code&gt; in that regard).
I think the right way to do this is to refactor the solver entry point and ensure all version requirements are encoded as integer immediates before entering the solver.
We could keep the &lt;code&gt;Gem::Version&lt;/code&gt; API as “user facing” and design a more internal API that the solver uses.
I am very interested in reading the version encoding scheme in uv.
My intuition is that minor numbers tend to get larger than major numbers, so would minor numbers have more dedicated bits?
Would it even matter with 64 bits?&lt;/p&gt;&lt;head rend="h2"&gt;Wrapping this up&lt;/head&gt;&lt;p&gt;I’m going to quote Andrew’s last 2 paragraphs:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;uv is fast because of what it doesnât do, not because of what language itâs written in. The standards work of PEP 518, 517, 621, and 658 made fast package management possible. Dropping eggs, pip.conf, and permissive parsing made it achievable. Rust makes it a bit faster still.&lt;/p&gt;&lt;p&gt;pip could implement parallel downloads, global caching, and metadata-only resolution tomorrow. It doesnât, largely because backwards compatibility with fifteen years of edge cases takes precedence. But it means pip will always be slower than a tool that starts fresh with modern assumptions.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I think these are very good points. The difference is that in RubyGems and Bundler, we already have the infrastructure in place for writing a “fast as uv” package manager. The difficult part is dealing with backwards compatibility, and navigating two legacy codebases. I think this is the real advantage the uv developers had. That said, I am very optimistic that we could “repair the plane mid-flight” so to speak, and have the best of both worlds: backwards compatibility and speed.&lt;/p&gt;&lt;p&gt;I mentioned at the top of the post I would address “rewrite it in Rust”, and I think Andrew’s own quote mostly does that for me. I think we could have 99% of the performance improvements while still maintaining a Ruby codebase. Of course if we rewrote it in Rust, you could squeeze an extra 1% out, but would it be worthwhile? I don’t think so.&lt;/p&gt;&lt;p&gt;I have a lot more to say about this topic, and I feel like this post is getting kind of long, so I’m going to end it here. Please look out for part 2, which I’m tentatively calling “What makes Bundler / RubyGems slow?” This post was very “can we make RubyGems / Bundler do what uv does?” (the answer is “yes”). In part 2 I want to get more hands-on by discussing how to profile Bundler and RubyGems, what specifically makes them slow in the real world, and what we can do about it.&lt;/p&gt;&lt;p&gt;I want to end this post by saying “thank you” to Andrew for writing such a great post about how uv got so fast.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46458302</guid><pubDate>Thu, 01 Jan 2026 21:37:10 +0000</pubDate></item><item><title>WebAssembly as a Python Extension Platform</title><link>https://nullprogram.com/blog/2026/01/01/</link><description>&lt;doc fingerprint="8c499a15de87b2c0"&gt;
  &lt;main&gt;
    &lt;p&gt; nullprogram.com/blog/2026/01/01/ &lt;/p&gt;
    &lt;p&gt; (The author is currently open to employment opportunities in the United States.) &lt;/p&gt;
    &lt;p&gt;Software above some complexity level tends to sport an extension language, becoming a kind of software platform itself. Lua fills this role well, and of course there’s JavaScript for web technologies. WebAssembly generalizes this, and any Wasm-targeting programming language can extend a Wasm-hosting application. It has more friction than supplying a script in a text file, but extension authors can write in their language of choice, and use more polished development tools — debugging, testing, etc. — than typically available for a typical extension language. Python is traditionally extended through native code behind a C interface, but it’s recently become practical to extend Python with Wasm. That is we can ship an architecture-independent Wasm blob inside a Python library, and use it without requiring a native toolchain on the host system. Let’s discuss two different use cases and their pitfalls.&lt;/p&gt;
    &lt;p&gt;Normally we’d extend Python in order to access an external interface that Python cannot access on its own. Wasm runs in a sandbox with no access to the outside world whatsoever, so it obviously isn’t useful for that case. Extensions may also grant Python more speed, which is one of Wasm’s main selling points. We can also use Wasm to access embeddable capabilities written in a different programming language which do not require external access.&lt;/p&gt;
    &lt;p&gt;For preferred non-WASI Wasm runtime is Volodymyr Shymanskyy’s wasm3. It’s plain old C and very friendly to embedding in the same was as, say, SQLite. Performance is middling, though a C program running on wasm3 is still quite a bit faster than an equivalent Python program. It has Python bindings, pywasm3, but it’s distributed only in source code form. That is, the host machine must have a C toolchain in order to use pywasm3, which defeats my purposes here. If there’s a C toolchain, I might as well just use that instead of going through Wasm.&lt;/p&gt;
    &lt;p&gt;For the use cases in this article, the best option is wasmtime-py. The distribution includes binaries for Windows, macOS, and Linux on x86-64 and ARM64, which covers nearly all Python installations. Hosts require nothing more than a Python interpreter, no native toolchains. It’s almost as good as having Wasm built into Python itself. In my tests it’s 3x–10x faster than wasm3, so for my first use case the situation is even better. The catch is that it currently weighs ~18MiB (installed), and in the future will likely rival the Python interpreter itself. The API also breaks on a monthly basis, so you’re signing up for the upgrade treadmill lest your own program perishes to bitrot after a couple of years. This article is about version 40.&lt;/p&gt;
    &lt;head rend="h3"&gt;Usage examples and gotchas&lt;/head&gt;
    &lt;p&gt;The official examples don’t do anything non-trivial or interesting, and so to figure things out I had to study the documentation, which does not offer many hints. Basic setup looks like this:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;import functools
import wasmtime

store    = wasmtime.Store()
module   = wasmtime.Module.from_file(store.engine, "example.wasm")
instance = wasmtime.Instance(store, module, ())
exports  = instance.exports(store)

memory = exports["memory"].get_buffer_ptr(store)
func1  = functools.partial(exports["func1"], store)
func2  = functools.partial(exports["func2"], store)
func3  = functools.partial(exports["func3"], store)
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;A store is an allocation region from which we allocate all Wasm objects. It is not possible to free individual objects except to discard the whole store. Quite sensible, honestly. What’s not sensible is how often I have to repeat myself, passing the store back into every object in order to use it. These objects are associated with exactly one store and cannot be used with different stores. Use the wrong store and it panics: It’s already keeping track internally! I do not understand why the interface works this way. So to make things simpler, I use &lt;code&gt;functools.partial&lt;/code&gt; to
bind the &lt;code&gt;store&lt;/code&gt; parameter and so get the interface I expect.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;get_buffer_ptr&lt;/code&gt; object is a buffer protocol object, and if you’re
moving anything other than bytes that’s probably what you want to use to
access memory. The usual caveats apply for this object: If you change the
memory size you probably want to grab a fresh buffer object. For
bytes (e.g. buffers and strings) I prefer the &lt;code&gt;read&lt;/code&gt; and &lt;code&gt;write&lt;/code&gt; methods.&lt;/p&gt;
    &lt;p&gt;Because multi-value is still in an experimental state in the Wasm ecosystem, you will likely not pass structs with Wasm. Anything more complicated than scalars will require pointers and copying data in and out of Wasm linear memory. This involves the usual trap that catches nearly everyone: Wasm interfaces make no distinction between pointers and integers, and Wasm runtimes interpret generally interpret all integers as signed. What that means is your pointers are signed unless you take action. Addresses start at 0, so this is bad, bad news.&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;malloc = functools.partial(exports["func1"], store)

hello = b"hello"
pointer = malloc(len(hello))
assert pointer
memory = exports["memory"].write(store, hello, pointer)  # WRONG!
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;To make matters worse, wasmtime-py adds its own footgun: The &lt;code&gt;read&lt;/code&gt; and
&lt;code&gt;write&lt;/code&gt; methods adopt the questionable Python convention of negative
indices acting from the end. If &lt;code&gt;malloc&lt;/code&gt; returns a pointer in the upper
half of memory, the negative pointer will pass the bounds check inside
&lt;code&gt;write&lt;/code&gt; because negative is valid, then quietly store to the wrong
address! Doh!&lt;/p&gt;
    &lt;p&gt;I wondered how common this error, so I searched online. I could find only one non-trivial wasmtime-py use in the wild, in a sandboxed PDF reader. It falls into the negative pointer trap as I expected. Not only that, it’s a buffer overflow into Python’s memory space:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;            buf_ptr = malloc(store, len(pdf_data))
            mem_data = memory.data_ptr(store)

            for i, byte in enumerate(pdf_data):
                mem_data[buf_ptr + i] = byte
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;The &lt;code&gt;data_ptr&lt;/code&gt; method returns a non-bounds-checked raw &lt;code&gt;ctypes&lt;/code&gt; pointer,
so this is actually a double mistake. First, it shouldn’t trust pointers
coming out of Wasm if it cares at all about sandboxing. The second is the
potential negative pointer, which in this case would write outside of the
Wasm memory and in Python’s memory, hopefully seg-faulting.&lt;/p&gt;
    &lt;p&gt;What’s one to do? Every pointer coming out of Wasm must be truncated with a mask:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;pointer = malloc(...) &amp;amp; 0xffffffff   # correct for wasm32!
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;This interprets the result as unsigned. 64-bit Wasm needs a 64-bit mask, though in practice you will never get a valid negative pointer from 64-bit Wasm. This rule applies to JavaScript as well, where the idiom is:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;let pointer = malloc(...) &amp;gt;&amp;gt;&amp;gt; 0
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Wasm runtimes cannot help — they lack the necessary information — and this is perhaps a fundamental flaw in Wasm’s design. Once you know about it you see this mistake happening everywhere.&lt;/p&gt;
    &lt;p&gt;Now that you have a proper address, you can apply it to a buffer protocol view of memory. If you’re using NumPy there are various ways to interact with this memory by wrapping it in NumPy types, though only if you’re on a little endian host. (If you’re on a big endian machine, just give up on running Wasm anyway.) The first use case I have in mind typically involves copying plain Python values in and out. The &lt;code&gt;struct&lt;/code&gt; package is
quite handy here:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;vec2   = malloc(...) &amp;amp; 0xffffffff
memory = exports["memory"].get_buffer_ptr(store)
struct.pack_into("&amp;lt;ii", memory, vec2, x, y)
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;It fills a similar role to JavaScript &lt;code&gt;DataView&lt;/code&gt;. If you’re copying
lots of numbers, with CPython it’s faster to construct a custom format
string rather than use a loop:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;nums: list[int] = ...
struct.pack_into(f"&amp;lt;{len(nums)}i", memory, buf, *nums)
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;To copy structures back out, use &lt;code&gt;struct.unpack_from&lt;/code&gt;. If you’re moving
strings, you’ll need to &lt;code&gt;.encode()&lt;/code&gt; and &lt;code&gt;.decode()&lt;/code&gt; to convert to and from
&lt;code&gt;bytes&lt;/code&gt;, which are well-suited to &lt;code&gt;read&lt;/code&gt; and &lt;code&gt;write&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;In practice with real Wasm programs you’re going to be interacting with the “guest” allocator from the outside, to request memory into which you copy inputs for a function. In my examples I’ve used &lt;code&gt;malloc&lt;/code&gt; because it
requires no elaboration, but as usual a bump allocator solves
this so much better, especially because it doesn’t require stuffing a
whole general purpose allocator inside the Wasm program. Have one global
arena — no other threads will sharing that Wasm instance — rapid fire a
bunch of allocations as needed without any concern for memory management
in the “host”, call the function, which might allocate a result from that
arena, then reset the arena to clean up. In essence a stack for passing
values in and out.&lt;/p&gt;
    &lt;head rend="h3"&gt;WebAssembly as faster Python&lt;/head&gt;
    &lt;p&gt;Suppose we noticed a computational hot spot in our Python program in a pure Python function (e.g. not calling out to an extension). Optimizing this function would be wise. Based on my experiments if I re-implement that function in C, compile it to Wasm, then run that bit of Wasm in place of the original function, I can expect around a 10x speed-up. In general C is more like 100x faster than Python, and the overhead of interfacing with Wasm — copying stuff in and out, etc. — can be high, but not so high as to not be profitable. This improves further if I can change the interface, e.g. require callers to use the buffer protocol.&lt;/p&gt;
    &lt;p&gt;Thanks to wasmtime-py, I could introduce this change without fussing with cross-compilers to build distribution binaries, nor require a toolchain on the target, just a hefty Python package. Might be worth it.&lt;/p&gt;
    &lt;p&gt;My main experimental benchmark is a variation on my solution to the “Two Sum” problem, which I originally wrote for JavaScript, then extended to pywasm3 and later wasmtime-py. It’s simple, just interesting enough, and representative of the sort of Wasm drop-in I have in mind. It has the same interface, but implements it with Wasm.&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;# Original Pythonic interface
def twosum(nums: list[int], target: int) -&amp;gt; tuple[int, int] | None:
    ...

# Stateful Wasm interface
class TwoSumWasm():
    def __init__(self):
        store    = wasmtime.Store()
        module   = wasmtime.Module.from_file(store.engine, ...)
        instance = wasmtime.Instance(store, module, ())
        ...

    def twosum(self, nums, target):
        # ... use wasm instance ...
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;There’s some state to it with the Wasm instance in tow. If you hide that by making it global you’ll need to synchronize your threads around it. In a multi-threaded program perhaps these would be lazily-constructed thread locals. I haven’t had to solve this yet.&lt;/p&gt;
    &lt;p&gt;However, the weakness of the wasmtime “store” really shows: Notice how compilation and instantiation are bound together in one store? I cannot compile once and then create disposable instances on the fly, e.g. as required for each run of a WASI program. Every instance permanently extends the compilation store. In practice we must wastefully re-compile the Wasm program for each disposable instance. Despite appearances, compilation and instantiation are not actually distinct steps, as they are in JavaScript’s Wasm API. &lt;code&gt;wasmtime.Instance&lt;/code&gt; accepts a store as its first
argument, suggesting use of a different store for instantiation. That
would solve this problem, but as of this writing it must be the same
store used to compile the module. This is a fatal flaw for certain real
use cases, particularly WASI.&lt;/p&gt;
    &lt;head rend="h3"&gt;WebAssembly as embedded capabilities&lt;/head&gt;
    &lt;p&gt;Loup Vaillant’s Monocypher is a wonderful cryptography library. Lean, efficient, and embedding-friendly, so much so it’s distributed in amalgamated form. It requires no libc or runtime, so we can compile it straight to Wasm with almost any Clang toolchain:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ clang --target=wasm32 -nostdlib -O2 -Wl,--no-entry -Wl,--export-all
        -o monocypher.wasm monocypher.c
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;It’s not “Wasm-aware” so I need &lt;code&gt;--export-all&lt;/code&gt; to expose the interface.
This is swell because, as single translation unit, anything with external
linkage is the interface. Though remember what I said about interacting
with the guest allocator? This has no allocator, nor should it. It’s not
so usable in this form because we’d need to manage memory from the
outside. Do-able, but it’s easy to improve by adding a couple more
functions, sticking to a single translation unit:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;#include "monocypher.c"

extern char  __heap_base[];
static char *heap_used;
static char *heap_high;

void *bump_alloc(ptrdiff_t size)
{
    // ...
}

void bump_reset()
{
    ptrdiff_t len = heap_used - __heap_base;
    __builtin_memset(__heap_base, 0, len);  // wipe keys, etc.
    heap_used = __heap_base;
}
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;I’ve discussed &lt;code&gt;__heap_base&lt;/code&gt; before, which is part of the ABI.
We’ll push keys, inputs, etc. onto this “stack”, run our cryptography
routine, copy out the result, then reset the bump allocator, which wipes
out all sensitive data. Often &lt;code&gt;memset&lt;/code&gt; is insufficient — typically it’s
zero-then-free, and compilers see the lifetime about to end — but no
lifetime ends here, and stores to this “heap” memory externally observable
as far as the abstract machine can tell. (Otherwise we couldn’t reliably
copy out our results!)&lt;/p&gt;
    &lt;p&gt;There’s a lot to this API, but I’m only going to look at the AEAD interface. We “lock” up some data in an encrypted box, write any unencrypted label we’d like on the outside. Then later we can unlock the box, which will only open for us if neither the contents of the box nor the label were tampered with. That’s some solid API design:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;void crypto_aead_lock(uint8_t       *cipher_text,
                      uint8_t        mac  [16],
                      const uint8_t  key  [32],
                      const uint8_t  nonce[24],
                      const uint8_t *ad,         size_t ad_size,
                      const uint8_t *plain_text, size_t text_size);
int crypto_aead_unlock(uint8_t       *plain_text,
                       const uint8_t  mac  [16],
                       const uint8_t  key  [32],
                       const uint8_t  nonce[24],
                       const uint8_t *ad,          size_t ad_size,
                       const uint8_t *cipher_text, size_t text_size);
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;By compiling to Wasm we can access this functionality from Python almost like it was pure Python, and interact with other systems using Monocypher.&lt;/p&gt;
    &lt;p&gt;Since Monocypher does not interact with the outside world on its own, it relies on callers to use their system’s CSPRNG to create those nonces and keys, which we’ll do using the &lt;code&gt;secrets&lt;/code&gt; built-in package:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;class Monocypher:
    def __init__(self):
        ...
        self._read   = functools.partial(memory.read, store)
        self._write  = functools.partial(memory.write, store)
        self.__alloc = functools.partial(exports["bump_alloc"], store)
        self._reset  = functools.partial(exports["bump_reset"], store)
        self._lock   = functools.partial(exports["crypto_aead_lock"], store)
        self._unlock = functools.partial(exports["crypto_aead_unlock"], store)
        self._csprng = secrets.SystemRandom()

    def _alloc(self, n):
        return self.__alloc(n) &amp;amp; 0xffffffff

    def generate_key(self):
        return self._csprng.randbytes(32)

    def generate_nonce(self):
        return self._csprng.randbytes(24)

    ...
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;With a solid foundation, all that follows comes easily. A &lt;code&gt;finally&lt;/code&gt;
guarantees secrets are always removed from Wasm memory, and the rest is
just about copying bytes around:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;    def aead_lock(self, text, key, ad = b""):
        assert len(key) == 32
        try:
            macptr   = self._alloc(16)
            keyptr   = self._alloc(32)
            nonceptr = self._alloc(24)
            adptr    = self._alloc(len(ad))
            textptr  = self._alloc(len(text))

            self._write(key, keyptr)
            nonce = self.generate_nonce()
            self._write(nonce, nonceptr)
            self._write(ad,    adptr)
            self._write(text,  textptr)

            self._lock(
                textptr,
                macptr,
                keyptr,
                nonceptr,
                adptr, len(ad),
                textptr, len(text),
            )
            return (
                self._read(macptr, macptr+16),
                nonce,
                self._read(textptr, textptr+len(text)),
            )
        finally:
            self._reset()
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;And &lt;code&gt;aead_unlock&lt;/code&gt; is basically the same in reverse, but throws if the box
fails to unlock, perhaps due to tampering:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;    def aead_unlock(self, text, mac, key, nonce, ad = b""):
        assert len(mac) == 16
        assert len(key) == 32
        assert len(nonce) == 24
        try:
            macptr   = self._alloc(16)
            keyptr   = self._alloc(32)
            nonceptr = self._alloc(24)
            adptr    = self._alloc(len(ad))
            textptr  = self._alloc(len(text))

            self._write(mac, macptr)
            self._write(key, keyptr)
            self._write(nonce, nonceptr)
            self._write(ad, adptr)
            self._write(text, textptr)

            if self._unlock(
                textptr,
                macptr,
                keyptr,
                nonceptr,
                adptr, len(ad),
                textptr, len(text),
            ):
                raise ValueError("AEAD mismatch")
            return self._read(textptr, textptr+len(text))
        finally:
            self._reset()
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Usage:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;mc = Monocypher()
key = mc.generate_key()
message = "Hello, world!"
mac, nonce, encrypted = mc.aead_lock(message.encode(), key)
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Transmit &lt;code&gt;mac&lt;/code&gt;, &lt;code&gt;nonce&lt;/code&gt;, and &lt;code&gt;encrypted&lt;/code&gt; to the other party (or your
future self), who already has the &lt;code&gt;key&lt;/code&gt;:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;decrypted = mc.aead_unlock(encrypted, mac, key, nonce)
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Find the complete source in my scratch repository.&lt;/p&gt;
    &lt;p&gt;While I have a few reservations about wasmtime-py, it fascinates me how well this all works. It’s been my hammer in search of a nail for some time now.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46458624</guid><pubDate>Thu, 01 Jan 2026 22:09:16 +0000</pubDate></item></channel></rss>