<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 25 Jan 2026 21:37:57 +0000</lastBuildDate><item><title>Introduction to PostgreSQL Indexes</title><link>https://dlt.github.io/blog/posts/introduction-to-postgresql-indexes/</link><description>&lt;doc fingerprint="791c02166dd40ba4"&gt;
  &lt;main&gt;
    &lt;p&gt;20 minutes&lt;/p&gt;
    &lt;head rend="h1"&gt;Introduction to PostgreSQL Indexes&lt;/head&gt;
    &lt;head rend="h2"&gt;Who’s this for&lt;/head&gt;
    &lt;p&gt;This text is for developers that have an intuitive knowledge of what database indexes are, but don’t necessarily know how they work internaly, what are the tradeoffs associated with indexes, what are the types of indexes provided by postgres and how you can use some of its more advanced options to make them more optimized for your use case.&lt;/p&gt;
    &lt;head rend="h2"&gt;Basics&lt;/head&gt;
    &lt;p&gt;Indexes are special database objects primarily designed to increase the speed of data access, by allowing the database to read less data from the disk. They can also be used to enforce constraints like primary keys, unique keys and exclusion. Indexes are important for performance but do not speedup a query unless the query matches the columns and data types in the index. Also, as a very rough rule of thumb, an index will only help if less than 15-20% of the table will be returned in the query, otherwise the query planner, a part of postgres used to determine how the query is going to be executed, might prefer a sequential scan. In fact, reality is much more complex than this rule of thumb. The query planner uses statistics and predefined costs associated with each type of scan to do its job, but we’re only going approach the query planner behavior tangentially in this article. So, if your query returns a large percentage of the table, consider refactoring it, using summary tables or other techniques before throwing an index at the problem. With that in mind, let’s give a closer look at how Postgres stores your data in the disk and how indexes help to speedup querying this data.&lt;/p&gt;
    &lt;p&gt;There are six types of indexes available in the default postgres installation and more types available through extensions. Typically, they work by associating a key value with a data location in one or more rows of the table containing that key. Each line is identified by a TID, or tuple id.&lt;/p&gt;
    &lt;head rend="h3"&gt;How data is stored in disk&lt;/head&gt;
    &lt;p&gt;To understand indexes, it is important to first understand how postgres stores table data on disk. Every table in postgres has one or more corresponding files on disk, depending on its size. This set of files is called a heap and it is divided into 8kb pagesh. All table rows, internally referred to as “tuples”, are saved in these files and do not have a specific order. The index is a tree structure that links the indexes columns to the row locators, also known as ctid, in the heap. We’ll zoom into the index internals later.&lt;/p&gt;
    &lt;p&gt;To see the heap files we can use a few postgres internal tables to see where they’re located in the disk. First, we can enter psql and use &lt;code&gt;show data_directory&lt;/code&gt; to show the directory Postgres uses to store databases physical files.&lt;/p&gt;
    &lt;code&gt; show data_directory;

         data_directory          
---------------------------------
 /opt/homebrew/var/postgresql@16&lt;/code&gt;
    &lt;p&gt;Now we can use the internal &lt;code&gt;pg_class&lt;/code&gt; to find the file where the heap table is stored:&lt;/p&gt;
    &lt;code&gt;create table foo (id int, name text);


select oid, datname
from pg_database
where datname = 'my_database';                                                                                

  oid  |         datname        
-------+-------------------------
 71122 | my_database
(1 row)&lt;/code&gt;
    &lt;code&gt;select relfilenode from pg_class where relname = 'foo';                                                                                                  
 relfilenode
-------------
       71123&lt;/code&gt;
    &lt;p&gt;Finally, we can check the file on disk by running this command in the shell (ls $PGDATA/base/&amp;lt;database_oid&amp;gt;/&amp;lt;table_oid&amp;gt;):&lt;/p&gt;
    &lt;code&gt;ls -lrt /opt/homebrew/var/postgresql@16/base/71122/71123
-rw-------  1 dlt  admin  0 16 Aug 14:20 /opt/homebrew/var/postgresql@16/base/71122/71123&lt;/code&gt;
    &lt;p&gt;The file has size 0 because we haven’t done any INSERTs in this table yet.&lt;/p&gt;
    &lt;p&gt;Let’s add a couple of rows to our table:&lt;/p&gt;
    &lt;code&gt;insert into foo (id, name) values (1, 'Ronaldo');
INSERT 0 1
insert into foo (id, name) values (2, 'Romario');
INSERT 0 1&lt;/code&gt;
    &lt;p&gt;We can add the &lt;code&gt;ctid&lt;/code&gt; field to the query to retrieve the ctid of each line. The ctid is an internal field that has the address of the line in the heap. Think of it as a pointer to the row location in the heap. It consists of a tuple in the format (m, n) where m is the block id and n is the tuple offset. “ctid” stands for “current tuple id”. Here you can note that the row with id one is stored in the page 0, offset 1.&lt;/p&gt;
    &lt;code&gt;select ctid, * from foo;
 ctid  | id |  name   
-------+----+---------
 (0,1) |  1 | Ronaldo
 (0,2) |  2 | Romario
(2 rows)&lt;/code&gt;
    &lt;head rend="h3"&gt;How indexes speedup access to data&lt;/head&gt;
    &lt;p&gt;Let’s add more players to the table so that the total rows is one million:&lt;/p&gt;
    &lt;code&gt;insert into foo (id, name);
select generate_series(3, 1000000), 'Player ' || generate_series(3, 1000000);&lt;/code&gt;
    &lt;p&gt;After adding more rows to the table its corresponding file is 30MB. Internally, it is divided into 8kb pages.&lt;/p&gt;
    &lt;code&gt;ls -lrtah /opt/homebrew/var/postgresql@16/base/71122/71123
-rw-------  1 dlt  admin    30M 16 Aug 16:32 /opt/homebrew/var/postgresql@16/base/71122/71133&lt;/code&gt;
    &lt;p&gt;When we query a table without an index, Postgres reads all tuples in every page and apply a filter. For example, let’s analyze the command below that searches for rows whose &lt;code&gt;name&lt;/code&gt; column value is equal to “Ronaldo” and show how the database performed this search. We use the explain command with the options &lt;code&gt;(analyse, buffers)&lt;/code&gt;. &lt;code&gt;analyse&lt;/code&gt; will actually execute the query instead of just using cost estimates, and the &lt;code&gt;buffers&lt;/code&gt; option shows how much IO work was done.&lt;/p&gt;
    &lt;code&gt; explain (analyze, buffers) select * from foo where name = 'Ronaldo';
                                                     QUERY PLAN
---------------------------------------------------------------------------------------------------------------------
 Gather  (cost=1000.00..12577.43 rows=1 width=18) (actual time=0.307..264.991 rows=1 loops=1)
   Workers Planned: 2
   Workers Launched: 2
   Buffers: shared hit=97 read=6272
   -&amp;gt;  Parallel Seq Scan on foo  (cost=0.00..11577.33 rows=1 width=18) (actual time=169.520..256.639 rows=0 loops=3)
         Filter: (name = 'Ronaldo'::text)
         Rows Removed by Filter: 333333
         Buffers: shared hit=97 read=6272
 Planning Time: 0.143 ms
 Execution Time: 265.021 ms&lt;/code&gt;
    &lt;p&gt;Note the in output the line starting with " -&amp;gt; Parallel Seq scan on foo". This line denotes that the database performed a sequential search and read all the rows in the table. The execution time for this query was 265.021ms. Also note the line that says “Buffers: shared hit=97 read=6272”. This mean that we needed to read 97 pages from memory, and 6272 pages from disk.&lt;/p&gt;
    &lt;p&gt;Now let’s add an index on the name column and see how the same query performs. We’re using the command &lt;code&gt;create index concurrently&lt;/code&gt; because we don’t want to block the table for writes.&lt;/p&gt;
    &lt;code&gt;create index concurrently on foo(name);
CREATE INDEX

explain (analyze, buffers) select * from foo where name = 'Ronaldo';
                                                    QUERY PLAN
-------------------------------------------------------------------------------------------------------------------
 Index Scan using foo_name_idx on foo  (cost=0.42..8.44 rows=1 width=18) (actual time=0.047..0.049 rows=1 loops=1)
   Index Cond: (name = 'Ronaldo'::text)
   Buffers: shared hit=4
 Planning Time: 0.129 ms
 Execution Time: 0.077 ms
(5 rows)&lt;/code&gt;
    &lt;p&gt;Here we see that the index was used and that in this case the execution time was reduced from 264.21 to 0.074 milliseconds, and the database only needed to read 4 pages! The reduction in execution time happens because, now, instead of reading all the rows in the table, the database uses the index. The index is a tree structure mapping the value “Ronaldo” to the ctid(s) of the rows that have this value in the &lt;code&gt;name&lt;/code&gt; column (in our example we only have one such row). The ctid is then used to quickly locate these rows on the heap.&lt;/p&gt;
    &lt;p&gt;If we use &lt;code&gt;\di+&lt;/code&gt; to show the indexes in our database we can see that the index we’ve created occupies &lt;code&gt;30MB&lt;/code&gt;, roughly the same size as the &lt;code&gt;foo&lt;/code&gt; table.&lt;/p&gt;
    &lt;code&gt;\di+

                                         List of relations
 Schema |     Name     | Type  | Owner | Table | Persistence | Access method | Size  | Description
--------+--------------+-------+-------+-------+-------------+---------------+-------+-------------
 public | foo_name_idx | index | dlt   | foo   | permanent   | btree         | 30 MB |
(1 row)&lt;/code&gt;
    &lt;head rend="h2"&gt;Costs associated with indexes&lt;/head&gt;
    &lt;p&gt;It is important to highlight that the extra speed brought by indices is associated with several costs that must be considered when deciding where and how to apply them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Disk Space&lt;/head&gt;
    &lt;p&gt;Indexes are stored in a separate area of the heap and take up additional disk space. The more indexes a table has, the greater the amount of disk space required to store them. This incurs in additional storage costs for your database and for backups, increased replication traffic, and increased backup and failover recovery times. Bear in mind that its not uncommon for btree indexes to be larger than the table itself. Learning about partial indexes, and multicolumn indexes, as well as about other more space efficient index types such as BRIN can be helpful.&lt;/p&gt;
    &lt;head rend="h3"&gt;Write operations&lt;/head&gt;
    &lt;p&gt;Also, there is a maintenance cost in writing operations such as UPDATE, INSERT and DELETE, if a field that is part of an index is modified, the corresponding index needs to be updated, which can add significant overhead to the writing process.&lt;/p&gt;
    &lt;head rend="h3"&gt;Query planner&lt;/head&gt;
    &lt;p&gt;The query planner (also known as query optimizer) is the component responsible for determining the best execution strategy for a query. With more indexes available, the query planner has more options to consider, which can increase the time needed to plan the query, especially in systems with many complex queries or where there are many indexes available.&lt;/p&gt;
    &lt;head rend="h3"&gt;Memory usage&lt;/head&gt;
    &lt;p&gt;PostgreSQL maintains a portion of frequently accessed data and index pages in memory in its shared buffers. When an index is used, the relevant index pages are loaded into shared buffers to speed up access. The more indexes you have and the more they are used, the more shared buffer memory is necessary. Since shared buffers are limited and are also used for caching data pages, filling the shared buffers with indexes can lead to less efficient caching of table data. It’s also good to keep in mind that the whole indexed column is copied in every node of the btree, since there’s a limit in node size capacity, the larger the indexed column the deeper the tree will be.&lt;/p&gt;
    &lt;p&gt;Another aspect of memory usage is that PostgreSQL uses work memory when it executes queries that involves sorting or complex index scans (involving multi-column or covering indexes). Larger indexes require more memory for these operations. Also, indexes require memory to store some metadata about their structure, column names and statistics in the system catalog cache. And finally indexes require memory for maintainance operations like vacuuming and reindexing operations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Types of Indexes&lt;/head&gt;
    &lt;head rend="h3"&gt;Btree&lt;/head&gt;
    &lt;p&gt;The B-Tree is a very powerful data structure, present not only in Postgres but in almost every database management system, since it is a very good general purpose index. It was invented by Rudolf Bayer and Edward M.McCreight while working at Boeing. Nobody really knows if the “B” in B-tree stands for Bayer, Boeing, balanced or better, and it doesn’t really matter. What really matters is that it enables us to search elements in the tree in O(log n) time. If you’re not familiar with Big-O notation, all you need to know is that is is really fast - you only need to make 20 comparisons in order to find an element in a set with 1 million items. Moreover, it can maintain O(log n) time complexity for data sets that are larger than the RAM available on a computer. This means that disks can be used to extend RAM, thanks to the btree efficient prevention of disk page accesses to find the desired data. In PostgreSQL the btree is the most common type of index and its the default, it’s also used to support system and TOAST indexes. Even an empty database has hundreds of btree indexes. It is the only index type that can be used for primary and unique key constraints.&lt;/p&gt;
    &lt;p&gt;In contrast with a binary tree, the BTree is a balanced tree and all of its leave nodes have the same distance from the root. The root nodes and inner nodes have pointers to lower levels, and the leaf nodes have the keys and pointers to the heap. Postgres btrees also have pointers to the left and right nodes for easier forward and backward scanning. Nodes can have multiple keys and these keys are sorted so that it’s easy to walk in ordered directions and to perform ORDER BY and JOIN operations. The values are only stored in the leaf nodes, this makes the tree more compact and facilitates a full traversal of the objects in a tree with just a linear pass through all the leaf nodes. This is just a simplified description of PostgreSQL Btree indexes, if you want to get into the low level details, I suggest you to read the README and the paper that inspired them. Below there’s a simplified illustration of a Postgres Btree.&lt;/p&gt;
    &lt;head rend="h4"&gt;Using multiple indexes&lt;/head&gt;
    &lt;p&gt;Postgres can use multiple indexes to handle cases that cannot be handled by single index scans, by forming &lt;code&gt;AND&lt;/code&gt; and &lt;code&gt;OR&lt;/code&gt; conditions across several index scans with the support of bitmaps. The bitmaps are ANDed or ORed together as needed by the query and finally the table rows are visited and returned. Let’s say we have a query like this:&lt;/p&gt;
    &lt;code&gt;select * from users where age = 30 and login_count = 100;&lt;/code&gt;
    &lt;p&gt;If the &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;login_count&lt;/code&gt; columns are indexed, postgres scans index &lt;code&gt;age&lt;/code&gt; for all pages with &lt;code&gt;age=30&lt;/code&gt; and makes a bitmap where the pages that might contain rows with &lt;code&gt;age=30&lt;/code&gt; are true. In a similar way, it builds a bitmap using the &lt;code&gt;login_count&lt;/code&gt; index. It then ANDs the two bitmaps to form a third bitmap, and performs a table scan, only reading the pages that might contain candidate values, and only adding the rows where &lt;code&gt;age=30 and login_count=100&lt;/code&gt; to the result set.&lt;/p&gt;
    &lt;head rend="h4"&gt;Multi-column indexes&lt;/head&gt;
    &lt;p&gt;Multi-column indexes are an alternative for using multiple indexes. They’re generaly going to be smaller and faster than using multiple indexes, but they’ll also be less flexible. That’s because the order of the columns matter, because the database can search for a subset of the indexed columns, as long as they are the leftmost columns. For example, if you have an index on column &lt;code&gt;a&lt;/code&gt; and another index on column &lt;code&gt;b&lt;/code&gt;, these indexes will serve all the of queries below:&lt;/p&gt;
    &lt;code&gt;select * from my_table where a = 42 and b = 420;

select * from my_table where a = 43;

select * from my_table where b = 99;&lt;/code&gt;
    &lt;p&gt;On the other hand, only the first two queries would use an index if you created a multi-column index on (a, b) with a command like &lt;code&gt;create index on my_table(a, b)&lt;/code&gt;; So, when building multi-column indexes choose the order of the columns well so that your index can be used by the most queries possible.&lt;/p&gt;
    &lt;head rend="h4"&gt;Partial indexes&lt;/head&gt;
    &lt;p&gt;Partial indexes allow you to use a conditional expression to control what subset of rows will be indexed, this can bring you many benefits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;your index can be smaller and more likely fit in RAM.&lt;/item&gt;
      &lt;item&gt;your index is shallower, so lookups are quicker&lt;/item&gt;
      &lt;item&gt;less overhead for index/update/delete (but can also mean more overhead if the column you’re using to filter rows in/out of the index is updated very frequently triggering constant index maintenance)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They’re mostly useful in situations where you don’t care about some rows, or when you’re indexing on a column where the proportion of one value is much greater than others. I’ll give two examples below.&lt;/p&gt;
    &lt;head rend="h5"&gt;When you don’t care about some rows&lt;/head&gt;
    &lt;p&gt;Let’s say you have a rules table where the rows can be marked as enabled/disabled, the vast majority of the rows are disabled and in your queries you only care about enabled rows. In this case, you would have a partial index, filtering out the disable rows like this:&lt;/p&gt;
    &lt;code&gt;create index on rules(status) where status = 'enabled';&lt;/code&gt;
    &lt;head rend="h5"&gt;When the distribution of values is skewed&lt;/head&gt;
    &lt;p&gt;Now imagine you’re building a todo application and the status column value can be either &lt;code&gt;TODO&lt;/code&gt;, &lt;code&gt;DOING&lt;/code&gt;, and &lt;code&gt;DONE&lt;/code&gt;. Suppose you have 1M rows and this is the current distribution of rows in each status:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Rows&lt;/cell&gt;
        &lt;cell role="head"&gt;Status&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;TODO&lt;/cell&gt;
        &lt;cell&gt;90%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;DOING&lt;/cell&gt;
        &lt;cell&gt;5%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;DONE&lt;/cell&gt;
        &lt;cell&gt;5%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Since postgres keeps statistics about the distribution of values in your table columns and knows that the vast majority of the rows are in the &lt;code&gt;TODO&lt;/code&gt; status, it would choose to do a sequential scan on the &lt;code&gt;tasks&lt;/code&gt; table when you have &lt;code&gt;status='TODO'&lt;/code&gt; in the &lt;code&gt;WHERE&lt;/code&gt; clause of your query, even if you have an index on status, leaving most part of the index unused and wasting space. In this case, a partial scan such as the one below is recommended:&lt;/p&gt;
    &lt;code&gt;create index on tasks(status) where status &amp;lt;&amp;gt; 'TODO';&lt;/code&gt;
    &lt;head rend="h4"&gt;Covering indexes&lt;/head&gt;
    &lt;p&gt;If you have a query that selects only columns in an index, Postgres has all information needed by the query in the index and doesn’t need to fetch pages from the heap to return the result. This optimization is called &lt;code&gt;index-only scan&lt;/code&gt;. To understand how it works, consider the following scenario:&lt;/p&gt;
    &lt;code&gt;create table bar (a int, b int, c int);
create index abc_idx on bar(a, b);

/* query 1 */
select a, b from bar;

/* query 2 */
select a, b, c from bar;&lt;/code&gt;
    &lt;p&gt;In the first query, postgres can do an index-only scan and avoid fetching data from the heap because the values &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are present in the index. In the second query, since &lt;code&gt;c&lt;/code&gt; isn’t in the index, posgres needs to follow the reference to the heap to fetch its value. In the first query we allowed postgres do to an index-only scan with the help of a multi-column index, but we could also achieve the same result by using a covering index. The syntax for creating a covering index looks like this:&lt;/p&gt;
    &lt;code&gt;create index abc_cov_idx on bar(a, b) including c;&lt;/code&gt;
    &lt;p&gt;This is more space efficient than creating a multi-column index on (a, b, c), because c will only be inserted at the leaf nodes of the btree. Also, we might want to use a covering index in cases where we want an unique index and &lt;code&gt;c&lt;/code&gt; would “break” the uniqueness of the index.&lt;/p&gt;
    &lt;head rend="h4"&gt;Expression indexes&lt;/head&gt;
    &lt;p&gt;Expression indexes to index the result of an expression or function, rather than just the raw column values. This can be extremely useful when you frequently query based on a transformed version of your data. It is necessary if you use a function as part of a where clause as in the example below:&lt;/p&gt;
    &lt;code&gt;CREATE TABLE customers (
    id SERIAL PRIMARY KEY,
    name TEXT
);

CREATE INDEX idx_name ON customers(name);
SELECT * FROM customers WHERE LOWER(name) = 'john doe';&lt;/code&gt;
    &lt;p&gt;In this example above, Postgres won’t use the index because it was was built against the &lt;code&gt;name&lt;/code&gt; column. In order to make it work, the index key has to call the &lt;code&gt;lower&lt;/code&gt; function just like it’s used in the where clase. To fix it, do:&lt;/p&gt;
    &lt;p&gt;Now, when you run a query like this:&lt;/p&gt;
    &lt;code&gt;CREATE INDEX idx_lower_name ON customers (lower(name));&lt;/code&gt;
    &lt;p&gt;Now PostgreSQL can use the expression index to efficiently find the matching rows.&lt;/p&gt;
    &lt;p&gt;Expression indexes can be created using various types of expressions:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Built-in functions: Like &lt;code&gt;lower()&lt;/code&gt;,&lt;code&gt;upper()&lt;/code&gt;, etc.&lt;/item&gt;
      &lt;item&gt;User-defined functions: As long as they are immutable.&lt;/item&gt;
      &lt;item&gt;String concatenations: Like &lt;code&gt;first_name || ' ' || last_name&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Hash&lt;/head&gt;
    &lt;p&gt;The hash index differs from B-Tree in strucutre, it is much more alike a hashmap data structure present in most programming languages (e.g. dict in Python, array in php, HashMap in java, etc). Instead of adding the full column value to the index, a 32bit hash code is derived from it and added to the hash. This makes hash indexes much smaller than btrees when indexing longer data such as UUIDs, URLs, etc. Any data type can be indexed with the help of postgres hashing functions. If you type &lt;code&gt;\df hash*&lt;/code&gt; and press TAB in psql, you’ll see that there are more then 50 hash related functions. Although it gracefully handles hash conflicts, it works better for even distribution of hash values and is most suited to unique or mostly unique data. Under the correct conditions it will not only be smaller than btree indexes, but also it will be faster for reads when compared with btress. Here’s what the official docs says about it:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“In a B-tree index, searches must descend through the tree until the leaf page is found. In tables with millions of rows, this descent can increase access time to data. The equivalent of a leaf page in a hash index is referred to as a bucket page. In contrast, a hash index allows accessing the bucket pages directly, thereby potentially reducing index access time in larger tables. This reduction in “logical I/O” becomes even more pronounced on indexes/data larger than shared_buffers/RAM.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;As for its limitations, it only supports equality operations and isn’t going to be helpful if you need to order by the indexed field. It also doesn’t support multi-column indexes and checking for uniqueness. For a in-depth analysis of how hash indexes fare in relation to btree, check Evgeniy Demin’s blog post on the subject.&lt;/p&gt;
    &lt;head rend="h3"&gt;BRIN&lt;/head&gt;
    &lt;p&gt;BRIN stands for Block Range Index and its name tells a lot about how it is implemented. Nodes in BRIN indexes store the minimum and maximum values of a range of values present in the page referred by the index. This makes the index more compact and cache friendly, but restricts the use cases for it. If you have a very large in a work load that is heavy on writes and low on deletes and updates. You can think of a BRIN index as an optimizer for sequential scans of large amounts of data in very large databases, and is a good optimization to try before partitioning a table. For a BRIN index to work well, the index key should be a column that strongly correlates to the location of the row in the heap.Some good use cases for BRIN are append-only tables and tables storing time series data.&lt;/p&gt;
    &lt;p&gt;BRIN won’t work well for tables where the rows are updated constantly, due to the nature of MVCC that duplicates rows and stores them in a different part of the heap. This tuple duplication and moving affect the correlation negatively and reduces the effectiveness of the index. Using extensions such as pg_repack or pg_squeeze isn’t recommended for tables that use BRIN indexes, since they change the internal data layour fo the table and mess up the correlation. Also, this index is lossy in the sense that the index leaf nodes point to pages taht might contain a value within a particular range. For this reason a BRIN is more helpful if you need to return large subset of data, and a btree would be more read performant for queries that only return one or few rows. You can make the index more or less lossy by adjusting the &lt;code&gt;page_per_range&lt;/code&gt; configuration, the trade off will be index size.&lt;/p&gt;
    &lt;head rend="h3"&gt;GIN&lt;/head&gt;
    &lt;p&gt;Generalized inverted index is appropriate for when you want to search for an item in composite data, such as finding a word in a blob of text, an item in an array or an object in a JSON. The GIN is generalized in the sense that it doesn’t need to know how it will acelerate the search for some item. Instead, there’s a set of custom strategies specific for each data type. Please note that in order to index an JSON value it needs to be stored in a JSONB column. Similarly, if you’re indexing text it’s better to store it as (or convert it to) tsvector or use the pg_trgm extension.&lt;/p&gt;
    &lt;head rend="h3"&gt;GiST &amp;amp; SP-GiST&lt;/head&gt;
    &lt;p&gt;The Generalized Search Tree and the Space-Partitioned Generalized Search Tree are tree structures that can be use as a base template to implement indexes for specific data types. You can think of them as framework for building indexes. The GiST is a balanced tree and the SP-GiST allow for the development of non-balanced data structures. They are useful for indexing points and geometric types, inet, ranges and text vectors. You can find an extensive list of the built-in strategies shipped with postgres in the official documentation. If you need an index to enable full-text search in your application, you’ll have to choose between GIN and GiST. Roughly speaking, GIN is faster for lookups but it’s bigger and has greater building and maintainance costs. So the right index type for you will depend on your application requirements.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Understanding and effectively using indexes is crucial for optimizing database performance in PostgreSQL. While indexes can greatly speed up query execution and improve overall efficiency, it’s important to be mindful of their impact on write operations and storage. By carefully selecting the appropriate types of indexes based on your specific use cases you can ensure that your PostgreSQL database remains both fast and efficient. I hope this article taught you at least one or two things you didn’t know about Postgres indexes, and that you’re better equiped to deal with different scenarios involving databases from now on.&lt;/p&gt;
    &lt;p&gt;4119 Words&lt;/p&gt;
    &lt;p&gt;2024-09-11 08:07&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46751826</guid><pubDate>Sun, 25 Jan 2026 08:07:03 +0000</pubDate></item><item><title>Deutsche Telekom is throttling the internet</title><link>https://netzbremse.de/en/</link><description>&lt;doc fingerprint="66133f28ed9a317f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Deutsche Telekom is throttling the internet. Let's do something about it!&lt;/head&gt;
    &lt;p&gt;If you are a customer of Deutsche Telekom and some websites just won't load, then we might have the solution to your problem!&lt;/p&gt;
    &lt;head rend="h2"&gt;Short Explanation!&lt;/head&gt;
    &lt;head rend="h2"&gt;What is this about?&lt;/head&gt;
    &lt;p&gt;Epicenter.works, the Society for Civil Rights, the Federation of German Consumer Organizations, and Stanford Professor Barbara van Schewick are filing an official complaint with the Federal Network Agency against Deutsche Telekom’s unfair business practices.&lt;/p&gt;
    &lt;p&gt;Deutsche Telekom is creating artificial bottlenecks at access points to its network. Financially strong services that pay Telekom get through quickly and work perfectly. Services that cannot afford this are slowed down and often load slowly or not at all.&lt;/p&gt;
    &lt;p&gt;This means Telekom decides which services we can use without issues, violating net neutrality. We are filing a complaint with the Federal Network Agency to stop this unfair practice together!&lt;/p&gt;
    &lt;head rend="h2"&gt;Testimonials&lt;/head&gt;
    &lt;head rend="h2"&gt;How can you help the project?&lt;/head&gt;
    &lt;p&gt;Are you a Deutsche Telekom customer and want to help? Get in touch with usâevery experience counts! Maybe you even have networking expertise and measurement data that could be relevant? Whether with or without measurements, weâd love to hear from you at netzbremse@epicenter.works.&lt;/p&gt;
    &lt;p&gt;Do you have experience with interconnection agreements with Deutsche Telekom and want to talk to us confidentially? Contact us via email or one of the following channels:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Signal: +43 670 404 98 89&lt;/item&gt;
      &lt;item&gt;Threema (ID: BXJMX4R5)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Donate now for a free internet!Donate now for a free internet!&lt;/head&gt;
    &lt;head rend="h2"&gt;Talk at Chaos Communication Congress 38C3&lt;/head&gt;
    &lt;head rend="h2"&gt;Media Coverage&lt;/head&gt;
    &lt;p&gt;VerbraucherschÃ¼tzer werfen der Telekom vor, bewusst EngpÃ¤sse im Internet zu schaffen, um zusÃ¤tzlich Geld fÃ¼r schnellere ZugÃ¤nge zu kassieren. Sie sehen darin eine Verletzung von EU-Recht. Die Telekom widerspricht. Von Markus Reher.&lt;/p&gt;
    &lt;p&gt;Ein BÃ¼ndnis aus mehreren Organisationen hat bei der Bundesnetzagentur eine Beschwerde wegen angeblicher Verletzungen von NetzneutralitÃ¤tspflichten eingereicht.&lt;/p&gt;
    &lt;p&gt;VerbraucherschÃ¼tzer werfen der Telekom vor, mit schlechtem Netz zusÃ¤tzliches Geld zu machen.&lt;/p&gt;
    &lt;p&gt;Ein BÃ¼ndnis mehrerer Organisationen wirft der Telekom vor, kÃ¼nstliche EngpÃ¤sse im Netz zu schaffen und damit Geld zu verdienen. Der Konzern weist die VorwÃ¼rfe zurÃ¼ck und holt zur Gegenkritik aus.&lt;/p&gt;
    &lt;p&gt;VerbraucherschÃ¼tzer werfen der Telekom vor, mit schlechtem Netz zusÃ¤tzliches Geld zu machen.&lt;/p&gt;
    &lt;p&gt;Die Telekom drosselt das Netz", beklagen die Macher einer neuen Kampagne. Das verstoÃe gegen die NetzneutralitÃ¤t. Eine offizielle Beschwerde soll bald folgen.&lt;/p&gt;
    &lt;p&gt;“Die Telekom drosselt das Netz”, beklagen die Macher einer neuen Kampagne. Das verstoÃe gegen die NetzneutralitÃ¤t. Eine offizielle Beschwerde soll bald folgen.&lt;/p&gt;
    &lt;p&gt;Die DeutÂsche Telekom muss endlich die Peering-KapaÂzitÃ¤ten zu anderen Internet-Knoten erhÃ¶hen.&lt;/p&gt;
    &lt;p&gt;VerbrauÂcherÂschÃ¼tzer wollen bei der BundesÂnetzÂagentur Beschwerde gegen Peering-Probleme im Telekom-Netz einreiÂchen.&lt;/p&gt;
    &lt;p&gt;Nicht nur als KrisenlÃ¶sung sucht das Deutsche Forschungsnetz den direkten Anschluss zur Deutschen Telekom. Die wollte sich aber zuerst auf nichts einlassen.&lt;/p&gt;
    &lt;p&gt;Schon seit mindestens Mai 2015 gibt es sie, eine Option fÃ¼r Hetzner-Kunden, die ihre Server fÃ¼r Kunden der Telekom zwischen 19 und 22 Uhr besser erreichbar machen wollen.&lt;/p&gt;
    &lt;p&gt;Die Telekom sieht sich mit schweren VorwÃ¼rfen konfrontiert, nach denen sie absichtlich gegen die NetzneutralitÃ¤t verstoÃen soll. Ãberzeugt davon ist nicht nur die Verbraucherzentrale.&lt;/p&gt;
    &lt;p&gt;Die Verbraucherzentrale sucht Betroffene, die Probleme im Netz der Telekom haben. Der Vorwurf: Verletzung der NetzneutralitÃ¤t.&lt;/p&gt;
    &lt;p&gt;Verbraucher und Organisationen wehren sich gegen Netzdrosselung von Telekom. Erste Beschwerden gehen ein.&lt;/p&gt;
    &lt;p&gt;Der Verbraucherschutz schlÃ¤gt Alarm: Die Deutsche Telekom bevorzugt bei der Internet-Geschwindigkeit offenbar Dienste und Webseiten, die fÃ¼r mehr Tempo zahlen, wÃ¤hrend sie andere drosselt. User sollen das nun bestÃ¤tigen.&lt;/p&gt;
    &lt;p&gt;Macht die Telekom Teile des Internets absichtlich langsam? Dieser Vorwurf hat es in sich und wird vom Verband der Verbraucherzentralen erhoben â ein Einblick in das Prinzip des Peerings und die Frage, wie die Telekom hier seit Jahren fÃ¼r Frust sorgt.&lt;/p&gt;
    &lt;p&gt;Macht die Telekom ihr Netz absichtlich langsamer? Der Verbraucherschutz wirft dem Unternehmen vor, die NetzneutralitÃ¤t absichtlich zu verletzen. Die Telekom verlange von Anbietern Zahlungen fÃ¼r bevorzugten Datentransfer und bremse andere Dienste aus.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46751899</guid><pubDate>Sun, 25 Jan 2026 08:22:17 +0000</pubDate></item><item><title>A flawed paper in management science has been cited more than 6k times</title><link>https://statmodeling.stat.columbia.edu/2026/01/22/aking/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46752151</guid><pubDate>Sun, 25 Jan 2026 09:04:30 +0000</pubDate></item><item><title>Jurassic Park - Tablet device on Nedry's desk? (2012)</title><link>https://www.therpf.com/forums/threads/jurassic-park-tablet-device-on-nedrys-desk.169883/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46752261</guid><pubDate>Sun, 25 Jan 2026 09:22:17 +0000</pubDate></item><item><title>150k lines of vibe coded Elixir: The good, the bad and the ugly</title><link>https://getboothiq.com/blog/150k-lines-vibe-coded-elixir-good-bad-ugly</link><description>&lt;doc fingerprint="f72c54daabbd7c64"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;150,000 Lines of Vibe Coded Elixir: The Good, The Bad, and The Ugly&lt;/head&gt;
    &lt;p&gt;TL;DR:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Good: AI is great at Elixir. It gets better as your codebase grows.&lt;/item&gt;
      &lt;item&gt;Bad: It defaults to defensive, imperative code. You need to be strict about what good Elixir looks like.&lt;/item&gt;
      &lt;item&gt;Ugly: It can’t debug concurrent test failures. It doesn’t understand that each test runs in an isolated transaction, or that processes have independent lifecycles. It spirals until you step in.&lt;/item&gt;
      &lt;item&gt;Bottom Line: Even with the drawbacks, the productivity gains are off the charts. I expect it will only get better.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;BoothIQ is a universal badge scanner for trade shows. AI writes 100% of our code. We have 150,000 lines of vibe coded Elixir running in production. Here’s what worked and what didn’t.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Good&lt;/head&gt;
    &lt;head rend="h3"&gt;Elixir is Small: It Gets It Right the First Time&lt;/head&gt;
    &lt;p&gt;Elixir is a small language. Few operators. Small standard library. Only so many ways to control flow. It hasn’t been around for decades. It hasn’t piled up paradigms like .NET or Java, where functional and OOP fight for space.&lt;/p&gt;
    &lt;p&gt;This matters. AI is bad at decisions. If you want your agent to succeed, have it make fewer decisions. With Elixir, Claude doesn’t need to pick between OOP and functional. It doesn’t need to navigate old syntax next to new patterns. There’s one way to skin the cat. Claude finds it.&lt;/p&gt;
    &lt;p&gt;This matters more if you’re adding AI to an existing codebase. In languages where paradigms came and went—often with whatever developer pushed them—Claude tries to match the existing code. The existing code is inconsistent. So Claude is inconsistent.&lt;/p&gt;
    &lt;head rend="h3"&gt;Elixir is Terse: Longer Sessions, Fewer Compactions&lt;/head&gt;
    &lt;p&gt;Small and terse are related but different. Small means few concepts. Terse means fewer tokens to express the same thing. Go is small but not terse—few concepts, but verbose syntax and explicit error handling everywhere. Elixir is both. We got lucky.&lt;/p&gt;
    &lt;p&gt;Context windows are a real constraint. Elixir uses fewer tokens than most languages. No braces. No semicolons. No verbose boilerplate. I can stay in a working session longer. More iterations. Fewer compactions—those moments when the AI summarizes and forgets earlier context. More context in memory.&lt;/p&gt;
    &lt;p&gt;When I built the React Native version of our app, I hit compactions constantly. JavaScript is small-ish, but it’s not terse. It burns tokens to do what Elixir does with fewer.&lt;/p&gt;
    &lt;p&gt;I also see more compactions when working on heavy HTML and Tailwind in LiveView. Adding, updating, or editing large sections of markup at once. HTML and HEEx templates are token-heavy. But even then, it’s less painful than JavaScript-heavy work.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tidewave: Longer Unassisted Runs&lt;/head&gt;
    &lt;p&gt;Tidewave supercharges Elixir-specific context. It lets the agent read logs from the running app—debug, info, error, warning—so you don’t copy/paste logs around. It can query the dev database, see Ecto schemas, and view package documentation. Fewer hallucinations. Longer unassisted runs. The agent can check and validate its own assumptions without human intervention.&lt;/p&gt;
    &lt;head rend="h3"&gt;Immutability: Fewer Decisions, Less Code&lt;/head&gt;
    &lt;p&gt;If a variable gets mutated by a function call, AI now has three problems instead of one. The actual feature you want implemented. Whether to work around the mutation or update other call sites to stop mutating. And the mutated data itself—what is it, what was it, what will it be, what can it be?&lt;/p&gt;
    &lt;p&gt;AI ponders all of this and contorts itself into an overly defensive mess. It writes nonsense validation checks and if-statements on mutated data. Defensive code that wouldn’t exist in an immutable language.&lt;/p&gt;
    &lt;p&gt;In Elixir, the data is what it is. It’s not going to change. Fewer decisions. Less code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Frontend: Higher Quality, Less Time&lt;/head&gt;
    &lt;p&gt;I prompt high-level changes—“give the top section more padding”—and Claude does it faster than I could. It’s especially good at modifying or moving large chunks of page structure. Mobile-first views? Easy. Way faster than me, and it’s a better designer than me too.&lt;/p&gt;
    &lt;p&gt;The quality floor has gone way up. You can’t hide behind “I’m not a designer” anymore.&lt;/p&gt;
    &lt;head rend="h3"&gt;Git Worktrees: Build Multiple Features in Parallel&lt;/head&gt;
    &lt;p&gt;I use three git worktrees, so I can work on up to three features at any given time. Typically a main feature, a slightly less important one, and the third reserved for quick fixes, low priority stuff, or quick experiments.&lt;/p&gt;
    &lt;p&gt;Three is about the limit. Any more and context switching between features becomes the bottleneck.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Bad&lt;/head&gt;
    &lt;head rend="h3"&gt;AI Can’t Organize: Architecture Is Still On You&lt;/head&gt;
    &lt;p&gt;AI is exceptional at churning out lines of code. It’s significantly less exceptional at deciding where those lines should go. It defaults to creating new files everywhere. It repeats code it’s already written. It introduces inconsistencies.&lt;/p&gt;
    &lt;p&gt;This is the “mess” people describe in vibe code projects as they grow. You still need a human making structural decisions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Trained on Imperative: It Writes Defensive Code&lt;/head&gt;
    &lt;p&gt; AI trained mostly on imperative code. Ruby, Python, JavaScript, C#. Elixir looks like Ruby. So Claude writes Ruby-style Elixir—&lt;code&gt;if/then/else&lt;/code&gt; chains, defensive nil-checking, early returns that don’t make sense in a functional context.&lt;/p&gt;
    &lt;p&gt;Elixir wants you to be assertive. Pattern match on what you expect. Let it crash if something’s wrong. The process restarts in a good state. This is foreign to most code Claude trained on.&lt;/p&gt;
    &lt;p&gt;This gets better as the codebase grows. Claude sees more assertive patterns. It starts to infer the style. But it still defaults to defensive. I still correct it regularly. Be strict about what good Elixir looks like.&lt;/p&gt;
    &lt;head rend="h3"&gt;Git Operations: Keep It Out of Context&lt;/head&gt;
    &lt;p&gt;Every git operation takes context window space. Checking status. Writing commit messages. Describing PRs. That space could go to actual work. Git context goes stale fast—a commit message from 20 minutes ago is worthless after three more changes.&lt;/p&gt;
    &lt;p&gt;When I’m babysitting a feature, I commit manually. Every point I’m happy with. It’s fast. It’s cheap version control. It doesn’t burn context.&lt;/p&gt;
    &lt;p&gt;Claude Code has “checkpoints” now. Internal version control that protects vibe coders without explicit commits. That’s better than AI managing git directly.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Ugly&lt;/head&gt;
    &lt;head rend="h3"&gt;OTP and Async: It Chases Ghosts&lt;/head&gt;
    &lt;p&gt;Claude is useless for debugging OTP, Task, or async issues. It doesn’t understand how processes, the actor model, and GenServers work together. When it tries to introspect the running system, it feeds itself bad data. It gets very lost.&lt;/p&gt;
    &lt;p&gt;It can course correct when you point out where it went wrong. But on its own, it chases ghosts.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ecto Sandbox: It Chases Red Herrings&lt;/head&gt;
    &lt;p&gt;In Elixir tests, each test runs in a database transaction that rolls back at the end. Tests run async without hitting each other. No test data persists.&lt;/p&gt;
    &lt;p&gt;Claude doesn’t understand this. It uses Tidewave’s dev DB connection and thinks it’s looking at the test DB—which is always empty. A test fails. Claude queries the database. Finds nothing. Thinks there’s a data problem.&lt;/p&gt;
    &lt;p&gt;I’ve watched Claude try to seed the test database so a test will pass. That’s clearly wrong.&lt;/p&gt;
    &lt;p&gt;Other times, two tests insert or query the same schema. Claude doesn’t understand transaction isolation—tests can’t see each other’s data. It confuses itself and recommends disabling async tests altogether. Manageable once you watch for it. But ugly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Bottom Line&lt;/head&gt;
    &lt;p&gt;AI writing all the code has been a massive win. The friction exists, but it’s manageable and doesn’t interfere much with day-to-day work. By far the most important thing: have a consistent, coherent codebase architecture. Without it, you’ll quickly end up with spaghetti code.&lt;/p&gt;
    &lt;p&gt;The goal for this year: automate myself out of a job. That means giving Claude more control over the entire software development lifecycle—from a simple problem statement to a fully tested, working PR that only needs a quick glance before it’s merged and deployed.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46752907</guid><pubDate>Sun, 25 Jan 2026 10:54:29 +0000</pubDate></item><item><title>Show HN: TUI for managing XDG default applications</title><link>https://github.com/mitjafelicijan/xdgctl</link><description>&lt;doc fingerprint="fe9cba7b7ba6b9a3"&gt;
  &lt;main&gt;
    &lt;p&gt;&lt;code&gt;xdgctl&lt;/code&gt; is a TUI for managing XDG default applications. View and set defaults for file categories without using &lt;code&gt;xdg-mime&lt;/code&gt; directly.&lt;/p&gt;
    &lt;p&gt;Built with C using GLib/GIO and termbox2.&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;xdgctl.mp4&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Browse by category (Browsers, Text Editors, etc.)&lt;/item&gt;
      &lt;item&gt;Current default marked with &lt;code&gt;*&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Key&lt;/cell&gt;
        &lt;cell role="head"&gt;Action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Arrow Up/Down&lt;/cell&gt;
        &lt;cell&gt;Navigate through categories or applications&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Arrow Right/Tab&lt;/cell&gt;
        &lt;cell&gt;Switch from category list to application list&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Arrow Left&lt;/cell&gt;
        &lt;cell&gt;Switch back to category list&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Enter&lt;/cell&gt;
        &lt;cell&gt;Set selected application as default for current category&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Esc / q&lt;/cell&gt;
        &lt;cell&gt;Quit the application&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;To build &lt;code&gt;xdgctl&lt;/code&gt;, you need the following development libraries:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;glib-2.0&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;gio-2.0&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;gio-unix-2.0&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;clang&lt;/code&gt;or&lt;code&gt;gcc&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# On Void Linux
sudo xbps-install glibc-devel pkg-config&lt;/code&gt;
    &lt;code&gt;git clone https://github.com/mitjafelicijan/xdgctl.git
cd xdgctl

# Build
make
sudo make install

# Using prefix
sudo make PREFIX=/usr/local install
make PREFIX=~/.local install&lt;/code&gt;
    &lt;p&gt;If you manually add new applications to your &lt;code&gt;~/.local/share/applications&lt;/code&gt; directory, you might need to run &lt;code&gt;update-desktop-database&lt;/code&gt; again.&lt;/p&gt;
    &lt;code&gt;ls /usr/share/applications
ls ~/.local/share/applications&lt;/code&gt;
    &lt;code&gt;xdg-mime query default text/plain
xdg-mime query default text/html
xdg-mime query default x-scheme-handler/http
xdg-mime query default x-scheme-handler/https
xdg-mime query default inode/directory&lt;/code&gt;
    &lt;code&gt;xdg-mime default brave.desktop x-scheme-handler/http
xdg-mime default brave.desktop x-scheme-handler/https&lt;/code&gt;
    &lt;code&gt;# ~/.local/share/applications/brave.desktop
[Desktop Entry]
Exec=/home/m/Applications/brave
Type=Application
Categories=Applications
Name=Brave Browser
MimeType=text/html;text/xml;application/xhtml+xml;x-scheme-handler/http;x-scheme-handler/https;&lt;/code&gt;
    &lt;code&gt;update-desktop-database ~/.local/share/applications
less ~/.config/mimeapps.list
less /usr/share/applications/mimeapps.list&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46753078</guid><pubDate>Sun, 25 Jan 2026 11:19:04 +0000</pubDate></item><item><title>Show HN: Bonsplit – Tabs and splits for native macOS apps</title><link>https://bonsplit.alasdairmonk.com</link><description>&lt;doc fingerprint="8d0973528a96f1e8"&gt;
  &lt;main&gt;
    &lt;p&gt;Bonsplit is a custom tab bar and layout split library for macOS apps. Enjoy out of the box 120fps animations, drag-and-drop reordering, SwiftUI support &amp;amp; keyboard navigation.&lt;/p&gt;
    &lt;quote&gt;.package(url: "https://github.com/almonk/bonsplit.git", from: "1.0.0")&lt;/quote&gt;
    &lt;p&gt;### Features&lt;/p&gt;
    &lt;p&gt;Create tabs with optional icons and dirty indicators. Target specific panes or use the focused pane.&lt;/p&gt;
    &lt;quote&gt;let tabId = controller.createTab(title: "Document.swift",icon: "swift",isDirty: false,inPane: paneId)&lt;/quote&gt;
    &lt;p&gt;Create tabs with optional icons and dirty indicators. Target specific panes or use the focused pane.&lt;/p&gt;
    &lt;quote&gt;let tabId = controller.createTab(title: "Document.swift",icon: "swift",isDirty: false,inPane: paneId)&lt;/quote&gt;
    &lt;p&gt;Split any pane horizontally or vertically. New panes are empty by default, giving you full control.&lt;/p&gt;
    &lt;quote&gt;// Split focused pane horizontallylet newPaneId = controller.splitPane(orientation: .horizontal)// Split with a tab already in the new panecontroller.splitPane(orientation: .vertical,withTab: Tab(title: "New", icon: "doc"))&lt;/quote&gt;
    &lt;p&gt;Split any pane horizontally or vertically. New panes are empty by default, giving you full control.&lt;/p&gt;
    &lt;quote&gt;// Split focused pane horizontallylet newPaneId = controller.splitPane(orientation: .horizontal)// Split with a tab already in the new panecontroller.splitPane(orientation: .vertical,withTab: Tab(title: "New", icon: "doc"))&lt;/quote&gt;
    &lt;p&gt;Update tab properties at any time. Changes animate smoothly.&lt;/p&gt;
    &lt;quote&gt;// Mark document as modifiedcontroller.updateTab(tabId, isDirty: true)// Rename tabcontroller.updateTab(tabId, title: "NewName.swift")// Change iconcontroller.updateTab(tabId, icon: "doc.text")&lt;/quote&gt;
    &lt;p&gt;Update tab properties at any time. Changes animate smoothly.&lt;/p&gt;
    &lt;quote&gt;// Mark document as modifiedcontroller.updateTab(tabId, isDirty: true)// Rename tabcontroller.updateTab(tabId, title: "NewName.swift")// Change iconcontroller.updateTab(tabId, icon: "doc.text")&lt;/quote&gt;
    &lt;p&gt;Programmatically navigate between panes using directional navigation.&lt;/p&gt;
    &lt;quote&gt;// Move focus between panescontroller.navigateFocus(direction: .left)controller.navigateFocus(direction: .right)controller.navigateFocus(direction: .up)controller.navigateFocus(direction: .down)// Or focus a specific panecontroller.focusPane(paneId)&lt;/quote&gt;
    &lt;p&gt;Programmatically navigate between panes using directional navigation.&lt;/p&gt;
    &lt;quote&gt;// Move focus between panescontroller.navigateFocus(direction: .left)controller.navigateFocus(direction: .right)controller.navigateFocus(direction: .up)controller.navigateFocus(direction: .down)// Or focus a specific panecontroller.focusPane(paneId)&lt;/quote&gt;
    &lt;p&gt;### Read this, agents...&lt;/p&gt;
    &lt;p&gt;Complete reference for all Bonsplit classes, methods, and configuration options.&lt;/p&gt;
    &lt;p&gt;The main controller for managing tabs and panes. Create an instance and pass it to BonsplitView.&lt;/p&gt;
    &lt;p&gt;Implement this protocol to receive callbacks about tab bar events. All methods have default implementations and are optional.&lt;/p&gt;
    &lt;p&gt;Configure behavior and appearance. Pass to BonsplitController on initialization.&lt;/p&gt;
    &lt;code&gt;allowSplits&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Enable split buttons and drag-to-split&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;allowCloseTabs&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Show close buttons on tabs&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;allowCloseLastPane&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Allow closing the last remaining pane&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;false&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;allowTabReordering&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Enable drag-to-reorder tabs within a pane&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;allowCrossPaneTabMove&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Enable moving tabs between panes via drag&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;autoCloseEmptyPanes&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Automatically close panes when their last tab is closed&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;contentViewLifecycle&lt;/code&gt;
    &lt;code&gt;ContentViewLifecycle&lt;/code&gt;
    &lt;p&gt;How tab content views are managed when switching tabs&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;.recreateOnSwitch&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;newTabPosition&lt;/code&gt;
    &lt;code&gt;NewTabPosition&lt;/code&gt;
    &lt;p&gt;Where new tabs are inserted in the tab list&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;.current&lt;/code&gt;&lt;/p&gt;
    &lt;quote&gt;let config = BonsplitConfiguration(allowSplits: true,allowCloseTabs: true,allowCloseLastPane: false,autoCloseEmptyPanes: true,contentViewLifecycle: .keepAllAlive,newTabPosition: .current)let controller = BonsplitController(configuration: config)&lt;/quote&gt;
    &lt;p&gt;Controls how tab content views are managed when switching between tabs.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Memory&lt;/cell&gt;
        &lt;cell role="head"&gt;State&lt;/cell&gt;
        &lt;cell role="head"&gt;Use Case&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;.recreateOnSwitch&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Low&lt;/cell&gt;
        &lt;cell&gt;None&lt;/cell&gt;
        &lt;cell&gt;Simple content&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;.keepAllAlive&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Higher&lt;/cell&gt;
        &lt;cell&gt;Full&lt;/cell&gt;
        &lt;cell&gt;Complex views, forms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Controls where new tabs are inserted in the tab list.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Behavior&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;.current&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Insert after currently focused tab, or at end if none&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;.end&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Always insert at the end of the tab list&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;tabBarHeight&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Height of the tab bar&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;33&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;tabMinWidth&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Minimum width of a tab&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;140&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;tabMaxWidth&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Maximum width of a tab&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;220&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;tabSpacing&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Spacing between tabs&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;0&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;minimumPaneWidth&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Minimum width of a pane&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;100&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;minimumPaneHeight&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Minimum height of a pane&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;100&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;showSplitButtons&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Show split buttons in the tab bar&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;animationDuration&lt;/code&gt;
    &lt;code&gt;Double&lt;/code&gt;
    &lt;p&gt;Duration of animations in seconds&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;0.15&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;enableAnimations&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Enable or disable all animations&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;.default&lt;/code&gt;
    &lt;code&gt;BonsplitConfiguration&lt;/code&gt;
    &lt;p&gt;Default configuration with all features enabled&lt;/p&gt;
    &lt;code&gt;.singlePane&lt;/code&gt;
    &lt;code&gt;BonsplitConfiguration&lt;/code&gt;
    &lt;p&gt;Single pane mode with splits disabled&lt;/p&gt;
    &lt;code&gt;.readOnly&lt;/code&gt;
    &lt;code&gt;BonsplitConfiguration&lt;/code&gt;
    &lt;p&gt;Read-only mode with all modifications disabled&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46753301</guid><pubDate>Sun, 25 Jan 2026 11:56:42 +0000</pubDate></item><item><title>Nango (YC W23, Dev Infrastructure) Is Hiring Remotely</title><link>https://jobs.ashbyhq.com/Nango</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46753336</guid><pubDate>Sun, 25 Jan 2026 12:02:01 +0000</pubDate></item><item><title>Doom has been ported to an earbud</title><link>https://doombuds.com</link><description>&lt;doc fingerprint="a6567235022113ff"&gt;
  &lt;main&gt;
    &lt;p&gt;It's almost your turn, get ready!&lt;/p&gt;
    &lt;p&gt;Player queue&lt;/p&gt;
    &lt;p&gt;Your position&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;p&gt;Players queued&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;p&gt;Wait time&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;p&gt;You know the 1993 classic DOOM? I made it run on an earbud, then I connected it to the internet and made it possible for visitors like you to sit in a queue for hours play the game remotely!.&lt;/p&gt;
    &lt;p&gt;Yeah but it won't just run on any old earbud, this only works with the Pinebuds Pro, the only earbuds with open source firmware.&lt;/p&gt;
    &lt;p&gt;You sure can! There are two relevant repos:&lt;/p&gt;
    &lt;p&gt;This was a necessary optimisation to avoid paying outgoing bandwidth fees, once you're 5th in the queue, the twitch player will switch to a low-latency MJPEG stream.&lt;/p&gt;
    &lt;p&gt;shhhh don't look don't look it's ok just join the queue&lt;/p&gt;
    &lt;p&gt;Let's switch to a more readable font first.&lt;/p&gt;
    &lt;p&gt; I'll put out an article / video diving deeper into this later, but here are a few bits of info:&lt;lb/&gt; This project is made up of four parts: &lt;/p&gt;
    &lt;p&gt;The firmware pushes up against a few hardware limitations:&lt;/p&gt;
    &lt;p&gt; Earbuds don't have displays, so the only way to transfer data to/from them is either via bluetooth, or the UART contact pads.&lt;lb/&gt; Bluetooth is pretty slow, you'd be lucky to get a consistent 1mbps connection, UART is easily the better option.&lt;lb/&gt; DOOM's framebuffer is (width * height) bytes, 320 * 200 = 96kB. (doom's internal framebuffer is 8-bit not 24-bit)&lt;lb/&gt; The UART connection provides us with 2.4mbps of usable bandwidth. 2,400,000 / 8 / 96,000 gives us... 3 frames per second.&lt;lb/&gt; Clearly we need to compress the video stream. Modern video codecs like h264 consume way too much CPU and RAM.&lt;lb/&gt; The only feasible approach is sending the video as an MJPEG stream. MJPEG is a stream of JPEG images shown one after the other.&lt;lb/&gt; I found an excellent JPEG encoder for embedded devices here, thanks Larry!&lt;lb/&gt; A conservative estimate for the average HIGH quality JPEG frame is around 13.5KB, but most scenes (without enemies) are around 11kb.&lt;lb/&gt; Theoretical maximum FPS:&lt;lb/&gt; - Optimistic: `2,400,000 / (11,000 * 8)` = 27.3 FPS&lt;lb/&gt; - Conservative: `2,400,000 / (13,500 * 8)` = 22.2 FPS &lt;/p&gt;
    &lt;p&gt; The stock open source firmware has the CPU set to 100mhz, so I cranked that up to 300mhz and disabled low power mode.&lt;lb/&gt; The Cortex-M4F running at 300mhz is actually more than enough for DOOM, however it struggles with JPEG encoding.&lt;lb/&gt; This is why it maxes out at ~18fps, I don't think there's much else I can do to speed it up. &lt;/p&gt;
    &lt;p&gt; By default, we only have access to 768KB of RAM, after disabling the co-processor it gets bumped up to the advertised 992KB.&lt;lb/&gt; DOOM requires 4MB of RAM, though there are plenty of optimisations that can reduce this amount.&lt;lb/&gt; Pre-generating lookup tables, making variables const, reading const variables from flash, disabling DOOM's caching system, removing unneeded variables. It all adds up! &lt;/p&gt;
    &lt;p&gt; The shareware DOOM 1 wad (assets file) is 4.2MB and the earbuds can only store 4MB of data.&lt;lb/&gt; Thankfully, fragglet, a well-known doom modder, has already solved this issue for me.&lt;lb/&gt; Squashware is his trimmed-down DOOM 1 wad that is only 1.7MB in size.&lt;lb/&gt; With this wad file, everything comfortably fits in flash. &lt;/p&gt;
    &lt;p&gt;I thought you'd never ask! (please hire me)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46753484</guid><pubDate>Sun, 25 Jan 2026 12:22:12 +0000</pubDate></item><item><title>Web-based image editor modeled after Deluxe Paint</title><link>https://github.com/steffest/DPaint-js</link><description>&lt;doc fingerprint="56223e185d452d87"&gt;
  &lt;main&gt;
    &lt;p&gt;Webbased image editor modeled after the legendary Deluxe Paint with a focus on retro Amiga file formats. Next to modern image formats, DPaint.js can read and write Amiga icon files and IFF ILBM images.&lt;/p&gt;
    &lt;p&gt;Online version available at https://www.stef.be/dpaint/&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fully Featured image editor with a.o. &lt;list rend="ul"&gt;&lt;item&gt;Layers&lt;/item&gt;&lt;item&gt;Selections&lt;/item&gt;&lt;item&gt;Masking&lt;/item&gt;&lt;item&gt;Transformation tools&lt;/item&gt;&lt;item&gt;Effects and filters&lt;/item&gt;&lt;item&gt;Multiple undo/redo&lt;/item&gt;&lt;item&gt;Copy/Paste from any other image program or image source&lt;/item&gt;&lt;item&gt;Customizable dither tools&lt;/item&gt;&lt;item&gt;Color Cycling&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Heavy focus on colour reduction with fine-grained dithering options&lt;/item&gt;
      &lt;item&gt;Amiga focus &lt;list rend="ul"&gt;&lt;item&gt;Read/write/convert Amiga icon files (all formats)&lt;/item&gt;&lt;item&gt;Reads IFF ILBM images (all formats including HAM and 24-bit)&lt;/item&gt;&lt;item&gt;Writes IFF ILBM images (up to 256 colors)&lt;/item&gt;&lt;item&gt;Read and write directly from Amiga Disk Files (ADF)&lt;/item&gt;&lt;item&gt;Embedded Amiga Emulator to preview your work in the real Deluxe Paint.&lt;/item&gt;&lt;item&gt;Limit the palette to 12 bit for Amiga OCS/ECS mode, or 9 bit for Atari ST mode.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Deluxe Paint Legacy &lt;list rend="ul"&gt;&lt;item&gt;Supports PBM files as used by the PC version of Deluxe Paint (Thanks to Michael Smith)&lt;/item&gt;&lt;item&gt;Supports Deluxe Paint Atari ST compression modes (Thanks to Nicolas Ramz)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It runs in your browser, works on any system and works fine on touch-screen devices like iPads.&lt;lb/&gt; It is written in 100% plain JavaScript and has no dependencies.&lt;lb/&gt; It's 100% free, no ads, no tracking, no accounts, no nothing.&lt;lb/&gt; All processing is done in your browser, no data is sent to any server.&lt;/p&gt;
    &lt;p&gt;The only part that is not included in this repository is the Amiga Emulator Files. (The emulator is based on the Scripted Amiga Emulator)&lt;/p&gt;
    &lt;p&gt;DPaint.js doesn't need building.&lt;lb/&gt; It also has zero dependencies so there's no need to install anything.&lt;lb/&gt; DPaint.js is written using ES6 modules and runs out of the box in modern browsers.&lt;lb/&gt; Just serve "index.html" from a webserver and you're good to go.&lt;/p&gt;
    &lt;p&gt;There's an optional build step to create a compact version of DPaint.js if you like.&lt;lb/&gt; I'm using Parcel.js for this.&lt;lb/&gt; For convenience, I've included a "package.json" file.&lt;lb/&gt; open a terminal and run &lt;code&gt;npm install&lt;/code&gt; to install Parcel.js and its dependencies.
Then run &lt;code&gt;npm run build&lt;/code&gt; to create a compact version of DPaint.js in the "dist" folder.&lt;/p&gt;
    &lt;p&gt;Documentation can be found at https://www.stef.be/dpaint/docs/&lt;/p&gt;
    &lt;p&gt;Dpaint.js is a web application, not an app that you install on your computer. That being said: DPaint.js has no online dependencies and runs fine offline if you want. One caveat: you have to serve the index.html file from a webserver, not just open it in your browser.&lt;lb/&gt; A quick way to do this is - for example - using the Spark app.&lt;lb/&gt; Download the binary for your platform, drop the Spark executable in the folder where you downloaded the Dpaint.js source files and run it. If you then point your browser to http://localhost:8080/ it should work.&lt;/p&gt;
    &lt;p&gt;If you are using Chrome, you can also "install" dpaint.js as app.&lt;lb/&gt; It will then show up your Chrome apps and work offline.&lt;/p&gt;
    &lt;p&gt;Current version is still alpha.&lt;lb/&gt; I'm sure there are bugs and missing features.&lt;lb/&gt; Bug reports and pull requests are welcome.&lt;/p&gt;
    &lt;p&gt;Planned for the next release, already in the works:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;del rend="overstrike"&gt;Color Cycling&lt;/del&gt;(done)&lt;/item&gt;
      &lt;item&gt;&lt;del rend="overstrike"&gt;Animation support (GIf and Amiga ANIM files)&lt;/del&gt;(done)&lt;/item&gt;
      &lt;item&gt;&lt;del rend="overstrike"&gt;Shading/transparency tools that stay within the palette.&lt;/del&gt;(done)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Planned for a future release if there's a need for it.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Support for non-square pixel modes such as HiRes and Interlaced&lt;/item&gt;
      &lt;item&gt;PSD import and export&lt;/item&gt;
      &lt;item&gt;SpriteSheet support&lt;/item&gt;
      &lt;item&gt;Write HAM,SHAM and Dynamic HiRes images&lt;/item&gt;
      &lt;item&gt;Commodore 64 graphics modes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please note that the Brave browser is using "farbling" that introduces random image noise in certain conditions. They claim this is to protect your privacy. Although I totally understand the sentiment, In my opinion a browser should not actively alter the content of a webpage or intentionally break functionality.&lt;lb/&gt; But hey, who am I to speak, it's a free world. Just be aware that if you are using Brave, you will run into issues, so please "lower your shields" for this app in Brave or use another browser.&lt;/p&gt;
    &lt;p&gt;Dpaint.js supports Color-Cycling - a long lost art of "animating" a static image by only rotating some colors in the palette. See an example here:&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;The_Vision_cycle.mp4&lt;/head&gt;
    &lt;p&gt;Open the layered source file of the above image directly in Dpaint.js&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46753708</guid><pubDate>Sun, 25 Jan 2026 12:54:53 +0000</pubDate></item><item><title>A macOS app that blurs your screen when you slouch</title><link>https://github.com/tldev/posturr</link><description>&lt;doc fingerprint="709cd7c437d6538d"&gt;
  &lt;main&gt;
    &lt;p&gt;A macOS app that blurs your screen when you slouch.&lt;/p&gt;
    &lt;p&gt;Posturr uses your Mac's camera and Apple's Vision framework to monitor your posture in real-time. When it detects that you're slouching, it progressively blurs your screen to remind you to sit up straight. Maintain good posture, and the blur clears instantly.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Real-time posture detection - Uses Apple's Vision framework for body pose and face tracking&lt;/item&gt;
      &lt;item&gt;Progressive screen blur - Gentle visual reminder that intensifies with worse posture&lt;/item&gt;
      &lt;item&gt;Menu bar controls - Easy access to settings, calibration, and status from the menu bar&lt;/item&gt;
      &lt;item&gt;Multi-display support - Works across all connected monitors&lt;/item&gt;
      &lt;item&gt;Privacy-focused - All processing happens locally on your Mac&lt;/item&gt;
      &lt;item&gt;Lightweight - Runs as a background app with minimal resource usage&lt;/item&gt;
      &lt;item&gt;No account required - No signup, no cloud, no tracking&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download the latest &lt;code&gt;Posturr-vX.X.X.zip&lt;/code&gt;from the Releases page&lt;/item&gt;
      &lt;item&gt;Unzip the downloaded file&lt;/item&gt;
      &lt;item&gt;Drag &lt;code&gt;Posturr.app&lt;/code&gt;to your Applications folder&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since Posturr is not signed with an Apple Developer certificate, macOS Gatekeeper will initially block it:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Right-click (or Control-click) on &lt;code&gt;Posturr.app&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Select "Open" from the context menu&lt;/item&gt;
      &lt;item&gt;Click "Open" in the dialog that appears&lt;/item&gt;
      &lt;item&gt;Grant camera access when prompted&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You only need to do this once. After the first launch, you can open Posturr normally.&lt;/p&gt;
    &lt;p&gt;Posturr requires camera access to monitor your posture. When you first launch the app, macOS will ask for permission. Click "OK" to grant access.&lt;/p&gt;
    &lt;p&gt;If you accidentally denied permission, you can grant it later:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Open System Settings &amp;gt; Privacy &amp;amp; Security &amp;gt; Camera&lt;/item&gt;
      &lt;item&gt;Find Posturr and enable the toggle&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once launched, Posturr appears in your menu bar with a person icon. The app continuously monitors your posture and applies screen blur when slouching is detected.&lt;/p&gt;
    &lt;p&gt;Click the menu bar icon to access:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Status - Shows current state (Monitoring, Slouching, Good Posture, etc.)&lt;/item&gt;
      &lt;item&gt;Enabled - Toggle posture monitoring on/off&lt;/item&gt;
      &lt;item&gt;Recalibrate - Reset your baseline posture (sit up straight, then click)&lt;/item&gt;
      &lt;item&gt;Sensitivity - Adjust how sensitive the slouch detection is (Low, Medium, High, Very High)&lt;/item&gt;
      &lt;item&gt;Dead Zone - Set the tolerance before blur kicks in (None, Small, Medium, Large)&lt;/item&gt;
      &lt;item&gt;Compatibility Mode - Use public macOS APIs for blur (try this if blur doesn't appear)&lt;/item&gt;
      &lt;item&gt;Quit - Exit the application (or press Escape anywhere)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Position your camera at eye level when possible&lt;/item&gt;
      &lt;item&gt;Ensure adequate lighting on your face&lt;/item&gt;
      &lt;item&gt;Sit at a consistent distance from your screen&lt;/item&gt;
      &lt;item&gt;The app works best when your shoulders are visible&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Posturr uses Apple's Vision framework to detect body pose landmarks:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Body Pose Detection: Tracks nose, shoulders, and their relative positions&lt;/item&gt;
      &lt;item&gt;Face Detection Fallback: When full body isn't visible, tracks face position&lt;/item&gt;
      &lt;item&gt;Posture Analysis: Measures the vertical distance between nose and shoulders&lt;/item&gt;
      &lt;item&gt;Blur Response: Applies screen blur proportional to posture deviation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The screen blur uses macOS's private CoreGraphics API by default for efficient, system-level blur. If the blur doesn't appear on your system, enable Compatibility Mode from the menu to use &lt;code&gt;NSVisualEffectView&lt;/code&gt; instead.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS 13.0 (Ventura) or later&lt;/item&gt;
      &lt;item&gt;Xcode Command Line Tools (&lt;code&gt;xcode-select --install&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/yourusername/posturr.git
cd posturr
./build.sh&lt;/code&gt;
    &lt;p&gt;The built app will be in &lt;code&gt;build/Posturr.app&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;# Standard build
./build.sh

# Build with release archive (.zip)
./build.sh --release&lt;/code&gt;
    &lt;code&gt;swiftc -O \
    -framework AppKit \
    -framework AVFoundation \
    -framework Vision \
    -framework CoreImage \
    -o Posturr \
    main.swift&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No code signing: Requires manual Gatekeeper bypass on first launch&lt;/item&gt;
      &lt;item&gt;Camera dependency: Requires a working camera with adequate lighting&lt;/item&gt;
      &lt;item&gt;Detection accuracy: Works best with clear view of upper body/face&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Posturr exposes a file-based command interface for external control:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Command&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;capture&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Take a photo and analyze pose&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;blur &amp;lt;0-64&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Set blur level manually&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;quit&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Exit the application&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Write commands to &lt;code&gt;/tmp/posturr-command&lt;/code&gt;. Responses appear in &lt;code&gt;/tmp/posturr-response&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS 13.0 (Ventura) or later&lt;/item&gt;
      &lt;item&gt;Camera (built-in or external)&lt;/item&gt;
      &lt;item&gt;Approximately 10MB disk space&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Posturr processes all video data locally on your Mac. No images or data are ever sent to external servers. The camera feed is used solely for posture detection and is never stored or transmitted.&lt;/p&gt;
    &lt;p&gt;MIT License - see LICENSE for details.&lt;/p&gt;
    &lt;p&gt;Contributions are welcome! Please feel free to submit issues and pull requests.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Built with Apple's Vision framework for body pose detection&lt;/item&gt;
      &lt;item&gt;Uses private CoreGraphics API for blur, with NSVisualEffectView fallback&lt;/item&gt;
      &lt;item&gt;Inspired by the need for better posture during long coding sessions&lt;/item&gt;
      &lt;item&gt;Thanks to @wklm for the compatibility mode implementation&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46754944</guid><pubDate>Sun, 25 Jan 2026 15:34:51 +0000</pubDate></item><item><title>Using PostgreSQL as a Dead Letter Queue for Event-Driven Systems</title><link>https://www.diljitpr.net/blog-post-postgresql-dlq</link><description>&lt;doc fingerprint="3f4c263785a28d80"&gt;
  &lt;main&gt;
    &lt;p&gt;While I was working on a project with Wayfair, I got the opportunity to work on a system that generated daily business reports aggregated from multiple data sources flowing through event streams across Wayfair. At a high level, Kafka consumers listened to these events, hydrated them with additional data by calling downstream services, and finally persisted the enriched events into a durable datastoreâCloudSQL PostgreSQL on GCP.&lt;/p&gt;
    &lt;p&gt;When everything was healthy, the pipeline worked exactly as expected. Events flowed in, got enriched, and were stored reliably. The real challenge started when things went wrong, which, in distributed systems, is not an exception but a certainty.&lt;/p&gt;
    &lt;p&gt;There were multiple failure scenarios we had to deal with. Sometimes the APIs we depended on for hydration were down or slow. Sometimes the consumer itself crashed midway through processing. In other cases, events arrived with missing or malformed fields that could not be processed safely. These were all situations outside our direct control, but they still needed to be handled gracefully.&lt;/p&gt;
    &lt;p&gt;This is where the concept of a Dead Letter Queue came into the picture. Whenever we knew an event could not be processed successfully, instead of dropping it or blocking the entire consumer, we redirected it to a DLQ so it could be inspected and potentially reprocessed later.&lt;/p&gt;
    &lt;p&gt;Our first instinct was to use Kafka itself as a DLQ. While this is a common pattern, it quickly became clear that it wasn't a great fit for our needs. Kafka is excellent for moving data, but once messages land in a DLQ topic, they are not particularly easy to inspect. Querying by failure reason, retrying a specific subset of events, or even answering simple questions like "what failed yesterday and why?" required extra tooling and custom consumers. For a system that powered business-critical daily reports, this lack of visibility was a serious drawback.&lt;/p&gt;
    &lt;p&gt;That's when we decided to treat PostgreSQL itself as the Dead Letter Queue.&lt;/p&gt;
    &lt;p&gt;Instead of publishing failed events to another Kafka topic, we persisted them directly into a DLQ table in PostgreSQL. We were already using CloudSQL as our durable store, so operationally this added very little complexity. Conceptually, it also made failures first-class citizens in the system rather than opaque messages lost in a stream.&lt;/p&gt;
    &lt;p&gt; Whenever an event failed processingâdue to an API failure, consumer crash, schema mismatch, or validation errorâwe stored the raw event payload along with contextual information about the failure. Each record carried a simple status field. When the event first landed in the DLQ, it was marked as &lt;code&gt;PENDING&lt;/code&gt;. Once it was successfully reprocessed, the status was updated to &lt;code&gt;SUCCEEDED&lt;/code&gt;. Keeping the state model intentionally minimal made it easy to reason about the lifecycle of a failed event.
                    &lt;/p&gt;
    &lt;head rend="h3"&gt;DLQ Table Schema and Indexing Strategy&lt;/head&gt;
    &lt;p&gt;To support inspection, retries, and long-term operability, the DLQ table was designed to be simple, query-friendly, and retry-aware.&lt;/p&gt;
    &lt;head rend="h4"&gt;Table Schema&lt;/head&gt;
    &lt;code&gt;CREATE TABLE dlq_events (
    id BIGSERIAL PRIMARY KEY,
    event_type VARCHAR(255) NOT NULL,
    payload JSONB NOT NULL,
    error_reason TEXT NOT NULL,
    error_stacktrace TEXT,
    status VARCHAR(20) NOT NULL, -- PENDING / SUCCEEDED
    retry_count INT NOT NULL DEFAULT 0,
    retry_after TIMESTAMP WITH TIME ZONE NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW()
);&lt;/code&gt;
    &lt;head rend="h4"&gt;Key Design Considerations&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;payload&lt;/code&gt;is stored as&lt;code&gt;JSONB&lt;/code&gt;to preserve the raw event without enforcing a rigid schema.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;status&lt;/code&gt;keeps the lifecycle simple and explicit.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;retry_after&lt;/code&gt;prevents aggressive retries when downstream systems are unstable.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;retry_count&lt;/code&gt;allows retry limits to be enforced without external state.&lt;/item&gt;
      &lt;item&gt;Timestamps make auditing and operational analysis straightforward.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Indexes&lt;/head&gt;
    &lt;code&gt;CREATE INDEX idx_dlq_status
ON dlq_events (status);

CREATE INDEX idx_dlq_status_retry_after
ON dlq_events (status, retry_after);

CREATE INDEX idx_dlq_event_type
ON dlq_events (event_type);

CREATE INDEX idx_dlq_created_at
ON dlq_events (created_at);&lt;/code&gt;
    &lt;p&gt;These indexes allow the retry scheduler to efficiently locate eligible events while still supporting fast debugging and time-based analysis without full table scans.&lt;/p&gt;
    &lt;head rend="h3"&gt;DLQ Retry Mechanism with ShedLock&lt;/head&gt;
    &lt;p&gt;Persisting failed events solved the visibility problem, but we still needed a safe and reliable way to retry them.&lt;/p&gt;
    &lt;p&gt; For this, we introduced a DLQ retry scheduler backed by ShedLock. The scheduler periodically scans the DLQ table for &lt;code&gt;PENDING&lt;/code&gt; events that are eligible for retry and attempts to process them again. Since the service runs on multiple instances, ShedLock ensures that only one instance executes the retry job at any given time. This eliminates duplicate retries without requiring custom leader-election logic.
                    &lt;/p&gt;
    &lt;head rend="h4"&gt;Retry Configuration&lt;/head&gt;
    &lt;code&gt;dlq:
  retry:
    enabled: true
    max-retries: 240
    batch-size: 50
    fixed-rate: 21600000 # 6 hours in milliseconds&lt;/code&gt;
    &lt;head rend="h4"&gt;How Retries Work&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The scheduler runs every six hours.&lt;/item&gt;
      &lt;item&gt;Up to fifty eligible events are picked up per run.&lt;/item&gt;
      &lt;item&gt;Events exceeding the maximum retry count are skipped.&lt;/item&gt;
      &lt;item&gt;Successful retries immediately transition the event status to &lt;code&gt;SUCCEEDED&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Failures remain in &lt;code&gt;PENDING&lt;/code&gt;and are retried in subsequent runs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Query Implementation&lt;/head&gt;
    &lt;p&gt; The retry scheduler uses a SQL query with &lt;code&gt;FOR UPDATE SKIP LOCKED&lt;/code&gt; to safely select eligible events across multiple instances. This PostgreSQL feature ensures that even if multiple scheduler instances run simultaneously, each will pick up different rows without blocking each other:
                    &lt;/p&gt;
    &lt;code&gt;@QueryHints(@QueryHint(name = "jakarta.persistence.lock.timeout", value = "-2"))
@Query(
    value = "SELECT * FROM dlq_table "
        + "WHERE messagetype = :messageType "
        + "AND retries &amp;lt; :maxRetries "
        + "AND (replay_status IS NULL OR replay_status NOT IN ('COMPLETED')) "
        + "ORDER BY created_at ASC "
        + "FOR UPDATE SKIP LOCKED",
    nativeQuery = true
)&lt;/code&gt;
    &lt;p&gt; The &lt;code&gt;FOR UPDATE SKIP LOCKED&lt;/code&gt; clause is crucial here. It allows each instance to lock and process different rows concurrently, preventing duplicate processing while maintaining high throughput. The query hint sets the lock timeout to &lt;code&gt;-2&lt;/code&gt;, which means "wait indefinitely" but combined with &lt;code&gt;SKIP LOCKED&lt;/code&gt;, it effectively means "skip any rows that are already locked by another transaction."
                    &lt;/p&gt;
    &lt;p&gt;This setup allowed the system to tolerate long downstream outages while avoiding retry storms and unnecessary load on dependent services.&lt;/p&gt;
    &lt;head rend="h3"&gt;Operational Benefits&lt;/head&gt;
    &lt;p&gt;With this approach, failures became predictable and observable rather than disruptive. Engineers could inspect failures using plain SQL, identify patterns, and reprocess only the events that mattered. If a downstream dependency was unavailable for hours or even days, events safely accumulated in the DLQ and were retried later without human intervention. If an event was fundamentally bad, it stayed visible instead of being silently dropped.&lt;/p&gt;
    &lt;p&gt;Most importantly, this design reduced operational stress. Failures were no longer something to fear; they were an expected part of the system with a clear, auditable recovery path.&lt;/p&gt;
    &lt;head rend="h3"&gt;My Thoughts&lt;/head&gt;
    &lt;p&gt;The goal was never to replace Kafka with PostgreSQL. Kafka remained the backbone for high-throughput event ingestion, while PostgreSQL handled what it does bestâdurability, querying, and observability around failures. By letting each system play to its strengths, we ended up with a pipeline that was resilient, debuggable, and easy to operate.&lt;/p&gt;
    &lt;p&gt;In the end, using PostgreSQL as a Dead Letter Queue turned failure handling into something boring and predictable. And in production systems, boring is exactly what you want.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46755115</guid><pubDate>Sun, 25 Jan 2026 15:51:03 +0000</pubDate></item><item><title>FAA institutes nationwide drone no-fly zones around ICE operations</title><link>https://www.aerotime.aero/articles/faa-drone-no-fly-zone-ice-dhs</link><description>&lt;doc fingerprint="5c9064d05e9c8590"&gt;
  &lt;main&gt;
    &lt;p&gt;The Federal Aviation Administration has issued a nationwide security notice, effectively creating a moving drone no-fly zone around operations conducted by Immigration and Customs Enforcement (ICE) and other components of the Department of Homeland Security.&lt;/p&gt;
    &lt;p&gt;The notice, NOTAM FDC 6/4375, prohibits unmanned aircraft systems from operating within 3,000 feet laterally and 1,000 feet vertically of DHS facilities and mobile assets, including ground vehicle convoys and their escorts. The restriction applies nationwide and continuously, rather than at fixed locations or during defined time windows.&lt;/p&gt;
    &lt;p&gt;Because ICE operates under DHS and routinely conducts enforcement actions using mobile vehicle convoys in public spaces, the restriction functions as a drone no-fly zone around ICE operations, including arrests, transport activities and other field actions.&lt;/p&gt;
    &lt;p&gt;The FAA classifies the restricted airspace as “national defense airspace,” and cites its authority under federal security statutes. Drone operators who violate the restriction may face criminal prosecution, civil penalties, administrative enforcement actions, or revocation of FAA operating privileges. The notice also states that drones deemed a credible security threat may be intercepted, seized, damaged, or destroyed.&lt;/p&gt;
    &lt;p&gt;Unlike traditional Temporary Flight Restrictions, the NOTAM does not provide geographic coordinates, activation times, or public notification when the restriction is in effect near a specific location. Instead, the restricted airspace moves with DHS assets, meaning the no-fly zone can appear wherever ICE or other DHS units operate.&lt;/p&gt;
    &lt;p&gt;The new NOTAM replaces an earlier security notice, FDC 5/6378, which covered similar federal agencies but was less explicit about mobile operations. The updated version removes ambiguity by clearly stating that the restriction applies to moving DHS assets, including vehicles and convoys, and not just fixed facilities such as offices or bases.&lt;/p&gt;
    &lt;p&gt;That clarification has drawn attention from drone operators and civil liberties groups because it creates dynamic, invisible exclusion zones that may be impossible to identify in real time. The FAA does not publish public tracking of DHS or ICE movements, and the NOTAM does not include a mechanism for drone pilots to determine when covered assets are nearby.&lt;/p&gt;
    &lt;p&gt;In practical terms, a drone operator flying legally in a public area could unknowingly enter restricted airspace if an ICE convoy passes within the protected radius. The FAA instructs operators to “exercise caution” when flying near DHS facilities and mobile assets, but offers no specific guidance on how to do so in environments where enforcement activity is not publicly disclosed.&lt;/p&gt;
    &lt;p&gt;The notice mentions limited exceptions. Drone operations conducted in direct support of national defense, homeland security, law enforcement, firefighting, search and rescue, or disaster response missions may be authorized with advance coordination. Operators seeking approval are instructed to coordinate with DHS or other covered agencies, or contact the FAA’s System Operations Support Center.&lt;/p&gt;
    &lt;p&gt;The FAA cites multiple federal statutes as the legal basis for the restriction, including laws governing national defense airspace and counter-UAS mitigation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46755999</guid><pubDate>Sun, 25 Jan 2026 17:24:42 +0000</pubDate></item><item><title>ICE using Palantir tool that feeds on Medicaid data</title><link>https://www.eff.org/deeplinks/2026/01/report-ice-using-palantir-tool-feeds-medicaid-data</link><description>&lt;doc fingerprint="ad964554d5aa1139"&gt;
  &lt;main&gt;
    &lt;p&gt;EFF last summer asked a federal judge to block the federal government from using Medicaid data to identify and deport immigrants.&lt;/p&gt;
    &lt;p&gt;We also warned about the danger of the Trump administration consolidating all of the government’s information into a single searchable, AI-driven interface with help from Palantir, a company that has a shaky-at-best record on privacy and human rights.&lt;/p&gt;
    &lt;p&gt;Now we have the first evidence that our concerns have become reality.&lt;/p&gt;
    &lt;p&gt;“Palantir is working on a tool for Immigration and Customs Enforcement (ICE) that populates a map with potential deportation targets, brings up a dossier on each person, and provides a “confidence score” on the person’s current address,” 404 Media reports today. “ICE is using it to find locations where lots of people it might detain could be based.”&lt;/p&gt;
    &lt;p&gt;The tool – dubbed Enhanced Leads Identification &amp;amp; Targeting for Enforcement (ELITE) – receives peoples’ addresses from the Department of Health and Human Services (which includes Medicaid) and other sources, 404 Media reports based on court testimony in Oregon by law enforcement agents, among other sources.&lt;/p&gt;
    &lt;p&gt;This revelation comes as ICE – which has gone on a surveillance technology shopping spree – floods Minneapolis with agents, violently running roughshod over the civil rights of immigrants and U.S. citizens alike; President Trump has threatened to use the Insurrection Act of 1807 to deploy military troops against protestors there. Other localities are preparing for the possibility of similar surges.&lt;/p&gt;
    &lt;p&gt;Different government agencies necessarily collect information to provide essential services or collect taxes, but the danger comes when the government begins pooling that data and using it for reasons unrelated to the purpose it was collected.&lt;/p&gt;
    &lt;p&gt;This kind of consolidation of government records provides enormous government power that can be abused. Different government agencies necessarily collect information to provide essential services or collect taxes, but the danger comes when the government begins pooling that data and using it for reasons unrelated to the purpose it was collected.&lt;/p&gt;
    &lt;p&gt;As EFF Executive Director Cindy Cohn wrote in a Mercury News op-ed last August, “While couched in the benign language of eliminating government ‘data silos,’ this plan runs roughshod over your privacy and security. It’s a throwback to the rightly mocked ‘Total Information Awareness’ plans of the early 2000s that were, at least publicly, stopped after massive outcry from the public and from key members of Congress. It’s time to cry out again.”&lt;/p&gt;
    &lt;p&gt;In addition to the amicus brief we co-authored challenging ICE’s grab for Medicaid data, EFF has successfully sued over DOGE agents grabbing personal data from the U.S. Office of Personnel Management, filed an amicus brief in a suit challenging ICE’s grab for taxpayer data, and sued the departments of State and Homeland Security to halt a mass surveillance program to monitor constitutionally protected speech by noncitizens lawfully present in the U.S.&lt;/p&gt;
    &lt;p&gt;But litigation isn’t enough. People need to keep raising concerns via public discourse and Congress should act immediately to put brakes on this runaway train that threatens to crush the privacy and security of each and every person in America.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46756117</guid><pubDate>Sun, 25 Jan 2026 17:36:19 +0000</pubDate></item><item><title>First, make me care</title><link>https://gwern.net/blog/2026/make-me-care</link><description>&lt;doc fingerprint="77b044a15c934274"&gt;
  &lt;main&gt;
    &lt;p&gt;First, Make Me Care Writing advice: some nonfiction fails because it opens with background instead of a hook—readers leave before reaching the good material. Find the single anomaly or question that makes your topic interesting, lead with that, and let the background follow once you’ve earned attention. [Return to blog index]&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46757067</guid><pubDate>Sun, 25 Jan 2026 19:03:40 +0000</pubDate></item><item><title>Spanish track was fractured before high-speed train disaster, report finds</title><link>https://www.bbc.com/news/articles/c1m77dmxlvlo</link><description>&lt;doc fingerprint="ae6089bcbd13d29b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Spanish track was fractured before high-speed train disaster, report finds&lt;/head&gt;
    &lt;p&gt;A fracture in a straight section of track "occurred prior to the passage" of a high-speed train that derailed, causing last Sunday's rail disaster in which 45 people died, an initial report has found.&lt;/p&gt;
    &lt;p&gt;A train run by private company Iryo derailed last Sunday and its rear carriages crossed on to the opposite track into the path of an oncoming train run by state-owned Renfe.&lt;/p&gt;
    &lt;p&gt;The CIAF rail investigation commission said not only did Iryo train's front carriages which stayed on the track have "notches" in their wheels, but three earlier trains that went over the track earlier did too.&lt;/p&gt;
    &lt;p&gt;A gap of almost 40cm (15in) in the track has become the focus of the investigation into the crash.&lt;/p&gt;
    &lt;p&gt;Sunday's deadly collision occurred at around 19:45 local time (18:45 GMT), about an hour after the Iryo train left Málaga for Madrid.&lt;/p&gt;
    &lt;p&gt;The train's last three carriages - carriages six to eight - derailed and collided with the Huelva-bound Renfe train. "Carriage six derailed due to a complete lack of continuity in the track," the preliminary report finds.&lt;/p&gt;
    &lt;p&gt;Most of those killed and injured were in the front carriages of the state-operated train.&lt;/p&gt;
    &lt;p&gt;Earlier this week, Spanish Transport Minister Óscar Puente confirmed reports that grooves were found on the wheels of the Iryo train's carriages, which had passed over the track safely.&lt;/p&gt;
    &lt;p&gt;"These notches in the wheels and the deformation observed in the track are compatible with the fact that the track was cracked," the CIAF preliminary report said.&lt;/p&gt;
    &lt;p&gt;It added that three trains that had gone over the tracks at 17:21 on Sunday, 19:01 and then 19:09 had similar notches "with a compatible geometric pattern".&lt;/p&gt;
    &lt;p&gt;Similar grooves are found on carriages two, three and four of the Iryo train, the report says, but carriage five - the last that did not derail - had a groove on its outer edge, suggesting the rail was already tilting outwards before carriage six derailed.&lt;/p&gt;
    &lt;p&gt;The CIAF called its report a "working hypothesis", adding that it "must be corroborated by later detailed calculations and analysis".&lt;/p&gt;
    &lt;p&gt;The transport minister appeared before reporters again on Friday to say that it was too early to have definitive answers, but that if the cause of the crash was the fracture, then it occurred in the minutes and hours before the derailment and could not have been detected.&lt;/p&gt;
    &lt;p&gt;The Adamuz disaster is is the country's worst rail crash in more than a decade.&lt;/p&gt;
    &lt;p&gt;In 2013, Spain suffered its worst high-speed train derailment in Galicia, north-west Spain, which left 80 people dead and 140 others injured.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46757162</guid><pubDate>Sun, 25 Jan 2026 19:12:50 +0000</pubDate></item><item><title>I was right about ATProto key management</title><link>https://notes.nora.codes/atproto-again/</link><description>&lt;doc fingerprint="eebcc91d3d2314dc"&gt;
  &lt;main&gt;
    &lt;p&gt;So, a while ago, I wrote a post called “Key Management, ATProto, and Decentralization” in which I complained about ATProto’s approach to decentralization. Since then, Blacksky has spun up an AppView, which makes it theoretically possible to have an actually decentralized experience on Bluesky. This was my line in the sand, stated many times; I would make an account when and only when it was possible to do so without using anything running on Bluesky-the-company’s hardware.&lt;/p&gt;
    &lt;p&gt;So, today, I tried that. Let’s walk through the process:&lt;/p&gt;
    &lt;p&gt;Set up the PDS software on a server I control. Because I use NixOS, this was basically trivial.&lt;/p&gt;
    &lt;p&gt;Create a did:web. This means creating a public-private keypair; I initially tried following this tutorial from Mai Lapyst, but it’s very out of date, and doesn’t include a critical step, as we’ll see.&lt;/p&gt;
    &lt;p&gt;With that did:web, upload the &lt;code&gt;did.json&lt;/code&gt; document to my webserver and set the appropriate DNS entries. Easy enough, except that I also had to set the CORS header for the &lt;code&gt;did.json&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Create an account on my shiny new PDS. I was able to get an invite and create an account, but it was in the “deactivated” status, and I couldn’t activate it. It was very frustrating, because I was making requests manually with &lt;code&gt;curl&lt;/code&gt; and reading the error outcomes in the PDS’s logs on my server.&lt;/p&gt;
    &lt;p&gt;Oh, and by the way, none of this is documented. Sure, the individual endpoints are - kind of - but the only place the whole process is collected in one place is in the comments to this GitHub issue… which is closed as WONTFIX.&lt;/p&gt;
    &lt;p&gt;Seek help in the ATProto Touchers Discord server, and at their advice delete the account (foreshadowing!).&lt;/p&gt;
    &lt;p&gt;Start over and re-create everything from scratch, finally noticing the comment line in the comment on the closed GitHub issue telling me to replace the public key in my DID with the public key from &lt;code&gt;getRecommendedDidCredentials&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The documentation for that endpoint, by the way, reads in full:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Describe the credentials that should be included in the DID doc of an account that is migrating to this service.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Note that I am not “migrating”; this account is new. Plus, the JSON keys it returns are almost, but not quite, the same as those in a DID document, and the key it returns actually has to be edited by hand in order to be usable.&lt;/p&gt;
    &lt;p&gt;Update the DID document with the correct key.&lt;/p&gt;
    &lt;p&gt;Log into Bluesky (bsky.app) and see the brave new world of… never mind, I get a “Profile does not exist error.”&lt;/p&gt;
    &lt;p&gt;It was at this point that I found this GitHub issue, which seems to imply that, since I deleted my (completely empty and unused) account, my did:web is blacklisted from the remaining mostly-centralized bit of the system, the AppView. The term for this, apparently, is being “burned”.&lt;/p&gt;
    &lt;p&gt;Asking in the Discord again, I was told as much, and that my best bet is to reach out to support. Support from the company that I am supposedly free from using. Ugh!&lt;/p&gt;
    &lt;p&gt;I feel that this completely vindicates my view in my other post that asking users - even pretty technical ones - to handle their own PKI without any tooling or even really any documentation is completely untenable. As a long time Fedi admin friend of mine said, this is somehow harder and more error-prone than setting up a Mastodon instance in 2017 - and you expect each and every user to choose between this and centralization?&lt;/p&gt;
    &lt;p&gt;ATProto may be decentralized, but Bluesky - the social network and social protocol on top of it - is not, at least not in a way anyone can actually use.&lt;/p&gt;
    &lt;p&gt;If you’ve been drinking the coolaid, ask yourself this: why is a centralized “burn” able to completely prevent me from interacting with people using Bluesky?&lt;/p&gt;
    &lt;p&gt;It’s no secret that I don’t like ATProto, but I do actually want to use Bluesky. I find Blacksky really interesting, and I have several friends who post exclusively there. I would love to use Bluesky, with my own domain name, from my own PDS, but I’m not allowed to - because of a centralized authority.&lt;/p&gt;
    &lt;p&gt;You may be aware that Mastodon has a similar system. If you set up a Mastodon server and then delete the database, anyone you’ve already federated with won’t federate with you again, because you can’t prove you’re the same instance. It’s a genuine issue - but it wouldn’t have resulted in this, because I hadn’t even made a post on my now-burned did:web identity, nor followed anyone.&lt;/p&gt;
    &lt;p&gt;Even if I had, though, that would have burned a connection, not all connections. My experience would be degraded, but not ruined, and I could work with the admins of the affected servers to remediate it. You could say the same here, of course; maybe (maybe) support will get back to me some day, but it’s just that - support. There is only, really, one connection that matters; maybe two, if you count Blacksky, but their AppView is not generally available yet.&lt;/p&gt;
    &lt;p&gt;That’s centralization. I don’t understand how you could call it anything else.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46757357</guid><pubDate>Sun, 25 Jan 2026 19:31:23 +0000</pubDate></item><item><title>Data Leak Exposes 149M Logins, Including Gmail, Facebook</title><link>https://www.techrepublic.com/article/news-149-million-passwords-exposed-infostealer-database/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46757465</guid><pubDate>Sun, 25 Jan 2026 19:45:10 +0000</pubDate></item><item><title>OnePlus update blocks downgrades and custom ROMs by blowing a fuse</title><link>https://consumerrights.wiki/w/Oneplus_phone_update_introduces_hardware_anti-rollback</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46757944</guid><pubDate>Sun, 25 Jan 2026 20:39:25 +0000</pubDate></item><item><title>AI Tribalism</title><link>https://nolanlawson.com/2026/01/24/ai-tribalism/</link><description>&lt;doc fingerprint="7f6d1d56aeb484db"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;“Heartbreaking: The Worst Person You Know Just Made a Great Point” – ClickHole&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;“When the facts change, I change my mind. What do you do, sir?” – John Maynard Keynes, paraphrased&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;2025 was a weird year for me. If you had asked me exactly a year ago, I would have said I thought LLMs were amusing toys but inappropriate for real software development. I couldn’t fathom why people would want a hyperactive five-year-old to grab their keyboard every few seconds and barf some gobbledygook into their IDE that could barely compile.&lt;/p&gt;
    &lt;p&gt;Today, I would say that about 90% of my code is authored by Claude Code. The rest of the time, I’m mostly touching up its work or doing routine tasks that it’s slow at, like refactoring or renaming.&lt;/p&gt;
    &lt;p&gt;By now the battle lines have been drawn, and these arguments are getting pretty tiresome. Every day there’s a new thinkpiece on Hacker News about how either LLMs are the greatest thing ever or they’re going to destroy the world. I don’t write blog posts unless I think I have something new to contribute though, so here goes.&lt;/p&gt;
    &lt;p&gt;What I’ve noticed about a lot of these debates, especially if you spend a lot of time on Mastodon, Bluesky, or Lobsters, is that it’s devolved into politics. And since politics long ago devolved into tribalism, that means it’s become tribalism.&lt;/p&gt;
    &lt;p&gt;I remember when LLMs first exploded onto the scene a few years ago, and the same crypto bros who were previously hawking monkey JPEGs suddenly started singing the praises of AI. Meanwhile upper management got wind of it, and the message I got (even if they tried to use euphemisms, bless their hearts) was “you are expendable now, learn these tools so I can replace you.” In other words, the people whose opinions on programming I respected least were the ones eagerly jumping from the monkey JPEGs to these newfangled LLMs. So you can forgive me for being a touch cynical and skeptical at the start.&lt;/p&gt;
    &lt;p&gt;Around the same time, the smartest engineers I knew were maybe dabbling with LLMs, but overall unimpressed with the hallucinations, the bugs, and just the overall lousiness of these tools. I remember looking at the slow, buggy output of an IDE autocomplete and thinking, “I can type faster than this. And make fewer mistakes.”&lt;/p&gt;
    &lt;p&gt;Something changed in 2025, though. I’m not an expert on this stuff, so I have no idea if it was Opus 4.5 or reinforcement learning or just that Claude Code was so cleverly designed, but some threshold was reached. And I noticed that, more and more, it just didn’t make sense for me to type stuff out by hand (and I’m a very fast typist!) when I could just write a markdown spec, work with Claude in plan mode to refine it, and have it do the busywork.&lt;/p&gt;
    &lt;p&gt;Of course the bugs are still there. It still makes dumb mistakes. But then I open a PR, and Cursor Bugbot works its magic, and it finds bugs that I never would have thought of (even if I had written the code myself). Then I plug it back into Claude, it fixes it, and I start to wonder what the hell my job as a programmer even is anymore.&lt;/p&gt;
    &lt;p&gt;So that’s why, when I read about Steve Yegge’s Gas Town or Geoffrey Huntley’s Ralph loops (or this great overview by Anil Dash), I no longer brush it off as pure speculation or fantasy. I’ve seen what these tools can do, I’ve seen what happens when you lash together some very stupid barnyard animals and they’ve suddenly built the Pyramids, so I’m not surprised when smart engineers say that the solution to bad AI is to just add more AI. This is already working for me today (in my own little baby systems I’ve built), and I don’t have to imagine some sci-fi future to see what’s coming next.&lt;/p&gt;
    &lt;p&gt;The models don’t have to get better, the costs don’t have to come down (heck, they could even double and it’d still be worth it), and we don’t need another breakthrough. The breakthrough is already here; it just needs a bit more tinkering and it will become a giant lurching Frankenstein-meets-Akira-meets-the-Death-Star monster, cranking out working code from all 28 of its sub-agent tentacles.&lt;/p&gt;
    &lt;p&gt;I can already hear the cries of protest from other engineers who (like me) are clutching onto their hard-won knowledge. “What about security?” I’ve had agents find security vulnerabilities. “What about performance?” I’ve had agents write benchmarks, run them, and iterate on solutions. “What about accessibility?” Yeah they’re dumb at that – but if you say the magic word “accessibility,” and give them a browser to check their work, then suddenly they’re doing a better job than the median web dev (which isn’t saying much, but hey, it’s an improvement).&lt;/p&gt;
    &lt;p&gt;And honestly, even if all that doesn’t work, then you could probably just add more agents with different models to fact-check the other models. Inefficient? Certainly. Harming the planet? Maybe. But if it’s cheaper than a developer’s salary, and if it’s “good enough,” then the last half-century of software development suggests it’s bound to happen, regardless of which pearls you clutch.&lt;/p&gt;
    &lt;p&gt;I frankly didn’t want to end up in this future, and I’m hardly dancing on the grave of the old world. But I see a lot of my fellow developers burying their heads in the sand, refusing to acknowledge the truth in front of their eyes, and it breaks my heart because a lot of us are scared, confused, or uncertain, and not enough of us are talking honestly about it. Maybe it’s because the initial tribal battle lines have clouded everybody’s judgment, or maybe it’s because we inhabit different worlds where the technology is either better or worse (I still don’t think LLMs are great at UI for example), but there’s just a lot of patently unhelpful discourse out there, and I’m tired of it.&lt;/p&gt;
    &lt;p&gt;To me, the truth is this: between the hucksters selling you a ready-built solution, the doomsayers crying the end of software development, and the holdouts insisting that the entire house of cards is on the verge of collapsing – nobody knows anything. That’s the hardest truth to acknowledge, and maybe it’s why so many of us are scared or lashing out.&lt;/p&gt;
    &lt;p&gt;My advice (and I’ve already said I know nothing) would just be to experiment, tinker, and try to remain curious. It certainly feels to me like software development is unrecognizable from where it was 3 years ago, so I have no idea where it will be 3 years from now. It’s gonna be a bumpy ride for everyone, so just try have some empathy for your fellow passengers in the other tribe.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46758175</guid><pubDate>Sun, 25 Jan 2026 21:01:37 +0000</pubDate></item></channel></rss>