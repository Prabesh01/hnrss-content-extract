<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 13 Jan 2026 19:36:42 +0000</lastBuildDate><item><title>Mozilla's open source AI strategy</title><link>https://blog.mozilla.org/en/mozilla/mozilla-open-source-ai-strategy/</link><description>&lt;doc fingerprint="2d620319cfc594f8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Owners, not renters: Mozilla‚Äôs open source AI strategy&lt;/head&gt;
    &lt;p&gt;The future of intelligence is being set right now, and the path we‚Äôre on leads somewhere I don‚Äôt want to go. We‚Äôre drifting toward a world where intelligence is something you rent ‚Äî where your ability to reason, create, and decide flows through systems you don‚Äôt control, can‚Äôt inspect, and didn‚Äôt shape. In that world, the landlord can change the terms anytime, and you have no recourse but to accept what you‚Äôre given.&lt;/p&gt;
    &lt;p&gt;I think we can do better. Making that happen is now central to what Mozilla is doing.&lt;/p&gt;
    &lt;head rend="h1"&gt;What we did for the web&lt;/head&gt;
    &lt;p&gt;Twenty-five years ago, Microsoft Internet Explorer controlled 95% of the browser market, which meant Microsoft controlled how most people experienced the internet and who could build what on what terms. Mozilla was born to change this, and Firefox succeeded beyond what most people thought possible ‚Äî dropping Internet Explorer‚Äôs market share to 55% in just a few years and ushering in the Web 2.0 era. The result was a fundamentally different internet. It was faster and richer for everyday users, and for developers it was a launchpad for open standards and open source that decentralized control over the core technologies of the web.&lt;/p&gt;
    &lt;p&gt;There‚Äôs a reason the browser is called a ‚Äúuser agent.‚Äù It was designed to be on your side ‚Äî blocking ads, protecting your privacy, giving you choices that the sites you visited never would have offered on their own. That was the first fight, and we held the line for the open web even as social networks and mobile platforms became walled gardens.&lt;/p&gt;
    &lt;p&gt;Now AI is becoming the new intermediary. It‚Äôs what I‚Äôve started calling ‚ÄúLayer 8‚Äù ‚Äî the agentic layer that mediates between you and everything else on the internet. These systems will negotiate on our behalf, filter our information, shape our recommendations, and increasingly determine how we interact with the entire digital world.&lt;/p&gt;
    &lt;p&gt;The question we have to ask is straightforward: Whose side will your new user agent be on?&lt;/p&gt;
    &lt;head rend="h1"&gt;Why closed systems are winning (for now)&lt;/head&gt;
    &lt;p&gt;We need to be honest about the current state of play: Closed AI systems are winning today because they are genuinely easier to use. If you‚Äôre a developer with an idea you want to test, you can have a working prototype in minutes using a single API call to one of the major providers. GPUs, models, hosting, guardrails, monitoring, billing ‚Äî it all comes bundled together in a package that just works. I understand the appeal firsthand, because I‚Äôve made the same choice myself on late-night side projects when I just wanted the fastest path from an idea in my head to something I could actually play with.&lt;/p&gt;
    &lt;p&gt;The open-source AI ecosystem is a different story. It‚Äôs powerful and advancing rapidly, but it‚Äôs also deeply fragmented ‚Äî models live in one repository, tooling in another, and the pieces you need for evaluation, orchestration, guardrails, memory, and data pipelines are scattered across dozens of independent projects with different assumptions and interfaces. Each component is improving at remarkable speed, but they rarely integrate smoothly out of the box, and assembling a production-ready stack requires expertise and time that most teams simply don‚Äôt have to spare. This is the core challenge we face, and it‚Äôs important to name it clearly: What we‚Äôre dealing with isn‚Äôt a values problem where developers are choosing convenience over principle. It‚Äôs a developer experience problem. And developer experience problems can be solved.&lt;/p&gt;
    &lt;head rend="h1"&gt;The ground is already shifting&lt;/head&gt;
    &lt;p&gt;We‚Äôve watched this dynamic play out before and the history is instructive. In the early days of the personal computer, open systems were rough, inconsistent, and difficult to use, while closed platforms offered polish and simplicity that made them look inevitable. Openness won anyway ‚Äî not because users cared about principles, but because open systems unlocked experimentation and scale that closed alternatives couldn‚Äôt match. The same pattern repeated on the web, where closed portals like AOL and CompuServe dominated the early landscape before open standards outpaced them through sheer flexibility and the compounding benefits of broad participation.&lt;/p&gt;
    &lt;p&gt;AI has the potential to follow the same path ‚Äî but only if someone builds it. And several shifts are already reshaping the landscape:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Small models have gotten remarkably good. 1 to 8 billion parameters, tuned for specific tasks ‚Äî and they run on hardware that organizations already own;&lt;/item&gt;
      &lt;item&gt;The economics are changing too. As enterprises feel the constraints of closed dependencies, self-hosting is starting to look like sound business rather than ideological commitment (companies like Pinterest have attributed millions of dollars in savings to migrating to open-source AI infrastructure);&lt;/item&gt;
      &lt;item&gt;Governments want control over their supply chain. Governments are becoming increasingly unwilling to depend on foreign platforms for capabilities they consider strategically important, driving demand for sovereign systems; and,&lt;/item&gt;
      &lt;item&gt;Consumer expectations keep rising. People want AI that responds instantly, understands their context, and works across their tools without locking them into a single platform.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The capability gap that once justified the dominance of closed systems is closing fast. What remains is a gap in usability and integration. The lesson I take from history is that openness doesn‚Äôt win by being more principled than the alternatives. Openness wins when it becomes the better deal ‚Äî cheaper, more capable, and just as easy to use&lt;/p&gt;
    &lt;head rend="h1"&gt;Where the cracks are forming&lt;/head&gt;
    &lt;p&gt;If openness is going to win, it won‚Äôt happen everywhere at once. It will happen at specific tipping points ‚Äî places where the defaults haven‚Äôt yet hardened, where a well-timed push can change what becomes normal. We see four.&lt;/p&gt;
    &lt;p&gt;The first is developer experience. Developers are the ones who actually build the future ‚Äî every default they set, every stack they choose, every dependency they adopt shapes what becomes normal for everyone else. Right now, the fastest path runs through closed APIs, and that‚Äôs where most of the building is happening. But developers don‚Äôt want to be locked in any more than users do. Give them open tools that work as well as the closed ones, and they‚Äôll build the open ecosystem themselves.&lt;/p&gt;
    &lt;p&gt;The second is data. For a decade, the assumption has been that data is free to scrape ‚Äî that the web is a commons to be harvested without asking. That norm is breaking, and not a moment too soon. The people and communities who create valuable data deserve a say in how it‚Äôs used and a share in the value it creates. We‚Äôre moving toward a world of licensed, provenance-based, permissioned data. The infrastructure for that transition is still being built, which means there‚Äôs still a chance to build it right.&lt;/p&gt;
    &lt;p&gt;The third is models. The dominant architecture today favors only the biggest labs, because only they can afford to train massive dense transformers. But the edges are accelerating: small models, mixtures of experts, domain-specific models, multilingual models. As these approaches mature, the ability to create and customize intelligence spreads to communities, companies, and countries that were previously locked out.&lt;/p&gt;
    &lt;p&gt;The fourth is compute. This remains the choke point. Access to specialized hardware still determines who can train and deploy at scale. More doors need to open ‚Äî through distributed compute, federated approaches, sovereign clouds, idle GPUs finding productive use.&lt;/p&gt;
    &lt;head rend="h1"&gt;What an open stack could look like&lt;/head&gt;
    &lt;p&gt;Today‚Äôs dominant AI platforms are building vertically integrated stacks: closed applications on top of closed models trained on closed data, running on closed compute. Each layer reinforces the next ‚Äî data improves models, models improve applications, applications generate more data that only the platform can use. It‚Äôs a powerful flywheel. If it continues unchallenged, we arrive at an AI era equivalent to AOL, except far more centralized. You don‚Äôt build on the platform; you build inside it.&lt;/p&gt;
    &lt;p&gt;There‚Äôs another path. The sum of Linux, Apache, MySQL, and PHP won because that combination became easier to use than the proprietary alternatives, and because they let developers build things that no commercial platform would have prioritized. The web we have today exists because that stack existed.&lt;/p&gt;
    &lt;p&gt;We think AI can follow the same pattern. Not one stack controlled by any single party, but many stacks shaped by the communities, countries, and companies that use them:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Open developer interfaces at the top. SDKs, guardrails, workflows, and orchestration that don‚Äôt lock you into a single vendor;&lt;/item&gt;
      &lt;item&gt;Open data standards underneath. Provenance, consent, and portability built in by default, so you know where your training data came from and who has rights to it;&lt;/item&gt;
      &lt;item&gt;An open model ecosystem below that. Smaller, specialized, interchangeable models that you can inspect, tune to your values, and run where you need them; and&lt;/item&gt;
      &lt;item&gt;Open compute infrastructure at the foundation. Distributed and federated hardware across cloud and edge, not routed through a handful of hyperscn/lallers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Pieces of this stack already exist ‚Äî good ones, built by talented people. The task now is to fill in the gaps, connect what‚Äôs there, and make the whole thing as easy to use as the closed alternatives. That‚Äôs the work.&lt;/p&gt;
    &lt;head rend="h1"&gt;Why open source matters here&lt;/head&gt;
    &lt;p&gt;If you‚Äôve followed Mozilla, you know the Manifesto. For almost 20 years, it‚Äôs guided what we build and how ‚Äî not as an abstract ideal, but as a tool for making principled decisions every single day. Three of its principles are especially urgent in the age of AI:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Human agency. In a world of AI agents, it‚Äôs more important than ever that technology lets people shape their own experiences ‚Äî and protects privacy where it matters most;&lt;/item&gt;
      &lt;item&gt;Decentralization and open source. An open, accessible internet depends on innovation and broad participation in how technology gets created and used. The success of open-source AI, built around transparent community practices, is critical to making this possible; and&lt;/item&gt;
      &lt;item&gt;Balancing commercial and public benefit. The direction of AI is being set by commercial players. We need strong public-benefit players to create balance in the overall ecosystem.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Open-source AI is how these principles become real. It‚Äôs what makes plurality possible ‚Äî many intelligences shaped by many communities, not one model to rule them all. It‚Äôs what makes sovereignty possible ‚Äî owning your infrastructure rather than renting it. And it‚Äôs what keeps the door open for public-benefit alternatives to exist alongside commercial ones.&lt;/p&gt;
    &lt;head rend="h1"&gt;What we‚Äôll do in 2026&lt;/head&gt;
    &lt;p&gt;The window to shape these defaults is still open, but it won‚Äôt stay open forever. Here‚Äôs where we‚Äôre putting our effort ‚Äî not because we have all the answers, but because we think these are the places where openness can still reset the defaults before they harden.&lt;/p&gt;
    &lt;p&gt;Make open AI easier than closed. Mozilla.ai is building any-suite, a modular framework that integrates the scattered components of the open AI stack ‚Äî model routing, evaluation, guardrails, memory, orchestration ‚Äî into something coherent that developers can actually adopt without becoming infrastructure specialists. The goal is concrete: Getting started with open AI should feel as simple as making a single API call.&lt;/p&gt;
    &lt;p&gt;Shift the economics of data. The Mozilla Data Collective is building a marketplace for data that is properly licensed, clearly sourced, and aligned with the values of the communities it comes from. It gives developers access to high-quality training data while ensuring that the people and institutions who contribute that data have real agency and share in the economic value it creates.&lt;/p&gt;
    &lt;p&gt;Learn from real deployments. Strategy that isn‚Äôt grounded in practical experience is just speculation, so we‚Äôre deepening our engagement with governments and enterprises adopting sovereign, auditable AI systems. These engagements are the feedback loops that tell us where the stack breaks and where openness needs reinforcement.&lt;/p&gt;
    &lt;p&gt;Invest in the ecosystem. We‚Äôre not just building; we‚Äôre backing others who are building too. Mozilla Ventures is investing in open-source AI companies that align with these principles. Mozilla Foundation is funding researchers and projects through targeted grants. We can‚Äôt do everything ourselves, and we shouldn‚Äôt try. The goal is to put resources behind the people and teams already doing the work.&lt;/p&gt;
    &lt;p&gt;Show up for the community. The open-source AI ecosystem is vast, and it‚Äôs hard to know what‚Äôs working, what‚Äôs hype, and where the real momentum is building. We want to be useful here. We‚Äôre launching a newsletter to track what‚Äôs actually happening in open AI. We‚Äôre running meetups and hackathons to bring builders together. We‚Äôre fielding developer surveys to understand what people actually need. And at MozFest this year, we‚Äôre adding a dedicated developer track focused on open-source AI. If you‚Äôre doing important work in this space, we want to help it find the people who need to see it.&lt;/p&gt;
    &lt;head rend="h1"&gt;Are you in?&lt;/head&gt;
    &lt;p&gt;Mozilla is one piece of a much larger movement, and we have no interest in trying to own or control it ‚Äî we just want to help it succeed. There‚Äôs a growing community of people who believe the open internet is still worth defending and who are working to ensure that AI develops along a different path than the one the largest platforms have laid out. Not everyone in that community uses the same language or builds exactly the same things, but something like a shared purpose is emerging. Mozilla sees itself as part of that effort.&lt;/p&gt;
    &lt;p&gt;We kept the web open not by asking anyone‚Äôs permission, but by building something that worked better than the alternatives. We‚Äôre ready to do that again.&lt;/p&gt;
    &lt;p&gt;So: Are you in?&lt;/p&gt;
    &lt;p&gt;If you‚Äôre a developer building toward an open source AI future, we want to work with you. If you‚Äôre a researcher, investor, policymaker, or founder aligned with these goals, let‚Äôs talk. If you‚Äôre at a company that wants to build with us rather than against us, the door is open. Open alternatives have to exist ‚Äî that keeps everyone honest.&lt;/p&gt;
    &lt;p&gt;The future of intelligence is being set now. The question is whether you‚Äôll own it, or rent it.&lt;/p&gt;
    &lt;p&gt;We‚Äôre launching a newsletter to track what‚Äôs happening in open-source AI ‚Äî what‚Äôs working, what‚Äôs hype, and where the real momentum is building. Sign up here to follow along as we build.&lt;/p&gt;
    &lt;p&gt;Read more here about our emerging strategy, and how we‚Äôre rewiring Mozilla for the era of AI.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46599897</guid><pubDate>Tue, 13 Jan 2026 12:00:12 +0000</pubDate></item><item><title>Show HN: SnackBase ‚Äì Open-source, GxP-compliant back end for Python teams</title><link>https://snackbase.dev</link><description>&lt;doc fingerprint="476a8dc0c5f4f4c4"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Build like a Startup. Comply like an Enterprise.&lt;/head&gt;&lt;p&gt;The open-source Python backend with immutable audit logs, GxP compliance, and row-level security built-in.&lt;/p&gt;&lt;p&gt;No credit card required&lt;/p&gt;&lt;p&gt;Setup in 5 minutes&lt;/p&gt;&lt;p&gt;The SnackBase Admin UI: Multi-tenancy and Audit Logs out of the box&lt;/p&gt;&lt;head rend="h2"&gt;Why SnackBase?&lt;/head&gt;&lt;head rend="h2"&gt;Instant APIs&lt;/head&gt;&lt;p&gt;Define your data schema and get fully functional CRUD REST APIs instantly. No boilerplate, no manual endpoint creation.&lt;/p&gt;&lt;head rend="h2"&gt;Multi-Tenancy&lt;/head&gt;&lt;p&gt;Built-in support for multiple accounts/organizations with complete data isolation. Perfect for SaaS applications.&lt;/p&gt;&lt;head rend="h2"&gt;Enterprise Security&lt;/head&gt;&lt;p&gt;Role-based access control (RBAC), field-level permissions, OAuth, SAML, and audit logs out of the box.&lt;/p&gt;&lt;head rend="h2"&gt;Everything You Need to Ship Fast&lt;/head&gt;SnackBase provides a complete backend foundation so you can focus on building your product.&lt;head rend="h2"&gt;Collections &amp;amp; Schemas&lt;/head&gt;&lt;p&gt;Define your data models with a simple schema syntax. Get validation, types, and auto-generated APIs.&lt;/p&gt;&lt;head rend="h2"&gt;Permissions Engine&lt;/head&gt;&lt;p&gt;Fine-grained access control at the collection, record, or field level. Write rules, not boilerplate.&lt;/p&gt;&lt;head rend="h2"&gt;Authentication&lt;/head&gt;&lt;p&gt;Built-in OAuth (Google, GitHub, Microsoft, Apple) and SAML support. Or bring your own auth provider.&lt;/p&gt;&lt;head rend="h2"&gt;Extensible&lt;/head&gt;&lt;p&gt;Write custom hooks, macros, and business logic in Python. Extend everything.&lt;/p&gt;&lt;head rend="h2"&gt;Quick Start in 3 Steps&lt;/head&gt;&lt;p&gt;1&lt;/p&gt;&lt;p&gt;Install SnackBase&lt;/p&gt;&lt;p&gt;Install SnackBase locally or deploy to your cloud in seconds.&lt;/p&gt;&lt;p&gt;2&lt;/p&gt;&lt;p&gt;Define Your Data&lt;/p&gt;&lt;p&gt;Create collections and define your schema using our intuitive syntax.&lt;/p&gt;&lt;p&gt;3&lt;/p&gt;&lt;p&gt;Start Building&lt;/p&gt;&lt;p&gt;Use the auto-generated REST APIs or React Admin UI immediately.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46600092</guid><pubDate>Tue, 13 Jan 2026 12:27:53 +0000</pubDate></item><item><title>Confer ‚Äì End to end encrypted AI chat</title><link>https://confer.to/</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46600839</guid><pubDate>Tue, 13 Jan 2026 13:45:45 +0000</pubDate></item><item><title>Local Journalism Is How Democracy Shows Up Close to Home</title><link>https://buckscountybeacon.com/2026/01/opinion-local-journalism-is-how-democracy-shows-up-close-to-home/</link><description>&lt;doc fingerprint="b887065a01c7065a"&gt;
  &lt;main&gt;
    &lt;p&gt;Democracy rarely collapses in a single dramatic moment. Lately, however, it can feel as though those type of moments are arriving faster and more frequently, piling up in ways that leave people disoriented and unsure where to look. What often gets lost in that rush is not concern, but orientation ‚Äì a shared sense of where we are, what matters, and how any of it connects.&lt;/p&gt;
    &lt;p&gt;Long before laws are tested or elections contested, something more basic starts to fray: the everyday understanding of how our communities work and who is accountable to whom.&lt;/p&gt;
    &lt;p&gt;I‚Äôve found myself asking a simple question more often lately: Where do people actually see themselves inside public life anymore?&lt;/p&gt;
    &lt;p&gt;That question keeps leading me back to local journalism and to why its decline should concern anyone who cares about democracy.&lt;/p&gt;
    &lt;p&gt;Democracy doesn‚Äôt live only in Washington or Harrisburg. It lives in school board meetings, zoning decisions, municipal budgets, local courts, and elections that rarely make national headlines. It lives where policy meets daily life. Local journalism is how those places stay visible.&lt;/p&gt;
    &lt;p&gt;When local reporters attend meetings most of us can‚Äôt, sift through public records, and follow issues over time, they make public life legible. They help citizens see not just what happened, but why it matters, who made the decision, and what the consequences may be. Without that work, power doesn‚Äôt disappear ‚Äì it simply operates out of view.&lt;/p&gt;
    &lt;p&gt;National media plays an important role, but it works at a distance. Democracy, however, is practiced close to home. I‚Äôve noticed that when local reporting weakens, people don‚Äôt just lose information ‚Äì they lose orientation. It becomes harder to tell where influence actually lives, or how individual participation connects to outcomes.&lt;/p&gt;
    &lt;p&gt;What often gets labeled as apathy looks different up close. Many people I speak with aren‚Äôt indifferent; they‚Äôre resigned. They‚Äôve absorbed the sense that nothing they do matters, or that no one is really listening. When that happens, public life shrinks. Engagement gives way to spectatorship, and frustration seeks expression through outrage or grievance rather than responsibility.&lt;/p&gt;
    &lt;p&gt;Local journalism quietly counters that drift by doing something deceptively simple: it keeps the public in the room.&lt;/p&gt;
    &lt;p&gt;It connects decisions to real people. It shows patterns rather than isolated moments. It reminds us that our communities are not abstract ‚Äì that they are shaped by named individuals, concrete choices, and shared consequences. In that way, local journalism doesn‚Äôt just report on democracy; it helps sustain it.&lt;/p&gt;
    &lt;p&gt;This is also why attacks on journalism, especially local journalism, feel so consequential. Undermining trust in reporters, starving newsrooms of resources, or dismissing local coverage as irrelevant all serve the same end: weakening the connective tissue that allows a community to hold itself accountable.&lt;/p&gt;
    &lt;p&gt;At the same time, I don‚Äôt think the responsibility for preserving local journalism rests with journalists alone.&lt;/p&gt;
    &lt;p&gt;Supporting local journalism isn‚Äôt charity. It‚Äôs civic participation.&lt;/p&gt;
    &lt;p&gt;I‚Äôve come to see local news outlets less as content providers and more as public infrastructure ‚Äì as essential to democratic functioning as schools, courts, or roads. Subscribing, donating, and sharing credible reporting are practical ways citizens invest in the health of their communities.&lt;/p&gt;
    &lt;p&gt;Engagement matters, too. Reading beyond headlines. Responding thoughtfully rather than reactively. Offering tips, context, and lived experience that strengthen reporting rather than distort it.&lt;/p&gt;
    &lt;p&gt;These are small acts, but they shape the quality of the public conversation we‚Äôre all part of.&lt;/p&gt;
    &lt;p&gt;READ: Democracy Begins Where We Live&lt;/p&gt;
    &lt;p&gt;Publications like Bucks County Beacon model what this can look like: careful reporting, transparency about sources, and a commitment to clarity over sensationalism. In a media environment driven by speed and outrage, that kind of work feels both grounded and rare.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt see democracy as something we inherit once and for all. I see it as something we practice ‚Äì in how we speak, what we support, and whether we stay engaged when the work feels slow or imperfect.&lt;/p&gt;
    &lt;p&gt;At a moment when democratic norms feel increasingly fragile, local journalism offers something quietly powerful: a shared, grounded understanding of our common life. Defending it may be one of the most practical and hopeful choices citizens can make.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46600850</guid><pubDate>Tue, 13 Jan 2026 13:46:34 +0000</pubDate></item><item><title>Apple Creator Studio</title><link>https://www.apple.com/newsroom/2026/01/introducing-apple-creator-studio-an-inspiring-collection-of-creative-apps/</link><description>&lt;doc fingerprint="b66e5b1d593679dd"&gt;
  &lt;main&gt;
    &lt;p&gt; PRESS RELEASE January 13, 2026 &lt;/p&gt;
    &lt;head rend="h1"&gt;Apple introduces Apple Creator Studio, an inspiring collection of the most powerful creative apps&lt;/head&gt;
    &lt;p&gt; Final Cut Pro, Logic Pro, Pixelmator Pro, Motion, Compressor, and MainStage ‚Äî plus new AI features and premium content in Keynote, Pages, and Numbers ‚Äî come together in a single subscription &lt;/p&gt;
    &lt;p&gt;CUPERTINO, CALIFORNIA Apple today unveiled Apple Creator Studio, a groundbreaking collection of powerful creative apps designed to put studio-grade power into the hands of everyone, building on the essential role Mac, iPad, and iPhone play in the lives of millions of creators around the world. The apps included with Apple Creator Studio for video editing, music making, creative imaging, and visual productivity give modern creators the features and capabilities they need to experience the joy of editing and tailoring their content while realizing their artistic vision. Exciting new intelligent features and premium content build on familiar experiences of Final Cut Pro, Logic Pro, Pixelmator Pro, Keynote, Pages, Numbers, and later Freeform to make Apple Creator Studio an exciting subscription suite to empower creators of all disciplines while protecting their privacy. &lt;/p&gt;
    &lt;p&gt;Final Cut Pro introduces exceptional new video editing tools and intelligent features for Mac and iPad to improve the efficiency of even the most complex workflows.1 For the first time, Pixelmator Pro is coming to iPad with a uniquely crafted experience that is optimized for touch and Apple Pencil.2 Music creation with Logic Pro for Mac and iPad introduces even more intelligent features like Synth Player and Chord ID to inspire anyone to write, produce, and mix a range of popular music.3 And with Keynote, Pages, Numbers, and Freeform, Apple Creator Studio subscribers can be more expressive and productive with new premium content and intelligent features across Mac, iPad, and iPhone.4 &lt;/p&gt;
    &lt;p&gt;Apple Creator Studio will be available on the App Store beginning Wednesday, January 28, for $12.99 per month or $129 per year, with a one-month free trial, and includes access to Final Cut Pro, Logic Pro, and Pixelmator Pro on Mac and iPad; Motion, Compressor, and MainStage on Mac; and intelligent features and premium content for Keynote, Pages, Numbers, and later Freeform for iPhone, iPad, and Mac. College students and educators can subscribe for $2.99 per month or $29.99 per year. Alternatively, users can also choose to purchase the Mac versions of Final Cut Pro, Pixelmator Pro, Logic Pro, Motion, Compressor, and MainStage individually as a one-time purchase on the Mac App Store.5 &lt;/p&gt;
    &lt;p&gt;‚ÄúApple Creator Studio is a great value that enables creators of all types to pursue their craft and grow their skills by providing easy access to the most powerful and intuitive tools for video editing, music making, creative imaging, and visual productivity ‚Äî all leveled up with advanced intelligent tools to augment and accelerate workflows,‚Äù said Eddy Cue, Apple‚Äôs senior vice president of Internet Software and Services. ‚ÄúThere‚Äôs never been a more flexible and accessible way to get started with such a powerful collection of creative apps for professionals, emerging artists, entrepreneurs, students, and educators to do their best work and explore their creative interests from start to finish.‚Äù &lt;/p&gt;
    &lt;head rend="h2"&gt;Video Creation Gets Smarter and Faster&lt;/head&gt;
    &lt;p&gt;Final Cut Pro for Mac and iPad empowers content creators, video editors, and filmmakers to elevate their projects with intuitive features. One-time-purchase Mac users and Apple Creator Studio subscribers can experience blazing-fast performance with Apple silicon for the most demanding workflows, and get into the creative flow faster than ever with new on-device intelligent features that make video creation effortless and easy. &lt;/p&gt;
    &lt;p&gt;With Transcript Search on Mac and iPad, users can now easily find the perfect soundbite in hours of footage by simply typing phrases into the search bar to see exact or related results.6 Video podcasts and interviews can be assembled quickly, eliminating extensive time spent skimming through footage. Looking for a specific video clip also gets an intelligence assist with Visual Search.7 Now, users can quickly pinpoint exact moments across all footage by searching for an object or action, and then add that visual to their timeline in seconds. &lt;/p&gt;
    &lt;p&gt;Final Cut Pro for Mac and iPad also makes editing video to the rhythm of music fast and fun with Beat Detection, an amazing new way to see musical beats, bars, and song parts right in the project timeline. Beat Detection uses an AI model from Logic Pro to instantly analyze any music track and display the Beat Grid, so users creating fast-paced videos can quickly and visually align their cuts to the music. Re-editing music tracks to different lengths is also easier than ever. &lt;/p&gt;
    &lt;p&gt;The new Montage Maker in Final Cut Pro for iPad lets users kick-start their edit in just seconds. Using the power of AI, Montage Maker will analyze and edit together a dynamic video based on the best visual moments within the footage, with the ability to change the pacing, cut to a music track, and intelligently reframe horizontal videos to vertical with Auto Crop to simplify sharing across social platforms. &lt;/p&gt;
    &lt;p&gt;Apple Creator Studio also unlocks full access to Motion, a powerful motion graphics tool for creating cinematic 2D and 3D effects with intelligent features like Magnetic Mask, which effortlessly isolates and tracks people and objects without a green screen. It also includes Compressor, which integrates with Final Cut Pro and Motion to seamlessly customize output settings for distribution. &lt;/p&gt;
    &lt;head rend="h2"&gt;Taking Music Creation to the Next Level&lt;/head&gt;
    &lt;p&gt;A new lineup of features for Logic Pro for Mac and iPad supports musical artists and helps creators deliver original music for their video content as an Apple Creator Studio subscriber or one-time-purchase Mac user. The new tools are sophisticated, intuitive, and intelligent to inspire beat making, songwriting, remixing, and more. &lt;/p&gt;
    &lt;p&gt;Synth Player joins the AI Session Player lineup,8 delivering incredible electronic music performances with a diverse range of chordal and synth bass parts ‚Äî all powered by AI and the advanced software instrument technology of Logic Pro. Using Synth Player is like having access to a skilled synthesist that can instantly take a musical idea in new directions when needed. Developed in-house using Apple‚Äôs own team of expert sound designers, Synth Player delivers incredible realism and fidelity fueled by the vast array of software synthesizers and samplers in Logic Pro. And like every AI Session Player, creators can direct Synth Player using intuitive controls for complexity and intensity, while additional parameters unlock access to advanced performance capabilities. Synth Player can also access any third-party plug-in Audio Units, or even control an external hardware synthesizer. &lt;/p&gt;
    &lt;p&gt;Tapping into the power of AI, Chord ID becomes a personal music theory expert by turning any audio or MIDI recording into a ready-to-use chord progression, eliminating tedious manual transcription and bringing demo ideas to life even faster. Designed to help everyone get the most out of the Session Player experience, Chord ID can analyze complex harmonic content from nearly any recording to automatically populate the chord track in Logic Pro. And since the chord track drives the performances of any AI Session Player, users can quickly audition different players, styles, and genres, allowing them creative freedom to experiment and dial in their favorite vibe. &lt;/p&gt;
    &lt;p&gt;The new Sound Library in Logic Pro for Mac delivers Apple-designed packs and Producer Packs with hundreds of royalty-free loops, samples, instrument patches, drum sounds, and more. Additionally, Logic Pro for iPad users will now have access to the industry-leading Quick Swipe Comping feature from Logic Pro for Mac, an indispensable tool for vocalists and producers who want to create seamless performances inside or outside the studio. &lt;/p&gt;
    &lt;p&gt;Logic Pro for iPad also presents Music Understanding features with natural language search in the Sound Browser to help users describe a loop or find similar loops ‚Äî no tags, guesses, or filters required. AI-based awareness of the massive collection of loops in Logic Pro makes it easy to search either through natural language or a recording to find a similar or complementary loop or sound. &lt;/p&gt;
    &lt;p&gt;Apple Creator Studio also unlocks access to MainStage, which turns Mac into an instrument, voice processor, or guitar rig. Now, the sound users love in their recording can be the sound their audience hears. Setup is fast, teardown is faster, and everything in between is more reliable. &lt;/p&gt;
    &lt;head rend="h2"&gt;Creative Imaging Designed for iPad&lt;/head&gt;
    &lt;p&gt;Pixelmator Pro, the award-winning image editor for Mac, comes with the all-new Apple Creator Studio, bringing an approachable and professional editing experience to even more creators. Pixelmator Pro is packed with powerful image editing tools, empowering Apple Creator Studio subscribers and one-time-purchase Mac users to design, draw, paint, and refine their creative vision, and so much more. For the first time, Pixelmator Pro is coming to iPad, bringing an all-new touch-optimized workspace, full Apple Pencil support, the ability to work between iPad and Mac, and all of the powerful editing tools users have come to appreciate on Mac. Pixelmator Pro for iPad offers fast and efficient image editing, leveraging the blazing performance of Apple silicon and built from scratch for the latest iPadOS. &lt;/p&gt;
    &lt;p&gt;Intuitive touch controls make it even easier to create desktop-class designs wherever users take their iPad. The full-featured Layers sidebar allows creators to build designs using a range of unique elements like images, shapes, text, and even video. Smart selection tools help users isolate and edit specific parts of images effortlessly, and with advanced bitmap and vector masks, users can hide or reveal discrete portions of their designs. The deep integration of hardware, software, and Apple silicon unlocks features like Super Resolution for intelligently upscaling photos, Deband for removing compression artifacts, and automatic composition suggestions with Auto Crop. With full support for Apple Pencil, digital artists can enjoy painting in the most natural way with a beautiful collection of pressure-sensitive brushes. And unmatched Apple Pencil precision ‚Äî combined with features like hover,9 squeeze,10 and double-tap11 ‚Äî gives creators the ability to craft pixel-perfect designs. &lt;/p&gt;
    &lt;p&gt;Additionally, for Apple Creator Studio subscribers, both Pixelmator Pro for Mac and iPad bring a powerful new Warp tool for twisting and shaping layers any way creatives can imagine, alongside a beautiful collection of Warp-powered product mockups. &lt;/p&gt;
    &lt;head rend="h2"&gt;Supercharging Visual Productivity&lt;/head&gt;
    &lt;p&gt;For more than 20 years, Apple‚Äôs visual productivity apps have empowered users to express themselves with beautiful presentations, documents, and spreadsheets using Keynote, Pages, and Numbers. And Freeform has brought endless possibilities for creative brainstorming and visual collaboration. &lt;/p&gt;
    &lt;p&gt;With Apple Creator Studio, productivity gets supercharged with all-new features that bring more intelligence and premium content to creators‚Äô fingertips so they can take their projects to the next level. The Content Hub is a new space where users can find curated, high-quality photos, graphics, and illustrations. A subscription also unlocks new premium templates and themes in Keynote, Pages, and Numbers. &lt;/p&gt;
    &lt;p&gt;In addition to Image Playground, advanced image creation and editing tools let users create high-quality images from text, or transform existing images, using generative models from OpenAI.12 On-device AI models enable Super Resolution to upscale images while keeping them sharp and detailed, and Auto Crop provides intelligent crop suggestions, helping users find eye-catching compositions for photos. &lt;/p&gt;
    &lt;p&gt;To help users prepare presentations even more quickly in Keynote, Apple Creator Studio includes access to features in beta, such as the ability to generate a first draft of a presentation from a text outline, or create presenter notes from existing slides. Subscribers can also quickly clean up slides to fix layout and object placement. And in Numbers, subscribers can generate formulas and fill in tables based on pattern recognition with Magic Fill. &lt;/p&gt;
    &lt;p&gt;Keynote, Pages, Numbers, and Freeform will remain free for all users to create, edit, and collaborate with others, including Apple Creator Studio subscribers. These apps will continue receiving updates, with the latest versions adopting the beautiful new visual design language with Liquid Glass on all platforms, and supporting the new windowing and menu bar improvements in iPadOS 26. &lt;/p&gt;
    &lt;p&gt;Pricing and Availability &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apple Creator Studio will be available beginning Wednesday, January 28, for $12.99 (U.S.) per month or $129 (U.S.) per year. All new subscribers will enjoy a one-month free trial of Apple Creator Studio, and with the purchase of a new Mac or qualifying iPad,13 customers can receive three months of Apple Creator Studio for free.14&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Education savings are available for college students and educators15 for $2.99 (U.S.) per month or $29.99 (U.S.) per year.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apple Creator Studio is available to download on the App Store as a universal purchase.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Up to six family members can share all of the apps and content included in Apple Creator Studio with Family Sharing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;One-time-purchase versions of Final Cut Pro ($299.99 U.S.), Logic Pro ($199.99 U.S.), Pixelmator Pro ($49.99 U.S.), Motion ($49.99 U.S.), Compressor ($49.99 U.S.), and MainStage ($29.99 U.S.) are available on the Mac App Store.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Free versions of Keynote, Pages, Numbers, and Freeform continue to be available and are included with every new iPhone, Mac, and iPad.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Share article&lt;/p&gt;
    &lt;head rend="h2"&gt;Media&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Text of this article&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Media in this article&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The Apple Creator Studio version of Final Cut Pro for Mac will be compatible with Mac models with Intel or Apple silicon chips running macOS 15.6 or later. Some features require Apple silicon. Final Cut Pro for iPad will be compatible with iPad models with the A16, A17 Pro, or M1 chip or later running iPadOS 18.6 or later.&lt;/item&gt;
      &lt;item&gt;Pixelmator Pro for iPad is compatible with iPad models with the A16, A17 Pro, or M1 chip or later running iPadOS 26 or later. The Apple Creator Studio version of Pixelmator Pro requires macOS 26.&lt;/item&gt;
      &lt;item&gt;The Apple Creator Studio version of Logic Pro for Mac requires macOS 15.6 or later and a Mac with Apple silicon. Logic Pro for iPad requires iPadOS 26 or later and an iPad with the Apple A12 Bionic chip or later. Some features require the Apple A17 Pro chip or later.&lt;/item&gt;
      &lt;item&gt;Apple Creator Studio versions of Keynote, Pages, and Numbers will require iOS 18.0 or later, iPadOS 18.0 or later, or macOS Sequoia 15.6 or later. Some intelligent features including image generation will require iOS 26, iPadOS 26, or macOS Tahoe. Premium content and features in Freeform are not currently available and are expected to be included in the Apple Creator Studio subscription later this year.&lt;/item&gt;
      &lt;item&gt;The one-time-purchase versions of Final Cut Pro requires macOS 15.6 or later, Logic Pro requires macOS 15.6 or later, and Pixelmator Pro requires macOS 12.0 or later. MainStage is available for any Mac supported by macOS 15.6 or later. Motion requires macOS 15.6 or later. Compressor requires macOS 15.6 or later and some features require a Mac with Apple silicon.&lt;/item&gt;
      &lt;item&gt;Transcript Search in Final Cut Pro requires a Mac with Apple silicon and macOS 15.6 or later, or an iPad with the Apple M1 chip or later, iPad (A16), or iPad mini (A17 Pro) and iPadOS 26 or later. Available only in U.S. English.&lt;/item&gt;
      &lt;item&gt;Visual Search requires a Mac with Apple silicon and macOS 15.6 or later, or an iPad with the Apple M1 chip or later, iPad (A16), or iPad mini (A17 Pro) and iPadOS 26 or later. Available only in U.S. English.&lt;/item&gt;
      &lt;item&gt;Session Players require iPad with the Apple M1 chip or later or Mac with Apple silicon is recommended.&lt;/item&gt;
      &lt;item&gt;Apple Pencil hover works with iPad Pro 13-inch (M4), iPad Pro 12.9‚Äëinch (6th generation), iPad Pro 11-inch (M4), iPad Pro 11‚Äëinch (4th generation), iPad Air 13-inch (M2), and iPad Air 11-inch (M2).&lt;/item&gt;
      &lt;item&gt;Apple Pencil squeeze is only available when using Apple Pencil Pro with iPad Pro 13- and 11-inch (M4 and M5), iPad Air 13- and 11-inch (M2 and M3), and iPad mini (A17 Pro).&lt;/item&gt;
      &lt;item&gt;Apple Pencil double-tap works with Apple Pencil (2nd generation) with iPad mini (6th generation), iPad Air (4th and 5th generations), iPad Pro 11-inch (1st, 2nd, 3rd, and 4th generations), and iPad Pro 12.9-inch (3rd, 4th, 5th, and 6th generations); and with Apple Pencil Pro with iPad mini (A17 Pro), iPad Air 11-inch and 13-inch (M2 and M3), and iPad Pro 11-inch and 13-inch (M4 and M5).&lt;/item&gt;
      &lt;item&gt;Some features of Apple Creator Studio require an Apple Intelligence-capable device. For a list of Apple Intelligence availability and technical requirements, see support.apple.com/121115. Some artificial intelligence features of Apple Creator Studio utilize third-party models and may have usage limits and restrictions.&lt;/item&gt;
      &lt;item&gt;A new Mac or iPad purchased from Apple or an Apple Authorized Reseller. iPad must have at least 6 GB memory and an A16, A17 Pro, or M-series chip or later.&lt;/item&gt;
      &lt;item&gt;New and qualified returning subscribers only. Plan renews at $12.99 per month or $129 per year based on plan selected. Only one offer per Apple Account and only one offer per family if they‚Äôre part of a Family Sharing group, regardless of the number of devices that they or their family purchase. This offer is not available if the account holder or their Family have previously accepted an Apple Creator Studio subscription three months free offer. Offer good for the latter of (i) three months after eligible device activation or (ii) three months after first availability for subscription to Apple Creator Studio. Plan automatically renews until cancelled. Restrictions and other terms apply.&lt;/item&gt;
      &lt;item&gt;New subscribers only. Education Savings Plan automatically renews at $2.99 per month or $29.99 per year based on plan selected until cancelled. Offer good for college students and educators only and does not extend to a Family Sharing group. Verification required. Terms apply. Limited-time offer; offer may end at any time.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46601157</guid><pubDate>Tue, 13 Jan 2026 14:14:18 +0000</pubDate></item><item><title>Show HN: FastScheduler ‚Äì Decorator-first Python task scheduler, async support</title><link>https://github.com/MichielMe/fastscheduler</link><description>&lt;doc fingerprint="e604a65c99a0e98a"&gt;
  &lt;main&gt;
    &lt;p&gt;Simple, lightweight task scheduler for Python with async support, timezone handling, cron expressions, and a beautiful real-time dashboard.&lt;/p&gt;
    &lt;p&gt;If this saves you time, ‚≠êÔ∏è the repo and open an issue for ideas ‚Äî I'm actively improving it.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üéØ Simple decorator-based API - Schedule tasks in one line&lt;/item&gt;
      &lt;item&gt;‚ö° Async/await support - Native async function support&lt;/item&gt;
      &lt;item&gt;üïê Timezone support - Schedule jobs in any timezone&lt;/item&gt;
      &lt;item&gt;üìÖ Cron expressions - Complex schedules with cron syntax&lt;/item&gt;
      &lt;item&gt;üíæ Persistent state - Survives restarts, handles missed jobs&lt;/item&gt;
      &lt;item&gt;üé® FastAPI dashboard - Beautiful real-time monitoring UI&lt;/item&gt;
      &lt;item&gt;üîÑ Automatic retries - Configurable retry with exponential backoff&lt;/item&gt;
      &lt;item&gt;‚è±Ô∏è Job timeouts - Kill long-running jobs automatically&lt;/item&gt;
      &lt;item&gt;‚è∏Ô∏è Pause/Resume - Control jobs without removing them&lt;/item&gt;
      &lt;item&gt;üìã Dead Letter Queue - Track and debug failed jobs&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Basic installation
pip install fastscheduler

# With FastAPI dashboard
pip install fastscheduler[fastapi]

# With cron expression support
pip install fastscheduler[cron]

# All features
pip install fastscheduler[all]&lt;/code&gt;
    &lt;code&gt;from fastscheduler import FastScheduler

scheduler = FastScheduler(quiet=True)

@scheduler.every(10).seconds
def task():
    print("Task executed")

@scheduler.daily.at("14:30")
async def daily_task():
    print("Daily task at 2:30 PM")

scheduler.start()&lt;/code&gt;
    &lt;code&gt;@scheduler.every(10).seconds
@scheduler.every(5).minutes
@scheduler.every(2).hours
@scheduler.every(1).days&lt;/code&gt;
    &lt;code&gt;@scheduler.daily.at("09:00")              # Daily at 9 AM
@scheduler.hourly.at(":30")               # Every hour at :30
@scheduler.weekly.monday.at("10:00")      # Every Monday at 10 AM
@scheduler.weekly.weekdays.at("09:00")    # Weekdays at 9 AM
@scheduler.weekly.weekends.at("12:00")    # Weekends at noon&lt;/code&gt;
    &lt;p&gt;Requires: &lt;code&gt;pip install fastscheduler[cron]&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;@scheduler.cron("0 9 * * MON-FRI")        # 9 AM on weekdays
def market_open():
    ...

@scheduler.cron("*/15 * * * *")           # Every 15 minutes
def frequent_check():
    ...

@scheduler.cron("0 0 1 * *")              # First day of each month
def monthly_report():
    ...&lt;/code&gt;
    &lt;code&gt;@scheduler.once(60)                        # Run once after 60 seconds
def delayed_task():
    ...

@scheduler.at("2024-12-25 00:00:00")      # Run at specific datetime
def christmas_task():
    ...&lt;/code&gt;
    &lt;p&gt;Schedule jobs in any timezone:&lt;/p&gt;
    &lt;code&gt;# Using the tz parameter
@scheduler.daily.at("09:00", tz="America/New_York")
def nyc_morning():
    print("Good morning, New York!")

# Using the .tz() method (chainable)
@scheduler.weekly.monday.tz("Europe/London").at("09:00")
def london_standup():
    print("Monday standup")

# With cron expressions
@scheduler.cron("0 9 * * MON-FRI").tz("Asia/Tokyo")
def tokyo_market():
    print("Tokyo market open")&lt;/code&gt;
    &lt;p&gt;Common timezones: &lt;code&gt;UTC&lt;/code&gt;, &lt;code&gt;America/New_York&lt;/code&gt;, &lt;code&gt;America/Los_Angeles&lt;/code&gt;, &lt;code&gt;Europe/London&lt;/code&gt;, &lt;code&gt;Europe/Paris&lt;/code&gt;, &lt;code&gt;Asia/Tokyo&lt;/code&gt;, &lt;code&gt;Asia/Shanghai&lt;/code&gt;, &lt;code&gt;Australia/Sydney&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Kill jobs that run too long:&lt;/p&gt;
    &lt;code&gt;@scheduler.every(1).minutes.timeout(30)   # Kill if runs &amp;gt; 30 seconds
def quick_task():
    ...

@scheduler.daily.at("02:00").timeout(3600)  # 1 hour max
def nightly_backup():
    ...&lt;/code&gt;
    &lt;p&gt;Configure automatic retries on failure:&lt;/p&gt;
    &lt;code&gt;@scheduler.every(5).minutes.retries(5)    # Retry up to 5 times
def flaky_api_call():
    ...&lt;/code&gt;
    &lt;p&gt;Retries use exponential backoff (2s, 4s, 8s, 16s, ...).&lt;/p&gt;
    &lt;p&gt;Don't run missed jobs after restart:&lt;/p&gt;
    &lt;code&gt;@scheduler.every(1).hours.no_catch_up()
def hourly_stats():
    ...&lt;/code&gt;
    &lt;code&gt;# Pause a job (stays in queue but won't execute)
scheduler.pause_job("job_0")

# Resume a paused job
scheduler.resume_job("job_0")

# Cancel and remove a job
scheduler.cancel_job("job_0")

# Cancel all jobs with a specific function name
scheduler.cancel_job_by_name("my_task")&lt;/code&gt;
    &lt;p&gt;Add a beautiful real-time dashboard to your FastAPI app:&lt;/p&gt;
    &lt;code&gt;from fastapi import FastAPI
from fastscheduler import FastScheduler
from fastscheduler.fastapi_integration import create_scheduler_routes

app = FastAPI()
scheduler = FastScheduler(quiet=True)

# Add dashboard at /scheduler/
app.include_router(create_scheduler_routes(scheduler))

@scheduler.every(30).seconds
def background_task():
    print("Background work")

scheduler.start()&lt;/code&gt;
    &lt;p&gt;Access at &lt;code&gt;http://localhost:8000/scheduler/&lt;/code&gt;&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Real-time updates via Server-Sent Events (SSE)&lt;/item&gt;
      &lt;item&gt;Job table with status indicators, last 5 run results, and countdown timers&lt;/item&gt;
      &lt;item&gt;Quick actions - Pause/Resume/Cancel directly from the UI&lt;/item&gt;
      &lt;item&gt;Execution history tab with filtering and search&lt;/item&gt;
      &lt;item&gt;Dead letter queue tab - view failed jobs with error details&lt;/item&gt;
      &lt;item&gt;Statistics - Success rate, uptime, active jobs count&lt;/item&gt;
      &lt;item&gt;Toast notifications - Alerts for job completions and failures&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Endpoint&lt;/cell&gt;
        &lt;cell role="head"&gt;Method&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;/scheduler/&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;GET&lt;/cell&gt;
        &lt;cell&gt;Dashboard UI&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;/scheduler/api/status&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;GET&lt;/cell&gt;
        &lt;cell&gt;Scheduler status&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;/scheduler/api/jobs&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;GET&lt;/cell&gt;
        &lt;cell&gt;List all jobs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;/scheduler/api/jobs/{job_id}&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;GET&lt;/cell&gt;
        &lt;cell&gt;Get specific job&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;/scheduler/api/jobs/{job_id}/pause&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;POST&lt;/cell&gt;
        &lt;cell&gt;Pause a job&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;/scheduler/api/jobs/{job_id}/resume&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;POST&lt;/cell&gt;
        &lt;cell&gt;Resume a job&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;/scheduler/api/jobs/{job_id}/cancel&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;POST&lt;/cell&gt;
        &lt;cell&gt;Cancel a job&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;/scheduler/api/history&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;GET&lt;/cell&gt;
        &lt;cell&gt;Execution history&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;/scheduler/api/dead-letters&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;GET&lt;/cell&gt;
        &lt;cell&gt;Dead letter queue (failed jobs)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;/scheduler/api/dead-letters&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;DELETE&lt;/cell&gt;
        &lt;cell&gt;Clear dead letter queue&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;/scheduler/events&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;GET&lt;/cell&gt;
        &lt;cell&gt;SSE event stream&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;scheduler = FastScheduler(
    state_file="scheduler.json",    # Persistence file (default: fastscheduler_state.json)
    quiet=True,                     # Suppress log messages (default: False)
    auto_start=False,               # Start immediately (default: False)
    max_history=5000,               # Max history entries to keep (default: 10000)
    max_workers=20,                 # Concurrent job threads (default: 10)
    history_retention_days=8,       # Delete history older than X days (default: 7)
    max_dead_letters=500,           # Max failed jobs in dead letter queue (default: 500)
)&lt;/code&gt;
    &lt;p&gt;History is automatically cleaned up based on two limits (both are enforced):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Count limit: &lt;code&gt;max_history&lt;/code&gt;- maximum number of entries&lt;/item&gt;
      &lt;item&gt;Time limit: &lt;code&gt;history_retention_days&lt;/code&gt;- maximum age in days&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Set &lt;code&gt;history_retention_days=0&lt;/code&gt; to disable time-based cleanup (only count limit applies).&lt;/p&gt;
    &lt;p&gt;Failed job executions are automatically stored in a separate dead letter queue for debugging:&lt;/p&gt;
    &lt;code&gt;# Get failed jobs
dead_letters = scheduler.get_dead_letters(limit=100)

# Clear the queue
scheduler.clear_dead_letters()&lt;/code&gt;
    &lt;p&gt;The dead letter queue:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Stores the last &lt;code&gt;max_dead_letters&lt;/code&gt;failed jobs (default: 500)&lt;/item&gt;
      &lt;item&gt;Persists to a separate JSON file (&lt;code&gt;*_dead_letters.json&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Includes error messages, timestamps, run counts, and execution times&lt;/item&gt;
      &lt;item&gt;Viewable in the dashboard "Failed" tab&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Get all jobs
jobs = scheduler.get_jobs()

# Get specific job
job = scheduler.get_job("job_0")

# Get execution history
history = scheduler.get_history(limit=100)
history = scheduler.get_history(func_name="my_task", limit=50)

# Get statistics
stats = scheduler.get_statistics()
# Returns: total_runs, total_failures, uptime, per_job stats

# Print simple status to console
scheduler.print_status()&lt;/code&gt;
    &lt;code&gt;with FastScheduler(quiet=True) as scheduler:
    @scheduler.every(5).seconds
    def task():
        print("Running")

    # Scheduler starts automatically
    time.sleep(30)
# Scheduler stops automatically on exit&lt;/code&gt;
    &lt;p&gt;FastScheduler automatically saves state to disk:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Job definitions and schedules&lt;/item&gt;
      &lt;item&gt;Execution history&lt;/item&gt;
      &lt;item&gt;Statistics&lt;/item&gt;
      &lt;item&gt;Job counter (ensures unique IDs across restarts)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On restart, it:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Restores all jobs&lt;/item&gt;
      &lt;item&gt;Calculates missed executions&lt;/item&gt;
      &lt;item&gt;Runs catch-up jobs (unless &lt;code&gt;no_catch_up()&lt;/code&gt;is set)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;import asyncio
import time
from fastscheduler import FastScheduler

scheduler = FastScheduler(quiet=True)

# Simple interval job
@scheduler.every(10).seconds
def heartbeat():
    print(f"[{time.strftime('%H:%M:%S')}] ‚ù§Ô∏è Heartbeat")

# Async job with timezone
@scheduler.daily.at("09:00", tz="America/New_York").timeout(60)
async def morning_report():
    print("Generating report...")
    await asyncio.sleep(5)
    print("Report sent!")

# Cron job with retries
@scheduler.cron("*/5 * * * *").retries(3)
def check_api():
    print("Checking API health")

# Weekly job
@scheduler.weekly.monday.at("10:00")
def weekly_standup():
    print("Time for standup!")

# Start scheduler
scheduler.start()

try:
    while True:
        time.sleep(60)
        scheduler.print_status()
except KeyboardInterrupt:
    scheduler.stop()&lt;/code&gt;
    &lt;code&gt;from contextlib import asynccontextmanager
from fastapi import FastAPI
from fastscheduler import FastScheduler
from fastscheduler.fastapi_integration import create_scheduler_routes

scheduler = FastScheduler(quiet=True)

@asynccontextmanager
async def lifespan(app: FastAPI):
    scheduler.start()
    yield
    scheduler.stop(wait=True)

app = FastAPI(lifespan=lifespan)
app.include_router(create_scheduler_routes(scheduler))

@scheduler.every(30).seconds
def background_job():
    print("Working...")&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Method&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;start()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Start the scheduler&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;stop(wait=True, timeout=30)&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Stop gracefully&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;get_jobs()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;List all scheduled jobs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;get_job(job_id)&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Get specific job by ID&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;get_history(func_name=None, limit=50)&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Get execution history&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;get_statistics()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Get runtime statistics&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;get_dead_letters(limit=100)&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Get dead letter queue (failed jobs)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;clear_dead_letters()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Clear all dead letter entries&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;pause_job(job_id)&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Pause a job&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;resume_job(job_id)&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Resume a paused job&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;cancel_job(job_id)&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Cancel and remove a job&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;cancel_job_by_name(func_name)&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Cancel all jobs by function name&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;print_status()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Print status to console&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Method&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;every(n).seconds/minutes/hours/days&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Interval scheduling&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;daily.at("HH:MM")&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Daily at specific time&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;hourly.at(":MM")&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Hourly at specific minute&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;weekly.monday/tuesday/.../sunday.at("HH:MM")&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Weekly scheduling&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;weekly.weekdays/weekends.at("HH:MM")&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Weekday/weekend scheduling&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;cron("expression")&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Cron expression scheduling&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;once(seconds)&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;One-time delayed execution&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;at("YYYY-MM-DD HH:MM:SS")&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;One-time at specific datetime&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Modifier&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;.timeout(seconds)&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Maximum execution time&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;.retries(n)&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Maximum retry attempts&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;.no_catch_up()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Skip missed executions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;.tz("timezone")&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Set timezone for schedule&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;MIT&lt;/p&gt;
    &lt;p&gt;Contributions welcome! Please open an issue or PR on GitHub.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46601631</guid><pubDate>Tue, 13 Jan 2026 14:45:44 +0000</pubDate></item><item><title>Anthropic invests $1.5M in the Python Software Foundation</title><link>https://discuss.python.org/t/anthropic-has-made-a-large-contribution-to-the-python-software-foundation-and-open-source-security/105694</link><description>&lt;doc fingerprint="bad2419551c5d20a"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;Loren
(Loren)
1&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;Hi all - I wanted to share here some exciting news we shared on our blog this morning: Anthropic has donated $1.5 million over two years to the PSF! Their landmark gift is focused on supporting our security work.&lt;/p&gt;
        &lt;p&gt;Here‚Äôs what we shared on social media:&lt;/p&gt;
        &lt;quote&gt;
          &lt;p&gt;Big news: Anthropic is investing $1.5 million in the Python Software Foundation, focused on Python ecosystem security. This gift will make an enormous impact on the PSF and the safety and security of millions of Python and PyPI users.&lt;/p&gt;
          &lt;p&gt;Anthropic‚Äôs funds will enable the PSF to make progress on our security roadmap, including work designed to protect millions of PyPI users from attempted supply-chain attacks. Anthropic‚Äôs support will also go towards the PSF‚Äôs core work, including the Developer in Residence program driving contributions to CPython, community support through grants and other programs, running core infrastructure such as PyPI, and more.&lt;/p&gt;
          &lt;p&gt;We couldn‚Äôt be more grateful for Anthropic‚Äôs remarkable support, and we hope you will join us in thanking them for their landmark investment in the PSF and the Python community.&lt;/p&gt;
          &lt;p&gt;Read more on our blog.&lt;/p&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; 30 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;franklinvp
(Franklinvp)
2&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;The projections look they being very grateful back.&lt;/p&gt;
        &lt;quote&gt;
          &lt;code&gt;&amp;gt;&amp;gt;&amp;gt; 1500000/30000000000
5e-05
&amp;gt;&amp;gt;&amp;gt; 1500000/260000000000
5.769230769230769e-06
&amp;gt;&amp;gt;&amp;gt; 1500000/700000000000
2.1428571428571427e-06
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46601902</guid><pubDate>Tue, 13 Jan 2026 15:03:58 +0000</pubDate></item><item><title>Scott Adams has died</title><link>https://www.youtube.com/watch?v=Rs_JrOIo3SE</link><description>&lt;doc fingerprint="50559455455d1642"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket ¬© 2026 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46602102</guid><pubDate>Tue, 13 Jan 2026 15:18:43 +0000</pubDate></item><item><title>Show HN: Self-host Reddit ‚Äì 2.38B posts, works offline, yours forever</title><link>https://github.com/19-84/redd-archiver</link><description>&lt;doc fingerprint="7998b81663b49ce2"&gt;
  &lt;main&gt;
    &lt;p&gt;Transform compressed data dumps into browsable HTML archives with flexible deployment options. Redd-Archiver supports offline browsing via sorted index pages OR full-text search with Docker deployment. Features mobile-first design, multi-platform support, and enterprise-grade performance with PostgreSQL full-text indexing.&lt;/p&gt;
    &lt;p&gt;Supported Platforms:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Platform&lt;/cell&gt;
        &lt;cell role="head"&gt;Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Status&lt;/cell&gt;
        &lt;cell role="head"&gt;Available Posts&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;.zst JSON Lines (Pushshift)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full support&lt;/cell&gt;
        &lt;cell&gt;2.38B posts (40,029 subreddits, through Dec 31 2024)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Voat&lt;/cell&gt;
        &lt;cell&gt;SQL dumps&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full support&lt;/cell&gt;
        &lt;cell&gt;3.81M posts, 24.1M comments (22,637 subverses, complete archive)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Ruqqus&lt;/cell&gt;
        &lt;cell&gt;.7z JSON Lines&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full support&lt;/cell&gt;
        &lt;cell&gt;500K posts (6,217 guilds, complete archive)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Tracked content: 2.384 billion posts across 68,883 communities (Reddit full Pushshift dataset through Dec 31 2024, Voat/Ruqqus complete archives)&lt;/p&gt;
    &lt;p&gt;Version 1.0 features multi-platform archiving, REST API with 30+ endpoints, MCP server for AI integration, and PostgreSQL-backed architecture for large-scale processing.&lt;/p&gt;
    &lt;p&gt;Try the live demo: Browse Example Archive ‚Üí&lt;/p&gt;
    &lt;p&gt;New to Redd-Archiver? Start here: QUICKSTART.md&lt;/p&gt;
    &lt;p&gt;Get running in 2-15 minutes with our step-by-step guide covering:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Local testing (5 minutes)&lt;/item&gt;
      &lt;item&gt;Tor homelab deployment (2 minutes) - no domain or port forwarding needed!&lt;/item&gt;
      &lt;item&gt;Production HTTPS (15 minutes)&lt;/item&gt;
      &lt;item&gt;Example data testing&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Archive content from multiple link aggregator platforms in a single unified archive:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Platform&lt;/cell&gt;
        &lt;cell role="head"&gt;Format&lt;/cell&gt;
        &lt;cell role="head"&gt;CLI Flag&lt;/cell&gt;
        &lt;cell role="head"&gt;URL Prefix&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;.zst JSON Lines&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;--subreddit&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/r/&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Voat&lt;/cell&gt;
        &lt;cell&gt;SQL dumps&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;--subverse&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/v/&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Ruqqus&lt;/cell&gt;
        &lt;cell&gt;.7z JSON Lines&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;--guild&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/g/&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Automatic Detection: Platform auto-detected from file extensions&lt;/item&gt;
      &lt;item&gt;Unified Search: PostgreSQL FTS searches across all platforms&lt;/item&gt;
      &lt;item&gt;Mixed Archives: Combine Reddit, Voat, and Ruqqus in single archive&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;29 MCP tools auto-generated from OpenAPI for AI assistants:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Full Archive Access: Query posts, comments, users, search via Claude Desktop or Claude Code&lt;/item&gt;
      &lt;item&gt;Token Overflow Prevention: Built-in LLM guidance with field selection and truncation&lt;/item&gt;
      &lt;item&gt;5 MCP Resources: Instant access to stats, top posts, subreddits, search help&lt;/item&gt;
      &lt;item&gt;Claude Code Ready: Copy-paste configuration for immediate use&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;{
  "mcpServers": {
    "reddarchiver": {
      "command": "uv",
      "args": ["--directory", "/path/to/mcp_server", "run", "python", "server.py"],
      "env": { "REDDARCHIVER_API_URL": "http://localhost:5000" }
    }
  }
}&lt;/code&gt;
    &lt;p&gt;See MCP Server Documentation for complete setup guide.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üì± Mobile-First Design: Responsive layout optimized for all devices with touch-friendly navigation&lt;/item&gt;
      &lt;item&gt;üîç Advanced Search System (Server Required): PostgreSQL full-text search optimized for Tor network. Search by keywords, subreddit, author, date, score. Requires Docker deployment - offline browsing uses sorted index pages.&lt;/item&gt;
      &lt;item&gt;‚ö° JavaScript Free: Complete functionality without JS, pure CSS interactions&lt;/item&gt;
      &lt;item&gt;üé® Theme Support: Built-in light/dark theme toggle with CSS-only implementation&lt;/item&gt;
      &lt;item&gt;‚ôø Accessibility: WCAG compliant with keyboard navigation and screen reader support&lt;/item&gt;
      &lt;item&gt;üöÑ Performance: Optimized CSS (29KB), designed for low-bandwidth networks&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üèóÔ∏è Modular Architecture: 18 specialized modules for maintainability and extensibility&lt;/item&gt;
      &lt;item&gt;üóÑÔ∏è PostgreSQL Backend: Large-scale processing with constant memory usage regardless of dataset size&lt;/item&gt;
      &lt;item&gt;‚ö° Lightning-Fast Search: PostgreSQL full-text search with GIN indexing&lt;/item&gt;
      &lt;item&gt;üåê REST API v1: 30+ endpoints with MCP/AI optimization for programmatic access to posts, comments, users, statistics, search, aggregations, and exports&lt;/item&gt;
      &lt;item&gt;üßÖ Tor-Optimized: Zero JavaScript, server-side search, no external dependencies&lt;/item&gt;
      &lt;item&gt;üìä Rich Statistics: Comprehensive analytics dashboard with file size tracking&lt;/item&gt;
      &lt;item&gt;üîó SEO Optimized: Complete meta tags, XML sitemaps, and structured data&lt;/item&gt;
      &lt;item&gt;üíæ Streaming Processing: Memory-efficient with automatic resume capability&lt;/item&gt;
      &lt;item&gt;üìà Progress Tracking: Real-time transfer rates, ETAs, and database metrics&lt;/item&gt;
      &lt;item&gt;üèÜ Instance Registry: Leaderboard system with completeness-weighted scoring for distributed archives&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üè† Local/Homelab: HTTP on localhost or LAN (2 commands)&lt;/item&gt;
      &lt;item&gt;üåê Production HTTPS: Automated Let's Encrypt setup (5 minutes)&lt;/item&gt;
      &lt;item&gt;üßÖ Tor Hidden Service: .onion access, zero networking config (2 minutes)&lt;/item&gt;
      &lt;item&gt;üîÄ Dual-Mode: HTTPS + Tor simultaneously&lt;/item&gt;
      &lt;item&gt;üìÑ Static Hosting: GitHub/Codeberg Pages for small archives (browse-only, no search)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Redd-Archiver generates static HTML files that can be browsed offline OR deployed with full-text search:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Search&lt;/cell&gt;
        &lt;cell role="head"&gt;Server&lt;/cell&gt;
        &lt;cell role="head"&gt;Setup Time&lt;/cell&gt;
        &lt;cell role="head"&gt;Use Case&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Offline Browsing&lt;/cell&gt;
        &lt;cell&gt;‚ùå Browse-only&lt;/cell&gt;
        &lt;cell&gt;None&lt;/cell&gt;
        &lt;cell&gt;0 min&lt;/cell&gt;
        &lt;cell&gt;USB drives, local archives, offline research&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Static Hosting&lt;/cell&gt;
        &lt;cell&gt;‚ùå Browse-only&lt;/cell&gt;
        &lt;cell&gt;GitHub/Codeberg Pages&lt;/cell&gt;
        &lt;cell&gt;10 min&lt;/cell&gt;
        &lt;cell&gt;Free public hosting (size limits)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Docker Local&lt;/cell&gt;
        &lt;cell&gt;‚úÖ PostgreSQL FTS&lt;/cell&gt;
        &lt;cell&gt;localhost&lt;/cell&gt;
        &lt;cell&gt;5 min&lt;/cell&gt;
        &lt;cell&gt;Development, testing&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Docker + Tor&lt;/cell&gt;
        &lt;cell&gt;‚úÖ PostgreSQL FTS&lt;/cell&gt;
        &lt;cell&gt;.onion hidden service&lt;/cell&gt;
        &lt;cell&gt;2 min&lt;/cell&gt;
        &lt;cell&gt;Private sharing, no port forwarding&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Docker + HTTPS&lt;/cell&gt;
        &lt;cell&gt;‚úÖ PostgreSQL FTS&lt;/cell&gt;
        &lt;cell&gt;Public domain&lt;/cell&gt;
        &lt;cell&gt;15 min&lt;/cell&gt;
        &lt;cell&gt;Production public archives&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Offline Browsing Features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sorted index pages (by score, comments, date)&lt;/item&gt;
      &lt;item&gt;Pagination for large subreddits&lt;/item&gt;
      &lt;item&gt;Full comment threads and user pages&lt;/item&gt;
      &lt;item&gt;Works by opening HTML files directly&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With Search Server:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PostgreSQL full-text search with GIN indexing&lt;/item&gt;
      &lt;item&gt;Search by keywords, subreddit, author, date, score&lt;/item&gt;
      &lt;item&gt;Sub-second results, Tor-compatible&lt;/item&gt;
      &lt;item&gt;Requires Docker deployment&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Internet content disappears every day. Communities get banned, platforms shut down, and valuable discussions vanish. You can help prevent this.&lt;/p&gt;
    &lt;p&gt;Don't wait for content to disappear. Download these datasets today:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Platform&lt;/cell&gt;
        &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;cell role="head"&gt;Posts&lt;/cell&gt;
        &lt;cell role="head"&gt;Download&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3.28TB&lt;/cell&gt;
        &lt;cell&gt;2.38B posts&lt;/cell&gt;
        &lt;cell&gt;Academic Torrents ¬∑ Magnet Link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Voat&lt;/cell&gt;
        &lt;cell&gt;~15GB&lt;/cell&gt;
        &lt;cell&gt;3.8M posts&lt;/cell&gt;
        &lt;cell&gt;Archive.org ‚Ä†&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Ruqqus&lt;/cell&gt;
        &lt;cell&gt;~752MB&lt;/cell&gt;
        &lt;cell&gt;500K posts&lt;/cell&gt;
        &lt;cell&gt;Archive.org ‚Ä°&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;‚Ä† Voat Performance Tip: Use pre-split files for 1000x faster imports (2-5 min vs 30+ min per subverse) ‚Ä° Ruqqus: Docker image includes p7zip for automatic .7z decompression&lt;/p&gt;
    &lt;p&gt;Every mirror matters. Store locally, seed torrents, share with researchers. Be part of the preservation network.&lt;/p&gt;
    &lt;p&gt;Already running an archive? Register it on our public leaderboard:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Deploy your instance (Quick Start - 2-15 minutes)&lt;/item&gt;
      &lt;item&gt;Submit via Registry Template&lt;/item&gt;
      &lt;item&gt;Join coordinated preservation efforts with other teams&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Benefits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Public visibility and traffic&lt;/item&gt;
      &lt;item&gt;Coordinated archiving to avoid duplication&lt;/item&gt;
      &lt;item&gt;Team collaboration opportunities&lt;/item&gt;
      &lt;item&gt;Leaderboard recognition&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;üëâ Register Your Instance Now ‚Üí&lt;/p&gt;
    &lt;p&gt;Found a new platform dataset? Help expand the archive network:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Lemmy databases&lt;/item&gt;
      &lt;item&gt;Hacker News archives&lt;/item&gt;
      &lt;item&gt;Alternative Reddit archives&lt;/item&gt;
      &lt;item&gt;Other link aggregator platforms&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why submit?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Makes data discoverable for other archivists&lt;/item&gt;
      &lt;item&gt;Prevents duplicate preservation efforts&lt;/item&gt;
      &lt;item&gt;Builds comprehensive multi-platform archive ecosystem&lt;/item&gt;
      &lt;item&gt;Tracks data availability before platforms disappear&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Main landing page showing archive overview with statistics for 9,592 posts across Reddit, Voat, and Ruqqus. Features customizable branding (site name, project URL), responsive cards, activity metrics, and content statistics. (Works offline)&lt;/p&gt;
    &lt;p&gt;Post listing with sorting options (score, comments, date), pagination, and badge coloring. Includes navigation and theme toggle. (Works offline - sorted by score/comments/date)&lt;/p&gt;
    &lt;p&gt;Individual post displaying nested comment threads with collapsible UI, user flair, and timestamps. Comments include anchor links for direct navigation from user pages. (Works offline)&lt;/p&gt;
    &lt;p&gt;Fully optimized for mobile devices with touch-friendly navigation and responsive layout.&lt;/p&gt;
    &lt;p&gt;PostgreSQL full-text search with Google-style operators. Supports filtering by subreddit, author, date range, and score. (Requires Docker deployment)&lt;/p&gt;
    &lt;p&gt;Search results with highlighted excerpts using PostgreSQL &lt;code&gt;ts_headline()&lt;/code&gt;. Sub-second response times with GIN indexing. (Server-based, Tor-compatible)&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Sample Archive: Multi-platform archive featuring programming and technology communities from Reddit, Voat, and Ruqqus ¬∑ See all screenshots ‚Üí&lt;/p&gt;
    &lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Python 3.7 or higher&lt;/item&gt;
      &lt;item&gt;PostgreSQL 12+ (required for v1.0+)&lt;/item&gt;
      &lt;item&gt;4GB+ RAM (PostgreSQL uses constant memory)&lt;/item&gt;
      &lt;item&gt;Disk space: ~1.5-2x your input .zst file size for PostgreSQL database&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Redd-Archiver uses modern, performance-focused dependencies:&lt;/p&gt;
    &lt;p&gt;Core:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;psycopg[binary,pool]==3.2.3&lt;/code&gt;- PostgreSQL adapter with connection pooling&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;zstandard==0.23.0&lt;/code&gt;- Fast .zst decompression&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;psutil==6.1.1&lt;/code&gt;- System resource monitoring&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;HTML Generation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;jinja2&amp;gt;=3.1.6&lt;/code&gt;- Modern template engine with inheritance&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;rcssmin&amp;gt;=1.1.2&lt;/code&gt;- CSS minification for smaller file sizes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Performance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;orjson&amp;gt;=3.11.4&lt;/code&gt;- Fast JSON parsing&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/19-84/redd-archiver.git
cd redd-archiver

# Copy environment template and configure
cp .env.example .env
# Edit .env with your settings (change default passwords!)

# Start PostgreSQL container
docker-compose up -d

# Install Python dependencies
pip install -r requirements.txt

# Configure database connection
export DATABASE_URL="postgresql://reddarchiver:your_password_here@localhost:5432/reddarchiver"

# Run the archive generator
python reddarc.py /path/to/data/ --output my-archive/&lt;/code&gt;
    &lt;code&gt;git clone https://github.com/19-84/redd-archiver.git
cd redd-archiver

# Install PostgreSQL (Ubuntu/Debian)
sudo apt update &amp;amp;&amp;amp; sudo apt install postgresql postgresql-contrib

# Or on macOS
brew install postgresql@16 &amp;amp;&amp;amp; brew services start postgresql@16

# Create database
sudo -u postgres createuser redd-archiver
sudo -u postgres createdb -O redd-archiver redd-archiver
sudo -u postgres psql -c "ALTER USER redd-archiver WITH PASSWORD 'your_password_here';"

# Install Python dependencies
pip install -r requirements.txt

# Configure database connection
export DATABASE_URL="postgresql://reddarchiver:your_password_here@localhost:5432/reddarchiver"

# Run the archive generator
python reddarc.py /path/to/data/ --output my-archive/&lt;/code&gt;
    &lt;p&gt;Review the CHANGELOG.md for version updates and changes.&lt;/p&gt;
    &lt;p&gt;Redd-Archiver processes data dumps from multiple platforms:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Platform&lt;/cell&gt;
        &lt;cell role="head"&gt;Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Data Sources&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;.zst JSON Lines&lt;/cell&gt;
        &lt;cell&gt;Pushshift Complete Dataset ¬∑ Magnet Link ¬∑ 3.28TB ¬∑ 2.38B posts ¬∑ 40K subreddits&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Voat&lt;/cell&gt;
        &lt;cell&gt;SQL dumps&lt;/cell&gt;
        &lt;cell&gt;Voat Archive 2021 ¬∑ 22,637 subverses ¬∑ 3.8M posts ¬∑ 24M comments ¬∑ Complete archive&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Ruqqus&lt;/cell&gt;
        &lt;cell&gt;.7z JSON Lines&lt;/cell&gt;
        &lt;cell&gt;Ruqqus Archive 2021 ¬∑ 6,217 guilds ¬∑ Complete archive&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Scanner Tools help you identify which communities to archive first based on priority scores:&lt;/p&gt;
    &lt;code&gt;# Scan Reddit data (generates subreddits_complete.json)
python tools/find_banned_subreddits.py /path/to/reddit-data/ --output tools/subreddits_complete.json

# Scan Voat data (generates subverses.json)
python tools/scan_voat_subverses.py /path/to/voat-data/ --output tools/subverses.json

# Scan Ruqqus data (generates guilds.json)
python tools/scan_ruqqus_guilds.py /path/to/ruqqus-data/ --output tools/guilds.json&lt;/code&gt;
    &lt;p&gt;What the scanners do:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Calculate archive priority scores (0-100) for each community&lt;/item&gt;
      &lt;item&gt;Track post counts, activity periods, deletion rates, NSFW content&lt;/item&gt;
      &lt;item&gt;Identify restricted, quarantined, or banned communities (highest priority)&lt;/item&gt;
      &lt;item&gt;Sort communities by archival importance&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example output:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reddit: 40,029 subreddits from 2.38B posts analyzed&lt;/item&gt;
      &lt;item&gt;Voat: 15,545 subverses from 3.81M posts + 24.1M comments analyzed&lt;/item&gt;
      &lt;item&gt;Ruqqus: 6,217 guilds from 500K posts analyzed&lt;/item&gt;
      &lt;item&gt;Status breakdown (Reddit): 26,552 active, 8,642 restricted, 4,803 inactive, 32 quarantined&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Use cases:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Targeted archiving: Archive high-risk communities first (restricted, quarantined)&lt;/item&gt;
      &lt;item&gt;Storage planning: Identify largest communities before downloading&lt;/item&gt;
      &lt;item&gt;Historical research: Find communities with high deletion/removal rates&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Output files (included in &lt;code&gt;tools/&lt;/code&gt; directory):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;subreddits_complete.json&lt;/code&gt;- Reddit subreddit statistics (40,029 communities, 46MB)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;subverses.json&lt;/code&gt;- Voat subverse statistics (22,585 communities, 14MB)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;guilds.json&lt;/code&gt;- Ruqqus guild statistics (6,217 communities, 3.6MB)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;View the complete data catalog to browse all communities and their priority scores.&lt;/p&gt;
    &lt;p&gt;Ensure DATABASE_URL is set (see Installation above):&lt;/p&gt;
    &lt;code&gt;export DATABASE_URL="postgresql://reddarchiver:password@localhost:5432/reddarchiver"&lt;/code&gt;
    &lt;p&gt;Reddit Archives (.zst files):&lt;/p&gt;
    &lt;code&gt;# Auto-discovery (processes all .zst files in directory)
python reddarc.py /path/to/pushshift-data/ --output my-archive/

# Single subreddit
python reddarc.py /data --subreddit privacy \
  --comments-file /data/privacy_comments.zst \
  --submissions-file /data/privacy_submissions.zst \
  --output my-archive/&lt;/code&gt;
    &lt;p&gt;Voat Archives (SQL dumps):&lt;/p&gt;
    &lt;code&gt;# Import Voat subverses
python reddarc.py /data --subverse voatdev,pics --output my-archive/ --import-only

# Export HTML after import
python reddarc.py /data --output my-archive/ --export-from-database&lt;/code&gt;
    &lt;p&gt;Ruqqus Archives (.7z files):&lt;/p&gt;
    &lt;code&gt;# Import Ruqqus guilds
python reddarc.py /data --guild Quarantine,News --output my-archive/ --import-only

# Export HTML after import
python reddarc.py /data --output my-archive/ --export-from-database&lt;/code&gt;
    &lt;p&gt;Multi-Platform Mixed Archive:&lt;/p&gt;
    &lt;code&gt;# Import from multiple platforms into single archive
python reddarc.py /reddit-data --subreddit privacy --output unified-archive/ --import-only
python reddarc.py /voat-data --subverse technology --output unified-archive/ --import-only
python reddarc.py /ruqqus-data --guild Tech --output unified-archive/ --import-only

# Generate HTML for all platforms
python reddarc.py /any-path --output unified-archive/ --export-from-database&lt;/code&gt;
    &lt;p&gt;With filtering and SEO:&lt;/p&gt;
    &lt;code&gt;python reddarc.py /data/ --output my-archive/ \
  --min-score 100 --min-comments 50 \
  --base-url https://example.com \
  --site-name "My Archive"&lt;/code&gt;
    &lt;p&gt;Import/Export workflow (for large datasets):&lt;/p&gt;
    &lt;code&gt;# Import data to PostgreSQL (no HTML generation)
python reddarc.py /data/ --output my-archive/ --import-only

# Export HTML from PostgreSQL (no data import)
python reddarc.py /data/ --output my-archive/ --export-from-database&lt;/code&gt;
    &lt;p&gt;Multiple deployment options available:&lt;/p&gt;
    &lt;p&gt;Local/Development (HTTP):&lt;/p&gt;
    &lt;code&gt;docker compose up -d
# Access: http://localhost&lt;/code&gt;
    &lt;p&gt;Production HTTPS (Let's Encrypt):&lt;/p&gt;
    &lt;code&gt;./docker/scripts/init-letsencrypt.sh
# Access: https://your-domain.com&lt;/code&gt;
    &lt;p&gt;Homelab/Tor (.onion hidden service):&lt;/p&gt;
    &lt;code&gt;docker compose -f docker-compose.yml -f docker-compose.tor-only.yml --profile tor up -d
# Access: http://[your-address].onion (via Tor Browser)
# No port forwarding or domain required!&lt;/code&gt;
    &lt;p&gt;Dual-Mode (HTTPS + Tor):&lt;/p&gt;
    &lt;code&gt;docker compose --profile production --profile tor up -d
# Access: Both https://your-domain.com and http://[address].onion&lt;/code&gt;
    &lt;p&gt;Static Hosting (GitHub/Codeberg Pages):&lt;/p&gt;
    &lt;code&gt;# Generate archive locally, push to GitHub/Codeberg
python reddarc.py /data --output archive/
cd archive/
git init &amp;amp;&amp;amp; git add . &amp;amp;&amp;amp; git commit -m "Initial archive"
git remote add origin https://github.com/username/repo.git
git push -u origin main
# Enable Pages in repository settings&lt;/code&gt;
    &lt;p&gt;See deployment guides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Docker Deployment Guide - Complete Docker setup with HTTPS and Tor&lt;/item&gt;
      &lt;item&gt;Tor Deployment Guide - Tor hidden service for homelab and privacy&lt;/item&gt;
      &lt;item&gt;Static Deployment Guide - GitHub Pages / Codeberg Pages (browse-only)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Processing Control:&lt;/p&gt;
    &lt;code&gt;--hide-deleted-comments    # Hide [deleted]/[removed] comments in output
--no-user-pages           # Skip user page generation (saves memory)
--dry-run                 # Preview discovered files without processing
--force-rebuild           # Ignore resume state and rebuild from scratch
--force-parallel-users    # Override auto-detection for parallel processing&lt;/code&gt;
    &lt;p&gt;Logging:&lt;/p&gt;
    &lt;code&gt;--log-file &amp;lt;path&amp;gt;         # Custom log file location (default: output/.archive-error.log)
--log-level DEBUG         # Set logging verbosity (DEBUG, INFO, WARNING, ERROR, CRITICAL)&lt;/code&gt;
    &lt;p&gt;Performance Tuning:&lt;/p&gt;
    &lt;code&gt;--debug-memory-limit 8.0      # Override memory limit in GB (default: auto-detect)
--debug-max-connections 8     # Override DB connection pool size (default: auto-detect)
--debug-max-workers 4         # Override parallel workers (default: auto-detect)&lt;/code&gt;
    &lt;p&gt;Environment Variables:&lt;/p&gt;
    &lt;code&gt;# Required
DATABASE_URL=postgresql://user:pass@host:5432/reddarchiver

# Optional Performance Tuning (auto-detected if not set)
REDDARCHIVER_MAX_DB_CONNECTIONS=8       # Connection pool size
REDDARCHIVER_MAX_PARALLEL_WORKERS=4     # Parallel processing workers
REDDARCHIVER_USER_BATCH_SIZE=2000       # User page batch size
REDDARCHIVER_QUEUE_MAX_BATCHES=10       # Queue backpressure control
REDDARCHIVER_CHECKPOINT_INTERVAL=10     # Progress save frequency
REDDARCHIVER_USER_PAGE_WORKERS=4        # User page generation workers&lt;/code&gt;
    &lt;p&gt;Redd-Archiver features a clean modular architecture with specialized components:&lt;/p&gt;
    &lt;code&gt;reddarc.py              # Main CLI entry point
search_server.py        # Flask search API server
version.py              # Version metadata

core/                   # Core processing &amp;amp; database
‚îú‚îÄ‚îÄ postgres_database.py    # PostgreSQL backend
‚îú‚îÄ‚îÄ postgres_search.py      # PostgreSQL FTS implementation
‚îú‚îÄ‚îÄ write_html.py           # HTML generation coordinator
‚îú‚îÄ‚îÄ watchful.py             # .zst streaming utilities
‚îú‚îÄ‚îÄ incremental_processor.py # Incremental processing
‚îî‚îÄ‚îÄ importers/              # Multi-platform importers
    ‚îú‚îÄ‚îÄ base_importer.py        # Abstract base class
    ‚îú‚îÄ‚îÄ reddit_importer.py      # .zst JSON Lines parser
    ‚îú‚îÄ‚îÄ voat_importer.py        # SQL dump coordinator
    ‚îú‚îÄ‚îÄ voat_sql_parser.py      # SQL INSERT parser
    ‚îî‚îÄ‚îÄ ruqqus_importer.py      # .7z JSON Lines parser

api/                    # REST API v1
‚îú‚îÄ‚îÄ __init__.py             # Blueprint registration
‚îî‚îÄ‚îÄ routes.py               # 30+ API endpoints

mcp_server/             # MCP Server for AI integration
‚îú‚îÄ‚îÄ server.py               # FastMCP server (29 tools)
‚îú‚îÄ‚îÄ README.md               # MCP documentation
‚îî‚îÄ‚îÄ tests/                  # MCP server tests

utils/                  # Utility functions
‚îú‚îÄ‚îÄ console_output.py       # Console output formatting
‚îú‚îÄ‚îÄ error_handling.py       # Error handling utilities
‚îú‚îÄ‚îÄ input_validation.py     # Input validation
‚îú‚îÄ‚îÄ regex_utils.py          # Regular expression utilities
‚îú‚îÄ‚îÄ search_operators.py     # Search query parsing
‚îî‚îÄ‚îÄ simple_json_utils.py    # JSON utilities

processing/             # Data processing modules
‚îú‚îÄ‚îÄ parallel_user_processing.py  # Parallel user page generation
‚îú‚îÄ‚îÄ batch_processing_utils.py    # Batch processing utilities
‚îî‚îÄ‚îÄ incremental_statistics.py    # Statistics tracking

monitoring/             # Performance &amp;amp; monitoring
‚îú‚îÄ‚îÄ performance_monitor.py      # Performance monitoring
‚îú‚îÄ‚îÄ performance_phases.py       # Phase tracking
‚îú‚îÄ‚îÄ performance_timing.py       # Timing utilities
‚îú‚îÄ‚îÄ auto_tuning_validator.py    # Auto-tuning validation
‚îú‚îÄ‚îÄ streaming_config.py         # Auto-detecting configuration
‚îî‚îÄ‚îÄ system_optimizer.py         # System optimization
&lt;/code&gt;
    &lt;code&gt;html_modules/
‚îú‚îÄ‚îÄ html_seo.py                # SEO, meta tags, sitemaps
‚îú‚îÄ‚îÄ html_pages_jinja.py        # Jinja2-based page generation
‚îú‚îÄ‚îÄ html_statistics.py         # Analytics and metrics
‚îú‚îÄ‚îÄ dashboard_helpers.py       # Dashboard utility functions
‚îú‚îÄ‚îÄ html_field_generation.py   # Dynamic field generation
‚îú‚îÄ‚îÄ jinja_filters.py           # Custom Jinja2 filters
‚îú‚îÄ‚îÄ html_pages.py              # Core page generation
‚îú‚îÄ‚îÄ html_comments.py           # Comment threading system
‚îú‚îÄ‚îÄ __init__.py                # Public API exports
‚îú‚îÄ‚îÄ jinja_env.py               # Jinja2 environment setup
‚îú‚îÄ‚îÄ html_utils.py              # File operations, utilities
‚îú‚îÄ‚îÄ html_dashboard_jinja.py    # Jinja2 dashboard rendering
‚îú‚îÄ‚îÄ css_minifier.py            # CSS minification
‚îú‚îÄ‚îÄ html_scoring.py            # Dynamic score badges
‚îú‚îÄ‚îÄ html_templates.py          # Template management
‚îú‚îÄ‚îÄ html_url.py                # URL processing, domains
‚îú‚îÄ‚îÄ html_dashboard.py          # Dashboard generation
‚îî‚îÄ‚îÄ html_constants.py          # Configuration values
&lt;/code&gt;
    &lt;code&gt;templates_jinja2/
‚îú‚îÄ‚îÄ base/
‚îÇ   ‚îî‚îÄ‚îÄ base.html              # Master layout template
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ dashboard_card.html    # Dashboard statistics cards
‚îÇ   ‚îú‚îÄ‚îÄ footer.html            # Site footer
‚îÇ   ‚îú‚îÄ‚îÄ global_summary.html    # Global statistics summary
‚îÇ   ‚îú‚îÄ‚îÄ navigation.html        # Site navigation bar
‚îÇ   ‚îú‚îÄ‚îÄ post_card.html         # Post display card
‚îÇ   ‚îú‚îÄ‚îÄ user_comment.html      # User comment display
‚îÇ   ‚îî‚îÄ‚îÄ user_post.html         # User post display
‚îú‚îÄ‚îÄ macros/
‚îÇ   ‚îú‚îÄ‚îÄ comment_macros.html    # Comment rendering macros
‚îÇ   ‚îî‚îÄ‚îÄ reddit_macros.html     # Reddit-specific macros
‚îî‚îÄ‚îÄ pages/
    ‚îú‚îÄ‚îÄ global_search.html     # Global search page
    ‚îú‚îÄ‚îÄ index.html             # Dashboard homepage
    ‚îú‚îÄ‚îÄ link.html              # Individual post page
    ‚îú‚îÄ‚îÄ subreddit.html         # Subreddit listing page
    ‚îî‚îÄ‚îÄ user.html              # User profile page
&lt;/code&gt;
    &lt;code&gt;sql/
‚îú‚îÄ‚îÄ schema.sql                 # PostgreSQL table definitions
‚îú‚îÄ‚îÄ indexes.sql                # Performance indexes (GIN, B-tree)
‚îú‚îÄ‚îÄ fix_statistics.sql         # Statistics maintenance queries
‚îî‚îÄ‚îÄ migrations/
    ‚îî‚îÄ‚îÄ 003_add_total_activity_column.sql  # Schema migration
&lt;/code&gt;
    &lt;p&gt;Redd-Archiver v1.0 uses PostgreSQL full-text search with GIN indexing for blazing-fast search capabilities:&lt;/p&gt;
    &lt;p&gt;Key Features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Database-Powered: Native PostgreSQL indexing with constant memory usage&lt;/item&gt;
      &lt;item&gt;Large-Scale: Efficiently search large datasets (tested with hundreds of GB)&lt;/item&gt;
      &lt;item&gt;Relevance Ranking: PostgreSQL &lt;code&gt;ts_rank()&lt;/code&gt;for intelligent result ordering&lt;/item&gt;
      &lt;item&gt;Highlighted Excerpts: &lt;code&gt;ts_headline()&lt;/code&gt;shows matching content in context&lt;/item&gt;
      &lt;item&gt;Advanced Filters: Search by subreddit, author, date range, score&lt;/item&gt;
      &lt;item&gt;Concurrent Queries: Handle multiple search requests simultaneously&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;PostgreSQL search is exposed via &lt;code&gt;postgres_search.py&lt;/code&gt; (CLI) and &lt;code&gt;search_server.py&lt;/code&gt; (Web API):&lt;/p&gt;
    &lt;p&gt;Command-Line Interface:&lt;/p&gt;
    &lt;code&gt;# Search command-line interface
python postgres_search.py "your query" --subreddit technology --limit 50

# Example: Search for posts about "machine learning" with high scores
python postgres_search.py "machine learning" --min-score 100 --limit 20&lt;/code&gt;
    &lt;p&gt;Web API (‚úÖ Implemented):&lt;/p&gt;
    &lt;code&gt;# Start search server with Docker Compose (recommended)
docker-compose up -d reddarchiver-search-server

# Or run directly
export DATABASE_URL="postgresql://user:pass@localhost:5432/reddarchiver"
python search_server.py

# Access at http://localhost:5000&lt;/code&gt;
    &lt;p&gt;Features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;RESTful search API with JSON responses&lt;/item&gt;
      &lt;item&gt;Real-time search with PostgreSQL FTS&lt;/item&gt;
      &lt;item&gt;Rate limiting and CSRF protection&lt;/item&gt;
      &lt;item&gt;Health check endpoint: &lt;code&gt;GET /health&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Search endpoint: &lt;code&gt;GET /search?q=query&amp;amp;subreddit=optional&amp;amp;limit=50&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Result highlighting with &lt;code&gt;ts_headline()&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Search suggestions and trending searches&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Full-featured API with 30+ endpoints for programmatic access and MCP/AI integration:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Category&lt;/cell&gt;
        &lt;cell role="head"&gt;Endpoints&lt;/cell&gt;
        &lt;cell role="head"&gt;Key Features&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;System (5)&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;/health&lt;/code&gt;, &lt;code&gt;/stats&lt;/code&gt;, &lt;code&gt;/schema&lt;/code&gt;, &lt;code&gt;/openapi.json&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Health checks, statistics, capability discovery, OpenAPI spec&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Posts (13)&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;/posts&lt;/code&gt;, &lt;code&gt;/posts/{id}&lt;/code&gt;, &lt;code&gt;/posts/{id}/comments&lt;/code&gt;, &lt;code&gt;/posts/{id}/context&lt;/code&gt;, &lt;code&gt;/posts/{id}/comments/tree&lt;/code&gt;, &lt;code&gt;/posts/{id}/related&lt;/code&gt;, &lt;code&gt;/posts/random&lt;/code&gt;, &lt;code&gt;/posts/aggregate&lt;/code&gt;, &lt;code&gt;/posts/batch&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;List, single, comments, context, tree, related, random, aggregate, batch&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Comments (7)&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;/comments&lt;/code&gt;, &lt;code&gt;/comments/{id}&lt;/code&gt;, &lt;code&gt;/comments/random&lt;/code&gt;, &lt;code&gt;/comments/aggregate&lt;/code&gt;, &lt;code&gt;/comments/batch&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;List, single, random, aggregate, batch&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Users (8)&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;/users&lt;/code&gt;, &lt;code&gt;/users/{username}&lt;/code&gt;, &lt;code&gt;/users/{username}/summary&lt;/code&gt;, &lt;code&gt;/users/{username}/posts&lt;/code&gt;, &lt;code&gt;/users/{username}/comments&lt;/code&gt;, &lt;code&gt;/users/aggregate&lt;/code&gt;, &lt;code&gt;/users/batch&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;List, profiles, summary, activity, aggregate, batch&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Subreddits (4)&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;/subreddits&lt;/code&gt;, &lt;code&gt;/subreddits/{name}&lt;/code&gt;, &lt;code&gt;/subreddits/{name}/summary&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;List, statistics, summary&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Search (3)&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;/search&lt;/code&gt;, &lt;code&gt;/search/explain&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Full-text search with operators, query debugging&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;MCP/AI-Optimized Features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Field Selection: &lt;code&gt;?fields=id,title,score&lt;/code&gt;for token optimization&lt;/item&gt;
      &lt;item&gt;Truncation Controls: &lt;code&gt;?max_body_length=500&amp;amp;include_body=false&lt;/code&gt;for response size management&lt;/item&gt;
      &lt;item&gt;Export Formats: &lt;code&gt;?format=csv|ndjson&lt;/code&gt;for data analysis&lt;/item&gt;
      &lt;item&gt;Batch Endpoints: Reduce N requests to 1 with &lt;code&gt;/posts|comments|users/batch&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Context Endpoints: Single-call discussion retrieval with &lt;code&gt;/posts/{id}/context&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Search Operators: Google-style syntax (&lt;code&gt;"exact"&lt;/code&gt;,&lt;code&gt;OR&lt;/code&gt;,&lt;code&gt;-exclude&lt;/code&gt;,&lt;code&gt;sub:&lt;/code&gt;,&lt;code&gt;author:&lt;/code&gt;,&lt;code&gt;score:&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Rate limited to 100 requests/minute. See API Documentation for complete reference.&lt;/p&gt;
    &lt;p&gt;Redd-Archiver supports a distributed registry system for tracking archive instances:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Instance Metadata: Configure via environment variables or CLI flags (&lt;code&gt;--site-name&lt;/code&gt;,&lt;code&gt;--contact&lt;/code&gt;,&lt;code&gt;--team-id&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Leaderboard Generator: Automated scoring based on archive completeness and content risk&lt;/item&gt;
      &lt;item&gt;Team Grouping: Group multiple instances under a team ID for coordinated archiving&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See Registry Setup Guide for configuration.&lt;/p&gt;
    &lt;p&gt;Constant Memory Usage:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;4GB RAM: Process large datasets efficiently (tested with hundreds of GB)&lt;/item&gt;
      &lt;item&gt;8GB RAM: Optimal for concurrent operations&lt;/item&gt;
      &lt;item&gt;16GB+ RAM: Ideal for parallel user page generation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Database Storage:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Input (.zst)&lt;/cell&gt;
        &lt;cell role="head"&gt;PostgreSQL DB&lt;/cell&gt;
        &lt;cell role="head"&gt;HTML Output&lt;/cell&gt;
        &lt;cell role="head"&gt;Example&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;93.6MB&lt;/cell&gt;
        &lt;cell&gt;~150MB&lt;/cell&gt;
        &lt;cell&gt;1.4GB&lt;/cell&gt;
        &lt;cell&gt;r/technology&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;100MB&lt;/cell&gt;
        &lt;cell&gt;~160MB&lt;/cell&gt;
        &lt;cell&gt;~1.5GB&lt;/cell&gt;
        &lt;cell&gt;Small archives&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;500MB&lt;/cell&gt;
        &lt;cell&gt;~800MB&lt;/cell&gt;
        &lt;cell&gt;~7.5GB&lt;/cell&gt;
        &lt;cell&gt;Research projects&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2GB&lt;/cell&gt;
        &lt;cell&gt;~3.2GB&lt;/cell&gt;
        &lt;cell&gt;~30GB&lt;/cell&gt;
        &lt;cell&gt;Large collections&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;100GB&lt;/cell&gt;
        &lt;cell&gt;~160GB&lt;/cell&gt;
        &lt;cell&gt;~1.5TB&lt;/cell&gt;
        &lt;cell&gt;Enterprise-scale&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Processing Speed:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Data Import: Fast streaming ingestion to PostgreSQL&lt;/item&gt;
      &lt;item&gt;HTML Generation: Efficient database-backed rendering&lt;/item&gt;
      &lt;item&gt;Search Index: Instant with PostgreSQL GIN indexes&lt;/item&gt;
      &lt;item&gt;Performance: Scales with dataset size, optimized for large archives&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Performance varies based on dataset size, query complexity, and hardware:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PostgreSQL FTS: Fast indexed search for large datasets&lt;/item&gt;
      &lt;item&gt;GIN Indexes: Optimized index lookups for text search&lt;/item&gt;
      &lt;item&gt;Concurrent Queries: Supports multiple simultaneous searches with connection pooling&lt;/item&gt;
      &lt;item&gt;Memory Efficient: Constant memory usage with streaming results&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;PostgreSQL v1.0 Features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Large-Scale Processing: Efficiently handle large datasets (tested with hundreds of GB)&lt;/item&gt;
      &lt;item&gt;Constant Memory: 4GB RAM regardless of dataset size&lt;/item&gt;
      &lt;item&gt;Fast Search: PostgreSQL FTS with GIN indexing&lt;/item&gt;
      &lt;item&gt;Resume Capability: Database-backed progress tracking&lt;/item&gt;
      &lt;item&gt;Concurrent Processing: Multi-connection pool for parallel operations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Redd-Archiver has been tested with archives up to hundreds of gigabytes. For optimal performance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tested scale: Hundreds of GB per instance&lt;/item&gt;
      &lt;item&gt;Memory usage: Constant 4GB RAM regardless of dataset size&lt;/item&gt;
      &lt;item&gt;Database: PostgreSQL handles large datasets efficiently&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For very large archive collections (multiple terabytes), deploy multiple instances divided by topic:&lt;/p&gt;
    &lt;p&gt;Architecture:&lt;/p&gt;
    &lt;code&gt;‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Instance 1     ‚îÇ     ‚îÇ  Instance 2     ‚îÇ     ‚îÇ  Instance 3     ‚îÇ
‚îÇ  Technology     ‚îÇ     ‚îÇ  Gaming         ‚îÇ     ‚îÇ  Science        ‚îÇ
‚îÇ  Subreddits     ‚îÇ     ‚îÇ  Subreddits     ‚îÇ     ‚îÇ  Subreddits     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;
    &lt;p&gt;Benefits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Efficient search: Each database stays manageable size&lt;/item&gt;
      &lt;item&gt;Distributed load: Parallel processing across instances&lt;/item&gt;
      &lt;item&gt;Topic organization: Logical grouping of related content&lt;/item&gt;
      &lt;item&gt;Independent scaling: Scale individual topics as needed&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Deployment Options:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Single server: Multiple Docker Compose stacks with different ports&lt;/item&gt;
      &lt;item&gt;Multiple servers: One instance per physical/virtual machine&lt;/item&gt;
      &lt;item&gt;Topic-based domains: tech.archive.com, gaming.archive.com, etc.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example Multi-Instance Setup:&lt;/p&gt;
    &lt;code&gt;# Instance 1: Technology topics (port 8080)
cd /archives/tech
docker compose up -d

# Instance 2: Gaming topics (port 8081)
cd /archives/gaming
docker compose -f docker-compose.yml up -d

# Instance 3: Science topics (port 8082)
cd /archives/science
docker compose -f docker-compose.yml up -d&lt;/code&gt;
    &lt;p&gt;When to Use:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Archive collection exceeds 500GB&lt;/item&gt;
      &lt;item&gt;Search performance degrades with single instance&lt;/item&gt;
      &lt;item&gt;Logical topic divisions exist in your archive&lt;/item&gt;
      &lt;item&gt;Want to distribute load across multiple servers&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Studying online discourse and community dynamics&lt;/item&gt;
      &lt;item&gt;Analyzing social movements and trends&lt;/item&gt;
      &lt;item&gt;Preserving internet culture&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Backing up subreddits before potential removal&lt;/item&gt;
      &lt;item&gt;Creating offline-accessible community resources&lt;/item&gt;
      &lt;item&gt;Distributing knowledge repositories&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pattern analysis in deleted/removed content&lt;/item&gt;
      &lt;item&gt;User behavior studies&lt;/item&gt;
      &lt;item&gt;Content moderation research&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Docker Deployment Guide - Complete Docker setup including PostgreSQL, nginx, HTTPS, and Tor&lt;/item&gt;
      &lt;item&gt;Tor Deployment Guide - Tor hidden service setup for homelab and privacy deployments&lt;/item&gt;
      &lt;item&gt;Static Deployment Guide - GitHub Pages and Codeberg Pages deployment (browse-only, no search)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;REST API Documentation - Complete API reference with 30+ endpoints&lt;/item&gt;
      &lt;item&gt;MCP Server Documentation - AI integration with Claude Desktop/Claude Code&lt;/item&gt;
      &lt;item&gt;Registry Setup Guide - Instance registry configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CONTRIBUTING.md - Development guidelines and contribution procedures&lt;/item&gt;
      &lt;item&gt;SECURITY.md - Security policy and vulnerability reporting&lt;/item&gt;
      &lt;item&gt;LICENSE - Unlicense (public domain)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We welcome contributions! Please see CONTRIBUTING.md for development guidelines, code structure, and testing procedures.&lt;/p&gt;
    &lt;p&gt;Key areas for contribution:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PostgreSQL query optimizations&lt;/item&gt;
      &lt;item&gt;Additional export formats&lt;/item&gt;
      &lt;item&gt;Enhanced search features&lt;/item&gt;
      &lt;item&gt;Documentation improvements&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See our modular architecture (18 specialized modules) for easy entry points to contribute.&lt;/p&gt;
    &lt;p&gt;This is free and unencumbered software released into the public domain. See the LICENSE file (Unlicense) for details.&lt;/p&gt;
    &lt;p&gt;Anyone is free to copy, modify, publish, use, compile, sell, or distribute this software for any purpose, commercial or non-commercial, and by any means.&lt;/p&gt;
    &lt;p&gt;This project leverages public datasets from the following sources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pushshift - Reddit data access and archival infrastructure&lt;/item&gt;
      &lt;item&gt;Watchful1's PushshiftDumps - Comprehensive data dump tools and torrent management&lt;/item&gt;
      &lt;item&gt;Arctic Shift - Making Reddit data accessible to researchers and the public&lt;/item&gt;
      &lt;item&gt;Ruqqus Public Dataset - 752 MB Ruqqus archive (comments and submissions)&lt;/item&gt;
      &lt;item&gt;SearchVoat Archive - 16.8 GB Voat.co complete backup&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project builds upon the work of several excellent archival projects:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;reddit-html-archiver by libertysoft3 - Original inspiration and foundation for static HTML generation&lt;/item&gt;
      &lt;item&gt;redarc - Self-hosted Reddit archiving with PostgreSQL and full-text search&lt;/item&gt;
      &lt;item&gt;red-arch - Static website generator for Reddit subreddit archives&lt;/item&gt;
      &lt;item&gt;zst_blocks_format - Efficient block-based compression format for processing large datasets&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GitHub Issues: Report bugs or request features&lt;/item&gt;
      &lt;item&gt;GitHub Discussions: Ask questions or share ideas&lt;/item&gt;
      &lt;item&gt;Security Issues: Report via GitHub Security Advisories&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Redd-Archiver was built by one person over 6 months as a labor of love to preserve internet history before it disappears forever.&lt;/p&gt;
    &lt;p&gt;This isn't backed by a company or institution‚Äîjust an individual committed to keeping valuable discussions accessible. Your support helps:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Continue development and bug fixes&lt;/item&gt;
      &lt;item&gt;Maintain documentation and support&lt;/item&gt;
      &lt;item&gt;Cover infrastructure costs (servers, storage, bandwidth)&lt;/item&gt;
      &lt;item&gt;Preserve more data sources and platforms&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Every donation, no matter the size, helps keep this preservation effort alive.&lt;/p&gt;
    &lt;code&gt;bc1q8wpdldnfqt3n9jh2n9qqmhg9awx20hxtz6qdl7
&lt;/code&gt;
    &lt;code&gt;42zJZJCqxyW8xhhWngXHjhYftaTXhPdXd9iJ2cMp9kiGGhKPmtHV746EknriN4TNqYR2e8hoaDwrMLfv7h1wXzizMzhkeQi
&lt;/code&gt;
    &lt;p&gt;Thank you for supporting internet archival efforts! Every contribution helps maintain and improve this project.&lt;/p&gt;
    &lt;p&gt;This software is provided "as is" under the Unlicense. See LICENSE for details. Users are responsible for compliance with applicable laws and terms of service when processing data.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46602324</guid><pubDate>Tue, 13 Jan 2026 15:35:09 +0000</pubDate></item><item><title>What a year of solar and batteries saved us in 2025</title><link>https://scotthelme.co.uk/what-a-year-of-solar-and-batteries-really-saved-us-in-2025/</link><description>&lt;doc fingerprint="af53e052cbb4a5d8"&gt;
  &lt;main&gt;
    &lt;p&gt;Throughout 2025, I spoke a few times about our home energy solution, including our grid usage, our solar array and our Tesla Powerwall batteries. Now that I have a full year of data, I wanted to take a look at exactly how everything is working out, and, in alignment with our objectives, how much money we've saved!&lt;/p&gt;
    &lt;head rend="h4"&gt;Our setup&lt;/head&gt;
    &lt;p&gt;Just to give a quick overview of what we're working with, here are the details on our solar, battery and tariff situation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚òÄÔ∏èSolar Panels: We have 14x Perlight solar panels managed by Enphase that make up the 4.2kWp array on our roof, and they produce energy when the sun shines, which isn't as often as I'd like in the UK!&lt;/item&gt;
      &lt;item&gt;üîãTesla Powerwalls: We have 3x Tesla Powerwall 2 in our garage that were purchased to help us load-shift our energy usage. Electricity is very expensive in the UK and moving from peak usage which is 05:30 to 23:30 at ~¬£0.28/kWh, to off-peak usage, which is 23:30 - 05:30 at ~¬£0.07/kWh, is a significant cost saving.&lt;/item&gt;
      &lt;item&gt;üí°Smart Tariff: My wife and I both drive electric cars and our electricity provider, Octopus Energy, has a Smart Charging tariff. If we plug in one of our cars, and cheap electricity is available, they will activate the charger and allow us to use the off-peak rate, even at peak times.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now that we have some basic info, let's get into the details!&lt;/p&gt;
    &lt;head rend="h4"&gt;Grid Import&lt;/head&gt;
    &lt;p&gt;I have 3 sources of data for our grid import, and all of them align pretty well in terms of their measurements. I have the amount our electricity supplier charged us for, I have my own CT Clamp going via a Shelly EM that feeds in to Home Assistant, and I have the Tesla Gateway which controls all grid import into our home.&lt;/p&gt;
    &lt;p&gt;Starting with my Home Assistant data, these are the relevant readings.&lt;/p&gt;
    &lt;p&gt;Jan 1st 2025 - 15,106.10 kWh&lt;lb/&gt;Dec 31st 2025 - 36,680.90 kWh&lt;lb/&gt;Total: 21,574.80 kWh&lt;lb/&gt;Total Import: 21.6 MWh&lt;/p&gt;
    &lt;p&gt;As you can see in the graph, during the summer months we have slightly lower grid usage and the graph line climbs at a lower rate, but overall, we have pretty consistent usage. Looking at what our energy supplier charged, us for, that comes in slightly lower.&lt;/p&gt;
    &lt;p&gt;Total Import: 20.1 MWh&lt;/p&gt;
    &lt;p&gt;I'm going to use the figure provided by our energy supplier in my calculations because their equipment is likely more accurate than mine, and also, what they're charging me is the ultimate thing that matters. The final source is our Tesla Gateway, which shows us having imported 21.0 MWh.&lt;/p&gt;
    &lt;p&gt;It's great to see how all of these sources of data align so poorly! üòÖ&lt;/p&gt;
    &lt;head rend="h4"&gt;Grid Export&lt;/head&gt;
    &lt;p&gt;Looking at our export, the graph tells a slightly different story because, as you can see, we didn't really start exporting properly until June, when our export tariff was activated. Prior to June, it simply wasn't worth exporting as we were only getting ¬£0.04/kWh but at the end of May, our export tariff went live and we were then getting paid ¬£0.15/kWh for export. My first and second blog posts cover the full details of this change when it happened if you'd like to read them but for now, just note that it will change the calculations a little later as we only had export for 60% of the year.&lt;/p&gt;
    &lt;p&gt;Total Export: 6.0 MWh&lt;/p&gt;
    &lt;p&gt;With our grid export covered the final piece of the puzzle is to look at our solar.&lt;/p&gt;
    &lt;head rend="h4"&gt;Solar Production&lt;/head&gt;
    &lt;p&gt;We're really not in the best part of the world for generating solar power, but we've still managed to produce quite a bit of power. Even in the most ideal, perfect scenario, our solar array can only generate 4.2kW of power, and we're definitely never getting near that. Our peak production was 2.841kW on 8th July at 13:00, and you can see our full annual production graph here.&lt;/p&gt;
    &lt;p&gt;Looking at the total energy production for the entire array, you can see it pick up through the sunnier months but remain quite flat during the darker days of the year.&lt;/p&gt;
    &lt;p&gt;Jan 1st 2025 - 2.709 MWh&lt;lb/&gt;Dec 31st 2025 - 5.874 MWh&lt;lb/&gt;Solar Production: 3.2 MWh&lt;/p&gt;
    &lt;p&gt;Just to confirm, I also took a look at the Enphase app, which is drawing it's data from the same source to be fair, and it agrees with the 3.2 MWh of generation.&lt;/p&gt;
    &lt;head rend="h4"&gt;Calculating the savings&lt;/head&gt;
    &lt;p&gt;This isn't exactly straightforward because of the combination of our solar array and excess import/export due to the batteries, but here are the numbers I'm currently working on.&lt;/p&gt;
    &lt;p&gt;Total Import: 20.1 MWh&lt;lb/&gt;Total Export: 6.0 MWh&lt;lb/&gt;Solar Production: 3.2 MWh&lt;/p&gt;
    &lt;p&gt;That gives us a total household usage of 17.3 MWh.&lt;/p&gt;
    &lt;p&gt;(20.1 MWh import + 3.2 MWh solar) ‚àí 6.0 MWh export = 17.3 MWh usage&lt;/p&gt;
    &lt;p&gt;If we didn't have the solar array providing power, the full 17.3 MWh of consumption would have been chargeable from our provider. If we had only the solar and no battery, assuming a perfect ability to utilise our solar generation, only 14.1 MWh of our usage would need to be imported. The cost of those units of solar generation can be viewed at the peak and off-peak rates as follows.&lt;/p&gt;
    &lt;p&gt;Peak rate: 3,200 kWh x ¬£0.28/kWh = ¬£896&lt;lb/&gt;Off-peak rate: 3,200 kWh x ¬£0.07/kWh = ¬£224&lt;/p&gt;
    &lt;p&gt;Given that solar panels only produce during peak electricity rates, it would be reasonable to use the higher price here. A consideration for us though is that we do have batteries, and we're able to load-shift all of our usage into the off-peak rate, so arguably the solar panels only made ¬£224 of electricity.&lt;/p&gt;
    &lt;p&gt;The bigger savings come when we start to look at the cost of the grid import. Assuming we had no solar panels, we'd have imported 17.3 MWh of electricity, and with the solar panels and perfect utilisation, we'd have imported 14.1 MWh of electricity. That's quite a lot of electricity and calculating the different costs of peak vs. off-peak by using batteries to load shift our usage gives some quite impressive results.&lt;/p&gt;
    &lt;p&gt;Peak rate: 17,300 kWh x ¬£0.28/kWh = ¬£4,844&lt;lb/&gt;Peak rate with solar: 14,100 kWh x ¬£0.28 = ¬£3,948&lt;/p&gt;
    &lt;p&gt;Off-peak rate: 17,300 kWh x ¬£0.07/kWh = ¬£1,211&lt;lb/&gt;Off-peak rate with solar: 14,100 kWh x ¬£0.07/kWh = ¬£987&lt;/p&gt;
    &lt;p&gt;This means there's a potential swing from ¬£4,844 down to ¬£987 with solar and battery, a total potential saving of ¬£3,857!&lt;/p&gt;
    &lt;p&gt;This also tracks if we look at our monthly spend on electricity which went from ¬£350-¬£400 per month down to ¬£50-¬£100 per month depending on the time of year. But it gets better.&lt;/p&gt;
    &lt;head rend="h4"&gt;Exporting excess energy&lt;/head&gt;
    &lt;p&gt;Our solar array generates almost nothing in the winter months so our batteries are sized to allow for a full day of usage with basically no solar support. We can go from the start of the peak rate at 05:30 all the way to the off-peak rate at 23:30 without using any grid power. When it comes to the summer months, though, our solar array is producing a lot of power and we clearly have a capability to export a lot more. The batteries can fill up on the off-peak rate overnight at ¬£0.07/kWh, and then export it during the peak rate for ¬£0.15/kWh, meaning any excess solar production or battery capacity can be exported for a reasonable amount.&lt;/p&gt;
    &lt;p&gt;If we take a look at the billing information from our energy supplier, we can see that during July, our best month for solar production, we exported a lot of energy. We exported so much energy that it actually fully offset our electricity costs and allowed us to go negative, meaning we were earning money back.&lt;/p&gt;
    &lt;p&gt;Here is our electricity import data:&lt;/p&gt;
    &lt;p&gt;And here is our electricity export data:&lt;/p&gt;
    &lt;p&gt;That's a pretty epic scenario, despite us being such high energy consumers, to still have the ability to full cover our costs and even earn something back! For clarity, we will still have the standing charge component of our bill, which is ¬£0.45/day so about ¬£13.50 per month to go on any given month, but looking at the raw energy costs, it's impressive.&lt;/p&gt;
    &lt;head rend="h4"&gt;The final calculation&lt;/head&gt;
    &lt;p&gt;I pulled all of our charges for electricity in 2025 to see just how close my calculations were and to double check everything I was thinking. Earlier, I gave these figures:&lt;/p&gt;
    &lt;p&gt;Off-peak rate: 17,300 kWh x ¬£0.07/kWh = ¬£1,211&lt;/p&gt;
    &lt;p&gt;If 100% of our electricity usage was at the off-peak rate, we should have paid ¬£1,211 for the year. Adding up all of our monthly charges, our total for the year was ¬£1,608.11 all in, but we need to subtract our standing charge from that.&lt;/p&gt;
    &lt;p&gt;Total cost = ¬£1,608.11 - (365 * ¬£0.45)&lt;lb/&gt;Total import = ¬£1,443.86&lt;/p&gt;
    &lt;p&gt;This means that we got almost all of our usage at the off-peak rate which is an awesome achievement! After the charges for electricity, I then tallied up all of our payments for export.&lt;/p&gt;
    &lt;p&gt;Total export = ¬£886.49&lt;/p&gt;
    &lt;p&gt;Another pretty impressive achievement, earning so much in export, which also helps to bring our net electricity cost in 2025 to ¬£557.37! To put this another way, the effective rate of our electricity is now just ¬£0.03/kWh.&lt;/p&gt;
    &lt;p&gt;¬£557.37 / 17,300kWh = ¬£0.03/kWh&lt;/p&gt;
    &lt;head rend="h4"&gt;But was it all worth it?&lt;/head&gt;
    &lt;p&gt;That's a tricky question to answer, and everyone will have different objectives and desired outcomes, but ours was pretty clear. Running two Electric Vehicles, having two adults working from home full time, me having servers and equipment at home, along with a power hungry hot tub, we were spending too much per month in electricity alone, and our goal was to reduce that.&lt;/p&gt;
    &lt;p&gt;Of course, it only makes sense to spend money reducing our costs if we reduce them enough to pay back the investment in the long term, and things are looking good so far. Here are the costs for our installations:&lt;/p&gt;
    &lt;p&gt;¬£17,580 - Powerwalls #1 and #2 installed.&lt;lb/&gt;¬£13,940 - Solar array installed.&lt;lb/&gt;¬£7,840 - Powerwall #3 installed.&lt;lb/&gt;Total cost = ¬£39,360&lt;/p&gt;
    &lt;p&gt;If we assume even a generous 2/3 - 1/3 split between peak and off-peak usage, with no Powerwalls or solar array, our electricity costs for 2025 would have been ¬£3,632.86:&lt;/p&gt;
    &lt;p&gt;11,533 kWh x ¬£0.28/kWh = ¬£3,229.24&lt;lb/&gt;5,766 kWh x ¬£0.07/kWh = ¬£403.62&lt;lb/&gt;Total = ¬£3,632.86&lt;/p&gt;
    &lt;p&gt;Instead, our costs were only ¬£557.37, meaning we saved ¬£3,078.49 this year. We also only had export capabilities for 7 months of 2025, so in 2026 when we will have 12 months of export capabilities, we should further reduce our costs. I anticipate that in 2026 our electricity costs for the year will be ~¬£0, and that's our goal.&lt;/p&gt;
    &lt;p&gt;Having our full costs returned in ~11 years is definitely something we're happy with, and we've also had protection against several power outages in our area along the way, which is a very nice bonus. Another way to look at this is that the investment is returning ~9%/year.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Year&lt;/cell&gt;
        &lt;cell role="head"&gt;Cumulative savings (¬£)&lt;/cell&gt;
        &lt;cell role="head"&gt;ROI (%)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;3,632.86&lt;/cell&gt;
        &lt;cell&gt;9.23%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;7,265.72&lt;/cell&gt;
        &lt;cell&gt;18.46%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;10,898.58&lt;/cell&gt;
        &lt;cell&gt;27.69%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;14,531.44&lt;/cell&gt;
        &lt;cell&gt;36.92%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;18,164.30&lt;/cell&gt;
        &lt;cell&gt;46.15%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;21,797.16&lt;/cell&gt;
        &lt;cell&gt;55.38%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;25,430.02&lt;/cell&gt;
        &lt;cell&gt;64.61%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;29,062.88&lt;/cell&gt;
        &lt;cell&gt;73.84%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;32,695.74&lt;/cell&gt;
        &lt;cell&gt;83.07%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;36,328.60&lt;/cell&gt;
        &lt;cell&gt;92.30%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;54,492.90&lt;/cell&gt;
        &lt;cell&gt;138.43%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;20&lt;/cell&gt;
        &lt;cell&gt;72,657.20&lt;/cell&gt;
        &lt;cell&gt;184.61%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;25&lt;/cell&gt;
        &lt;cell&gt;90,821.50&lt;/cell&gt;
        &lt;cell&gt;230.76%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Of course, at some point during that period, the effective value of the installation will reduce to almost ¬£0, and we have to consider that, but it's doing pretty darn good. If we hadn't needed to add that third Powerwall, this would have been so much better too. We'll see what the future holds, but with the inevitable and continued rise of energy costs, and talk of moving the standing charge on to our unit rate, things might look even better in the future.&lt;/p&gt;
    &lt;head rend="h4"&gt;Onwards to 2026!&lt;/head&gt;
    &lt;p&gt;Now that we have everything properly set up, and I'm happy with all of our Home Assistant automations, we're going to see how 2026 goes. I will definitely circle back in a year from now and see how the numbers played out, and until then, I hope the information here has been useful or interesting üëç&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46602532</guid><pubDate>Tue, 13 Jan 2026 15:49:27 +0000</pubDate></item><item><title>Influencers and OnlyFans models are dominating U.S. O-1 visa requests</title><link>https://www.theguardian.com/us-news/2026/jan/11/onlyfans-influencers-us-o-1-visa</link><description>&lt;doc fingerprint="32439b71b1a3afb9"&gt;
  &lt;main&gt;
    &lt;p&gt;Content creators and influencers in the US are now increasingly applying for O-1 work visas. Astoundingly, the number of O-1 visas granted each year increased by 50% between 2014 and 2024, as noted by recent reporting in the Financial Times.&lt;/p&gt;
    &lt;p&gt;These visas allow non-immigrants to work temporarily in the US. The O-1 category includes the O-1A, which is designated for individuals with extraordinary ability in the sciences, education, business or athletics and the O-1B, reserved for those with ‚Äúextraordinary ability or achievement‚Äù.&lt;/p&gt;
    &lt;p&gt;The Guardian spoke with some influencers who have had success in obtaining or are still trying to obtain the coveted O-1 visa and talked about what was involved in their process.&lt;/p&gt;
    &lt;p&gt;Julia Ain decided to post some videos of herself on social media at the height of the Covid-19 lockdown, when she was a student at McGill University.&lt;/p&gt;
    &lt;p&gt;‚ÄúI was bored during the pandemic ‚Äì like everyone else ‚Äì and started posting on TikTok,‚Äù she told the Guardian. ‚ÄúI started livestreaming, and I grew a fanbase kind of quickly.‚Äù&lt;/p&gt;
    &lt;p&gt;Five years later, the 25-year-old Canadian content creator now has 1.3 million followers combined across various social media platforms. Her influencer success led her to an O-1 visa.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt became really obvious that you could make a lot of money doing this in a short period of time,‚Äù she said. ‚ÄúIt felt like a very time-sensitive thing. Nobody knows how long this is going to last for.‚Äù&lt;/p&gt;
    &lt;p&gt;Ain posts photos and videos across Instagram, TikTok, X and Snapchat, sometimes in collaboration with other creators. Of her brand, she says: ‚ÄúMy whole thing is being the funny Jewish girl with big boobs.‚Äù The majority of Ain‚Äôs income is from Fanfix, a safe-for-work subscription based platform for influencers to monetize their content. She first applied for the O-1B Visa after launching on the platform in August 2023, and the company ended up sponsoring her application. She now says she makes five figures per month on the platform.&lt;/p&gt;
    &lt;p&gt;Luca Mornet also began making content during the pandemic while he was a student at the Fashion Institute of Technology in New York. Mornet, who is from France, realized soon that his F-1 student visa was holding him back from making money as an influencer.&lt;/p&gt;
    &lt;p&gt;‚ÄúI became friends with so many [other influencers], and I would always see them work with so many people and brands and agencies. And I always was so annoyed that I couldn‚Äôt because I was a student,‚Äù he said.&lt;/p&gt;
    &lt;p&gt;He applied for the O-1B Visa shortly after graduating, during which he could finally make money from influencing while on his OPT, a 12-month work authorization for international students post-graduation.&lt;/p&gt;
    &lt;p&gt;The O-1B visa, once reserved for Hollywood titans and superstar musicians, has evolved over the years.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe started doing [O-1 visa applications] for kids who are e-sport players and influencers and the OnlyFans crew,‚Äù said Michael Wildes, an immigration attorney and managing partner of Wildes &amp;amp; Weinberg. ‚ÄúIt‚Äôs the new, sexy medium for people to be a part of.‚Äù&lt;/p&gt;
    &lt;p&gt;Wildes has worked with the likes of musician Sin√©ad O‚ÄôConnor, soccer star Pel√©, and restaurateur Jean-Georges Vongerichten. His father, Leon Wildes, who started the firm in 1960, defended John Lennon and Yoko Ono against deportation during the Nixon administration, and helped facilitate the creation of the O-1B visa, which was established by the Immigration Act of 1990. Wildes‚Äôs client roster now includes social media influencers and Twitch streamers.&lt;/p&gt;
    &lt;p&gt;To qualify for an O-1B visa, applicants must submit evidence of at least three of the six regulatory criteria, which include performing in a distinguished production or event, national or international recognition for achievements, and a record of commercial or critically acclaimed successes. In 2026, though, these criteria are being stretched to encompass the accolades of an influencer.&lt;/p&gt;
    &lt;p&gt;In Ain‚Äôs application, she highlighted her sizable income and social media metrics.&lt;/p&gt;
    &lt;p&gt;‚ÄúPart of my application was: ‚ÄòI have 200,000 followers on this app, 300,000 followers on this app, 10 million people watch me here every month,‚Äô‚Äù she said. ‚ÄúThis isn‚Äôt just, ‚ÄòOh, you had one viral video and people watched that.‚Äô No, you‚Äôve got a following now that are not only watching you, but also paying for your content actively month after month.‚Äù&lt;/p&gt;
    &lt;p&gt;Social media was an integral part of the O-1B visa application of Dina Belenkaya, a Russian Israeli chess player and content creator ‚Äì which was approved in December 2023.&lt;/p&gt;
    &lt;p&gt;‚ÄúMy followings on Instagram (1.2 million), Twitch (108,000) and YouTube (799,000) were included as part of my profile, and I listed my follower counts on each platform,‚Äù she said. After her visa approval, she moved to Charlotte, North Carolina ‚Äì widely considered the chess capital of the United States.&lt;/p&gt;
    &lt;p&gt;While a certain number of followers may not be an automatic ticket to the US, one viral music group has been trying their luck. Boy Throb, comprising Anthony Key, Evan Papier, Zachary Sobania and Darshan Magdum, spent the past few months campaigning to reach 1 million followers on TikTok so that Magdum could use the stat on his O-1 visa application. Clad in matching pink jumpsuits, the three US-based bandmates danced together on screen to parody lyrics of hit songs, while Magdum was edited in from India.&lt;/p&gt;
    &lt;p&gt;Within a month of their first post, Boy Throb reached their goal of 1 million followers. Whether it will help Magdum get a visa remains unclear.&lt;/p&gt;
    &lt;p&gt;‚ÄúHonestly, the entire immigration process has been so complicated and there have been so many people who don‚Äôt believe us when we say we‚Äôre doing everything in our power to get Darshan here,‚Äù the group said.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe‚Äôre not sure how much longer we want to keep going without Darshan here and the process has been really expensive,‚Äù they added. In total, the band has spent more than $10,000 in legal and processing fees.&lt;/p&gt;
    &lt;p&gt;The rise in content creators applying for visas given out on the basis of ‚Äúextraordinary ability‚Äù has garnered a variety of reactions. Dominic Michael Tripi, a political analyst and writer, posted on X that the trend was indicative of ‚Äúend-stage empire conditions. It‚Äôs sad.‚Äù Legal professionals like Wildes, however, argue that the creator economy is the next frontier of American exceptionalism.&lt;/p&gt;
    &lt;p&gt;‚ÄúInfluencers are filling a large gap in the retail and commercial interests of the world,‚Äù he said. ‚ÄúThey‚Äôre moving content and purchases like no other. Immigration has to keep up with this.‚Äù&lt;/p&gt;
    &lt;p&gt;Ain also takes issue with the criticism of influencers applying for O-1 visas, as well as the notion that influencing is not a legitimate profession.&lt;/p&gt;
    &lt;p&gt;‚ÄúI don‚Äôt think [people] realize how much work actually goes into it,‚Äù she said. ‚ÄúYou might not agree with the way the money is being made, or what people are watching, but people are still watching and paying for it.‚Äù&lt;/p&gt;
    &lt;p&gt;She continued: ‚ÄúMaybe 50 years ago, this isn‚Äôt what people imagined the American dream would look like. But this is what the American dream is now.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46603535</guid><pubDate>Tue, 13 Jan 2026 16:47:39 +0000</pubDate></item><item><title>Legion Health (YC S21) Hiring Cracked Founding Eng for AI-Native Ops</title><link>https://jobs.ashbyhq.com/legionhealth/ffdd2b52-eb21-489e-b124-3c0804231424</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46603829</guid><pubDate>Tue, 13 Jan 2026 17:01:55 +0000</pubDate></item><item><title>The Tulip Creative Computer</title><link>https://github.com/shorepine/tulipcc</link><description>&lt;doc fingerprint="204746de23261bd9"&gt;
  &lt;main&gt;
    &lt;p&gt;Welcome to the Tulip Creative Computer (Tulip CC)!&lt;/p&gt;
    &lt;p&gt;Tulip is a low power and affordable self-contained portable computer, with a touchscreen display and sound. It's fully programmable - you write code to define your music, games or anything else you can think of. It boots instantaneously into a Python prompt with a lot of built in support for music synthesis, fast graphics and text, hardware MIDI, network access and external sensors. Dive right into making something without distractions or complications.&lt;/p&gt;
    &lt;p&gt;The entire system is dedicated to your code, the display and sound, running in real time, on specialized hardware. The hardware and software are fully open source and anyone can buy one or build one. You can use Tulip to make music, code, art, games, or just write.&lt;/p&gt;
    &lt;p&gt;You can now even run Tulip on the web and share your creations with anyone!&lt;/p&gt;
    &lt;p&gt;Tulip is powered by MicroPython, AMY, and LVGL. The Tulip hardware runs on the ESP32-S3 chip using the ESP-IDF.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Get a Tulip from our friends at Makerfabs for only US$59&lt;/item&gt;
      &lt;item&gt;Just got a Tulip CC? Check out our getting started guide!&lt;/item&gt;
      &lt;item&gt;Want to make music with your Tulip? See our music tutorial&lt;/item&gt;
      &lt;item&gt;See the full Tulip API&lt;/item&gt;
      &lt;item&gt;Try out Tulip on the web!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Chat about Tulip on our Discord!&lt;/p&gt;
    &lt;p&gt;Check out this video!&lt;/p&gt;
    &lt;p&gt;You can use Tulip one of three ways:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tulip is available both as an off the shelf or DIY hardware project (Tulip CC)&lt;/item&gt;
      &lt;item&gt;Tulip runs on the web with (almost) all the same features.&lt;/item&gt;
      &lt;item&gt;Tulip can also run as a native app for Mac or Linux (or WSL in Windows) as Tulip Desktop&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you're nervous about getting or building the hardware, try it out on the web!&lt;/p&gt;
    &lt;p&gt;The hardware Tulip CC supports:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;8.5MB of RAM - 2MB is available to MicroPython, and 1.5MB is available for OS memory. The rest is used for the graphics framebuffers (which you can use as storage) and the firmware cache.&lt;/item&gt;
      &lt;item&gt;32MB flash storage, as a filesystem accesible in Python (24MB left over after OS in ROM)&lt;/item&gt;
      &lt;item&gt;An AMY stereo 120-voice synthesizer engine running locally, or as a wireless controller for an Alles mesh. Tulip's synth supports additive and subtractive oscillators, an excellent FM synthesis engine, samplers, karplus-strong, high quality analog style filters, a sequencer, and much more. We ship Tulip with a drum machine, voices / patch app, and Juno-6 editor.&lt;/item&gt;
      &lt;item&gt;Text frame buffer layer, 128 x 50, with ANSI support for 256 colors, inverse, bold, underline, background color&lt;/item&gt;
      &lt;item&gt;Up to 32 sprites on screen, drawn per scanline, with collision detection, from a total of 32KB of bitmap memory (1 byte per pixel)&lt;/item&gt;
      &lt;item&gt;A 1024 (+128 overscan) by 600 (+100 overscan) background frame buffer to draw arbitrary bitmaps to, or use as RAM, and which can scroll horizontally / vertically&lt;/item&gt;
      &lt;item&gt;WiFi, access http via Python requests or TCP / UDP sockets&lt;/item&gt;
      &lt;item&gt;Adjustable display clock and resolution, defaults to 30 FPS at 1024x600.&lt;/item&gt;
      &lt;item&gt;256 colors&lt;/item&gt;
      &lt;item&gt;Can load PNGs from disk to set sprites or background, or generate bitmap data from code&lt;/item&gt;
      &lt;item&gt;Built in code and text editor&lt;/item&gt;
      &lt;item&gt;Built in BBS chat room and file transfer area called TULIP ~ WORLD&lt;/item&gt;
      &lt;item&gt;USB keyboard, MIDI and mouse support, including hubs&lt;/item&gt;
      &lt;item&gt;Capactive multi-touch support (mouse on Tulip Desktop and Tulip Web)&lt;/item&gt;
      &lt;item&gt;MIDI input and output&lt;/item&gt;
      &lt;item&gt;I2C / Grove / Mabee connector, compatible with many I2C devices like joysticks, keyboard, GPIO, DACs, ADCs, hubs&lt;/item&gt;
      &lt;item&gt;575mA power usage @ 5V including display, at medium display brightness, can last for hours on LiPo, 18650s, or USB battery pack&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I've been working on Tulip on and off for years over many hardware iterations and hope that someone out there finds it as fun as I have, either making things with Tulip or working on Tulip itself. I'd love feedback, your own Tulip experiments or pull requests to improve the system.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Any issues with your Tulip CC? Here's our troubleshooting guide&lt;/item&gt;
      &lt;item&gt;Learn about our roadmap and find out what we're working on next&lt;/item&gt;
      &lt;item&gt;Build your own Tulip&lt;/item&gt;
      &lt;item&gt;You can read more about the "why" or "how" of Tulip on my website!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A new small option: get yourself a T-Deck and install Tulip CC on it directly! Check out our T-Deck page for more detail.&lt;/p&gt;
    &lt;p&gt;Once you've bought a Tulip, opened Tulip Web, built a Tulip or installed Tulip Desktop, you'll see that Tulip boots right into a Python prompt and all interaction with the system happens there. You can make your own Python programs with Tulip's built in editor and execute them, or just experiment on the Tulip REPL prompt in real time.&lt;/p&gt;
    &lt;p&gt;See the full Tulip API for more details on all the graphics, sound and input functions.&lt;/p&gt;
    &lt;p&gt;Below are a few getting started tips and small examples. The full API page has more detail on everything you can do on a Tulip. See a more complete getting started page or a music making tutorial as well!&lt;/p&gt;
    &lt;code&gt;# Run a saved Python file. Control-C stops it
cd('ex') # The ex folder has a few examples and graphics in it
execfile("parallax.py")
# If you want to run a Tulip package (folder with other files in it)
run("game")&lt;/code&gt;
    &lt;p&gt;Tulip ships with a text editor, based on pico/nano. It supports syntax highlighting, search, save/save-as.&lt;/p&gt;
    &lt;code&gt;# Opens the Tulip editor to the given filename. 
edit("game.py")&lt;/code&gt;
    &lt;p&gt;Tulip supports USB keyboard and mice input as well as touch input. (On Tulip Desktop and Web, mouse clicks act as touch points.) It also comes with UI elements like buttons and sliders to use in your applications, and a way to run mulitple applications as once using callbacks. More in the full API.&lt;/p&gt;
    &lt;code&gt;(x0, y0, x1, y1, x2, y2) = tulip.touch()&lt;/code&gt;
    &lt;p&gt;Tulip CC has the capability to connect to a Wi-Fi network, and Python's native requests library will work to access TCP and UDP. We ship a few convenience functions to grab data from URLs as well. More in the full API.&lt;/p&gt;
    &lt;code&gt;# Join a wifi network (not needed on Tulip Desktop or Web)
tulip.wifi("ssid", "password")

# Get IP address or check if connected
ip_address = tulip.ip() # returns None if not connected

# Save the contents of a URL to disk (needs wifi)
bytes_read = tulip.url_save("https://url", "filename.ext")&lt;/code&gt;
    &lt;p&gt;Tulip comes with the AMY synthesizer, a very full featured 120-oscillator synth that supports FM, PCM, additive synthesis, partial synthesis, filters, and much more. We also provide a useful "music computer" for scales, chords and progressions. More in the full API and in the music tutorial. Tulip's version of AMY comes with stereo sound, which you can set per oscillator with the &lt;code&gt;pan&lt;/code&gt; parameter.&lt;/p&gt;
    &lt;code&gt;amy.drums() # plays a test song
amy.send(volume=4) # change volume
amy.reset() # stops all music / sounds playing&lt;/code&gt;
    &lt;head class="px-3 py-2"&gt;music.mov&lt;/head&gt;
    &lt;p&gt;Tulip supports MIDI in and out to connect to external music hardware. You can set up a Python callback to respond immediately to any incoming MIDI message. You can also send messages out to MIDI out. More in the full API and music tutorial.&lt;/p&gt;
    &lt;code&gt;m = tulip.midi_in() # returns bytes of the last MIDI message received
tulip.midi_out((144,60,127)) # sends a note on message
tulip.midi_out(bytes) # Can send bytes or list&lt;/code&gt;
    &lt;p&gt;The Tulip GPU supports a scrolling background layer, hardware sprites, and a text layer. Much more in the full API.&lt;/p&gt;
    &lt;code&gt;# Set or get a pixel on the BG
pal_idx = tulip.bg_pixel(x,y)

# Set the contents of a PNG file on the background.
tulip.bg_png(png_filename, x, y)

tulip.bg_scroll(line, x_offset, y_offset, x_speed, y_speed)&lt;/code&gt;
    &lt;head class="px-3 py-2"&gt;scroll.mov&lt;/head&gt;
    &lt;p&gt;Hardware sprites are supported. They draw over the background and text layer per scanline per frame:&lt;/p&gt;
    &lt;code&gt;(w, h, bytes) = tulip.sprite_png("filename.png", mem_pos)

...

# Set a sprite x and y position
tulip.sprite_move(12, x, y)&lt;/code&gt;
    &lt;head class="px-3 py-2"&gt;game.mov&lt;/head&gt;
    &lt;p&gt;Still very much early days, but Tulip supports a native chat and file sharing BBS called TULIP ~ WORLD where you can hang out with other Tulip owners. You're able to pull down the latest messages and files and send messages and files yourself. More in the full API.&lt;/p&gt;
    &lt;code&gt;import world
world.post_message("hello!!") # Sends a message to Tulip World. username required. will prompt if not set
world.upload(filename) # Uploads a file to Tulip World. username required
world.ls() # lists most recent unique filenames/usernames&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Get a Tulip!&lt;/item&gt;
      &lt;item&gt;Build your own Tulip Creative Computer with FOUR different options.&lt;/item&gt;
      &lt;item&gt;How to compile and flash Tulip hardware&lt;/item&gt;
      &lt;item&gt;How to run or compile Tulip Desktop&lt;/item&gt;
      &lt;item&gt;The full Tulip API&lt;/item&gt;
      &lt;item&gt;File any code issues or pull requests!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Chat about Tulip on our Discord!&lt;/p&gt;
    &lt;p&gt;Two important development guidelines if you'd like to help contribute!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Be nice and helpful and don't be afraid to ask questions! We're all doing this for fun and to learn.&lt;/item&gt;
      &lt;item&gt;Any change or feature must be equivalent across Tulip Desktop and Tulip CC. There are of course limited exceptions to this rule, but please test on hardware before proposing a new feature / change.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Have fun!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46603995</guid><pubDate>Tue, 13 Jan 2026 17:10:42 +0000</pubDate></item><item><title>How to make a damn website (2024)</title><link>https://lmnt.me/blog/how-to-make-a-damn-website.html</link><description>&lt;doc fingerprint="260e12710bf61f95"&gt;
  &lt;main&gt;
    &lt;p&gt;A lot of people want to make a website but don√¢t know where to start or they get stuck. That√¢s in part because our perception of what websites should be has changed so dramatically over the last 20 years.&lt;/p&gt;
    &lt;p&gt;It√¢s easy to forget how simple a website can be. A website can be just one page. It doesn√¢t even need CSS. You don√¢t need a content management system like Wordpress. All you have to do is write some HTML and drag that file to a server over FTP.&lt;/p&gt;
    &lt;p&gt;For years now, people have tried to convince us that this is the √¢hard√¢ way of making a website, but in reality, it may be the easiest.&lt;/p&gt;
    &lt;p&gt;It doesn√¢t have to be super complicated. However, with this post, I will assume you√¢ve written at least some HTML and CSS before, and that you know how to upload files to a server. If you√¢ve never done these things, it may seem like I√¢m skipping over some things. I am.&lt;/p&gt;
    &lt;p&gt;Let me begin with what I think you shouldn√¢t start with. Don√¢t shop around for a CMS. Don√¢t even design or outline your website. Don√¢t buy a domain or hosting yet. Don√¢t set up a GitHub repository; I don√¢t care how fast you can make one.&lt;/p&gt;
    &lt;p&gt;Instead, just write your first blog post. The very first thing I did was open TextEdit and write my first post with HTML, ye olde way. Not with Markdown. Not with Nova or BBEdit or another code editor. Just TextEdit (in plain text). Try it, even if just this once. It√¢s kinda refreshing. You can go back to using a code editor later.&lt;/p&gt;
    &lt;p&gt;Here√¢s what a draft of this blog post looks like:&lt;/p&gt;
    &lt;code&gt;&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html lang="en"&amp;gt;
	&amp;lt;head&amp;gt;
		&amp;lt;meta charset="utf-8"&amp;gt;
		&amp;lt;title&amp;gt;How to Make a Damn Website&amp;lt;/title&amp;gt;
	&amp;lt;/head&amp;gt;
	&amp;lt;body&amp;gt;

		&amp;lt;h1&amp;gt;&amp;lt;a href="how-to-make-a-damn-website.html"&amp;gt;How to Make a Damn Website&amp;lt;/a&amp;gt;&amp;lt;/h1&amp;gt;
		&amp;lt;p&amp;gt;A lot of people want to make a website but don√¢t know where to start or they get stuck.&amp;lt;/p&amp;gt;

	&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;&lt;/code&gt;
    &lt;p&gt;This is honestly all you need. It√¢s kind of charming.&lt;/p&gt;
    &lt;p&gt;Make sure you rely exclusively on HTML elements for your formatting. Your page should render clearly with raw HTML. Do not let yourself get distracted by writing CSS. Don√¢t even imagine the CSS you√¢ll use later. Don√¢t write in IDs or classes yet. Do yourself a favor and don√¢t make a single &lt;code&gt;div&lt;/code&gt; element.&lt;/p&gt;
    &lt;p&gt;Just write the post in the plainest HTML. And don√¢t you dare write a √¢Hello World√¢ post or a √¢Lorem Ipsum√¢ post. Write an actual blog post. If you want, make it about why you√¢re making a website.&lt;/p&gt;
    &lt;p&gt;Writing this way helps you stay focused on writing for the web. The most important thing here is shipping something. You can (and should) update your site later. Now, name the HTML file something sensible, like the post name.&lt;/p&gt;
    &lt;code&gt;how-to-make-a-damn-website.html&lt;/code&gt;
    &lt;p&gt;Finished? Great. If you have a domain and hosting, make a new folder on your server called blog and upload your first post in there. Don√¢t worry about index pages yet. You have only one post, there√¢s not much to index. We√¢ll get there.&lt;/p&gt;
    &lt;p&gt;If you don√¢t have a domain or hosting yet, now√¢s the time to buckle down and do that. Unfortunately, I don√¢t have good advice for you here. Just know that it√¢s going to be stupid and tedious and bad and unfun. That√¢s just the way this is.&lt;/p&gt;
    &lt;p&gt;Try not to let it deter you. Once you have the ability to upload files to an FTP server, you√¢ve reached the √¢set it and forget it√¢ phase.&lt;/p&gt;
    &lt;p&gt;Direct your web browser to the HTML file you uploaded. Wow! There it is. A real, actual page on the web! You shipped it. Congratulations. Times New Roman, black on white. Hyperlinks that are blue and underlined. Useful. Classic.&lt;/p&gt;
    &lt;p&gt;Look at your unstyled HTML page and appreciate it for what it is. Always remember, this is all a website has to be. Good websites can be reduced to this and still work.&lt;/p&gt;
    &lt;p&gt;A broken escalator is just stairs. Even if it√¢s a little less convenient, it remains functional. This is important.&lt;/p&gt;
    &lt;p&gt;If you get this far, I want you to know this is truly the hardest part. Some people will ignore what I√¢ve said. They will spend significant time designing a website, hunting around for a good CMS, doing a wide variety of busywork, neglecting the part where they write actual content for their site. But if you shipped a single blog post, you have a website, and they don√¢t.&lt;/p&gt;
    &lt;p&gt;A website is nothing without content. You can spend months preparing to make a website, tacking up what I√¢m sure was intended to be a √¢temporary√¢ page telling people that you√¢re √¢working on a new website,√¢ but it will inevitably become a permanent reminder that you haven√¢t done it yet. So focus on what matters, and ship one blog post. Do the rest later.&lt;/p&gt;
    &lt;p&gt;You may think CSS is the next logical step, or maybe an index page, but I don√¢t think so. It takes only a few minutes to hand-write an XML file, and once it√¢s done, people will be able to read your blog via an RSS reader.&lt;/p&gt;
    &lt;p&gt;On your site, you√¢re in control of publishing now. When you post to your blog, part of the process is syndicating it to those who want to stay updated. If you provide an RSS feed, people can follow it. If you don√¢t, they can√¢t.&lt;/p&gt;
    &lt;p&gt;While the best time to make an RSS feed was 20 years ago, the second best time is now.&lt;/p&gt;
    &lt;p&gt;It should be noted that most people who have an RSS feed are probably not making it manually, so you won√¢t find a lot of documentation out there for doing it this way. But it√¢s not too hard. And once you make a habit, it√¢ll be a totally reasonable component of your publishing flow.&lt;/p&gt;
    &lt;p&gt;Here√¢s what my XML file looks like (without any entries):&lt;/p&gt;
    &lt;code&gt;&amp;lt;?xml version="1.0" encoding="utf-8"?&amp;gt;
&amp;lt;rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"&amp;gt;
	&amp;lt;channel&amp;gt;

		&amp;lt;title&amp;gt;LMNT&amp;lt;/title&amp;gt;
		&amp;lt;link&amp;gt;https://lmnt.me/&amp;lt;/link&amp;gt;
		&amp;lt;description&amp;gt;Louie Mantia√¢s weblog.&amp;lt;/description&amp;gt;
		&amp;lt;language&amp;gt;en-us&amp;lt;/language&amp;gt;
		&amp;lt;atom:link href="https://lmnt.me/feed.xml" rel="self" type="application/rss+xml" /&amp;gt;

	&amp;lt;/channel&amp;gt;
&amp;lt;/rss&amp;gt;&lt;/code&gt;
    &lt;p&gt;The elements inside the &lt;code&gt;channel&lt;/code&gt; element are for your feed as a whole (&lt;code&gt;title&lt;/code&gt;, &lt;code&gt;link&lt;/code&gt;, &lt;code&gt;description&lt;/code&gt;, &lt;code&gt;language&lt;/code&gt;, and &lt;code&gt;atom:link&lt;/code&gt;). After the ones about your feed√¢s metadata, we can add a blog post to the XML file, which will look like this:&lt;/p&gt;
    &lt;code&gt;&amp;lt;?xml version="1.0" encoding="utf-8"?&amp;gt;
&amp;lt;rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"&amp;gt;
	&amp;lt;channel&amp;gt;
		&amp;lt;title&amp;gt;LMNT&amp;lt;/title&amp;gt;
		&amp;lt;link&amp;gt;https://lmnt.me/&amp;lt;/link&amp;gt;
		&amp;lt;description&amp;gt;Louie Mantia√¢s weblog.&amp;lt;/description&amp;gt;
		&amp;lt;language&amp;gt;en-us&amp;lt;/language&amp;gt;
		&amp;lt;atom:link href="https://lmnt.me/feed.xml" rel="self" type="application/rss+xml" /&amp;gt;

		&amp;lt;item&amp;gt;
			&amp;lt;title&amp;gt;How to Make a Damn Website&amp;lt;/title&amp;gt;
			&amp;lt;pubDate&amp;gt;Mon, 25 Mar 2024 09:05:00 GMT&amp;lt;/pubDate&amp;gt;
			&amp;lt;guid&amp;gt;C5CC4199-E380-4851-B621-2C1AEF2CE7A1&amp;lt;/guid&amp;gt;
			&amp;lt;link&amp;gt;https://lmnt.me/blog/how-to-make-a-damn-website.html&amp;lt;/link&amp;gt;
			&amp;lt;description&amp;gt;&amp;lt;![CDATA[

				&amp;lt;h1&amp;gt;&amp;lt;a href="how-to-make-a-damn-website.html"&amp;gt;How to Make a Damn Website&amp;lt;/a&amp;gt;&amp;lt;/h1&amp;gt;
				&amp;lt;p&amp;gt;A lot of people want to make a website but don√¢t know where to start or they get stuck.&amp;lt;/p&amp;gt;

			]]&amp;gt;&amp;lt;/description&amp;gt;
		&amp;lt;/item&amp;gt;

	&amp;lt;/channel&amp;gt;
&amp;lt;/rss&amp;gt;&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;item&lt;/code&gt; element represents an entry, and goes inside the &lt;code&gt;channel&lt;/code&gt; element as well. There are a few self-explanatory elements for the post metadata (&lt;code&gt;title&lt;/code&gt;, &lt;code&gt;pubDate&lt;/code&gt;, &lt;code&gt;guid&lt;/code&gt;, and &lt;code&gt;link&lt;/code&gt;), but the content inside the &lt;code&gt;description&lt;/code&gt; element can be the same HTML from your actual post. Handy!&lt;/p&gt;
    &lt;p&gt;Writing your first post with HTML and understanding how it looks √¢unstyled√¢ really works in your favor here, because RSS readers use their own stylesheets. How they render pages will not be too different from how a raw HTML page is rendered in your browser. If you make your own stylesheet too early, you may neglect how the raw HTML could be parsed in an RSS reader.&lt;/p&gt;
    &lt;p&gt;For the &lt;code&gt;pubDate&lt;/code&gt;, you can use GMT time. Ask Siri what time it is in Reykjavik, and enter that. You can use your local time zone instead, but be sure it√¢s formatted correctly. Also, note that it needs to be 24-hour time.&lt;/p&gt;
    &lt;p&gt;If you have images or other media in your post, be sure to use the absolute URL to a resource rather than a relative one. Relative URLs are fine for content that only lives on your site, but when you syndicate via RSS, that content loads outside of your website. Absolute URLs are better for content inside your blog posts, especially in the XML.&lt;/p&gt;
    &lt;p&gt;Once you√¢ve got your first post in the XML file, upload it to the root folder of your website. If you don√¢t already have an RSS reader, get one. I recommend NetNewsWire. Go to the XML file in your browser, and it should automatically open in your RSS reader and let you subscribe.&lt;/p&gt;
    &lt;p&gt;There it is! Your blog post is on the web and now also available via RSS! You can share that link now.&lt;/p&gt;
    &lt;p&gt;Now would be a good time to reference your RSS feed in your HTML. You√¢ll want to do this on all pages going forward, too. It helps browsers and plugins detect that there√¢s an RSS feed for people to subscribe to.&lt;/p&gt;
    &lt;code&gt;&amp;lt;link rel="alternate" type="application/rss+xml" title="LMNT" href="https://lmnt.me/feed.xml" /&amp;gt;&lt;/code&gt;
    &lt;p&gt;When you add a new &lt;code&gt;item&lt;/code&gt; (a new blog post), put it above the previous one in your XML file. Keep in mind that your XML file will be updated periodically from devices that subscribe to it. RSS readers will be downloading this file when updating, so keep an eye on the file size. It probably won√¢t ever be that big, because it√¢s just text, but it√¢s customary to keep only a certain amount of recent entries in the XML file, or a certain time period. But there√¢s no rule here.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;guid&lt;/code&gt; should be a unique string. Some people use URLs thinking they√¢re unique, but those can change. The right way is to generate a unique string for each post, which you can do easily with my app Tulip.&lt;/p&gt;
    &lt;p&gt;Changing the &lt;code&gt;guid&lt;/code&gt; (unique identifier) for your posts makes an RSS reader think it√¢s a different entry, resulting in a post being marked √¢unread.√¢ If you go the route of using a URL as your &lt;code&gt;guid&lt;/code&gt; for each post, you√¢ll want to think harder about the file structure of your website, right? It√¢s probably fine if you change your file structure once or twice (I did), but just be sure to update your &lt;code&gt;link&lt;/code&gt; elements in the RSS feed, and redirect old URLs to new ones with an .htaccess file. Just don√¢t change the contents of the &lt;code&gt;guid&lt;/code&gt; element.&lt;/p&gt;
    &lt;p&gt;Alright, we can make index pages now. This is going to be super easy, because you don√¢t have a lot to index yet.&lt;/p&gt;
    &lt;p&gt;At the root, you want a link to the blog directory, and at the blog directory, you want a link to your first post. Put titles on each page, maybe a link back to the home page from your blog index. If you want, write a little description of your site on the root index.&lt;/p&gt;
    &lt;p&gt;Keep using basic HTML! Titles can be &lt;code&gt;h1&lt;/code&gt;, and descriptions can be &lt;code&gt;p&lt;/code&gt;. Keep it simple.
		&lt;/p&gt;
    &lt;p&gt;Once you got those uploaded, you got three pages and an RSS feed. You√¢re doing great!&lt;/p&gt;
    &lt;p&gt;I recommend writing a couple more posts next. Try using some HTML elements that you didn√¢t use in the first post, maybe an &lt;code&gt;hr&lt;/code&gt; element. Fancy! &lt;code&gt;ol&lt;/code&gt; and &lt;code&gt;ul&lt;/code&gt;. Maybe some &lt;code&gt;img&lt;/code&gt;, &lt;code&gt;video&lt;/code&gt;, and &lt;code&gt;audio&lt;/code&gt; elements.&lt;/p&gt;
    &lt;p&gt;In addition to being more posts for your blog, these will also help prioritize which elements need styling, providing you with a few sample pages to check while you write CSS.&lt;/p&gt;
    &lt;p&gt;Upload the posts as you write them, one after the next, adding them to your XML file. Don√¢t forget to update your index pages, too. Always check your links and your feed.&lt;/p&gt;
    &lt;p&gt;Before you get ahead of yourself with layout, I recommend first styling the basic HTML elements you already defined in your first few posts: &lt;code&gt;h1&lt;/code&gt;, &lt;code&gt;h2&lt;/code&gt;, &lt;code&gt;h3&lt;/code&gt;, &lt;code&gt;hr&lt;/code&gt;, &lt;code&gt;p&lt;/code&gt;, &lt;code&gt;strong&lt;/code&gt;, &lt;code&gt;em&lt;/code&gt;, &lt;code&gt;ol&lt;/code&gt;, &lt;code&gt;ul&lt;/code&gt;. Define the &lt;code&gt;body&lt;/code&gt; font and width, text sizes, and colors.&lt;/p&gt;
    &lt;p&gt;Like the rest of your site, stylesheets are mutable. Expect them to change with your website. Incremental updates are what makes this whole process work. Ship tiny updates to your CSS. You can upload your stylesheet in a second. Heck, work directly on the server if you want. I did that.&lt;/p&gt;
    &lt;p&gt;If you√¢ve done all this, then you√¢ve cleared the hurdle. Now you get to just keep doing the fun stuff. Write more blog posts. Make more web pages. It√¢s your website, you can make pages for anything you want. You can style them however you want. You can update people via RSS whenever you make something new.&lt;/p&gt;
    &lt;p&gt;Manually making a website like this may seem silly to engineers who would rather build or rely on systems that automate this stuff. But it doesn√¢t seem like there√¢s actually a whole lot that needs automation, does it?&lt;/p&gt;
    &lt;p&gt;A lot of modern solutions may not save time as much as they introduce complexity and reliance on more tools than you need. This whole process is not that complex.&lt;/p&gt;
    &lt;p&gt;It√¢s not doing this manually that√¢s hard.&lt;/p&gt;
    &lt;p&gt;The hard part is just shipping.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46604250</guid><pubDate>Tue, 13 Jan 2026 17:23:46 +0000</pubDate></item><item><title>Show HN: Ayder ‚Äì HTTP-native durable event log written in C (curl as client)</title><link>https://github.com/A1darbek/ayder</link><description>&lt;doc fingerprint="8858ee55c525499a"&gt;
  &lt;main&gt;
    &lt;p&gt;HTTP-native durable event log / message bus ‚Äî written in C&lt;/p&gt;
    &lt;p&gt;A single-binary event streaming system where &lt;code&gt;curl&lt;/code&gt; is your client. No JVM, no ZooKeeper, no thick client libraries.&lt;/p&gt;
    &lt;code&gt;# Produce
curl -X POST 'localhost:1109/broker/topics/orders/produce?partition=0' \
  -H 'Authorization: Bearer dev' \
  -d '{"item":"widget"}'

# Consume
curl 'localhost:1109/broker/consume/orders/mygroup/0?encoding=b64' \
  -H 'Authorization: Bearer dev'&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sustained: ~50K msg/s (wrk2 @ 50K req/s)&lt;/item&gt;
      &lt;item&gt;Client P99: 3.46ms&lt;/item&gt;
      &lt;item&gt;Server P99.999: 1.22ms (handler only)&lt;/item&gt;
      &lt;item&gt;Recovery after SIGKILL: 40‚Äì50s (8M offsets)&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Kafka&lt;/cell&gt;
        &lt;cell role="head"&gt;Redis Streams&lt;/cell&gt;
        &lt;cell role="head"&gt;Ayder&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Protocol&lt;/cell&gt;
        &lt;cell&gt;Binary (requires thick client)&lt;/cell&gt;
        &lt;cell&gt;RESP&lt;/cell&gt;
        &lt;cell&gt;HTTP (curl works)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Durability&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Replicated log&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Raft consensus (sync-majority)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Operations&lt;/cell&gt;
        &lt;cell&gt;ZooKeeper/KRaft + JVM tuning&lt;/cell&gt;
        &lt;cell&gt;Single node or Redis Cluster&lt;/cell&gt;
        &lt;cell&gt;Single binary, zero dependencies&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Latency (P99)&lt;/cell&gt;
        &lt;cell&gt;10-50ms&lt;/cell&gt;
        &lt;cell&gt;N/A (async only)&lt;/cell&gt;
        &lt;cell&gt;3.5ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Recovery time&lt;/cell&gt;
        &lt;cell&gt;2+ hours (unclean shutdown)&lt;/cell&gt;
        &lt;cell&gt;Minutes&lt;/cell&gt;
        &lt;cell&gt;40-50 seconds&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;First message&lt;/cell&gt;
        &lt;cell&gt;~30 min setup&lt;/cell&gt;
        &lt;cell&gt;~5 min setup&lt;/cell&gt;
        &lt;cell&gt;~60 seconds&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Kafka is battle-tested but operationally heavy. JVM tuning, partition rebalancing, and config sprawl add up.&lt;/p&gt;
    &lt;p&gt;Redis Streams is simple and fast, but replication is async-only ‚Äî no majority quorum, no strong durability guarantees.&lt;/p&gt;
    &lt;p&gt;Ayder sits in the middle: Kafka-grade durability (Raft sync-majority) with Redis-like simplicity (single binary, HTTP API). Think of it as what Nginx did to Apache ‚Äî same pattern applied to event streaming.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Append-only logs with per-partition offsets&lt;/item&gt;
      &lt;item&gt;Consumer groups with committed offsets&lt;/item&gt;
      &lt;item&gt;Durability via sealed append-only files (AOF) + crash recovery&lt;/item&gt;
      &lt;item&gt;HA replication with Raft consensus (3 / 5 / 7 node clusters)&lt;/item&gt;
      &lt;item&gt;KV store with CAS and TTL&lt;/item&gt;
      &lt;item&gt;Stream processing with filters, aggregations, and windowed joins (including cross-format Avro+Protobuf joins)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All benchmarks use real network (not loopback). Numbers are real, not marketing.&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;3-node Raft cluster on DigitalOcean (8 vCPU AMD)&lt;/item&gt;
      &lt;item&gt;Sync-majority writes (2/3 nodes confirm before ACK)&lt;/item&gt;
      &lt;item&gt;64B payload&lt;/item&gt;
      &lt;item&gt;Separate machines, real network&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Client-side&lt;/cell&gt;
        &lt;cell role="head"&gt;Server-side&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Throughput&lt;/cell&gt;
        &lt;cell&gt;49,871 msg/s&lt;/cell&gt;
        &lt;cell&gt;‚Äî&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;P50&lt;/cell&gt;
        &lt;cell&gt;1.60ms&lt;/cell&gt;
        &lt;cell&gt;‚Äî&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;P99&lt;/cell&gt;
        &lt;cell&gt;3.46ms&lt;/cell&gt;
        &lt;cell&gt;‚Äî&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;P99.9&lt;/cell&gt;
        &lt;cell&gt;12.94ms&lt;/cell&gt;
        &lt;cell&gt;‚Äî&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;P99.999&lt;/cell&gt;
        &lt;cell&gt;154.49ms&lt;/cell&gt;
        &lt;cell&gt;1.22ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Server-side breakdown at P99.999:&lt;/p&gt;
    &lt;code&gt;Handler:     1.22ms
Queue wait:  0.47ms
HTTP parse:  0.41ms
&lt;/code&gt;
    &lt;p&gt;The 154ms client-side tail is network/kernel scheduling ‚Äî the broker itself stays under 2ms even at P99.999. HTTP is not the bottleneck.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Throughput&lt;/cell&gt;
        &lt;cell&gt;93,807 msg/s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;P50&lt;/cell&gt;
        &lt;cell&gt;3.78ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;P99&lt;/cell&gt;
        &lt;cell&gt;10.22ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Max&lt;/cell&gt;
        &lt;cell&gt;224.51ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Full wrk output (3-node cluster, max throughput)&lt;/head&gt;
    &lt;code&gt;Running 1m test @ http://10.114.0.3:8001
  12 threads and 400 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     4.26ms    2.97ms 224.51ms   93.76%
    Req/Sec     7.86k     1.19k   13.70k    67.61%
  Latency Distribution
     50%    3.78ms
     75%    4.93ms
     90%    6.44ms
     99%   10.22ms
  5634332 requests in 1.00m, 2.99GB read
Requests/sec:  93807.95
Transfer/sec:     50.92MB
&lt;/code&gt;
    &lt;head&gt;Full wrk2 output (3-node cluster, rate-limited)&lt;/head&gt;
    &lt;code&gt;Running 1m test @ http://10.114.0.2:9001
  12 threads and 400 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.72ms    1.19ms 216.19ms   96.39%
    Req/Sec     4.35k     1.17k    7.89k    79.58%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    1.60ms
 75.000%    2.03ms
 90.000%    2.52ms
 99.000%    3.46ms
 99.900%   12.94ms
 99.990%   31.76ms
 99.999%  154.49ms
100.000%  216.32ms

  2991950 requests in 1.00m, 1.80GB read
Requests/sec:  49871.12

SERVER  server_us p99.999=1219us (1.219ms)
SERVER  queue_us p99.999=473us (0.473ms)
SERVER  recv_parse_us p99.999=411us (0.411ms)
&lt;/code&gt;
    &lt;p&gt;Ayder runs natively on ARM64. Here's a benchmark on consumer hardware:&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Snapdragon X Elite laptop (1.42 kg)&lt;/item&gt;
      &lt;item&gt;WSL2 Ubuntu, 16GB RAM&lt;/item&gt;
      &lt;item&gt;Running on battery (unplugged)&lt;/item&gt;
      &lt;item&gt;3-node Raft cluster (same machine ‚Äî testing code efficiency)&lt;/item&gt;
      &lt;item&gt;wrk: 12 threads, 400 connections, 60 seconds&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Client-side&lt;/cell&gt;
        &lt;cell role="head"&gt;Server-side&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Throughput&lt;/cell&gt;
        &lt;cell&gt;106,645 msg/s&lt;/cell&gt;
        &lt;cell&gt;‚Äî&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;P50&lt;/cell&gt;
        &lt;cell&gt;3.57ms&lt;/cell&gt;
        &lt;cell&gt;‚Äî&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;P99&lt;/cell&gt;
        &lt;cell&gt;7.62ms&lt;/cell&gt;
        &lt;cell&gt;‚Äî&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;P99.999&lt;/cell&gt;
        &lt;cell&gt;250.84ms&lt;/cell&gt;
        &lt;cell&gt;0.65ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Server-side breakdown at P99.999:&lt;/p&gt;
    &lt;code&gt;Handler:     0.65ms
Queue wait:  0.29ms
HTTP parse:  0.29ms
&lt;/code&gt;
    &lt;p&gt;Comparison: Snapdragon vs Cloud VMs (Server-side P99.999)&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Environment&lt;/cell&gt;
        &lt;cell role="head"&gt;Throughput&lt;/cell&gt;
        &lt;cell role="head"&gt;Server P99.999&lt;/cell&gt;
        &lt;cell role="head"&gt;Hardware&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Snapdragon X Elite (WSL2, battery)&lt;/cell&gt;
        &lt;cell&gt;106,645/s&lt;/cell&gt;
        &lt;cell&gt;0.65ms&lt;/cell&gt;
        &lt;cell&gt;1.42kg laptop&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;DigitalOcean (8-vCPU AMD, 3 VMs)&lt;/cell&gt;
        &lt;cell&gt;93,807/s&lt;/cell&gt;
        &lt;cell&gt;1.22ms&lt;/cell&gt;
        &lt;cell&gt;Cloud infrastructure&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The laptop's server-side latency is 47% faster while handling 14% more throughput ‚Äî on battery, in WSL2.&lt;/p&gt;
    &lt;p&gt;What this proves:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ARM64 is ready for server workloads&lt;/item&gt;
      &lt;item&gt;Efficient C code runs beautifully on Snapdragon&lt;/item&gt;
      &lt;item&gt;WSL2 overhead is minimal for async I/O&lt;/item&gt;
      &lt;item&gt;You can test full HA clusters on your laptop&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;Full wrk output (Snapdragon X Elite)&lt;/head&gt;
    &lt;code&gt;Running 1m test @ http://172.31.76.127:7001
  12 threads and 400 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     3.81ms    3.80ms 289.49ms   99.00%
    Req/Sec     8.94k     1.16k   22.81k    80.11%
  Latency Distribution
     50%    3.57ms
     75%    4.01ms
     90%    4.51ms
     99%    7.62ms
  6408525 requests in 1.00m, 3.80GB read
Requests/sec: 106645.65
Transfer/sec:     64.83MB

CLIENT  p99.999=250843us (250.843ms)  max=289485us (289.485ms)
SERVER  server_us p99.999=651us (0.651ms)  max=11964us (11.964ms)
SERVER  queue_us p99.999=285us (0.285ms)  max=3920us (3.920ms)
SERVER  recv_parse_us p99.999=293us (0.293ms)  max=4149us (4.149ms)
&lt;/code&gt;
    &lt;p&gt;Kafka in 2025 is like starting a car with a hand crank. It works, but why are we still doing this?&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Scenario&lt;/cell&gt;
        &lt;cell role="head"&gt;Kafka&lt;/cell&gt;
        &lt;cell role="head"&gt;Ayder&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Cluster restart (unclean)&lt;/cell&gt;
        &lt;cell&gt;2+ hours (reported in production)&lt;/cell&gt;
        &lt;cell&gt;40-50 seconds&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Broker sync after failure&lt;/cell&gt;
        &lt;cell&gt;181 minutes for 1TB data&lt;/cell&gt;
        &lt;cell&gt;Auto catch-up in seconds&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;50+ broker rolling restart&lt;/cell&gt;
        &lt;cell&gt;2+ hours (2 min per broker)&lt;/cell&gt;
        &lt;cell&gt;N/A ‚Äî single binary&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Tested crash recovery:&lt;/p&gt;
    &lt;code&gt;# 3-node cluster with 8 million offsets
1. SIGKILL a follower mid-write
2. Leader continues, follower misses offsets
3. Restart follower
4. Follower replays local AOF ‚Üí asks leader for missing offsets
5. Leader streams missing data ‚Üí follower catches up
6. Cluster fully healthy in 40-50 seconds
7. Zero data loss&lt;/code&gt;
    &lt;p&gt;No manual intervention. No partition reassignment. No ISR drama.&lt;/p&gt;
    &lt;code&gt;# Clone and run with Docker Compose (includes Prometheus + Grafana)
git clone https://github.com/A1darbek/ayder.git
cd ayder
docker compose up -d --build

# Or build and run standalone
docker build -t ayder .
docker run -p 1109:1109 --shm-size=2g ayder

# That's it. Now produce:
curl -X POST localhost:1109/broker/topics \
  -H 'Authorization: Bearer dev' \
  -H 'Content-Type: application/json' \
  -d '{"name":"events","partitions":4}'

curl -X POST 'localhost:1109/broker/topics/events/produce?partition=0' \
  -H 'Authorization: Bearer dev' \
  -d 'hello world'&lt;/code&gt;
    &lt;code&gt;# Dependencies: libuv 1.51+, openssl, zlib, liburing
make clean &amp;amp;&amp;amp; make
./ayder --port 1109&lt;/code&gt;
    &lt;p&gt;The included &lt;code&gt;docker-compose.yml&lt;/code&gt; brings up:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ayder on port &lt;code&gt;1109&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Prometheus on port &lt;code&gt;9090&lt;/code&gt;(metrics scraping)&lt;/item&gt;
      &lt;item&gt;Grafana on port &lt;code&gt;3000&lt;/code&gt;(dashboards, default password:&lt;code&gt;admin&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# docker-compose.yml
services:
  ayder:
    build: .
    ports:
      - "1109:1109"
    shm_size: 2g
    environment:
      - RF_BEARER_TOKENS=dev&lt;/code&gt;
    &lt;code&gt;# Create a topic
curl -X POST localhost:1109/broker/topics \
  -H 'Authorization: Bearer dev' \
  -H 'Content-Type: application/json' \
  -d '{"name":"events","partitions":8}'

# Produce a message
curl -X POST 'localhost:1109/broker/topics/events/produce?partition=0' \
  -H 'Authorization: Bearer dev' \
  -d 'hello world'

# Consume messages (binary-safe with base64)
curl 'localhost:1109/broker/consume/events/mygroup/0?limit=10&amp;amp;encoding=b64' \
  -H 'Authorization: Bearer dev'

# Commit offset
curl -X POST localhost:1109/broker/commit \
  -H 'Authorization: Bearer dev' \
  -H 'Content-Type: application/json' \
  -d '{"topic":"events","group":"mygroup","partition":0,"offset":10}'&lt;/code&gt;
    &lt;p&gt;A topic contains N partitions. Each partition is an independent append-only log with its own offset sequence.&lt;/p&gt;
    &lt;p&gt;Consumers read from &lt;code&gt;/broker/consume/{topic}/{group}/{partition}&lt;/code&gt;. Progress is tracked per &lt;code&gt;(topic, group, partition)&lt;/code&gt; tuple via explicit commits.&lt;/p&gt;
    &lt;p&gt;If you consume without specifying &lt;code&gt;?offset=&lt;/code&gt;, Ayder resumes from the last committed offset for that consumer group.&lt;/p&gt;
    &lt;p&gt;Ayder acknowledges writes in two modes:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;batch_id&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;durable&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sealed&lt;/cell&gt;
        &lt;cell&gt;Non-zero&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;true&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Appended to AOF, survives crashes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Rocket&lt;/cell&gt;
        &lt;cell&gt;Zero&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;false&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;In-memory fast path, not persisted&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Use &lt;code&gt;timeout_ms&lt;/code&gt; to wait for sync confirmation.&lt;/p&gt;
    &lt;code&gt;GET  /health      # ‚Üí {"ok":true}
GET  /ready       # ‚Üí {"ready":true}
GET  /metrics     # ‚Üí Prometheus format
GET  /metrics_ha  # ‚Üí HA cluster metrics&lt;/code&gt;
    &lt;p&gt;Create topic&lt;/p&gt;
    &lt;code&gt;POST /broker/topics
{"name":"events","partitions":8}&lt;/code&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{"ok":true,"topic":"events","partitions":8}&lt;/code&gt;
    &lt;p&gt;Single message (raw bytes in body)&lt;/p&gt;
    &lt;code&gt;POST /broker/topics/{topic}/produce
&lt;/code&gt;
    &lt;p&gt;Query parameters:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Parameter&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;partition&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Target partition (optional; auto-assigned if omitted)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;key&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Message key, URL-encoded (optional)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;idempotency_key&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Deduplication key, URL-encoded (optional)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;timeout_ms&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Wait for sync confirmation (optional)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;timing&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Set to &lt;code&gt;1&lt;/code&gt; to include timing breakdown (optional)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{
  "ok": true,
  "offset": 123,
  "partition": 0,
  "batch_id": 9991,
  "sealed": true,
  "durable": true,
  "mode": "sealed",
  "synced": true
}&lt;/code&gt;
    &lt;p&gt;Duplicate detection (when &lt;code&gt;idempotency_key&lt;/code&gt; matches):&lt;/p&gt;
    &lt;code&gt;{"ok":true,"offset":123,"partition":0,"sealed":true,"synced":null,"duplicate":true}&lt;/code&gt;
    &lt;p&gt;Batch produce (NDJSON ‚Äî one message per line)&lt;/p&gt;
    &lt;code&gt;POST /broker/topics/{topic}/produce-ndjson
&lt;/code&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{
  "ok": true,
  "first_offset": 1000,
  "count": 250,
  "partition": 0,
  "batch_id": 424242,
  "sealed": true,
  "durable": true,
  "mode": "sealed",
  "synced": false
}&lt;/code&gt;
    &lt;code&gt;GET /broker/consume/{topic}/{group}/{partition}
&lt;/code&gt;
    &lt;p&gt;Query parameters:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Parameter&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;offset&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Start offset, inclusive (resumes from commit if omitted)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;limit&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Max messages to return (default: 100, max: 1000)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;encoding&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Set to &lt;code&gt;b64&lt;/code&gt; for binary-safe base64 encoding&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{
  "messages": [
    {"offset": 0, "partition": 0, "value_b64": "aGVsbG8=", "key_b64": "a2V5"}
  ],
  "count": 1,
  "next_offset": 1,
  "committed_offset": 0,
  "truncated": false
}&lt;/code&gt;
    &lt;p&gt;Use &lt;code&gt;next_offset&lt;/code&gt; as the &lt;code&gt;?offset=&lt;/code&gt; parameter for subsequent reads.&lt;/p&gt;
    &lt;code&gt;POST /broker/commit
{"topic":"events","group":"g1","partition":0,"offset":124}
&lt;/code&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{"ok":true}&lt;/code&gt;
    &lt;p&gt;Commits are stored per &lt;code&gt;(topic, group, partition)&lt;/code&gt;. Backward commits are ignored.&lt;/p&gt;
    &lt;p&gt;Delete before offset (hard floor)&lt;/p&gt;
    &lt;code&gt;POST /broker/delete-before
{"topic":"events","partition":0,"before_offset":100000}
&lt;/code&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{"ok":true,"deleted_count":12345,"freed_bytes":987654}&lt;/code&gt;
    &lt;p&gt;Set retention policy&lt;/p&gt;
    &lt;code&gt;POST /broker/retention
&lt;/code&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;// TTL + size cap for specific partition
{"topic":"events","partition":0,"ttl_ms":60000,"max_bytes":104857600}

// TTL for all topics
{"topic":"*","ttl_ms":300000}&lt;/code&gt;
    &lt;p&gt;Ayder includes a key-value store with CAS (compare-and-swap) and TTL support.&lt;/p&gt;
    &lt;p&gt;Put&lt;/p&gt;
    &lt;code&gt;POST /kv/{namespace}/{key}?cas=&amp;lt;u64&amp;gt;&amp;amp;ttl_ms=&amp;lt;u64&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Body contains raw value bytes.&lt;/p&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{"ok":true,"cas":2,"sealed":true,"durable":true,"mode":"sealed","synced":true,"batch_id":123}&lt;/code&gt;
    &lt;p&gt;Get&lt;/p&gt;
    &lt;code&gt;GET /kv/{namespace}/{key}
&lt;/code&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{"value":"&amp;lt;base64&amp;gt;","cas":2}&lt;/code&gt;
    &lt;p&gt;Get metadata&lt;/p&gt;
    &lt;code&gt;GET /kv/{namespace}/{key}/meta
&lt;/code&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{"cas":2,"ttl_ms":12345}&lt;/code&gt;
    &lt;p&gt;Delete&lt;/p&gt;
    &lt;code&gt;DELETE /kv/{namespace}/{key}?cas=&amp;lt;u64&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{"ok":true,"deleted":true,"sealed":true,"durable":true,"mode":"sealed","synced":false,"batch_id":456}&lt;/code&gt;
    &lt;p&gt;Built-in stream processing ‚Äî no separate service required.&lt;/p&gt;
    &lt;code&gt;POST /broker/query
&lt;/code&gt;
    &lt;p&gt;Consume JSON objects from a topic/partition with:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Row filtering (eq, ne, lt, gt, in, contains)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;group_by&lt;/code&gt;with aggregations (count, sum, avg, min, max)&lt;/item&gt;
      &lt;item&gt;Field projection&lt;/item&gt;
      &lt;item&gt;Tumbling windows&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;POST /broker/join
&lt;/code&gt;
    &lt;p&gt;Windowed join between two sources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Join types: inner / left / right / full&lt;/item&gt;
      &lt;item&gt;Composite keys&lt;/item&gt;
      &lt;item&gt;Window size and allowed lateness&lt;/item&gt;
      &lt;item&gt;Optional &lt;code&gt;dedupe_once&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Cross-format support (Avro + Protobuf in same join)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Ayder supports 3, 5, or 7 node clusters with Raft-based replication.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Acknowledgment&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;async&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Leader appends locally, replicates in background&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;sync-majority&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Waits for majority (e.g., 2/3 nodes)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;sync-all&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Waits for all nodes&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Writes must go to the leader. If you send a write to a follower, it returns an HTTP redirect with the leader's address in the &lt;code&gt;Location&lt;/code&gt; header.&lt;/p&gt;
    &lt;p&gt;Options:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Follow redirects automatically&lt;/item&gt;
      &lt;item&gt;Discover the leader via &lt;code&gt;/metrics_ha&lt;/code&gt;and pin writes to it&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When a follower rejoins after downtime:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Replays local AOF&lt;/item&gt;
      &lt;item&gt;Connects to leader&lt;/item&gt;
      &lt;item&gt;Requests missing offsets&lt;/item&gt;
      &lt;item&gt;Leader streams missing data&lt;/item&gt;
      &lt;item&gt;Follower catches up automatically&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example scenario:&lt;/p&gt;
    &lt;code&gt;# 3-node cluster
node1 (7001) = LEADER
node2 (8001) = FOLLOWER
node3 (9001) = FOLLOWER

# Write to leader
curl -X POST 'localhost:7001/broker/topics/test/produce?partition=0' \
  -H 'Authorization: Bearer dev' -d 'msg-0'
# ‚Üí offset 0

# Kill node2
kill -9 $(pgrep -f "port 8001")

# Write while node2 is down
curl -X POST 'localhost:7001/broker/topics/test/produce?partition=0' \
  -H 'Authorization: Bearer dev' -d 'msg-1'
# ‚Üí offset 1

curl -X POST 'localhost:7001/broker/topics/test/produce?partition=0' \
  -H 'Authorization: Bearer dev' -d 'msg-2'
# ‚Üí offset 2

# Restart node2 ‚Äî automatically catches up to offset 2

# Verify all data is present on recovered node
curl 'localhost:8001/broker/consume/test/g1/0?offset=0&amp;amp;limit=10' \
  -H 'Authorization: Bearer dev'
# ‚Üí offsets 0, 1, 2 all present&lt;/code&gt;
    &lt;code&gt;# Default port is 1109
./ayder --port 1109

# Or specify custom port
./ayder --port 7001&lt;/code&gt;
    &lt;p&gt;Ayder uses Raft for consensus. Here's a complete 3-node setup:&lt;/p&gt;
    &lt;p&gt;Environment variables:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Variable&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_ENABLED&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Enable HA mode (&lt;code&gt;1&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_NODE_ID&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Unique node identifier&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_NODES&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Cluster topology: &lt;code&gt;id:host:raft_port:priority,...&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_BOOTSTRAP_LEADER&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Set to &lt;code&gt;1&lt;/code&gt; on initial leader only&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_WRITE_CONCERN&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Nodes to wait for: &lt;code&gt;1&lt;/code&gt;=leader only, &lt;code&gt;2&lt;/code&gt;=majority, &lt;code&gt;N&lt;/code&gt;=all&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_DEDICATED_WORKER&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Set to &lt;code&gt;0&lt;/code&gt; for best P99 latency (highly recommended)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_TLS&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Enable mTLS for Raft (&lt;code&gt;1&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_TLS_CA&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Path to CA certificate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_TLS_CERT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Path to node certificate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_TLS_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Path to node private key&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_BEARER_TOKENS&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;HTTP auth tokens (format: &lt;code&gt;token1@scope:token2:...&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;3-Node Example:&lt;/p&gt;
    &lt;code&gt;# Node 1 (bootstrap leader)
export RF_HA_ENABLED=1
export RF_HA_NODE_ID=node1
export RF_HA_BOOTSTRAP_LEADER=1
export RF_HA_NODES='node1:10.0.0.1:7000:100,node2:10.0.0.2:8000:50,node3:10.0.0.3:9000:25'
export RF_HA_WRITE_CONCERN=2  # sync-majority (2/3 nodes)
export RF_HA_DEDICATED_WORKER=0  # critical for low P99
export RF_BEARER_TOKENS='dev@scope:token2:token3'
export RF_HA_TLS=1
export RF_HA_TLS_CA=./certs/ca.crt
export RF_HA_TLS_CERT=./certs/node1.crt
export RF_HA_TLS_KEY=./certs/node1.key
./ayder --port 7001

# Node 2
export RF_HA_ENABLED=1
export RF_HA_NODE_ID=node2
export RF_HA_NODES='node1:10.0.0.1:7000:100,node2:10.0.0.2:8000:50,node3:10.0.0.3:9000:25'
export RF_HA_WRITE_CONCERN=2
export RF_HA_DEDICATED_WORKER=0
export RF_BEARER_TOKENS='dev@scope:token2:token3'
export RF_HA_TLS=1
export RF_HA_TLS_CA=./certs/ca.crt
export RF_HA_TLS_CERT=./certs/node2.crt
export RF_HA_TLS_KEY=./certs/node2.key
./ayder --port 8001

# Node 3 (same pattern, port 9001)&lt;/code&gt;
    &lt;p&gt;5-Node and 7-Node Clusters:&lt;/p&gt;
    &lt;code&gt;# 5-node topology
export RF_HA_NODES='node1:host1:7000:100,node2:host2:8000:80,node3:host3:9000:60,node4:host4:10000:40,node5:host5:11000:20'
export RF_HA_WRITE_CONCERN=3  # majority of 5

# 7-node topology
export RF_HA_NODES='node1:host1:7000:100,node2:host2:8000:90,node3:host3:9000:80,node4:host4:10000:70,node5:host5:11000:60,node6:host6:12000:50,node7:host7:13000:40'
export RF_HA_WRITE_CONCERN=4  # majority of 7&lt;/code&gt;
    &lt;p&gt;Generate TLS certificates:&lt;/p&gt;
    &lt;code&gt;# Create CA
openssl req -x509 -newkey rsa:4096 -keyout ca.key -out ca.crt \
  -days 365 -nodes -subj "/CN=ayder-ca"

# Create node certificate (repeat for each node)
openssl req -newkey rsa:2048 -nodes -keyout node1.key -out node1.csr \
  -subj "/CN=node1" -addext "subjectAltName=DNS:node1,IP:10.0.0.1"

openssl x509 -req -in node1.csr -CA ca.crt -CAkey ca.key -CAcreateserial \
  -out node1.crt -days 365 -copy_extensions copy&lt;/code&gt;
    &lt;p&gt;Write concern tradeoffs:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;RF_HA_WRITE_CONCERN&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;Durability&lt;/cell&gt;
        &lt;cell role="head"&gt;Latency&lt;/cell&gt;
        &lt;cell role="head"&gt;Survives&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;1&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Low&lt;/cell&gt;
        &lt;cell&gt;~1ms&lt;/cell&gt;
        &lt;cell&gt;Nothing (leader only)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;&lt;code&gt;2&lt;/code&gt; (3-node)&lt;/cell&gt;
        &lt;cell&gt;High&lt;/cell&gt;
        &lt;cell&gt;~3ms&lt;/cell&gt;
        &lt;cell&gt;1 node failure&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;&lt;code&gt;3&lt;/code&gt; (5-node)&lt;/cell&gt;
        &lt;cell&gt;High&lt;/cell&gt;
        &lt;cell&gt;~3ms&lt;/cell&gt;
        &lt;cell&gt;2 node failures&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;code&gt;N&lt;/code&gt; (all nodes)&lt;/cell&gt;
        &lt;cell&gt;Maximum&lt;/cell&gt;
        &lt;cell&gt;Higher&lt;/cell&gt;
        &lt;cell&gt;N-1 failures, but blocks if any node slow&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Built by Aydarbek Romanuly ‚Äî solo founder from Kazakhstan üá∞üáø&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GitHub: @A1darbek&lt;/item&gt;
      &lt;item&gt;Email: aidarbekromanuly@gmail.com&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Errors follow a consistent format:&lt;/p&gt;
    &lt;code&gt;{
  "ok": false,
  "error": "missing_topic",
  "message": "Topic name is required",
  "docs": "https://ayder.dev/docs/api/produce"
}&lt;/code&gt;
    &lt;p&gt;‚úÖ HTTP-native event log with partitions and offsets&lt;lb/&gt; ‚úÖ Fast writes with cursor-based consumption&lt;lb/&gt; ‚úÖ Durable with crash recovery&lt;lb/&gt; ‚úÖ Horizontally scalable with Raft replication&lt;lb/&gt; ‚úÖ Built-in stream processing with cross-format joins&lt;lb/&gt; ‚úÖ ARM64-native (tested on Snapdragon X Elite)&lt;/p&gt;
    &lt;p&gt;‚ùå Kafka protocol compatible&lt;lb/&gt; ‚ùå A SQL database&lt;lb/&gt; ‚ùå Magic exactly-once without client-side idempotency discipline&lt;/p&gt;
    &lt;p&gt;MIT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46604862</guid><pubDate>Tue, 13 Jan 2026 17:55:37 +0000</pubDate></item><item><title>The Case for Blogging in the Ruins</title><link>https://www.joanwestenberg.com/the-case-for-blogging-in-the-ruins/</link><description>&lt;doc fingerprint="2cac1f510ee48e99"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Case for Blogging in the Ruins&lt;/head&gt;
    &lt;p&gt;In 1751, Denis Diderot began publishing his Encyclop√©die, a project that would eventually span 28 volumes and take more than two decades to complete. The French government banned it twice. The Catholic Church condemned it, Diderot's collaborators abandoned him, his publisher secretly censored entries behind his back, and he worked himself into periodic breakdowns trying to finish the damn thing.&lt;/p&gt;
    &lt;p&gt;When people talk about the Enlightenment as if it were an intellectual garden party where everyone sipped wine and agreed about reason, they're missing the part where producing and distributing ideas was (in fact) dangerous and thankless work.&lt;/p&gt;
    &lt;p&gt;Diderot has been on my mind lately, spending the Xmas period scrolling through the dwindling numbers of social media platforms that haven't yet been purchased by an "eccentric" (read: race-science obsessed) billionaire or banned by a foreign government.&lt;/p&gt;
    &lt;p&gt;Diderot's project was fundamentally about building infrastructure for thinking. He wanted to create a shared repository of human knowledge that anyone could access, organized in a way that invited exploration and cross-referencing. He believed that structuring information properly could change how people thought.&lt;/p&gt;
    &lt;p&gt;He was right.&lt;/p&gt;
    &lt;p&gt;270 years later, we have more information than any civilization in history. But aside from Wikipedia, we've organized the sum total of our collective knowledge into formats optimized for making people angry at strangers in pursuit of private profitability.&lt;/p&gt;
    &lt;p&gt;Something has gone terribly wrong.&lt;/p&gt;
    &lt;p&gt;And I think the fix, or at least part of it = going backwards to a technology we've largely abandoned: the blog, humble // archaic as it may seem.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Pamphlet Problem&lt;/head&gt;
    &lt;p&gt;Before social media ate the internet, and before the internet ate everything else, and before everything else ate itself, blogs occupied a wonderful and formative niche in the information ecosystem. They were personal but public, permanent but updateable, long-form but informal. A blog post could be three paragraphs or thirty pages. It could be rigorously researched or entirely speculative. It could build an argument over weeks or months, with each post serving as a chapter in an ongoing intellectual project that readers could follow, critique, and respond to.&lt;/p&gt;
    &lt;p&gt;The blogosphere of the mid-2000s had its problems: It was insular and often smug, prone to flame wars between people who agreed on 95% of everything but found the remaining 5% absolutely unforgivable. But it also produced actual intellectual communities.&lt;/p&gt;
    &lt;p&gt;Remember those?&lt;/p&gt;
    &lt;p&gt;People wrote long responses to each other's posts, those responses generated further responses, and you could follow the thread of an argument across multiple sites and weeks of discussion. The format rewarded careful thinking because careful thinking was legible in a way that it simply isn't on platforms designed for rapid-fire engagement.&lt;/p&gt;
    &lt;p&gt;Social media removed the friction of publishing, and in doing so removed the selection pressure that separated signal from noise. We "democratized" the ability to publish (good?) while simultaneously destroying the conditions that made publishing meaningful (bad!).&lt;/p&gt;
    &lt;p&gt;Diderot spent twenty years on his infrastructure.&lt;/p&gt;
    &lt;p&gt;We handed ours to advertising companies and - like Pilate - washed our hands of it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Architecture of Attention&lt;/head&gt;
    &lt;p&gt;When you write a blog post, you're creating a standalone document with a permanent URL. It exists at a specific address on the web, and that address doesn't change based on who's looking at it, when they're looking at it, or what algorithm has decided they should see next. The post is there, stable, waiting for whoever wants to find it.&lt;/p&gt;
    &lt;p&gt;Compare this to a tweet (by God I'll not call them "X's") or a Facebook post, which exists primarily as an item in a feed, algorithmically sorted, personalized to each viewer. Your post might appear at the top of someone's feed for an hour and then disappear into an infinite scroll of other content, never to be seen again. The platform has no interest in whether your post is found next week or next year; it has a vested interest in keeping users scrolling through new content right now.&lt;/p&gt;
    &lt;p&gt;When I write a blog post, I'm writing for an imagined reader who has arrived at this specific URL because they're interested in this specific topic; I can assume a baseline of engagement; I can make my case over several thousand words, trusting that anyone who's made it to paragraph twelve probably intends to make it to paragraph twenty.&lt;/p&gt;
    &lt;p&gt;When I write for social media, I'm writing for someone who is one thumb-flick away from a video of either a hate crime or a dog riding a skateboard. Everything I produce has to compete, in real-time, with everything else that could possibly occupy that user's attention. The incentives push toward provocation and emotional activation. The format actively punishes nuance, which means that a thoughtful caveat reads as weakness and any acknowledgment of uncertainty looks like waffling.&lt;/p&gt;
    &lt;p&gt;Diderot understood that the container shapes the contents. The Encyclop√©die was a collection of facts, yes, but more fundamentally it was an argument about how knowledge should be organized. Cross-references between entries were themselves a form of commentary, connecting ideas that authorities wanted kept separate. We've restructured the presentation of ideas around the needs of advertising platforms, and not by accident, and we're living with the consequences.&lt;/p&gt;
    &lt;head rend="h2"&gt;Montaigne's Heirs&lt;/head&gt;
    &lt;p&gt;Michel de Montaigne arguably invented the essay in the 1570s, sitting in a tower in his French ch√¢teau, writing about whatever interested him: cannibals, thumbs, the education of children, how to talk to people who are dying. He called these writings essais, meaning "attempts" or "tries." The form was explicitly provisional. Montaigne was trying out ideas, seeing where they led, acknowledging uncertainty as a fundamental feature rather than a bug to be eliminated.&lt;/p&gt;
    &lt;p&gt;The blog, at its best (a best I aspire one day to reach) is Montaigne's direct descendant. It's a form that allows for intellectual exploration without demanding premature certainty. You can write a post working through an idea, acknowledge in the post itself that you're not sure where you'll end up, and invite readers to think alongside you. You can return to the topic weeks later with updated thoughts. The format accommodates the actual texture of thinking, which is messy and recursive and full of wrong turns.&lt;/p&gt;
    &lt;p&gt;Social media flattens all of this into statements: Everything you post is implicitly a declaration. Even if you add caveats, the format strips them away. What travels is the hot take, the dunked-on screenshot, the increasingly-shitty meme, the version of your argument that fits in a shareable image with the source cropped out.&lt;/p&gt;
    &lt;p&gt;I keep thinking about how many interesting folks have essentially stopped writing anything substantial because they've moved their entire intellectual presence to Twitter or Substack Notes. These are people who used to produce ten-thousand-word explorations of complex topics, and now they produce dozens of disconnected fragments per day, each one optimized for immediate engagement and none of them building toward anything coherent.&lt;/p&gt;
    &lt;p&gt;It's like watching someone who used to compose symphonies decide to only produce ringtones.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Makes a Blog Actually Work&lt;/head&gt;
    &lt;p&gt;Most blogs are abandoned after three posts. The ones that persist // accumulate value have a few things on common:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;They have a perspective, not just a topic. A blog about "marketing" is forgettable. A blog about "why most marketing advice is wrong and what actually works" has a point of view that gives every post something to push against.&lt;/item&gt;
      &lt;item&gt;They build. The best blogs create posts that reference and extend earlier posts, developing ideas over time rather than starting from scratch each week. Gwern's site is an extreme example, with entries that get updated for years, accumulating evidence and refinement. But even a modest version of this works: a body of work that compounds.&lt;/item&gt;
      &lt;item&gt;They're written for someone specific. Not "everyone interested in X" but "the person who already knows Y and is trying to figure out Z." Specificity creates resonance.&lt;/item&gt;
      &lt;item&gt;They exist at a consistent address. A blog at your own domain, with permanent URLs, can be found and referenced for years. Content locked inside platforms disappears when the platform changes or dies.&lt;/item&gt;
      &lt;item&gt;Their authors accept that most posts won't go viral, and that's fine. The value is cumulative. A blog with fifty solid posts is an asset even if no single post ever breaks through. Most social media content has a half-life of hours; a good blog post can draw readers for a decade.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The Discovery Problem (And Why It's Overstated)&lt;/head&gt;
    &lt;p&gt;There are rote objections: nobody reads blogs anymore. The discovery mechanisms are broken. How is anyone supposed to find a new blog when they're competing against algorithmic feeds specifically designed to capture and hold attention?&lt;/p&gt;
    &lt;p&gt;Etc, etc, etc.&lt;/p&gt;
    &lt;p&gt;But there are a few things worth noting:&lt;/p&gt;
    &lt;p&gt;Search engines still index blogs far better than social media posts. A well-written blog post on a specific topic can draw readers for years through Google (or Kagi // DuckDuckGo if you're nasty, and by nasty I mean excellent); a tweet is lucky to get attention for twelve hours. Hell, call it six. Hell, call it three and call me an optimist at that. If you're trying to build a body of work, or to create something that will outlast the platform of the moment, a blog is simply a better tool.&lt;/p&gt;
    &lt;p&gt;What else?&lt;/p&gt;
    &lt;p&gt;RSS never actually died. It went underground. Feedly, Unread, NetNewsWire, and other readers still have millions of users. The people who read blogs tend to be the people worth reaching: curious, patient, willing to engage with longer arguments.&lt;/p&gt;
    &lt;p&gt;Newsletters are still a discovery layer, no matter how many people pronounce their untimely death. You can write on your own site and distribute via email, getting the permanence of a blog with the push distribution of a newsletter. The writing lives at your domain; the email is notification infrastructure.&lt;/p&gt;
    &lt;p&gt;And the fragmentation of social media is actually creating demand for alternatives. Every time a platform implodes (Twitter's ongoing collapse, Instagram's slow retirement // decay into a metaphorical Floridian condo, TikTok's uncertain status, Facebook's demographic hollowing) people start looking for more stable ground. The infrastructure exists. It's waiting.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Room of Your Own&lt;/head&gt;
    &lt;p&gt;Virginia Woolf wrote about the importance of having a room of one's own: physical space for creative work, free from interruption and control. A blog is a room of your own on the internet. It's a place where you decide what to write about and how to write about it, where you're not subject to the algorithmic whims of platforms that profit from your engagement regardless of whether that engagement makes you or anyone else nebulously smarter.&lt;/p&gt;
    &lt;p&gt;Diderot built the Encyclop√©die because he believed that organizing knowledge properly could change how people thought. He spent two decades on it. He went broke. He watched collaborators quit and authorities try to destroy his work. He kept going because the infrastructure mattered, because how we structure the presentation of ideas affects the ideas themselves.&lt;/p&gt;
    &lt;p&gt;We're not going to get a better internet by waiting for platforms to become less extractive. We build it by building it. By maintaining our own spaces, linking to each other, creating the interconnected web of independent sites that the blogosphere once was and could be again.&lt;/p&gt;
    &lt;p&gt;So:&lt;/p&gt;
    &lt;p&gt;Start a blog. Start one because the practice of writing at length, for an audience you respect, about things that matter to you, is itself valuable. Start one because owning your own platform is a form of independence that becomes more important as centralized platforms become less trustworthy. Start one because the format shapes the thought, and this format is good for thinking.&lt;/p&gt;
    &lt;p&gt;The blog won't save us. But it's one of the tools we'll need if we're going to save ourselves.&lt;/p&gt;
    &lt;head rend="h2"&gt;Start Today&lt;/head&gt;
    &lt;p&gt;If you're convinced, the practical options:&lt;/p&gt;
    &lt;p&gt;Write.as is minimalist, privacy-focused, no-frills. You can start anonymously and upgrade to a custom domain later. The editor gets out of your way. Good for people who want to write without fiddling with settings.&lt;/p&gt;
    &lt;p&gt;Bear Blog is extremely lightweight, fast-loading, no tracking. Free tier is generous. The aesthetic is deliberately simple, which enforces a focus on writing over design tinkering. Privacy-first.&lt;/p&gt;
    &lt;p&gt;Ghost (powering this blog) is more fully-featured, with built-in membership and newsletter tools. You can self-host (free) or use their managed hosting (paid). Good if you want to eventually build a paid subscriber base but want to own your infrastructure. Open source.&lt;/p&gt;
    &lt;p&gt;Micro.blog was built explicitly as an alternative to social media, with cross-posting, a community timeline, and support for short and long posts. Has an indie web ethos baked in. Good if you want the social layer without the algorithmic manipulation. Manton (founder) is one of my favourite folks to follow // read lately. Good insights.&lt;/p&gt;
    &lt;p&gt;All of these let you use a custom domain, which you should do. Buy yourname.com. It costs ten dollars a year and your writing will live at an address you control, regardless of what happens to any particular platform.&lt;/p&gt;
    &lt;p&gt;Pick one. Set it up this week. Write something. Send me a link (email joan@joanwestenberg.com) and I'll read it. The first post doesn't have to be good; it has to exist. The second one can be better. That's how this works. That's how it's always worked.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46604959</guid><pubDate>Tue, 13 Jan 2026 18:00:20 +0000</pubDate></item><item><title>The rapid rise and slow decline of Sam Altman</title><link>https://garymarcus.substack.com/p/the-rapid-rise-and-slow-decline-of</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46605452</guid><pubDate>Tue, 13 Jan 2026 18:29:02 +0000</pubDate></item><item><title>AI Generated Music Barred from Bandcamp</title><link>https://old.reddit.com/r/BandCamp/comments/1qbw8ba/ai_generated_music_on_bandcamp/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46605490</guid><pubDate>Tue, 13 Jan 2026 18:31:50 +0000</pubDate></item><item><title>Signal leaders warn agentic AI is an insecure, unreliable surveillance risk</title><link>https://coywolf.com/news/productivity/signal-president-and-vp-warn-agentic-ai-is-insecure-unreliable-and-a-surveillance-nightmare/</link><description>&lt;doc fingerprint="bfc31855efebb4cf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;‚ÄòSignal‚Äô President and VP warn agentic AI is insecure, unreliable, and a surveillance nightmare&lt;/head&gt;
    &lt;p&gt;With agentic AI embedded at the OS level, databases storing entire digital lives accessible to malware, tasks whose reliability quickly breaks down at each step, and being opted-in without consent, Signal leadership is sounding the alarm for the industry to pull back until threats can be mitigated.&lt;/p&gt;
    &lt;p&gt;At the 39th Chaos Communication Congress (39C3) in Hamburg, Germany, Signal President Meredith Whittaker and VP of Strategy and Global Affairs Udbhav Tiwari gave a presentation titled AI Agent, AI Spy. In it, they shared the many vulnerabilities and concerns they have about how agentic AI is being implemented, the very real threat it‚Äôs bringing to enterprise companies, and how they recommend the industry change to mitigate a disaster in the making.&lt;/p&gt;
    &lt;p&gt;A key component of AI agents is that they must know enough about you and have access to sensitive data so that they can autonomously take actions on your behalf, such as making purchases, scheduling events, and responding to messages. However, the way AI agents are being implemented is making them insecure, unreliable, and open to surveillance.&lt;/p&gt;
    &lt;head rend="h2"&gt;How AI agents are vulnerable to threats&lt;/head&gt;
    &lt;p&gt;Microsoft is trying to bring agentic AI to its Windows 11 users via Recall. Recall takes a screenshot of your screen every few seconds, OCRs the text, and does semantic analysis of the context and actions. It then creates a forensic dossier of everything you do into a single database on your computer. The database includes a precise timeline of actions, full raw text (via OCR), dwell time, and focus on specific apps and actions. Additionally, it assigns topics to specific activities.&lt;/p&gt;
    &lt;p&gt;Tiwari says the problem with this approach is that it doesn‚Äôt mitigate the threat of malware (via online attacks) and indirect (hidden) prompt injection attacks, which can all gain access to the database. These vulnerabilities subsequently circumvent end-to-end encryption (E2EE), prompting Signal to add a flag in its app to prevent its screen from being recorded, but Tiwari says that‚Äôs not a reliable or long-term solution.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why complex agentic tasks aren‚Äôt reliable&lt;/head&gt;
    &lt;p&gt;Whittaker emphasized that agentic AI isn‚Äôt just intrusive and vulnerable to threats; it‚Äôs also unreliable. She said AI agents are probabilistic, not deterministic, and that each step they take in a task degrades their accuracy and the final action.&lt;/p&gt;
    &lt;p&gt;She said if an AI agent could perform each step with 95% accuracy‚Äìwhich currently isn‚Äôt possible‚Äìa 10-step task would yield an action with a ~59.9% success rate. And if you had a 30-step task, the success rate would be ~21.4%. Furthermore, if we used a more realistic accuracy rate of 90%, then a 30-step task would drop down to a success rate of 4.2%. She added that the best agent models failed 70% of the time.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to make AI agents private and secure&lt;/head&gt;
    &lt;p&gt;Whittaker said there currently isn‚Äôt a solution for making AI agents preserve privacy, security, and control; there‚Äôs only triage, but companies can take steps now to mitigate it.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Stop the reckless deployment of AI agents to avoid plain-text database access to malware.&lt;/item&gt;
      &lt;item&gt;Make opting out the default, with mandatory developer opt-ins.&lt;/item&gt;
      &lt;item&gt;AI companies must provide radical (or any) transparency about how everything works and make it auditable at the granular level.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If the industry doesn‚Äôt heed Whittaker‚Äôs and Tiwari‚Äôs warnings, the age of agentic AI could be in jeopardy, primarily because consumers could quickly lose their trust in a technology that is already overhyped and over-invested in.&lt;/p&gt;
    &lt;p&gt;Jon Henshaw is the founder of Coywolf and an industry veteran with almost three decades of SEO, digital marketing, and web technologies experience. Follow @jon@henshaw.social&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46605553</guid><pubDate>Tue, 13 Jan 2026 18:35:52 +0000</pubDate></item><item><title>The Housing Market Isn't for Single People</title><link>https://thewalrus.ca/the-housing-market-isnt-for-single-people/</link><description>&lt;doc fingerprint="3fd8015738adce97"&gt;
  &lt;main&gt;
    &lt;p&gt;There‚Äôs a running joke in one of my friend circles that, if we ever pooled our resources and lived together, we‚Äôd need a cat room. We daydream about what our space would be like beyond feline accommodations: Everyone would have their own bedroom with en suite bathroom, naturally. We‚Äôd have a communal kitchen and living space, so we could hang out and get some friend time in. We‚Äôd have a housekeeper come in once a week, and as we get older, maybe a nurse to check in on us. It‚Äôs a great dream, and I think we‚Äôre half serious about living together, especially because half of us still rent. As my friend Diane once said, ‚ÄúAs soon as I think I‚Äôve saved enough for a down payment, prices jump.‚Äù&lt;/p&gt;
    &lt;p&gt;Housing is undeniably one of the major questions of our time. In Canada, not only is home ownership out of reach for many people, but even paying rent is getting harder. A 2024 Abacus survey showed nearly three in five Canadians are somewhat or very concerned about losing their home or rental because of financial issues. And that precarity is even more pronounced for single people.&lt;/p&gt;
    &lt;p&gt;The old rule of thumb was that housing should be 30 percent of your net income. At 40 percent, the Organisation of Economic Co-operation and Development (OECD) considers a household ‚Äúoverburdened.‚Äù Overburdened is starting to look like the new normal.&lt;/p&gt;
    &lt;p&gt;In 2025, when I wrote this, surveys found that some Canadians were spending way more than 30 percent of their income on a place to live. The median gross income for single people was $45,069, and 30 percent of that is $13,521, or $1,125 per month. The average rent for a one-bedroom apartment in Canada, in January 2025, was $2,109, almost double the 30 percent guideline for a median income Canadian. The average monthly mortgage payment in 2024, according to the Canadian Mortgage and Housing Corporation, ranged between $1,337 in New Brunswick and $2,836 in British Columbia. In the United States, the average monthly mortgage payment in 2024 was $2,209 (US). California has the highest monthly payments at $2,500 (US), compared to West Virginia‚Äôs $960 (US).&lt;/p&gt;
    &lt;p&gt;But where did this 30 percent guideline even come from? I asked Carolyn Whitzman, a housing and social policy consultant who has worked as an expert adviser to the University of British Columbia‚Äôs Housing Assessment Resource Tools project, which developed standardized best practices for analyzing housing needs using detailed, open data. When we spoke, she worked as a senior housing researcher at University of Toronto‚Äôs School of Cities, researching best practices to scale affordable ‚Äúmissing middle,‚Äù modular, and replicable housing.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt‚Äôs both arbitrary and standard,‚Äù she said. The standard part of the housing costs calculator was set by the OECD in the 1980s, and Canada has used it since. Other countries, such as the US, the United Kingdom, and Australia, also follow this guideline.&lt;/p&gt;
    &lt;p&gt;‚ÄúBack in the early days of labour rights, there was a notion of eight hours for rest, eight hours for work, and eight hours for what you will. Out of that same era, there was advocacy for one day‚Äôs work for one week‚Äôs rent, which means 20 percent, not 30 percent. And when Canada started doing housing policy, for instance, there‚Äôs a report called the Curtis Report on postwar reconstruction that came out in 1944. They used 20 percent as their standard of affordability, which slowly crept up to first 25 percent and then 30 percent. Quebec still uses 25 percent as its definition of affordability.‚Äù&lt;/p&gt;
    &lt;p&gt;The Curtis Report was produced by a committee set up to analyze and manage possible problems of postwar reconstruction, policies, and programs. It used income category measures, a clear definition of affordability based on proportion of household income, as well as a housing need assessment method that included both ‚Äúaccumulated needs‚Äù and future ‚Äúneeds arising from population growth and [affordable housing] replacement.‚Äù It recommended one-third of new construction be nonprofit public housing, one-third regulated rental, and one-third private market home ownership.&lt;/p&gt;
    &lt;p&gt;But the federal government decided to, instead, let the market decide. They put policies in place that would increase home ownership rates but ignored low-income households.&lt;/p&gt;
    &lt;p&gt;Whitzman pointed out that we‚Äôre still operating under cultural assumptions that have been in place for decades. One of those ideas is that owning a home is a pathway to creating wealth. ‚ÄúWhat started happening is that our finance ministers were really worried about pensions and people living longer,‚Äù she said. ‚ÄúSo, they thought, ‚ÄòWouldn‚Äôt it be great if their home was their pension fund? What if everyone, except for these failures, owned a home and, [w]hen they sold the home, they made a profit?‚Äô‚Äù She points out that life expectancy used to be shorter, and these days, people are living well into their eighties. In fact, if you‚Äôre in your seventies, you have a 25 percent chance of living into your nineties, according to actuaries.&lt;/p&gt;
    &lt;p&gt;We‚Äôre not keeping up with demographic trends in other ways either. Whitzman said that a lot of our housing policy is still locked in ‚Äúthe identification of a normal family that‚Äôs going to do certain things and die at certain times and not get divorced. And it‚Äôs just so absurd.‚Äù She points out that demographics are completely different now, and those housing policies are still locked in assumptions from another century when people were expected to get married and have children in their twenties.&lt;/p&gt;
    &lt;p&gt;I was very, very lucky when I bought my condo. It was early 2009, right after the 2008 financial crisis started and just before prices in Toronto started climbing. I took out $12,500 from my retirement account through the Home Buyers‚Äô Plan, which was less than the 20 percent needed for a down payment. I was making about $76,000 a year as a senior lifestyle editor and qualified for a $250,000 mortgage. (Those were the days.) What helped me afford my place through years of layoffs and freelance work was the purchase price and nearly seventeen years of ridiculously low interest rates. It was good timing that had very little to do with financial planning on my end.&lt;/p&gt;
    &lt;p&gt;Recently, I was doing my monthly troll through real estate websites, looking at condos, and I came to the realization that I am priced out of my own neighbourhood. If I sold my place, I could afford a smaller, less well-built condo if I wanted to stay where I am. Or I could move out of the city, get a roommate, or move to a less-expensive province. I could move back in with my parents or move in with my brother and sister-in-law. I love my family, but none of those options are appealing right now, or ever.&lt;/p&gt;
    &lt;p&gt;And just because I own doesn‚Äôt mean I‚Äôm secure. I live in an area designated for high-density housing, all those fifty-story condos. Rumours abound that developers have had their eyes on my building for a while, so there is a lingering wait-and-see hope that an eventual offer would be sweet enough to convince the majority of homeowners in my building to sell. I imagine being close to retirement (lol) when we‚Äôd get an offer that‚Äôs too good to refuse, and I have to find housing right at the time when I would rather not.&lt;/p&gt;
    &lt;p&gt;I‚Äôm one of the few people in my friend group of Gen Xers and elder millennials who owns, and I had a bit of help from my family. The rest of us rent. None of us want anything big. A little bit of outdoor space would be nice, as we all learned during the pandemic that being able to literally touch grass is good for mental health. Not worrying about whether your landlord will raise your rent (beyond the yearly increase) or renovict you is great for mental health. Partners with dual incomes likely split their housing costs, whether mortgage payments or monthly rent, but we singles pay for that on our own, along with everything from utilities to condo maintenance, insurance, and repair costs.&lt;/p&gt;
    &lt;p&gt;For single Canadians, home ownership has become increasingly out of reach. In 2024, you needed a household income of $137,000 to buy a condo in Toronto and $195,420 to buy a single-family detached home. In Vancouver, you‚Äôd need an income of $214,460 to afford an average-priced home. The average income in Canada is $57,100, so you can do the math.&lt;/p&gt;
    &lt;p&gt;Even for renters, carrying housing costs solo is difficult. The average rent in Canada, in 2024, was $2,185, and in the big cities, like Toronto, Vancouver, Burnaby, and Mississauga, rents were higher. Smaller cities had lower rents, but salaries were also generally lower, and they also saw more people moving in who were looking for cheaper rents, which increased those rents. This is why it‚Äôs vital to maintain strong rent control‚Äîwhich generally limits the frequency and extent of rent increases. Rent control exists, in some form, in all Canadian provinces and territories, though in the US, thirty-seven states outright prohibit it. If there aren‚Äôt limits on landlords, tenants‚Äîespecially single tenants‚Äîare even more vulnerable, and prices are primed to climb even more quickly.&lt;/p&gt;
    &lt;p&gt;Jo Pavlov, an education worker from Hamilton, Ontario, rents a two-bedroom place and said that housing costs are one of the biggest expenses of being single. They told me a few years ago, ‚ÄúI started [renting my current house] with a friend, and we split the bills down the middle. When she left, all responsibilities fell to me, and it‚Äôs more than I can afford.‚Äù Pavlov estimated that paying for all housing costs alone, including bills, ate up over 60 percent of their net income. Since speaking to Pavlov, they have found another roommate and got a raise, but they still don‚Äôt think they can carry the rental cost of their apartment for an extended period.&lt;/p&gt;
    &lt;p&gt;Some people are fine with getting a roommate, but what if you‚Äôre not? Maybe you are fifty, sixty, sixty-five, or seventy, and you want your own space to do your own thing. Maybe you have kids, so you already have roommates and don‚Äôt want another random adult around. Needing a roommate should not be the default for owning or renting. ‚ÄúI mean, it is an option. I think most of us have had roommates during our college years,‚Äù said financial journalist David Aston. ‚ÄúYou know, being able to live independently on one‚Äôs own without a roommate is certainly something most people would aspire to because roommates are complicated. Usually, people get to a certain stage of life, and they like their independence.‚Äù&lt;/p&gt;
    &lt;p&gt;My friend Celia and I rented a good-sized apartment in Toronto‚Äôs Davisville neighbourhood for nearly five years. It was about 900 square feet, which was perfect for two tall women and a cat. The apartment had two bedrooms: one was a whole square foot bigger than the other, so Celia got that, and her cat got the extra square foot . . . and the rest of the apartment, including the bathroom and my bedroom, sometimes. The apartment was right on the subway line, everything except cable and phone was included, and we paid‚Äîwait for it‚Äî$1,100 when we moved in together and $1,400 when we moved out. Total. That included utilities.&lt;/p&gt;
    &lt;p&gt;When we decided to look for condos to buy, we originally talked about pooling our mortgage approvals and finding a two-bedroom place for us and future cats. We found an agent and started looking. (The cat stayed home.) We wanted to stay in the city, as we both worked in it and didn‚Äôt own cars. And there were two-bedroom condos, but the rooms weren‚Äôt the same size. One would be the average size of a condo bedroom (ten by twelve feet) while the other would be a den, which, no, real estate agents‚Äîif it doesn‚Äôt have a door, it‚Äôs not a room. I don‚Äôt care what your fish-eye lens photographs say. It‚Äôs a large nook. Eventually, we became dissatisfied with what we were seeing.&lt;/p&gt;
    &lt;p&gt;We talked and realized that, while we were friends, there were small things that irritated us about each other. Celia didn‚Äôt like the fact I delayed doing dishes until I absolutely had to (I still do this, which is why a dishwasher was on my must-have list), and I wasn‚Äôt a huge fan of cat hair everywhere. Cats, yes. Cat hair, not so much. I shed enough. So, we ended up buying our own places, and we‚Äôre still friends.&lt;/p&gt;
    &lt;p&gt;We may not have become co-owners, but other people are making it work. In Toronto, six people, ranging from their late twenties to late thirties, made headlines by pooling their money to buy a $1.3-million house together downtown. There were no couples or family members among the six. Just friends. They named the house Clarens Commons and secured a co-ownership mortgage, which may have been the first in the country from a big bank.&lt;/p&gt;
    &lt;p&gt;I spoke with Valery Navarrete, one of the original owners and residents of Clarens Commons, about how they made it work. Prior to Clarens Commons, she had lived alone and with a partner, and now her goal was to live in a stable community rather than buy a house as a financial investment. ‚ÄúWe‚Äôre very much coming from the housing-as-a-human-right kind of perspective,‚Äù she said. ‚ÄúWe were just seeing friend after friend find a place, be happy to stay for a while, and then get renovicted.‚Äù&lt;/p&gt;
    &lt;p&gt;While the goal was to create a community, Navarrete said they did their due diligence around finances. ‚ÄúWhen you‚Äôre co-owners and you‚Äôre all on the lease, you are all individually responsible if, suddenly, your other co-owners don‚Äôt meet their commitment. So, we did look into that, but primarily for us, we were looking for the experience of living in community, which is one of really intermingling our lives together and relying on each other and having fun together.‚Äù&lt;/p&gt;
    &lt;p&gt;The co-owners not only hammered out the tangible financial responsibilities but they also talked about the intangibles. They created a spreadsheet where they each individually filled in their fiscal information and then talked together about it. ‚ÄúWe dubbed it financial nudity,‚Äù she said. ‚ÄúIt was equal parts ‚ÄòWhat‚Äôs your income and what are your expenditures?‚Äô and ‚ÄòHow did your family treat money? How did you grow up? What are your current views on money? Have they changed?‚Äô‚Äù She said that they did, later, have conversations about a renovation, budgets, and how people felt about the idea of borrowing more money.&lt;/p&gt;
    &lt;p&gt;While it was difficult to find an institution that had a formal mortgage offering for more than four people on it, Navarrete said that‚Äôs changing: ‚ÄúSince then, we‚Äôve realized that with a lot of banks, if you have a relationship, and you go and talk to them, they can make it happen.‚Äù&lt;/p&gt;
    &lt;p&gt;They ran into the same issue when looking for home insurance, and again, it was relationships that helped them get insurance. They had to pay a slightly higher premium than a couple likely would have, but they were satisfied securing insurance so they could get the mortgage. Still, this kind of trailblazing wasn‚Äôt all angst and stress. ‚ÄúThere were some really humorous moments as well,‚Äù she said. ‚ÄúWe were with the lawyers who handled the transfer, and we were filling out forms, and there wasn‚Äôt enough room for all of our names.‚Äù&lt;/p&gt;
    &lt;p&gt;While not all single people are lonely (there‚Äôs a big difference between solitude and loneliness), social isolation and loneliness are recognized by the World Health Organization as a priority public health problem and policy issue across all age groups, and thoughtful co-living could be one way to help address that. The joy of this kind of arrangement is that you can seek companionship when you want and retreat to your own space when you need to relax and recharge.&lt;/p&gt;
    &lt;p&gt;Navarrete moved out due to a change in her personal life circumstances, but she would absolutely do it again. She said that, once her child is an adult, she and her partner would move back into a house like Clarens Commons. ‚ÄúIn those five years of living on my own, I think I had done quite a good job of it. I had a lot of friendships; I had a busy work, volunteer, and social life. I could fill what would have been solo time with hangouts with friends, and I felt really full in terms of my sense of community and connection. But it was a lot of work to just coordinate all of that. I would have a Sunday to myself, and I would go do my groceries and get organized for the week ahead and cook some food, and I would go, ‚ÄòOh, you know what? It would be really nice to eat with someone, but who‚Äôs around?‚Äô That was my desire, with co-owning and co-living, to have more built-in connection.‚Äù&lt;/p&gt;
    &lt;p&gt;Adapted and excerpted, with permission, from The Singles Tax: No-Nonsense Financial Advice for Solo Earners by Ren√©e Sylvestre-Williams, published by ECW Press, 2026.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46606038</guid><pubDate>Tue, 13 Jan 2026 19:03:37 +0000</pubDate></item></channel></rss>