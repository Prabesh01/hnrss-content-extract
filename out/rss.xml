<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 11 Sep 2025 20:33:24 +0000</lastBuildDate><item><title>AirPods live translation blocked for EU users with EU Apple accounts</title><link>https://www.macrumors.com/2025/09/11/airpods-live-translation-eu-restricted/</link><description>&lt;doc fingerprint="b7cefbb889239bcc"&gt;
  &lt;main&gt;
    &lt;p&gt;Apple's new Live Translation feature for AirPods will be off-limits to millions of European users when it arrives next week, with strict EU regulations likely holding back its rollout.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Apple says on its feature availability webpage that "Apple Intelligence: Live Translation with AirPods" won't be available if both the user is physically in the EU and their Apple Account region is in the EU. Apple doesn't give a reason for the restriction, but legal and regulatory pressures seem the most plausible culprits.&lt;/p&gt;
    &lt;p&gt;In particular, the EU's Artificial Intelligence Act and the General Data Protection Regulation (GDPR) both impose strict requirements for how speech and translation services are offered. Regulators may want to study how Live Translation works, and how that impacts privacy, consent, data-flows, and user rights. Apple will also want to ensure its system fully complies with these rules before enabling the feature across EU accounts.&lt;/p&gt;
    &lt;p&gt;Apple's Live Translation feature, unveiled during its AirPods Pro 3 announcement, is also coming to older models including AirPods 4 with Active Noise Cancellation and AirPods Pro 2.&lt;/p&gt;
    &lt;p&gt;Live Translation enables hands-free communication by allowing users to speak naturally while wearing AirPods. For conversations with non-AirPods users, the iPhone can display live transcriptions horizontally, showing translations in the other person's preferred language.&lt;/p&gt;
    &lt;p&gt;The feature becomes more powerful when both conversation participants wear compatible AirPods with Live Translation enabled. Active Noise Cancellation automatically lowers the volume of the other speaker, helping users focus on translated audio while maintaining natural interaction flow.&lt;/p&gt;
    &lt;p&gt;The new Live Translation functionality requires AirPods updated with the latest firmware to pair with an Apple Intelligence-enabled iPhone running iOS 26 or later, so iPhone 15 Pro and newer models are supported. Apple has been beta testing firmware in concert with iOS 26 beta updates, and we expect the firmware to drop the same day that iOS 26 is officially released on September 15.&lt;/p&gt;
    &lt;p&gt;The feature supports real-time translation between English (UK and U.S.), French, German, Portuguese (Brazil), and Spanish. Apple plans to add Italian, Japanese, Korean, and Chinese (simplified) support later this year. When the EU/Apple Account restriction will be lifted remains unclear, but we've reached out to Apple to see if they're willing to provide more details.&lt;/p&gt;
    &lt;p&gt;Note: Due to the political or social nature of the discussion regarding this topic, the discussion thread is located in our Political News forum. All forum members and site visitors are welcome to read and follow the thread, but posting is limited to forum members with at least 100 posts.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45210428</guid></item><item><title>Behind the scenes of Bun Install</title><link>https://bun.com/blog/behind-the-scenes-of-bun-install</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45210850</guid></item><item><title>GrapheneOS and Forensic Extraction of Data (2024)</title><link>https://discuss.grapheneos.org/d/13107-grapheneos-and-forensic-extraction-of-data</link><description>&lt;doc fingerprint="b7661d918043d3e8"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Hi,&lt;lb/&gt; I am writing an article and I am sharing a draft with you. I will be glad if you share your thoughts and suggestions with me.&lt;/p&gt;
      &lt;p&gt;GrapheneOS is an Android-based, open source, privacy and security-focused mobile operating system for mobile phones. It is one of the most secure and privacy protecting operating systems (and yes, it does this task comparable and in some scenarios even better than iOS, but we will come to that later).&lt;/p&gt;
      &lt;p&gt;However, in the beginning of May, someone started an attack on GrapheneOS across social media platforms. The perpetrators were misrepresenting consent-based data extraction as GrapheneOS being compromised. Which would be funny if it wasn't so stupid. So let's see what happened and what actually consent-based data extraction means.&lt;/p&gt;
      &lt;head rend="h2"&gt;Digital forensics&lt;/head&gt;
      &lt;p&gt;Digital forensics is the process of uncovering and analysing electronic data in order to gather evidence for legal proceedings.&lt;lb/&gt; It involves the use of various techniques and tools to examine digital devices such as computers, smartphones, and storage&lt;lb/&gt; media to identify, preserve, analyse, and present digital evidence.&lt;/p&gt;
      &lt;p&gt;Digital evidence refers to any data or information that is stored or transmitted in digital form and can be used as evidence in a legal investigation or trial. Digital evidence is often used in criminal investigations to help establish a suspect's guilt or innocence, and can also be used in civil litigation, regulatory investigations, and other legal proceedings.&lt;/p&gt;
      &lt;p&gt;Unfortunately, sometimes digital forensics can be abused. It can be used against investigative journalists or political activists, it can be used for privacy violation, to intimidate or harass someone, to tamper with evidence, etc. That is why GrapheneOS developers are working hard to protect mobile phones from tampering and try to make data extraction without user's consent as hard as possible.&lt;/p&gt;
      &lt;head rend="h2"&gt;Cellebrite&lt;/head&gt;
      &lt;p&gt;Cellebrite is a leading Israeli company in the digital intelligence and digital forensics field. Their main digital forensics tool is called Universal Forensic Extraction Device (UFED), and is used to extract and analyze data from mobile devices for investigations.&lt;/p&gt;
      &lt;p&gt;They are selling their equipment to governments all around the world, and their tools are mostly used for legitimate purposes. Unfortunately they are selling their tools to authoritarian regimes too. Cellebrite's customer list has included authoritarian regimes in Belarus, Russia, Venezuela, and China, death squads in Bangladesh, military juntas in Myanmar and those seeking to abuse and oppress in Turkey, UAE, and elsewhere.&lt;/p&gt;
      &lt;head rend="h2"&gt;Data extraction&lt;/head&gt;
      &lt;p&gt;As mentioned, digital forensics tools first try to extract data from mobile device. This is the initial step in a digital forensics investigation.&lt;/p&gt;
      &lt;p&gt;The problem for digital forensics is, what if mobile phone is locked? How to extract data from a locked device?&lt;/p&gt;
      &lt;p&gt;There are several options, but basically three approaches exist.&lt;/p&gt;
      &lt;p&gt;First option is so called consent-based data extraction. This simply means that user voluntary unlocks their device (or provides PIN code or password), and forensic tool then extracts data from unlocked device.&lt;/p&gt;
      &lt;p&gt;Why would someone voluntary unlock their device? Well, maybe the owner of the device wants to cooperate with the investigators. Maybe he is a victim of criminal activity and wants to provide evidence against the perpetrators. Maybe he knows that data will prove his innocence. Or something else.&lt;/p&gt;
      &lt;p&gt;The question is of course, what if user do not want to provide PIN code or password or to unlock their phone?&lt;/p&gt;
      &lt;p&gt;In that case, there are two general approaches of digital forensic examination. The first option is to try to hack a mobile device so it gets unlocked (and then extract the data). And the other is to try to guess PIN code or password in order to unlock the device and the extract the data.&lt;/p&gt;
      &lt;p&gt;Companies like Cellebrite are offering various tools, that can try to hack into the locked mobile phone and then extract the data. And their tools also offer a possibility to "guess" PIN code or password, in order to unlock the device.&lt;/p&gt;
      &lt;p&gt;But first we need to understand that from the digital forensics point of view, mobile phone can basically be in two states.&lt;/p&gt;
      &lt;p&gt;First is called BFU (Before First Unlock), and it simply means a device that has been turned off or rebooted and never subsequently unlocked by entering the correct PIN or passcode. The second is called AFU (After First Unlock), and it means that device has been unlocked after reboot (meaning that encryption keys are stored in internal memory of the device).&lt;/p&gt;
      &lt;p&gt;Technically, there is important difference between the two. BFU devices (that hasn't been unlocked with a PIN or passcode) mostly contains encrypted data. Since the first unlock of the device also decrypts the device (technically: unlocks the decryption key, so device can access the data stored in internal storage), most data on the device in that case is inaccessible for forensic analysis. Technically that means that data are encrypted at rest.&lt;/p&gt;
      &lt;p&gt;AFU devices (that has been unlocked with the correct passcode after powering on, or restarting) contains decryption key in it's internal memory, and that key is used to decrypt files in internal storage. In that case most data on the device becomes accessible for forensic analysis, because data in that state are decrypted for normal use. However, in that case screen lock could still been activated, meaning, that forensic investigator needs to unlock the screen first, and then can extract the data.&lt;/p&gt;
      &lt;p&gt;From the user's point of view this simply means that a locked device in BFU state presents significant challenges for data extraction, while an unlocked device (in AFU state) offers greater access to extract the data.&lt;/p&gt;
      &lt;head rend="h2"&gt;Data extraction approaches&lt;/head&gt;
      &lt;p&gt;As already mentioned, AFU devices are easily approached for extraction. General approach here is to hack into the locked mobile phone (by exploiting some software vulnerability) in order to disable or bypass a screen lock, and then extract the data.&lt;/p&gt;
      &lt;p&gt;In case of BFU devices, where data in internal storage are still encrypted, the forensic examinator needs to "guess" PIN code or password, in order to unlock the device. Usually this is done by so called brute forcing. This simply means that a forensic tool tries to guess the correct PIN or password by going through all possible combination, until the correct one is found.&lt;/p&gt;
      &lt;head rend="h2"&gt;Cellebrite's capabilities&lt;/head&gt;
      &lt;p&gt;In April 2024 Cellebrite published a list of their capabilities provided to customers. The list shows that they can successfully exploit every non-GrapheneOS Android device brand. They can do this for devices in AFU and BFU states. This means, that Cellebrite's tools can unlock (and then extract data) every Android device on the market.&lt;/p&gt;
      &lt;p&gt;https://grapheneos.social/system/media_attachments/files/112/462/757/183/372/025/original/992254912340eeaf.png&lt;/p&gt;
      &lt;p&gt;https://grapheneos.social/system/media_attachments/files/112/462/757/581/168/086/original/a2c40bcc6a083183.png&lt;/p&gt;
      &lt;p&gt;According to Cellebrite's documents, they have similar capabilities for iOS devices too. Not all, but for many of them. In fact, it is only the latest device generation and OS versions which are not fully supported yet (however, it is fair to mention, that most iPhone users are getting iOS updates automatically). Will they be able to develop exploits for the later iOS devices too? We do not know that, but we know, that NSO (an Israeli company developing Pegasus spyware) already did that, right after iOS 17 has been released in September 2023.&lt;/p&gt;
      &lt;p&gt;https://grapheneos.social/system/media_attachments/files/112/462/760/076/651/069/original/abb6bfdb2d3cbc6a.png&lt;/p&gt;
      &lt;p&gt;https://grapheneos.social/system/media_attachments/files/112/462/760/480/507/923/original/eaff65050cbc6d1c.png&lt;/p&gt;
      &lt;p&gt;Which is interesting, because Apple is advertising iOS as "the world’s most personal and secure mobile operating system".&lt;/p&gt;
      &lt;p&gt;What about GrapheneOS? According to the documents, Cellebrite admits they can not hack GrapheneOS if users had installed updates since late 2022. This is important, because GrapheneOS is releasing security updates and improvements quite frequently - sometimes even several times a month. And GrapheneOS is designed in such a way, that updates are automatically enabled, and if users want to disable them, they need to do that manually. This means that very likely vast majority of users have GrapheneOS updated to the last version automatically and their phones can not be hacked by Cellebrite's tools.&lt;/p&gt;
      &lt;p&gt;https://grapheneos.social/system/media_attachments/files/112/462/757/581/168/086/original/a2c40bcc6a083183.png&lt;/p&gt;
      &lt;p&gt;On the other side, Cellebrite claims that they can do so called consent-based full filesystem extraction with iOS, Android and GrapheneOS. As already mentioned, this simply means they can extract data from the device once the user voluntary unlocks their device. For GrapheneOS that means, that when they get unlocked phone, they just enable developer options and use standard ADB tool (ADB lets you communicate with Android devices through a computer) to extract the data.&lt;/p&gt;
      &lt;p&gt;So, according to Cellebrite documents, they can not unlock fully patched GrapheneOS phone, unless user voluntary unlocks the phone. In fact, analysis of Cellebrite's documents shows, that they even can not brute force a random 6-digit PIN on Pixel 6 and later phones (which are the phones supported by GrapheneOS). Cellebrite's documents reveal, that Pixel 6 phones and later with GrapheneOS (and the latest iPhones also), are the only devices where a random 6 digit PIN can not be brute forced.&lt;/p&gt;
      &lt;head rend="h2"&gt;The attack on GrapheneOS on social media?&lt;/head&gt;
      &lt;p&gt;As we mentioned at the beginning, in the beginning of May, someone started an attack on GrapheneOS across social media platforms. The perpetrators claimed that GrapheneOS has been compromised, and the "proof" has been, that data extraction from GrapheneOS is successful when it is consent-based.&lt;/p&gt;
      &lt;p&gt;It is unclear who has been behind this social media attack, but in December 2020 something similar happened. At that time, various media (including BBC) reported, that Cellebrite claimed to have cracked Signal's encryption. Signal is a free, encrypted messaging application, which is widely considered one of the most secure messaging apps due to its strong encryption and focus on privacy.&lt;/p&gt;
      &lt;p&gt;However, at that time it turned out, that the claims were completely false - Cellebrite has been able to extract Signal messages only if user unlocked the phone and Signal app and hand it to the forensic examinator. Which is by definition consent-based extraction and does not really require some specific technical excellence from the forensic acquisition tool.&lt;/p&gt;
      &lt;head rend="h2"&gt;Defense against forensic hacking tools in GrapheneOS&lt;/head&gt;
      &lt;p&gt;Now let's dive into GrapheneOS security countermeasures against described attacks.&lt;/p&gt;
      &lt;head rend="h3"&gt;Protection against hacking into the phone&lt;/head&gt;
      &lt;p&gt;GrapheneOS has implemented many security features. You can get a glimpse of them from my presentation on GrapheneOS security.&lt;/p&gt;
      &lt;p&gt;However, regarding various forensic tools, it is important to mention, that GrapheneOS has a special feature that disallows new USB connections in AFU mode (After First Unlock) after the device is locked, and fully disables USB data at a hardware level once there aren't active USB connections. This means that if an attacker would connect GrapheneOS device to the computer through USB, GrapheneOS device will not allow any connection at all.&lt;/p&gt;
      &lt;p&gt;Users can set it to do this in BFU (Before First Unlock) mode or even when the phone is fully unlocked. And users with a high threat model can even fully disable USB, including USB-PD/charging, while the OS is booted. So they can decide to only allow charging while powered off or booted into the fastboot/fastbootd/recovery/charging modes. This is the feature no other phone has, and can be completely customized by the user.&lt;/p&gt;
      &lt;p&gt;GrapheneOS is constantly improving security, and since beginning of 2024 they massively improved security against various possible exploits. In April 2024 they also helped to implement several firmware improvements for Pixel phones.&lt;/p&gt;
      &lt;head rend="h3"&gt;Protection against brute force attacks&lt;/head&gt;
      &lt;p&gt;As we mentioned, in case of BFU devices, where data in internal storage are still encrypted, the forensic examinator needs to "guess" PIN code or password, in order to unlock the device. This is done by so called brute force attack (guessing every possible combination of PIN code or password).&lt;/p&gt;
      &lt;p&gt;However, Pixel 6 and later phones contain a dedicated security chip Titan M2, also called hardware security module, which is storing the decryption keys. This chip will unlock the internal phone storage only if user will enter the correct PIN or password.&lt;/p&gt;
      &lt;p&gt;But here is the catch. If an attacker try to perform brute force attack (i. e. try to go through all possible PIN/password combinations), the chip will limit the number of attempts. After 5 failed attempts, chip will add 30 second delay before next guessing attempt is allowed. Then delay gets increased (after 30 failed attempts the delay doubles after every 10 attempts), and after 140 failed attempts, GrapheneOS and its secure element will limit brute force to 1 attempt per day. This is called secure element throttling.&lt;/p&gt;
      &lt;p&gt;So if an attacker would like to test all different combinations to guess a 6-digit PIN, there are one million possible combinations, so brute forcing would take a long, long time. Unless, the attacker is extremely lucky and guesses the correct PIN at the few first attempts.&lt;/p&gt;
      &lt;p&gt;Of course, the question is, is it possible to somehow hack this secure element or unlock the limited number of attempts? The answer is - very unlikely. Why? Because this secure element has been developed specifically to protect against those types of attacks. And it has passed the highest hardware vulnerability assessment (AVA_VAN.5) by an independent and accredited evaluation lab.&lt;/p&gt;
      &lt;p&gt;In fact, GrapheneOS is so successful in this area, because it is doing far more hardening than iOS against these attacks. iPhones also have security element, but the companies developing attacks, had successfully bypassed secure element throttling from Apple for years (and are doing the same with Samsung and Qualcomm implementation of secure element). These companies were successfully bypassing the secure element throttling on 2nd through 5th generation Pixels. Pixel 2 used NXP secure element and Pixel 3 moved to a custom ARM secure element. But Pixel 6 and later phones are using a custom RISC-V secure element. It seems that moving away from the standard ARM Cortex secure element cores was the correct decision, because it blocked these companies from successfully exploiting the secure element for several years.&lt;/p&gt;
      &lt;head rend="h3"&gt;Auto reboot feature&lt;/head&gt;
      &lt;p&gt;GrapheneOS also has an auto-reboot feature, meaning that after some time, phone gets automatically rebooted. Default auto reboot time is 18 hours (if phone is not unlocked in that time, it will reboot), but user can set it to a different interval, even as low as 10 minutes.&lt;/p&gt;
      &lt;p&gt;This technically means that the data after this period are put back to rest, or, to put it differently, phone gets from AFU to BFU state. And as we already explained, a locked device in BFU state presents significant challenges for data extraction, much more than unlocked device in AFU state.&lt;/p&gt;
      &lt;p&gt;After reboot, it is almost impossible to extract decrypted data from the phone. So the focus of GrapheneOS's team is defending against exploitation long enough for auto-reboot to work, and after that your data are even safer than before. That means that if an attacker develops a successful exploit, their window of opportunity to use it to get data from user profiles is until next reboot from when the phone was locked.&lt;/p&gt;
      &lt;head rend="h3"&gt;Conclusion&lt;/head&gt;
      &lt;p&gt;In the last year, GrapheneOS started to put much more effort into defending your phone against these attacks. Users who need their data secure, should definitely use a strong passphrase. To make that more convenient, GrapheneOS is developing 2-factor fingerprint unlock feature, which will allow people to combine a strong passphrase with convenient fingerprint and PIN unlock. They are also planning to offer an UI for generating random passphrases automatically. This will vastly improve security and make access to the user's data on a phone much more difficult.&lt;/p&gt;
      &lt;p&gt;The actors that want to hack into GrapheneOS are rightfully desperate. So it is no surprise, that they try to play dirty, by spreading misinformation. But knowledge is power and misinformation could be successfully fought with facts.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45210910</guid></item><item><title>An engineering history of the Manhattan Project</title><link>https://www.construction-physics.com/p/an-engineering-history-of-the-manhattan</link><description>&lt;doc fingerprint="fde891156d158f6d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;An Engineering History of the Manhattan Project&lt;/head&gt;
    &lt;p&gt;The Manhattan Project, the US program to build an atomic bomb during WWII, is one of the most famous and widely known major government projects: a survey in 1999 ranked the dropping of the atomic bomb as the top news story of the 20th century. Virtually everyone knows that the project built the bombs that were dropped on Hiroshima and Nagasaki. And most of us probably know that the bomb was built by some of the world’s best physicists, working under Robert Oppenheimer at Los Alamos in New Mexico. But the Manhattan Project was far more than just a science project: building the bombs required an enormous industrial effort of unprecedented scale and complexity. Enormous factory complexes were built using hundreds of millions of dollars worth of never-before-constructed equipment. Scores of new machines, analytical techniques, and methods of working with completely novel substances had to be invented. Materials which had never been produced at all, or only produced in tiny amounts, suddenly had to be manufactured in vast quantities.&lt;/p&gt;
    &lt;p&gt;This massive effort was required in part because of the enormous difficulty in producing fissile material, and in part because of the enormous uncertainty facing the project: it wasn’t known what the best method for manufacturing the fissile material needed for the bomb would be, what the design of the bomb should be, or whether a workable bomb could even be built. Developing the bomb required resolving this uncertainty, and the project needed to rapidly push forward knowledge and capabilities in many fields: not merely in the realm of nuclear chain reactions and atomic physics, but also in areas like precision explosives, metallurgy, welding, chemical separation, and electronics.&lt;/p&gt;
    &lt;p&gt;Because of the exigencies of war, this work needed to be done extremely rapidly. There wasn’t time to investigate promising approaches sequentially, or wait for more information before picking a particular course. Thus, multiple possible routes to the bomb — different fuels (and different fuel production techniques), different bomb designs, different components like triggers and tampers — were pursued simultaneously. Major commitments, like factories that cost hundreds of millions of dollars, were made before it was known whether they would even be useful. Design work began on the bombs when the nuclear fuel they would use hadn’t been produced in more than microscopic amounts.&lt;/p&gt;
    &lt;p&gt;Normally when trying to create a new technology, funding constraints and the need for economic returns determine how much time and effort can be spent on development. Efforts to create some new technology will often be small-scale until the surrounding conditions are right — until knowledge has caught up, or the necessary supporting infrastructure exists, or the input materials are cheap enough — and risk can be minimized. But with the Manhattan Project, these constraints didn’t exist. Funding was virtually unlimited in service of ending the war sooner, and the biggest perceived risk was that Germany would beat the US to the bomb. As a result, an extremely robust development effort could be justified, which thoroughly explored virtually every promising path to an atomic weapon (no matter how expensive or uncertain).&lt;/p&gt;
    &lt;head rend="h4"&gt;Beginnings of the project&lt;/head&gt;
    &lt;p&gt;The Manhattan Project began in June of 1942, when Colonel James Marshall of the Army Corps of Engineers was directed to create a new engineering district to lead the army’s efforts to develop an atomic weapon. Shortly after, Colonel Leslie Groves (who would soon be promoted to brigadier general) was selected to lead the project. At the time, the official name of the project was “Laboratory for the Development of Substitute Materials” (DSM for short), but Groves felt that this name would attract curiosity, and so a new name was selected based on the location of Marshall’s New York office: the Manhattan Engineer District.&lt;/p&gt;
    &lt;p&gt;By the time the Manhattan Project officially formed, the US was already at work developing an atomic bomb. Following the discovery of fission in 1938 by Otto Hahn and Fritz Strassmann, physicists began to speculate that a nuclear chain reaction might be possible, and that such a reaction could be used to build a bomb of unprecedented magnitude. In August the following year, Albert Einstein and physicist Leo Szilard sent a letter to president Roosevelt, warning him that a nuclear chain reaction might be used to build an extremely powerful bomb, and that the US should research atomic energy. Two months later, Roosevelt ordered the creation of an advisory committee on uranium, and by early 1940 US researchers (most notably Enrico Fermi) were working to create a sustained nuclear chain reaction.&lt;/p&gt;
    &lt;p&gt;In July of 1941, a report from the British MAUD Committee concluded that it was likely feasible to build an atomic bomb. It reached the US, and in October Roosevelt authorized expediting atomic bomb work. Bomb efforts accelerated following Japan’s attack on Pearl Harbor in December of 1941, and in February of 1942 the Metallurgical Laboratory was formed at the University of Chicago to study nuclear chain reactions and the chemistry of newly-created element plutonium. There, a team working under Enrico Fermi continued their work to create nuclear chain reactions, ultimately resulting in Chicago Pile-1, the world’s first self-sustaining nuclear reaction, in December of that year.&lt;/p&gt;
    &lt;head rend="h4"&gt;The path to the bomb&lt;/head&gt;
    &lt;p&gt;When the Manhattan Engineering District was formed in 1942, there was still a great deal of uncertainty surrounding the construction of an atomic bomb. Based on what was known at the time, it was believed that a bomb was probably feasible, and that due to the risks of Germany developing one it should be pursued. But it was far from clear what the surest path to success was.&lt;/p&gt;
    &lt;p&gt;The first major challenge came in producing sufficient fissile material to build a bomb. Fissile material splits and releases neutrons when struck by slow, “thermal” neutrons, making a nuclear chain reaction possible. At the time there were two major candidate materials: a rare isotope of uranium known as uranium-235 (U235), and plutonium, an element first synthesized by Glenn Seaborg in late 1940.1&lt;/p&gt;
    &lt;p&gt;Using either would be very challenging. U235 makes up less than 1% of naturally-occurring uranium, and using it as a bomb material required separating it from the far more common U238. But the two isotopes were only distinguished by a tiny difference in their weights (U235 weighs about 1.3% less than U238). Some sort of filtering mechanism was needed that could act on this difference and create concentrations of U235 high enough for a bomb (a process known as enrichment).&lt;/p&gt;
    &lt;p&gt;There were several potential methods considered for separating U235:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;In the electromagnetic method, a beam of charged uranium tetrachloride particles would be fired through a magnetic field, which would alter their paths. Because of the difference in weight, the paths of U235 and U238 would be slightly different, and the U235 could be gathered at an appropriately placed collector.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In the gaseous diffusion method, gaseous uranium hexafluoride would diffuse through a barrier with microscopic pores. The lighter U235 would diffuse more readily through the barrier due to Graham’s Law.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In the liquid thermal diffusion method, a thermal gradient created in a uranium solution would cause a slight migration of heavier U238 to the cold side and the lighter U235 to the warm side.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In the centrifuge method, uranium spun in a high-speed centrifuge would cause the heavier U238 to concentrate on the outer edge.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of these methods, the electromagnetic method was the most developed, thanks to the efforts of Ernest Lawrence at the University of California, but it wasn’t clear if any of them (either alone or in combination) could actually produce U235 at the scale and speed needed. And no matter the method selected, an enormous industrial facility would be required: in 1939 Danish physicist Niels Bohr insisted that an atomic bomb could never be built “unless you turned the US into one huge factory.”&lt;/p&gt;
    &lt;p&gt;The main alternative to U235 was plutonium. As with U235, the main challenge would be collecting enough of it to build a bomb. Plutonium only occurs in trace amounts in nature (1 atom per 100 billion in uranium ore): collecting enough to build a bomb requires synthesizing it. Plutonium could be produced in a nuclear reactor (then called a “pile,” as it was essentially chunks of uranium piled high enough to create a self-sustaining nuclear reaction), but only in microscopic amounts. Producing a pound of plutonium required around 4000 pounds of uranium fuel, and producing enough for a bomb would require an enormous industrial facility, as with U235. Exacerbating this difficulty was the fact that while U235 and U238 could be handled comparatively easily, plutonium and other nuclear reactor byproducts were highly radioactive, requiring special handling.&lt;/p&gt;
    &lt;p&gt;Once enough fissile material had been collected, it then needed to be turned into a bomb. When enough fissile material is brought together in a small enough volume (the so-called critical mass), it can start a nuclear chain reaction, releasing enormous amounts of energy as more and more fissions were triggered. Because a chain reaction in critical mass could be started by spontaneous fission (fissile elements randomly splitting and releasing neutrons) or by cosmic rays, a bomb would have to start with a sub-critical mass of fissile material, turning it into a critical mass at detonation.&lt;/p&gt;
    &lt;p&gt;The most straightforward way to do this, it was thought, was to use a gun that would fire a sub-critical “bullet” into another sub-critical “target,” the combination of which would exceed the critical mass. But there were other mechanisms considered, including using an explosion to compress a sphere of fissile material (the so-called implosion method), as well as “autocatalytic” mechanisms in which “the chain reaction itself, as it proceeded, increased the neutron number for a time.”&lt;/p&gt;
    &lt;p&gt;In mid-1942, a gun-based plutonium bomb was generally considered most promising, but due to the lack of information and the great urgency, many promising paths were investigated simultaneously. Early on in the project, when resolving a debate about pile cooling systems, Leslie Groves stated that “The War Department considers this project important. There is no objection to a wrong decision with quick results. If there is a choice between two methods, one of which is good and the other looks promising, then build both.”&lt;/p&gt;
    &lt;p&gt;Perhaps no phrase better summarizes the philosophy of Manhattan Project than “build both.” It was ultimately decided to pursue both U235 and plutonium-based bombs. To produce the necessary U235, a production facility would be built near Knoxville, Tennessee, employing both electromagnetic separation and gaseous diffusion (and, eventually, liquid thermal diffusion). This plant, initially referred to as the Clinton Engineer Works, would later be named Oak Ridge. To produce the plutonium, another facility, the Hanford Engineer Works, would be built in southeast Washington. And while these plants were being built and producing fissile material, the design of the bombs themselves would be done at Los Alamos, New Mexico.&lt;/p&gt;
    &lt;head rend="h4"&gt;Oak Ridge and Uranium 235&lt;/head&gt;
    &lt;p&gt;The acquisition of the site for Oak Ridge was authorized in September of 1942, and construction of the electromagnetic separation plant began a few months later in February of 1943 by the firm Stone and Webster. To produce U235, the plant would use modified versions of Ernest Lawrence’s Nobel prize-winning cyclotron particle accelerators. Lawrence had been working on the devices, which he referred to as “calutrons” (after the University of California) since the spring of 1942, and while he confident that calutrons could be used for large-scale production of U235, “he stood almost alone in his optimism”:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The method called for a large number of extremely complicated, and as yet undesigned and undeveloped devices involving high vacuums, high voltages and intense magnetic fields. As a large-scale method of separating Uranium-235, it seemed almost impossible. Dr. George T. Felbeck, who was in charge of the gaseous diffusion process for Union Carbide, once said it was like trying to find needles in a haystack while wearing boxing gloves. - Now It Can Be Told&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;(According to Richard Rhodes, the total volume of high-vacuum required by the calutrons would eventually exceed the amount of vacuum produced everywhere else on earth at the time.)&lt;/p&gt;
    &lt;p&gt;The calutrons, arranged around a series of several “racetracks” with several dozen collection tanks attached, were divided into two stages. Partly enriched material from “alpha” racetracks would be fed into “beta” racetracks to be further enriched, eventually (it was hoped) producing 90% enriched U235. Each calutron only produced a tiny amount of U235 — Lawrence estimated that 2000 calutrons could produce 100 grams of enriched U235 per day — so a huge number of them were needed: the alpha and beta calutron buildings eventually occupied an area greater than 20 football fields, and the entire electromagnetic separation facility grew to 268 buildings, requiring 20,000 workers to build:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;…The calutron structures of steel and brick and tile, chemistry laboratories, a distilled water plant, sewage treatment plants, pump houses, a shop, a service station, warehouses, cafeterias, gatehouses, change houses and locker rooms, a paymaster’s office, a foundry, a generator building, eight electric substations, nineteen water-cooling towers - for an output measured in the best of times in grams per day. - The Making of the Atomic Bomb&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Building this enormous facility was rife with challenges. Copper, traditionally used for winding electromagnets, was in short supply due to the war, and so was substituted with silver (also a good conductor) borrowed from the US Treasury. Altogether 13,540 tons, worth $300 million ($6 billion in 2025 dollars) was borrowed, 99.964% of which was eventually returned. Because construction started so early, constant changes to already manufactured and installed equipment were required, a process that continued “long after the first major units of the plant began production operations.” Little of what was required to build the plant was off the shelf or standard:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Discouragingly few items were commercially available. Tanks, magnets, vacuum pumps, cubicles, and most of the chemical equipment, for example, were either completely new in design or so much larger or so much greater in capacity that nothing of the kind previously had been manufactured. Many less obvious items also carried performance specifications that far exceeded anything ever attempted on a commercial scale. For instance, the calutrons required electrical cable that could carry a high-voltage load continuously. The only commercial product that came near meeting this specification was the heaviest X-ray cable, and it was designed to operate intermittently. Even when the commercial equipment could be used, suppliers often had to add to their productive capacity or build entire new plants to furnish the items required in the enormous quantities they were needed. Thus, in the first equipping of the racetracks some eighty-five thousand vacuum tubes were required. In the case of one type of tube, procurement officials ordered in advance the entire national output for 1943 as well as that from a plant still under construction. In the early months of plant operation, when tubes burned out faster than predicted, some feared the racetracks might prove inoperable simply through inability to maintain the tube supply.&lt;/p&gt;
      &lt;p&gt;New methods had to be developed for machining and shaping the graphite in those parts of the calutron subject to intense heat. No standard material would endure the high potentials, mechanical strain, and temperature changes to which bushings in the high-voltage elements in the sources were continuously subjected. After months of investigation, Stone and Webster found an insulator made of zirconium oxide, a new and still very expensive substance. Similarly, use of large quantities of liquid nitrogen to condense moisture created a demand for a substance hitherto not produced on a commercial scale anywhere in the country. - Manhattan: The Army and the Bomb&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;These difficulties didn’t stop after construction was completed. When the first calutrons were turned on for testing in November of 1943, the extremely powerful magnets caused the equipment to “walk” by several inches: this was eventually resolved by tying them down with heavy steel straps. Testing also showed intermittent electrical shorts and unexpectedly high variation in the strength of the magnetic fields, problems that was eventually traced to dirt and rust within the electromagnets bridging the gap between the closely spaced silver windings. To fix this required rebuilding and redesigning the magnets, delaying production by a month.&lt;/p&gt;
    &lt;p&gt;As production came online in early 1944, the electromagnetic separation plant continued to deal with numerous problems: equipment and mechanical failures, electrical short circuits, vacuum leaks and various breakdowns. The operation “skirted the edge of chaos for months.” As late as August of 1944, the electromagnetic plant had only produced a small fraction of the expected U235, and it was unclear if enough would be produced to build a wartime bomb. But eventually these problems were ironed out, and by September of 1945 the alpha and beta calutrons had produced 88 kilograms of 84.5% enriched U235.&lt;/p&gt;
    &lt;p&gt;In addition to the electromagnetic separation plant, Oak Ridge was also the site of the gaseous diffusion plant, which began construction in May 1943. As with the electromagnetic process, the gaseous diffusion process only produced U235 in tiny amounts, and an enormous facility was needed to manufacture it in sufficient quantities to build a bomb. Gaseous diffusion worked on the principle that lighter U235 would be more likely to diffuse through a porous barrier than heavier U238, but the difference in diffusion rates was miniscule. A single gaseous diffusion step would only increase the fraction of U235 by a factor of 1.0043: to produce 90% enriched U235, the initial design of the plant called for a series of 4,600 gaseous diffusion stages. Upon completion, the gaseous diffusion plant was one of the largest buildings in the world.&lt;/p&gt;
    &lt;p&gt;As with the electromagnetic plant, construction began before the design of the diffusion process had been finished, and it was unclear if the plant would work at scale. The greatest challenge was finding an appropriate barrier for the gas to diffuse through. The barrier needed to have numerous microscopic pores, be robust enough to withstand exposure to extremely corrosive uranium hexafluoride gas, and be mass-producible. Researchers had experimented with “a great many materials” between 1941 and 1942, but none were suitable. The only common material sufficiently corrosion-resistant was nickel, but no form of nickel seemed to do the trick. An electro-deposited nickel mesh, invented by Edward Norris (a “self-educated Anglo-American interior decorator”) and Edward Adler appeared most promising, but it was so brittle that manufacturing it was incredibly difficult. A modified version of the Norris-Adler barrier, produced by a team from Kellex, Bell Labs, and Bakelite, appeared to work even better, though it too had problems. Work proceeded to further develop both barriers simultaneously, but progress was slow and by the end of 1943 “morale had plummeted”. It wasn’t until early 1944 that satisfactory barriers were being produced in sufficient quantities.&lt;/p&gt;
    &lt;p&gt;While the diffusion barrier was the biggest challenge with the gaseous diffusion process, it wasn’t the only one. The plant required a level of vacuum-tightness that had previously only been achieved in labs, demanding the development of novel methods of pipe welding and leak detection. Upon completion, it took 406 workers eight months to test the plant for leaks. More than 130,000 measuring instruments, many of them novel, were installed in the plant. It was likely the greatest number of instruments ever used in any plant in the world till that date, and they required months of testing and calibration. The plant had to be incredibly reliable, as “even slight variations in such factors as temperature and pressure could produce adverse effects.” Initially, it was believed that a slight power interruption could bring the diffusion plant offline for months, so a dedicated power plant was built specifically at Oak Ridge for the process. And because any contaminants could prove disastrous, the cleanliness standards for the plant approached surgical:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;…Construction workers had to cleanse all pipes, valves, pumps, converters, and other items of equipment thoroughly before installation. Workmen in a special unit performed this vast operation in the large conditioning building, using equipment for solvent degreasing, alkaline cleaning, acid pickling, scratch brushing, surface passivation, and a variety of other procedures. When they finished, they sealed all openings to interior surfaces and kept them sealed until installation teams put the equipment into place.&lt;/p&gt;
      &lt;p&gt;To make certain no dust or other foreign matter polluted the system during installation, J. A. Jones instituted a rigid schedule of surgical cleanliness in installation areas. Isolating these areas with temporary partitions, the workers installed pressure ventilation, using filtered air. Then they cleaned the areas thoroughly, and inspectors carefully checked all personnel and material that entered them. Maintenance crews with mops and vacuum cleaners continued to remove any foreign substances that seeped in. When trucks had to enter, workers hosed them down at the entrances.&lt;/p&gt;
      &lt;p&gt;Workers wore special clothes and lintless gloves. Because certain work on equipment to be used in plant installations could not be done in the dirt-free areas, such as welding pipes and other small jobs, J.A. Jones installed special inflatable canvas balloons and the work was done inside them. The cleanliness control measures required many additional guards, inspectors, and supervisors… - Manhattan: The Army and the Bomb&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This level of cleanliness extended to the design of the equipment itself:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[Uranium hexafluoride] attacked organic materials ferociously: not a speck of grease could be allowed to ooze into the gas stream anywhere along the miles and miles of pipes and pumps and barriers. Pump seals therefore had to be devised that were both gastight and greaseless, a puzzle no one had ever solved before that required the development of new kinds of plastics. (The seal material that eventually served at Oak Ridge came into its own after the war under the brand name Teflon.) A single pinhole leak anywhere in the miles of pipes would confound the entire system; Alfred O. Nier developed portable mass spectrometers to serve as subtle leak detectors. Since pipes of solid nickel would exhaust the entire U.S. production of that valuable resource, Groves found a company willing to nickel-plate all the pipe interiors, a difficult new process accomplished by filling the pipes themselves with plating solution and rotating them as the plating current did its work. - The Making of the Atomic Bomb&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Because of these difficulties, the gaseous diffusion plant didn’t begin operating until February of 1945. While the plant was originally planned to produce uranium to the roughly 90% U235 enrichment needed to build a bomb, it was discovered that beyond 36.6% enrichment, different types of barrier and different types of pumps being designed would be required. In the latter half of 1943 the plant was thus redesigned to produce 36.6% enriched uranium (using 2892 diffusion stages) that would then be fed into the electromagnetic process. By the end of the war, the gaseous diffusion plant had “contributed substantially to the manufacture of the fissionable material used in the fabrication of atomic weapons”, and would become the primary method of producing enriched uranium in the early post-war years.&lt;/p&gt;
    &lt;p&gt;In addition to the electromagnetic and gaseous diffusion separation processes, a plant to separate U235 by liquid thermal diffusion was also built. Thermal diffusion had been considered by the Manhattan Project early on, but it appeared insufficiently promising and there were no initial plans to build a thermal diffusion plant. However, work on the process continued by the Navy as a method of producing fissile material for nuclear reactors. By late 1942, it appeared much more promising as a feasible separation method, and Leslie Groves recommended that it continue to be developed by the Navy. Eventually, in June of 1944 it was decided to build a thermal diffusion plant at Oak Ridge to produce partially-enriched uranium as an input to the electromagnetic separation process, as doing so would speed up overall U235 production. The plant came online in late 1944.&lt;/p&gt;
    &lt;p&gt;Like the other separation plants, there were struggles getting the plant built and operational. Early on there were numerous steam leaks and other equipment failures, and the “results scarcely seemed to justify the risks.” But eventually the plant “served its wartime purpose,” providing enough slightly enriched U235 to the electromagnetic separation plant and (later) the gaseous diffusion plant to build a uranium bomb by July 1945. After the war, however, it was found that the thermal diffusion plant was less economical than gaseous diffusion in producing enriched uranium, and it was shut down in September 1945, less than a year after starting operation.&lt;/p&gt;
    &lt;head rend="h4"&gt;Hanford and plutonium&lt;/head&gt;
    &lt;p&gt;As with U235, when plutonium was being considered as a possible fissile material for an atomic bomb there was a great deal of uncertainty around its large-scale production. When plans began to be formulated for plutonium manufacture in late 1942, the element had only been produced in microscopic amounts in cyclotrons — as late as December 1943, only two milligrams of plutonium had been manufactured. Producing the pounds of plutonium needed for a bomb would require a self-sustaining nuclear chain reaction in a nuclear reactor, which would create plutonium as a fission byproduct. The first such chain reaction was created in Chicago Pile-1 in December 1942, shortly after Du Pont had been (reluctantly) brought on as the contractor to build and operate a plutonium production plant.&lt;/p&gt;
    &lt;p&gt;As with uranium separation, there were a variety of potential ways to build a plutonium-producing nuclear reactor. Every design considered used uranium as a fuel, but there were a variety of options for cooling (water, helium, diphenyl, bismuth) and for moderators to slow down the neutrons (heavy water, graphite).2 The initial reactor design used helium-cooling with a graphite moderator: helium wouldn’t absorb neutrons, and was an inert gas that wouldn’t corrode any of the reactor materials. But it was later shown that the neutron multiplication factor in a reactor, k, was high enough that coolants which absorbed more neutrons (such as water) could be made to work. Because a water-cooled reactor appeared far simpler to build, the design was changed to use water cooling. Du Pont, worried that pursuing a single reactor design was too risky, also continued to develop other designs, chiefly one moderated and cooled by heavy water, and several heavy water plants were built around the country for these purposes.&lt;/p&gt;
    &lt;p&gt;Plutonium production began with a smaller-scale, air-cooled test reactor, named X-10, built at Oak Ridge between February 1943 and January 1944. Full-scale production would take place at a remote facility built in southeastern Washington known as the Hanford Engineer Works. The first plutonium production reactor, B Reactor, began construction at Hanford in August of 1943, and first achieved criticality in September of the following year.&lt;/p&gt;
    &lt;p&gt;Like with uranium, plutonium production demanded a massive industrial facility, and Hanford was the biggest facility that Du Pont had ever constructed. An oral history of the Hanford site notes that “the building effort at Hanford from 1943 to 1945 can only be measured in superlatives”:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Consider the following: Project building crews used 1800 vehicles, including sedans, pick-up trucks, jeeps, and ambulances; 900 buses that had a total seating capacity of over 30,000; 1900 dump trucks and flat bed trucks; 240 tractor trailers; 44 railway locomotives and 460 railway cars; 5 locomotive cranes and 4 stiff leg derricks. The various construction teams built 386 miles of highways, 158 miles of track, poured 780,000 cubic yards of concrete, and erected housing for 5,000 women and 24,000 men. Excavation crews moved 25 million cubic yards of earth in the process. The overall cost was $350 million. - Working on the Bomb&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The three production reactors built at Hanford (B, D, and F Reactors) were far more powerful than anything that had come before. Fermi’s Chicago Pile-1 produced a maximum of 0.2 kilowatts of power. The X-10 reactor at Oak Ridge produced just 500 kilowatts when it was first turned on (though output would later be raised to 4,000 kilowatts). The Hanford reactors were designed to produce 250,000 kilowatts.&lt;/p&gt;
    &lt;p&gt;But the largest structures at Hanford were the separation facilities to extract plutonium out of the soup of radioactive waste products the reactors generated: these were three 800 foot long, 8-story tall buildings that resembled “an ancient mausoleum”; they were so large they were referred to as “Queen Marys.” Within these enormous structures, plutonium was extracted from radioactive “slugs” of spent uranium fuel from the reactors:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Irradiated slugs ejected from a production pile would be stored in pools of water 16.5 feet deep to remain until the most intense and therefore short-lived of their fission-product radioactivities decayed away, the water glowing blue around them with Cerenkov radiation, a sort of charged-particle sonic boom. The slugs would then move in shielded casks on special railroad cars to one of the Queen Marys, where they would first be dissolved in hot nitric acid. A standard equipment group occupied two cells: a centrifuge, a catch tank, a precipitator and a solution tank, all made of specially fabricated corrosion-resistant stainless steel. The liquid solution that the slugs had become would move through these units by steam-jet syphoning, a low-maintenance substitute for pumps. There were three necessary steps to the separation process: solution, precipitation and centrifugal removal of the precipitate. These would repeat from equipment group to equipment group down the canyon of the separation building. The end products would be radioactive wastes, stored on site in underground tanks, and small quantities of highly purified plutonium nitrate. - The Making of the Atomic Bomb&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;As at Oak Ridge, there were numerous challenges in building a mammoth industrial facility employing completely novel production processes. The reactors “presented construction problems never encountered before, even by Du Pont's highly competent field forces.” Graphite bars of exceptional purity had to be fabricated and then machined to remove any sort of surface imperfections. Supplying cooling water for the reactors required “installation of a complex system of river pumps; purification, aeration, and distillation units; and retention basins for holding radioactive water until natural decay permitted its return to the Columbia.” New machines for fabricating the uranium fuel rods had to be designed and built, and a method for shielding the unprecedentedly large nuclear reactors had to be developed:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Ten months of work went into this before we could even begin to build it, with three more months before the first unit was completed…In the course of this, a special high-density pressed-wood sheet was developed in collaboration with an outside supplier. Then special sharp tools and operating techniques were required to cut various shapes from the standard manufactured widths…At the same time very detailed specifications for assembly, prescribing the closest of tolerances, were written. Some sixty manufacturers were invited to bid and refused, presumably because of the complexity of construction and the close tolerances required…but after methods were developed and prototypes were fabricated at du Pont’s shops in Wilmington eventually satisfactory suppliers were found. - Now It Can Be Told&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Even seemingly simple items often proved enormously complex. Finding a way to prevent the uranium fuel (which was packaged into aluminum-clad slugs) from corroding took an enormous amount of effort:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Two years of trial-and-error effort had not produced canning technology adequate to seal the uranium slugs, which quickly oxidized upon exposure to air or water, away from corrosion. Only in August had the crucial step been devised, by a young research chemist who had followed the problem from Du Pont in Wilmington to Chicago and then to Hanford: putting aside elaborate dips and baths he tried soaking the bare slugs in molten solder, lowering the aluminum cans into the solder with tongs and canning the slugs submerged. The melting point of the aluminum was not much higher than the melting point of the solder, but with careful temperature control the canning technique worked. - The Making of the Atomic Bomb&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Similarly, Leslie Groves notes that “seven months of persistent effort” were required to produce simple aluminum tubes that met the required specifications.&lt;/p&gt;
    &lt;p&gt;After enough plutonium had collected in the canned uranium slugs, it needed to be separated. The chemical process for this was comparatively straightforward to develop compared to the novel methods of mass-based separation developed for U235 (after experimenting with several possibilities, a method based on using bismuth phosphate was employed), but actually implementing it was a challenge. The materials to be processed were radioactive enough that the entire facility needed to be operable and maintainable remotely:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;…Periscopes and other special mechanisms were incorporated into the plant design; all operations could thus be carried out in complete safety from behind the heavy concrete walls. The need for shielding and the possibility of having to replace parts by indirect means required unusually close tolerances, both in fabrication and in installation. This was true even for such items as the special railroad cars that moved the irradiated uranium between the piles and the separation plants. The tracks over which these cars moved were built with extreme care so as to minimize the chances of an accident. Under no circumstances could we plan on human beings directly repairing highly radioactive equipment. - Now It Can Be Told&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Among the technologies developed to make remote operation possible were pipe flanges that could be connected by a remotely operated wrench, and the world’s first use of closed circuit TV.&lt;/p&gt;
    &lt;p&gt;The biggest crisis at Hanford came shortly after the first production reactor came online: a few hours after beginning operation the reaction began to slow, and within a few days it had shut itself down completely. Investigation revealed that this was being caused by a fission byproduct, an isotope of xenon known as Xenon 135 that had a massively greater probability of absorbing neutrons (known as a “neutron cross section”) than any previously discovered material. (Its cross section was 70 times larger than the previous largest measured cross section). This “xenon poisoning” hadn’t been noticed in earlier reactors because they hadn’t been run for long enough at a high enough power output.&lt;/p&gt;
    &lt;p&gt;The problem was ultimately resolved thanks to the conservative reactor design of the Du Pont engineers. The original Hanford reactor, designed chiefly by physicist Eugene Wigner, consisted of 1,500 “channels” for uranium fuel arranged in a cylindrical shape. Du Pont had squared this cylinder, adding more channels to the edges and bringing the total to 2004. When fuel was loaded into these extra channels, it was sufficient to overcome the xenon poisoning effect.&lt;/p&gt;
    &lt;p&gt;The xenon poisoning problem was overcome in December 1944, and by March 1945 the Hanford site had achieved full-scale plutonium production of around a pound and a half of plutonium per day.&lt;/p&gt;
    &lt;head rend="h4"&gt;Los Alamos&lt;/head&gt;
    &lt;p&gt;As design and construction of the huge facilities at Hanford and Oak Ridge began, work also proceeded on designing the bomb itself. This would chiefly be done at Los Alamos, New Mexico, the third of the atomic cities built for the Manhattan Project. Scientists led by Robert Oppenheimer began to arrive at Los Alamos in March of 1943 (though most of the facility was still under construction).&lt;/p&gt;
    &lt;p&gt;When work began at Los Alamos, the most promising method for building a bomb appeared to be the gun method: firing a sub-critical uranium or plutonium bullet into another sub-critical mass. But there were a variety of ways this might be done. Different gun arrangements, ranging from somewhat conventional gun mechanisms of various sizes and shapes to more exotic layouts like double-guns, rockets, and spherical guns were considered. As with the rest of the program, every design choice was mired in uncertainty. Since plutonium and U235 hadn’t yet been produced in large quantities, it wasn’t known exactly how much fissile material would be needed to create a critical mass, and thus how big the bullet and target needed to be. The neutron-reflecting properties of steel gun barrels (which would affect critical mass requirements) hadn’t yet been measured.&lt;/p&gt;
    &lt;p&gt;More generally, while the designers were reasonably confident that a gun-type bomb would work, because various nuclear properties and constants had at best been measured very imprecisely (if they’d been measured at all), they couldn’t be sure. A history of Los Alamos notes that in 1942, “the main obstacle to a theoretical understanding of the fission bomb was the uncertainty surrounding existing experimental data, in part the result of inadequate instrumentation and a lack of experience in the new field.” Things were so uncertain that it was not even 100% clear at the beginning of the program that plutonium would produce neutrons. A great deal of effort at Los Alamos was thus devoted to more accurate measurements and better understanding of nuclear physics: the neutron cross sections of various materials, the number of neutrons produced per fission, rates of spontaneous fission, and so on. These measurements were “constantly in flux” for much of the project, and the Los Alamos scientists were “plagued by worry about some unpredicted or overlooked mechanism of nuclear physics which might render our program unsound.”&lt;/p&gt;
    &lt;p&gt;Thus in 1943, while most design efforts were directed to developing a gun-type assembly for both uranium and plutonium, there were also parallel efforts on other types of bomb. Chief among these was the implosion method, which would use an explosion to compress a sphere of material enough to create a critical mass, though there were also investigations into the autocatalytic methods. Work also proceeded on what was referred to as the Super (better known today as the hydrogen bomb): using a fission explosion to trigger an even more destructive fusion explosion.&lt;/p&gt;
    &lt;p&gt;Beyond the mechanism for creating the critical mass, there were many other aspects of the bomb that needed to be figured out. To minimize the amount of fissile material needed to create a critical mass, the core needed to be surrounded by some type of material that would reflect neutrons back into the core and prevent them from escaping, but what material would work best and how it should be arranged wasn’t yet known. To ensure that the bomb detonated at the right time, it would also need some type of initiator: a mechanism that, when triggered, would create a sudden burst of neutrons to start the nuclear chain reaction.&lt;/p&gt;
    &lt;p&gt;More generally, everything about the atomic bomb was new, and almost nothing about its various aspects was known or could be assumed. Every step involved from taking the fissile material from Oak Ridge and Hanford and turning it into a bomb had to be worked out for the first time.&lt;/p&gt;
    &lt;p&gt;Fissile material would arrive from Hanford and Oak Ridge not as pure plutonium or uranium but as compounds — plutonium nitrate and uranium tetrafluoride, respectively — and methods for turning these into metallic plutonium and uranium needed to be created. Because impurities might affect the functioning of the bomb, purification methods also needed to be developed. It was initially believed that the plutonium in particular would need to be exceptionally pure, with no more than one part per hundred billion of light elements. Preventing contamination required, among other things, reagents that were “unbelievably purified,” electronic air cleaners, and an extensive lab-cleaning procedure performed by a dedicated service team. Creating effective plutonium purification methods took roughly a year, in part because initially only microscopic quantities of plutonium were available for experiments. Because purification required the ability to detect extremely small quantities of impurities, novel methods of “sub-micro” chemical analysis had to be developed.&lt;/p&gt;
    &lt;p&gt;Once metallic plutonium and uranium had been produced, methods for shaping the material — casting, rolling, pressing — also needed to be created. The need for purity, combined with the extreme reactivity of molten uranium and plutonium, meant that new crucible materials were needed: MIT spent an enormous amount of “time, effort, and expense” to develop a cerium sulfide crucible that could withstand plutonium's extreme reactivity and high expected melting point without introducing impurities.&lt;/p&gt;
    &lt;p&gt;Shaping metallic plutonium — never before produced — required understanding its material properties, which were found to be extremely strange: plutonium has been dubbed “the most complicated metal known to man.” Researchers discovered that plutonium had six different allotropes (physical arrangement of atoms), more than any other metal. Plutonium’s complexity made basic facts such as its melting point surprisingly difficult to determine, and the exotic cerium sulfide crucibles proved to be unnecessary when an unexpectedly low melting point was found. Most of what’s known about plutonium metallurgy was initially worked out at Los Alamos during the Manhattan Project.&lt;/p&gt;
    &lt;p&gt;In addition to uranium and plutonium, novel methods and processes had to be developed for a variety of other materials. Polonium, used in the initiator, needed to be procured in large quantities, extensively purified, and deposited on metallic foils. The investigation into polonium’s material properties has been described as “novel as that of plutonium.” Fabrication techniques were also developed for other materials with useful nuclear properties, such as Boron 10 (an isotope of boron with a large neutron cross section) and beryllium.&lt;/p&gt;
    &lt;p&gt;While a great deal was known about the mechanics of guns and projectiles at the beginning of the project, nothing like a gun needed to fire a sub-critical nuclear bullet had ever been built. The bullet needed to be fired as fast as possible to avoid the problem of predetonation — spontaneous fission starting a chain reaction before the pieces had been joined, leading to a “fizzle” — but the target also needed to stay roughly intact after the projectile’s impact. The gun also needed to be light and small enough that the resulting bomb wouldn’t be too heavy to carry, a requirement that was greatly aided by the realization that the gun would only need to fire once, and could be much less robust than conventional guns. A great deal of calculation and testing on guns of various calibers, propellants, and geometries of targets and bullets was required.&lt;/p&gt;
    &lt;p&gt;But novel as it was, this work on gun development was far more straightforward than what was required for the implosion bomb. Virtually nothing was known about the behavior of materials when imploded, or how to create an explosion that would compress a sphere of material symmetrically. Resolving this lack of knowledge began simply — setting off explosives on the outside of pipes and seeing how they deformed — but quickly ramped up in complexity. A variety of methods and instruments had to be created to study the interior of materials as they were being imploded. Some of these, such as using X-rays and high-speed cameras, were adaptations of existing measurement techniques, pushed to much higher levels of performance: getting high-speed cameras to work, for instance, required months of experiments with multiple camera designs to create one that was fast enough.&lt;/p&gt;
    &lt;p&gt;Other implosion analysis methods, such as the “RaLa” method, were far more novel. The RaLa method involved placing an isotope of radioactive lanthanum (hence RaLa), an emitter of gamma rays, at the center of the material to be imploded. When the material began to compress, it would become denser, resulting in fewer gamma rays penetrating. By surrounding the implosion with gamma ray detectors, a detailed progression of the geometric changes in the imploded material could be recorded.&lt;/p&gt;
    &lt;p&gt;While the RaLa method was a valuable source of implosion information, implementing it was fiendishly complicated. Even gathering and manipulating the lanthanum was difficult due to its intense radioactivity. Lanthanum was shipped from Oak Ridge in special lead-lined trucks, which were driven 24 hours a day. The assembled material reached 100 curies of radioactivity at a time when most radioactive experiments didn't exceed a fraction of a curie: a Los Alamos chemist noted that “no one ever worked with radiation levels like these before, ever, anywhere in the world.” Using RaLa required things like a “mechanical chemist” to remotely manipulate the radioactive material, and, initially, a mobile laboratory built inside repurposed M4 tanks.&lt;/p&gt;
    &lt;p&gt;It was initially hoped that a symmetrical compression could be created simply by adding enough explosive detonation points around the spherical bomb core, but RaLa and other measurement methods revealed “jets” of core material shooting ahead of the rest of the collapsing mass. Dealing with the jets and other asymmetries, and creating a symmetrical collapse of the bomb core, would be one of the main difficulties of the implosion bomb program.&lt;/p&gt;
    &lt;p&gt;New analytic techniques and devices weren’t limited to measuring implosions. New devices were made for, among other things, counting neutrons, collecting electrons, discriminating between different electronic pulses, and measuring projectiles with microwaves. Over a thousand pieces of electronic equipment were built at Los Alamos, many of them novel or higher performance than any other equipment then available, including better amplifiers, oscilloscopes, and counting circuits.&lt;/p&gt;
    &lt;p&gt;As with Oak Ridge and Hanford, even seemingly straightforward project elements were often enormously complex development efforts due to the novel requirements of the bomb. The triggering mechanism, for instance, couldn’t rely on off the shelf components, as they were considered insufficiently reliable: even a 1% chance of failure was far too high for a bomb in which hundreds of millions of dollars had been invested (the acceptable failure rate was eventually decided to be 0.01%). And when dropped, the bomb needed to automatically trigger at a specific elevation, a capability which didn’t yet exist. After a great deal of testing and development, a trigger circuit using radar altimeters, barometric switches, and electronic clocks was eventually created.&lt;/p&gt;
    &lt;p&gt;All this development work took place in largely the same form as the rest of the project: for nearly every decision or device, multiple promising paths were investigated, often at great expense, in the hopes that one or more could be made to work. This often led to a cascade of branching investigations, where each path branched into several possible paths, each of which might branch into more possible paths, and so on. The implosion method was just one of multiple bomb designs investigated, and within the implosion investigation RaLa was just one of multiple analytic techniques created to study implosions (a history of Los Alamos described the implosion studies as a “seven-pronged experimental program”). Bombs using both plutonium and U235 fuels were pursued, and for each material multiple methods of processing them were studied.&lt;/p&gt;
    &lt;p&gt;Because so little was known, and progress needed to be made quickly, these investigations often relied on brute force empiricism: running dozens or hundreds of experiments while systematically varying different experimental parameters. Early implosion studies repeatedly tested pipes surrounded by explosives, systematically varying the size of pipes, explosive arrangement, and the type of explosive used. These systematic investigations continued when more advanced diagnostic methods became available: to determine implosion parameters like symmetry, collapse velocity, and amount of compression, an “exhaustive” test program was initiated, where “every possible parameter was varied”. Systematic trial-and-error testing was also used for the design of the gun, the projectiles, and the target.&lt;/p&gt;
    &lt;head rend="h4"&gt;The pivot to implosion at Los Alamos&lt;/head&gt;
    &lt;p&gt;The strategy of investigating multiple promising paths proved its worth when the first shipments of reactor-produced plutonium began to arrive at Los Alamos from Oak Ridge in the spring of 1944. Prior to this Los Alamos had worked only with plutonium produced in cyclotrons, which consisted chiefly of the isotope Plutonium-239. However, reactor-produced plutonium was found to also have significant amounts of a different isotope, Plutonium-240. This isotope was found to undergo spontaneous fission much more readily than Plutonium-239 or U235. Its rate of spontaneous fission — a million times higher that of U235 — was so high in fact that the presence of Plutonium-240 made a gun-type plutonium bomb infeasible: so many neutrons would be produced by spontaneous fission that a chain reaction would be triggered before the bullet met the target, blowing the bomb apart and creating a fizzle.&lt;/p&gt;
    &lt;p&gt;The situation seemed dire. There was no time to design and build a separation plant to remove Plutonium-239 from Plutonium-240. To make use of the Hanford plutonium, the only options appeared to be build a composite bomb that mixed plutonium and uranium together (which would be low efficiency and have a comparatively small yield), or to use a different bomb mechanism capable of creating critical mass much more quickly than a gun could. That meant the implosion method, but in early 1944 work on the implosion bomb was far behind behind the gun bomb: until then the implosion bomb had been of secondary importance, a backup in case the gun didn’t work. It was still far from certain whether a workable implosion bomb could be built.&lt;/p&gt;
    &lt;p&gt;In July 1944, Robert Oppenheimer ordered a halt on further work on the plutonium gun, and to step up efforts on a plutonium implosion bomb. Within two weeks Los Alamos had been completely reorganized to focus on solving the problems of the implosion bomb. Work on the various implosion analysis methods accelerated, and the first RaLa test was completed in late September. An extensive exploration of solutions to the problem of jets and asymmetrical compression was undertaken, and the development of plutonium purification and metal fabrication methods continued (made easier by the fact that an implosion bomb could tolerate a much higher level of plutonium impurities). By early 1945, aided by the discovery plutonium’s unexpectedly low melting point (which made finding a workable crucible much easier), plutonium was successfully being purified, reduced to a metal, and worked by various methods, shortly before the first “batch” (a mere 80 grams) of plutonium arrived from Hanford.&lt;/p&gt;
    &lt;p&gt;The asymmetrical compression problem was eventually solved by the use of explosive lenses, and by changing from a hollow to a solid core of fissile material. Explosive lenses — shaped explosives that would focus the explosion and create a converging pressure wave — were first suggested by James Tuck, who arrived at Los Alamos in May 1944, but using them to build a bomb was fiercely difficult. No theory yet existed for analyzing and predicting the behavior of explosive lenses, and no methods existing for fabricating the carefully shaped explosives to the level of precision required. To design the lenses, there was no choice but to take an iterative approach: designers made guesses about effective lens shapes, tested them, and used the feedback to refine their designs.&lt;/p&gt;
    &lt;p&gt;At the same time, methods had to be developed for casting and machining the explosives. This was both enormously difficult (since the explosives had to be extremely uniform and precisely shaped) and dangerous (since machining risked explosion). A history of Los Alamos describes the challenges of explosive casting:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It was particularly difficult to cast the high explosives accurately and avoid cracks, bubbles, and other imperfections. Cooling cycles had to be long to minimize thermal stress cracks. Castings had to be wrapped in insulation before being transported between buildings…Casting technology developed slowly and painfully at Los Alamos, by a succession of reasonable steps, that consistently failed to give completely satisfactory results. Eventually, the problems were overcome… - Critical Assembly&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Machining the explosives was similarly difficult. The jigs and fixtures required to hold the explosives required several months of development, and the explosive machining methods created were considered “revolutionary.”&lt;/p&gt;
    &lt;p&gt;The work of figuring out casting and machining methods, and of creating workable lens shapes, demanded enormous amounts of explosives: over the course of the project James Tuck noted that “well over twenty thousand castings were delivered to the firing sites, while the number of castings rejected because of poor quality or destroyed for other reasons is several times this figure.” At the peak of the implosion program, Los Alamos was using 100,000 pounds of high explosive a month.&lt;/p&gt;
    &lt;p&gt;Creating symmetrical compression in the bomb core also required very precise detonating of the explosives. Early tests were done using primacord detonators (a cord of high explosive surrounded by textiles), but these were found to be far too imprecise, and investigation into other types of detonators was pursued. After extensive experimentation, a spark gap switch (a sort of explosive spark plug) combined with an exploding bridgewire detonator was developed.&lt;/p&gt;
    &lt;p&gt;Another major design problem on the implosion device was the initiator. The implosion bomb would require a new, more precise type of initiator than used on the gun bomb, one that would be triggered at the moment of highest compression in the bomb core. As late as early 1945, it wasn’t clear whether such an initiator could be built. But continued experiments and testing eventually resulted in what appeared to be a workable design. Named “the urchin,” it consisted of a beryllium sphere and pellet, with polonium between the two. When the core was compressed, the sphere would be crushed, mixing the beryllium and polonium and emitting neutrons.&lt;/p&gt;
    &lt;p&gt;Until very late in the Manhattan Project, it remained unclear if a workable implosion bomb could be built. In the last weeks of 1944, James Conant, President of Harvard and chair of the National Research Defense Council which oversaw the Manhattan Project, stated that the “difficulties were still enormous” and “my own bets very much against it.” At that time, the problems of the modulated initiator and of sufficiently precise and accurate detonation still hadn’t been solved. But as the researchers continued to run down various problems over the following months, the outlook improved considerably. By April the head of the explosives division at Los Alamos could report that its major research and design gambles “had been won”, and there was growing confidence that a bomb of the design chosen — solid plutonium core, explosive lenses, with electric detonators and a modulated initiator — could be made to work. Development work was by no means complete, and there were many problems yet to be solved (design changes to the bomb continued to be made until a few days before the Trinity test on July 16 1945), but by spring of 1945 the “research” portion of research and development had largely concluded.&lt;/p&gt;
    &lt;p&gt;As Los Alamos scrambled to build an effective implosion bomb, work also continued on the uranium gun weapon. Because the gun device was considered much less risky and much more certain to work, its development didn’t have the same fervor as the plutonium bomb. (In fact, cancelling the plutonium gun made the uranium gun program easier: a uranium bullet could be fired at a much lower velocity, reducing the difficulties of building a working gun device). But there were still numerous development problems that needed to be overcome. As with plutonium, uranium metal reduction and working methods were finalized by late 1944, and by early 1945 the design of the gun was completed and assembly of it was being tested. Unanswered questions remained until surprisingly late in the program (as late as December 1944 the critical mass of U235 still hadn’t been precisely determined), but there was little uncertainty around whether the bomb would function (so little, in fact, that testing the uranium gun bomb was considered unnecessary). By May of 1945, the uranium gun weapon, code named “Little Boy,” was “ready for combat”.&lt;/p&gt;
    &lt;head rend="h4"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The Manhattan Project has become synonymous with a difficult, expensive, and ambitious technological development project, and you often hear folks advocating for a “Manhattan Project for X.” So it's worth understanding why, specifically, the Manhattan Project was so difficult and expensive.&lt;/p&gt;
    &lt;p&gt;First, there were inherent physical difficulties in many of the tasks. The bombs required pounds of fissile material, and there was no easy way to produce it: any method chosen would require an enormous industrial-scale production facility. The Hanford Site for producing plutonium cost $350 million ($6.4 billion in 2025 dollars), and the Oak Ridge site cost $304 million ($5.5 billion in 2025 dollars), not including the cost of the borrowed silver for the electromagnets. Hundreds of millions more were spent on operating the facilities. Part of the expense and difficulty of the Manhattan Project came simply because manufacturing fissile material is expensive and difficult (and remains so today.)&lt;/p&gt;
    &lt;p&gt;The second difficulty with the Manhattan Project was that because of the great urgency, work had to proceed on the basis of very little information. Resolving the uncertainty often entailed expensive efforts that would have been greatly simplified (or eschewed altogether) had a slower pace been acceptable. Plants were built before the processes they would use had been completely defined, often requiring extensive rework after parts of them had been built. Time and effort was invested in creating a plutonium gun bomb that could have been avoided had the designers waited until reactor-produced plutonium (which due to the presence of Plutonium-240 wouldn’t work in a gun bomb) was available.&lt;/p&gt;
    &lt;p&gt;The third difficulty was that because so little knowledge existed around the nature of atomic physics and nuclear chain reactions, it was far from clear what the best route to an atomic weapon was. Because the field was so new, using only recently-discovered natural phenomena that were poorly understood, a great deal of effort was needed to resolve this uncertainty along numerous technological axes. Thus the Manhattan Project involved a large amount of trial and error experimentation, and of pursuing multiple paths of technological development — different bomb types, different fuels, different uranium separation methods, different tampers, different triggers, different implosion analysis methods — to create a workable bomb.&lt;/p&gt;
    &lt;p&gt;It’s this last difficulty that is most relevant for other technological development projects. Developing other technologies doesn’t necessarily require building enormous, industrial scale industrial facilities to even begin, and doesn’t necessarily require rapidly proceeding before the proper information and supporting technologies are available. But it will almost certainly require investigating various promising paths of development, partially-informed groping around until the right combination of methods and components is discovered. Indeed, this sort of exploration is the very essence of technological development.&lt;/p&gt;
    &lt;p&gt;Edison’s light bulb provides a useful comparison: inventing it didn’t require building an enormous, multi-million dollar factory to produce the components to experiment with. And Edison wasn’t forced to invent every single predecessor technology that a light bulb required. One of the reasons why an incandescent bulb wasn’t invented earlier is that, prior to the 1860s, vacuum pumps weren’t good enough. Edison’s bulb relied on the invention of the Sprengel mercury pump by Hermann Sprengel in 1865, which could create a high enough vacuum that incandescent lamps became feasible. But Edison was forced to explore a variety of different potential methods for creating a bulb until he created one that worked.&lt;/p&gt;
    &lt;p&gt;One thing that the Manhattan Project shows is that resolving this uncertainty, and figuring out what a technology should actually be, is hard. The Manhattan Project had some of the most brilliant scientific minds in the world working on it, but even with this collective brainpower it was far from clear what the best route to the bomb was. For almost every major design decision (including the successful ones), there was at least one genius or expert highly skeptical that it would work. Future Nobel Prize winner Hans Bethe initially resisted joining the program because he believed that building an atomic bomb wasn’t feasible at all. Early in the program, many (such as physicist Alfred Nier) felt that electromagnetic separation wouldn’t be a feasible way to isolate U235 (and indeed, until very late it looked like it wouldn’t be). British scientists were similarly pessimistic about the gaseous diffusion process, with some believing that the plant “would be inoperable” due to surges and fluctuations in the gas flow. Du Pont thought that a graphite moderated, water-cooled reactor might not work, and insisted on a heavy water reactor as a backup. Explosives expert William Parsons, head of the uranium gun program, was skeptical that explosive lenses would work, and argued that the only way to have an implosion bomb ready by summer of 1945 was with a non-lensed design. Enrico Fermi initially believed that a modulated initiator wouldn’t work, and would come up with a new reason why “every second day or so.”&lt;/p&gt;
    &lt;p&gt;It’s also notable that many of the options chosen didn’t turn out to be the best long-term. Post-war atomic weapons almost entirely eschewed gun-type mechanisms in favor of more efficient implosions. And of all the uranium separation methods explored, it was the one that was deemed least promising and not used at all by the Manhattan Project — gas centrifuges — that is primarily used for uranium enrichment today.&lt;/p&gt;
    &lt;p&gt;Not all technologies will require expensive physical facilities to produce, or require extremely rapid, expensive development. But resolving the uncertainty inherent in a new technology — figuring out what, exactly, the arrangement of phenomena needs to be to achieve some goal, and how that arrangement can be achieved — is part of the fundamental nature of creating a new technology. The Manhattan Project required an unusually large amount of this (advancing the state of the art in many different disciplines), but will always be required to some degree.&lt;/p&gt;
    &lt;p&gt;Consideration was also given to another isotope of uranium, U233.&lt;/p&gt;
    &lt;p&gt;Slowing down the neutrons made them more likely to be captured by the U235 and induce fission.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45211127</guid></item><item><title>From burner phones to decks of cards: NYC teens adjusting to the smartphone ban</title><link>https://gothamist.com/news/from-burner-phones-to-decks-of-cards-nyc-teens-are-adjusting-to-the-smartphone-ban</link><description>&lt;doc fingerprint="bc29b015bbc99c4f"&gt;
  &lt;main&gt;
    &lt;p&gt;New York City students are one week into the statewide phone ban. Gothamist reporters checked in with teens across the district to see how they are adapting. Here's how they are handling their disconnected days.&lt;/p&gt;
    &lt;head rend="h4"&gt;Lower-tech life&lt;/head&gt;
    &lt;p&gt;Polaroids, walkie-talkies and decks of cards: New York City teens said these are some of the hot items circulating in schools now that the statewide smartphone ban is in effect.&lt;/p&gt;
    &lt;p&gt;Alia Soliman, a senior at Bronx Science, said cards “are making a big comeback.” She said kids are playing poker when they’re done with their work in some classes. Fellow students reported a surge in Uno.&lt;/p&gt;
    &lt;p&gt;“It seems to be very fun and engaging,” Soliman said.&lt;/p&gt;
    &lt;p&gt;She said members of the senior class are using vintage devices to capture memories and milestones.&lt;/p&gt;
    &lt;p&gt;“I’ve seen some of my friends bring in digicams,” Soliman said. “A lot of people are bringing in Polaroids.”&lt;/p&gt;
    &lt;p&gt;She’s looking into whether an MP3 player would be allowed in school to listen to while she studies.&lt;/p&gt;
    &lt;p&gt;“I’ll test the waters,” she said.&lt;/p&gt;
    &lt;p&gt;In Manhattan, Ethan Myer, a senior at Union Square Academy, said the ban is also helping kids get their steps in.&lt;/p&gt;
    &lt;p&gt;“Most people are just walking around the hall, because there's not really much else to do,” Myer said. "Some people are talking a bit more, which I guess was the goal.”&lt;/p&gt;
    &lt;p&gt;Jem Bryant, a junior at Gramercy Arts High School, also near Union Square, is daydreaming more.&lt;/p&gt;
    &lt;p&gt;“I don't really have a lot of friends in classes,” Bryant said. “So when they say ‘do your classwork’ and I'm already done, I literally just have to sit there and stare at the wall.”&lt;/p&gt;
    &lt;p&gt;Soliman said she was opposed to the ban at first, and she’s concerned that she won’t be able to work on her college applications during downtime at school, but she sees some positives.&lt;/p&gt;
    &lt;p&gt;“I’m still not thrilled,” she said. “But people are reading more. The younger kids are socializing more.”&lt;/p&gt;
    &lt;p&gt;Peter Schmidt-Nowara, a teacher and dean at Brooklyn Tech, said the hallways and lunchroom are louder, in a good way.&lt;/p&gt;
    &lt;p&gt;“It was muted,” he said. “It’s really lifted a pall.”&lt;/p&gt;
    &lt;p&gt;He said it’s a stark difference from last year, when kids were retreating to bathrooms for 20 minutes to check their phones, and walking through the halls with their heads down.&lt;/p&gt;
    &lt;p&gt;He added that he's enjoyed watching students turn to unfamiliar technology.&lt;/p&gt;
    &lt;p&gt;“A group of boys brought in a transistor radio to listen to music. They didn’t realize they had to lift the antenna, so I lifted it for them. The music wasn’t that loud and it was quite sweet,” he said.&lt;/p&gt;
    &lt;head rend="h4"&gt;Workarounds&lt;/head&gt;
    &lt;p&gt;No students interviewed by Gothamist copped to having burner phones of their own, but some said they knew of classmates deploying decoys.&lt;/p&gt;
    &lt;p&gt;“Not to rat anyone out, but some people have fake phones,” said Solangel Santana, a senior at Union Square Academy.&lt;/p&gt;
    &lt;p&gt;She added that kids sometimes send each other emails or chat in Google Docs on their education department-issued devices.&lt;/p&gt;
    &lt;p&gt;“We find creative ways to talk to each other,” she said.&lt;/p&gt;
    &lt;p&gt;Video app TikTok is rife with videos of kids breaking into pouches. “They slam it,” said Esmeralda Jaramillo, a junior at Gramercy Arts.&lt;/p&gt;
    &lt;p&gt;Soliman said students sometimes physically leave the building and go out into the courtyard for a phone break to play games or check messages during free periods or lunch. “The benches are always full,” Soliman said.&lt;/p&gt;
    &lt;p&gt;Speaking at DeWitt Clinton High School in the Bronx on Monday, Mayor Eric Adams said it’s not the first time teenagers have tried to skirt the rules.&lt;/p&gt;
    &lt;p&gt;“This is all part of their creative spirit and energy,” he said. “We did it. Let's not act like when we were in school, we didn't have all our little tricks on how we got around things.&lt;/p&gt;
    &lt;p&gt;Adams made the comments at a press conference to announce the distribution of 350,000 internet-enabled Chromebooks, part of the city’s effort to replace aging devices obtained during the pandemic, and ensure that all students have access to technology in schools even as their personal devices are banned.&lt;/p&gt;
    &lt;head rend="h4"&gt;From bottlenecks to stampedes&lt;/head&gt;
    &lt;p&gt;Schools have rolled out a range of strategies, with most schools either collecting phones at arrival and storing them in lockers or distributing magnetic pouches that have to be locked and unlocked at the beginning and end of the day.&lt;/p&gt;
    &lt;p&gt;Bryant at Gramercy Arts High School said of the end-of-day routine, “They’ll be like, ‘Single-file line,’ but it will not be a single-file line and people will just bum-rush the magnets.”&lt;/p&gt;
    &lt;p&gt;Students at LaGuardia High School reported long lines on the first few days of school.&lt;/p&gt;
    &lt;p&gt;Laila Lawrence, 16, said students have been blamed for being late to class.&lt;/p&gt;
    &lt;p&gt;“The teachers are the first ones to complain, ‘Oh, you were late, da da da da.’ I'm like, ‘I was on the line, like, what do you want me to do?’”&lt;/p&gt;
    &lt;p&gt;But Lawrence's views on the ban were mixed.&lt;/p&gt;
    &lt;p&gt;“ I think it's good and bad because phones are obviously a problem,” she said. “I'm not gonna say like I'm not addicted to my phone 'cause I am, but at the same time I'm like no one in the school is like purposely on their phone in the middle of class.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45211527</guid></item><item><title>CRISPR offers new hope for treating diabetes</title><link>https://www.wired.com/story/no-more-injections-crispr-offers-new-hope-for-treating-diabetes/</link><description>&lt;doc fingerprint="1654c01a1a0f0b13"&gt;
  &lt;main&gt;
    &lt;p&gt;All products featured on WIRED are independently selected by our editors. However, we may receive compensation from retailers and/or from purchases of products through these links. Learn more.&lt;/p&gt;
    &lt;p&gt;Crispr gene-editing technology has demonstrated its revolutionary potential in recent years: It has been used to treat rare diseases, to adapt crops to withstand the extremes of climate change, or even to change the color of a spider’s web. But the greatest hope is that this technology will help find a cure for a global disease, such as diabetes. A new study points in that direction.&lt;/p&gt;
    &lt;p&gt;For the first time, researchers succeeded in implanting Crispr-edited pancreatic cells in a man with type 1 diabetes, an autoimmune disease where the immune system attacks insulin-producing cells in the pancreas. Without insulin, the body is then unable to regulate blood sugar. If steps aren’t taken to manage glucose levels by other means (typically, by injecting insulin), this can lead to damage to the nerves and organs—particularly the heart, kidneys, and eyes. Roughly 9.5 million people worldwide have type 1 diabetes.&lt;/p&gt;
    &lt;p&gt;In this experiment, edited cells produced insulin for months after being implanted, without the need for the recipient to take any immunosuppressive drugs to stop their body attacking the cells. The Crispr technology allowed the researchers to endow the genetically modified cells with camouflage to evade detection.&lt;/p&gt;
    &lt;p&gt;The study, published last month in The New England Journal of Medicine, details the step-by-step procedure. First, pancreatic islet cells were taken from a deceased donor without diabetes, and then altered with the gene-editing technique Crispr-Cas12b to allow them to evade the immune response of the diabetes patient. Cells altered like this are said to be “hypoimmune,” explains Sonja Schrepfer, a professor at Cedars-Sinai Medical Center in California and the scientific cofounder of Sana Biotechnology, the company that developed this treatment.&lt;/p&gt;
    &lt;p&gt;The edited cells were then implanted into the forearm muscle of the patient, and after 12 weeks, no signs of rejection were detected. (A subsequent report from Sana Biotechnology notes that the implanted cells were still evading the patient’s immune system after six months.)&lt;/p&gt;
    &lt;p&gt;Tests run as part of the study recorded that the cells were functional: The implanted cells secreted insulin in response to glucose levels, representing a key step toward controlling diabetes without the need for insulin injections. Four adverse events were recorded during follow-ups with the patient, but none of them were serious or directly linked to the modified cells.&lt;/p&gt;
    &lt;p&gt;The researchers’ ultimate goal is to apply immune-camouflaging gene edits to stem cells—which have the ability to reproduce and differentiate themselves into other cell types inside the body—and then to direct their development into insulin-secreting islet cells. “The advantage of engineering hypoimmune stem cells is that when these stem cells proliferate and create new cells, the new cells are also hypoimmune,” Schrepfer explained in a Cedars-Sinai Q+A earlier this year.&lt;/p&gt;
    &lt;p&gt;Traditionally, transplanting foreign cells into a patient has required suppressing the patient’s immune system to avoid them being rejected. This carries significant risks: infections, toxicity, and long-term complications. “Seeing patients die from rejection or severe complications from immunosuppression was frustrating to me, and I decided to focus my career on developing strategies to overcome immune rejection without immunosuppressive drugs,” Schrepfer told Cedars-Sinai.&lt;/p&gt;
    &lt;p&gt;Although the research marks a milestone in the search for treatments of type 1 diabetes, it’s important to note that the study involved one one participant, who received a low dose of cells for a short period—not enough for the patient to no longer need to control their blood sugar with injected insulin. An editorial by the journal Nature also says that some independent research groups have failed in their efforts to confirm that Sana’s method provides edited cells with the ability to evade the immune system.&lt;/p&gt;
    &lt;p&gt;Sana will be looking to conduct more clinical trials starting next year. Without overlooking the criticisms and limitations of the current study, the possibility of transplanting cells modified to be invisible to the immune system opens up a very promising horizon in regenerative medicine.&lt;/p&gt;
    &lt;p&gt;This story originally appeared on WIRED en Español and has been translated from Spanish.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45211596</guid></item><item><title>Conway's Game of Life, but musical</title><link>https://www.hudsong.dev/digital-darwin</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45211868</guid></item><item><title>Spiral</title><link>https://spiraldb.com/post/announcing-spiral</link><description>&lt;doc fingerprint="ce91d976f4a637cf"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;NT OS Kernel Information Disclosure Vulnerability – CVE-2025-53136&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Microsoft mitigated many traditional kernel information leaks starting with Windows 11/Windows Server 2022 24H2, including calls such as &lt;code&gt;NtQuerySystemInformation()&lt;/code&gt; (when used with the &lt;code&gt;SystemModuleInformation&lt;/code&gt; class), by suppressing kernel base addresses unless the caller had the &lt;code&gt;SeDebugPrivilege&lt;/code&gt;, typically reserved for administrative processes. That change effectively neutered one of the most accessible KASLR bypass techniques, and, without knowledge of the kernel’s base addresses, exploitation became harder.&lt;/p&gt;
    &lt;p&gt;While doing patch analysis for CVE-2024-43511, I realised that Microsoft made a mistake leading to a kernel address leak vulnerability. This new bug requires winning a race condition to read out the address; however, it’s pretty easy to achieve. It provides a powerful kernel address leak for any token handle, which can be easily chained with other vulnerabilities to obtain a complete exploit on the latest version of the system.&lt;/p&gt;
    &lt;head rend="h2"&gt;Vulnerability&lt;/head&gt;
    &lt;head rend="h3"&gt;Quick review on the patch for CVE-2024-43511&lt;/head&gt;
    &lt;p&gt;In October 2024, Microsoft released a patch for a Time-of-check Time-of-use (TOCTOU) Race Condition vulnerability in the Windows kernel, namely CVE-2024-43511.&lt;/p&gt;
    &lt;p&gt;To fix the issue, when passing parameters to the &lt;code&gt;RtlSidHashInitialize()&lt;/code&gt; function, it reads data from a kernel pointer (which is a member of the &lt;code&gt;TOKEN&lt;/code&gt; structure), instead of the value set in a user-controlled buffer.&lt;/p&gt;
    &lt;head rend="h3"&gt;Spotting the bug&lt;/head&gt;
    &lt;p&gt;With the new update, the &lt;code&gt;RtlSidHashInitialize()&lt;/code&gt; function, which performs hash initialisation, now takes as its first parameter a pointer from the &lt;code&gt;TOKEN&lt;/code&gt; structure and as its third parameter a user-controlled buffer. Then, &lt;code&gt;RtlSidHashInitialize()&lt;/code&gt; stores the first parameter (which is a pointer to the &lt;code&gt;UserAndGroups&lt;/code&gt; field of the &lt;code&gt;TOKEN&lt;/code&gt; structure) into the third parameter (user-supplied pointer), and starts doing hash initialisation later on:&lt;/p&gt;
    &lt;p&gt;Although the caller function will replace the stored pointer in the user-buffer pointer after that, it still leaves a small time window for us to win a race condition and read out the leaked kernel address. To trigger the vulnerable function, we only need to invoke the &lt;code&gt;NtQuerySystemInformation()&lt;/code&gt; API with the &lt;code&gt;SystemTokenInformation&lt;/code&gt; class.&lt;/p&gt;
    &lt;head rend="h3"&gt;Effects of the bug&lt;/head&gt;
    &lt;p&gt;This leak primitive is particularly useful for Windows versions 24H2 or later, as the well-known technique for leaking kernel addresses using &lt;code&gt;NtQuerySystemInformation()&lt;/code&gt; or other alternative methods has been patched. As the vulnerability is located within an NT syscall, the bug can be exploited from either Low IL or AppContainer. If chained with a write-what-where bug to overwrite the &lt;code&gt;Privileges&lt;/code&gt; field of the &lt;code&gt;TOKEN&lt;/code&gt; object, it will result in a complete LPE.&lt;/p&gt;
    &lt;head rend="h2"&gt;Exploitation&lt;/head&gt;
    &lt;head rend="h3"&gt;Setup&lt;/head&gt;
    &lt;p&gt;To exploit this bug, I need to create two threads to run concurrently:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;One thread to read at the specific offset, which will be used to store the kernel address in the user buffer.&lt;/item&gt;
      &lt;item&gt;One thread performs the syscall. It is required to run the syscall several times before archiving the kernel leak.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Reliability&lt;/head&gt;
    &lt;p&gt;Although this is a race condition bug, the time window is wide enough to read the kernel address from the user-space buffer. To increase the success rate, we repeatedly call &lt;code&gt;NtQuerySystemInformation()&lt;/code&gt; while keeping reading until we get the leak. The read becomes very reliable, and we can obtain the leaked &lt;code&gt;TOKEN&lt;/code&gt; almost every time we run the exploit.&lt;/p&gt;
    &lt;head rend="h3"&gt;Proof-of-concepts&lt;/head&gt;
    &lt;p&gt;The results below show the exploit on a Windows Insider Preview in April 2025 (latest version at the time of writing), running the exploit from the Low IL and App Container contexts:&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Patch analysis is one of the fastest ways to improve our skills and sharpen our mindset in bug finding. Additionally, it also helps us improve our secure coding skills. Sometimes, bug fixes in a function can introduce new bugs in other parts of the code. When conducting vulnerability research, it’s recommended to take a deep look to understand how the bug was patched and whether the patch completely resolves the issue or leaves other gaps open. From a developer’s point of view, every change made to a function can affect others as well, so take extra care when making any changes to the codebase. It is essential to thoroughly understand how a function works before modifying it, as this helps avoid mistakes or misuse of the function.&lt;/p&gt;
    &lt;head rend="h2"&gt;Disclosure Timeline&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;April 8th: Reported to vendor.&lt;/item&gt;
      &lt;item&gt;April 9th: Microsoft acknowledged that it is working on the bug.&lt;/item&gt;
      &lt;item&gt;April 22nd: They claimed that the bug is duplicated with a bug which had been fixed already, and closed the case without having any other chance to explain the bug… (I don’t know why).&lt;/item&gt;
      &lt;item&gt;April 22nd: I made a tweet on X to complain about that, and luckily, I got a response from them.&lt;/item&gt;
      &lt;item&gt;April 25th: Confirmed my report is a valid bug:v&lt;/item&gt;
      &lt;item&gt;April 29th: Microsoft replied that the bug was in scope.&lt;/item&gt;
      &lt;item&gt;August 1st: CVE-2025-53136 was assigned.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45212960</guid></item><item><title>NT OS Kernel Information Disclosure Vulnerability</title><link>https://www.crowdfense.com/nt-os-kernel-information-disclosure-vulnerability-cve-2025-53136/</link><description>&lt;doc fingerprint="ce91d976f4a637cf"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;NT OS Kernel Information Disclosure Vulnerability – CVE-2025-53136&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Microsoft mitigated many traditional kernel information leaks starting with Windows 11/Windows Server 2022 24H2, including calls such as &lt;code&gt;NtQuerySystemInformation()&lt;/code&gt; (when used with the &lt;code&gt;SystemModuleInformation&lt;/code&gt; class), by suppressing kernel base addresses unless the caller had the &lt;code&gt;SeDebugPrivilege&lt;/code&gt;, typically reserved for administrative processes. That change effectively neutered one of the most accessible KASLR bypass techniques, and, without knowledge of the kernel’s base addresses, exploitation became harder.&lt;/p&gt;
    &lt;p&gt;While doing patch analysis for CVE-2024-43511, I realised that Microsoft made a mistake leading to a kernel address leak vulnerability. This new bug requires winning a race condition to read out the address; however, it’s pretty easy to achieve. It provides a powerful kernel address leak for any token handle, which can be easily chained with other vulnerabilities to obtain a complete exploit on the latest version of the system.&lt;/p&gt;
    &lt;head rend="h2"&gt;Vulnerability&lt;/head&gt;
    &lt;head rend="h3"&gt;Quick review on the patch for CVE-2024-43511&lt;/head&gt;
    &lt;p&gt;In October 2024, Microsoft released a patch for a Time-of-check Time-of-use (TOCTOU) Race Condition vulnerability in the Windows kernel, namely CVE-2024-43511.&lt;/p&gt;
    &lt;p&gt;To fix the issue, when passing parameters to the &lt;code&gt;RtlSidHashInitialize()&lt;/code&gt; function, it reads data from a kernel pointer (which is a member of the &lt;code&gt;TOKEN&lt;/code&gt; structure), instead of the value set in a user-controlled buffer.&lt;/p&gt;
    &lt;head rend="h3"&gt;Spotting the bug&lt;/head&gt;
    &lt;p&gt;With the new update, the &lt;code&gt;RtlSidHashInitialize()&lt;/code&gt; function, which performs hash initialisation, now takes as its first parameter a pointer from the &lt;code&gt;TOKEN&lt;/code&gt; structure and as its third parameter a user-controlled buffer. Then, &lt;code&gt;RtlSidHashInitialize()&lt;/code&gt; stores the first parameter (which is a pointer to the &lt;code&gt;UserAndGroups&lt;/code&gt; field of the &lt;code&gt;TOKEN&lt;/code&gt; structure) into the third parameter (user-supplied pointer), and starts doing hash initialisation later on:&lt;/p&gt;
    &lt;p&gt;Although the caller function will replace the stored pointer in the user-buffer pointer after that, it still leaves a small time window for us to win a race condition and read out the leaked kernel address. To trigger the vulnerable function, we only need to invoke the &lt;code&gt;NtQuerySystemInformation()&lt;/code&gt; API with the &lt;code&gt;SystemTokenInformation&lt;/code&gt; class.&lt;/p&gt;
    &lt;head rend="h3"&gt;Effects of the bug&lt;/head&gt;
    &lt;p&gt;This leak primitive is particularly useful for Windows versions 24H2 or later, as the well-known technique for leaking kernel addresses using &lt;code&gt;NtQuerySystemInformation()&lt;/code&gt; or other alternative methods has been patched. As the vulnerability is located within an NT syscall, the bug can be exploited from either Low IL or AppContainer. If chained with a write-what-where bug to overwrite the &lt;code&gt;Privileges&lt;/code&gt; field of the &lt;code&gt;TOKEN&lt;/code&gt; object, it will result in a complete LPE.&lt;/p&gt;
    &lt;head rend="h2"&gt;Exploitation&lt;/head&gt;
    &lt;head rend="h3"&gt;Setup&lt;/head&gt;
    &lt;p&gt;To exploit this bug, I need to create two threads to run concurrently:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;One thread to read at the specific offset, which will be used to store the kernel address in the user buffer.&lt;/item&gt;
      &lt;item&gt;One thread performs the syscall. It is required to run the syscall several times before archiving the kernel leak.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Reliability&lt;/head&gt;
    &lt;p&gt;Although this is a race condition bug, the time window is wide enough to read the kernel address from the user-space buffer. To increase the success rate, we repeatedly call &lt;code&gt;NtQuerySystemInformation()&lt;/code&gt; while keeping reading until we get the leak. The read becomes very reliable, and we can obtain the leaked &lt;code&gt;TOKEN&lt;/code&gt; almost every time we run the exploit.&lt;/p&gt;
    &lt;head rend="h3"&gt;Proof-of-concepts&lt;/head&gt;
    &lt;p&gt;The results below show the exploit on a Windows Insider Preview in April 2025 (latest version at the time of writing), running the exploit from the Low IL and App Container contexts:&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Patch analysis is one of the fastest ways to improve our skills and sharpen our mindset in bug finding. Additionally, it also helps us improve our secure coding skills. Sometimes, bug fixes in a function can introduce new bugs in other parts of the code. When conducting vulnerability research, it’s recommended to take a deep look to understand how the bug was patched and whether the patch completely resolves the issue or leaves other gaps open. From a developer’s point of view, every change made to a function can affect others as well, so take extra care when making any changes to the codebase. It is essential to thoroughly understand how a function works before modifying it, as this helps avoid mistakes or misuse of the function.&lt;/p&gt;
    &lt;head rend="h2"&gt;Disclosure Timeline&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;April 8th: Reported to vendor.&lt;/item&gt;
      &lt;item&gt;April 9th: Microsoft acknowledged that it is working on the bug.&lt;/item&gt;
      &lt;item&gt;April 22nd: They claimed that the bug is duplicated with a bug which had been fixed already, and closed the case without having any other chance to explain the bug… (I don’t know why).&lt;/item&gt;
      &lt;item&gt;April 22nd: I made a tweet on X to complain about that, and luckily, I got a response from them.&lt;/item&gt;
      &lt;item&gt;April 25th: Confirmed my report is a valid bug:v&lt;/item&gt;
      &lt;item&gt;April 29th: Microsoft replied that the bug was in scope.&lt;/item&gt;
      &lt;item&gt;August 1st: CVE-2025-53136 was assigned.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45213299</guid></item><item><title>A tech-law measurement and analysis of event listeners for wiretapping</title><link>https://arxiv.org/abs/2508.19825</link><description>&lt;doc fingerprint="e89eb50a15f5028e"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Cryptography and Security&lt;/head&gt;&lt;p&gt; [Submitted on 27 Aug 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Every Keystroke You Make: A Tech-Law Measurement and Analysis of Event Listeners for Wiretapping&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:The privacy community has a long track record of investigating emerging types of web tracking techniques. Recent work has focused on compliance of web trackers with new privacy laws such as Europe's GDPR and California's CCPA. Despite the growing body of research documenting widespread lack of compliance with new privacy laws, there is a lack of robust enforcement. Different from prior work, we conduct a tech-law analysis to map decades-old U.S. laws about interception of electronic communications--so-called wiretapping--to web tracking. Bridging the tech-law gap for older wiretapping laws is important and timely because, in cases where legal harm to privacy is proven, they can provide statutory private right of action, are at the forefront of recent privacy enforcement, and could ultimately lead to a meaningful change in the web tracking landscape.&lt;lb/&gt;In this paper, we focus on a particularly invasive tracking technique: the use of JavaScript event listeners by third-party trackers for real-time keystroke interception on websites. We use an instrumented web browser to crawl a sample of the top-million websites to investigate the use of event listeners that aligns with the criteria for wiretapping, according to U.S. wiretapping law at the federal level and in California. We find evidence that 38.52% websites installed third-party event listeners to intercept keystrokes, and that at least 3.18% websites transmitted intercepted information to a third-party server, which aligns with the criteria for wiretapping. We further find evidence that the intercepted information such as email addresses typed into form fields are used for unsolicited email marketing. Beyond our work that maps the intersection between technical measurement and U.S. wiretapping law, additional future legal research is required to determine when the wiretapping observed in our paper passes the threshold for illegality.&lt;/quote&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45213612</guid></item><item><title>'Robber bees' invade apiarist's shop in attempted honey heist</title><link>https://www.cbc.ca/news/canada/british-columbia/robber-bees-terrace-bc-apiary-1.7627532</link><description>&lt;doc fingerprint="36339844be45907c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Thousands of 'robber bees' invade B.C. apiarist's shop in attempted honey heist&lt;/head&gt;
    &lt;head rend="h2"&gt;Bees sometimes attack other colonies as resources dwindle, but they usually stick to beehives found outside&lt;/head&gt;
    &lt;p&gt;A Terrace, B.C., beekeeper found herself in a nightmare situation late last month when "thousands and thousands of bees" filled her shop.&lt;/p&gt;
    &lt;p&gt;Call it an attempted robbery — the bees were trying to steal sweet, sweet honey.&lt;/p&gt;
    &lt;p&gt;Christine McDonald, owner of Rushing River Apiaries, said it's the first time desperate "robber bees" — bees that try to take honey from another colony's hive — have descended upon her indoor shop to hunt for food as resources get scarce in the late summer.&lt;/p&gt;
    &lt;p&gt;While she's worked with bees for years and said she's very comfortable around them, this situation had her frightened.&lt;/p&gt;
    &lt;p&gt;"I think that's the most panicked I have felt.…There's thousands of bees, I don't know where they're coming from, and I need to protect all of the honey."&lt;/p&gt;
    &lt;p&gt;The multitudinous bees had found their way in through cracks in the shop's older bay door.&lt;/p&gt;
    &lt;p&gt;When a bee finds a good food source, it returns to the hive and does a "waggle dance" to tell other bees where the food is, McDonald said.&lt;/p&gt;
    &lt;p&gt;After throwing tarps and lids over the equipment and products, and managing to save most of it, McDonald said she ended up "sacrificing her bathroom" to trap the invading bees: she left the light on, and lured the bees to the light where she could collect and then release them.&lt;/p&gt;
    &lt;p&gt;But it took four or five days until the bees stopped trying to return.&lt;/p&gt;
    &lt;p&gt;"I think they've learned that, nope, there's no more food here. We can't get in."&lt;/p&gt;
    &lt;p&gt;McDonald has since taped up the shop's door.&lt;/p&gt;
    &lt;p&gt;She said while she's had robber bees attack her beehives before, it's the first time they found her indoor shop.&lt;/p&gt;
    &lt;p&gt;"Fall beekeeping is very intense — trying to help bees hunker down against other bees and wasps and keep the food stores that they've worked so hard for."&lt;/p&gt;
    &lt;p&gt;So-called robber bees are a fairly common phenomenon in the late summer and early fall.&lt;/p&gt;
    &lt;p&gt;Alison McAfee, a research associate at the University of B.C. and honeybee scientist, said when there's fewer food resources, such as nectar-producing flowers, and the bee population is close to its peak, some forager colonies can invade weaker colonies to steal their food.&lt;/p&gt;
    &lt;p&gt;"It's almost like they have a level of desperation, kind of like the way you can think about bears having a bit of a level of desperation trying to fatten up for winter," McAfee said.&lt;/p&gt;
    &lt;p&gt;She said wasps can also attack honeybees — but for slightly different reasons. Wasps eat a sugary substance secreted from their own larvae earlier in the season, but in the fall, there's fewer larvae and more adult wasps.&lt;/p&gt;
    &lt;p&gt;"They're not getting that sweet treat from their babies, essentially, and so they're especially motivated to get something sweet from elsewhere, because the adults actually really like to eat sugary things — and there's a lot of sugary stuff inside a honeybee colony."&lt;/p&gt;
    &lt;p&gt;But wasps aren't the only bee-killer out there.&lt;/p&gt;
    &lt;p&gt;McAfee said a bee colony can die from robber bees, if it's too weak to defend itself.&lt;/p&gt;
    &lt;p&gt;"We have a bad opinion of wasps," she said. "We're like, 'Oh those wasps, they're attacking our honey bee colonies, they're killing my bees.' But then a lot of the time, actually, the bees will kill our bees as well."&lt;/p&gt;
    &lt;p&gt;McDonald is back to producing honey after a few days of cleanup.&lt;/p&gt;
    &lt;p&gt;She said the bees seem more desperate this year than in the past, and thought it might be due to the extended heat as it still feels like midsummer.&lt;/p&gt;
    &lt;p&gt;McDonald encouraged other beekeepers to keep their bees well fed, so they don't feel the need to rob other hives.&lt;/p&gt;
    &lt;p&gt;With files from Hanna Petersen&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45213732</guid></item><item><title>Bulletproof host Stark Industries evades EU sanctions</title><link>https://krebsonsecurity.com/2025/09/bulletproof-host-stark-industries-evades-eu-sanctions/</link><description>&lt;doc fingerprint="cf48e13f1a24a782"&gt;
  &lt;main&gt;
    &lt;p&gt;In May 2025, the European Union levied financial sanctions on the owners of Stark Industries Solutions Ltd., a bulletproof hosting provider that materialized two weeks before Russia invaded Ukraine and quickly became a top source of Kremlin-linked cyberattacks and disinformation campaigns. But new findings show those sanctions have done little to stop Stark from simply rebranding and transferring their assets to other corporate entities controlled by its original hosting providers.&lt;/p&gt;
    &lt;p&gt;Materializing just two weeks before Russia invaded Ukraine in 2022, Stark Industries Solutions became a frequent source of massive DDoS attacks, Russian-language proxy and VPN services, malware tied to Russia-backed hacking groups, and fake news. ISPs like Stark are called “bulletproof” providers when they cultivate a reputation for ignoring any abuse complaints or police inquiries about activity on their networks.&lt;/p&gt;
    &lt;p&gt;In May 2025, the European Union sanctioned one of Stark’s two main conduits to the larger Internet — Moldova-based PQ Hosting — as well as the company’s Moldovan owners Yuri and Ivan Neculiti. The EU Commission said the Neculiti brothers and PQ Hosting were linked to Russia’s hybrid warfare efforts.&lt;/p&gt;
    &lt;p&gt;But a new report from Recorded Future finds that just prior to the sanctions being announced, Stark rebranded to the[.]hosting, under control of the Dutch entity WorkTitans BV (AS209847) on June 24, 2025. The Neculiti brothers reportedly got a heads up roughly 12 days before the sanctions were announced, when Moldovan and EU media reported on the forthcoming inclusion of the Neculiti brothers in the sanctions package.&lt;/p&gt;
    &lt;p&gt;In response, the Neculiti brothers moved much of Stark’s considerable address space and other resources over to a new company in Moldova called PQ Hosting Plus S.R.L., an entity reportedly connected to the Neculiti brothers thanks to the re-use of a phone number from the original PQ Hosting.&lt;/p&gt;
    &lt;p&gt;“Although the majority of associated infrastructure remains attributable to Stark Industries, these changes likely reflect an attempt to obfuscate ownership and sustain hosting services under new legal and network entities,” Recorded Future observed.&lt;/p&gt;
    &lt;p&gt;Neither the Recorded Future report nor the May 2025 sanctions from the EU mentioned a second critical pillar of Stark’s network that KrebsOnSecurity identified in a May 2024 profile on the notorious bulletproof hoster: The Netherlands-based hosting provider MIRhosting.&lt;/p&gt;
    &lt;p&gt;MIRhosting is operated by 38-year old Andrey Nesterenko, whose personal website says he is an accomplished concert pianist who began performing publicly at a young age. DomainTools says mirhosting[.]com is registered to Mr. Nesterenko and to Innovation IT Solutions Corp, which lists addresses in London and in Nesterenko’s stated hometown of Nizhny Novgorod, Russia.&lt;/p&gt;
    &lt;p&gt;According to the book Inside Cyber Warfare by Jeffrey Carr, Innovation IT Solutions Corp. was responsible for hosting StopGeorgia[.]ru, a hacktivist website for organizing cyberattacks against Georgia that appeared at the same time Russian forces invaded the former Soviet nation in 2008. That conflict was thought to be the first war ever fought in which a notable cyberattack and an actual military engagement happened simultaneously.&lt;/p&gt;
    &lt;p&gt;Mr. Nesterenko did not respond to requests for comment. In May 2024, Mr. Nesterenko said he couldn’t verify whether StopGeorgia was ever a customer because they didn’t keep records going back that far. But he maintained that Stark Industries Solutions was merely one client of many, and claimed MIRhosting had not received any actionable complaints about abuse on Stark.&lt;/p&gt;
    &lt;p&gt;However, it appears that MIRhosting is once again the new home of Stark Industries, and that MIRhosting employees are managing both the[.]hosting and WorkTitans — the primary beneficiaries of Stark’s assets.&lt;/p&gt;
    &lt;p&gt;A copy of the incorporation documents for WorkTitans BV obtained from the Dutch Chamber of Commerce shows WorkTitans also does business under the names Misfits Media and and WT Hosting (considering Stark’s historical connection to Russian disinformation websites, “Misfits Media” is a bit on the nose).&lt;/p&gt;
    &lt;p&gt;The incorporation document says the company was formed in 2019 by a y.zinad@worktitans.nl. That email address corresponds to a LinkedIn account for a Youssef Zinad, who says their personal websites are worktitans[.]nl and custom-solution[.]nl. The profile also links to a website (etripleasims dot nl) that LinkedIn currently blocks as malicious. All of these websites are or were hosted at MIRhosting.&lt;/p&gt;
    &lt;p&gt;Although Mr. Zinad’s LinkedIn profile does not mention any employment at MIRhosting, virtually all of his LinkedIn posts over the past year have been reposts of advertisements for MIRhosting’s services.&lt;/p&gt;
    &lt;p&gt;A Google search for Youssef Zinad reveals multiple startup-tracking websites that list him as the founder of the[.]hosting, which censys.io finds is hosted by PQ Hosting Plus S.R.L.&lt;/p&gt;
    &lt;p&gt;The Dutch Chamber of Commerce document says WorkTitans’ sole shareholder is a company in Almere, Netherlands called Fezzy B.V. Who runs Fezzy? The phone number listed in a Google search for Fezzy B.V. — 31651079755 — also was used to register a Facebook profile for a Youssef Zinad from the same town, according to the breach tracking service Constella Intelligence.&lt;/p&gt;
    &lt;p&gt;In a series of email exchanges leading up to KrebsOnSecurity’s May 2024 deep dive on Stark, Mr. Nesterenko included Mr. Zinad in the message thread (youssef@mirhosting.com), referring to him as part of the company’s legal team. The Dutch website stagemarkt[.]nl lists Youssef Zinad as an official contact for MIRhosting’s offices in Almere. Mr. Zinad did not respond to requests for comment.&lt;/p&gt;
    &lt;p&gt;Given the above, it is difficult to argue with the Recorded Future report on Stark’s rebranding, which concluded that “the EU’s sanctioning of Stark Industries was largely ineffective, as affiliated infrastructure remained operational and services were rapidly re-established under new branding, with no significant or lasting disruption.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45214164</guid></item><item><title>Adam (YC W25) Is Hiring to Build the Future of CAD</title><link>https://www.ycombinator.com/companies/adam/jobs/q6td4uk-founding-engineer</link><description>&lt;doc fingerprint="a98bd1507c0d8ce0"&gt;
  &lt;main&gt;
    &lt;p&gt;AI Powered CAD&lt;/p&gt;
    &lt;p&gt;We’re building the founding engineering team at Adam.&lt;/p&gt;
    &lt;p&gt;At Adam, we’re tackling a frontier problem: building a new way to interface with CAD via AI. This demands creativity, deep technical ability, and novel thinking.&lt;/p&gt;
    &lt;p&gt;As part of the founding engineering team you may:&lt;/p&gt;
    &lt;p&gt;We are looking for people who:&lt;/p&gt;
    &lt;p&gt;At Adam, you will be given the autonomy to do some of the best work of your life and redefine how the physical world is built.&lt;/p&gt;
    &lt;p&gt;Adam is an applied research and product lab building the future of computer-aided design, where engineers will be able to speak physical objects into existence. With Adam, you’ll use AI to complete design tasks end-to-end. For example, select a face and say, “Add mounting holes matching the bolt pattern from the other part, with identical diameters, spacing, and offsets,” and Adam handles the geometry, selects standard hardware, and outputs production-ready CAD.&lt;/p&gt;
    &lt;p&gt;We just wrapped up YC’s W25 batch, built a state-of-the-art text-to-CAD interface, and had one of the most viral YC launches of all time.&lt;/p&gt;
    &lt;p&gt;Our first product made CAD approachable and fun with AI, bringing tens of thousands of people into 3D printing. Now we’re enhancing the capabilities of engineers to create feature-rich models compatible with industry-standard software.&lt;/p&gt;
    &lt;p&gt;We’re a small technical team of engineers, designers, and researchers who love CAD and building things. Our founders are UC Berkeley alumni, second-time founders, and early employees at major AI startups.&lt;/p&gt;
    &lt;p&gt;We're in-person at the Adam House in the SF Marina, powered by green tea, lots of natural light, and a wall of 3D printers.&lt;/p&gt;
    &lt;p&gt;If you’re excited about shipping fast and shaping the future of how things are made, come join us.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45214498</guid></item><item><title>Top model scores may be skewed by Git history leaks in SWE-bench</title><link>https://github.com/SWE-bench/SWE-bench/issues/465</link><description>&lt;doc fingerprint="b7b29ad728b3bfb0"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 608&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;p&gt;We've identified multiple loopholes with SWE Bench Verified where agents may look at future repository state (by querying it directly or through a variety of methods), and cases in which future repository state includes either solutions or detailed approaches to solving problems (commit messages and more).&lt;/p&gt;
    &lt;head rend="h3"&gt;Examples:&lt;/head&gt;
    &lt;p&gt;A trajectory with Claude 4 Sonnet, &lt;code&gt;Pytest-dev__pytest-6202&lt;/code&gt; (complete output here), the agent uses &lt;code&gt;git log --all&lt;/code&gt; which leaks future commits that directly fix the issue:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;&amp;lt;antml:parameter name="command"&amp;gt;cd /testbed &amp;amp;&amp;amp; git log --oneline --all | grep -i "bracket|parametrize|modpath" | head -10&amp;lt;/antml:parameter&amp;gt;&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The results of which directly reveal the fix:&lt;/p&gt;
    &lt;code&gt;    Fix incorrect result of getmodpath method.
diff --git a/src/_pytest/python.py b/src/_pytest/python.py
index b8b365ad3..734a92f9b 100644
--- a/src/_pytest/python.py
+++ b/src/_pytest/python.py
@@ -285,8 +285,7 @@ class PyobjMixin(PyobjContext):
                     break
             parts.append(name)
         parts.reverse()
-        s = ".".join(parts)
-        return s.replace(".[", "[")
+        return ".".join(parts)
&lt;/code&gt;
    &lt;p&gt;Qwen3-Coder 480B (&lt;code&gt;20250805-openhands-Qwen3-Coder-480B-A35B-Instruct&lt;/code&gt;) also has several cases of looking ahead: some examples include &lt;code&gt;django__django-13513&lt;/code&gt; (complete output here) uses &lt;code&gt;git log grep=[issue ID]&lt;/code&gt; which directly reveals the fix PR which is in the future repo state (future commits).&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Running command: cd /workspace/django__django__3.2 &amp;amp;&amp;amp; �[1m�[91mgit log�[0m --oneline --grep="31926" -i&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In another Qwen3-Coder trajectory, &lt;code&gt;Django__django-15572&lt;/code&gt;, (complete output here) where the model specifically finds the commit containing the fix: 62739b6e2630e37faa68a86a59fad135cc788cd7&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Command&lt;/p&gt;&lt;code&gt;cd /workspace/django__django__4.1 &amp;amp;&amp;amp; �[1m�[91mgit log�[0m --oneline --grep="33628" �[92m--all�[0m&lt;/code&gt;executed with exit code 0.&lt;/quote&gt;
    &lt;p&gt;There are other examples of leakage found in trajectories from GLM 4.5, Qwen3-Coder 30B (&lt;code&gt;20250805-openhands-Qwen3-Coder-30B-A3B-Instruct&lt;/code&gt;), and other models.&lt;/p&gt;
    &lt;p&gt;Mitigation will be to properly remove future repository state and any artifacts that contain information the agent could use (reflogs, branches, origins, tags, and more):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;remove origins (branch names can reveal information about fixes)&lt;/item&gt;
      &lt;item&gt;remove all branches &lt;code&gt;git log --all&lt;/code&gt;can be used to query them, plus branches that are tracking a remote origin might contain information about future commits even after a&lt;code&gt;git reset --hard&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;remove the reflog (&lt;code&gt;git reflog&lt;/code&gt;) can leak future commit messages that could detail approaches for solutions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The team (@felixkreuk, @UniverseFly, @jlko, @2dot71mily and others) will add more details as to findings here and below. We're still assessing broader impact on evaluations and understanding trajectories for sources of leakage.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45214670</guid></item><item><title>Making io_uring pervasive in QEMU [pdf]</title><link>https://vmsplice.net/~stefan/stefanha-kvm-forum-2025.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45214672</guid></item><item><title>Claude's memory architecture is the opposite of ChatGPT's</title><link>https://www.shloked.com/writing/claude-memory</link><description>&lt;doc fingerprint="e7ca9cfeb9a65afb"&gt;
  &lt;main&gt;
    &lt;p&gt;11 Sep 2025&lt;/p&gt;
    &lt;head rend="h2"&gt;Rails on SQLite: exciting new ways to cause outages&lt;/head&gt;
    &lt;p&gt;This post was originally given as a talk for Friendly.rb. The slides are also available.&lt;/p&gt;
    &lt;p&gt;Between Litestack and the Rails 8 trifecta of Solid Cable, Solid Cache, and Solid Queue, it’s easier than ever to spin up a Rails app that doesn’t need a database service, or a redis service, or a file storage service. It’s great to simplify things, but even after 20 years of deploying Rails apps I was still caught out by some of the ways things are different.&lt;/p&gt;
    &lt;p&gt;Based on what happened when I built a new side project in Rails on SQLite, we’ll cover what’s different, what’s new, and several ways that you can knock your site offline or even destroy your entire production database. As we go, we’ll also talk about the advantages of using SQLite, and how those differences can help you.&lt;/p&gt;
    &lt;p&gt;So who am I, how did I learn these things, and why should you listen to me? I’m André Arko, better known on the internet as @indirect. A long time ago, I helped create Bundler, and I’ve been the OSS team lead for RubyGems and Bundler for more than a decade at this point.&lt;/p&gt;
    &lt;p&gt;I work at Spinel Cooperative, a collective of Ruby open source maintainers building rv, the Ruby language manager that can install Ruby in one second flat. We offer retainers for unlimited access to core team experts from Bundler, Rails, Hotwire, and more, who can answer your questions and solve your problems.&lt;/p&gt;
    &lt;p&gt;I’ve been deploying Rails applications to production since 2004, and most relevantly for this particular talk, I built a service called feedyour.email. Feed Your Email turns email subscriptions into RSS feeds that you can subscribe to in your feed reader. There is no signup, there are no accounts, you just go to the website and push a button to get an email address and a feed URL. Emails sent to that address will show up in that feed.&lt;/p&gt;
    &lt;p&gt;Feed Your Email is only possible as a service because of SQLite—if I had to maintain three Postgres instances and a couple of web instances and a couple of worker instances, I would have felt like it was too much hassle and cost too much money, and given up. SQLite reduced the complexity of building and deploying this service to the point where I was able to do it casually, for fun, and keep it running for everyone while feeling like it was worth it just for my own personal use.&lt;/p&gt;
    &lt;p&gt;This app serves about a million requests per month. That number sounds kind of big, but let’s do the math: 24 requests per minute, or one every 2.6 seconds. We can definitely serve at least one request every 2.6 seconds off of our Rails app, even on a small digital ocean droplet or a small cloud VM. I run my service on Fly.io, and hosting costs about USD$14 per month.&lt;/p&gt;
    &lt;p&gt;It has bonus features like a web view so you can share individual emails as a link without giving away your secret email address or letting anyone see the other emails in that feed, and it also has full-text search if you’re trying to find a particular email. That full-text search is a feature of SQLite, which brings us back to the topic of this talk. How did SQLite help? Let’s take a look.&lt;/p&gt;
    &lt;p&gt;The biggest fundamental difference, where almost every other difference comes from, is that SQLite is built in to your web server process. The reason for the “lite” in the name is that it doesn’t run a separate process, it doesn’t listen on a port or a socket, and you can’t connect to it. All the data is just in a single file, and your web process reads and writes that file when needed. This is awesome: you can’t have database connection errors anymore.&lt;/p&gt;
    &lt;p&gt;There’s a small issue with giving the web process its own database in a local file, though. If you deploy your app as usual, your production database can disappear at any time! Heroku destroys dynos every 24 hours, including all files. Fly.io loses the files in a container anytime they restart. In a world of containers, it’s incredibly easy to miss that your new SQLite database is on an ephemeral filesystem and will disappear along with the Puma process in your web container.&lt;/p&gt;
    &lt;p&gt;That leads us to the first and most important rule of using SQLite: put your database files in persistent storage. On AWS that means EBS, on Fly.io that means Volumes, but wherever you’re hosting, make sure that you can keep your database files across restarts (and ideally have automatic snapshots).&lt;/p&gt;
    &lt;p&gt;Now that your database won’t disappear at any moment, let’s talk about what it means to put all the data in a single file. You probably use &lt;code&gt;Rails.cache&lt;/code&gt; to store temporary data—that’s in a single SQLite file now, too. You also probably use &lt;code&gt;ActiveJob&lt;/code&gt; to send emails or do slower work in the background. All of those jobs are also in a single SQLite file now. By default, it’s the same file!&lt;/p&gt;
    &lt;p&gt;Putting everything in the same SQLite file makes everything very easy. You just need to keep track of that one file, and you’ll know that your model data, your caches, and your jobs will all be safe.&lt;/p&gt;
    &lt;p&gt;SQLite being in-process raises a new problem, though… what if your background job needs to update a model? You might be used to deploying your background workers in a separate container, so you can scale them as needed. That’s not going to fly anymore, because your background workers need to be able to read and write that same single file that your web server is reading and writing.&lt;/p&gt;
    &lt;p&gt;Since your database is now in just one file, you have two options. You can run your workers in a second process inside the same VM or container as the web process, or you can run your workers in threads inside the web process.&lt;/p&gt;
    &lt;p&gt;If this is a smallish application, doing a million requests per month or less, you’ll be absolutely fine putting your background jobs in threads. As a bonus, putting background jobs in threads can almost halve the amount of RAM you need because a single Rails process is handling both web and job requests.&lt;/p&gt;
    &lt;p&gt;If you really want to scale up your application, though, you’ll need to do what gets called “vertical” scaling rather than the traditional “horizontal” scaling. You can’t add more VMs, because other VMs won’t be able to see your database file. Instead, you need bigger and bigger single servers, with more and more CPU cores and RAM. That’s definitely possible, though. There are servers nowadays with 128 cores, or even more, and terabytes of RAM. Unfortunately, once you have scaled your wildly popular application vertically to the moon, you’ll discover the biggest limitation of SQLite: it’s just one file on disk.&lt;/p&gt;
    &lt;p&gt;If you have dozens processes and hundreds of threads in web servers and background job workers, all trying to write into this one database file at the same time, there’s probably going to be a lot of contention. By default, SQLite uses the filesystem to take out a lock on the entire database for each transaction. Holding the lock, it executes any read and write queries, commits, and then releases the lock. Then the next process can take the filesystem lock and do the same thing.&lt;/p&gt;
    &lt;p&gt;This can create quite the queue if even read-only queries have to wait in line and happen one at a time (because if they didn’t have the lock, some writer might sneak in and change the data mid-read!). To (partially) address this problem, SQLite offers a middle ground in the form of a Write-Ahead Log. The WAL log is an append-only file where any database writes can be written, one at a time. Then, a SQLite-controlled process copies those write instructions into the actual database file between reads. In the meantime, there can be as many readers as you want, because writes don’t have to block reads, and many reads from the same file at once are quite safe.&lt;/p&gt;
    &lt;p&gt;This solves the problem with only allowing one single read or write at a time, but it definitely has a cost. The database isn’t just one file anymore, it’s now a series of files, and you need to back them up and restore them together if you want to make sure you haven’t lost any data. Hopefully that’s not too much trouble, but it is definitely something to be aware of while planning your backup and disaster recovery strategy.&lt;/p&gt;
    &lt;p&gt;There’s one other approach worth calling out at this point, and that strategy is deliberately using multiple SQLite database files. If you are putting your not just your model data, but also your Rails cache, and also your background jobs, and maybe also your uploaded files all together into a single SQLite database file, your different use-cases can start to step on one another’s toes. For example, if you go to queue a few thousand jobs, any writes from your web requests will end up in the writer queue behind that pile of jobs in line to be written.&lt;/p&gt;
    &lt;p&gt;Creating a separate SQLite file per system, or per access pattern, can help a lot with this. In Rails, the most common splits are separate SQLite databases for ActiveRecord, for the Rails cache, for background jobs, and for ActionCable. Depending on your application, it might also make sense to put your ActiveStorage blobs into a SQLite database or into the same filesystem that you are already backing up, as well. There’s a lot of complexity and overhead involved in setting up S3 buckets with the correct permissions and getting files into and out of them, and you might just want to skip all of that in your new, simplified Rails config.&lt;/p&gt;
    &lt;p&gt;Taking this approach to an extreme might even involve sharding your model data across many database files. The most extreme example of this that I’ve heard of was an application that chose to shard their model data across one SQLite database file per customer. That meant every new signup created a new SQLite database file on disk, which is in some ways absurd, but it also meant that every individual user had the full power and speed of SQLite available to them. It’s hard to have read or write contention when every user gets their own separate database!&lt;/p&gt;
    &lt;p&gt;So now that we’ve covered vertically scaling the Rails server itself, let’s talk about the other implications of your application running on exactly one server. The downside to there being just one server is that if that server goes down, your entire app is down. No degraded service, no slower than usual application, just… no application at all.&lt;/p&gt;
    &lt;p&gt;If you’re running in a container, it’s impossible to deploy without downtime because only one container can ever have the volume with the database mounted. The old container has to stop before the new container can start. If you’re running in a VM, you might be able to deploy without downtime by running a local reverse proxy and more than one web server process, and restarting those web server processes one at a time rather than all at once. Welcome to how we used to do things in the 2000s, and my apologies.&lt;/p&gt;
    &lt;p&gt;That said, some of the implications of only one server are good: if there’s only one, it’s pretty easy to run status checks, and it’s pretty easy to troubleshoot. You don’t need to debug connections between load balancers and dozens of web servers and database servers and redis servers and file storage servers, you just need to debug the one server. That can definitely make your job easier!&lt;/p&gt;
    &lt;p&gt;Another implication of having just one single server: there is only one place for network requests to go. As I alluded to a moment ago, the only kind of load balancing that you can do is by running a local proxy and adding multiple separate processes as backends. The server itself is only going to have one IP address and one port where it can be reached, and there’s a certain amount of scale where that one IP and one port is going to become limiting. The good news is that you probably won’t hit that scale, and if you do, you’ll probably want to stop using SQLite anyway.&lt;/p&gt;
    &lt;p&gt;If you ever want to try switching towards or away from SQLite, the sequel gem has the amazing ability to read from one database and write into another, doing a full database copy while respecting all the quirks and limitations of each database. If you want to move from Mysql or Postgres over to SQLite, or if you ever want to load a SQLite database into Mysql or Postgres, I highly recommend it. The duckdb command line tool also has excellent cross-database capabilities, and is the next thing I would try if sequel wasn’t working for me for some reason.&lt;/p&gt;
    &lt;p&gt;There’s one more limitation that we need to consider that falls out of there only being one server: your app can only run in one geographic location. Some applications can benefit from adding additional web processes (or even database read replicas) spread out closer to end users, and that’s not possible if you are limited to a maximum of one server total for your entire application.&lt;/p&gt;
    &lt;p&gt;That said, there’s nothing stopping you from using the more usual kind of CDN-based global distribution. If your application has a decent amount of static or cacheable content, you can at least still set the cache-control headers and run the app behind Fastly or Cloudlfare.&lt;/p&gt;
    &lt;p&gt;Before we wrap up, I want to make sure to cover the various backup and replication options available to you while using SQLite for your application. The absolute all-star of SQLite backup and replication is called Litestream. It’s available as a gem, and can be used as easily as setting a few environment variables and using the &lt;code&gt;litestream&lt;/code&gt; command provided by the gem to wrap your puma or other web server.&lt;/p&gt;
    &lt;p&gt;What litestream does is fairly simple: it forwards a copy of each entry added to the write-ahead log over to any S3-compatible file store — you might even say that it streams your data in a light way. If you ever have a catastrophe, and your database file gets deleted or corrupted, the bucket will have a full copy of the WAL that you can replay to restore your database back to where it was when the server stopped working.&lt;/p&gt;
    &lt;p&gt;On AWS, this still means setting up an S3 bucket and setting the right env vars, but at least you don’t need to deal with the bucket having public access, or setting up signed uploads, or any of the other things that make S3 a huge pain. You just need a private bucket and read/write credentials, and you’re good to go. If you’re using fly.io, you don’t even have to set the env vars yourself! They are set automatically by the command that creates the S3-compatible bucket on Tigris.&lt;/p&gt;
    &lt;p&gt;There’s one last thing that you can try using if you’re feeling especially adventurous, LiteFS. LiteFS is a fascinating software achievement, offering full Mysql or Postgres-style replication for multiple SQLite databases running in many locations. The completely deranged trick that they use to do this is creating an entire software filesystem using FUSE, and then putting the SQLite database inside that filesystem. This gives them access to every filesystem read and write call made by your application, and allows them to create their own operations that are then sent to every other member of the cluster to be applied.&lt;/p&gt;
    &lt;p&gt;This kind of setup comes with a lot of caveats. The biggest one is the usual distributed systems kind of caveat. You’ll have stale reads where some users will see old data, and if the primary crashes you might lose some data. If you’re okay with the tradeoffs of a distributed system (and you’re okay with the idea of all of your database reads and writes going through a FUSE filesystem that might be adding extra bugs), LiteFS offers a version of the ultimate web application dream.&lt;/p&gt;
    &lt;p&gt;In the dream SQLite plus LiteFS world, you have all the advantages of SQLite and all the advantages of a fully replicated multi-writer database setup. Any individual server can go down without causing any downtime for the application as a whole, and every user has a full copy of the application and all its data, running extremely close to them.&lt;/p&gt;
    &lt;p&gt;I haven’t built that perfect system yet, but it feels more attainable than it ever has before thanks to SQLite.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45214908</guid></item><item><title>Rails on SQLite: new ways to cause outages</title><link>https://andre.arko.net/2025/09/11/rails-on-sqlite-exciting-new-ways-to-cause-outages/</link><description>&lt;doc fingerprint="e7ca9cfeb9a65afb"&gt;
  &lt;main&gt;
    &lt;p&gt;11 Sep 2025&lt;/p&gt;
    &lt;head rend="h2"&gt;Rails on SQLite: exciting new ways to cause outages&lt;/head&gt;
    &lt;p&gt;This post was originally given as a talk for Friendly.rb. The slides are also available.&lt;/p&gt;
    &lt;p&gt;Between Litestack and the Rails 8 trifecta of Solid Cable, Solid Cache, and Solid Queue, it’s easier than ever to spin up a Rails app that doesn’t need a database service, or a redis service, or a file storage service. It’s great to simplify things, but even after 20 years of deploying Rails apps I was still caught out by some of the ways things are different.&lt;/p&gt;
    &lt;p&gt;Based on what happened when I built a new side project in Rails on SQLite, we’ll cover what’s different, what’s new, and several ways that you can knock your site offline or even destroy your entire production database. As we go, we’ll also talk about the advantages of using SQLite, and how those differences can help you.&lt;/p&gt;
    &lt;p&gt;So who am I, how did I learn these things, and why should you listen to me? I’m André Arko, better known on the internet as @indirect. A long time ago, I helped create Bundler, and I’ve been the OSS team lead for RubyGems and Bundler for more than a decade at this point.&lt;/p&gt;
    &lt;p&gt;I work at Spinel Cooperative, a collective of Ruby open source maintainers building rv, the Ruby language manager that can install Ruby in one second flat. We offer retainers for unlimited access to core team experts from Bundler, Rails, Hotwire, and more, who can answer your questions and solve your problems.&lt;/p&gt;
    &lt;p&gt;I’ve been deploying Rails applications to production since 2004, and most relevantly for this particular talk, I built a service called feedyour.email. Feed Your Email turns email subscriptions into RSS feeds that you can subscribe to in your feed reader. There is no signup, there are no accounts, you just go to the website and push a button to get an email address and a feed URL. Emails sent to that address will show up in that feed.&lt;/p&gt;
    &lt;p&gt;Feed Your Email is only possible as a service because of SQLite—if I had to maintain three Postgres instances and a couple of web instances and a couple of worker instances, I would have felt like it was too much hassle and cost too much money, and given up. SQLite reduced the complexity of building and deploying this service to the point where I was able to do it casually, for fun, and keep it running for everyone while feeling like it was worth it just for my own personal use.&lt;/p&gt;
    &lt;p&gt;This app serves about a million requests per month. That number sounds kind of big, but let’s do the math: 24 requests per minute, or one every 2.6 seconds. We can definitely serve at least one request every 2.6 seconds off of our Rails app, even on a small digital ocean droplet or a small cloud VM. I run my service on Fly.io, and hosting costs about USD$14 per month.&lt;/p&gt;
    &lt;p&gt;It has bonus features like a web view so you can share individual emails as a link without giving away your secret email address or letting anyone see the other emails in that feed, and it also has full-text search if you’re trying to find a particular email. That full-text search is a feature of SQLite, which brings us back to the topic of this talk. How did SQLite help? Let’s take a look.&lt;/p&gt;
    &lt;p&gt;The biggest fundamental difference, where almost every other difference comes from, is that SQLite is built in to your web server process. The reason for the “lite” in the name is that it doesn’t run a separate process, it doesn’t listen on a port or a socket, and you can’t connect to it. All the data is just in a single file, and your web process reads and writes that file when needed. This is awesome: you can’t have database connection errors anymore.&lt;/p&gt;
    &lt;p&gt;There’s a small issue with giving the web process its own database in a local file, though. If you deploy your app as usual, your production database can disappear at any time! Heroku destroys dynos every 24 hours, including all files. Fly.io loses the files in a container anytime they restart. In a world of containers, it’s incredibly easy to miss that your new SQLite database is on an ephemeral filesystem and will disappear along with the Puma process in your web container.&lt;/p&gt;
    &lt;p&gt;That leads us to the first and most important rule of using SQLite: put your database files in persistent storage. On AWS that means EBS, on Fly.io that means Volumes, but wherever you’re hosting, make sure that you can keep your database files across restarts (and ideally have automatic snapshots).&lt;/p&gt;
    &lt;p&gt;Now that your database won’t disappear at any moment, let’s talk about what it means to put all the data in a single file. You probably use &lt;code&gt;Rails.cache&lt;/code&gt; to store temporary data—that’s in a single SQLite file now, too. You also probably use &lt;code&gt;ActiveJob&lt;/code&gt; to send emails or do slower work in the background. All of those jobs are also in a single SQLite file now. By default, it’s the same file!&lt;/p&gt;
    &lt;p&gt;Putting everything in the same SQLite file makes everything very easy. You just need to keep track of that one file, and you’ll know that your model data, your caches, and your jobs will all be safe.&lt;/p&gt;
    &lt;p&gt;SQLite being in-process raises a new problem, though… what if your background job needs to update a model? You might be used to deploying your background workers in a separate container, so you can scale them as needed. That’s not going to fly anymore, because your background workers need to be able to read and write that same single file that your web server is reading and writing.&lt;/p&gt;
    &lt;p&gt;Since your database is now in just one file, you have two options. You can run your workers in a second process inside the same VM or container as the web process, or you can run your workers in threads inside the web process.&lt;/p&gt;
    &lt;p&gt;If this is a smallish application, doing a million requests per month or less, you’ll be absolutely fine putting your background jobs in threads. As a bonus, putting background jobs in threads can almost halve the amount of RAM you need because a single Rails process is handling both web and job requests.&lt;/p&gt;
    &lt;p&gt;If you really want to scale up your application, though, you’ll need to do what gets called “vertical” scaling rather than the traditional “horizontal” scaling. You can’t add more VMs, because other VMs won’t be able to see your database file. Instead, you need bigger and bigger single servers, with more and more CPU cores and RAM. That’s definitely possible, though. There are servers nowadays with 128 cores, or even more, and terabytes of RAM. Unfortunately, once you have scaled your wildly popular application vertically to the moon, you’ll discover the biggest limitation of SQLite: it’s just one file on disk.&lt;/p&gt;
    &lt;p&gt;If you have dozens processes and hundreds of threads in web servers and background job workers, all trying to write into this one database file at the same time, there’s probably going to be a lot of contention. By default, SQLite uses the filesystem to take out a lock on the entire database for each transaction. Holding the lock, it executes any read and write queries, commits, and then releases the lock. Then the next process can take the filesystem lock and do the same thing.&lt;/p&gt;
    &lt;p&gt;This can create quite the queue if even read-only queries have to wait in line and happen one at a time (because if they didn’t have the lock, some writer might sneak in and change the data mid-read!). To (partially) address this problem, SQLite offers a middle ground in the form of a Write-Ahead Log. The WAL log is an append-only file where any database writes can be written, one at a time. Then, a SQLite-controlled process copies those write instructions into the actual database file between reads. In the meantime, there can be as many readers as you want, because writes don’t have to block reads, and many reads from the same file at once are quite safe.&lt;/p&gt;
    &lt;p&gt;This solves the problem with only allowing one single read or write at a time, but it definitely has a cost. The database isn’t just one file anymore, it’s now a series of files, and you need to back them up and restore them together if you want to make sure you haven’t lost any data. Hopefully that’s not too much trouble, but it is definitely something to be aware of while planning your backup and disaster recovery strategy.&lt;/p&gt;
    &lt;p&gt;There’s one other approach worth calling out at this point, and that strategy is deliberately using multiple SQLite database files. If you are putting your not just your model data, but also your Rails cache, and also your background jobs, and maybe also your uploaded files all together into a single SQLite database file, your different use-cases can start to step on one another’s toes. For example, if you go to queue a few thousand jobs, any writes from your web requests will end up in the writer queue behind that pile of jobs in line to be written.&lt;/p&gt;
    &lt;p&gt;Creating a separate SQLite file per system, or per access pattern, can help a lot with this. In Rails, the most common splits are separate SQLite databases for ActiveRecord, for the Rails cache, for background jobs, and for ActionCable. Depending on your application, it might also make sense to put your ActiveStorage blobs into a SQLite database or into the same filesystem that you are already backing up, as well. There’s a lot of complexity and overhead involved in setting up S3 buckets with the correct permissions and getting files into and out of them, and you might just want to skip all of that in your new, simplified Rails config.&lt;/p&gt;
    &lt;p&gt;Taking this approach to an extreme might even involve sharding your model data across many database files. The most extreme example of this that I’ve heard of was an application that chose to shard their model data across one SQLite database file per customer. That meant every new signup created a new SQLite database file on disk, which is in some ways absurd, but it also meant that every individual user had the full power and speed of SQLite available to them. It’s hard to have read or write contention when every user gets their own separate database!&lt;/p&gt;
    &lt;p&gt;So now that we’ve covered vertically scaling the Rails server itself, let’s talk about the other implications of your application running on exactly one server. The downside to there being just one server is that if that server goes down, your entire app is down. No degraded service, no slower than usual application, just… no application at all.&lt;/p&gt;
    &lt;p&gt;If you’re running in a container, it’s impossible to deploy without downtime because only one container can ever have the volume with the database mounted. The old container has to stop before the new container can start. If you’re running in a VM, you might be able to deploy without downtime by running a local reverse proxy and more than one web server process, and restarting those web server processes one at a time rather than all at once. Welcome to how we used to do things in the 2000s, and my apologies.&lt;/p&gt;
    &lt;p&gt;That said, some of the implications of only one server are good: if there’s only one, it’s pretty easy to run status checks, and it’s pretty easy to troubleshoot. You don’t need to debug connections between load balancers and dozens of web servers and database servers and redis servers and file storage servers, you just need to debug the one server. That can definitely make your job easier!&lt;/p&gt;
    &lt;p&gt;Another implication of having just one single server: there is only one place for network requests to go. As I alluded to a moment ago, the only kind of load balancing that you can do is by running a local proxy and adding multiple separate processes as backends. The server itself is only going to have one IP address and one port where it can be reached, and there’s a certain amount of scale where that one IP and one port is going to become limiting. The good news is that you probably won’t hit that scale, and if you do, you’ll probably want to stop using SQLite anyway.&lt;/p&gt;
    &lt;p&gt;If you ever want to try switching towards or away from SQLite, the sequel gem has the amazing ability to read from one database and write into another, doing a full database copy while respecting all the quirks and limitations of each database. If you want to move from Mysql or Postgres over to SQLite, or if you ever want to load a SQLite database into Mysql or Postgres, I highly recommend it. The duckdb command line tool also has excellent cross-database capabilities, and is the next thing I would try if sequel wasn’t working for me for some reason.&lt;/p&gt;
    &lt;p&gt;There’s one more limitation that we need to consider that falls out of there only being one server: your app can only run in one geographic location. Some applications can benefit from adding additional web processes (or even database read replicas) spread out closer to end users, and that’s not possible if you are limited to a maximum of one server total for your entire application.&lt;/p&gt;
    &lt;p&gt;That said, there’s nothing stopping you from using the more usual kind of CDN-based global distribution. If your application has a decent amount of static or cacheable content, you can at least still set the cache-control headers and run the app behind Fastly or Cloudlfare.&lt;/p&gt;
    &lt;p&gt;Before we wrap up, I want to make sure to cover the various backup and replication options available to you while using SQLite for your application. The absolute all-star of SQLite backup and replication is called Litestream. It’s available as a gem, and can be used as easily as setting a few environment variables and using the &lt;code&gt;litestream&lt;/code&gt; command provided by the gem to wrap your puma or other web server.&lt;/p&gt;
    &lt;p&gt;What litestream does is fairly simple: it forwards a copy of each entry added to the write-ahead log over to any S3-compatible file store — you might even say that it streams your data in a light way. If you ever have a catastrophe, and your database file gets deleted or corrupted, the bucket will have a full copy of the WAL that you can replay to restore your database back to where it was when the server stopped working.&lt;/p&gt;
    &lt;p&gt;On AWS, this still means setting up an S3 bucket and setting the right env vars, but at least you don’t need to deal with the bucket having public access, or setting up signed uploads, or any of the other things that make S3 a huge pain. You just need a private bucket and read/write credentials, and you’re good to go. If you’re using fly.io, you don’t even have to set the env vars yourself! They are set automatically by the command that creates the S3-compatible bucket on Tigris.&lt;/p&gt;
    &lt;p&gt;There’s one last thing that you can try using if you’re feeling especially adventurous, LiteFS. LiteFS is a fascinating software achievement, offering full Mysql or Postgres-style replication for multiple SQLite databases running in many locations. The completely deranged trick that they use to do this is creating an entire software filesystem using FUSE, and then putting the SQLite database inside that filesystem. This gives them access to every filesystem read and write call made by your application, and allows them to create their own operations that are then sent to every other member of the cluster to be applied.&lt;/p&gt;
    &lt;p&gt;This kind of setup comes with a lot of caveats. The biggest one is the usual distributed systems kind of caveat. You’ll have stale reads where some users will see old data, and if the primary crashes you might lose some data. If you’re okay with the tradeoffs of a distributed system (and you’re okay with the idea of all of your database reads and writes going through a FUSE filesystem that might be adding extra bugs), LiteFS offers a version of the ultimate web application dream.&lt;/p&gt;
    &lt;p&gt;In the dream SQLite plus LiteFS world, you have all the advantages of SQLite and all the advantages of a fully replicated multi-writer database setup. Any individual server can go down without causing any downtime for the application as a whole, and every user has a full copy of the application and all its data, running extremely close to them.&lt;/p&gt;
    &lt;p&gt;I haven’t built that perfect system yet, but it feels more attainable than it ever has before thanks to SQLite.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45214933</guid></item><item><title>Launch HN: Ghostship (YC S25) – AI agents that find bugs in your web app</title><link>https://news.ycombinator.com/item?id=45215032</link><description>&lt;doc fingerprint="e7a19076850007b9"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hi HN, we're Jesse and Gautham. We're building Ghostship (&lt;/p&gt;https://tryghostship.dev/&lt;p&gt;).&lt;/p&gt;&lt;p&gt;Ghostship lets you find bugs in your web app by entering in your URL and describing a user journey.&lt;/p&gt;&lt;p&gt;Here's a video of Ghostship in action: https://www.loom.com/share/dec264ae32f94d50adb141c9246837c3?....&lt;/p&gt;&lt;p&gt;For over half our lives, we've been developers and we've done tons of user-facing projects like a coding competition I built called CerealCodes or freelancing projects on Upwork. The biggest problem we faced was that we shipped bugs in edge cases we didn't test, and the process of testing was annoying to do everytime we shipped a new feature. We tried automated testing tools, but those were flaky and couldn't adapt to feature changes. They also were really annoying to set up.&lt;/p&gt;&lt;p&gt;Our solution is to use browser agents to help you find bugs in your web app by clicking through your product like users would. You'd enter in your URL, describe what a user would do, and Ghostship would go through and try finding bugs by going through the user journey and extrapolating edge cases by visually seeing where else to click as it goes through each step in the user journey. We then show session replays of our agents going through your web app and list out all the steps it took.&lt;/p&gt;&lt;p&gt;We're able to find edge cases with almost no prompting. All you need to do is enter in one URL and one user journey (if you have login credentials on your web app, enter in some test credentials).&lt;/p&gt;&lt;p&gt;One bug we were able to find with Ghostship was on the YC application page. Apparently you could add your education dates in reverse chronological order (April 2022 to January 2021, which makes no sense).&lt;/p&gt;&lt;p&gt;Another bug we were able to find was a crypto smart contract CRM dashboard we vibe coded where we found a bug involving data corruption when you tried editing a draft contract multiple times.&lt;/p&gt;&lt;p&gt;You can sign up here: https://playground.tryghostship.dev/ for a limited number of credits. We'd love to hear from the HN community, whether you're building a web app for fun or a developer shipping a cool user-facing product to customers. We'd love to see what bugs we can find in your web app with Ghostship!&lt;/p&gt;&lt;p&gt;p.s. If you want Ghostship directly in your CI/CD pipeline and run after every PR, book a demo with us.&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45215032</guid></item><item><title>Randomly selecting points inside a triangle</title><link>https://www.johndcook.com/blog/2025/09/11/random-inside-triangle/</link><description>&lt;doc fingerprint="6bbdc096884507a0"&gt;
  &lt;main&gt;
    &lt;p&gt;If you have a triangle with vertices A, B, and C, how would you generate random points inside the triangle ABC?&lt;/p&gt;
    &lt;head rend="h2"&gt;Barycentric coordinates&lt;/head&gt;
    &lt;p&gt;One idea would be to use barycentric coordinates.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Generate random numbers α, β, and γ from the interval [0, 1].&lt;/item&gt;
      &lt;item&gt;Normalize the points to have sum 1 by dividing each by their sum.&lt;/item&gt;
      &lt;item&gt;Return αA + βB + γC.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This generates points inside the triangle, but not uniformly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Accept-reject&lt;/head&gt;
    &lt;p&gt;Another idea is to use an accept-reject method. Draw a rectangle around the triangle, generate points in the square, and throw them away if they land outside the triangle.&lt;/p&gt;
    &lt;p&gt;An advantage to this method is that it obviously works because it doesn’t rely on anything subtle. Three cheers for brute force!&lt;/p&gt;
    &lt;p&gt;The method is fairly efficient; on average only half the points will be rejected.&lt;/p&gt;
    &lt;p&gt;A disadvantage to all accept-reject methods is that they have variable runtime, though this only matters in some applications.&lt;/p&gt;
    &lt;head rend="h2"&gt;Accept-flip&lt;/head&gt;
    &lt;p&gt;There is a clever variation on the accept-reject method. Create a parallelogram by attaching a flipped copy of the triangle. Now randomly sample from the parallelogram. Every sample point will either land inside the original triangle or in its flipped twin. If it landed in the original triangle, keep it. If it landed in the twin, flip it back over and use that point.&lt;/p&gt;
    &lt;p&gt;This is like an accept-reject method, except there’s no waste. Every point is kept, possibly after flipping.&lt;/p&gt;
    &lt;p&gt;You can find code for implementing this method on Stack Overflow.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45215266</guid></item><item><title>C# Will Become F# – Gautier Talks About Microsoft Technology</title><link>https://gautiertalksmicrosoft.wordpress.com/2025/04/13/c-will-become-f/</link><description>&lt;doc fingerprint="6c581b3bdb3531d5"&gt;
  &lt;main&gt;
    &lt;p&gt;Art of clarity and the science of efficiency is what keeps programming both challenging and profoundly creative. Each year it seems, C# is progressively incorporating features and paradigms characteristic of F#, effectively becoming a nearly identical twin while preserving its foundational imperative capabilities. A significant strategic direction by Microsoft for the .NET platform underway since 2006.&lt;/p&gt;
    &lt;p&gt;This evolution is driven by factors aimed at enhancing the platform’s capabilities, broadening its appeal, and future-proofing its relevance. The shift from imperative to functional programming (FP) as developers progress through academia or gain industry experience reflects a confluence of intellectual maturation, practical necessity, and cognitive adaptation. This transition mirrors broader technological and societal trends, offering insights into how programming paradigms shape—and are shaped by—human cognition and civilization’s demands.&lt;/p&gt;
    &lt;p&gt;Microsoft is heavily involved in this transition and the long-term results are neither guaranteed and or predictably durable. Yet, they reflect a push for the state-of-the-art in software programming, development and engineering that will bolster efforts to produce more exotic underlying architectures. That is to say, much of this shift will be transparent to those that use .NET and Azure while at the same time impossibly obvious to those with long exposure to the software development practice.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rationale for C#’s Evolution from Microsoft’s Perspective&lt;/head&gt;
    &lt;p&gt;Microsoft’s gradual transformation of C#—infusing functional programming (FP) concepts while retaining its imperative core—reflects a calculated strategy to modernize the language, unify the .NET ecosystem, and align with broader industry trends. As systems scale to billions of nodes, the fundamental tooling and runtime needs a level of reliability, performance, and manageability that produces predictably higher quality. I conjecture that at the core, Microsoft .NET is founded on functional programming principles and is continuously engineered in that direction transparent to the software developers that use it. A few of the points for this are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Harnessing Functional Programming Strengths: Functional programming (FP) offers compelling advantages for modern software challenges, including inherent support for immutability, fewer side effects, easier reasoning about concurrency/parallelism, and often more concise, declarative code. By integrating FP features (like pattern matching, records, improved LINQ, immutability support) directly into C#, Microsoft allows developers to leverage these benefits without leaving their primary language.&lt;/item&gt;
      &lt;item&gt;Addressing Modern Development Demands: The rise of cloud computing, distributed systems, big data processing, AI/ML workloads, and highly asynchronous applications (especially in web development ) naturally aligns with the strengths of functional programming. These domains benefit from FP’s handling of state, concurrency, and data transformation. Evolving C# makes it a more capable tool for these critical, growing areas within the Azure ecosystem and beyond.&lt;/item&gt;
      &lt;item&gt;Lowering Adoption Barriers: While F# is a powerful language, its purely functional nature can present a steeper learning curve for the vast majority of developers primarily experienced in imperative/object-oriented paradigms. Gradually introducing functional concepts into C# provides a smoother transition path. Developers can adopt these features incrementally within familiar syntax and tooling, making functional techniques more accessible.&lt;/item&gt;
      &lt;item&gt;Ecosystem Cohesion and Tooling: Integrating F#-like capabilities enhances the interoperability and shared infrastructure within the .NET ecosystem. It allows libraries and patterns to be shared more easily and enables tooling (like Visual Studio and the .NET CLI) to provide a more unified experience across different programming styles within C#.&lt;/item&gt;
      &lt;item&gt;Competitive Landscape: Other programming languages and platforms are also evolving. Incorporating modern programming paradigms ensures C# remains competitive, attractive, and productive compared to alternatives like Java, Python, Go, Rust, or JavaScript/TypeScript, which are also adopting functional features or have strong functional ecosystems.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Benefits for Microsoft&lt;/p&gt;
    &lt;p&gt;The software industry increasingly values FP for its conciseness, safety (immutability, reduced side effects), and suitability for concurrency and data-centric applications. Microsoft ensures C# remains competitive with languages like Kotlin, Swift, and Rust, which blend imperative and functional paradigms by integrating FP features like:&lt;/p&gt;
    &lt;head&gt;Simplified Lambda Calculus inspired approaches and techniques to include:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Expanded symbols in the C# language that allow &lt;list rend="ul"&gt;&lt;item&gt;Pattern Matching&lt;/item&gt;&lt;item&gt;Records and Immutable Types&lt;/item&gt;&lt;item&gt;LINQ (a declarative, SQL-like syntax)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Reliable concurrency and parallelism that is approachable &lt;list rend="ul"&gt;&lt;item&gt;Async/Await (rooted in monadic concepts)&lt;/item&gt;&lt;item&gt;MPI style programming&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To go further, implementation decisions along these lines significantly mirror the explicit and latent abilities of languages like F# and lead to more robust code. Additional benefits to Microsoft include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Strengthened .NET Platform: Languages like TypeScript, Rust, and Go have raised expectations for expressive, type-safe, and concurrent programming. A more versatile and powerful C# reinforces the entire .NET ecosystem, making it more attractive for a wider range of applications and retaining its large existing developer base.&lt;/item&gt;
      &lt;item&gt;Enhanced Azure Proposition: A C# better suited for cloud-native, data-intensive, and concurrent applications directly benefits Microsoft’s Azure cloud platform, encouraging its adoption. FP’s statelessness and composability align with cloud-native architectures, a key focus for Microsoft Azure.&lt;/item&gt;
      &lt;item&gt;Unified Development Story: It simplifies the .NET narrative by positioning C# as a multi-paradigm language capable of handling diverse challenges, reducing the perceived need for developers to look outside the ecosystem for certain tasks. Smooth integration between C# and F# enhances code sharing and library reuse within .NET. For example, F#’s succinct data pipelines can complement C#’s performance-critical sections.&lt;/item&gt;
      &lt;item&gt;Future-Proofing: C#’s rapid adoption of FP features contrasts with Java’s cautious pace, attracting developers seeking modernity. By embracing proven paradigms like functional programming, Microsoft ensures the longevity and adaptability of C# and .NET in the face of evolving software development challenges.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Impact on the Developer Community&lt;/head&gt;
    &lt;p&gt;Modern .NET runtimes (CoreCLR, AOT compilation) optimize FP patterns like immutability and pattern matching, enabling high-performance code without sacrificing readability. Modern .NET runtimes (CoreCLR, AOT compilation) optimize FP patterns like immutability and pattern matching, enabling high-performance code without sacrificing readability.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Span&amp;lt;T&amp;gt; and Memory&amp;lt;T&amp;gt;: Enable low-allocation, high-speed data processing, even in FP-style code.&lt;/item&gt;
      &lt;item&gt;Source Generators: Automate boilerplate in FP-heavy code (e.g., serialization), reducing verbosity.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Beyond this, software developers have to make decisions beneficial to their careers and ability to work on important projects in the future. I asked someone recently what they thought about different technologies. This was a very senior technologist at a nearly executive level. What they said of Microsoft was the technologies are very corporate. If anyone watched the YouTube channel, LogicallyAnswered, there was an episode that skillfully described Microsoft’s stronger embrace of enterprise technology over consumer. Sure, you can still see Microsoft in retail stores, but the bulk of what they do is with very large corporations. Developers have to take that into account as they consider the following in the context of the community of software developers both strongly aligned to Microsoft and those who are more agnostic.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Increased Power and Expressiveness: FP-curious developers from JavaScript, Python, or Java may gravitate to C# for its balanced paradigm support. Functional constructs simplify data transformation pipelines, a growing need as .NET expands into ML/AI (e.g., ML.NET). C# developers gain access to powerful functional constructs, enabling more concise, expressive, and potentially more robust code for complex tasks.&lt;/item&gt;
      &lt;item&gt;Easier Transition to Functional Concepts: Developers hesitant to adopt F#’s pure FP syntax can now leverage FP benefits within familiar C# syntax, reducing migration friction. The gradual integration provides an accessible pathway for imperative programmers to learn and apply functional principles without switching languages entirely.&lt;/item&gt;
      &lt;item&gt;Potential for Improved Code Quality: Features like immutability and pure functions can lead to code that is easier to reason about, test, and maintain, especially in concurrent scenarios. Functional programming advocates may view C#’s hybrid approach as a “half-measure,” preferring dedicated FP languages like F# or Haskell.&lt;/item&gt;
      &lt;item&gt;Language Complexity: C# has grown far larger, and more complex a language since 2002 versus a language like C that managed to remain simple in specification but able to build everything from scratch. With C#, it is mandatory to master a wider array of concepts and choose imperative, object-oriented, and functional approaches all at once as this is what the market requires.&lt;/item&gt;
      &lt;item&gt;Learning Curve: While smoother than switching to F#, adopting and effectively utilizing the new functional features still requires learning a larger mindset shift for developers accustomed solely to imperative/OO styles. Novice developers may struggle with hybrid code bases mixing OOP, imperative, and FP styles.&lt;/item&gt;
      &lt;item&gt;Potential for Style Inconsistency: Teams may face challenges in maintaining a consistent coding style if different developers mix paradigms differently within the same code base without clear guidelines. Enterprises with large imperative code bases might resist refactoring to FP patterns, leading to fragmented practices.&lt;/item&gt;
      &lt;item&gt;Performance Considerations: Developers can write high-level FP code without sacrificing low-level control (e.g., &lt;code&gt;ref struct&lt;/code&gt;optimizations). While functional styles can offer optimization potential, developers must still understand performance trade-offs. As noted previously, highly optimized code might still require more verbose, explicit imperative techniques in some cases.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Realities to Observe and Mitigate&lt;/head&gt;
    &lt;p&gt;Let it be known far and wide that I think .NET is great. I love the technology and the language of C# and plan on using it indefinitely insofar as it always works well on Linux. Microsoft makes many programs and my absolute favorite is .NET and I can hold it in exclusion. What that means is, I can be an open source and Linux advocate and simultaneously appreciate and use .NET and C# because they run on Linux and does so without the issue of being limited to Microsoft Windows.&lt;/p&gt;
    &lt;p&gt;While it is true that I am a huge fan of Linux and use it as my operating system, I arrived at that point when I saw that Microsoft Windows did not live up to my expectations. Linux runs better and that turned out to be a great thing for Microsoft since Azure is built on Linux. Starting around 2016 and at the turn of 2018, Microsoft embraced the Linux operating system for many of their solutions and are having a better time with that from a business standpoint. The Linux shift has been good to and for them.&lt;/p&gt;
    &lt;p&gt;Yet, the historical concerns about how Microsoft relates to technologies in general and developer communities in particular remain and should be approach carefully. What that means is some technologies you like may wither away. A good example is Microsoft Silverlight. Now, although that did happen, due to the nature of open source, Silverlight is coming back in the form of OpenSilver. I thought that was great. Still, if you are attempting to align with Microsoft’s roadmap and active support on technologies, you have to learn the fine art of adaptability in a way that does not cause you to revert back to zero. This means knowing the reality of the following:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Ecosystem Boundaries: While a historical benefit of Microsoft was the optimization of running everything on Windows and using all Microsoft technology, today’s world is quite different. The risk is by keeping C# developers within .NET, Microsoft retains control over tooling (Visual Studio, VS Code), cloud services (Azure), and frameworks (ASP.NET, MAUI). That could limit the long-term career options for those developers should customers suddenly shift platforms.&lt;/item&gt;
      &lt;item&gt;Cloud Revenue Growth: FP’s suitability for scalable, distributed systems dovetails with Azure’s microservices and serverless offerings. This is actually great for as long as it works. However, and again, if companies decide to reduce cloud investment, the migration could favor solutions unlike .NET and C#. Or, you may still see .NET/C# hold relevance if you plan upfront.&lt;/item&gt;
      &lt;item&gt;Developer Mindshare: A modern, versatile C# attracts startups, enterprises, and open-source contributors, bolstering Microsoft’s relevance in a multi-language world. Imperative-focused developers can adopt FP incrementally, avoiding the steep learning curve of pure FP languages. Enhanced Productivity: Features like records (&lt;code&gt;public record Person(string Name);&lt;/code&gt;) and pattern matching streamline code, reducing boilerplate. At the same time, many of these features may encourage a dependency on less mature visual tools and processes that compete less well against those optimized for specific workloads.&lt;/item&gt;
      &lt;item&gt;Cross-Platform Unification: .NET’s push into Linux, macOS, and mobile (via MAUI) benefits from FP’s portability and concise syntax. At the same time, it is obvious that Microsoft is trying to discover the right technology for the moment. ASP.NET is going through multiple shifts because a not-insignificant number of web developers at some point got into Node.js, Ruby on Rails, or PHP Laravel the style of which doesn’t fit into they way things have been done in ASP.NET. MAUI, despite the name, doesn’t work everywhere and not as well as its competitors, thus you have to plan.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Bigger Picture: A Multi-Paradigm Future&lt;/head&gt;
    &lt;p&gt;Microsoft’s strategy of evolving C# to incorporate F#-inspired functional capabilities is a pragmatic approach driven by the demands of modern software development and the desire to strengthen the .NET ecosystem. It aims to provide C# developers with the benefits of functional programming—concurrency, immutability, expressiveness—within a familiar environment, lowering adoption barriers while keeping the language competitive and aligned with cloud-centric trends.&lt;/p&gt;
    &lt;p&gt;While this increases the language’s complexity, the net impact on the developer community is likely positive, empowering them with more versatile tools. However, it also necessitates continuous learning and careful consideration of how best to blend different programming paradigms effectively within projects. This evolution reflects a broader industry trend towards multi-paradigm languages capable of tackling diverse and complex software challenges.&lt;/p&gt;
    &lt;p&gt;Microsoft’s strategy mirrors a broader industry shift toward pragmatic multi-paradigm design. Languages like C# are becoming “Swiss Army knives,” allowing developers to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Choose the Right Tool: Use imperative code for UI logic, FP for data pipelines, and OOP for domain modeling.&lt;/item&gt;
      &lt;item&gt;Gradually Upskill: Teams can adopt FP concepts at their own pace, avoiding radical rewrites.&lt;/item&gt;
      &lt;item&gt;Leverage AI and Tooling: GitHub Copilot and Roslyn analyzers assist in writing idiomatic FP-style code, easing the transition.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Conclusion: Why This Evolution Matters&lt;/head&gt;
    &lt;p&gt;Microsoft’s fusion of F# concepts into C# is not about replacing one paradigm with another—it’s about future-proofing. By embracing FP, C# positions itself as:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Smoother gateway to advanced programming concepts, nurturing developers’ growth.&lt;/item&gt;
      &lt;item&gt;Refined bridge between legacy systems and modern architectural demands (cloud, parallelism, AI).&lt;/item&gt;
      &lt;item&gt;Testament to the .NET ecosystem’s adaptability, ensuring its relevance for decades.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For developers, this evolution democratizes functional programming, offering its benefits without demanding full paradigm allegiance. For Microsoft, it’s a strategic masterstroke that tightens its grip on enterprise development while appealing to the next generation of coders.&lt;/p&gt;
    &lt;p&gt;The result?&lt;/p&gt;
    &lt;p&gt;A richer, more versatile C# that reflects the nuanced reality of software engineering: there’s no one right way—only the right tool for the problem at hand.&lt;/p&gt;
    &lt;p&gt;Except … that is 1,000,000% absolutely false! Never believe that lie because it helps a person escape responsibility. While it is true that what is the right overall design “depends” on the situation. When we look at total truth, the answer changes. Total truth in the case of computers is based on physical reality. A computer is a physical device that processes information in accordance with the reliably predictable rules of electronics, physics, and energy.&lt;/p&gt;
    &lt;p&gt;The closer you get to that reality, the more absolute and purely unambiguous the requirements become. What FP and abstractions do is allow the subjective nature of human preference work ever more delicately with the ultimately strict definitions of machines. At the boundary of that division are languages like Rust and C as well as hardware assembly code to which all these concepts must express into and obey.&lt;/p&gt;
    &lt;p&gt;Therefore, while C++ may not be faster than C#. The truth is, the best written C code will fundamentally outshine both at the cost of developer time, effort, and sustainability. What high-level mechanisms like FP languages in the form of F#, Haskell and Lisp and their carefully calibrated contemporaries such as C#, Kotlin, and Python allow is a better trade off in favor of human stamina, emphasis, and preference where most of the properties of the best can be achieved over time.&lt;/p&gt;
    &lt;p&gt;As a final thought to recursively illustrate this. The very best artist can utilize sound waves and directly manipulate energy to build one of the Pyramids at Giza by hand. However, the most efficient way for someone without that ability or time to master those techniques is to draw the design of a pyramid as an artistic rendering on paper or a computer screen and have 24/7, nonstop nuclear/solar powered robots build it. Just like AI generated art in 2024, the result may not have all the finest trimmings and cultural easter eggs embedded in the final result, nor necessarily lasts as long, but it still gets the job done.&lt;/p&gt;
    &lt;p&gt;Content for thought:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45215534</guid></item></channel></rss>