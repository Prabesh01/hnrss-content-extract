<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 07 Dec 2025 00:56:51 +0000</lastBuildDate><item><title>Touching the Elephant – TPUs</title><link>https://considerthebulldog.com/tte-tpu/</link><description>&lt;doc fingerprint="bfb8d835cc3501c8"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Touching the Elephant - TPUs&lt;/head&gt;&lt;head rend="h4"&gt;Understanding the Tensor Processing Unit&lt;/head&gt;&lt;head rend="h2"&gt;Something New&lt;/head&gt;&lt;p&gt;There is mythological reverence for Google’s Tensor Processing Unit. While the world presently watches NVIDIA’s gravity drag more companies into its orbit, there sits Google, imperial and singular. Lots of companies participate in the “Cambrian-style explosion of new-interesting accelerators”[14] – Groq, Amazon, and Tenstorrent come to mind – but the TPU is the original existence proof. NVIDIA should take credit for the reemergence of deep learning, but the GPU wasn’t designed with deep learning in mind. What’s strange is that the TPU isn’t a secret. This research is indebted to Google’s public chest-thumping, but the devices themselves have long been exclusive to Google’s datacenters. That is over a decade of work on a hardware system sequestered behind their walls. That the TPU is so well documented yet without a true counterpart creates a strange asymmetry. Google is well positioned in the AI race because of their decision over a decade ago to build a hardware accelerator. It is because of the TPU.&lt;/p&gt;&lt;p&gt;On the back of DistBelief Google had gotten neural networks running at scale. In 2013 however they realized that they would need to double their datacenter capacity to meet the growing demand for these new services. “Even if this was economically reasonable, it would still take significant time, as it would involve pouring concrete, striking arrangements for windmill farm contracts, ordering and installing lots of computers, etc.” [14] The race against the clock began, and 15 months later the TPU was born. Fast forward to April of this year when Sundar Pichai announced the 7th generation TPU, Ironwood, at Google Cloud Next. The headline figures were eye-popping. 9,216 chips in a pod, 42.5 Exaflops, 10 MW [21]. In 12 years the TPU went from a research project to a goliath rack-scale system.&lt;/p&gt;&lt;p&gt;Perhaps reverence is warranted. The development of the TPU is set against the backdrop of a changing hardware scaling landscape. It used to be that to get better programs you just had to wait. With each new generation of chip Moore’s Law and Dennard Scaling brought enormous tailwinds in transistor density, power efficiency, and wall clock improvements. But in the aughts and 2010s there was no more sitting and no more waiting. The advancements in chip physics were not producing exponential returns as they once had, and workload demands continued growing.&lt;/p&gt;&lt;p&gt;Casting this as mythology however obscures the details and risks making the TPU seem like magic. The development of the TPU is the story of trade-offs and constraints and co-design. It touches hardware, software, algorithms, systems, network topology, and everything in between. It did not happen by accident, but through the deliberate process of design and iteration. When thinking about the TPU it’s natural to ask:&lt;/p&gt;&lt;p&gt;How did we get here?&lt;/p&gt;&lt;head rend="h2"&gt;Slowing Down&lt;/head&gt;&lt;p&gt;For decades the industry relied on Moore’s Law to pack more transistors into a smaller area and on Dennard Scaling to get more energy efficiency from those transistors. This netted out to smaller, faster, and more efficient devices. You didn’t need to change your software or architecture to realize significant gains, regardless of the domain. CPU performance doubled every 1.5 years from 1985-2003, and every 2 years from 2003-2010. The doubling speed since is closer to every 20 years [14]. The AlexNet moment in 2012 charted a course to the current renaissance in neural networks. Different hardware suddenly opened the door for new questions to be asked. The range of problems that neural networks were suited to solve, along with their appetite for bigger data and bigger models, meant that this algorithmic paradigm was taking off as our scaling paradigms began to languish.&lt;/p&gt;&lt;p&gt;Degradation in the reliability of chip performance scaling under different regimes [14]&lt;/p&gt;&lt;p&gt;The TPU falls into the broad classification of hardware accelerators, of which the marquee distinction is that it is specialized for certain computational domains, hence the name Domain Specific Accelerator. Whereas general purpose devices are designed to accommodate the maximum number of program shapes, specialized designs are defined as much by what they can do as what they can’t. They trade off generality for performance. If we can’t rely on Moore’s Law and Dennard Scaling, and there are new workloads demanding attention, the goal is to optimize for the characteristics of those workloads and to discard everything else. Specialization asks what the optimal way to spend a fixed transistor and energy budget is to squeeze out performance.&lt;/p&gt;&lt;p&gt;Linear algebra is ripe for specialization because a relatively small set of parallelizable operations dominate neural networks. For the TPU that meant a monastic focus on those primitives. Neural networks are simple compositions of Matrix-Vector, Matrix-Matrix, and Elementwise computations over large tensors. Consider that matrix multiplication has cubic complexity. While computationally expensive, this one class of operations is the spine for a large fraction of what is required for a neural network. This narrows the window of optimizations that need to be baked into silicon. Matrix multiplies have the property that as the size of inputs grow, the ratio of compute, O(n^3), to data access, O(n^2), improves [15]. If you can dedicate hardware to speeding up arithmetic and coordinating data movement you can exploit this, and the arithmetic properties are complemented by the runtime properties. Neural networks can be fully specified ahead of time. With clever planning a program can be entirely mapped out before an instruction is issued. There was rarely a need before to design, tape out, and deploy custom silicon. Free performance gains made the economics of simply waiting versus the cost of designing an ASIC a non-starter. The decline of hardware scaling made exploring these realities attractive.&lt;/p&gt;&lt;p&gt;Horowitz Energy per Operation [11]&lt;/p&gt;&lt;p&gt;This opportunity is best exploited in the power budget. Compare the relative cost of arithmetic to control, memory access, and data movement. Horowitz [11] notes that over 50% of processor die energy is dissipated in caches and register files. These inefficiencies exist to mitigate the even greater inefficiency of large memory accesses. In [12] they cite that the energy to fetch and interpret instructions is 10-4000x more expensive than to perform simple operations. Moving and accessing data costs significantly more power, and what is required of deep learning is more arithmetic per unit control. Finding ways to circumvent relative power inefficiencies with specialization means rearchitecting chips to remove that waste.&lt;/p&gt;&lt;head rend="h2"&gt;The Inference Chip&lt;/head&gt;&lt;p&gt;Block diagram of TPUv1 [1]&lt;/p&gt;&lt;p&gt;Datacenter expansions plans are a hell of a drug. To stem the tide of models devouring datacenter capacity, the first ASIC needed to focus on inference. Inference only needs a forward pass through the neural network. A simple neural network layer might look like this:&lt;/p&gt;&lt;p&gt;$$ ReLU( (X \cdot W) + b ) $$&lt;/p&gt;&lt;p&gt;Where X and W are input data and model weights, ReLU is a non-linear activation function, and b is a bias term. A matrix multiply followed by some elementwise addition and an elementwise maximum function. Imagine that chaining a handful of these layers together forms the totality of an inference. This simplified view on early model architectures gives us the general template for designing TPUv1. Matrix multiply, some activation looking functions on that result, feed the results to storage, repeat. To meet the initial deadlines the TPU design exploited this loop-like behavior.&lt;/p&gt;&lt;p&gt;TPUv1 is a single-threaded co-processor connected over PCIe with a 24MiB software-controlled Unified Buffer, an 8-bit integer systolic array, and 8GiB DDR3 DRAM. The device runtime lays out tensors, plans memory transfers with a programmable DMA controller between the host and the Unified Buffer (on-chip SRAM), and tiles compute operands. The host sends 12-bit CISC instructions to the device’s instruction buffer which the in-order sequencer consumes to move data to DRAM and issue MXU ops. The datapath consumes ~2/3 of the die area of the chip [1]. Take care to notice what it is not. It is not a multi-level cache hierarchy. There is no multi-threading or branch prediction or prefetching or TLB. The systolic array executes arithmetic and the runtime eliminates control overhead. TPUv1 is a spartan device aimed at making inference fast.&lt;/p&gt;&lt;p&gt;The heart of the device is the Matrix Multiplication Unit (MXU). It is a 256x256, 2D weight-stationary systolic array of processing elements, in this case MACs. The MXU targets dense GEMMs to maximize arithmetic intensity. The TPU is designed to keep the MXU busy. You can find nice animated demonstrations of data moving through the systolic array here or here.&lt;/p&gt;&lt;p&gt;MXU Cycle Timing&lt;/p&gt;&lt;p&gt;We’ll start with a simplified 4x4 systolic array. Although there are design variations of systolic execution [18][36], we are concerned with the 2D weight-stationary variant. The weights are pre-loaded into the array from the right hand side (the top in this diagram), and the inputs stream in from the left hand side (conveniently on the left). Once the weights are loaded they sit resident in the MACs, one weight per MAC. As the inputs flow from left to right, the MACs compute the product of the resident weight and the streamed input each cycle. The result of that computation is passed downward to the next processing element. If a MAC has one of these partial sums, it adds it to the result of the weight/input product and passes that new sum downward. At the bottom edge of the array there are no more computations and the result is passed to a 4096 row x 256-element bank of 32-bit accumulators.&lt;/p&gt;&lt;p&gt;MXU Double Buffering&lt;/p&gt;&lt;p&gt;Notice that weight pre-loading doesn’t happen all at once. It would waste cycles to wait for each MAC to have a resident weight before streaming in inputs. Weight pre-loading instead happens diagonally, with the left-most part of the systolic array receiving weights first. When the left column of processing elements has weights, the inputs begin streaming diagonally top to bottom. This imposes significant timing coordination for such a simple component. Much of the rest of the chips’ design can be thought of as accommodating these timing needs, and a particular instantiation of that is the liberal use of double buffering.&lt;/p&gt;&lt;p&gt;MXUs can perform immense amounts of arithmetic, but data movement/control stops at the edges of the systolic array. Between processing elements there is only result-passing with chains of two-input adders. If either weight or input data is not where it needs to be, stalls burn cycles that hurt MXU utilization. Spelling it out:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The MXU holds two 64KiB tiles of weights with one reserved for double buffering&lt;/item&gt;&lt;item&gt;Four 64KiB weight tiles act as a FIFO queue to decouple memory accesses and weight loads between DRAM and the MXU&lt;/item&gt;&lt;item&gt;The Unified Buffer stores intermediate results from the accumulators and prepares new data to feed to the systolic array&lt;/item&gt;&lt;item&gt;The bank of accumulators logically splits 4096 rows into two chunks of 2048 rows, one to feed outputs and one to drain them&lt;/item&gt;&lt;/list&gt;&lt;head&gt;Sizing the MXU&lt;/head&gt;The number of processing elements that touch data before it reaches the accumulators grows quadratically with the array size which affects the speed of the computation. For a 256x256 array that is 65,536 MACs vs. 262,144 MACs in the 512x512 configuration. During fill/drain you pay an O(num_edges) cost to populate the buffers. Fewer edges better amortize this overhead. As arrays shrink they are penalized by wiring constraints. They perform less compute per data access and require running many wires between components. Sizing this device is a delicate balance between compute intensity and layout constraints, which we will see again in later generations.&lt;p&gt;The runtime knows how long each operation it issues should take, so it can intelligently overlap them with one another. During matrix multiplications the UB prepares the next batch of inputs, the fixed activation units operate on the results in the accumulators, and the Weight FIFO banks more weights. Matrix multiplies are relatively long latency, which leaves lots of cycles between when work starts and when work ends. The runtime schedules memory accesses, data movement and computation deterministically to minimize stop-the-world pauses rather than make coordination dependent on the MXU. Hiding latency with overlapping improves parallelism, improves data reuse, and conserves energy otherwise wasted in control flow.&lt;/p&gt;&lt;p&gt;The headline figures from their paper are anachronistic by now, but they help contextualize the accomplishment of the first gen chip. 25x as many MACs and 3.5x the on-chip memory of the K80 GPU. 15-30x the inference speed and 30-80x the perf/W of the K80 and the Haswell CPU [1]. The fixed-latency, software-managed design created a hardware accelerator that eschewed prevailing designs that spent energy in cache hierarchies and control overhead. Maniacal focus on mitigating inference bottlenecks with large SRAM and coordinated data movement proved that TPUv1 worked.&lt;/p&gt;&lt;head rend="h2"&gt;The Training Chip&lt;/head&gt;&lt;p&gt;Neural networks need to be trained before they can be used for inference, and TPUv1 was not designed for training. Requirements include backpropagation to modify weights during execution, gradients with higher precision than int8, and support for diverse activation functions. This costs orders of magnitude more FLOPs [2], and those FLOPs must be distributed over multiple devices while maintaining deterministic execution. TPUv1’s fixed activation units were not flexible enough for experimenting with new algorithms. The memory subsystem was not flexible enough to coordinate work between multiple devices. The UB was not flexible enough to tuck more Matrix-Vector work in behind the MXU. The whole device was too tightly coupled. Adding that flexibility, without reverting to a general-purpose processor, needed a radically different datapath.&lt;/p&gt;&lt;p&gt;TPUv2 Block Diagram [2]&lt;/p&gt;&lt;p&gt;TPUv2 was animated from the bones of TPUv1, but only the MXU feels familiar. TPUv2 is a dual-core chip. Each core pairs a scalar controller with programmable vector units, local SRAM, a 128x128 MXU, and HBM. It adds inter-core interconnects (ICI) to communicate between the memory systems of each core and across chips. Two 128x128 MXUs combine to total the same 256x256 array from TPUv1 but simplify the circuit design. Unequal logic, wire, and SRAM scaling on smaller process nodes made arithmetic improvements comparatively free, enabling the chip designers to focus on the laggard scaling axes [2]. For the second generation MXUs that meant two efficiencies over their predecessor: BrainFloat16 and wire routing.&lt;/p&gt;&lt;p&gt;BF16 floating point format [3]&lt;/p&gt;&lt;p&gt;Dynamic range matters more than precision for neural network training. Gradients represented as integers don’t produce adequate convergence behavior; you need floating point numbers to make fine-grained weight updates. Accessing higher precision numerics however means sacrificing die area. Logic circuits need more adders to handle mantissa bits. Floating point adder arrays scale as (M+1) * (M+1), where M is the size of the mantissa, – 576 adders for fp32 and 121 adders for fp16 [14] – totalling more die area and more energy spent on arithmetic. Notice that although bf16 is the same number of bits as fp16, the proportion of exponent bits to mantissa bits is higher. bf16 only requires 64 adders in the MAC circuitry, and less circuitry means more MACs in the same package and power budget [2][14].&lt;/p&gt;&lt;p&gt;MXU Sizing Considerations [32]&lt;/p&gt;&lt;p&gt;Chip geometry considerations extend beyond individual processing elements. Big cores need long, global wires routed to/from functional units, FIFOs, and control units. Though wire diameters shrink on improved process nodes, their resistance and capacitance scale unevenly. Long wires are chunked into shorter segments connected with repeaters, but this induces signal delay making circuit timings more complex [5]. MXU configurations with multiple smaller cores shorten average wire lengths but need wires routed all over the chip. The trade off is between compute bandwidth and array utilization. Compute utilization scales down quadratically with the array area, but smaller arrays use more energy-efficient wires. Splitting the die into two cores and running fewer, shorter wires to the vector and control units balances wiring scaling with utilization.&lt;/p&gt;&lt;p&gt;TPU Scalar Unit [32]&lt;/p&gt;&lt;p&gt;All those wires have to lead to somewhere. To drive the new datapath, TPUv2 introduces the scalar unit. When a user submits a program, the XLA compiler performs static analysis, lowering the program into 322-bit VLIW instruction bundles. XLA schedules DMAs, vector ops, and MXU work in a deterministic stream. The complexity of organizing program control flow is absorbed by software, keeping the scalar unit relatively simple. It is single-threaded and contains 4KB of scratchpad SRAM (SMEM), small instruction memory (IMEM), and a 32 element, 32-bit register file (SReg) connected to a dual-issue ALU. Sync registers flag when arithmetic and memory blocks are busy to explicitly synchronize execution. The host sends instructions over PCIe to HBM, where they are DMA’d into the Scalar Unit’s IMEM as overlays. Scalar instruction slots execute locally, and the vector/matrix slots are decoded and dispatched to the VPU/MXU [3]. There is no dynamic runtime scheduling, just instruction fetch, decode, and forward.&lt;/p&gt;&lt;p&gt;Two programmable vector processing units (VPU) consolidate the fixed function blocks from TPUv1. The VPU is a 2D SIMD processor designed to increase the ratio of vector operations to matrix operations. Each VPU has 128 vector lanes with 8 sublanes. Each sublane is connected to 32 dual-issue ALUs with lane-local register files (Vregs). The VPU is backed by 16MiB on-chip Vector Memory (VMEM) that mediates data movement to the MXU with pushes/pops onto a Result FIFO [3]. Each core’s VMEM has local access to half of the chip’s HBM, and DMAs to VMEM are strided to fetch contiguous tiles of data rather than issuing many small DMAs. The VPU accesses VMEM with explicit loads/stores to Vregs which remove the need for a cache hierarchy.&lt;/p&gt;&lt;p&gt;The simplicity of describing the rearchitected datapath belies the complexity that the subsystems represent. Whereas general purpose devices use branch predictors, TLBs, Out of Order execution, and a bevy of techniques to shuttle data and instructions, the TPU routes around a cache-centric design with software-managed execution. The aforementioned general purpose mechanisms alleviate runtime dependencies at the expense of more hardware and more energy. Control and caches consume massive amounts of the limited energy budget, so redesigning this subsystem is the difference between an economic chip and a renegotiated contract with power providers. When you know what operations you need, the order you need them in, and the operational characteristics of the hardware, you can move control flow to compile time. The VPU and Scalar Units are co-designed to leverage this operating paradigm, moving program orchestration to software.&lt;/p&gt;&lt;p&gt;Sample VLIW Instructions&lt;/p&gt;&lt;p&gt;VLIW instructions expose this complexity. They contain slots for 2 scalar, 4 vector, 2 matrix, 1 miscellaneous, and 6 immediate instructions [3]. Slots map to scalar/vector/matrix arithmetic, loads and stores, DMAs, synchronization flags, and data literals. Though innocuously named, the miscellaneous slot controls heaven and earth. It is reserved for kernel launches, DMAs, and synchronization guards which we can think of as WAITs. Data dependencies must be carefully sequenced to ensure operation A finishes before operation B uses its results. XLA utilizes the misc slot to keep subsystems working while guarding against illegal instruction sequences. Operational latencies are known constants at compile time, and XLA can use those values to place WAIT instructions at exactly the right point in the VLIW stream to minimize stalls.&lt;/p&gt;&lt;p&gt;Simplified TPU Instruction Overlay&lt;/p&gt;&lt;p&gt;Subsystems operate with different latencies: scalar arithmetic might take single digit cycles, vector arithmetic 10s, and matrix multiplies 100s. DMAs, VMEM loads/stores, FIFO buffer fill/drain, etc. all must be coordinated with precise timing. The MXU might be busy executing a matrix multiply for 128 cycles, meanwhile the VPU is preparing the next tile of weights for the Result FIFO. While DMAs prepare new data for VMEM a DMA_OVERLAY instruction gets inserted to fetch new instructions for IMEM. When the MXU finishes a tile, the hardware sends a signal to clear the MXU_BUSY bit in the scalar unit’s sync registers. When the scalar unit evaluates a WAIT_MXU instruction it sees that the bit is unset and hops to the next instruction for decoding. The scalar unit JUMPs to the new VLIW bundle region and the program continues. Seamlessly overlapping the work of an arbitrary DAG requires extraordinary co-design between the device and the software.&lt;/p&gt;&lt;p&gt;Decoupling the hardware gave software the capacity to drive massive data and instruction level parallelism. VLIW slots can launch 8 operations per cycle. That is 2048 vector ALUs and two 128x128 systolic arrays with minimal control overhead. HBM, VMEM, Vregs, and the MXU all remain busy with the same pipelining and overlap philosophy from TPUv1, only now massively scaled up. XLA wrests power away from control and back into the arithmetic units with coordinated, deterministic execution. Determinism across devices requires explicit communication between chips.&lt;/p&gt;&lt;p&gt;ICI forms the backbone of the training pods. It creates a coherent communication fabric that lets chips operate locally while composing into a mesh of devices acting as one large core. Two on-chip ICI links route data between the HBM and VMEM of each core. Four 496Gbit/s bidirectional off-chip links connect a TPU to its neighbors in the rack with OSFP passive copper. RDMAs over this fabric let chips treat remote HBM as explicitly addressable endpoints. Racks arrange 256 chips as a 16x16 2D torus over ICI to form the full supercomputer pod. ICI removes frequent host communication, skipping the cost of network cards, switches, and communication delays. All this sacrifices 13% of the die area for gains in distributing computations [3].&lt;/p&gt;&lt;p&gt;One dimensional torus wraparound&lt;/p&gt;&lt;p&gt;Let’s imagine that we’re playing a game of telephone. You and 8 friends are arranged in a 3x3 grid, and you can only communicate with your adjacent neighbors. Your goal is to send a message from the person at (0,0) to the person at (2,2) in the fewest messages. Many paths achieve this, but the shortest one is always four. Now imagine that the people on the left edge of the grid can wrap messages around to people on the right edge of the grid. This is logically like mirroring you and all your friends over that wraparound axis. These 3 new connections make our shortest path 3 instead of 4.&lt;/p&gt;&lt;p&gt;Logical mirroring in two dimensional torus wraparound, adapted from [5]&lt;/p&gt;&lt;p&gt;ICI plays this game of telephone in two dimensions. During backpropagation and optimizer state updates intermediate values accumulate across different partitions of the model located on different chips. Results must be broadcast to all the chips participating in the computation for synchronization. Whereas on-chip work is explicitly synchronized with hardware flags, work across chips is implicitly synchronized with MPI-style collectives (All-to-All, AllReduce, etc.). Torus topologies improve communication bandwidth and increase access to different communication patterns during synchronization.&lt;/p&gt;&lt;p&gt;32 wraparound links at 496Gbit/s enable 15.9Tbit/s of bisection bandwidth [3], which tells us how much data can move through the network. In a 4x4 array, a cut down the middle would sever 4 connections. That same cut down the middle of a 2D torus severs 8 connections. Even if each connection carries the same amount of data, there are more paths for data to move through which helps reduce congestion. XLA absorbs the complexity of cross-device scheduling. Software can trust that RDMAs will reach their intended stacks of HBM traveling along the ICI interface.&lt;/p&gt;&lt;p&gt;The same DNA ostensibly runs through TPUv1, yet the chips look and feel utterly different. The microarchitecture, software, and networking each became independently sophisticated parts of a larger system. Subsystems decoupled from one another yet still composed neatly. Where TPUv1 tightly choreographed everything, TPUv2 divided components into independent, asynchronously operating units communicating through explicit queues and synchronization points. TPUv3 was a minor revision in comparison. It has two MXUs per core, an increased clock, double the HBM capacity with 30% higher bus speeds, higher ICI bandwidth, and scales up to a 1024 node liquid-cooled rack. The dies only increased 6% relative to TPUv2 because engineers learned how to better lay the chip out [3]. Scaling the system to meet the continued growth of neural networks pushed future designs into new territory.&lt;/p&gt;&lt;head rend="h2"&gt;Scaling Up&lt;/head&gt;&lt;p&gt;As the footprint of the system grew, so too did the complexity of operating it. Our focus up to now has emphasized chip-local comparisons, e.g. How expensive are these operations relative to one another? How does the memory hierarchy work? How do subsystems A and B communicate on-device? While the TPUs remain the atom of the supercomputer, as we zoom out we observe the crystalline structure of the racks and pods. The fourth generation TPU is better examined thinking about memory as one unified domain. Specialization forces care in the microarchitecture, but the questions change. Where are collectives slow? How are larger tensors handled? Can we scale the racks further? Viewing the world from low altitude we find that TPUv4’s design emphasizes system scaling and energy management.&lt;/p&gt;&lt;p&gt;Peeking behind the accounting curtain for a moment, they note that “most OpEx cost is for provisioning power and not for electricity use, so saving power already provisioned doesn’t improve TCO as much as one might hope” [5]. Total Cost of Ownership (TCO) tries to consider the all in cost of the pods. On the back of a napkin we break this out into CapEx (equipment, installation, etc.) and OpEx (personnel, maintenance, power, etc.). Initially CapEx might dominate ASIC design, but as the platform matures, thinking through operational requirements produces different sets of optimizations. The need for fast, power efficient devices remains but extends out into the unknowable future. As model demands increase, better economics need compositional scalability in an efficient power envelope.&lt;/p&gt;&lt;p&gt;A brief note: TPUv4 is the training design and TPUv4i is the inference design. The impetus was to keep training and inference chips nearly identical so that there weren’t two separate designs awkwardly diverging into separate projects [5]. The relevant change is that the inference chip has one core while the training chip is dual-core.&lt;/p&gt;&lt;p&gt;Simplified Model of TPUv4 Systolic Execution&lt;/p&gt;&lt;p&gt;Fourth generation chips keep TPUv3’s MXU footprint, totaling 4 MXUs per core. In previous MXU designs partial sums moved downwards each cycle through a series of N two-input adders, where N is the size of the array, before reaching the output accumulators. TPUv4 batches groups of four products before passing them to custom 4-input adders. Batching products reduces the length of the adder chain from N to N/4, quartering the operational latency. Above we see 12 PEs bank four multiplies to reduce hops from 12 to 3. The specific implementation of these circuits isn’t clear from the paper, but this should provide enough motivation to understand the change. This circuit design decreases die area 40% and reduces peak power 12% [5].&lt;/p&gt;&lt;p&gt;CMEM Speed Ups [4]&lt;/p&gt;&lt;p&gt;Accessing DRAM is still expensive, and inference workloads underutilize chips. TPUv4 adds 128MiB shared CMEM that is like an L3 cache but with the niceties of software-managed programmability. CMEM helps to keep all 4 MXUs busy with computations at the cost of 28% of the TPUv4 die area. On the 7nm process node, SRAM memory accesses are 20x more energy-efficient than DRAM accesses [5]. CMEM’s memory bandwidth sits in between HBM and VMEM, but unlike HBM it can both read and write data. Expanding the memory hierarchy and keeping data closer to the arithmetic units allows XLA to cut out expensive trips to DRAM. During inference, prefetching model weights into SRAM for multi-tenancy drives higher utilization of chip resources that may otherwise be sitting idle. The ability to swap weights out from SRAM rather than DRAM makes paying the context switching cost feasible. All that die area and upfront CapEx gets amortized over the life of the chip in TCO so long as XLA can effectively leverage it. SparseCore Block Diagram [4] Contrary to the prevailing LLMs-everywhere paradigm, ad serving and recommendation models (DLRMs) run the world. SparseCores (SC) are built to accelerate these models at the cost of 5% die area and power [4]. The key features of these models are their usage of embeddings. Embeddings map data into enormous sparse matrices. Efficiently handling these sparse matrices requires clever strategies to shard tensors across devices and to make looking up the correct slice of data fast. Unstructured sparsity suffers massive memory traffic and imbalances between compute, communication, and data-dependent execution. The MXU is ill-suited to make progress on sparse workloads because they waste cycles on empty computations and don’t directly manage communication. SparseCores address this class of models with a “Sea of Cores” architecture designed to accelerate collectives and memory accesses [4]. SCs are segmented into 16 individual compute elements (tiles) near DRAM that support multiple outstanding memory accesses [4]. Tiles communicate over a data crossbar with one another and over the on-chip fabric to the rest of the device. A stream of CISC instructions enabling data-dependent communication gets issued by the processor’s core sequencer. The Fetch unit (8-wide SIMD vector processor, scVPU) reads data from HBM into 2.5MiB of sparse memory (Spmem), and the Flush Unit writes data out to HBM. Five on-board cross channel units (XPU) perform embedding specific operations. When embeddings are distributed across remote devices SCs leverage the existing ICI bandwidth to access remote memory. The dataflow looks as follows: SCs alleviate the need for the MXU to handle computation and memory traffic on sparse data. They remove the CPU/DRAM bottleneck and shift sparse phases off the MXU path. The cores issue many RDMAs across the global address space of the TPUv4 pods, speeding up embeddings based models 30.1x versus CPUs [4]. Dedicating a small amount of die area to the gather/scatter intensive DLRMs allows the device to be flexible and efficient under multiple algorithmic regimes.&lt;head&gt;SparseCores&lt;/head&gt;&lt;/p&gt;&lt;p&gt;Cores are getting crowded: MXUs, SparseCores, VPUs, HBM, and ICI routers. We see this component management pressure in the VLIW bundles. Driving the additional MXUs and CMEM required the VLIW bundle size to expand ~25% [5]. Adding new subsystems to the microarchitecture adds efficiencies that bubble up to system level performance, but lurking behind each of these changes is the specter of wiring. Fitting more efficient work onto the package with point-to-point connections became too great a tax. Training racks need to be close to one another in the datacenter to amortize the cost of cooling infrastructure, and this physical constraint forces the usage of optical fiber. ICI cabling in TPUv2/v3 coupled rack deployments so that a supercomputer couldn’t go into operation until the full pod was deployed [5]. To realize the TCO and energy wins of the microarchitecture system scaling needed to decouple and compose.&lt;/p&gt;&lt;p&gt;TPUv4i Floorplan [5]&lt;/p&gt;&lt;p&gt;The ICI needed to breathe. Previous revisions of ICI handled both on-chip communication and off-chip communication. More wires needed to be routed to/from the ICI interface as the number of components grew. This circuit layout pressure was complemented by the equally frustrating reality that handling on-chip and off-chip communication increased contention for ICI bandwidth. TPUv4 separates these concerns by adding a dedicated on-chip interconnect (OCI) fabric. The OCI interface handles data movement on-chip so that ICI can solely route traffic across chips. Notice in the fourth generation floorplan how much die area is reserved for OCI [5]. Shorter wires run between components and OCI rather than point-to-point. The OCI interface acts as the mailman. The Scalar Unit drops a message off at the OCI to submit a DMA to DRAM, and the OCI routes it to the memory controller. It tucks subsystem communication behind a unified data exchange interface that shortens wire routes and opens a path to flexible scaling in future designs.&lt;/p&gt;&lt;p&gt;Arbitrating memory accesses between HBM, VMEM, IMEM, SMEM and now CMEM meant maintaining too many sets of independent lanes. OCI uses 512B-wide native data paths segmented into four, 128B-wide groups across the memory hierarchy. Each group serves a quarter of the total HBM bandwidth (153GB/s) so that independent transfers don’t serialize behind one another [5]. Transferring small IMEM overlays shouldn’t have to wait on the completion of a long-latency tensor DMA. This partitioning strategy gives software more flexibility when scheduling work across a device. The full HBM bandwidth is available to each group, but software can schedule multiple concurrent transfers instead of funneling everything through one contested path. XLA plans large transfers to CMEM, CMEM feeds the arithmetic units, OCI handles message passing, and ICI routes and manages RDMAs. OCI and CMEM jointly help to improve spatial locality and reduce trips to HBM. TPUv2/v3 used two-dimensional, relaxed order DMAs to stride along two axes when moving data. This forced XLA to decompose complex tensor reshapes into multiple DMA operations. TPUv4(i) uses four-dimensional DMAs that stride along three axes moving 512-byte chunks [5]. Operations that previously required multiple round-trips to memory now happen in a single DMA. The architecture distributes DMA engines throughout the chip rather than centralizing them. Each engine acts as a co-processor that can decode and execute tensor operations independently. The unified design works identically for on-chip transfers, cross-chip transfers, and host transfers. XLA inserts explicit synchronization, but in exchange it gets predictable performance and the freedom to schedule data movement aggressively. The compiler knows the latency and pipelines around it.&lt;head&gt;4D Tensor (R)DMAs&lt;/head&gt;&lt;/p&gt;&lt;p&gt;TPUv3 had already resorted to optical fiber across racks to enable the full 2D torus, but the 1024 node supercomputer could not expand its physical footprint. Rigid ICI wiring constraints meant individual racks couldn’t be used until each pod was deployed, and the system topology was fixed as configured unless a technician recabled the pod. Rack maintenance brought the whole pod offline with it. Optical Circuit Switching (OCS) infrastructure was the cure. Even though optical solutions are expensive, OCS optical components represent less than five percent of both system and power costs [4][10]. Centralizing cross-rack communications inserted massive programmability into the system. Substituting the cross-rack links with a programmable OCS provided massive gains in “scale, availability, utilization, modularity, deployment, security, power, and performance” [4], unlocking a new scaling paradigm.&lt;/p&gt;&lt;p&gt;OCS Logical Diagram&lt;/p&gt;&lt;p&gt;Each rack in TPUv4 is a 4x4x4 cube, where this cube configuration is chosen to optimize all-to-all communications. Previous pod sizes (16x16 in v2, up to 128x32 in v3) were topology-limited. Devices could communicate between racks over ICI, but the system topology was statically programmed by the cabling. OCS removed these hard limits by centralizing cross-rack communication over an optical switching fiber. OCS offloads link establishment to an array of MEMS mirrors that dynamically configure links between devices in milliseconds [4]. New system topologies can be programmed on the fly by software, placing workloads on idle, non-contiguous machines. Dynamically reconfiguring the OCS improves system availability, tolerating outages in 0.1% - 1.0% of the CPU hosts [6]. TPUv4 pods scale up to 8x8 racks totaling a 4096 node cluster connected over OCS.&lt;/p&gt;&lt;p&gt;The OCSes isolate scaling complexity. Each rack contains 64 chips laid out logically as a cube. With 6 cube faces (+/- X/Y/Z), and 16 (4x4) chips per face, 96 optical links go to the OCS per rack. In the full 64 (8x8) rack pod, that is 6,144 uplinks to the OCS. This requires 48 OCSes that have 128 active ports to connect all the uplinks [4]. Moving cross-rack interconnects to a dedicated optical panel at this scale enabled programmable topologies, eased deployment by decoupling racks, and allowed software to effectively use OCS as a “plugboard” to route around node and link failures. MEMS Mirrors [10] OCSes use micro-electro-mechanical systems (MEMS) mirrors that tilt in three dimensions to steer optical beams. Each OCS contains two arrays of 136 mirrors. Each mirror has four voltage-controlled actuators that rotate it along two axes, steering light from any input port to any output port with sub-degree accuracy. Rather than monitoring each of the 136 mirrors with a dedicated photodetector, OCS uses a single camera per array with an 850nm monitoring laser. Image processing algorithms optimize the high-voltage driver signals to minimize insertion loss across the entire array. Once positioned, each mirror draws 10s of milliwatts to maintain alignment [10]. Circulators [10] Circulators double the OCS’s effective capacity by enabling bidirectional communication. A circulator is a three-port optical device. Light entering port 1 exits port 2, light entering port 2 exits port 3. This cyclic property means a single fiber and a single OCS port can carry traffic in both directions simultaneously halving the required fiber count and OCS ports [10].&lt;head&gt;Mirror, Mirror on the Wall&lt;/head&gt;&lt;/p&gt;&lt;p&gt;Full connectivity of the OCS across the pods meant that the torus topologies of the previous generations could now add a third wraparound dimension. The distance between racks was no longer a constraint, and since the OCS can program chip-to-chip connections on the fly a path to new topologies emerged. Not only could the connections between racks wrap around the z-dimension, they could twist.&lt;/p&gt;&lt;p&gt;Sample 1D Twisted Tori&lt;/p&gt;&lt;p&gt;We’ll make one modification to our previous wraparound topology diagram. Instead of wraparounds connecting only to the other side of their respective row/column, OCS programmability means that these connections can be offset. Adding twists to the wraparounds is an option not a requirement. Having the option to twist the network topology allows for new questions, e.g. given the communication pattern of this model, how should data be sent between participating chips? Twists make algorithmic experimentation and optimization two independently tractable targets and broadens the horizon of available efficiencies. Even without twisted topologies a third wraparound dimension adds bisection bandwidth to the network. The bisection bandwidth of 2D tori scales with the side length of the interconnects, N^(1/2). Adding the additional wraparound dimension scales bisection bandwidth with the area of the interconnects, N^(2/3). More paths in the topology shorten hops between participating nodes and alleviate system congestion along busy routes during synchronization. OCS better utilizes available devices and diversifies achievable topologies.&lt;/p&gt;&lt;p&gt;TPUv4(i) requires our thinking to broaden. We shouldn’t forget the impacts that microarchitecture improvements drive, but we need to consider the economics of the system holistically. Building warehouse scale solutions requires thinking about power provisioning, rack availability, interconnects, network topology, and accounting. Energy efficiency is still the overarching principle, but at datacenter scale. The message is simple: Target TCO over CapEx [5]. Adding CMEM is more expensive now but less expensive over time. Optical interconnects are expensive now but cost &amp;lt;3% of the fully operational system [4]. The duration of the design trade-offs became smeared into the future. All the same apparitions motivating TPUv1 go bump in the night, but they cast shorter shadows. TCO implies a system that requires operation, and the software that keeps the system available is an equal part of TPU’s development.&lt;/p&gt;&lt;head rend="h2"&gt;Island Hopping&lt;/head&gt;&lt;p&gt;Up to now we have enjoyed the quiet refuge of spreadsheet analysis, but the world is imperfect. Hardware dies, electricity spikes, and networks suffer congestion. The triumph of composing the system into decoupled, single responsibility units is not trivial, but infrastructure needs to serve real users. A cast of supporting software must keep chips available. Rock solid hardware relies on software to rationalize TCO obsession. The software is as much a part of the TPU story as the hardware.&lt;/p&gt;&lt;p&gt;We want to train a model. We decide which devices we need, pay rent, and start gawking at loss curves. When we submit our job for execution we don’t worry about the thousands of eager folks just like us. This mass of users vying for a fixed number of TPUs in sporadic intervals presents a problem. As the infrastructure provider what matters is that users don’t experience downtime. Components regularly fail and workloads are hard to predict. Once power has been provisioned every second of idle chip time or suboptimal workload allocation works against your best TCO approximations. Whether by underutilization or oversubscription, wasted resources are the enemy. Outer loop software that manages TPUs coordinates with XLA to find available nodes, check resource health, and configure ICI/OCS [6]. XLA needs to know which TPUs the computation will run on as well as the requested network topology because device placement is part of the program. Optimizing the system for high availability means dealing with the constraints imposed by ahead of time scheduling.&lt;/p&gt;&lt;p&gt;TPU Fragmentation [6]&lt;/p&gt;&lt;p&gt;Slices, Single Program Multiple Data (SPMD), and gang scheduling undergird TPU execution. Most workloads don’t consume an entire pod. Slices are declarations in code that allow developers to request an &amp;lt;X,Y,Z&amp;gt; device mesh which XLA uses to partition and shard models. This abstraction squirrels away both topology size and communication patterns. Pipeline parallelism may want a 2x2x1024 slice while data parallelism wants a 16x16x16 slice. The topology choice optimizes which communications are fast and which are slow. Mapping communications to a slice topology gives developers the freedom to experiment with parallelism strategies.&lt;/p&gt;&lt;p&gt;ICI coupling in TPUv3 meant the scheduler needed to find contiguous, healthy chips for workload placement. OCS lifted that restriction in TPUv4, but in both generations once a set of devices is selected the topology remains static for the duration of the program. A program owns the devices that it runs on until the program exits [8]. Concurrent users submitting unknowable slice sizes makes assigning devices like Tetris. The scheduler must place new jobs onto devices as old jobs pop in and out of existence. It needs mechanisms to rebalance suboptimal device allocations.&lt;/p&gt;&lt;p&gt;A single executable distributed to each participating device runs an identical program. SPMD encapsulates this many devices, single program framework. Developers write models as if they are running on one giant device, and the complexity of managing device-level data placement disappears from view. XLA’s partitioner rewrites every operation in the model to work on local tensor shards, inserting an AllReduce where gradients need to sync, scattering data where it needs to spread, and gathering results where they need to combine [7]. The single logical program becomes thousands of coordinated physical programs each operating on its local slice of data. Control is synchronized explicitly on-device with VLIW barriers and implicitly between devices by collectives. Gang scheduled execution means that each device launches the program all at once, trading off runtime resilience for performance. When a fault crops up during execution the job must be checkpointed and relocated [8]. The hardware stays simple, the software stays deterministic, but the orchestration layer must handle outages, link failures, and maintenance.&lt;/p&gt;&lt;p&gt;TPU Job Lifecycle [6]&lt;/p&gt;&lt;p&gt;Software must anticipate failures to juggle pre-allocated workloads. In [6] they note “To train a model, all TPU processes must be simultaneously up to synchronously update their weights via ICI collectives. A single failed, or interrupted process will interrupt the whole training process.” When a user submits a job, the cluster management client Borg queues it. If resources are fragmented or a job fails, Borg can preempt running workloads to shuffle them to different devices. When a job is ready to be scheduled, Borg selects a subset of devices and publishes an xconnect to the Pod Manager. The PM discovers pending xconnects and sends commands to the appropriate OCSes to connect the requested ICI channels. Once ICI connections stabilize, libtpunet configures the device’s ICI and programs its forwarding tables. XLA consumes the topology built by libtpunet to shard the model. Once execution begins, each device has its compiled program in local memory, knows its neighbors via ICI routing tables, and has its slice of the model weights in HBM. Thousands of devices execute in lockstep, synchronizing through collectives, without a single global runtime controller. The user does not see any of this background orchestration. ICI Interface [6] Packets hop through a path of ICI switches and optical fibers to arbitrary pairs of TPUs determined by libtpunet once during setup. xconnects initiate mirror configuration in the OCS, triggering on-chip device managers to initialize physical connections between ICIs. When libtpunet issues an ICI session start it clears and rightsizes the ICI buffers in the data layer for new RDMAs. Routing is handled by forwarding tables that provide a simple abstraction to locate destination TPUs. XLA emits sets of RDMA operations called transactions for collective communications. On-chip DMA engines read data from HBM and send the data to the ICI’s transaction layer to send over the network [6]. All the required hardware for training drags down MTBF [6], so the system needs to be resilient to outages without bringing everything down. TPU Fault Tolerance [6] The system manages faulty links with fault tolerant routing. An offline integer linear program simulates link outages and frames the route selection as a max flow problem, using an all-to-all collective as the canonical use case. The results from the ILP are cached and accessible by libtpunet. Fault tolerant routing uses Wild First Routing as its heuristic. Packets can take a wild hop around faulty links before reverting to fault free routing. Though using fault tolerant routing may induce network congestion, TPU availability benefits [6].&lt;head&gt;Fault Tolerant Routing&lt;/head&gt;&lt;/p&gt;&lt;p&gt;Getting the whole system to cooperate at scale needs clear boundaries and hand-offs. Borg, PM, and libtpunet bless the configuration of the workload before triggering execution. When TCO skews towards operation, getting these pieces right is as important as systolic arrays and memory hierarchies. But this presentation of how the software works is also subject to the constant evolution of the TPU. Cores communicate over OCI. Chips communicate over ICI. Racks connect remote ICI links over OCS. That leaves us with one final communication frontier: the datacenter network.&lt;/p&gt;&lt;p&gt;Mixture of Experts Routing [38]&lt;/p&gt;&lt;p&gt;SPMD assumes every device can communicate over ICI with predictable latency, which constrains developers to slice sizes that fit on a single pod. Islands of accelerators [8] leave idle capacity stranded across pods, and under contention, jobs struggle to get the right-shaped device allocation. Individual pods also constrain algorithmic flexibility. Unlike traditional transformers, Mixture-of-Experts models include runtime data dependencies. The gating mechanism in MoEs introduces non-deterministic routing during execution. The SPMD model has to be stretched to express the fine-grained, data-dependent control flow these models need. If you want to shard experts across pods there is no natural way to do so. Without the DCN there is no dynamic routing, resource sharing, or use of idle chips across pods.&lt;/p&gt;&lt;p&gt;The datacenter network (DCN) connects islands using Google’s Jupiter fabric [9]. From the TPU’s point of view it is the communication that doesn’t occur over ICI. Extending the many cores, one logical system scaling approach gets complicated by varying latency and bandwidth characteristics. Two solutions emerged from these limitations. Multislice extends SPMD across pod boundaries. It is a conservative but compatible approach with existing code. Pathways abandoned synchronous execution for asynchronous dataflow. It is more complex but necessary for true heterogeneity.&lt;/p&gt;&lt;p&gt;Logical diagram of Multislice over DCN [26]&lt;/p&gt;&lt;p&gt;Multislice extends existing SPMD code across pod boundaries with minimal changes. Pod boundaries are treated as just another level in the communication hierarchy. SPMD still uses gang-scheduled execution, but XLA understands that some collectives happen over ICI and others happen over slower DCN. The familiar declarative slice syntax adds a parameter to select devices across islands. The compiler optimizes collective placement to minimize cross-pod traffic. Multislice expands the number of devices available for training by providing access to resources across pods [26].&lt;/p&gt;&lt;p&gt;Pathways System Overview [8]&lt;/p&gt;&lt;p&gt;Pathways is a plug-in replacement for JAX’s backend that virtualizes the datacenter [8]. Instead of one giant SPMD program running in lockstep, it models execution as a DAG of compiled functions distributed across islands. Gang scheduling still happens within each island, but between islands coordination is asynchronous. There’s no single global runtime controller for the whole job. Mixture-of-Experts models can route activations dynamically to experts on different pods, and pipeline parallel stages can span multiple islands connected over DCN. Multiple programs can time-multiplex accelerators without context-switching overhead. Users request devices and the client compiles programs into a device-agnostic Pathways IR. XLA analyzes the program, the resource manager assigns physical TPUs, and the system inserts data movement operations between shards. Orchestration is complete by the time execution begins. Each device knows its program, its neighbors, and its slice of model weights.&lt;/p&gt;&lt;p&gt;Pathways uses a sharded dataflow model built on Plaque [8]. Each node represents a compiled function executing across thousands of TPU shards. The system uses parallel asynchronous dispatch. Pathways pipelines host side work in parallel instead of waiting for computation A to finish before preparing computation B. A control-plane scheduler per island enforces gang scheduling across programs. Between islands, Pathways uses centralized dispatch to coordinate placement and data movement. Data moves directly between accelerators over ICI within islands and DCN between islands. Pathways matches multi-controller performance by front-loading coordination work, even though cross-island dispatch is mediated by the control plane rather than issued independently by each host. This execution model performs as well as JAX and lifts restrictions on algorithmic expressibility [8].&lt;/p&gt;&lt;p&gt;A dedicated upstart could reproduce the hardware design philosophy, but the software co-design makes the TPU a mammoth. Borg allocates resources and preempts jobs. The Pod Manager configures optical switches. libtpunet knows every ICI routing edge case and manages fault tolerance. XLA compiles with full knowledge of topology and latencies. SPMD partitions models while maintaining the illusion of one giant device. Multislice extends that illusion across pods. Pathways rethinks distributed execution and virtualizes the datacenter as one programmable pool. Schedulers, compilers, and coordination systems all play one long song. Building a TPU competitor needs generations of hard earned experience points. Each new design reconsiders which approaches were dead ends. Admitting you were wrong and doubling back is the game. Thinking about the TPU is thinking about Everything Else.&lt;/p&gt;&lt;head rend="h2"&gt;Ceci n’est pas une TPU&lt;/head&gt;&lt;p&gt;After TPUv4 the well of detailed microarchitecture papers runs dry. You can still find information scattered across the internet, but not in the same succinct, curated way. Maybe more papers will be released publicly and we’ll be able to study these designs in greater detail, but until then we have to cobble together an understanding of our own. TPUv4 and v4i are followed by TPUv5p (performance) and v5e (efficiency), Trillium (v6e), and Ironwood (v7). We know that the inference (e) optimized designs retain a single-core architecture and use 2D tori instead of 3D tori. We know the interconnect and HBM performance numbers for the fifth, sixth, and seventh generation chips. We know that Trillium and Ironwood revert to 256x256 systolic arrays. We know that Ironwood scales up to 9,216 chips for training and 256 for inference with 1.77PB HBM that delivers 42.5 FP8 ExaFlops (6x Perf/W improvement over TPUv4) with a chiplet design for next generation reasoning and MoE workloads [16][20][21][23][24].&lt;/p&gt;&lt;p&gt;And I know that all of this fails to capture the totality of the enhancements since TPUv4. But a spec sheet like the one here or a primer like the one here could have told us that. The subsequent papers have focused on the system, but discussions of the system hide the simple origins of the device behind a hodgepodge of specs and new thundering heights. The essence of the thing becomes a folklorish amalgam of TPU lore. Myths are about meaning. Moore’s Law was never free in the literal sense. It required diligent engineering and enduring frustration, but decade after decade the compounding continued. The idea of Moore’s Law cast a spell that actualized its reality.&lt;/p&gt;&lt;p&gt;By nature the TPU is what it is not. The thrust and posturing of papers, talks, slides, and internet chatter focus on the technical minutiae, but the seams that hold this constellation of facts and figures together are the ordinary and the human. They are long emails and oscilloscopes in equal measure. How many of these choices go unseen? Hand-wringing about the system internals helps us to glimpse the creative act, but we mistake the painting for the paint chemistry. In this new world where nothing is free, every decision comes at an intentional, excruciating cost. The weight of the space of possibilities grows heavier knowing that each decision may foreclose another. Each choice is an act of reinvention in the face of a future that folds onto itself.&lt;/p&gt;&lt;p&gt;The TPU is an artifact born out of the quiet solace of steady hands doing careful engineering. AI DSAs are unlikely to be self-fulfilling in the same infinite feeling way as Moore’s Law. They will be five hundred ordinary decisions that compose into something greater. Can we make it smaller? Can we make it bigger? Can we make it easier to use? When we skim specs like the ones strewn above we notice the changes and feel the weight of what they represent. As new pressures get applied new entities emerge. For a moment we sense each decision branching into some unknown. Our new AI-obsessed world brings with it the demands of new ways of thinking. It is a reminder that the future is always at hand, and that if we participate in the myth-making we find that there are dragons after all.&lt;/p&gt;&lt;head rend="h2"&gt;References:&lt;/head&gt;&lt;p&gt;[1]: In-Datacenter Performance Analysis of a Tensor Processing Unit&lt;/p&gt;&lt;p&gt;[2]: The Design Process for Google’s Training Chips: TPUv2 and TPUv3&lt;/p&gt;&lt;p&gt;[3]: A Domain-Specific Supercomputer for Training Deep Neural Networks&lt;/p&gt;&lt;p&gt;[4]: TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings&lt;/p&gt;&lt;p&gt;[5]: Ten Lessons From Three Generations Shaped Google’s TPUv4i&lt;/p&gt;&lt;p&gt;[6]: Resiliency at Scale: Managing Google’s TPUv4 Machine Learning Supercomputer&lt;/p&gt;&lt;p&gt;[7]: GSPMD: General and Scalable Parallelization for ML Computation Graphs&lt;/p&gt;&lt;p&gt;[8]: PATHWAYS: ASYNCHRONOUS DISTRIBUTED DATAFLOW FOR ML&lt;/p&gt;&lt;p&gt;[9]: Jupiter Evolving: Transforming Google’s Datacenter Network via Optical Circuit Switches and Software-Defined Networking&lt;/p&gt;&lt;p&gt;[10]: Mission Apollo: Landing Optical Circuit Switching at Datacenter Scale&lt;/p&gt;&lt;p&gt;[11]: Computing’s Energy Problem&lt;/p&gt;&lt;p&gt;[12]: Domain-Specific Hardware Accelerators&lt;/p&gt;&lt;p&gt;[13]: The Accelerator Wall: Limits of Chip Specialization&lt;/p&gt;&lt;p&gt;[14]: The Deep Learning Revolution and Its Implications for Computer Architecture and Chip Design&lt;/p&gt;&lt;p&gt;[15]: Domain specific architectures for AI inference&lt;/p&gt;&lt;p&gt;[16]: How to Think About TPUs – Chapter 2&lt;/p&gt;&lt;p&gt;[17]: TPU Deep Dive&lt;/p&gt;&lt;p&gt;[18]: Understanding Matrix Multiplication on a Weight-Stationary Systolic Architecture&lt;/p&gt;&lt;p&gt;[19]: First in-depth look at Google’s TPU Architecture&lt;/p&gt;&lt;p&gt;[20]: WITH “IRONWOOD” TPU, GOOGLE PUSHES THE AI ACCELERATOR TO THE FLOOR&lt;/p&gt;&lt;p&gt;[21]: Ironwood: The first Google TPU for the age of inference&lt;/p&gt;&lt;p&gt;[22]: TPU Architecture – Google Documentation&lt;/p&gt;&lt;p&gt;[23]: Google Ironwood TPU Swings for Reasoning Model Leadership at Hot Chips 2025&lt;/p&gt;&lt;p&gt;[24]: Announcing Trillium, the sixth generation of Google Cloud TPU&lt;/p&gt;&lt;p&gt;[25]: A deep dive into SparseCore for Large Embedding Models&lt;/p&gt;&lt;p&gt;[26]: How to scale AI training to up to tens of thousands of Cloud TPU chips with Multislice&lt;/p&gt;&lt;p&gt;[27]: A Machine Learning Supercomputer With An Optically Reconfigurable Interconnect and Embeddings Support – HotChips Slides&lt;/p&gt;&lt;p&gt;[28]: Challenges in large scale training of Giant Transformers on Google TPU machines – HotChips Slides&lt;/p&gt;&lt;p&gt;[29]: Exploring Limits of ML Training on Google TPUs – HotChips Slides&lt;/p&gt;&lt;p&gt;[30]: Cloud TPU: Codesigning Architecture and Infrastructure – HotChips Slides&lt;/p&gt;&lt;p&gt;[31]: A DOMAIN-SPECIFIC TPU SUPERCOMPUTER FOR TRAINING DEEP NEURAL NETWORKS – Slides&lt;/p&gt;&lt;p&gt;[32]: Google’s Training Chips Revealed: TPUv2 and TPUv3 – Slides&lt;/p&gt;&lt;p&gt;[33]: A Decade of Machine Learning Accelerators: Lessons Learned and Carbon Footprint – MLSys Slides&lt;/p&gt;&lt;p&gt;[34]: Sparse-TPU: Adapting Systolic Arrays for Sparse Matrices&lt;/p&gt;&lt;p&gt;[35]: Systolic Array For VLSi&lt;/p&gt;&lt;p&gt;[36]: Why Systolic Architectures?&lt;/p&gt;&lt;p&gt;[37]: Doubly Twisted Torus Networks for VLSI Processor Arrays&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46172797</guid><pubDate>Sat, 06 Dec 2025 12:29:28 +0000</pubDate></item><item><title>GrapheneOS is the only Android OS providing full security patches</title><link>https://grapheneos.social/@GrapheneOS/115647408229616018</link><description>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46173407</guid><pubDate>Sat, 06 Dec 2025 13:58:31 +0000</pubDate></item><item><title>Tiny Core Linux: a 23 MB Linux distro with graphical desktop</title><link>http://www.tinycorelinux.net/</link><description>&lt;doc fingerprint="9ddb391819bfbc66"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Welcome to The Core Project - Tiny Core Linux&lt;/head&gt;
    &lt;p&gt;The Core Project is a highly modular based system with community build extensions.&lt;/p&gt;
    &lt;p&gt;It starts with a recent Linux kernel, vmlinuz, and our root filesystem and start-up scripts packaged with a basic set of kernel modules in core.gz. Core (11MB) is simply the kernel + core.gz - this is the foundation for user created desktops, servers, or appliances. TinyCore is Core + Xvesa.tcz + Xprogs.tcz + aterm.tcz + fltk-1.3.tcz + flwm.tcz + wbar.tcz&lt;/p&gt;
    &lt;p&gt;TinyCore becomes simply an example of what the Core Project can produce, an 16MB FLTK/FLWM desktop.&lt;/p&gt;
    &lt;p&gt;CorePlus ofers a simple way to get started using the Core philosophy with its included community packaged extensions enabling easy embedded frugal or pendrive installation of the user's choice of supported desktop, while maintaining the Core principal of mounted extensions with full package management.&lt;/p&gt;
    &lt;p&gt;It is not a complete desktop nor is all hardware completely supported. It represents only the core needed to boot into a very minimal X desktop typically with wired internet access.&lt;/p&gt;
    &lt;p&gt;The user has complete control over which applications and/or additional hardware to have supported, be it for a desktop, a netbook, an appliance, or server, selectable by the user by installing additional applications from online repositories, or easily compiling most anything you desire using tools provided.&lt;/p&gt;
    &lt;p&gt;The latest version: 16.2&lt;/p&gt;
    &lt;head rend="h3"&gt;News&lt;/head&gt;
    &lt;head rend="h3"&gt;About Our Project&lt;/head&gt;
    &lt;p&gt;Our goal is the creation of a nomadic ultra small graphical desktop operating system capable of booting from cdrom, pendrive, or frugally from a hard drive. The desktop boots extremely fast and is able to support additional applications and hardware of the users choice. While Tiny Core always resides in ram, additional applications extensions can either reside in ram, mounted from a persistent storage device, or installed into a persistent storage device.&lt;/p&gt;
    &lt;p&gt;We invite interested users and developers to explore Tiny Core. Within our forums we have an open developement model. We encourage shared knowledge. We promote community involvement and community built application extensions. Anyone can contribute to our project by packaging their favorite application or hardware support to run in Tiny Core. The Tiny Core Linux Team currently consists of eight members who peruse the forums to assist from answering questions to helping package new extensions.&lt;/p&gt;
    &lt;p&gt;Join us here and on IRC Freenode #tinycorelinux.&lt;/p&gt;
    &lt;p&gt;Learn. Share. Grow your knowledge of Linux.&lt;/p&gt;
    &lt;p&gt;Robert Shingledecker, December 01, 2008&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46173547</guid><pubDate>Sat, 06 Dec 2025 14:18:42 +0000</pubDate></item><item><title>HTML as an Accessible Format for Papers</title><link>https://info.arxiv.org/about/accessible_HTML.html</link><description>&lt;doc fingerprint="e3d1ea5238d7dc4a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;HTML as an accessible format for papers&lt;/head&gt;
    &lt;p&gt;Accessibility barriers in research are not new, but they are urgent. The message we have heard from our community is that arXiv can have the most impact in the shortest time by offering HTML papers alongside the existing PDF.&lt;/p&gt;
    &lt;p&gt;arXiv has successfully launched papers in HTML format. We are gradually backfilling HTML for arXiv's corpus of over 2 million papers over time. Not every paper can be successfully converted, so a small percentage of papers will not have an HTML version. We will work to improve conversion over time.&lt;/p&gt;
    &lt;p&gt;The link to the HTML format will appear on abstract pages below the existing PDF download link. Authors will have the opportunity to preview their paperâs HTML as a part of the submission process.&lt;/p&gt;
    &lt;p&gt;The beta rollout is just the beginning. We have a long way to go to improve HTML papers and will continue to solicit feedback from authors, readers, and the entire arXiv community to improve conversions from LaTeX.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why "experimental" HTML?&lt;/head&gt;
    &lt;p&gt;Did you know that 90% of submissions to arXiv are in TeX format, mostly LaTeX? That poses a unique accessibility challenge: to accurately convert from TeXâa very extensible language used in myriad unique ways by authorsâto HTML, a language that is much more accessible to screen readers and text-to-speech software, screen magnifiers, and mobile devices. In addition to the technical challenges, the conversion must be both rapid and automated in order to maintain arXivâs core service of free and fast dissemination.&lt;/p&gt;
    &lt;p&gt;Because of these challenges we know there will be some conversion and rendering issues. We have decided to launch in beta with âexperimentalâ HTML because:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Accessible papers are needed now. We have talked to the arXiv community, especially researchers with accessibility needs, and they overwhelmingly asked us not to wait.&lt;/item&gt;
      &lt;item&gt;We need your help. The obvious work is done. Reports from the community will help us identify issues we can track back to specific LaTeX packages that are not converting correctly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Error messages you may see in HTML papers&lt;/head&gt;
    &lt;p&gt;HTML papers on arXiv.org are a work in progress and will sometimes display errors. As we work to improve accessibility we share with you the causes of these errors and what authors can do to help minimize them. Learn more about error messages you may see in HTML papers&lt;/p&gt;
    &lt;head rend="h2"&gt;Ways to help&lt;/head&gt;
    &lt;head rend="h3"&gt;1) Read HTML papers and report issues&lt;/head&gt;
    &lt;p&gt;We encourage the community to try out HTML papers in your field:&lt;/p&gt;
    &lt;head rend="h4"&gt;Report an issue&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Go to the abstract page for a paper you are interested in reading.&lt;/item&gt;
      &lt;item&gt;Look in the section where you find the link to the PDF download, and click the new link for HTML.&lt;/item&gt;
      &lt;item&gt;Report issues by either a) clicking on the Open Issue button b) selecting text and clicking on the Open Issue for Selection button or c) use &lt;code&gt;Ctrl+?&lt;/code&gt;on your keyboard. If you are using a screen reader, use&lt;code&gt;Alt+y&lt;/code&gt;to toggle accessible reporting buttons per paragraph.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please do not create reports that the HTML paper doesn't look exactly like the PDF paper&lt;/p&gt;
    &lt;p&gt;Our primary goal for this project is to make papers more accessible, so the focus during the beta phase will value function over form. HTML layouts that are incorrect or are illegible are important to report. But we do expect the HTML papers to present differently than the same paper rendered in PDF. Line breaks will occur in different places and there is likely to be more white space. In general, the HTML paper won't present as compactly. Intricate typographic layouts will not be rendered so intricately. This is by design.&lt;/p&gt;
    &lt;p&gt;HTML is a different medium and brings its own advantages versus PDF. In addition to being much more compatible with assistive technologies, HTML does a far better job adapting to the characteristics of the device you are reading on, including mobile devices.&lt;/p&gt;
    &lt;head rend="h3"&gt;2) Help improve the conversion from LaTeX&lt;/head&gt;
    &lt;p&gt;If you are an author you can help us improve conversions to HTML by following our guide to LaTeX Markup Best Practices for Successful HTML Papers.&lt;/p&gt;
    &lt;p&gt;If you are a developer and have free development cycles, help us improve conversions! Our collaborators at LaTeXML maintain a list of issues and welcome feedback and developer contributions.&lt;/p&gt;
    &lt;p&gt;If you are a publisher, member of a society, or conference organizer you can help us improve conversions to HTML by reviewing the .cls files your organization recommends to authors for unsupported packages. Providing .cls files that use supported packages is an easy way to support and sow accessibility in the scientific community.&lt;/p&gt;
    &lt;head rend="h2"&gt;Thank you to our collaborators&lt;/head&gt;
    &lt;p&gt;First, we want to share a special thank you to all the scientists with disabilities who have generously shared their insights, expertise, and guidance throughout this project.&lt;/p&gt;
    &lt;p&gt;We want to thank two organizations without which HTML papers on arXiv would not be possible: The LaTeX Project, and the LaTeXML team from NIST. We deeply thank each member of these teams for their knowledge, incredible work, and commitment to accessibility.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46173825</guid><pubDate>Sat, 06 Dec 2025 14:59:52 +0000</pubDate></item><item><title>Infisical (YC W23) Is Hiring Engineers to Build the Modern OSS Security Stack</title><link>https://www.ycombinator.com/companies/infisical/jobs/2pwGcK9-senior-full-stack-engineer-us-canada</link><description>&lt;doc fingerprint="c51f69845b7e12a"&gt;
  &lt;main&gt;
    &lt;p&gt;Unified platform for secrets, certs, and privileged access management&lt;/p&gt;
    &lt;p&gt;Infisical is looking to hire exceptional talent to join our teams in building the open source security infrastructure stack for the AI era.&lt;/p&gt;
    &lt;p&gt;We're building a generational company with a world-class engineering team. This isn’t a place to coast — but if you want to grow fast, take ownership, and solve tough problems, you’ll be challenged like nowhere else.&lt;/p&gt;
    &lt;p&gt;What We’re Looking For&lt;/p&gt;
    &lt;p&gt;We’re looking for an exceptional Full Stack Engineer to help us build, optimize, and expand the foundation of the platform.&lt;/p&gt;
    &lt;p&gt;We’ve kept our hiring standards exceptionally high since we expect engineers to tackle a broad range of challenges on a day-to-day basis. Examples of past engineering initiatives include developing strategies for secret rotation and dynamic secrets, a gateway to provide secure access to private resources, protocols like EST and KMIP, integrations for syncing secrets across cloud providers, and entire new product lines such as Infisical PKI and Infisical SSH.&lt;/p&gt;
    &lt;p&gt;You’ll be working closely with our CTO and the rest of the engineering team to:&lt;/p&gt;
    &lt;p&gt;Requirements&lt;/p&gt;
    &lt;p&gt;Bonus&lt;/p&gt;
    &lt;p&gt;How You’ll Grow&lt;/p&gt;
    &lt;p&gt;In this role, you’ll play a pivotal part in shaping Infisical’s future—making key technical decisions, establishing foundational processes, and tackling complex scalability challenges. As you gain experience and the team expands, you'll have the opportunity to take full ownership of specific areas of our platform, driving them end-to-end with autonomy and impact.&lt;/p&gt;
    &lt;p&gt;Overall, you’ll be one of the defining pieces of our team as we scale to thousands of customers over the next 18 months.&lt;/p&gt;
    &lt;p&gt;Team, Values &amp;amp; Benefits&lt;/p&gt;
    &lt;p&gt;Our team brings experience from companies like Figma, AWS, and Red Hat. We operate primarily as a remote team but maintain a strong presence in San Francisco, where we have an office. We also get together in person throughout the year for off-sites, conferences, and team gatherings.&lt;/p&gt;
    &lt;p&gt;At Infisical, we offer competitive compensation, including both salary and equity options. Additional benefits, such as a lunch stipend and a work setup budget, are available with more details to be found on our careers page.&lt;/p&gt;
    &lt;p&gt;About Us&lt;/p&gt;
    &lt;p&gt;Infisical is the open source security infrastructure platform that engineers use for secrets management, internal PKI, key management, and SSH workflow orchestration. We help developers and organizations securely manage over 1.5 billion secrets each month including application configuration, database credentials, certificates, and more.&lt;/p&gt;
    &lt;p&gt;We’ve raised $19M from Y Combinator, Google, and Elad Gil, and our customers include Hugging Face, Lucid, and LG.&lt;/p&gt;
    &lt;p&gt;Join us on a mission to make security easier for all developers — starting with secrets management.&lt;/p&gt;
    &lt;p&gt;Infisical is the #1 open source secret management platform – used by tens of thousands of developers.&lt;/p&gt;
    &lt;p&gt;We raised $3M from Y Combinator, Gradient Ventures (Google's VC fund), and awesome angel investors like Elad Gil, Arash Ferdowsi (founder/ex-CTO of Dropbox), Paul Copplestone (founder/CEO of Supabase), James Hawkins (founder/CEO of PostHog), Andrew Miklas (founder/ex-CTO of PagerDuty), Diana Hu (GP at Y Combinator), and more.&lt;/p&gt;
    &lt;p&gt;We are default alive, and have signed many customers ranging from fastest growing startups to post-IPO enterprises.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46174789</guid><pubDate>Sat, 06 Dec 2025 17:01:53 +0000</pubDate></item><item><title>Running Claude Code in a loop to mirror human development practices</title><link>https://anandchowdhary.com/blog/2025/running-claude-code-in-a-loop</link><description>&lt;doc fingerprint="f44284162f911aeb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Running Claude Code in a loop&lt;/head&gt;
    &lt;p&gt;This all started because I was contractually obligated to write unit tests for a codebase with hundreds of thousands of lines of code and go from 0% to 80%+ coverage in the next few weeks - seems like something Claude should do. So I built Continuous Claude, a CLI tool to run Claude Code in a loop that maintains a persistent context across multiple iterations.&lt;/p&gt;
    &lt;p&gt;Current AI coding tools tend to halt after completing a task once they think the job is done and they don’t really have an opportunity for self-criticism or further improvement. And this one-shot pattern then makes it difficult to tackle larger projects. So in contrast to running Claude Code “as is” (which provides help in isolated bursts), what you want is to run Claude code for a long period of time without exhausting the context window.&lt;/p&gt;
    &lt;p&gt;Turns out, it’s as simple as just running Claude Code in a continuous loop - but drawing inspiration from CI/CD practices and persistent agents - you can take it a step further by running it on a schedule or through triggers and connecting it to your GitHub pull requests workflow. And by persisting relevant context and results from one iteration to the next, this process ensures that knowledge gained in earlier steps is not lost, which is currently not possible in stateless AI queries and something you have to slap on top by setting up markdown files to store progress and context engineer accordingly.&lt;/p&gt;
    &lt;head rend="h2"&gt;While + git + persistence&lt;/head&gt;
    &lt;p&gt;The first version of this idea was a simple while loop:&lt;/p&gt;
    &lt;code&gt;while true; do
  claude --dangerously-skip-permissions "Increase test coverage [...] write notes for the next developer in TASKS.md, [etc.]"
  sleep 1
done
&lt;/code&gt;
    &lt;p&gt;to which my friend Namanyay of Giga AI said “genius and hilarious”. I spent all of Saturday building the rest of the tooling. Now, the Bash script acts as the conductor, repeatedly invoking Claude Code with the appropriate prompts and handling the surrounding tooling. For each iteration, the script:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Creates a new branch and runs Claude Code to generate a commit&lt;/item&gt;
      &lt;item&gt;Pushes changes and creates a pull request using GitHub’s CLI&lt;/item&gt;
      &lt;item&gt;Monitors CI checks and reviews via &lt;code&gt;gh pr checks&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Merges on success or discards on failure&lt;/item&gt;
      &lt;item&gt;Pulls the updated main branch, cleans up, and repeats&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When an iteration fails, it closes the PR and discards the work. This is wasteful, but with knowledge of test failures, the next attempt can try something different. Because it piggybacks on GitHub’s existing workflows, you get code review and preview environments without additional work - if your repo requires code owner approval or specific CI checks, it will respect those constraints.&lt;/p&gt;
    &lt;head rend="h2"&gt;Context continuity&lt;/head&gt;
    &lt;p&gt;A shared markdown file serves as external memory where Claude records what it has done and what should be done next. Without specific prompting instructions, it would create verbose logs that harm more than help - the intent is to keep notes as a clean handoff package between runs. So the key instruction to the model is: “This is part of a continuous development loop… you don’t need to complete the entire goal in one iteration, just make meaningful progress on one thing, then leave clear notes for the next iteration… think of it as a relay race where you’re passing the baton.”&lt;/p&gt;
    &lt;p&gt;Here’s an actual production example: the previous iteration ended with “Note: tried adding tests to X but failed on edge case, need to handle null input in function Y” and the very next Claude invocation saw that and prioritized addressing it. A single small file reduces context drift, where it might forget earlier reasoning and go in circles.&lt;/p&gt;
    &lt;p&gt;What’s fascinating is how the markdown file enables self-improvement. A simple “increase coverage” from the user becomes “run coverage, find files with low coverage, do one at a time” as the system teaches itself through iteration and keeps track of its own progress.&lt;/p&gt;
    &lt;head rend="h2"&gt;Continuous AI&lt;/head&gt;
    &lt;p&gt;My friends at GitHub Next have been exploring this idea in their project Continuous AI and I shared Continuous Claude with them.&lt;/p&gt;
    &lt;p&gt;One compelling idea from the team was running specialized agents simultaneously - one for development, another for tests, a third for refactoring. While this could divide and conquer complex tasks more efficiently, it possibly introduces coordination challenges. I’m trying a similar approach for adding tests in different parts of a monorepository at the same time.&lt;/p&gt;
    &lt;p&gt;The agentics project combines an explicit research phase with pre-build steps to ensure the software is restored before agentic work begins. “The fault-tolerance of Agent in a Loop is really important. If things go wrong it just hits the resource limits and tries again. Or the user just throws the generated PR away if it’s not helpful. It’s so much better than having a frustrated user trying to guide an agent that’s gone down a wrong path,” said GitHub Next Principal Researcher Don Syme.&lt;/p&gt;
    &lt;p&gt;It reminded me of a concept in economics/mathematics called “radiation of probabilities” (I know, pretty far afield, but bear with me) and here, each agent run is like a random particle - not analyzed individually, but the general direction emerges from the distribution. Each run can even be thought of as idempotent: if GitHub Actions kills the process after six hours, you only lose some dirty files that the next agent will pick up anyway. All you care about is that it’s moving in the right direction in general, for example increasing test coverage, rather than what an individual agent does. This wasteful-but-effective approach becomes viable as token costs approach zero, similar to Cursor’s multiple agents.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dependabot on steroids&lt;/head&gt;
    &lt;p&gt;Tools like Dependabot handle dependency updates, but Continuous Claude can also fix post-update breaking changes using release notes. You could run a GitHub Actions workflow every morning that checks for updates and continuously fixes issues until all tests pass.&lt;/p&gt;
    &lt;p&gt;Large refactoring tasks become manageable: breaking a monolith into modules, modernizing callbacks to async/await, or updating to new style guidelines. It could perform a series of 20 pull requests over a weekend, each doing part of the refactor with full CI validation. There’s a whole class of tasks that are too mundane for humans but still require attention to avoid breaking the build.&lt;/p&gt;
    &lt;p&gt;The model mirrors human development practices. Claude Code handles the grunt work, but humans remain in the loop through familiar mechanisms like PR reviews. Download the CLI from GitHub to get started: AnandChowdhary/continuous-claude&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46175662</guid><pubDate>Sat, 06 Dec 2025 18:53:57 +0000</pubDate></item><item><title>OMSCS Open Courseware</title><link>https://sites.gatech.edu/omscsopencourseware/</link><description>&lt;doc fingerprint="e76af2125443a7a2"&gt;
  &lt;main&gt;
    &lt;p&gt;Georgia Tech’s Online Master of Science in Computer Science (OMSCS) program is proud to make the course content* for many of its courses publicly available through Ed Lessons. Select a course below to view the public content for that course.&lt;/p&gt;
    &lt;p&gt;Note that students enrolled in OMSCS should access their course content through Canvas, as the for-credit versions of these courses may include graded components or recent content updates not available through OMSCS Open Courseware.&lt;/p&gt;
    &lt;p&gt;*Course content typically includes things such as lecture videos and exercises; it will not include things like homeworks, projects quizzes, exams, or other graded assignments.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46175826</guid><pubDate>Sat, 06 Dec 2025 19:14:35 +0000</pubDate></item><item><title>Zebra-Llama: Towards Efficient Hybrid Models</title><link>https://arxiv.org/abs/2505.17272</link><description>&lt;doc fingerprint="c1d340b445bd08b"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Machine Learning&lt;/head&gt;&lt;p&gt; [Submitted on 22 May 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Zebra-Llama: Towards Extremely Efficient Hybrid Models&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:With the growing demand for deploying large language models (LLMs) across diverse applications, improving their inference efficiency is crucial for sustainable and democratized access. However, retraining LLMs to meet new user-specific requirements is prohibitively expensive and environmentally unsustainable. In this work, we propose a practical and scalable alternative: composing efficient hybrid language models from existing pre-trained models. Our approach, Zebra-Llama, introduces a family of 1B, 3B, and 8B hybrid models by combining State Space Models (SSMs) and Multi-head Latent Attention (MLA) layers, using a refined initialization and post-training pipeline to efficiently transfer knowledge from pre-trained Transformers. Zebra-Llama achieves Transformer-level accuracy with near-SSM efficiency using only 7-11B training tokens (compared to trillions of tokens required for pre-training) and an 8B teacher. Moreover, Zebra-Llama dramatically reduces KV cache size -down to 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants, respectively-while preserving 100%, 100%, and &amp;gt;97% of average zero-shot performance on LM Harness tasks. Compared to models like MambaInLLaMA, X-EcoMLA, Minitron, and Llamba, Zebra-Llama consistently delivers competitive or superior accuracy while using significantly fewer tokens, smaller teachers, and vastly reduced KV cache memory. Notably, Zebra-Llama-8B surpasses Minitron-8B in few-shot accuracy by 7% while using 8x fewer training tokens, over 12x smaller KV cache, and a smaller teacher (8B vs. 15B). It also achieves 2.6x-3.8x higher throughput (tokens/s) than MambaInLlama up to a 32k context length. We will release code and model checkpoints upon acceptance.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Mehdi Rezagholizadeh [view email]&lt;p&gt;[v1] Thu, 22 May 2025 20:39:57 UTC (12,646 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46176289</guid><pubDate>Sat, 06 Dec 2025 20:15:54 +0000</pubDate></item><item><title>Show HN: Tascli, a command line based (human) task and record manager</title><link>https://github.com/Aperocky/tascli</link><description>&lt;doc fingerprint="e35cd7e9d322ee4"&gt;
  &lt;main&gt;
    &lt;p&gt;A simple, fast, local CLI tool for tracking tasks and records from unix terminal.&lt;/p&gt;
    &lt;p&gt;Installation:&lt;/p&gt;
    &lt;code&gt;cargo install tascli
# or use brew
brew tap Aperocky/tascli
brew install tascli&lt;/code&gt;
    &lt;p&gt;Tasks and records are stored in &lt;code&gt;~/.local/share/tascli/tascli.db&lt;/code&gt; (configurable) with &lt;code&gt;rusqlite&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Create tasks with deadlines:&lt;/p&gt;
    &lt;code&gt;# Basic tasks
tascli task "Create readme" today
tascli task "Publish package" tomorrow
tascli task "Do taxes" 4/15

# With category
tascli task -c work "Read emails" week&lt;/code&gt;
    &lt;p&gt;Create recurring tasks:&lt;/p&gt;
    &lt;code&gt;tascli task "write diary" daily
tascli task "mortgage payment" "monthly 17th"&lt;/code&gt;
    &lt;p&gt;List tasks:&lt;/p&gt;
    &lt;code&gt;# List active tasks
$ tascli list task&lt;/code&gt;
    &lt;p&gt;output:&lt;/p&gt;
    &lt;code&gt;Task List:
----------------------------------------------------------------------------------------------
| Index  | Category            | Content                               | Deadline            |
----------------------------------------------------------------------------------------------
| 1      | life (recurring)    | write diary                           | Today               |
----------------------------------------------------------------------------------------------
| 2      | tascli              | Add pagination capability for tascli  | Sunday              |
|        |                     | list actions                          |                     |
----------------------------------------------------------------------------------------------
| 3      | tascli              | Add readme section on timestring      | Sunday              |
|        |                     | format                                |                     |
----------------------------------------------------------------------------------------------
| 4      | life                | Do state taxes                        | Sunday              |
----------------------------------------------------------------------------------------------
| 5      | tascli              | Sort list output by time instead of   | Sunday              |
|        |                     | internal id                           |                     |
----------------------------------------------------------------------------------------------
| 6      | tascli              | Fix length issue for unicode chars    | Sunday              |
----------------------------------------------------------------------------------------------
| 7      | life                | Two month pictures - follow the lead  | 4/23                |
|        |                     | from the previous one month pictures  |                     |
----------------------------------------------------------------------------------------------
&lt;/code&gt;
    &lt;p&gt;Complete tasks:&lt;/p&gt;
    &lt;code&gt;# Mark index 1 as done
tascli done 1&lt;/code&gt;
    &lt;p&gt;Completing a task or a recurring tasks will generate a corresponding record.&lt;/p&gt;
    &lt;p&gt;Search tasks:&lt;/p&gt;
    &lt;code&gt;tascli list task --search "rust"&lt;/code&gt;
    &lt;p&gt;List all tasks in &lt;code&gt;tascli&lt;/code&gt; category (including completed)&lt;/p&gt;
    &lt;code&gt;tascli list task -s all -c tascli&lt;/code&gt;
    &lt;p&gt;Example output:&lt;/p&gt;
    &lt;code&gt;Task List:
----------------------------------------------------------------------------------------------
| Index  | Category            | Content                               | Deadline            |
----------------------------------------------------------------------------------------------
| 1      | baby (Recurring)    | Mix egg yolk milk for Rowan           | Daily (fulfilled)   |
----------------------------------------------------------------------------------------------
| 2      | tascli              | Fix addition and modification commands| Today (completed)   |
|        |                     | output to have N/A for index          |                     |
----------------------------------------------------------------------------------------------
| 3      | tascli              | Insert guardrail against accidental   | Today (completed)   |
|        |                     | valid syntax like 'task list' that is |                     |
|        |                     | mistakenly made                       |                     |
----------------------------------------------------------------------------------------------
| 4      | tascli              | Create a gif for readme               | Today (completed)   |
----------------------------------------------------------------------------------------------
| 5      | tascli              | Add pagination capability for tascli  | Sunday              |
|        |                     | list actions                          |                     |
----------------------------------------------------------------------------------------------
| 6      | tascli              | Add readme section on timestring      | Sunday              |
|        |                     | format                                |                     |
----------------------------------------------------------------------------------------------
&lt;/code&gt;
    &lt;p&gt;Create records (for tracking events):&lt;/p&gt;
    &lt;code&gt;# With current time
tascli record -c feeding "100ML"

# With specific time
tascli record -c feeding -t 11:20AM "100ML"&lt;/code&gt;
    &lt;p&gt;List records:&lt;/p&gt;
    &lt;code&gt;# -d 1 stand for only get last 1 day of record
tascli list record -d 1&lt;/code&gt;
    &lt;p&gt;Search records:&lt;/p&gt;
    &lt;code&gt;tascli list record --search "secret"&lt;/code&gt;
    &lt;p&gt;Example output:&lt;/p&gt;
    &lt;code&gt;Records List:
----------------------------------------------------------------------------------------------
| Index  | Category            | Content                               | Created At          |
----------------------------------------------------------------------------------------------
| 1      | feeding             | 110ML                                 | Today 1:00AM        |
----------------------------------------------------------------------------------------------
| 2      | feeding             | breastfeeding                         | Today 4:10AM        |
----------------------------------------------------------------------------------------------
| 3      | feeding             | 100ML                                 | Today 7:30AM        |
----------------------------------------------------------------------------------------------
| 3      | life (Recurring)    | write diary                           | Today 10:30AM       |
----------------------------------------------------------------------------------------------
| 4      | feeding             | 110ML                                 | Today 11:20AM       |
----------------------------------------------------------------------------------------------
&lt;/code&gt;
    &lt;p&gt;This application accepts flexible time strings in various formats:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simple dates: &lt;code&gt;today&lt;/code&gt;,&lt;code&gt;tomorrow&lt;/code&gt;,&lt;code&gt;yesterday&lt;/code&gt;,&lt;code&gt;friday&lt;/code&gt;,&lt;code&gt;eom&lt;/code&gt;(end of month),&lt;code&gt;eoy&lt;/code&gt;(end of year)&lt;/item&gt;
      &lt;item&gt;Date formats: &lt;code&gt;YYYY-MM-DD&lt;/code&gt;,&lt;code&gt;MM/DD/YYYY&lt;/code&gt;,&lt;code&gt;MM/DD&lt;/code&gt;(current year)&lt;/item&gt;
      &lt;item&gt;Time formats: &lt;code&gt;HH:MM&lt;/code&gt;,&lt;code&gt;3:00PM&lt;/code&gt;,&lt;code&gt;3PM&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Combined: &lt;code&gt;2025-03-24 15:30&lt;/code&gt;,&lt;code&gt;tomorrow 3PM&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When only a date is provided, the time defaults to end of day (23:59:59). When only a time is provided, the date defaults to today.&lt;/p&gt;
    &lt;p&gt;Recurring Formats (schedules) are applicable to tasks:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Recurring Formats: &lt;code&gt;daily&lt;/code&gt;,&lt;code&gt;daily 9PM&lt;/code&gt;,&lt;code&gt;weekly&lt;/code&gt;,&lt;code&gt;weekly Friday 9AM&lt;/code&gt;,&lt;code&gt;weekly mon-fri&lt;/code&gt;,&lt;code&gt;monthly 1st&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Recurring Formats (II): &lt;code&gt;every day&lt;/code&gt;,&lt;code&gt;every 9PM&lt;/code&gt;,&lt;code&gt;every monday&lt;/code&gt;,&lt;code&gt;every 9th of the month&lt;/code&gt;,&lt;code&gt;every 2/14&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If storing the db file in location other than &lt;code&gt;~/.local/share/tascli/tascli.db&lt;/code&gt; is preferred, create a config file:&lt;/p&gt;
    &lt;code&gt;{
    "data_dir": "/where/you/want/it"
}
&lt;/code&gt;
    &lt;p&gt;at &lt;code&gt;~/.config/tascli/config.json&lt;/code&gt; to adjust the location of the stored file. Note, if you already have existing tasks, you may want to move/copy the db file there first.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;tascli&lt;/code&gt; uses &lt;code&gt;clap&lt;/code&gt; for argument parsing, use &lt;code&gt;--help&lt;/code&gt; to get help on all levels of this cli:&lt;/p&gt;
    &lt;code&gt;aperocky@~$ tascli -h
Usage: tascli &amp;lt;COMMAND&amp;gt;

Commands:
  task    add task with end time
  record  add record
  done    Finish tasks
  update  Update tasks or records wording/deadlines
  delete  Delete Records or Tasks
  list    list tasks or records
  help    Print this message or the help of the given subcommand(s)

Options:
  -h, --help     Print help
  -V, --version  Print version
aperocky@~$ tascli task -h
add task with end time

Usage: tascli task [OPTIONS] &amp;lt;CONTENT&amp;gt; [TIMESTR]

Arguments:
  &amp;lt;CONTENT&amp;gt;  Description of the task
  [TIMESTR]  Time the task is due, default to EOD

Options:
  -c, --category &amp;lt;CATEGORY&amp;gt;  Category of the task
  -h, --help                 Print help
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46176533</guid><pubDate>Sat, 06 Dec 2025 20:56:38 +0000</pubDate></item><item><title>Coffee linked to slower biological ageing among those with severe mental illness</title><link>https://www.kcl.ac.uk/news/coffee-linked-to-slower-biological-ageing-among-those-with-severe-mental-illness-up-to-a-limit</link><description>&lt;doc fingerprint="776a119b602981bc"&gt;
  &lt;main&gt;
    &lt;quote&gt;&lt;p&gt;We know that coffee can help slow biological ageing in the general population, but little is known about its effect on people with severe mental illness – a population whose lifespan is already shortened, in part due to age-related diseases. Our study shows that up to four cups of coffee per day is linked to longer telomeres among people with bipolar disorder and schizophrenia. This is comparable to a biological age of five years younger than non-coffee drinkers.&lt;/p&gt;Vid Mlakar, PhD student at King’s College London and first author of the study&lt;/quote&gt;
    &lt;p&gt;26 November 2025&lt;/p&gt;
    &lt;head rend="h2"&gt;Coffee linked to slower biological ageing among those with severe mental illness – up to a limit&lt;/head&gt;
    &lt;p&gt;New research from King’s College London finds that coffee consumption within the NHS recommended limit is linked to longer telomere lengths – a marker of biological ageing – among people with bipolar disorder and schizophrenia. The effect is comparable to roughly five years younger biological age.&lt;/p&gt;
    &lt;p&gt;Telomeres are structures that protect DNA. As people get older, their telomeres shorten as part of the natural human ageing process. This process has been shown to be accelerated among people with severe mental illness, such as bipolar disorder and schizophrenia, who have an average life expectancy 15 years shorter than the general population.&lt;/p&gt;
    &lt;p&gt;Previous research shows that coffee possesses health benefits. It may reduce oxidative stress in the general population, helping slow biological ageing processes like telomere shortening. The new study, published in BMJ Mental Health, explores whether coffee consumption could slow this ageing process among those with severe mental illness.&lt;/p&gt;
    &lt;p&gt;Researchers at the Institute of Psychiatry, Psychology &amp;amp; Neuroscience measured the effects of coffee consumption on telomere length among 436 participants aged 18 to 65 with schizophrenia, bipolar disorder or major depressive disorder with psychosis.&lt;/p&gt;
    &lt;p&gt;They found that coffee consumption of up to four cups per day was linked to longer telomeres, comparable to a biological age five years younger than non-coffee drinkers.&lt;/p&gt;
    &lt;p&gt;The longest telomeres were seen among those who consumed three to four cups per day. Too much coffee reduced this positive effect, with participants who consumed more than four cups having shorter telomeres than those who consumed between three and four cups.&lt;/p&gt;
    &lt;p&gt;These effects remained after accounting for variations in age, sex, ethnicity, medication and tobacco use.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Coffee is a beverage that many people consume daily. On one hand, we know that excessive coffee consumption can have negative effects on health, such as reducing sleep quality. However, our new study suggests that coffee consumption up to a certain point may have benefits for biological ageing. Many of the factors that are known to affect biological ageing, such as genetics and negative stressful life experiences, are beyond our control. Lifestyle factors like coffee consumption are something we can actively modify, making research like this particularly valuable.&lt;/p&gt;Dr Monica Aas, MRC Research Fellow at King’s College London and senior author of the study&lt;/quote&gt;
    &lt;p&gt;Dr Aas added: "Studies such as this also support the idea that we should move away from viewing coffee as simply “good or bad”, and instead consider a more balanced view. Still, these results need to be confirmed in other independent studies and longitudinal research before we can determine if this is a causal effect."&lt;/p&gt;
    &lt;p&gt;Data were from the Norwegian TOP study, collected between 2007 and 2018. The researchers included participants who had available data on mental health diagnosis (assessed using the Structured Clinical Interview for DSM-IV), telomere length (measured by extracting DNA from blood samples) and self-reported coffee consumption.&lt;/p&gt;
    &lt;p&gt;The researchers note that the study did not have information on the type of coffee consumed (instant versus filter) or the caffeine concentration of each cup. The NHS advises limiting caffeine intake to 400 mg/day (approximately four cups of coffee).&lt;/p&gt;
    &lt;p&gt;The study was funded by the Research Council of Norway, the KG Jebsen Stiftelsen and an Medical Research Council Fellowship. The team has recently received funding from the British Medical Association’s Margaret Temple grant to investigate telomere shortening in a longitudinal cohort of patients with psychosis. This project will allow them to explore further how several lifestyle factors, as well as stress, influence the rate of telomere shortening over time.&lt;/p&gt;
    &lt;p&gt;"Coffee intake is associated with telomere length in severe mental disorders" (Vid Mlakar et al.) was published in BMJ Mental Health. DOI: 10.1136/bmjment-2025-301700&lt;/p&gt;
    &lt;p&gt;For more information, please contact Milly Remmington (School of Mental Health &amp;amp; Psychological Sciences Communications Manager).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46176766</guid><pubDate>Sat, 06 Dec 2025 21:33:03 +0000</pubDate></item><item><title>Screenshots from developers: 2002 vs. 2015 (2015)</title><link>https://anders.unix.se/2015/12/10/screenshots-from-developers--2002-vs.-2015/</link><description>&lt;doc fingerprint="5e3b5b5dcbbf3eec"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Screenshots from developers: 2002 vs. 2015&lt;/head&gt;
    &lt;p&gt;In 2002 I asked a number of developers/Unix people for screenshots of their desktops. I recently republished them, and, seeing the interest this generated, I thought it’d be fun to ask the same people* again 13 years later. To my delight I managed to reach many of them.&lt;/p&gt;
    &lt;p&gt;* Sans Dennis Ritchie and itojun, who are no longer with us.&lt;/p&gt;
    &lt;p&gt;So, without further ado:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;my desktop is pretty boring, since it consists of xterm windows to whatever unix system i am using at the moment. the machine itself is likely to be running some x-window server like exceed on some flavor of windows, though for many years i just used an x terminal.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;If you thought it was boring last time, check this out!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;2002:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I don’t know how to make a screenshot, because I normally use my computer in text-mode. I have X and GNOME installed, but I use them only occasionally.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;2015:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Under X, I use the standard environment of Trisquel, but mostly I type at Emacs in a console.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Well, my desktop is quite boring. I mostly work with four xterms and a few Netscape windows. The KDE bar hides automatically, you can only see a thin grey line at the bottom.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Here is the new one. You'll see that, like before, I have lots of xterms where I work on Vim, Zimbu and email. Now using the Chrome browser, showing off the Zimbu homepage. But clearly everything has become bigger!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Linux (2.4.20-pre5), Gnome2, vim, Pine.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Not that much has changed in 13 years. Still using Linux. Still just a browser window and a ton of terminals hiding behind them. The main change is that switched from Pine to Thunderbird for email at some point. The OS on my laptop here is Ubuntu with Unity although there are a lot of Debian packages installed so it is a bit of a hybrid at this point. Oh, and yes, my son Carl is a lot older now.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Ah, my desktop is pretty boring, I used fvwm 1.24 as my window manager and I try to have no more than 1 or 2 windows open per virtual desktop. I use FreeBSD 4-STABLE as my operating system. I first came across Unix when I got an account on a Pyramid 90x running OSx. This had a dual-universe setup: both AT&amp;amp;T and BSD-style environments, chosen by an environment variable. Initially I was given the AT&amp;amp;T environment, but my friends convinced me to ``come over” to BSD. Since then I’ve been a BSD afficionado.&lt;/p&gt;
      &lt;p&gt;After OSx, SunOS 3.5 and later SunOS releases, until 386BSD 0.1 came out and I started to run BSD at home. Then when 386BSD transmogrified to FreeBSD, I went with FreeBSD.&lt;/p&gt;
      &lt;p&gt;In terms of desktop, I’m a command-line guy, always will be. My favourite editor is vi, my favourite shell is tcsh (but kudos to rc for elegance). So I don’t really feel the need for GUI things like Gnome or KDE :-)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;How things have (and have not changed). I'm still a command-line junkie with at least two xterm windows open. I'm still using a 3x3 virtual desktop. However, instead of fvwm, it is now LXDE. I've also switched from FreeBSD to Linux and I'm running Lubuntu as my distribution.&lt;/p&gt;
      &lt;p&gt;There are a lot of indispensable GUI tools that I use. These include Firefox, lyx, Gimp, KeepassX, Shutter, viking, dia, Wireshark, calibre, audacity, Handbrake and VLC. But where possible I still prefer to script things. My main development languages are still shell, Perl and C.&lt;/p&gt;
      &lt;p&gt;My shell is now bash. The vi keystrokes are burned into my fingertips and, as long as vim can be ported to new systems, that will be my text editor until I pass on. My mail client is now mutt (definitely not a web client) and my mail is stored locally, not on someone else's server.&lt;/p&gt;
      &lt;p&gt;The only issue I have is that, since a job change, I now have to deal with Windoze things. Thus, I have VirtualBox, libreoffice and Wine to help me do that.&lt;/p&gt;
      &lt;p&gt;I started with Unix on a Pyramid 90x. I now have a smart phone that blows the 90x out of the water on performance, RAM and storage. But I'm so very happy that, somewhere down underneath, there is still a Bourne shell and an operating system that does open(), close(), read(), write(), fork() and exec()!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;You’ll probably be sad (or perhaps not) to hear that my desktop hasn’t really changed much at all - still OS X, though because OS X has virtual desktops now I have multiple “desktops” (6 of them) where Mail.app runs on one, Safari on another, Calendar, Slack, etc - all on separate desktops. This makes it a bit boring, but here’s the one I probably spend the most time in - the terminal window desktop. :)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;There we go. Actually, that’s a condensate in one workspace cause I usually use about 4. Some of my favourite apps:&lt;/p&gt;
      &lt;item&gt;http://anjuta.sf.net/ (IDE)&lt;/item&gt;
      &lt;item&gt;http://quirc.org/ (IRC)&lt;/item&gt;
      &lt;item&gt;http://gaim.sf.net/ (IM)&lt;/item&gt;
      &lt;item&gt;http://multignometerm.sf.net/ (Term)&lt;/item&gt;
      &lt;p&gt;not on the shot, but worth noted&lt;/p&gt;
      &lt;item&gt;http://sylpheed.good-day.net/ (Email Client)&lt;/item&gt;
      &lt;p&gt;and of course a shot of RTCW&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;'screenshot as code', I maintain my desktop configuration through saltstack: https://github.com/TTimo/linux-salted/commits/master&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Discussion: Hacker News; reddit: /r/programming, /r/linux&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46176905</guid><pubDate>Sat, 06 Dec 2025 21:55:09 +0000</pubDate></item><item><title>Catala – Law to Code</title><link>https://catala-lang.org</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46177022</guid><pubDate>Sat, 06 Dec 2025 22:11:33 +0000</pubDate></item><item><title>United States Antarctic Program Field Manual (2024) [pdf]</title><link>https://www.usap.gov/usapgov/travelAndDeployment/documents/Continental-Field-Manual-2024.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46177132</guid><pubDate>Sat, 06 Dec 2025 22:26:26 +0000</pubDate></item><item><title>Wave of (Open Street Map) Vandalism in South Korea</title><link>https://www.openstreetmap.org/user/KennyDap/diary/407844</link><description>&lt;doc fingerprint="722999e04011753"&gt;
  &lt;main&gt;
    &lt;p&gt;About a week ago, vandals began defacing the map in South Korea. Over the course of that week, I rolled back hundreds of changes, and with the help of the site’s moderators, I banned over 50 malicious accounts.&lt;/p&gt;
    &lt;p&gt;In fact, the problem arose not even a week ago, but about a month ago. Back then, South Korean media reported that one account had allegedly leaked the locations of all the country’s military bases to the public on Openstreetmap. (link)&lt;/p&gt;
    &lt;p&gt;Even then, this false media claim struck me as disgusting, but it didn’t cause any major problems. Almost simultaneously, another Korean media report about Openstreetmap appeared: allegedly, an error in the domestic mapping services NaverMap and KakaoMap displaying a river in North Korea was linked to Openstreetmap’s activities: (link)&lt;/p&gt;
    &lt;p&gt;It’s hard to say whether this is a deliberate attack on OpenStreetMap or whether the Korean journalists simply lack the basic journalistic training to understand the issue. I’m leaning toward the latter, as if there was a deliberate intent, they would have acted more intelligently. But the result is what matters. This news quickly found its way into the ultra-conservative circles of the Korean right, who are obsessed with conspiracy theories, set up their accounts under the hashtag #YoonAgain, and believe that OpenStreetMap is a creation of Chinese communists and Russian Putinists. They say that yesterday, Russia bombed Ukraine with OpenStreetMap, and tomorrow, North Korea will start bombing the South. So they raided OpenStreetMap with the goal of once again saving (and embarrassing) their country.&lt;/p&gt;
    &lt;p&gt;But what’s funniest about this whole situation for me is that it wasn’t even Korean military bases that were vandalized, but power plants. Apparently, some employee at one of them (or even at a ministry) was such a crazy person that he managed to convince his superiors that there was data somewhere online that needed to be urgently deleted. He didn’t send an official request, didn’t write on the forum, and refuses to engage in dialogue (I tried). He and at least a few other people simply persistently create accounts on OpenStreetMap and try to delete things that I can restore with a single click.&lt;/p&gt;
    &lt;p&gt;That’s how it goes :)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46177198</guid><pubDate>Sat, 06 Dec 2025 22:34:56 +0000</pubDate></item><item><title>Saving Japan's exceptionally rare 'snow monsters'</title><link>https://www.bbc.com/future/article/20251203-japans-disappearing-snow-monsters</link><description>&lt;doc fingerprint="bf6c94c72441867b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;'Nothing else looks like them': Saving Japan's exceptionally rare 'snow monsters'&lt;/head&gt;
    &lt;p&gt;A unique natural wonder is being eroded. Can Japan bring its breathtaking "juhyo" back from the brink?&lt;/p&gt;
    &lt;p&gt;Each winter, the upper slopes of Mount Zao in northern Japan – one of the country's best-known ski areas – are transformed. Fir trees coated in thick frost and snow swell into ghostly figures known as "juhyo" or "snow monsters".&lt;/p&gt;
    &lt;p&gt;Juhyo form only under exceptionally rare atmospheric conditions, emerging when strong, persistent winter winds carry supercooled water droplets that freeze on contact with the local evergreen Aomori todomatsu trees, gradually layering into rime ice.&lt;/p&gt;
    &lt;p&gt;At Mount Zao, these formations occur during sustained westerly winds of up to 26m per second (85ft per second), with surface air temperatures between -6.3C to -0.1C (21-31F) and unusually high cloud liquid water content. Under these precise conditions, the rime thickens on the windward side of trees into overlapping ridges known as "shrimp tails", the distinctive shapes that cluster together to form the towering juhyo figures.&lt;/p&gt;
    &lt;p&gt;"Because such precise meteorological and ecological conditions align in very few places, Zao's snow monsters are a phenomenon almost unique to northern Japan," says Fumitaka Yanagisawa, an emeritus professor of geochemistry who studies the juhyo at Yamagata University.&lt;/p&gt;
    &lt;p&gt;The snow monsters are the biggest winter draw of the Zao area, a mountain range which lies between Japan's Yamagata and Miyagi prefectures and attracts tens of thousands of visitors annually.&lt;/p&gt;
    &lt;p&gt;But recent research indicates that the monsters are becoming slimmer.&lt;/p&gt;
    &lt;p&gt;In August 2025, a research team led by Yanagisawa announced findings that quantified what locals have long observed. By analysing identical-angle photographs of Zao's summit taken since 1933, the team measured the thickness of the figures on a six-point scale. The findings (which have not yet been published in a scientific journal) indicate a widespread shrinking of the juhyo.&lt;/p&gt;
    &lt;p&gt;"In the 1930s, we saw juhyo five to six metres [16-20ft] across," Yanagisawa says. "By the postwar decades, they were often two to three metres [7-10ft]. Since 2019, many are half a metre [1.6ft] or less. Some are barely columns."&lt;/p&gt;
    &lt;p&gt;The cause is twofold, says Yanagisawa: a warming climate and a forest under attack. The host tree, Aomori todomatsu, suffered a moth outbreak in 2013 that stripped its needles. Bark beetles followed in 2015, boring into weakened trunks. Yamagata officials report that around 23,000 firs, about a fifth of the prefectural side's stands, have died. With fewer branches and leaves, there is little surface for snow and ice to cling to.&lt;/p&gt;
    &lt;p&gt;Another 2019 study found that in nearby Yamagata City, average temperatures from December to March have risen by about 2C (3.6F) over the past 120 years. The lower altitude limit of juhyo formation has shifted upward in step with this warming, it found, while the juhyo also last for fewer days of the year.&lt;/p&gt;
    &lt;p&gt;"Unique landscapes are already being lost to climate change," says Akihiko Ito, an ecologist who specialises in forests and climate change at the University of Tokyo.&lt;/p&gt;
    &lt;p&gt;Research shows that Japan's warming climate and extreme weather are already damaging many of its high mountain forests. "Seasonal shifts in spring and autumn can harm leaves, and insect outbreaks are expanding. These stresses may reduce forest growth and density," Ito says.&lt;/p&gt;
    &lt;p&gt;Across Japan's alpine zones, temperatures have been rising faster than the global average since the 1980s. "In scenarios where climate change continues to advance significantly by the end of this century, it is possible that in warmer-than-usual winters, juhyo may no longer form at all," Ito says.&lt;/p&gt;
    &lt;p&gt;The threat has prompted action across Yamagata. In March 2023, the prefecture launched the Juhyo Revival Conference – a permanent council bringing together researchers, officials, local businesses and residents to coordinate long-term efforts to restore the fir forests and preserve Mount Zao's snow monsters.&lt;/p&gt;
    &lt;p&gt;Juhyo are not only a natural spectacle but also a pillar of the local economy. "The influx of tourists supports hotels, restaurants and souvenir shops throughout the area," says Genji Akiba, deputy director of the Zao Onsen Tourism Association. "If the juhyo disappear, it would be a huge blow."&lt;/p&gt;
    &lt;p&gt;"Revival is a strong wish of our citizens," says Yoko Honma, a conservation specialist at Yamagata Prefecture's nature division. Since 2019, the local forest office has transplanted more than 190 naturally regenerated saplings from lower slopes to the summit zone near the ropeway station. "Because it takes 50 to 70 years for these firs to mature, the key is sustaining conservation across generations," says Honma. "We need patience and continuity."&lt;/p&gt;
    &lt;p&gt;In Murayama, about 20km (12 miles) north-west of Zao, students from a forestry and environmental science course at Murayama Technical High School have also taken up the challenge of reviving the firs.&lt;/p&gt;
    &lt;p&gt;More like this:&lt;/p&gt;
    &lt;p&gt;• The overlooked benefits of real Christmas trees&lt;/p&gt;
    &lt;p&gt;• The secrets of the Amazon's most mysterious river&lt;/p&gt;
    &lt;p&gt;• The mysterious black fungus from Chernobyl that may eat radiation&lt;/p&gt;
    &lt;p&gt;Since 2022, the students have been planting Aomori todomatsu trees and studying how to propagate and protect the species. Together with staff from the Yamagata Forest Office, they visit Mount Zao to collect young fir saplings and bring them back to their school for research. There, they cultivate stems through cuttings and experiment with methods for artificially propagating and efficiently producing seedlings.&lt;/p&gt;
    &lt;p&gt;"It's been challenging," says Rin Oizumi, a second-year student in the course. "When the seeds we sowed in heavy rain finally sprouted, I felt both relief and excitement. But it was heartbreaking to find that some plots had been damaged by field mice, which had eaten the young shoots." The students have also conducted preliminary experiments using branches of a related fir species, which have shown successful germination.&lt;/p&gt;
    &lt;p&gt;Kanon Taniai, Oizumi's classmate, recalls seeing more and more fallen or dead trees as she and other students neared the summit one day in July 2024. "It made me feel really sad," she says. "Growing seedlings is hard work, but we want to do what we can to help bring Mount Zao back to life."&lt;/p&gt;
    &lt;p&gt;For Taniai, protecting the Juhyo means passing their legacy to the next generation. "They are called snow monsters because nothing else looks like them," she says. "I want the world to see them, and to feel how special Japan's nature is."&lt;/p&gt;
    &lt;p&gt;--&lt;/p&gt;
    &lt;p&gt;For essential climate news and hopeful developments to your inbox, sign up to the Future Earth newsletter, while The Essential List delivers a handpicked selection of features and insights twice a week.&lt;/p&gt;
    &lt;p&gt;For more science, technology, environment and health stories from the BBC, follow us on Facebook and Instagram.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46177418</guid><pubDate>Sat, 06 Dec 2025 23:06:53 +0000</pubDate></item><item><title>PatchworkOS: An OS for x86_64, built from scratch in C and assembly</title><link>https://github.com/KaiNorberg/PatchworkOS</link><description>&lt;doc fingerprint="3f1fdc1e6ca59fdc"&gt;
  &lt;main&gt;
    &lt;p&gt;PatchworkOS is currently in a very early stage of development, and may have both known and unknown bugs.&lt;/p&gt;
    &lt;p&gt;PatchworkOS is a modular non-POSIX operating system for the x86_64 architecture that rigorously follows an "everything is a file" philosophy, in the style of Plan9. Built from scratch in C and assembly, its intended to be an educational and experimental operating system.&lt;/p&gt;
    &lt;p&gt;In the end this is a project made for fun, but the goal is to make a "real" operating system, one that runs on real hardware and has the performance one would expect from a modern operating system without jumping ahead to user space features, a floppy disk driver and a round-robin scheduler is not enough.&lt;/p&gt;
    &lt;p&gt;Also, this is not a UNIX clone, its intended to be a (hopefully) interesting experiment in operating system design by attempting to use unique algorithms and designs over tried and tested ones. Sometimes this leads to bad results, and sometimes, with a bit of luck, good ones.&lt;/p&gt;
    &lt;p&gt;Finally, despite its experimental nature and scale, the project aims to remain approachable and educational, something that can work as a middle ground between fully educational operating systems like xv6 and production operating system like Linux.&lt;/p&gt;
    &lt;p&gt;Will this project ever reach its goals? Probably not, but thats not the point.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Stress test showing ~100% utilization across 12 CPUs.&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;DOOM running on PatchworkOS using a doomgeneric port.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fully preemptive and tickless EEVDF scheduler based upon the original paper and implemented using a Augmented Red-Black tree to achieve &lt;code&gt;O(log n)&lt;/code&gt;worst case complexity. EEVDF is the same algorithm used in the modern Linux kernel, but ours is obviously a lot less mature.&lt;/item&gt;
      &lt;item&gt;Multithreading and Symmetric Multi Processing with fine-grained locking.&lt;/item&gt;
      &lt;item&gt;Physical and virtual memory management is &lt;code&gt;O(1)&lt;/code&gt;per page and&lt;code&gt;O(n)&lt;/code&gt;where&lt;code&gt;n&lt;/code&gt;is the number of pages per allocation/mapping operation, see benchmarks for more info.&lt;/item&gt;
      &lt;item&gt;File based IPC including pipes, shared memory, sockets and Plan9 inspired "signals" called notes.&lt;/item&gt;
      &lt;item&gt;File based device APIs, including framebuffers, keyboards, mice and more.&lt;/item&gt;
      &lt;item&gt;Synchronization primitives including mutexes, read-write locks, sequential locks, futexes and others.&lt;/item&gt;
      &lt;item&gt;Highly Modular design, even SMP Bootstrapping is done in a module.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unix-style VFS with mountpoints, hardlinks, per-process namespaces, etc.&lt;/item&gt;
      &lt;item&gt;Custom Framebuffer BitMaP (.fbmp) image format, allows for faster loading by removing the need for parsing.&lt;/item&gt;
      &lt;item&gt;Custom Grayscale Raster Font (.grf) font format, allows for antialiasing and kerning without complex vector graphics.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Custom C standard library and system libraries.&lt;/item&gt;
      &lt;item&gt;Highly modular shared memory based desktop environment.&lt;/item&gt;
      &lt;item&gt;Theming via config files.&lt;/item&gt;
      &lt;item&gt;Note that currently a heavy focus has been placed on the kernel and low-level stuff, so user space is quite small... for now.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And much more...&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Replaced &lt;code&gt;fork(), exec()&lt;/code&gt;with&lt;code&gt;spawn()&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;No "user" concept.&lt;/item&gt;
      &lt;item&gt;Non-POSIX standard library.&lt;/item&gt;
      &lt;item&gt;Even heavier focus on "everything is a file".&lt;/item&gt;
      &lt;item&gt;File flags instead of file modes/permissions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Currently limited to RAM disks only (Waiting for USB support).&lt;/item&gt;
      &lt;item&gt;Only support for x86_64.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fully Asynchronous I/O and syscalls (io_uring?).&lt;/item&gt;
      &lt;item&gt;USB support (The holy grail).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As one of the main goals of PatchworkOS is to be educational, I have tried to document the codebase as much as possible along with providing citations to any sources used. Currently, this is still a work in progress, but as old code is refactored and new code is added, I try to add documentation.&lt;/p&gt;
    &lt;p&gt;If you are interested in knowing more, then you can check out the Doxygen generated documentation. For an overview check the &lt;code&gt;topics&lt;/code&gt; section in the sidebar.&lt;/p&gt;
    &lt;p&gt;PatchworkOS uses a "modular" kernel design, meaning that instead of having one big kernel binary, the kernel is split into several smaller "modules" that can be loaded and unloaded at runtime. In effect, the kernel can rewrite itself by adding and removing functionality as needed.&lt;/p&gt;
    &lt;p&gt;This is highly convenient for development but it also has practical advantages, for example, there is no need to load a driver for a device that is not attached to the system, saving memory.&lt;/p&gt;
    &lt;p&gt;The process of making a module is intended to be as straightforward as possible. For the sake of demonstration, we will create a simple "Hello, World!" module.&lt;/p&gt;
    &lt;p&gt;First, we create a new directory in &lt;code&gt;src/kernel/modules/&lt;/code&gt; named &lt;code&gt;hello&lt;/code&gt;, and inside that directory we create a &lt;code&gt;hello.c&lt;/code&gt; file to which we write the following code:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;kernel/module/module.h&amp;gt;
#include &amp;lt;kernel/log/log.h&amp;gt;

#include &amp;lt;stdint.h&amp;gt;

uint64_t _module_procedure(const module_event_t* event)
{
    switch (event-&amp;gt;type)
    {
    case MODULE_EVENT_LOAD:
        LOG_INFO("Hello, World!\n");
        break;
    default:
        break;
    }

    return 0;
}

MODULE_INFO("Hello", "&amp;lt;author&amp;gt;", "A simple hello world module", "1.0", "MIT", "BOOT_ALWAYS");&lt;/code&gt;
    &lt;p&gt;An explanation of the code will be provided later.&lt;/p&gt;
    &lt;p&gt;Now we need to add the module to the build system. To do this, just copy a existing module's &lt;code&gt;.mk&lt;/code&gt; file without making any modifications. For example, we can copy &lt;code&gt;src/modules/drivers/ps2/ps2.mk&lt;/code&gt; to &lt;code&gt;src/modules/hello/hello.mk&lt;/code&gt;. The build system will handle the rest, including copying the module to the final image.&lt;/p&gt;
    &lt;p&gt;Now, we can build and run PatchworkOS using &lt;code&gt;make all run&lt;/code&gt;, or we could use &lt;code&gt;make all&lt;/code&gt; and then flash the generated &lt;code&gt;bin/PatchworkOS.img&lt;/code&gt; file to a USB drive.&lt;/p&gt;
    &lt;p&gt;Now to validate that the module is working, you can either watch the boot log and spot the &lt;code&gt;Hello, World!&lt;/code&gt; message, or you could use &lt;code&gt;grep&lt;/code&gt; on the &lt;code&gt;/dev/klog&lt;/code&gt; file in the terminal program like so:&lt;/p&gt;
    &lt;code&gt;cat /dev/klog | grep "Hello, World!"&lt;/code&gt;
    &lt;p&gt;This should output something like:&lt;/p&gt;
    &lt;code&gt;[   0.747-00-I] Hello, World!&lt;/code&gt;
    &lt;p&gt;Thats all, if this did not work, make sure you followed all the steps correctly, if there is still issues, feel free to open an issue.&lt;/p&gt;
    &lt;p&gt;Whatever you want. You can include any kernel header, or even headers from other modules, create your own modules and include their headers or anything else. There is no need to worry about linking, dependencies or exporting/importing symbols, the kernels module loader will handle all of it for you. Go nuts.&lt;/p&gt;
    &lt;p&gt;This code in the &lt;code&gt;hello.c&lt;/code&gt; file does a few things. First, it includes the relevant kernel headers.&lt;/p&gt;
    &lt;p&gt;Second, it defines a &lt;code&gt;_module_procedure()&lt;/code&gt; function. This function serves as the entry point for the module and will be called by the kernel to notify the module of events, for example the module being loaded or a device attached. On the load event, it will print using the kernels logging system &lt;code&gt;"Hello, World!"&lt;/code&gt;, resulting in the message being readable from &lt;code&gt;/dev/klog&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Finally, it defines the modules information. This information is, in order, the name of the module, the author of the module (thats you), a short description of the module, the module version, the licence of the module, and finally a list of "device types", in this case just &lt;code&gt;BOOT_ALWAYS&lt;/code&gt;, but more could be added by separating them with a semicolon (&lt;code&gt;;&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;The list of device types is what causes the kernel to actually load the module. I will avoid going into to much detail (you can check the documentation for that), but I will explain it briefly.&lt;/p&gt;
    &lt;p&gt;The module loader itself has no idea what these type strings actually are, but subsytems within the kernel can specify that "a device of the type represented by this string is now available", the module loader can then load either one or all modules that have specified in their list of device types that it can handle the specified type. This means that any new subsystem, ACPI, USB, PCI, etc, can implement dynamic module loading using whatever types they want.&lt;/p&gt;
    &lt;p&gt;So what is &lt;code&gt;BOOT_ALWAYS&lt;/code&gt;? It is the type of a special device that the kernel will pretend to "attach" during boot. In this case, it simply causes our hello module to be loaded during boot.&lt;/p&gt;
    &lt;p&gt;For more information, check the Module Doxygen Documentation.&lt;/p&gt;
    &lt;p&gt;PatchworkOS features a from-scratch ACPI implementation and AML parser, with the goal of being, atleast by ACPI standards, easy to understand and educational. It is tested on the Tested Configurations below and against ACPICA's runtime test suite, but remains a work in progress (and probably always will be).&lt;/p&gt;
    &lt;p&gt;See ACPI Doxygen Documentation for a progress checklist.&lt;/p&gt;
    &lt;p&gt;See ACPI specification Version 6.6 as the main reference.&lt;/p&gt;
    &lt;p&gt;ACPI or Advanced Configuration and Power Interface is used for alot of things in modern systems but mainly power management and device enumeration/configuration. Its not possible to go over everything here, instead a brief overview of the parts most likely to cause confusion while reading the code will be provided.&lt;/p&gt;
    &lt;p&gt;It consists of two main parts, the ACPI tables and AML bytecode. If you have completed a basic operating systems tutorial, you have probably seen the ACPI tables before, for example the RSDP, FADT, MADT, etc. These tables are static in memory data structures storing information about the system, they are very easy to parse but are limited in what they can express.&lt;/p&gt;
    &lt;p&gt;AML or ACPI Machine Language is a turning complete "mini language", and the source of mutch frustration, that is used to express more complex data, primarily device configuration. This is needed as its impossible for any specification to account for every possible hardware configuration that exists currently, much less that may exist in the future. So instead of trying to design that, what if we could just had a small program generate whatever data we wanted dynamically? Well thats more or less what AML is.&lt;/p&gt;
    &lt;p&gt;To demonstrate how ACPI is used for device configuration, we will use the PS/2 driver as an example.&lt;/p&gt;
    &lt;p&gt;If you have followed a basic operating systems tutorial, you have probably implemented a PS/2 keyboard driver at some point, and most likely you hardcoded the I/O ports &lt;code&gt;0x60&lt;/code&gt; and &lt;code&gt;0x64&lt;/code&gt; for data and commands respectively, and IRQ &lt;code&gt;1&lt;/code&gt; for keyboard interrupts.&lt;/p&gt;
    &lt;p&gt;Using this hardcoded approach will work for the vast majority of systems, but, perhaps surprisingly, there is no standard that guarantees that these ports and IRQs will actually be used for PS/2 devices. Its just a silent agreement that pretty much all systems adhere to for legacy reasons.&lt;/p&gt;
    &lt;p&gt;But this is where the device configuration from AML comes in, it lets us query the system for the actual resources used by the PS/2 keyboard, so we dont have to rely on hardcoded values.&lt;/p&gt;
    &lt;p&gt;If you where to decompile the AML bytecode into its original ASL (ACPI Source Language), you might find something like this:&lt;/p&gt;
    &lt;code&gt;Device (KBD)
{
    Name (_HID, EisaId ("PNP0303") /* IBM Enhanced Keyboard (101/102-key, PS/2 Mouse) */)  // _HID: Hardware ID
    Name (_STA, 0x0F)  // _STA: Status
    Name (_CRS, ResourceTemplate ()  // _CRS: Current Resource Settings
    {
        IO (Decode16,
            0x0060,             // Range Minimum
            0x0060,             // Range Maximum
            0x01,               // Alignment
            0x01,               // Length
            )
        IO (Decode16,
            0x0064,             // Range Minimum
            0x0064,             // Range Maximum
            0x01,               // Alignment
            0x01,               // Length
            )
        IRQNoFlags ()
            {1}
    })
}&lt;/code&gt;
    &lt;p&gt;Note that just like C compiles to assembly, ASL compiles to AML bytecode, which is what the OS actually parses.&lt;/p&gt;
    &lt;p&gt;In the example ASL, we se a &lt;code&gt;Device&lt;/code&gt; object representing a PS/2 keyboard. It has a hardware ID (&lt;code&gt;_HID&lt;/code&gt;), which we can cross reference with a online database to confirm that it is indeed a PS/2 keyboard, a status (&lt;code&gt;_STA&lt;/code&gt;), which is just a bitfield indicating if the device is present, enabled, etc, and finally the current resource settings (&lt;code&gt;_CRS&lt;/code&gt;), which is the thing we are really after.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;_CRS&lt;/code&gt; might look a bit complicated but focus on the &lt;code&gt;IO&lt;/code&gt; and &lt;code&gt;IRQNoFlags&lt;/code&gt; entries. Notice how they are specifying the I/O ports and IRQ used by the keyboard? Which in this case are indeed &lt;code&gt;0x60&lt;/code&gt;, &lt;code&gt;0x64&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt; respectively. So in this case the standard held true.&lt;/p&gt;
    &lt;p&gt;So how is this information used? Durring boot, the &lt;code&gt;_CRS&lt;/code&gt; information of each device is parsed by the ACPI subsystem, it then queries the kernel for the needed resources, assigned them to each device and makes the final configuration available to drivers.&lt;/p&gt;
    &lt;p&gt;Then when the PS/2 driver is loaded, it gets told "you are handling a device with the name &lt;code&gt;\_SB_.PCI0.SF8_.KBD_&lt;/code&gt; (which is just the full path to the device object in the ACPI namespace) and the type &lt;code&gt;PNP0303&lt;/code&gt;", it can then query the ACPI subsystem for the resources assigned to that device, and use them instead of hardcoded values.&lt;/p&gt;
    &lt;p&gt;Having access to this information for all devices also allows us to avoid resource conflicts, making sure two devices are not trying to use the same IRQ(s) or I/O port(s).&lt;/p&gt;
    &lt;p&gt;Of course, it gets way, way worse than this, but hopefully this clarifies why the PS/2 driver and other drivers, might look a bit different than what you might be used to.&lt;/p&gt;
    &lt;p&gt;PatchworkOS strictly follows the "everything is a file" philosophy in a way similar to Plan9, this can often result in unorthodox APIs or could just straight up seem overly complicated, but it has its advantages. We will use sockets to demonstrate the kinds of APIs this produces.&lt;/p&gt;
    &lt;p&gt;In order to create a local seqpacket socket, you open the &lt;code&gt;/net/local/seqpacket&lt;/code&gt; file. The opened file can be read to return the ID of your created socket. We provide several helper functions to make this easier, first, without any helpers, you would do&lt;/p&gt;
    &lt;code&gt;fd_t fd = open("/net/local/seqpacket");
if (fd == ERR) 
{
    /// ... handle error ...
}
char id[32] = {0};
if (read(fd, id, 31) == ERR) 
{
    /// ... handle error ...
}
close(fd);&lt;/code&gt;
    &lt;p&gt;Using the &lt;code&gt;sread()&lt;/code&gt; helper that reads the entire contents of a file descriptor to simplify this to&lt;/p&gt;
    &lt;code&gt;fd_t fd = open("/net/local/seqpacket");
if (fd == ERR) 
{
    /// ... handle error ...
}
char* id = sread(fd); 
if (id == NULL) 
{
    /// ... handle error ...
}
close(fd);
// ... do stuff ...
free(id);&lt;/code&gt;
    &lt;p&gt;Finally, you can use the &lt;code&gt;sreadfile()&lt;/code&gt; helper that reads the entire contents of a file from its path to simplify this even further to&lt;/p&gt;
    &lt;code&gt;char* id = sreadfile("/net/local/seqpacket"); 
if (id == NULL) 
{
    /// ... handle error ...
}
// ... do stuff ...
free(id);&lt;/code&gt;
    &lt;p&gt;Note that the socket will persist until the process that created it and all its children have exited. Additionally, for error handling, all functions will return either &lt;code&gt;NULL&lt;/code&gt; or &lt;code&gt;ERR&lt;/code&gt; on failure, depending on if they return a pointer or an integer type respectively. The per-thread &lt;code&gt;errno&lt;/code&gt; variable is used to indicate the specific error that occurred, both in user space and kernel space.&lt;/p&gt;
    &lt;p&gt;The ID that we have retrieved is the name of a directory in the &lt;code&gt;/net/local&lt;/code&gt; directory, in which are three files that we use to interact with the socket. These files are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;data&lt;/code&gt;: used to send and retrieve data&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ctl&lt;/code&gt;: Used to send commands&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;accept&lt;/code&gt;: Used to accept incoming connections&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So, for example, the sockets data file is located at &lt;code&gt;/net/local/[id]/data&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Say we want to make our socket into a server, we would then use the bind and listen commands with the &lt;code&gt;ctl&lt;/code&gt; file, Once again, we provide several helper functions to make this easier. First, without any helpers, you would do&lt;/p&gt;
    &lt;code&gt;char ctlPath[MAX_PATH] = {0};
if (snprintf(ctlPath, MAX_PATH, "/net/local/%s/ctl", id) &amp;lt; 0) 
{
    /// ... handle error ...
}
fd_t ctl = open(ctlPath);
if (ctl == ERR) 
{
    /// ... handle error ...
}
if (write(ctl, "bind myserver &amp;amp;&amp;amp; listen") == ERR) // We use &amp;amp;&amp;amp; to chain commands.
{
    /// ... handle error ...
}
close(ctl);&lt;/code&gt;
    &lt;p&gt;Using the &lt;code&gt;F()&lt;/code&gt; macro which allocates formatted strings on the stack and the &lt;code&gt;swrite()&lt;/code&gt; helper that writes a null-terminated string to a file descriptor, we can simplify this to&lt;/p&gt;
    &lt;code&gt;fd_t ctl = open(F("/net/local/%s/ctl", id));
if (ctl == ERR) 
{
    /// ... handle error ...
}
if (swrite(ctl, "bind myserver &amp;amp;&amp;amp; listen") == ERR)
{
    /// ... handle error ...
}
close(ctl);&lt;/code&gt;
    &lt;p&gt;Finally, using the &lt;code&gt;swritefile()&lt;/code&gt; helper that writes a null-terminated string to a file from its path, we can simplify this even further to&lt;/p&gt;
    &lt;code&gt;if (swritefile(F("/net/local/%s/ctl", id), "bind myserver &amp;amp;&amp;amp; listen") == ERR)
{
    /// ... handle error ...
}&lt;/code&gt;
    &lt;p&gt;Note that we name our server &lt;code&gt;myserver&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If we wanted to accept a connection using our newly created server, we just open its accept file by writing&lt;/p&gt;
    &lt;code&gt;fd_t fd = open(F("/net/local/%s/accept", id));
if (fd == ERR) 
{
    /// ... handle error ...
}
/// ... do stuff ...
close(fd);&lt;/code&gt;
    &lt;p&gt;The file descriptor returned when the accept file is opened can be used to send and receive data, just like when calling &lt;code&gt;accept()&lt;/code&gt; in for example Linux or other POSIX operating systems. The entire socket API attempts to mimic the POSIX socket API, apart from using these weird files everything (should) work as expected.&lt;/p&gt;
    &lt;p&gt;For the sake of completeness, if we wanted to connect to this server, we can do&lt;/p&gt;
    &lt;code&gt;char* id = sreadfile("/net/local/seqpacket"); // Create new socket and get its ID.
if (id == NULL) 
{
    /// ... handle error ...
}
if (swritefile(F("/net/local/%s/ctl", id), "connect myserver") == ERR) // Connect to the server named "myserver".
{
    /// ... handle error ...
}
/// ... do stuff ...
free(id);&lt;/code&gt;
    &lt;p&gt;Namespaces are a set of mountpoints that is unique per process, which allows each process a unique view of the file system and is utilized for access control.&lt;/p&gt;
    &lt;p&gt;Think of it like this, in the common case, you can mount a drive to &lt;code&gt;/mnt/mydrive&lt;/code&gt; and all processes can then open the &lt;code&gt;/mnt/mydrive&lt;/code&gt; path and see the contents of that drive. However, for security reasons we might not want every process to be able to see that drive, this is what namespaces enable, allowing mounted file systems or directories to only be visible to a subset of processes.&lt;/p&gt;
    &lt;p&gt;As an example, the "id" directories mentioned in the socket example are a separate "sysfs" instance mounted in the namespace of the creating process, meaning that only that process and its children can see their contents.&lt;/p&gt;
    &lt;p&gt;To control which processes can see a newly mounted or bound file system or directory, we use a propegation system, where a the newly created mountpoint can be made visible to either just the creating process, the creating process and its children, or the creating process, its children and its parents. Additionally, its possible to specify the behaviour of mountpoint inheritance when a new process is spawned.&lt;/p&gt;
    &lt;p&gt;In cases where the propagation system is not sufficient, it's possible for two processes to voluntarily share a mountpoint in their namespaces using &lt;code&gt;bind()&lt;/code&gt; in combination with two new system calls &lt;code&gt;share()&lt;/code&gt; and &lt;code&gt;claim()&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;For example, if process A wants to share its &lt;code&gt;/net/local/5&lt;/code&gt; directory from the socket example with process B, they can do&lt;/p&gt;
    &lt;code&gt;// In process A
fd_t dir = open("/net/local/5:directory");

// Create a "key" for the file descriptor, this is a unique one time use randomly generated token that can be used to retrieve the file descriptor in another process.
key_t key;
share(&amp;amp;key, dir, CLOCKS_PER_SEC * 60); // Key valid for 60 seconds (CLOCKS_NEVER is also allowed)

// In process B
// The key is somehow communicated to B via IPC, for example a pipe, socket, argv, etc.
key_t key = ...;

// Use the key to open a file descriptor to the directory, this will invalidate the key.
fd_t dir = claim(key);
// Will error here if the original file descriptor in process A has been closed, process A exited, or the key expired.

// Make "dir" ("/net/local/5" in A) available in B's namespace at "/any/path/it/wants". In practice it might be best to bind it to the same path as in A to avoid confusion.
bind(dir, "/any/path/it/wants");

// Its also possible to just open paths in the shared directory without polluting the namespace using openat().
fd_t somePath = openat(dir, "data");&lt;/code&gt;
    &lt;p&gt;Note that error checking is ommited for brevity.&lt;/p&gt;
    &lt;p&gt;This system guarantees consent between processes, and can be used to implement more complex access control systems.&lt;/p&gt;
    &lt;p&gt;An interesting detail is that when process A opens the &lt;code&gt;net/local/5&lt;/code&gt; directory, the dentry underlying the file descriptor is the root of the mounted file system, if process B were to try to open this directory, it would still succeed as the directory itself is visible, however process B would instead retrieve the dentry of the directory in the parent superblock, and would instead see the content of that directory in the parent superblock. If this means nothing to you, don't worry about it.&lt;/p&gt;
    &lt;p&gt;You may have noticed that in the above section sections, the &lt;code&gt;open()&lt;/code&gt; function does not take in a flags argument. This is because flags are part of the file path directly so if you wanted to create a non-blocking socket, you can write&lt;/p&gt;
    &lt;code&gt;open("/net/local/seqpacket:nonblock");&lt;/code&gt;
    &lt;p&gt;Multiple flags are allowed, just separate them with the &lt;code&gt;:&lt;/code&gt; character, this means flags can be easily appended to a path using the &lt;code&gt;F()&lt;/code&gt; macro. Each flag also has a short hand version for which the &lt;code&gt;:&lt;/code&gt; character is ommited, for example to open a file as create and exclusive, you can do&lt;/p&gt;
    &lt;code&gt;open("/some/path:create:exclusive");&lt;/code&gt;
    &lt;p&gt;or&lt;/p&gt;
    &lt;code&gt;open("/some/path:ce");&lt;/code&gt;
    &lt;p&gt;For a full list of available flags, check the Doxygen documentation.&lt;/p&gt;
    &lt;p&gt;Permissions are also specified using file paths there are three possible permissions, read, write and execute. For example to open a file as read and write, you can do&lt;/p&gt;
    &lt;code&gt;open("/some/path:read:write");&lt;/code&gt;
    &lt;p&gt;or&lt;/p&gt;
    &lt;code&gt;open("/some/path:rw");&lt;/code&gt;
    &lt;p&gt;Permissions are inherited, you cant use a file with lower permissions to get a file with higher permissions. Consider the namespace section, if a directory was opened using only read permissions and that same directory was bound, then it would be impossible to open any files within that directory with any permissions other than read.&lt;/p&gt;
    &lt;p&gt;For a full list of available permissions, check the Doxygen documentation.&lt;/p&gt;
    &lt;p&gt;Im sure you have heard many an argument for and against the "everything is a file" philosophy. So I wont go over everything, but the primary reason for using it in PatchworkOS is "emergent behavior" or "composability" which ever term you prefer.&lt;/p&gt;
    &lt;p&gt;Take the namespace sharing example, notice how there isent any actually dedicated "namespace sharing" system? There are instead a series of small, simple building blocks that when added together form a more complex whole. That is emergent behavior, by keeping things simple and most importantly composable, we can create very complex behaviour without needing to explicitly design it.&lt;/p&gt;
    &lt;p&gt;Lets take another example, say you wanted to wait on multiple processes with a &lt;code&gt;waitpid()&lt;/code&gt; syscall. Well, thats not possible. So now we suddenly need a new system call. Meanwhile, in a "everything is a file system" we just have a pollable &lt;code&gt;/proc/[pid]/wait&lt;/code&gt; file that blocks untill the process dies and returns the exit status, now any behaviour that can be implemented with &lt;code&gt;poll()&lt;/code&gt; can be used while waiting on processes, including waiting on multiple processes at once, waiting on a keyboard and a process, waiting with a timeout, or any weird combination you can think of.&lt;/p&gt;
    &lt;p&gt;Plus its fun.&lt;/p&gt;
    &lt;p&gt;All benchmarks were run on real hardware using a Lenovo ThinkPad E495. For comparison, I've decided to use the Linux kernel, specifically Fedora since It's what I normally use.&lt;/p&gt;
    &lt;p&gt;Note that Fedora will obviously have a lot more background processes running and security features that might impact performance, so these benchmarks are not exactly apples to apples, but they should still give a good baseline for how PatchworkOS performs.&lt;/p&gt;
    &lt;p&gt;All code for benchmarks can be found in the benchmark program, all tests were run using the optimization flag &lt;code&gt;-O3&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The test maps and unmaps memory in varying page amounts for a set amount of iterations using generic mmap and munmap functions. Below is the results from PatchworkOS as of commit &lt;code&gt;4b00a88&lt;/code&gt; and Fedora 40, kernel version &lt;code&gt;6.14.5-100.fc40.x86_64&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;xychart-beta
title "Blue: PatchworkOS, Green: Linux (Fedora), Lower is Better"
x-axis "Page Amount (in 50s)" [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
y-axis "Time (ms)" 0 --&amp;gt; 40000
line [157, 275, 420, 519, 622, 740, 838, 955, 1068, 1175, 1251, 1392, 1478, 1601, 1782, 1938, 2069, 2277, 2552, 2938, 3158, 3473, 3832, 4344, 4944, 5467, 6010, 6554, 7114, 7486]
line [1138, 2226, 3275, 4337, 5453, 6537, 7627, 8757, 9921, 11106, 12358, 13535, 14751, 16081, 17065, 18308, 20254, 21247, 22653, 23754, 25056, 26210, 27459, 28110, 29682, 31096, 33547, 34840, 36455, 37660]
&lt;/code&gt;
    &lt;p&gt;We see that PatchworkOS performs better across the board, and the performance difference increases as we increase the page count.&lt;/p&gt;
    &lt;p&gt;There are a few potential reasons for this, one is that PatchworkOS does not use a separate structure to manage virtual memory, instead it embeds metadata directly into the page tables, and since accessing a page table is just walking some pointers, its highly efficient, additionally it provides better caching since the page tables are likely already in the CPU cache.&lt;/p&gt;
    &lt;p&gt;In the end we end up with a &lt;/p&gt;
    &lt;p&gt;Note that as the number of pages increases we start to see less and less linear performance, this is most likely due to CPU cache saturation.&lt;/p&gt;
    &lt;p&gt;For fun, we can throw the results into desmos to se that around &lt;/p&gt;
    &lt;p&gt;Performing quadratic regression on the same data gives us&lt;/p&gt;
    &lt;p&gt;From this we see that for &lt;/p&gt;
    &lt;p&gt;Of course, there are limitations to this approach, for example, it is in no way portable (which isn't a concern in our case), each address space can only contain &lt;code&gt;spawn()&lt;/code&gt; instead of a &lt;code&gt;fork()&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;All in all, this algorithm would not be a viable replacement for existing algorithms, but for PatchworkOS, it serves its purpose very efficiently.&lt;/p&gt;
    &lt;p&gt;PatchworkOS includes its own shell utilities designed around its file flags system, when file flags are used we also demonstrate the short form. Included is a brief overview with some usage examples. For convenience the shell utilities are named after their POSIX counterparts, however they are not drop-in replacements.&lt;/p&gt;
    &lt;p&gt;Opens a file path and then immediately closes it.&lt;/p&gt;
    &lt;code&gt;# Create the file.txt file only if it does not exist.
touch file.txt:create:exclusive
touch file.txt:ce

# Create the mydir directory.
touch mydir:create:directory
touch mydir:cd&lt;/code&gt;
    &lt;p&gt;Reads from stdin or provided files and outputs to stdout.&lt;/p&gt;
    &lt;code&gt;# Read the contents of file1.txt and file2.txt.
cat file1.txt file2.txt

# Read process exit status (blocks until process exits)
cat /proc/1234/wait

# Copy contents of file.txt to dest.txt and create it.
cat &amp;lt; file.txt &amp;gt; dest.txt:create
cat &amp;lt; file.txt &amp;gt; dest.txt:c&lt;/code&gt;
    &lt;p&gt;Writes to stdout.&lt;/p&gt;
    &lt;code&gt;# Write to file.txt.
echo "..." &amp;gt; file.txt

# Append to file.txt, makes "&amp;gt;&amp;gt;" unneeded.
echo "..." &amp;gt; file.txt:append
echo "..." &amp;gt; file.txt:a&lt;/code&gt;
    &lt;p&gt;Reads the contents of a directory to stdout.&lt;/p&gt;
    &lt;code&gt;# Prints the contents of mydir.
ls mydir

# Recursively print the contents of mydir.
ls mydir:recursive
ls mydir:R&lt;/code&gt;
    &lt;p&gt;Removes a file or directory.&lt;/p&gt;
    &lt;code&gt;# Remove file.txt.
rm file.txt

# Recursively remove mydir and its contents.
rm mydir:directory:recursive
rm mydir:dR&lt;/code&gt;
    &lt;p&gt;There are other utils available that work as expected, for example &lt;code&gt;stat&lt;/code&gt; and &lt;code&gt;link&lt;/code&gt;.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Requirement&lt;/cell&gt;
        &lt;cell role="head"&gt;Details&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;OS&lt;/cell&gt;
        &lt;cell&gt;Linux (WSL might work, but I make no guarantees)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Tools&lt;/cell&gt;
        &lt;cell&gt;GCC, make, NASM, mtools, QEMU (optional)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;# Clone this repository, you can also use the green Code button at the top of the Github.
git clone https://github.com/KaiNorberg/PatchworkOS
cd PatchworkOS

# Build (creates PatchworkOS.img in bin/)
make all

# Run using QEMU
make run&lt;/code&gt;
    &lt;code&gt;# Clean build files
make clean

# Build with debug mode enabled
make all DEBUG=1

# Build with debug mode enabled and testing enabled (you will need to have iasl installed)
make all DEBUG=1 TESTING=1

# Debug using qemu with one cpu and GDB
make all run DEBUG=1 QEMU_CPUS=1 GDB=1

# Debug using qemu and exit on panic
make all run DEBUG=1 QEMU_EXIT_ON_PANIC=1

# Generate doxygen documentation
make doxygen

# Create compile commands file
make compile_commands&lt;/code&gt;
    &lt;p&gt;Source code can be found in the &lt;code&gt;src/&lt;/code&gt; directory, with public API headers in the &lt;code&gt;include/&lt;/code&gt; directory, private API headers are located alongside their respective source files.&lt;/p&gt;
    &lt;code&gt;.
├── meta              // Meta files including screenshots, doxygen, etc.
├── lib               // Third party files, for example doomgeneric.
├── root              // Files to copy to the root of the generated image.
└── &amp;lt;src|include&amp;gt;     // Source code and public API headers.
    ├── kernel        // The kernel and its core subsystems.
    ├── modules       // Kernel modules, drivers, filesystems, etc.
    ├── programs      // User space programs.
    ├── libstd        // The C standard library.
    └── libpatchwork  // The PatchworkOS system library, gui, etc.
&lt;/code&gt;
    &lt;p&gt;For frequent testing, it might be inconvenient to frequently flash to a USB. You can instead set up the &lt;code&gt;.img&lt;/code&gt; file as a loopback device in GRUB.&lt;/p&gt;
    &lt;p&gt;Add this entry to the &lt;code&gt;/etc/grub.d/40_custom&lt;/code&gt; file:&lt;/p&gt;
    &lt;code&gt;menuentry "Patchwork OS" {
        set root="[The grub identifer for the drive. Can be retrived using: sudo grub2-probe --target=drive /boot]"
        loopback loop0 /PatchworkOS.img # Might need to be modified based on your setup.
        set root=(loop0)
        chainloader /efi/boot/bootx64.efi
}&lt;/code&gt;
    &lt;p&gt;Regenerate grub configuration using &lt;code&gt;sudo grub2-mkconfig -o /boot/grub2/grub.cfg&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Finally copy the generated &lt;code&gt;.img&lt;/code&gt; file to your &lt;code&gt;/boot&lt;/code&gt; directory, this can also be done with &lt;code&gt;make grub_loopback&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;You should now see a new entry in your GRUB boot menu allowing you to boot into the OS, like dual booting, but without the need to create a partition.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;QEMU boot failure: Check if you are using QEMU version 10.0.0, as that version has previously caused issues. These issues appear to be fixed currently however consider using version 9.2.3&lt;/item&gt;
      &lt;item&gt;Any other errors?: If an error not listed here occurs or is not resolvable, please open an issue in the GitHub repository.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Testing uses a GitHub action that compiles the project and runs it for some amount of time using QEMU with &lt;code&gt;DEBUG=1&lt;/code&gt;, &lt;code&gt;TESTING=1&lt;/code&gt; and &lt;code&gt;QEMU_EXIT_ON_PANIC=1&lt;/code&gt; set. This will run some additional tests in the kernel (for example it will clone ACPICA and run all its runtime tests), and if QEMU has not crashed by the end of the allotted time, it is considered a success.&lt;/p&gt;
    &lt;p&gt;Note that the &lt;code&gt;QEMU_EXIT_ON_PANIC&lt;/code&gt; flag will cause any failed test, assert or panic in the kernel to exit QEMU using their "-device isa-debug-exit" feature with a non-zero exit code, thus causing the GitHub action to fail.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;QEMU emulator version 9.2.3 (qemu-9.2.3-1.fc42)&lt;/item&gt;
      &lt;item&gt;Lenovo ThinkPad E495&lt;/item&gt;
      &lt;item&gt;Ryzen 5 3600X | 32GB 3200MHZ Corsair Vengeance&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Currently untested on Intel hardware (broke student, no access to hardware). Let me know if you have different hardware, and it runs (or doesn't) for you!&lt;/p&gt;
    &lt;p&gt;Contributions are welcome! Anything from bug reports/fixes, performance improvements, new features, or even just fixing typos or adding documentation.&lt;/p&gt;
    &lt;p&gt;If you are unsure where to start, check the Todo List.&lt;/p&gt;
    &lt;p&gt;Check out the contribution guidelines to get started.&lt;/p&gt;
    &lt;p&gt;The first Reddit post and image of PatchworkOS from back when getting to user space was a massive milestone and the kernel was supposed to be a UNIX-like microkernel.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46177615</guid><pubDate>Sat, 06 Dec 2025 23:33:23 +0000</pubDate></item><item><title>Kilauea erupts, destroying webcam [video]</title><link>https://www.youtube.com/watch?v=TK2N99BDw7A</link><description>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket © 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46177645</guid><pubDate>Sat, 06 Dec 2025 23:39:02 +0000</pubDate></item><item><title>OpenTelemetry Distribution Builder</title><link>https://github.com/observIQ/otel-distro-builder</link><description>&lt;doc fingerprint="d80343bd7f07083"&gt;
  &lt;main&gt;
    &lt;p&gt;Build custom OpenTelemetry Collector Distributions from manifest files with a local build utility, Docker, Google Cloud Build, or a GitHub Action.&lt;/p&gt;
    &lt;p&gt;Built on top of the OpenTelemetry Collector Builder (OCB), it uses a &lt;code&gt;manifest.yaml&lt;/code&gt; to define the components you need, then automates packaging for multiple platforms and manages version releases via GitHub.&lt;/p&gt;
    &lt;p&gt;While OCB (OpenTelemetry Collector Builder) focuses on building single collector binaries, the OpenTelemetry Distribution Builder provides a complete distribution management solution:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🔨 Builds multi-platform binaries using OCB under the hood&lt;/item&gt;
      &lt;item&gt;📦 Generates installation packages following OTel community best practices&lt;/item&gt;
      &lt;item&gt;🚀 Automates versioned releases through GitHub Actions&lt;/item&gt;
      &lt;item&gt;🔄 Simplifies updates through manifest-based configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It handles all the complex aspects of managing your own distribution that have historically made building custom collectors challenging. With the OpenTelemetry Distribution Builder, you can focus on defining your components while the tooling takes care of the rest.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🎯 Custom Component Selection: Build distributions with exactly the components you need&lt;/item&gt;
      &lt;item&gt;🌐 Multi-Platform Support: Build for multiple architectures (amd64, arm64)&lt;/item&gt;
      &lt;item&gt;📦 Multiple Package Formats: Generate APK, DEB, RPM, and TAR.GZ packages&lt;/item&gt;
      &lt;item&gt;🔄 GitHub Actions Integration: Seamless CI/CD integration&lt;/item&gt;
      &lt;item&gt;🚀 Automated Releases: Streamlined versioning and release process&lt;/item&gt;
      &lt;item&gt;🔍 Platform-Specific Builds: Optimize for your target environment&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Create a new repository&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Add your manifest file (&lt;/p&gt;&lt;code&gt;manifest.yaml&lt;/code&gt;):&lt;quote&gt;dist: name: my-otelcol description: My Custom OpenTelemetry Collector Distro # ... extensions: - # ... exporters: - # ... processors: - # ... receivers: - # ... connectors: - # ... providers: - # ...&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Set up GitHub Actions (&lt;/p&gt;&lt;code&gt;.github/workflows/build.yml&lt;/code&gt;):&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;name: OpenTelemetry Distribution Build

on:
   push:
     tags:
       - "v*"
   workflow_dispatch:

  permissions:
    contents: write # This is required for creating/modifying releases

  jobs:
    build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      # Build the OpenTelemetry distribution using this custom action
      - uses: observiq/otel-distro-builder@v1
        with:
          manifest: "./manifest.yaml"

      # Create a GitHub Release and attach the build artifacts
      # This makes the artifacts available for download from the Releases page
      - name: Create Release
        uses: softprops/action-gh-release@v2
        with:
          files: ${{ github.workspace }}/artifacts/*&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Trigger a build:&lt;/p&gt;
        &lt;code&gt;git tag v1.0.0 &amp;amp;&amp;amp; git push --tags&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;(Optional) Build with Docker:&lt;/p&gt;
        &lt;quote&gt;docker pull ghcr.io/observiq/otel-distro-builder:main docker run --rm -v $(pwd):/workspace -v $(pwd)/build:/build ghcr.io/observiq/otel-distro-builder:main \ --manifest /workspace/manifest.yaml&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To view detailed guides, see the docs directory.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Input&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;manifest&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Path to manifest file&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;./manifest.yaml&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;platforms&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Target platforms&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;linux/amd64&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;All generated packages and binaries are available in the &lt;code&gt;${{ github.workspace }}/artifacts/*&lt;/code&gt; folder.&lt;/p&gt;
    &lt;code&gt;# Pull the latest version
docker pull ghcr.io/observiq/otel-distro-builder:latest

# Pull specific version
docker pull ghcr.io/observiq/otel-distro-builder:v1.0.5

# Run a build
docker run --rm -v $(pwd):/workspace -v $(pwd)/build:/build ghcr.io/observiq/otel-distro-builder:main \
  --manifest /workspace/manifest.yaml \
  # Optional
  --artifacts /workspace/artifacts \
  --goos linux \
  --goarch amd64 \
  --ocb-version 0.121.0 \
  --go-version 1.22.1 \ 
  --supervisor-version 0.122.0&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Python 3&lt;/item&gt;
      &lt;item&gt;Docker&lt;/item&gt;
      &lt;item&gt;Make&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Show all commands
make help

# Setup development environment
make setup

# Run tests
make test

# Run linting
make lint
&lt;/code&gt;
    &lt;p&gt;Triggers a build using Google Cloud Build:&lt;/p&gt;
    &lt;code&gt;./scripts/run_cloud_build.sh -m manifest.yaml -p project_id -b artifact_bucket&lt;/code&gt;
    &lt;p&gt;Options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-m&lt;/code&gt;: Path to manifest file (required)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-p&lt;/code&gt;: Google Cloud project ID&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-b&lt;/code&gt;: Artifact bucket name&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This script is used to build a custom OpenTelemetry Collector distribution using a local Docker container:&lt;/p&gt;
    &lt;code&gt;./scripts/run_local_build.sh -m manifest.yaml [-o output_dir] [-v ocb_version] [-g go_version]

# Optionally, run it with
make build-local # to get the latest version of the otelcol and ocb
# Or
make build output_dir=./artifacts ocb_version=0.121.0 go_version=1.22.1 supervisor_version=0.122.0&lt;/code&gt;
    &lt;p&gt;Options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-m&lt;/code&gt;: Path to manifest file (required)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-o&lt;/code&gt;: Directory to store build artifacts (default: ./artifacts)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-v&lt;/code&gt;: OpenTelemetry Collector Builder version (default: auto-detected from manifest)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-g&lt;/code&gt;: Go version to use for building (default: 1.24.1)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The artifacts will be saved to the specified output directory (default: &lt;code&gt;./artifacts&lt;/code&gt;).&lt;/p&gt;
    &lt;code&gt;otel-distro-builder/
├── builder/                # Builder application
│   ├── src/               # Core builder code
│   ├── templates/         # Build templates
│   ├── tests/            # Test suite
│   └── Dockerfile        # Builder image definition
├── action/                # GitHub Action
├── scripts/              # Build scripts
└── Makefile              # Development commands
&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Builder Image Preparation: Build and push to registry&lt;/item&gt;
      &lt;item&gt;Manifest Processing: Upload and validate manifest configuration&lt;/item&gt;
      &lt;item&gt;Build Execution: &lt;list rend="ul"&gt;&lt;item&gt;Download OpenTelemetry Collector Builder (OCB)&lt;/item&gt;&lt;item&gt;Generate Go source files&lt;/item&gt;&lt;item&gt;Build platform-specific packages&lt;/item&gt;&lt;item&gt;Create SBOMs and checksums&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Artifact Management: Upload to GitHub, Google Cloud Storage, or save locally&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The builder produces:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;📦 Binary packages (APK, DEB, RPM)&lt;/item&gt;
      &lt;item&gt;📚 Source tarball&lt;/item&gt;
      &lt;item&gt;🔧 Raw binary&lt;/item&gt;
      &lt;item&gt;📋 SBOM files&lt;/item&gt;
      &lt;item&gt;🔍 Checksums&lt;/item&gt;
      &lt;item&gt;📝 Build metadata&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Local builds: &lt;code&gt;./artifacts&lt;/code&gt;directory&lt;/item&gt;
      &lt;item&gt;Cloud builds: &lt;code&gt;gs://&amp;lt;bucket&amp;gt;/&amp;lt;build_id&amp;gt;/&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We follow semantic versioning. The builder is available in several forms:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GitHub Action: Use &lt;code&gt;@v1&lt;/code&gt;for latest 1.x version, or&lt;code&gt;@v1.0.5&lt;/code&gt;for specific versions&lt;/item&gt;
      &lt;item&gt;Docker Image: Use &lt;code&gt;main&lt;/code&gt;for latest, or version tags like&lt;code&gt;v1.0.5&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Container Registry: &lt;code&gt;ghcr.io/observiq/otel-distro-builder:main&lt;/code&gt;or&lt;code&gt;ghcr.io/observiq/otel-distro-builder:v1.0.5&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Check out our example workflows in &lt;code&gt;.github/workflows/examples/&lt;/code&gt; for common use cases:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multi-platform builds&lt;/item&gt;
      &lt;item&gt;Container publishing&lt;/item&gt;
      &lt;item&gt;Custom package configurations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We welcome contributions! Please see our Contributing Guide for details.&lt;/p&gt;
    &lt;p&gt;This project is licensed under the Apache 2.0 License - see the LICENSE file for details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46177664</guid><pubDate>Sat, 06 Dec 2025 23:41:18 +0000</pubDate></item><item><title>Show HN: FuseCells – a handcrafted logic puzzle game with 2,500 levels</title><link>https://apps.apple.com/us/app/fusecells-logic-grid-puzzle/id6754704139</link><description>&lt;doc fingerprint="af0fffcd9b43ed25"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;FuseCells: Logic Grid Puzzle&lt;/head&gt;
    &lt;head rend="h2"&gt;Strategic Thinking Deduction&lt;/head&gt;
    &lt;p&gt;Free Â· InâApp Purchases Â· Designed for iPad&lt;/p&gt;
    &lt;p&gt; FuseCells blends Sudoku deduction, Minesweeper logic, and Nonogram-style reasoning into a clean cosmic puzzle experience. Every puzzle is handcrafted and fully logical. FuseCells is a clean, no ads, satisfying logic puzzle that blends the deduction of Sudoku, the neighbor logic of Minesweeper, and the pattern reasoning of Nonogram into one fresh cosmic experience. Every puzzle is handcrafted and fully solvable using pure logic, no guessing, no randomness. How It Works: Fill the grid with symbols (Planet, Star, Moon) using number hints. Each hint shows how many neighbors must share the same symbol, in either 4-way or 8-way mode. Green means correct, yellow means possible, red means impossible â follow the logic and the solution emerges naturally. Whatâs Inside: â¢ Three sectors with unique atmosphere: Solar System, Milky Way, Deep Space â¢ 2500 handcrafted puzzles with progressive difficulty â¢ Crystal rating system that rewards perfect logic â¢ Daily Challenges with bonus rewards â¢ Smart hint system for difficult moments â¢ Beautiful cosmic visuals and a relaxing, minimalist design Why Players Love It: â¢ Pure logical deduction â never guess â¢ No ads in the free version â¢ Works fully offline â¢ Clean, color-coded feedback â¢ Perfect for short sessions or long logic runs â¢ Inspired by classic puzzles but completely original in design Free vs Pro Free: 50 levels per sector, Daily Challenges, all game modes Pro: All 2500 levels, future sectors Start your cosmic logic journey today and see why so many players enjoy this unique puzzle experience. The grid is waiting. &lt;/p&gt;
    &lt;p&gt; Winter Style Added! â¢ Fresh winter-themed UI design â¢ Bug fixes and improvements â¢ Enhanced performance Stay cozy and enjoy puzzling! &lt;/p&gt;
    &lt;p&gt;The developer, Igor Cuiumju, indicated that the appâs privacy practices may include handling of data as described below. For more information, see the developerâs privacy policy .&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;head rend="h2"&gt;Data Not Linked to You&lt;/head&gt;
        &lt;p&gt;The following data may be collected but it is not linked to your identity:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Purchases&lt;/item&gt;
          &lt;item&gt;Identifiers&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Accessibility&lt;/head&gt;
    &lt;p&gt;The developer has not yet indicated which accessibility features this app supports. Learn More&lt;/p&gt;
    &lt;head rend="h2"&gt;Information&lt;/head&gt;
    &lt;list rend="dl"&gt;
      &lt;item&gt;
        &lt;item class="svelte-z7zy89" rend="dt-1"&gt;Seller&lt;/item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Igor Cuiumju&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;item class="svelte-z7zy89" rend="dt-1"&gt;Size&lt;/item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;28.2 MB&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;item class="svelte-z7zy89" rend="dt-1"&gt;Category&lt;/item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Puzzle&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;item class="svelte-z7zy89" rend="dt-1"&gt;Compatibility&lt;/item&gt;
        &lt;head class="svelte-lyqho4"&gt;Requires iOS 15.6 or later.&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;iPhone&lt;lb/&gt;Requires iOS 15.6 or later.&lt;/item&gt;
          &lt;item&gt;iPad&lt;lb/&gt;Requires iPadOS 15.6 or later.&lt;/item&gt;
          &lt;item&gt;iPod touch&lt;lb/&gt;Requires iOS 15.6 or later.&lt;/item&gt;
          &lt;item&gt;Mac&lt;lb/&gt;Requires macOS 12.5 or later and a Mac with Apple M1 chip or later.&lt;/item&gt;
          &lt;item&gt;Apple Vision&lt;lb/&gt;Requires visionOS 1.0 or later.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;item class="svelte-z7zy89" rend="dt-1"&gt;Languages&lt;/item&gt;
        &lt;head class="svelte-lyqho4"&gt;English and 9 more&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;English, French, German, Italian, Japanese, Korean, Portuguese, Russian, Simplified Chinese, Spanish&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;item class="svelte-z7zy89" rend="dt-1"&gt;Age Rating&lt;/item&gt;
        &lt;head class="svelte-lyqho4"&gt;4+&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;4+&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;item class="svelte-z7zy89" rend="dt-1"&gt;In-App Purchases&lt;/item&gt;
        &lt;head class="svelte-lyqho4"&gt;Yes&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;FuseCells 10 Hints Pack $0.99&lt;/item&gt;
          &lt;item&gt;FuseCells 30 Hints Pack $1.99&lt;/item&gt;
          &lt;item&gt;FuseCells 100 Hints Pack $4.99&lt;/item&gt;
          &lt;item&gt;FuseCells PRO Lifetime $3.99&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;item class="svelte-z7zy89" rend="dt-1"&gt;Copyright&lt;/item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Â© Igor Cuiumju&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46177737</guid><pubDate>Sat, 06 Dec 2025 23:51:08 +0000</pubDate></item><item><title>Magnitude-7.0 earthquake hits in remote wilderness along Alaska-Canada border</title><link>https://apnews.com/article/earthquake-alaska-canada-yukon-7c0f68370e387b1b23fa7fe7fc9c2c71</link><description>&lt;doc fingerprint="69a235076b45aabc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Magnitude-7.0 earthquake hits in remote wilderness along Alaska-Canada border&lt;/head&gt;
    &lt;p&gt;JUNEAU, Alaska (AP) — A powerful, magnitude-7.0 earthquake struck in a remote area near the border between Alaska and the Canadian territory of Yukon on Saturday. There was no tsunami warning, and officials said there were no immediate reports of damage or injury.&lt;/p&gt;
    &lt;p&gt;The U.S. Geological Survey said it struck about 230 miles (370 kilometers) northwest of Juneau, Alaska, and 155 miles (250 kilometers) west of Whitehorse, Yukon.&lt;/p&gt;
    &lt;p&gt;In Whitehorse, Royal Canadian Mounted Police Sgt. Calista MacLeod said the detachment received two 911 calls about the earthquake.&lt;/p&gt;
    &lt;p&gt;“It definitely was felt,” MacLeod said. “There are a lot of people on social media, people felt it.”&lt;/p&gt;
    &lt;p&gt;Alison Bird, a seismologist with Natural Resources Canada, said the part of Yukon most affected by the temblor is mountainous and has few people.&lt;/p&gt;
    &lt;p&gt;“Mostly people have reported things falling off shelves and walls,” Bird said. “It doesn’t seem like we’ve seen anything in terms of structural damage.”&lt;/p&gt;
    &lt;p&gt;The Canadian community nearest to the epicenter is Haines Junction, Bird said, about 80 miles (130 kilometers) away. The Yukon Bureau of Statistics lists its population count for 2022 as 1,018.&lt;/p&gt;
    &lt;p&gt;The quake was also about 56 miles (91 kilometers) from Yakutat, Alaska, which the USGS said has 662 residents.&lt;/p&gt;
    &lt;p&gt;It struck at a depth of about 6 miles (10 kilometers) and was followed by multiple smaller aftershocks.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46177926</guid><pubDate>Sun, 07 Dec 2025 00:11:47 +0000</pubDate></item></channel></rss>