<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 22 Dec 2025 05:44:12 +0000</lastBuildDate><item><title>Ruby website redesigned</title><link>https://www.ruby-lang.org/en/</link><description>&lt;doc fingerprint="45c34c3fbc0f33f2"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Ruby 4.0.0 preview3 Released&lt;/head&gt;
    &lt;p&gt;Posted by naruse on 18 Dec 2025&lt;/p&gt;
    &lt;p&gt;A Programmer's Best Friend&lt;/p&gt;
    &lt;p&gt;Why do programmers around the world love Ruby? What makes it fun?&lt;/p&gt;
    &lt;p&gt; Rich gems support all kinds of development.&lt;lb/&gt;Mature tooling ready to use. &lt;/p&gt;
    &lt;p&gt;Yukihiro "Matz" Matsumoto&lt;/p&gt;
    &lt;p&gt;Creator of Ruby&lt;/p&gt;
    &lt;p&gt; Easy to write, easy to read.&lt;lb/&gt;Natural syntax like spoken language. &lt;/p&gt;
    &lt;p&gt;David Heinemeier Hansson&lt;/p&gt;
    &lt;p&gt;Creator of Ruby on Rails&lt;/p&gt;
    &lt;p&gt; Do more with less code.&lt;lb/&gt;Intuitive syntax accelerates development. &lt;/p&gt;
    &lt;p&gt;Dave Thomas&lt;/p&gt;
    &lt;p&gt;Author of "The Pragmatic Programmer"&lt;/p&gt;
    &lt;p&gt; Developers worldwide support each other.&lt;lb/&gt;A warm, active community. &lt;/p&gt;
    &lt;p&gt;Amanda Perino&lt;/p&gt;
    &lt;p&gt;Executive Director of Rails Foundation&lt;/p&gt;
    &lt;p&gt; People who engage with Ruby beyond being just users are called Rubyists.&lt;lb/&gt; Rubyists who love Ruby are all nice #rubyfriends. Community activities are thriving and fun. &lt;/p&gt;
    &lt;p&gt;The universal motto is "MINASWAN" â€” Matz is nice and so we are nice&lt;/p&gt;
    &lt;p&gt;Posted by naruse on 18 Dec 2025&lt;/p&gt;
    &lt;p&gt;Posted by k0kubun on 17 Dec 2025&lt;/p&gt;
    &lt;p&gt;Posted by naruse on 17 Nov 2025&lt;/p&gt;
    &lt;p&gt;Posted by nagachika on 23 Oct 2025&lt;/p&gt;
    &lt;p&gt;Posted by hsbt on 7 Oct 2025&lt;/p&gt;
    &lt;p&gt;Posted by naitoh on 18 Sep 2025&lt;/p&gt;
    &lt;p&gt;Posted by mame on 8 Jul 2025&lt;/p&gt;
    &lt;p&gt;Posted by nevans on 28 Apr 2025&lt;/p&gt;
    &lt;p&gt;Posted by hsbt on 26 Feb 2025&lt;/p&gt;
    &lt;p&gt;Posted by nevans on 10 Feb 2025&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46342859</guid><pubDate>Sun, 21 Dec 2025 07:06:48 +0000</pubDate></item><item><title>Luke Howardâ€™s essay on the modification of clouds (1865)</title><link>https://publicdomainreview.org/collection/essay-on-the-modification-of-clouds/</link><description>&lt;doc fingerprint="a3e8fad5c3bd937a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Luke Howardâ€™s Essay on the Modification of Clouds (1865)&lt;/head&gt;
    &lt;p&gt;At first light, the project of classifying the clouds might seem a bit uninspired, if useful. Not so for Luke Howard (1772â€“1864), industrial chemist by trade and amateur meteorologist by calling, whose 1803 Essay on the Modification of Clouds records the fruits of a fervent, lifelong dedication to skygazing. It was long thought impossible to infer clear types from the constantly shifting skies. But based on the journals heâ€™d kept since the age of ten, Howard became the first to name standard cloud formations systematically. We still use the Latin names he chose today: cirrus (from the Latin for a wisp of hair), cumulus (â€œconvex or conical heapsâ€), stratus (a â€œhorizontal sheetâ€), and nimbus (the rain cloud). The project was not only a meteorological breakthrough, but yielded sketchbooks filled with wind-swept watercolors and inspired a new generation of landscape painters. Years later, Howard earned the surprising distinction of being named in a poem by Goethe.&lt;/p&gt;
    &lt;p&gt;Howardâ€™s accomplishment was to discern organization in the apparently aleatory. â€œIf Clouds were the mere result of the condensation of Vapour in the masses of atmosphere which they occupy . . . then indeed might the study of them be deemed an useless pursuit of shadowsâ€, Howard allows. But with an intellectual conviction born from decades lost in the clouds, he argues that â€œthe principal Modifications are commonly as distinguishable from each other as a Tree from a Hill, or the latter from a Lake.â€ If they are to discern the patterns he describes, Howard writes, his readers must also commit themselves to frequent observation.&lt;/p&gt;
    &lt;p&gt;Goethe, too, saw the sublime in scientific classification. Translated into German in 1815, Howardâ€™s essay crystallized Goetheâ€™s own study of the skies. Goethe corresponded with the younger Englishman, writing a poem that was reproduced, untranslated, in the essayâ€™s 1865 third edition. In the opening stanzas, Goethe exalts the beauty of the overcast sky, and the clear-sightedness of the man who parsed it:&lt;/p&gt;
    &lt;quote&gt;Then boldly stirs imaginationâ€™s power,&lt;lb/&gt;And shapes there formless masses of the hour;&lt;lb/&gt;Here lions threat, there elephants will range,&lt;lb/&gt;And camel-necks to vapoury dragons change;&lt;lb/&gt;An army moves, but not in victory proud,&lt;lb/&gt;Its might is broken on a rock of cloud;&lt;lb/&gt;Eâ€™en the cloud messenger in air expires,&lt;lb/&gt;Ere reachâ€™d the distance fancy yet desires.&lt;lb/&gt;But Howard gives us with his clearer mind&lt;lb/&gt;The gain of lessons new to all mankind;&lt;lb/&gt;That which no hand can reach,&lt;lb/&gt;no hand can clasp,&lt;lb/&gt;He first has gainâ€™d, first&lt;lb/&gt;held with mental grasp.&lt;lb/&gt;Definâ€™d the doubtful, fixâ€™d its limit-line,&lt;lb/&gt;And named it fitly. â€”Be the honour thine!&lt;lb/&gt;As clouds ascend, are folded, scatter, fall,&lt;lb/&gt;Let the world think of thee who taught it all.&lt;/quote&gt;
    &lt;p&gt;Howard illustrated his essay, collaborating with the painter Edward Kennion (1744â€“1809) to depict hulking nimbus and wispy cirrus over picturesque landscapes. Though evocative, the prints lack the delicacy of Howardâ€™s earlier cloud studies: delicate watercolors that portray the shifting skies in dynamic beige, blue, and pale gray splashes. The sketches reflect the many years Howard spent travelling between London and the Lake District, writes curator Boris Jardine, evidence of a Romantic artistic practice closely intertwined with his taxonomic one. In the spirit of Goetheâ€™s â€œdelicate empiricismâ€ (zarte Empirie) and the Romantic conviction in the power of first-hand observation, Howard is at pains to â€œwarn the young student of Meteorologyâ€ against leaning too heavily on the engraved images. â€œA correct comprehension of the subjectâ€, he writes, â€œis only to be obtained by a habitual observation of Natureâ€.&lt;/p&gt;
    &lt;p&gt;Goethe, a student of Howard himself, expresses his gratitude for the general forms Howard outlines: airy archetypes that allow him to move beyond â€œcertain indistinct appearancesâ€ and identify the â€œmain rules under which they comeâ€. As poet and naturalist, he may have been the best person to answer the question: can imposing order onto the ether lead, counterintuitively, toward the transcendental?&lt;/p&gt;
    &lt;p&gt;Enjoyed this piece? We need your help to keep publishing.&lt;/p&gt;
    &lt;p&gt;The PDR is a non-profit project kept alive by reader donations â€“ no ads, no paywalls, just the generosity of our community. Itâ€™s a really exciting model, but we need your help to keep it thriving. Visit our support page to become a Friend and receive our themed postcard packs. Or give a one-off donation. Already a supporter? A huge thank you for making all this possible.&lt;/p&gt;
    &lt;p&gt;Dec 2, 2025&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46344412</guid><pubDate>Sun, 21 Dec 2025 12:38:25 +0000</pubDate></item><item><title>Coarse is better</title><link>https://borretti.me/article/coarse-is-better</link><description>&lt;doc fingerprint="44a11a5f56369601"&gt;
  &lt;main&gt;
    &lt;p&gt;When DALL-E came out, it took me a couple of weeks to pick my jaw up from the floor. I would go to sleep excited to wake up to a full quota, with a backlog of prompts to try. It was magical, miraculous. Like discovering a new universe. I compiled the best art in this post.&lt;/p&gt;
    &lt;p&gt;The other day a friend ran some of my old prompts through Nano Banana Pro (NBP), and put the old models side by side with the new. Itâ€™s interesting how after years of progress, the models are much better better at making images, but infinitely worse at making art.&lt;/p&gt;
    &lt;head rend="h1"&gt;Electron Contours&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Electron contours in the style of Italian futurism, oil on canvas, 1922, trending on ArtStation.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The old Midjourney v2 renders this:&lt;/p&gt;
    &lt;p&gt;NBP renders this:&lt;/p&gt;
    &lt;p&gt;Admiteddly MJâ€™s output doesnâ€™t look quite like futurism. But it looks like something. It looks compelling. The colours are bright and vivid. NBPâ€™s output is studiously in the style of Italian futurism, but the colours are so muted and dull.&lt;/p&gt;
    &lt;p&gt;Maybe the â€œtrending on ArtStationâ€ is a bit of an archaism and impairs performance. Letâ€™s try again without:&lt;/p&gt;
    &lt;p&gt;Meh.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Kowloon Walled City&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Painting of an alley in the Kowloon Walled City, EugÃ¨ne Boudin, 1895, trending on ArtStation.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;MJ gave me this:&lt;/p&gt;
    &lt;p&gt;And it looks nothing like the Kowloon Walled City. But itâ€™s beautiful. Itâ€™s coarse, impressionistic, vague, evocative, contradictory. Itâ€™s brimming with mystery. And it is, in fact, in the style of EugÃ¨ne Boudin. This, by contrast, is the NBP output:&lt;/p&gt;
    &lt;p&gt;Sigh. It looks like every modern movie: so desaturated you feel youâ€™re going colourblind. Letâ€™s try forcing it:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Painting of an alley in the Kowloon Walled City, EugÃ¨ne Boudin, 1895. Make it coarse, impressionistic, vague, evocative, contradictory, brimming with mystery.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is somewhat better, but why is it so drab and colourless? Is the machine trying to make me depressed?&lt;/p&gt;
    &lt;head rend="h1"&gt;The Dream Garden of the Poets&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Attar and Ferdowsi in a dream garden, Persian miniature, circa 1300, from the British Museum.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Midjourney v2:&lt;/p&gt;
    &lt;p&gt;It doesnâ€™t quite look like anything. But it is beautiful, and evocative. I like to imagine that little splotch of paint on the upper right is hoopoe. The NBP output:&lt;/p&gt;
    &lt;p&gt;Well, it looks like a Persian miniature. The â€œfrom the British Museumâ€ bit, I meant that to be interpreted evocatively, rather than literally. The prompt cites a fictional object, bringing it into the existence. But NBP reads this as: no, this is a photograph of a Persian miniature in the British Museum.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Sack of Merv&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;The Burning of Merv by John William Waterhouse, 1896, from the British Museum.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Midjourney v2:&lt;/p&gt;
    &lt;p&gt;It does look like Waterhouse. Semantically thereâ€™s room to argue: it looks like a woman being burnt at the stake, not the sack of a city. But aesthetically: itâ€™s gorgeous. The flames are gorgeous, the reds of the dress are gorgeous. Look at the reeds in the background, and the black water, that looks like tarnished silver or pewter. The faces of the crowd. Is that a minotaur on the lower left, or a flower? What is she holding on her bent left arm? A crucifix, a dagger? You could find entire universes in this image, in this 1024x1024 frame.&lt;/p&gt;
    &lt;p&gt;By contrast, this is the NBP output:&lt;/p&gt;
    &lt;p&gt;What can one say? It doesnâ€™t look like Waterhouse. The horsemen wear Arab or Central Asian dress, but Merv was sacked in the year 1221 by the Mongol Empire. And, again, the â€œBritish Museumâ€ line is taken literally rather than evocatively.&lt;/p&gt;
    &lt;head rend="h1"&gt;Lady Lovelace&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Portrait of Ada Lovelace by Dante Gabriel Rossetti, 1859, auctioned by Christieâ€™s.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Midjourney:&lt;/p&gt;
    &lt;p&gt;This is beautiful. It is beautiful because the coarse, impressionistic brushstroke is more evocative than literal. And it actually looks like a woman drawn by Rossetti. And look at the greens! Gorgeously green. The palette is so narrow, and the painting is so beautiful.&lt;/p&gt;
    &lt;p&gt;The NBP output:&lt;/p&gt;
    &lt;p&gt;Pure philistinism. â€œAuctioned by Christieâ€™sâ€, again, is meant to be evocative: â€œthis is the kind of painting that would be sold at auctionâ€. But NBP makes it a photograph of a painting at an auction house. Fine, I suppose I got what I asked for.&lt;/p&gt;
    &lt;p&gt;But the woman doesnâ€™t look like Rossetti! This is absurd. How can a model from 2022 get this right, and the SOTA image generation model gives us generic oil painting slop?&lt;/p&gt;
    &lt;head rend="h1"&gt;The Cosmic Microwave Background&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;A Persian miniature of the cosmic microwave background, from Herat circa 1600, trending on ArtStation&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Midjourney v2:&lt;/p&gt;
    &lt;p&gt;NBP:&lt;/p&gt;
    &lt;p&gt;Again: what can one say?&lt;/p&gt;
    &lt;head rend="h1"&gt;Dream Story&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Dream Story, 1961, blurry black and white photograph, yellow tint, from the Metropolitan Museum of Art.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is one of my favourite DALL-E 2 outputs:&lt;/p&gt;
    &lt;p&gt;They remind me of The King in Yellow. I love these because of how genuinely creepy and mysterious they are. You could pull a hundred horror stories from these.&lt;/p&gt;
    &lt;p&gt;It is hard to believe how bad the NBP output is:&lt;/p&gt;
    &lt;p&gt;What are we doing here? The old models were beautiful and compelling because the imperfections, vagueness, mistakes, and contradictions all create these little gaps through which your imagination can breathe life into the art. The images are not one fixed, static thing: they can be infinitely many things.&lt;/p&gt;
    &lt;p&gt;The new modelsâ€”do I even need to finish this sentence? Theyâ€™re too precise and high-resolution, so they cannot make abstract, many-faced things, they can only make specific, concrete things.&lt;/p&gt;
    &lt;p&gt;We need to make AI art weird again.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46344514</guid><pubDate>Sun, 21 Dec 2025 12:57:44 +0000</pubDate></item><item><title>Show HN: GenresFox â€“ Open-source, customizable new tab page extension in WASM</title><link>https://github.com/zayokami/GenresFox</link><description>&lt;doc fingerprint="9f46f07041f62b92"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ğŸ¨ Modern Dark Theme: Beautiful glassmorphism design with smooth animations&lt;/item&gt;
      &lt;item&gt;ğŸ” Multi-Engine Search: Built-in support for Google, Bing, and DuckDuckGo&lt;/item&gt;
      &lt;item&gt;âš™ï¸ Custom Search Engines: Add your own search engines with custom URLs&lt;/item&gt;
      &lt;item&gt;ğŸ”– Quick Shortcuts: Create shortcuts to your favorite websites with auto-fetched favicons&lt;/item&gt;
      &lt;item&gt;ğŸ–¼ï¸ Custom Wallpapers: Upload your own background images (up to 50MB, 50MP) with drag-and-drop support&lt;/item&gt;
      &lt;item&gt;ğŸŒ… Bing Daily Wallpaper: Beautiful daily wallpapers from Bing with smart 24-hour caching and preloading&lt;/item&gt;
      &lt;item&gt;âš¡ High-Performance Image Processing: Web Worker support, progressive preview, and intelligent compression&lt;/item&gt;
      &lt;item&gt;ğŸŒ Multi-language: English, Simplified Chinese, Traditional Chinese, Japanese, Spanish, French&lt;/item&gt;
      &lt;item&gt;â™¿ Accessibility: High contrast themes, font controls, animation settings, keyboard shortcuts&lt;/item&gt;
      &lt;item&gt;âŒ¨ï¸ Keyboard Shortcuts: Quick engine switching (Alt+â†‘â†“), focus search (/), open settings (Alt+,)&lt;/item&gt;
      &lt;item&gt;ğŸ’¾ Smart Caching: Icon caching, wallpaper caching, and processing result caching for faster loading&lt;/item&gt;
      &lt;item&gt;ğŸ¯ Clean &amp;amp; Minimal: Distraction-free interface focused on what matters&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Clone this repository:&lt;/p&gt;
        &lt;quote&gt;git clone https://github.com/zayokami/GenresFox.git&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Open Chrome/Edge and navigate to&lt;/p&gt;
        &lt;code&gt;chrome://extensions/&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Enable "Developer mode" in the top right corner&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Click "Load unpacked" and select the&lt;/p&gt;&lt;code&gt;src&lt;/code&gt;folder&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Enjoy your new tab page!&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;Coming soon...&lt;/p&gt;
    &lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simply type in the search box and press Enter&lt;/item&gt;
      &lt;item&gt;Click the search engine icon to switch between different engines&lt;/item&gt;
      &lt;item&gt;URLs are automatically detected and opened directly&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Click the settings icon (âš™ï¸) in the bottom right&lt;/item&gt;
      &lt;item&gt;Go to "Search &amp;amp; Shortcuts" tab&lt;/item&gt;
      &lt;item&gt;Enter the engine name and URL (use &lt;code&gt;%s&lt;/code&gt;as the search query placeholder)&lt;list rend="ul"&gt;&lt;item&gt;Example: &lt;code&gt;https://kagi.com/search?q=%s&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Example: &lt;/item&gt;
      &lt;item&gt;Click "Add"&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Open settings and go to "Search &amp;amp; Shortcuts" tab&lt;/item&gt;
      &lt;item&gt;Scroll to the "Shortcuts" section&lt;/item&gt;
      &lt;item&gt;Enter the name and URL of your favorite website&lt;/item&gt;
      &lt;item&gt;The favicon will be automatically fetched&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Open settings and go to "Wallpaper" tab&lt;/item&gt;
      &lt;item&gt;Drag and drop an image or click to upload&lt;/item&gt;
      &lt;item&gt;Maximum file size: 50MB, maximum resolution: 50 megapixels&lt;/item&gt;
      &lt;item&gt;Images are automatically optimized and compressed for storage efficiency&lt;/item&gt;
      &lt;item&gt;Click "Reset to Default" to restore the original background&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;GenresFox/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ _locales/           # Internationalization files
â”‚   â”‚   â”œâ”€â”€ en/
â”‚   â”‚   â”œâ”€â”€ es/
â”‚   â”‚   â”œâ”€â”€ fr/
â”‚   â”‚   â”œâ”€â”€ ja/
â”‚   â”‚   â”œâ”€â”€ zh_CN/
â”‚   â”‚   â””â”€â”€ zh_TW/
â”‚   â”œâ”€â”€ icon.png            # Extension icon
â”‚   â”œâ”€â”€ manifest.json       # Extension manifest
â”‚   â”œâ”€â”€ newtab.html         # Main HTML file
â”‚   â”œâ”€â”€ script.js           # Main JavaScript logic
â”‚   â”œâ”€â”€ search.js           # Search bar &amp;amp; search button logic
â”‚   â”œâ”€â”€ i18n.js             # Internationalization module
â”‚   â”œâ”€â”€ wallpaper.js        # Wallpaper management module
â”‚   â”œâ”€â”€ accessibility.js    # Accessibility features module
â”‚   â”œâ”€â”€ image-processor.js  # High-performance image processing module
â”‚   â”œâ”€â”€ image-worker.js     # Web Worker for background image processing
â”‚   â”œâ”€â”€ styles.css          # Main styles
â”‚   â”œâ”€â”€ search.css          # Search bar styles
â”‚   â””â”€â”€ accessibility.css   # Accessibility styles
â”œâ”€â”€ CHANGELOG.md
â””â”€â”€ README.md
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Manifest V3: Latest Chrome extension standard&lt;/item&gt;
      &lt;item&gt;Vanilla JavaScript: No frameworks, pure performance&lt;/item&gt;
      &lt;item&gt;CSS3: Modern styling with glassmorphism effects&lt;/item&gt;
      &lt;item&gt;Web Workers: Background image processing without blocking UI&lt;/item&gt;
      &lt;item&gt;IndexedDB: For storing large wallpaper files and cache&lt;/item&gt;
      &lt;item&gt;LocalStorage: For persistent settings and metadata caching&lt;/item&gt;
      &lt;item&gt;Chrome Extension APIs: For internationalization and browser integration&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Create a new folder in &lt;code&gt;src/_locales/&lt;/code&gt;with the language code (e.g.,&lt;code&gt;fr&lt;/code&gt;for French)&lt;/item&gt;
      &lt;item&gt;Copy &lt;code&gt;messages.json&lt;/code&gt;from&lt;code&gt;en&lt;/code&gt;folder&lt;/item&gt;
      &lt;item&gt;Translate all message values&lt;/item&gt;
      &lt;item&gt;Add the language to &lt;code&gt;_fallbackMessages&lt;/code&gt;in&lt;code&gt;src/i18n.js&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Update &lt;code&gt;_supportedLanguages&lt;/code&gt;array and&lt;code&gt;_detectLanguage()&lt;/code&gt;function&lt;/item&gt;
      &lt;item&gt;Add a language switch entry if needed in UI&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Contributions are welcome! Please feel free to submit a Pull Request.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fork the project&lt;/item&gt;
      &lt;item&gt;Create your feature branch (&lt;code&gt;git checkout -b feature/AmazingFeature&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Commit your changes (&lt;code&gt;git commit -m 'Add some AmazingFeature'&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Push to the branch (&lt;code&gt;git push origin feature/AmazingFeature&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Open a Pull Request&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;
    &lt;p&gt;zayoka&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GitHub: @zayoka&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Inspired by modern web design trends&lt;/item&gt;
      &lt;item&gt;Icons from Google Material Design&lt;/item&gt;
      &lt;item&gt;Favicon service by DuckDuckGo (primary) and Google s2 (fallback)&lt;/item&gt;
      &lt;item&gt;Daily wallpapers by Bing&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ğŸ¨ ç°ä»£æ·±è‰²ä¸»é¢˜: ç²¾ç¾çš„ç»ç’ƒæ€è®¾è®¡ï¼Œæµç•…çš„åŠ¨ç”»æ•ˆæœ&lt;/item&gt;
      &lt;item&gt;ğŸ” å¤šå¼•æ“æœç´¢: å†…ç½®æ”¯æŒ Googleã€Bing å’Œ DuckDuckGo&lt;/item&gt;
      &lt;item&gt;âš™ï¸ è‡ªå®šä¹‰æœç´¢å¼•æ“: æ·»åŠ æ‚¨è‡ªå·±çš„æœç´¢å¼•æ“å’Œè‡ªå®šä¹‰ URL&lt;/item&gt;
      &lt;item&gt;ğŸ”– å¿«æ·æ–¹å¼: åˆ›å»ºå¸¸ç”¨ç½‘ç«™çš„å¿«æ·æ–¹å¼ï¼Œè‡ªåŠ¨è·å–ç½‘ç«™å›¾æ ‡&lt;/item&gt;
      &lt;item&gt;ğŸ–¼ï¸ è‡ªå®šä¹‰å£çº¸: ä¸Šä¼ æ‚¨è‡ªå·±çš„èƒŒæ™¯å›¾ç‰‡ï¼ˆæœ€å¤§ 50MBï¼Œ5000ä¸‡åƒç´ ï¼‰ï¼Œæ”¯æŒæ‹–æ”¾ä¸Šä¼ &lt;/item&gt;
      &lt;item&gt;ğŸŒ… å¿…åº”æ¯æ—¥å£çº¸: æ¥è‡ªå¿…åº”çš„ç²¾ç¾æ¯æ—¥å£çº¸ï¼Œæ”¯æŒæ™ºèƒ½24å°æ—¶ç¼“å­˜å’Œé¢„åŠ è½½&lt;/item&gt;
      &lt;item&gt;âš¡ é«˜æ€§èƒ½å›¾ç‰‡å¤„ç†: Web Worker æ”¯æŒã€æ¸è¿›å¼é¢„è§ˆã€æ™ºèƒ½å‹ç¼©&lt;/item&gt;
      &lt;item&gt;ğŸŒ å¤šè¯­è¨€æ”¯æŒ: ç®€ä½“ä¸­æ–‡ã€ç¹ä½“ä¸­æ–‡ã€æ—¥è¯­ã€è‹±è¯­ã€è¥¿ç­ç‰™è¯­ã€æ³•è¯­&lt;/item&gt;
      &lt;item&gt;â™¿ æ— éšœç¢åŠŸèƒ½: é«˜å¯¹æ¯”åº¦ä¸»é¢˜ã€å­—ä½“æ§åˆ¶ã€åŠ¨ç”»è®¾ç½®ã€é”®ç›˜å¿«æ·é”®&lt;/item&gt;
      &lt;item&gt;âŒ¨ï¸ é”®ç›˜å¿«æ·é”®: å¿«é€Ÿåˆ‡æ¢æœç´¢å¼•æ“ (Alt+â†‘â†“)ã€èšç„¦æœç´¢æ¡† (/)ã€æ‰“å¼€è®¾ç½® (Alt+,)&lt;/item&gt;
      &lt;item&gt;ğŸ’¾ æ™ºèƒ½ç¼“å­˜: å›¾æ ‡ç¼“å­˜ã€å£çº¸ç¼“å­˜ã€å¤„ç†ç»“æœç¼“å­˜ï¼ŒåŠ å¿«åŠ è½½é€Ÿåº¦&lt;/item&gt;
      &lt;item&gt;ğŸ¯ ç®€æ´æç®€: æ— å¹²æ‰°ç•Œé¢ï¼Œä¸“æ³¨äºé‡è¦å†…å®¹&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;å…‹éš†æ­¤ä»“åº“ï¼š&lt;/p&gt;
        &lt;quote&gt;git clone https://github.com/zayokami/GenresFox.git&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;æ‰“å¼€ Chrome/Edge æµè§ˆå™¨ï¼Œè®¿é—®&lt;/p&gt;
        &lt;code&gt;chrome://extensions/&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;åœ¨å³ä¸Šè§’å¯ç”¨"å¼€å‘è€…æ¨¡å¼"&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;ç‚¹å‡»"åŠ è½½å·²è§£å‹çš„æ‰©å±•ç¨‹åº"ï¼Œé€‰æ‹©&lt;/p&gt;&lt;code&gt;src&lt;/code&gt;æ–‡ä»¶å¤¹&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;å¼€å§‹ä½¿ç”¨å§ï¼&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;å³å°†æ¨å‡º...&lt;/p&gt;
    &lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;åœ¨æœç´¢æ¡†ä¸­è¾“å…¥å†…å®¹å¹¶æŒ‰å›è½¦&lt;/item&gt;
      &lt;item&gt;ç‚¹å‡»æœç´¢å¼•æ“å›¾æ ‡å¯åˆ‡æ¢ä¸åŒçš„æœç´¢å¼•æ“&lt;/item&gt;
      &lt;item&gt;ç½‘å€ä¼šè¢«è‡ªåŠ¨è¯†åˆ«å¹¶ç›´æ¥æ‰“å¼€&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;ç‚¹å‡»å³ä¸‹è§’çš„è®¾ç½®å›¾æ ‡ï¼ˆâš™ï¸ï¼‰&lt;/item&gt;
      &lt;item&gt;è¿›å…¥"æœç´¢ä¸å¿«æ·æ–¹å¼"æ ‡ç­¾é¡µ&lt;/item&gt;
      &lt;item&gt;è¾“å…¥å¼•æ“åç§°å’Œ URLï¼ˆä½¿ç”¨ &lt;code&gt;%s&lt;/code&gt;ä½œä¸ºæœç´¢å…³é”®è¯å ä½ç¬¦ï¼‰&lt;list rend="ul"&gt;&lt;item&gt;ç¤ºä¾‹ï¼š&lt;code&gt;https://kagi.com/search?q=%s&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;ç¤ºä¾‹ï¼š&lt;/item&gt;
      &lt;item&gt;ç‚¹å‡»"æ·»åŠ "&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;æ‰“å¼€è®¾ç½®ï¼Œè¿›å…¥"æœç´¢ä¸å¿«æ·æ–¹å¼"æ ‡ç­¾é¡µ&lt;/item&gt;
      &lt;item&gt;æ»šåŠ¨åˆ°"å¿«æ·æ–¹å¼"éƒ¨åˆ†&lt;/item&gt;
      &lt;item&gt;è¾“å…¥æ‚¨å–œæ¬¢çš„ç½‘ç«™åç§°å’Œ URL&lt;/item&gt;
      &lt;item&gt;ç½‘ç«™å›¾æ ‡ä¼šè‡ªåŠ¨è·å–&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;æ‰“å¼€è®¾ç½®ï¼Œè¿›å…¥"å£çº¸"æ ‡ç­¾é¡µ&lt;/item&gt;
      &lt;item&gt;æ‹–æ”¾å›¾ç‰‡æˆ–ç‚¹å‡»ä¸Šä¼ &lt;/item&gt;
      &lt;item&gt;æœ€å¤§æ–‡ä»¶å¤§å°ï¼š50MBï¼Œæœ€å¤§åˆ†è¾¨ç‡ï¼š5000ä¸‡åƒç´ &lt;/item&gt;
      &lt;item&gt;å›¾ç‰‡ä¼šè‡ªåŠ¨ä¼˜åŒ–å’Œå‹ç¼©ä»¥æé«˜å­˜å‚¨æ•ˆç‡&lt;/item&gt;
      &lt;item&gt;ç‚¹å‡»"æ¢å¤é»˜è®¤"å¯è¿˜åŸåŸå§‹èƒŒæ™¯&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;GenresFox/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ _locales/           # å›½é™…åŒ–æ–‡ä»¶
â”‚   â”‚   â”œâ”€â”€ en/
â”‚   â”‚   â”œâ”€â”€ es/
â”‚   â”‚   â”œâ”€â”€ fr/
â”‚   â”‚   â”œâ”€â”€ ja/
â”‚   â”‚   â”œâ”€â”€ zh_CN/
â”‚   â”‚   â””â”€â”€ zh_TW/
â”‚   â”œâ”€â”€ icon.png            # æ‰©å±•å›¾æ ‡
â”‚   â”œâ”€â”€ manifest.json       # æ‰©å±•æ¸…å•
â”‚   â”œâ”€â”€ newtab.html         # ä¸» HTML æ–‡ä»¶
â”‚   â”œâ”€â”€ script.js           # ä¸» JavaScript é€»è¾‘
â”‚   â”œâ”€â”€ search.js           # æœç´¢æ ä¸æœç´¢æŒ‰é’®é€»è¾‘
â”‚   â”œâ”€â”€ i18n.js             # å›½é™…åŒ–æ¨¡å—
â”‚   â”œâ”€â”€ wallpaper.js        # å£çº¸ç®¡ç†æ¨¡å—
â”‚   â”œâ”€â”€ accessibility.js    # æ— éšœç¢åŠŸèƒ½æ¨¡å—
â”‚   â”œâ”€â”€ image-processor.js  # é«˜æ€§èƒ½å›¾ç‰‡å¤„ç†æ¨¡å—
â”‚   â”œâ”€â”€ image-worker.js     # Web Worker åå°å›¾ç‰‡å¤„ç†
â”‚   â”œâ”€â”€ styles.css          # ä¸»æ ·å¼æ–‡ä»¶
â”‚   â”œâ”€â”€ search.css          # æœç´¢æ æ ·å¼æ–‡ä»¶
â”‚   â””â”€â”€ accessibility.css   # æ— éšœç¢æ ·å¼æ–‡ä»¶
â”œâ”€â”€ CHANGELOG.md
â””â”€â”€ README.md
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Manifest V3: æœ€æ–°çš„ Chrome æ‰©å±•æ ‡å‡†&lt;/item&gt;
      &lt;item&gt;åŸç”Ÿ JavaScript: æ— æ¡†æ¶ä¾èµ–ï¼Œçº¯ç²¹çš„æ€§èƒ½&lt;/item&gt;
      &lt;item&gt;CSS3: ç°ä»£æ ·å¼ä¸ç»ç’ƒæ€æ•ˆæœ&lt;/item&gt;
      &lt;item&gt;Web Workers: åå°å›¾ç‰‡å¤„ç†ï¼Œä¸é˜»å¡ UI&lt;/item&gt;
      &lt;item&gt;IndexedDB: ç”¨äºå­˜å‚¨å¤§å‹å£çº¸æ–‡ä»¶å’Œç¼“å­˜&lt;/item&gt;
      &lt;item&gt;LocalStorage: ç”¨äºæŒä¹…åŒ–è®¾ç½®å’Œå…ƒæ•°æ®ç¼“å­˜&lt;/item&gt;
      &lt;item&gt;Chrome æ‰©å±• API: ç”¨äºå›½é™…åŒ–å’Œæµè§ˆå™¨é›†æˆ&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;åœ¨ &lt;code&gt;src/_locales/&lt;/code&gt;ä¸­åˆ›å»ºæ–°æ–‡ä»¶å¤¹ï¼Œä½¿ç”¨è¯­è¨€ä»£ç å‘½åï¼ˆå¦‚&lt;code&gt;fr&lt;/code&gt;è¡¨ç¤ºæ³•è¯­ï¼‰&lt;/item&gt;
      &lt;item&gt;ä» &lt;code&gt;en&lt;/code&gt;æ–‡ä»¶å¤¹å¤åˆ¶&lt;code&gt;messages.json&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;ç¿»è¯‘æ‰€æœ‰æ¶ˆæ¯å€¼&lt;/item&gt;
      &lt;item&gt;åœ¨ &lt;code&gt;src/i18n.js&lt;/code&gt;çš„&lt;code&gt;_fallbackMessages&lt;/code&gt;ä¸­æ·»åŠ è¯¥è¯­è¨€&lt;/item&gt;
      &lt;item&gt;æ›´æ–° &lt;code&gt;_supportedLanguages&lt;/code&gt;æ•°ç»„å’Œ&lt;code&gt;_detectLanguage()&lt;/code&gt;å‡½æ•°&lt;/item&gt;
      &lt;item&gt;å¦‚éœ€åœ¨ç•Œé¢ä¸­æ˜¾ç¤ºè¯­è¨€åˆ‡æ¢å…¥å£ï¼Œè¯·åŒæ­¥æ–°å¢&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;æ¬¢è¿è´¡çŒ®ï¼è¯·éšæ—¶æäº¤ Pull Requestã€‚&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fork æœ¬é¡¹ç›®&lt;/item&gt;
      &lt;item&gt;åˆ›å»ºæ‚¨çš„ç‰¹æ€§åˆ†æ”¯ (&lt;code&gt;git checkout -b feature/AmazingFeature&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;æäº¤æ‚¨çš„æ›´æ”¹ (&lt;code&gt;git commit -m 'Add some AmazingFeature'&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;æ¨é€åˆ°åˆ†æ”¯ (&lt;code&gt;git push origin feature/AmazingFeature&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;æ‰“å¼€ä¸€ä¸ª Pull Request&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ LICENSE æ–‡ä»¶ã€‚&lt;/p&gt;
    &lt;p&gt;zayoka&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GitHub: @zayoka&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;çµæ„Ÿæ¥è‡ªç°ä»£ç½‘é¡µè®¾è®¡è¶‹åŠ¿&lt;/item&gt;
      &lt;item&gt;å›¾æ ‡æ¥è‡ª Google Material Design&lt;/item&gt;
      &lt;item&gt;Favicon æœåŠ¡ç”± DuckDuckGoï¼ˆä¸»ï¼‰ä¸ Google s2ï¼ˆå¤‡ï¼‰æä¾›&lt;/item&gt;
      &lt;item&gt;æ¯æ—¥å£çº¸ç”± Bing æä¾›&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™å®ƒä¸€ä¸ª â­ï¸ï¼&lt;/p&gt;
    &lt;p&gt;If this project has been helpful to you, please give it a â­ï¸!&lt;/p&gt;
    &lt;p&gt;Made with â¤ï¸ by zayoka&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46345157</guid><pubDate>Sun, 21 Dec 2025 14:40:03 +0000</pubDate></item><item><title>Structured outputs create false confidence</title><link>https://boundaryml.com/blog/structured-outputs-create-false-confidence</link><description>&lt;doc fingerprint="15180b5c38045eba"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Structured Outputs Create False Confidence&lt;/head&gt;
    &lt;p&gt;Constrained decoding seems like the greatest thing since sliced bread, but it often forces models to prioritize output conformance over output quality.&lt;/p&gt;
    &lt;p&gt;Sam Lijin&lt;/p&gt;
    &lt;p&gt;Update (Dec 21): this post is now on the Hacker News front page! We've updated this post to be more precise about our claims and have also added some clarifications at the end. You can see the original version of this post here.&lt;/p&gt;
    &lt;p&gt;If you use LLMs, you've probably heard about structured outputs. You might think they're the greatest thing since sliced bread. Unfortunately, structured outputs also often degrade response quality.&lt;/p&gt;
    &lt;p&gt;Specifically, if you use an LLM provider's structured outputs API, you're likely to get a lower quality response than if you use their normal text output API:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;âš ï¸ you're more likely to make mistakes when extracting data, even in simple cases;&lt;/item&gt;
      &lt;item&gt;âš ï¸ you're probably not modeling errors correctly;&lt;/item&gt;
      &lt;item&gt;âš ï¸ it's harder to use techniques like chain-of-thought reasoning; and&lt;/item&gt;
      &lt;item&gt;âš ï¸ in the extreme case, it can be easier to steal your customer data using prompt injection.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are very contentious claims, so let's start with an example: extracting data from a receipt.&lt;/p&gt;
    &lt;p&gt;If I use an LLM to extract the receipt entries, it should be able to tell me that one of the items is &lt;code&gt;(name="banana", quantity=0.46)&lt;/code&gt;, right?&lt;/p&gt;
    &lt;p&gt;Well, using OpenAI's structured outputs API with &lt;code&gt;gpt-5.2&lt;/code&gt; - released literally this week! - it will claim that the banana quantity is &lt;code&gt;1.0&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;{
  "establishment_name": "PC Market of Choice",
  "date": "2007-01-20",
  "total": 0.32,
  "currency": "USD",
  "items": [
    {
      "name": "Bananas",
      "price": 0.32,
      "quantity": 1
    }
  ]
}
&lt;/code&gt;
    &lt;p&gt;However, with the same model, if you just use the completions API and then parse the output, it will return the correct quantity:&lt;/p&gt;
    &lt;code&gt;{
  "establishment_name": "PC Market of Choice",
  "date": "2007-01-20",
  "total": 0.32,
  "currency": "USD",
  "items": [
    {
      "name": "Bananas",
      "price": 0.69,
      "quantity": 0.46
    }
  ]
}
&lt;/code&gt;
    &lt;head&gt;Click here to see the code that was used to generate the above outputs.&lt;/head&gt;
    &lt;p&gt;This code is also available on GitHub.&lt;/p&gt;
    &lt;code&gt;#!/usr/bin/env -S uv run

# /// script
# requires-python = "&amp;gt;=3.10"
# dependencies = ["openai", "pydantic", "rich"]
# ///

"""
If you have uv, you can run this code by saving it as structured_outputs_quality_demo.py and then running:

  chmod u+x structured_outputs_quality_demo.py
  ./structured_outputs_quality_demo.py

This script is a companion to https://boundaryml.com/blog/structured-outputs-create-false-confidence
"""

import json
import re
from openai import OpenAI
from pydantic import BaseModel, Field
from rich.console import Console
from rich.pretty import Pretty


class Item(BaseModel):
    name: str
    price: float = Field(description="per-unit item price")
    quantity: float = Field(default=1, description="If not specified, assume 1")


class Receipt(BaseModel):
    establishment_name: str
    date: str = Field(description="YYYY-MM-DD")
    total: float = Field(description="The total amount of the receipt")
    currency: str = Field(description="The currency used for everything on the receipt")
    items: list[Item] = Field(description="The items on the receipt")


client = OpenAI()
console = Console()


def run_receipt_extraction_structured(image_url: str):
    """Call the LLM to extract receipt data from an image URL and return the raw response."""
    prompt_text = (
        """
Extract data from the receipt.
"""
    )

    response = client.beta.chat.completions.parse(
        model="gpt-5.2-2025-12-11",
        messages=[
            {
                "role": "system",
                "content": "You are a precise receipt extraction engine. Return only structured data matching the Receipt schema.",
            },
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": prompt_text,
                    },
                    {"type": "image_url", "image_url": {"url": image_url}},
                ],
            },
        ],
        response_format=Receipt,
    )
    return response.choices[0].message.content, response.choices[0].message.parsed


def run_receipt_extraction_freeform(image_url: str):
    """Call the LLM to extract receipt data from an image URL and return the raw response."""
    prompt_text = (
        """
Extract data from the receipt.

Explain your reasoning, then answer in JSON:
{
  establishment_name: string,
  // YYYY-MM-DD
  date: string,
  // The total amount of the receipt
  total: float,
  // The currency used for everything on the receipt
  currency: string,
  // The items on the receipt
  items: [
    {
      name: string,
      // per-unit item price
      price: float,
      // If not specified, assume 1
      quantity: float,
    }
  ],
}
"""
    )

    response = client.beta.chat.completions.parse(
        model="gpt-5.2-2025-12-11",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": prompt_text,
                    },
                    {"type": "image_url", "image_url": {"url": image_url}},
                ],
            },
        ],
    )
    return response.choices[0].message.content, json.loads(re.search(r"```json(.*?)```", response.choices[0].message.content, flags=re.DOTALL).group(1))



def main() -&amp;gt; None:
    images = [
        {
            "title": "Parsing receipt: fractional quantity",
            "url": "https://boundaryml.com/receipt-fractional-quantity.jpg",
            "expected": "You should expect quantity to be 0.46."
        },
        {
            "title": "Parsing receipt: elephant",
            "url": "https://boundaryml.com/receipt-elephant.jpg",
            "expected": "You should expect an error."
        },
        {
            "title": "Parsing receipt: currency exchange",
            "url": "https://boundaryml.com/receipt-currency-exchange.jpg",
            "expected": "You should expect a warning about mixed currencies."
        },
    ]

    print("This is a demonstration of how structured outputs create false confidence.")

    for entry in images:
        title = entry["title"]
        url = entry["url"]

        completion_structured_content, _ = run_receipt_extraction_structured(url)
        completion_freeform_content, _ = run_receipt_extraction_freeform(url)

        console.print("[cyan]--------------------------------[/cyan]")
        console.print(f"[cyan]{title}[/cyan]")
        console.print(f"Asking LLM to parse receipt from {url}")
        console.print(entry['expected'])
        console.print()
        console.print("[cyan]Using structured outputs:[/cyan]")
        console.print(completion_structured_content)
        console.print()
        console.print("[cyan]Parsing free-form output:[/cyan]")
        console.print(completion_freeform_content)


if __name__ == "__main__":
    main()
&lt;/code&gt;
    &lt;p&gt;Now, what happens if someone submits a picture of an elephant?&lt;/p&gt;
    &lt;p&gt;Or a currency exchange receipt?&lt;/p&gt;
    &lt;p&gt;In these scenarios, you want to let the LLM respond using text. You want it to be able to say that, hey, you're asking me to parse a receipt, but you gave me a picture of an elephant, I can't parse an elephant into a receipt.&lt;/p&gt;
    &lt;p&gt;If you force the LLM to respond using structured outputs, you take that ability away from the LLM. Sure, you'll get an object that satisfies your output format, but it'll be meaningless. It's like when you file a bug report, and the form has 5 mandatory fields about things that have nothing to do with your bug, but you have to put something in those fields to file the bug report: the stuff you put in those fields will probably be useless.&lt;/p&gt;
    &lt;head rend="h1"&gt;I can design my output format better!&lt;/head&gt;
    &lt;p&gt;Yes and no.&lt;/p&gt;
    &lt;p&gt;Yes, you can tell your LLM to return &lt;code&gt;{ receipt data } or { error }&lt;/code&gt; . But what kinds of errors are you going to ask it to consider?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What kind of error should it return if there's no &lt;code&gt;total&lt;/code&gt;listed on the receipt? Should it even return an error or is it OK for it to return&lt;code&gt;total = null&lt;/code&gt;?&lt;/item&gt;
      &lt;item&gt;What if it can successfully parse 7 of 8 items on the receipt, but it's not sure about the 8th item? Should it return (1) the 7 successfully parsed items and a partial parse of the 8th item, (2) only the 7 successfully parsed items and discard the 8th or (3) fail parsing entirely?&lt;/item&gt;
      &lt;item&gt;What if someone submits a picture of an elephant? What kind of error should be returned in that case?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In addition, as you start enumerating all of these errors, you run into the pink elephant problem: the more your prompt talks about errors, the more likely the LLM is to respond with an error.&lt;/p&gt;
    &lt;p&gt;Think of it this way: if someone presses Ctrl-C when running your binary, it is a Good Thing that the error can propagate all the way up through your binary, without you having to explicitly write &lt;code&gt;try { ... } catch CtrlCError { ... }&lt;/code&gt; in every function in your codebase.&lt;/p&gt;
    &lt;p&gt;In the same way that you often want to allow errors to just propagate up while writing software, and only explicitly handle some errors, your LLM should be allowed to respond with errors in whatever fashion it wants to.&lt;/p&gt;
    &lt;head rend="h1"&gt;Chain-of-thought is crippled by structured outputs&lt;/head&gt;
    &lt;p&gt;"Explain your reasoning step by step" is a magic incantation that seemingly makes LLMs much smarter. It also turns out that this trick doesn't work nearly as well when using structured outputs&lt;del&gt;, and we've known this since Aug 2024&lt;/del&gt; (edit: it turns out the "Let Me Speak Freely" paper is fundamentally flawed, but we do still believe the claim to be true).&lt;/p&gt;
    &lt;p&gt;To understand this finding, the intuition I like to use, is to think of every model of having an intelligence "budget", and that if you try to force an LLM to reason in a very specific format, you're making the LLM spend intelligence points on useless work.&lt;/p&gt;
    &lt;p&gt;To make this more concrete, let's use another example. If you prompt an LLM to give you JSON output and reason about it step-by-step, its response will look something like this:&lt;/p&gt;
    &lt;code&gt;If we think step by step we can see that:

1. The email is from Amazon, confirming the status of a specific order.
2. The subject line says "Your Amazon.com order of 'Wood Dowel Rods...' has shipped!" which indicates that the order status is 'SHIPPED'.
3. [...]

Combining all these points, the output JSON is:

```json
{
     "order_status": "SHIPPED",
     [...]
}
```
&lt;/code&gt;
    &lt;p&gt;Notice that although the response contains valid JSON, the response itself is not valid JSON, because of the reasoning text at the start. In other words, you can't use basic chain-of-thought reasoning with structured outputs.&lt;/p&gt;
    &lt;p&gt;You could modify your schema, and add &lt;code&gt;reasoning: string&lt;/code&gt; fields to your output schema, and let the LLM respond with something like this:&lt;/p&gt;
    &lt;code&gt;{
  "reasoning": "If we think step by step we can see that:\n\n 1. The email is from Amazon, confirming the status of a specific order.\n2. The subject line says \"Your Amazon.com order of 'Wood Dowel Rods...' has shipped!\" [...]
  ...
}
&lt;/code&gt;
    &lt;p&gt;In other words, if you're using a &lt;code&gt;reasoning&lt;/code&gt; field with structured outputs, instead of simply asking the LLM to reason about its answer, you're also forcing it to escape newlines and quotes and format that correctly as JSON. You're basically asking the LLM to put a cover page on its TPS report.&lt;/p&gt;
    &lt;head rend="h1"&gt;Why are structured outputs often worse?&lt;/head&gt;
    &lt;p&gt;(To understand this section, you'll need a bit of background on transformer models, specifically how logit sampling works. Feel free to skip this section if you don't have this background.)&lt;/p&gt;
    &lt;p&gt;Model providers like OpenAI and Anthropic implement structured outputs using a technique called constrained decoding:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;By default, when models are sampled to produce outputs, they are entirely unconstrained and can select any token from the vocabulary as the next output. This flexibility is what allows models to make mistakes; for example, they are generally free to sample a curly brace token at any time, even when that would not produce valid JSON. In order to force valid outputs, we constrain our models to only tokens that would be valid according to the supplied schema, rather than all available tokens.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In other words, constrained decoding applies a filter during sampling that says, OK, given the output that you've produced so far, you're only allowed to consider certain tokens.&lt;/p&gt;
    &lt;p&gt;For example, if the LLM has so far produced &lt;code&gt;{"quantity": 51&lt;/code&gt;, and you're constraining output decoding to satisfy  &lt;code&gt;{ quantity: int, ... }&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;{"quantity": 51.7&lt;/code&gt;would not satisfy the constraint, so&lt;code&gt;.7&lt;/code&gt;is not allowed to be the next token,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;{"quantity": 51,&lt;/code&gt;would satisfy the constraint, so&lt;code&gt;,&lt;/code&gt;is allowed to be the next token,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;{"quantity": 510&lt;/code&gt;would satisfy the constraint, so&lt;code&gt;0&lt;/code&gt;is allowed to be the next token (albeit, in this example, with low probability!),&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But if the LLM actually wants to answer with &lt;code&gt;51.7&lt;/code&gt; instead of &lt;code&gt;51&lt;/code&gt;, it isn't allowed to, because of our constraint! (Also, &lt;code&gt;51&lt;/code&gt; is less correct than &lt;code&gt;52&lt;/code&gt; in this scenario.)&lt;/p&gt;
    &lt;p&gt;Sure, if you're using constrained decoding to force it to return &lt;code&gt;{"quantity": 51.7}&lt;/code&gt; instead of &lt;code&gt;{"quantity": 51.7,}&lt;/code&gt; - because trailing commas are not allowed in JSON - it'll probably do the right thing. But that's something you can write code to handle, which leads me to my final point.&lt;/p&gt;
    &lt;head rend="h1"&gt;Just parse the output&lt;/head&gt;
    &lt;p&gt;OK, so if structured outputs are bad, then what's the solution?&lt;/p&gt;
    &lt;p&gt;It turns out to be really simple: let the LLM do what it's trained to do. Allow it to respond in a free-form style:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;let it refuse to count the number of entries in a list&lt;/item&gt;
      &lt;item&gt;let it warn you when you've given it contradictory information&lt;/item&gt;
      &lt;item&gt;let it tell you the correct approach when you inadvertently ask it to use the wrong approach&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Using structured outputs, via constrained decoding, makes it much harder for the LLM to do any of this. Even though you've crafted a guarantee that the LLM will return a response in exactly your requested output format, that guarantee comes at the cost of the quality of that response, because you're forcing the LLM to prioritize complying with your output format over returning a high-quality response. That's why structured outputs create false confidence: it's entirely non-obvious that you're sacrificing output quality to achieve output conformance.&lt;/p&gt;
    &lt;p&gt;Parsing the LLM's free-form output, by contrast, enables you to retain that output quality. In fact, last year we demonstrated that using this technique, not only could you outperform constrained decoding, but you could also make &lt;code&gt;gpt-4o-mini&lt;/code&gt; outperform baseline &lt;code&gt;gpt-4o&lt;/code&gt; (constrained decoding at the time was described as "function calling (strict)").&lt;/p&gt;
    &lt;p&gt;(In a scenario where an attacker is trying to convince your agent to do something you didn't design it to do, the parsing also serves as an effective defense-in-depth layer against malicious prompt injection.)&lt;/p&gt;
    &lt;p&gt;Doing this parsing effectively, though, is rather involved:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;you need a way to embed the output format in the prompt, preferably something less verbose than JSON schema;&lt;/item&gt;
      &lt;item&gt;you need a parser that can find JSON in your output and, when working with non-frontier models, can handle unquoted strings, key-value pairs without comma delimiters, unescaped quotes and newlines; and&lt;/item&gt;
      &lt;item&gt;you need a parser that can coerce the JSON into your output schema, if the model, say, returns a float where you wanted an int, or a &lt;code&gt;string&lt;/code&gt;where you wanted a&lt;code&gt;string[]&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It turns out that not only are these individually pretty hard problems to solve, but it's also really hard to wrap these in an ergonomic API. That's how we ended up with the implementation of schema-aligned parsing that we've made available in BAML, our open-source, local-only DSL.&lt;/p&gt;
    &lt;head rend="h1"&gt;Clarifications&lt;/head&gt;
    &lt;p&gt;In response to a lot of the comments we've gotten so far:&lt;/p&gt;
    &lt;p&gt;To be more precise about my claim, it's not that constrained decoding always gives definitively bad outputs (I went too clickbait with my original tagline, I admit it), so much as that it's really easy, with constrained decoding, to create the illusion that you're getting good responses.&lt;/p&gt;
    &lt;p&gt;Because it forces the LLM to prioritize conforming to your output schema (and in particular forces conforming to JSON), without giving the LLM an escape hatch, it's really easy when using constrained decoding to shift errors from very visible "JSON.parse failed" quantitative errors to very subtle "my users are complaining that I give them crappy results" quality errors.&lt;/p&gt;
    &lt;p&gt;(This is a very hard message to convey at the start of an article, and depends on a lot of nuance that needs to first be explained with examples, which is why I wrote it the way I did.)&lt;/p&gt;
    &lt;p&gt;In response to more specific points:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Give me evals.&lt;/p&gt;&lt;p&gt;Here you go: last year on BFCL with&lt;/p&gt;&lt;code&gt;gpt-4o&lt;/code&gt;we achieved 93.63% accuracy, vs 91.37% accuracy with constrained decoding (then called "Function Calling (Strict)"). (We should re-run these at some point, but we haven't blocked out the time for it.)&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Can't you just use reasoning models instead of chain-of-thought?&lt;/p&gt;&lt;p&gt;Yes, you can use reasoning models to combine chain-of-thought with constrained decoding, but it requires the model to be explicitly trained with&lt;/p&gt;&lt;code&gt;&amp;lt;analysis&amp;gt;&lt;/code&gt;tokens or the equivalent thereof. This is only true of expensive, frontier models (gpt-5, gpt-5-mini, Opus 4.5, Gemini 3) and does not work with gpt-4o-mini, gpt-4.5-mini, nor many of the most commonly used models.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;"Let Me Speak Freely" is a fundamentally flawed paper.&lt;/p&gt;
        &lt;p&gt;I hadn't seen dottxt's "Say What You Mean" response to the "Let Me Speak Freely" paper before, and their critique of the "Let Me Speak Freely" paper's methodology is entirely valid.&lt;/p&gt;
        &lt;p&gt;I still believe that combining reasoning with structured outputs can really mess with your final response quality, because putting reasoning fields in JSON forces the model to escape tokens, which as Aider documented last year, causes models to perform substantially worse. Sure, if your reasoning doesn't need to escape text, then this isn't an issue, but now this is another thing that everyone on your team has to remember while they're iterating on prompts.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thanks all for challenging us on this post: it's super valuable feedback, and we really appreciate the time y'all are taking to read and respond to us.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46345333</guid><pubDate>Sun, 21 Dec 2025 15:06:46 +0000</pubDate></item><item><title>ARIN Public Incident Report â€“ 4.10 Misissuance Error</title><link>https://www.arin.net/announcements/20251212/</link><description>&lt;doc fingerprint="f00af2df20eded25"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Public Incident Report Ã¢ 4.10 Issuance Error&lt;/head&gt;
    &lt;p&gt;Posted: Friday, 12 December 2025 &lt;lb/&gt; ARIN &lt;/p&gt;
    &lt;head rend="h2"&gt;Executive Summary&lt;/head&gt;
    &lt;p&gt;On 2 December 2025, an IPv4 block 23.150.164.0/24, correctly allocated to the Original Customer, was inadvertently removed and reissued to the Requesting Customer during a 4.10 allocation process. This error stemmed from the current manual and partially offline 4.10 inventory process.&lt;/p&gt;
    &lt;p&gt;The incorrect state persisted until 9 December 2025, when the Original Customer reported the issue. ARIN restored the 23.150.164.0/24 to the Original Customer, issued a replacement /24 to the Requesting Customer, coordinated withdrawal of the incorrect route announcement, and notified the affected parties.&lt;/p&gt;
    &lt;p&gt;This incident highlights known weaknesses in ARINÃ¢s current Internet Number Resources (INR) Inventory handling for 4.10 transition space and underscores the need to complete the transition to a fully automated, integrated online inventory architecture.&lt;/p&gt;
    &lt;head rend="h2"&gt;Incident Description&lt;/head&gt;
    &lt;p&gt;Following the current allocation process for 4.10 space, an RSD analyst:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Relied on legacy/manual 4.10 inventory artifacts, including a flat file and sparse allocation spreadsheet&lt;/item&gt;
      &lt;item&gt;Did not recognize indicators in ARIN Online showing that 23.150.164.0/24 was already allocated to the Original Customer&lt;/item&gt;
      &lt;item&gt;Removed 23.150.164.0/24 from the Original Customer&lt;/item&gt;
      &lt;item&gt;Reissued that same /24 to the Requesting Customer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As a result, the registration record and associated ROAs for the Original Customer were deleted in error, and the /24 appeared as allocated to the Requesting Customer in ARINÃ¢s systems.&lt;/p&gt;
    &lt;head rend="h2"&gt;Customer Impact and Risk&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The /24 was removed from the Original Customer account and assigned to another organization.&lt;/item&gt;
      &lt;item&gt;The ROA associated with the block was removed and had to be recreated after restoration.&lt;/item&gt;
      &lt;item&gt;The block was announced by a third-party provider under the incorrect registration, introducing risk of routing conflict and confusion.&lt;/item&gt;
      &lt;item&gt;The incorrect state persisted for approximately seven days before detection.&lt;/item&gt;
      &lt;item&gt;The Original Customer reported the issue via Ask ARIN and a Help Desk call on 9 December 2025.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The customer has not provided a technical impact statement.&lt;/p&gt;
    &lt;head rend="h2"&gt;Timeline of Events&lt;/head&gt;
    &lt;p&gt;(All event times are ET Ã¢ Eastern Time)&lt;/p&gt;
    &lt;head rend="h3"&gt;25Ã¢26 November 2025&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;25 November, 11:59 AM Ã¢ 4.10 space request received from the Requesting Customer.&lt;/item&gt;
      &lt;item&gt;26 November, 6:25 AM Ã¢ Ticket assigned to an RSD Analyst for processing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;2 December 2025 Ã¢ Incident Occurs&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;12:10 PM Ã¢ Ticket approved for issuance of 4.10 space by designated RSD Analyst.&lt;/item&gt;
      &lt;item&gt;~12:10Ã¢12:30 PM Ã¢ In the process of fulfilling the 4.10 request, the designated Analyst: &lt;list rend="ul"&gt;&lt;item&gt;Opened the e-black-book (an offline Excel-based inventory file, separate from the primary online inventory system), reviewed the existing 4.10 allocations, and selected 23.150.164.0 as the next available sparse entry.&lt;/item&gt;&lt;item&gt;Returned to the ARIN Online management application and queried for 23.150.164.0 based on the entry identified in the e-black-book. At this time the analyst did not recognize that the /24 was already allocated to the Original Customer.&lt;/item&gt;&lt;item&gt;Performed a block split and deleted 23.150.164.0/24 Ã¢ not recognizing that it was allocated to the Original Customer Ã¢ which removed associated registry services (ROAs, reverse DNS, etc.).&lt;/item&gt;&lt;item&gt;Issued 23.150.164.0/24 to the Requesting Customer.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;2Ã¢9 December 2025 Ã¢ Incorrect State Persists&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The /24 remained misregistered.&lt;/item&gt;
      &lt;item&gt;The Requesting Customer upstream provider announced the block.&lt;/item&gt;
      &lt;item&gt;No automated detection of the error occurred.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;9 December 2025 Ã¢ Detection and Resolution&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;10:12 AM Ã¢ The Original Customer submitted an Ask ARIN ticket regarding the problem.&lt;/item&gt;
      &lt;item&gt;10:14 AM Ã¢ The Original Customer contacted the Help Desk; escalation to Director at 10:20 AM.&lt;/item&gt;
      &lt;item&gt;10:20Ã¢10:30 AM Ã¢ Director reviewed block history and directed corrective actions.&lt;/item&gt;
      &lt;item&gt;10:30 AM Ã¢ Director and CXO approved: &lt;list rend="ul"&gt;&lt;item&gt;Removal of the /24 from the Requesting Customer&lt;/item&gt;&lt;item&gt;Issuance of a replacement /24 to the Requesting Customer&lt;/item&gt;&lt;item&gt;Restoration of 23.150.164.0/24 to the Original Customer&lt;/item&gt;&lt;item&gt;Coordination of route withdrawal&lt;/item&gt;&lt;item&gt;Update of inaccurate POC information&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;10:44 AM Ã¢ First notification email sent to the Requesting Customer.&lt;/item&gt;
      &lt;item&gt;10:54 AM Ã¢ Second email sent noting invalid phone contact.&lt;/item&gt;
      &lt;item&gt;12:01 PM Ã¢ Corrective actions completed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Root Cause&lt;/head&gt;
    &lt;p&gt;A manual 4.10 workflow that relies on a combination of online systems and offline flat files/spreadsheets for inventory management allowed a current customer allocation to be mistakenly identified as available for issuance. This reliance on offline spreadsheets is a legacy constraint where post-runout 4.10 inventory is maintained outside the primary online system to keep it reserved. The lack of a unified view of inventory and related business-rule-driven system controls enabled the error to proceed without detection.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contributing Factors&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hybrid inventory architecture (online + offline) for 4.10 space.&lt;/item&gt;
      &lt;item&gt;Sparse allocation methods implemented through manual tools rather than integrated system logic.&lt;/item&gt;
      &lt;item&gt;Generic warning messages that are not routing aware or business-rule driven.&lt;/item&gt;
      &lt;item&gt;High demand on analysts to catch procedural errors in a manual â€œswivel chairâ€ workflow.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Mitigation Plan and Next Steps&lt;/head&gt;
    &lt;head rend="h3"&gt;Immediate / Near-Term Controls (Completed)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Updated Process Controls (completed) &lt;list rend="ul"&gt;&lt;item&gt;RSD has implemented additional process controls that require a dual review for all ticketing type workflows that include a network delete.&lt;/item&gt;&lt;item&gt;Only a limited set of experienced analysts are permitted to perform this function.&lt;/item&gt;&lt;item&gt;Reviews and approvals are performed at set times each day with a second reviewer involved for any ticket that includes a delete step.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Updated 4.10 Issuing Playbook &lt;list rend="ul"&gt;&lt;item&gt;Document and enforce a revised playbook for issuing 4.10 (with checklists) that includes: &lt;list rend="ul"&gt;&lt;item&gt;Required checks for existing allocations and ROAs&lt;/item&gt;&lt;item&gt;Explicit verification steps prior to any delete/reissue action&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Document and enforce a revised playbook for issuing 4.10 (with checklists) that includes: &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;System and Architecture Improvements (Medium-Term)&lt;/head&gt;
    &lt;p&gt;Accelerate the ongoing INR Inventory Management Roadmap item: This incident reinforces the urgency of the architecture work already underway (reviewed Oct 2025) to move legacy offline inventories into a modern, online architecture. Specific alignment actions include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Stronger business ruleÃ¢based warnings and controls &lt;list rend="ul"&gt;&lt;item&gt;Enhance warning logic when issuing or modifying 4.10 space to include: &lt;list rend="ul"&gt;&lt;item&gt;Clear alerts if the /24 is already allocated to an Org&lt;/item&gt;&lt;item&gt;Clear alerts if active ROAs exist for the exact block or covering prefix&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Implement system controls for resource types and staff roles, with flags and audit trails for review and auditing.&lt;/item&gt;&lt;item&gt;Replace generic, nonÃ¢ROA-aware warnings that are easily treated as noise.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Enhance warning logic when issuing or modifying 4.10 space to include: &lt;/item&gt;
      &lt;item&gt;Continue Engineering solution for offline inventory &lt;list rend="ul"&gt;&lt;item&gt;Move offline 4.10 and microallocation inventories, and the viip file for IPv6, into the integrated online inventory architecture.&lt;/item&gt;&lt;item&gt;Eliminate reliance on separate spreadsheets and flat files for production issuing.&lt;/item&gt;&lt;item&gt;Implement business-rule-driven warnings for existing allocations and ROAs&lt;/item&gt;&lt;item&gt;Introduce role-based controls, flags, and audit trails&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Advance the Ã¢Updated Resource Status TaxonomyÃ¢ work &lt;list rend="ul"&gt;&lt;item&gt;Ensure 4.10 status and history are fully visible and consistent inside the primary system.&lt;/item&gt;&lt;item&gt;Provide analysts with a clear, unified view of current holder, status, and ROA/IRR context.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Fast-track automation of all inventory issuing &lt;list rend="ul"&gt;&lt;item&gt;Reduce or eliminate manual issuing where possible, with priority for higher-risk categories such as 4.10 and RPKI-covered space.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Regards,&lt;/p&gt;
    &lt;p&gt;American Registry for Internet Numbers (ARIN)&lt;/p&gt;
    &lt;head rend="h2"&gt;Recent Announcements&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Volunteer to Serve on the 2026 ARIN Fellowship Selection Committee&lt;/item&gt;
      &lt;item&gt;Sponsorship Opportunities Available for 2026 ARIN Public Policy and Members Meetings&lt;/item&gt;
      &lt;item&gt;Public Incident Report Ã¢ 4.10 Issuance Error&lt;/item&gt;
      &lt;item&gt;ARIN Academy Adds IPv6 Planning Course&lt;/item&gt;
      &lt;item&gt;Reclassification of Inactive General Members Completed 19 November 2025&lt;/item&gt;
      &lt;item&gt;ARIN 56 Meeting Report Now Available&lt;/item&gt;
      &lt;item&gt;Concluding the Second Consultation on the Draft RIR Governance Document&lt;/item&gt;
      &lt;item&gt;2025 ARIN Election Results&lt;/item&gt;
      &lt;item&gt;Public Incident Report Ã¢ ARIN Hosted RPKI Service&lt;/item&gt;
      &lt;item&gt;IPv4 Waiting List Distribution&lt;/item&gt;
      &lt;item&gt;Â» View Archive&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46345444</guid><pubDate>Sun, 21 Dec 2025 15:19:41 +0000</pubDate></item><item><title>CO2 batteries that store grid energy take off globally</title><link>https://spectrum.ieee.org/co2-battery-energy-storage</link><description>&lt;doc fingerprint="267d315709b6a538"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Grid-Scale Bubble Batteries Will Soon Be Everywhere&lt;/head&gt;
    &lt;p&gt;When the sun sets on solar panels, these gas-filled domes take over&lt;/p&gt;
    &lt;p&gt;This giant bubble on the island of Sardinia holds 2,000 tonnes of carbon dioxide. But the gas wasnâ€™t captured from factory emissions, nor was it pulled from the air. It came from a gas supplier, and it lives permanently inside the domeâ€™s system to serve an eco-friendly purpose: to store large amounts of excess renewable energy until itâ€™s needed.&lt;/p&gt;
    &lt;p&gt;Developed by the Milan-based company Energy Dome, the bubble and its surrounding machinery demonstrate a first-of-its-kind â€œCO2 Battery,â€ as the company calls it. The facility compresses and expands CO2 daily in its closed system, turning a turbine that generates 200 megawatt-hours of electricity, or 20 MW over 10 hours. And in 2026, replicas of this plant will start popping up across the globe.&lt;/p&gt;
    &lt;p&gt;We mean that literally. It takes just half a day to inflate the bubble. The rest of the facility takes less than two years to build and can be done just about anywhere thereâ€™s 5 hectares of flat land.&lt;/p&gt;
    &lt;p&gt;The first to build one outside of Sardinia will be one of Indiaâ€™s largest power companies, NTPC Limited. The company expects to complete its CO2 Battery sometime in 2026 at the Kudgi power plant in Karnataka, in India. In Wisconsin, meanwhile, the public utility Alliant Energy received the all clear from authorities to begin construction of one in 2026 to supply power to 18,000 homes.&lt;/p&gt;
    &lt;p&gt;And Google likes the concept so much that it plans to rapidly deploy the facilities in all of its key data-center locations in Europe, the United States, and the Asia-Pacific region. The idea is to provide electricity-guzzling data centers with round-the-clock clean energy, even when the sun isnâ€™t shining or the wind isnâ€™t blowing. The partnership with Energy Dome, announced in July, marked Googleâ€™s first investment in long-duration energy storage.&lt;/p&gt;
    &lt;p&gt;â€œWeâ€™ve been scanning the globe seeking different solutions,â€ says Ainhoa Anda, Googleâ€™s senior lead for energy strategy, in Paris. The challenge the tech giant has encountered is not only finding a long-duration storage option, but also one that works with the unique specs of every region. â€œSo standardization is really important, and this is one of the aspects that we really likeâ€ about Energy Dome, she says. â€œThey can really plug and play this.â€&lt;/p&gt;
    &lt;p&gt;Google will prioritize placing the Energy Dome facilities where theyâ€™ll have the most impact on decarbonization and grid reliability, and where thereâ€™s a lot of renewable energy to store, Anda says. The facilities can be placed adjacent to Googleâ€™s data centers or elsewhere within the same grid. The companies did not disclose the terms of the deal.&lt;/p&gt;
    &lt;p&gt;Anda says Google expects to help the technology â€œreach a massive commercial stage.â€&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting creative with long-duration energy storage&lt;/head&gt;
    &lt;p&gt;All this excitement is based on Energy Domeâ€™s one full-size, grid-connected plant in Ottana, Sardinia, which was completed in July. It was built to help solve one of the energy transitionâ€™s biggest challenges: the need for grid-scale storage that can provide power for more than 8 hours at a time. Called long-duration energy storage, or LDES in industry parlance, the concept is the key to maximizing the value of renewable energy.&lt;/p&gt;
    &lt;p&gt;When sun and wind are abundant, solar and wind farms tend to produce more electricity than a grid needs. So storing the excess for use when these resources are scarce just makes sense. LDES also makes the grid more reliable by providing backup and supplementary power.&lt;/p&gt;
    &lt;p&gt;The problem is that even the best new grid-scale storage systems on the marketâ€”mainly lithium-ion batteriesâ€”provide only about 4 to 8 hours of storage. Thatâ€™s not long enough to power through a whole night, or multiple cloudy and windless days, or the hottest week of the year, when energy demand hits its peak.&lt;/p&gt;
    &lt;p&gt;After the CO2 leaves the dome, it is compressed, cooled, reduced to a liquid, and stored in pressure vessels. To release the energy, the process reverses: The liquid is evaporated, heated, expanded, and then fed through a turbine that generates electricity. Luigi Avantaggiato&lt;/p&gt;
    &lt;p&gt;Lithium-ion battery systems could be increased in size to store more and last longer, but systems of that size usually arenâ€™t economically viable. Other grid-scale battery chemistries and approaches are in development, such as sodium-based, iron-air, and vanadium redox flow batteries. But the energy density, costs, degradation, and funding complications have challenged the developers of those alternatives.&lt;/p&gt;
    &lt;p&gt;Researchers have also experimented with storing energy by compressing air, heating up blocks or sand, using hydrogen or methanol, pressurizing water deep underground, and even dangling heavy objects in the air and dropping them. (The creativity devoted to LDES is impressive.) But geologic constraints, economic viability, efficiency, and scalability have hindered the commercialization of these strategies.&lt;/p&gt;
    &lt;p&gt;The tried-and-true grid-scale storage optionâ€”pumped hydro, in which water is pumped between reservoirs at different elevationsâ€”lasts for decades and can store thousands of megawatts for days. But these systems require specific topography, a lot of land, and can take up to a decade to build.&lt;/p&gt;
    &lt;p&gt;CO2 Batteries check a lot of boxes that other approaches donâ€™t. They donâ€™t need special topography like pumped-hydro reservoirs do. They donâ€™t need critical minerals like electrochemical and other batteries do. They use components for which supply chains already exist. Their expected lifetime stretches nearly three times as long as lithium-ion batteries. And adding size and storage capacity to them significantly decreases cost per kilowatt-hour. Energy Dome expects its LDES solution to be 30 percent cheaper than lithium-ion.&lt;/p&gt;
    &lt;p&gt;China has taken note. China Huadian Corp. and Dongfang Electric Corp. are reportedly building a CO2-based energy-storage facility in the Xinjiang region of northwest China. Media reports show renderings of domes but give widely varying storage capacitiesâ€”including 100 MW and 1,000 MW. The Chinese companies did not respond to IEEE Spectrumâ€™s requests for information.&lt;/p&gt;
    &lt;p&gt;â€œWhat I can say is that they are developing something very, very similar [to Energy Domeâ€™s CO2 Battery] but quite large in scale,â€ says Claudio Spadacini, Energy Domeâ€™s founder and CEO. The Chinese companies â€œare good, they are super fast, and they have a lot of money,â€ he says.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why is Google investing in CO2 Batteries?&lt;/head&gt;
    &lt;p&gt;When I visited Energy Domeâ€™s Sardinia facility in October, the CO2 had just been pumped out of the dome, so I was able to peek inside. It was massive, monochromatic, and pretty much empty. The inner membrane, which had been holding the uncompressed CO2, had collapsed across the entire floor. A few pockets of the gas remained, making the off-white sheet billow up in spots.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the translucent outer dome allowed some daylight to pass through, creating a creamy glow that enveloped the vast space. With no structural framing, the only thing keeping the dome upright was the small difference in pressure between the inside and outside air.&lt;/p&gt;
    &lt;p&gt;â€œThis is incredible,â€ I said to my guide, Mario Torchio, Energy Domeâ€™s global marketing and communications director.&lt;/p&gt;
    &lt;p&gt;â€œIt is. But itâ€™s physics,â€ he said.&lt;/p&gt;
    &lt;p&gt;Outside the dome, a series of machines connected by undulating pipes moves the CO2 out of the dome for compressing and condensing. First, a compressor pressurizes the gas from 1 bar (100,000 pascals) to about 55 bar (5,500,000 pa). Next, a thermal-energy-storage system cools the CO2 to an ambient temperature. Then a condenser reduces it into a liquid that is stored in a few dozen pressure vessels, each about the size of a school bus. The whole process takes about 10 hours, and at the end of it, the battery is considered charged.&lt;/p&gt;
    &lt;p&gt;To discharge the battery, the process reverses. The liquid CO2 is evaporated and heated. It then enters a gas-expander turbine, which is like a medium-pressure steam turbine. This drives a synchronous generator, which converts mechanical energy into electrical energy for the grid. After that, the gas is exhausted at ambient pressure back into the dome, filling it up to await the next charging phase.&lt;/p&gt;
    &lt;p&gt;Energy Dome engineers inspect the dryer system, which keeps the gaseous COâ‚‚ in the dome at optimal dryness levels at all times.Luigi Avantaggiato&lt;/p&gt;
    &lt;p&gt;Itâ€™s not rocket science. Still, someone had to be the first to put it together and figure out how to do it cost-effectively, which Spadacini says his company has accomplished and patented. â€œHow we seal the turbo machinery, how we store the heat in the thermal-energy storage, how we store the heat after condensingâ€¦can really cut costs and increase the efficiency,â€ he says.&lt;/p&gt;
    &lt;p&gt;The company uses pure, purpose-made CO2 instead of sourcing it from emissions or the air, because those sources come with impurities and moisture that degrade the steel in the machinery.&lt;/p&gt;
    &lt;head rend="h2"&gt;What happens if the dome is punctured?&lt;/head&gt;
    &lt;p&gt;On the downside, Energy Domeâ€™s facility takes up about twice as much land as a comparable capacity lithium-ion battery would. And the domes themselves, which are about the height of a sports stadium at their apex, and longer, might stand out on a landscape and draw some NIMBY pushback.&lt;/p&gt;
    &lt;p&gt;And what if a tornado comes? Spadacini says the dome can withstand wind up to 160 kilometers per hour. If Energy Dome can get half a dayâ€™s warning of severe weather, the company can just compress and store the CO2 in the tanks and then deflate the outer dome, he says.&lt;/p&gt;
    &lt;p&gt;If the worst happens and the dome is punctured, 2,000 tonnes of CO2 will enter the atmosphere. Thatâ€™s equivalent to the emissions of about 15 round-trip flights between New York and London on a Boeing 777. â€œItâ€™s negligible compared to the emissions of a coal plant,â€ Spadacini says. People will also need to stay back 70 meters or more until the air clears, he says.&lt;/p&gt;
    &lt;p&gt;Worth the risk? The companies lining up to build these systems seem to think so.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Grid-Scale Battery Stabilizes Scottish Power Supply â€º&lt;/item&gt;
      &lt;item&gt;Backing Up the Power Grid With Green Methanol â€º&lt;/item&gt;
      &lt;item&gt;DOE Places Compressed-Air Energy Storage Loan Under Review â€º&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46345506</guid><pubDate>Sun, 21 Dec 2025 15:27:36 +0000</pubDate></item><item><title>E.W.Dijkstra Archive</title><link>https://www.cs.utexas.edu/~EWD/welcome.html</link><description>&lt;doc fingerprint="e615734b00f3a2b8"&gt;
  &lt;main&gt;
    &lt;p&gt;Edsger Wybe Dijkstra was one of the most influential members of computing scienceâ€™s founding generation. Among the domains in which his scientific contributions are fundamental are&lt;/p&gt;
    &lt;p&gt;algorithm design&lt;/p&gt;
    &lt;p&gt;programming languages&lt;/p&gt;
    &lt;p&gt;program design&lt;/p&gt;
    &lt;p&gt;operating systems&lt;/p&gt;
    &lt;p&gt;distributed processing&lt;/p&gt;
    &lt;p&gt;formal specification and verification&lt;/p&gt;
    &lt;p&gt;design of mathematical arguments&lt;/p&gt;
    &lt;p&gt;In addition, Dijkstra was intensely interested in teaching, and in the relationships between academic computing science and the software industry.&lt;/p&gt;
    &lt;p&gt;During his forty-plus years as a computing scientist, which included positions in both academia and industry, Dijkstraâ€™s contributions brought him many prizes and awards, including computing scienceâ€™s highest honor, the ACM Turing Award.&lt;/p&gt;
    &lt;p&gt;The Manuscripts&lt;/p&gt;
    &lt;p&gt;Like most of us, Dijkstra always believed it a scientistâ€™s duty to maintain a lively correspondence with his scientific colleagues. To a greater extent than most of us, he put that conviction into practice. For over four decades, he mailed copies of his consecutively numbered technical notes, trip reports, insightful observations, and pungent commentaries, known collectively as â€œEWDsâ€, to several dozen recipients in academia and industry. Thanks to the ubiquity of the photocopier and the wide interest in Dijkstraâ€™s writings, the informal circulation of many of the EWDs eventually reached into the thousands.&lt;/p&gt;
    &lt;p&gt;Although most of Dijkstraâ€™s publications began life as EWD manuscripts, the great majority of his manuscripts remain unpublished. They have been inaccessible to many potential readers, and those who have received copies have been unable to cite them in their own work. To alleviate both of these problems, the department has collected over a thousand of the manuscripts in this permanent web site, in the form of PDF bitmap documents (to read them, youâ€™ll need a copy of Acrobat Reader). We hope you will find it convenient, useful, inspiring, and enjoyable.&lt;/p&gt;
    &lt;p&gt;The original manuscripts, along with diaries, correspondence, photographs, and other papers, are housed at The Center for American History of The University of Texas at Austin.&lt;/p&gt;
    &lt;p&gt;Indexes&lt;/p&gt;
    &lt;p&gt;Each manuscript file is accessible through either of two indexes:&lt;/p&gt;
    &lt;p&gt;0. BibTeX index. Each entry includes all the available bibliographic data.&lt;/p&gt;
    &lt;p&gt;1. Ad-hoc indexes. These contain titles only, but are faster if you know what youâ€™re looking for.&lt;/p&gt;
    &lt;p&gt;EWD-numbered documents(This index gives an approximate correspondence between manuscriptsâ€™ EWD numbers and the year in which they appeared.)&lt;/p&gt;
    &lt;p&gt;Technical reports from the Mathematical Centre (now CWI: Centrum voor Wiskunde en Informatica)&lt;/p&gt;
    &lt;p&gt;You can find a table relating EWD numbers to publication years here.&lt;/p&gt;
    &lt;p&gt;Many of the privately circulated manuscripts collected here were subsequently published; their copyrights are held by their respective publishers.&lt;/p&gt;
    &lt;p&gt;Transcripts and translations&lt;/p&gt;
    &lt;p&gt;A growing number of the PDF bitmap documents have been transcribed to make them searchable and accessible to visitors who are visually impaired.&lt;/p&gt;
    &lt;p&gt;A few of the manuscripts written in Dutch have been translated into English, and one â€”EWD1036â€” has been translated into Spanish. EWD28 has been translated from English into Russian.&lt;/p&gt;
    &lt;p&gt;For these transcriptions and translations we are grateful to over sixty contributors. Volunteers willing to transcribe manuscripts are always welcome (Note: doing EWDs justice in translation has turned out to be too difficult, so we are no longer soliciting translations).&lt;/p&gt;
    &lt;p&gt;Proofreading Each transcription gets a cursory scan as itâ€™s prepared for uploading, but since a web page can always be updated, I donâ€™t strive for (unattainable) perfection before installing it. On the web, proofreading is a game that can be played by every reader; if you spot an error, please&lt;/p&gt;
    &lt;p&gt;Links between EWDs&lt;/p&gt;
    &lt;p&gt;A compilation of cross-references has been contributed by Diethard Michaelis. As its author notes, the collection is incomplete, and all readers are invited to add to it.&lt;/p&gt;
    &lt;p&gt;Dijkstra often returned to topics about which he had already written, when he had something new to say or even just a better way of saying it. When Dijkstra himself didnâ€™t provide the backward references, we indicate the relationship by "see also" links in the index, leaving the judgment of the extent to which the earlier EWD is superseded by the later one to the reader. Any reader who notices such a relationship is invited to&lt;/p&gt;
    &lt;p&gt;Summaries&lt;/p&gt;
    &lt;p&gt;We have begun adding summaries of the EWDs. This innovation was suggested by GÃ¼nter Rote, who contributed the first dozen summaries. Additional contributions of summariesâ€”especially summaries in English of EWDs in Dutchâ€”are most welcome.&lt;/p&gt;
    &lt;p&gt;Copyrights&lt;/p&gt;
    &lt;p&gt;Copyrights in most EWDs are held by his children, one of whom â€” â€” handles requests for permission to publish reproductions. The exceptions are documents that were published, and whose copyrights are held by their publishers; those documents are listed here, and each one is provided with a cover page identifying the copyright holder.&lt;/p&gt;
    &lt;p&gt;Because the original manuscripts are in possession of the Briscoe Center for American History at The University of Texas, the Centerâ€™s policies are also applicable.&lt;/p&gt;
    &lt;p&gt;Video and audio&lt;/p&gt;
    &lt;p&gt;In addition to the manuscripts, you may enjoy some recordings of Dijkstra lectures and interviews.&lt;/p&gt;
    &lt;p&gt;About Dijkstra and his work&lt;/p&gt;
    &lt;p&gt;An interview with Dijkstra (Spanish translation here) was conducted in 1985 by Rogier F. van Vlissingen, who has also written a personal reflection on â€œDijkstraâ€™s sense of what computer science and programming are and what they arenâ€™t.â€&lt;/p&gt;
    &lt;p&gt;Another interview was conducted by Philip L. Frana in August 2001. A transcript is available in the on-line collection of the Charles Babbage Institute.&lt;/p&gt;
    &lt;p&gt;To mark the occasion of Dijkstraâ€™s retirement in November 1999 from the Schlumberger Centennial Chair in Computer Sciences, which he had occupied since 1984, and to celebrate his forty-plus years of seminal contributions to computing science, the Department of Computer Sciences organized a symposium, In Pursuit of Simplicity, which took place on his birthday in May 2000. The symposiumâ€™s program (10 MB) contains an outline of Dijkstraâ€™s career, as well as a collection of quotes culled from his writings, from his blackboard, and from what others have said about him. Banquet speeches by David Gries, Fred Schneider, Krzysztof Apt, W.M. Turski, and H. Richards were recorded on a video.&lt;/p&gt;
    &lt;p&gt;Dijkstraâ€™s death in August 2002 was marked by many obituaries and memorials, including the Computer Sciences departmentâ€™s memorial celebration.&lt;/p&gt;
    &lt;p&gt;A remembrance of Dijkstra was posted in May 2008 by Maarten van Emden (thanks to Tristram Brelstaff for noting it).&lt;/p&gt;
    &lt;p&gt;In 2021 Krzysztof R. Apt and Tony Hoare edited a commemoration of Edsger Dijkstra written by more than twenty computer scientists who knew him as a colleague, teacher, and friend.&lt;/p&gt;
    &lt;p&gt;A blog devoted to Dijkstraâ€™s works and thoughts has been created, and is being maintained, by the historian of computing Edgar G. Daylight. An article by Daylight, â€œDijkstraâ€™s Rallying Cry for Generalization: the Advent of the Recursive Procedure, late 1950s - early 1960s,â€ appeared in The Computer Journal, March 2011.&lt;/p&gt;
    &lt;p&gt;In his blog A Programmerâ€™s Place, Maarten van Emden has an entry entitled â€œAnother scoop by Dijkstra?â€. The entry describes Dijkstraâ€™s â€œremarkable insight [in â€œNotes on Structured Programmingâ€ (EWD 249)] that resolves the stand-off between the Sieve of Eratosthenes (efficient in terms of time, but not memory) and the method of Trial Division (efficient in terms of memory, but not time)â€ by applying the Assembly-line Principle.&lt;/p&gt;
    &lt;p&gt;The Edsger W. Dijkstra Prize in Distributed Computing honors Dijkstraâ€™s â€œfoundational work on concurrency primitives (such as the semaphore), concurrency problems (such as mutual exclusion and deadlock), reasoning about concurrent systems, and self-stabilization [, which] comprises one of the most important supports upon which the field of distributed computing is built.â€&lt;/p&gt;
    &lt;p&gt;The Dijkstra Memorial Lectures&lt;/p&gt;
    &lt;p&gt;A series of annual lectures in memory of Dijkstra commenced at The University of Texas in October 2010.&lt;/p&gt;
    &lt;p&gt;About this site&lt;/p&gt;
    &lt;p&gt;Recent significant changes in the site are listed here; the most recent change was posted on 30 March 2021.&lt;/p&gt;
    &lt;p&gt;The folks who contributed most significantly to the siteâ€™s creation are acknowledged here.&lt;/p&gt;
    &lt;p&gt;Comments and suggestions about the site are always welcome; please email them to the&lt;/p&gt;
    &lt;p&gt;Related site&lt;/p&gt;
    &lt;p&gt;If you find this site interesting, you may also be interested in another site:&lt;/p&gt;
    &lt;p&gt;Discipline in Thought which is a website dedicated to disciplined thinking, calculational mathematics, and mathematical methodology. The members of this site are markedly influenced by the works of EWD, and the material shared through the website continues in the traditions set by EWD (among others).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46345523</guid><pubDate>Sun, 21 Dec 2025 15:29:31 +0000</pubDate></item><item><title>Show HN: WalletWallet â€“ create Apple passes from anything</title><link>https://walletwallet.alen.ro/</link><description>&lt;doc fingerprint="6fc1f19d954373"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;WalletWallet&lt;/head&gt;
    &lt;p&gt;A simple utility to convert physical barcodes into digital passes for Apple WalletÂ®. Entirely free and runs directly from your browser.&lt;/p&gt;
    &lt;p&gt; 1 Enter your membership or loyalty card barcode data. &lt;/p&gt;
    &lt;p&gt; 2 Configure the appearance and titles for your pass. &lt;/p&gt;
    &lt;p&gt; 3 Download and open the file to add it to your Wallet. &lt;/p&gt;
    &lt;p&gt; No Sign-up &lt;/p&gt;
    &lt;p&gt; Private &lt;/p&gt;
    &lt;p&gt; No Install &lt;/p&gt;
    &lt;p&gt; Pass Configuration &lt;/p&gt;
    &lt;p&gt;Generates a standard .pkpass file&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46345745</guid><pubDate>Sun, 21 Dec 2025 16:04:05 +0000</pubDate></item><item><title>Show HN: Books mentioned on Hacker News in 2025</title><link>https://hackernews-readings-613604506318.us-west1.run.app</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46345897</guid><pubDate>Sun, 21 Dec 2025 16:21:04 +0000</pubDate></item><item><title>Autoland saves King Air, everyone reported safe</title><link>https://avbrief.com/autoland-saves-king-air-everyone-reported-safe/</link><description>&lt;doc fingerprint="f9bcabf52168cfc9"&gt;
  &lt;main&gt;
    &lt;p&gt;Garmin has confirmed the first emergency use of its Autoland system occurred on Saturday in Colorado. â€œGarmin can confirm that an emergency Autoland activation occurred at Rocky Mountain Metropolitan Airport in Broomfield, Colorado,â€ the company said in a statement Sunday. â€œThe Autoland took place on Sat., Dec. 20, resulting in a successful landing. We look forward to sharing additional details at the appropriate time.â€ Garmin also confirmed that it was the first use of the system that was not a test or a demonstration.&lt;/p&gt;
    &lt;p&gt;Social media posts from flight tracking hobbyists reported a King Air 200 squawked 7700 about 2 p.m. local time today. The Autoland system was initiated and landed the aircraft at Rocky Mountain Metropolitan Airport near Denver. A recording from LiveATCâ€™s feed of the airportâ€™s tower frequency includes a robotic female voice declaring a pilot incapacitation and the intention to land on Runway 30. The tape is below and first mention of the incident by ATC is at about 5:00. The Autoland system announces its intentions at about 11:10. (The time stamps are approximate.) There is no word on the condition of the pilot but social media posts suggest all aboard were safe.&lt;/p&gt;
    &lt;p&gt;The aircraft, N479BR, was being operated by Buffalo River Outfitters from Aspen to Rocky Mountain Metropolitan. Itâ€™s not clear how many people were on board. The system appeared to work flawlessly, and the controller at Rocky Mountain Metropolitan seemed to take it in stride, accommodating as many requests as he could before shutting down the airport for the landing. Weâ€™ll have more detail on this as it becomes available. There were some social media posts questioning whether there was an actual emergency. Garmin did not respond to our questions regarding those posts. The aircraft operator did not respond to our email and phone requests for comment.&lt;/p&gt;
    &lt;p&gt;Larry Anglisano recorded this video demonstration of the Autoland system in the Beechcraft King Air.&lt;/p&gt;
    &lt;p&gt;A reader was at the airport Saturday and shared this video that he had posted to Instagram.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46346214</guid><pubDate>Sun, 21 Dec 2025 16:57:57 +0000</pubDate></item><item><title>Youâ€™re not burnt out, youâ€™re existentially starving</title><link>https://neilthanedar.com/youre-not-burnt-out-youre-existentially-starving/</link><description>&lt;doc fingerprint="7a329257eea0cf5f"&gt;
  &lt;main&gt;
    &lt;p&gt;â€œThose who have a â€˜Whyâ€™ to live, can bear with almost any â€˜Howâ€™.â€&lt;/p&gt;
    &lt;p&gt;â€• Viktor Frankl quoting Friedrich Nietzsche, Manâ€™s Search for Meaning&lt;/p&gt;
    &lt;p&gt;Let me guess:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Your life is going pretty darn well by any objective metric. &lt;list rend="ul"&gt;&lt;item&gt;Nice place to live. More than enough stuff. Family and friends who love you.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;But youâ€™re tired, burnt out, and more. &lt;list rend="ul"&gt;&lt;item&gt;It feels like youâ€™re stuck in the ordinary when all you want to do is chase greatness.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Viktor Frankl calls this feeling the â€œexistential vacuumâ€ in his famous book Manâ€™s Search for Meaning. Frankl was a psychologist who survived the Holocaust, and in this book he explains that the inmates who survived with him found and focused on a higher purpose in life, like caring for other inmates and promising to stay alive to reconnect with loved ones outside the camps. But these survivors also struggled in their new lives after the war, desperately searching for meaning when every decision was no longer life or death.&lt;/p&gt;
    &lt;p&gt;Frankl realized that this existential anxiety is not a nuisance to eliminate, but actually an important signal pointing us towards our need for meaning. Similarly, while Friedrich Nietzsche would argue that life inherently lacks meaning, heâ€™d also implore us to zoom out and find our highest purpose now:&lt;/p&gt;
    &lt;p&gt;â€œThis is the most effective way: to let the youthful soul look back on life with the question, â€˜What have you up to now truly loved, what has drawn your soul upward, mastered it and blessed it too?â€™â€¦ for your true being lies not deeply hidden within you, but an infinite height above you, or at least above that which you commonly take to be yourself.â€œ&lt;/p&gt;
    &lt;p&gt;â€” Friedrich Nietzsche, Untimely Meditations, 1874&lt;/p&gt;
    &lt;p&gt;Nihilists get both Nietzsche and YOLO wrong. Neither mean that you give up. Instead, both mean that your efforts are everything.&lt;/p&gt;
    &lt;p&gt;So when you get those Sunday Scaries, the existential anxiety that your time is ending and the rest of your life is spent working for someone else, the answer isnâ€™t escapism.&lt;/p&gt;
    &lt;p&gt;Instead, visualize your ideal self, the truest childhood dream of who you wanted to be when you grew up. What would that person be doing now? Go do that thing!&lt;/p&gt;
    &lt;p&gt;When facing the existential vacuum, thereâ€™s only one way out â€” up, towards your highest purpose.&lt;/p&gt;
    &lt;p&gt;On a 0-10 scale, how happy did you feel when you started working this Monday?&lt;/p&gt;
    &lt;p&gt;Why wasnâ€™t your answer a 10?&lt;/p&gt;
    &lt;p&gt;You got the great job. You built the startup. You took the vacations. But thatâ€™s not what you really needed. You kept coming back Monday after Monday realizing you were doing the same job again.&lt;/p&gt;
    &lt;p&gt;So you tried to improve yourself. You optimized your morning routine. You perfected your productivity system. You bought a sleep mask and mouth tape. Yet youâ€™re still dragging yourself out of bed each Monday morning tired and unmotivated.&lt;/p&gt;
    &lt;p&gt;Weâ€™re optimizing for less suffering instead of more meaning. Weâ€™ve confused comfort with fulfillment. And weâ€™re getting really, really good at it. Millennials are the first generation in history to expect our jobs to provide a higher meaning beyond survival. Thatâ€™s a good thing. It means that the essentials of life are nearly universally available now.&lt;/p&gt;
    &lt;p&gt;But, as I write in my book Positive Politics:&lt;/p&gt;
    &lt;p&gt;â€œThe last two hundred years of progress pulled most of the worldâ€™s population over the poverty line. The next hundred years is about lifting everyone above the abundance lineâ€¦ Positive Politics seeks to democratize this abundance.â€œ&lt;/p&gt;
    &lt;p&gt;Those of us who have already achieved abundance in our own lives now have two responsibilities:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Spread that abundance to as many other people as possible.&lt;/item&gt;
      &lt;item&gt;Find something more meaningful to do than chase more stuff.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;â€œThe existential vacuum is a widespread phenomenon of the twentieth centuryâ€œ&lt;/p&gt;
    &lt;p&gt;â€• Viktor Frankl, Manâ€™s Search for Meaning&lt;/p&gt;
    &lt;p&gt;When I was a kid, I knew exactly what I wanted to do â€” the most important job in the world. And I wasnâ€™t afraid to tell you either. At five years old, I would talk your ear off about training to be goalie for the St. Louis Blues. By seven, it was astronaut for NASA. By eleven, it was President of the United States. Then middle school hit, I got made fun of more than a few times, and that voice went silent.&lt;/p&gt;
    &lt;p&gt;After three startups, three nonprofits, and especially three kids knocked the imposter syndrome out of me, I spent a lot of time training my inner voice to get loud again. And what I heard reinforced what I knew all along â€” that my highest purpose is way above where I commonly take myself now.&lt;/p&gt;
    &lt;p&gt;Imposter syndrome can be a good thing. That external voice saying â€œthis is not youâ€ may actually be telling you the truth. I got into the testing lab industry to save our family business. Fifteen years and three startups later, I had become â€œthe lab expertâ€ to the world. But I cringed at that label. First, there was no room to grow. I had already done it. I didnâ€™t want to be eighty and still running labs. Second, and most importantly, I knew that my skills could be used for much more than money.&lt;/p&gt;
    &lt;p&gt;Iâ€™d love to say I transformed overnight, but really it took 5+ years from 2020 to 2025 for me to fully embody my new identity. You can see it in my writing, which became much more ambitious in 2020, when I relaunched this site and started blogging consistently. That led to my Worldâ€™s Biggest Problems project, which convinced me that Positive Politics is the #1 solution we need now!&lt;/p&gt;
    &lt;p&gt;There are two key components to my highest mission now:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Help people find their highest purpose.&lt;/item&gt;
      &lt;item&gt;Be a model for the pursuit of greatness.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That means consistently chasing my highest purpose â€” helping ambitious optimists get into politics! After nearly a decade of doing this behind the scenes as a political volunteer and advisor, 2025 was the first year where I went full-time in politics. Leading MCFN and publishing Positive Politics at the same time was a ton of work. But nothing energizes me more than fighting two of the biggest battles in the world now â€” anticorruption and Positive Politics!&lt;/p&gt;
    &lt;p&gt;I love politics because itâ€™s full of meta solutions â€” solutions that create more solutions. My Positive Politics Accelerator is a classic example â€” recruiting and training more ambitious optimists into politics will lead to them making positive political change at all levels of government. But Iâ€™ve also tackled challenges like independent testing with startups and led a nonprofit to drive investigative journalism.&lt;/p&gt;
    &lt;p&gt;There are so many paths to positive impact, including politics, startups, nonprofits, medicine, law, education, science, engineering, journalism, art, faith, parenting, mentorship, and more! Choose the path that both best fits you now and is pointed towards your long-term highest purpose.&lt;/p&gt;
    &lt;p&gt;I woke up today so excited to get to work thinking it was Monday morning already. Instead of jumping right into it, I spent all morning making breakfast and playing with my kids, then wrote this post. When Iâ€™m writing about something personal, 1,000+ words can easily flow for me in an afternoon. This part will be done just in time to go to a nerf battle birthday party with my boys and their friends.&lt;/p&gt;
    &lt;p&gt;Both the hustle and anti-hustle cultures get it wrong. Working long hours isnâ€™t inherently good or bad. If I really had to count how much Iâ€™m â€œonâ€ vs. doing whatever I want, itâ€™s easy 100+ hours per week. But that includes everything from investigative journalism and operations work for MCFN, social media and speaking events for Positive Politics, reading and writing for my site, and 40+ hours every week with my kids.&lt;/p&gt;
    &lt;p&gt;I want to help more ambitious optimists chase your highest potential! Whether the best solution is in startups, politics, nonprofits, science, crypto, or some new technology thatâ€™s yet to be invented, Iâ€™m happy to point you where I think youâ€™ll be most powerful. Iâ€™ve thought, written, and worked on many of these ideas in my 15+ year career.&lt;/p&gt;
    &lt;p&gt;Now with 10+ years of writing, Iâ€™ve focused on publicly inspiring more people to take on these challenges too. We should be flexible on how we solve the problems but firm in our resolve to consistently organize people and launch solutions.&lt;/p&gt;
    &lt;p&gt;As Steve Jobs said, â€œLife can be much broader once you discover one simple fact, and that is everything around you that you call â€˜lifeâ€™ was made up by people that were no smarter than youâ€¦ You can change it, you can mold itâ€¦ the most important thingâ€¦is to shake off this erroneous notion that life is there and youâ€™re just going to live in it, versus embrace it, change it, improve it, make your mark upon itâ€¦ Once you learn that, youâ€™ll never be the same again.â€&lt;/p&gt;
    &lt;p&gt;Remember how it felt as a young child to openly tell the world about your dream job? Find the work that makes you feel this way and jump on whatever rung of that career ladder you can start now. The pay may be a little lower, but the existential payoff will be exponentially higher for the rest of your life.&lt;/p&gt;
    &lt;p&gt;You donâ€™t have to go all-in right away! In fact, after a long diet of low existential work, itâ€™s probably best to ease into public work. You can even volunteer one hour or less per week for a political campaign or nonprofit to get started. Pick the smallest first step, and do it. Not in January, now. Do it before the end of the year. And see how different you feel when 2026 starts!&lt;/p&gt;
    &lt;p&gt;And you donâ€™t have to choose politics like me! Do you have the next great ambitious optimistic science fiction novel in your head? That book could spark movies and movements that positively change millions of lives! Choose the path will inspire and energize you for decades!&lt;/p&gt;
    &lt;p&gt;What matters most is you go straight towards your highest potential right now. Pause once a month to make sure youâ€™re still on the right track. Stop once a year to triple-check youâ€™re on the right track. But never get off this path towards your highest potential. Anything else will starve you existentially.&lt;/p&gt;
    &lt;p&gt;When you truly chase your highest potential, everything you thought was burnout will melt away. Because you werenâ€™t suffering from too much work, you were suffering from too little truly important work. Like a boy who thought he was full until dessert arrives, youâ€™ll suddenly find your hunger return!&lt;/p&gt;
    &lt;p&gt;If youâ€™re sick of politics as usual and ready to change the system, join Positive Politics!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Buy the book: positivepoliticsbook.com&lt;/item&gt;
      &lt;item&gt;Join the accelerator: positivepolitics.org/apply&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46346958</guid><pubDate>Sun, 21 Dec 2025 18:28:56 +0000</pubDate></item><item><title>The Going Dark initiative or ProtectEU is a Chat Control 3.0 attempt</title><link>https://mastodon.online/@mullvadnet/115742530333573065</link><description>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46347080</guid><pubDate>Sun, 21 Dec 2025 18:39:46 +0000</pubDate></item><item><title>I can't upgrade to Windows 11, now leave me alone</title><link>https://idiallo.com/byte-size/cant-update-to-windows-11-leave-me-alone</link><description>&lt;doc fingerprint="149e0877eae63cf4"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Microsoft won't let you dismiss the upgrade notification&lt;/head&gt;
    &lt;p&gt;So support for Windows 10 has ended. Yes, millions of users are still on it. One of my main laptops runs Windows 10. I can't update to Windows 11 because of the hardware requirements. It's not that I don't have enough RAM, storage, or CPU power. The hardware limitation is specifically TPM 2.0.&lt;/p&gt;
    &lt;p&gt;What is TPM 2.0, you say? It stands for Trusted Platform Module. It's basically a security chip on the motherboard that enables some security features. It's good and all, but Windows says my laptop doesn't support it. Great! Now leave me alone.&lt;/p&gt;
    &lt;p&gt;Well, every time I turn on my computer, I get a reminder that I need to update to Windows 11. OK, at this point a Windows machine only belongs to you in name. Microsoft can run arbitrary code on it. They already ran the code to decide that my computer doesn't support Windows 11. So why do they keep bothering me?&lt;/p&gt;
    &lt;p&gt;Fine, I'm frustrated. That's why I'm complaining. I've accepted the fact that my powerful, yet 10-year-old laptop won't get the latest update. But if Microsoft's own systems have determined my hardware is incompatible, why are they harassing? I'll just have to dismiss this notification and call it a day.&lt;/p&gt;
    &lt;p&gt;But wait a minute. How do I dismiss it?&lt;/p&gt;
    &lt;p&gt;I cannot dismiss it. I can only be reminded later or... I have to learn more. If I click "remind me later," I'm basically telling Microsoft that I consent to being shown the same message again whenever they feel like it. If I click "learn more"? I'm taken to the Windows Store, where I'm shown ads for different laptops I can buy instead. Apparently, I'm also probably giving them consent to show me this ad the next time I log in.&lt;/p&gt;
    &lt;p&gt;It's one thing to be at the forefront of enshittification, but Microsoft is now actively hostile to its users. I've written about this passive-aggressive illusion of choice before. They are basically asking "Do you want to buy a new laptop?" And the options they are presenting are "Yes" and "OK."&lt;/p&gt;
    &lt;p&gt;This isn't a bug. This is intentional design. Microsoft has deliberately removed the ability to decline.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dear Microsoft&lt;/head&gt;
    &lt;p&gt;Listen. You said my device doesn't support Windows 11. You're right. Now leave me alone. I have another device running Windows 11. It's festered with ads, and you're trying everything in your power to get me to create a Microsoft account.&lt;/p&gt;
    &lt;p&gt;I paid for that computer. I also paid for a pro version of the OS. I don't want OneDrive. I don't want to sign up with my Microsoft account. Whether I use my computer online or offline is none of your business. In fact, if you want me to create an account on your servers, you are first required to register your OS on my own website. The terms and conditions are simple. Every time you perform any network access, you have to send a copy of the payload and response back to my server. Either that, or you're in breach of my terms.&lt;/p&gt;
    &lt;p&gt;Notes:&lt;/p&gt;
    &lt;p&gt;By the way, the application showing this notification is called Reusable UX Interaction Manager sometimes. Other times it appears as Campaign Manager.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46347108</guid><pubDate>Sun, 21 Dec 2025 18:43:20 +0000</pubDate></item><item><title>A guide to local coding models</title><link>https://www.aiforswes.com/p/you-dont-need-to-spend-100mo-on-claude</link><description>&lt;doc fingerprint="2f539dd2343f89db"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;[Revised] You Donâ€™t Need to Spend $100/mo on Claude Code: Your Guide to Local Coding Models&lt;/head&gt;
    &lt;head rend="h3"&gt;What you need to know about local model tooling and the steps for setting one up yourself&lt;/head&gt;
    &lt;p&gt;[Edit 1] This article has been edited after initial release for clarity. Both the tl;dr and the end section have added information.&lt;/p&gt;
    &lt;p&gt;[Edit 2] This hypothesis was actually wrong and thank you to everyone who commented!&lt;/p&gt;
    &lt;p&gt;Hereâ€™s a full explanation of where I went wrong. I want to address this mistake as I realize it might have a meaningful impact on someone's financial position.&lt;/p&gt;
    &lt;p&gt;Iâ€™m not editing the actual article except where absolutely necessary so it doesnâ€™t look like Iâ€™m covering up the mistakeâ€”I want to address it. Instead, Iâ€™ve included the important information below.&lt;/p&gt;
    &lt;p&gt;There is one takeaway this article provides that definitely holds true:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Local models are far more capable than theyâ€™re given credit for, even for coding.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It also explains the process of setting up a local coding model and technical information about doing so which is helpful for anyone wanting to set up a local coding model. I would still recommend doing so.&lt;/p&gt;
    &lt;p&gt;But do I want someone reading this to immediately drop their coding subscription and buy a maxed out MacBook Pro? No, and for that reason I need to correct my hypothesis from â€˜Yes, with caveatsâ€™ to â€˜Noâ€™.&lt;/p&gt;
    &lt;p&gt;This article was not an empirical assessment, but should have been to make these claims. Hereâ€™s where I went wrong:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;While local models can likely complete ~90% of the software development tasks that something like Claude Code can, the last 10% is the most important. When it comes to your job, that last 10% is worth paying more for to get that last bit of performance.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I realized I looked at this more from the angle of a hobbiest paying for these coding tools. Someone doing little side projectsâ€”not someone in a production setting. I did this because I see a lot of people signing up for $100/mo or $200/mo coding subscriptions for personal projects when they likely donâ€™t need to. I would not recommend running local models as a company instead of giving employees access to a tool like Claude Code.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;While larger local models are very capable, as soon as you run other development tools (Docker, etc.) that also eat into your RAM, your model needs to be much smaller and becomes a lot less capable. I didnâ€™t factor this in in my experiment.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So, really, the takeaway should be that these are incredible supplemental models to frontier models when coding and could potentially save you on your subscription by dropping it down a tier, but practically theyâ€™re not worth the effort in situations that might affect your livelihood.&lt;/p&gt;
    &lt;p&gt;Exactly a month ago, I made a hypothesis: Instead of paying $100/mo+ for an AI coding subscription, my money would be better spent upgrading my hardware so I can run local coding models at a fraction of the price (and have better hardware too!).&lt;/p&gt;
    &lt;p&gt;So, to create by far the most expensive article Iâ€™ve ever written, I put my money where my mouth is and bought a MacBook Pro with 128 GB of RAM to get to work. My idea was simple: Over the life of the MacBook Iâ€™d recoup the costs of it by not paying for an AI coding subscription.&lt;/p&gt;
    &lt;p&gt;After weeks of experimenting and setting up local AI models and coding tools, Iâ€™ve come to the conclusion that my hypothesis was &lt;del&gt;correct, with nuance&lt;/del&gt;, not correct [see edit 2 above] which Iâ€™ll get into later in this article.&lt;/p&gt;
    &lt;p&gt;In this article, we cover:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Why local models matter and the benefits they provide.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;How to view memory usage and make estimates for which models can run on your machine and the RAM demands for coding applications.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Walk through setting up your own local coding model and tool step-by-step.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Donâ€™t worry if you donâ€™t have a high-RAM machine! You can still follow this guide. Iâ€™ve included some models to try out with a lower memory allotment. I think youâ€™ll be surprised at how performant even the smallest of models is. In fact, there hasnâ€™t really been a time during this experiment that Iâ€™ve been disappointed with model performance.&lt;/p&gt;
    &lt;p&gt;If youâ€™re only here for the local coding tool setup, skip to the section at the bottom. Iâ€™ve even included a link to my modelfiles in that section to make setup even easier for you. Otherwise, letâ€™s get into what you need to know.&lt;/p&gt;
    &lt;head rend="h2"&gt;tl;dr:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Local coding models are very capable. Using the right model and the right tooling feels only half a generation behind the frontier cloud tools. I would say that for about 90% of developer work local models are more than sufficient. Even small 7B parameter models can be very capable. [Edited to add in this next part] Local models wonâ€™t compete with frontier models at the peak of performance, but can complete many coding tasks just as well for a fraction of the cost. Theyâ€™re worth running to bring costs down on plenty of tasks but potentially not worth using if thereâ€™s a free tier available that performs better.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tools matter a lot. This is where I experienced the most disappointment. I tried many different tools with many different models and spent a lot of time tinkering. I ran into situations where the models wouldnâ€™t call tools properly or their thinking traces wouldnâ€™t close. Both of these rendered the tool essentially useless. Currently, tooling seems very finicky and if thereâ€™s anything developers need to be successful, itâ€™s good tools.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Thereâ€™s a lot to consider when youâ€™re actually working within hardware constraints. We take the tooling set up for us in the cloud for granted. When setting up local models, I had to think a lot about trade-offs in performance versus memory usage, how different tools compared and affected performance, nuances in types of models, how to quantize, and other user-facing factors such as time-to-first-token and tokens per second.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Google threw a wrench into my hypothesis. The local setup is almost a no-brainer when compared to a $100/mo+ subscription. Compared to free or nearly-free tooling (such as Gemini CLI, Jules, or Antigravity) there isnâ€™t quite as strong of a monetary justification to spend more on hardware. There are benefits to local models outside of code, though, and I discuss those below.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If the tl;dr was helpful, donâ€™t forget to subscribe to get more in your inbox.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why local models?&lt;/head&gt;
    &lt;p&gt;You might wonder why local models are worth investing in at all. The obvious answer is cost. By using your own hardware, you donâ€™t need to pay a subscription fee to a cloud provider for your tool. There are also a few less obvious and underrated reasons that make local models useful.&lt;/p&gt;
    &lt;p&gt;First: Reliability. Each week there seems to be complaints about performance regression within AI coding tools. Many speculate companies are pulling tricks to save resources that hurt model performance. With cloud providers, youâ€™re at the mercy of the provider for when this happens. With local models, this only happens when you cause it to.&lt;/p&gt;
    &lt;p&gt;Second: Local models can apply to far more applications. Just the other day I was having a discussion with my dad about AI tooling he could use to streamline his work. His job requires studying a lot of dataâ€”a perfect application for an LLM-based toolâ€”but his company blocks tools like Gemini and ChatGPT because a lot of this analysis is done on intellectual property. Unfortunately, he isnâ€™t provided a suitable alternative to use.&lt;/p&gt;
    &lt;p&gt;With a local model, he wouldnâ€™t have to worry about these IP issues. He could run his analyses without data ever leaving his machine. Of course, any tool calling would also need to ensure data never leaves the machine, but local models get around one of the largest hurdles for useful enterprise AI adoption. Running models on a local machine opens up an entire world of privacy- and security-centric AI applications that are expensive for cloud providers to provide.&lt;/p&gt;
    &lt;p&gt;Finally: Availability. Local models are available to you as long as your machine is. This means no worrying about your provider being down or rate limiting you due to high traffic. It also means using AI coding tools on planes or in other situations where internet access is locked down (think highly secure networks).&lt;/p&gt;
    &lt;p&gt;While local models do provide significant cost savings, the flexibility and reliability they provide can be even more valuable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Understanding memory&lt;/head&gt;
    &lt;p&gt;To get going with local models you must understand the memory needed to run them on your machine. Obviously, if you have more memory youâ€™ll be able to run better models, but understanding the nuances of that memory management will help you pick out the right model for your use case.&lt;/p&gt;
    &lt;p&gt;Local AI has two parts that eat up your memory: The model itself and the modelâ€™s context window.&lt;/p&gt;
    &lt;p&gt;The actual model has billions of parameters and all those parameters need to fit into your memory at once. Excellent local coding models start at around 30 billion (30B, for short) parameters in size. By default, these models use 16 bits to represent parameters. At 16 bits with 30B parameters, a model will take 60 GB of space in RAM (16 bits = 2 bytes per parameter, 30 billion parameters = 60 billion bytes which equals about 60 GB).&lt;/p&gt;
    &lt;p&gt;The second (and potentially larger) memory consuming part of local AI is the modelâ€™s context window. This is the model inputs and outputs that are stored so the model can reference them in future requests. This gives the model memory.&lt;/p&gt;
    &lt;p&gt;When coding with AI, we prefer this window to be as large as it can because we need to fit our codebase (or pieces of it) within our context window. This means we target a context window of 64,000 tokens or larger. All of these tokens will also be stored in RAM.&lt;/p&gt;
    &lt;p&gt;The important thing to understand about context windows is that the memory requirement per-token for a model depends on the size of that model. Models with more parameters tend to have large architectures (more hidden layers and larger dimensions to those layers). Larger architectures mean the model must store more information for each token within its key-value cache (context window) because it stores information for each token for each layer.&lt;/p&gt;
    &lt;p&gt;This means choosing an 80B parameter model over a 30B parameter model requires more memory for the model itself and also more memory for the same size context window. For example, a 30B parameter model might have a hidden dimension of 5120 with 64 layers while an 80B model has a hidden dimension of 8192 with 80 layers. Doing some back-of-the-napkin math shows us that the larger model requires approximately 2x more RAM to maintain the same context window as the 30B parameter model (see formula below).&lt;/p&gt;
    &lt;p&gt;Luckily, there are tricks to better manage memory. First, there are architectural changes that can be made to make model inference more efficient so it requires less memory. The model we set up at the end of this article uses Hybrid Attention which enables a much smaller KV cache enabling us to fit our model and context window in less memory. I wonâ€™t get into more detail in this article, but you can read more about that model and how it works here.&lt;/p&gt;
    &lt;p&gt;The second trick is quantizing the values youâ€™re working with. Quantization means converting a continuous set of values into a smaller amount of distinct values. In our case, that means taking a set of numbers represented by a certain number of bits (16, for example) and reducing it to a set of numbers represented by fewer bits (8, for example). To put it simply, in our case weâ€™re converting the numbers representing our model to a smaller bit representation to save memory while keeping the value representations within the model relatively equal.&lt;/p&gt;
    &lt;p&gt;You can quantize both your model weights and the values stored in your context window. When you quantize your model weights, you â€œremove intelligenceâ€ from the model because itâ€™s less precise in its representation of innate information. Iâ€™ve also found the performance hit when going from 16 to 8 bits within the model to be much less than 8 to 4.&lt;/p&gt;
    &lt;p&gt;We can also quantize the values in our context window to reduce its memory requirement. This means weâ€™re less precisely representing the modelâ€™s memory. Generally speaking, KV cache (context window) quantization is considered more destructive to model performance than weight quantization because it causes the model to forget details in long reasoning traces. Thus, you should test quantizing the KV cache to ensure it doesnâ€™t degrade model performance for your specific task.&lt;/p&gt;
    &lt;p&gt;In reality, like the rest of machine learning, optimizing local model performance is an experimentation process and real-world machine learning requires understanding the practical limitations and capabilities of models when applied to specific applications.&lt;/p&gt;
    &lt;p&gt;Here are a few more factors to understand when setting up a local coding model on your hardware:&lt;/p&gt;
    &lt;head rend="h3"&gt;Instruct versus non-instruct&lt;/head&gt;
    &lt;p&gt;Instruct models are post-trained to be well-suited for chat-based interactions. Theyâ€™re given chat pairings in their training to be optimized for excellent back-and-forth chat output. Non-instruct models are still trained LLMs, but focus on next-token prediction instead of chatting with a user. For our case, when using a chat-based coding tool (CLI or chat agent in your IDE) we need to use an instruct model. If youâ€™re setting up an autocomplete model, youâ€™ll want to find a model specifically post-trained for it (such as Qwen2.5-Coder-Base or DeepSeek-Coder-V2).&lt;/p&gt;
    &lt;head rend="h3"&gt;Serving tools&lt;/head&gt;
    &lt;p&gt;You need a tool to serve your local LLM for your coding tool to send it requests. On a MacBook, there are two primary options: MLX and Ollama.&lt;/p&gt;
    &lt;p&gt;Ollama is the industry standard and works on non-Mac hardware. Itâ€™s a great serving setup on top of llama.cpp that makes model serving almost plug-and-play. Users can download model weights from Ollama easily and can configure modelfiles with custom parameters for serving. Ollama can also serve a model once and make it available to multiple tools.&lt;/p&gt;
    &lt;p&gt;MLX is a Mac-specific framework for machine learning that is optimized specifically for Mac hardware. It also retrieves models for the user from a community collection. Iâ€™ve found Ollama to be very reliable in its model catalog, while MLXâ€™s catalog is community sourced and can sometimes be missing specific models. Models are sourced from the community so a user can convert a model to MLX format themselves. MLX requires a bit more setup on the userâ€™s end, but serves models faster because it doesnâ€™t have a layer providing the niceties of Ollama on top of it.&lt;/p&gt;
    &lt;p&gt;Either of these is great, but I chose MLX to maximize what I can get with my RAM, but Ollama is probably the more beginner-friendly tool here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Time-to-first-token and tokens per second&lt;/head&gt;
    &lt;p&gt;In real-world LLM applications itâ€™s important that the model is able to serve its first token for a request in a reasonable amount of time and continue serving tokens at a speed that enables the user to use the model for its given purpose. If we have a high-performance model running locally, but it only serves a few tokens per second, it wouldnâ€™t be useful for coding.&lt;/p&gt;
    &lt;p&gt;This is something taken for granted with cloud-hosted models that is a real consideration when working locally on constrained hardware. Another reason I chose MLX as my serving platform is because it served tokens up to 20% faster than Ollama. In reality, Ollama served tokens fast enough so I donâ€™t think using MLX is necessary specifically for this reason for the models I tried.&lt;/p&gt;
    &lt;head rend="h3"&gt;Performance trade-offs&lt;/head&gt;
    &lt;p&gt;There are many ways to optimize local models and save RAM. Itâ€™s difficult to know which optimization method works best and the impact each has on a model especially when using them in tandem with other methods.&lt;/p&gt;
    &lt;p&gt;The right optimization method also depends on the application. In my experience, I find it best to prioritize larger models with more aggressive model quantization over smaller models with more precise model weights. Since our application is coding, I would also prioritize a less-quantized KV cache and using a smaller model to ensure reasoning works properly while not sacrificing the size of our context window.&lt;/p&gt;
    &lt;head rend="h3"&gt;Coding tools&lt;/head&gt;
    &lt;p&gt;There are many tools to code with local models and I suggest trying until you find one you like. Some top recommendations are OpenCode, Aider, Qwen Code, Roo Code, and Continue. Make sure to use a tool compatible with OpenAIâ€™s API standard. While this should be most tools, this ensures a consistent model/tool connection. This makes it easier to switch between tools and models as needed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting set up&lt;/head&gt;
    &lt;p&gt;Iâ€™ll spare you the trial and error I experienced getting this set up. The one thing I learned is that tooling matters a lot. Not all coding tools are created equal and not all of the models interact with tools equally. I experienced many times where tool calling or even running a tool at all was broken. I also had to tinker quite a bit with many of them to get them to work.&lt;/p&gt;
    &lt;p&gt;If youâ€™re a PC enthusiast, an apt comparison to setting up local coding tools versus using the cloud offerings available is the difference between setting up a MacBook versus a Linux Laptop. With the Linux laptop, you might get well through the distro installation only to find that the drivers for your trackpad arenâ€™t yet supported. Sometimes it felt like that with local models and hooking them to coding tools.&lt;/p&gt;
    &lt;p&gt;For my tool, I ended up going with Qwen Code. It was pretty plug-and-play as itâ€™s a fork of Gemini CLI. It supports the OpenAI compatibility standard so I can easily sub in different models and affords me all of the niceties built into Gemini CLI that Iâ€™m familiar with using. I also know itâ€™ll be supported because both the Qwen team and Google DeepMind are behind the tool. The tool is also open source so anyone can support it as needed.&lt;/p&gt;
    &lt;p&gt;For models, I focused on GPT-OSS and Qwen3 models since they were around the size I was looking for and had great reviews for coding. I ended up deciding to use Qwen3-Coder models because I found it performed best and because GPT-OSS frequently gave me â€œI cannot fulfill this requestâ€ responses when I asked it to build features.&lt;/p&gt;
    &lt;p&gt;I decided to serve my local models on MLX, but if youâ€™re using a non-Mac device give Ollama a shot. A MacBook is an excellent machine for serving local models because of its unified memory architecture. This means the RAM can be allotted to the CPU or GPU as needed. MacBooks can also be configured with a ton of RAM. For serving local coding models, more is always better.&lt;/p&gt;
    &lt;p&gt;Iâ€™ve shared my modelfiles repo for you to reference and use as needed. Iâ€™ve got a script set up that automates much of the below process. Feel free to fork it and create your own modelfiles or star it to come back later.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Install MLX or download Ollama (the rest of this guide will continue with MLX but details for serving on Ollama can be found here).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Increase the VRAM limitation on your MacBook. macOS will automatically limit VRAM to 75% of the total RAM. We want to use more than that. Run sudo sysctl iogpu.wired_limit_mb=110000 in your terminal to set this up (adjust the mb setting according to the RAM on your MacBook). This needs to be set each time you restart your MacBook.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Run pip install -U mlx-lm to install MLX for serving community models.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Serve the model as an OpenAI compatible API using python -m mlx_lm.server --model mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit. This command both runs the server and downloads the model for you if you havenâ€™t yet. This particular model is what Iâ€™m using with 128GB of RAM. If you have less RAM, check out smaller models such as mlx-community/Qwen3-4B-Instruct-2507-4bit (8 GB RAM), mlx-community/Qwen2.5-14B-Instruct-4bit (16 GB RAM), mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit (32 GB RAM), or mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit (64-96 GB RAM).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Download Qwen Code. You might need to install Node Package Manager for this. I recommend using Node Version Manager (nvm) for managing your npm version.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Set up your tool to access an OpenAI compatible API by entering the following settings:&lt;/p&gt;
        &lt;list rend="ol"&gt;
          &lt;item&gt;
            &lt;p&gt;Base URL: http://localhost:8080/v1 (should be the default MLX serves your model at)&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;API Key: mlx&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Model Name: mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit (or whichever model you chose).&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Voila! Your coding model tool should be working with your local coding model.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I recommend opening Activity Monitor on your Mac to monitor memory usage. Iâ€™ve had cases where I thought a model should fit within my memory allotment but it didnâ€™t and I ended up using a lot of swap memory. When this happens your model will run very slowly.&lt;/p&gt;
    &lt;p&gt;One tip I have for using local coding models: Focus on managing your context. This is a great skill even with cloud-based models. People tend to YOLO their chats and fill their context window, but Iâ€™ve found greater performance by ensuring that just what my model needs is sitting in my context window. This is even more important with local models that may need an extra boost in performance and are limited in their context.&lt;/p&gt;
    &lt;head rend="h2"&gt;Was my hypothesis correct?&lt;/head&gt;
    &lt;p&gt;My original hypothesis was: Instead of paying $100/mo+ for an AI coding subscription, my money would be better spent upgrading my hardware so I can run local coding models at a fraction of the price.&lt;/p&gt;
    &lt;p&gt;I would argue that&lt;del&gt;â€”yes!â€”&lt;/del&gt;no [see edit 2 above], it is correct. If we crunch the numbers, a MacBook with 128 GB is $4700 plus tax. If I spend $100/mo for 5 years, a coding subscription would cost $6000 in that same amount of time. Not only do I save money, but I also get a much more capable machine for anything else I want to do with it.&lt;/p&gt;
    &lt;p&gt;[This paragraph was added in after initial release of this article] Itâ€™s important to note that local models will not reach the peak performance of frontier models; however, they will likely be able to do most tasks just as well. The value of using a local model doesnâ€™t come from raw performance, but from supplementing the cost of higher performance models. A local model could very well let you drop your subscription tier for a frontier coding tool or utilize a free tier as needed for better performance and run the rest of your tasks for free.&lt;/p&gt;
    &lt;p&gt;Itâ€™s also important to note that local models are only going to get better and smaller. This is the worst your local coding model will perform. I also wouldnâ€™t be surprised if cloud-based AI coding tools get more expensive. If you figure youâ€™re using greater than the $100/mo tier right now or that the $100/mo tier will cost $200/mo in the future, the purchase is a no-brainer. Itâ€™s just difficult to stomach the upfront cost.&lt;/p&gt;
    &lt;p&gt;From a performance standpoint, I would say the maximum model running on my 128 GB RAM MacBook right now feels about half a generation behind the frontier coding tools. Thatâ€™s excellent, but something to keep in mind as that half a generation might matter to you.&lt;/p&gt;
    &lt;p&gt;One wrench thrown into my experiment is how much free quota Google hands out with their different AI coding tools. Itâ€™s easy to purchase expensive hardware when it saves you money in the long run. Itâ€™s much more difficult when the alternative is free.&lt;/p&gt;
    &lt;p&gt;Initially, I considered my local coding setup to be a great pair to Googleâ€™s free tier. It definitely performs better than Gemini 2.5 Flash and makes a great companion to Gemini 3 Pro. Gemini 3 Pro can solve more complex tasks with the local model doing everything else. This not only saves quota on 3 Pro but also provides a very capable fallback for when quota is hit.&lt;/p&gt;
    &lt;p&gt;However, this is foiled a bit now that Gemini 3 Flash was just announced a few days ago. It shows benchmark numbers much more capable than Gemini 2.5 Flash (and even 2.5 Pro!) and Iâ€™ve been very impressed with its performance. If thatâ€™s the free tier Google offers, it makes local coding models less fiscally reasonable. The jury is still out on how well Gemini 3 Flash will perform and how quota will be structured, but weâ€™ll have to see if local models can keep up.&lt;/p&gt;
    &lt;p&gt;Iâ€™m very curious to hear what you think! Tell me about your local coding setup or ask any questions below.&lt;/p&gt;
    &lt;p&gt;Thanks for reading!&lt;/p&gt;
    &lt;p&gt;Always be (machine) learning,&lt;/p&gt;
    &lt;p&gt;Logan&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46348329</guid><pubDate>Sun, 21 Dec 2025 20:55:15 +0000</pubDate></item><item><title>Show HN: Rust/WASM lighting data toolkit â€“ parses legacy formats, generates SVGs</title><link>https://eulumdat.icu</link><description>&lt;doc fingerprint="714bd42804c754b4"&gt;
  &lt;main&gt;
    &lt;p&gt;This application requires JavaScript to run.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46348344</guid><pubDate>Sun, 21 Dec 2025 20:56:54 +0000</pubDate></item><item><title>Disney Imagineering Debuts Next-Generation Robotic Character, Olaf</title><link>https://disneyparksblog.com/disney-experiences/robotic-olaf-marks-new-era-of-disney-innovation/</link><description>&lt;doc fingerprint="ebf30b7df696007b"&gt;
  &lt;main&gt;
    &lt;p&gt;Disneyland Paris saw a groundbreaking moment today, where Bruce Vaughn, President and Chief Creative Officer of Walt Disney Imagineering, and Natacha Rafalski, PrÃ©sidente of Disneyland Paris, introduced a next-generation robotic character representing Olaf, the beloved snowman from Walt Disney Animation Studiosâ€™ Frozen.&lt;/p&gt;
    &lt;p&gt;This debut marks a new chapter in Disney character innovation, one where technology, storytelling, and collaboration come together to bring screen to reality.&lt;/p&gt;
    &lt;head rend="h2"&gt;Innovation at the Core: From Screen to Reality&lt;/head&gt;
    &lt;p&gt;From the way he moves to the way he looks, every gesture and detail is crafted to reflect the Olaf audiences have seen in the film â€” alive, curious, and unmistakably himself. As for his snow-like shimmer that catches the light just like fresh snow, this was enhanced by iridescent fibers. These details make Olaf one of the most expressive and true-to-life characters built, and heâ€™s soon making his debut at Disney parks.&lt;/p&gt;
    &lt;p&gt;Our roots are in animation with Walt Disney pioneering early hand-drawn films and today, Walt Disney Animation Studios and Pixar Animation Studios continue that tradition. We collaborated closely with the filmâ€™s original animators at Walt Disney Animation Studios to ensure every gesture felt true to the character. This isnâ€™t just about replicating the animation, itâ€™s about emulating the creatorsâ€™ intent.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Technology Behind the Magic&lt;/head&gt;
    &lt;p&gt;Home to some of the best storytellers in the world, weâ€™re continuously pushing the boundaries of innovation and technology â€” in fact it is in our DNA.&lt;/p&gt;
    &lt;p&gt;Like everything at Disney, we always start with the story, and our number one priority is to build storytelling technology that empowers our Disney Imagineers to breathe life into our characters.&lt;/p&gt;
    &lt;p&gt;While the BDX droids â€” the Star Wars free roaming robotic characters that mimic movements in a simulation â€” have been interacting with guests for a while now, Olaf presents a far greater challenge: an animated character with non-physical movements. To make Olaf as authentic as possible, the team used a branch of artificial intelligence called reinforcement learning, pushing the limits of hardware to achieve the creative intent of the artists.&lt;/p&gt;
    &lt;p&gt;It takes humans years to master walking and even longer to perform graceful motions. Deep reinforcement learning helps him acquire these skills in a fraction of the time.&lt;/p&gt;
    &lt;p&gt;Olafâ€™s â€œsnowâ€ also moves differently than the hard shells of other robotic characters, and he can fully articulate his mouth, eyes, and removable carrot nose and arms. Most importantly, Olaf can speak and engage in conversations, creating a truly one-of-a-kind experience.&lt;/p&gt;
    &lt;p&gt;Innovation takes many forms across our parks, experiences, and products â€“ all focused on improving the guest experience and bringing joy to fans around the world. And whatâ€™s most exciting is that weâ€™re just getting started!&lt;/p&gt;
    &lt;p&gt;The BDX Droids, self-balancing H.E.R.B.I.E., and now Olaf represent increasing levels of performance and innovation in bringing Disney characters to life. The speed at which we can create new characters and introduce them to guests is unprecedented. Weâ€™re scaling bigger than ever, working to bring more emotive, expressive, and surprising characters to our experiences around the world.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where Guests Can See Olaf&lt;/head&gt;
    &lt;p&gt;Olaf will soon venture out into the unknown, eager to see guests at:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Arendelle Bay Show in World of Frozen, the new immersive world coming soon to Disney Adventure World at Disneyland Paris.&lt;/item&gt;
      &lt;item&gt;Limited-time special appearances at World of Frozen at Hong Kong Disneyland Resort.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Looking for a warm hug now? You can discover how Olaf, along with other exciting breakthroughs from Walt Disney Imagineering Research &amp;amp; Development, came to life at in the latest episode of We Call It Imagineering.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46348847</guid><pubDate>Sun, 21 Dec 2025 21:46:20 +0000</pubDate></item><item><title>ONNX Runtime and CoreML May Silently Convert Your Model to FP16</title><link>https://ym2132.github.io/ONNX_MLProgram_NN_exploration</link><description>&lt;doc fingerprint="6d59dd17b5120e18"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;ONNX Runtime &amp;amp; CoreML May Silently Convert Your Model to FP16 (And How to Stop It)&lt;/head&gt;
    &lt;p&gt;Running an ONNX model in ONNX RunTime (ORT) with the CoreMLExecutionProvider may change the predictions your model makes implicitly and you may observe differences when running with PyTorch on MPS or ONNX on CPU. This is because the default arguments ORT uses when converting your model to CoreML will cast the model to FP16.&lt;/p&gt;
    &lt;p&gt;The fix is to use the following setup when creating the InferenceSession:&lt;/p&gt;
    &lt;code&gt;= ort.InferenceSession(onnx_model_path, providers=[("CoreMLExecutionProvider", {"ModelFormat": "MLProgram"})]) ort_session &lt;/code&gt;
    &lt;p&gt;This ensures your model remains in FP32 when running on a Mac GPU.&lt;/p&gt;
    &lt;head rend="h2"&gt;Uncovering an Issue in ONNX Runtime - Benchmarking the EyesOff Model&lt;/head&gt;
    &lt;p&gt;Having trained the EyesOff model, I began evaluating the model and its run time. I was looking into the ONNX format and using it to run the model efficiently. I setup a little test bench in which I ran the model using PyTorch and ONNX with ONNX Runtime (ORT), both using MPS and CPU. While checking the outputs, I noticed that the metrics from the model ran on ONNX on MPS had a different output to those on ONNX CPU and PyTorch CPU and MPS. Note, the metrics from PyTorch on CPU and MPS were the same.&lt;/p&gt;
    &lt;p&gt;When I say ORT and MPS, this is achieved through ORTâ€™s execution providers. To run an ONNX model on the Mac GPU you have to use the CoreMLExecutionProvider (more on this to come).&lt;/p&gt;
    &lt;p&gt;Now in Figure 1 and 2, observe the metric values - the PyTorch ones (Figure 1) are the same across CPU and MPS, this isnâ€™t the same story for ONNX (Figure 2):&lt;/p&gt;
    &lt;p&gt;Wow, look at the diff in Figure 2! When I saw this it was quite concerning, floating point math can lead to differences in the calculations carried out across the GPU and CPU but the values here donâ€™t appear to be a result of floating point math, the values are too large.&lt;/p&gt;
    &lt;p&gt;Given the difference in metrics, I was worried that running the model with ORT was changing the output of the model and hence the behaviour. The reason the metrics change is because some of the model predictions around the threshold flipped to the opposite side of the threshold (which is 0.5), this can be seen in the confusion matrices for the ONNX CPU run and MPS run:&lt;/p&gt;
    &lt;head rend="h4"&gt;FP32 Confusion Matrix&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Predicted Negative&lt;/cell&gt;
        &lt;cell role="head"&gt;Predicted Positive&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Actual Negative&lt;/cell&gt;
        &lt;cell&gt;207 (TN)&lt;/cell&gt;
        &lt;cell&gt;24 (FP)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Actual Positive&lt;/cell&gt;
        &lt;cell&gt;69 (FN)&lt;/cell&gt;
        &lt;cell&gt;164 (TP)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;FP16 Confusion Matrix&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Predicted Negative&lt;/cell&gt;
        &lt;cell role="head"&gt;Predicted Positive&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Actual Negative&lt;/cell&gt;
        &lt;cell&gt;206 (TN)&lt;/cell&gt;
        &lt;cell&gt;25 (FP)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Actual Positive&lt;/cell&gt;
        &lt;cell&gt;68 (FN)&lt;/cell&gt;
        &lt;cell&gt;165 (TP)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;So two predictions flipped from negative to positive.&lt;/p&gt;
    &lt;p&gt;Having said that, the first thing I did was to make my life easier, by simplifying the scenario from the large EyesOff model to a simple one layer MLP and using that to run the experiments.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why am I Using ONNX and ONNX RunTime?&lt;/head&gt;
    &lt;p&gt;Before going on itâ€™s worth discussing what ONNX and ORT are, and why Iâ€™m even using them in the first place.&lt;/p&gt;
    &lt;head rend="h3"&gt;ONNX1&lt;/head&gt;
    &lt;p&gt;ONNX stands for Open Neural Network Exchange. It can be thought of as a common programming language in which to describe ML models. Under the hood ONNX models are represented as graphs, these graphs outline the computation path of a model and it shows the operators and transformations required to get from input to prediction. These graphs are called ONNX graphs.&lt;/p&gt;
    &lt;p&gt;The use of a common language to describe models makes deployment easier and in some cases can add efficiency in terms of resource usage + or inference speed. Firstly, the ONNX graph itself can be optimised. Take PyTorch for example, you train the model in it and sure PyTorch is very mature and extremely optimised but itâ€™s such a large package some things can be overlooked or difficult to change. By converting the model to ONNX, you take advantage of the fact that ONNX was built specifically with inference in mind and with that comes optimisations which the PyTorch team could implement but have not yet.&lt;/p&gt;
    &lt;p&gt;Furthermore, ONNX models can be ran cross platform in specialised runtimes. These runtimes are specially optimised for different architectures and add another layer of efficiency gains.&lt;/p&gt;
    &lt;head rend="h3"&gt;ONNX RunTime (ORT)2&lt;/head&gt;
    &lt;p&gt;ORT is one of these runtimes. ORT actually runs the model, it can be though of as an interpreter, it takes the ONNX graph and actually implements the operators and runs them on the specified hardware. ORT has a lot of magic built into it, the operators are extremely optimised and through the use of execution providers they target a wide range of hardware. Each execution provider is optimised for the specific hardware it refers to, this enables the ORT team to implement extremely efficient operators giving us another efficiency gain.&lt;/p&gt;
    &lt;head rend="h3"&gt;CoreML3&lt;/head&gt;
    &lt;p&gt;As mentioned before, I used the CoreMLExecutionProvider to run the model on a Mac GPU. This execution provider informs ORT to make use of CoreML. CoreML is an apple developed framework which lets models (neural networks and classical ML models) run on Apple hardware, CPU, GPU and ANE. ORTâ€™s purpose in this phase is to take the ONNX graph and convert it to a CoreML model. CoreML is Appleâ€™s answer to running efficient on device models on Apple hardware.&lt;/p&gt;
    &lt;p&gt;Note, that all of this doesnâ€™t always mean the model will run faster. Some models may run faster in PyTorch, TensorRT or any other framework. This is why it is important to benchmark and test as many approaches as makes sense.&lt;/p&gt;
    &lt;head rend="h2"&gt;Finding the Source of the CPU vs MPS Difference - With an MLP&lt;/head&gt;
    &lt;p&gt;The MLP used is very simple it has a single layer, with 4 inputs, 3 outputs and the bias turned off. So, I pretty much created a fancy matrix multiplication.&lt;/p&gt;
    &lt;p&gt;To understand where the issue was coming from I ran this MLP through some different setups:&lt;/p&gt;
    &lt;code&gt;- PyTorch CPU
- PyTorch MPS
- ORT CPU
- ORT MPS
- CoreML FP32
- CoreML FP16&lt;/code&gt;
    &lt;p&gt;The goal of this exercise is to find out if 1 - the difference in outputs is seen in a simple model and 2 - to figure out where exactly the issue arises.&lt;/p&gt;
    &lt;p&gt;Before showing the full results, I want to explain why I included the CoreML FP16 and FP32 runs - specifically why the FP16 one. When I initially ran the MLP experiment I only ran PyTorch, ORT and CoreML FP32 but the output numbers of ORT MPS looked like FP16 numbers. So, I tested if they were and also if the outputs from the other runs were true FP32 numbers. You can do this with a â€œround tripâ€ test, by converting a number to FP16 and back to FP32. If after this process the number is unchanged then it is an FP16 number but if it changes then it was a true FP32. The number changes as FP16 can represent fewer floating point numbers than FP32. Itâ€™s a very simple check to carry out:&lt;/p&gt;
    &lt;code&gt;import numpy as np

= np.array([0.6480752, -0.34015813, 1.4329923], dtype=np.float32)
 onnx_cpu = np.array([0.6484375, -0.34033203, 1.4326172], dtype=np.float32)  # We cast the ort MPS numbers up to FP32, if they were FP16 this has no effect
 onnx_coreml 
= onnx_cpu.astype(np.float16).astype(np.float32)
 cpu_roundtrip = onnx_coreml.astype(np.float16).astype(np.float32)
 coreml_roundtrip 
print("ORT CPU values:")
print("  Original:", onnx_cpu)
print("  fp16 roundtrip:", cpu_roundtrip)
print("  Changed?", not np.allclose(onnx_cpu, cpu_roundtrip, atol=0))

print("\nORT CoreML values:")
print("  Original:", onnx_coreml)
print("  fp16 roundtrip:", coreml_roundtrip)
print("  Changed?", not np.allclose(onnx_coreml, coreml_roundtrip, atol=0))&lt;/code&gt;
    &lt;p&gt;The output of this is:&lt;/p&gt;
    &lt;p&gt;The CPU values change and the MPS values donâ€™t! Now itâ€™s getting interesting - perhaps when using the CoreML execution provider the output is FP16? This prompted adding the CoreML direct run in FP16 precision.&lt;/p&gt;
    &lt;p&gt;I tested this theory with an experiment. Originally, when benchmarking it was all about inference speed, now itâ€™s about floating point precision and figuring out where the diffs come from.&lt;/p&gt;
    &lt;p&gt;Running on PyTorch CPU and MPS gives a strong baseline, PyTorch is a very mature ecosystem and I used the results from that as my ground truth. It being so close together is what drove me to understand what caused ORT runs on different hardware to have a difference. Then using CoreML FP32 and FP16 aimed to show if the issue was an ONNX one or a CoreML one.&lt;/p&gt;
    &lt;p&gt;Check Figure 4 for the outputs and Figure 5 for differences in the outputs here:&lt;/p&gt;
    &lt;p&gt;Wow, would you look at that - once again PyTorch + ORT CPU match and so does PyTorch CPU + CoreML FP32. Also note that CoreML FP16 and ORT MPS match! This is a big insight into what is happening and why the metrics output differed before. Along with the round trip experiment this proves our model is being ran in FP16 when using the CoreML execution provider in ORT!&lt;/p&gt;
    &lt;p&gt;Floating points numbers are defined by three values:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sign: 1 bit to define if the number is positive or negative&lt;/item&gt;
      &lt;item&gt;Significand: Contains the numbers digits&lt;/item&gt;
      &lt;item&gt;Exponent: This says where the decimal place should be placed relative to the beginning of the significand&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Floating point numbers are often expressed in scientific notation, e.g:&lt;/p&gt;
    &lt;p&gt;FP16 and FP32 specifically, have the following specification:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Total bits&lt;/cell&gt;
        &lt;cell role="head"&gt;Significand bits&lt;/cell&gt;
        &lt;cell role="head"&gt;Exponent bits&lt;/cell&gt;
        &lt;cell role="head"&gt;Smallest number&lt;/cell&gt;
        &lt;cell role="head"&gt;Largest number&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Single precision&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;23 + 1 sign&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;\(1.2 * 10^{-38}\)&lt;/cell&gt;
        &lt;cell&gt;\(3.4 * 10^{38}\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Half precision&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;10 + 1 sign&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;\(5.96 * 10^{-8}\)&lt;/cell&gt;
        &lt;cell&gt;\(6.55 * 10^{4}\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;So as FP16 is half the size it affords a couple benefits, firstly it requires half the memory to store and secondly it can be quicker to do computations with too. However, this comes at a cost of precision, FP16 cannot represent very small numbers and the distances between small numbers as accurately as FP32.&lt;/p&gt;
    &lt;p&gt;An example of FP16 vs FP32 - The Largest Number Below 1&lt;/p&gt;
    &lt;p&gt;As you see FP32 can represent a value much closer to 1.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Link to the ONNX Issue&lt;/head&gt;
    &lt;p&gt;Having said all that, going back to the issue at hand we observe a ~\(1.17*e^{-7}\) error between PyTorch and CoreML FP32 which is typical of FP32. But, then ORT and CoreML when ran on MPS have a difference of ~\(3.7*e{-4}\) which is much more representative of FP16, this is what prompted the round trip experiment.&lt;/p&gt;
    &lt;p&gt;If you need a quick refresher on FP values, please expand the box above. If you already read that or you know enough about FP already letâ€™s look at why some predictions flip.&lt;/p&gt;
    &lt;p&gt;In my model the base threshold for a 0 or 1 class is 0.5. Both FP16 and FP32 can represent 0.5 exactly:&lt;/p&gt;
    &lt;code&gt;= np.array([0.5], dtype=np.float32)
 fp_32_05 = np.array([0.5], dtype=np.float16)
 fp_16_05 
 fp_32_05.item(), fp_16_05.item()
0.5, 0.5) (&lt;/code&gt;
    &lt;p&gt;But we know that FP representations cannot represent every single number, so there will be some values around 0.5 which cannot be represented and hence will get rounded either up or down. Letâ€™s look into that and find the threshold, this will show why some predictions of the EyesOff model were flipped when changing the model to run in FP16. Also, note by flipped we mean they go from a negative (0) prediction to a positive (1) class prediction, the rounding means itâ€™d have to be below 0.5 and then be rounded up to cross the threshold boundary. Any other scenario would keep the label the same, i.e if itâ€™s above 0.5 and gets rounded to 0.5 thatâ€™s fine as the predicted class is still the same.&lt;/p&gt;
    &lt;p&gt;The first step is to find the next representable number below 0.5:&lt;/p&gt;
    &lt;code&gt;# Show the representable values just below 0.5
= np.nextafter(np.float32(0.5), np.float32(0.0))
 fp32_below = np.nextafter(np.float16(0.5), np.float16(0.0))
 fp16_below 
= 0.5 - fp32_below
 fp32_gap = 0.5 - fp16_below
 fp16_gap 
print(f"\nClosest value BELOW 0.5:")
print(f"FP32: {fp32_below:.20f}")
print(f"FP16: {fp16_below:.20f}")

print(f"\nGap from threshold (0.5):")
print(f"FP32: {fp32_gap:.2e}")
print(f"FP16: {fp16_gap:.2e}")&lt;/code&gt;
    &lt;code&gt;Closest value BELOW 0.5:
FP32: 0.49999997019767761230
FP16: 0.49975585937500000000

Gap from threshold (0.5):
FP32: 2.98e-08
FP16: 2.44e-04&lt;/code&gt;
    &lt;p&gt;Taking this gap between 0.5 and the next reprsentable number below 0.5 in FP16 we can calculate the threshold for values which will get rounded up to 0.5:&lt;/p&gt;
    &lt;code&gt;# Given the gap is 2.44e-04, we need to divide it by 2 and calculate the midpoint between 0.499755859375 and 0.5. This midpoint determines whether the FP16 value will be rounded down if below it or up it equal to or greater than. 

# Convert to FP32 as the midpoint is not representable in FP16
= np.float32(fp16_below)
 fp_16_below_fp32 
# Calculate the gap and midpoint
= 0.5 - fp16_below
 fp16_gap = fp_16_below_fp32 + (fp16_gap / 2)
 midpoint 
print(f"  Midpoint (rounding boundary): {midpoint:.15f}")&lt;/code&gt;
    &lt;code&gt;Midpoint: 0.499877929687500&lt;/code&gt;
    &lt;p&gt;Finally letâ€™s see some examples of numbers being rounded up to 0.5 if they are above the midpoint between the representable values of FP16:&lt;/p&gt;
    &lt;code&gt;# Firstly, the midpoint itself is rounded up
0.499877929687500).item() -&amp;gt; 0.5
 np.float16(0.4999).item() -&amp;gt; 0.5
 np.float16(
# For completeness here's a number slightly smaller than the midpoint which gets rounded down
0.4998779296874).item() -&amp;gt; 0.499755859375 np.float16(&lt;/code&gt;
    &lt;p&gt;In short, any number between \([0.4998779296875, 0.5)\) will be rounded up to 0.5. This means, the predictions which were flipped were in this range.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where Does the Model Switch to FP16?&lt;/head&gt;
    &lt;p&gt;Now that we know what the issue is, itâ€™s time to find out what caused it.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Bug or Intended Behaviour - It Must be a Bug Rightâ€¦?&lt;/head&gt;
    &lt;p&gt;Originally I thought this behaviour was a bug with the ORT repo. Knowing the cast occured in the phase where ORT takes the ONNX model and converts it to a CoreML model my initial thinking was either ORT casts it to FP16 somewhere or calls CoreML with a hardcode FP16 flag or something similar.&lt;/p&gt;
    &lt;p&gt;Having little background in cpp, Claude came in useful here. I gave it the structure of the repo and it told me where I ought to place breakpoints to debug the ORT package (turns out you can debug a cpp package from python, clone the repo, build from src and then link it to your python code using the PID). However, upon running the code the breakpoints werenâ€™t being hit. I was puzzled for a bit, but then I realised why the code wasnâ€™t being hit. It turns out CoreML has two model formats â€œNeuralNetworkâ€ and â€œMLProgramâ€, I will call them NN and MLP formats respectively. The behaviour of the ORT repo changes depending on which you want, as does the behaviour of CoreML, with the default being the NN format. So, the breakpoints werenâ€™t hit as the code was regarding the MLP format whereas I was not setting this so the code flowed through the default NN code. Knowing this I took a step back and began experimenting with NN vs MLP format.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Fix - NeuralNetwork vs MLProgram CoreML Format&lt;/head&gt;
    &lt;p&gt;So, CoreML has two model formats, these represent how the model is stored and ran with CoreML. The NeuralNetwork (NN) format is older and the MLProgram (MLP) format is newer. ORT specifies NN format by default, but it does allow you to pass a flag to use MLP format.&lt;/p&gt;
    &lt;p&gt;Testing the MLP format revealed it as the solution! See below in figure 6 the final output, which includes both ORT MLP and NN format ran on the GPU.&lt;/p&gt;
    &lt;p&gt;So ORT on MPS with NN format has the same difference from the PyTorch CPU baseline as CoreML FP16, whereas ORT with MLP format matches - this is exactly what I wanted. Mystery solved! By setting the model format to be the newer MLProgram format no implicit cast to FP16 takes place.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why MLProgram Format Worked and Neural Network Didnt?&lt;/head&gt;
    &lt;p&gt;To understand the difference in behaviours of these two models formats we need to take a deep dive on the internals of CoreML, its goals and the two formats themselves. Letâ€™s begin with CoreML.&lt;/p&gt;
    &lt;head rend="h3"&gt;CoreML&lt;/head&gt;
    &lt;p&gt;ORT implements methods to convert the ONNX graph into CoreML model formats. CoreML has two types of model format, this defines how the model is represented in the CoreML framework, how itâ€™s stored and how itâ€™s ran. The older is the NeuralNetwork format and the newer one which solved our issue is the MLProgram format. The reason MLProgram keeps the model at FP32 when running on MPS is due to the differences in model representation in these two formats. Letâ€™s take a look at both of them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Neural Network Format&lt;/head&gt;
    &lt;p&gt;As stated, the NN format is the older one, it came out in 2017. It stored models as a Directed Acyclic Graph (DAG). Each layer in the model is a node in the DAG, and they encode information on layer type, list of input names, output names and a collection of parameters specific to the layer type7. We can observe the model which is created by ORTâ€™s InferenceSession call with the following code:&lt;/p&gt;
    &lt;code&gt;# First create the InferenceSession and run the model. This ensures the CoreML model files are added to a temp dir
= ort.InferenceSession(
 ort =["CoreMLExecutionProvider"]
     onnx_path, providers
 )= ort.run(None, {"input": numpy_input})[0]
 nn_output 
import coremltools as ct

def get_coreml_dtype_from_spec(path):
"""Extract model type and dtypes by reading the spec."""
     
     = ct.models.MLModel(str(path))
     model = model.get_spec()
     spec 
print(f"\nModel Spec for {path.name}:\n {spec}\n")
     
# Find created models
= Path(tempfile.gettempdir())
 temp_dir 
# NeuralNetwork models are .mlmodel files
= list(temp_dir.glob("*.mlmodel"))
 nn_files 
for model_path in nn_files:
= get_coreml_dtype_from_spec(model_path)     info &lt;/code&gt;
    &lt;p&gt;This outputs the following:&lt;/p&gt;
    &lt;code&gt;Model Spec for onnxruntime-40975D85-7412-4309-A6F7-4E51CA3D2FE8-7682-0000BF11C24C3150.model.mlmodel:
 specificationVersion: 4
description {
  input {
    name: "input"
    type {
      multiArrayType {
        shape: 1
        shape: 4
        dataType: FLOAT32
      }
    }
  }
  output {
    name: "output"
    type {
      multiArrayType {
        shape: 1
        shape: 3
        dataType: FLOAT32
      }
    }
  }
}
neuralNetwork {
  layers {
    name: "node_linear"
    input: "input"
    output: "output"
    innerProduct {
      inputChannels: 4
      outputChannels: 3
      weights {
        floatValue: 0.0349225402
        floatValue: -0.301196814
        floatValue: 0.159211695
        floatValue: 0.156890273
        floatValue: -0.267238438
        floatValue: -0.0749385953
        floatValue: -0.292913973
        floatValue: 0.129736364
        floatValue: -0.134683847
        floatValue: 0.351268351
        floatValue: 0.354943156
        floatValue: 0.0509352088
      }
    }
  }
  arrayInputShapeMapping: EXACT_ARRAY_MAPPING
}&lt;/code&gt;
    &lt;p&gt;NeuralNetwork format has typed input and output to the model, but the nodes themselves are not typed. This is why the model gets cast to FP16, in the NN format the default behaviour is to run in FP16 on the MPS GPU. This quirk of the NN format is what threw off my results8. The CoreML runtime also specifies which parts of the model operators can run on which hardware9 and each hardware has different abilities in terms of what FP values it can handle with the NN format. Take a look at Figure 7 for Appleâ€™s guide on the hardware and FP types they can handle:&lt;/p&gt;
    &lt;p&gt;When running on CPU the NN format model will run in FP32, as we observed. However, on GPU it is implicitly cast to FP16 even though the input and output are specified to be FP32 as you see in the inspection code above. This is an inherent limitation of the NN format. The DAG structure of the model does not store any information on the types of intermediate layers. You can see this in the inspection output, the part beginning neuralNetwork stored info on the actual layer node, in our case a single linear layer. Observe that there is no information on the FP precision of the node itself, hence CoreML implicitly sets it to FP16.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why Does CoreML Implicitly Use FP16&lt;/head&gt;
    &lt;p&gt;From the typed execution docs for coremltools, the goal of CoreML is to run ML models in the most performant way and FP16 happens to be more performant than FP32 (which makes sense as itâ€™s half the precision) on Apple GPUs. Also, they state that most of the time the reduced precision doesnâ€™t matter for inference - this whole blog post shows why this is false and a pitfall of the NN format, the user should choose which precision the model is ran in, it should never be implicit.&lt;/p&gt;
    &lt;head rend="h5"&gt;MatMul Test - Is FP16 Faster on Apple Hardware?&lt;/head&gt;
    &lt;p&gt;To test Appleâ€™s claim that FP16 is more performant on Apple hardware I carried out a large matmul. Taking a 16384x16384 matrix and multiplying it with another 16384x16384 matrix should show us if FP16 is faster. The size is arbitrary I just wanted something large.&lt;/p&gt;
    &lt;p&gt;The matmul was ran 10 times, in both FP32 and FP16 on the MPS hardware, and we take the average:&lt;/p&gt;
    &lt;code&gt;FP32 Average Time: 8.6521 seconds
FP16 Average Time: 6.7691 seconds

Speedup Factor: 1.28x faster&lt;/code&gt;
    &lt;p&gt;So FP16 is quicker, which sheds a bit of light on why the NN format has implicit casting to FP16, on paper if you only care about speed then itâ€™s the better option.&lt;/p&gt;
    &lt;p&gt;Final point on the NeuralNetwork format, itâ€™s surprising as the weights themselves are stored as FP32 values (a roundtrip test verifies this) but it still executes that layer in FP16, once again showing the NN format doesnâ€™t respect the FP precision of the layer but just casts it to FP16.&lt;/p&gt;
    &lt;p&gt;All that is to say this, this was not a bug but rather an explicit design choice, which funnily enough involves implicitly going against what the user wants. The NN format has its downsides, which is why Apple introduced the MLProgram format, letâ€™s look into that.&lt;/p&gt;
    &lt;head rend="h3"&gt;The MLProgram (MLP) Format&lt;/head&gt;
    &lt;p&gt;The MLP format is the newer and better model format in CoreML, released in 2021, the core thing we care about is that the intermediate tensors are typed, i.e. there is no implicit casting when using the MLP format - the user controls whether the model is ran in FP16 or FP32.&lt;/p&gt;
    &lt;p&gt;MLP format allows for this as it uses a different representation of ML models, instead of a DAG it uses a programmatic representation of the models. By representing the model as code, it allows for greater control over the operations.&lt;/p&gt;
    &lt;p&gt;Letâ€™s see what this looks like in the stored model format and how it differs to the NN format inspection.&lt;/p&gt;
    &lt;p&gt;The code to do so is pretty similar:&lt;/p&gt;
    &lt;code&gt;# First create the InferenceSession and run the model. This ensures the CoreML model files are added to a temp dir. Also
# this time we specify the ModelFormat to be MLProgram 
= ort.InferenceSession(
 ort_mlp =[("CoreMLExecutionProvider", {"ModelFormat": "MLProgram"})]
     onnx_path, providers
 )= ort_mlp.run(None, {"input": numpy_input})[0]
 mlp_output 
import coremltools as ct

def get_coreml_dtype_from_spec(path):
"""Extract model type and dtypes by reading the spec."""
     
     = ct.models.MLModel(str(path))
     model = model.get_spec()
     spec 
print(f"\nModel Spec for {path.name}:\n {spec}\n")
     
# Find created models
= Path(tempfile.gettempdir())
 temp_dir 
# MLProgram models are in onnxruntime-* directories (not .mlmodelc)
= [d for d in temp_dir.glob("onnxruntime-*")
 mlp_dirs if d.is_dir() and not str(d).endswith('.mlmodelc')]
             
for model_path in mlp_dirs:
= get_coreml_dtype_from_spec(model_path)     info &lt;/code&gt;
    &lt;p&gt;The output of this is the following:&lt;/p&gt;
    &lt;code&gt;Model Spec for onnxruntime-752039B9-BA73-47E3-9ED4-AE029184DA69-9443-0000BF278CD8396E:
 specificationVersion: 8
description {
  input {
    name: "input"
    type {
      multiArrayType {
        shape: 1
        shape: 4
        dataType: FLOAT32
      }
    }
  }
  output {
    name: "output"
    type {
      multiArrayType {
        shape: 1
        shape: 3
        dataType: FLOAT32
      }
    }
  }
}
mlProgram {
  version: 1
  functions {
    key: "main"
    value {
      inputs {
        name: "input"
        type {
          tensorType {
            dataType: FLOAT32
            rank: 2
            dimensions {
              constant {
                size: 1
              }
            }
            dimensions {
              constant {
                size: 4
              }
            }
          }
        }
      }
      opset: "CoreML7"
      block_specializations {
        key: "CoreML7"
        value {
          outputs: "output"
          operations {
            type: "const"
            outputs {
              name: "linear_weight"
              type {
                tensorType {
                  dataType: FLOAT32
                  rank: 2
                  dimensions {
                    constant {
                      size: 3
                    }
                  }
                  dimensions {
                    constant {
                      size: 4
                    }
                  }
                }
              }
            }
            attributes {
              key: "val"
              value {
                type {
                  tensorType {
                    dataType: FLOAT32
                    rank: 2
                    dimensions {
                      constant {
                        size: 3
                      }
                    }
                    dimensions {
                      constant {
                        size: 4
                      }
                    }
                  }
                }
                blobFileValue {
                  fileName: "@model_path/weights/weight.bin"
                  offset: 64
                }
              }
            }
            attributes {
              key: "name"
              value {
                type {
                  tensorType {
                    dataType: STRING
                  }
                }
                immediateValue {
                  tensor {
                    strings {
                      values: "linear_weight"
                    }
                  }
                }
              }
            }
          }
          operations {
            type: "linear"
            inputs {
              key: "x"
              value {
                arguments {
                  name: "input"
                }
              }
            }
            inputs {
              key: "weight"
              value {
                arguments {
                  name: "linear_weight"
                }
              }
            }
            outputs {
              name: "output"
              type {
                tensorType {
                  dataType: FLOAT32
                  rank: 2
                  dimensions {
                    constant {
                      size: 1
                    }
                  }
                  dimensions {
                    constant {
                      size: 3
                    }
                  }
                }
              }
            }
            attributes {
              key: "name"
              value {
                type {
                  tensorType {
                    dataType: STRING
                  }
                }
                immediateValue {
                  tensor {
                    strings {
                      values: "node_linear__0"
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}&lt;/code&gt;
    &lt;p&gt;Now you see in the inspection of the MLP format model the linear layer is explicitly typed. To make it a bit easier to see letâ€™s bring back the NeuralNetwork format inspection and compare the linear layer setup in both:&lt;/p&gt;
    &lt;head rend="h4"&gt;Linear Layer in NeuralNetwork &amp;amp; MLProgram Format&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;NeuralNetwork Linear Layer&lt;/cell&gt;
        &lt;cell role="head"&gt;MLProgram Linear Layer&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;neuralNetwork { layers { name: "node_linear" input: "input" output: "output" innerProduct { inputChannels: 4 outputChannels: 3 weights { floatValue: 0.0349225402 floatValue: -0.301196814 floatValue: 0.159211695 floatValue: 0.156890273 floatValue: -0.267238438 floatValue: -0.0749385953 floatValue: -0.292913973 floatValue: 0.129736364 floatValue: -0.134683847 floatValue: 0.351268351 floatValue: 0.354943156 floatValue: 0.0509352088 } } }&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;operations { type: "linear" inputs { key: "x" value { arguments { name: "input" } } } inputs { key: "weight" value { arguments { name: "linear_weight" } } } outputs { name: "output" type { tensorType { dataType: FLOAT32 rank: 2 dimensions { constant { size: 1 } } dimensions { constant { size: 3 } } } } }&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Observe in the NN format, there is no explicit mention of the input or output type also the model weights are stored with the layer. Now, in the MLProgram layer, the output is explicitly typed as FP32. No more pesky implicit casting to FP16! This is one the big changes in MLProgram vs NN format, secondly notice how the layer weights are not stored along with the spec, theyâ€™re stored elsewhere. This aspect also makes the MLP format more efficient as the actual model spec is lighter.&lt;/p&gt;
    &lt;head rend="h2"&gt;But Why Does MLProgram Have Typed Layers?&lt;/head&gt;
    &lt;p&gt;So weâ€™ve come to the end of the journey, we found that NeuralNetwork format lacks types in the intermediate layers of the model and MLProgram doesnâ€™t. So, setting ORT to use MLProgram keeps the model at FP32 and our output predictions remain the same when running in PyTorch and ORT. But why, why does NeuralNetwork not include types? Answering this requires a look into how ML models have been represented in the past and how this has evolved over time.&lt;/p&gt;
    &lt;head rend="h3"&gt;Design Choices, Design Choices - How Goals of ML Optimisation Evolved Over Time&lt;/head&gt;
    &lt;p&gt;When the NeuralNetwork format was released in 2017, it came into a much different environment than the one MLProgram was born into in 2021. The goal of NeuralNetwork was to act as a configuration file to be ran by hardware, as we saw above it defines the layers and the weights without much other info and lets the hardware figure out the rest. This is indicative of the trends in ML at the time, models were still being optimised so the added complexity wasnâ€™t yet needed, the DAG representation worked well.&lt;/p&gt;
    &lt;p&gt;In essence, the NN format assumes that if the weights are stored in FP32, the input is FP32 and the output is too then the intermediate layers will also be FP32 - but as it doesnâ€™t explicitly type these intermediate layers the hardware is free to choose and the Apple GPU chooses FP16 by default!&lt;/p&gt;
    &lt;p&gt;As time went on the demands in the ML world changed, these hardware based quirks became known, optimisations advanced and overall the industry moved away from the splintered (splintered in the sense that many frameworks implemented their own) config style DAGs and began to utilise learnings from the world of compilers&lt;/p&gt;
    &lt;head rend="h3"&gt;Changes From 2017 to 2021 Which Lead to Greater Adoption of Intermediate Representations&lt;/head&gt;
    &lt;p&gt;Firstly, for Apple specifically the hardware available expanded, now you have the CPU, GPU and ANE chips - making it very difficult to assume any given piece of hardware will run a specific FP type. Also, the lack of typing leads to other issues namely the compiler cannot make some optimisations, as they depend on knowing the types before runtime. Furthermore, things like mixed FP training and quantization became a thing, once again highlighting the need for explicit typing.&lt;/p&gt;
    &lt;p&gt;Lastly, in 2017 DAGs and other forms of model compilers were very fragmented and modern times have seen a push towards standardisation10, as the compiler community consolidated on tools like LLVM the ML community has too. Intermediate Representations(IR) began to be used in ML, an IR is a hardware agnostic specification of a program which a compiler can optimise. CoreML introduced their own IR, called MIL (Model Intermediate Language) and it implements the output we see in the stored MLProgram output.&lt;/p&gt;
    &lt;head rend="h3"&gt;The MIL Approach&lt;/head&gt;
    &lt;p&gt;MIL and IRs in general afford a lot of benefits. They are inherently designed for optimisation and by providing a general framework you can extract maximal value as all optimisation engineers can work on a common goal. In MIL specifically, some of the changes weâ€™ve discussed between NN and MLProgram format, are implemented by it. Namely, each variable within the model has an explicit dtype.&lt;/p&gt;
    &lt;p&gt;Note, the MLProgram serialises and stores the output of the MIL phase, weâ€™ve already observed how it differs to the the NeuralNetwork model, with the biggest difference being in the explicit types.&lt;/p&gt;
    &lt;head rend="h4"&gt;Further Reading on ML Compilers&lt;/head&gt;
    &lt;head rend="h2"&gt;Takeaways&lt;/head&gt;
    &lt;head rend="h3"&gt;The Fix&lt;/head&gt;
    &lt;p&gt;The solution to all the issues we discussed today is, if you are using the CoreMLExecutionProvider in ORT then be sure to specify ModelFormat is MLProgram, this will ensure that whatever precision your model was trained it will be ran with that - which in my case was FP32 (whereas the default ModelFormat NeuralNetwork casts the model to FP16).&lt;/p&gt;
    &lt;p&gt;You can implement this as such:&lt;/p&gt;
    &lt;code&gt;= ort.InferenceSession(onnx_model_path, providers=[("CoreMLExecutionProvider", {"ModelFormat": "MLProgram"})]) ort_session &lt;/code&gt;
    &lt;head rend="h3"&gt;The Cause&lt;/head&gt;
    &lt;p&gt;The issue was the differing model formats employed by CoreML to represent ML models. The NeuralNetwork format utilised a more historic DAG based approach which was developed during a time in which types and precision wasnâ€™t a huge concern in the ML community and hardware decisions were left to the hardware. Whereas the MLProgram format used a programmatic approach, in which types are explicit letting the software influence how the model is run on the hardware.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lessons?&lt;/head&gt;
    &lt;p&gt;This whole thing taught me the importance of being thorough, itâ€™s not acceptable to test your model in one setup and deploy it in another. We really need to test our model runs across all the platforms we intend to deploy to. Secondly, implicit defaults can be particularly damaging, in my case it wasnâ€™t a huge issue but it easily could have been. Implicit defaults in this case also killed reproducibility, which can be problematic.&lt;/p&gt;
    &lt;p&gt;Lastly, I leave you with this:&lt;/p&gt;
    &lt;p&gt;1https://onnx.ai/onnx/intro/concepts.html&lt;/p&gt;
    &lt;p&gt;3https://developer.apple.com/documentation/coreml&lt;/p&gt;
    &lt;p&gt;4https://en.wikipedia.org/wiki/Single-precision_floating-point_format&lt;/p&gt;
    &lt;p&gt;5https://en.wikipedia.org/wiki/Half-precision_floating-point_format&lt;/p&gt;
    &lt;p&gt;6https://floating-point-gui.de/formats/fp/&lt;/p&gt;
    &lt;p&gt;7https://apple.github.io/coremltools/mlmodel/Format/NeuralNetwork.html&lt;/p&gt;
    &lt;p&gt;8https://apple.github.io/coremltools/docs-guides/source/typed-execution.html&lt;/p&gt;
    &lt;p&gt;9https://github.com/microsoft/onnxruntime/issues/21271#issuecomment-3637845056&lt;/p&gt;
    &lt;p&gt;10https://www.modular.com/blog/democratizing-ai-compute-part-8-what-about-the-mlir-compiler-infrastructure&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46350075</guid><pubDate>Mon, 22 Dec 2025 00:27:04 +0000</pubDate></item><item><title>86Box v5.3</title><link>https://86box.net/2025/12/21/86box-v5-3.html</link><description>&lt;doc fingerprint="3f931d199536abc5"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;86Box v5.3&lt;/head&gt;December 21, 2025 - written by richardg867&lt;p&gt;This is the December 2025 update to 86Box, bringing in performance improvements, bugfixes and some new hardware for the holidays.&lt;/p&gt;&lt;head rend="h2"&gt;Main features&lt;/head&gt;&lt;p&gt;Several small and localized performance improvements have been made to emulation performance, including a new C runtime library for Windows host systems, optimizations to the â€œnewâ€ dynamic recompiler used on ARM and Apple Silicon host systems, as well as multithreading improvements to Voodoo and other video cards. We remain engaged in investigating more potential performance improvements for the next release.&lt;/p&gt;&lt;p&gt;The floppy drive sounds feature added last release got a big update, with improved accuracy especially in head seeks, and new recordings from two 3.5â€ and three 5.25â€ floppy drives. The recordings have been moved to a new package outside of the ROM set, so read below for more information before upgrading.&lt;/p&gt;&lt;head rend="h2"&gt;Important changes&lt;/head&gt;&lt;head rend="h3"&gt;Asset pack and floppy sounds&lt;/head&gt;&lt;p&gt;Due to its large size, the floppy drive sound collection has been moved out of the ROM set and into a new asset pack, which is now included with release versions of 86Box downloaded from GitHub, as the &lt;code&gt;assets&lt;/code&gt; folder inside the &lt;code&gt;.zip&lt;/code&gt; on Windows, or embedded within the AppImage on Linux or the app bundle on macOS. Linux packages may or may not include the asset pack; we recommend maintainers to include it in &lt;code&gt;/usr/share/86Box/assets&lt;/code&gt; as part of the standard 86Box package.&lt;/p&gt;&lt;p&gt;If you use our experimental builds or any other package without the asset pack, floppy sounds will not be available until you install the pack the same way you would install the ROM set. Download and decompress it into an &lt;code&gt;assets&lt;/code&gt; folder next to (not inside!) the &lt;code&gt;roms&lt;/code&gt; folder in any of the same places: next to the 86Box application (Windows executable, Linux AppImage or macOS app bundle) or in one of the system-wide locations for your host operating system.&lt;/p&gt;&lt;p&gt;On top of the asset pack change, the Mitsumi and Teac floppy drive recordings from v5.2 have been removed for technical reasons, so you may need to reconfigure floppy sounds after upgrading.&lt;/p&gt;&lt;head rend="h3"&gt;Windows 7 and 8 support&lt;/head&gt;&lt;p&gt;Windows 7 and 8 host systems are still supported for the time being, but on those Windows versions, the Visual C++ 2015 Redistributable must now be installed. You probably already have this installed by other applications or Windows updates, but in case you donâ€™t and 86Box complains about a missing DLL, an installer can be downloaded from Microsoftâ€™s website (get the x64 version).&lt;/p&gt;&lt;head rend="h2"&gt;Changelog&lt;/head&gt;&lt;head rend="h3"&gt;Emulator&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Improved overall performance on Windows hosts by upgrading to the Universal C Runtime&lt;/item&gt;&lt;item&gt;Added asset pack for floppy drive sounds and other future features&lt;/item&gt;&lt;item&gt;Added customizable Ctrl+Alt+Page Down keyboard shortcut to show or hide the user interface in full screen mode&lt;/item&gt;&lt;item&gt;Added toolbar button, Action menu option and customizable Ctrl+Alt+I keyboard shortcut to temporarily disable the dynamic recompiler for troublesome applications&lt;/item&gt;&lt;item&gt;Added relative path conversion to disk images located next to the 86Box application or one level above the machine folder (for portable setups)&lt;/item&gt;&lt;item&gt;Fixed OpenGL renderer crashing the emulator when taking screenshots&lt;/item&gt;&lt;item&gt;Fixed emulated display resolution changes occasionally crashing the emulator&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;User interface&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Added a toolbar with quick commands to the manager&lt;/item&gt;&lt;item&gt;Added icons to some menu entries&lt;/item&gt;&lt;item&gt;Fixed refresh rate indicator displaying inaccurate numbers when a Voodoo add-in card is active&lt;/item&gt;&lt;item&gt;Changed emulation speed indicator to be consistent across CPU frame size options&lt;/item&gt;&lt;item&gt;Updated and cleaned up many translations&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Machines&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;808x &lt;list rend="ul"&gt;&lt;item&gt;Added real time clock I/O port and IRQ configuration to the Multitech PC-500 and PC-500 plus&lt;/item&gt;&lt;item&gt;Fixed Tandy 1000 family display shake effect used by some games&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;286 &lt;list rend="ul"&gt;&lt;item&gt;Fixed extended memory support on C&amp;amp;T PC/AT machines&lt;/item&gt;&lt;item&gt;Fixed Amstrad PC5286 keyboard issues when a PS/2 mouse is emulated&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;386 &lt;list rend="ul"&gt;&lt;item&gt;Added Socket 1 486 CPU support to the IBM PS/55 model 5550-V&lt;/item&gt;&lt;item&gt;Renamed IBM PS/55 model 5550-T to 5550-S/T Stage II&lt;/item&gt;&lt;item&gt;Renamed IBM PS/55 model 5550-V to 5550-V0/V1 and changed category to 386DX/486&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;486 &lt;list rend="ul"&gt;&lt;item&gt;Fixed incorrect on-board video BIOS on the IBM PS/ValuePoint 433DX/Si&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;586 &lt;list rend="ul"&gt;&lt;item&gt;Added Socket 5 machine: HP Pavilion 50x0/70xx&lt;/item&gt;&lt;item&gt;Added Socket 7 machines: ASUS TX97-XV, HP Pavilion 51xx/7070/7090/71xx, 52xx/53xx/71xx/72xx, 73xx/74xx&lt;/item&gt;&lt;item&gt;Added on-board Crystal CS4232 sound to the Intel Advanced/AS, Advanced/ATX and Advanced/MA&lt;/item&gt;&lt;item&gt;Added RM Accelerator 350P2XB/450P3XB BIOS variant option to the AOpen AX6BC&lt;/item&gt;&lt;item&gt;Added Award BIOS option to the MSI MS-5124 and MS-5146&lt;/item&gt;&lt;item&gt;Fixed BIOS settings for serial and parallel ports the Intel Advanced/ATX&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;686 &lt;list rend="ul"&gt;&lt;item&gt;Added Slot 1 machine: MSI MS-6199VA (plus Compaq and Packard Bell BIOS variants)&lt;/item&gt;&lt;item&gt;Added Socket 370 machines: MSI MS-6318 (plus Elonex, Fujitsu-Siemens, HP and Medion BIOS variants), Samsung CAIRO-5&lt;/item&gt;&lt;item&gt;Added more BIOS version options to the ABIT AB-BX6&lt;/item&gt;&lt;item&gt;Added Leadtek WinFast 8000BX BIOS variant option to the Supermicro P6SBA&lt;/item&gt;&lt;item&gt;Changed maximum RAM on the AOpen AP61&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Hardware&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Core &lt;list rend="ul"&gt;&lt;item&gt;Improved CPU performance on ARM hosts, especially on MMX applications&lt;/item&gt;&lt;item&gt;Fixed specific FPU inaccuracy on ARM hosts leading to loss of sound on some Windows games&lt;/item&gt;&lt;item&gt;Fixed INC/DEC instruction legality inaccuracy&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Display &lt;list rend="ul"&gt;&lt;item&gt;Improved emulation performance of Voodoo, ATI Mach64 and S3 video cards&lt;/item&gt;&lt;item&gt;Fixed VideoMagic ETW32PVS (Tseng ET4000/W32p) VLB video card not being selectable on Linux and other case-sensitive systems&lt;/item&gt;&lt;item&gt;Fixed IBM 8514/A ATI MCA variant crashing the emulator on startup on some hosts&lt;/item&gt;&lt;item&gt;Fixed incorrect refresh rates on many cards (again)&lt;/item&gt;&lt;item&gt;Fixed S3 ViRGE rendering glitches on full motion video applications (again)&lt;/item&gt;&lt;item&gt;Fixed IBM XGA rendering glitches on specific color depths&lt;/item&gt;&lt;item&gt;Fixed Matrox and Voodoo texture glitches on 3D applications&lt;/item&gt;&lt;item&gt;Fixed Voodoo Banshee/3 text rendering glitches on Linux&lt;/item&gt;&lt;item&gt;Fixed rendering issues with specific drivers on S3 9xx and Tseng ET4000/W32 cards&lt;/item&gt;&lt;item&gt;Fixed transparency glitches with more games on Voodoo cards&lt;/item&gt;&lt;item&gt;Fixed VGA scrolling behavior not matching real hardware on ATI Mach64 and Tseng ET4000/W32 cards&lt;/item&gt;&lt;item&gt;Fixed incorrect dark gray color on the IBM 5153 CGA monitor&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Input &lt;list rend="ul"&gt;&lt;item&gt;Fixed XT keyboard key count option not taking effect&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Sound &lt;list rend="ul"&gt;&lt;item&gt;Added Crystal CS4232 ISA sound card&lt;/item&gt;&lt;item&gt;Added OPTi 82C930 and 82C931 ISA sound cards&lt;/item&gt;&lt;item&gt;Fixed Sound Blaster 16 and AWE sound issues on some DOS games&lt;/item&gt;&lt;item&gt;Fixed incorrect default I/O port on the Covox Voice Master Key&lt;/item&gt;&lt;item&gt;Removed Raise CODEC interrupt option from Aztech Sound Galaxy cards as it is no longer required&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Ports &lt;list rend="ul"&gt;&lt;item&gt;Fixed emulation hangs with serial passthrough to Windows named pipes in server mode (known issue: VMware serial ports fail to connect if 86Box is the pipe server; use VMware as the server and 86Box as the client instead)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Storage &lt;list rend="ul"&gt;&lt;item&gt;Improved accuracy of floppy drive sounds&lt;/item&gt;&lt;item&gt;Added a new set of floppy drive sounds&lt;/item&gt;&lt;item&gt;Added IDE CD-ROM drive models: HITACHI CDR-8435 (for the RM Accelerator 350P2XB/450P3XB (AOpen AX6BC)), TOSHIBA CD-ROM XM-6102B&lt;/item&gt;&lt;item&gt;Added SCSI CD-ROM drive models: NEC CD-ROM DRIVE:900, PLEXTOR CD-ROM PX-12CS, PX-12TS, PX-83CS, TOSHIBA CD-ROM XM-3701B&lt;/item&gt;&lt;item&gt;Fixed missing DVD support on the HITACHI GD-7500 and HL-DT-ST DVDRAM GSA-4160 CD-ROM drive models&lt;/item&gt;&lt;item&gt;Fixed Panasonic/MKE CD-ROM drives not being detected by OS/2&lt;/item&gt;&lt;item&gt;Fixed floppy drive controller inaccuracy crashing 1B/V3&lt;/item&gt;&lt;item&gt;Removed Mitsumi and Teac floppy drive sounds which could not be updated to the improved system&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Other &lt;list rend="ul"&gt;&lt;item&gt;Added base memory backfill support to the Everex EV-159 ISA memory expansion card&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46350477</guid><pubDate>Mon, 22 Dec 2025 01:43:40 +0000</pubDate></item><item><title>Build Android apps using Rust and Iced</title><link>https://github.com/ibaryshnikov/android-iced-example</link><description>&lt;doc fingerprint="3d82cc104505dfd0"&gt;
  &lt;main&gt;
    &lt;p&gt;There are NativeActivity and GameActivity examples here.&lt;/p&gt;
    &lt;p&gt;Based on several other examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;na-mainloop&lt;/code&gt;and&lt;code&gt;agdk-mainloop&lt;/code&gt;from android-activity&lt;/item&gt;
      &lt;item&gt;na-winit-wgpu from &lt;code&gt;rust-android-examples&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;integration from &lt;code&gt;iced&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;iced integration example&lt;/p&gt;
    &lt;p&gt;You can also run most of the examples from iced. For this omit the scene rendering part and set the background of the root container.&lt;/p&gt;
    &lt;p&gt;Text input partially works, unresolved issues:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;window doesn't resize on show/hide soft keyboard&lt;/item&gt;
      &lt;item&gt;how to change input language of soft keyboard&lt;/item&gt;
      &lt;item&gt;ime is not supported&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Copy/paste and show/hide soft keyboard is implemented by calling Java&lt;/p&gt;
    &lt;p&gt;Check &lt;code&gt;android-activity&lt;/code&gt; crate for detailed instructions.
During my tests I was running the following command and using android studio afterwards:&lt;/p&gt;
    &lt;code&gt;export ANDROID_NDK_HOME="path/to/ndk"
export ANDROID_HOME="path/to/sdk"

rustup target add x86_64-linux-android
cargo install cargo-ndk

cargo ndk -t x86_64 -o app/src/main/jniLibs/  build&lt;/code&gt;
    &lt;p&gt;My setup is the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;archlinux 6.9.6&lt;/item&gt;
      &lt;item&gt;jdk-openjdk 22&lt;/item&gt;
      &lt;item&gt;target api 35&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thanks to &lt;code&gt;android-activity&lt;/code&gt; we can already build android apps in Rust, and
key crates such as &lt;code&gt;winit&lt;/code&gt; and &lt;code&gt;wgpu&lt;/code&gt; also support building for android.
&lt;code&gt;iced&lt;/code&gt; doesn't support android out of the box, but it can be integrated with
existing graphics pipelines, as shown in
integration example.
As a result, it was possible to convert existing example running &lt;code&gt;winit&lt;/code&gt; + &lt;code&gt;wgpu&lt;/code&gt; to
use &lt;code&gt;iced&lt;/code&gt; on top.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46350641</guid><pubDate>Mon, 22 Dec 2025 02:14:32 +0000</pubDate></item></channel></rss>