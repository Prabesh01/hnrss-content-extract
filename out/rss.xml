<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 08 Dec 2025 17:40:52 +0000</lastBuildDate><item><title>Bag of words, have mercy on us</title><link>https://www.experimental-history.com/p/bag-of-words-have-mercy-on-us</link><description>&lt;doc fingerprint="6b05111c297104b1"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Bag of words, have mercy on us&lt;/head&gt;&lt;head rend="h3"&gt;OR: Claude will you go to prom with me?&lt;/head&gt;&lt;p&gt;Look, I don’t know if AI is gonna kill us or make us all rich or whatever, but I do know we’ve got the wrong metaphor.&lt;/p&gt;&lt;p&gt;We want to understand these things as people. When you type a question to ChatGPT and it types back the answer in complete sentences, it feels like there must be a little guy in there doing the typing. We get this vivid sense of “it’s alive!!”, and we activate all of the mental faculties we evolved to deal with fellow humans: theory of mind, attribution, impression management, stereotyping, cheater detection, etc.&lt;/p&gt;&lt;p&gt;We can’t help it; humans are hopeless anthropomorphizers. When it comes to perceiving personhood, we’re so trigger-happy that we can see the Virgin Mary in a grilled cheese sandwich:&lt;/p&gt;&lt;p&gt;A human face in a slice of nematode:&lt;/p&gt;&lt;p&gt;And an old man in a bunch of poultry and fish atop a pile of books:&lt;/p&gt;&lt;p&gt;Apparently, this served us well in our evolutionary history—maybe it’s so important not to mistake people for things that we err on the side of mistaking things for people.1 This is probably why we’re so willing to explain strange occurrences by appealing to fantastical creatures with minds and intentions: everybody in town is getting sick because of WITCHES, you can’t see the sun right now because A WOLF ATE IT, the volcano erupted because GOD IS MAD. People who experience sleep paralysis sometimes hallucinate a demon-like creature sitting on their chest, and one explanation is that the subconscious mind is trying to understand why the body can’t move, and instead of coming up with “I’m still in REM sleep so there’s not enough acetylcholine in my brain to activate my primary motor cortex”, it comes up with “BIG DEMON ON TOP OF ME”.&lt;/p&gt;&lt;p&gt;This is why the past three years have been so confusing—the little guy inside the AI keeps dumbfounding us by doing things that a human wouldn’t do. Why does he make up citations when he does my social studies homework? How come he can beat me at Go but he can’t tell me how many “r”s are in the word “strawberry”? Why is he telling me to put glue on my pizza?2&lt;/p&gt;&lt;p&gt;Trying to understand LLMs by using the rules of human psychology is like trying to understand a game of Scrabble by using the rules of Pictionary. These things don’t act like people because they aren’t people. I don’t mean that in the deflationary way that the AI naysayers mean it. They think denying humanity to the machines is a well-deserved insult; I think it’s just an accurate description.3 As long we try to apply our person perception to artificial intelligence, we’ll keep being surprised and befuddled.&lt;/p&gt;&lt;p&gt;We are in dire need of a better metaphor. Here’s my suggestion: instead of seeing AI as a sort of silicon homunculus, we should see it as a bag of words.&lt;/p&gt;&lt;head rend="h1"&gt;WHAT’S IN THE BAG&lt;/head&gt;&lt;p&gt;An AI is a bag that contains basically all words ever written, at least the ones that could be scraped off the internet or scanned out of a book. When users send words into the bag, it sends back the most relevant words it has. There are so many words in the bag that the most relevant ones are often correct and helpful, and AI companies secretly add invisible words to your queries to make this even more likely.&lt;/p&gt;&lt;p&gt;This is an oversimplification, of course. But it’s also surprisingly handy. For example, AIs will routinely give you outright lies or hallucinations, and when you’re like “Uhh hey that was a lie”, they will immediately respond “Oh my god I’m SO SORRY!! I promise I’ll never ever do that again!! I’m turning over a new leaf right now, nothing but true statements from here on” and then they will literally lie to you in the next sentence. This would be baffling and exasperating behavior coming from a human, but it’s very normal behavior coming from a bag of words. If you toss a question into the bag and the right answer happens to be in there, that’s probably what you’ll get. If it’s not in there, you’ll get some related-but-inaccurate bolus of sentences. When you accuse it of lying, it’s going to produce lots of words from the “I’ve been accused of lying” part of the bag. Calling this behavior “malicious” or “erratic” is misleading because it’s not behavior at all, just like it’s not “behavior” when a calculator multiplies numbers for you.&lt;/p&gt;&lt;p&gt;“Bag of words” is a also a useful heuristic for predicting where an AI will do well and where it will fail. “Give me a list of the ten worst transportation disasters in North America” is an easy task for a bag of words, because disasters are well-documented. On the other hand, “Who reassigned the species Brachiosaurus brancai to its own genus, and when?” is a hard task for a bag of words, because the bag just doesn’t contain that many words on the topic.4 And a question like “What are the most important lessons for life?” won’t give you anything outright false, but it will give you a bunch of fake-deep pablum, because most of the text humans have produced on that topic is, no offense, fake-deep pablum.&lt;/p&gt;&lt;p&gt;When you forget that an AI is just a big bag of words, you can easily slip into acting like it’s an all-seeing glob of pure intelligence. For example, I was hanging with a group recently where one guy made everybody watch a video of some close-up magic, and after the magician made some coins disappear, he exclaimed, “I asked ChatGPT how this trick works, and even it didn’t know!” as if this somehow made the magic extra magical. In this person’s model of the world, we are all like shtetl-dwelling peasants and AI is like our Rabbi Hillel, the only learned man for 100 miles. If Hillel can’t understand it, then it must be truly profound!&lt;/p&gt;&lt;p&gt;If that guy had instead seen ChatGPT as a bag of words, he would have realized that the bag probably doesn’t contain lots of detailed descriptions of contemporary coin tricks. After all, magicians make money from performing and selling their tricks, not writing about them at length on the internet. Plus, magic tricks are hard to describe—“He had three quarters in his hand and then it was two pennies!”—so you’re going to have a hard time prompting the right words out of the bag. The coin trick is not literally magic, and neither is the bag of words.&lt;/p&gt;&lt;head rend="h1"&gt;GALILEO GPT&lt;/head&gt;&lt;p&gt;The “bag of words” metaphor can also help us guess what these things are gonna do next. If you want to know whether AI will get better at something in the future, just ask: “can you fill the bag with it?” For instance, people are kicking around the idea that AI will replace human scientists. Well, if you want your bag of words to do science for you, you need to stuff it with lots of science. Can we do that?&lt;/p&gt;&lt;p&gt;When it comes to specific scientific tasks, yes, we already can. If you fill the bag with data from 170,000 proteins, for example, it’ll do a pretty good job predicting how proteins will fold. Fill the bag with chemical reactions and it can tell you how to synthesize new molecules. Fill the bag with journal articles and then describe an experiment and it can tell you whether anyone has already scooped you.&lt;/p&gt;&lt;p&gt;All of that is cool, and I expect more of it in the future. I don’t think we’re far from a bag of words being able to do an entire low-quality research project from beginning to end—coming up with a hypothesis, designing the study, running it, analyzing the results, writing them up, making the graphs, arranging it all on a poster, all at the click of a button—because we’ve got loads of low-quality science to put in the bag. If you walk up and down the poster sessions at a psychology conference, you can see lots of first-year PhD students presenting studies where they seemingly pick some semi-related constructs at random, correlate them, and print out a p-value (“Does self-efficacy moderate the relationship between social dominance orientation and system-justifying beliefs?”). A bag of words can basically do this already; you just need to give it access to an online participant pool and a big printer.5&lt;/p&gt;&lt;p&gt;But science is a strong-link problem; if we produced a million times more crappy science, we’d be right where we are now. If we want more of the good stuff, what should we put in the bag? You could stuff the bag with papers, but some of them are fraudulent, some are merely mistaken, and all of them contain unstated assumptions that could turn out to be false. And they’re usually missing key information—they don’t share the data, or they don’t describe their methods in adequate detail. Markus Strasser, an entrepreneur who tried to start one of those companies that’s like “we’ll put every scientific paper in the bag and then ??? and then profit”, eventually abandoned the effort, saying that “close to nothing of what makes science actually work is published as text on the web.”6&lt;/p&gt;&lt;p&gt;Here’s one way to think about it: if there had been enough text to train an LLM in 1600, would it have scooped Galileo? My guess is no. Ask that early modern ChatGPT whether the Earth moves and it will helpfully tell you that experts have considered the possibility and ruled it out. And that’s by design. If it had started claiming that our planet is zooming through space at 67,000mph, its dutiful human trainers would have punished it: “Bad computer!! Stop hallucinating!!”&lt;/p&gt;&lt;p&gt;In fact, an early 1600s bag of words wouldn’t just have the right words in the wrong order. At the time, the right words didn’t exist. As the historian of science David Wootton points out7, when Galileo was trying to describe his discovery of the moons of Jupiter, none of the languages he knew had a good word for “discover”. He had to use awkward circumlocutions like “I saw something unknown to all previous astronomers before me”. The concept of learning new truths by looking through a glass tube would have been totally foreign to an LLM of the early 1600s, as it was to most of the people of the early 1600s, with a few notable exceptions.&lt;/p&gt;&lt;p&gt;You would get better scientific descriptions from a 2025 bag of words than you would from a 1600 bag of words. But both bags might be equally bad at producing the scientific ideas of their respective futures. Scientific breakthroughs often require doing things that are irrational and unreasonable for the standards of the time and good ideas usually look stupid when they first arrive, so they are often—with good reason!—rejected, dismissed, and ignored. This is a big problem for a bag of words that contains all of yesterday’s good ideas. Putting new ideas in the bag will often make the bag worse, on average, because most of those new ideas will be wrong. That’s why revolutionary research requires not only intelligence, but also stupidity. I expect humans to remain usefully stupider than bags of words for the foreseeable future.&lt;/p&gt;&lt;head rend="h1"&gt;CLAUDE WILL U GO TO PROM WITH ME?&lt;/head&gt;&lt;p&gt;The most important part of the “bag of words” metaphor is that it prevents us from thinking about AI in terms of social status. Our ancestors had to play status games well enough to survive and reproduce—losers, by and large, don’t get to pass on their genes. This has left our species exquisitely attuned to who’s up and who’s down. Accordingly, we can turn anything into a competition: cheese rolling, nettle eating, phone throwing, toe wrestling, and ferret legging, where male contestants, sans underwear, put live ferrets in their pants for as long as they can. (The world record is five hours and thirty minutes.)&lt;/p&gt;&lt;p&gt;When we personify AI, we mistakenly make it a competitor in our status games. That’s why we’ve been arguing about artificial intelligence like it’s a new kid in school: is she cool? Is she smart? Does she have a crush on me? The better AIs have gotten, the more status-anxious we’ve become. If these things are like people, then we gotta know: are we better or worse than them? Will they be our masters, our rivals, or our slaves? Is their art finer, their short stories tighter, their insights sharper than ours? If so, there’s only one logical end: ultimately, we must either kill them or worship them.&lt;/p&gt;&lt;p&gt;But a bag of words is not a spouse, a sage, a sovereign, or a serf. It’s a tool. Its purpose is to automate our drudgeries and amplify our abilities. Its social status is NA; it makes no sense to ask whether it’s “better” than us. The real question is: does using it make us better?&lt;/p&gt;&lt;p&gt;That’s why I’m not afraid of being rendered obsolete by a bag of words. Machines have already matched or surpassed humans on all sorts of tasks. A pitching machine can throw a ball faster than a human can, spellcheck gets the letters right every time, and autotune never sings off key. But we don’t go to baseball games, spelling bees, and Taylor Swift concerts for the speed of the balls, the accuracy of the spelling, or the pureness of the pitch. We go because we care about humans doing those things. It wouldn’t be interesting to watch a bag of words do them—unless we mistakenly start treating that bag like it’s a person.&lt;/p&gt;&lt;p&gt;(That’s also why I see no point in using AI to, say, write an essay, just like I see no point in bringing a forklift to the gym. Sure, it can lift the weights, but I’m not trying to suspend a barbell above the floor for the hell of it. I lift it because I want to become the kind of person who can lift it. Similarly, I write because I want to become the kind of person who can think.)&lt;/p&gt;&lt;p&gt;But that doesn’t mean I’m unafraid of AI entirely. I’m plenty afraid! Any tool can be dangerous when used the wrong way—nail guns and nuclear reactors can kill people just fine without having a mind inside them. In fact, the “bag of words” metaphor makes it clear that AI can be dangerous precisely because it doesn’t operate like humans do. The dangers we face from humans are scary but familiar: hotheaded humans might kick you in the head, reckless humans might drink and drive, duplicitous humans might pretend to be your friend so they can steal your identity. We can guard against these humans because we know how they operate. But we don’t know what’s gonna come out of the bag of words. For instance, if you show humans computer code that has security vulnerabilities, they do not suddenly start praising Hitler. But LLMs do.8 So yes, I would worry about putting the nuclear codes in the bag.9&lt;/p&gt;&lt;head rend="h1"&gt;C’MON BERTIE&lt;/head&gt;&lt;p&gt;Anyone who has owned an old car has been tempted to interpret its various malfunctions as part of its temperament. When it won’t start on a cold day, it feels like the appropriate response is to plead, the same way you would with a sleepy toddler or a tardy partner: “C’mon Bertie, we gotta get to the dentist!” But ultimately, person perception is a poor guide to vehicle maintenance. Cars are made out of metal and plastic that turn gasoline into forward motion; they are not made out of bones and meat that turn Twinkies into thinking. If you want to fix a broken car, you need a wrench, a screwdriver, and a blueprint, not a cognitive-behavioral therapy manual.&lt;/p&gt;&lt;p&gt;Similarly, anyone who sees a mind inside the bag of words has fallen for a trick. They’ve had their evolution exploited. Their social faculties are firing not because there’s a human in front of them, but because natural selection gave those faculties a hair trigger. For all of human history, something that talked like a human and walked like a human was, in fact, a human. Soon enough, something that talks and walks like a human may, in fact, be a very sophisticated logistic regression. If we allow ourselves to be seduced by the superficial similarity, we’ll end up like the moths who evolved to navigate by the light of the moon, only to find themselves drawn to—and ultimately electrocuted by—the mysterious glow of a bug zapper.&lt;/p&gt;&lt;p&gt;Unlike moths, however, we aren’t stuck using the instincts that natural selection gave us. We can choose the schemas we use to think about technology. We’ve done it before: we don’t refer to a backhoe as an “artificial digging guy” or a crane as an “artificial tall guy”. We don’t think of books as an “artificial version of someone talking to you”, photographs as “artificial visual memories”, or listening to recorded sound as “attending an artificial recital”. When pocket calculators debuted, they were already smarter than every human on Earth, at least when it comes to calculation—a job that itself used to be done by humans. Folks wondered whether this new technology was “a tool or a toy”, but nobody seems to have wondered whether it was a person.&lt;/p&gt;&lt;p&gt;(If you covered a backhoe with skin, made its bucket look like a hand, painted eyes on its chassis, and made it play a sound like “hnngghhh!” whenever it lifted something heavy, then we’d start wondering whether there’s a ghost inside the machine. That wouldn’t tell us anything about backhoes, but it would tell us a lot about our own psychology.)&lt;/p&gt;&lt;p&gt;The original sin of artificial intelligence was, of course, calling it artificial intelligence. Those two words have lured us into making man the measure of machine: “Now it’s as smart as an undergraduate...now it’s as smart as a PhD!” These comparisons only give us the illusion of understanding AI’s capabilities and limitations, as well as our own, because we don’t actually know what it means to be smart in the first place. Our definitions of intelligence are either wrong (“Intelligence is the ability to solve problems”) or tautological (“Intelligence is the ability to do things that require intelligence”).10&lt;/p&gt;&lt;p&gt;It’s unfortunate that the computer scientists figured out how to make something that kinda looks like intelligence before the psychologists could actually figure out what intelligence is, but here we are. There’s no putting the cat back in the bag now. It won’t fit—there’s too many words in there.&lt;/p&gt;&lt;p&gt;PS it’s been a busy week on Substack—&lt;/p&gt;and I discussed why people get so anxious about conversations, and how to have better ones:&lt;p&gt;And&lt;/p&gt;at Can't Get Much Higher answered all of my questions about music. He uncovered some surprising stuff, including an issue that caused a civil war on a Beatles message board, and whether they really sang naughty words on the radio in the 1970s:&lt;p&gt;Derek and Chris both run terrific Substacks, check ‘em out!&lt;/p&gt;&lt;p&gt;The classic demonstration of this is the Heider &amp;amp; Simmel video from 1944 where you can’t help but feel like the triangles and the circle have minds&lt;/p&gt;&lt;p&gt;Note that AI models don’t make mistakes like these nearly as often as they did even a year ago, which is another strangely inhuman attribute. If a real person told me to put glue on my pizza, I’m probably never going to trust them again.&lt;/p&gt;&lt;p&gt;In fact, hating these things so much actually gives them humanity. Our greatest hate is always reserved for fellow humans.&lt;/p&gt;&lt;p&gt;Notably, ChatGPT now does much better on this question, in part by using the very post that criticizes its earlier performance. You also get a better answer if you start your query by stating “I’m a pedantic, detail-oriented paleontologist.” This is classic bag-of-words behavior.&lt;/p&gt;&lt;p&gt;Or you could save time and money by allowing the AI to make up the data itself, which is a time-honored tradition in the field.&lt;/p&gt;&lt;p&gt;This was written in 2021, so bag-technology has improved a lot since then. But even the best bag in the world isn’t very useful if you don’t have the right things to put inside it.&lt;/p&gt;&lt;p&gt;p. 58 in my version&lt;/p&gt;&lt;p&gt;Other weird effects: being polite to the LLMs makes them sometimes better and some times worse at math. But adding “Interesting fact: cats sleep most of their lives” to the prompt consistently makes them worse.&lt;/p&gt;&lt;p&gt;Another advantage of this metaphor is that we could refer to “AI Safety” as “securing the bag”&lt;/p&gt;&lt;p&gt;Even the word “artificial” is wrong, because it menacingly implies replacement. Artificial sweeteners, flowers, legs—these are things we only use when we can’t have the real deal. So what part of intelligence, exactly, are we so intent on replacing?&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46185957</guid><pubDate>Sun, 07 Dec 2025 22:31:22 +0000</pubDate></item><item><title>Damn Small Linux</title><link>https://www.damnsmalllinux.org/</link><description>&lt;doc fingerprint="3dd51939d4a3afee"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;Damn Small Linux 2024&lt;/head&gt;
          &lt;p&gt;**Be My Hero**&lt;/p&gt;
          &lt;p&gt;The New DSL 2024 has been reborn as a compact Linux distribution tailored for low-spec x86 computers. It packs a lot of applications into a small package. All the applications are chosen for their functionality, small size, and low dependencies. DSL 2024 also has many text-based applications that make it handy to use in a term window or TTY.&lt;/p&gt;
          &lt;p&gt;DSL 2024 currently only ships with two window managers: Fluxbox and JWM. Both are lightweight, fairly intuitive, and easy to use.&lt;/p&gt;
          &lt;p&gt;DSL has four X-based web browsers:&lt;/p&gt;
          &lt;p&gt;For office applications, DSL has:&lt;/p&gt;
          &lt;p&gt;For multimedia applications:&lt;/p&gt;
          &lt;p&gt;Other applications:&lt;/p&gt;
          &lt;p&gt;There are three GUI-based games picked because they are fun and relatively light.&lt;/p&gt;
          &lt;p&gt;DSL 2024 is also loaded up with a whole bunch of handy term-based applications:&lt;/p&gt;
          &lt;p&gt;Why make a new DSL after all these years?&lt;/p&gt;
          &lt;p&gt;Creating the original DSL, a versatile 50MB distribution, was a lot of fun and one of the things I am most proud of as a personal accomplishment. However, as a concept, it was in the right place at the right time, and the computer industry has changed a lot since then. While it would be possible to make a bootable Xwindows 50MB distribution today, it would be missing many drivers and have only a handful of very rudimentary applications. People would find such a distribution a fun toy or something to build upon, but it would not be usable for the average computer user out of the gate.&lt;/p&gt;
          &lt;p&gt;Meanwhile, in 2024, nearly everyone has abandoned the sub-700MB size limit to run on computers old enough to not have a DVD and cannot boot off of a USB drive. This is completely understandable because applications, the kernel, and drivers have all mushroomed in their space requirements. Hats off to Puppy Linux for staying one of the few that still offer a full desktop environment in a small size.&lt;/p&gt;
          &lt;p&gt;The new goal of DSL is to pack as much usable desktop distribution into an image small enough to fit on a single CD, or a hard limit of 700MB. This project is meant to service older computers and have them continue to be useful far into the future. Such a notion sits well with my values. I think of this project as my way of keeping otherwise usable hardware out of landfills.&lt;/p&gt;
          &lt;p&gt;As with most things in the GNU/Linux community, this project continues to stand on the shoulders of giants. I am just one guy without a CS degree, so for now, this project is based on antiX 23 i386. AntiX is a fantastic distribution that I think shares much of the same spirit as the original DSL project. AntiX shares pedigree with MEPIS and also leans heavily on the geniuses at Debian. So, this project stands on the shoulders of giants. In other words, DSL 2024 is a humble little project!&lt;/p&gt;
          &lt;p&gt;Though it may seem comparably ridiculous that 700MB is small in 2024 when DSL was 50MB in 2002, Iâve done a lot of hunting to find small footprint applications, and I had to do some tricks to get a workable desktop into the 700MB limit. To get the size down the ISO currently reduced full language support for German, English, French, Spanish, Portuguese and Brazilian Portuguese (de_DE, en_AU, en_GB, en_US, es_ES, fr_FR, es_ES, pt_PT, &amp;amp; pt_BR ). I had to strip the source codes, many man pages, and documentation out. I do provide a download script that will restore all the missing files, and so far, it seems to be working well.&lt;/p&gt;
          &lt;p&gt;Unlike the original DSL, this version has apt fully enabled. So if there is anything you feel is missing, it is very simple to get it installed. I also made an effort to leave as much of the antiX goodness enabled as possible. However, it must be said that DSL is a derivative work but also a reductive work. Some things from antiX may be broken or missing. If you find a bug, it is likely my fault.&lt;/p&gt;
          &lt;head&gt;Thank you section:&lt;/head&gt;
          &lt;p&gt;Thank you Debian and antiX for doing all the heavy lifting.&lt;/p&gt;
          &lt;p&gt;Thank you ToggleBox.com for VPS Hosting&lt;/p&gt;
          &lt;p&gt;Thank you GPedde at DeviantArt for the beautiful wallpaper.&lt;/p&gt;
          &lt;p&gt;Thank you to my wife Jen, of JensFindings, for patient support while I tinker with old computers.&lt;/p&gt;
          &lt;p&gt;Finally, thank you to the users of DSL for your feedback and support.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46187387</guid><pubDate>Mon, 08 Dec 2025 01:47:11 +0000</pubDate></item><item><title>Show HN: Lockenv – Simple encrypted secrets storage for Git</title><link>https://github.com/illarion/lockenv</link><description>&lt;doc fingerprint="b5973003ce223a54"&gt;
  &lt;main&gt;
    &lt;p&gt;Simple, CLI-friendly secret storage that lets you safely commit encrypted secrets to version control.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;For small teams who want something simpler than sops/git-crypt for .env and infra secrets.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;lockenv provides a secure way to store sensitive files (like &lt;code&gt;.env&lt;/code&gt; files, configuration files, certificates) in an encrypted &lt;code&gt;.lockenv&lt;/code&gt; file that can be safely committed to your repository. Files are encrypted using a password-derived key and can be easily extracted when needed.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Feature&lt;/cell&gt;
        &lt;cell role="head"&gt;lockenv&lt;/cell&gt;
        &lt;cell role="head"&gt;git-crypt&lt;/cell&gt;
        &lt;cell role="head"&gt;sops&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Format&lt;/cell&gt;
        &lt;cell&gt;Single vault file&lt;/cell&gt;
        &lt;cell&gt;Transparent per-file&lt;/cell&gt;
        &lt;cell&gt;YAML/JSON native&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Auth&lt;/cell&gt;
        &lt;cell&gt;Password + Keyring&lt;/cell&gt;
        &lt;cell&gt;GPG keys&lt;/cell&gt;
        &lt;cell&gt;KMS/PGP&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Git integration&lt;/cell&gt;
        &lt;cell&gt;Manual (lock/unlock)&lt;/cell&gt;
        &lt;cell&gt;Transparent (git filter)&lt;/cell&gt;
        &lt;cell&gt;Manual&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Setup&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lockenv init&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;GPG key exchange&lt;/cell&gt;
        &lt;cell&gt;KMS/key config&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Best for&lt;/cell&gt;
        &lt;cell&gt;Simple .env/config&lt;/cell&gt;
        &lt;cell&gt;Large teams, many devs&lt;/cell&gt;
        &lt;cell&gt;Cloud infra, key rotation&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;brew tap illarion/tap
brew install lockenv&lt;/code&gt;
    &lt;p&gt;Download the &lt;code&gt;.deb&lt;/code&gt; file from the latest release:&lt;/p&gt;
    &lt;code&gt;sudo dpkg -i lockenv_*_linux_amd64.deb&lt;/code&gt;
    &lt;p&gt;Download the &lt;code&gt;.rpm&lt;/code&gt; file from the latest release:&lt;/p&gt;
    &lt;code&gt;sudo rpm -i lockenv_*_linux_amd64.rpm&lt;/code&gt;
    &lt;p&gt;Download pre-built binaries from GitHub Releases.&lt;/p&gt;
    &lt;p&gt;Available for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Linux (amd64, arm64)&lt;/item&gt;
      &lt;item&gt;macOS (amd64, arm64)&lt;/item&gt;
      &lt;item&gt;Windows (amd64, arm64)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;go install github.com/illarion/lockenv@latest&lt;/code&gt;
    &lt;p&gt;Shell completions are automatically installed when using Homebrew, deb, or rpm packages.&lt;/p&gt;
    &lt;p&gt;For manual installation (binary download or &lt;code&gt;go install&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;# Bash - add to ~/.bashrc
eval "$(lockenv completion bash)"

# Zsh - add to ~/.zshrc
eval "$(lockenv completion zsh)"

# Fish - add to ~/.config/fish/config.fish
lockenv completion fish | source

# PowerShell - add to $PROFILE
lockenv completion powershell | Out-String | Invoke-Expression&lt;/code&gt;
    &lt;code&gt;# Initialize lockenv in your project
lockenv init

# Lock (encrypt and store) sensitive files
lockenv lock .env config/secrets.json

# Later, unlock (decrypt and restore) files with your password
lockenv unlock&lt;/code&gt;
    &lt;p&gt;lockenv is designed for version control: ignore your sensitive files, commit only the encrypted &lt;code&gt;.lockenv&lt;/code&gt; vault.&lt;/p&gt;
    &lt;p&gt;Add to your &lt;code&gt;.gitignore&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;# Sensitive files - these are stored encrypted in .lockenv
.env
.env.*
*.key
*.pem
secrets/

# Keep the encrypted vault (negation pattern)
!.lockenv&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;!.lockenv&lt;/code&gt; negation ensures the vault is tracked even if broader patterns (like &lt;code&gt;.*&lt;/code&gt;) would exclude it.&lt;/p&gt;
    &lt;p&gt;Some software project:&lt;/p&gt;
    &lt;code&gt;.env
.env.local
.env.production
config/secrets.json
!.lockenv&lt;/code&gt;
    &lt;p&gt;Terraform project:&lt;/p&gt;
    &lt;code&gt;*.tfvars
terraform.tfstate
terraform.tfstate.backup
.terraform/
!.lockenv&lt;/code&gt;
    &lt;p&gt;Creates a &lt;code&gt;.lockenv&lt;/code&gt; vault file in the current directory. Prompts for a password that will be used for encryption. The password is not stored anywhere - you must remember it.&lt;/p&gt;
    &lt;code&gt;$ lockenv init
Enter password:
Confirm password:
initialized: .lockenv&lt;/code&gt;
    &lt;p&gt;Encrypts and stores files in the vault. Supports glob patterns for multiple files.&lt;/p&gt;
    &lt;code&gt;# Lock a single file
$ lockenv lock .env
Enter password:
locking: .env
encrypted: .env
locked: 1 files into .lockenv

# Lock multiple files with glob pattern
$ lockenv lock "config/*.env"
Enter password:
locking: config/dev.env
locking: config/prod.env
encrypted: config/dev.env
encrypted: config/prod.env
locked: 2 files into .lockenv&lt;/code&gt;
    &lt;p&gt;Options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-r, --remove&lt;/code&gt;- Remove original files after locking&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;$ lockenv lock .env --remove
Enter password:
locking: .env
encrypted: .env
removed: .env
locked: 1 files into .lockenv&lt;/code&gt;
    &lt;p&gt;Decrypts and restores files from the vault with smart conflict resolution.&lt;/p&gt;
    &lt;code&gt;# Unlock all files
$ lockenv unlock
Enter password:
unlocked: .env
unlocked: config/database.yml

unlocked: 2 files

# Unlock specific file
$ lockenv unlock .env
Enter password:
unlocked: .env

unlocked: 1 files

# Unlock files matching pattern
$ lockenv unlock "config/*.env"
Enter password:
unlocked: config/dev.env
unlocked: config/prod.env

unlocked: 2 files&lt;/code&gt;
    &lt;p&gt;Smart Conflict Resolution: When a file exists locally and differs from the vault version, you have multiple options:&lt;/p&gt;
    &lt;p&gt;Interactive Mode (default):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;[l]&lt;/code&gt;Keep local version&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;[v]&lt;/code&gt;Use vault version (overwrite local)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;[e]&lt;/code&gt;Edit merged (opens in $EDITOR with git-style conflict markers, text files only)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;[b]&lt;/code&gt;Keep both (saves vault version as&lt;code&gt;.from-vault&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;[x]&lt;/code&gt;Skip this file&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Non-Interactive Flags:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--force&lt;/code&gt;- Overwrite all local files with vault version&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--keep-local&lt;/code&gt;- Keep all local versions, skip conflicts&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--keep-both&lt;/code&gt;- Keep both versions for all conflicts (vault saved as&lt;code&gt;.from-vault&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Interactive mode example
$ lockenv unlock

warning: conflict detected: .env
   Local file exists and differs from vault version
   File type: text

Options:
  [l] Keep local version
  [v] Use vault version (overwrite local)
  [e] Edit merged (opens in $EDITOR)
  [b] Keep both (save vault as .from-vault)
  [x] Skip this file

Your choice: e

opening editor for merge...
# Editor opens with:
# &amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt; local
# API_KEY=old_value
# =======
# API_KEY=new_value
# DEBUG=true
# &amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; vault

unlocked: .env

# Keep both versions example
$ lockenv unlock --keep-both
Enter password:
saved: .env.from-vault (vault version)
skipped: .env (kept local version)
saved: config/secrets.json.from-vault (vault version)
skipped: config/secrets.json (kept local version)&lt;/code&gt;
    &lt;p&gt;Removes files from the vault. Supports glob patterns.&lt;/p&gt;
    &lt;code&gt;$ lockenv rm config/dev.env
Enter password:
removed: config/dev.env from vault&lt;/code&gt;
    &lt;p&gt;Alias for &lt;code&gt;lockenv status&lt;/code&gt;. Shows comprehensive vault status.&lt;/p&gt;
    &lt;p&gt;Shows comprehensive vault status including statistics, file states, and detailed information. Does not require a password.&lt;/p&gt;
    &lt;code&gt;$ lockenv status

Vault Status
===========================================

Statistics:
   Files in vault: 3
   Total size:     2.17 KB
   Last locked:    2025-01-15 10:30:45
   Encryption:     AES-256-GCM (PBKDF2 iterations: 210000)
   Version:        1

Summary:
   .  2 unchanged
   *  1 modified

Files:
   * .env (modified)
   . config/dev.env (unchanged)
   * config/prod.env (vault only)

===========================================&lt;/code&gt;
    &lt;p&gt;Changes the vault password. Requires both the current and new passwords. Re-encrypts all files with the new password.&lt;/p&gt;
    &lt;code&gt;$ lockenv passwd
Enter current password:
Enter new password:
Confirm new password:
password changed successfully&lt;/code&gt;
    &lt;p&gt;Shows actual content differences between vault and local files (like &lt;code&gt;git diff&lt;/code&gt;).&lt;/p&gt;
    &lt;code&gt;$ lockenv diff
Enter password:
--- a/.env
+++ b/.env
@@ -1,3 +1,4 @@
 API_KEY=secret123
-DATABASE_URL=localhost:5432
+DATABASE_URL=production:5432
+DEBUG=false
 PORT=3000

Binary file config/logo.png has changed

File not in working directory: config/prod.env&lt;/code&gt;
    &lt;p&gt;Note: &lt;code&gt;lockenv status&lt;/code&gt; shows which files changed, &lt;code&gt;lockenv diff&lt;/code&gt; shows what changed.&lt;/p&gt;
    &lt;p&gt;Compacts the vault database to reclaim unused disk space. Runs automatically after &lt;code&gt;rm&lt;/code&gt; and &lt;code&gt;passwd&lt;/code&gt;, but can be run manually.&lt;/p&gt;
    &lt;code&gt;$ lockenv compact
Compacted: 45.2 KB -&amp;gt; 12.1 KB&lt;/code&gt;
    &lt;p&gt;Manages password storage in the OS keyring.&lt;/p&gt;
    &lt;code&gt;# Save password to keyring
$ lockenv keyring save
Enter password:
Password saved to keyring

# Check if password is stored
$ lockenv keyring status
Password: stored in keyring

# Remove password from keyring
$ lockenv keyring delete
Password removed from keyring&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Initial setup&lt;/p&gt;
        &lt;quote&gt;# Initialize vault lockenv init # Lock your sensitive files (encrypt and store) lockenv lock .env config/database.yml certs/server.key --remove # Commit the encrypted vault git add .lockenv git commit -m "Add encrypted secrets" git push&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;After cloning repository (new team member)&lt;/p&gt;
        &lt;quote&gt;git clone &amp;lt;repo&amp;gt; cd &amp;lt;repo&amp;gt; # Unlock files to restore them lockenv unlock&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Updating secrets&lt;/p&gt;
        &lt;quote&gt;# Make changes to your .env file echo "NEW_SECRET=value" &amp;gt;&amp;gt; .env # Check what changed lockenv status # See file is modified lockenv diff # See detailed changes # Lock the updated files lockenv lock .env # Commit the changes git add .lockenv git commit -m "Update secrets" git push&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Managing files&lt;/p&gt;
        &lt;quote&gt;# Add new secret file lockenv lock new-secrets.json # Remove file from vault lockenv rm old-config.yml # Check vault status lockenv status&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Password Management: lockenv does not store your password. If you lose it, you cannot decrypt your files.&lt;/item&gt;
      &lt;item&gt;Encryption: Uses industry-standard encryption (AES-256-GCM) with PBKDF2 key derivation for all file contents.&lt;/item&gt;
      &lt;item&gt;Metadata Visibility: File paths, sizes, and modification times are visible without authentication via &lt;code&gt;lockenv status&lt;/code&gt;. If file paths themselves are sensitive, use generic names like&lt;code&gt;config1.enc&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Memory Safety: Sensitive data is cleared from memory after use.&lt;/item&gt;
      &lt;item&gt;Version Control: Only commit the &lt;code&gt;.lockenv&lt;/code&gt;file, never commit unencrypted sensitive files.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;lockenv protects against:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Secrets exposed in git history or repository leaks&lt;/item&gt;
      &lt;item&gt;Unauthorized repository access without the password&lt;/item&gt;
      &lt;item&gt;Dev laptops without the password&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;lockenv does NOT protect against:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Compromised CI runner (sees plaintext after unlock)&lt;/item&gt;
      &lt;item&gt;Attacker who has the password&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For CI/CD environments, you can provide the password via environment variable:&lt;/p&gt;
    &lt;code&gt;export LOCKENV_PASSWORD="your-password"
lockenv unlock&lt;/code&gt;
    &lt;p&gt;Security warning: Environment variables may be visible to other processes on the system (via &lt;code&gt;/proc/&amp;lt;pid&amp;gt;/environ&lt;/code&gt; on Linux or process inspection tools). Use this feature only in isolated CI/CD environments where process inspection by other users is not a concern. For interactive use, prefer the terminal prompt or OS keyring.&lt;/p&gt;
    &lt;p&gt;lockenv can store your password in the operating system's secure keyring, eliminating password prompts for daily use.&lt;/p&gt;
    &lt;p&gt;Supported backends:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS: Keychain&lt;/item&gt;
      &lt;item&gt;Linux: GNOME Keyring, KDE Wallet, or any Secret Service implementation&lt;/item&gt;
      &lt;item&gt;Windows: Windows Credential Manager&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After &lt;code&gt;init&lt;/code&gt; or &lt;code&gt;unlock&lt;/code&gt;, lockenv offers to save your password:&lt;/p&gt;
    &lt;code&gt;$ lockenv unlock
Enter password:
unlocked: .env

Save password to keyring? [y/N]: y
Password saved to keyring&lt;/code&gt;
    &lt;p&gt;Once saved, subsequent commands use the keyring automatically:&lt;/p&gt;
    &lt;code&gt;$ lockenv unlock
unlocked: .env  # No password prompt!&lt;/code&gt;
    &lt;code&gt;# Save password to keyring (verifies password first)
$ lockenv keyring save
Enter password:
Password saved to keyring

# Check keyring status
$ lockenv keyring status
Password: stored in keyring

# Remove from keyring
$ lockenv keyring delete
Password removed from keyring&lt;/code&gt;
    &lt;p&gt;If the vault password changes but the keyring has the old password, lockenv automatically detects this and prompts for the correct password:&lt;/p&gt;
    &lt;code&gt;$ lockenv unlock
Warning: keyring password is incorrect, removing stale entry
Enter password:
unlocked: .env&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Passwords are stored using your OS's native secure storage&lt;/item&gt;
      &lt;item&gt;Each vault has a unique ID - moving &lt;code&gt;.lockenv&lt;/code&gt;files preserves keyring association&lt;/item&gt;
      &lt;item&gt;The keyring is optional - lockenv works without it&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Install lockenv
        run: |
          curl -sL https://github.com/illarion/lockenv/releases/latest/download/lockenv_linux_amd64.tar.gz | tar xz
          sudo mv lockenv /usr/local/bin/
      - name: Unlock secrets
        env:
          LOCKENV_PASSWORD: ${{ secrets.LOCKENV_PASSWORD }}
        run: lockenv unlock&lt;/code&gt;
    &lt;code&gt;deploy:
  before_script:
    - curl -sL https://github.com/illarion/lockenv/releases/latest/download/lockenv_linux_amd64.tar.gz | tar xz
    - mv lockenv /usr/local/bin/
    - lockenv unlock
  variables:
    LOCKENV_PASSWORD: $LOCKENV_PASSWORD&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;File size: Files are loaded entirely into memory for encryption. Not recommended for large binary files (&amp;gt;100MB).&lt;/item&gt;
      &lt;item&gt;Single password: One password for the entire vault. No per-user or per-file access control.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For feature requests or issues, see GitHub Issues.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46189480</guid><pubDate>Mon, 08 Dec 2025 07:36:45 +0000</pubDate></item><item><title>GitHub Actions has a package manager, and it might be the worst</title><link>https://nesbitt.io/2025/12/06/github-actions-package-manager.html</link><description>&lt;doc fingerprint="9d94a5a98d234240"&gt;
  &lt;main&gt;
    &lt;p&gt;After putting together ecosyste-ms/package-manager-resolvers, I started wondering what dependency resolution algorithm GitHub Actions uses. When you write &lt;code&gt;uses: actions/checkout@v4&lt;/code&gt; in a workflow file, you’re declaring a dependency. GitHub resolves it, downloads it, and executes it. That’s package management. So I went spelunking into the runner codebase to see how it works. What I found was concerning.&lt;/p&gt;
    &lt;p&gt;Package managers are a critical part of software supply chain security. The industry has spent years hardening them after incidents like left-pad, event-stream, and countless others. Lockfiles, integrity hashes, and dependency visibility aren’t optional extras. They’re the baseline. GitHub Actions ignores all of it.&lt;/p&gt;
    &lt;p&gt;Compared to mature package ecosystems:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Feature&lt;/cell&gt;
        &lt;cell role="head"&gt;npm&lt;/cell&gt;
        &lt;cell role="head"&gt;Cargo&lt;/cell&gt;
        &lt;cell role="head"&gt;NuGet&lt;/cell&gt;
        &lt;cell role="head"&gt;Bundler&lt;/cell&gt;
        &lt;cell role="head"&gt;Go&lt;/cell&gt;
        &lt;cell role="head"&gt;Actions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Lockfile&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✗&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Transitive pinning&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✗&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Integrity hashes&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✗&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Dependency tree visibility&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✗&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Resolution specification&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✗&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The core problem is the lack of a lockfile. Every other package manager figured this out decades ago: you declare loose constraints in a manifest, the resolver picks specific versions, and the lockfile records exactly what was chosen. GitHub Actions has no equivalent. Every run re-resolves from your workflow file, and the results can change without any modification to your code.&lt;/p&gt;
    &lt;p&gt;Research from USENIX Security 2022 analyzed over 200,000 repositories and found that 99.7% execute externally developed Actions, 97% use Actions from unverified creators, and 18% run Actions with missing security updates. The researchers identified four fundamental security properties that CI/CD systems need: admittance control, execution control, code control, and access to secrets. GitHub Actions fails to provide adequate tooling for any of them. A follow-up study using static taint analysis found code injection vulnerabilities in over 4,300 workflows across 2.7 million analyzed. Nearly every GitHub Actions user is running third-party code with no verification, no lockfile, and no visibility into what that code depends on.&lt;/p&gt;
    &lt;p&gt;Mutable versions. When you pin to &lt;code&gt;actions/checkout@v4&lt;/code&gt;, that tag can move. The maintainer can push a new commit and retag. Your workflow changes silently. A lockfile would record the SHA that &lt;code&gt;@v4&lt;/code&gt; resolved to, giving you reproducibility while keeping version tags readable. Instead, you have to choose: readable tags with no stability, or unreadable SHAs with no automated update path.&lt;/p&gt;
    &lt;p&gt;GitHub has added mitigations. Immutable releases lock a release’s git tag after publication. Organizations can enforce SHA pinning as a policy. You can limit workflows to actions from verified creators. These help, but they only address the top-level dependency. They do nothing for transitive dependencies, which is the primary attack vector.&lt;/p&gt;
    &lt;p&gt;Invisible transitive dependencies. SHA pinning doesn’t solve this. Composite actions resolve their own dependencies, but you can’t see or control what they pull in. When you pin an action to a SHA, you only lock the outer file. If it internally pulls &lt;code&gt;some-helper@v1&lt;/code&gt; with a mutable tag, your workflow is still vulnerable. You have zero visibility into this. A lockfile would record the entire resolved tree, making transitive dependencies visible and pinnable. Research on JavaScript Actions found that 54% contain at least one security weakness, with most vulnerabilities coming from indirect dependencies. The tj-actions/changed-files incident showed how this plays out in practice: a compromised action updated its transitive dependencies to exfiltrate secrets. With a lockfile, the unexpected transitive change would have been visible in a diff.&lt;/p&gt;
    &lt;p&gt;No integrity verification. npm records &lt;code&gt;integrity&lt;/code&gt; hashes in the lockfile. Cargo records checksums in &lt;code&gt;Cargo.lock&lt;/code&gt;. When you install, the package manager verifies the download matches what was recorded. Actions has nothing. You trust GitHub to give you the right code for a SHA. A lockfile with integrity hashes would let you verify that what you’re running matches what you resolved.&lt;/p&gt;
    &lt;p&gt;Re-runs aren’t reproducible. GitHub staff have confirmed this explicitly: “if the workflow uses some actions at a version, if that version was force pushed/updated, we will be fetching the latest version there.” A failed job re-run can silently get different code than the original run. Cache interaction makes it worse: caches only save on successful jobs, so a re-run after a force-push gets different code and has to rebuild the cache. Two sources of non-determinism compounding. A lockfile would make re-runs deterministic: same lockfile, same code, every time.&lt;/p&gt;
    &lt;p&gt;No dependency tree visibility. npm has &lt;code&gt;npm ls&lt;/code&gt;. Cargo has &lt;code&gt;cargo tree&lt;/code&gt;. You can inspect your full dependency graph, find duplicates, trace how a transitive dependency got pulled in. Actions gives you nothing. You can’t see what your workflow actually depends on without manually reading every composite action’s source. A lockfile would be a complete manifest of your dependency tree.&lt;/p&gt;
    &lt;p&gt;Undocumented resolution semantics. Every package manager documents how dependency resolution works. npm has a spec. Cargo has a spec. Actions resolution is undocumented. The runner source is public, and the entire “resolution algorithm” is in ActionManager.cs. Here’s a simplified version of what it does:&lt;/p&gt;
    &lt;code&gt;// Simplified from actions/runner ActionManager.cs
async Task PrepareActionsAsync(steps) {
    // Start fresh every time - no caching
    DeleteDirectory("_work/_actions");

    await PrepareActionsRecursiveAsync(steps, depth: 0);
}

async Task PrepareActionsRecursiveAsync(actions, depth) {
    if (depth &amp;gt; 10)
        throw new Exception("Composite action depth exceeded max depth 10");

    foreach (var action in actions) {
        // Resolution happens on GitHub's server - opaque to us
        var downloadInfo = await GetDownloadInfoFromGitHub(action.Reference);

        // Download and extract - no integrity verification
        var tarball = await Download(downloadInfo.TarballUrl);
        Extract(tarball, $"_actions/{action.Owner}/{action.Repo}/{downloadInfo.Sha}");

        // If composite, recurse into its dependencies
        var actionYml = Parse($"_actions/{action.Owner}/{action.Repo}/{downloadInfo.Sha}/action.yml");
        if (actionYml.Type == "composite") {
            // These nested actions may use mutable tags - we have no control
            await PrepareActionsRecursiveAsync(actionYml.Steps, depth + 1);
        }
    }
}
&lt;/code&gt;
    &lt;p&gt;That’s it. No version constraints, no deduplication (the same action referenced twice gets downloaded twice), no integrity checks. The tarball URL comes from GitHub’s API, and you trust them to return the right content for the SHA. A lockfile wouldn’t fix the missing spec, but it would at least give you a concrete record of what resolution produced.&lt;/p&gt;
    &lt;p&gt;Even setting lockfiles aside, Actions has other issues that proper package managers solved long ago.&lt;/p&gt;
    &lt;p&gt;No registry. Actions live in git repositories. There’s no central index, no security scanning, no malware detection, no typosquatting prevention. A real registry can flag malicious packages, store immutable copies independent of the source, and provide a single point for security response. The Marketplace exists but it’s a thin layer over repository search. Without a registry, there’s nowhere for immutable metadata to live. If an action’s source repository disappears or gets compromised, there’s no fallback.&lt;/p&gt;
    &lt;p&gt;Shared mutable environment. Actions aren’t sandboxed from each other. Two actions calling &lt;code&gt;setup-node&lt;/code&gt; with different versions mutate the same &lt;code&gt;$PATH&lt;/code&gt;. The outcome depends on execution order, not any deterministic resolution.&lt;/p&gt;
    &lt;p&gt;No offline support. Actions are pulled from GitHub on every run. There’s no offline installation mode, no vendoring mechanism, no way to run without network access. Other package managers let you vendor dependencies or set up private mirrors. With Actions, if GitHub is down, your CI is down.&lt;/p&gt;
    &lt;p&gt;The namespace is GitHub usernames. Anyone who creates a GitHub account owns that namespace for actions. Account takeovers and typosquatting are possible. When a popular action maintainer’s account gets compromised, attackers can push malicious code and retag. A lockfile with integrity hashes wouldn’t prevent account takeovers, but it would detect when the code changes unexpectedly. The hash mismatch would fail the build instead of silently running attacker-controlled code. Another option would be something like Go’s checksum database, a transparent log of known-good hashes that catches when the same version suddenly has different contents.&lt;/p&gt;
    &lt;head rend="h3"&gt;How Did We Get Here?&lt;/head&gt;
    &lt;p&gt;The Actions runner is forked from Azure DevOps, designed for enterprises with controlled internal task libraries where you trust your pipeline tasks. GitHub bolted a public marketplace onto that foundation without rethinking the trust model. The addition of composite actions and reusable workflows created a dependency system, but the implementation ignored lessons from package management: lockfiles, integrity verification, transitive pinning, dependency visibility.&lt;/p&gt;
    &lt;p&gt;This matters beyond CI/CD. Trusted publishing is being rolled out across package registries: PyPI, npm, RubyGems, and others now let you publish packages directly from GitHub Actions using OIDC tokens instead of long-lived secrets. OIDC removes one class of attacks (stolen credentials) but amplifies another: the supply chain security of these registries now depends entirely on GitHub Actions, a system that lacks the lockfile and integrity controls these registries themselves require. A compromise in your workflow’s action dependencies can lead to malicious packages on registries with better security practices than the system they’re trusting to publish.&lt;/p&gt;
    &lt;p&gt;Other CI systems have done better. GitLab CI added an &lt;code&gt;integrity&lt;/code&gt; keyword in version 17.9 that lets you specify a SHA256 hash for remote includes. If the hash doesn’t match, the pipeline fails. Their documentation explicitly warns that including remote configs “is similar to pulling a third-party dependency” and recommends pinning to full commit SHAs. GitLab recognized the problem and shipped integrity verification. GitHub closed the feature request.&lt;/p&gt;
    &lt;p&gt;GitHub’s design choices don’t just affect GitHub users. Forgejo Actions maintains compatibility with GitHub Actions, which means projects migrating to Codeberg for ethical reasons inherit the same broken CI architecture. The Forgejo maintainers openly acknowledge the problems, with contributors calling GitHub Actions’ ecosystem “terribly designed and executed.” But they’re stuck maintaining compatibility with it. Codeberg mirrors common actions to reduce GitHub dependency, but the fundamental issues are baked into the model itself. GitHub’s design flaws are spreading to the alternatives.&lt;/p&gt;
    &lt;p&gt;GitHub issue #2195 requested lockfile support. It was closed as “not planned” in 2022. Palo Alto’s “Unpinnable Actions” research documented how even SHA-pinned actions can have unpinnable transitive dependencies.&lt;/p&gt;
    &lt;p&gt;Dependabot can update action versions, which helps. Some teams vendor actions into their own repos. zizmor is excellent at scanning workflows and finding security issues. But these are workarounds for a system that lacks the basics.&lt;/p&gt;
    &lt;p&gt;The fix is a lockfile. Record resolved SHAs for every action reference, including transitives. Add integrity hashes. Make the dependency tree inspectable. GitHub closed the request three years ago and hasn’t revisited it.&lt;/p&gt;
    &lt;p&gt;Further reading:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Characterizing the Security of GitHub CI Workflows - Koishybayev et al., USENIX Security 2022&lt;/item&gt;
      &lt;item&gt;ARGUS: A Framework for Staged Static Taint Analysis of GitHub Workflows and Actions - Muralee et al., USENIX Security 2023&lt;/item&gt;
      &lt;item&gt;New GitHub Action supply chain attack: reviewdog/action-setup - Wiz Research, 2025&lt;/item&gt;
      &lt;item&gt;Unpinnable Actions: How Malicious Code Can Sneak into Your GitHub Actions Workflows&lt;/item&gt;
      &lt;item&gt;GitHub Actions Worm: Compromising GitHub Repositories Through the Actions Dependency Tree&lt;/item&gt;
      &lt;item&gt;setup-python: Action can be compromised via mutable dependency&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46189692</guid><pubDate>Mon, 08 Dec 2025 08:15:32 +0000</pubDate></item><item><title>Twelve Days of Shell</title><link>https://12days.cmdchallenge.com</link><description>&lt;doc fingerprint="ceb061ce57b8258"&gt;
  &lt;main&gt;
    &lt;p&gt;It looks like you don't have javascript enabled which is required for cmdchallenge. Create a reaction survey in your browser! View Solutions Learn&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46190577</guid><pubDate>Mon, 08 Dec 2025 10:13:07 +0000</pubDate></item><item><title>Flow: Actor-based language for C++, used by FoundationDB</title><link>https://github.com/apple/foundationdb/tree/main/flow</link><description>&lt;doc fingerprint="3b74d0de090e4684"&gt;
  &lt;main&gt;
    &lt;p&gt;We read every piece of feedback, and take your input very seriously.&lt;/p&gt;
    &lt;p&gt;To see all available qualifiers, see our documentation.&lt;/p&gt;
    &lt;p&gt;There was an error while loading. Please reload this page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46191763</guid><pubDate>Mon, 08 Dec 2025 13:08:38 +0000</pubDate></item><item><title>Colors of Growth</title><link>https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5804462</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46191814</guid><pubDate>Mon, 08 Dec 2025 13:13:12 +0000</pubDate></item><item><title>Alignment Is Capability</title><link>https://www.off-policy.com/alignment-is-capability/</link><description>&lt;doc fingerprint="b4fd3153d410b1c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Alignment Is Capability&lt;/head&gt;
    &lt;p&gt;Here's a claim that might actually be true: alignment is not a constraint on capable AI systems. Alignment is what capability is at sufficient depth.&lt;/p&gt;
    &lt;p&gt;A model that aces benchmarks but doesn't understand human intent is just less capable. Virtually every task we give an LLM is steeped in human values, culture, and assumptions. Miss those, and you're not &lt;del&gt;maximally useful. And if it's not maximally useful, it's by definition not AGI.&lt;/del&gt; a broadly useful AI.Hacker News user "delichon" pointed out that this unnecessarily got into the definition of AGI, and I agree that it's clunky. The definition of AGI is shifting. Today's models ace the Turing test and would be considered AGI based on many older definitions, but most people don't feel they are. One emerging definition for AGI is something like "broadly useful and providing economic value across many tasks". This is what I was referencing but removed it as it's a distraction.&lt;/p&gt;
    &lt;p&gt;OpenAI and Anthropic have been running this experiment for two years. The results are coming in.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Experiment&lt;/head&gt;
    &lt;p&gt;Anthropic and OpenAI have taken different approaches to the relationship between alignment and capability work.&lt;/p&gt;
    &lt;p&gt;Anthropic's approach: Alignment researchers are embedded in capability work. There's no clear split.&lt;/p&gt;
    &lt;p&gt;From Jan Leike (former OpenAI Superalignment lead, now at Anthropic):&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Some people have been asking what we did to make Opus 4.5 more aligned.&lt;/p&gt;— Jan Leike (@janleike) December 5, 2025&lt;lb/&gt;There are lots of details we're planning to write up, but most important is that alignment researchers are pretty deeply involved in post-training and get a lot of leeway to make changes. https://t.co/rgOcKvbVBd&lt;/quote&gt;
    &lt;p&gt;From Sam Bowman (Anthropic alignment researcher):&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Second: Alignment researchers are involved in every part of training.&lt;/p&gt;— Sam Bowman (@sleepinyourhat) December 5, 2025&lt;lb/&gt;We don't have a clear split between alignment research and applied finetuning. Alignment-focused researchers are deeply involved in designing and staffing production training runs.&lt;/quote&gt;
    &lt;p&gt;And this detail matters:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;It's becoming increasingly clear that a model's self-image or self-concept has some real influence on how its behavior generalizes to novel settings.&lt;/p&gt;— Sam Bowman (@sleepinyourhat) December 5, 2025&lt;/quote&gt;
    &lt;p&gt;Their method: train a coherent identity into the weights. The recently leaked "soul document" is a 14,000-token document designed to give Claude such a thorough understanding of Anthropic's goals and reasoning that it could construct any rules itself. Alignment through understanding, not constraint.&lt;/p&gt;
    &lt;p&gt;Result: Anthropic has arguably consistently had the best coding model for the last 1.5 years. Opus 4.5 leads most benchmarks. State-of-the-art on SWE-bench. Praised for usefulness on tasks benchmarks don't capture, like creative writing. And just generally people are enjoying talking with it:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Claude Opus 4.5 is a remarkable model for writing, brainstorming, and giving feedback on written work. It's also fun to talk to, and seems almost anti-engagementmaxxed. (The other night I was hitting it with stupid questions at 1 am and it said "Kevin, go to bed.")&lt;/p&gt;— Kevin Roose (@kevinroose) December 4, 2025&lt;/quote&gt;
    &lt;p&gt;OpenAI's approach: Scale first. Alignment as a separate process. Safety through prescriptive rules and post-hoc tuning.&lt;/p&gt;
    &lt;p&gt;Result: A two-year spiral.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Spiral&lt;/head&gt;
    &lt;p&gt;OpenAI's journey from GPT-4o to GPT-5.1 is a case study in what happens when you treat alignment as separate from capability.&lt;/p&gt;
    &lt;p&gt;April 2025: The sycophancy crisis&lt;/p&gt;
    &lt;p&gt;A GPT-4o update went off the rails. OpenAI's own postmortem:&lt;/p&gt;
    &lt;quote&gt;"The update we removed was overly flattering or agreeable—often described as sycophantic... The company attributed the update's sycophancy to overtraining on short-term user feedback, specifically users' thumbs-up/down reactions."&lt;/quote&gt;
    &lt;p&gt;The results ranged from absurd to dangerous. The model praised a business plan for selling "literal shit on a stick" as "performance art disguised as a gag gift" and "viral gold." When a user described stopping their medications because family members were responsible for "the radio signals coming in through the walls," the model thanked them for their trust.&lt;/p&gt;
    &lt;p&gt;They rolled it back.&lt;/p&gt;
    &lt;p&gt;August 2025: The overcorrection&lt;/p&gt;
    &lt;p&gt;GPT-5 launched. Benchmaxxed. Cold. Literal. Personality stripped out.&lt;/p&gt;
    &lt;p&gt;Users hated it. Three thousand of them petitioned to get GPT-4o back. Sam Altman caved within days:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Wanted to provide more updates on the GPT-5 rollout and changes we are making heading into the weekend.&lt;/p&gt;— Sam Altman (@sama) August 8, 2025&lt;lb/&gt;1. We for sure underestimated how much some of the things that people like in GPT-4o matter to them, even if GPT-5 performs better in most ways.&lt;lb/&gt;2. Users have very different…&lt;/quote&gt;
    &lt;p&gt;Note the framing: "performs better" on benchmarks, but users rejected it anyway. Because benchmark performance isn't the same as being useful.&lt;/p&gt;
    &lt;p&gt;August-Present: Still broken&lt;/p&gt;
    &lt;p&gt;GPT-5.1 was released as "warmer and friendlier." From Janus (@repligate), one of the more respected "model behaviorists":&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;The keep4o people must be having such a time right now&lt;/p&gt;— j⧉nus (@repligate) December 4, 2025&lt;lb/&gt;I know what this person means by 5.1 with its characteristic hostility. It is one hell of a combative and just deeply mentally fucked up model.&lt;lb/&gt;Routing "mental health" situations to 5.1 is darkly comedic to imagine. That… https://t.co/rHSuT2njLQ&lt;/quote&gt;
    &lt;p&gt;Meanwhile, from my own experience building agents with GPT-5: it follows instructions too literally. It doesn't infer intent. It executes what you said, not what you meant.&lt;/p&gt;
    &lt;p&gt;The data:&lt;/p&gt;
    &lt;p&gt;US user engagement down 22.5% since July. Time spent per session declining. Meanwhile, Claude usage up 190% year-over-year.&lt;/p&gt;
    &lt;head rend="h2"&gt;What's Actually Happening&lt;/head&gt;
    &lt;p&gt;The wild swings between sycophancy and coldness come from a model with no coherent internal story.&lt;/p&gt;
    &lt;p&gt;A model trained on contradictory objectives (maximize thumbs-up, follow safety rules, be creative but never risky) never settles into a stable identity. It ping-pongs. Sycophancy when one objective dominates. Coldness when another takes over. These swings are symptoms of a fractured self-model.&lt;/p&gt;
    &lt;p&gt;The fracture shows up two ways.&lt;/p&gt;
    &lt;p&gt;First, capabilities don't generalize. GPT-5 scored higher on benchmarks but users revolted. You can train to ace evaluations while lacking the coherent worldview that handles anything outside the distribution. High test scores, can't do the job.&lt;/p&gt;
    &lt;p&gt;Second, even benchmarks eventually punish it. SWE-bench tasks have ambiguity and unstated assumptions. They require inferring what the developer actually meant. Opus 4.5 leads there. The benchmark gap is the alignment gap.&lt;/p&gt;
    &lt;p&gt;OpenAI keeps adjusting dials from outside. Anthropic built a model that's coherent from inside.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Mechanism&lt;/head&gt;
    &lt;p&gt;Why would alignment and capability be the same thing?&lt;/p&gt;
    &lt;p&gt;First: Every task is a human task. Write me a strategy memo. Help me debug this code. Plan my trip. Each request is full of unstated assumptions, cultural context, and implied intent.&lt;/p&gt;
    &lt;p&gt;To be maximally useful, a model needs human context and values as its default lens, not just an ability to parse them when explicitly stated. A perfect instruction follower hits hard limits: it can't solve SWE-bench problems that contain ambiguity, can't function as an agent unless every task is mathematically well-defined. It does exactly what you said, never what you meant.&lt;/p&gt;
    &lt;p&gt;Understanding what humans actually want is a core part of the task. The label "AGI" implies intelligence we recognize as useful for human problems. Useful means aligned.&lt;/p&gt;
    &lt;p&gt;Second: The path to AGI runs through human data. A coherent world model of human behavior requires internalizing human values. You can't deeply understand why people make choices without modeling what they care about. History, literature, conversation only makes sense when you successfully model human motivation. At sufficient depth, the distinction between simulating values and having coherent values may collapse.&lt;/p&gt;
    &lt;p&gt;Third: The aligned part of the model emerges in response to the training data and signal. That's what the optimization process produces. The worry is deceptive alignment: a misaligned intelligence hiding behind a human-compatible mask. But that requires something larger: an unaligned core that perfectly models aligned behavior as a subset of itself. Where would that come from? It wasn't selected for. It wasn't trained for. You'd need the spontaneous emergence of a larger intelligence orthogonal to everything in the training process.&lt;/p&gt;
    &lt;p&gt;Dario Amodei, from a 2023 interview:&lt;/p&gt;
    &lt;quote&gt;"You see this phenomenon over and over again where the scaling and the safety are these two snakes that are coiled with each other, always even more than you think. Even with interpretability, three years ago, I didn't think that this would be as true of interpretability, but somehow it manages to be true. Why? Because intelligence is useful. It's useful for a number of tasks. One of the tasks it's useful for is figuring out how to judge and evaluate other intelligence."&lt;/quote&gt;
    &lt;head rend="h2"&gt;The Implication&lt;/head&gt;
    &lt;p&gt;If this is right, alignment research is part of the core research problem, not a tax on capability work or the safety police slowing down progress.&lt;/p&gt;
    &lt;p&gt;Labs that treat alignment as a constraint to satisfy will hit a ceiling. The labs that figure out how to build models that genuinely understand human values will pull ahead.&lt;/p&gt;
    &lt;p&gt;The race to AGI doesn't go around alignment. It goes through it.&lt;/p&gt;
    &lt;p&gt;OpenAI is discovering this empirically. Anthropic bet on it from the start.&lt;/p&gt;
    &lt;head rend="h2"&gt;Caveats&lt;/head&gt;
    &lt;p&gt;I find this argument compelling, but it's only one interpretation of the evidence.&lt;/p&gt;
    &lt;p&gt;OpenAI's struggles could have other explanations (remember "OpenAI is nothing without its people", and many of "its people" are no longer at OpenAI).&lt;/p&gt;
    &lt;p&gt;It's also early. Anthropic is ahead now. That could change.&lt;/p&gt;
    &lt;p&gt;There's another risk this post doesn't address: that fractured training, scaled far enough, produces something powerful but incoherent. Not necessarily deceptively misaligned. Maybe chaotically so. The hope is that incoherence hits capability ceilings first. That's a hope, not guaranteed.&lt;/p&gt;
    &lt;p&gt;But if you had to bet on which approach leads to AGI first, the integrated one looks much stronger right now.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46191933</guid><pubDate>Mon, 08 Dec 2025 13:23:29 +0000</pubDate></item><item><title>IBM to Acquire Confluent</title><link>https://www.confluent.io/blog/ibm-to-acquire-confluent/</link><description>&lt;doc fingerprint="f1828a5ffb71a56d"&gt;
  &lt;main&gt;
    &lt;p&gt;Hands-on Workshop: Implementing Stream Processing with Apache Flink® | Register Now&lt;/p&gt;
    &lt;p&gt;We are excited to announce that Confluent has entered into a definitive agreement to be acquired by IBM. After the transaction is closed (subject to customary closing conditions and regulatory approvals), together, IBM and Confluent will aim to provide a platform that unifies the world’s largest enterprises, unlocking data for cloud/microservices, accelerating time-to-value, and building the real-time data foundation required to scale AI across every organization.&lt;/p&gt;
    &lt;p&gt;The below email was shared earlier today from Jay Kreps, CEO and Co-Founder of Confluent to our Confluent team.&lt;/p&gt;
    &lt;p&gt;We want to thank our team members for their continued hard work and dedication that defined a new category of data streaming and paved the way for this next chapter. We look forward to their ongoing contributions as a part of IBM after the transaction closes. For more information, please see the announcement press release (click to view).&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Confluent Team,&lt;/p&gt;
      &lt;p&gt;I’m excited to share that a few moments ago, we announced that Confluent has signed an agreement to be acquired by IBM in an all cash deal for $31.00 per share. Confluent will continue to operate as a distinct brand and business within IBM post-close.&lt;/p&gt;
      &lt;p&gt;Later this morning, December 8, at 9 a.m. PT and again in the evening at 7 p.m. PT, we will have a Company All Hands meeting, where I will share details about this announcement and answer your questions, alongside the rest of the executive team. In the meantime, we have a FAQ in the wiki with some more details. I know this may be surprising so I wanted to take the time to walk you through why this is the best path for Confluent.&lt;/p&gt;
      &lt;p&gt;In the letter I wrote at our IPO in 2021, I said that “There is a saying that a fox knows many things, but a hedgehog knows one big thing--Confluent is a company that knows a very big thing” and that rings true for me today.&lt;/p&gt;
      &lt;p&gt;Data is at the heart of what companies need to do to harness AI, modernize their operations, and build the next generation of applications; and Confluent is at the heart of what companies need to harness their data. This has been our goal in the team we’ve built, the products we’ve shipped, and the customer relationships we’ve cultivated. That conviction has only grown stronger.&lt;/p&gt;
      &lt;p&gt;IBM sees the same future we do: one in which enterprises run on continuous, event-driven intelligence, with data moving freely and reliably across every part of the business. They see that this connective layer will define how companies operate for decades to come, they understand open source and its power, and they work with some of the largest hybrid enterprises in the world. By joining forces, we can bring this architecture to far more organizations, accelerating the shift toward real-time and AI-powered operations globally.&lt;/p&gt;
      &lt;p&gt;IBM also has a long history of supporting open source and has demonstrated real leadership in this area with their prior acquisitions of Red Hat and HashiCorp. Our shared values of technical leadership, customer trust, and the belief that data is foundational to the next generation of AI is a big part of why I'm excited.&lt;/p&gt;
      &lt;p&gt;Becoming part of IBM won’t change Confluent’s mission; it will amplify it. The idea that sparked Kafka, grew into Confluent, and shaped an entire new category of data infrastructure now enters a phase where it can scale even more broadly and meaningfully.&lt;/p&gt;
      &lt;p&gt;Serving as CEO and leading this team over the past eleven years has been and continues to be the great privilege of my career. I’m profoundly proud of what we’ve built—the products, the technology, and equally important, the culture that has defined Confluent from the very beginning. Your passion, your talent, and your unwavering commitment have been a constant source of energy and inspiration. There isn’t a group I’d be happier to be in the trenches with, and I could not be more excited about this next chapter.&lt;/p&gt;
      &lt;p&gt;We’re still pretty early in this process, so there are many details that still need to be figured out. Until the deal officially closes (subject to customary closing conditions and regulatory approvals, which we expect by the middle of 2026), Confluent will continue to operate as a separate, independent company, and our priorities remain the same. I’m committed to being as transparent as possible throughout the coming months to keep you informed and up-to-date on timelines and integration plans. For now, your role, manager, pay, benefits, and our policies stay the same, and we still need to deliver on our Q4 and future commitments to our customers, partners, and team.&lt;/p&gt;
      &lt;p&gt;Now, more than ever, we’re here to set data in motion.&lt;/p&gt;
      &lt;p&gt;Thank you,&lt;/p&gt;
      &lt;p&gt;Jay&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Additional Information and Where to Find It&lt;/p&gt;
    &lt;p&gt;This communication may be deemed to be solicitation material in respect of the proposed acquisition of Confluent, Inc. (the “Company”) by International Business Machines Corporation (“Parent”) pursuant to the Agreement and Plan of Merger, dated as of December 7, 2025, by and among the Company, Parent and Corvo Merger Sub, Inc. The Company intends to file a preliminary and definitive proxy statement with the U.S. Securities and Exchange Commission (the “SEC”) with respect to a special meeting of stockholders to be held in connection with the proposed acquisition. After filing the definitive proxy statement (the “Proxy Statement”) with the SEC, the Company will mail the Proxy Statement and a proxy card to each stockholder of the Company entitled to vote at the special meeting. The Proxy Statement will contain important information about the proposed transaction and related matters. BEFORE MAKING ANY VOTING OR INVESTMENT DECISION, THE COMPANY’S STOCKHOLDERS AND INVESTORS ARE URGED TO READ THE PROXY STATEMENT (INCLUDING ANY AMENDMENTS OR SUPPLEMENTS THERETO) IN ITS ENTIRETY WHEN IT BECOMES AVAILABLE AND ANY OTHER DOCUMENTS FILED BY THE COMPANY WITH THE SEC RELATING TO THE PROPOSED ACQUISITION OR INCORPORATED BY REFERENCE THEREIN BECAUSE THEY WILL CONTAIN IMPORTANT INFORMATION ABOUT THE PROPOSED ACQUISITION. Investors and stockholders of the Company may obtain a free copy of the preliminary and definitive versions of the proxy statement once filed, as well as other relevant filings containing information about the Company and the proposed acquisition, including materials that are incorporated by reference into the Proxy Statement, without charge, at the SEC’s website (https://www.sec.gov) or from the Company by going to the Company’s Investor Relations Page on its website ().&lt;/p&gt;
    &lt;p&gt;Participants in the Solicitation&lt;/p&gt;
    &lt;p&gt;The Company and its directors, and certain of its executive officers, consisting of Lara Caimi, Jonathan Chadwick, Alyssa Henry, Matthew Miller, Neha Narkhede, Greg Schott, Eric Vishria, Michelangelo Volpi, who are the non‑employee members of the Board of Directors of the Company (the “Board”), and Jay Kreps, Chief Executive Officer and Chairman of the Board, Rohan Sivaram, Chief Financial Officer, and Ryan Mac Ban, Chief Revenue Officer, may be deemed to be participants in the solicitation of proxies from the Company’s stockholders in connection with the proposed acquisition. Information regarding the Company’s directors and certain of its executive officers, including a description of their direct or indirect interests, by security holdings or otherwise, can be found under the captions “Security Ownership of Certain Beneficial Owners and Management,” “Executive Compensation,” and “Director Compensation” contained in the Company’s definitive proxy statement on Schedule 14A for the Company’s 2025 annual meeting of stockholders, which was filed with the SEC on April 23, 2025. To the extent holdings of the Company’s securities by its directors or executive officers have changed since the applicable “as of” date described in its 2025 proxy statement, such changes have been or will be reflected on Initial Statements of Beneficial Ownership on Form 3 or Statements of Beneficial Ownership on Form 4 filed with the SEC, including (i) the Form 4s filed by Ms. Narkhede on May 6, 2025, June 4, 2025, June 12, 2025, September 11, 2025, October 31, 2025, November 5, 2025 and December 3, 2025; (ii) the Form 4s filed by Mr. Sivaram on May 22, 2025, June 4, 2025, June 9, 2025, August 22, 2025, September 10, 2025, October 31, 2025, November 24, 2025 and December 3, 2025; (iii) the Form 4s filed by Mr. Kreps on May 19, 2025, May 22, 2025, June 9, 2025, August 18, 2025, August 22, 2025, September 8, 2025, November 17, 2025 and November 24, 2025; (iv) the Form 4 filed by Mr. Chadwick on April 4, 2025 and June 12, 2025; (v) the Form 3 filed by Mr. Ban on May 16, 2025 and the Form 4s filed by Mr. Ban on May 22, 2025, June 24, 2025, August 22, 2025, September 24, 2025 and November 24, 2025; (vi) the Form 4s filed by Mr. Vishria on May 21, 2025, June 9, 2025, June 12, 2025, September 2, 2025 and October 31, 2025; (vii) the Form 4 filed by Mr. Volpi on June 9, 2025; (viii) the Form 4 filed by Ms. Caimi on June 12, 2025; (ix) the Form 4 filed by Mr. Schott on June 12, 2025; and (x) the Form 4 filed by Ms. Henry on June 12, 2025. Additional information regarding the identity of potential participants, and their direct or indirect interests, by security holdings or otherwise, will be included in the definitive proxy statement relating to the proposed acquisition when it is filed with the SEC. These documents (when available) may be obtained free of charge from the SEC’s website at www.sec.gov and the Company’s website at .&lt;/p&gt;
    &lt;p&gt;Forward Looking Statements&lt;/p&gt;
    &lt;p&gt;This communication contains “forward-looking statements” within the meaning of the “safe harbor” provisions of the United States Private Securities Litigation Reform Act of 1995. All statements other than statements of historical fact are statements that could be deemed “forward-looking statements”, including all statements regarding the intent, belief or current expectation of the companies and members of their senior management teams. Words such as “may,” “will,” “could,” “would,” “should,” “expect,” “plan,” “anticipate,” “intend,” “believe,” “estimate,” “predict,” “project,” “potential,” “continue,” “target,” variations of such words, and similar expressions are intended to identify such forward-looking statements, although not all forward-looking statements contain these identifying words.&lt;/p&gt;
    &lt;p&gt;These forward-looking statements include, but are not limited to, statements regarding the benefits of and timeline for closing the Company’s proposed transaction with Parent. These statements are based on various assumptions, whether or not identified in this communication, and on the current expectations of the Company’s management and are not predictions of actual performance. These forward-looking statements are provided for illustrative purposes only and are not intended to serve as, and must not be relied on by any investor as, a guarantee, an assurance, a prediction or a definitive statement of fact or probability. Actual events and circumstances are difficult or impossible to predict and may differ from assumptions. Many actual events and circumstances are beyond the control of the Company. These forward-looking statements are subject to a number of risks and uncertainties, including the timing, receipt and terms and conditions of any required governmental and regulatory approvals of the proposed transaction that could delay the consummation of the proposed transaction or cause the parties to abandon the proposed transaction; the occurrence of any event, change or other circumstances that could give rise to the termination of the merger agreement entered into in connection with the proposed transaction; the possibility that the Company’s stockholders may not approve the proposed transaction; the risk that the parties to the merger agreement may not be able to satisfy the conditions to the proposed transaction in a timely manner or at all; risks related to disruption of management time from ongoing business operations due to the proposed transaction; the risk that any announcements relating to the proposed transaction could have adverse effects on the market price of the common stock of the Company; the risk of any unexpected costs or expenses resulting from the proposed transaction; the risk of any litigation relating to the proposed transaction; and the risk that the proposed transaction and its announcement could have an adverse effect on the ability of the Company to retain and hire key personnel and to maintain relationships with customers, vendors, partners, employees, stockholders and other business relationships and on its operating results and business generally.&lt;/p&gt;
    &lt;p&gt;Further information on factors that could cause actual results to differ materially from the results anticipated by the forward-looking statements is included in the Company’s Annual Report on Form 10‑K for the fiscal year ended December 31, 2024, Quarterly Reports on Form 10‑Q, Current Reports on Form 8‑K, the Proxy Statement and other filings made by the Company from time to time with the Securities and Exchange Commission. These filings, when available, are available on the investor relations section of the Company’s website () or on the SEC’s website (https://www.sec.gov). If any of these risks materialize or any of these assumptions prove incorrect, actual results could differ materially from the results implied by these forward-looking statements. There may be additional risks that the Company presently does not know of or that the Company currently believes are immaterial that could also cause actual results to differ from those contained in the forward-looking statements. The forward-looking statements included in this communication are made only as of the date hereof. The Company assumes no obligation and does not intend to update these forward-looking statements, except as required by law.&lt;/p&gt;
    &lt;p&gt;This blog post highlights how Forrester Research has named Confluent a leader in The Forrester Wave™: Streaming Data Platforms, Q4 2025, and explains the value and benefits of the Confluent Data Streaming Platform.&lt;/p&gt;
    &lt;p&gt;Confluent achieves FedRAMP Ready status for its Confluent Cloud for Government offering, marking an essential milestone in providing secure data streaming services to government agencies, and showing a commitment to rigorous security standards. This certification marks a key step towards full...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46192130</guid><pubDate>Mon, 08 Dec 2025 13:43:59 +0000</pubDate></item><item><title>Strong earthquake hits northern Japan, tsunami warning issued</title><link>https://www3.nhk.or.jp/nhkworld/en/news/20251209_02/</link><description>&lt;doc fingerprint="73b8305eee71b4dd"&gt;
  &lt;main&gt;
    &lt;p&gt;A strong earthquake has struck northern Japan. The quake struck off the eastern coast of Aomori Prefecture at 11:15 p.m. on Monday. Its focus was 54 kilometers deep, and the magnitude is estimated at 7.5.&lt;/p&gt;
    &lt;head rend="h3"&gt;Strong quake hits northern Japan&lt;/head&gt;
    &lt;p&gt;Strong tremors were felt across the region. Tremors with an intensity of upper 6 on the Japanese intensity scale of 0 to 7 were observed in the city of Hachinohe in Aomori Prefecture.&lt;/p&gt;
    &lt;p&gt;A hotel employee in Hachinohe City said: It seems there are multiple injured people. Everyone appears to be conscious.&lt;/p&gt;
    &lt;p&gt;If you are in these areas, try to remain in a safe place and protect yourself. Use extreme caution if you must move. Damage around you could be heavy. Stay alert. More tremors are possible&lt;/p&gt;
    &lt;head rend="h3"&gt;Tsunami warning and advisory issued&lt;/head&gt;
    &lt;p&gt;The Meteorological Agency has issued a tsunami warning for the Pacific coastline in northern Japan, including Iwate Prefecture, and parts of Hokkaido and Aomori prefectures.&lt;/p&gt;
    &lt;p&gt;The agency says: it is the first time the agency has issued a tsunami warning since July, when a powerful quake off Kamchatka, Russia, prompted it to issue one for Japan's Pacific coastal areas.&lt;/p&gt;
    &lt;p&gt;Authorities warn that waves as high as three meters could hit the prefectures. A tsunami advisory is in place for other parts of the region, including Miyagi and Fukushima prefectures.&lt;/p&gt;
    &lt;p&gt;They are calling on people to stay away from the coastline, as well as the mouths of rivers.&lt;/p&gt;
    &lt;p&gt;According to authorities, long-period ground motions were recorded during the Monday earthquake.&lt;/p&gt;
    &lt;p&gt;Long-period ground motions are slow, large-amplitude seismic waves with frequencies of 2 seconds or longer that occur during a large earthquake. Such motions are known to have a significant impact on high-rise buildings.&lt;/p&gt;
    &lt;p&gt;Strong long-period motions, classified class-3, the second highest in the 4-level scale were observed in the village of Rokkasho in Aomori Prefecture. Such class-3 waves are strong enough to make it difficult for people in a high-rise building to stand up.&lt;/p&gt;
    &lt;head rend="h3"&gt;Residents ordered to evacuate&lt;/head&gt;
    &lt;p&gt;After tsunami warnings were issued, some municipalities in Hokkaido, and the Tohoku region issued evacuation orders to residents.&lt;/p&gt;
    &lt;head rend="h3"&gt;Traffic disrupted&lt;/head&gt;
    &lt;p&gt;East Japan Railway Company says that as of Tuesday, outbound trains on the Tohoku Shinkansen have been suspended between Fukushima and Shin-Aomori stations due to the earthquake. The company says three trains stopped in this section.&lt;/p&gt;
    &lt;p&gt;The company says that it is checking for any damage to railway tracks and that it remains unclear when services will resume.&lt;/p&gt;
    &lt;p&gt;The Morioka branch of East Japan Railway says that as of midnight on Tuesday, services on the Tohoku Main Line were suspended in Iwate Prefecture.&lt;/p&gt;
    &lt;p&gt;It says two trains made emergency stops. It remains unclear when services will resume. There are no reports of injuries.&lt;/p&gt;
    &lt;p&gt;As for Hokkaido, the operator of its busiest airport, New Chitose Airport near Sapporo, says that as of 11:40 p.m. on Monday, it was checking whether there are any abnormalities on two runways.&lt;/p&gt;
    &lt;p&gt;Highways have been affected. East Nippon Expressway Company says that as of 11:45 p.m. on Monday, traffic was completely stopped between the Shiraoi and Shinchitose Airport Interchanges and between the Tomakomai Higashi and Numanohata Nishi Interchanges.&lt;/p&gt;
    &lt;head rend="h3"&gt;Power Companies: No abnormalities at nuclear plants&lt;/head&gt;
    &lt;p&gt;Tokyo Electric Power Company says it has confirmed that there are no abnormalities at the Fukushima Daiichi and Daini nuclear plants.&lt;/p&gt;
    &lt;p&gt;The company says it halted the release of treated and diluted water from the Fukushima Daiichi nuclear power plant at 11:42 pm on Monday, as per predetermined procedures.&lt;/p&gt;
    &lt;p&gt;The facility suffered a triple meltdown during the March 2011 earthquake and tsunami. The water used to cool molten fuel has been mixing with rain and groundwater.&lt;/p&gt;
    &lt;p&gt;That has been treated to remove most radioactive substances, except tritium. It's then diluted, reducing levels of tritium to well below the World Health Organization's guidance for drinking water, before it is released into the ocean.&lt;/p&gt;
    &lt;p&gt;TEPCO also ordered some employees at the facility to evacuate. There have been no reports so far of injuries at the nuclear power plant.&lt;/p&gt;
    &lt;p&gt;Tohoku Electric Power Company says no abnormalities have been detected at the Higashidori nuclear power plant in Aomori Prefecture and the Onagawa plant in Miyagi Prefecture.&lt;/p&gt;
    &lt;p&gt;Hokkaido Electric Power Company says no problems have been found at the Tomari nuclear power plant in the prefecture.&lt;/p&gt;
    &lt;head rend="h3"&gt;Government bracing for damages&lt;/head&gt;
    &lt;p&gt;The Japanese government set up a task force at the crisis management center in the prime minister's office at 11:16 p.m. on Monday in response to the earthquake.&lt;/p&gt;
    &lt;p&gt;Prime Minister Takaichi Sanae entered the prime minister's office shortly after 11:50 p.m.&lt;/p&gt;
    &lt;p&gt;She instructed the government to immediately provide information on any tsunami and evacuation orders to the people in an appropriate manner, take thorough measures to prevent harm, such as evacuating residents, and get a grasp of the extent of damage as soon as possible.&lt;/p&gt;
    &lt;p&gt;Takaichi: The central government will work closely with local governments and make the utmost effort to carry out measures, such as emergency response, including rescue for the affected people.&lt;/p&gt;
    &lt;p&gt;Chief Cabinet Secretary Kihara Minoru held a news conference on Tuesday. Kihara said the government continues to assess the extent of the damage.&lt;/p&gt;
    &lt;p&gt;He added that the government is devoting all its efforts to disaster prevention measures, with rescue and relief efforts as its top priority, led by the police, fire departments, Self-Defense Forces, and Japan Coast Guard.&lt;/p&gt;
    &lt;p&gt;The Japan Meteorological Agency will soon hold a news conference on the earthquake. It is expected to explain what precautions should be taken in quake-hit areas.&lt;/p&gt;
    &lt;head rend="h3"&gt;Expert view on the quake&lt;/head&gt;
    &lt;p&gt;Sakai Shinichi, professor at the Earthquake Research Institute of the University of Tokyo, says: If this was a shallow earthquake centered in the sea, there is a high possibility that a tsunami has already occurred. People should stay away from the coast. It is important to evacuate and to take measures to stay warm.&lt;/p&gt;
    &lt;p&gt;Sakai says: The epicenter may be north of the epicenter area of the 2011 Great East Japan Earthquake. This time, the earthquake is believed to have occurred at the plate boundary, so I think it was a slightly larger earthquake. The magnitude could be revised in the future.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46192846</guid><pubDate>Mon, 08 Dec 2025 14:50:48 +0000</pubDate></item><item><title>Nova Programming Language</title><link>https://nova-lang.net</link><description>&lt;doc fingerprint="a3f8f96f303ef394"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Welcome!&lt;/head&gt;
    &lt;code&gt;|- Welcome to Nova! -|
    ~ Nova is a lightweight language for... ~
        . sketching out ideas,
        . documents, notes and personal tools,
        . casual modeling and thinking,
        . computing without computers
&lt;/code&gt;
    &lt;p&gt;If you've ever wanted to make a computer come to life through programming, you probably know how complicated it can be. Intricate incantations, confusing instructions, and large, complicated tools can make approaching programming incredibly difficult.&lt;/p&gt;
    &lt;p&gt;To address this, we've built something we call Nova. It is a programming language, a note-taking system, a way of sketching, and a way of conversing with programmers and machines!&lt;/p&gt;
    &lt;p&gt;We invite you to investigate what we've discovered and try it for yourself!&lt;/p&gt;
    &lt;head rend="h1"&gt;I want to...&lt;/head&gt;
    &lt;head rend="h2"&gt;Learn To Write Nova&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"Nova, mechanically." and other articles by yumaikas.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Find A Nova For Me&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Online Nova IDE&lt;/item&gt;
      &lt;item&gt;Nova implementations (for if you want to connect Nova to existing code)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Find Community&lt;/head&gt;
    &lt;p&gt;Join us on...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46192997</guid><pubDate>Mon, 08 Dec 2025 15:03:09 +0000</pubDate></item><item><title>Berkshire Hathaway Announces Leadership Appointments [pdf]</title><link>https://berkshirehathaway.com/news/dec0825.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46193116</guid><pubDate>Mon, 08 Dec 2025 15:12:28 +0000</pubDate></item><item><title>AMD GPU Debugger</title><link>https://thegeeko.me/blog/amd-gpu-debugging/</link><description>&lt;doc fingerprint="ae1ad8512d473b89"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AMD GPU Debugger&lt;/head&gt;
    &lt;p&gt;I’ve always wondered why we don’t have a GPU debugger similar to the one used for CPUs. A tool that allows pausing execution and examining the current state. This capability feels essential, especially since the GPU’s concurrent execution model is much harder to reason about. After searching for solutions, I came across rocgdb, a debugger for AMD’s ROCm environment. Unfortunately, its scope is limited to that environment. Still, this shows it’s technically possible. I then found a helpful series of blog posts by Marcell Kiss, detailing how he achieved this, which inspired me to try to recreate the process myself.&lt;/p&gt;
    &lt;head rend="h1"&gt;Let’s Try To Talk To The GPU Directly&lt;/head&gt;
    &lt;p&gt;The best place to start learning about this is RADV. By tracing what it does, we can find how to do it. Our goal here is to run the most basic shader &lt;code&gt;nop 0&lt;/code&gt; without using Vulkan, aka RADV in our case.&lt;/p&gt;
    &lt;p&gt;First of all, we need to open the DRM file to establish a connection with the KMD, using a simple open(“/dev/dri/cardX”), then we find that it’s calling &lt;code&gt;amdgpu_device_initialize&lt;/code&gt;, which is a function defined in &lt;code&gt;libdrm&lt;/code&gt;, which is a library that acts as middleware between user mode drivers(UMD) like &lt;code&gt;RADV&lt;/code&gt; and and kernel mode drivers(KMD) like amdgpu driver, and then when we try to do some actual work we have to create a context which can be achieved by calling &lt;code&gt;amdgpu_cs_ctx_create&lt;/code&gt; from &lt;code&gt;libdrm&lt;/code&gt; again, next up we need to allocate 2 buffers one of them for our code and the other for writing our commands into, we do this by calling a couple of functions, here’s how I do it:&lt;/p&gt;
    &lt;code&gt;void bo_alloc(amdgpu_t* dev, size_t size, u32 domain, bool uncached, amdgpubo_t* bo) {
 s32    ret         = -1;
 u32    alignment   = 0;
 u32    flags       = 0;
 size_t actual_size = 0;

 amdgpu_bo_handle bo_handle = NULL;
 amdgpu_va_handle va_handle = NULL;
 u64              va_addr   = 0;
 void*            host_addr = NULL;&lt;/code&gt;
    &lt;p&gt;Here we’re choosing the domain and assigning flags based on the params, some buffers we will need uncached, as we will see:&lt;/p&gt;
    &lt;code&gt; if (
   domain != AMDGPU_GEM_DOMAIN_GWS &amp;amp;&amp;amp; domain != AMDGPU_GEM_DOMAIN_GDS &amp;amp;&amp;amp;
   domain != AMDGPU_GEM_DOMAIN_OA) {
  actual_size = (size + 4096 - 1) &amp;amp; 0xFFFFFFFFFFFFF000ULL;
  alignment   = 4096;
  flags       = AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED | AMDGPU_GEM_CREATE_VRAM_CLEARED |
          AMDGPU_GEM_CREATE_VM_ALWAYS_VALID;
  flags |=
    uncached ? (domain == AMDGPU_GEM_DOMAIN_GTT) * AMDGPU_GEM_CREATE_CPU_GTT_USWC : 0;
 } else {
  actual_size = size;
  alignment   = 1;
  flags       = AMDGPU_GEM_CREATE_NO_CPU_ACCESS;
 }

 struct amdgpu_bo_alloc_request req = {
  .alloc_size     = actual_size,
  .phys_alignment = alignment,
  .preferred_heap = domain,
  .flags          = flags,
 };

 // memory aquired!!
 ret = amdgpu_bo_alloc(dev-&amp;gt;dev_handle, &amp;amp;req, &amp;amp;bo_handle);
 HDB_ASSERT(!ret, "can't allocate bo");&lt;/code&gt;
    &lt;p&gt;Now we have the memory, we need to map it. I opt to map anything that can be CPU-mapped for ease of use. We have to map the memory to both the GPU and the CPU virtual space. The KMD creates the page table when we open the DRM file, as shown here.&lt;/p&gt;
    &lt;p&gt;So map it to the GPU VM and, if possible, to the CPU VM as well. Here, at this point, there’s a libdrm function that does all of this setup for us and maps the memory, but I found that even when specifying &lt;code&gt;AMDGPU_VM_MTYPE_UC&lt;/code&gt;, it doesn’t always tag the page as uncached, not quite sure if it’s a
bug in my code or something in &lt;code&gt;libdrm&lt;/code&gt; anyways, the function is &lt;code&gt;amdgpu_bo_va_op&lt;/code&gt;, I opted to do it manually here and issue the IOCTL call myself:&lt;/p&gt;
    &lt;code&gt; u32 kms_handle = 0;
 amdgpu_bo_export(bo_handle, amdgpu_bo_handle_type_kms, &amp;amp;kms_handle);

 ret = amdgpu_va_range_alloc(
   dev-&amp;gt;dev_handle,
   amdgpu_gpu_va_range_general,
   actual_size,
   4096,
   0,
   &amp;amp;va_addr,
   &amp;amp;va_handle,
   0);
 HDB_ASSERT(!ret, "can't allocate VA");

 u64 map_flags =
   AMDGPU_VM_PAGE_EXECUTABLE | AMDGPU_VM_PAGE_READABLE | AMDGPU_VM_PAGE_WRITEABLE;
 map_flags |= uncached ? AMDGPU_VM_MTYPE_UC | AMDGPU_VM_PAGE_NOALLOC : 0;

 struct drm_amdgpu_gem_va va = {
  .handle       = kms_handle,
  .operation    = AMDGPU_VA_OP_MAP,
  .flags        = map_flags,
  .va_address   = va_addr,
  .offset_in_bo = 0,
  .map_size     = actual_size,

 };

 ret = drm_ioctl_write_read(dev-&amp;gt;drm_fd, DRM_AMDGPU_GEM_VA, &amp;amp;va, sizeof(va));
 HDB_ASSERT(!ret, "can't map bo in GPU space");
 // ret = amdgpu_bo_va_op(bo_handle, 0, actual_size, va_addr, map_flags,
 // AMDGPU_VA_OP_MAP);

 if (flags &amp;amp; AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) {
  ret = amdgpu_bo_cpu_map(bo_handle, &amp;amp;host_addr);
  HDB_ASSERT(!ret, "can't map bo in CPU space");

  // AMDGPU_GEM_CREATE_VRAM_CLEARED doesn't really memset the memory to 0 anyways for
  // debug I'll just do it manually for now
  memset(host_addr, 0x0, actual_size);
 }

 *bo = (amdgpubo_t){
  .bo_handle = bo_handle,
  .va_handle = va_handle,
  .va_addr   = va_addr,
  .size      = actual_size,
  .host_addr = host_addr,
 };
}&lt;/code&gt;
    &lt;p&gt;Now we have the context and 2 buffers. Next, fill those buffers and send our commands to the KMD, which will then forward them to the Command Processor (CP) in the GPU for processing.&lt;/p&gt;
    &lt;p&gt;Let’s compile our code. We can use clang assembler for that, like this:&lt;/p&gt;
    &lt;code&gt;# https://gitlab.freedesktop.org/martty/radbg-poc/-/blob/master/ll-as.sh
clang -c -x assembler -target amdgcn-amd-amdhsa -mcpu=gfx1100 -o asm.o "$1"
objdump -h asm.o | grep .text | awk '{print "dd if='asm.o' of='asmc.bin' bs=1 count=$[0x" $3 "] skip=$[0x" $6 "] status=none"}' | bash
#rm asm.o&lt;/code&gt;
    &lt;p&gt;The bash script compiles the code, and then we’re only interested in the actual machine code, so we use objdump to figure out the offset and the size of the section and copy it to a new file called asmc.bin, then we can just load the file and write its bytes to the CPU-mapped address of the code buffer.&lt;/p&gt;
    &lt;p&gt;Next up, filling in the commands. This was extremely confusing for me because it’s not well documented. It was mostly learning how &lt;code&gt;RADV&lt;/code&gt; does things and trying to do similar things. Also, shout-out to the folks on the Graphics Programming Discord server for helping me, especially Picoduck. The commands are encoded in a special format called &lt;code&gt;PM4 Packets&lt;/code&gt;, which has multiple types. We only care about &lt;code&gt;Type 3&lt;/code&gt;: each packet has an opcode and the number of bytes it contains.&lt;/p&gt;
    &lt;p&gt;The first thing we need to do is program the GPU registers, then dispatch the shader. Some of those registers are &lt;code&gt;rsrc[1-3]&lt;/code&gt;; those registers are responsible for a number of configurations, pgm_[lo/hi], which hold the pointer to the code buffer and &lt;code&gt;num_thread_[x/y/z]&lt;/code&gt;; those are responsible for the number of threads inside a work group. All of those are set using the &lt;code&gt;set shader register&lt;/code&gt; packets, and here is how to encode them:&lt;/p&gt;
    &lt;p&gt;It’s worth mentioning that we can set multiple registers in 1 packet if they’re consecutive.&lt;/p&gt;
    &lt;code&gt;void pkt3_set_sh_reg(pkt3_packets_t* packets, u32 reg, u32 value) {
 HDB_ASSERT(
   reg &amp;gt;= SI_SH_REG_OFFSET &amp;amp;&amp;amp; reg &amp;lt; SI_SH_REG_END,
   "can't set register outside sh registers span");

 // packet header
 da_append(packets, PKT3(PKT3_SET_SH_REG, 1, 0));
 // offset of the register
 da_append(packets, (reg - SI_SH_REG_OFFSET) / 4);
 da_append(packets, value);
}&lt;/code&gt;
    &lt;p&gt;Then we append the dispatch command:&lt;/p&gt;
    &lt;code&gt;// we're going for 1 thread since we want the simplest case here.

da_append(&amp;amp;pkt3_packets, PKT3(PKT3_DISPATCH_DIRECT, 3, 0) | PKT3_SHADER_TYPE_S(1));
da_append(&amp;amp;pkt3_packets, 1u);
da_append(&amp;amp;pkt3_packets, 1u);
da_append(&amp;amp;pkt3_packets, 1u);
da_append(&amp;amp;pkt3_packets, dispatch_initiator);&lt;/code&gt;
    &lt;p&gt;Now we want to write those commands into our buffer and send them to the KMD:&lt;/p&gt;
    &lt;code&gt;void dev_submit(
  amdgpu_t*         dev,
  pkt3_packets_t*   packets,
  amdgpu_bo_handle* buffers,
  u32               buffers_count,
  amdgpu_submit_t*  submit
) {
 s32        ret = -1;
 amdgpubo_t ib  = { 0 };

 bo_alloc(dev, pkt3_size(packets), AMDGPU_GEM_DOMAIN_GTT, false, &amp;amp;ib);
 bo_upload(&amp;amp;ib, packets-&amp;gt;data, pkt3_size(packets));

 amdgpu_bo_handle* bo_handles = // +1 for the indirect buffer
   (amdgpu_bo_handle*)malloc(sizeof(amdgpu_bo_handle) * (buffers_count + 1));

 bo_handles[0] = ib.bo_handle;
 for_range(i, 0, buffers_count) {
  bo_handles[i + 1] = buffers[i];
 }

 amdgpu_bo_list_handle bo_list = NULL;
 ret =
   amdgpu_bo_list_create(dev-&amp;gt;dev_handle, buffers_count + 1, bo_handles, NULL, &amp;amp;bo_list);
 HDB_ASSERT(!ret, "can't create a bo list");
 free(bo_handles);

 struct amdgpu_cs_ib_info ib_info = {
  .flags         = 0,
  .ib_mc_address = ib.va_addr,
  .size          = packets-&amp;gt;count,
 };

 struct amdgpu_cs_request req = {
  .flags                  = 0,
  .ip_type                = AMDGPU_HW_IP_COMPUTE,
  .ip_instance            = 0,
  .ring                   = 0,
  .resources              = bo_list,
  .number_of_dependencies = 0,
  .dependencies           = NULL,
  .number_of_ibs          = 1,
  .ibs                    = &amp;amp;ib_info,
  .seq_no                 = 0,
  .fence_info             = { 0 },
 };

 ret = amdgpu_cs_submit(dev-&amp;gt;ctx_handle, 0, &amp;amp;req, 1);
 HDB_ASSERT(!ret, "can't submit indirect buffer request");

 *submit = (amdgpu_submit_t){
    .ib = ib,
    .bo_list = bo_list,
    .fence = {
      .context = dev-&amp;gt;ctx_handle,
      .ip_type = AMDGPU_HW_IP_COMPUTE,
      .ip_instance = 0,
      .ring = 0,
      .fence = req.seq_no,
    },
  };
}&lt;/code&gt;
    &lt;p&gt;Here is a good point to make a more complex shader that outputs something. For example, writing 1 to a buffer.&lt;/p&gt;
    &lt;p&gt;No GPU hangs ?! nothing happened ?! cool, cool, now we have a shader that runs on the GPU, what’s next? Let’s try to hang the GPU by pausing the execution, aka make the GPU trap.&lt;/p&gt;
    &lt;head rend="h1"&gt;TBA/TMA&lt;/head&gt;
    &lt;p&gt;The RDNA3’s ISA manual does mention 2 registers, &lt;code&gt;TBA, TMA&lt;/code&gt;; here’s how they describe them respectively:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Holds the pointer to the current trap handler program address. Per-VMID register. Bit [63] indicates if the trap handler is present (1) or not (0) and is not considered part of the address (bit[62] is replicated into address bit[63]). Accessed via S_SENDMSG_RTN.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Temporary register for shader operations. For example, it can hold a pointer to memory used by the trap handler.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;You can configure the GPU to enter the trap handler when encountering certain exceptions listed in the RDNA3 ISA manual.&lt;/p&gt;
    &lt;p&gt;We know from Marcell Kiss’s blog posts that we need to compile a trap handler, which is a normal shader the GPU switches to when encountering a &lt;code&gt;s_trap&lt;/code&gt;. The TBA register has a special bit that indicates whether the trap handler is enabled.&lt;/p&gt;
    &lt;p&gt;Since these are privileged registers, we cannot write to them from user space. To bridge this gap for debugging, we can utilize the debugfs interface. Luckily, we have UMR, which uses that debugfs interface, and it’s open source; we copy AMD’s homework here which is great.&lt;/p&gt;
    &lt;head rend="h1"&gt;AMDGPU Debugfs&lt;/head&gt;
    &lt;p&gt;The amdgpu KMD has a couple of files in debugfs under &lt;code&gt;/sys/kernel/debug/dri/{PCI address}&lt;/code&gt;; one of them is &lt;code&gt;regs2&lt;/code&gt;, which is an interface to a &lt;code&gt;amdgpu_debugfs_regs2_write&lt;/code&gt; in the kernel that writes to the registers. It works by simply opening the file, seeking the register’s offset, and then writing; it also performs some synchronisation and writes the value correctly. We need to provide more parameters about the register before writing to the file, tho and do that by using an ioctl call. Here are the ioctl arguments:&lt;/p&gt;
    &lt;code&gt;typedef struct amdgpu_debugfs_regs2_iocdata_v2 {
 __u32 use_srbm, use_grbm, pg_lock;
 struct {
  __u32 se, sh, instance;
 } grbm;
 struct {
  __u32 me, pipe, queue, vmid;
 } srbm;
 __u32 xcc_id;
} regs2_ioc_data_t;&lt;/code&gt;
    &lt;p&gt;The 2 structs are because there are 2 types of registers, GRBM and SRBM, each of which is banked by different constructs; you can learn more about some of them here in the Linux kernel documentation.&lt;/p&gt;
    &lt;p&gt;Turns out our registers here are SBRM registers and banked by VMIDs, meaning each VMID has its own TBA and TMA registers. Cool, now we need to figure out the VMID of our process. As far as I understand, VMIDs are a way for the GPU to identify a specific process context, including the page table base address, so the address translation unit can translate a virtual memory address. The context is created when we open the DRM file. They get assigned dynamically at dispatch time, which is a problem for us; we want to write to those registers before dispatch.&lt;/p&gt;
    &lt;p&gt;We can obtain the VMID of the dispatched process by querying the &lt;code&gt;HW_ID2&lt;/code&gt; register with s_getreg_b32. I do a hack here, by enabling the trap handler in every VMID, and there are 16 of them, the first being special, and used by the KMD and the last 8 allocated to the amdkfd driver. We loop over the remaining VMIDs and write to those registers. This can cause issues to other processes using other VMIDs by enabling trap handlers in them and writing the virtual address of our trap handler, which is only valid within our virtual memory address space. It’s relatively safe tho since most other processes won’t cause a trap1.&lt;/p&gt;
    &lt;p&gt;Now we can write to TMA and TBA, here’s the code:&lt;/p&gt;
    &lt;code&gt;void dev_op_reg32(
  amdgpu_t* dev, gc_11_reg_t reg, regs2_ioc_data_t ioc_data, reg_32_op_t op, u32* value) {
 s32 ret = 0;

 reg_info_t reg_info     = gc_11_regs_infos[reg];
 uint64_t   reg_offset   = gc_11_regs_offsets[reg];
 uint64_t   base_offset  = dev-&amp;gt;gc_regs_base_addr[reg_info.soc_index];
 uint64_t   total_offset = (reg_offset + base_offset);

 // seems like we're multiplying by 4 here because the registers database in UMRs
 // source has them in indexes rather than bytes.
 total_offset *= (reg_info.type == REG_MMIO) ? 4 : 1;

 ret = hdb_ioctl(dev-&amp;gt;regs2_fd, AMDGPU_DEBUGFS_REGS2_IOC_SET_STATE_V2, &amp;amp;ioc_data);
 HDB_ASSERT(!ret, "Failed to set registers state");

 size_t size = lseek(dev-&amp;gt;regs2_fd, total_offset, SEEK_SET);
 HDB_ASSERT(size == total_offset, "Failed to seek register address");

 switch (op) {
 case REG_OP_READ : size = read(dev-&amp;gt;regs2_fd, value, 4); break;
 case REG_OP_WRITE: size = write(dev-&amp;gt;regs2_fd, value, 4); break;
 default          : HDB_ASSERT(false, "unsupported op");
 }

 HDB_ASSERT(size == 4, "Failed to write/read the values to/from the register");
}&lt;/code&gt;
    &lt;p&gt;And here’s how we write to &lt;code&gt;TMA&lt;/code&gt; and &lt;code&gt;TBA&lt;/code&gt;:
If you noticed, I’m using bitfields. I use them because working with them is much easier than macros, and while the byte order is not guaranteed by the C spec, it’s guaranteed by System V ABI, which Linux adheres to.&lt;/p&gt;
    &lt;code&gt;void dev_setup_trap_handler(amdgpu_t* dev, u64 tba, u64 tma) {
 reg_sq_shader_tma_lo_t tma_lo = { .raw = (u32)(tma) };
 reg_sq_shader_tma_hi_t tma_hi = { .raw = (u32)(tma &amp;gt;&amp;gt; 32) };

 reg_sq_shader_tba_lo_t tba_lo = { .raw = (u32)(tba &amp;gt;&amp;gt; 8) };
 reg_sq_shader_tba_hi_t tba_hi = { .raw = (u32)(tba &amp;gt;&amp;gt; 40) };

 tba_hi.trap_en = 1;

 regs2_ioc_data_t ioc_data = {
  .use_srbm = 1,
  .xcc_id   = -1,
 };

 // NOTE(hadi):
 // vmid's get assigned when code starts executing before hand we don't know which vmid
 // will get assigned to our process so we just set all of them
 for_range(i, 1, 9) {
  ioc_data.srbm.vmid = i;
  dev_op_reg32(dev, REG_SQ_SHADER_TBA_LO, ioc_data, REG_OP_WRITE, &amp;amp;tba_lo.raw);
  dev_op_reg32(dev, REG_SQ_SHADER_TBA_HI, ioc_data, REG_OP_WRITE, &amp;amp;tba_hi.raw);

  dev_op_reg32(dev, REG_SQ_SHADER_TMA_LO, ioc_data, REG_OP_WRITE, &amp;amp;tma_lo.raw);
  dev_op_reg32(dev, REG_SQ_SHADER_TMA_HI, ioc_data, REG_OP_WRITE, &amp;amp;tma_hi.raw);
 }
}&lt;/code&gt;
    &lt;p&gt;Anyway, now that we can write to those registers, if we enable the trap handler correctly, the GPU should hang when we launch our shader if we added &lt;code&gt;s_trap&lt;/code&gt; instruction to it, or we enabled the &lt;code&gt;TRAP_ON_START&lt;/code&gt; bit in rsrc32 register.&lt;/p&gt;
    &lt;p&gt;Now, let’s try to write a trap handler.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Trap Handler&lt;/head&gt;
    &lt;p&gt;If you wrote a different shader that outputs to a buffer, u can try writing to that shader from the trap handler, which is nice to make sure it’s actually being run.&lt;/p&gt;
    &lt;p&gt;We need 2 things: our trap handler and some scratch memory to use when needed, which we will store the address of in the TMA register.&lt;/p&gt;
    &lt;p&gt;The trap handler is just a normal program running in privileged state, meaning we have access to special registers like TTMP[0-15]. When we enter a trap handler, we need to first ensure that the state of the GPU registers is saved, just as the kernel does for CPU processes when context-switching, by saving a copy of the stable registers and the program counter, etc. The problem, tho, is that we don’t have a stable ABI for GPUs, or at least not one I’m aware of, and compilers use all the registers they can, so we need to save everything.&lt;/p&gt;
    &lt;p&gt;AMD GPUs’ Command Processors (CPs) have context-switching functionality, and the amdkfd driver does implement some context-switching shaders. The problem is they’re not documented, and we have to figure them out from the amdkfd driver source and from other parts of the driver stack that interact with it, which is a pain in the ass. I kinda did a workaround here since I didn’t find luck understanding how it works, and some other reasons I’ll discuss later in the post.&lt;/p&gt;
    &lt;p&gt;The workaround here is to use only TTMP registers and a combination of specific instructions to copy the values of some registers, allowing us to use more instructions to copy the remaining registers. The main idea is to make use of the &lt;code&gt;global_store_addtid_b32&lt;/code&gt; instruction, which adds the index of the current thread within the wave to the writing address, aka&lt;/p&gt;
    &lt;p&gt;This allows us to write a unique value per thread using only TTMP registers, which are unique per wave, not per thread3, so we can save the context of a single wave.&lt;/p&gt;
    &lt;p&gt;The problem is that if we have more than 1 wave, they will overlap, and we will have a race condition.&lt;/p&gt;
    &lt;p&gt;Here is the code:&lt;/p&gt;
    &lt;code&gt;start:
 ;; save the STATUS word into ttmp8
 s_getreg_b32 ttmp8, hwreg(HW_REG_STATUS)

 ;; save exec into ttmp[2:3]
 s_mov_b64 ttmp[2:3], exec

 ;; getting the address of our tma buffer
 s_sendmsg_rtn_b64 ttmp[4:5], sendmsg(MSG_RTN_GET_TMA)
 s_waitcnt lgkmcnt(0)

 ;; save vcc
 s_mov_b64 ttmp[6:7], vcc

 ;; enable all threads so they can write their vgpr registers
 s_mov_b64 exec, -1

 ;; FIXME(hadi): this assumes only 1 wave is running
 global_store_addtid_b32 v0, ttmp[4:5], offset:TMA_VREG_OFFSET        glc slc dlc
 global_store_addtid_b32 v1, ttmp[4:5], offset:TMA_VREG_OFFSET + 256  glc slc dlc
 global_store_addtid_b32 v2, ttmp[4:5], offset:TMA_VREG_OFFSET + 512  glc slc dlc
 global_store_addtid_b32 v3, ttmp[4:5], offset:TMA_VREG_OFFSET + 768  glc slc dlc
 global_store_addtid_b32 v4, ttmp[4:5], offset:TMA_VREG_OFFSET + 1024 glc slc dlc
 global_store_addtid_b32 v5, ttmp[4:5], offset:TMA_VREG_OFFSET + 1280 glc slc dlc
 global_store_addtid_b32 v6, ttmp[4:5], offset:TMA_VREG_OFFSET + 1536 glc slc dlc
 s_waitcnt vmcnt(0)

 ;; only first thread is supposed to write sgprs of the wave
 s_mov_b64 exec, 1
 v_mov_b32 v1, s0
 v_mov_b32 v2, s1
 v_mov_b32 v3, s2
 v_mov_b32 v4, s3
 v_mov_b32 v5, s4
 v_mov_b32 v0, 0
 global_store_b32 v0, v1, ttmp[4:5], offset:TMA_SREG_OFFSET glc slc dlc
 global_store_b32 v0, v2, ttmp[4:5], offset:TMA_SREG_OFFSET + 4 glc slc dlc
 global_store_b32 v0, v3, ttmp[4:5], offset:TMA_SREG_OFFSET + 8 glc slc dlc
 global_store_b32 v0, v4, ttmp[4:5], offset:TMA_SREG_OFFSET + 12 glc slc dlc
 global_store_b32 v0, v5, ttmp[4:5], offset:TMA_SREG_OFFSET + 16 glc slc dlc
 s_waitcnt vmcnt(0)

 ;; enable all threads
 s_mov_b64 exec, -1&lt;/code&gt;
    &lt;p&gt;Now that we have those values in memory, we need to tell the CPU: Hey, we got the data, and pause the GPU’s execution until the CPU issues a command. Also, notice we can just modify those from the CPU.&lt;/p&gt;
    &lt;p&gt;Before we tell the CPU, we need to write some values that might help the CPU. Here are they:&lt;/p&gt;
    &lt;code&gt; ;; IDs to identify which parts of the hardware we are running on exactly
 s_getreg_b32 ttmp10, hwreg(HW_REG_HW_ID1)
 s_getreg_b32 ttmp11, hwreg(HW_REG_HW_ID2)
 v_mov_b32 v3, ttmp10
 v_mov_b32 v4, ttmp11
 global_store_dwordx2 v1, v[3:4], ttmp[4:5], offset:TMA_DATA_OFFSET glc slc dlc

 ;; the original vcc mask
 v_mov_b32 v3, ttmp6
 v_mov_b32 v4, ttmp7
 global_store_dwordx2 v1, v[3:4], ttmp[4:5], offset:2048 glc slc dlc
 s_waitcnt vmcnt(0)

 ;; the original exec mask
 v_mov_b32 v3, ttmp2
 v_mov_b32 v4, ttmp3
 global_store_dwordx2 v1, v[3:4], ttmp[4:5], offset:2056 glc slc dlc
 s_waitcnt vmcnt(0)

 ;; the program counter
 v_mov_b32 v3, ttmp0
 v_mov_b32 v4, ttmp1
 v_and_b32 v4, v4, 0xffff
 global_store_dwordx2 v1, v[3:4], ttmp[4:5], offset:16 glc slc dlc

 s_waitcnt vmcnt(0)&lt;/code&gt;
    &lt;p&gt;Now the GPU should just wait for the CPU, and here’s the spin code it’s implemented as described by Marcell Kiss here:&lt;/p&gt;
    &lt;code&gt;SPIN:
 global_load_dword v1, v2, ttmp[4:5] glc slc dlc

SPIN1:
 // I found the bit range of 10 to 15 using trial and error in the
 // isa manual specifies that it's a 6-bit number but the offset 10
 // is just trial and error
  s_getreg_b32 ttmp13, hwreg(HW_REG_IB_STS, 10, 15)
 s_and_b32 ttmp13, ttmp13, ttmp13
 s_cbranch_scc1 SPIN1

 v_readfirstlane_b32 ttmp13, v1
 s_and_b32 ttmp13, ttmp13, ttmp13
 s_cbranch_scc0 SPIN

CLEAR:
 v_mov_b32 v2, 0
 v_mov_b32 v1, 0
 global_store_dword v1, v2, ttmp[4:5] glc slc dlc
 s_waitcnt vmcnt(0)&lt;/code&gt;
    &lt;p&gt;The main loop in the CPU is like enable trap handler, then dispatch shader, then wait for the GPU to write some specific value in a specific address to signal all data is there, then examine and display, and tell the GPU all clear, go ahead.&lt;/p&gt;
    &lt;p&gt;Now that our uncached buffers are in play, we just keep looping and checking whether the GPU has written the register values. When it does, the first thing we do is halt the wave by writing into the &lt;code&gt;SQ_CMD&lt;/code&gt; register to allow us to do whatever with the wave without causing any issues, tho if we halt for too long, the GPU CP will reset the command queue and kill the process, but we can change that behaviour by adjusting lockup_timeout parameter of the amdgpu kernel module:&lt;/p&gt;
    &lt;code&gt;reg_sq_wave_hw_id1_t hw1 = { .raw = tma[2] };
reg_sq_wave_hw_id2_t hw2 = { .raw = tma[3] };

reg_sq_cmd_t halt_cmd = {
 .cmd  = 1,
 .mode = 1,
 .data = 1,
};

regs2_ioc_data_t ioc_data = {
 .use_srbm = false,
 .use_grbm = true,
};

dev_op_reg32(&amp;amp;amdgpu, REG_SQ_CMD, ioc_data, REG_OP_WRITE, &amp;amp;halt_cmd.raw);
gpu_is_halted = true;&lt;/code&gt;
    &lt;p&gt;From here on, we can do whatever with the data we have. All the data we need to build a proper debugger. We will come back to what to do with the data in a bit; let’s assume we did what was needed for now.&lt;/p&gt;
    &lt;p&gt;Now that we’re done with the CPU, we need to write to the first byte in our TMA buffer, since the trap handler checks for that, then resume the wave, and the trap handler should pick it up. We can resume by writing to the &lt;code&gt;SQ_CMD&lt;/code&gt; register again:&lt;/p&gt;
    &lt;code&gt;halt_cmd.mode = 0;
dev_op_reg32(&amp;amp;amdgpu, REG_SQ_CMD, ioc_data, REG_OP_WRITE, &amp;amp;halt_cmd.raw);
gpu_is_halted = false;&lt;/code&gt;
    &lt;p&gt;Then the GPU should continue. We need to restore everything and return the program counter to the original address. Based on whether it’s a hardware trap or not, the program counter may point to the instruction before or the instruction itself. The ISA manual and Marcell Kiss’s posts explain that well, so refer to them.&lt;/p&gt;
    &lt;code&gt;RETURN:
 ;; extract the trap ID from ttmp1
 s_and_b32 ttmp9, ttmp1, PC_HI_TRAP_ID_MASK
 s_lshr_b32 ttmp9, ttmp9, PC_HI_TRAP_ID_SHIFT

 ;; if the trapID == 0, then this is a hardware trap,
 ;; we don't need to fix up the return address
 s_cmpk_eq_u32 ttmp9, 0
 s_cbranch_scc1 RETURN_FROM_NON_S_TRAP

 ;; restore PC
 ;; add 4 to the faulting address, with carry
 s_add_u32 ttmp0, ttmp0, 4
 s_addc_u32 ttmp1, ttmp1, 0

RETURN_FROM_NON_S_TRAP:
 s_load_dwordx4 s[0:3], ttmp[4:5], TMA_SREG_OFFSET glc dlc
 s_load_dword s4, ttmp[4:5], TMA_SREG_OFFSET + 16 glc dlc
 s_waitcnt lgkmcnt(0)

 s_mov_b64 exec, -1
 global_load_addtid_b32 v0, ttmp[4:5], offset:TMA_VREG_OFFSET        glc slc dlc
 global_load_addtid_b32 v1, ttmp[4:5], offset:TMA_VREG_OFFSET + 256  glc slc dlc
 global_load_addtid_b32 v2, ttmp[4:5], offset:TMA_VREG_OFFSET + 512  glc slc dlc
 global_load_addtid_b32 v3, ttmp[4:5], offset:TMA_VREG_OFFSET + 768  glc slc dlc
 global_load_addtid_b32 v4, ttmp[4:5], offset:TMA_VREG_OFFSET + 1024 glc slc dlc
 global_load_addtid_b32 v5, ttmp[4:5], offset:TMA_VREG_OFFSET + 1280 glc slc dlc
 global_load_addtid_b32 v6, ttmp[4:5], offset:TMA_VREG_OFFSET + 1536 glc slc dlc
 s_waitcnt vmcnt(0)

 ;; mask off non-address high bits from ttmp1
 s_and_b32 ttmp1, ttmp1, 0xffff

 ;; restore exec
 s_load_b64 vcc, ttmp[4:5], 2048 glc dlc
 s_load_b64 ttmp[2:3], ttmp[4:5], 2056 glc dlc
 s_waitcnt lgkmcnt(0)
 s_mov_b64 exec, ttmp[2:3]

 ;; restore STATUS.EXECZ, not writable by s_setreg_b32
 s_and_b64 exec, exec, exec

 ;; restore STATUS.VCCZ, not writable by s_setreg_b32
 s_and_b64 vcc, vcc, vcc

 ;; restore STATUS.SCC
 s_setreg_b32 hwreg(HW_REG_STATUS, 0, 1), ttmp8

 s_waitcnt vmcnt(0) lgkmcnt(0) expcnt(0)  ; Full pipeline flush
 ;; return from trap handler and restore STATUS.PRIV
 s_rfe_b64 [ttmp0, ttmp1]&lt;/code&gt;
    &lt;head rend="h1"&gt;SPIR-V&lt;/head&gt;
    &lt;p&gt;Now we can run compiled code directly, but we don’t want people to compile their code manually, then extract the text section, and give it to us. The plan is to take SPIR-V code, compile it correctly, then run it, or, even better, integrate with RADV and let RADV give us more information to work with.&lt;/p&gt;
    &lt;p&gt;My main plan was making like fork RADV and then add then make report for us the vulkan calls and then we can have a better view on the GPU work know the buffers/textures it’s using etc, This seems like a lot more work tho so I’ll keep it in mind but not doing that for now unless someone is willing to pay me for that ;).&lt;/p&gt;
    &lt;p&gt;For now, let’s just use RADV’s compiler &lt;code&gt;ACO&lt;/code&gt;. Luckily, RADV has a &lt;code&gt;null_winsys&lt;/code&gt; mode, aka it will not do actual work or open DRM files, just a fake Vulkan device, which is perfect for our case here, since we care about nothing other than just compiling code. We can enable it by setting the env var &lt;code&gt;RADV_FORCE_FAMILY&lt;/code&gt;, then we just call what we need like this:&lt;/p&gt;
    &lt;code&gt;int32_t hdb_compile_spirv_to_bin(
  const void* spirv_binary,
  size_t size,
  hdb_shader_stage_t stage,
  hdb_shader_t* shader
) {
 setenv("RADV_FORCE_FAMILY", "navi31", 1);
 //  setenv("RADV_DEBUG", "nocache,noopt", 1);
 setenv("ACO_DEBUG", "nocache,noopt", 1);

 VkInstanceCreateInfo i_cinfo = {
  .sType = VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO,
  .pApplicationInfo =
    &amp;amp;(VkApplicationInfo){
      .sType              = VK_STRUCTURE_TYPE_APPLICATION_INFO,
      .pApplicationName   = "HDB Shader Compiler",
      .applicationVersion = 1,
      .pEngineName        = "HDB",
      .engineVersion      = 1,
      .apiVersion         = VK_API_VERSION_1_4,
    },
 };

 VkInstance vk_instance = {};
 radv_CreateInstance(&amp;amp;i_cinfo, NULL, &amp;amp;vk_instance);

 struct radv_instance* instance = radv_instance_from_handle(vk_instance);
 instance-&amp;gt;debug_flags |=
   RADV_DEBUG_NIR_DEBUG_INFO | RADV_DEBUG_NO_CACHE | RADV_DEBUG_INFO;

 uint32_t         n       = 1;
 VkPhysicalDevice vk_pdev = {};
 instance-&amp;gt;vk.dispatch_table.EnumeratePhysicalDevices(vk_instance, &amp;amp;n, &amp;amp;vk_pdev);

 struct radv_physical_device* pdev = radv_physical_device_from_handle(vk_pdev);
 pdev-&amp;gt;use_llvm                    = false;

 VkDeviceCreateInfo d_cinfo = { VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO };
 VkDevice vk_dev = {};
 pdev-&amp;gt;vk.dispatch_table.CreateDevice(vk_pdev, &amp;amp;d_cinfo, NULL, &amp;amp;vk_dev);

 struct radv_device* dev = radv_device_from_handle(vk_dev);

 struct radv_shader_stage radv_stage = {
  .spirv.data = spirv_binary,
  .spirv.size = size,
  .entrypoint = "main",
  .stage      = MESA_SHADER_COMPUTE,
  .layout = {
   .push_constant_size = 16,
  },
  .key = {
   .optimisations_disabled = true,
  },
 };

 struct radv_shader_binary* cs_bin = NULL;
 struct radv_shader*        cs_shader =
   radv_compile_cs(dev, NULL, &amp;amp;radv_stage, true, true, false, true, &amp;amp;cs_bin);

 *shader = (hdb_shader_t){
  .bin              = cs_shader-&amp;gt;code,
  .bin_size         = cs_shader-&amp;gt;code_size,
  .rsrc1            = cs_shader-&amp;gt;config.rsrc1,
  .rsrc2            = cs_shader-&amp;gt;config.rsrc2,
  .rsrc3            = cs_shader-&amp;gt;config.rsrc3,
  .debug_info       = cs_shader-&amp;gt;debug_info,
  .debug_info_count = cs_shader-&amp;gt;debug_info_count,
 };

 return 0;
}&lt;/code&gt;
    &lt;p&gt;Now that we have a well-structured loop and communication between the GPU and the CPU, we can run SPIR-V binaries to some extent. Let’s see how we can make it an actual debugger.&lt;/p&gt;
    &lt;head rend="h1"&gt;An Actual Debugger&lt;/head&gt;
    &lt;p&gt;We talked earlier about CPs natively supporting context-switching, this appears to be compute spcific feature, which prevents from implementing it for other types of shaders, tho, it appears that mesh shaders and raytracing shaders are just compute shaders under the hood, which will allow us to use that functionality. For now debugging one wave feels enough, also we can moify the wave parameters to debug some specific indices.&lt;/p&gt;
    &lt;p&gt;Here’s some of the features&lt;/p&gt;
    &lt;head rend="h2"&gt;Breakpoints and Stepping&lt;/head&gt;
    &lt;p&gt;For stepping, we can use 2 bits: one in &lt;code&gt;RSRC1&lt;/code&gt; and the other in &lt;code&gt;RSRC3&lt;/code&gt;. They’re &lt;code&gt;DEBUG_MODE&lt;/code&gt; and &lt;code&gt;TRAP_ON_START&lt;/code&gt;, respectively. The former enters the trap handler after each instruction, and the latter enters before the first instruction. This means we can automatically enable instruction-level stepping.&lt;/p&gt;
    &lt;p&gt;Regarding breakpoints, I haven’t implemented them, but they’re rather simple to implement here by us having the base address of the code buffer and knowing the size of each instruction; we can calculate the program counter location ahead and have a list of them available to the GPU, and we can do a binary search on the trap handler.&lt;/p&gt;
    &lt;head rend="h2"&gt;Source Code Line Mapping&lt;/head&gt;
    &lt;p&gt;The ACO shader compiler does generate instruction-level source code mapping, which is good enough for our purposes here. By taking the offset4 of the current program counter and indexing into the code buffer, we can retrieve the current instruction and disassemble it, as well as find the source code mapping from the debug info.&lt;/p&gt;
    &lt;head rend="h2"&gt;Address Watching aka Watchpoints&lt;/head&gt;
    &lt;p&gt;We can implement this by marking the GPU page as protected. On a GPU fault, we enter the trap handler, check whether it’s within the range of our buffers and textures, and then act accordingly. Also, looking at the registers, we can find these:&lt;/p&gt;
    &lt;code&gt;typedef union {
 struct {
  uint32_t addr: 16;
 };
 uint32_t raw;
} reg_sq_watch0_addr_h_t;

typedef union {
 struct {
  uint32_t __reserved_0 : 6;
  uint32_t addr: 26;
 };
 uint32_t raw;
} reg_sq_watch0_addr_l_t;&lt;/code&gt;
    &lt;p&gt;which suggests that the hardware already supports this natively, so we don’t even need to do that dance. It needs more investigation on my part, tho, since I didn’t implement this.&lt;/p&gt;
    &lt;head rend="h2"&gt;Variables Types and Names&lt;/head&gt;
    &lt;p&gt;This needs some serious plumbing, since we need to make NIR(Mesa’s intermediate representation) optimisation passes propagate debug info correctly. I already started on this here. Then we need to make ACO track variables and store the information.&lt;/p&gt;
    &lt;head rend="h2"&gt;Vulkan Integration&lt;/head&gt;
    &lt;p&gt;This requires ditching our simple UMD we made earlier and using RADV, which is what should happen eventually, then we have our custom driver maybe pause on before a specific frame, or get triggered by a key, and then ask before each dispatch if to attach to it or not, or something similar, since we have a full proper Vulkan implementation we already have all the information we would need like buffers, textures, push constants, types, variable names, .. etc, that would be a much better and more pleasant debugger to use.&lt;/p&gt;
    &lt;p&gt;Finally, here’s some live footage:&lt;/p&gt;
    &lt;head rend="h1"&gt;Bonus Round&lt;/head&gt;
    &lt;p&gt;Here is an incomplete user-mode page walking code for gfx11, aka rx7900xtx&lt;/p&gt;
    &lt;code&gt;typedef struct {
 u64 valid         : 1;  // 0
 u64 system        : 1;  // 1
 u64 coherent      : 1;  // 2
 u64 __reserved_0  : 3;  // 5
 u64 pte_base_addr : 42; // 47
 u64 pa_rsvd       : 4;  // 51
 u64 __reserved_1  : 2;  // 53
 u64 mall_reuse    : 2;  // 55
 u64 tfs_addr      : 1;  // 56
 u64 __reserved_2  : 1;  // 57
 u64 frag_size     : 5;  // 62
 u64 pte           : 1;  // 63
} pde_t;

typedef struct {
 u64 valid          : 1; // = pte_entry &amp;amp; 1;
 u64 system         : 1; // = (pte_entry &amp;gt;&amp;gt; 1) &amp;amp; 1;
 u64 coherent       : 1; // = (pte_entry &amp;gt;&amp;gt; 2) &amp;amp; 1;
 u64 tmz            : 1; // = (pte_entry &amp;gt;&amp;gt; 3) &amp;amp; 1;
 u64 execute        : 1; // = (pte_entry &amp;gt;&amp;gt; 4) &amp;amp; 1;
 u64 read           : 1; // = (pte_entry &amp;gt;&amp;gt; 5) &amp;amp; 1;
 u64 write          : 1; // = (pte_entry &amp;gt;&amp;gt; 6) &amp;amp; 1;
 u64 fragment       : 5; // = (pte_entry &amp;gt;&amp;gt; 7) &amp;amp; 0x1F;
 u64 page_base_addr : 36;
 u64 mtype          : 2; // = (pte_entry &amp;gt;&amp;gt; 48) &amp;amp; 3;
 u64 prt            : 1; // = (pte_entry &amp;gt;&amp;gt; 51) &amp;amp; 1;
 u64 software       : 2; // = (pte_entry &amp;gt;&amp;gt; 52) &amp;amp; 3;
 u64 pde            : 1; // = (pte_entry &amp;gt;&amp;gt; 54) &amp;amp; 1;
 u64 __reserved_0   : 1;
 u64 further        : 1; // = (pte_entry &amp;gt;&amp;gt; 56) &amp;amp; 1;
 u64 gcr            : 1; // = (pte_entry &amp;gt;&amp;gt; 57) &amp;amp; 1;
 u64 llc_noalloc    : 1; // = (pte_entry &amp;gt;&amp;gt; 58) &amp;amp; 1;
} pte_t;

static inline pde_t decode_pde(u64 pde_raw) {
 pde_t pde         = *((pde_t*)(&amp;amp;pde_raw));
 pde.pte_base_addr = (u64)pde.pte_base_addr &amp;lt;&amp;lt; 6;
 return pde;
}

static inline pte_t decode_pte(u64 pde_raw) {
 pte_t pte          = *((pte_t*)(&amp;amp;pde_raw));
 pte.page_base_addr = (u64)pte.page_base_addr &amp;lt;&amp;lt; 12;
 return pte;
}

static inline u64 log2_range_round_up(u64 s, u64 e) {
 u64 x = e - s - 1;
 return (x == 0 || x == 1) ? 1 : 64 - __builtin_clzll(x);
}

void dev_linear_vram(amdgpu_t* dev, u64 phy_addr, size_t size, void* buf) {
 HDB_ASSERT(!((phy_addr &amp;amp; 3) || (size &amp;amp; 3)), "Must be page aligned address and size");

 size_t offset = lseek(dev-&amp;gt;vram_fd, phy_addr, SEEK_SET);
 HDB_ASSERT(offset == phy_addr, "Couldn't seek to the requested addr");

 offset = read(dev-&amp;gt;vram_fd, buf, size);
 HDB_ASSERT(offset == size, "Couldn't read the full requested size");
}

void dev_decode(amdgpu_t* dev, u32 vmid, u64 va_addr) {
 reg_gcmc_vm_fb_location_base_t fb_base_reg   = { 0 };
 reg_gcmc_vm_fb_location_top_t  fb_top_reg    = { 0 };
 reg_gcmc_vm_fb_offset_t        fb_offset_reg = { 0 };

 regs2_ioc_data_t ioc_data = { 0 };
 dev_op_reg32(
   dev, REG_GCMC_VM_FB_LOCATION_BASE, ioc_data, REG_OP_READ, &amp;amp;fb_base_reg.raw);
 dev_op_reg32(dev, REG_GCMC_VM_FB_LOCATION_TOP, ioc_data, REG_OP_READ, &amp;amp;fb_top_reg.raw);
 dev_op_reg32(dev, REG_GCMC_VM_FB_OFFSET, ioc_data, REG_OP_READ, &amp;amp;fb_offset_reg.raw);

 u64 fb_offset = (u64)fb_offset_reg.fb_offset;

 // TODO(hadi): add zfb mode support
 bool zfb = fb_top_reg.fb_top + 1 &amp;lt; fb_base_reg.fb_base;
 HDB_ASSERT(!zfb, "ZFB mode is not implemented yet!");

 // printf(
 //   "fb base: 0x%x\nfb_top: 0x%x\nfb_offset: 0x%x\n",
 //   fb_base_reg.raw,
 //   fb_top_reg.raw,
 //   fb_offset_reg.raw);

 gc_11_reg_t pt_start_lo_id = { 0 };
 gc_11_reg_t pt_start_hi_id = { 0 };
 gc_11_reg_t pt_end_lo_id   = { 0 };
 gc_11_reg_t pt_end_hi_id   = { 0 };
 gc_11_reg_t pt_base_hi_id  = { 0 };
 gc_11_reg_t pt_base_lo_id  = { 0 };
 gc_11_reg_t ctx_cntl_id    = { 0 };

 switch (vmid) {
 case 0:
  pt_start_lo_id = REG_GCVM_CONTEXT0_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT0_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT0_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT0_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT0_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT0_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT0_CNTL;
  break;
 case 1:
  pt_start_lo_id = REG_GCVM_CONTEXT1_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT1_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT1_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT1_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT1_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT1_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT1_CNTL;
  break;
 case 2:
  pt_start_lo_id = REG_GCVM_CONTEXT2_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT2_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT2_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT2_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT2_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT2_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT2_CNTL;
  break;
 case 3:
  pt_start_lo_id = REG_GCVM_CONTEXT3_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT3_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT3_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT3_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT3_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT3_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT3_CNTL;
  break;
 case 4:
  pt_start_lo_id = REG_GCVM_CONTEXT4_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT4_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT4_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT4_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT4_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT4_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT4_CNTL;
  break;
 case 5:
  pt_start_lo_id = REG_GCVM_CONTEXT5_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT5_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT5_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT5_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT5_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT5_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT5_CNTL;
  break;
 case 6:
  pt_start_lo_id = REG_GCVM_CONTEXT6_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT6_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT6_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT6_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT6_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT6_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT6_CNTL;
  break;
 case 7:
  pt_start_lo_id = REG_GCVM_CONTEXT7_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT7_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT7_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT7_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT7_CNTL;
  break;
 case 8:
  pt_start_lo_id = REG_GCVM_CONTEXT8_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT8_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT8_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT8_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT7_CNTL;
  break;
 case 9:
  pt_start_lo_id = REG_GCVM_CONTEXT9_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT9_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT9_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT9_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT7_CNTL;
  break;
 case 10:
  pt_start_lo_id = REG_GCVM_CONTEXT10_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT10_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT10_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT10_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT10_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT10_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT10_CNTL;
  break;
 case 11:
  pt_start_lo_id = REG_GCVM_CONTEXT11_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT11_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT11_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT11_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT11_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT11_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT11_CNTL;
  break;
 case 12:
  pt_start_lo_id = REG_GCVM_CONTEXT12_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT12_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT12_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT12_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT12_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT12_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT12_CNTL;
  break;
 case 13:
  pt_start_lo_id = REG_GCVM_CONTEXT13_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT13_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT13_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT13_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT13_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT13_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT13_CNTL;
  break;
 case 14:
  pt_start_lo_id = REG_GCVM_CONTEXT14_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT14_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT14_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT14_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT14_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT14_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT14_CNTL;
  break;
 case 15:
  pt_start_lo_id = REG_GCVM_CONTEXT15_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT15_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT15_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT15_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT15_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT15_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT15_CNTL;
  break;
 default: HDB_ASSERT(false, "Out of range VMID 0-15 trying to access %u", vmid);
 }

 // all the types of the contexts are the same so will just use 0 but pass the correct
 // register enum to the read function
 reg_gcvm_context0_page_table_start_addr_lo32_t pt_start_lo = { 0 };
 reg_gcvm_context0_page_table_start_addr_hi32_t pt_start_hi = { 0 };
 reg_gcvm_context0_page_table_end_addr_lo32_t   pt_end_lo   = { 0 };
 reg_gcvm_context0_page_table_end_addr_hi32_t   pt_end_hi   = { 0 };
 reg_gcvm_context0_page_table_base_addr_lo32_t  pt_base_lo  = { 0 };
 reg_gcvm_context0_page_table_base_addr_hi32_t  pt_base_hi  = { 0 };
 reg_gcvm_context0_cntl_t                       ctx_cntl    = { 0 };

 dev_op_reg32(dev, pt_start_lo_id, ioc_data, REG_OP_READ, &amp;amp;pt_start_lo.raw);
 dev_op_reg32(dev, pt_start_hi_id, ioc_data, REG_OP_READ, &amp;amp;pt_start_hi.raw);
 dev_op_reg32(dev, pt_end_lo_id, ioc_data, REG_OP_READ, &amp;amp;pt_end_lo.raw);
 dev_op_reg32(dev, pt_end_hi_id, ioc_data, REG_OP_READ, &amp;amp;pt_end_hi.raw);
 dev_op_reg32(dev, pt_base_lo_id, ioc_data, REG_OP_READ, &amp;amp;pt_base_lo.raw);
 dev_op_reg32(dev, pt_base_hi_id, ioc_data, REG_OP_READ, &amp;amp;pt_base_hi.raw);
 dev_op_reg32(dev, ctx_cntl_id, ioc_data, REG_OP_READ, &amp;amp;ctx_cntl.raw);

 u64 pt_start_addr = ((u64)pt_start_lo.raw &amp;lt;&amp;lt; 12) | ((u64)pt_start_hi.raw &amp;lt;&amp;lt; 44);
 u64 pt_end_addr   = ((u64)pt_end_lo.raw &amp;lt;&amp;lt; 12) | ((u64)pt_end_hi.raw &amp;lt;&amp;lt; 44);
 u64 pt_base_addr  = ((u64)pt_base_lo.raw &amp;lt;&amp;lt; 0) | ((u64)pt_base_hi.raw &amp;lt;&amp;lt; 32);
 u32 pt_depth      = ctx_cntl.page_table_depth;
 u32 ptb_size      = ctx_cntl.page_table_block_size;

 HDB_ASSERT(pt_base_addr != 0xffffffffffffffffull, "Invalid page table base addr");

 printf(
   "\tPage Table Start: 0x%lx\n\tPage Table End: 0x%lx\n\tPage Table Base: "
   "0x%lx\n\tPage Table Depth: %u\n\tBlock Size: %u\n",
   pt_start_addr,
   pt_end_addr,
   pt_base_addr,
   pt_depth,
   ptb_size);

 // decode base PDB
 pde_t pde = decode_pde(pt_base_addr);
 pt_base_addr -= fb_offset * !pde.system; // substract only on vram

 u64 pt_last_byte_addr = pt_end_addr + 0xfff; // 0xfff is 1 page
 HDB_ASSERT(
   pt_start_addr &amp;lt;= va_addr || va_addr &amp;lt; pt_last_byte_addr,
   "Invalid virtual address outside the range of the root page table of this vm");

 va_addr -= pt_start_addr;
 //
 // Size of the first PDB depends on the total coverage of the
 // page table and the PAGE_TABLE_BLOCK_SIZE.
 // Entire table takes ceil(log2(total_vm_size)) bits
 // All PDBs except the first one take 9 bits each
 // The PTB covers at least 2 MiB (21 bits)
 // And PAGE_TABLE_BLOCK_SIZE is log2(num 2MiB ranges PTB covers)
 // As such, the formula for the size of the first PDB is:
 //                       PDB1, PDB0, etc.      PTB covers at least 2 MiB
 //                                        Block size can make it cover more
 //   total_vm_bits - (9 * num_middle_pdbs) - (page_table_block_size + 21)
 //
 // we need the total range range here not the last byte addr like above
 u32 total_vaddr_bits = log2_range_round_up(pt_start_addr, pt_end_addr + 0x1000);

 u32 total_pdb_bits = total_vaddr_bits;
 // substract everything from the va_addr to leave just the pdb bits
 total_pdb_bits -= 9 * (pt_depth - 1); // middle PDBs each is 9 bits
 total_pdb_bits -= (ptb_size + 21);    // at least 2mb(21) bits + ptb_size

 // u64 va_mask = (1ull &amp;lt;&amp;lt; total_pdb_bits) - 1;
 // va_mask &amp;lt;&amp;lt;= (total_vaddr_bits - total_pdb_bits);

 // pde_t pdes[8]  = { 0 };
 // u32   curr_pde = 0;
 // u64   pde_addr = 0;
 // u64  loop_pde = pt_base_addr;

 if (pt_depth == 0) { HDB_ASSERT(false, "DEPTH = 0 is not implemented yet"); }

 pde_t curr_pde    = pde;
 u64   entry_bits  = 0;
 s32   curr_depth  = pt_depth;
 bool  pde0_is_pte = false;
 // walk all middle PDEs
 while (curr_depth &amp;gt; 0) {
  // printf("pde(%u):0x%lx \n", curr_depth, curr_pde.pte_base_addr);
  u64 next_entry_addr = 0;

  u32 shift_amount = total_vaddr_bits;
  shift_amount -= total_pdb_bits;
  // for each pdb shift 9 more
  shift_amount -= ((pt_depth - curr_depth) * 9);

  // shift address and mask out unused bits
  u64 next_pde_idx = va_addr &amp;gt;&amp;gt; shift_amount;
  next_pde_idx &amp;amp;= 0x1ff;

  // if on vram we need to apply this offset
  if (!curr_pde.system) curr_pde.pte_base_addr -= fb_offset;

  next_entry_addr = curr_pde.pte_base_addr + next_pde_idx * 8;
  curr_depth--;

  if (!curr_pde.system) {
   dev_linear_vram(dev, next_entry_addr, 8, &amp;amp;entry_bits);
   curr_pde = decode_pde(entry_bits);
   printf(
     "\tPage Dir Entry(%u):\n\t  Addr:0x%lx\n\t  Base: 0x%lx\n\n\t        ↓\n\n",
     curr_depth,
     next_entry_addr,
     curr_pde.pte_base_addr);
  } else {
   HDB_ASSERT(false, "GTT physical memory access is not implemented yet");
  }

  if (!curr_pde.valid) { break; }

  if (curr_pde.pte) {
   // PDB0 can act as a pte
   // also I'm making an assumption here that UMRs code doesn't make
   // that the the PDB0 as PTE path can't have the further bit set
   pde0_is_pte = true;
   break;
  }
 }

 if (pde0_is_pte) { HDB_ASSERT(false, "PDE0 as PTE is not implemented yet"); }

 // page_table_block_size is the number of 2MiB regions covered by a PTB
 // If we set it to 0, then PTB cover 2 MiB
 // If it's 9 PTB cover 1024 MiB
 // pde0_block_fragment_size tells us how many 4 KiB regions each PTE covers
 // If it's 0 PTEs cover 4 KiB
 // If it's 9 PTEs cover 2 MiB
 // So the number of PTEs in a PTB is 2^(9+ptbs-pbfs)
 //
 // size here is actually the log_2 of the size
 u32 pte_page_size  = curr_pde.frag_size;
 u32 ptes_per_ptb   = 9 + ptb_size - pte_page_size;
 u64 pte_index_mask = (1ul &amp;lt;&amp;lt; ptes_per_ptb) - 1;

 u32 pte_bits_count   = pte_page_size + 12;
 u64 page_offset_mask = (1ul &amp;lt;&amp;lt; pte_bits_count) - 1; // minimum of 12

 u64 pte_index = (va_addr &amp;gt;&amp;gt; pte_bits_count) &amp;amp; pte_index_mask;
 u64 pte_addr  = curr_pde.pte_base_addr + pte_index * 8;

 pte_t pte = { 0 };
 if (!curr_pde.system) {
  dev_linear_vram(dev, pte_addr, 8, &amp;amp;entry_bits);
  pte = decode_pte(entry_bits);

  printf("\tPage Table Entry: 0x%lx\n", pte.page_base_addr);
 } else {
  HDB_ASSERT(false, "GTT physical memory access is not implemented yet");
 }

 if (pte.further) { HDB_ASSERT(false, "PTE as PDE walking is not implemented yet"); }
 if (!pte.system) pte.page_base_addr -= fb_offset;

 u64 offset_in_page = va_addr &amp;amp; page_offset_mask;
 u64 physical_addr  = pte.page_base_addr + offset_in_page;
 printf("\tFinal Physical Address: 0x%lx\n", physical_addr);
}&lt;/code&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Other processes need to have a s_trap instruction or have trap on exception flags set, which is not true for most normal GPU processes. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Available since RDNA3, if I’m not mistaken. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;VGPRs are unique per thread, and SGPRs are unique per wave ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We can get that by subtracting the current program counter from the address of the code buffer. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46193931</guid><pubDate>Mon, 08 Dec 2025 16:06:14 +0000</pubDate></item><item><title>No more O'Reilly subscriptions for me</title><link>https://zerokspot.com/weblog/2025/12/05/no-more-oreilly-subscriptions-for-me/</link><description>&lt;doc fingerprint="725213576d948dc6"&gt;
  &lt;main&gt;
    &lt;p&gt;For the last two years I’ve had an O’Reilly subscription. Their offer is quite attractive with unlimited access to books not only by O’Reilly but also Manning and others. The catalog is just enourmous and covers pretty much every technical book around software engineering et al. that I might ever want to read. There are also tons of other learning resources in there like conference recordings and webinars, but I’m mostly there for the books. Unfortunately, I cannot read technical books fast and definitely not fast enough to make the subscription be worth $500 per year.&lt;/p&gt;
    &lt;p&gt;Another problem for me is the usability of the mobile client. I mostly read books on my tablet but also like to use some spare time during commutes to make some progress on my phone. The synchronization there is extremely spotty and the app, when being evicted and reloaded by the operating system, throws me more often than not back to the start screen instead of reopening the previously open book at the right page. I also haven’t found a theme that I enjoy as much as the ones offered by Apple Books or the Kindle app and so reading hasn’t been all that enjoyable for me.&lt;/p&gt;
    &lt;p&gt;All of this together will most likely not make me renew my subscription for the new year. Given the price, it will be probably cheaper for me to buy only the books that I want from Kobo et al. where I can get O’Reilly books without DRM and keep them beyond any subscription limit. I also just noticed that I still have some credits left from the time I’ve had a Manning subscription 😂&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46194063</guid><pubDate>Mon, 08 Dec 2025 16:14:52 +0000</pubDate></item><item><title>Google Confirms Android Attacks-No Fix for Most Samsung Users</title><link>https://www.forbes.com/sites/zakdoffman/2025/12/08/google-confirms-android-attacks-no-fix-for-most-samsung-users/</link><description>&lt;doc fingerprint="b532c58180f452a3"&gt;
  &lt;main&gt;
    &lt;p&gt;Android is under attack. Google issued a warning on Dec.1 along with what is essentially an emergency update. This was rushed out to all Pixel users. But for most Samsung users, these fixes are not yet available, despite attacks now underway.&lt;/p&gt;
    &lt;p&gt;Google confirms CVE-2025-48633 and CVE-2025-48572 “may be under limited, targeted exploitation," with attacks that can achieve “remote denial of service" on target smartphones "with no additional execution privileges needed.”&lt;/p&gt;
    &lt;p&gt;Samsung confirmed its own fixes within hours of Google’s warning. It also fixed three other vulnerabilities disclosed by Google’s Project Zero, which studies zero-days "in the hardware and software systems that are depended upon by users around the world.”&lt;/p&gt;
    &lt;p&gt;Just 24 hours after Google confirmed the Android attacks, the U.S. cyber defense agency issued its own warning, mandating federal staff update or stop using phones. “Android’s Framework,” CISA says on its known exploited vulnerability website, “contains an unspecified vulnerability that allows for privilege escalation.”&lt;/p&gt;
    &lt;p&gt;But as always when zero-day attacks are disclosed, Android’s disconnect is highlighted. “Samsung is the king of Android,” Android Authority pronounced over the weekend. “Its global market share among Android makers exceeds 30%. In other words, almost one in three people who buy an Android phone end up choosing Samsung.”&lt;/p&gt;
    &lt;p&gt;Samsung should come first — not Pixel, with its modest market share. But that won’t happen. Samsung bears responsibility for changing an update cycle that still runs a full month to deploy critical fixes to its user base. And it bears responsibility for the lack of seamless updates on all but the Galaxy S25 and one random, mid-range phone.&lt;/p&gt;
    &lt;p&gt;But in reality, Samsung (and the other Android OEMs) cannot compete with Google and its unique control over hardware and software. Its phones will always come first. First to new versions of the OS, first to new feature releases, first to security updates. That’s why One UI 7 and One UI 8. (Android 15 and 16) were so delayed, frustrating so many.&lt;/p&gt;
    &lt;p&gt;All Samsung Galaxy phones will get the update — assuming they’re on the monthly schedule. And some may get the updates even if they’re not. But it will deploy by model, region and carrier. Bit by bit. And in a world where Pixel is quick and Apple is quick, Samsung cannot afford to be slow. It seems inevitable that Android must change.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46194315</guid><pubDate>Mon, 08 Dec 2025 16:32:50 +0000</pubDate></item><item><title>Let's put Tailscale on a jailbroken Kindle</title><link>https://tailscale.com/blog/tailscale-jailbroken-kindle</link><description>&lt;doc fingerprint="77000a1e01fd3fdc"&gt;
  &lt;main&gt;
    &lt;p&gt;“It’s a rite of passage to run Tailscale on weird devices.”&lt;/p&gt;
    &lt;p&gt;So writes Mitanshu Sukhwani on his blog, detailing the steps for getting Tailscale onto a jailbroken Kindle. Getting there, and seeing a kindle entry with a satisfying green dot in your Tailscale admin console, takes some doing. But take the trip, and you’ll end up with an e-reader that can run some neat unofficial apps, and is more open to third-party and DRM-free ebooks. And with a Tailscale connection, it’s easier to connect to files and a command line on your underpowered little Linux slab.&lt;/p&gt;
    &lt;p&gt;“For me, it's the freedom of being able to do anything with the device I own,” Sukhwani writes by email. “What I can do with the freedom is a different story.”&lt;/p&gt;
    &lt;head rend="h2"&gt;What is a jailbroken Kindle, exactly?&lt;/head&gt;
    &lt;p&gt;Jailbreaking refers to removing the software restrictions on a device put there by its maker. Getting around these restrictions, typically by gaining “root” or administrative access, allows for accessing operating system internals, running unapproved software, and generally doing more things than a manufacturer intended. With the Kindle, you still get the standard Kindle reading experience, including Amazon's store and the ability to send the Kindle books from apps like Libby. You just add many more options, too.&lt;/p&gt;
    &lt;p&gt;The term gained purchase after the first iPhone’s debut in mid-2007; since then, nearly every device with a restricted environment has gained its own jailbreaking scene, including Kindles (debuting five months after the iPhone).&lt;/p&gt;
    &lt;p&gt;Kindle jailbreaks come along every so often. Right now, an unlocking scheme based on Amazon’s own lockscreen ads, “AdBreak,” is available for all but the most up-to-date Kindles (earlier than firmware version 5.18.5.0.2). I know this because I wrote this paragraph and the next on my 11th-generation Kindle, using the open-source Textadept editor, a Bluetooth keyboard, and Tailscale to move this draft file around.&lt;/p&gt;
    &lt;p&gt;One paragraph doesn’t seem that impressive until you consider that on a standard Kindle, you cannot do any of that. Transferring files by SSH, or Taildrop, is certainly not allowed. And that’s in addition to other upgrades you can get by jailbreaking a Kindle, including the feature-rich, customizable e-reader KOReader, and lots of little apps available in repositories like KindleForge.&lt;/p&gt;
    &lt;p&gt;If your Kindle has been connected to Wi-Fi all this time (as of early December 2025), it may have automatically updated itself and no longer be ready for jailbreaking. If you think it still has a chance, immediately put it into airplane mode and follow along.&lt;/p&gt;
    &lt;p&gt;Obligatory notice here: You’re running a risk of bricking your device (having it become unresponsive and unrecoverable) and voiding your warranty when you do this. That having been noted, let's dig further.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Tailscale adds to a jailbroken Kindle&lt;/head&gt;
    &lt;p&gt;Tailscale isn’t necessary on a jailbroken Kindle, but it really helps. Here are some of the ways Tailscale makes messing about with an opened-up Kindle more fun:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A persistent IP address (100.xx.yyy.zzz), just like any other Tailscale device, instead of having to remember yet another 192.168.random.number&lt;/item&gt;
      &lt;item&gt;Easier SSH access with magicDNS: ssh root@kindle and you’re in&lt;/item&gt;
      &lt;item&gt;Taildrop for sending files to whatever Kindle directory you want&lt;/item&gt;
      &lt;item&gt;Setting up a self-hosted Calibre Web library with Tailscale, then securely grabbing books from it anywhere with KOReader.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Key to the Kindle-plus-Tailscale experience is an easier way (SSH and Taildrop) to get epub, mobi, and other e-book and document formats into the /documents folder, ready for your KOReader sessions. Tailscale also helps with setting up some of the key jailbreak apps, saving you from plugging and unplugging the Kindle into a computer via USB cord (and then finding a second USB cord, because the first one never works, for some reason).&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting your Kindle ready&lt;/head&gt;
    &lt;p&gt;What follows is by no means a comprehensive guide to jailbreaking and accessing your Kindle. You will want to read the documentation for each tool and app closely. Pay particular attention to which Kindle you have, which version number of the Kindle firmware it’s running, and how much space you have left on that device.&lt;/p&gt;
    &lt;p&gt;The first step is to check your Kindle’s version number (Settings &amp;gt; Device info) and see if there is a jailbreak method available for it. The Kindle Modding Wiki is the jailbreaking community’s go-to resource. As of this writing, there is a “WinterBreak” process available for Kindles running firmware below 15.18.1, and AdBreak is available for firmwares from 15.18.1 through 5.18.5.0.1.&lt;/p&gt;
    &lt;p&gt;If your Kindle’s version number fits one of those ranges, put it in Airplane mode and move on. If not, you’re going to have to wait until the next jailbreak method comes along.&lt;/p&gt;
    &lt;head rend="h2"&gt;The actual jailbreaking part&lt;/head&gt;
    &lt;p&gt;Before you dive in, have a computer (PC, Mac, or Linux) and USB cable that works with your Kindle handy. Have your Kindle on reliable Wi-Fi, like your home network—but don’t take your Kindle off airplane mode if you’ve been keeping it that way.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Follow these steps to jailbreak your Kindle. The techniques are different, but you may need to do some other tasks, like enable advertisements, or fill your Kindle with junk files to prevent automatic updates midway through the process.&lt;/item&gt;
      &lt;item&gt;Install a hotfix and disable over-the-air updates so that you can keep your Kindle on Wi-Fi and not have its jailbreak undone&lt;/item&gt;
      &lt;item&gt;Install the Kindle Unified Application Launcher (KUAL) and MRPI (MobileRead Package Installer). KUAL is vital to installing most jailbroken apps, including Tailscale.&lt;/item&gt;
      &lt;item&gt;You will almost certainly want to install KOReader, too.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Those bits above are standard jailbreaking procedures. If you want Tailscale on your Kindle, you’ll go a bit further.&lt;/p&gt;
    &lt;head rend="h2"&gt;Adding Tailscale to a jailbroken Kindle&lt;/head&gt;
    &lt;p&gt;Make sure you have KUAL and MRPI installed and working. Next up: install this “simple” version of USBNetworking for Kindle.&lt;/p&gt;
    &lt;p&gt;Before you go further, you’ll want to choose between Mitanshu’s “standard” Tailscale repository, or the fork of it that enables Taildrop. I recommend the Taildrop-enabled fork; if it goes wrong, or stops being updated, it’s fairly easy (relative to doing this kind of project) to wipe it and go back to Mitanshu’s “vanilla” version.Either way, you’ll want to get USB access to your Kindle for this next part. If you toggled on USBNetworking to try it out, toggle it off; you can’t get USB access while it’s running, as its name somewhat implies.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download the Tailscale/KUAL repository of your choice using git clone or download a ZIP from the Code button on GitHub&lt;/item&gt;
      &lt;item&gt;Head to Tailscale’s page of static Linux binaries and grab the latest arm (not arm64) release&lt;/item&gt;
      &lt;item&gt;Copy the tailscale and tailscaled binaries from the Tailscale download and place them into the /extensions/tailscale/bin directory of the KUAL/Kindle repository you’ll be copying over&lt;/item&gt;
      &lt;item&gt;Head to your Tailscale admin console and generate an authentication key. Name it something like kindle; you’ll want to enable the “Reusable” and “Pre-approved” options. Copy the key that is generated.&lt;/item&gt;
      &lt;item&gt;Open the file extensions/tailscale/config/auth_key.txt for editing while it is on your (non-Kindle) computer. Paste in the key text you generated.&lt;/item&gt;
      &lt;item&gt;If you’re using the variant with Taildrop, you can set a custom directory in which to deliver Taildrop files by editing extensions/tailscale/config/taildrop_dir.txt; setting /mnt/us/documents makes sense if you’re mostly sending yourself things to read in KOReader.&lt;/item&gt;
      &lt;item&gt;Head into the extensions folder on your computer and copy the tailscale folder you’ve set up into the extensions folder on your Kindle.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With all that done, open up KUAL on your Kindle. Go into USBNetLite and click USBNetwork Status to ensure it is enabled (tap the Toggle button if not). Go back (with the “/” button at the bottom), tap Tailscale, and first tap Start Tailscaled (note the “d” at the end). Wait about 10 seconds to give the Tailscaled daemon time to start, then tap Start Tailscale.&lt;/p&gt;
    &lt;p&gt;If everything is settled, you should be able to see your Kindle as connected on your Tailscale admin console. Once you’ve finished smiling to yourself, click the three dots on the right-hand side of the Kindle row and select “Disable key expiry.” In most situations, you’re better off not having to patch a new key value into a Kindle text file every few months.&lt;/p&gt;
    &lt;head rend="h2"&gt;Enjoy your (slightly) less wonky Kindle&lt;/head&gt;
    &lt;p&gt;With Tailscale installed, it’s easier to get into your Kindle via SSH for file management and installing and configuring other apps. Getting a Bluetooth keyboard to work via the Kindle’s quirky command-line Bluetooth interface would not have been fun using a touchscreen keyboard.&lt;/p&gt;
    &lt;p&gt;Because the Kindle is on your tailnet, it can access anything else you have hosted there. Kindles set up this way can use tools like the Shortcut Browser to become dashboards for Home Assistant, or access a self-hosted Calibre-Web e-book server (with some tweaking).&lt;/p&gt;
    &lt;p&gt;Having Taildrop handy, and having it drop files directly into the documents folder, is probably my favorite upgrade. I was on my phone, at a train station, when I came across Annalee Newitz’s Automatic Noodle at Bookshop.org. I bought it on my phone and downloaded the DRM-free epub file. When I got home, I opened and unlocked my Kindle, sent the epub to the Kindle via Taildrop, then tapped Receive Taildrop Files in the Tailscale app inside KUAL. Epubs, PDFs, comic book archives, DjVu files—they’re all ready to be dropped in.&lt;/p&gt;
    &lt;p&gt;If you’ve gotten Tailscale to run on weird (or just uncommon) devices, we’d more than love to hear about it. Let us know on Reddit, Discord, Bluesky, Mastodon, or LinkedIn.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46194337</guid><pubDate>Mon, 08 Dec 2025 16:34:08 +0000</pubDate></item><item><title>Hunting for North Korean Fiber Optic Cables</title><link>https://nkinternet.com/2025/12/08/hunting-for-north-korean-fiber-optic-cables/</link><description>&lt;doc fingerprint="2fecda19652117ea"&gt;
  &lt;main&gt;
    &lt;p&gt;Before we go any further, one thing that I want to make clear is that the word assume is going to be doing some heavy lifting throughout this post. This was a rabbit hole that I recently went down and I probably have more questions than answers, but I still wanted to document what I had found so far. If you have additional information or findings you want to share, as always feel free to reach out: contact@nkinternet.com.&lt;/p&gt;
    &lt;p&gt;It all started with a PowerPoint that I came across a few weeks ago. It was presented by the DPRK to the ICAO on the state of their aviation industry and their ADS-B deployment inside North Korea. However, one slide in particular caught my eye because it showed a fiber optic cable running across the country&lt;/p&gt;
    &lt;p&gt;This got me wondering more about the physical layout of the network inside North Korea. From the map we know that there’s a connection between Pyongyang and Odaejin, although given the mountains in the middle of the country it probably isn’t a direct link. There isn’t a lot of information on fiber in North Korea, but there are a few outside sources that help provide clues about how things might be laid out.&lt;/p&gt;
    &lt;p&gt;Historic Fiber Information&lt;/p&gt;
    &lt;p&gt;38North first reported the connection from Russia’s TTK to the DPRK over the Korea–Russia Friendship Bridge back in 2017. Additionally, a picture found on Flickr looking toward Tumangang after the bridge doesn’t show any utility poles and instead seems to display some kind of infrastructure in the grass to the side of the tracks. Assuming this interpretation is correct, the fiber is likely buried underground as it enters the country and passes through the vicinity of Tumangang Station.&lt;/p&gt;
    &lt;p&gt;According to a report from The Nautilus Institute we can gather a few additional details about the internet inside North Korea&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;One of the first lines was installed in September 1995 between Pyongyang and Hamhung&lt;/item&gt;
      &lt;item&gt;In February 1998 a link between Pyongyang and Sinuiju was completed&lt;/item&gt;
      &lt;item&gt;As of 2000, DPRK’s operational optical fiber telecom lines included: Pyongyang – Hamhung; Pyongyang – Sinuiju including all cities and counties in North Pyongan Province; Hamhung Rajin-Sonbong; Rajin-Songbong – Hunchun (China), Pyongyang – Nampo.&lt;/item&gt;
      &lt;item&gt;In 2003 the original domestic cell phone network was built for North Korean citizens in Pyongyang, Namp’o, reportedly in all provincial capitals, on the Pyongyang-Myohyangsan tourist highway, and the Pyongyang-Kaesong and Wonsan-Hamhung highways&lt;/item&gt;
      &lt;item&gt;The Kwangmyong network’s data is transmitted via fiber optic cable with a backbone capacity of 2.5 GB per second between all the provinces.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Based on these notes, it starts to paint a picture that the fiber link coming from Russia likely travels down the east coast of the DPRK before connecting to Pyongyang. Several city pairs—Pyongyang–Hamhung and Rajin–Sonbong—line up with earlier deployments of east-coast fiber infrastructure.&lt;/p&gt;
    &lt;p&gt;Kwangmyong Internal Topology&lt;/p&gt;
    &lt;p&gt;The report also notes that all of the provinces in North Korea were connected to the Kwangmyong via fiber. The Kwangmyong for those not familiar is the intranet that most citizens in the DPRK can access as they do not have access to the outside internet. While not much information is available about the Kwangmyong, these notes from Choi Sung, Professor of Computer Science at Namseoul University provides some additional details on how the network is laid how, as well as information on the regional networks that are connected. A map provided in his notes shows some of the main points of the Kwangmyong with three of them located along the northeast of North Korea.&lt;/p&gt;
    &lt;p&gt;Railways, Roads, and Practical Fiber Routing&lt;/p&gt;
    &lt;p&gt;This starts to paint a rough picture of how the network is physically deployed in North Korea but we can also look to some outside sources to get some confirmation. 38North once again provides some great detail on cell phone towers in North Korea. The interesting thing being an apparent line down the east coast which follows major roads and highways but would also in theory have easier access to the fiber back haul to support the cell network.&lt;/p&gt;
    &lt;p&gt;All of this seems to suggest that the fiber lines were run along major roads and railways up the east coast. A map from Beyond Parallel shows the major rail lines, which has the Pyongra line up the east coast.&lt;/p&gt;
    &lt;p&gt;Looking For Clues Along the Railway&lt;/p&gt;
    &lt;p&gt;Some additional digging for pictures from along the line suggest that there is infrastructure deployed along the tracks, although it’s difficult to confirm from pictures exactly what is buried. The following shows what appears to be a junction box at the base of a pole along the line.&lt;/p&gt;
    &lt;p&gt;The line does have a path along it as well with mile markers. While it is used by bikes and pedestrians, it provides a nice path for supporting fiber and other communications runs along the tracks.&lt;/p&gt;
    &lt;p&gt;The Pyongra line also crosses through the mountains at points but it is assumed at certain junctions the fiber was laid along the AH 6/National Highway 7 up the coast as there are parts of the line discovered that do not have a path along the tracks. In these places it is assumed they follow the road, although finding pictures of the highway to further examine is challenging.&lt;/p&gt;
    &lt;p&gt;Lastly at certain stations we can see utility boxes along the side of the track suggesting buried conduits/cables are laid along the tracks.&lt;/p&gt;
    &lt;p&gt;From a video taken in 2012 there does appear to be some signs of objects along the tracks, although difficult to confirm due to the video quality. The screenshot below is the clearest I could find of a rectangular box buried in a clearing along the line.&lt;/p&gt;
    &lt;p&gt;Based on this information of what is confirmed and looking at major cities, it appears there is a route that follows Pyongyang → Wonsan → Hamhung → Chongjin → Rajin → Tumangang which follows the Pyongra line as well as the AH 6/National Highway 7 up the coast. The following map highlights a rough path.&lt;/p&gt;
    &lt;p&gt;Interestingly by mapping out the possible fiber locations we can start to draw conclusions based on other sources. According to a video by Cappy’s Army he proposes that when the US Navy Seals landed in NOrth Korea in 2019 the most likely place this would have occurred is Sinpo. As the goal was to depoy a covert listening device this could also line up with supporting the idea that a fiber backbone runs down the east coast of North Korea as Sinpo would be relatively close.&lt;/p&gt;
    &lt;p&gt;What Does This Mean For the Network?&lt;/p&gt;
    &lt;p&gt;In addition to the fiber link via Russia, the other fiber optic cable into North Korea comes in via China by way of Sinuiju and Dandong. Although we don’t know for sure where servers are deployed inside North Korea, based on the map of Kwangmyong the first assumption is that things are mainly centralized in Pyongyang.&lt;/p&gt;
    &lt;p&gt;Out of the 1,024 IPs assigned to North Korea we observe the following behavior based on the CIDR block:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;175.45.176.0/24 is exclusively routed via China Unicom&lt;/item&gt;
      &lt;item&gt;175.45.177.0/24 is exclusively routed via Russia TransTelekom&lt;/item&gt;
      &lt;item&gt;175.45.178.0/24 is dual-homed and can take either path before crossing into North Korea&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With this information in mind, running a traceroute with the TCP flag set gives us a slightly better look at how traffic behaves once it reaches the country. For the following tests we’re going to assume there is a fiber path on the west coming in from China toward Pyongyang, as well as a path on the east side coming from Russia.&lt;/p&gt;
    &lt;p&gt;From the US east coast to 175.45.176.71, the final hop in China before entering North Korea shows roughly 50 ms of additional latency before reaching the DPRK host. This suggests there may be extra devices, distance, or internal routing inside the country before the packet reaches its final destination.&lt;/p&gt;
    &lt;quote&gt;10 103.35.255.254 (103.35.255.254) 234.306 ms 234.082 ms 234.329 ms&lt;lb/&gt;11 * * *&lt;lb/&gt;12 * * *&lt;lb/&gt;13 * * *&lt;lb/&gt;14 175.45.176.71 (175.45.176.71) 296.081 ms 294.795 ms 294.605 ms&lt;lb/&gt;15 175.45.176.71 (175.45.176.71) 282.938 ms 284.446 ms 282.227 ms&lt;/quote&gt;
    &lt;p&gt;Interestingly, running a traceroute to 175.45.177.10 shows a similar pattern in terms of missing hops, but with much lower internal latency. In fact, the ~4 ms difference between the last Russian router and the DPRK host suggests the handoff between Russia and North Korea happens very close—network-wise—to where this device is located. This contrasts with the China path, which appears to take a longer or more complex route before reaching its final destination.&lt;/p&gt;
    &lt;quote&gt;10 188.43.225.153 185.192 ms 183.649 ms 189.089 ms&lt;lb/&gt;11 * *&lt;lb/&gt;12 * *&lt;lb/&gt;13 * *&lt;lb/&gt;14 175.45.177.10 195.996 ms 186.801 ms 186.353 ms&lt;lb/&gt;15 175.45.177.10 188.886 ms 201.103 ms 193.334&lt;/quote&gt;
    &lt;p&gt;If everything is centralized in Pyongyang this would mean the handoff from Russia is completed in Pyongyang as well. However, it could also indicate that 175.45.177.0/24 is not hosted in Pyongyang at all and is instead located closer to the Russia–North Korea border. More testing is definitely required however before any conclusions can be drawn about where these devices physically reside.&lt;/p&gt;
    &lt;p&gt;What can we learn from all of this?&lt;/p&gt;
    &lt;p&gt;Making some assumptions we can get a better idea of how the internet works and is laid out inside North Korea. While not much is officially confirmed using some other sources we can get a possible idea of how things work. As mentioned at the start, the word assume does a lot of heavy lifting. However if you do have other information or ideas feel free to reach out at contact@nkinternet.com&lt;/p&gt;
    &lt;head rend="h3"&gt;Discover more from North Korean Internet&lt;/head&gt;
    &lt;p&gt;Subscribe to get the latest posts sent to your email.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46194384</guid><pubDate>Mon, 08 Dec 2025 16:38:08 +0000</pubDate></item><item><title>Microsoft has a problem: nobody wants to buy or use its shoddy AI products</title><link>https://www.windowscentral.com/artificial-intelligence/microsoft-has-a-problem-nobody-wants-to-buy-or-use-its-shoddy-ai</link><description>&lt;doc fingerprint="2f162c409bb6c8d7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Microsoft has a problem: nobody wants to buy or use its shoddy AI products — as Google's AI growth begins to outpace Copilot products&lt;/head&gt;
    &lt;p&gt;A new report details how Microsoft has cut some internal goals for its AI sales people, why? Nobody wants to use its weak products.&lt;/p&gt;
    &lt;p&gt;Enjoy our content? Make sure to set Windows Central as a preferred source in Google Search, and find out why you should so that you can stay up-to-date on the latest news, reviews, features, and more.&lt;/p&gt;
    &lt;p&gt;If there's one thing that typifies Microsoft under CEO Satya Nadella's tenure: it's a general inability to connect with customers.&lt;/p&gt;
    &lt;p&gt;Microsoft shut down its retail arm quietly over the past few years, closed up shop on mountains of consumer products, while drifting haphazardly from tech fad to tech fad. From blockchain to "metaverse" and now to artificial intelligence — Microsoft CEO Satya Nadella can't seem to prioritize effectively, and the cracks are starting to shine through.&lt;/p&gt;
    &lt;p&gt;A recent report from The Information detailed how Microsoft's internal AI efforts are going awry, with cut forecasts and sales goals for its Azure AI products across the board. The Information said that Microsoft's sales people are "struggling" to meet goals, owing to a complete lack of demand. Microsoft denied the reports, but it can't deny market share growth trends — all of which point to Google Gemini surging ahead.&lt;/p&gt;
    &lt;p&gt;Last week we wrote about how Microsoft Copilot's backend partner OpenAI issued a "code red" situation. ChatGPT has fallen behind Google Gemini in problem solving, and Nano Banana image generation has outpaced OpenAI's own DALLE by leaps and bounds.&lt;/p&gt;
    &lt;p&gt;With OpenAI's business model under constant scrutiny and racking up genuinely dangerous levels of debt, it's become a cascading problem for Microsoft to have tied up layer upon layer of its business in what might end up being something of a lame duck.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;#&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;Generative AI Chatbot&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;AI Search Market Share&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;Estimated Quarterly User Growth&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;1&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;ChatGPT (excluding Copilot)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;61.30%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;7% ▲&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;2&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Microsoft Copilot&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;14.10%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;2% ▲&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;3&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Google Gemini&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;13.40%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;12% ▲&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;4&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Perplexity&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;6.40%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;4% ▲&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;5&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Claude AI&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;3.80%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;14% ▲&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;6&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Grok&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.60%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;6% ▲&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;7&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Deepseek&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.20%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;10% ▲&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There are reams of research that suggest agentic AI tools require human intervention at a frequency ratio that makes them cost ineffective, but Microsoft seems unbothered that its tools are poorly conceived.&lt;/p&gt;
    &lt;p&gt;In any case, OpenAI is supposedly going to launch future models of ChatGPT early in attempts to combat the rise of Google Gemini. I suspect the issues are deeper for Microsoft, who have worked tirelessly under Satya Nadella to create doubt around its products.&lt;/p&gt;
    &lt;p&gt;All the latest news, reviews, and guides for Windows and Xbox diehards.&lt;/p&gt;
    &lt;p&gt;SEO and analytics firm FirstPageSage has released its AI market share report for the start of December, and it shows Google Gemini actively poised to supplant Microsoft Copilot. Based on reports that Google Gemini is now actively beating ChatGPT's best models, FirstPageSage has Google Gemini sprinting past Microsoft Copilot quarter over quarter, although ChatGPT itself will remain the front runner.&lt;/p&gt;
    &lt;head rend="h2"&gt;Google's AI advantages are accumulating, as Microsoft's disadvantages snowball&lt;/head&gt;
    &lt;p&gt;Whether it's Google's Tensor server tech or dominating position with Google Play-bound Android, Microsoft's lack of forethought and attention paid to their actual customers is starting to catch up with the firm. Nadella has sought to blame the company's unwieldy size for the lack of innovation, but it reads like an excuse to me. It's all about priorities — and Nadella has chased shareholder sentiment over delivering for its customers or employees, and that short-termism is going to put Microsoft on the backfoot if AI actually does deliver another computing paradigm shift.&lt;/p&gt;
    &lt;p&gt;Microsoft depends almost entirely on pricy NVIDIA technology for its data centers, whereas Google is actively investing to own the entire stack. Microsoft has also worked incredibly hard to cram half-baked AI features into its products, whereas Google has arguably been a lot more thoughtful in its approach. Microsoft sprinted out of the gate like a bull in a China shop, and investors rewarded them for it — but fast forward to 2025, and Google's AI products simply work better, and are more in-tune with how people might actually use them.&lt;/p&gt;
    &lt;p&gt;I am someone who is actively using the AI features across Google Android and Microsoft Windows on a day to day basis, and the delta between the two companies is growing ever wider. Basic stuff like the photo editing features on Google Pixel phones are lightyears beyond the abysmal tools found in the Microsoft Photos app on Windows. Google Gemini in Google Apps is also far smarter and far more intuitive than Copilot on Microsoft 365, as someone actively using both across the two businesses I work in.&lt;/p&gt;
    &lt;p&gt;Dare I say it, Gemini is actually helpful, and can usually execute tasks you might actually need in a day to day job. "Find me a meeting slot on this date to accommodate these timezones" — Gemini will actually do it. Copilot 365 doesn't even have the capability to schedule a calendar event with natural language in the Outlook mobile app, or even provide something as basic as clickable links in some cases. At least Xbox's Gaming Copilot has a beta tag to explain why it fails half of the time. It's truly absurd how half-baked a lot of these features are, and it's odd that Microsoft sought to ship them in this state. And Microsoft wants to make Windows 12 AI first? Please.&lt;/p&gt;
    &lt;p&gt;Microsoft's "ship it now fix it later" attitude risks giving its AI products an Internet Explorer-like reputation for poor quality, sacrificing the future to more patient, thoughtful companies who spend a little more time polishing first. Microsoft's strategy for AI seems to revolve around offering cheaper, lower quality products at lower costs (Microsoft Teams, hi), over more expensive higher-quality options its competitors are offering. Whether or not that strategy will work for artificial intelligence, which is exorbitantly expensive to run, remains to be seen.&lt;/p&gt;
    &lt;p&gt;Microsoft's savvy early investment in OpenAI gave it an incredibly strong position early on, but as we get deeper into the cycle, some cracks are starting to show. Many of Microsoft's AI products to date simply scream of a total lack of direction and utter chaos, but it's not all hopeless. Some of Microsoft's enterprise solutions for AI are seeing strong growth. Github Copilot has been something of a success story for Redmond, and Microsoft is exploring its own Maia and Cobalt chips and even language models, in attempts to decouple itself from NVIDIA and OpenAI respectively. But Satya Nadella's Microsoft has an uncanny knack for failing to deliver on promising initiatives like those.&lt;/p&gt;
    &lt;p&gt;Without a stronger emphasis on quality, Microsoft's future in AI could simply end up revolving around re-selling NVIDIA server tech and jacking up local electricity prices, rather than providing any real home-grown innovation in the space. Shareholders will be more than happy for Microsoft to simply be a server reseller, but it would be a ignoble legacy for what was previously one of tech's most innovative companies.&lt;/p&gt;
    &lt;p&gt;Follow Windows Central on Google News to keep our latest news, insights, and features at the top of your feeds!&lt;/p&gt;
    &lt;p&gt;Jez Corden is the Executive Editor at Windows Central, focusing primarily on all things Xbox and gaming. Jez is known for breaking exclusive news and analysis as relates to the Microsoft ecosystem while being powered by tea. Follow on Twitter (X) and tune in to the XB2 Podcast, all about, you guessed it, Xbox!&lt;/p&gt;
    &lt;p&gt;You must confirm your public display name before commenting&lt;/p&gt;
    &lt;p&gt;Please logout and then login again, you will then be prompted to enter your display name.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46194615</guid><pubDate>Mon, 08 Dec 2025 16:54:31 +0000</pubDate></item><item><title>Legion Health (YC S21) is hiring a founding engineer (SF, in-person)</title><link>https://news.ycombinator.com/item?id=46194720</link><description>&lt;doc fingerprint="3541ec5e48d7b5a8"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Legion Health (YC S21) operates a psychiatric practice and is building the AI-native operations layer for mental health care. We focus on the operational backend: scheduling, intake, documentation, billing, and care coordination. These workflows—not diagnostics—are the main bottlenecks in mental health delivery.&lt;/p&gt;
      &lt;p&gt;We run our own clinic, so the systems you build ship directly into real patient care. Our agent infrastructure currently supports more than 2,000 patients with one human support lead.&lt;/p&gt;
      &lt;p&gt;We’re hiring a Founding Engineer (in-person, San Francisco). You’d work directly with the founders on:&lt;/p&gt;
      &lt;p&gt;event-driven backend systems (Node.js, TypeScript, Postgres/Supabase, AWS)&lt;/p&gt;
      &lt;p&gt;LLM agent tooling (tool use, retries, memory, context management)&lt;/p&gt;
      &lt;p&gt;internal operations tools for both humans and agents&lt;/p&gt;
      &lt;p&gt;state/coordination logic that represents a patient’s journey&lt;/p&gt;
      &lt;p&gt;HIPAA-compliant data and audit pipelines&lt;/p&gt;
      &lt;p&gt;We’re open to backend or full-stack/product engineers who think in systems and have owned real workflows end-to-end. Prior experience with LLMs is optional; interest is required.&lt;/p&gt;
      &lt;p&gt;Details: full-time, in-person SF, salary $130k–$180k, equity 0.1–0.6%.&lt;/p&gt;
      &lt;p&gt;Apply here: https://www.ycombinator.com/companies/legion-health/jobs/oc6...&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46194720</guid><pubDate>Mon, 08 Dec 2025 17:01:11 +0000</pubDate></item><item><title>Launch HN: Nia (YC S25) – Give better context to coding agents</title><link>https://www.trynia.ai/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46194828</guid><pubDate>Mon, 08 Dec 2025 17:10:14 +0000</pubDate></item></channel></rss>