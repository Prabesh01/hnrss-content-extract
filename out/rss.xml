<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sat, 31 Jan 2026 11:39:59 +0000</lastBuildDate><item><title>BoldVoice (YC S21) Is Hiring Fullstack and Machine Learning Engineers</title><link>https://boldvoice.notion.site/careers-page?p=2e871a9bf729806c81f6e47f32e32622&amp;pm=s</link><description>&lt;doc fingerprint="10f452a104a33a8"&gt;
  &lt;main&gt;
    &lt;p&gt;JavaScript must be enabled in order to use Notion. Please enable JavaScript to continue.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46823430</guid><pubDate>Fri, 30 Jan 2026 12:00:12 +0000</pubDate></item><item><title>Pangolin (YC S25) is hiring software engineers (open-source, Go, networking)</title><link>https://docs.pangolin.net/careers/join-us</link><description>&lt;doc fingerprint="578d4934f0eed0f"&gt;
  &lt;main&gt;
    &lt;div&gt;Skip to main content&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;We are looking for talented engineers to join our team and help build secure remote access. If you‚Äôre passionate about open-source software, networking, and security, we‚Äôd love to hear from you.&lt;head rend="h2"&gt;About Pangolin&lt;/head&gt; Pangolin delivers identity-aware remote access to internal apps and services. Our platform replaces legacy VPNs and simplifies secure access to infrastructure, applications, and developer environments. We build in the open and are self‚Äëhosted by default so teams retain control over data and infrastructure. The system is policy‚Äëdriven, integrates with standard IdPs, exposes clear observability and health, and provides an API for automation. If you‚Äôre interested in open-source auth and networking infrastructure, we‚Äôd love to chat. &lt;head rend="h2"&gt;Open Roles&lt;/head&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46823544</guid><pubDate>Fri, 30 Jan 2026 12:11:49 +0000</pubDate></item><item><title>HTTP Cats</title><link>https://http.cat/</link><description>&lt;doc fingerprint="edf6e74fda190558"&gt;
  &lt;main&gt;
    &lt;p&gt;https://http.cat/[status_code]&lt;/p&gt;
    &lt;p&gt;Note: If you need an extension at the end of the URL just add .jpg.&lt;/p&gt;
    &lt;p&gt;.jpg&lt;/p&gt;
    &lt;p&gt;Continue&lt;/p&gt;
    &lt;p&gt;Switching Protocols&lt;/p&gt;
    &lt;p&gt;Processing&lt;/p&gt;
    &lt;p&gt;Early Hints&lt;/p&gt;
    &lt;p&gt;OK&lt;/p&gt;
    &lt;p&gt;Created&lt;/p&gt;
    &lt;p&gt;Accepted&lt;/p&gt;
    &lt;p&gt;Non-Authoritative Information&lt;/p&gt;
    &lt;p&gt;No Content&lt;/p&gt;
    &lt;p&gt;Reset Content&lt;/p&gt;
    &lt;p&gt;Partial Content&lt;/p&gt;
    &lt;p&gt;Multi-Status&lt;/p&gt;
    &lt;p&gt;Already Reported&lt;/p&gt;
    &lt;p&gt;Transformation Applied&lt;/p&gt;
    &lt;p&gt;IM Used&lt;/p&gt;
    &lt;p&gt;Multiple Choices&lt;/p&gt;
    &lt;p&gt;Moved Permanently&lt;/p&gt;
    &lt;p&gt;Found&lt;/p&gt;
    &lt;p&gt;See Other&lt;/p&gt;
    &lt;p&gt;Not Modified&lt;/p&gt;
    &lt;p&gt;Use Proxy&lt;/p&gt;
    &lt;p&gt;Temporary Redirect&lt;/p&gt;
    &lt;p&gt;Permanent Redirect&lt;/p&gt;
    &lt;p&gt;Bad Request&lt;/p&gt;
    &lt;p&gt;Unauthorized&lt;/p&gt;
    &lt;p&gt;Payment Required&lt;/p&gt;
    &lt;p&gt;Forbidden&lt;/p&gt;
    &lt;p&gt;Not Found&lt;/p&gt;
    &lt;p&gt;Method Not Allowed&lt;/p&gt;
    &lt;p&gt;Not Acceptable&lt;/p&gt;
    &lt;p&gt;Proxy Authentication Required&lt;/p&gt;
    &lt;p&gt;Request Timeout&lt;/p&gt;
    &lt;p&gt;Conflict&lt;/p&gt;
    &lt;p&gt;Gone&lt;/p&gt;
    &lt;p&gt;Length Required&lt;/p&gt;
    &lt;p&gt;Precondition Failed&lt;/p&gt;
    &lt;p&gt;Payload Too Large&lt;/p&gt;
    &lt;p&gt;Request-URI Too Long&lt;/p&gt;
    &lt;p&gt;Unsupported Media Type&lt;/p&gt;
    &lt;p&gt;Request Range Not Satisfiable&lt;/p&gt;
    &lt;p&gt;Expectation Failed&lt;/p&gt;
    &lt;p&gt;I√¢m a teapot&lt;/p&gt;
    &lt;p&gt;Page Expired&lt;/p&gt;
    &lt;p&gt;Enhance Your Calm&lt;/p&gt;
    &lt;p&gt;Misdirected Request&lt;/p&gt;
    &lt;p&gt;Unprocessable Entity&lt;/p&gt;
    &lt;p&gt;Locked&lt;/p&gt;
    &lt;p&gt;Failed Dependency&lt;/p&gt;
    &lt;p&gt;Too Early&lt;/p&gt;
    &lt;p&gt;Upgrade Required&lt;/p&gt;
    &lt;p&gt;Precondition Required&lt;/p&gt;
    &lt;p&gt;Too Many Requests&lt;/p&gt;
    &lt;p&gt;Request Header Fields Too Large&lt;/p&gt;
    &lt;p&gt;No Response&lt;/p&gt;
    &lt;p&gt;Blocked by Windows Parental Controls&lt;/p&gt;
    &lt;p&gt;Unavailable For Legal Reasons&lt;/p&gt;
    &lt;p&gt;SSL Certificate Error&lt;/p&gt;
    &lt;p&gt;SSL Certificate Required&lt;/p&gt;
    &lt;p&gt;HTTP Request Sent to HTTPS Port&lt;/p&gt;
    &lt;p&gt;Token expired/invalid&lt;/p&gt;
    &lt;p&gt;Client Closed Request&lt;/p&gt;
    &lt;p&gt;Internal Server Error&lt;/p&gt;
    &lt;p&gt;Not Implemented&lt;/p&gt;
    &lt;p&gt;Bad Gateway&lt;/p&gt;
    &lt;p&gt;Service Unavailable&lt;/p&gt;
    &lt;p&gt;Gateway Timeout&lt;/p&gt;
    &lt;p&gt;Variant Also Negotiates&lt;/p&gt;
    &lt;p&gt;Insufficient Storage&lt;/p&gt;
    &lt;p&gt;Loop Detected&lt;/p&gt;
    &lt;p&gt;Bandwidth Limit Exceeded&lt;/p&gt;
    &lt;p&gt;Not Extended&lt;/p&gt;
    &lt;p&gt;Network Authentication Required&lt;/p&gt;
    &lt;p&gt;Web Server Is Down&lt;/p&gt;
    &lt;p&gt;Connection Timed Out&lt;/p&gt;
    &lt;p&gt;Origin Is Unreachable&lt;/p&gt;
    &lt;p&gt;SSL Handshake Failed&lt;/p&gt;
    &lt;p&gt;Site Frozen&lt;/p&gt;
    &lt;p&gt;Network Connect Timeout Error&lt;/p&gt;
    &lt;p&gt;Developed by @rogeriopvl&lt;/p&gt;
    &lt;p&gt;Original Images by Tomomi Imura (@girlie_mac)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46824422</guid><pubDate>Fri, 30 Jan 2026 13:56:51 +0000</pubDate></item><item><title>Kimi K2.5 Technical Report [pdf]</title><link>https://github.com/MoonshotAI/Kimi-K2.5/blob/master/tech_report.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46826597</guid><pubDate>Fri, 30 Jan 2026 16:43:50 +0000</pubDate></item><item><title>Antirender: remove the glossy shine on architectural renderings</title><link>https://antirender.com/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46829147</guid><pubDate>Fri, 30 Jan 2026 20:05:24 +0000</pubDate></item><item><title>Peerweb: Decentralized website hosting via WebTorrent</title><link>https://peerweb.lol/</link><description>&lt;doc fingerprint="868e3ff18d2cd634"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;ü™ê PeerWeb&lt;/head&gt;
    &lt;head rend="h2"&gt;Decentralized Website Hosting via WebTorrent&lt;/head&gt;
    &lt;head rend="h3"&gt;ü§î What is PeerWeb?&lt;/head&gt;
    &lt;p&gt;PeerWeb is a revolutionary way to host and share websites using WebTorrent technology. Instead of relying on centralized servers, websites are distributed across a peer-to-peer network, making them censorship-resistant and always available. üåç‚ú®&lt;/p&gt;
    &lt;head rend="h3"&gt;üì§ Quick Upload&lt;/head&gt;
    &lt;head rend="h4"&gt;Drag &amp;amp; Drop Your Website&lt;/head&gt;
    &lt;p&gt;Drop a folder with your website files&lt;/p&gt;
    &lt;head rend="h3"&gt;üìö How to Use PeerWeb&lt;/head&gt;
    &lt;head rend="h3"&gt;üí° Load Existing Site&lt;/head&gt;
    &lt;p&gt;To load a website from a torrent hash, enter it below:&lt;/p&gt;
    &lt;p&gt;üéØ Just the hash! PeerWeb automatically adds the magnet link prefix and trackers.&lt;/p&gt;
    &lt;head rend="h3"&gt;üß™ Demos&lt;/head&gt;
    &lt;p&gt; Functionality test page: &lt;lb/&gt;https://peerweb.lol/?orc=90c020bd252639622a14895a0fad713b91e0130c &lt;/p&gt;
    &lt;p&gt; SomaFM on PeerWeb:&lt;lb/&gt;https://peerweb.lol/?orc=908d19242ae1461f333a516d1f8b89c13ef2d259 &lt;/p&gt;
    &lt;p&gt; Chess on PeerWeb:&lt;lb/&gt;https://peerweb.lol/?orc=1e14b1ba7fcd03e5f165d53ed8223a333349db04 &lt;/p&gt;
    &lt;p&gt; Text Editor app on PeerWeb:&lt;lb/&gt;https://peerweb.lol/?orc=4e5f1204dcec68195bfcc89f9410a0b70a0ddfac &lt;/p&gt;
    &lt;head rend="h3"&gt;üêõ Debug Mode&lt;/head&gt;
    &lt;p&gt;For developers and troubleshooting, add &amp;amp;debug=true to see detailed progress:&lt;/p&gt;
    &lt;code&gt;https://peerweb.lol?orc=ABC123DEF456...&amp;amp;debug=true&lt;/code&gt;
    &lt;head rend="h3"&gt;üöÄ Advanced Options&lt;/head&gt;
    &lt;head rend="h3"&gt;üíæ Smart Caching&lt;/head&gt;
    &lt;p&gt;PeerWeb caches visited sites for lightning-fast loading! üöÄ&lt;/p&gt;
    &lt;head rend="h3"&gt;üõ°Ô∏è Security Features&lt;/head&gt;
    &lt;p&gt;Enhanced security with DOMPurify integration! üîí&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46829582</guid><pubDate>Fri, 30 Jan 2026 20:40:00 +0000</pubDate></item><item><title>P vs. NP and the Difficulty of Computation: A ruliological approach</title><link>https://writings.stephenwolfram.com/2026/01/p-vs-np-and-the-difficulty-of-computation-a-ruliological-approach/</link><description>&lt;doc fingerprint="fe12981443a180ed"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Empirical Theoretical Computer Science&lt;/head&gt;
    &lt;p&gt;‚ÄúCould there be a faster program for that?‚Äù It‚Äôs a fundamental type of question in theoretical computer science. But except in special cases, such a question has proved fiendishly difficult to answer. And, for example, in half a century, almost no progress has been made even on the rather coarse (though very famous) P vs. NP question‚Äîessentially of whether for any nondeterministic program there will always be a deterministic one that is as fast. From a purely theoretical point of view, it‚Äôs never been very clear how to even start addressing such a question. But what if one were to look at the question empirically, say in effect just by enumerating possible programs and explicitly seeing how fast they are, etc.?&lt;/p&gt;
    &lt;p&gt;One might imagine that any programs one could realistically enumerate would be too small to be interesting. But what I discovered in the early 1980s is that this is absolutely not the case‚Äîand that in fact it‚Äôs very common for programs even small enough to be easily enumerated to show extremely rich and complex behavior. With this intuition I already in the 1990s began some empirical exploration of things like the fastest ways to compute functions with Turing machines. But now‚Äîparticularly with the concept of the ruliad‚Äîwe have a framework for thinking more systematically about the space of possible programs, and so I‚Äôve decided to look again at what can be discovered by ruliological investigations of the computational universe about questions of computational complexity theory that have arisen in theoretical computer science‚Äîincluding the P vs. NP question.&lt;/p&gt;
    &lt;p&gt;We won‚Äôt resolve the P vs. NP question. But we will get a host of definite, more restricted results. And by looking ‚Äúunderneath the general theory‚Äù at explicit, concrete cases we‚Äôll get a sense of some of the fundamental issues and subtleties of the P vs. NP question, and why, for example, proofs about it are likely to be so difficult.&lt;/p&gt;
    &lt;p&gt;Along the way, we‚Äôll also see lots of evidence of the phenomenon of computational irreducibility‚Äîand the general pattern of the difficulty of computation. We‚Äôll see that there are computations that can be ‚Äúreduced‚Äù, and done more quickly. But there are also others where we‚Äôll be able to see with absolute explicitness that‚Äîat least within the class of programs we‚Äôre studying‚Äîthere‚Äôs simply no faster way to get the computations done. In effect this is going to give us lots of proofs of restricted forms of computational irreducibility. And seeing these will give us ways to further build our intuition about the ever-more-central phenomenon of computational irreducibility‚Äîas well as to see how in general we can use the methodology of ruliology to explore questions of theoretical computer science.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Basic Setup&lt;/head&gt;
    &lt;p&gt;Click any diagram to get Wolfram Language code to reproduce it.&lt;/p&gt;
    &lt;p&gt;How can we enumerate possible programs? We could pick any model of computation. But to help connect with traditional theoretical computer science, I‚Äôll use a classic one: Turing machines.&lt;/p&gt;
    &lt;p&gt;Often in theoretical computer science one concentrates on yes/no decision problems. But here it‚Äôll typically be convenient instead to think (more ‚Äúmathematically‚Äù) about Turing machines that compute integer functions. The setup we‚Äôll use is as follows. Start the Turing machine with the digits of some integer n on its tape. Then run the Turing machine, stopping if the Turing machine head goes further to the right than where it started. The value of the function with input n is then read off from the binary digits that remain on its tape when the Turing machine stops. (There are many other ‚Äúhalting‚Äù criteria we could use, but this is a particularly robust and convenient one.)&lt;/p&gt;
    &lt;p&gt;So for example, given a Turing machine with rule&lt;/p&gt;
    &lt;p&gt;we can feed it successive integers as input, then run the machine to find the successive values it computes:&lt;/p&gt;
    &lt;p&gt;In this case, the function that the Turing machine computes is&lt;/p&gt;
    &lt;p&gt;or in graphical form:&lt;/p&gt;
    &lt;p&gt;For each input, the Turing machine takes a certain number of steps to stop and give its output (i.e. the value of the function):&lt;/p&gt;
    &lt;p&gt;But this particular Turing machine isn‚Äôt the only one that can compute this function. Here are two more:&lt;/p&gt;
    &lt;p&gt;The outputs are the same as before, but the runtimes are different:&lt;/p&gt;
    &lt;p&gt;Indicating these respectively by and plotting them together, we see that there are definite trends‚Äîbut no clear winner for ‚Äúfastest program‚Äù:&lt;/p&gt;
    &lt;p&gt;In computational complexity theory, it‚Äôs common to discuss how runtime varies with input size‚Äîwhich here means taking each block of inputs with a given number of digits, and just finding its maximum:&lt;/p&gt;
    &lt;p&gt;And what we see is that in this case the first Turing machine shown is ‚Äúsystematically faster‚Äù than the other two‚Äîand in fact provides the fastest way to compute this particular function among Turing machines of the size we‚Äôre using.&lt;/p&gt;
    &lt;p&gt;Since we‚Äôll be dealing with lots of Turing machines here, it‚Äôs convenient to be able to specify them just with numbers‚Äîand we‚Äôll do it the way TuringMachine in the Wolfram Language does. And with this setup, the machines we‚Äôve just considered have numbers 261, 3333 and 1285.&lt;/p&gt;
    &lt;p&gt;In thinking about functions computed by Turing machines, there is one immediate subtlety to consider. We‚Äôve said that we find the output by reading off what‚Äôs on the Turing machine tape when the Turing machine stops. But what if the machine never stops? (Or in our case, what if the head of the Turing machine never reaches the right-hand end?) Well, then there‚Äôs no output value defined. And in general, the functions our Turing machines compute will only be partial functions‚Äîin the sense that for some of their inputs, there may be no output value defined (as here for machine 2189):&lt;/p&gt;
    &lt;p&gt;When we plot such partial functions, we‚Äôll just have a gap where there are undefined values:&lt;/p&gt;
    &lt;p&gt;In what follows, we‚Äôll be exploring Turing machines of different ‚Äúsizes‚Äù. We‚Äôll assume that there are two possible colors for each position on the tape‚Äîand that there are s possible states for the head. The total number of possible Turing machines with k = 2 colors and s states is (2ks)ks‚Äîwhich grows rapidly with s:&lt;/p&gt;
    &lt;p&gt;For any given function we‚Äôll then be able to ask what machine (or machines) up to a given size compute it the fastest. In other words, by explicitly studying possible Turing machines, we‚Äôll be able to establish an absolute lower bound on the computational difficulty of computing a function, at least when that computation is done by a Turing machine of at most a given size. (And, yes, the size of the Turing machine can be thought of as characterizing its ‚Äúalgorithmic information content‚Äù.)&lt;/p&gt;
    &lt;p&gt;In traditional computational complexity theory, it‚Äôs usually been very difficult to establish lower bounds. But our ruliological approach here will allow us to systematically do it (at least relative to machines of a given size, i.e. with given algorithmic information content). (It‚Äôs worth pointing out that if a machine is big enough, it can include a lookup table for any number of cases of any given function‚Äîmaking questions about the difficulty of computing at least those cases rather moot.)&lt;/p&gt;
    &lt;head rend="h2"&gt;The s = 1, k = 2 Turing Machines&lt;/head&gt;
    &lt;p&gt;To begin our systematic investigation of possible programs, let‚Äôs consider what is essentially the simplest possible case: Turing machines with one state and two possible colors of cells on their tape &lt;/p&gt;
    &lt;p&gt;Here‚Äôs what each of these machines does for successive integer inputs:&lt;/p&gt;
    &lt;p&gt;Looking at the outputs in each case, we can plot the functions these compute:&lt;/p&gt;
    &lt;p&gt;And here are the corresponding runtimes:&lt;/p&gt;
    &lt;p&gt;Out of all 16 machines, 8 compute total functions (i.e. the machines always terminate, so the values of the functions are defined for every input), and 8 don‚Äôt. Four machines produce ‚Äúcomplicated-looking‚Äù functions; an example is machine 14, which computes the function:&lt;/p&gt;
    &lt;p&gt;There are a variety of representations for this function, including&lt;/p&gt;
    &lt;p&gt;and:&lt;/p&gt;
    &lt;p&gt;The way the function is computed by the Turing machine is&lt;/p&gt;
    &lt;p&gt;and the runtime is given by&lt;/p&gt;
    &lt;p&gt;which is simply:&lt;/p&gt;
    &lt;p&gt;For input of size n, this implies the worst-case time complexity for computing this function is &lt;/p&gt;
    &lt;p&gt;Each one of the 1-state machines works at least slightly differently. But in the end, all of them are simple enough in their behavior that one can readily give a ‚Äúclosed-form formula‚Äù for the value of f[i] for any given i:&lt;/p&gt;
    &lt;p&gt;One thing that‚Äôs notable is that‚Äîexcept in the trivial case where all values are undefined‚Äîthere are no examples among &lt;/p&gt;
    &lt;head rend="h2"&gt;s = 2, k = 2 Turing Machines&lt;/head&gt;
    &lt;p&gt;There are a total of 4096 possible 2-state, 2-color Turing machines. Running all these machines, we find that they compute a total of 350 distinct functions‚Äîof which 189 are total. Here are plots of these distinct total functions‚Äîtogether with a count of how many machines generate them (altogether 2017 of the 4096 machines always terminate, and therefore compute total functions):&lt;/p&gt;
    &lt;p&gt;Plotting the values of all these functions in 3D, we see that the vast majority have values f[i] that are close to their inputs i‚Äîindicating that in a sense the Turing machines usually ‚Äúdon‚Äôt do much‚Äù to their input:&lt;/p&gt;
    &lt;p&gt;To see more clearly what the machines ‚Äúactually do‚Äù, we can look at the quantity &lt;/p&gt;
    &lt;p&gt;though in 6 cases it is 2, and in 3 cases (which include the ‚Äúmost popular‚Äù case &lt;/p&gt;
    &lt;p&gt;Dropping periodic cases, the remaining distinct &lt;/p&gt;
    &lt;p&gt;Some of what we see here is similar to the 1-state case. An example of different behavior occurs for machine 2223&lt;/p&gt;
    &lt;p&gt;which gives for &lt;/p&gt;
    &lt;p&gt;In this case f[i] turns out to be expressible simply as&lt;/p&gt;
    &lt;p&gt;or:&lt;/p&gt;
    &lt;p&gt;Another example is machine 2079&lt;/p&gt;
    &lt;p&gt;which gives for &lt;/p&gt;
    &lt;p&gt;This function once again turns out to be expressible in ‚Äúclosed form‚Äù:&lt;/p&gt;
    &lt;p&gt;Some functions grow rapidly. For example, machine 3239&lt;/p&gt;
    &lt;p&gt;has values:&lt;/p&gt;
    &lt;p&gt;These have the property that &lt;/p&gt;
    &lt;p&gt;There are many subtleties even in dealing with 2-state Turing machines. For example, different machines may ‚Äúlook like‚Äù they‚Äôre generating the same function f[i] up to a certain value of i, and only then deviate. The most extreme example of such a ‚Äúsurprise‚Äù among machines generating total functions occurs among:&lt;/p&gt;
    &lt;p&gt;Up to &lt;/p&gt;
    &lt;p&gt;What about partial functions? At least for 2-state machines, if undefined values in f[i] are ever going to occur, they always already occur for small i. The ‚Äúlongest holdouts‚Äù are machines 1960 and 2972, which are both first undefined for input 8&lt;/p&gt;
    &lt;p&gt;but which ‚Äúbecome undefined‚Äù in different ways: in machine 1960, the head systematically moves to the left, while in machine 2972, it moves periodically back and forth forever, without ever reaching the right-hand end. (Despite their different mechanisms, both rules share the feature of being undefined for all inputs that are multiples of 8.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Runtimes in s = 2, k = 2 Machines&lt;/head&gt;
    &lt;p&gt;What about runtimes? If a function f[i] is computed by several different Turing machines, the details of how it‚Äôs computed by each machine will normally be at least slightly different. Still, in many cases the mechanisms are similar enough that their runtimes are the same. And in the end, among all the 2017 machines that compute our 189 distinct total functions, there are only 103 distinct ‚Äúprofiles‚Äù of runtime vs. input (and indeed many of these are very similar):&lt;/p&gt;
    &lt;p&gt;The picture gets simpler if, rather than plotting runtimes for each specific input value, we instead plot the worst-case runtime for all inputs of a given size. (In effect we‚Äôre plotting against IntegerLength[i, 2] or Ceiling[Log2[i + 1]].) There turn out to be just 71 distinct profiles for such worst-case time complexity&lt;/p&gt;
    &lt;p&gt;and indeed all of these have fairly simple closed forms‚Äîwhich for even n are (with directly analogous forms for odd n):&lt;/p&gt;
    &lt;p&gt;If we consider the behavior of these worst-case runtimes for large input lengths n, we find that fairly few distinct growth rates occur‚Äînotably with linear, quadratic and exponential cases, but nothing in between:&lt;/p&gt;
    &lt;p&gt;The machines with the fastest growth &lt;/p&gt;
    &lt;p&gt;For a size-n input, the maximum value of the function is just the maximum integer with n digits, or &lt;/p&gt;
    &lt;p&gt;And at these maxima, the machine is effectively operating like a binary counter, generating all the states it can, with the head moving in a very regular nested pattern:&lt;/p&gt;
    &lt;p&gt;It turns out that for &lt;/p&gt;
    &lt;p&gt;Of the 8 machines with runtimes growing like &lt;/p&gt;
    &lt;p&gt;The two machines with asymptotic runtime growth &lt;/p&gt;
    &lt;p&gt;Here‚Äôs the actual behavior of these machines when given inputs 1 through 10:&lt;/p&gt;
    &lt;p&gt;(The lack of runtimes intermediate between quadratic and exponential is notable‚Äîand perhaps reminiscent of the rarity of ‚Äúintermediate growth‚Äù seen for example in the cases of finitely generated groups and multiway systems.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Runtime Distributions&lt;/head&gt;
    &lt;p&gt;Our emphasis so far has been on worst-case runtimes: the largest runtimes required for inputs of any given size. But we can also ask about the distribution of runtimes within inputs of a given size.&lt;/p&gt;
    &lt;p&gt;So, for example, here are the runtimes for all size-11 inputs for a particular, fairly typical Turing machine (&lt;/p&gt;
    &lt;p&gt;The maximum (‚Äúworst-case‚Äù) value here is 43‚Äîbut the median is only 13. In other words, while some computations take a while, most run much faster‚Äîso that the runtime distribution is peaked at small values:&lt;/p&gt;
    &lt;p&gt;(The way our Turing machines are set up, they always run for an even number of steps before terminating‚Äîsince to terminate, the head must move one position to the right for every position it moved to the left.)&lt;/p&gt;
    &lt;p&gt;If we increase the size of the inputs, we see that the distribution, at least in this case, is close to exponential:&lt;/p&gt;
    &lt;p&gt;It turns out that this kind of exponential distribution is typical of what we see in almost all Turing machines. (It‚Äôs notable that this is rather different from the t ‚Äì1/2 ‚Äústopping time‚Äù distribution we‚Äôd expect if the Turing machine head was ‚Äúon average‚Äù executing a random walk with an absorbing boundary.) There are nevertheless machines whose distributions deviate significantly from exponential, examples being:&lt;/p&gt;
    &lt;p&gt;Some simply have long tails to their exponentials. Others, however, have an overall non-exponential form.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Fast Can Functions Be Computed?&lt;/head&gt;
    &lt;p&gt;We‚Äôve now seen lots of functions‚Äîand runtime profiles‚Äîthat &lt;/p&gt;
    &lt;p&gt;We‚Äôve seen that there are machines that compute functions quite slowly‚Äîlike in exponential time. But are these machines the fastest that compute those particular functions? It turns out the answer is no.&lt;/p&gt;
    &lt;p&gt;And if we look across all 189 total functions computed by &lt;/p&gt;
    &lt;p&gt;In other words, there are 8 functions that are the ‚Äúmost difficult to compute‚Äù for &lt;/p&gt;
    &lt;p&gt;What are these functions? Here‚Äôs one of them (computed by machine 1511):&lt;/p&gt;
    &lt;p&gt;If we plot this function, it seems to have a nested form&lt;/p&gt;
    &lt;p&gt;which becomes somewhat more obvious on a log-log plot:&lt;/p&gt;
    &lt;p&gt;As it turns out, there‚Äôs what amounts to a ‚Äúclosed form‚Äù for this function&lt;/p&gt;
    &lt;p&gt;though unlike the closed forms we saw above, this one involves Nest, and effectively computes its results recursively:&lt;/p&gt;
    &lt;p&gt;How about machine 1511? Well, here‚Äôs how it computes this function‚Äîin effect visibly using recursion:&lt;/p&gt;
    &lt;p&gt;The runtimes are&lt;/p&gt;
    &lt;p&gt;giving worst-case runtimes for inputs of size n of the form:&lt;/p&gt;
    &lt;p&gt;It turns out all 8 functions with minimum runtimes growing like &lt;/p&gt;
    &lt;p&gt;For the functions with fastest computation times &lt;/p&gt;
    &lt;p&gt;So what can we conclude? Well, we now know some functions that cannot be computed by &lt;/p&gt;
    &lt;head rend="h2"&gt;Computing the Same Functions at Different Speeds&lt;/head&gt;
    &lt;p&gt;We now know the fastest that certain functions can be computed by &lt;/p&gt;
    &lt;p&gt;And in fact it‚Äôs common for there to be only one machine that computes a given function. Out of the 189 total functions that can be computed by &lt;/p&gt;
    &lt;p&gt;OK, but if multiple machines compute the same function, we can then ask how their speeds compare. Well, it turns out that for 145 of our 189 total functions all the different machines that compute the same function do so with the same ‚Äúruntime profile‚Äù (i.e. with the same runtime for each input i). But that leaves 44 functions for which there are multiple runtime profiles:&lt;/p&gt;
    &lt;p&gt;Here are all these 44 functions, together with the distinct runtime profiles for machines that compute them:&lt;/p&gt;
    &lt;p&gt;Much of the time we see that the possible runtime profiles for computing a given function differ only very little. But sometimes the difference is more significant. For example, for the identity function &lt;/p&gt;
    &lt;p&gt;Within these 10 profiles, there are 3 distinct rates of growth for the worst-case runtime by input size: constant, linear, and exponential&lt;/p&gt;
    &lt;p&gt;exemplified by machines 3197, 3589 and 3626 respectively:&lt;/p&gt;
    &lt;p&gt;Of course, there‚Äôs a trivial way to compute this particular function‚Äîjust by having a Turing machine that doesn‚Äôt change its input. And, needless to say, such a machine has runtime 1 for all inputs:&lt;/p&gt;
    &lt;p&gt;It turns out that for &lt;/p&gt;
    &lt;p&gt;But although there are not different ‚Äúorders of growth‚Äù for worst-case runtimes among any other (total) functions computed by &lt;/p&gt;
    &lt;p&gt;by slightly different methods&lt;/p&gt;
    &lt;p&gt;with different worst-case runtime profiles&lt;/p&gt;
    &lt;p&gt;or:&lt;/p&gt;
    &lt;p&gt;By the way, if we consider partial instead of total functions, nothing particularly different happens, at least with &lt;/p&gt;
    &lt;p&gt;that are again essentially computing the identity function.&lt;/p&gt;
    &lt;p&gt;Another question is how &lt;/p&gt;
    &lt;p&gt;But how fast are the computations? This compares the possible worst-case runtimes for &lt;/p&gt;
    &lt;p&gt;There must always be &lt;/p&gt;
    &lt;p&gt;But can &lt;/p&gt;
    &lt;head rend="h2"&gt;Absolute Lower Bounds and the Efficiency of Machines&lt;/head&gt;
    &lt;p&gt;We‚Äôve seen that different Turing machines can take different times to compute particular functions. But how fast can any conceivable Turing machine‚Äîeven in principle‚Äîcompute a given function?&lt;/p&gt;
    &lt;p&gt;There‚Äôs an obvious absolute lower bound to the runtime: with the way we‚Äôve set things up, if a Turing machine is going to take input i and generate output j, its head has to at least be able to go far enough to the left to reach all the bits that need to change in going from i to j‚Äîas well as making it back to the right-hand end so that the machine halts. The number of steps required for this is&lt;/p&gt;
    &lt;p&gt;which for values of i and j up to 8 bits is:&lt;/p&gt;
    &lt;p&gt;So how do the runtimes of actual Turing machine computations compare with these absolute lower bounds?&lt;/p&gt;
    &lt;p&gt;Here‚Äôs the behavior of s = 1, k = 2 machines 1 and 3, where for each input we‚Äôre giving the actual runtime along with the absolute lower bound:&lt;/p&gt;
    &lt;p&gt;In the second case, the machine is always as efficient as it absolutely can be; in the first case, it only sometimes is‚Äîthough the maximum slowdown is only 2 steps.&lt;/p&gt;
    &lt;p&gt;For s = 2, k = 2 machines, the differences can be much larger. For example, machine 378 can take exponential time‚Äîeven though the absolute lower bound in this case is just 1 step, since this machine computes the identity function:&lt;/p&gt;
    &lt;p&gt;Here‚Äôs another example (machine 1447) in which the actual runtime is always roughly twice the absolute lower bound:&lt;/p&gt;
    &lt;p&gt;But how does the smallest (worst-case) runtime for any s = 2 Turing machine to compute a given function compare to the absolute lower bound? Well, in a result that presages what we‚Äôll see later in discussing the P vs. NP question, the difference can be increasingly large:&lt;/p&gt;
    &lt;p&gt;The functions being computed here are&lt;/p&gt;
    &lt;p&gt;and the fastest s = 2 Turing machines that do this are (machines 2205, 3555 and 2977):&lt;/p&gt;
    &lt;p&gt;Our absolute lower bound determines how fast a Turing machine can possibly generate a given output. But one can also think of it as something that measures how much a Turing machine has ‚Äúachieved‚Äù when it generates a given output. If the output is exactly the same as the input, the Turing machine has effectively ‚Äúachieved nothing‚Äù. The more they differ, the more one can think of the machine having ‚Äúachieved‚Äù.&lt;/p&gt;
    &lt;p&gt;So now a question one can ask is: are there functions where little is achieved in the transformation from input to output, but where the minimum runtime to perform this transformation is still long? One might wonder about the identity function‚Äîwhere in effect ‚Äúnothing is achieved‚Äù. And indeed we‚Äôve seen that there are Turing machines that compute this function, but only slowly. However, there are also machines that compute it quickly‚Äîso in a sense its computation doesn‚Äôt need to be slow.&lt;/p&gt;
    &lt;p&gt;The function above computed by machine 2205 is a somewhat better example. The (worst-case) ‚Äúdistance‚Äù between input and output grows like 2n with the input size n, but the fastest the function can be computed is what machine 2205 does, with a runtime that grows like 10n. Yes, these are still both linear in n. But at least to some extent this is an example of a function that ‚Äúdoesn‚Äôt need to be slow to compute‚Äù, but is at least somewhat slow to compute‚Äîat least for any &lt;/p&gt;
    &lt;head rend="h2"&gt;Space Complexity&lt;/head&gt;
    &lt;p&gt;How difficult is it to compute the value of a function, say with a Turing machine? One measure of that is the time it takes, or, more specifically, how many Turing machine steps it takes. But another measure is how much ‚Äúspace‚Äù it takes, or, more specifically, with our setup, how far to the left the Turing machine head goes‚Äîwhich determines how much ‚ÄúTuring machine memory‚Äù or ‚Äútape‚Äù has to be present.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs a typical example of the comparison between ‚Äúspace‚Äù and ‚Äútime‚Äù used in a particular Turing machine:&lt;/p&gt;
    &lt;p&gt;If we look at all possible space usage profiles as a function of input size we see that‚Äîat least for &lt;/p&gt;
    &lt;p&gt;(One could also consider different measures of ‚Äúcomplexity‚Äù‚Äîperhaps appropriate for different kinds of idealized hardware. Examples include seeing the total length of path traversed by the head, the total area of the region delimited by the head, the number of times 1 is written to the tape during the computation, etc.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Runtime Distributions for Particular Inputs across Machines&lt;/head&gt;
    &lt;p&gt;We‚Äôve talked quite a lot about how runtime varies with input (or input size) for a particular machine. But what about the complementary question: given a particular input, how does runtime vary across different machines? Consider, for example, the &lt;/p&gt;
    &lt;p&gt;The runtimes for these machines are:&lt;/p&gt;
    &lt;p&gt;Here‚Äôs what we see if we continue to larger inputs:&lt;/p&gt;
    &lt;p&gt;The maximum (finite) runtime across all &lt;/p&gt;
    &lt;p&gt;or in closed form:&lt;/p&gt;
    &lt;p&gt;For s = 2, k = 2 machines, the distribution of runtimes with input 1 is&lt;/p&gt;
    &lt;p&gt;where the maximum value of 17 is achieved for machine 1447. For larger inputs the maximum runtimes are:&lt;/p&gt;
    &lt;p&gt;Plotting these maximum runtimes&lt;/p&gt;
    &lt;p&gt;we see a big peak at input 127, corresponding to runtime 509 (achieved by machines 378 and 1351). And, yes, plotting the distribution for input 127 of runtimes for all machines, we see that this is a significant outlier:&lt;/p&gt;
    &lt;p&gt;If one computes runtimes maximized over all machines and all inputs for successively larger sizes of inputs, one gets (once again dominated by machines 378 and 1351):&lt;/p&gt;
    &lt;p&gt;By the way, one can compute not only runtimes but also values and widths maximized across machines:&lt;/p&gt;
    &lt;p&gt;And, no, the maximum value isn‚Äôt always of the form &lt;/p&gt;
    &lt;head rend="h2"&gt;s = 3, k = 2 Turing Machines and the Problem of Undecidability&lt;/head&gt;
    &lt;p&gt;We‚Äôve so far looked at &lt;/p&gt;
    &lt;p&gt;The issue‚Äîas so often‚Äîis of computational irreducibility. Let‚Äôs say you have a machine and you‚Äôre trying to figure out if it computes a particular function. Or you‚Äôre even just trying to figure out if for input i it gives output j. Well, you might say, why not just run the machine? And of course you can do that. But the problem is: how long should you run it for? Let‚Äôs say the machine has been running for a million steps, and still hasn‚Äôt generated any output. Will the machine eventually stop, producing either output j or some other output? Or will the machine just keep running forever, and never generate any output at all?&lt;/p&gt;
    &lt;p&gt;If the behavior of the machine was computationally reducible, then you could expect to be able to ‚Äújump ahead‚Äù and figure out what it would do, without following all the steps. But if it‚Äôs computationally irreducible, then you can‚Äôt expect to do that. It‚Äôs a classic halting problem situation. And you have to conclude that the general problem of determining whether the machine will generate, say, output j is undecidable.&lt;/p&gt;
    &lt;p&gt;Of course, in lots of particular cases (say, for lots of particular inputs) it may be easy enough to tell what‚Äôs going to happen, either just by running for some number of steps, or by using some kind of proof or other abstract derivation. But the point is that‚Äîbecause of computational irreducibility‚Äîthere‚Äôs no upper bound on the amount of computational effort that could be needed. And so the problem of ‚Äúalways getting an answer‚Äù has to be considered formally undecidable.&lt;/p&gt;
    &lt;p&gt;But what happens in practice? Let‚Äôs say we look at the behavior of all &lt;/p&gt;
    &lt;p&gt;And we then conclude that a bit more than half the machines halt‚Äîwith the largest finite runtime being the fairly modest 53, achieved by machine 630283 (essentially equivalent to 718804):&lt;/p&gt;
    &lt;p&gt;But is this actually correct? Or do some of the machines we think don‚Äôt halt based on running for a million steps actually eventually halt‚Äîbut only after more steps?&lt;/p&gt;
    &lt;p&gt;Here are a few examples of what happens:&lt;/p&gt;
    &lt;p&gt;And, yes, in all these cases we can readily see that the machines will never halt‚Äîand instead, potentially after some transient, their heads just move essentially periodically forever. Here‚Äôs the distribution of periods one finds&lt;/p&gt;
    &lt;p&gt;with the longest-period cases being:&lt;/p&gt;
    &lt;p&gt;And here‚Äôs the distribution of transients&lt;/p&gt;
    &lt;p&gt;with the longest-transient cases being:&lt;/p&gt;
    &lt;p&gt;But this doesn‚Äôt quite account for all the machines that don‚Äôt halt after a million steps: there are still 1938 left over. There are 91 distinct patterns of growth‚Äîand here are samples of what happens:&lt;/p&gt;
    &lt;p&gt;All of these eventually have a fundamentally nested structure. The patterns grow at different rates‚Äîbut always in a regular succession of steps. Sometimes the spacings between these steps are polynomials, sometimes exponentials‚Äîimplying either fractional power or logarithmic growth of the corresponding pattern. But the important point for our purposes here is that we can be confident that‚Äîat least with input 1‚Äîwe know which &lt;/p&gt;
    &lt;p&gt;But what happens if we increase the input value we provide? Here are the first 20 maximum finite lifetimes we get:&lt;/p&gt;
    &lt;p&gt;In the ‚Äúpeak case‚Äù of input 10, the distribution of runtimes is&lt;/p&gt;
    &lt;p&gt;with, yes, the maximum value being a somewhat strange outlier.&lt;/p&gt;
    &lt;p&gt;What is that outlier? It‚Äôs machine 600720 (along with the related machine 670559)‚Äîand we‚Äôll be discussing it in more depth in the next section. But suffice it to say now that 600720 shows up repeatedly as the &lt;/p&gt;
    &lt;p&gt;What about for larger inputs? Well, things get wilder then. Like, for example, consider the case of machine 1955095. For all inputs up to 41, the machine halts after a modest number of steps:&lt;/p&gt;
    &lt;p&gt;But then, at input 42, there‚Äôs suddenly a surprise‚Äîand the machine never halts:&lt;/p&gt;
    &lt;p&gt;And, yes, we can immediately tell it never halts, because we can readily see that the same pattern of growth repeats periodically‚Äîevery 24 steps. (A more extreme example is &lt;/p&gt;
    &lt;p&gt;And, yes, things like this are the ‚Äúlong arm‚Äù of undecidability reaching in. But by successively investigating both larger inputs and longer runtimes, one can develop reasonable confidence that‚Äîat least most of the time‚Äîone is correctly identifying both cases that lead to halting, and ones that do not. And from this one can estimate that of all the 2,985,984 possible &lt;/p&gt;
    &lt;p&gt;Summarizing our results we find that‚Äîsomewhat surprisingly‚Äîthe halting fraction is quite similar for different numbers of states, and always close to 1/2:&lt;/p&gt;
    &lt;p&gt;And based on our census of halting machines, we can then conclude that the number of distinct total functions computed by &lt;/p&gt;
    &lt;head rend="h2"&gt;Machine 600720&lt;/head&gt;
    &lt;p&gt;In looking at the runtimes of &lt;/p&gt;
    &lt;p&gt;I actually first noticed this machine in the 1990s as part of my work on A New Kind of Science‚Äîand with considerable effort was able to give a rather elaborate analysis of at least some of its behavior:&lt;/p&gt;
    &lt;p&gt;The first remarkable thing about the machine is the dramatic peaks it exhibits in the output values it generates:&lt;/p&gt;
    &lt;p&gt;These peaks are accompanied by corresponding (somewhat less dramatic) peaks in runtime:&lt;/p&gt;
    &lt;p&gt;The first of the peaks shown here occurs at input i = 34‚Äîwith runtime 315,391, and output &lt;/p&gt;
    &lt;p&gt;but the basic point is that the machine seems to behave in a very ‚Äúdeliberate‚Äù way that one might imagine could be analyzed.&lt;/p&gt;
    &lt;p&gt;It turns out, though, that the analysis is surprisingly complicated. Here‚Äôs a table of maximum (worst-case) runtimes (and corresponding inputs and outputs):&lt;/p&gt;
    &lt;p&gt;For odd n &amp;gt; 3, the maximum runtime occurs when the input value i is:&lt;/p&gt;
    &lt;p&gt;The corresponding initial states for the Turing machine are of the form:&lt;/p&gt;
    &lt;p&gt;The output value with such an input (for odd n &amp;gt; 3) is then&lt;/p&gt;
    &lt;p&gt;while the runtime‚Äîderived effectively by ‚Äúmathematicizing‚Äù what the Turing machine does for these inputs‚Äîis given by the bizarrely complex formula:&lt;/p&gt;
    &lt;p&gt;What is the asymptotic behavior? It‚Äôs roughly 6Œ±n where Œ± varies with n according to:&lt;/p&gt;
    &lt;p&gt;So this is how long it can take the Turing machine to compute its output. But can we find that output faster, say just by finding a ‚Äúmathematical formula‚Äù for it? For inputs i with some particular forms (like the one above) it is indeed possible to find such formulas:&lt;/p&gt;
    &lt;p&gt;But in the vast majority of cases there doesn‚Äôt seem to be any simple mathematical-style formula. And indeed one can expect that this Turing machine is a typical computationally irreducible system: you can always find its output (here the value f[i]) by explicitly running the machine, but there‚Äôs no general way to shortcut this, and to systematically get to the answer by some reduced, shorter computation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Runtimes in s = 3, k = 2 Turing Machines&lt;/head&gt;
    &lt;p&gt;We discussed above that out of the 2.99 million possible &lt;/p&gt;
    &lt;p&gt;There are machines that give asymptotically constant runtime&lt;/p&gt;
    &lt;p&gt;with all odd asymptotic runtime values up to 21 (along with 25) being possible:&lt;/p&gt;
    &lt;p&gt;Then there are machines that give asymptotically linear runtimes, with even coefficients from 2 to 20 (along with 24)‚Äîfor example:&lt;/p&gt;
    &lt;p&gt;By the way, note that (as we mentioned before) some machines realize their worst-case runtimes for many specific inputs, while in other machines such runtimes are rare (here illustrated for machines with asymptotic runtimes 24n):&lt;/p&gt;
    &lt;p&gt;Sometimes there are machines whose worst-case runtimes increase linearly, but in effect with fractional slopes:&lt;/p&gt;
    &lt;p&gt;There are many machines whose worst-case runtimes increase in an ultimately linear way‚Äîbut with ‚Äúoscillations‚Äù:&lt;/p&gt;
    &lt;p&gt;Averaging out the oscillations gives an overall growth rate of the form Œ±n, where Œ± is an integer or rational number with (as it turns out) denominator 2 or 3; the possible values for Œ± are:&lt;/p&gt;
    &lt;p&gt;There are also machines with worst-case runtimes growing like Œ±n2, with Œ± an integer from 1 to 10 (though missing 7):&lt;/p&gt;
    &lt;p&gt;And then there are a few machines (such as 129559 and 1166261) with cubic growth rates.&lt;/p&gt;
    &lt;p&gt;The next‚Äîand, in fact, single largest‚Äîgroup of machines have worst-case runtimes that asymptotically grow exponentially, following linear recurrences. The possible asymptotic growth rates seem to be (œï is the golden ratio ):&lt;/p&gt;
    &lt;p&gt;Some particular examples of machines with these growth rates include (we‚Äôll see 5n/2 and 4n examples in the next section):&lt;/p&gt;
    &lt;p&gt;The first of these is machine 1020827, and the exact worst-case runtime for input size n in this case is:&lt;/p&gt;
    &lt;p&gt;The second case shown (machine 117245) has exact worst-case runtime&lt;/p&gt;
    &lt;p&gt;which satisfies the linear recurrence:&lt;/p&gt;
    &lt;p&gt;The third case (machine 1007039) has exact worst-case runtime:&lt;/p&gt;
    &lt;p&gt;It‚Äôs notable that in all of these cases, the maximum runtime for input size n occurs for input &lt;/p&gt;
    &lt;p&gt;Continuing and squashing the results, it becomes clear that there‚Äôs a nested structure to these patterns:&lt;/p&gt;
    &lt;p&gt;By the way, it‚Äôs certainly not necessary that the worst-case runtime must occur at the largest input of a given size. Here‚Äôs an example (machine 888388) where that‚Äôs not what happens&lt;/p&gt;
    &lt;p&gt;and where in the end the 2n/2 growth is achieved by having the same worst-case runtime for input sizes n and n + 1 for all even n:&lt;/p&gt;
    &lt;p&gt;One feature of everything we‚Äôve seen here is the runtimes we‚Äôve deduced are either asymptotically powers or asymptotically exponentials. There‚Äôs nothing in between‚Äîfor example nothing like nLog[n] or 4Sqrt[n]:&lt;/p&gt;
    &lt;p&gt;No doubt there are Turing machines with such intermediate growth, but apparently none with &lt;/p&gt;
    &lt;head rend="h2"&gt;Functions and Their Runtimes in s = 3, k = 2 Turing Machines&lt;/head&gt;
    &lt;p&gt;As we discussed above, out of the 2.99 million possible &lt;/p&gt;
    &lt;p&gt;The functions computed by the most machines are (where, not surprisingly, the identity function &lt;/p&gt;
    &lt;p&gt;The minimum number of machines that can compute a given function is always 2‚Äîbecause there‚Äôs always one machine with a transition, and another with a transition, as in:&lt;/p&gt;
    &lt;p&gt;But altogether there are about 13,000 of these ‚Äúisolate‚Äù machines, where no other &lt;/p&gt;
    &lt;p&gt;So what are these functions‚Äîand how long do they take to compute? And remember, these are functions that are computed by isolate machines‚Äîso whatever the runtime of those machines is, this can be thought of as defining a lower bound on the runtime to compute that function, at least by any &lt;/p&gt;
    &lt;p&gt;So what are the functions with the longest runtimes computed by isolate machines? The overall winner seems to be the function computed by machine 600720 that we discussed above.&lt;/p&gt;
    &lt;p&gt;Next appears to come machine 589111&lt;/p&gt;
    &lt;p&gt;with its asymptotically 4n runtime:&lt;/p&gt;
    &lt;p&gt;And although the values here, say for &lt;/p&gt;
    &lt;p&gt;Next appear to come machines like 599063&lt;/p&gt;
    &lt;p&gt;with asymptotic &lt;/p&gt;
    &lt;p&gt;Despite the seemingly somewhat regular pattern of values for this function, the machine that computes it is an isolate, so we know that at least among &lt;/p&gt;
    &lt;p&gt;What about the other machines with asymptotically exponential runtimes that we saw in the previous section? Well, the particular machines we used as examples there aren‚Äôt even close to isolates. But there are other machines that have the same exponentially growing runtimes, and that are isolates. And, just for once, there‚Äôs a surprise.&lt;/p&gt;
    &lt;p&gt;For asymptotic runtime 2n, it turns out that there is just a single isolate machine: 1342057:&lt;/p&gt;
    &lt;p&gt;But look at how simple the function this machine computes is. In fact, &lt;/p&gt;
    &lt;p&gt;But despite the simplicity of this, it still takes the Turing machine worst-case runtime &lt;/p&gt;
    &lt;p&gt;And, yes, after a transient at the beginning, all the machine is ultimately doing is to compute &lt;/p&gt;
    &lt;p&gt;Going on to asymptotic runtimes of the form 3n/2, it turns out there‚Äôs only one function for which there‚Äôs a machine (1007039) with this asymptotic runtime‚Äîand this function can be computed by over a hundred machines, many with faster runtimes, though some with slower (2n) runtimes (e.g. 879123).&lt;/p&gt;
    &lt;p&gt;What about asymptotic runtimes of order ? It‚Äôs more or less the same story as with 3n/2. There are 48 functions which can be computed by machines with this worst-case runtime. But in all cases there are also many other machines, with many other runtimes, that compute the same functions.&lt;/p&gt;
    &lt;p&gt;But now there‚Äôs another surprise. For asymptotic runtime 2n/2 there are two functions computed only by isolate machines (889249 and 1073017):&lt;/p&gt;
    &lt;p&gt;So, once again, these functions have the feature that they can‚Äôt be computed any faster by any other &lt;/p&gt;
    &lt;p&gt;When we looked at &lt;/p&gt;
    &lt;p&gt;Among &lt;/p&gt;
    &lt;p&gt;There are, of course, many more &lt;/p&gt;
    &lt;p&gt;Isolate machines immediately define lower bounds on runtime for the functions they compute. But in general (as we saw above) there can be many machines that compute a given function. For example, as mentioned above, there are 210,792 &lt;/p&gt;
    &lt;p&gt;with asymptotic runtimes ranging from constant to linear, quadratic and exponential. (The most rapidly increasing runtime is ~2n.)&lt;/p&gt;
    &lt;p&gt;For each function that can be computed, there‚Äôs a slightly different collection of runtime profiles; here are the ones for the functions computed by the next largest numbers of machines:&lt;/p&gt;
    &lt;head rend="h2"&gt;Can Bigger Machines Compute Functions Faster?&lt;/head&gt;
    &lt;p&gt;We saw above that there are functions which cannot be computed asymptotically faster than particular bounds by, say, any &lt;/p&gt;
    &lt;p&gt;The first thing to say is that (as we discussed before for &lt;/p&gt;
    &lt;p&gt;Among &lt;/p&gt;
    &lt;p&gt;where the exact worst-case runtime is:&lt;/p&gt;
    &lt;p&gt;But now we can ask whether this function can be computed faster by any &lt;/p&gt;
    &lt;p&gt;An example is machine 1069163:&lt;/p&gt;
    &lt;p&gt;We can think of what‚Äôs happening as being that we start from the &lt;/p&gt;
    &lt;p&gt;and in effect optimize this by using a slightly more complicated ‚Äúinstruction set‚Äù:&lt;/p&gt;
    &lt;p&gt;In looking at &lt;/p&gt;
    &lt;p&gt;As an example, consider the function computed by the isolate machine 1342057:&lt;/p&gt;
    &lt;p&gt;This has asymptotic runtime 4n. But now if we look at &lt;/p&gt;
    &lt;p&gt;There are also machines with linearly and quadratically increasing runtimes‚Äîthough, confusingly, for the first few input sizes, they seem to be increasing just as fast as our original &lt;/p&gt;
    &lt;p&gt;Here are the underlying rules for these particular Turing machines:&lt;/p&gt;
    &lt;p&gt;And here‚Äôs the full spectrum of runtime profiles achieved by &lt;/p&gt;
    &lt;p&gt;There are runtimes that are easy to recognize as exponentials‚Äîthough with bases like 2,, 3/2, that are smaller than 4. Then there are linear and polynomial runtimes of the kind we just saw. And there‚Äôs some slightly exotic ‚Äúoscillatory‚Äù behavior, like with machine 1418699063&lt;/p&gt;
    &lt;p&gt;that seems to settle down to a periodic sequence of ratios, growing asymptotically like 2n/4.&lt;/p&gt;
    &lt;p&gt;What about other functions that are difficult to compute by &lt;/p&gt;
    &lt;p&gt;One of these follows exactly the runtimes of 600720; the other is not the same, but is very close, with about half the runtimes being the same, and the other half having maximal differences that grow linearly with n.&lt;/p&gt;
    &lt;p&gt;And what this means is that‚Äîunlike the function computed by &lt;/p&gt;
    &lt;p&gt;Looking at other functions that are ‚Äúhard to compute‚Äù with &lt;/p&gt;
    &lt;head rend="h2"&gt;With a Sufficiently Large Turing Machine‚Ä¶&lt;/head&gt;
    &lt;p&gt;We‚Äôve been talking so far about very small Turing machines‚Äîwith at most a handful of distinct cases in their rules. But what if we consider much larger Turing machines? Would these allow us to systematically do computations much faster?&lt;/p&gt;
    &lt;p&gt;Given a particular (finite) mapping from input to output values, say&lt;/p&gt;
    &lt;p&gt;it‚Äôs quite straightforward to construct a Turing machine&lt;/p&gt;
    &lt;p&gt;whose state transitions in effect just ‚Äúimmediately look up‚Äù these values:&lt;/p&gt;
    &lt;p&gt;(If we try to compute a value that hasn‚Äôt been defined, the Turing machine will simply not halt.)&lt;/p&gt;
    &lt;p&gt;If we stay with a fixed value of k, then for a ‚Äúfunction lookup table‚Äù of size v, the number of states we need for a ‚Äúdirect representation‚Äù of the lookup table is directly proportional to v. Meanwhile, the runtime is just equal to the absolute lower bound we discussed above, which is linearly proportional to the sizes of input and output.&lt;/p&gt;
    &lt;p&gt;Of course, with this setup, as we increase v we increase the size of the Turing machine. And we can‚Äôt guarantee to encode a function defined, say, for all integers, with anything less than an infinite Turing machine.&lt;/p&gt;
    &lt;p&gt;But by the time we‚Äôre dealing with an infinite Turing machine we don‚Äôt really need to be computing anything; we can just be looking everything up. And indeed computation theory always in effect assumes that we‚Äôre limiting the size of our machines. And as soon as we do this, there starts to be all sorts of richness in questions like which functions are computable, and what runtime is required to compute them.&lt;/p&gt;
    &lt;p&gt;In the past, we might just have assumed that there is some arbitrary bound on the size of Turing machines, or, in effect, a bound on their ‚Äúalgorithmic information content‚Äù or ‚Äúprogram size‚Äù. But the point of what we‚Äôre doing here is to explore what happens not with arbitrary bounds, but with bounds that are small enough to allow us to do exhaustive empirical investigations.&lt;/p&gt;
    &lt;p&gt;In other words, we‚Äôre restricting ourselves to low algorithmic (or program) complexity and&lt;lb/&gt; asking what then happens with time complexity, space complexity, etc. And what we find is that even in that domain, there‚Äôs remarkable richness in the behavior we‚Äôre able to see. And from the Principle of Computational Equivalence we can expect that this richness is already characteristic of what we‚Äôd see even with much larger Turing machines, and thus larger algorithmic complexity. &lt;/p&gt;
    &lt;head rend="h2"&gt;Beyond Binary Turing Machines&lt;/head&gt;
    &lt;p&gt;In everything we‚Äôve done so far, we‚Äôve been looking at ‚Äúbinary‚Äù (i.e. &lt;/p&gt;
    &lt;p&gt;The setup we‚Äôve been using translates immediately:&lt;/p&gt;
    &lt;p&gt;The simplest case is &lt;/p&gt;
    &lt;p&gt;Of these machines, 88 always halt‚Äîand compute 77 distinct functions. The possible runtimes are:&lt;/p&gt;
    &lt;p&gt;And unlike what we saw even for &lt;/p&gt;
    &lt;p&gt;For s = 2, k = 3, we have &lt;/p&gt;
    &lt;p&gt;In both cases, of the machines that don‚Äôt halt, the vast majority become periodic. For &lt;/p&gt;
    &lt;p&gt;Just as for &lt;/p&gt;
    &lt;p&gt;or in squashed form:&lt;/p&gt;
    &lt;p&gt;If we look beyond input 1, we find that about 1.12 million &lt;/p&gt;
    &lt;p&gt;A notable feature is that the tail consists of functions computed by only one machine. In the &lt;/p&gt;
    &lt;p&gt;What about runtimes? The results for &lt;/p&gt;
    &lt;p&gt;In a sense it should not be surprising that there is so much similarity between the behavior of &lt;/p&gt;
    &lt;p&gt;However, if we look not at the kind of ‚Äúone-sided‚Äù (or ‚Äúhalt if you go to the right‚Äù) Turing machines we are considering here, but instead at Turing machines where the head can go freely in either direction, then one difference emerges. Starting with a blank tape, all &lt;/p&gt;
    &lt;p&gt;And this fact provides a clue that such a machine (or, actually, the 14 essentially equivalent machines of which this is one example) might be capable of universal computation. And indeed it can be shown that‚Äîat least with appropriate (infinite) initial conditions, the machine can successfully be ‚Äúprogrammed‚Äù to emulate systems that are known to be universal, thereby proving that it itself is universal.&lt;/p&gt;
    &lt;p&gt;How does this machine fare with our one-sided setup? Here‚Äôs what it does with the first few inputs:&lt;/p&gt;
    &lt;p&gt;And what one finds is that for any input, the head of the machine eventually goes to the right, so with our one-sided setup we consider the machine to halt:&lt;/p&gt;
    &lt;p&gt;It turns out that the worst-case runtime for input of size n grows according to:&lt;/p&gt;
    &lt;p&gt;But if we look at the function computed by this machine we can ask whether there are ways to compute it faster. And it turns out there are 11 other s = 2, k = 3 machines (though, for example, no &lt;/p&gt;
    &lt;p&gt;one might think they would be simple enough to have shorter runtimes. But in fact in the one-sided setup their behavior is basically identical to our original machine.&lt;/p&gt;
    &lt;p&gt;OK, but what about &lt;/p&gt;
    &lt;head rend="h2"&gt;Recognizable Functions&lt;/head&gt;
    &lt;p&gt;We‚Äôve been talking a lot about how fast Turing machines can compute functions. But what can we say about what functions they compute? With appropriate encoding of inputs and decoding of outputs, we know that (essentially by definition) any computable function can be computed by some Turing machine. But what about the simple Turing machines we‚Äôve been using here? And what about ‚Äúwithout encodings‚Äù?&lt;/p&gt;
    &lt;p&gt;The way we‚Äôve set things up, we‚Äôre taking both the input and the output to our Turing machines to be the sequences of values on their tapes‚Äîand we‚Äôre interpreting these values as digits of integers. So that means we can think of our Turing machines as defining functions from integers to integers. But what functions are they?&lt;/p&gt;
    &lt;p&gt;Here are two &lt;/p&gt;
    &lt;p&gt;There are a total of 17 &lt;/p&gt;
    &lt;p&gt;Still, for &lt;/p&gt;
    &lt;p&gt;If we restrict ourselves to even inputs, then we can compute &lt;/p&gt;
    &lt;p&gt;Similarly, there are &lt;/p&gt;
    &lt;p&gt;What about other ‚Äúmathematically simple‚Äù functions, say &lt;/p&gt;
    &lt;p&gt;We‚Äôve already seen a variety of examples where our Turing machines can be interpreted as evaluating bitwise functions of their inputs. A more minimal case would be something like a single bitflip‚Äîand indeed there is an &lt;/p&gt;
    &lt;p&gt;To be able to flip a higher-order digit, one needs a Turing machine with more states. There are two &lt;/p&gt;
    &lt;p&gt;And in general‚Äîas these pictures suggest‚Äîflipping the mth bit can be done with a Turing machine with at least &lt;/p&gt;
    &lt;p&gt;What about Turing machines that compute periodic functions? Strict (nontrivial) periodicity seems difficult to achieve. But here, for example, is an &lt;/p&gt;
    &lt;p&gt;With both &lt;/p&gt;
    &lt;p&gt;Another thing one might ask is whether one Turing machine can emulate another. And indeed that‚Äôs what we see happening‚Äîvery directly‚Äîwhenever one Turing machine computes the same function as another.&lt;/p&gt;
    &lt;p&gt;(We also know that there exist universal Turing machines‚Äîthe simplest having &lt;/p&gt;
    &lt;head rend="h2"&gt;Empirical Computational Irreducibility&lt;/head&gt;
    &lt;p&gt;Computational irreducibility has been central to much of the science I‚Äôve done in the past four decades or so. And indeed it‚Äôs guided our intuition in much of what we‚Äôve been exploring here. But the things we‚Äôve discussed now also allow us to take an empirical look at the core phenomenon of computational irreducibility itself.&lt;/p&gt;
    &lt;p&gt;Computational irreducibility is ultimately about the idea that there can be computations where in effect there is no shortcut: there is no way to systematically find their results except by running each of their steps. In other words, given an irreducible computation, there‚Äôs basically no way to come up with another computation that gives the same result, but in fewer steps. Needless to say, if one wants to tighten up this intuitive idea, there are lots of detailed issues to consider. For example, what about just using a computational system that has ‚Äúbigger primitives‚Äù? Like many other foundational concepts in theoretical science, it‚Äôs difficult to pin down exactly how one should set things up‚Äîso that one doesn‚Äôt either implicitly assume what one‚Äôs trying to explain, or so restrict things that everything becomes essentially trivial.&lt;/p&gt;
    &lt;p&gt;But using what we‚Äôve done here, we can explore a definite‚Äîif restricted‚Äîversion of computational irreducibility in a very explicit way. Imagine we‚Äôre computing a function using a Turing machine. What would it mean to say that that function‚Äîand the underlying behavior of the Turing machine that computes it‚Äîis computationally irreducible? Essentially it‚Äôs that there‚Äôs no other faster way to compute that function.&lt;/p&gt;
    &lt;p&gt;But if we restrict ourselves to computation by a certain size of Turing machine, that‚Äôs exactly what we‚Äôve studied at great length here. And, for example, whenever we have what we‚Äôve called an ‚Äúisolate‚Äù Turing machine, we know that no other Turing machine of the same size can compute the same function. So that means one can say that the function is computationally irreducible with respect to Turing machines of the given size.&lt;/p&gt;
    &lt;p&gt;How robust is such a notion? We‚Äôve seen examples above where a given function can be computed, say, only in exponential time by an &lt;/p&gt;
    &lt;p&gt;But the important point here is that we can already see a restricted version of computational irreducibility just by looking explicitly at Turing machines of a given size. And this allows us to get concrete results about computational irreducibility, or at least about this restricted version of it.&lt;/p&gt;
    &lt;p&gt;One of the remarkable discoveries in looking at lots of kinds of systems over the years has been just how common the phenomenon of computational irreducibility seems to be. But usually we haven‚Äôt had a way to rigorously say that we‚Äôre seeing computational irreducibility in any particular case. All we typically know is that we can‚Äôt ‚Äúvisually decode‚Äù what‚Äôs going on, nor can particular methods we try. (And, yes, the fact that a wide variety of different methods almost always agree about what‚Äôs ‚Äúcompressible‚Äù and what‚Äôs not encourages our conclusions about the presence of computational irreducibility.)&lt;/p&gt;
    &lt;p&gt;In looking at Turing machines here, we‚Äôre often seeing ‚Äúvisual complexity‚Äù, not so much in the detailed‚Äîoften ponderous‚Äîbehavior with a particular initial condition, but more, for example, in what we get by plotting function values against inputs. But now we have a more rigorous‚Äîif restricted‚Äîtest for computational irreducibility: we can ask whether the function that‚Äôs being computed is irreducible with respect to this size of Turing machine, or, typically equivalently, whether the Turing machine we‚Äôre looking at is an isolate.&lt;/p&gt;
    &lt;p&gt;So now we can, for example, explore how common irreducibility defined in this way might be. Here are results for some of the classes of small Turing machines we‚Äôve studied above:&lt;/p&gt;
    &lt;p&gt;And what we see is that‚Äîmuch like our impression from computational systems like cellular automata‚Äîcomputational irreducibility is indeed very common among small Turing machines, where now we‚Äôre using our rigorous, if restricted, notion of computational irreducibility.&lt;/p&gt;
    &lt;p&gt;(It‚Äôs worth commenting that while ‚Äúglobal‚Äù features of Turing machines‚Äîlike the functions they compute‚Äîmay be computationally irreducible, there can still be lots of computational reducibility in their more detailed properties. And indeed what we‚Äôve seen here is that there are plenty of features of the behavior of Turing machines‚Äîlike the back-and-forth motion of their heads‚Äîthat look visually simple, and that we can expect to compute in dramatically faster ways than just running the Turing machine itself.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Nondeterministic (Multiway) Turing Machines&lt;/head&gt;
    &lt;p&gt;So far, we‚Äôve made a fairly extensive study of ordinary, deterministic (‚Äúsingle-way‚Äù) Turing machines. But the P vs. NP question is about comparing the capabilities of such deterministic Turing machines with the capabilities of nondeterministic‚Äîor multiway‚ÄîTuring machines.&lt;/p&gt;
    &lt;p&gt;An ordinary (deterministic) Turing machine has a rule such as&lt;/p&gt;
    &lt;p&gt;that specifies a unique sequence of successive configurations for the Turing machine&lt;/p&gt;
    &lt;p&gt;which we can represent as:&lt;/p&gt;
    &lt;p&gt;A multiway Turing machine, on the other hand, can have multiple rules, such as&lt;/p&gt;
    &lt;p&gt;which are applied in all possible ways to generate a whole multiway graph of successive configurations for the Turing machine&lt;/p&gt;
    &lt;p&gt;where we have indicated edges in the multiway graph associated with the application of each rule respectively by and , and where identical Turing machine configurations are merged.&lt;/p&gt;
    &lt;p&gt;Just as we have done for ordinary (deterministic) Turing machines, we take multiway Turing machines to reach a halting configuration whenever the head goes further to the right than it started‚Äîthough now this may happen on multiple branches‚Äîso that the Turing machine in effect can generate multiple outputs.&lt;/p&gt;
    &lt;p&gt;With the way we have set things up, we can think of an ordinary (deterministic) Turing machine as taking an input i and giving as output some value f[i] (where that value might be undefined if the Turing machine doesn‚Äôt halt for a given i). In direct analogy, we can think of a multiway Turing machine as taking an input i and giving potentially a whole collection of corresponding outputs:&lt;/p&gt;
    &lt;p&gt;Among the immediate complications is the fact that the machine may not halt, at least on some branches‚Äîas happens for input 3 here, indicated by a red dot in the plot above:&lt;/p&gt;
    &lt;p&gt;(In addition, we see that there can be multiple paths that lead to a given output, in effect defining multiple runtimes for that output. There can also be cycles, but in defining ‚Äúruntimes‚Äù we ignore these.)&lt;/p&gt;
    &lt;p&gt;When we construct a multiway graph we are effectively setting up a representation for all possible paths in the evolution of a (multiway) system. But when we talk about nondeterministic evolution we are typically imagining that just a single path is going to be followed, but we don‚Äôt know which.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs say that we have a multiway Turing machine that for every given input generates a certain set of outputs. If we were to pick just one of the outputs from each of these sets, we would effectively in each case be picking one path in the multiway Turing machine. Or, in other words, we would be ‚Äúdoing a nondeterministic computation‚Äù, or in effect getting output from a nondeterministic Turing machine.&lt;/p&gt;
    &lt;p&gt;As an example, let‚Äôs take our multiway Turing machine from above. Here is an example of how this machine‚Äîthought of as a nondeterministic Turing machine‚Äîcan generate a certain sequence of output values:&lt;/p&gt;
    &lt;p&gt;Each of these output values is achieved by following a certain path in the multiway graph obtained with each input:&lt;/p&gt;
    &lt;p&gt;Keeping only the path taken (and including the underlying Turing machine configuration) this represents how each output value was ‚Äúderived‚Äù:&lt;/p&gt;
    &lt;p&gt;The length of the path can then be thought of as the runtime required for the nondeterministic Turing machine to reach the output value. (When there are multiple paths to a given output value, we‚Äôll typically consider ‚Äúthe runtime‚Äù to be the length of the shortest of these paths.) So now we can summarize the runtimes from our example as follows:&lt;/p&gt;
    &lt;p&gt;The core of the P vs. NP problem is to compare the runtime for a particular function obtained by deterministic and nondeterministic Turing machines.&lt;/p&gt;
    &lt;p&gt;So, for example, given a deterministic Turing machine that computes a certain function, we can ask whether there is a nondeterministic Turing machine which‚Äîif you picked the right branch‚Äîcan compute that same function, but faster.&lt;/p&gt;
    &lt;p&gt;In the case of the example above, there are two possible underlying Turing machine rules indicated by and . For each input we can choose at each step a different rule to apply in order to get to the output:&lt;/p&gt;
    &lt;p&gt;The possibility of using different rules at different steps in effect allows much more freedom in how our computation can be done. The P vs. NP question concerns whether this freedom allows one to fundamentally speed up the computation of a given function.&lt;/p&gt;
    &lt;p&gt;But before we explore that question further, let‚Äôs take a look at what multiway (nondeterministic) Turing machines typically do; in other words, let‚Äôs study their ruliology.&lt;/p&gt;
    &lt;head rend="h2"&gt;Multiway (Nondeterministic) s = 1, k = 2 Turing Machines&lt;/head&gt;
    &lt;p&gt;As our first example of doing ruliology for multiway Turing machines, let‚Äôs consider the case of pairs of &lt;/p&gt;
    &lt;p&gt;Sometimes, as in machine {1,9}, there turns out to be a unique output value for every input:&lt;/p&gt;
    &lt;p&gt;Sometimes, as in machine {5,9}, there is usually a unique value, but sometimes not:&lt;/p&gt;
    &lt;p&gt;Something similar happens with {3,7}:&lt;/p&gt;
    &lt;p&gt;There are cases‚Äîlike {1,3}‚Äîwhere for some inputs there‚Äôs a ‚Äúburst‚Äù of possible outputs:&lt;/p&gt;
    &lt;p&gt;There are also plenty of cases where for some inputs&lt;/p&gt;
    &lt;p&gt;or for all inputs, there are branches that do not halt:&lt;/p&gt;
    &lt;p&gt;What about runtimes? Well, for each possible output in a nondeterministic Turing machine, we can see how many steps it takes to reach that output on any branch of the multiway graph, and we can consider that minimum number to be the ‚Äúnondeterministic runtime‚Äù needed to compute that output.&lt;/p&gt;
    &lt;p&gt;It‚Äôs the quintessential setup for NP computations: if you can successfully guess what branch to follow, you can potentially get to an answer quickly. But if you have to explicitly check each branch in turn, that can be a slow process.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs an example showing possible outputs and possible runtimes for a sequence of inputs for the {3,7} nondeterministic machine&lt;/p&gt;
    &lt;p&gt;or, combined in 3D:&lt;/p&gt;
    &lt;p&gt;So what functions can a nondeterministic machine like this ‚Äúnondeterministically‚Äù generate? For each input we have to pick one of the possible corresponding (‚Äúmultiway‚Äù) outputs. And in effect the possible functions correspond to possible ‚Äúthreadings‚Äù through these values&lt;/p&gt;
    &lt;p&gt;or:&lt;/p&gt;
    &lt;p&gt;To each function one can then associate a ‚Äúnondeterministic runtime‚Äù for each output, here:&lt;/p&gt;
    &lt;head rend="h2"&gt;Nondeterministic vs. Deterministic Machines&lt;/head&gt;
    &lt;p&gt;We‚Äôve seen how a nondeterministic machine can in general generate multiple functions, with each output from the function being associated with a minimum (‚Äúnondeterministic‚Äù) runtime. But how do the functions that a particular nondeterministic machine can generate compare with the functions that deterministic machines can generate? Or, put another way, given a function that a nondeterministic machine can generate (or ‚Äúcompute‚Äù), what deterministic machine is required to compute the same function?&lt;/p&gt;
    &lt;p&gt;Let‚Äôs look at the &lt;/p&gt;
    &lt;p&gt;Can we find a deterministic &lt;/p&gt;
    &lt;p&gt;so that it inevitably gives the same results as deterministic machine 3.&lt;/p&gt;
    &lt;p&gt;But (apart from the other trivial case based on following ‚Äúmachine 7‚Äù branches) none of the other functions we can generate from this nondeterministic machine can be reproduced by any &lt;/p&gt;
    &lt;p&gt;What about &lt;/p&gt;
    &lt;p&gt;And here are the paths through the multiway graphs for that machine that get to these values&lt;/p&gt;
    &lt;p&gt;with the ‚Äúpaths on their own‚Äù being&lt;/p&gt;
    &lt;p&gt;yielding ‚Äúnondeterministic runtimes‚Äù:&lt;/p&gt;
    &lt;p&gt;This is how the deterministic &lt;/p&gt;
    &lt;p&gt;Here are the pair of underlying rules for the nondeterministic machine&lt;/p&gt;
    &lt;p&gt;and here is the deterministic machine that reproduces a particular function it can generate:&lt;/p&gt;
    &lt;p&gt;This example is rather simple, and has the feature that even the deterministic machine always has a very small runtime. But now the question we can ask is whether a function that takes a deterministic machine of a certain class a certain time to compute can be computed in a smaller time if its results are ‚Äúpicked out of‚Äù a nondeterministic machine.&lt;/p&gt;
    &lt;p&gt;We saw above that &lt;/p&gt;
    &lt;p&gt;But what about a nondeterministic machine? How fast can this be?&lt;/p&gt;
    &lt;p&gt;It turns out that there are 15 nondeterministic machines based on pairs of &lt;/p&gt;
    &lt;p&gt;Here are the paths within the multiway graph for the nondeterministic machine that are sampled to generate the deterministic Turing machine result:&lt;/p&gt;
    &lt;p&gt;And here are these ‚Äúpaths on their own‚Äù:&lt;/p&gt;
    &lt;p&gt;We can compare these with the computations needed in the deterministic machine:&lt;/p&gt;
    &lt;p&gt;With our rendering, the lengths of the nondeterministic paths might look longer. But in fact they are considerably shorter, as we see by plotting them (in orange) along with the deterministic runtimes (in gray):&lt;/p&gt;
    &lt;p&gt;Looking now at the worst-case runtimes for inputs of size n, we get:&lt;/p&gt;
    &lt;p&gt;For the deterministic machine we found above for input size n, this worst-case runtime is given by:&lt;/p&gt;
    &lt;p&gt;But now the runtime in the nondeterministic machine turns out to be:&lt;/p&gt;
    &lt;p&gt;In other words, we‚Äôre seeing that nondeterminism makes it substantially faster to compute this particular function‚Äîat least by small Turing machines.&lt;/p&gt;
    &lt;p&gt;In a deterministic machine, it‚Äôs always the same underlying rule that‚Äôs applied at each step. But in a nondeterministic machine with the setup we‚Äôre using, we‚Äôre independently choosing one of two different rules to apply at each step. The result is that for every function value we compute, we‚Äôre making a sequence of choices:&lt;/p&gt;
    &lt;p&gt;And the core question that underlies things like the P vs. NP problem is how much advantage the freedom to make these choices conveys‚Äîand whether, for example, it allows us to ‚Äúnondeterministically‚Äù compute in polynomial time what takes more than polynomial (say, exponential) time to compute deterministically.&lt;/p&gt;
    &lt;p&gt;As a first example, let‚Äôs look at the function computed by the &lt;/p&gt;
    &lt;p&gt;Well, it turns out that the &lt;/p&gt;
    &lt;p&gt;And indeed, while the deterministic machine takes exponentially increasing runtime, the nondeterministic machine has a runtime that quickly approaches the fixed constant value of 5:&lt;/p&gt;
    &lt;p&gt;But is this somehow trivial? As the plot above suggests, the nondeterministic machine (at least eventually) generates all possible odd output values (and for even input i, also generates &lt;/p&gt;
    &lt;p&gt;What makes the runtime end up being constant, however, is that in this particular case, the output f[i] is always close to i (in fact, &lt;/p&gt;
    &lt;p&gt;There are actually no fewer than &lt;/p&gt;
    &lt;p&gt;And while all of them are in a sense straightforward in their operation, they illustrate the point that even when a function requires exponential time for a deterministic Turing machine, it can require much less time for a nondeterministic machine‚Äîand even a nondeterministic machine that has a much smaller rule.&lt;/p&gt;
    &lt;p&gt;What about other cases of functions that require exponential time for deterministic machines? The functions computed by the &lt;/p&gt;
    &lt;p&gt;Something slightly different happens with &lt;/p&gt;
    &lt;head rend="h2"&gt;The Limit of Nondeterminism and the Ruliad&lt;/head&gt;
    &lt;p&gt;A deterministic Turing machine has a single, definite rule that‚Äôs applied at each step. In the previous sections we‚Äôve explored what‚Äôs in a sense a minimal case of nondeterminism in Turing machines‚Äîwhere we allow not just one, but two different possible rules to be applied at each step. But what if we increase the nondeterminism‚Äîsay by allowing more possible rules at each step?&lt;/p&gt;
    &lt;p&gt;We‚Äôve seen that there‚Äôs a big difference between determinism‚Äîwith one rule‚Äîand even our minimal case of nondeterminism, with two rules. But if we add in, say, a third rule, it doesn‚Äôt seem to typically make any qualitative difference. So what about the limiting case of adding in all conceivable rules?&lt;/p&gt;
    &lt;p&gt;We can think of what we get as an ‚Äúeverything machine‚Äù‚Äîa machine that has every possible rule case for any possible Turing machine, say for &lt;/p&gt;
    &lt;p&gt;Running this ‚Äúeverything machine‚Äù for one step starting with input 1 we get:&lt;/p&gt;
    &lt;p&gt;Four of the rule cases just lead back to the initial state. Then of the other four, two lead to halting states, and two do not. Dropping self-loops, going another couple of steps, and using a different graph rendering, we see that outputs 2 and 3 now appear:&lt;/p&gt;
    &lt;p&gt;Here are the results for input 2:&lt;/p&gt;
    &lt;p&gt;So where can the ‚Äúeverything machine‚Äù reach, and how long does it take? The answer is that from any input i it can eventually reach absolutely any output value j. The minimum number of steps required (i.e. the minimum path length in the multiway graph) is just the absolute lower bound that we found for runtimes in deterministic machines above:&lt;/p&gt;
    &lt;p&gt;Starting with input 1, the nondeterministic runtime to reach output j is then&lt;/p&gt;
    &lt;p&gt;which grows logarithmically with output value, or linearly with output size.&lt;/p&gt;
    &lt;p&gt;So what this means is that the ‚Äúeverything machine‚Äù lets one nondeterministically go from a given input to a given output in the absolutely minimum number of steps structurally possible. In other words, with enough nondeterminism every function becomes nondeterministically ‚Äúeasy to compute‚Äù.&lt;/p&gt;
    &lt;p&gt;An important feature of the ‚Äúeverything machine‚Äù is that we can think of it as being a fragment of the ruliad. The full ruliad‚Äîwhich appears at the foundations of physics, mathematics and much more‚Äîis the entangled limit of all possible computations. There are many possible bases for the ruliad; Turing machines are one. In the full ruliad, we‚Äôd have to consider all possible Turing machines, with all possible sizes. The ‚Äúeverything machine‚Äù we‚Äôve been discussing here gives us just part of that, corresponding to all possible Turing machine rules with a specific number of states and colors.&lt;/p&gt;
    &lt;p&gt;In representing all possible computations, the ruliad‚Äîlike the ‚Äúeverything machine‚Äù‚Äîis maximally nondeterministic, so that it in effect includes all possible computational paths. But when we apply the ruliad in science (and even mathematics) we are interested not so much in its overall form as in particular slices of it which are sampled by observers that, like us, are computationally bounded. And indeed in the past few years it‚Äôs become clear that there‚Äôs a lot to say about the foundations of many fields by thinking in this way.&lt;/p&gt;
    &lt;p&gt;And one feature of computationally bounded observers is that they‚Äôre not maximally nondeterministic. Instead of following all possible paths in the multiway system, they tend to follow specific paths or bundles of paths‚Äîfor example reflecting the single thread of experience that characterizes our human perception of things. So‚Äîwhen it comes to observers‚Äîthe ‚Äúeverything machine‚Äù is somehow too nondeterministic. An actual (computationally bounded) observer will be concerned with one or just a few ‚Äúthreads of history‚Äù. In other words, if we‚Äôre interested in slices of the ruliad that observers will sample, what will be relevant is not so much the ‚Äúeverything machine‚Äù but rather deterministic machines, or at most machines with the kind of limited nondeterminism that we‚Äôve studied the past few sections.&lt;/p&gt;
    &lt;p&gt;But just how does what the ‚Äúeverything machine‚Äù can do compare with what all possible deterministic machines can do? In some ways, this is a core question in the comparison between determinism and nondeterminism. And it‚Äôs straightforward to start studying it empirically.&lt;/p&gt;
    &lt;p&gt;For example, here are successive steps in the multiway graph for the (&lt;/p&gt;
    &lt;p&gt;In a sense these pictures illustrate the ‚Äúreach‚Äù of deterministic vs. nondeterministic computation. In this particular case, with &lt;/p&gt;
    &lt;p&gt;For &lt;/p&gt;
    &lt;p&gt;and the values that can be reached by deterministic machines are:&lt;/p&gt;
    &lt;p&gt;But how long does it take to reach these values? This shows as dots the possible (deterministic) runtimes; the filling represents the minimum (nondeterministic) runtimes for the ‚Äúeverything machine‚Äù:&lt;/p&gt;
    &lt;p&gt;The most dramatic outlier occurs with value 31, which is reached deterministically only by machine 1447, in 15 steps, but which can be reached in 9 (nondeterministic) steps by the ‚Äúeverything machine‚Äù:&lt;/p&gt;
    &lt;p&gt;For &lt;/p&gt;
    &lt;head rend="h2"&gt;What Does It All Mean for P vs. NP?&lt;/head&gt;
    &lt;p&gt;The P vs. NP question asks whether every computation that can be done by any nondeterministic Turing machine with a runtime that increases at most polynomially with input size can also be done by some deterministic Turing machine with a runtime that also increases at most polynomially. Or, put more informally, it asks whether introducing nondeterminism can fundamentally speed up computation.&lt;/p&gt;
    &lt;p&gt;In its full form, this is an infinite question, that talks about limiting behavior over all possible inputs, in all possible Turing machines. But within this infinite question, there are definite, finite subquestions we can ask. And one of the things we‚Äôve done here is in effect to explore some of these questions in an explicit, ruliological way. Looking at these finite subquestions won‚Äôt in any direct way be able to resolve the full P vs. NP question.&lt;/p&gt;
    &lt;p&gt;But it can give us important intuition about the P vs. NP question, and what some of the difficulties and subtleties involved in it are. When one analyzes specific, constructed algorithms, it‚Äôs common to see that their runtimes vary quite smoothly with input size. But one of the things we‚Äôve seen here is that for arbitrary Turing machines ‚Äúin the wild‚Äù, it‚Äôs very typical for the runtimes to jump around in complicated ways. It‚Äôs also not uncommon to see dramatic outliers that occur only for very specific inputs.&lt;/p&gt;
    &lt;p&gt;If there was just one outlier, then in the limit of arbitrarily large input size it would eventually become irrelevant. But what if there were an unending sequence of outliers, of unpredictable sizes at unpredictable positions? Ultimately we expect all sorts of computational irreducibility, which in the limit can make it infinitely difficult to determine in any particular case the limiting behavior of the runtime‚Äîand, for example, to find out if it‚Äôs growing like a polynomial or not.&lt;/p&gt;
    &lt;p&gt;One might imagine, though, that if one looked at enough inputs, enough Turing machines, etc. then somehow any wildness would get in some way averaged out. But our ruliological results don‚Äôt encourage that idea. And indeed they tend to show that ‚Äúthere‚Äôs always more wildness‚Äù, and it‚Äôs somehow ubiquitous. One might have imagined that computational irreducibility‚Äîor undecidability‚Äîwould be sufficiently rare that it wouldn‚Äôt affect investigations of ‚Äúglobal‚Äù questions like the P vs. NP one. But our results suggest that, to the contrary, there are all sorts of complicated details and ‚Äúexceptions‚Äù that seem to get in the way of general conclusions.&lt;/p&gt;
    &lt;p&gt;Indeed, there seem to be issues at every turn. Some are related to unexpected behavior and outliers in runtimes. Some are related to the question of whether a particular machine ever even halts at all for certain inputs. And yet others are related to taking limits of sizes of inputs versus sizes of Turing machines, or amounts of nondeterminism. What our ruliological explorations have shown is that such issues are not obscure corner cases; rather they are generic and ubiquitous.&lt;/p&gt;
    &lt;p&gt;One has the impression, though, that they are more pronounced in deterministic than in nondeterministic machines. Nondeterministic machines in some sense ‚Äúaggregate‚Äù over paths, and in doing so, wash out the ‚Äúcomputational coincidences‚Äù which seem ubiquitous in determining the behavior of deterministic machines.&lt;/p&gt;
    &lt;p&gt;Certainly the specific experiments we‚Äôve done on machines of limited size do seem to support the idea that there are indeed computations that can be done quickly by a nondeterministic machine, but for which in deterministic machines there are for example at least occasional large runtime outliers, which imply longer general runtimes.&lt;/p&gt;
    &lt;p&gt;I had always suspected that the P vs. NP question would ultimately get ensnared in issues of computational irreducibility and undecidability. But from our explicit ruliological explorations we get an explicit sense of how this can happen. Will it nevertheless ultimately be possible to resolve the P vs. NP question with a finite mathematical-style proof based, say, on standard mathematical axioms? The results here make me doubt it.&lt;/p&gt;
    &lt;p&gt;Yes, it will be possible to get at least certain restricted global results‚Äîin effect by ‚Äúmining‚Äù pockets of computational reducibility. And, as we already know from what we have seen repeatedly here, it‚Äôs also possible to get definite results for, say, specific (ultimately finite) classes of Turing machines.&lt;/p&gt;
    &lt;p&gt;I‚Äôve only scratched the surface here of the ruliological results that can be found. In some cases to find more just requires expending more computer time. In other cases, though, we can expect that new methodologies, particularly around ‚Äúbulk‚Äù automated theorem proving, will be needed.&lt;/p&gt;
    &lt;p&gt;But what we‚Äôve seen here already makes it clear that there is much to be learned by ruliological methods about questions of theoretical computer science‚ÄîP vs. NP among them. In effect, we‚Äôre seeing that theoretical computer science can be done not only ‚Äúpurely theoretically‚Äù‚Äîsay with methods from traditional mathematics‚Äîbut also ‚Äúempirically‚Äù, finding results and developing intuition by doing explicit computational experiments and enumerations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Some Personal Notes&lt;/head&gt;
    &lt;p&gt;My efforts on what I now call ruliology started at the beginning of the 1980s, and in the early years I almost exclusively studied cellular automata. A large part of the reason was just that these were the first types of simple programs I‚Äôd investigated, and in them I had made a series of discoveries. I was certainly aware of Turing machines, but viewed them as less connected than cellular automata to my goal of studying actual systems in nature and elsewhere‚Äîthough ultimately theoretically equivalent.&lt;/p&gt;
    &lt;p&gt;It wasn‚Äôt until 1991, when I started systematically studying different types of simple programs as I embarked on my book A New Kind of Science that I actually began to do simulations of Turing machines. (Despite their widespread use in theoretical science for more than half a century, I think almost nobody else‚Äîfrom Alan Turing on‚Äîhad ever actually simulated them either.) At first I wasn‚Äôt particularly enamored of Turing machines. They seemed a little less elegant than mobile automata, and had much less propensity to show interesting and complex behavior than cellular automata.&lt;/p&gt;
    &lt;p&gt;Towards the end of the 1990s, though, I was working to connect my discoveries in what became A New Kind of Science to existing results in theoretical computer science‚Äîand Turing machines emerged as a useful bridge. In particular, as part of the final chapter of A New Kind of Science‚Äî‚ÄúThe Principle of Computational Equivalence‚Äù‚ÄîI had a section entitled ‚ÄúUndecidability and Intractability‚Äù. And in that section I used Turing machines as a way to explore the relation of my results to existing results on computational complexity theory.&lt;/p&gt;
    &lt;p&gt;And it was in the process of that effort that I invented the kind of one-sided Turing machines I‚Äôve used here:&lt;/p&gt;
    &lt;p&gt;I concentrated on the s = 2, k = 2 machines (for some reason I never looked at s = 1, k = 2), and found classes of machines that compute the same function‚Äîsometimes at different speeds:&lt;/p&gt;
    &lt;p&gt;And even though the computers I was using at the time were much slower than the ones I use today, I managed to extend what I was doing to s = 3, k = 2. At every turn, though, I came face to face with computational irreducibility and undecidability. I tried quite hard do things like resolve the exact number of distinct functions for &lt;/p&gt;
    &lt;p&gt;Nearly three decades later, I think I finally have the exact number. (Note that some of the details from A New Kind of Science are also different from what I have here, because in A New Kind of Science I included partial functions in my enumeration; here I‚Äôm mostly insisting on total functions, that halt and give a definite result for all inputs.)&lt;/p&gt;
    &lt;p&gt;After A New Kind of Science was released in 2002, I made another foray into Turing machines in 2007, putting up a prize on the fifth anniversary of the book for a proof (or refutation) of my suspicion that s = 2, k = 3 machine 596440 was capable of universal computation. The prize was soon won, establishing this machine as the very simplest universal Turing machine:&lt;/p&gt;
    &lt;p&gt;Many years passed. I occasionally suggested projects on Turing machines to students at the summer research program we started in 2003 (more on that later‚Ä¶). And I participated in celebrations of Alan Turing‚Äôs centenary in 2012. Then in 2020 we announced the Wolfram Physics Project‚Äîand I looked at Turing machines again, now as an example of a computational system that could be encoded with hypergraph rewriting, and studied using physics-inspired causal graphs, etc.:&lt;/p&gt;
    &lt;p&gt;Less than two months after the launch of our Physics Project I was studying what I now call the ruliad‚Äîand I decided to use Turing machines as a model for it:&lt;/p&gt;
    &lt;p&gt;A crucial part of this was the idea of multiway Turing machines:&lt;/p&gt;
    &lt;p&gt;I‚Äôd introduced multiway systems in A New Kind of Science, and had examples close to multiway Turing machines in the book. But now multiway Turing machines were more central to what I was doing‚Äîand in fact I started studying essentially what I‚Äôve here called the ‚Äúeverything machine‚Äù (though the details were different, because I wasn‚Äôt considering Turing machines that can halt):&lt;/p&gt;
    &lt;p&gt;I also started looking at the comparison between what can be reached deterministically and nondeterministically‚Äîand discussed the potential relation of this to the P vs. NP question:&lt;/p&gt;
    &lt;p&gt;By the next year, I was expanding my study of multiway systems, and exploring many different examples‚Äîwith one of them being multiway Turing machines:&lt;/p&gt;
    &lt;p&gt;Soon I realized that the general approach I was taking could be applied not only to the foundations of physics, but also to foundations of other fields. I studied the foundations of mathematics, of thermodynamics, of machine learning and of biology. But what about the foundations of theoretical computer science?&lt;/p&gt;
    &lt;p&gt;Over the years, I‚Äôd explored the ruliology of many kinds of systems studied in theoretical computer science‚Äîdoing deep dives into combinators for their centenary in 2020, as well as (last year) into lambdas. In all these investigations, I was constantly seeing concrete versions of phenomena discussed in theoretical computer science‚Äîeven though my emphasis tended to be different. But I was always curious what one might be able to say about central questions in theoretical computer science‚Äîlike P vs. NP.&lt;/p&gt;
    &lt;p&gt;I had imagined that the principal problem in doing an empirical investigation of something like P vs. NP would just be to enumerate enough cases. But when I got into it, I realized that the shadow of computational irreducibility loomed even larger than I‚Äôd imagined‚Äîand that even within particular cases it could be irreducibly difficult to figure out what one needed to know about their behavior.&lt;/p&gt;
    &lt;p&gt;Fairly late in the project I was trying to look up some ‚Äúconventional wisdom‚Äù about NP problems. Most of it was couched in rather traditional mathematical terms, and didn‚Äôt seem likely to have too much to say about what I was doing. But then I found a paper entitled ‚ÄúProgram-size versus Time Complexity: Slowdown and Speed-up Phenomena in the Micro-cosmos of Small Turing Machines‚Äù‚Äîand I was excited to see that it was following up on what I‚Äôd done in A New Kind of Science, and doing ruliology. But then I realized: the lead author of the paper, Joost Joosten, had been an (already-a-professor) student at our summer program in 2009, and I‚Äôd in fact suggested the original version of the project (though the paper had taken it further, and in some slightly different directions than I‚Äôd anticipated).&lt;/p&gt;
    &lt;p&gt;Needless to say, what I‚Äôve now done here raises a host of new questions, which can now be addressed by future projects done at our summer programs, and beyond‚Ä¶.&lt;/p&gt;
    &lt;p&gt;Note: For general historical background see my related writings from 2002 and 2021.&lt;/p&gt;
    &lt;head rend="h2"&gt;Thanks&lt;/head&gt;
    &lt;p&gt;Thanks to Willem Nielsen, Nik Murzin and Brian Mboya of the Wolfram Institute for extensive help. Thanks also to Wolfram Institute affiliate Anneline Daggelinckx, as well as to Richard Assar and Pavel Hajek of the Wolfram Institute for additional help. Work at the Wolfram Institute on this project was supported in part by the John Templeton Foundation.&lt;/p&gt;
    &lt;p&gt;Additional input on the project was provided by Lenore &amp;amp; Manuel Blum, Christopher Gilbert, Josh Grochow, Don Knuth and Michael Sun. Matthew Szudzik also contributed relevant work in 1999 during the development of A New Kind of Science.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46830027</guid><pubDate>Fri, 30 Jan 2026 21:17:21 +0000</pubDate></item><item><title>Stonebraker on CAP theorem and Databases (2010)</title><link>https://perspectives.mvdirona.com/2010/04/stonebraker-on-cap-theorem-and-databases/</link><description>&lt;doc fingerprint="d53793bb302c2f86"&gt;
  &lt;main&gt;
    &lt;p&gt;Mike Stonebraker published an excellent blog posting yesterday at the CACM site: Errors in Database Systems, Eventual Consistency, and the CAP Theorem. In this article, Mike challenges the application of Eric Brewer‚Äôs CAP Theorem by the NoSQL database community. Many of the high-scale NoSQL system implementers have argued that the CAP theorem forces them to go with an eventual consistent model. &lt;/p&gt;
    &lt;p&gt;Mike challenges this assertion pointing that some common database errors are not avoided by eventual consistency and CAP really doesn‚Äôt apply in these cases. If you have an application error, administrative error, or database implementation bug that losses data, then it is simply gone unless you have an offline copy. This, by the way, is why I‚Äôm a big fan of deferred delete. This is a technique where deleted items are marked as deleted but not garbage collected until some days or preferably weeks later. Deferred delete is not full protection but it has saves my butt more than once and I‚Äôm a believer. See On Designing and Deploying Internet-Scale Services for more detail.&lt;/p&gt;
    &lt;p&gt;CAP and the application of eventual consistency doesn‚Äôt directly protect us against application or database implementation errors. And, in the case of a large scale disaster where the cluster is lost entirely, again, neither eventual consistency nor CAP offer a solution. Mike also notes that network partitions are fairly rare. I could quibble a bit on this one. Network partitions should be rare but net gear continues to cause more issues than it should. Networking configuration errors, black holes, dropped packets, and brownouts, remain a popular discussion point in post mortems industry-wide. I see this improving over the next 5 years but we have a long way to go. In Networking: the Last Bastion of Mainframe Computing, I argue that net gear is still operating on the mainframe business model: large, vertically integrated and expensive equipment, deployed in pairs. When it comes to redundancy at scale, 2 is a poor choice.&lt;/p&gt;
    &lt;p&gt;Mike‚Äôs article questions whether eventual consistency is really the right answer for these workloads. I made some similar points in ‚ÄúI love eventual consistency but‚Ä¶‚Äù In that posting, I argued that many applications are much easier to implement with full consistency and full consistency can be practically implemented at high scale. In fact, Amazon SimpleDB recently announced support for full consistency. Apps needed full consistency are now easier to write and, where only eventual consistency is needed, its available as well.&lt;/p&gt;
    &lt;p&gt;Don‚Äôt throw full consistency out too early. For many applications, it is both affordable and helps reduce application implementation errors.&lt;/p&gt;
    &lt;p&gt;‚Äìjrh&lt;/p&gt;
    &lt;p&gt;Thanks to Deepak Singh for pointing me to this article.&lt;/p&gt;
    &lt;p&gt;b: http://blog.mvdirona.com / http://perspectives.mvdirona.com &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46831592</guid><pubDate>Fri, 30 Jan 2026 23:47:28 +0000</pubDate></item><item><title>Direct Current Data Centers</title><link>https://terraformindustries.wordpress.com/2026/01/30/direct-current-data-centers/</link><description>&lt;doc fingerprint="b712f05342b5d51a"&gt;
  &lt;main&gt;
    &lt;p&gt;Casey Handmer, Matt Weickert&lt;/p&gt;
    &lt;p&gt;This post explains our current views on how humanity will achieve Kardashev Level 1 status by exploiting the full energy resources of an entire planet. More specifically, how pure solar+batteries will power AI scaleup beyond gas turbine manufacturing limits.&lt;/p&gt;
    &lt;p&gt;It is an extension to my earlier post of March 2024 on using solar to power AI datacenters, and a response of sorts to the Scale Microgrids paper that showed a mix of solar and gas could reduce emissions for the developers and operators of next gen AI datacenters. In that paper, Kyle Baranko, Duncan Campbell and co-authors showed that around 90% solar with local natural gas backup generators would be the fastest way to get power. In this work, we show that taking this trend to its obvious conclusion and deleting all the legacy fuel-based power components can be even faster and cheaper. We also include a discussion of space-based inference.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs examine this problem from first principles. What is silicon cognition?&lt;/p&gt;
    &lt;p&gt;You can call it a tensor core, a Blackwell, a GPU, but these are all versions of the same thing. A sliver of silicon with billions of transistors, through which cascade a torrent of electrons converting the entropy of a few volts to the entropy of information generation, and the entropy of waste heat. A GPU is a very complicated switch that regulates current flow, with some other side effects.&lt;/p&gt;
    &lt;p&gt;For the foreseeable future, the GPU will be the expensive part, currently valued at around $50,000/kW. All it needs to continue to operate is an infinite supply of moderately spicy electrons, that is, a DC power supply at a few volts. Given that making power is much simpler than thinking, the job of the power supply is to be uncomplicated and relatively cheap. In no universe should providing power be the hard part.&lt;/p&gt;
    &lt;p&gt;Solar and batteries are a natural match to this demand. A solar panel is a slice of silicon (without logic gates) that absorbs solar photons and drives electrons uphill. To a good approximation, a solar module is a constant current source that maxes out at about 40 V. A battery is a reversible chemical reaction that stores and releases electrons, and to a good approximation is a constant voltage source. Modern lithium chemistries hold at about 3.9 V across nearly their entire state-of-charge range.&lt;/p&gt;
    &lt;p&gt;For logistical reasons related to the relative scarcity of copper in the crust of the Earth, it makes sense to operate solar cells, batteries, and GPUs in series so that the entire system runs at about 1000 V and each electron can be reused a few hundred times.&lt;/p&gt;
    &lt;p&gt;Our radical claim is that, in the limit, Earth-based AI compute will look like this:&lt;/p&gt;
    &lt;p&gt;By area, thousands of acres of solar panels.&lt;/p&gt;
    &lt;p&gt;By cost, a pile of GPUs.&lt;/p&gt;
    &lt;p&gt;In the limit, Earth-based AI compute will be a direct current (DC) solar array connected to a DC battery bank connected to a DC GPU rack.&lt;/p&gt;
    &lt;p&gt;This approach brings numerous other advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No grid connection.&lt;/item&gt;
      &lt;item&gt;No moving parts.&lt;/item&gt;
      &lt;item&gt;No turbines.&lt;/item&gt;
      &lt;item&gt;No gas connection.&lt;/item&gt;
      &lt;item&gt;No nuclear fuel.&lt;/item&gt;
      &lt;item&gt;No emissions.&lt;/item&gt;
      &lt;item&gt;No power conversion.&lt;/item&gt;
      &lt;item&gt;No transformers.&lt;/item&gt;
      &lt;item&gt;No inverters.&lt;/item&gt;
      &lt;item&gt;No power transmission.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;None of these parts make the AI smarter, and all of them can potentially intrude onto the critical path.&lt;/p&gt;
    &lt;p&gt;Delete.&lt;/p&gt;
    &lt;p&gt;It sounds nice in theory, but how can this work?&lt;/p&gt;
    &lt;p&gt;The key metric to optimize is tokens per dollar. For example, take Scale Microgrids‚Äô work on a 90-10 AI system, increase the size of the solar and battery farm enough to get to 99+% uptime, delete the gas power side, and compare overall economic productivity. A gas system that‚Äôs used only 1% or 0.1% of the time still costs time and money, and that‚Äôs the core reason why deleting it can end up reducing overall cost.&lt;/p&gt;
    &lt;p&gt;The graph below shows the tokens per dollar landscape for two hypothetical solar powered AI systems, one with a gas powerplant (blue) and one without (orange). Both have a solar array (size given in nameplate multiples of the peak AI load) and battery size (given in hours of capacity at full load).&lt;/p&gt;
    &lt;p&gt;The key insight is that there are two stable attractors. One with a pure gas energy supply, with solar and battery supplementation for vibes, CO2 reduction, or marginal capacity expansion. The other with pure solar and batteries, no gas. The pure gas system capex is minimized with no solar and batteries, as natural gas itself is relatively cheap given no preference for emissions reduction. But the two manifolds intersect along a frontier, and beyond that the solar array and battery are capable enough that it‚Äôs actually cheaper to delete the gas powerplant entirely.&lt;/p&gt;
    &lt;p&gt;This tradeoff does not come at zero cost. In exchange for deleting the cost, complexity, and schedule risk of a gas powerplant comes the sizable land demands of a solar array. To a rough approximation, 15 acres of solar are required per MW of DC AI load. For reference, the USA has about 150 million acres of unpopulated desert west of the Mississippi, enough for 10 TW of AI development. 10 TW is much more than total global electricity generation today. There is plenty.&lt;/p&gt;
    &lt;p&gt;On the other hand, while fracked gas is relatively abundant (for now) the turbines that convert it into power are hard to make, hard to ramp, and largely already spoken for. If AI seeks growth beyond the production ramp of turbines, it is clear which way the wind is blowing.&lt;/p&gt;
    &lt;p&gt;Before we get to the methods section, I‚Äôll give a rough heuristic for performance. Assuming an on-off binary state on the load, a 15 MW solar + 15 MWh battery can get to ~99% utilization anywhere in the US south west, but is that good enough? The short answer is yes ‚Äì maximizing tokens per dollar spent, or ROI, justifies throttling demand on a few of the longest, coldest nights of the year.&lt;/p&gt;
    &lt;p&gt;But it‚Äôs actually better than that. Remember that a GPU is a glorified silicon switch intermediating the flow of electrons downhill. Power consumption is proportional to clock frequency multiplied by the square of the voltage (P ~ f V2). GPU power consumption is not fundamental: Token production rate is. If we‚Äôve deleted DC-DC converters then voltage is set by the state of the battery, and frequency is controlled by software. This means that a 3% reduction in token production rate can buy us a 9% reduction in power consumption. So the math changes from 99% utilization to more like 99.7%. This shifts the economics around solar and battery plant sizing considerably, given that GPU frequency modulation allows for a 3x discount in actual utilization and token production.&lt;/p&gt;
    &lt;p&gt;There is one other implication of these wildly capable and versatile solar+battery AI data centers. They have enough power to operate at full, or nearly full, capacity for the entire year. For 10 months of the year they are oversupplied, and can provide electricity and low grade heat (from their cooling systems) to neighboring customers essentially for the marginal cost of power transport. These could be seasonal or intermittently friendly loads such as the synthetic hydrocarbons and primary materials being pioneered at Terraform, and/or local communities. At Terraform, we believe that power should be as cheap as possible.&lt;/p&gt;
    &lt;head rend="h2"&gt;Methods&lt;/head&gt;
    &lt;p&gt;Epistemology. How is it possible for this lightly evolved monkey to know these things?&lt;/p&gt;
    &lt;p&gt;You will need:&lt;/p&gt;
    &lt;p&gt;A year (at least) of real time solar data from a target location. This is data for an EW fixed tilt array in Texas that we generated by feeding fixed south tilt data into a slightly non-trivial geometry model.&lt;/p&gt;
    &lt;p&gt;A solar PV module IV curve model. This is based on the JAM72S30-540/MR/1500V but they‚Äôre all pretty similar.&lt;/p&gt;
    &lt;p&gt;A Li-ion discharge voltage curve.&lt;/p&gt;
    &lt;p&gt;A frequency-power curve for a typical GPU.&lt;/p&gt;
    &lt;p&gt;Plug these all together in a model that charges the battery when the sun is up, provided the panel voltage is high enough. We initially simulated a system with no power electronics whatsoever, but found that battery charging efficiency was inhibited when the battery state of charge was low, because pulling the panels to a lower voltage actually decreased their efficiency. Given that MPPTs are not that expensive, we could put them back in.&lt;/p&gt;
    &lt;p&gt;Then, provided the battery can deliver power, the GPUs are powered and we count how many tokens are generated.&lt;/p&gt;
    &lt;p&gt;Throw in a basic ‚Äúgovernor‚Äù that throttles the GPU when it predicts the battery will be exhausted before dawn.&lt;/p&gt;
    &lt;p&gt;This graph shows performance over a ten day period in winter. Note how the governor throttles output early on the fourth day by rationing power until the following morning. The cubic power consumption of GPUs means that throttling a little bit early is much better for token production than running full blast into a wall and then dropping to zero production until the sun comes back up.&lt;/p&gt;
    &lt;p&gt;Now run thousands of simulations for every combination of battery size and array size, measuring overall utilization of the load.&lt;/p&gt;
    &lt;p&gt;This chart shows yearly utilization of a GPU asset given solar and battery sizes, including our basic governor. Note that a ‚Äústeepest‚Äù ascent starting at zero solar and zero batteries turns first on adequate solar, then adequate batteries, then marginal solar, then marginal batteries. This reflects the shape of the resource curve and the degree of exploitation required to get to the marginal nth 9 of reliability.&lt;/p&gt;
    &lt;p&gt;This chart shows curtailment reduction with adoption of the minimum viable governor vs some naive on/off operator, showing 2.3-2.6x improvement, which is close to the 3x implied by the GPU‚Äôs cubic power consumption. This governor is not very sophisticated, for example, it has no ability to take weather prediction into account. It merely assesses the time of day, the state of the battery and of solar generation and curtails GPU utilization accordingly.&lt;/p&gt;
    &lt;p&gt;Throwing in assumptions about capex, we can assess capital efficiency.&lt;/p&gt;
    &lt;p&gt;This chart shows token production per dollar (in arbitrary units), showing a rather broad peak with considerable flexibility. Adding too much solar or batteries degrades capital efficiency ‚Äì the correct response is to add more GPUs in this case.&lt;/p&gt;
    &lt;p&gt;Because the peak is so broad, there is freedom to choose for one additional preference. That is, we can alter the size of the array and the battery by 20-30% with respect to the load and still get much the same return on capital. Given that land is finite, we may want to maximize tokens per acre while holding development cost constant, which puts us towards the lower edge of the peak in the diagram above. Then, holding land use and tokens per dollar equal, adding more battery towards the bottom right of the peak increases absolute token production on a fixed GPU and solar array asset base. This mirrors actual operational optimization, which is to say, pave all available land with solar, then add GPUs and batteries until revenue peaks.&lt;/p&gt;
    &lt;p&gt;At last, the machinery to perform a comparison with a gas or gas-solar hybrid system is in place. Plug in some assumptions around GPU cost, solar cost, battery cost, gas turbine cost, gas fuel cost, and amortization period, and you can produce this chart.&lt;/p&gt;
    &lt;p&gt;Here we assume that GPUs are $50,000/kW, batteries (including all ancillary power electronics) are $200/kWh, solar is $200/kW, gas turbines are $2500/kW, gas is $55/MWh, and we‚Äôre amortizing over 10 years.&lt;/p&gt;
    &lt;p&gt;The chart suggests the possibility that under some set of assumptions, it‚Äôs actually cheaper to delete the gas power system entirely, so what are those assumptions? For any given cost, select the peak utility point for solar array and battery size and marginalize across these parameters.&lt;/p&gt;
    &lt;p&gt;The left side of this chart shows where a pure solar system is the best value. As a rough rule of thumb, this is where 1.3 x battery cost + solar cost &amp;lt; $500/kW(h).&lt;/p&gt;
    &lt;p&gt;As a sanity check, in early 2026 we‚Äôre seeing large scale integrated battery storage systems ship from China for well below $150/kWh, while the cheapest industrial scale solar systems are going in for under $200/kW-DC-nameplate. This is beyond the critical cost threshold ‚Äì delete the gas system.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs recap.&lt;/p&gt;
    &lt;p&gt;It is possible and even optimal to run a datacenter on pure solar and battery. The optimum level of availability is between 99% and 99.9% utilization, with the balance taking the form, primarily, of throttled use rather than lights out.&lt;/p&gt;
    &lt;p&gt;The pure solar+battery data center is cheaper than a gas-assisted or pure gas data center if solar and battery costs per kW are &amp;lt;~10% of the turbine/gas generator cost. For example, if a 1 GW gas turbine costs $2.5b and the solar array costs $250m/GW and the BESS costs $250m/GWh, then the utility is roughly at parity.&lt;/p&gt;
    &lt;p&gt;We estimated the opportunity cost of a month of delay on a 1 GW gas turbine being delivered at about $20m, which probably isn‚Äôt high enough to justify deleting the component out of pure suspicion, provided that you are confident it will be delivered eventually.&lt;/p&gt;
    &lt;p&gt;On the other hand, we found that it is possible to commission a pure solar+battery data center with high utility and then backfill additional GPU capacity if/when a gas turbine becomes available.&lt;/p&gt;
    &lt;p&gt;There is broad latitude for design flexibility within the peak utility (tokens/$) range. For example, there are points with equal utility that have 20% less solar or significantly more availability, depending on secondary constraints such as land availability.&lt;/p&gt;
    &lt;p&gt;Given that, long term, trends will favor pure solar+batteries and the performance relative to complexity is already favorable, there is an argument that one hyperscaler should probably move aggressively in that direction, so as to obtain differentiation.&lt;/p&gt;
    &lt;p&gt;We investigated what the performance hit would be for a pure solar+battery DC power system that deleted non-computational silicon, that is, inverters, converters, and MPPTs between the solar and battery components, and even in the racks themselves. We found that the power system performance relative to cost improved with the deletion of AC and DC-DC conversion components between the arrays and batteries. Again, this is a nod to the future we will converge on.&lt;/p&gt;
    &lt;head rend="h2"&gt;Space AI&lt;/head&gt;
    &lt;p&gt;Late 2025 saw much speculation about space-based AI. It seems to me that SpaceX, with their incumbent advantages in launch and Starlink hardware expertise, may be able to ship gigawatts of inference compute into Earth orbit for something like 2x the per token cost of ground-based AI, but that this would still be quite profitable. Why bother? It‚Äôs a separate delivery and distribution channel that isn‚Äôt congested by the usual permitting and regulatory nonsense at play on the surface, or at least, a different and uniform set. And if you have unlimited launch upmass it helps to have a profitable use case, like Starlink or orbital AI, to soak up that supply.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs list their respective advantages and disadvantages.&lt;/p&gt;
    &lt;p&gt;Space AI&lt;/p&gt;
    &lt;p&gt;Positives:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simplified and unified regulatory regime.&lt;/item&gt;
      &lt;item&gt;High altitude (800 km ‚Äì 2400 km) dawn dusk sun synchronous orbits are never in shade (except momentarily during rare lunar eclipses) so don‚Äôt need batteries.&lt;/item&gt;
      &lt;item&gt;Don‚Äôt need to be cheaper than ground AI, as long as they‚Äôre cash flow positive.&lt;/item&gt;
      &lt;item&gt;Infinite source of marginal launch demand can fill in gaps from other customers for a very large rocket.&lt;/item&gt;
      &lt;item&gt;Passive stabilization with dihedral solar arrays is possible.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Negatives:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Launch cost is not free.&lt;/item&gt;
      &lt;item&gt;No maintenance capability.&lt;/item&gt;
      &lt;item&gt;Higher latency (50 ms worst case).&lt;/item&gt;
      &lt;item&gt;Radiation can accelerate GPU degradation.&lt;/item&gt;
      &lt;item&gt;Thermal environment requires relatively large radiators and (probably) active cooling.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Ground solar AI&lt;/p&gt;
    &lt;p&gt;Positives:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mostly a land development play, plenty of unused land on the ground.&lt;/item&gt;
      &lt;item&gt;Batteries are relatively cheap compared to GPUs and enable operation overnight, getting cheaper all the time.&lt;/item&gt;
      &lt;item&gt;Almost certainly cheaper than launch and space rating of components.&lt;/item&gt;
      &lt;item&gt;Don‚Äôt require a million tonnes a year to low Earth orbit to deploy.&lt;/item&gt;
      &lt;item&gt;Easier to maintain/retrofit.&lt;/item&gt;
      &lt;item&gt;Lower latency (closer to end users).&lt;/item&gt;
      &lt;item&gt;Default human Earth surface environment is less hostile to hardware.&lt;/item&gt;
      &lt;item&gt;Can easily cool using HVAC and air.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Negatives:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Regulatory/permitting is painful, byzantine, and locally variable.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Fundamentally this is a bet that GPUs are so valuable on a per gram basis that even launching them to space helps improve the economic utility of a Watt of solar power.&lt;/p&gt;
    &lt;p&gt;Finally, an application of space-based solar power that can justify something like the vision of Gerry O‚ÄôNeill. To be clear, this is because the value of a watt of space-transmitted microwave power encoding an intelligent token of data is about a trillion times higher than the value of a watt of space-transmitted microwave power competing with your local power plant to supply the grid.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Whether in space or on the ground, turbines are irrelevant to reaching Kardashev Level 1. The fastest growing AI will win, and the fastest growing AI must delete all non-essential parts. The only essential parts are a solar array, a battery, and the GPU itself.&lt;/p&gt;
    &lt;head rend="h2"&gt;Aside: Carl Sagan‚Äôs extension of Kardashev Levels from integers to reals&lt;/head&gt;
    &lt;p&gt;Planetary power: 1016 W -&amp;gt; K = 1&lt;lb/&gt;Stellar power: 1026 W -&amp;gt; K = 2&lt;lb/&gt;Galactic power: 1036 W -&amp;gt; K = 3&lt;/p&gt;
    &lt;p&gt;One step on the Kardashev scale is equivalent to increasing power consumption by a factor of 10 billion.&lt;/p&gt;
    &lt;p&gt;What is the current level of humanity?&lt;/p&gt;
    &lt;p&gt;Global electricity production: 3.5 TW -&amp;gt; K = 0.65.&lt;lb/&gt;Global fuel consumption: 20 TW -&amp;gt; K = 0.73.&lt;lb/&gt;Global planetary surface use for agriculture: 13% -&amp;gt; K = 0.91.&lt;/p&gt;
    &lt;p&gt;If all of Earth‚Äôs land was paved with solar PV at 26% efficiency -&amp;gt; K = 1.01. &lt;lb/&gt;If the entire Earth including oceans was paved with solar PV at 26% efficiency -&amp;gt; K = 1.06.&lt;lb/&gt;If we fill the unshaded dawn/dusk sun synchronous orbital (SSO) band (800 km to 2500 km) with SpaceX AI satellites, 10^17 W available -&amp;gt; K = 1.04.&lt;lb/&gt;All of Earth plus the SSO orbital band -&amp;gt; K = 1.085.&lt;/p&gt;
    &lt;p&gt;Convert the entire Moon into 2 kg/m^2 solar inference at Earth‚Äôs solar orbital radius -&amp;gt; K = 1.91.&lt;lb/&gt;Convert Mercury into a Dyson sphere (7 kg/m^2 density) @ 26% PV efficiency -&amp;gt; K = 1.9998.&lt;/p&gt;
    &lt;p&gt;No way to get to K2 without a slightly more efficient solar panel!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46831736</guid><pubDate>Sat, 31 Jan 2026 00:06:06 +0000</pubDate></item><item><title>Show HN: I trained a 9M speech model to fix my Mandarin tones</title><link>https://simedw.com/2026/01/31/ear-pronunication-via-ctc/</link><description>&lt;doc fingerprint="2d64271217290110"&gt;
  &lt;main&gt;
    &lt;p&gt;TL;DR: Mandarin pronunciation has been hard for me, so I took ~300 hours of transcribed speech and trained a small CTC model to grade my pronunciation. You can try it here.&lt;/p&gt;
    &lt;p&gt;In my previous post about Langseed, I introduced a platform for defining words using only vocabulary I had already mastered. My vocabulary has grown since then, but unfortunately, people still struggle to understand what I'm saying.&lt;/p&gt;
    &lt;p&gt;Part of the problem is tones. They're fairly foreign to me, and I'm bad at hearing my own mistakes, which is deeply frustrating when you don‚Äôt have a teacher.&lt;/p&gt;
    &lt;head rend="h2"&gt;First attempt: pitch visualisation&lt;/head&gt;
    &lt;p&gt;My initial plan was to build a pitch visualiser: split incoming audio into small chunks, run an FFT, extract the dominant pitch over time, and map it using an energy-based heuristic, loosely inspired by Praat.&lt;/p&gt;
    &lt;p&gt;But this approach quickly became brittle. There were endless special cases: background noise, coarticulation, speaker variation, voicing transitions, and so on.&lt;/p&gt;
    &lt;p&gt;And if there‚Äôs one thing we‚Äôve learned over the last decade, it‚Äôs the bitter lesson: when you have enough data and compute, learned representations usually beat carefully hand-tuned systems.&lt;/p&gt;
    &lt;p&gt;So instead, I decided to build a deep learning‚Äìbased Computer-Assisted Pronunciation Training (CAPT) system that could run entirely on-device. There are already commercial APIs that do this, but hey, where‚Äôs the fun in that?&lt;/p&gt;
    &lt;head rend="h2"&gt;Architecture&lt;/head&gt;
    &lt;p&gt;I treated this as a specialised Automatic Speech Recognition (ASR) task. Instead of just transcribing text, the model needs to be pedantic about how something was said.&lt;/p&gt;
    &lt;p&gt;I settled on a Conformer encoder trained with CTC (Connectionist Temporal Classification) loss.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Conformer?&lt;/head&gt;
    &lt;p&gt;Speech is weird: you need to catch both local and global patterns:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Local interactions&lt;/p&gt;&lt;lb/&gt;The difference between a retroflex zh and an alveolar z happens in a split second. CNNs are excellent at capturing these short-range spectral features.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Global interactions&lt;/p&gt;&lt;lb/&gt;Mandarin tones are relative (a "high" pitch for me might be low for a child) and context-dependent (tone sandhi)1. Transformers excel at modeling this longer-range context.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Conformers combine both: convolution for local detail, attention for global structure.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why CTC?&lt;/head&gt;
    &lt;p&gt;Most modern ASR models (e.g. Whisper) are sequence-to-sequence: they turn audio into the most likely text. The downside is they'll happily auto-correct you.&lt;/p&gt;
    &lt;p&gt;That‚Äôs a feature for transcription, but it‚Äôs a bug for language learning. If my tone is wrong, I don‚Äôt want the model to guess what I meant. I want it to tell me what I actually said.&lt;/p&gt;
    &lt;p&gt;CTC works differently. It outputs a probability distribution for every frame of audio (roughly every 40 ms). To handle alignment, it introduces a special &lt;code&gt;&amp;lt;blank&amp;gt;&lt;/code&gt; token.&lt;/p&gt;
    &lt;p&gt;If the audio is "hello", the raw output might look like:&lt;/p&gt;
    &lt;code&gt;h h h &amp;lt;blank&amp;gt; e e &amp;lt;blank&amp;gt; l l l l &amp;lt;blank&amp;gt; l l o o o
&lt;/code&gt;
    &lt;p&gt;Collapsing repeats and removing blanks gives &lt;code&gt;hello&lt;/code&gt;. This forces the model has to deal with what I actually said, frame by frame.&lt;/p&gt;
    &lt;head rend="h2"&gt;Forced alignment: knowing when you said it&lt;/head&gt;
    &lt;p&gt;CTC tells us what was said, but not exactly when.&lt;/p&gt;
    &lt;p&gt;For a 3-second clip, the model might output a matrix with ~150 time steps (columns), each containing probabilities over all tokens (rows). Most of that matrix is just &lt;code&gt;&amp;lt;blank&amp;gt;&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If the user reads "N«ê h«éo" (ni3, hao3), we expect two regions of high probability: one for &lt;code&gt;ni3&lt;/code&gt;, one for &lt;code&gt;hao3&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We need to find a single, optimal path through this matrix that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Starts at the beginning&lt;/item&gt;
      &lt;item&gt;Ends at the end&lt;/item&gt;
      &lt;item&gt;Passes through &lt;code&gt;ni3&lt;/code&gt;‚Üí&lt;code&gt;hao3&lt;/code&gt;in order&lt;/item&gt;
      &lt;item&gt;Maximises total probability&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is exactly what the Viterbi algorithm computes, using dynamic programming.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tokenisation: Pinyin + tone as first-class tokens&lt;/head&gt;
    &lt;p&gt;Most Mandarin ASR systems output Hanzi. That hides pronunciation errors, because the writing system encodes meaning rather than pronunciation.&lt;/p&gt;
    &lt;p&gt;Instead, I created a token for every Pinyin syllable + tone:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;zhong1&lt;/code&gt;is one token&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;zhong4&lt;/code&gt;is a completely different token&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If I say the wrong tone, the model explicitly predicts the wrong token ID.&lt;/p&gt;
    &lt;p&gt;I also normalised the neutral tone by forcing it to be tone 5 (&lt;code&gt;ma5&lt;/code&gt;). This resulted in a vocabulary of 1,254 tokens, plus &lt;code&gt;&amp;lt;unk&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;blank&amp;gt;&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Training&lt;/head&gt;
    &lt;p&gt;I combined the AISHELL-1 and Primewords datasets (~300 hours total), augmented by SpecAugment (time/frequency masking). On 4√ó NVIDIA GeForce RTX 4090s, training took about 8 hours. Instead of obsessing over loss, I mostly focused on these metrics:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;TER (Token Error Rate): overall accuracy.&lt;/item&gt;
      &lt;item&gt;Tone Accuracy: accuracy over tones 1-5.&lt;/item&gt;
      &lt;item&gt;Confusion Groups: errors between difficult initial pairs like zh/ch/sh vs z/c/s.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Honey, I shrank the model&lt;/head&gt;
    &lt;p&gt;I started with a "medium" model (~75M parameters). It worked well, but I wanted something that could run in a browser or on a phone without killing the battery.&lt;/p&gt;
    &lt;p&gt;So I kept shrinking it, and I was honestly surprised by how little accuracy I lost:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;# Parameters&lt;/cell&gt;
        &lt;cell role="head"&gt;TER&lt;/cell&gt;
        &lt;cell role="head"&gt;Tone accuracy&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;75M&lt;/cell&gt;
        &lt;cell&gt;4.83%&lt;/cell&gt;
        &lt;cell&gt;98.47%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;35M&lt;/cell&gt;
        &lt;cell&gt;5.16%&lt;/cell&gt;
        &lt;cell&gt;98.36%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;9M&lt;/cell&gt;
        &lt;cell&gt;5.27%&lt;/cell&gt;
        &lt;cell&gt;98.29%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The 9M-parameter model was barely worse. This strongly suggests the task is data-bound, not compute-bound.&lt;/p&gt;
    &lt;p&gt;The FP32 model was ~37 MB. After INT8 quantisation, it shrank to ~11 MB with a negligible accuracy drop (+0.0003 TER). Small enough to load instantly via &lt;code&gt;onnxruntime-web&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Alignment bug: silence ruins everything&lt;/head&gt;
    &lt;p&gt;To highlight mistakes, we need forced alignment. But I hit a nasty bug with leading silence.&lt;/p&gt;
    &lt;p&gt;I recorded myself saying "ÊàëÂñúÊ¨¢‚Ä¶" and paused for a second before speaking. The model confidently told me my first syllable was wrong. Confidence score: 0.0.&lt;/p&gt;
    &lt;p&gt;Why?&lt;/p&gt;
    &lt;p&gt;The alignment assigned the silent frames to &lt;code&gt;wo3&lt;/code&gt;. When I averaged probabilities over that span, the overwhelming &lt;code&gt;&amp;lt;blank&amp;gt;&lt;/code&gt; probability completely drowned out &lt;code&gt;wo3&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;The fix&lt;/head&gt;
    &lt;p&gt;I decoupled UI spans (what gets highlighted) from scoring frames (what contributes to confidence).&lt;/p&gt;
    &lt;p&gt;We simply ignore frames where the model is confident it‚Äôs seeing silence:&lt;/p&gt;
    &lt;code&gt;def _filter_nonblank_frames(span_logp: torch.Tensor, blank_id: int = 0, thr: float = 0.7):
    """
    Only keep frames where the probability of &amp;lt;blank&amp;gt; is below a threshold.
    If we filter everything (total silence), we fall back to scoring the whole span.
    """
    p_blank = span_logp[:, blank_id].exp()
    keep = p_blank &amp;lt; thr
    if keep.any():
        return span_logp[keep]
    return span_logp  # Fallback
&lt;/code&gt;
    &lt;p&gt;This single change moved my confidence score for the first syllable from 0.0 ‚Üí 0.99.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;I can already feel my pronunciation improving while beta testing this. It‚Äôs strict and unforgiving, exactly what I needed.&lt;/p&gt;
    &lt;p&gt;Native speakers, interestingly, complained that they had to over-enunciate to get marked correct. That‚Äôs likely a domain-shift issue: AISHELL is mostly read speech, while casual speech is faster and more slurred. Kids do poorly too: their pitch is higher, and they're basically absent from the training data. Adding conversational datasets like Common Voice feels like the obvious next step.&lt;/p&gt;
    &lt;p&gt;You can try the live demo here. It runs entirely in your browser. The download is ~13MB, still smaller than most websites today.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;For example, Tone 3 followed by Tone 3 is pronounced as Tone 2 followed by Tone 3 (‰Ω†Â•Ω ‚Üí n√≠ h«éo). ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46832074</guid><pubDate>Sat, 31 Jan 2026 00:51:27 +0000</pubDate></item><item><title>Coding is when we're least productive</title><link>https://codemanship.wordpress.com/2026/01/30/coding-is-when-were-least-productive/</link><description>&lt;doc fingerprint="2ac9c1558b395518"&gt;
  &lt;main&gt;
    &lt;p&gt;One old dragon that‚Äôs reared its head again in this ‚Äúage of AI‚Äù is the very wrongheaded notion that productivity == code. Managers aim to maximise the amount of code their dev teams produce, and so they maximise the time devs spend writing code.&lt;/p&gt;
    &lt;p&gt;Let me tell you a story about the value of code.&lt;/p&gt;
    &lt;p&gt;When I first started contracting, I worked on a Point Of Sale system upgrade for a major retailer. I was stuck on a feature, and just couldn‚Äôt wrap my head around the use case because I‚Äôd never actually seen their existing system in operation.&lt;/p&gt;
    &lt;p&gt;So I walked to a local branch on the high street, showed them my security pass, and asked if I could observe and ‚Äì when the cashier wasn‚Äôt busy ‚Äì ask questions.&lt;/p&gt;
    &lt;p&gt;They told me they could do better than that. Upstairs they had a room with a working till in it, which they used to train new staff. They also used it to test system updates, which was the first time I‚Äôd seen what we now call a ‚Äúmodel office‚Äù for software testing.&lt;/p&gt;
    &lt;p&gt;We were able to run through the use case scenarios I was stuck on, with them showing me how to use the POS system to achieve specific goals. The mist cleared. It was like I‚Äôd been reading a book on how to ride a bicycle that had no pictures, and then someone gave me a bicycle.&lt;/p&gt;
    &lt;p&gt;Enlightened, I walked back to the office and changed about three lines of code. That was all the coding I did that day. Three lines, in 8 hours.&lt;/p&gt;
    &lt;p&gt;But if I hadn‚Äôt made that trip and seen for myself, and had a chance to talk to a department manager in that store, those three lines would have been applying special offers wrong. (If only the person who wrote the spec had done this in the first place‚Ä¶)&lt;/p&gt;
    &lt;p&gt;Now multiply that error by 250 branches nationwide. I potentially saved my client a bunch of money and embarrassment with that 3-line change.&lt;/p&gt;
    &lt;p&gt;Now, I consider that a productive day.&lt;/p&gt;
    &lt;p&gt;But had I been measured on my contribution by lines of code, or commits, or features finished, it would have been seen as a very unproductive day by my manager.&lt;/p&gt;
    &lt;p&gt;I may have felt pressured to stay at my workstation, bashing out more code, compounding the mistake and costing my client more money.&lt;/p&gt;
    &lt;p&gt;A very teachable moment for me early in my career. The lightbulb pinged: some code is worth more than others, and coding and productivity aren‚Äôt the same thing.&lt;/p&gt;
    &lt;p&gt;We could even argue that coding is the interruption. What‚Äôs the shortcut in IntelliJ that tells you if you‚Äôre writing the wrong code? Oh, that‚Äôs right. There isn‚Äôt one.&lt;/p&gt;
    &lt;p&gt;When I‚Äôm heads-down-coding, I‚Äôm not seeing, I‚Äôm not asking, and I‚Äôm not learning about the problem. To do that, I have to get up from my desk, go to where the problem is and/or the people I need to ask are, and have a conversation. Let the dog see the rabbit.&lt;/p&gt;
    &lt;p&gt;This takes time. And if we‚Äôre producing code faster than we can validate it ‚Äì either by exploring the problem ourselves, or learning from user feedback if our release cycles are fast enough ‚Äì then we‚Äôre piling assumptions on top of assumptions.&lt;/p&gt;
    &lt;p&gt;I‚Äôve seen so many times how 10 lines of code can end up being worth ¬£millions, and 10,000 ends up being worthless.&lt;/p&gt;
    &lt;p&gt;If productivity, in reality, is a measure of how much net value we create, then that learning feedback loop is where the real productivity happens, and not at our desks punching keys. Coding is when we‚Äôre least productive.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46832625</guid><pubDate>Sat, 31 Jan 2026 02:08:26 +0000</pubDate></item><item><title>Naples' 1790s civil war was intensified by moral panic over Real Analysis (2023)</title><link>https://lareviewofbooks.org/article/foundational-anxieties-modern-mathematics-and-the-political-imagination/</link><description>&lt;doc fingerprint="af48931038ab4553"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Foundational Anxieties, Modern Mathematics, and the Political Imagination&lt;/head&gt;
    &lt;head rend="h2"&gt;Massimo Mazzotti uses a forgotten episode in revolutionary Naples to demonstrate the entanglement of mathematics and politics.&lt;/head&gt;
    &lt;head rend="h3"&gt;By Massimo MazzottiJune 2, 2023&lt;/head&gt;
    &lt;head rend="h4"&gt;Did you know LARB is a reader-supported nonprofit?&lt;/head&gt;
    &lt;p&gt;LARB publishes daily without a paywall as part of our mission to make rigorous, incisive, and engaging writing on every aspect of literature, culture, and the arts freely accessible to the public. Help us continue this work with your tax-deductible donation today!&lt;/p&gt;
    &lt;p&gt;This essay is adapted from Massimo Mazzotti‚Äôs 2023 book Reactionary Mathematics: A Genealogy of Purity, available now from the University of Chicago Press.&lt;/p&gt;
    &lt;p&gt;¬§&lt;/p&gt;
    &lt;p&gt;A FORGOTTEN EPISODE in French-occupied Naples in the years around 1800‚Äîjust after the French Revolution‚Äîillustrates why it makes sense to see mathematics and politics as entangled. The protagonists of this story were gravely concerned about how mainstream mathematical methods were transforming their world‚Äîsomewhat akin to our current-day concerns about how digital algorithms are transforming ours. But a key difference was their straightforward moral and political reading of those mathematical methods. By contrast, in our own era we seem to think that mathematics offers entirely neutral tools for ordering and reordering the world‚Äîwe have, in other words, forgotten something that was obvious to them.&lt;/p&gt;
    &lt;p&gt;In this essay, I‚Äôll use the case of revolutionary Naples to argue that the rise of a new and allegedly neutral mathematics‚Äîcharacterized by rigor and voluntary restriction‚Äîwas a mathematical response to pressing political problems. Specifically, it was a response to the question of how to stabilize social order after the turbulence of the French Revolution. Mathematics, I argue, provided the logical infrastructure for the return to order. This episode, then, shows how and why mathematical concepts and methods are anything but timeless or neutral; they define what ‚Äúreason‚Äù is, and what it is not, and thus the concrete possibilities of political action. The technical and political are two sides of the same coin‚Äîand changes in notions like mathematical rigor, provability, and necessity simultaneously constitute changes in our political imagination.&lt;/p&gt;
    &lt;p&gt;¬§&lt;/p&gt;
    &lt;p&gt;In 1806, the Kingdom of Naples was occupied by a French army and integrated into Napoleon‚Äôs imperial system. The French and their local supporters had a clear agenda: they wanted to transform the semifeudal society into a centralized administrative monarchy with a liberal economy. This ambitious plan, however, soon ran up against obdurate realities like muddy roads, brigandage, popular insurgencies, and the thinly disguised hostility of powerful local elites. There was another problem, too. Open a Neapolitan university textbook of the time and you will see that the French had to fight their battles in a land where their mathematics was wrong.&lt;/p&gt;
    &lt;p&gt;While armed and cultural resistance against the French invaders‚Äô imperial ambitions happened in other parts of Europe, the Neapolitan case is particularly interesting because it includes a mathematical resistance. This resistance took the form of a distinctive mathematical culture that was hegemonic in that kingdom for several decades‚Äîfrom the late 1790s to the 1830s. Contemporaries called it the Neapolitan synthetic school. The name referred to synthetic (or pure) geometry, a geometry that does not use coordinates and algebraic formulas to study figures and solve problems. Leading Neapolitan mathematicians embraced it as the veritable foundation of all mathematics. Only its methods and assumptions, they believed, could be trusted.&lt;/p&gt;
    &lt;p&gt;What the Neapolitans most adamantly did not trust was what they called, not without irony, the ‚Äúvery modern mathematics.‚Äù This body of knowledge, associated mainly with France, was characterized by the rapid advancements of an algebraized form of infinitesimal calculus and by its stunning and far-reaching practical applications. It had severed its connections with Euclidean geometry, and was referred to as ‚Äúanalysis‚Äù‚Äîa term that, in this context, meant a vast array of algebraic methods and algorithmic procedures that could be used to represent how things change, whether those things were, say, the trajectory of a cannonball or agricultural productivity.&lt;/p&gt;
    &lt;p&gt;Driven by eminently practical goals, analysis had become highly abstract: a versatile tool that could be applied to describe and control natural and social phenomena. The Neapolitans were quite sure it was morally suspect‚Äîa degenerate form of knowledge, and dangerous for the stability of society. Its proliferation across Europe and globally was, to them, an unmitigated disaster. While these allegations sound extravagant to our ears, some of their concerns resonate with ones in this century‚Äîe.g., about how even programmers who fashion certain complex algorithms do not understand their inner workings or why they come to the conclusions they do. Synthetics, for instance, pointed out that the analysts prioritized practical success over understanding: they aimed to model phenomena using algebraic tools that they could not fully justify, neither through some form of intellectual intuition nor through logic. By contrast, synthetic geometers clarified and grounded every single step of their procedures. They often used metaphors of sight to make this point: synthetic geometry allowed practitioners to see with clarity, and this is why their results could be trusted; analysts were blind when they manipulated their formulas. Using another set of metaphors: Analysts followed the fast flights of their feverish and uncontrolled imagination, while synthetics kept their feet on the ground. Their procedures were slow but safe.&lt;/p&gt;
    &lt;p&gt;¬§&lt;/p&gt;
    &lt;p&gt;The mathematical 19th century was suffused with a distinctive foundational anxiety. We can see an early and radical manifestation of this anxiety in revolutionary Naples‚Äîin its bizarre and apparently backward attempt to return to a Greek-like pure geometry. The champion of this new old mathematics was Nicola Fergola (1753‚Äì1824), the charismatic and mystically inclined leader of a group of mathematicians and scientists who understood themselves as the last heirs of an ancient tradition, a tradition that was now under attack and needed to be defended.&lt;/p&gt;
    &lt;p&gt;Skeptics, however, understood the synthetic school‚Äôs mathematical resistance as backwardness, the rearguard action of a group of isolated practitioners. And yet, Fergola‚Äôs puzzling quest for purity was something more. For one thing, there was no established tradition of synthetic geometry worth defending in Naples. The tradition Fergola invoked was largely an invention‚Äîan imaginary mathematical lineage that ran through ancient Greece, late antiquity, and Christian Europe, all the way down to these self-proclaimed final paladins. In fact, Fergola, who was well aware of recent mathematical developments, breathed new life into forgotten mathematical techniques. Neapolitan synthetic mathematics, in other words, was not a remnant of the past, but a new way of understanding mathematics, characterized by new canons of rigor and founded on a core of ‚Äúpure‚Äù mathematics. In the name of a mythical tradition, it imposed a new discipline on its practitioners by emphasizing self-restraint as the key epistemic virtue. Fergola, his admirers reported, was a champion of self-control: he controlled his body, passions, and imagination, and knew well when to stop trusting his own mathematical techniques.&lt;/p&gt;
    &lt;p&gt;¬§&lt;/p&gt;
    &lt;p&gt;The Neapolitans did not reject modern analysis simply because they considered it French. What triggered their anxiety were its technical features‚Äîthe way it worked. Following the example of mathematicians like Condorcet, analysts were aiming to create a repertoire of finite and infinite algebraic methods that were abstract and general enough to apply to any kind of problem, be it in geometry, physics, economics, or even politics. This zealous quest for universal problem-solving algorithms is precisely what made the synthetics uneasy. Interestingly, the analysts themselves knew well that the manipulation of these algorithms‚Äîthe way they produced results‚Äîwas not grounded in either geometric intuition or logical arguments. What warranted their use, they believed, was that algebraic procedures mirrored the fundamental workings of the human mind. If, as the analysts believed, the mind was an analytic machine, then analysis was the quintessential expression of human reason‚Äîand, as such, isomorphic to nature: analysis was effective because it mirrored the deep structures of reality. This was a mathematics essentially interwoven with the world of experience‚Äîone that, as d‚ÄôAlembert had written, ‚Äúgives us the most perfect examples of the manner in which one should use the art of reasoning.‚Äù&lt;/p&gt;
    &lt;p&gt;When asked to solve a geometric problem, the analysts would find an appropriate system of coordinates that would allow them to turn figures into algebraic formulas, then would manipulate these formulas to obtain the ‚Äúsolving equation,‚Äù as they called it. They would interpret the solutions as solutions to the original geometric problem. Their operations had thus shifted from geometry to algebra. Was this a legitimate move? The synthetics would say that it was legitimate only when they could see the geometry behind the formulas. But for complex problems this was not always possible, and in these instances algebra was blind; there was no way to reconstruct the geometrical meaning of the algebraic operations that led to the solution. It followed that the nature of the problem had changed. For the analysts, this was irrelevant: algebra captured the essential relations expressed by the terms of the problem, which then served to guide the mathematician toward the solution. For the synthetics, by contrast, a solution to the original geometrical problem could only be geometrical in nature; and so, what the analysts were offering were not solutions but meaningless numbers.&lt;/p&gt;
    &lt;p&gt;While the analysts strove for maximum generality, the synthetics argued for the specificity and locality of all mathematical methods. They saw this as a question of jurisdiction: there are many different ways of reasoning and many different methods, and they all have their legitimate function and scope. It would be illusory‚Äîand deceitful‚Äîto try to solve a geometrical problem using purely algebraic methods, or a political problem using the methods of geometry. Even more misleading would be to believe that there is a single universal method that can be applied to all kinds of problems. The synthetics‚Äô world was, so to speak, epistemologically stratified. They recognized many kinds of truth, and thought it essential to keep them separated from one another. The truth of the geometer, they claimed, has nothing to do with the truths of the theologian, historian, or politician.&lt;/p&gt;
    &lt;p&gt;These two mathematical cultures differed sharply in the way they conceived mathematical reasoning. For the synthetics, mathematical knowledge was the product of a process of recognition, the imperfect representation of metaphysical states of affairs that the gifted mathematician would be able to glimpse. Their teaching reflected this view: a close-knit school with an inner circle of students who worked with their maestro, engaging in an endless reflection on geometrical problems received from antiquity. For the analysts, mathematical reasoning was just a particular case of analytic reasoning‚Äîcalculus, especially, was where analytic reason could be best seen in action. They saw themselves as the standard bearers of modernization and the promoters of rational action across both scientific and social life. For them, mathematical training was a matter of learning to frame problems analytically and then solving them following a set of standard procedures. It was not a matter of intellectual intuition, gift, and genius. On the contrary, anyone, with proper training, could become an effective problem-solver.&lt;/p&gt;
    &lt;p&gt;Analysts enthusiastically compared their method to the clunky workings of a machine: to them, the machine was an emblem of rational thinking. It was also a way of arguing for the algorithmic nature and therefore accessibility of the method, as its standardized procedures could be easily learned, and deployed across different contexts. At the core of their science was not the ‚Äúpure and simple knowledge of truths,‚Äù they argued, but the knowledge of methods and their relative ‚Äústrength‚Äù in getting useful results, including approximate ones, through the sheer power of calculation. The synthetics countered that results needed always to be precise and perfectly interpretable. The analysts were teaching their students blind methods that deformed their young minds; they were turning them into automata‚Äîsoulless, machinelike number-crunchers‚Äîwho ignored at their peril the meaning of the formulas they were manipulating. Well before the dawn of digital computing, the questions of the meaning of formal procedures and of the social implications of their extensive use were at the core of a debate that, using the language of morals, addressed basic questions of social order.&lt;/p&gt;
    &lt;p&gt;¬§&lt;/p&gt;
    &lt;p&gt;Why did some mathematicians perceive logical gaps in analysis in the years around 1800, and why did they consider them unbearable? It turns out that anxieties about the foundations of analysis were growing even among its supporters. By the 1820s, these anxieties were spreading across Europe. Consider Augustin-Louis Cauchy (1789‚Äì1857), who played a key role in the creation of modern mathematics as we know it. His much-celebrated revolution had less to do with his specific contributions and more with his overall transformation of mathematics as a discipline.&lt;/p&gt;
    &lt;p&gt;He brought order to the world of mathematics. He modernized it by imposing rigor. Mathematicians in the 18th century had achieved stunning results in algebra and infinitesimal calculus, but to Cauchy‚Äôs eyes, they had been too casual in how they defined their concepts and devised and applied their methods. This blithe attitude needed to end, he declared, and a new Euclidean spirit‚Äîa spirit of rigor‚Äîneeded to replace it. Cauchy was not interested in bringing back synthetic geometry. Rather, he aimed to reinterpret analysis within a new logical framework in which every concept and procedure would be logically justified. Cauchy‚Äôs program of rigorization redefined the meaning of mathematical techniques, providing precise definitions and limits for the application of each method. He set boundaries, in other words, within which certain techniques could be legitimately deployed. The modern mathematicians were those who, following Cauchy, could discipline themselves through a new kind of technical precision.&lt;/p&gt;
    &lt;p&gt;Cauchy‚Äôs rigorous analysis seems distant from Fergola‚Äôs Greek-like geometry. But it was shaped by the same foundational anxiety and urgency to restore order to a mathematical world that‚Äîin the views of both men‚Äîhad gone badly astray. What we learn from the comparison is that the fundamental opposition was not between geometry and algebra but between mathematics as a pure, rigorous, self-contained, and reliable body of knowledge and mathematics as a set of highly general and universally applicable algorithmic procedures expressing an all-encompassing analytic rationality. The fact that Fergola tried literally to reinstate geometry at the core of mathematics while Cauchy injected a rigorous Euclidean spirit within analysis was more about their local conditions than anything else. What really mattered is that both programs promised a return to mathematical order.&lt;/p&gt;
    &lt;p&gt;¬§&lt;/p&gt;
    &lt;p&gt;Fergola and his students were initially marginal to Neapolitan scientific life. Their geometrical program was perceived as outdated, while the world of the salons scoffed at their baroque religious devotion and ascetic lifestyle. But this changed dramatically after the storming of the Bastille, when they quickly acquired an unprecedented cultural relevance. The Neapolitan government would take a decisive anti-French stance, reorient its entire cultural politics accordingly, and align itself with the Catholic Church. Within the Church itself, enlightened reformism gave way to a Jesuitic religiosity that reactivated baroque devotions and mobilized popular piety to support the alliance of throne and altar.&lt;/p&gt;
    &lt;p&gt;In 1794, the discovery of a Jacobin conspiracy to overthrow the monarchy sent the court into a state of panic. The Jacobins would succeed five years later, in 1799, when Naples became a republic. The leading revolutionaries were mathematicians. The chief conspirator of 1794, and the first president of the republic, Carlo Lauberg (1752‚Äì1834), was a teacher of chemistry and mathematics. It is no accident that almost every noteworthy figure in Neapolitan Jacobinism received some mathematical training: a basic understanding of analysis was an essential part of their worldview, as were republicanism, egalitarianism, and anticlericalism. The very structure of their secret society‚Äîa network of Jacobin clubs‚Äîwas a working model of how analysis could be deployed in matters of social organization.&lt;/p&gt;
    &lt;p&gt;Neapolitan Jacobins aimed to find universal methods to address pressing social and political problems‚Äîabove all, the problems of political representation and wealth redistribution. By the mid-1790s, they had become convinced that their vision of a just and equal society could be realized only through the universal implementation of analysis, which they understood as a revolutionary mathematics. Analysis was already being applied to the natural sciences and now, they said, it was time to apply it to the science of society as well, and to political matters. The analytic method would turn the art of politics into a science, replacing tradition, prejudice, and private interests with rational decision-making. To apply analysis to politics meant to reduce it to its elementary components, study their relations, and use algebraic procedures to intervene. This would detach politics from its metaphysical assumptions, turning it into a matter of rational and transparent administration. The analytic revolution could now be expected to transform society by making it possible to operationalize ‚Äúthe will of the people.‚Äù&lt;/p&gt;
    &lt;p&gt;Neapolitan Jacobins thus took the tools and basic assumptions of analysis and turned them into a militant mathematics‚Äîthe veritable ‚Äúbackbone of society.‚Äù A programmatically impure mathematics, it was a universal language and reasoning style that could be applied across disciplinary boundaries to bring about immediate social change. In fact, mathematics and politics merged seamlessly in the lived experience of these Jacobins, who saw themselves as agents of change, able to escape the logic of reform and the apparent fatalities of history.&lt;/p&gt;
    &lt;p&gt;The counterrevolutionaries reacted by turning these analytic features into the ‚ÄúJacobin machine,‚Äù a deadly device for the control of public opinion, political life, and the state. The metaphor emphasized discipline, organization, and the capacity for control, but also gestured toward the extraneous and polluting character of what was described as a set of manipulation techniques. In Naples, the Jacobin machine was viewed as foreign, disconnected from local political traditions. But in France too, its effect was seen as one of contamination, this time from the inside. In both cases, the purity of the body politic had to be defended from a malignant mechanical-analytic threat.&lt;/p&gt;
    &lt;p&gt;The breathtaking adventure of the Jacobin Republic, characterized by sweeping plans for popular education and redistribution of wealth, ended abruptly five months after it started, in June 1799, when British, Russian, and Turkish forces joined a local counterrevolutionary army and stormed the walls of Naples. In its aftermath, about 120 prominent Jacobins were put on show trials and executed. Fergola had moved to the countryside during these months. A biographer reported that he could not stand ‚Äúthe noise‚Äù of the city. When he returned, he and his students were asked to reorganize scientific life in the university and in the entire system of public education. Education, which had been ‚Äúinfected‚Äù by the Jacobins, a royal dispatch read, now needed to be brought back to its ancient order.&lt;/p&gt;
    &lt;p&gt;Fergola, a celibate vegetarian who found the presence of women extremely unpleasant, was a tormented man. He was not someone who could easily fit into the salons of Enlightenment Naples. His religiosity was deliberately untimely and baroque. He chose the most anti-modern and anti-rationalist religiosity, the popular piety of the Neapolitan crowds, when this religiosity was under attack. A spate of crying and bleeding Madonnas signaled the crisis of a subaltern agrarian world that would soon explode in massive counterrevolutionary insurgencies. Embracing popular religion in the 1790s meant embracing it as resistance. In general terms, it was a resistance against the modern state‚Äôs secularized and rational principles of organization. In this sense, Fergola‚Äôs public display of his rosary and bloodied scourge was a politicized act of resistance. And in this sense, his mathematics, too, was a politicized act of resistance.&lt;/p&gt;
    &lt;p&gt;Fergola suffered numerous and often inexplicable ‚Äúorganic‚Äù and ‚Äúmoral‚Äù ailments that progressively hampered his activity. Yet, we are told, he gazed serenely at his sore body as if that flesh was not his own: his entire life could be recounted as a triumph of spirit over matter. Exhausted by mysterious convulsions and prostrated by horrific demonic visions, Fergola felt that his faith was constantly put to the test. But even as his own health failed, his school prospered. To the end, he railed about the degeneration of learning and the ‚Äúsacrilegious horde,‚Äù which included Freemasons, Jacobins, liberals, and even some of his legitimist colleagues.&lt;/p&gt;
    &lt;p&gt;¬§&lt;/p&gt;
    &lt;p&gt;With the French occupation of 1806, the synthetic school had to face, once again, the threat of analysis. The world, however, had changed. Most surviving Jacobins had come to terms with the Napoleonic normalization. The synthetics controlled the university, but outside of it, analysis thrived, constituting the core training in the new schools of engineering fostered by the French. The continuity, however, was mostly apparent. Many former revolutionaries, in France as in Naples, had turned the question of modernization into a technical problem, and had refashioned their personas and social function in terms of scientific neutrality and technocratic efficiency. Historian Ken Alder has aptly labeled them ‚Äútechno-Jacobins.‚Äù In this normalized context, mathematics was a neutral tool, the distinctive expertise of technical elites who served the state. The direct connection between mathematics, egalitarianism, and republicanism, built through the notion of a universal analytic reason, had been severed, and with it vanished the very possibility of a revolutionary mathematics. But reframing modernization as a technical rather than a political problem meant detaching analysis‚Äôs formal tools from their original source of legitimation. Intriguingly, the political choice of reframing modernization in exclusively technical terms had produced a profound and pervasive mathematical problem.&lt;/p&gt;
    &lt;p&gt;Led by Fergola‚Äôs students, the synthetic school fought against the technical elites of the modern state, mostly civil engineers and statisticians, for scientific hegemony. The old regime had long been imploding in Naples, but a new order struggled to consolidate itself. The French had arrived in Naples with a promise of order through modernization, and had found receptive interlocutors in the landed elites who could most benefit from the abolition of the feudal-communal system. The new technical experts had been charged with changing the kingdom‚Äôs physical and social landscape accordingly. Technical disciplines such as statistics or topography became key sites for negotiation, collaboration, and conflict between landed elites and the central government. On this technical terrain, the new experts would continuously clash with the synthetics.&lt;/p&gt;
    &lt;p&gt;It is not a coincidence that the synthetic school faded into oblivion when the reactionary position lost ground as a viable political option. From the 1820s onward, what was really at stake was the form of the new relations between the centralized administrative state, the landed elites, and the largely dispossessed peasant masses. Analysis had morphed into a set of allegedly neutral administrative tools, and the controversy between analytics and synthetics, which had long defined Neapolitan academic life, became increasingly meaningless. The technicians who supported the state‚Äôs modernizing action now argued for a mathematical reconciliation. What the two groups were defending, it was now believed, were simply two different ways of looking at mathematics, which should not be seen as opposed to each other but rather as complementary. The synthetics approach was useful for didactic purposes, while the analytic one was best suited for research and the discovery of new mathematical truths. This compromise was an elegant way of disposing of what, at that point, was an embarrassing anomaly for Neapolitan science. This normalized reconstruction eliminated revolutionary and reactionary scientific aberrations, emphasized continuity in the history of mathematics, and aligned with the political life of Restoration-age Naples, which was hegemonized by new landed elites and their liberal and constitutional ambitions. Emblematic of this cultural climate was the success of philosophical positions grounded on consciousness, which insisted that, within certain limits, individual reason was autonomous and legislative, and that fighting for liberty of conscience coincided with fighting for political and economic liberty.&lt;/p&gt;
    &lt;p&gt;The mathematical controversy between synthetics and analytics had been a controversy about the nature of reason all along. At stake was reason‚Äôs nature and limits. The Jacobin‚Äôs analytic reason was universal, active, calculative, individual, a priori, and ahistorical; it was a completely autonomous reason that, when not obstructed, could truthfully describe and legitimately change the world‚Äîthrough revolutionary action, if necessary. The reason of the synthetic, by contrast, was local, passive, intuitive, collective, a posteriori, and eminently suited to historical thinking; it was a dependent reason, whose outcomes needed to be warranted by external sources of legitimation like tradition, custom, experience, religion, and metaphysical principles. It was, as such, a reactionary reason that envisioned the return to order as a return to hierarchy‚Äîorder produced by subordination. This reactionary reason was a militant reason, its arguments forged in battle.&lt;/p&gt;
    &lt;p&gt;It is only by contrast to an abhorred revolutionary reason, political theorist Corey Robin reminds us, that the invocation of ancient forms of wisdom can captivate the modern mind. The image of reason that emerged with the consolidation of the modern liberal state valued individual reason while acknowledging its clear limits. The principles of a liberal economy and the new relationships of subordination between landed elites and peasant masses were not to be questioned. The autonomy of individual reason was celebrated against prerevolutionary obscurantism and absolutism, but it was only a relative autonomy, to be exercised within the boundaries of postfeudal order. The newly rigorous mathematics and the constitutional project set those boundaries with precision. If absolutism was a thing of the past, so should be revolutionary anarchy.&lt;/p&gt;
    &lt;p&gt;¬§&lt;/p&gt;
    &lt;p&gt;Logical inference, Wittgenstein quipped, is how we refer to what we do not intend to question. The case of the Neapolitan mathematical resistance should not be seen as one in which mathematics was temporarily distorted by politics. When we craft logico-mathematical concepts and techniques, we design ways of ordering the natural and social world. These ways of ordering the world open up certain possibilities for action‚Äîincluding political action‚Äîwhile closing down others. They discriminate between what is visible, plausible, and logical, and what is none of those things. Jacobin mathematics was deployed to critique and radically transform the existing social order, empowering traditionally subordinate social groups and bringing them into the space of politics as legitimate autonomous agents. The mathematics of the synthetics was designed to deny this possibility, to turn it, in fact, into a logical impossibility‚Äîhence it was, strictly speaking, a reactionary mathematics.&lt;/p&gt;
    &lt;p&gt;Modern mathematics, as it took shape with Cauchy and those who continued his program, was constitutive of the postrevolutionary political normalization. It was the logico-mathematical infrastructure of the new moderate liberal discourse. It retained analysis‚Äôs operative orientation while embracing the synthetics‚Äô quest for a foundational core of mathematical knowledge. The image of reason it embodied was bounded and self-disciplined, and while it legitimated a neutral, instrumental technical dimension, it confined the truth of mathematics to the ethereal and otherworldly realm of pure mathematics. Fergola‚Äôs mathematics, it turns out, was modern.&lt;/p&gt;
    &lt;p&gt;¬§&lt;/p&gt;
    &lt;p&gt;Massimo Mazzotti is a professor at UC Berkeley, where he holds the Thomas M. Siebel Presidential Chair in the History of Science. He is the author of Reactionary Mathematics: A Genealogy of Purity (2023).&lt;/p&gt;
    &lt;p&gt;LARB Contributor&lt;/p&gt;
    &lt;head rend="h4"&gt;Massimo Mazzotti is a professor at UC Berkeley, where he holds the Thomas M. Siebel Presidential Chair in the History of Science. He is the co-editor of Algorithmic Modernity: Mechanizing Thought and Action, 1500‚Äì2000 (2023), and the author of Reactionary Mathematics: A Genealogy of Purity (2023).&lt;/head&gt;
    &lt;head rend="h3"&gt;LARB Staff Recommendations&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;head rend="h2"&gt;Weird Science&lt;/head&gt;
        &lt;p&gt;From anti-vaxxers to Flat Earthers, the public‚Äôs (and scholars‚Äô) perception of science shifted sometime between 1990-2010, writes Michael Gordin.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h2"&gt;‚ÄúI Don‚Äôt Really Care. Do You?‚Äù: Scientists in the Grey Zone in 1930s Italy&lt;/head&gt;
        &lt;p&gt;Massimo Mazzotti reflects on how Italian scientists failed as a bulwark against fascist politics in the 1930s.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Did you know LARB is a reader-supported nonprofit?&lt;/head&gt;
    &lt;p&gt;LARB publishes daily without a paywall as part of our mission to make rigorous, incisive, and engaging writing on every aspect of literature, culture, and the arts freely accessible to the public. Help us continue this work with your tax-deductible donation today!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46833254</guid><pubDate>Sat, 31 Jan 2026 03:53:06 +0000</pubDate></item><item><title>Show HN: Phage Explorer</title><link>https://phage-explorer.org/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46833754</guid><pubDate>Sat, 31 Jan 2026 05:22:03 +0000</pubDate></item><item><title>Sumerian Star Map Recorded the Impact of an Asteroid (2024)</title><link>https://archaeologyworlds.com/5500-year-old-sumerian-star-map-recorded/</link><description>&lt;doc fingerprint="31845f72a1c499a"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;5,500-Year-Old Sumerian Star Map Recorded the Impact of a Massive Asteroid&lt;/head&gt;
    &lt;p&gt;For more than 150 years scientists have tried to solve the mystery of a notorious cuneiform clay tablet that reveals that in the past the impact case of so-called K√∂fel was detected. The circular stone-cast tablet was discovered in the late 1800s from the 650 BC King Ashurbanipal ‚Äòs underground library in Nineveh, Iraq.&lt;/p&gt;
    &lt;p&gt;Data processing, which was long believed to be an Assyrian tablet, mirrored the sky over Mesopotamia in 3300 BC and proved to be much more ancient Sumerian origin.&lt;/p&gt;
    &lt;p&gt;The tablet is the first astronomical instrument, the ‚ÄúAstrolabe.‚Äù It consists of a segmented, disk-shaped star chart with marked units of angle measure inscribed upon the rim.&lt;/p&gt;
    &lt;p&gt;Unfortunately considerable parts of the planisphere on this tablet are missing (approximately 40%), damage which dates to the sacking of Nineveh. The reverse of the tablet is not inscribed.&lt;/p&gt;
    &lt;p&gt;Still under study by modern scholars, the cuneiform tablet in the British Museum collection No K8538 (known as ‚Äúthe Planisphere‚Äù) provides extraordinary proof for the existence of sophisticated Sumerian astronomy.&lt;/p&gt;
    &lt;p&gt;In 2008 two authors, Alan Bond and Mark Hempsell published a book about the tablet called ‚ÄúA Sumerian Observation of the Kofels‚Äô Impact Event‚Äù.&lt;/p&gt;
    &lt;p&gt;Raising a storm in archaeological circles, they re-translated the cuneiform text and asserted the tablet records an ancient asteroid strike, the K√∂fels‚Äô Impact, which struck Austria sometime around 3100 BC.&lt;/p&gt;
    &lt;p&gt;The giant landslide centred at K√∂fels in Austria is 500m thick and five kilometres in diameter and has long been a mystery since geologists first looked at it in the 19th century.&lt;/p&gt;
    &lt;p&gt;The conclusion drawn by research in the middle 20th century was that it must be due to a very large meteor impact because of the evidence of crushing pressures and explosions. But this view lost favor as a much better understanding of impact sites developed in the late 20th century.&lt;/p&gt;
    &lt;p&gt;In the case of K√∂fels there is no crater, so to modern eyes it does not look as an impact site should look. However, the evidence that puzzled the earlier researchers remains unexplained by the view that it is just another landslide.&lt;/p&gt;
    &lt;p&gt;So what is the connection between the sophisticated Sumerian star chart discovered in the underground library in Nineveh and mysterious impact that took place in Austria?&lt;/p&gt;
    &lt;p&gt;Examination of the clay tablet reveals that it is an astronomical work as it has drawings of constellations on it and the text has known constellation names. It has attracted a lot of attention but in over a hundred years nobody has come up with a convincing explanation as to what it is.&lt;/p&gt;
    &lt;p&gt;With modern computer programs that can simulate trajectories and reconstruct the night sky thousands of years ago the researchers have established what the Planisphere tablet refers to. It is a copy of the night notebook of a Sumerian astronomer as he records the events in the sky before dawn on 29 June 3123 BC (Julian calendar).&lt;/p&gt;
    &lt;p&gt;Half the tablet records planet positions and cloud cover, the same as any other night, but the other half of the tablet records an object large enough for its shape to be noted even though it is still in space.&lt;/p&gt;
    &lt;p&gt;The astronomers made an accurate note of its trajectory relative to the stars, which to an error better than one degree is consistent with an impact at K√∂fels.&lt;/p&gt;
    &lt;p&gt;The observation suggests the asteroid is over a kilometer in diameter and the original orbit about the Sun was an Aten type, a class of asteroids that orbit close to the Earth, that are resonant with the Earth‚Äôs orbit.&lt;/p&gt;
    &lt;p&gt;This trajectory explains why there is no crater at K√∂fels. The incoming angle was very low (six degrees) and means the asteroid clipped a mountain called Gamskogel above the town of L√§ngenfeld, 11 kilometers from K√∂fels, and this caused the asteroid to explode before it reached its final impact point. As it traveled down the valley it became a fireball, around five kilometers in diameter (the size of the landslide).&lt;/p&gt;
    &lt;p&gt;When it hit K√∂fels it created enormous pressures that pulverized the rock and caused the landslide but because it was no longer a solid object it did not create a classic impact crater.&lt;/p&gt;
    &lt;p&gt;Mark Hempsell, discussing the K√∂fels event, said: ‚ÄúAnother conclusion can be made from the trajectory. The back plume from the explosion (the mushroom cloud) would be bent over the Mediterranean Sea re-entering the atmosphere over the Levant, Sinai, and Northern Egypt.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe ground heating though very short would be enough to ignite any flammable material ‚Äì including human hair and clothes. It is probable more people died under the plume than in the Alps due to the impact blast.‚Äù&lt;/p&gt;
    &lt;p&gt;In other words, the remarkable ancient star map shows that the Sumerians made an observation of an Aten asteroid over a kilometer in diameter that impacted K√∂fels in Austria in the early morning of 29th June 3123 BC.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46834313</guid><pubDate>Sat, 31 Jan 2026 07:32:51 +0000</pubDate></item><item><title>We have ipinfo at home or how to geolocate IPs in your CLI using latency</title><link>https://blog.globalping.io/we-have-ipinfo-at-home-or-how-to-geolocate-ips-in-your-cli-using-latency/</link><description>&lt;doc fingerprint="2ca4589a35236a90"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;We have ipinfo at home or how to geolocate IPs in your CLI using latency&lt;/head&gt;
    &lt;quote&gt;&lt;p&gt;TLDR: I made a CLI tool that can resolve an IP address to a country, US state and even a city. https://github.com/jimaek/geolocation-tool&lt;/p&gt;&lt;lb/&gt;It works well and confirms ipinfo's findings.&lt;/quote&gt;
    &lt;p&gt;Recently, I read how ipinfo finally proved what most technical people assumed: VPN providers don't actually maintain a crazy amount of infrastructure in hundreds of countries. They simply fake the IP geolocation by intentionally providing wrong location data to ARIN, RIPE, and Geo DB providers via geofeeds.&lt;/p&gt;
    &lt;p&gt;They achieved their results using a novel approach compared to other geo IP providers. Based on their blog and HackerNews comments, they built a large probe network and used it to trace and ping every (or most) IP addresses on the internet.&lt;/p&gt;
    &lt;p&gt;This latency and hop data, most likely along with advanced algorithms and data cross-reference, provides a reliable way of correctly detecting the physical geolocation of an IP address, without relying on faked data available in public sources.&lt;/p&gt;
    &lt;p&gt;This is a very interesting approach that makes total sense, and I'm sure their clients appreciate it and heavily rely on it.&lt;/p&gt;
    &lt;p&gt;While I can't ping every single IP address on the internet from hundreds of locations just yet, I can do it to a limited subset using Globalping. So I decided to try it out and see if I can replicate their results and build a small tool to allow anyone to do the same.&lt;/p&gt;
    &lt;p&gt;Globalping is an open-source, community-powered project that allows users to self-host container-based probes. These probes then become part of our public network, which allows anyone to use them to run network testing tools such as ping and traceroute.&lt;/p&gt;
    &lt;p&gt;At the moment, the network has more than 3000 probes, which in theory should be plenty to geolocate almost any IP address down to a country and even a US state level.&lt;/p&gt;
    &lt;p&gt;To automate and simplify this process, I made a little CLI tool using the globalping-ts library. My original idea was simple:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Accept a single IP as input&lt;/item&gt;
      &lt;item&gt;Ping it a few times per continent to select the continent&lt;/item&gt;
      &lt;item&gt;Then ping the IP from many different probes on that continent&lt;/item&gt;
      &lt;item&gt;Group and sort the results; the country with the lowest latency should be the correct one&lt;/item&gt;
      &lt;item&gt;And as a bonus, repeat the same process for USA states if the winning country was the US&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Essentially, what I had to do was simply create a few measurements and pass the location I needed using Globalping‚Äôs magic field, which would automatically figure out what I was looking for and select a few pseudo-random probes that fit the location and limit.&lt;/p&gt;
    &lt;p&gt;Now initially, I used &lt;code&gt;ping&lt;/code&gt; with 2 packets to run all measurements as quickly as possible, but I quickly realized it wasn‚Äôt a good idea as most networks block ICMP traffic. Next, I tried switching to TCP-based &lt;code&gt;ping&lt;/code&gt;, which required trying a few popular ports to get it to work. I quickly realized this was too complicated and unreliable and switched to &lt;code&gt;traceroute&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;It worked perfectly. Even though &lt;code&gt;traceroute&lt;/code&gt; uses ICMP by default, it did not matter to me if the target IP‚Äôs network allowed ICMP or not, I simply analyzed the latency of the last available hop. Even if you block ICMP, your upstream most likely allows it, and in most cases, it‚Äôs located in the same country.&lt;/p&gt;
    &lt;p&gt;Of course, this means the resulting data is not 100% perfect. A better approach would be to analyze each IP using different methods, including TCP and UDP-based &lt;code&gt;traceroute&lt;/code&gt; on different ports, and expand to the last few hops instead of just one. Maybe even try to figure out the location of the registered ASNs and use a weights system in combination with public whois info in order to ‚Äúvote‚Äù for the right location based on different inputs. Probably even mark low certainty IPs to be retested with a double amount of probes. (end of rant)&lt;/p&gt;
    &lt;p&gt;But that‚Äôs something for a commercial provider to figure out, which it seems they did.&lt;/p&gt;
    &lt;p&gt;For continent detection, I decided to use just 5 probes per continent; the results were extremely accurate. Although for IPs just on the "border" of continents it might be ineffective, a higher amount of probes would generate better results. For this use case, it was good enough.&lt;/p&gt;
    &lt;p&gt;My home IP in central Europe was too easy to detect:&lt;/p&gt;
    &lt;code&gt;Phase 1: Detecting continent...
  North America: 137.18 ms
  Europe: 32.39 ms
  Asia: 174.54 ms
  South America: 215.08 ms
  Oceania: 244.15 ms
  Africa: 156.83 ms
&lt;/code&gt;
    &lt;p&gt;In phase 2, all we need to do is run a single measurement with the winning continent as the location and a higher limit. Initially, I started with 250 probes with great accuracy.&lt;/p&gt;
    &lt;p&gt;Eventually, I decided to drop down to 50 as the default. Based on my tests, the results continued to look really good, and it would allow the tool to be run even without authentication, as the Globalping API allows 250 tests per hour per IP and 50 probes per measurement.&lt;/p&gt;
    &lt;p&gt;Although I recommend registering for a free account at https://dash.globalping.io/ and authenticating with a token to get up to 500 tests per hour and run more tests.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: If you need more tests than that, you can either host a probe to generate passive credits to be used as tests, or donate via GitHub Sponsors. We will automatically detect it and credit your account.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;Phase 2: Detecting country...
  Measuring from 50 probes...

  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.0%   50/50 - Best: PL (7.29 ms)                    

Top 3 Locations:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  1.. Poland, EU                               7.29 ms
  2.. Germany, EU                              13.42 ms
  3.. Lithuania, EU                            17.65 ms

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                      SUMMARY
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  Location: Poland, EU
  Minimum Latency: 7.29 ms
  Confidence: Medium
&lt;/code&gt;
    &lt;p&gt;Great, now we have a basic IP-to-country resolver that only takes a few seconds to provide a response, and I didn‚Äôt even have to understand or write any complicated math. Although I‚Äôm sure someone smarter could use a formula to geolocate IPs with even fewer probes and higher accuracy.&lt;/p&gt;
    &lt;p&gt;For phase 3, we want to resolve the US to a specific state or territory, just like ipinfo did, and luckily they even provided a few sample IPs and locations to benchmark against during testing.&lt;/p&gt;
    &lt;p&gt;Again, this was as simple as creating a new measurement with the USA as the location. I used 50 probes as the default limit and tested the NordVPN IP advertised as Bahamas but resolved to Miami by ipinfo.&lt;/p&gt;
    &lt;code&gt;Phase 3: Detecting US state...
  Measuring from 50 probes...

  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.0%   50/50 - Best: FL (0.45 ms)                    

Top 3 Locations:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  1. Florida, USA                             0.45 ms
  2. South Carolina, USA                      12.23 ms
  3. Georgia, USA                             15.01 ms

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                      SUMMARY
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  Location: Florida, United States
  Minimum Latency: 0.45 ms
  Confidence: Very High
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
&lt;/code&gt;
    &lt;p&gt;The tool agrees, Florida is the correct location. But how accurate can this system be? Can we expand it to show the city too?&lt;/p&gt;
    &lt;p&gt;Let's make a new phase, which again, will simply set the resulting country or state as the location and extract the city of the probe with the lowest latency. Here, since there are too many possible cities and towns per state and country, I expect the accuracy to be low and only point to the closest major hub. But in theory, this should be more than enough for use cases like routing or performance debugging.&lt;/p&gt;
    &lt;p&gt;And here we go, the same result ipinfo got&lt;/p&gt;
    &lt;code&gt;Phase 4: Detecting city...
  Measuring from 36 probes...

  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.0%   36/36 - Best: Miami (0.00 ms)                 

Top 3 Locations:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  1. Miami, Florida, USA                      0.00 ms
  2. West Palm Beach, Florida, USA            4.36 ms
  3. Tampa, Florida, USA                      5.85 ms

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                      SUMMARY
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  Location: Miami, Florida, United States
  Minimum Latency: 0.00 ms
  Confidence: Very High
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
&lt;/code&gt;
    &lt;p&gt;The current results are good but could be better. The main problem is with how the magic field works: when setting, for example, 'Europe' as the location, it tries to spread the tests across all European probes but does not guarantee that every single country is going to be included.&lt;/p&gt;
    &lt;p&gt;This results in inconsistencies where a probe in the same country as the target IP was not selected, and so the tool assumes the IP is located in a different neighbouring country.&lt;/p&gt;
    &lt;p&gt;To fix this and make the results more consistent, you would need to change the selection logic and manually set every country per continent and US state. By passing the full list of countries/states to the Globalping API, you ensure that at least one probe in that location is going to be selected. Additionally, you fully control the number of probes per location, which is very important to control the accuracy.&lt;/p&gt;
    &lt;p&gt;For example, North America technically contains 43 countries and territories. This means you can't just set a limit of one probe per country, it is not enough to properly understand the latency to the target IP from the disproportionately larger USA. A better limit would be around 200 probes for the USA, 20 for Canada, and 10 for Mexico.&lt;/p&gt;
    &lt;p&gt;But the goal of this tool was to use a minimum amount of probes to allow unauthenticated users to test it out. The current approach works great, it is simple to implement and it is very easy to control the accuracy by simply setting a higher limit of probes.&lt;/p&gt;
    &lt;p&gt;Overall, latency-based geolocation detection seems to be a great way to verify the location of any IP as long as you have enough vantage points. It will most likely fall apart in regions with minimal or no coverage.&lt;/p&gt;
    &lt;p&gt;The tool itself is open source and you can run it like this:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;geolocate $IP&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;You can also use the ‚Äìlimit parameter to use more probes per phase. But be careful as it applies the set value to all phases and this will very quickly eat through your limit. Check the full docs in GitHub.&lt;/p&gt;
    &lt;p&gt;Pull requests with improvements are welcome!&lt;/p&gt;
    &lt;p&gt;Feel free to email me if you need some free credits to play around with d@globalping.io&lt;/p&gt;
    &lt;p&gt;And of course consider hosting a probe, it‚Äôs as simple as running a container https://github.com/jsdelivr/globalping-probe&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46834953</guid><pubDate>Sat, 31 Jan 2026 09:30:05 +0000</pubDate></item><item><title>Ask HN: Do you still physical calculators?</title><link>https://news.ycombinator.com/item?id=46834977</link><description>&lt;doc fingerprint="df0fd97bdc676ae2"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;I‚Äôve noticed that most physical scientific and graphing calculators are easily outdone in terms of performance, capability and ease of use by the likes of Desmos and the default calculators on OS‚Äôes like the iOS, Android, and Windows.&lt;/p&gt;
      &lt;p&gt;It kind of makes me wonder whether people still use physical calculators from Texas Instruments, Casio, etc&lt;/p&gt;
      &lt;p&gt;If you do, I‚Äôd love to know why and how it is different/better for you than the ones I‚Äôve mentioned and others like them and vice verse.&lt;/p&gt;
      &lt;p&gt;Cheers!&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46834977</guid><pubDate>Sat, 31 Jan 2026 09:34:02 +0000</pubDate></item><item><title>CERN accepts $1B in private cash towards Future Circular Collider</title><link>https://physicsworld.com/a/cern-accepts-1bn-in-private-cash-towards-future-circular-collider/</link><description>&lt;doc fingerprint="f44d155c29849e32"&gt;
  &lt;main&gt;
    &lt;p&gt;Mark Thomson takes the reins at the CERN particle-physics lab, which recently received $1bn in private donations for its next collider project, as Michael Banks reports&lt;/p&gt;
    &lt;p&gt;The CERN particle-physics lab near Geneva has received $1bn from private donors towards the construction of the Future Circular Collider (FCC). The cash marks the first time in the lab‚Äôs 72-year history that individuals and philanthropic foundations have agreed to support a major CERN project. If built, the FCC would be the successor to the Large Hadron Collider (LHC), where the Higgs boson was discovered.&lt;/p&gt;
    &lt;p&gt;CERN originally released a four-volume conceptual design report for the FCC in early 2019, with more detail included in a three-volume feasibility study that came out last year. It calls for a giant tunnel some 90.7 km in circumference ‚Äì roughly three times as long as the LHC ‚Äì that would be built about 200 m underground on average.&lt;/p&gt;
    &lt;p&gt;The FCC has been recommended as the preferred option for the next flagship collider at CERN in the ongoing process to update the European Strategy for Particle Physics, which will be passed over to the CERN Council in May 2026.If the plans are given the green light by CERN Council in 2028, construction on the FCC electron-positron machine, dubbed FCC-ee, would begin in 2030. It would start operations in 2047, a few years after the High Luminosity LHC (HL-LHC) closes down, and run for about 15 years until the early 2060s.&lt;/p&gt;
    &lt;p&gt;The FCC-ee would focus on creating a million Higgs particles in total to allow physicists to study its properties with an accuracy an order of magnitude better that possible with the LHC. The FCC feasibility study then calls for a hadron machine, dubbed FCC-hh, to replace the FCC-ee in the existing 91 km tunnel. It would be a ‚Äúdiscovery machine‚Äù, smashing together protons at high energy ‚Äì about 85 TeV ‚Äì with the aim of creating new particles. If built, the FCC-hh will begin operation in 2073 and run to the end of the century.&lt;/p&gt;
    &lt;p&gt;The funding model for the FCC-ee, which is expected to have a price tag of about $18bn, is still a work in progress. But it is estimated that at least two-thirds of the construction costs will come from CERN‚Äôs 24 member states with the rest needing to be found elsewhere. One option to plug that gap is private donations and in late December CERN received a significant boost from several organizations including the Breakthrough Prize Foundation, the Eric and Wendy Schmidt Fund for Strategic Innovation, and the entrepreneurs John Elkann and Xavier Niel. Together, they pledged a total of $1bn towards the FCC-ee.&lt;/p&gt;
    &lt;p&gt;Costas Fountas, president of the CERN Council, says CERN is ‚Äúextremely grateful‚Äù for the interest. ‚ÄúThis once again demonstrates CERN‚Äôs relevance and positive impact on society, and the strong interest in CERN‚Äôs future that exists well beyond our own particle physics community,‚Äù he notes.&lt;/p&gt;
    &lt;p&gt;Eric Schmidt, who founded Google, claims that he and Wendy Schmidt were ‚Äúinspired by the ambition of this project and by what it could mean for the future of humanity‚Äù. The FCC, he believes, is an instrument that ‚Äúcould push the boundaries of human knowledge and deepen our understanding of the fundamental laws of the Universe‚Äù and could lead to technologies that could benefit society ‚Äúin profound ways‚Äù from medicine to computing to sustainable energy.&lt;/p&gt;
    &lt;p&gt;The cash promised has been welcomed by outgoing CERN director-general Fabiola Gianotti. ‚ÄúIt‚Äôs the first time in history that private donors wish to partner with CERN to build an extraordinary research instrument that will allow humanity to take major steps forward in our understanding of fundamental physics and the universe,‚Äù she said. ‚ÄúI am profoundly grateful to them for their generosity, vision, and unwavering commitment to knowledge and exploration.‚Äù&lt;/p&gt;
    &lt;head rend="h3"&gt;Further boost&lt;/head&gt;
    &lt;p&gt;The cash comes a few months after the Circular Electron‚ÄìPositron Collider (CEPC) ‚Äì a rival collider to the FCC-ee that also involves building a huge 100 km tunnel to study the Higgs in unprecedented detail ‚Äì was not considered for inclusion in China‚Äôs next five-year plan, which runs from 2026 to 2030. There has been much discussion in China about whether the CEPC is the right project for the country, with the collider facing criticism from particle physicist and Nobel laureate Chen-Ning Yang, before he died last year.&lt;/p&gt;
    &lt;p&gt;Wang Yifang of the Institute of High Energy Physics (IHEP) in Beijing says they will submit the CEPC for consideration again in 2030 unless FCC is officially approved before then. But for particle theorist John Ellis from Kings College London, China‚Äôs decision to effectively put the CEPC on the back burner ‚Äúcertainly simplifies the FCC discussion‚Äù. ‚ÄúHowever, an opportunity for growing the world particle physics community has been lost, or at least deferred [by the decision],‚Äù Ellis told Physics World.&lt;/p&gt;
    &lt;p&gt;Ellis adds, however, that he would welcome China‚Äôs participation in the FCC. ‚ÄúTheir accelerator and detector [technical design reviews] show that they could bring a lot to the table, if the political obstacles can be overcome,‚Äù he says. CERN releases plans for the ‚Äòmost extraordinary instrument ever built‚Äô&lt;/p&gt;
    &lt;p&gt;However, if the FCC-ee goes ahead China could perhaps make significant ‚Äúin-kind‚Äù contributions rather like those that occur with the ITER experimental fusion reactor, which is currently being built in France. In this case, instead of cash payments, the countries provide components, equipment and other materials.&lt;/p&gt;
    &lt;p&gt;Those considerations and more will now fall to the British physicist Mark Thomson, who took over from Gianotti as CERN director-general on 1 January for a five-year term. As well as working on funding requirements for the FCC-ee, top of his in-tray will actually be shutting down the LHC in June to make way for further work on the HL-LHC, which involves installing powerful new superconducting magnets and improving the detection.&lt;/p&gt;
    &lt;p&gt;About 90% of the 27 km LHC accelerator will be affected by the upgrade with a major part being to replace the magnets in the final focus systems of the two large experiments, ATLAS and CMS. These magnets will take the incoming beams and then focus them down to less than 10 ¬µm in cross section. The upgrade includes the installation of brand new state-of-the-art niobium-tin (Nb3Sn) superconducting focusing magnets.&lt;/p&gt;
    &lt;p&gt;The HL-LHC will probably not turn on until 2030, at which time Thomson‚Äôs term will nearly be over, but that doesn‚Äôt deter him from leading the world‚Äôs foremost particle-physics lab. ‚ÄúIt‚Äôs an incredibly exciting project,‚Äù Thomson told the Guardian. ‚ÄúIt‚Äôs more interesting than just sitting here with the machine hammering away.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46835124</guid><pubDate>Sat, 31 Jan 2026 09:58:39 +0000</pubDate></item><item><title>Automatic Programming</title><link>https://antirez.com/news/159</link><description>&lt;doc fingerprint="6f109d7f48a74099"&gt;
  &lt;main&gt;
    &lt;quote&gt;In my YouTube channel, for some time now I started to refer to the process of writing software using AI assistance (soon to become just "the process of writing software", I believe) with the term "Automatic Programming". In case you didn't notice, automatic programming produces vastly different results with the same LLMs depending on the human that is guiding the process with their intuition, design, continuous steering and idea of software. Please, stop saying "Claude vibe coded this software for me". Vibe coding is the process of generating software using AI without being part of the process at all. You describe what you want in very general terms, and the LLM will produce whatever happens to be the first idea/design/code it would spontaneously, given the training, the specific sampling that happened to dominate in that run, and so forth. The vibe coder will, at most, report things not working or not in line with what they expected. When the process is actual software production where you know what is going on, remember: it is the software *you* are producing. Moreover remember that the pre-training data, while not the only part where the LLM learns (RL has its big weight) was produced by humans, so we are not appropriating something else. We can pretend AI generated code is "ours", we have the right to do so. Pre-training is, actually, our collective gift that allows many individuals to do things they could otherwise never do, like if we are now linked in a collective mind, in a certain way. That said, if vibe coding is the process of producing software without much understanding of what is going on (which has a place, and democratizes software production, so it is totally ok with me), automatic programming is the process of producing software that attempts to be high quality and strictly following the producer's vision of the software (this vision is multi-level: can go from how to do, exactly, certain things, at a higher level, to stepping in and tell the AI how to write a certain function), with the help of AI assistance. Also a fundamental part of the process is, of course, *what* to do. I'm a programmer, and I use automatic programming. The code I generate in this way is mine. My code, my output, my production. I, and you, can be proud. If you are not completely convinced, think to Redis. In Redis there is not much technical novelty, especially at its start it was just a sum of basic data structures and networking code that every competent system programmer could write. So, why it became a very useful piece of software? Because of the ideas and visions it contained. Programming is now automatic, vision is not (yet).&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46835208</guid><pubDate>Sat, 31 Jan 2026 10:11:25 +0000</pubDate></item><item><title>htmx: Server Sent Event (SSE) Extension</title><link>https://htmx.org/extensions/sse/</link><description>&lt;doc fingerprint="3e8ebe6b09a3cc9c"&gt;
  &lt;main&gt;&lt;p&gt;The &lt;code&gt;Server Sent Events&lt;/code&gt; extension connects to
an EventSource directly
from HTML. It manages the connections to your web server, listens for server events, and then swaps their contents into
your htmx webpage in real-time.&lt;/p&gt;&lt;p&gt;SSE is a lightweight alternative to WebSockets that works over existing HTTP connections, so it is easy to use through proxy servers and firewalls. Remember, SSE is a uni-directional service, so you cannot send any messages to an SSE server once the connection has been established. If you need bi-directional communication, then you should consider using WebSockets instead.&lt;/p&gt;&lt;p&gt;This extension replaces the experimental &lt;code&gt;hx-sse&lt;/code&gt; attribute built into previous versions of htmx. For help migrating
from older versions, see the migration guide at the bottom of this page.&lt;/p&gt;&lt;p&gt;Use the following attributes to configure how SSE connections behave:&lt;/p&gt;&lt;code&gt;sse-connect="&amp;lt;url&amp;gt;"&lt;/code&gt; - The URL of the SSE server.&lt;code&gt;sse-swap="&amp;lt;message-name&amp;gt;"&lt;/code&gt; - The name of the message to swap into the DOM.&lt;code&gt;hx-trigger="sse:&amp;lt;message-name&amp;gt;"&lt;/code&gt; - SSE messages can also trigger HTTP callbacks using
the &lt;code&gt;hx-trigger&lt;/code&gt; attribute.&lt;code&gt;sse-close=&amp;lt;message-name&amp;gt;&lt;/code&gt; - To close the EventStream gracefully when that message is received. This might be helpful
if you want to send information to a client that will eventually stop.&lt;p&gt;The fastest way to install &lt;code&gt;sse&lt;/code&gt; is to load it via a CDN. Remember to always include the core htmx library before the extension and enable the extension.&lt;/p&gt;&lt;code&gt;&amp;lt;head&amp;gt;
    &amp;lt;script src="https://cdn.jsdelivr.net/npm/htmx.org@2.0.8/dist/htmx.min.js" integrity="sha384-/TgkGk7p307TH7EXJDuUlgG3Ce1UVolAOFopFekQkkXihi5u/6OCvVKyz1W+idaz" crossorigin="anonymous"&amp;gt;&amp;lt;/script&amp;gt;
    &amp;lt;script src="https://cdn.jsdelivr.net/npm/htmx-ext-sse@2.2.4" integrity="sha384-A986SAtodyH8eg8x8irJnYUk7i9inVQqYigD6qZ9evobksGNIXfeFvDwLSHcp31N" crossorigin="anonymous"&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body hx-ext="sse"&amp;gt;
&lt;/code&gt;
&lt;p&gt;An unminified version is also available at https://cdn.jsdelivr.net/npm/htmx-ext-sse/dist/sse.js.&lt;/p&gt;&lt;p&gt;While the CDN approach is simple, you may want to consider not using CDNs in production. The next easiest way to install &lt;code&gt;sse&lt;/code&gt; is to simply copy it into your project. Download the extension from &lt;code&gt;https://cdn.jsdelivr.net/npm/htmx-ext-sse&lt;/code&gt;, add it to the appropriate directory in your project and include it where necessary with a &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; tag.&lt;/p&gt;&lt;p&gt;For npm-style build systems, you can install &lt;code&gt;sse&lt;/code&gt; via npm:&lt;/p&gt;&lt;code&gt;npm install htmx-ext-sse
&lt;/code&gt;
&lt;p&gt;After installing, you‚Äôll need to use appropriate tooling to bundle &lt;code&gt;node_modules/htmx-ext-sse/dist/sse.js&lt;/code&gt; (or &lt;code&gt;.min.js&lt;/code&gt;). For example, you might bundle the extension with htmx core from &lt;code&gt;node_modules/htmx.org/dist/htmx.js&lt;/code&gt; and project-specific code.&lt;/p&gt;&lt;p&gt;If you are using a bundler to manage your javascript (e.g. Webpack, Rollup):&lt;/p&gt;&lt;code&gt;htmx.org&lt;/code&gt; and &lt;code&gt;htmx-ext-sse&lt;/code&gt; via npm&lt;code&gt;index.js&lt;/code&gt;&lt;code&gt;import `htmx.org`;
import `htmx-ext-sse`; 
&lt;/code&gt;
&lt;code&gt;
&amp;lt;div hx-ext="sse" sse-connect="/chatroom" sse-swap="message"&amp;gt;
    Contents of this box will be updated in real time
    with every SSE message received from the chatroom.
&amp;lt;/div&amp;gt;
&lt;/code&gt;
&lt;p&gt;To connect to an SSE server, use the &lt;code&gt;hx-ext="sse"&lt;/code&gt; attribute to install the extension on that HTML element, then
add &lt;code&gt;sse-connect="&amp;lt;url&amp;gt;"&lt;/code&gt; to the element to make the connection.&lt;/p&gt;&lt;p&gt;When designing your server application, remember that SSE works just like any HTTP request. Although you cannot send any messages to the server after you have established a connection, you can send parameters to the server along with your request. So, instead of making an SSE connection to your server at &lt;code&gt;https://my-server/chat-updates&lt;/code&gt; you can also connect
to &lt;code&gt;https://my-server/chat-updates?friends=true&amp;amp;format=detailed&lt;/code&gt;. This allows your server to customize its responses to
what your client needs.&lt;/p&gt;&lt;p&gt;SSE messages consist of an event name and a data packet. No other metadata is allowed in the message. Here is an example:&lt;/p&gt;&lt;code&gt;event: EventName
data: &amp;lt;div&amp;gt;Content to swap into your HTML page.&amp;lt;/div&amp;gt;
&lt;/code&gt;
&lt;p&gt;We‚Äôll use the &lt;code&gt;sse-swap&lt;/code&gt; attribute to listen for this event and swap its contents into our webpage.&lt;/p&gt;&lt;code&gt;
&amp;lt;div hx-ext="sse" sse-connect="/event-source" sse-swap="EventName"&amp;gt;&amp;lt;/div&amp;gt;
&lt;/code&gt;
&lt;p&gt;Notice that the name &lt;code&gt;EventName&lt;/code&gt; from the server‚Äôs message must match the value in the &lt;code&gt;sse-swap&lt;/code&gt; attribute. Your server
can use as many different event names as necessary, but be careful: browsers can only listen for events that have been
explicitly named. So, if your server sends an event named &lt;code&gt;ChatroomUpdate&lt;/code&gt; but your browser is only listening for events
named &lt;code&gt;ChatUpdate&lt;/code&gt; then the extra event will be discarded.&lt;/p&gt;&lt;p&gt;SSE messages can also be sent without any event name. In this case, the browser uses the default name &lt;code&gt;message&lt;/code&gt; in its
place. The same rules specified above still apply. If your server sends an unnamed message, then you must listen for it
by including &lt;code&gt;sse-swap="message"&lt;/code&gt;. There is no option for using a catch-all name. Here‚Äôs how this looks:&lt;/p&gt;&lt;code&gt;data: &amp;lt;div&amp;gt;Content to swap into your HTML page.&amp;lt;/div&amp;gt;
&lt;/code&gt;
&lt;code&gt;
&amp;lt;div hx-ext="sse" sse-connect="/event-source" sse-swap="message"&amp;gt;&amp;lt;/div&amp;gt;
&lt;/code&gt;
&lt;p&gt;You can also listen to multiple events (named or unnamed) from a single EventSource. Listeners must be either 1) the same element that contains the &lt;code&gt;hx-ext&lt;/code&gt; and &lt;code&gt;sse-connect&lt;/code&gt; attributes, or 2) child elements of the element containing
the &lt;code&gt;hx-ext&lt;/code&gt; and &lt;code&gt;sse-connect&lt;/code&gt; attributes.&lt;/p&gt;&lt;code&gt;
Multiple events in the same element
&amp;lt;div hx-ext="sse" sse-connect="/server-url" sse-swap="event1,event2"&amp;gt;&amp;lt;/div&amp;gt;

Multiple events in different elements (from the same source).
&amp;lt;div hx-ext="sse" sse-connect="/server-url"&amp;gt;
    &amp;lt;div sse-swap="event1"&amp;gt;&amp;lt;/div&amp;gt;
    &amp;lt;div sse-swap="event2"&amp;gt;&amp;lt;/div&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;
&lt;p&gt;When a connection for server sent events has been established, child elements can listen for these events by using the special &lt;code&gt;hx-trigger&lt;/code&gt; syntax &lt;code&gt;sse:&amp;lt;event_name&amp;gt;&lt;/code&gt;. This, when combined with
an &lt;code&gt;hx-get&lt;/code&gt; or similar will trigger the element to make a request.&lt;/p&gt;&lt;p&gt;Here is an example:&lt;/p&gt;&lt;code&gt;
&amp;lt;div hx-ext="sse" sse-connect="/event_stream"&amp;gt;
    &amp;lt;div hx-get="/chatroom" hx-trigger="sse:chatter"&amp;gt;
        ...
    &amp;lt;/div&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;
&lt;p&gt;This example establishes an SSE connection to the &lt;code&gt;event_stream&lt;/code&gt; end point which then triggers
a &lt;code&gt;GET&lt;/code&gt; to the &lt;code&gt;/chatroom&lt;/code&gt; url whenever the &lt;code&gt;chatter&lt;/code&gt; event is seen.&lt;/p&gt;&lt;p&gt;If the SSE Event Stream is closed unexpectedly, browsers are supposed to attempt to reconnect automatically. However, in rare situations this does not work and your browser can be left hanging. This extension adds its own reconnection logic (using an exponential-backoff algorithm) on top of the browser‚Äôs automatic reconnection, so that your SSE streams will always be as reliable as possible.&lt;/p&gt;&lt;p&gt;Htmx includes a demo SSE server written in Node.js that will help you to see SSE in action, and begin bootstrapping your own SSE code. It is located in the /test/ws-sse folder of the &lt;code&gt;htmx-extensions&lt;/code&gt; repository. Look at /test/ws-sse/README.md
for instructions on running and using the test server.&lt;/p&gt;&lt;p&gt;Previous versions of htmx used a built-in tag &lt;code&gt;hx-sse&lt;/code&gt; to implement Server Sent Events. This code has been migrated into
an extension instead. Here are the steps you need to take to migrate to this version:&lt;/p&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell role="head"&gt;Old Attribute&lt;/cell&gt;&lt;cell role="head"&gt;New Attribute&lt;/cell&gt;&lt;cell role="head"&gt;Comments&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;&lt;code&gt;hx-sse=""&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;hx-ext="sse"&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Use the &lt;code&gt;hx-ext="sse"&lt;/code&gt; attribute to install the SSE extension into any HTML element.&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;&lt;code&gt;hx-sse="connect:&amp;lt;url&amp;gt;"&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;sse-connect="&amp;lt;url&amp;gt;"&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Add a new attribute &lt;code&gt;sse-connect&lt;/code&gt; to the tag that specifies the URL of the Event Stream.  This attribute must be in the same tag as the &lt;code&gt;hx-ext&lt;/code&gt; attribute.&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;&lt;code&gt;hx-sse="swap:&amp;lt;EventName&amp;gt;"&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;sse-swap="&amp;lt;EventName&amp;gt;"&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Add a new attribute &lt;code&gt;sse-swap&lt;/code&gt; to any elements that will be swapped in via the SSE extension.  This attribute must be placed on or inside of the tag containing the &lt;code&gt;hx-ext&lt;/code&gt; attribute.&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;&lt;code&gt;hx-trigger="sse:&amp;lt;EventName&amp;gt;"&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;NO CHANGE&lt;/cell&gt;&lt;cell&gt;any &lt;code&gt;hx-trigger&lt;/code&gt; attributes do not need to change.  The extension will identify these attributes and add listeners for any events prefixed with &lt;code&gt;sse:&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;This extension dispatches several events. You can listen for these events like so:&lt;/p&gt;&lt;code&gt;document.body.addEventListener('htmx:sseBeforeMessage', function (e) {
    // do something before the event data is swapped in
})
&lt;/code&gt;
&lt;p&gt;Each event object has a &lt;code&gt;detail&lt;/code&gt; field that contains details of the event.&lt;/p&gt;&lt;code&gt;htmx:sseOpen&lt;/code&gt;&lt;p&gt;This event is dispatched when an SSE connection has been successfully established.&lt;/p&gt;&lt;code&gt;detail.elt&lt;/code&gt; - The element on which the SSE connection was setup. This is the element which has the &lt;code&gt;sse-connect&lt;/code&gt;
attribute.&lt;code&gt;detail.source&lt;/code&gt; - The EventSource object.&lt;code&gt;htmx:sseError&lt;/code&gt;&lt;p&gt;This event is dispatched when an SSE connection could not be established.&lt;/p&gt;&lt;code&gt;detail.error&lt;/code&gt; - The error that occurred while creating
an EventSource.&lt;code&gt;detail.source&lt;/code&gt; - The EventSource.&lt;code&gt;htmx:sseBeforeMessage&lt;/code&gt;&lt;p&gt;This event is dispatched just before the SSE event data is swapped into the DOM. If you don‚Äôt want to swap call &lt;code&gt;preventDefault()&lt;/code&gt; on the event. Additionally the &lt;code&gt;detail&lt;/code&gt; field is
a MessageEvent - this is the event created
by EventSource when it receives an SSE message.&lt;/p&gt;&lt;code&gt;detail.elt&lt;/code&gt; - The swap target.&lt;code&gt;htmx:sseMessage&lt;/code&gt;&lt;p&gt;This event is dispatched after the SSE event data has been swapped into the DOM. The &lt;code&gt;detail&lt;/code&gt; field is
a MessageEvent - this is the event created
by EventSource when it receives an SSE message.&lt;/p&gt;&lt;code&gt;htmx:sseClose&lt;/code&gt;&lt;p&gt;This event is dispatched in three different closing scenario. To control for the scenario the user can control for the evt.detail.sseclose property.&lt;/p&gt;&lt;code&gt;document.body.addEventListener('htmx:sseClose', function (e) {
    const reason = e.detail.type
    switch (reason) {
        case "nodeMissing":
            // Parent node is missing and therefore connection was closed
        ...
        case "nodeReplaced":
            // Parent node replacement caused closing of connection
        ...
        case "message":
            // connection was closed due to reception of message sse-close
        ...
    }
})
&lt;/code&gt;
&lt;code&gt;detail.elt&lt;/code&gt; - The swap target.&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46835318</guid><pubDate>Sat, 31 Jan 2026 10:30:59 +0000</pubDate></item><item><title>Euro firms must ditch Uncle Sam's clouds and go EU-native</title><link>https://www.theregister.com/2026/01/30/euro_firms_must_ditch_us/</link><description>&lt;doc fingerprint="bf389010bb04ad4c"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Euro firms must ditch Uncle Sam's clouds and go EU-native&lt;/head&gt;&lt;head rend="h2"&gt;Just because you're paranoid about digital sovereignty doesn't mean they're not after you&lt;/head&gt;&lt;p&gt;Opinion I'm an eighth-generation American, and let me tell you, I wouldn't trust my data, secrets, or services to a US company these days for love or money. Under our current government, we're simply not trustworthy.&lt;/p&gt;&lt;p&gt;In the Trump‚Äëredux era of 2026, European enterprises are finally taking data seriously, and that means packing up from Redmond-by-Seattle and moving their most sensitive workloads home. This isn't just compliance theater; it's a straight‚Äëup national economic security play.&lt;/p&gt;&lt;head rend="h2"&gt;Open source's new mission: Rebuild a continent's tech stack&lt;/head&gt;READ MORE&lt;p&gt;Europe's digital sovereignty paranoia, long waved off as regulatory chatter, is now feeding directly into procurement decisions. Gartner told The Reg last year that IT spending in Europe is set to grow by 11 percent in 2026, hitting $1.4 trillion, with a big chunk rolling into "sovereign cloud" options and on‚Äëprem/edge architectures.&lt;/p&gt;&lt;p&gt;The kicker? Fully 61 percent of European CIOs and tech leaders say they want to increase their use of local cloud providers. More than half say geopolitics will prevent them from leaning further on US‚Äëbased hyperscalers.&lt;/p&gt;&lt;p&gt;The American hypercloud vendors have figured this out. AWS recently made its European Sovereign Cloud available. This AWS cloud, Amazon claims, is "entirely located within the EU, and physically and logically separate from other AWS Regions." On top of that, EU residents will "independently operate it" and "be backed by strong technical controls, sovereign assurances, and legal protections designed to meet the needs of European governments and enterprises for sensitive data."&lt;/p&gt;&lt;p&gt;Many EU-based companies aren't pleased with this Euro-washing of American hypercloud services. The Cloud Infrastructure Service Providers in Europe (CISPE) trade association accuses the EU Cloud Sovereignty Framework of being set up to favor the incumbent (American) hypercloud providers.&lt;/p&gt;&lt;p&gt;They're not wrong.&lt;/p&gt;&lt;p&gt;You don't need a DEA warrant or a Justice Department subpoena to see the trend: Europe's 90‚Äëplus‚Äëpercent dependency on US cloud infrastructure, as former European Commission advisor Cristina Caffarra put it, is a single‚Äëshock‚Äëevent security nightmare waiting to rupture the EU's digital stability.&lt;/p&gt;&lt;p&gt;Seriously. What will you do if Washington decides to unplug you? Say Trump gets up on the wrong side of the bed and decides to invade Greenland. There goes NATO, and in all the saber-rattling leading up to the 10th Mountain Division being shipped to Nuuk, he orders American companies to cut their services to all EU countries and the UK.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;When AI 'builds a browser,' check the repo before believing the hype&lt;/item&gt;&lt;item&gt;Just because Linus Torvalds vibe codes doesn't mean it's a good idea&lt;/item&gt;&lt;item&gt;The most durable tech is boring, old, and everywhere&lt;/item&gt;&lt;item&gt;What the Linux desktop really needs to challenge Windows&lt;/item&gt;&lt;/list&gt;&lt;p&gt;With the way things are going, they're not going to say no. I mean, CEOs Tim Cook of Apple, Eric Yuan of Zoom, Lisa Su of AMD, and ‚Äì pay attention ‚Äì Amazon's Andy Jassy all went obediently to watch a feature-length White House screening of Melania, the universally-loathed, 104‚Äëminute Amazon‚Äëproduced documentary about First Lady Melania Trump.&lt;/p&gt;&lt;head rend="h2"&gt;Europe's cloud challenge: Building an Airbus for the digital age&lt;/head&gt;READ MORE&lt;p&gt;Sure, that's a silly example, but for American companies to do business today, they're kowtowing to Trump. Or, take a far more serious example, when Minnesota company CEOs called for "de-escalation" in the state, there was not one word about ICE or the government's role in the bloodshed. It was the corporate equivalent of the mealy-mouthed "thoughts and prayers" American right-wingers always say after a US school shooting.&lt;/p&gt;&lt;p&gt;Some companies have already figured out which way the wind is blowing. Airbus, the European aerospace titan, has put out a ‚Ç¨50 million, decade‚Äëlong tender to migrate its mission‚Äëcritical applications to a "sovereign European cloud." Airbus wants its whole stack ‚Äì data at rest, data in transit, logging, IAM, and security‚Äëmonitoring infrastructure ‚Äì all rooted in EU law and overseen by EU operators. As Catherine Jestin, Airbus's executive vice president of digital, told The Register: "We want to ensure this information remains under European control."&lt;/p&gt;&lt;p&gt;Who can blame them? Thanks to the American CLOUD Act and related US surveillance statutes, US‚Äëheadquartered providers must hand over European data regardless of where the bytes sit. Exhibit A is that Microsoft has already conceded that it cannot guarantee data independence from US law enforcement. Airbus is betting that "data residency on paper" from AWS‚Äëstyled "EU sections" is not enough. Real sovereignty demands EU‚Äëowned and run operations with full contractual and legal firewalls. Sure, your data may live in Frankfurt, but your fate still rests in Seattle, Redmond, or Mountain View if an American company owns your cloud provider.&lt;/p&gt;&lt;p&gt;Besides, do you really want some Trump apparatchik getting their hands on your data? I mean, this is a government where Madhu Gottumukkala, the acting director of the US Cybersecurity and Infrastructure Security Agency, uploaded sensitive data into ChatGPT!&lt;/p&gt;&lt;head rend="h2"&gt;UK urged to unplug from US tech giants as digital sovereignty fears grow&lt;/head&gt;READ MORE&lt;p&gt;In response, Brussels is pushing an open source‚Äëled exit from hyperscaler lock‚Äëin. Ministries are standardizing on Nextcloud‚Äëstyle collaboration stacks instead of Microsoft 365 to fund Euro‚Äënative clouds via the European Cloud Alliance. Some countries, like France, are already shoving Zoom, Teams, and other US videoconferencing platforms out the door in favor of a local service.&lt;/p&gt;&lt;p&gt;If you're running an EU‚Äëbased firm in 2026, the takeaway isn't that AWS‚Äëin‚ÄëFrankfurt is evil; it's that for certain workloads, especially national security, industrial IP, or high‚Äëprofile consumer data franchises, EU‚Äënative cloud and services are no longer a nice‚Äëto‚Äëhave but a business continuity plan requirement.&lt;/p&gt;&lt;p&gt;It's time to get serious about digital sovereignty. The clock is ticking, and there's no telling when Trump will go off. ¬Æ&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46835336</guid><pubDate>Sat, 31 Jan 2026 10:34:07 +0000</pubDate></item></channel></rss>