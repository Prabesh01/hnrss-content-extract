<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 02 Nov 2025 20:34:52 +0000</lastBuildDate><item><title>Visopsys: OS maintained by a single developer since 1997</title><link>https://visopsys.org/</link><description>&lt;doc fingerprint="a1fd7da091abb4c3"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Welcome!&lt;/head&gt;
    &lt;p&gt;Visopsys is an alternative operating system for PC compatible computers. In development since 1997, this system is small, fast, and open source. It features a simple but functional graphical interface, pre-emptive multitasking, and virtual memory. Though it attempts to be compatible in a number of ways, Visopsys is not a clone of any other operating system. You can demo the distribution from a “live” USB stick, CD/DVD, or floppy disk … (read more).&lt;/p&gt;
    &lt;head rend="h4"&gt;Features of Visopsys&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Small &amp;amp; fast&lt;/item&gt;
      &lt;item&gt;Graphical user interface&lt;/item&gt;
      &lt;item&gt;Fully multitasking&lt;/item&gt;
      &lt;item&gt;100% protected mode&lt;/item&gt;
      &lt;item&gt;Open source, free software&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;News&lt;/head&gt;
    &lt;p&gt;[21.09.2023] – Version 0.92 is now available from the download page&lt;lb/&gt; [30.07.2021] – Version 0.91 was released&lt;lb/&gt; [16.04.2020] – Version 0.9 was released&lt;lb/&gt; [21.01.2020] – Version 0.85 was released&lt;lb/&gt; [15.05.2019] – Version 0.84 was released&lt;lb/&gt; [09.08.2018] – Version 0.83 was released&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45785858</guid><pubDate>Sat, 01 Nov 2025 22:07:49 +0000</pubDate></item><item><title>How I use every Claude Code feature</title><link>https://blog.sshh.io/p/how-i-use-every-claude-code-feature</link><description>&lt;doc fingerprint="36cf0efb2d9d82af"&gt;
  &lt;main&gt;
    &lt;p&gt;I use Claude Code. A lot.&lt;/p&gt;
    &lt;p&gt;As a hobbyist, I run it in a VM several times a week on side projects, often with &lt;code&gt;--dangerously-skip-permissions&lt;/code&gt; to vibe code whatever idea is on my mind. Professionally, part of my team builds the AI-IDE rules and tooling for our engineering team that consumes several billion tokens per month just for codegen.&lt;/p&gt;
    &lt;p&gt;The CLI agent space is getting crowded and between Claude Code, Gemini CLI, Cursor, and Codex CLI, it feels like the real race is between Anthropic and OpenAI. But TBH when I talk to other developers, their choice often comes down to what feels like superficials—a “lucky” feature implementation or a system prompt “vibe” they just prefer. At this point these tools are all pretty good. I also feel like folks often also over index on the output style or UI. Like to me the “you’re absolutely right!” sycophancy isn’t a notable bug; it’s a signal that you’re too in-the-loop. Generally my goal is to “shoot and forget”—to delegate, set the context, and let it work. Judging the tool by the final PR and not how it gets there.&lt;/p&gt;
    &lt;p&gt;Having stuck to Claude Code for the last few months, this post is my set of reflections on Claude Code’s entire ecosystem. We’ll cover nearly every feature I use (and, just as importantly, the ones I don’t), from the foundational &lt;code&gt;CLAUDE.md&lt;/code&gt; file and custom slash commands to the powerful world of Subagents, Hooks, and GitHub Actions. This post ended up a bit long and I’d recommend it as more of a reference than something to read in entirety. &lt;/p&gt;
    &lt;head rend="h2"&gt;CLAUDE.md&lt;/head&gt;
    &lt;p&gt;The single most important file in your codebase for using Claude Code effectively is the root &lt;code&gt;CLAUDE.md&lt;/code&gt;. This file is the agent’s “constitution,” its primary source of truth for how your specific repository works.&lt;/p&gt;
    &lt;p&gt;How you treat this file depends on the context. For my hobby projects, I let Claude dump whatever it wants in there.&lt;/p&gt;
    &lt;p&gt;For my professional work, our monorepo’s &lt;code&gt;CLAUDE.md&lt;/code&gt; is strictly maintained and currently sits at 13KB (I could easily see it growing to 25KB). &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;It only documents tools and APIs used by 30% (arbitrary) or more of our engineers (else tools are documented in product or library specific markdown files)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;We’ve even started allocating effectively a max token count for each internal tool’s documentation, almost like selling “ad space” to teams. If you can’t explain your tool concisely, it’s not ready for the&lt;/p&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Tips and Common Anti-Patterns&lt;/head&gt;
    &lt;p&gt;Over time, we’ve developed a strong, opinionated philosophy for writing an effective &lt;code&gt;CLAUDE.md&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Start with Guardrails, Not a Manual. Your&lt;/p&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt;should start small, documenting based on what Claude is getting wrong.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Don’t&lt;/p&gt;&lt;code&gt;@&lt;/code&gt;-File Docs. If you have extensive documentation elsewhere, it’s tempting to&lt;code&gt;@&lt;/code&gt;-mention those files in your&lt;code&gt;CLAUDE.md&lt;/code&gt;. This bloats the context window by embedding the entire file on every run. But if you just mention the path, Claude will often ignore it. You have to pitch the agent on why and when to read the file. “For complex … usage or if you encounter a&lt;code&gt;FooBarError&lt;/code&gt;, see&lt;code&gt;path/to/docs.md&lt;/code&gt;for advanced troubleshooting steps.”&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Don’t Just Say “Never.” Avoid negative-only constraints like “Never use the&lt;/p&gt;&lt;code&gt;--foo-bar&lt;/code&gt;flag.” The agent will get stuck when it thinks it must use that flag. Always provide an alternative.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Use&lt;/p&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt;as a Forcing Function. If your CLI commands are complex and verbose, don’t write paragraphs of documentation to explain them. That’s patching a human problem. Instead, write a simple bash wrapper with a clear, intuitive API and document that. Keeping your&lt;code&gt;CLAUDE.md&lt;/code&gt;as short as possible is a fantastic forcing function for simplifying your codebase and internal tooling.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s a simplified snapshot:&lt;/p&gt;
    &lt;code&gt;# Monorepo

## Python
- Always ...
- Test with &amp;lt;command&amp;gt;
... 10 more ...

## &amp;lt;Internal CLI Tool&amp;gt;
... 10 bullets, focused on the 80% of use cases ...
- &amp;lt;usage example&amp;gt;
- Always ...
- Never &amp;lt;x&amp;gt;, prefer &amp;lt;Y&amp;gt;

For &amp;lt;complex usage&amp;gt; or &amp;lt;error&amp;gt; see path/to/&amp;lt;tool&amp;gt;_docs.md

...&lt;/code&gt;
    &lt;p&gt;Finally, we keep this file synced with an &lt;code&gt;AGENTS.md&lt;/code&gt; file to maintain compatibility with other AI IDEs that our engineers might be using.&lt;/p&gt;
    &lt;p&gt;If you are looking for more tips for writing markdown for coding agents see “AI Can’t Read Your Docs”, “AI-powered Software Engineering”, and “How Cursor (AI IDE) Works”.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Treat your &lt;code&gt;CLAUDE.md&lt;/code&gt; as a high-level, curated set of guardrails and pointers. Use it to guide where you need to invest in more AI (and human) friendly tools, rather than trying to make it a comprehensive manual.&lt;/p&gt;
    &lt;head rend="h2"&gt;Compact, Context, &amp;amp; Clear&lt;/head&gt;
    &lt;p&gt;I recommend running &lt;code&gt;/context&lt;/code&gt; mid coding session at least once to understand how you are using your 200k token context window (even with Sonnet-1M, I don’t trust that the full context window is actually used effectively). For us a fresh session in our monorepo costs a baseline ~20k tokens (10%) with the remaining 180k for making your change — which can fill up quite fast.&lt;/p&gt;
    &lt;p&gt;I have three main workflows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/compact&lt;/code&gt;(Avoid): I avoid this as much as possible. The automatic compaction is opaque, error-prone, and not well-optimized.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/clear&lt;/code&gt;+&lt;code&gt;/catchup&lt;/code&gt;(Simple Restart): My default reboot. I&lt;code&gt;/clear&lt;/code&gt;the state, then run a custom&lt;code&gt;/catchup&lt;/code&gt;command to make Claude read all changed files in my git branch.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;“Document &amp;amp; Clear” (Complex Restart): For large tasks. I have Claude dump its plan and progress into a&lt;/p&gt;&lt;code&gt;.md&lt;/code&gt;,&lt;code&gt;/clear&lt;/code&gt;the state, then start a new session by telling it to read the&lt;code&gt;.md&lt;/code&gt;and continue.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Takeaway: Don’t trust auto-compaction. Use &lt;code&gt;/clear&lt;/code&gt; for simple reboots and the “Document &amp;amp; Clear” method to create durable, external “memory” for complex tasks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Custom Slash Commands&lt;/head&gt;
    &lt;p&gt;I think of slash commands as simple shortcuts for frequently used prompts, nothing more. My setup is minimal:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/catchup&lt;/code&gt;: The command I mentioned earlier. It just prompts Claude to read all changed files in my current git branch.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/pr&lt;/code&gt;: A simple helper to clean up my code, stage it, and prepare a pull request.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;IMHO if you have a long list of complex, custom slash commands, you’ve created an anti-pattern. To me the entire point of an agent like Claude is that you can type almost whatever you want and get a useful, mergable result. The moment you force an engineer (or non-engineer) to learn a new, documented-somewhere list of essential magic commands just to get work done, you’ve failed.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Use slash commands as simple, personal shortcuts, not as a replacement for building a more intuitive &lt;code&gt;CLAUDE.md&lt;/code&gt; and better-tooled agent.&lt;/p&gt;
    &lt;head rend="h2"&gt;Custom Subagents&lt;/head&gt;
    &lt;p&gt;On paper, custom subagents are Claude Code’s most powerful feature for context management. The pitch is simple: a complex task requires &lt;code&gt;X&lt;/code&gt; tokens of input context (e.g., how to run tests), accumulates &lt;code&gt;Y&lt;/code&gt; tokens of working context, and produces a &lt;code&gt;Z&lt;/code&gt; token answer. Running &lt;code&gt;N&lt;/code&gt; tasks means &lt;code&gt;(X + Y + Z) * N&lt;/code&gt; tokens in your main window.&lt;/p&gt;
    &lt;p&gt;The subagent solution is to farm out the &lt;code&gt;(X + Y) * N&lt;/code&gt; work to specialized agents, which only return the final &lt;code&gt;Z&lt;/code&gt; token answers, keeping your main context clean.&lt;/p&gt;
    &lt;p&gt;I find they are a powerful idea that, in practice, custom subagents create two new problems:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;They Gatekeep Context: If I make a&lt;/p&gt;&lt;code&gt;PythonTests&lt;/code&gt;subagent, I’ve now hidden all testing context from my main agent. It can no longer reason holistically about a change. It’s now forced to invoke the subagent just to know how to validate its own code.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;They Force Human Workflows: Worse, they force Claude into a rigid, human-defined workflow. I’m now dictating how it must delegate, which is the very problem I’m trying to get the agent to solve for me.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;My preferred alternative is to use Claude’s built-in &lt;code&gt;Task(...)&lt;/code&gt; feature to spawn clones of the general agent.&lt;/p&gt;
    &lt;p&gt;I put all my key context in the &lt;code&gt;CLAUDE.md&lt;/code&gt;. Then, I let the main agent decide when and how to delegate work to copies of itself. This gives me all the context-saving benefits of subagents without the drawbacks. The agent manages its own orchestration dynamically.&lt;/p&gt;
    &lt;p&gt;In my “Building Multi-Agent Systems (Part 2)” post, I called this the “Master-Clone” architecture, and I strongly prefer it over the “Lead-Specialist” model that custom subagents encourage.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Custom subagents are a brittle solution. Give your main agent the context (in &lt;code&gt;CLAUDE.md&lt;/code&gt;) and let it use its own &lt;code&gt;Task/Explore(...)&lt;/code&gt; feature to manage delegation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Resume, Continue, &amp;amp; History&lt;/head&gt;
    &lt;p&gt;On a simple level, I use &lt;code&gt;claude --resume&lt;/code&gt; and &lt;code&gt;claude --continue&lt;/code&gt; frequently. They’re great for restarting a bugged terminal or quickly rebooting an older session. I’ll often &lt;code&gt;claude --resume&lt;/code&gt; a session from days ago just to ask the agent to summarize how it overcame a specific error, which I then use to improve our &lt;code&gt;CLAUDE.md&lt;/code&gt; and internal tooling.&lt;/p&gt;
    &lt;p&gt;More in the weeds, Claude Code stores all session history in &lt;code&gt;~/.claude/projects/&lt;/code&gt; to tap into the raw historical session data. I have scripts that run meta-analysis on these logs, looking for common exceptions, permission requests, and error patterns to help improve agent-facing context.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Use &lt;code&gt;claude --resume&lt;/code&gt; and &lt;code&gt;claude --continue &lt;/code&gt;to restart sessions and uncover buried historical context.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hooks&lt;/head&gt;
    &lt;p&gt;Hooks are huge. I don’t use them for hobby projects, but they are critical for steering Claude in a complex enterprise repo. They are the deterministic “must-do” rules that complement the “should-do” suggestions in &lt;code&gt;CLAUDE.md&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We use two types:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Block-at-Submit Hooks: This is our primary strategy. We have a&lt;/p&gt;&lt;code&gt;PreToolUse&lt;/code&gt;hook that wraps any&lt;code&gt;Bash(git commit)&lt;/code&gt;command. It checks for a&lt;code&gt;/tmp/agent-pre-commit-pass&lt;/code&gt;file, which our test script only creates if all tests pass. If the file is missing, the hook blocks the commit, forcing Claude into a “test-and-fix” loop until the build is green.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Hint Hooks: These are simple, non-blocking hooks that provide “fire-and-forget” feedback if the agent is doing something suboptimal.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We intentionally do not use “block-at-write” hooks (e.g., on &lt;code&gt;Edit&lt;/code&gt; or &lt;code&gt;Write&lt;/code&gt;). Blocking an agent mid-plan confuses or even “frustrates” it. It’s far more effective to let it finish its work and then check the final, completed result at the commit stage.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Use hooks to enforce state validation at commit time (&lt;code&gt;block-at-submit&lt;/code&gt;). Avoid blocking at write time—let the agent finish its plan, then check the final result.&lt;/p&gt;
    &lt;head rend="h2"&gt;Planning Mode&lt;/head&gt;
    &lt;p&gt;Planning is essential for any “large” feature change with an AI IDE.&lt;/p&gt;
    &lt;p&gt;For my hobby projects, I exclusively use the built-in planning mode. It’s a way to align with Claude before it starts, defining both how to build something and the “inspection checkpoints” where it needs to stop and show me its work. Using this regularly builds a strong intuition for what minimal context is needed to get a good plan without Claude botching the implementation.&lt;/p&gt;
    &lt;p&gt;In our work monorepo, we’ve started rolling out a custom planning tool built on the Claude Code SDK. Its similar to native plan mode but heavily prompted to align its outputs with our existing technical design format. It also enforces our internal best practices—from code structure to data privacy and security—out of the box. This lets our engineers “vibe plan” a new feature as if they were a senior architect (or at least that’s the pitch).&lt;/p&gt;
    &lt;p&gt;The Takeaway: Always use the built-in planning mode for complex changes to align on a plan before the agent starts working.&lt;/p&gt;
    &lt;head rend="h2"&gt;Skills&lt;/head&gt;
    &lt;p&gt;I agree with Simon Willison’s: Skills are (maybe) a bigger deal than MCP.&lt;/p&gt;
    &lt;p&gt;If you’ve been following my posts, you’ll know I’ve drifted away from MCP for most dev workflows, preferring to build simple CLIs instead (as I argued in “AI Can’t Read Your Docs”). My mental model for agent autonomy has evolved into three stages:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Single Prompt: Giving the agent all context in one massive prompt. (Brittle, doesn’t scale).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tool Calling: The “classic” agent model. We hand-craft tools and abstract away reality for the agent. (Better, but creates new abstractions and context bottlenecks).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Scripting: We give the agent access to the raw environment—binaries, scripts, and docs—and it writes code on the fly to interact with them.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With this model in mind, Agent Skills are the obvious next feature. They are the formal productization of the “Scripting” layer.&lt;/p&gt;
    &lt;p&gt;If, like me, you’ve already been favoring CLIs over MCP, you’ve been implicitly getting the benefit of Skills all along. The &lt;code&gt;SKILL.md&lt;/code&gt; file is just a more organized, shareable, and discoverable way to document these CLIs and scripts and expose them to the agent.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Skills are the right abstraction. They formalize the “scripting”-based agent model, which is more robust and flexible than the rigid, API-like model that MCP represents.&lt;/p&gt;
    &lt;head rend="h2"&gt;MCP (Model Context Protocol)&lt;/head&gt;
    &lt;p&gt;Skills don’t mean MCP is dead (see also “Everything Wrong with MCP”). Previously, many built awful, context-heavy MCPs with dozens of tools that just mirrored a REST API (&lt;code&gt;read_thing_a()&lt;/code&gt;, &lt;code&gt;read_thing_b()&lt;/code&gt;, &lt;code&gt;update_thing_c()&lt;/code&gt;). &lt;/p&gt;
    &lt;p&gt;The “Scripting” model (now formalized by Skills) is better, but it needs a secure way to access the environment. This to me is the new, more focused role for MCP.&lt;/p&gt;
    &lt;p&gt;Instead of a bloated API, an MCP should be a simple, secure gateway that provides a few powerful, high-level tools:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;download_raw_data(filters…)&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;take_sensitive_gated_action(args…)&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;execute_code_in_environment_with_state(code…)&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In this model, MCP’s job isn’t to abstract reality for the agent; its job is to manage the auth, networking, and security boundaries and then get out of the way. It provides the entry point for the agent, which then uses its scripting and &lt;code&gt;markdown&lt;/code&gt; context to do the actual work.&lt;/p&gt;
    &lt;p&gt;The only MCP I still use is for Playwright, which makes sense—it’s a complex, stateful environment. All my stateless tools (like Jira, AWS, GitHub) have been migrated to simple CLIs.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Use MCPs that act as data gateways. Give the agent one or two high-level tools (like a raw data dump API) that it can then script against.&lt;/p&gt;
    &lt;head rend="h2"&gt;Claude Code SDK&lt;/head&gt;
    &lt;p&gt;Claude Code isn’t just an interactive CLI; it’s also a powerful SDK for building entirely new agents—for both coding and non-coding tasks. I’ve started using it as my default agent framework over tools like LangChain/CrewAI for most new hobby projects.&lt;/p&gt;
    &lt;p&gt;I use it in three main ways:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Massive Parallel Scripting: For large-scale refactors, bug fixes, or migrations, I don’t use the interactive chat. I write simple bash scripts that call&lt;/p&gt;&lt;code&gt;claude -p “in /pathA change all refs from foo to bar”&lt;/code&gt;in parallel. This is far more scalable and controllable than trying to get the main agent to manage dozens of subagent tasks.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Building Internal Chat Tools: The SDK is perfect for wrapping complex processes in a simple chat interface for non-technical users. Like an installer that, on error, falls back to the Claude Code SDK to just fix the problem for the user. Or an in-house “v0-at-home” tool that lets our design team vibe-code mock frontends in our in-house UI framework, ensuring their ideas are high-fidelity and the code is more directly usable in frontend production code.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Rapid Agent Prototyping: This is my most common use. It’s not just for coding. If I have an idea for any agentic task (e.g., a “threat investigation agent” that uses custom CLIs or MCPs), I use the Claude Code SDK to quickly build and test the prototype before committing to a full, deployed scaffolding.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Takeaway: The Claude Code SDK is a powerful, general-purpose agent framework. Use it for batch-processing code, building internal tools, and rapidly prototyping new agents before you reach for more complex frameworks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Claude Code GHA&lt;/head&gt;
    &lt;p&gt;The Claude Code GitHub Action (GHA) is probably one of my favorite and most slept on features. It’s a simple concept: just run Claude Code in a GHA. But this simplicity is what makes it so powerful.&lt;/p&gt;
    &lt;p&gt;It’s similar to Cursor’s background agents or the Codex managed web UI but is far more customizable. You control the entire container and environment, giving you more access to data and, crucially, much stronger sandboxing and audit controls than any other product provides. Plus, it supports all the advanced features like Hooks and MCP.&lt;/p&gt;
    &lt;p&gt;We’ve used it to build custom “PR-from-anywhere” tooling. Users can trigger a PR from Slack, Jira, or even a CloudWatch alert, and the GHA will fix the bug or add the feature and return a fully tested PR1.&lt;/p&gt;
    &lt;p&gt;Since the GHA logs are the full agent logs, we have an ops process to regularly review these logs at a company level for common mistakes, bash errors, or unaligned engineering practices. This creates a data-driven flywheel: Bugs -&amp;gt; Improved CLAUDE.md / CLIs -&amp;gt; Better Agent.&lt;/p&gt;
    &lt;code&gt;$ query-claude-gha-logs --since 5d | claude -p “see what the other claudes were getting stuck on and fix it, then put up a PR“&lt;/code&gt;
    &lt;p&gt;The Takeaway: The GHA is the ultimate way to operationalize Claude Code. It turns it from a personal tool into a core, auditable, and self-improving part of your engineering system.&lt;/p&gt;
    &lt;head rend="h2"&gt;settings.json&lt;/head&gt;
    &lt;p&gt;Finally, I have a few specific &lt;code&gt;settings.json&lt;/code&gt; configurations that I’ve found essential for both hobby and professional work.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;HTTPS_PROXY&lt;/code&gt;/&lt;code&gt;HTTP_PROXY&lt;/code&gt;: This is great for debugging. I’ll use it to inspect the raw traffic to see exactly what prompts Claude is sending. For background agents, it’s also a powerful tool for fine-grained network sandboxing.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;MCP_TOOL_TIMEOUT&lt;/code&gt;/&lt;code&gt;BASH_MAX_TIMEOUT_MS&lt;/code&gt;: I bump these. I like running long, complex commands, and the default timeouts are often too conservative. I’m honestly not sure if this is still needed now that bash background tasks are a thing, but I keep it just in case.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;: At work, we use our enterprise API keys (via apiKeyHelper). It shifts us from a “per-seat” license to “usage-based” pricing, which is a much better model for how we work.&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;It accounts for the massive variance in developer usage (We’ve seen 1:100x differences between engineers).&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;It lets engineers to tinker with non-Claude-Code LLM scripts, all under our single enterprise account.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;“permissions”&lt;/code&gt;: I’ll occasionally self-audit the list of commands I’ve allowed Claude to auto-run.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Takeaway: Your &lt;code&gt;settings.json&lt;/code&gt; is a powerful place for advanced customization.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;That was a lot, but hopefully, you find it useful. If you’re not already using a CLI-based agent like Claude Code or Codex CLI, you probably should be. There are rarely good guides for these advanced features, so the only way to learn is to dive in.&lt;/p&gt;
    &lt;p&gt;To me, a fairly interesting philosophical question is how many reviewers should a PR get that was generated directly from a customer request (no internal human prompter)? We’ve settled on 2 human approvals for any AI-initiated PR for now, but it is kind of a weird paradigm shift (for me at least) when it’s no longer a human making something for another human to review.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45786738</guid><pubDate>Sun, 02 Nov 2025 00:13:27 +0000</pubDate></item><item><title>Backpropagation is a leaky abstraction (2016)</title><link>https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b</link><description>&lt;doc fingerprint="f70910553d224cb1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Yes you should understand backprop&lt;/head&gt;
    &lt;p&gt;When we offered CS231n (Deep Learning class) at Stanford, we intentionally designed the programming assignments to include explicit calculations involved in backpropagation on the lowest level. The students had to implement the forward and the backward pass of each layer in raw numpy. Inevitably, some students complained on the class message boards:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Why do we have to write the backward pass when frameworks in the real world, such as TensorFlow, compute them for you automatically?”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is seemingly a perfectly sensible appeal - if you’re never going to write backward passes once the class is over, why practice writing them? Are we just torturing the students for our own amusement? Some easy answers could make arguments along the lines of “it’s worth knowing what’s under the hood as an intellectual curiosity”, or perhaps “you might want to improve on the core algorithm later”, but there is a much stronger and practical argument, which I wanted to devote a whole post to:&lt;/p&gt;
    &lt;p&gt;&amp;gt; The problem with Backpropagation is that it is a leaky abstraction.&lt;/p&gt;
    &lt;p&gt;In other words, it is easy to fall into the trap of abstracting away the learning process — believing that you can simply stack arbitrary layers together and backprop will “magically make them work” on your data. So lets look at a few explicit examples where this is not the case in quite unintuitive ways.&lt;/p&gt;
    &lt;head rend="h3"&gt;Vanishing gradients on sigmoids&lt;/head&gt;
    &lt;p&gt;We’re starting off easy here. At one point it was fashionable to use sigmoid (or tanh) non-linearities in the fully connected layers. The tricky part people might not realize until they think about the backward pass is that if you are sloppy with the weight initialization or data preprocessing these non-linearities can “saturate” and entirely stop learning — your training loss will be flat and refuse to go down. For example, a fully connected layer with sigmoid non-linearity computes (using raw numpy):&lt;/p&gt;
    &lt;code&gt;z = 1/(1 + np.exp(-np.dot(W, x))) # forward pass&lt;lb/&gt;dx = np.dot(W.T, z*(1-z)) # backward pass: local gradient for x&lt;lb/&gt;dW = np.outer(z*(1-z), x) # backward pass: local gradient for W&lt;/code&gt;
    &lt;p&gt;If your weight matrix W is initialized too large, the output of the matrix multiply could have a very large range (e.g. numbers between -400 and 400), which will make all outputs in the vector z almost binary: either 1 or 0. But if that is the case, z*(1-z), which is local gradient of the sigmoid non-linearity, will in both cases become zero (“vanish”), making the gradient for both x and W be zero. The rest of the backward pass will come out all zero from this point on due to multiplication in the chain rule.&lt;/p&gt;
    &lt;p&gt;Another non-obvious fun fact about sigmoid is that its local gradient (z*(1-z)) achieves a maximum at 0.25, when z = 0.5. That means that every time the gradient signal flows through a sigmoid gate, its magnitude always diminishes by one quarter (or more). If you’re using basic SGD, this would make the lower layers of a network train much slower than the higher ones.&lt;/p&gt;
    &lt;p&gt;TLDR: if you’re using sigmoids or tanh non-linearities in your network and you understand backpropagation you should always be nervous about making sure that the initialization doesn’t cause them to be fully saturated. See a longer explanation in this CS231n lecture video.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dying ReLUs&lt;/head&gt;
    &lt;p&gt;Another fun non-linearity is the ReLU, which thresholds neurons at zero from below. The forward and backward pass for a fully connected layer that uses ReLU would at the core include:&lt;/p&gt;
    &lt;code&gt;z = np.maximum(0, np.dot(W, x)) # forward pass&lt;lb/&gt;dW = np.outer(z &amp;gt; 0, x) # backward pass: local gradient for W&lt;/code&gt;
    &lt;p&gt;If you stare at this for a while you’ll see that if a neuron gets clamped to zero in the forward pass (i.e. z=0, it doesn’t “fire”), then its weights will get zero gradient. This can lead to what is called the “dead ReLU” problem, where if a ReLU neuron is unfortunately initialized such that it never fires, or if a neuron’s weights ever get knocked off with a large update during training into this regime, then this neuron will remain permanently dead. It’s like permanent, irrecoverable brain damage. Sometimes you can forward the entire training set through a trained network and find that a large fraction (e.g. 40%) of your neurons were zero the entire time.&lt;/p&gt;
    &lt;p&gt;TLDR: If you understand backpropagation and your network has ReLUs, you’re always nervous about dead ReLUs. These are neurons that never turn on for any example in your entire training set, and will remain permanently dead. Neurons can also die during training, usually as a symptom of aggressive learning rates. See a longer explanation in CS231n lecture video.&lt;/p&gt;
    &lt;head rend="h3"&gt;Exploding gradients in RNNs&lt;/head&gt;
    &lt;p&gt;Vanilla RNNs feature another good example of unintuitive effects of backpropagation. I’ll copy paste a slide from CS231n that has a simplified RNN that does not take any input x, and only computes the recurrence on the hidden state (equivalently, the input x could always be zero):&lt;/p&gt;
    &lt;p&gt;This RNN is unrolled for T time steps. When you stare at what the backward pass is doing, you’ll see that the gradient signal going backwards in time through all the hidden states is always being multiplied by the same matrix (the recurrence matrix Whh), interspersed with non-linearity backprop.&lt;/p&gt;
    &lt;p&gt;What happens when you take one number a and start multiplying it by some other number b (i.e. a*b*b*b*b*b*b…)? This sequence either goes to zero if |b| &amp;lt; 1, or explodes to infinity when |b|&amp;gt;1. The same thing happens in the backward pass of an RNN, except b is a matrix and not just a number, so we have to reason about its largest eigenvalue instead.&lt;/p&gt;
    &lt;p&gt;TLDR: If you understand backpropagation and you’re using RNNs you are nervous about having to do gradient clipping, or you prefer to use an LSTM. See a longer explanation in this CS231n lecture video.&lt;/p&gt;
    &lt;head rend="h3"&gt;Spotted in the Wild: DQN Clipping&lt;/head&gt;
    &lt;p&gt;Lets look at one more — the one that actually inspired this post. Yesterday I was browsing for a Deep Q Learning implementation in TensorFlow (to see how others deal with computing the numpy equivalent of Q[:, a], where a is an integer vector — turns out this trivial operation is not supported in TF). Anyway, I searched “dqn tensorflow”, clicked the first link, and found the core code. Here is an excerpt:&lt;/p&gt;
    &lt;p&gt;If you’re familiar with DQN, you can see that there is the target_q_t, which is just [reward * \gamma \argmax_a Q(s’,a)], and then there is q_acted, which is Q(s,a) of the action that was taken. The authors here subtract the two into variable delta, which they then want to minimize on line 295 with the L2 loss with tf.reduce_mean(tf.square()). So far so good.&lt;/p&gt;
    &lt;p&gt;The problem is on line 291. The authors are trying to be robust to outliers, so if the delta is too large, they clip it with tf.clip_by_value. This is well-intentioned and looks sensible from the perspective of the forward pass, but it introduces a major bug if you think about the backward pass.&lt;/p&gt;
    &lt;p&gt;The clip_by_value function has a local gradient of zero outside of the range min_delta to max_delta, so whenever the delta is above min/max_delta, the gradient becomes exactly zero during backprop. The authors are clipping the raw Q delta, when they are likely trying to clip the gradient for added robustness. In that case the correct thing to do is to use the Huber loss in place of tf.square:&lt;/p&gt;
    &lt;code&gt;def clipped_error(x): &lt;lb/&gt;  return tf.select(tf.abs(x) &amp;lt; 1.0, &lt;lb/&gt;                   0.5 * tf.square(x), &lt;lb/&gt;                   tf.abs(x) - 0.5) # condition, true, false&lt;/code&gt;
    &lt;p&gt;It’s a bit gross in TensorFlow because all we want to do is clip the gradient if it is above a threshold, but since we can’t meddle with the gradients directly we have to do it in this round-about way of defining the Huber loss. In Torch this would be much more simple.&lt;/p&gt;
    &lt;p&gt;I submitted an issue on the DQN repo and this was promptly fixed.&lt;/p&gt;
    &lt;head rend="h3"&gt;In conclusion&lt;/head&gt;
    &lt;p&gt;Backpropagation is a leaky abstraction; it is a credit assignment scheme with non-trivial consequences. If you try to ignore how it works under the hood because “TensorFlow automagically makes my networks learn”, you will not be ready to wrestle with the dangers it presents, and you will be much less effective at building and debugging neural networks.&lt;/p&gt;
    &lt;p&gt;The good news is that backpropagation is not that difficult to understand, if presented properly. I have relatively strong feelings on this topic because it seems to me that 95% of backpropagation materials out there present it all wrong, filling pages with mechanical math. Instead, I would recommend the CS231n lecture on backprop which emphasizes intuition (yay for shameless self-advertising). And if you can spare the time, as a bonus, work through the CS231n assignments, which get you to write backprop manually and help you solidify your understanding.&lt;/p&gt;
    &lt;p&gt;That’s it for now! I hope you’ll be much more suspicious of backpropagation going forward and think carefully through what the backward pass is doing. Also, I’m aware that this post has (unintentionally!) turned into several CS231n ads. Apologies for that :)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45787993</guid><pubDate>Sun, 02 Nov 2025 05:20:12 +0000</pubDate></item><item><title>Notes by djb on using Fil-C</title><link>https://cr.yp.to/2025/fil-c.html</link><description>&lt;doc fingerprint="afe1387bca053cc2"&gt;
  &lt;main&gt;&lt;p&gt;I'm impressed with the level of compatibility of the new memory-safe C/C++ compiler Fil-C (filcc, fil++). Many libraries and applications that I've tried work under Fil-C without changes, and the exceptions haven't been hard to get working.&lt;/p&gt;&lt;p&gt;I've started accumulating miscellaneous notes on this page regarding usage of Fil-C. My selfish objective here is to protect various machines that I manage by switching them over to code compiled with Fil-C, but maybe you'll find something useful here too.&lt;/p&gt;&lt;p&gt;Timings below are from a mini-PC named phoenix except where otherwise mentioned. This mini-PC has a 6-core (12-thread) AMD Ryzen 5 7640HS (Zen 4) CPU, 12GB RAM, and 36GB swap. The OS is Debian 13. (I normally run LTS software, periodically upgrading from software that's 4â5 years old such as Debian 11 today to software that's 2â3 years old such as Debian 12 today; but some of the packages included in Fil-C expect newer utilities to be available.)&lt;/p&gt;&lt;p&gt;Related:&lt;/p&gt;&lt;p&gt;Another way to run Fil-C is via Filnix from Mikael Brockman. For example, an unprivileged user under Debian 12 with about 10GB of free disk space can download, compile, and install Fil-C, and run a Fil-C-compiled Nethack, as follows:&lt;/p&gt;&lt;code&gt;unshare --user --pid echo YES # just to test
git clone https://github.com/nix-community/nix-user-chroot
cd nix-user-chroot
cargo build --release
mkdir -m 0755 ~/nix
~/nix-user-chroot/target/release/nix-user-chroot ~/nix \
  bash -c 'curl -L https://nixos.org/nix/install | sh'
env TERM=vt102 \
  ~/nix-user-chroot/target/release/nix-user-chroot ~/nix \
  ~/nix/store/*-nix-2*/bin/nix \
  --extra-experimental-features 'nix-command flakes' \
  run 'github:mbrock/filnix#nethack'
&lt;/code&gt;&lt;p&gt;Current recommendations for things to do at the beginning as root:&lt;/p&gt;&lt;code&gt;mkdir -p /var/empty
apt install \
  autoconf-dickey build-essential bison clang cmake flex gawk \
  gettext ninja-build patchelf quilt ruby texinfo time
&lt;/code&gt;&lt;p&gt;I created an unprivileged filc user. Everything else is as that user.&lt;/p&gt;&lt;p&gt;I downloaded the Fil-C source package:&lt;/p&gt;&lt;code&gt;git clone https://github.com/pizlonator/fil-c.git
cd fil-c
&lt;/code&gt;&lt;p&gt;This isn't just the compiler; there's also glibc and quite a few higher-level libraries and applications. There are also binary Fil-C packages, but I've worked primarily with the source package at this point.&lt;/p&gt;&lt;p&gt;I compiled Fil-C and glibc:&lt;/p&gt;&lt;code&gt;time ./build_all_fast_glibc.sh
&lt;/code&gt;&lt;p&gt;There are also options to use musl instead of glibc, but musl is incompatible with some of the packages shipped with Fil-C: attr needs basename, elfutils needs argp_parse, sed's test suite needs the glibc variant of calloc, and vim's build needs iconv to be able to convert from CP932 to UTF-8.&lt;/p&gt;&lt;p&gt;I had originally configured the server phoenix with only 12GB swap. I then had to restart ./build_all_fast_glibc.sh a few times because the Fil-C compilation ran out of memory. Switching to 36GB swap made everything work with no restarts; monitoring showed that almost 19GB swap (plus 12GB RAM) was used at one point. A larger server, 128 cores with 512GB RAM, took 8 minutes for Fil-C plus 6 minutes for musl, with no restarts needed.&lt;/p&gt;&lt;p&gt;Fil-C includes a ./build_all_slow.sh that builds many more libraries and applications (sometimes with patches from the Fil-C author). I wrote a replacement script https://cr.yp.to/2025/build-parallel-20251023.py with the following differences:&lt;/p&gt;&lt;p&gt;On phoenix, running time PATH="$HOME/bin:$HOME/fil-c/build/bin:$HOME/fil-c/pizfix/bin:$PATH" ./build-parallel.py went through 61 targets in 101 minutes real time (467 minutes user time, 55 minutes system time), successfully compiling 60 of them.&lt;/p&gt;&lt;p&gt;libcap. This is the one that didn't compile: /home/filc/fil-c/pizfix/bin/ld: /usr/libexec/gcc/x86_64-linux-gnu/14/liblto_plugin.so: error loading plugin: libc.so.6: cannot open shared object file: No such file or directory&lt;/p&gt;&lt;p&gt;util-linux. I skipped this one. It does compile, but the compiled taskset utility needs to be patched to use sched_getaffinity and sched_setaffinity as library functions rather than via syscall, or Fil-C needs to be patched for those syscalls. This is an issue for build-parallel since build-parallel relies on taskset; maybe build-parallel should instead use Python's affinity functions.&lt;/p&gt;&lt;p&gt;attr, bash, benchmarks, binutils, bison, brotli, bzip2, bzip3, check, cmake, coreutils, cpython, curl, dash, diffutils, elfutils, emacs, expat, ffi, gettext, git, gmp, grep, icu, jpeg-6b, libarchive, libcap, libedit, libevent, libpipeline, libuev, libuv, lua, lz4, m4, make, mg, ncurses, nghttp2, openssh, openssl, pcre2, pcre, perl, pkgconf, procps, quickjs, sed, shadow, simdutf, sqlite, tcl, tmux, toybox, vim, wg14_signals, xml_parser, xz, zlib, zsh, zstd. No problems encountered so far (given whatever patches were already applied from the Fil-C author!). The benchmarks package is supplied with Fil-C and does a few miscellaneous measurements.&lt;/p&gt;&lt;p&gt;I did export PATH="$HOME/bin:$HOME/fil-c/build/bin:$HOME/fil-c/pizfix/bin:$PATH" before these.&lt;/p&gt;&lt;p&gt;boost 1.89.0: Seems to mostly work. Most of the package is header-only; a few simple tests worked fine.&lt;/p&gt;&lt;p&gt;I also looked a bit at the compiled parts. Running ./bootstrap.sh --with-toolset=clang --prefix=$HOME ran into vfork, which Fil-C doesn't support, but editing tools/build/src/engine/execunix.cpp to use defined(__APPLE__) || defined(__FILC__) for the no-fork test got past this.&lt;/p&gt;&lt;p&gt;Running ./b2 install --prefix=$HOME toolset=clang address-model=64 architecture=x86_64 binary-format=elf produced an error message since I should have said x86 instead of x86_64; Fil-C said it caught a safety issue in the b2 program after the error message: filc safety error: argument size mismatch (actual = 8, expected = 16). I didn't compile with debugging so Fil-C didn't say where this is in b2.&lt;/p&gt;&lt;p&gt;cdb-20251021: Seems to work. One regression test, an artificial out-of-memory regression test, currently produces a different error message with Fil-C: filc panic: src/libpas/pas_compact_heap_reservation.c:65: pas_aligned_allocation_result pas_compact_heap_reservation_try_allocate(size_t, size_t): assertion page_result.result failed.&lt;/p&gt;&lt;p&gt;libcpucycles-20250925: Seems to work. I commented out the first three lines of cpucycles/options.&lt;/p&gt;&lt;p&gt;libgc: I replaced this with a small gcshim package (https://cr.yp.to/2025/gcshim-20251022.tar.gz) that simply calls malloc etc. So far this seems to be an adequate replacement. (Fil-C includes a garbage collector.)&lt;/p&gt;&lt;p&gt;libntruprime-20241021: Seems to work after a few tweaks but I didn't collect full notes yet. chmod +t crypto_hashblocks/sha512/avx2 disables assembly and makes things compile; configured with --no-valgrind since Fil-C doesn't support valgrind; did a bit more tweaking to make cpuid work.&lt;/p&gt;&lt;p&gt;lpeg-1.1.0: Compiles, maybe works (depends on lua, dependency of neovim):&lt;/p&gt;&lt;code&gt;cd
PREFIX=$(dirname $(dirname $(which lua)))
wget https://www.inf.puc-rio.br/~roberto/lpeg/lpeg-1.1.0.tar.gz
tar -xf lpeg-1.1.0.tar.gz
cd lpeg-1.1.0
make CC=`which filcc` DLLFLAGS='-shared -fPIC' test
cp lpeg.so $PREFIX/lib
&lt;/code&gt;&lt;p&gt;luv-1.51.0: Compiles, maybe works (depends on lua, dependency of neovim):&lt;/p&gt;&lt;code&gt;cd
PREFIX=$(dirname $(dirname $(which lua)))
wget https://github.com/luvit/luv/releases/download/1.51.0-1/luv-1.51.0-1.tar.gz
tar -xf luv-1.51.0-1.tar.gz
cd luv-1.51.0-1
mkdir build
cd build
LUA_DIR=$HOME/fil-c/projects/lua-5.4.7
# lua install should probably do this:
cp $LUA_DIR/lua.h $PREFIX/include/
cp $LUA_DIR/lauxlib.h $PREFIX/include/
cp $LUA_DIR/luaconf.h $PREFIX/include/
cp $LUA_DIR/lualib.h $PREFIX/include/
# and then:
cmake -DCMAKE_C_COMPILER=`which filcc` -DCMAKE_INSTALL_PREFIX=$PREFIX -DWITH_LUA_ENGINE=Lua -DLUA_DIR=$HOME/fil-c/projects/lua-5.4.7/ ..
make test
make install
&lt;/code&gt;&lt;p&gt;mutt-2-2-15-rel (depends on ncurses):&lt;/p&gt;&lt;code&gt;wget https://github.com/muttmua/mutt/archive/refs/tags/mutt-2-2-15-rel.tar.gz
tar -xf mutt-2-2-15-rel.tar.gz
cd mutt-mutt-2-2-15-rel
CC=`which clang` ./prepare --prefix=$HOME/fil-c/pizfix --with-homespool
make -j12 install
&lt;/code&gt;
Seems to work, at least for reading email.


&lt;p&gt;tig (depends on ncurses and maybe more):&lt;/p&gt;&lt;code&gt;wget https://github.com/jonas/tig/releases/download/tig-2.6.0/tig-2.6.0.tar.gz
tar -xf tig-2.6.0.tar.gz
cd tig-2.6.0
CC=`which filcc` ./configure --prefix=$(dirname $(dirname $(which git)))
make -j12
make test
make -j12 install
&lt;/code&gt;
Seems to work, at least for viewing the Fil-C repo.


&lt;p&gt;w3m (depends on gcshim and ncurses): Seems to work. I tried the Debian version: git clone https://salsa.debian.org/debian/w3m.git. I used CFLAGS=-Wno-incompatible-function-pointer-types (which is probably needed for clang anyway even without Fil-C).&lt;/p&gt;&lt;p&gt;I've built and installed some replacement Debian packages using Fil-C as the compiler on a Debian 13 machine, as explained below. Hopefully this can rapidly scale to many packages, taking advantage of the basic compile-install-test knowledge already built into Debian source packages, although some packages will take more work because they need extra patches to work with Fil-C.&lt;/p&gt;&lt;p&gt;Structure. Debian already understands how to have packages for multiple architectures (ABIs; Debian "ports") installed at once. For example, dpkg --add-architecture i386; apt update; apt install bash:i386 installs a 32-bit version of bash, replacing the usual 64-bit version; you can do apt install bash:amd64 to revert to the 64-bit version. Meanwhile the 32-bit libraries and 64-bit libraries are installed in separate locations, basically /lib/i386-linux-gnu or /usr/lib/i386-linux-gnu vs. /lib/x86_64-linux-gnu or /usr/lib/x86_64-linux-gnu. (On Debian 11 and newer, and on Ubuntu 22.04 and newer, /lib is symlinked to /usr/lib.)&lt;/p&gt;&lt;p&gt;I'm following this model for plugging Fil-C into Debian: the goal is for apt install bash:amd64fil0 to install a Fil-C-compiled (amd64fil0) version of bash, replacing the usual (amd64) version of bash, while the amd64 and amd64fil0 libraries are installed in separate locations.&lt;/p&gt;&lt;p&gt;The include-file complication. Debian expects library packages compiled for multiple ABIs to all provide the same include files: for example, /usr/include/ncurses.h is provided by libncurses-dev:i386, libncurses-dev:amd64, etc. This is safe because Debian forces libncurses-dev:i386 and libncurses-dev:amd64 and so on to all have the same version. An occasional package with ABI-dependent include files can still use /usr/include/x86_64-linux-gnu etc.&lt;/p&gt;&lt;p&gt;Fil-C instead omits /usr/include in favor of a Fil-C-specific directory (which will typically be different from /usr/include: even if Fil-C is compiled with glibc, probably the glibc version won't be the same as in /usr/include). This difference is the top source of messiness below. I'm planning to tweak the Fil-C driver to use /usr/include on Debian. [This is done in the filian-install-compiler script.]&lt;/p&gt;&lt;p&gt;Something else I'm planning to tweak is Fil-C's glibc compilation, so that it uses the final system prefix. [This is also done in the filian-install-compiler script.] The approach described below instead requires /home/filian/fil-c to stay in place for compiling and running programs.&lt;/p&gt;&lt;p&gt;Building Debian packages. How does Debian package building work? First, more packages to install as root:&lt;/p&gt;&lt;code&gt;apt install dpkg-dev devscripts docbook2x \
  dh-exec dh-python python3-setuptools fakeroot \
  sbuild mmdebstrap uidmap piuparts
&lt;/code&gt;
&lt;p&gt;Debian has multiple options for building a package. The option that has the best isolation, and that Debian uses to continually build new packages for distribution, is sbuild, but for fast development I'll focus on directly using the lower-level dpkg-buildpackage.&lt;/p&gt;&lt;p&gt;Baseline 1: using sbuild without Fil-C. In case you do want to try sbuild, here's the basic setup, and then an example of building a small package (tinycdb):&lt;/p&gt;&lt;code&gt;mkdir -p ~/shared/sbuild
time mmdebstrap --include=ca-certificates --skip=output/dev --variant=buildd unstable ~/shared/sbuild/unstable-amd64.tar.zst https://deb.debian.org/debian

mkdir -p ~/.config/sbuild
cat &amp;lt;&amp;lt; "EOF" &amp;gt; ~/.config/sbuild/config.pl
$chroot_mode = 'unshare';
$external_commands = { "build-failed-commands" =&amp;gt; [ [ '%SBUILD_SHELL' ] ] };
$build_arch_all = 1;
$build_source = 1;
$source_only_changes = 1;
$run_lintian = 1;
$lintian_opts = ['--display-info', '--verbose', '--fail-on', 'error,warning', '--info'];
$run_autopkgtest = 1;
$run_piuparts = 1;
$piuparts_opts = ['--no-eatmydata', '--distribution=%r', '--fake-essential-packages=systemd-sysv'];
EOF

mkdir -p ~/shared/packages
cd ~/shared/packages
apt source tinycdb
cd tinycdb-*/
time sbuild
&lt;/code&gt;

&lt;p&gt;Baseline 2: using dpkg-buildpackage without Fil-C. Here's what it looks like compiling the same small package with dpkg-buildpackage:&lt;/p&gt;&lt;code&gt;mkdir -p ~/shared/packages
cd ~/shared/packages
apt source tinycdb
cd tinycdb-*/
time dpkg-buildpackage -us -uc -b
&lt;/code&gt;

&lt;p&gt;The goal: Using dpkg-buildpackage with Fil-C. As root, teach dpkg basic features of the new architecture, imitating the current line amd64 x86_64 (amd64|x86_64) 64 little in the same file:&lt;/p&gt;&lt;code&gt;echo amd64fil0 x86_64+fil0 amd64fil0 64 little &amp;gt;&amp;gt; /usr/share/dpkg/cputable
&lt;/code&gt;

&lt;p&gt;Also, allow apt to install packages compiled for this architecture (beware that this will also later make apt update look for that architecture on servers, and whimper a bit for not finding it, but nothing breaks):&lt;/p&gt;&lt;code&gt;dpkg --add-architecture amd64fil0
&lt;/code&gt;
&lt;p&gt;Also, teach autoconf to accept amd64fil0 (the third of these lines is what's critical for Debian builds):&lt;/p&gt;&lt;code&gt;sed -i '/| x86_64 / a| x86_64+fil0 \\' /usr/share/autoconf/build-aux/config.sub
sed -i '/| x86_64 / a| x86_64+fil0 \\' /usr/share/libtool/build-aux/config.sub
sed -i '/| x86_64 / a| x86_64+fil0 \\' /usr/share/misc/config.sub
&lt;/code&gt;
&lt;p&gt;[Not necessary if you've used filian-install-compiler:] As a filian user, compile Fil-C and its standard library:&lt;/p&gt;&lt;code&gt;cd
git clone https://github.com/pizlonator/fil-c.git
cd fil-c
time ./build_all_fast_glibc.sh
&lt;/code&gt;
&lt;p&gt;[Not necessary if you've used filian-install-compiler:] As root, copy Fil-C and its standard library into system locations:&lt;/p&gt;&lt;code&gt;mkdir -p /usr/libexec/fil/amd64/compiler
time cp -r /home/filian/fil-c/pizfix /usr/libexec/fil/amd64/
rm -rf /usr/lib/x86_64+fil0-linux-gnu
mv /usr/libexec/fil/amd64/pizfix/lib /usr/lib/x86_64+fil0-linux-gnu
ln -s /usr/lib/x86_64+fil0-linux-gnu /usr/libexec/fil/amd64/pizfix/lib
rm -rf /usr/include/x86_64+fil0-linux-gnu
mv /usr/libexec/fil/amd64/pizfix/include /usr/include/x86_64+fil0-linux-gnu
ln -s /usr/include/x86_64+fil0-linux-gnu /usr/libexec/fil/amd64/pizfix/include
time cp -r /home/filian/fil-c/build/bin /usr/libexec/fil/amd64/compiler/
time cp -r /home/filian/fil-c/build/include /usr/libexec/fil/amd64/compiler/
time cp -r /home/filian/fil-c/build/lib /usr/libexec/fil/amd64/compiler/
( echo '#!/bin/sh'
  echo 'exec /usr/libexec/fil/amd64/compiler/bin/filcc "$@"' ) &amp;gt; /usr/bin/x86_64+fil0-linux-gnu-gcc
chmod 755 /usr/bin/x86_64+fil0-linux-gnu-gcc
( echo '#!/bin/sh'
  echo 'exec /usr/libexec/fil/amd64/compiler/bin/fil++ "$@"' ) &amp;gt; /usr/bin/x86_64+fil0-linux-gnu-g++
chmod 755 /usr/bin/x86_64+fil0-linux-gnu-g++
ln -s /usr/libexec/fil/amd64/compiler/bin/llvm-objdump /usr/bin/x86_64+fil0-linux-gnu-objdump
ln -s x86_64+fil0-linux-gnu-gcc /usr/bin/filcc
ln -s x86_64+fil0-linux-gnu-g++ /usr/bin/fil++
&lt;/code&gt;
&lt;p&gt;Now, as user filian (or whichever other user), let's make a little helper script to adjust a Debian source package:&lt;/p&gt;&lt;code&gt;mkdir -p $HOME/bin
( echo '#!/bin/sh'
  echo 'sed -i '\''s/^ \([^"]*\)$/ pizlonated_\1/'\'' debian/*.symbols'
  echo 'find . -name '\''*.map'\'' | while read fn'
  echo 'do'
  echo '  awk '\''{'
  echo '    if ($1 == "local:") global = 0'
  echo '    if ($1 == "}") global = 0'
  echo '    if (global &amp;amp;&amp;amp; NF &amp;gt; 0 &amp;amp;&amp;amp; !index($0,"c++")) $1 = "pizlonated_"$1'
  echo '    if ($1 == "global:") global = 1'
  echo '    print'
  echo '  }'\'' &amp;lt; $fn &amp;gt; $fn.tmp'
  echo '  mv $fn.tmp $fn'
  echo 'done'
  echo 'find debian -name '\''*.install'\'' | while read fn'
  echo 'do'
  echo '  awk '\''{'
  echo '    if (NF == 2 &amp;amp;&amp;amp; $2 == "usr/include") $2 = $2"/${DEB_HOST_MULTIARCH}"'
  echo '    if (NF == 1 &amp;amp;&amp;amp; $1 == "usr/include") { $2 = $1"/${DEB_HOST_MULTIARCH}"; $1 = $1"/*" }'
  echo '    print'
  echo '  }'\'' &amp;lt; $fn &amp;gt; $fn.tmp'
  echo '  mv $fn.tmp $fn'
  echo 'done'
) &amp;gt; $HOME/bin/fillet
chmod 755 $HOME/bin/fillet
&lt;/code&gt;
And now let's try building a small package:

&lt;code&gt;mkdir -p ~/shared/packages
cd ~/shared/packages
apt source tinycdb
cd tinycdb-*/
$HOME/bin/fillet
time env DPKG_GENSYMBOLS_CHECK_LEVEL=0 \
  DEB_BUILD_OPTIONS='crossbuildcanrunhostbinaries nostrip' \
  dpkg-buildpackage -d -us -uc -b -a amd64fil0
&lt;/code&gt;

&lt;p&gt;Explanation of the differences from a normal build:&lt;/p&gt;&lt;p&gt;For me this worked and produced three ../*.deb packages. Installing them as root also worked:&lt;/p&gt;&lt;code&gt;apt install /home/filian/shared/packages/*.deb
# some sanity checks:
apt list | grep tinycdb
# prints "tinycdb/stable 0.81-2 amd64" (available package)
# and prints "tinycdb/now 0.81-2 amd64fil0 [installed,local]"
dpkg -L tinycdb:amd64fil0
# lists various files such as /usr/bin/cdb
nm /usr/bin/cdb
# shows various symbols including "pizlonated" (Fil-C) symbols
ldd /usr/bin/cdb
# shows dependence on libraries in /usr/libexec/fil
/usr/bin/cdb -h
# prints a help message: "cdb: Constant DataBase" etc.
&lt;/code&gt;
&lt;p&gt;Compiling a deliberately wrong test program with the newly installed library also works, and triggers Fil-C's run-time protection:&lt;/p&gt;&lt;code&gt;cd /root
( echo '#include &amp;lt;cdb.h&amp;gt;'
  echo 'int main() { cdb_init(0,0); return 0; }' ) &amp;gt; usecdb.c
filcc -o usecdb usecdb.c -lcdb
./usecdb &amp;lt; /bin/bash
# ... "filc panic: thwarted a futile attempt to violate memory safety."
&lt;/code&gt;

&lt;p&gt;libc-dev. Some packages depend on libc-dev, so let's build a fake libc-dev package (probably there's an easier way to do this):&lt;/p&gt;&lt;code&gt;FAKEPACKAGE=libc-dev
mkdir -p ~/shared/packages/$FAKEPACKAGE/debian
cd ~/shared/packages/$FAKEPACKAGE
( echo $FAKEPACKAGE' (0.0) unstable; urgency=medium'
  echo ''
  echo '  * Initial Release.'
  echo ''
  echo ' -- djb &amp;lt;djb@cr.yp.to&amp;gt;  Sun, 26 Oct 2025 16:05:17 +0000'
) &amp;gt; debian/changelog
( echo 'Source: '$FAKEPACKAGE
  echo 'Build-Depends: debhelper-compat (= 13)'
  echo 'Maintainer: djb &lt;/code&gt;
&lt;p&gt;ncurses.&lt;/p&gt;&lt;code&gt;mkdir -p ~/shared/packages
cd ~/shared/packages
apt source ncurses
cd ncurses-*/
$HOME/bin/fillet
time env DPKG_GENSYMBOLS_CHECK_LEVEL=0 \
  DEB_BUILD_OPTIONS='crossbuildcanrunhostbinaries nostrip' \
  dpkg-buildpackage -d -us -uc -b -a amd64fil0
rm ../ncurses-*deb # apt won't let us touch the binaries
&lt;/code&gt;
&lt;p&gt;As root, install the above libraries:&lt;/p&gt;&lt;code&gt;apt install /home/filian/shared/packages/lib*.deb
&lt;/code&gt;
&lt;p&gt;libmd. Seems to work. At first this didn't install since the compiled version (for amd64fil0) was 1.1.0-2 while the installed version (for amd64) was 1.1.0-2+b1. Debian requires the same version number across architectures (see above regarding include-file compatibility), so apt said that 1.1.0-2+b1 breaks 1.1.0-2. I resolved this by compiling and installing 1.1.0-2 for both amd64 and amd64fil0. This is a downgrade since "+b" refers to a "binNMU", a "binary-only non-maintainer upload", a patch beyond the official source; I don't know what the patch is.&lt;/p&gt;&lt;p&gt;readline. Needs ln -s /usr/include/readline /usr/include/x86_64+fil0-linux-gnu/readline after installation. Could have tweaks in debian/rules (which seems to predate *.install), but this is in any case an example of the messiness that I'm planning to get rid of.&lt;/p&gt;&lt;p&gt;lua5.4. Seems to work. Depends on readline.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45788040</guid><pubDate>Sun, 02 Nov 2025 05:32:02 +0000</pubDate></item><item><title>Using FreeBSD to make self-hosting fun again</title><link>https://jsteuernagel.de/posts/using-freebsd-to-make-self-hosting-fun-again/</link><description>&lt;doc fingerprint="3f1431577be6acd1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Using FreeBSD to make self-hosting fun again&lt;/head&gt;
    &lt;p&gt;2025-11-01 - Feeling like a kid in a candy store, once more&lt;/p&gt;
    &lt;p&gt;As evident by my last blog post "A prison of my own making", I needed to change something about my relationship with technology. How I was doing things didn't work anymore, but I also felt unable to change anything about it, as the way I was doing things seemed like the way that I was supposed to use.&lt;/p&gt;
    &lt;p&gt;What I needed was a fresh start. And I managed to find that fresh start in the BSD family of operating systems.&lt;/p&gt;
    &lt;p&gt;I had already given FreeBSD and OpenBSD a try at the time and I liked what I saw. OpenBSD had already established itself in my workflow as an easy to use and reliable router and general OS for single-purpose VMs. But it isn't able to fullfill my needs for a multi-purpose system, where I'd want to run multiple separated workloads in something like a container or VM. But FreeBSD could.&lt;/p&gt;
    &lt;p&gt;I know that I generally operate best by just committing to using a thing and then figuring out what I need, as I need it. So I committed to using FreeBSD and found a really nice server to do just that on the Hetzner server auction.&lt;/p&gt;
    &lt;p&gt;I started setting it up with BastilleBSD for jails and vm-bhyve for VMs. I didn't know how to do most things and felt kinda lost. But there it was again, that feeling of excitement to learn something new, which got my into self-hosting in the first place.&lt;/p&gt;
    &lt;p&gt;After some trial and error I managed to find a setup that works for me. As per usual, it deviates a bit from what might be the most common setup, but it's undoubtedly me (I'll probably explain more about it in the future, when things have settled).&lt;/p&gt;
    &lt;p&gt;What I've come to appreciate about FreeBSD, and the BSD operating systems in general, is their simplicity and good documentation. Most tasks are just a few commands to run via SSH and if that isn't the case, someone has probably written a decent wrapper around it. If I need to find a piece of information, I still instinctively search online for it, just to be greeted by an online version of the corresponding man page. So I could also have just gathered that information on the CLI, oh well.&lt;/p&gt;
    &lt;p&gt;I also love the focus on long-term compatibility. I can find a solution to a problem in a forum post from 2008 and not even for a second do I have to doubt whether it will work, because it always does. At the same time, that doesn't mean there are no new features. The system doesn't feel old.&lt;/p&gt;
    &lt;p&gt;Sure, not everything was all roses and some of that was probably due to my way of just jumping into a problem and digging myself through it one step at a time, instead of reading up on it a lot beforehand. For example I was confused for a long time about the release cycle of the base system and whether that somehow related to pkg and ports (It does not). And I was not able to properly phrase the question in a way that would result in a helpful result while searching. Luckily the BSD community has been nothing but kind and helpful so far. I've had multiple people on the Fediverse offer their help and when I had a specific question, I would always get multiple solid answers explaining it to me. Thanks to everyone that replied, it's genuinely a blast to feel like a newbie again!&lt;/p&gt;
    &lt;p&gt;I don't know whether I will actually stick with all of what I'm doing right now, in the long term. But that's not important. What is important is that I'm having fun, learning a new thing, right now. I'll see what sticks long-term.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;@Joel: See? I wrote a blog post! :D&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45789424</guid><pubDate>Sun, 02 Nov 2025 11:01:23 +0000</pubDate></item><item><title>URLs are state containers</title><link>https://alfy.blog/2025/10/31/your-url-is-your-state.html</link><description>&lt;doc fingerprint="abc21e28a1a26d89"&gt;
  &lt;main&gt;
    &lt;p&gt;Couple of weeks ago when I was publishing The Hidden Cost of URL Design I needed to add SQL syntax highlighting. I headed to PrismJS website trying to remember if it should be added as a plugin or what. I was overwhelmed with the amount of options in the download page so I headed back to my code. I checked the file for PrismJS and at the top of the file, I found a comment containing a URL:&lt;/p&gt;
    &lt;code&gt;/* https://prismjs.com/download.html#themes=prism&amp;amp;languages=markup+css+clike+javascript+bash+css-extras+markdown+scss+sql&amp;amp;plugins=line-highlight+line-numbers+autolinker */
&lt;/code&gt;
    &lt;p&gt;I had completely forgotten about this. I clicked the URL, and it was the PrismJS download page with every checkbox, dropdown, and option pre-selected to match my exact configuration. Themes chosen. Languages selected. Plugins enabled. Everything, perfectly reconstructed from that single URL.&lt;/p&gt;
    &lt;p&gt;It was one of those moments where something you once knew suddenly clicks again with fresh significance. Here was a URL doing far more than just pointing to a page. It was storing state, encoding intent, and making my entire setup shareable and recoverable. No database. No cookies. No localStorage. Just a URL.&lt;/p&gt;
    &lt;p&gt;This got me thinking: how often do we, as frontend engineers, overlook the URL as a state management tool? We reach for all sorts of abstractions to manage state such as global stores, contexts, and caches while ignoring one of the webâs most elegant and oldest features: the humble URL.&lt;/p&gt;
    &lt;p&gt;In my previous article, I wrote about the hidden costs of bad URL design. Today, I want to flip that perspective and talk about the immense value of good URL design. Specifically, how URLs can be treated as first-class state containers in modern web applications.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Overlooked Power of URLs&lt;/head&gt;
    &lt;p&gt;Scott Hanselman famously said âURLs are UIâ and heâs absolutely right. URLs arenât just technical addresses that browsers use to fetch resources. Theyâre interfaces. Theyâre part of the user experience.&lt;/p&gt;
    &lt;p&gt;But URLs are more than UI. Theyâre state containers. Every time you craft a URL, youâre making decisions about what information to preserve, what to make shareable, and what to make bookmarkable.&lt;/p&gt;
    &lt;p&gt;Think about what URLs give us for free:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Shareability: Send someone a link, and they see exactly what you see&lt;/item&gt;
      &lt;item&gt;Bookmarkability: Save a URL, and youâve saved a moment in time&lt;/item&gt;
      &lt;item&gt;Browser history: The back button just works&lt;/item&gt;
      &lt;item&gt;Deep linking: Jump directly into a specific application state&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;URLs make web applications resilient and predictable. Theyâre the webâs original state management solution, and theyâve been working reliably since 1991. The question isnât whether URLs can store state. Itâs whether weâre using them to their full potential.&lt;/p&gt;
    &lt;p&gt;Before we dive into examples, letâs break down how URLs encode state. Hereâs a typical stateful URL:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;For many years, these were considered the only components of a URL. That changed with the introduction of Text Fragments, a feature that allows linking directly to a specific piece of text within a page. You can read more about it in my article Smarter than âCtrl+Fâ: Linking Directly to Web Page Content.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Different parts of the URL encode different types of state:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Path Segments (&lt;code&gt;/path/to/myfile.html&lt;/code&gt;). Best used for hierarchical resource navigation:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;/users/123/posts&lt;/code&gt;- User 123âs posts&lt;/item&gt;&lt;item&gt;&lt;code&gt;/docs/api/authentication&lt;/code&gt;- Documentation structure&lt;/item&gt;&lt;item&gt;&lt;code&gt;/dashboard/analytics&lt;/code&gt;- Application sections&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Query Parameters (&lt;code&gt;?key1=value1&amp;amp;key2=value2&lt;/code&gt;). Perfect for filters, options, and configuration:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;?theme=dark&amp;amp;lang=en&lt;/code&gt;- UI preferences&lt;/item&gt;&lt;item&gt;&lt;code&gt;?page=2&amp;amp;limit=20&lt;/code&gt;- Pagination&lt;/item&gt;&lt;item&gt;&lt;code&gt;?status=active&amp;amp;sort=date&lt;/code&gt;- Data filtering&lt;/item&gt;&lt;item&gt;&lt;code&gt;?from=2025-01-01&amp;amp;to=2025-12-31&lt;/code&gt;- Date ranges&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;del rend="overstrike"&gt;Anchor&lt;/del&gt;Fragment (&lt;code&gt;#SomewhereInTheDocument&lt;/code&gt;). Ideal for client-side navigation and page sections:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;#L20-L35&lt;/code&gt;- GitHub line highlighting&lt;/item&gt;&lt;item&gt;&lt;code&gt;#features&lt;/code&gt;- Scroll to section&lt;/item&gt;&lt;item&gt;&lt;code&gt;#/dashboard&lt;/code&gt;- Single-page app routing (though itâs rarely used these days)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Common Patterns That Work for Query Parameters&lt;/head&gt;
    &lt;head rend="h4"&gt;Multiple values with delimiters&lt;/head&gt;
    &lt;p&gt;Sometimes youâll see multiple values packed into a single key using delimiters like commas or plus signs. Itâs compact and human-readable, though it requires manual parsing on the server side.&lt;/p&gt;
    &lt;code&gt;?languages=javascript+typescript+python
?tags=frontend,react,hooks
&lt;/code&gt;
    &lt;head rend="h4"&gt;Nested or structured data&lt;/head&gt;
    &lt;p&gt;Developers often encode complex filters or configuration objects into a single query string. A simple convention uses keyâvalue pairs separated by commas, while others serialize JSON or even Base64-encode it for safety.&lt;/p&gt;
    &lt;code&gt;?filters=status:active,owner:me,priority:high
?config=eyJyaWNrIjoicm9sbCJ9==  (base64-encoded JSON)
&lt;/code&gt;
    &lt;head rend="h4"&gt;Boolean flags&lt;/head&gt;
    &lt;p&gt;For flags or toggles, itâs common to pass booleans explicitly or to rely on the keyâs presence as truthy. This keeps URLs shorter and makes toggling features easy.&lt;/p&gt;
    &lt;code&gt;?debug=true&amp;amp;analytics=false
?mobile  (presence = true)
&lt;/code&gt;
    &lt;head rend="h4"&gt;Arrays (Bracket notation)&lt;/head&gt;
    &lt;code&gt;?tags[]=frontend&amp;amp;tags[]=react&amp;amp;tags[]=hooks
&lt;/code&gt;
    &lt;p&gt;Another old pattern is bracket notation, which represents arrays in query parameters. It originated from early web frameworks like PHP where appending &lt;code&gt;[]&lt;/code&gt; to a parameter name signals that multiple values should be grouped together.&lt;/p&gt;
    &lt;code&gt;?tags[]=frontend&amp;amp;tags[]=react&amp;amp;tags[]=hooks
?ids[0]=42&amp;amp;ids[1]=73
&lt;/code&gt;
    &lt;p&gt;Many modern frameworks and parsers (like Nodeâs &lt;code&gt;qs&lt;/code&gt; library or Express middleware) still recognize this pattern automatically. However, itâs not officially standardized in the URL specification, so behavior can vary depending on the server or client implementation. Notice how it even breaks the syntax highlighting on my website.&lt;/p&gt;
    &lt;p&gt;The key is consistency. Pick patterns that make sense for your application and stick with them.&lt;/p&gt;
    &lt;head rend="h2"&gt;State via URL Parameters&lt;/head&gt;
    &lt;p&gt;Letâs look at real-world examples of URLs as state containers:&lt;/p&gt;
    &lt;p&gt;PrismJS Configuration&lt;/p&gt;
    &lt;code&gt;https://prismjs.com/download.html#themes=prism&amp;amp;languages=markup+css+clike+javascript&amp;amp;plugins=line-numbers
&lt;/code&gt;
    &lt;p&gt;The entire syntax highlighter configuration encoded in the URL. Change anything in the UI, and the URL updates. Share the URL, and someone else gets your exact setup. This one uses anchor and not query parameters, but the concept is the same.&lt;/p&gt;
    &lt;p&gt;GitHub Line Highlighting&lt;/p&gt;
    &lt;code&gt;https://github.com/zepouet/Xee-xCode-4.5/blob/master/XeePhotoshopLoader.m#L108-L136
&lt;/code&gt;
    &lt;p&gt;It links to a specific file while highlighting lines 108 through 136. Click this link anywhere, and youâll land on the exact code section being discussed.&lt;/p&gt;
    &lt;p&gt;Google Maps&lt;/p&gt;
    &lt;code&gt;https://www.google.com/maps/@22.443842,-74.220744,19z
&lt;/code&gt;
    &lt;p&gt;Coordinates, zoom level, and map type all in the URL. Share this link, and anyone can see the exact same view of the map.&lt;/p&gt;
    &lt;p&gt;Figma and Design Tools&lt;/p&gt;
    &lt;code&gt;https://www.figma.com/file/abc123/MyDesign?node-id=123:456&amp;amp;viewport=100,200,0.5
&lt;/code&gt;
    &lt;p&gt;Before shareable design links, finding an updated screen or component in a large file was a chore. Someone had to literally show you where it lived, scrolling and zooming across layers. Today, a Figma link carries all that context like canvas position, zoom level, selected element. Literally everything needed to drop you right into the workspace.&lt;/p&gt;
    &lt;p&gt;E-commerce Filters&lt;/p&gt;
    &lt;code&gt;https://store.com/laptops?brand=dell+hp&amp;amp;price=500-1500&amp;amp;rating=4&amp;amp;sort=price-asc
&lt;/code&gt;
    &lt;p&gt;This is one of the most common real-world patterns youâll encounter. Every filter, sort option, and price range preserved. Users can bookmark their exact search criteria and return to it anytime. Most importantly, they can come back to it after navigating away or refreshing the page.&lt;/p&gt;
    &lt;head rend="h2"&gt;Frontend Engineering Patterns&lt;/head&gt;
    &lt;p&gt;Before we discuss implementation details, we need to establish a clear guideline for what should go into the URL. Not all state belongs in URLs. Hereâs a simple heuristic:&lt;/p&gt;
    &lt;p&gt;Good candidates for URL state:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Search queries and filters&lt;/item&gt;
      &lt;item&gt;Pagination and sorting&lt;/item&gt;
      &lt;item&gt;View modes (list/grid, dark/light)&lt;/item&gt;
      &lt;item&gt;Date ranges and time periods&lt;/item&gt;
      &lt;item&gt;Selected items or active tabs&lt;/item&gt;
      &lt;item&gt;UI configuration that affects content&lt;/item&gt;
      &lt;item&gt;Feature flags and A/B test variants&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Poor candidates for URL state:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sensitive information (passwords, tokens, PII)&lt;/item&gt;
      &lt;item&gt;Temporary UI states (modal open/closed, dropdown expanded)&lt;/item&gt;
      &lt;item&gt;Form input in progress (unsaved changes)&lt;/item&gt;
      &lt;item&gt;Extremely large or complex nested data&lt;/item&gt;
      &lt;item&gt;High-frequency transient states (mouse position, scroll position)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you are not sure if a piece of state belongs in the URL, ask yourself: If someone else clicking this URL, should they see the same state? If so, it belongs in the URL. If not, use a different state management approach.&lt;/p&gt;
    &lt;head rend="h3"&gt;Implementation using Plain JavaScript&lt;/head&gt;
    &lt;p&gt;The modern &lt;code&gt;URLSearchParams&lt;/code&gt; API makes URL state management straightforward:&lt;/p&gt;
    &lt;code&gt;// Reading URL parameters
const params = new URLSearchParams(window.location.search);
const view = params.get('view') || 'grid';
const page = params.get('page') || 1;

// Updating URL parameters
function updateFilters(filters) {
  const params = new URLSearchParams(window.location.search);

  // Update individual parameters
  params.set('status', filters.status);
  params.set('sort', filters.sort);

  // Update URL without page reload
  const newUrl = `${window.location.pathname}?${params.toString()}`;
  window.history.pushState({}, '', newUrl);

  // Now update your UI based on the new filters
  renderContent(filters);
}

// Handling back/forward buttons
window.addEventListener('popstate', () =&amp;gt; {
  const params = new URLSearchParams(window.location.search);
  const filters = {
    status: params.get('status') || 'all',
    sort: params.get('sort') || 'date'
  };
  renderContent(filters);
});
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;popstate&lt;/code&gt; event fires when the user navigates with the browserâs Back or Forward buttons. It lets you restore the UI to match the URL, which is essential for keeping your appâs state and history in sync. Usually your frameworkâs router handles this for you, but itâs good to know how it works under the hood.&lt;/p&gt;
    &lt;head rend="h3"&gt;Implementation using React&lt;/head&gt;
    &lt;p&gt;React Router and Next.js provide hooks that make this even cleaner:&lt;/p&gt;
    &lt;code&gt;import { useSearchParams } from 'react-router-dom';
// or for Next.js 13+: import { useSearchParams } from 'next/navigation';

function ProductList() {
  const [searchParams, setSearchParams] = useSearchParams();

  // Read from URL (with defaults)
  const color = searchParams.get('color') || 'all';
  const sort = searchParams.get('sort') || 'price';

  // Update URL
  const handleColorChange = (newColor) =&amp;gt; {
    setSearchParams(prev =&amp;gt; {
      const params = new URLSearchParams(prev);
      params.set('color', newColor);
      return params;
    });
  };

  return (
    &amp;lt;div&amp;gt;
      &amp;lt;select value={color} onChange={e =&amp;gt; handleColorChange(e.target.value)}&amp;gt;
        &amp;lt;option value="all"&amp;gt;All Colors&amp;lt;/option&amp;gt;
        &amp;lt;option value="silver"&amp;gt;Silver&amp;lt;/option&amp;gt;
        &amp;lt;option value="black"&amp;gt;Black&amp;lt;/option&amp;gt;
      &amp;lt;/select&amp;gt;

      {/* Your filtered products render here */}
    &amp;lt;/div&amp;gt;
  );
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Best Practices for URL State Management&lt;/head&gt;
    &lt;p&gt;Now that weâve seen how URLs can hold application state, letâs look at a few best practices that keep them clean, predictable, and user-friendly.&lt;/p&gt;
    &lt;head rend="h4"&gt;Handling Defaults Gracefully&lt;/head&gt;
    &lt;p&gt;Donât pollute URLs with default values:&lt;/p&gt;
    &lt;code&gt;// Bad: URL gets cluttered with defaults
?theme=light&amp;amp;lang=en&amp;amp;page=1&amp;amp;sort=date

// Good: Only non-default values in URL
?theme=dark  // light is default, so omit it
&lt;/code&gt;
    &lt;p&gt;Use defaults in your code when reading parameters:&lt;/p&gt;
    &lt;code&gt;function getTheme(params) {
  return params.get('theme') || 'light'; // Default handled in code
}
&lt;/code&gt;
    &lt;head rend="h4"&gt;Debouncing URL Updates&lt;/head&gt;
    &lt;p&gt;For high-frequency updates (like search-as-you-type), debounce URL changes:&lt;/p&gt;
    &lt;code&gt;import { debounce } from 'lodash';

const updateSearchParam = debounce((value) =&amp;gt; {
  const params = new URLSearchParams(window.location.search);
  if (value) {
    params.set('q', value);
  } else {
    params.delete('q');
  }
  window.history.replaceState({}, '', `?${params.toString()}`);
}, 300);

// Use replaceState instead of pushState to avoid flooding history
&lt;/code&gt;
    &lt;head rend="h4"&gt;pushState vs. replaceState&lt;/head&gt;
    &lt;p&gt;When deciding between &lt;code&gt;pushState&lt;/code&gt; and &lt;code&gt;replaceState&lt;/code&gt;, think about how you want the browser history to behave. &lt;code&gt;pushState&lt;/code&gt; creates a new history entry, which makes sense for distinct navigation actions like changing filters, pagination, or navigating to a new view â users can then use the Back button to return to the previous state. On the other hand, &lt;code&gt;replaceState&lt;/code&gt; updates the current entry without adding a new one, making it ideal for refinements such as search-as-you-type or minor UI adjustments where you donât want to flood the history with every keystroke.&lt;/p&gt;
    &lt;head rend="h2"&gt;URLs as Contracts&lt;/head&gt;
    &lt;p&gt;When designed thoughtfully, URLs become more than just state containers. They become contracts between your application and its consumers. A good URL defines expectations for humans, developers, and machines alike&lt;/p&gt;
    &lt;head rend="h3"&gt;Clear Boundaries&lt;/head&gt;
    &lt;p&gt;A well-structured URL draws the line between whatâs public and whatâs private, client and server, shareable and session-specific. It clarifies where state lives and how it should behave. Developers know whatâs safe to persist, users know what they can bookmark, and machines know whats worth indexing.&lt;/p&gt;
    &lt;p&gt;URLs, in that sense, act as interfaces: visible, predictable, and stable.&lt;/p&gt;
    &lt;head rend="h3"&gt;Communicating Meaning&lt;/head&gt;
    &lt;p&gt;Readable URLs explain themselves. Consider the difference between the two URLs below.&lt;/p&gt;
    &lt;code&gt;https://example.com/p?id=x7f2k&amp;amp;v=3
https://example.com/products/laptop?color=silver&amp;amp;sort=price
&lt;/code&gt;
    &lt;p&gt;The first one hides intent. The second tells a story. A human can read it and understand what theyâre looking at. A machine can parse it and extract meaningful structure.&lt;/p&gt;
    &lt;p&gt;Jim Nielsen calls these âexamples of great URLsâ. URLs that explain themselves.&lt;/p&gt;
    &lt;head rend="h3"&gt;Caching and Performance&lt;/head&gt;
    &lt;p&gt;URLs are cache keys. Well-designed URLs enable better caching strategies:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Same URL = same resource = cache hit&lt;/item&gt;
      &lt;item&gt;Query params define cache variations&lt;/item&gt;
      &lt;item&gt;CDNs can cache intelligently based on URL patterns&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can even visualize a userâs journey without any extra tracking code:&lt;/p&gt;
    &lt;quote&gt;graph LR A["/products"] --&amp;gt; |selects category| B["/products?category=laptops"] B --&amp;gt; |adds price filter| C["/products?category=laptops&amp;amp;price=500-1000"] style A fill:#e9edf7,stroke:#455d8d,stroke-width:2px; style B fill:#e9edf7,stroke:#455d8d,stroke-width:2px; style C fill:#e9edf7,stroke:#455d8d,stroke-width:2px;&lt;/quote&gt;
    &lt;p&gt;Your analytics tools can track this flow without additional instrumentation. Every URL parameter becomes a dimension you can analyze.&lt;/p&gt;
    &lt;head rend="h3"&gt;Versioning and Evolution&lt;/head&gt;
    &lt;p&gt;URLs can communicate API versions, feature flags, and experiments:&lt;/p&gt;
    &lt;code&gt;?v=2                   // API version
?beta=true             // Beta features
?experiment=new-ui     // A/B test variant
&lt;/code&gt;
    &lt;p&gt;This makes gradual rollouts and backwards compatibility much more manageable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Anti-Patterns to Avoid&lt;/head&gt;
    &lt;p&gt;Even with the best intentions, itâs easy to misuse URL state. Here are common pitfalls:&lt;/p&gt;
    &lt;head rend="h3"&gt;âState Only in Memoryâ SPAs&lt;/head&gt;
    &lt;p&gt;The classic single-page app mistake:&lt;/p&gt;
    &lt;code&gt;// User hits refresh and loses everything
const [filters, setFilters] = useState({});
&lt;/code&gt;
    &lt;p&gt;If your app forgets its state on refresh, youâre breaking one of the webâs fundamental features. Users expect URLs to preserve context. I remember a viral video from years ago where a Reddit user vented about an e-commerce site: every time she hit âBack,â all her filters disappeared. Her frustration summed it up perfectly. If users lose context, they lose patience.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sensitive Data in URLs&lt;/head&gt;
    &lt;p&gt;This one seems obvious, but itâs worth repeating:&lt;/p&gt;
    &lt;code&gt;// NEVER DO THIS
?password=secret123
&lt;/code&gt;
    &lt;p&gt;URLs are logged everywhere: browser history, server logs, analytics, referrer headers. Treat them as public.&lt;/p&gt;
    &lt;head rend="h3"&gt;Inconsistent or Opaque Naming&lt;/head&gt;
    &lt;code&gt;// Unclear and inconsistent
?foo=true&amp;amp;bar=2&amp;amp;x=dark

// Self-documenting and consistent
?mobile=true&amp;amp;page=2&amp;amp;theme=dark
&lt;/code&gt;
    &lt;p&gt;Choose parameter names that make sense. Future you (and your team) will thank you.&lt;/p&gt;
    &lt;head rend="h3"&gt;Overloading URLs with Complex State&lt;/head&gt;
    &lt;code&gt;?config=eyJtZXNzYWdlIjoiZGlkIHlvdSByZWFsbHkgdHJpZWQgdG8gZGVjb2RlIHRoYXQ_IiwiZmlsdGVycyI6eyJzdGF0dXMiOlsiYWN0aXZlIiwicGVuZGluZyJdLCJwcmlvcml0eSI6WyJoaWdoIiwibWVkaXVtIl0sInRhZ3MiOlsiZnJvbnRlbmQiLCJyZWFjdCIsImhvb2tzIl0sInJhbmdlIjp7ImZyb20iOiIyMDI0LTAxLTAxIiwidG8iOiIyMDI0LTEyLTMxIn19LCJzb3J0Ijp7ImZpZWxkIjoiY3JlYXRlZEF0Iiwib3JkZXIiOiJkZXNjIn0sInBhZ2luYXRpb24iOnsicGFnZSI6MSwibGltaXQiOjIwfX0==
&lt;/code&gt;
    &lt;p&gt;If you need to base64-encode a massive JSON object, the URL probably isnât the right place for that state.&lt;/p&gt;
    &lt;head rend="h3"&gt;URL Length Limits&lt;/head&gt;
    &lt;p&gt;Browsers and servers impose practical limits on URL length (usually between 2,000 and 8,000 characters) but the reality is more nuanced. As this detailed Stack Overflow answer explains, limits come from a mix of browser behavior, server configurations, CDNs, and even search engine constraints. If youâre bumping against them, itâs a sign you need to rethink your approach.&lt;/p&gt;
    &lt;head rend="h3"&gt;Breaking the Back Button&lt;/head&gt;
    &lt;code&gt;// Replacing state incorrectly
history.replaceState({}, '', newUrl); // Used when pushState was needed
&lt;/code&gt;
    &lt;p&gt;Respect browser history. If a user action should be âundoableâ via the back button, use &lt;code&gt;pushState&lt;/code&gt;. If itâs a refinement, use &lt;code&gt;replaceState&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Closing Thought&lt;/head&gt;
    &lt;p&gt;That PrismJS URL reminded me of something important: good URLs donât just point to content. They describe a conversation between the user and the application. They capture intent, preserve context, and enable sharing in ways that no other state management solution can match.&lt;/p&gt;
    &lt;p&gt;Weâve built increasingly sophisticated state management libraries like Redux, MobX, Zustand, Recoil and others. They all have their place but sometimes the best solution is the one thatâs been there all along.&lt;/p&gt;
    &lt;p&gt;In my previous article, I wrote about the hidden costs of bad URL design. Today, weâve explored the flip side: the immense value of good URL design. URLs arenât just addresses. Theyâre state containers, user interfaces, and contracts all rolled into one.&lt;/p&gt;
    &lt;p&gt;If your app forgets its state when you hit refresh, youâre missing one of the webâs oldest and most elegant features.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45789474</guid><pubDate>Sun, 02 Nov 2025 11:12:58 +0000</pubDate></item><item><title>Mock – An API creation and testing utility: Examples</title><link>https://dhuan.github.io/mock/latest/examples.html</link><description>&lt;doc fingerprint="9c9aac8da400f802"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How-tos &amp;amp; Examples¶&lt;/head&gt;
    &lt;head rend="h2"&gt;Delaying specific endpoints¶&lt;/head&gt;
    &lt;p&gt;Making an existing API slow can be easily accomplished combining mock’s Base APIs and the delay option.&lt;/p&gt;
    &lt;code&gt;$ mock serve -p 8000 --base example.com --delay 2000
&lt;/code&gt;
    &lt;p&gt;You may want however to make a specific endpoint slow instead of the whole API. This can be achieved using middlewares:&lt;/p&gt;
    &lt;code&gt;$ mock serve -p 8000 --base example.com --middleware '
if [ "${MOCK_REQUEST_ENDPOINT}" = "some/endpoint" ]
then
    sleep 2 # wait two seconds
fi
'
&lt;/code&gt;
    &lt;p&gt;With that last example, our API at &lt;code&gt;localhost:8000&lt;/code&gt; will act as a proxy to
&lt;code&gt;example.com&lt;/code&gt;. All requests will be responded immediately except
&lt;code&gt;some/endpoint&lt;/code&gt; which will have a delay of 2 seconds.&lt;/p&gt;
    &lt;head rend="h2"&gt;An API powered by multiple languages¶&lt;/head&gt;
    &lt;code&gt;$ mock serve -p 3000 \
    --route js \
    --exec '
node &amp;lt;&amp;lt;EOF | mock write
console.log("Hello from Node.js!")
EOF
' \
    --route python \
    --exec '
python3 &amp;lt;&amp;lt;EOF | mock write
print("Hello from Python!")
EOF
' \
    --route php \
    --exec '
php &amp;lt;&amp;lt;EOF | mock write
&amp;lt;?php
echo "Hello from PHP!\n";
?&amp;gt;
EOF
'
&lt;/code&gt;
    &lt;p&gt;Let’s test it:&lt;/p&gt;
    &lt;code&gt;$ curl localhost:3000/js
# Prints out: Hello from Node.js!
$ curl localhost:3000/python
# Prints out: Hello from Python!
$ curl localhost:3000/php
# Prints out: Hello from PHP!
&lt;/code&gt;
    &lt;head rend="h2"&gt;A stateful API¶&lt;/head&gt;
    &lt;code&gt;$ export TMP=$(mktemp)
$ printf "0" &amp;gt; "${TMP}"

$ mock serve -p 3000 \
    --route '/hello' \
    --exec '
printf "%s + 1\n" "$(cat ${TMP})" | bc | sponge "${TMP}"

printf "This server has received %s request(s) so far." "$(cat '"${TMP}"')" | mock write
'
&lt;/code&gt;
    &lt;p&gt;Let’s test it:&lt;/p&gt;
    &lt;code&gt;$ curl localhost:3000/hello
# Prints out: This server has received 1 request(s) so far.
$ curl localhost:3000/hello
# Prints out: This server has received 2 request(s) so far.
$ curl localhost:3000/hello
# Prints out: This server has received 3 request(s) so far.
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45789556</guid><pubDate>Sun, 02 Nov 2025 11:30:50 +0000</pubDate></item><item><title>HyperRogue – A non-Euclidean roguelike</title><link>https://roguetemple.com/z/hyper/</link><description>&lt;doc fingerprint="39a78a5a89f4b621"&gt;
  &lt;main&gt;
    &lt;div&gt;See the Gallery for the high quality images of all lands.&lt;p&gt; You are a lone adventurer in a strange world, where geometry does not work in the expected way. Gather as much treasure as you can before the nasty monsters get you. Explore about 50 different worlds, each with its own unique treasures, enemies, and terrain obstacles. Your quest is to find the legendary treasure, the Orbs of Yendor. Collect one of them to win! Or just ignore your quest and collect smaller treasures. &lt;/p&gt;&lt;p&gt; The twist is the unique, unusual geometry of the world: it is one of just few games which takes place on the &lt;/p&gt;hyperbolic plane&lt;p&gt;. Witness a grid composed of hexagons and heptagons, straight lines which seem to be parallel, but then they diverge and never cross, triangles whose angles add up to less than 180 degrees, how extremely unlikely is it to reach the same place twice, and how the world seems to be rotated when you do return. All this matters for the gameplay. The game is inspired by the roguelike genre (although in a very minimalist way), works of &lt;/p&gt;M. C. Escher&lt;p&gt;, and by puzzle games such as &lt;/p&gt;Deadly Rooms of Death&lt;p&gt;. &lt;/p&gt;&lt;head rend="h3"&gt;A very infinite world&lt;/head&gt;&lt;p&gt; With more space than anything Euclidean. The game dynamically generates new parts of the world as you move. No previous understanding of hyperbolic geometry is required -- actually, playing HyperRogue is probably the best way to learn about this, much better and deeper than any mathematical formulas. It is virtually impossible to get back to a place where you have been before, unless you go back exactly the same way. Show your true mastery of hyperbolic navigation by finding the &lt;/p&gt;Orb of Yendor&lt;p&gt;, &lt;/p&gt;Holy Grail&lt;p&gt;, rescuing the &lt;/p&gt;Prince(ss)&lt;p&gt;! &lt;/p&gt;&lt;head rend="h3"&gt;Lots of variety&lt;/head&gt; 72 lands&lt;p&gt; (72 in the free version), each with unique theme, mechanics, graphics, terrain features, native monsters, treasure type, and magical Orb power. The ultimate &lt;/p&gt;Hyperstone Quest&lt;p&gt; requires you to get 10 treasures in each of the lands! &lt;/p&gt;&lt;head rend="h3"&gt;Simple but hard to master mechanics&lt;/head&gt;&lt;p&gt; In many ways, HyperRogue is closer to boardgames like Chess, than to mainstream computer games -- except that its "chessboard" is a hyperbolic plane, with randomly generated features. Enemies move predictably, and most can be killed simply by moving into them -- however, they could kill your character with a single attack too! Even though the game disallows you from making moves which would lead to this immediately ("check" in Chess), fighting large groups is still a challenge. &lt;/p&gt;&lt;head rend="h3"&gt;Even more challenge!&lt;/head&gt;&lt;p&gt; If you want even more challenge, you will get it easily, due to HyperRogue's difficulty/high score system. The more treasures you collect in a given land, the more monster chase you there. Collect 10 treasures in the given land to show the basic understanding of it, 25 treasures to show that you have mastered it, or go for even more! The game never ends, but it gets harder and harder. &lt;/p&gt;&lt;head rend="h3"&gt;Multiple special modes&lt;/head&gt;&lt;p&gt; Enable the shoot'em up mode, and the game is no longer turn-based or grid-based. Play together with your friend (shmup mode is recommended). Try the Euclidean, elliptic, or spherical modes, to see why the &lt;/p&gt;geometry&lt;p&gt; matters, or enable the heptagonal mode to make the hyperbolic effects stronger. Try extra challenges such as the Yendor Challenge or the Pure Tactics Mode, or make the game &lt;/p&gt;look differently&lt;p&gt; with the Hypersian Rug or Conformal mode. The recently added Orb Strategy mode emphasizes the resource management by giving you harder challenges while allowing you to use your limited magical powers in difficult situations. &lt;/p&gt;&lt;head rend="h3"&gt;Great game, educational thing, or maybe an artistic or research tool?&lt;/head&gt;&lt;p&gt; HyperRogue has started as a small, weird technical experiment, but it turned out that hyperbolic geometry combined with basic roguelike rules makes for exceptionally great gameplay, even if you do not care about geometry! Further work improved the gameplay, but also turned HyperRogue into probably the most fully featured engine for truly non-Euclidean geometry in existence. Even if you do not care about roguelikes, roguelites and block puzzles, you can play the tutorial as an explorable explanation about hyperbolic geometry, use HyperRogue for &lt;/p&gt;research in applied hyperbolic geometry&lt;p&gt;, or use the texture mode and vector graphics editor to create mathematical art. The possibilities are endless! &lt;/p&gt;&lt;head rend="h2"&gt;How to get it&lt;/head&gt;&lt;p&gt; HyperRogue can be &lt;/p&gt;downloaded freely&lt;p&gt; from this website, or bought on &lt;/p&gt;Steam&lt;p&gt; or &lt;/p&gt;itch.io&lt;p&gt;; the paid versions are updated more frequently and include social features such as achievements and leaderboards. There are also Android and iOS versions. &lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45789596</guid><pubDate>Sun, 02 Nov 2025 11:40:47 +0000</pubDate></item><item><title>Tongyi DeepResearch – open-source 30B MoE Model that rivals OpenAI DeepResearch</title><link>https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/</link><description>&lt;doc fingerprint="efd86393ad1b861a"&gt;
  &lt;main&gt;
    &lt;p&gt;GITHUB HUGGINGFACE MODELSCOPE SHOWCASE&lt;/p&gt;
    &lt;head rend="h2"&gt;From Chatbot to Autonomous Agent&lt;/head&gt;
    &lt;p&gt;We are proud to present Tongyi DeepResearch, the first fully open‑source Web Agent to achieve performance on par with OpenAI’s DeepResearch across a comprehensive suite of benchmarks. Tongyi DeepResearch demonstrates state‑of‑the‑art results, scoring 32.9 on the academic reasoning task Humanity’s Last Exam (HLE), 43.4 on BrowseComp and 46.7 on BrowseComp‑ZH in extremely complex information‑seeking tasks, and achieving a score of 75 on the user‑centric xbench‑DeepSearch benchmark, systematically outperforming all existing proprietary and open‑source Deep Research agents.&lt;/p&gt;
    &lt;p&gt;Beyond the model, we share a complete and battle‑tested methodology for creating such advanced agents. Our contribution details a novel data synthesis solution applied across the entire training pipeline, from Agentic Continual Pre‑training (CPT) and Supervised Fine‑Tuning (SFT) for cold‑starting, to the final Reinforcement Learning (RL) stage. For RL, we provide a full‑stack solution, including algorithmic innovations, automated data curation, and robust infrastructure. For inference, the vanilla ReAct framework showcases the model’s powerful intrinsic capabilities without any prompt engineering, while the advanced Heavy Mode (test‑time‑scaling) demonstrates the upper limits of its complex reasoning and planning potential.&lt;/p&gt;
    &lt;head rend="h2"&gt;Continual Pre‑training and Post‑training Empowered by Fully Synthetic Data&lt;/head&gt;
    &lt;head rend="h3"&gt;Continual Pre‑training Data&lt;/head&gt;
    &lt;p&gt;We introduce Agentic CPT to deep research agent training, creating powerful agentic foundation models for post‑training. We propose AgentFounder, a systematic and scalable solution for large‑scale data synthesis that creates a data flywheel with data from the post‑training pipeline.&lt;/p&gt;
    &lt;p&gt;Data Reorganization and Question Construction. We continuously collect data from various sources, including documents, publicly available crawled data, knowledge graphs, and historical trajectories and tool invocation records (e.g., search results with links). As shown in the figure, these diverse data sources are restructured into an entity‑anchored open‑world knowledge memory. Based on randomly sampled entities and their corresponding knowledge, we generate multi‑style (question,answer) pairs.&lt;/p&gt;
    &lt;p&gt;Action Synthesis. Based on diverse problems and historical trajectories, we construct first‑order action synthesis data and higher‑order action synthesis data. Our method enables large‑scale and comprehensive exploration of the potential reasoning‑action space within offline environments, thereby thereby eliminating the need for additional commercial tool API calls. Specifically, for the higher‑order action synthesis, we remodel trajectories as multi‑step decision‑making processes to enhance the model’s decision‑making capabilities.&lt;/p&gt;
    &lt;head rend="h3"&gt;Post-training Data&lt;/head&gt;
    &lt;p&gt;High-quality synthetic QA pairs&lt;/p&gt;
    &lt;p&gt;We develop an end‑to‑end solution for synthetic data generation. This fully automated process requires no human intervention to construct super‑human quality datasets, designed to push the boundaries of AI agent performance. Through long‑term exploration and iteration‑from early methods like reverse‑engineering QA pairs from clickstreams (WebWalker) to the more systematic graph‑based synthesis (WebSailor and WebSailor‑V2), then the formalized task modeling (WebShaper)‑our approach ensures both exceptional data quality and massive scalability, breaking through the upper limits of model capabilities.&lt;/p&gt;
    &lt;p&gt;To address complex, high‑uncertainty questions, we synthesize web‑based QA data through a novel pipeline. The process begins by constructing a highly interconnected knowledge graph via random walks and isomorphic tables towards tabular data fusion from real‑world websites , ensuring a realistic information structure. We then sample subgraphs and subtables to generate initial questions and answers. The crucial step involves intentionally increasing difficulty by strategically obfuscating or blurring information within the question. This practical approach is grounded in a complete theoretical framework, where we formally model QA difficulty as a series of controllable “atomic operations” (e.g., merging entities with similar attributes) on entity relationships, allowing us to systematically increase complexity.&lt;/p&gt;
    &lt;p&gt;To further reduce inconsistencies between the organized information structure and the reasoning structure of QA, enable more controllable difficulty and structure scaling of reasoning, we proposed a formal modeling of the information‑seeking problem based on set theory. With this formalization, we developed agents that expands the problem in a controlled manner, and minimizes reasoning shortcuts and structural redundancy, leading to further improved QA quality. Moreover, this formal modeling also allows for efficient verification of QA correctness, effectively addressing the challenge of validating synthetic information‑seeking data for post‑training.&lt;/p&gt;
    &lt;p&gt;Furthermore, we have developed an automated data engine to scale up the creation of PhD‑level research questions. This engine begins with a multi‑disciplinary knowledge base, generating “seed” QA pairs that require multi‑source reasoning. Each seed then enters a self‑guided loop of “iterative complexity upgrades”, where a question‑crafting agent is equipped with a powerful toolset including web search, academic retrieval, and a Python execution environment. In each iteration, the agent expands knowledge boundaries, deepens conceptual abstraction, and even constructs computational tasks, creating a virtuous cycle where the output of one round becomes the more complex input for the next, ensuring a controllable and systematic escalation of task difficulty.&lt;/p&gt;
    &lt;p&gt;Unleashing Agent Capabilities with Diverse Reasoning Pattern&lt;/p&gt;
    &lt;p&gt;To bootstrap the model’s initial capabilities, we constructed a set of trajectories via rejection sampling, based on the ReAct and IterResearch frameworks (for details, see below). On one hand, ReAct, as a classic and foundational multi-turn reasoning format, instills rich reasoning behaviors and reinforces the model’s ability to adhere to structured formats.&lt;/p&gt;
    &lt;p&gt;On the other hand, we introduce IterResearch, an innovative agent paradigm (detailed below). It unleashes the model’s full reasoning potential by dynamically reconstructing a streamlined workspace in each turn, ensuring that every decision is deliberate and well-considered. Leveraging IterResearch, we constructed a set of trajectories that integrate reasoning, planning, and tool-use, thereby strengthening the model’s capacity for sustained planning when confronted with Long-Horizon tasks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rollout Mode&lt;/head&gt;
    &lt;p&gt;We have conducted extensive exploration into the rollout paradigms for DeepResearch‑type agents. As a result, our final model supports multiple rollout formats, including the native ReAct Mode and the context‑managing Heavy Mode.&lt;/p&gt;
    &lt;head rend="h3"&gt;Native ReAct Mode&lt;/head&gt;
    &lt;p&gt;Our model demonstrates excellent performance using the native ReAct reasoning paradigm without any prompt engineering. It strictly adheres to the Thought‑Action‑Observation cycle, performing multiple iterations to solve problems. With a model context length of 128K, it can handle a large number of interaction rounds, fully achieving scaling in its interaction with the environment. ReAct’s simplicity and universality provide the clearest benchmark for a model’s intrinsic capabilities and the efficacy of our training pipeline.&lt;/p&gt;
    &lt;p&gt;Our choice of ReAct is heavily informed by “The Bitter Lesson”, which posits that general methods leveraging scalable computation ultimately outperform approaches that rely on complex, human‑engineered knowledge and intricate designs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Heavy Mode&lt;/head&gt;
    &lt;p&gt;In addition to the native ReAct mode, we have developed a “Heavy Mode” for complex, multi‑step research tasks. This mode is built on our new IterResearch paradigm, designed to push the agent’s capabilities to their limit.&lt;/p&gt;
    &lt;p&gt;The IterResearch paradigm was created to solve the “cognitive suffocation” and “noise pollution” that occurs when agents accumulate all information into a single, ever‑expanding context. Instead, IterResearch deconstructs a task into a series of “research rounds”.&lt;/p&gt;
    &lt;p&gt;In each round, the agent reconstructs a streamlined workspace using only the most essential outputs from the previous round. Within this focused workspace, the agent analyzes the problem, integrates key findings into a continuously evolving central report, and then decides its next action‑either gathering more information or providing a final answer. This iterative process of “synthesis and reconstruction” allows the agent to maintain a clear “cognitive focus” and high reasoning quality throughout long tasks.&lt;/p&gt;
    &lt;p&gt;Building on this, we propose the Research‑Synthesis framework. In this model, multiple Research Agents use the IterResearch process to explore a problem in parallel. A final Synthesis Agent then integrates their refined reports and conclusions to produce a more comprehensive final answer. This parallel structure enables the model to consider a wider range of research paths within a limited context window, pushing its performance to the limit.&lt;/p&gt;
    &lt;head rend="h2"&gt;End-to‑End Agent Training Pipeline&lt;/head&gt;
    &lt;p&gt;Training an agentic model like this required us to rethink the entire model training pipeline, from pre‑training to fine‑tuning to reinforcement learning. We established a new paradigm for agent model training that connects Agentic CPT → Agentic SFT → Agentic RL, creating a seamless end‑to‑end training loop for an AI agent. Here’s how we tackled the final stage with reinforcement learning, which was crucial for aligning the agent’s behavior with high‑level goals:&lt;/p&gt;
    &lt;head rend="h3"&gt;On‑Policy Agent Reinforcement Learning (RL)&lt;/head&gt;
    &lt;p&gt;Constructing a high‑quality agent through RL is a complex system engineering challenge; if this entire development process is viewed as a “reinforcement learning” loop, any instability or lack of robustness in its components can lead to erroneous “reward” signals. We will now share our practices in RL, covering both the algorithmic and infrastructure sides.&lt;/p&gt;
    &lt;p&gt;For RL algorithm, we made several algorithmic breakthroughs, using a customized on‑policy Group Relative Policy Optimization (GRPO). We employ a strictly on‑policy training regimen, ensuring that the learning signal is always relevant to the model’s current capabilities. The training objective is optimized using a token‑level policy gradient loss. Second, to further reduce variance in the advantage estimation, we adopt a leave‑one‑out strategy. Furthermore, we employ a conservative strategy for negative samples, having observed that an unfiltered set of negative trajectories significantly degrades training stability. This can manifest as a “format collapse” phenomenon after extended training. To mitigate this, we selectively exclude certain negative samples from the loss calculation, for instance, those that do not yield a final answer because they exceed a length limit. For the sake of efficiency, we do not employ dynamic sampling. We instead leverage larger batch and group sizes, which serve to maintain smaller variance and provide adequate supervision.&lt;/p&gt;
    &lt;p&gt;The training dynamics demonstrate effective learning, with a consistent upward trend in reward. Meanwhile, policy entropy remains consistently high, indicating sustained exploration and preventing premature convergence. We attribute this to the non‑stationary nature of the web environment, which naturally fosters a robust, adaptive policy and obviates the need for explicit entropy regularization.&lt;/p&gt;
    &lt;p&gt;We consider that the algorithm is important but not the only decisive factor in the success of Agentic RL. We have experimented with many different algorithms and tricks, and find that data and stability of the training environment are likely the more critical components in determining whether the RL works. Interestingly, we have tested to train the model directly on the BrowseComp testing set, but the results are substantially poorer than when using our synthetic data. We hypothesize that this disparity arises because the synthetic data offers a more consistent distribution, which allows the model to be more effectively tailored. Conversely, the human‑annotated data (such as BrowseComp) is inherently noisier. Given its limited scale, it is difficult to approximate a learnable underlying distribution, which consequently hinders the model to learn and generalize from it.&lt;/p&gt;
    &lt;p&gt;On the infrastructure side, training an agent with tools required us to develop a highly stable and efficient environment:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Synthetic Training Environment: Relying on live web APIs for development is expensive, slow, and inconsistent. We addressed this by creating a simulated training environment using an offline Wikipedia database and a custom tool suite. By adapting our data pipeline to generate high‑quality, complex tasks for this environment, we created a cost‑effective, fast, and controllable platform that dramatically accelerates our research and iteration.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Stable &amp;amp; Efficient Tool Sandbox: To ensure reliable tool use during agent training and evaluation, we developed a unified sandbox. The sandbox handles concurrency and failure gracefully by caching results, retrying failed calls, and using redundant providers as fallbacks (e.g., a backup search API). This provides the agent with a fast and deterministic experience, which is crucial for preventing tool errors from corrupting its learning trajectory.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Automatic Data Curation: Data is the core driver of model capability enhancement; its importance even surpasses that of the algorithm. The quality of the data directly determines the upper bound on the model’s ability to generalize to out‑of‑distribution scenarios through self‑exploration. To address this challenge, we optimize data in real time, guided by training dynamics. This optimization is achieved through a fully automated data synthesis and filtering pipeline that dynamically adjusts the training set. By closing the loop between data generation and model training, this approach not only ensures training stability but also delivers substantial performance gains.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;On‑Policy Asynchronous Framework: We implemented a custom step‑level asynchronous RL training loop on top of rLLM. Multiple agent instances interact with the (simulated or real) environment in parallel, each producing trajectories.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Through these measures, we “closed the loop” on agent training. Starting from a raw model, we did Agentic pre‑training to initialize tool‑use skills, then supervised finetuning on expert‑like data to cold start, and finally on‑policy RL to let the model conduct self‑evolution. This full‑stack approach ‑ now proven with Tongyi DeepResearch ‑ presents a new paradigm for training AI agents that can robustly solve complex tasks in dynamic environments.&lt;/p&gt;
    &lt;p&gt;(Our RL approach is inspired by several past work from Agentica. We adapt their rLLM framework and extend it to train our web agents.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Real‑World Applications and Impact&lt;/head&gt;
    &lt;p&gt;Tongyi DeepResearch is not just a research showcase; it’s already powering real applications within Alibaba and beyond, demonstrating its value in practical scenarios:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gaode Mate (Map &amp;amp; Navigation Agent): Collaborating with Amap (Gaode) Team, we co‑developed “Xiao Gao,” an AI copilot that leverages the app’s rich toolset. It can execute complex travel planning commands, like creating a multi‑day driving tour that includes specific scenic spots and pet‑friendly hotels. Through multi‑step reasoning, Xiao Gao autonomously researches and integrates information to produce a detailed, personalized itinerary, offering an intelligent planning experience that far surpasses standard navigation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tongyi FaRui (Legal Research Agent): Empowered by our DeepResearch architecture, FaRui now functions as a true legal agent. It autonomously executes complex, multi‑step research tasks that mirror a junior attorney’s workflow‑systematically retrieving case law, cross‑referencing statutes, and synthesizing analysis. Crucially, all conclusions are grounded in verifiable judicial sources and delivered with precise case and statute citations, ensuring professional‑grade accuracy and credibility.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Limitations&lt;/head&gt;
    &lt;p&gt;Our future work will address three key limitations. First, the current 128k context length is still insufficient for the most complex long‑horizon tasks, requiring us to explore expanded context windows and more sophisticated information management. Second, our training pipeline’s scalability remains unproven on foundation models significantly larger than our 30B‑scale MoE, and we plan to validate our methods on larger‑scale models. Lastly, we aim to improve the efficiency of our reinforcement learning framework by investigating techniques like partial rollouts, which will necessitate solving the challenges of off‑policy training, such as distributional shift.&lt;/p&gt;
    &lt;head rend="h2"&gt;Series Work&lt;/head&gt;
    &lt;p&gt;Tongyi DeepResearch also has an extensive deep research agent family. You can find more information in the following papers:&lt;/p&gt;
    &lt;p&gt;[1] WebWalker: Benchmarking LLMs in Web Traversal&lt;/p&gt;
    &lt;p&gt;[2] WebDancer: Towards Autonomous Information Seeking Agency&lt;/p&gt;
    &lt;p&gt;[3] WebSailor: Navigating Super‑human Reasoning for Web Agent&lt;/p&gt;
    &lt;p&gt;[4] WebShaper: Agentically Data Synthesizing via Information‑Seeking Formalization&lt;/p&gt;
    &lt;p&gt;[5] WebWatcher: Breaking New Frontier of Vision‑Language Deep Research Agent&lt;/p&gt;
    &lt;p&gt;[6] WebResearch: Unleashing reasoning capability in Long‑Horizon Agents&lt;/p&gt;
    &lt;p&gt;[7] ReSum: Unlocking Long‑Horizon Search Intelligence via Context Summarization&lt;/p&gt;
    &lt;p&gt;[8] WebWeaver: Structuring Web‑Scale Evidence with Dynamic Outlines for Open‑Ended Deep Research&lt;/p&gt;
    &lt;p&gt;[10] Scaling Agents via Continual Pre‑training&lt;/p&gt;
    &lt;p&gt;[11] Towards General Agentic Intelligence via Environment Scaling&lt;/p&gt;
    &lt;p&gt;Our team has a long‑standing commitment to the research and development of deep research agents. Over the past six months, we have consistently published one technical report per month, totaling five to date. Today, we are excited to simultaneously release six new reports and share our Tongyi DeepResearch‑30B‑A3B model with the community.&lt;/p&gt;
    &lt;p&gt;Stay tuned for our next generation of agentic models.&lt;/p&gt;
    &lt;code&gt;@misc{tongyidr,
  author={Tongyi DeepResearch Team},
  title={Tongyi DeepResearch: A New Era of Open-Source AI Researchers},
  year={2025},
  howpublished={\url{https://github.com/Alibaba-NLP/DeepResearch}}
}
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45789602</guid><pubDate>Sun, 02 Nov 2025 11:43:26 +0000</pubDate></item><item><title>X.org Security Advisory: multiple security issues X.Org X server and Xwayland</title><link>https://lists.x.org/archives/xorg-announce/2025-October/003635.html</link><description>&lt;doc fingerprint="981a9199600a0298"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;X.Org Security Advisory: multiple security issues X.Org X server and Xwayland&lt;/head&gt; Olivier Fourdan ofourdan at redhat.com &lt;lb/&gt;Tue Oct 28 13:22:18 UTC 2025&lt;quote&gt;====================================================================== X.Org Security Advisory: October 28, 2025 Issues in X.Org X server prior to 21.1.18 and Xwayland prior to 24.1.8 ====================================================================== Multiple issues have been found in the X server and Xwayland implementations published by X.Org for which we are releasing security fixes for in xorg-server-21.1.19 and xwayland-24.1.9. 1) CVE-2025-62229: Use-after-free in XPresentNotify structures creation Using the X11 Present extension, when processing and adding the notifications after presenting a pixmap, if an error occurs, a dangling pointer may be left in the error code path of the function causing a use-after-free when eventually destroying the notification structures later. Introduced in: Xorg 1.15 Fixed in: xorg-server-21.1.19 and xwayland-24.1.9 Fix: https://gitlab.freedesktop.org/xorg/xserver/-/commit/5a4286b1 Found by: Jan-Niklas Sohn working with Trend Micro Zero Day Initiative. 2) CVE-2025-62230: Use-after-free in Xkb client resource removal When removing the Xkb resources for a client, the function XkbRemoveResourceClient() will free the XkbInterest data associated with the device, but not the resource associated with it. As a result, when the client terminates, the resource delete function triggers a use-after-free. Introduced in: X11R6 Fixed in: xorg-server-21.1.19 and xwayland-24.1.9 Fix: https://gitlab.freedesktop.org/xorg/xserver/-/commit/99790a2c https://gitlab.freedesktop.org/xorg/xserver/-/commit/10c94238 Found by: Jan-Niklas Sohn working with Trend Micro Zero Day Initiative. 3) CVE-2025-62231: Value overflow in Xkb extension XkbSetCompatMap() The XkbCompatMap structure stores some of its values using an unsigned short, but fails to check whether the sum of the input data might overflow the maximum unsigned short value. Introduced in: X11R6 Fixed in: xorg-server-21.1.19 and xwayland-24.1.9 Fix: https://gitlab.freedesktop.org/xorg/xserver/-/commit/475d9f49 Found by: Jan-Niklas Sohn working with Trend Micro Zero Day Initiative. ------------------------------------------------------------------------ X.Org thanks all of those who reported and fixed these issues, and those who helped with the review and release of this advisory and these fixes. -------------- next part -------------- A non-text attachment was scrubbed... Name: OpenPGP_0x14706DBE1E4B4540.asc Type: application/pgp-keys Size: 2988 bytes Desc: OpenPGP public key URL: &amp;lt;https://lists.x.org/archives/xorg-announce/attachments/20251028/ff11c77e/attachment.key&amp;gt; -------------- next part -------------- A non-text attachment was scrubbed... Name: OpenPGP_signature.asc Type: application/pgp-signature Size: 203 bytes Desc: OpenPGP digital signature URL: &amp;lt;https://lists.x.org/archives/xorg-announce/attachments/20251028/ff11c77e/attachment.sig&amp;gt; &lt;/quote&gt;&lt;lb/&gt;More information about the xorg-announce
mailing list&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45790015</guid><pubDate>Sun, 02 Nov 2025 13:07:31 +0000</pubDate></item><item><title>Writing FreeDOS Programs in C</title><link>https://www.freedos.org/books/cprogramming/</link><description>&lt;doc fingerprint="3ceffed571b967d9"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;This project was backed by Patreon supporters&lt;/head&gt;
    &lt;p&gt;This web programming guide started out as a video series on YouTube, supported through Patreon. Patrons at the "C programming" level and above (Patreon) got access to these extras:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Early access to the "C programming" videos&lt;/item&gt;
      &lt;item&gt;Exclusive access to the rest of the "programming guide" with more detail and information that didn't make it into the videos&lt;/item&gt;
      &lt;item&gt;A weekly Patreon forum to ask questions about that week's "C programming" topics (if you were following along with the videos and need help, this was the place to ask)&lt;/item&gt;
      &lt;item&gt;After the video series was finished, I edited the programming guide into a "teach yourself programming" book, via publishing partner Lulu. Patrons could purchase the book at cost.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45790293</guid><pubDate>Sun, 02 Nov 2025 13:43:08 +0000</pubDate></item><item><title>Show HN: Anki-LLM – Bulk process and generate Anki flashcards with LLMs</title><link>https://github.com/raine/anki-llm</link><description>&lt;doc fingerprint="9c78ccdf08b67ab0"&gt;
  &lt;main&gt;
    &lt;p&gt;A CLI toolkit for bulk-processing and generating Anki flashcards with LLMs.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Bulk-verify translations – End-to-end pipeline for cleaning large decks. Read more&lt;/item&gt;
      &lt;item&gt;Add a Key Vocabulary field – Create a per-note field highlighting 1–3 key words with readings, meanings, and HTML context. Read more&lt;/item&gt;
      &lt;item&gt;Generate new cards – Interactively create multiple contextual flashcards for a vocabulary word or concept from a single command. Read more&lt;/item&gt;
      &lt;item&gt;Scriptable collection access – Query AnkiConnect directly from the CLI or AI agents. Command reference&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Manually editing hundreds or thousands of Anki cards is tedious, error-prone, and time-consuming. Whether it's verifying translations, adding grammar notes, or generating contextual examples, doing it by hand doesn't scale.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;anki-llm&lt;/code&gt; provides a bridge between your Anki collection and modern AI models.&lt;/p&gt;
    &lt;p&gt;Batch processing&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;File-based: Export deck to file, process with LLM, import results back to Anki.&lt;/item&gt;
      &lt;item&gt;Direct: Process and update notes in-place.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Card generation&lt;/p&gt;
    &lt;p&gt;Generate multiple contextual flashcard examples for a term, review interactively, and add selected cards to your deck.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Batch processing workflows: File-based (with resume) or direct-to-Anki (one command).&lt;/item&gt;
      &lt;item&gt;Export Anki decks to clean CSV or YAML files.&lt;/item&gt;
      &lt;item&gt;Batch process note fields using OpenAI or Google Gemini models.&lt;/item&gt;
      &lt;item&gt;Custom prompts: Use flexible template files to define exactly how the LLM should process your cards.&lt;/item&gt;
      &lt;item&gt;Concurrent processing: Make multiple parallel API requests to speed up large jobs.&lt;/item&gt;
      &lt;item&gt;Resilient: Automatically retries failed requests and saves progress incrementally (file mode).&lt;/item&gt;
      &lt;item&gt;Automatic resume: Pick up where you left off if processing is interrupted (file mode).&lt;/item&gt;
      &lt;item&gt;Copy mode: Alternatively, generate cards without API keys by pasting LLM responses from browser interfaces (ChatGPT, Claude, etc.).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Install globally via npm:&lt;/p&gt;
    &lt;code&gt;npm install -g anki-llm&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Node.js (v18 or higher)&lt;/item&gt;
      &lt;item&gt;Anki Desktop must be running.&lt;/item&gt;
      &lt;item&gt;The AnkiConnect add-on must be installed in Anki.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;code&gt;anki-llm&lt;/code&gt; uses LLM APIs to process your notes. You need to configure an API key
for the model provider you want to use.&lt;/p&gt;
    &lt;p&gt;The tool supports two API providers:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Input&lt;/cell&gt;
        &lt;cell role="head"&gt;Output&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OpenAI models&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;gpt-4.1&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;$2.50/M&lt;/cell&gt;
        &lt;cell&gt;$10.00/M&lt;/cell&gt;
        &lt;cell&gt;🔗&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;gpt-4o&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;$2.50/M&lt;/cell&gt;
        &lt;cell&gt;$10.00/M&lt;/cell&gt;
        &lt;cell&gt;🔗&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;gpt-4o-mini&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;$0.15/M&lt;/cell&gt;
        &lt;cell&gt;$0.60/M&lt;/cell&gt;
        &lt;cell&gt;🔗&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;gpt-5&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;$1.25/M&lt;/cell&gt;
        &lt;cell&gt;$10.00/M&lt;/cell&gt;
        &lt;cell&gt;🔗&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;gpt-5-mini&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;$0.25/M&lt;/cell&gt;
        &lt;cell&gt;$2.00/M&lt;/cell&gt;
        &lt;cell&gt;🔗&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;gpt-5-nano&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;$0.05/M&lt;/cell&gt;
        &lt;cell&gt;$0.40/M&lt;/cell&gt;
        &lt;cell&gt;🔗&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Google Gemini models&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;gemini-2.0-flash&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;$0.10/M&lt;/cell&gt;
        &lt;cell&gt;$0.40/M&lt;/cell&gt;
        &lt;cell&gt;🔗&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;gemini-2.5-flash&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;$0.30/M&lt;/cell&gt;
        &lt;cell&gt;$2.50/M&lt;/cell&gt;
        &lt;cell&gt;🔗&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;gemini-2.5-flash-lite&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;$0.10/M&lt;/cell&gt;
        &lt;cell&gt;$0.40/M&lt;/cell&gt;
        &lt;cell&gt;🔗&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;gemini-2.5-pro&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;$1.25/M&lt;/cell&gt;
        &lt;cell&gt;$10.00/M&lt;/cell&gt;
        &lt;cell&gt;🔗&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Pricing is per million tokens (M). Check the latest prices on the provider's website to be sure.&lt;/p&gt;
    &lt;p&gt;Set the appropriate environment variable for your chosen model provider:&lt;/p&gt;
    &lt;p&gt;For OpenAI models:&lt;/p&gt;
    &lt;code&gt;export OPENAI_API_KEY="your-api-key-here"&lt;/code&gt;
    &lt;p&gt;Get your API key from: https://platform.openai.com/api-keys&lt;/p&gt;
    &lt;p&gt;For Gemini models:&lt;/p&gt;
    &lt;code&gt;export GEMINI_API_KEY="your-api-key-here"&lt;/code&gt;
    &lt;p&gt;Get your API key from: https://aistudio.google.com/api-keys&lt;/p&gt;
    &lt;p&gt;Use &lt;code&gt;anki-llm config&lt;/code&gt; to store defaults (for example, the model) so you don't
have to repeat flags on every command.&lt;/p&gt;
    &lt;code&gt;# Set or override defaults
anki-llm config set model gpt-4o-mini&lt;/code&gt;
    &lt;p&gt;Config file lives at &lt;code&gt;~/.config/anki-llm/config.json&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;export&lt;/code&gt;- Export deck to file&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;import&lt;/code&gt;- Import data to deck&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;process-file&lt;/code&gt;- Process notes from file with AI&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;process-deck&lt;/code&gt;- Process notes from deck with AI&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;generate-init&lt;/code&gt;- Create prompt template for generate&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;generate&lt;/code&gt;- Generate new cards for a term&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;query&lt;/code&gt;- Query AnkiConnect API&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Exports notes from an Anki deck.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;&amp;lt;deck&amp;gt;&lt;/code&gt;: The name of the Anki deck to export (must be in quotes if it contains spaces).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;[output]&lt;/code&gt;: Optional output file path. If omitted, automatically generates a filename from the deck name (e.g.,&lt;code&gt;"My Deck"&lt;/code&gt;→&lt;code&gt;my-deck.yaml&lt;/code&gt;). You can also provide just a file extension (e.g.,&lt;code&gt;.csv&lt;/code&gt;) to auto-generate the filename with your preferred format.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;# Auto-generate filename with default .yaml format
anki-llm export "Japanese Core 1k"
# → japanese-core-1k.yaml

# Auto-generate filename with .csv format
anki-llm export "Japanese Core 1k" .csv
# → japanese-core-1k.csv

# Specify custom filename
anki-llm export "Japanese Core 1k" my-custom-name.yaml&lt;/code&gt;
    &lt;p&gt;Imports data from a file into an Anki deck. Existing notes (matched by key field) are updated, while new entries create new notes.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;&amp;lt;input&amp;gt;&lt;/code&gt;: Path to the data file to import (CSV or YAML).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Required options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-d, --deck&lt;/code&gt;: The name of the target Anki deck.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Common options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-n, --note-type&lt;/code&gt;: The Anki note type to use when creating new notes. If not specified, it will be inferred from existing notes in the deck.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-k, --key-field&lt;/code&gt;: Field to use for identifying existing notes. If not specified, auto-detects using this priority: (1)&lt;code&gt;noteId&lt;/code&gt;column if present, (2) first field of the note type, (3) error if neither found.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Batch-process notes from a CSV/YAML file using an LLM and user-defined prompts. This command saves the transformed results to an output file and features automatic resume, allowing it to safely skip completed notes if interrupted or re-run.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;&amp;lt;input&amp;gt;&lt;/code&gt;: Input file path (CSV or YAML).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Required options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-o, --output&lt;/code&gt;: Output file path (CSV or YAML).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-p, --prompt&lt;/code&gt;: Path to the prompt template text file.&lt;/item&gt;
      &lt;item&gt;Either &lt;code&gt;--field&lt;/code&gt;or&lt;code&gt;--json&lt;/code&gt;(mutually exclusive):&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;--field &amp;lt;name&amp;gt;&lt;/code&gt;: Update a single field with the AI response.&lt;/item&gt;&lt;item&gt;&lt;code&gt;--json&lt;/code&gt;: Expect JSON response and merge all fields into the note.&lt;/item&gt;&lt;item&gt;See Understanding &lt;code&gt;--field&lt;/code&gt;vs&lt;code&gt;--json&lt;/code&gt;modes for more details.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Common options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-m, --model&lt;/code&gt;: AI model to use (required unless set via&lt;code&gt;config set model&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-b, --batch-size&lt;/code&gt;: Number of concurrent API requests (default:&lt;code&gt;5&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-r, --retries&lt;/code&gt;: Number of retries for failed requests (default:&lt;code&gt;3&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-d, --dry-run&lt;/code&gt;: Preview the operation without making API calls (recommended for testing).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-f, --force&lt;/code&gt;: Re-process all rows, ignoring existing output.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--limit&lt;/code&gt;: Limit the number of new rows to process (useful for testing prompts on a small sample before processing large datasets).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--require-result-tag&lt;/code&gt;: Only extracts content from within&lt;code&gt;&amp;lt;result&amp;gt;&amp;lt;/result&amp;gt;&lt;/code&gt;tags in the AI response.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--log&lt;/code&gt;: Generate a log file with detailed debug information.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--very-verbose&lt;/code&gt;: Log full LLM responses to the log file (automatically enables&lt;code&gt;--log&lt;/code&gt;). Useful for debugging prompts and understanding model outputs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Workflow:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Export deck to file: &lt;code&gt;anki-llm export "My Deck" notes.yaml&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Process file: &lt;code&gt;anki-llm process-file notes.yaml -o output.yaml --field Translation -p prompt.txt -m gpt-4o-mini&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Import results: &lt;code&gt;anki-llm import output.yaml -d "My Deck"&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;# Process a file and update a single field
anki-llm process-file notes.yaml -o output.yaml --field Translation -p prompt.txt -m gpt-4o-mini

# Process with JSON mode (update multiple fields)
anki-llm process-file notes.yaml -o output.yaml --json -p prompt.txt -m gpt-4o-mini

# Test on 10 notes first (dry run)
anki-llm process-file notes.yaml -o output.yaml --field Translation -p prompt.txt --limit 10 --dry-run -m gpt-4o-mini

# Resume processing after interruption (automatic - just re-run the same command)
anki-llm process-file notes.yaml -o output.yaml --field Translation -p prompt.txt -m gpt-4o-mini

# Force re-process all notes (ignore existing output)
anki-llm process-file notes.yaml -o output.yaml --field Translation -p prompt.txt --force -m gpt-4o-mini&lt;/code&gt;
    &lt;p&gt;Key features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ Automatic resume: Skips already-processed notes&lt;/item&gt;
      &lt;item&gt;✅ Incremental saves: Progress saved continuously&lt;/item&gt;
      &lt;item&gt;✅ Review before import: You can inspect/edit the output file before importing&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When to use this command:&lt;/p&gt;
    &lt;p&gt;This command provides a file-based workflow for batch processing notes. It is the primary alternative to the &lt;code&gt;process-deck&lt;/code&gt; command, which modifies notes directly in your Anki collection.&lt;/p&gt;
    &lt;p&gt;Use &lt;code&gt;process-file&lt;/code&gt; instead of &lt;code&gt;process-deck&lt;/code&gt; when you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Require a manual review step. The command outputs to a file, creating a safe staging area to inspect results before you commit them to your Anki deck.&lt;/item&gt;
      &lt;item&gt;Need to process a large number of notes where interruptions are possible. Its resume capability ensures you don't lose progress if the process fails midway.&lt;/item&gt;
      &lt;item&gt;Are operating in an environment without a running Anki instance. This command is fully self-contained and does not need to connect to the Anki application.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Batch-process notes directly from an Anki deck using an LLM and user-defined prompts, updating them in-place. No intermediate files needed. This is faster and more convenient when you've tested your prompt and know the end result is safe to run.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;&amp;lt;deck&amp;gt;&lt;/code&gt;: Name of the Anki deck to process (must be in quotes if it contains spaces).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Required options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-p, --prompt&lt;/code&gt;: Path to the prompt template text file.&lt;/item&gt;
      &lt;item&gt;Either &lt;code&gt;--field&lt;/code&gt;or&lt;code&gt;--json&lt;/code&gt;(mutually exclusive):&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;--field &amp;lt;name&amp;gt;&lt;/code&gt;: Update a single field with the AI response.&lt;/item&gt;&lt;item&gt;&lt;code&gt;--json&lt;/code&gt;: Expect JSON response and merge all fields into the note.&lt;/item&gt;&lt;item&gt;See Understanding &lt;code&gt;--field&lt;/code&gt;vs&lt;code&gt;--json&lt;/code&gt;modes for more details.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Common options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-m, --model&lt;/code&gt;: AI model to use (required unless set via&lt;code&gt;config set model&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-b, --batch-size&lt;/code&gt;: Number of concurrent API requests (default:&lt;code&gt;5&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-r, --retries&lt;/code&gt;: Number of retries for failed requests (default:&lt;code&gt;3&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-d, --dry-run&lt;/code&gt;: Preview the operation without making API calls (recommended for testing).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--limit&lt;/code&gt;: Limit the number of notes to process (useful for testing prompts on a small sample before processing entire deck).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--require-result-tag&lt;/code&gt;: Only extracts content from within&lt;code&gt;&amp;lt;result&amp;gt;&amp;lt;/result&amp;gt;&lt;/code&gt;tags in the AI response.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--log&lt;/code&gt;: Generate a log file with detailed debug information.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--very-verbose&lt;/code&gt;: Log full LLM responses to the log file (automatically enables&lt;code&gt;--log&lt;/code&gt;). Useful for debugging prompts and understanding model outputs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Prerequisites:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Anki Desktop must be running&lt;/item&gt;
      &lt;item&gt;AnkiConnect add-on must be installed&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Workflow:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Single command: &lt;code&gt;anki-llm process-deck "My Deck" --field Translation -p prompt.txt -m gpt-4o-mini&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;# Process a deck directly and update a single field
anki-llm process-deck "Japanese Core 1k" --field Translation -p prompt.txt

# Direct mode with JSON (update multiple fields)
anki-llm process-deck "Vocabulary" --json -p prompt.txt

# Test on 10 notes first (recommended before processing entire deck)
anki-llm process-deck "My Deck" --field Notes -p prompt.txt --limit 10 --dry-run

# Use a different model for a specific run
anki-llm process-deck "Spanish" --field Translation -p prompt.txt&lt;/code&gt;
    &lt;p&gt;Key features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ No intermediate files: Process and update in one step&lt;/item&gt;
      &lt;item&gt;✅ Batch updates: Efficient bulk updates to Anki&lt;/item&gt;
      &lt;item&gt;✅ Error logging: Failed notes logged to &lt;code&gt;[deck-name]-errors.jsonl&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;❌ No resume support: Must complete in one run (use &lt;code&gt;process-file&lt;/code&gt;for large datasets)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both &lt;code&gt;process-file&lt;/code&gt; and &lt;code&gt;process-deck&lt;/code&gt; support two response formats for the LLM:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--field&lt;/code&gt;mode (single field update): The LLM response is saved to the specified field.&lt;quote&gt;anki-llm process-file notes.yaml -o out.yaml -p prompt.txt --field Translation&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--json&lt;/code&gt;mode (multi-field merge): The LLM must return valid JSON. All fields in the JSON are merged into your note.&lt;quote&gt;anki-llm process-file notes.yaml -o out.yaml -p prompt.txt --json&lt;/quote&gt;&lt;p&gt;Example: If your note has&lt;/p&gt;&lt;code&gt;Japanese&lt;/code&gt;and&lt;code&gt;Grammar&lt;/code&gt;fields, and the LLM returns:&lt;quote&gt;{ "Japanese": "こんにちは", "Grammar": "greeting" }&lt;/quote&gt;&lt;p&gt;Both fields will be updated. Only fields present in the JSON are updated (partial updates are allowed). If the response is not valid JSON, the operation will fail and retry.&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Interactively creates a prompt template file for the &lt;code&gt;generate&lt;/code&gt; command. The
wizard guides you through selecting a deck and note type, then uses an LLM to
analyze your existing cards and generate a tailored prompt that matches your
deck's style and formatting. This is the recommended way to get started with
card generation.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;[output]&lt;/code&gt;: Optional output file path. If omitted, automatically generates a filename from the deck name.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Common options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-m, --model&lt;/code&gt;: The LLM model to use for the smart prompt generation step (e.g.,&lt;code&gt;gemini-2.5-pro&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-t, --temperature&lt;/code&gt;: Temperature for LLM generation (0.0-2.0, default varies by model). Lower values produce more consistent output.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--copy&lt;/code&gt;: Copy the LLM prompt to clipboard and wait for manual response pasting. Useful when you don't have API access and want to use a browser LLM interface like ChatGPT.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;Using a more capable reasoning model like &lt;code&gt;gemini-2.5-pro&lt;/code&gt; for the
&lt;code&gt;generate-init&lt;/code&gt; step can produce higher-quality prompt templates that better
capture the nuances and style of your existing cards.&lt;/p&gt;
    &lt;p&gt;Workflow:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Run the wizard: &lt;code&gt;anki-llm generate-init&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Follow the interactive steps to select a deck and note type.&lt;/item&gt;
      &lt;item&gt;A prompt file (e.g., &lt;code&gt;my-deck-prompt.md&lt;/code&gt;) is created for you.&lt;/item&gt;
      &lt;item&gt;Review and customize the generated prompt file.&lt;/item&gt;
      &lt;item&gt;Use the file with the &lt;code&gt;generate&lt;/code&gt;command:&lt;code&gt;anki-llm generate "term" -p my-deck-prompt.md&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Generates multiple new Anki card examples for a given term, lets you review and select which ones to keep, and adds them directly to your deck.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;&amp;lt;term&amp;gt;&lt;/code&gt;: The word or phrase to generate cards for (must be in quotes if it contains spaces).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Required options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-p, --prompt&lt;/code&gt;: Path to the prompt template file (created with&lt;code&gt;generate-init&lt;/code&gt;).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Common options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-c, --count&lt;/code&gt;: Number of card examples to generate (default:&lt;code&gt;3&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-m, --model&lt;/code&gt;: AI model to use (defaults to&lt;code&gt;gpt-5-mini&lt;/code&gt;or&lt;code&gt;gemini-2.5-flash&lt;/code&gt;depending on your API key; can also be set via&lt;code&gt;config set model&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-d, --dry-run&lt;/code&gt;: Display generated cards without starting the interactive selection or import process.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-r, --retries&lt;/code&gt;: Number of retries for failed requests (default:&lt;code&gt;3&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-t, --temperature&lt;/code&gt;: LLM temperature, a value between 0 and 2 that controls creativity (default:&lt;code&gt;1.0&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--max-tokens&lt;/code&gt;: Set a maximum number of tokens for the LLM response.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-o, --output&lt;/code&gt;: Export cards to a file instead of importing to Anki (e.g.,&lt;code&gt;cards.yaml&lt;/code&gt;,&lt;code&gt;cards.csv&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--log&lt;/code&gt;: Enable logging of LLM responses to a file (useful for debugging).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--copy&lt;/code&gt;: Copy the LLM prompt to clipboard and wait for manual response pasting. Useful when you don't have API access and want to use a browser LLM interface like ChatGPT.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The &lt;code&gt;--prompt&lt;/code&gt; file is a text or markdown file that contains two parts: YAML
frontmatter for configuration and a prompt body with instructions for the LLM.&lt;/p&gt;
    &lt;p&gt;Frontmatter (Required)&lt;/p&gt;
    &lt;p&gt;The frontmatter is a YAML block at the top of the file enclosed by &lt;code&gt;---&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;deck&lt;/code&gt;: The target Anki deck name.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;noteType&lt;/code&gt;: The name of the Anki note type (model) to use.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;fieldMap&lt;/code&gt;: Maps the keys from the LLM's JSON output to your actual Anki field names. The LLM will be instructed to generate JSON with the keys on the left, and&lt;code&gt;anki-llm&lt;/code&gt;will use them to populate the Anki fields on the right.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Prompt Body&lt;/p&gt;
    &lt;p&gt;The body contains your instructions for the LLM. It must:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Include the &lt;code&gt;{term}&lt;/code&gt;placeholder, which will be replaced by the&lt;code&gt;&amp;lt;term&amp;gt;&lt;/code&gt;you provide on the command line.&lt;/item&gt;
      &lt;item&gt;Include the &lt;code&gt;{count}&lt;/code&gt;placeholder, which will be replaced by the number of cards requested.&lt;/item&gt;
      &lt;item&gt;Instruct the LLM to return a JSON array of objects, where each object represents one card and uses the keys defined in &lt;code&gt;fieldMap&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Include a "one-shot" example showing the exact JSON array structure and desired formatting (e.g., HTML for bolding or lists).&lt;/item&gt;
      &lt;item&gt;Encourage the LLM to generate diverse cards that highlight different nuances, contexts, or usage examples of the term.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example Prompt File (&lt;code&gt;japanese-vocab-prompt.md&lt;/code&gt;)&lt;/p&gt;
    &lt;code&gt;---
deck: Japanese::Vocabulary
noteType: Japanese (recognition)
fieldMap:
  en: English
  jp: Japanese
  context: Context
---

You are an expert assistant who creates {count} distinct Anki flashcards for a
Japanese vocabulary word. The term to create cards for is: **{term}**

IMPORTANT: Your output must be a single, valid JSON array of objects and nothing
else. Each object in the array should represent a unique flashcard. All field
values must be strings.

Follow the structure shown in this example precisely:

```json
[
  {
    "en": "How was your day?",
    "jp": "今日はどうでしたか？",
    "context": "A natural and common way to ask about someone's day politely. You can say 「今日どうだった？」 in casual speech."
  }
]
```

Return only a valid JSON array matching this structure. Ensure you generate
{count} varied and high-quality cards that highlight different nuances, contexts,
or usage examples of the term.&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;--copy&lt;/code&gt; flag allows you to generate cards without API keys by manually
copying prompts to a browser-based LLM interface (like ChatGPT, Claude, Gemini,
etc.) and pasting responses back.&lt;/p&gt;
    &lt;p&gt;Workflow:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Run the command with &lt;code&gt;--copy&lt;/code&gt;:&lt;code&gt;anki-llm generate "今日" -p prompt.md --copy&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;The program automatically copies the LLM prompt to your clipboard.&lt;/item&gt;
      &lt;item&gt;Paste the prompt into your preferred LLM interface (ChatGPT, Claude, etc.).&lt;/item&gt;
      &lt;item&gt;Copy the complete JSON response from the LLM.&lt;/item&gt;
      &lt;item&gt;Paste it into the terminal.&lt;/item&gt;
      &lt;item&gt;Type &lt;code&gt;END&lt;/code&gt;on a new line and press Enter to submit.&lt;/item&gt;
      &lt;item&gt;The program validates and processes your cards normally.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Benefits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No API key required&lt;/item&gt;
      &lt;item&gt;Use any LLM interface you prefer&lt;/item&gt;
      &lt;item&gt;Works with free-tier LLM services&lt;/item&gt;
      &lt;item&gt;Full control over the LLM interaction&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;# Generate 3 cards for a term using a prompt file
anki-llm generate "新しい" -p japanese-vocab-prompt.md

# Generate 5 cards and preview them without importing
anki-llm generate "ambiguous" -p english-vocab-prompt.md --count 5 --dry-run

# Use a different model for a specific run
anki-llm generate "maison" -p french-prompt.md -m gemini-2.5-pro

# Generate cards and export to YAML for later review/import
anki-llm generate "今日" -p japanese-vocab-prompt.md -o cards.yaml

# Import the exported cards when ready
anki-llm import cards.yaml --deck "Japanese::Vocabulary"

# Enable logging for debugging
anki-llm generate "新しい" -p prompt.md --log

# Use manual copy-paste mode (no API key required)
anki-llm generate "今日" -p japanese-vocab-prompt.md --copy&lt;/code&gt;
    &lt;p&gt;Key features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ Interactive selection: Review and choose which generated cards to keep.&lt;/item&gt;
      &lt;item&gt;✅ Duplicate detection: Automatically flags cards that may already exist in your deck.&lt;/item&gt;
      &lt;item&gt;✅ Export option: Save generated cards to YAML/CSV for review before importing.&lt;/item&gt;
      &lt;item&gt;✅ Highly customizable: Full control over card generation via the prompt file.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Query the AnkiConnect API directly with any supported action. This command is especially useful for AI agents (like Claude Code) to explore and interact with your Anki collection programmatically.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;&amp;lt;action&amp;gt;&lt;/code&gt;: The AnkiConnect API action to perform (e.g.,&lt;code&gt;deckNames&lt;/code&gt;,&lt;code&gt;findNotes&lt;/code&gt;,&lt;code&gt;cardsInfo&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;[params]&lt;/code&gt;: Optional JSON string of parameters for the action.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why this is useful for AI agents:&lt;/p&gt;
    &lt;p&gt;AI assistants can use this command to dynamically query your Anki collection without you having to manually provide information. For example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"List all my decks" → &lt;code&gt;anki-llm query deckNames&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;"Show me statistics for my Japanese deck" → &lt;code&gt;anki-llm query getDeckStats '{"decks":["Japanese"]}'&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;"Find all cards with tag 'vocabulary'" → &lt;code&gt;anki-llm query findNotes '{"query":"tag:vocabulary"}'&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The command outputs clean JSON that AI agents can parse and reason about, making it easy to build custom workflows or answer questions about your Anki collection.&lt;/p&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;# Get all deck names
anki-llm query deckNames

# Get all model (note type) names
anki-llm query modelNames

# Find notes in a specific deck
anki-llm query findNotes '{"query":"deck:Japanese"}'

# Get detailed information about specific cards
anki-llm query cardsInfo '{"cards":[1498938915662]}'

# Get statistics for a deck
anki-llm query getDeckStats '{"decks":["Default"]}'

# Check AnkiConnect version
anki-llm query version

# Get full AnkiConnect API documentation (useful for AI agents to understand available actions)
anki-llm query docs&lt;/code&gt;
    &lt;p&gt;Example: Sampling random cards from decks&lt;/p&gt;
    &lt;p&gt;AI agents can use &lt;code&gt;anki-llm query&lt;/code&gt; to discover information about your
collection and then take action. Here's an example of Claude Code using the
&lt;code&gt;query&lt;/code&gt; command to sample random cards from multiple decks. Given the
instruction: "Use anki-llm to pick random cards from Glossika decks, and print
the English and Japanese fields for each, pick 10 cards from each deck, and
save to a markdown file"&lt;/p&gt;
    &lt;p&gt;This demonstrates how the &lt;code&gt;query&lt;/code&gt; command enables AI agents to build custom
scripts for data analysis and extraction tasks autonomously.&lt;/p&gt;
    &lt;p&gt;Special actions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;docs&lt;/code&gt;or&lt;code&gt;help&lt;/code&gt;: Returns the complete AnkiConnect API documentation. This is especially useful for AI agents that need to understand what actions are available and how to use them. The agent can query this once to get the full documentation and then use that context to make informed decisions about which API calls to make.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See ANKI_CONNECT.md for the complete list of available actions and their parameters.&lt;/p&gt;
    &lt;p&gt;Let's say you have an Anki deck named "Japanese Core 1k" with 1000 notes. Each note has a &lt;code&gt;Japanese&lt;/code&gt; field with a sentence and a &lt;code&gt;Translation&lt;/code&gt; field with an
English translation that you suspect is inaccurate. We'll use &lt;code&gt;anki-llm&lt;/code&gt; and
GPT-4o mini to generate better translations for all 1000 notes.&lt;/p&gt;
    &lt;p&gt;First, export the notes from your Anki deck into a YAML file. YAML is great for multiline text fields and for using &lt;code&gt;git diff&lt;/code&gt; to see what has changed after
processing is complete.&lt;/p&gt;
    &lt;code&gt;anki-llm export "Japanese Core 1k" notes.yaml&lt;/code&gt;
    &lt;p&gt;This command will connect to Anki, find all notes in that deck, and save them to a YAML file.&lt;/p&gt;
    &lt;code&gt;============================================================
Exporting deck: Japanese Core 1k
============================================================

✓ Found 1000 notes in 'Japanese Core 1k'.

Discovering model type and fields...
✓ Model type: Japanese Model
✓ Fields: Japanese, Translation, Reading, Sound, noteId

Fetching note details...
✓ Retrieved information for 1000 notes.

Writing to notes.yaml...
✓ Successfully exported 1000 notes to notes.yaml
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;notes.yaml&lt;/code&gt; file will look something like this:&lt;/p&gt;
    &lt;code&gt;- noteId: 1512345678901
  Japanese: 猫は机の上にいます。
  Translation: The cat is on the desk.
- noteId: 1512345678902
  Japanese: 彼は毎日公園を散歩します。
  Translation: He strolls in the park every day.
# ... 998 more notes&lt;/code&gt;
    &lt;p&gt;Next, create a prompt file (&lt;code&gt;prompt-ja-en.txt&lt;/code&gt;) to instruct the AI. Use
&lt;code&gt;{field_name}&lt;/code&gt; syntax for variables that will be replaced with data from each
note. We want to process the &lt;code&gt;Japanese&lt;/code&gt; field.&lt;/p&gt;
    &lt;p&gt;File: &lt;code&gt;prompt-ja-en.txt&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;You are an expert Japanese-to-English translator.

Translate this Japanese sentence to English: {Japanese}

Guidelines:
- Translate accurately while preserving nuance and meaning.
- Be natural and idiomatic in English.
- If possible, structure the English so the original Japanese grammar can be inferred.

Instructions:
1. First, analyze the sentence structure and key elements.
2. Think through the translation choices and any nuances.
3. Provide your final translation wrapped in &amp;lt;result&amp;gt;&amp;lt;/result&amp;gt; XML tags.

Format your response like this:
- Analysis: [your analysis of the sentence]
- Translation considerations: [your thought process]
- &amp;lt;result&amp;gt;[your final English translation here]&amp;lt;/result&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;&amp;lt;result&amp;gt;&lt;/code&gt; tag (used with &lt;code&gt;--require-result-tag&lt;/code&gt;) is optional. You could instruct the LLM to respond with only the translation directly. However, asking the model to "think out loud" by analyzing the sentence first tends to produce higher-quality translations, as it encourages deeper reasoning before generating the final output.&lt;/p&gt;
    &lt;p&gt;Now, run the &lt;code&gt;process-file&lt;/code&gt; command. We'll tell it to use our &lt;code&gt;notes.yaml&lt;/code&gt; file
as input, write to a new &lt;code&gt;notes-translated.yaml&lt;/code&gt; file, process the &lt;code&gt;Translation&lt;/code&gt;
field, and use our prompt template.&lt;/p&gt;
    &lt;p&gt;The tool will read the &lt;code&gt;Japanese&lt;/code&gt; field from each note to fill the prompt, then
the AI's response will overwrite the &lt;code&gt;Translation&lt;/code&gt; field.&lt;/p&gt;
    &lt;code&gt;anki-llm process-file notes.yaml \
  --output notes-translated.yaml \
  --field Translation \
  --prompt prompt-ja-en.txt \
  --model gemini-2.5-flash \
  --batch-size 10 \
  --require-result-tag&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;notes.yaml&lt;/code&gt;: The input file.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--output notes-translated.yaml&lt;/code&gt;: The output file.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--field Translation&lt;/code&gt;: The field we want the AI to generate and place its result into.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--prompt prompt-ja-en.txt&lt;/code&gt;: Our instruction template.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--model gemini-2.5-flash&lt;/code&gt;: The AI model to use.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--batch-size 10&lt;/code&gt;: Process 10 notes concurrently for speed.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--require-result-tag&lt;/code&gt;: Ensures the tool only saves the content inside the&lt;code&gt;&amp;lt;result&amp;gt;&lt;/code&gt;tag, ignoring the AI's analysis.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You will see real-time progress as it processes the notes:&lt;/p&gt;
    &lt;code&gt;============================================================
File-Based Processing
============================================================
Input file:        notes.yaml
Output file:       notes-translated.yaml
Field to process:  Translation
Model:             gpt-4o-mini
Batch size:        10
...
============================================================

Reading notes.yaml...
✓ Found 1000 rows in YAML

Loading existing output...
✓ Found 0 already-processed rows

Processing 1000 rows...
Processing |████████████████████████████████████████| 100% | 1000/1000 rows | Cost: $0.0234 | Tokens: 152340

✓ Processing complete

============================================================
Summary
============================================================
- Successes:         1000
- Failures:          0
- Total Processed:   1000
- Total Time:        85.32s
- Model:             gpt-4o-mini
- Dry Run:           false
---
- Total Tokens:      152,340
- Input Tokens:      120,100
- Output Tokens:     32,240
- Est. Cost:         $0.02
============================================================
&lt;/code&gt;
    &lt;p&gt;The final step is to import the newly generated translations back into Anki. The tool uses the &lt;code&gt;noteId&lt;/code&gt; to find and update the existing notes.&lt;/p&gt;
    &lt;code&gt;anki-llm import notes-translated.yaml --deck "Japanese Core 1k"&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;notes-translated.yaml&lt;/code&gt;: The file with our improved translations.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--deck "Japanese Core 1k"&lt;/code&gt;: The destination deck.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The model type will be automatically inferred from the existing notes in the deck. You can also explicitly specify it with &lt;code&gt;--model "Japanese Model"&lt;/code&gt; if
needed.&lt;/p&gt;
    &lt;code&gt;============================================================
Importing from notes-translated.yaml to deck: Japanese Core 1k
Model: Japanese Model
Key field: noteId
============================================================

✓ Found 1000 rows in notes-translated.yaml.

✓ Valid fields to import: Japanese, Translation, Reading, Sound

✓ Found 1000 existing notes with a 'noteId' field.

✓ Partitioning complete:
  - 0 new notes to add.
  - 1000 existing notes to update.

Updating 1000 existing notes...
✓ Update operation complete: 1000 notes updated successfully.

Import process finished.
&lt;/code&gt;
    &lt;p&gt;That's it! All 1000 notes in your Anki deck have now been updated with high-quality translations.&lt;/p&gt;
    &lt;p&gt;Sentence flashcards often benefit from a focused vocabulary breakdown. You can use &lt;code&gt;anki-llm&lt;/code&gt; to populate a dedicated &lt;code&gt;Key Vocabulary&lt;/code&gt; field with structured
HTML that spotlights the most important words in each sentence.&lt;/p&gt;
    &lt;p&gt;Create a prompt that instructs the model to reason about the sentence, pick the top 1–3 items, and return clean HTML. This example assumes your notes have &lt;code&gt;Japanese&lt;/code&gt; and &lt;code&gt;English&lt;/code&gt; fields. You can start from the full prompt example in
&lt;code&gt;examples/key_vocabulary.md&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;File: &lt;code&gt;prompt-key-vocab.txt&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;You are an expert Japanese vocabulary AI assistant designed for language learners. Your primary role is to analyze Japanese sentences, identify the most significant vocabulary words, and produce clear, concise, and educational explanations formatted in clean, semantic HTML.

The user is an intermediate learner who uses sentence flashcards to practice. Your output will populate a "Key Vocabulary" field on their Anki flashcard. The HTML you generate must be well-structured to allow for easy and flexible styling with CSS.

English: {English}
Japanese: {Japanese}

Analysis: Explain which vocabulary items you chose and why they matter for an intermediate learner.
Always produce between 1 and 3 key vocabulary entries using the following HTML structure (use dictionary form in the heading and include the kana reading in parentheses):

&amp;lt;h3&amp;gt;WORD (reading)&amp;lt;/h3&amp;gt;
&amp;lt;dl class="vocab-entry"&amp;gt;
  &amp;lt;dt&amp;gt;Type&amp;lt;/dt&amp;gt;
  &amp;lt;dd&amp;gt;Part of speech&amp;lt;/dd&amp;gt;

  &amp;lt;dt&amp;gt;Meaning&amp;lt;/dt&amp;gt;
  &amp;lt;dd&amp;gt;Concise English definition&amp;lt;/dd&amp;gt;

  &amp;lt;dt&amp;gt;Context&amp;lt;/dt&amp;gt;
  &amp;lt;dd&amp;gt;Sentence-specific explanation, including any conjugation or nuance notes.&amp;lt;/dd&amp;gt;
&amp;lt;/dl&amp;gt;

Replace the placeholder content with the actual vocabulary analysis. Within the `&amp;lt;result&amp;gt;` tags, output only the completed HTML entries with no additional commentary.

&amp;lt;result&amp;gt;
&amp;lt;/result&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Process your exported notes and overwrite the &lt;code&gt;Key Vocabulary&lt;/code&gt; field with the
HTML generated by the prompt:&lt;/p&gt;
    &lt;code&gt;anki-llm process-file sentences.yaml \
  --output sentences-key-vocab.yaml \
  --field "Key Vocabulary" \
  --prompt prompt-key-vocab.txt \
  --model gemini-2.5-flash-lite \
  --require-result-tag&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--field "Key Vocabulary"&lt;/code&gt;: Updates that specific field on each note.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--require-result-tag&lt;/code&gt;: Keeps only the HTML between&lt;code&gt;&amp;lt;result&amp;gt;&lt;/code&gt;tags and drops the analysis from the prompt.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When you open the processed YAML/CSV, the generated field will look like this:&lt;/p&gt;
    &lt;code&gt;Key Vocabulary: |
  &amp;lt;h3&amp;gt;控える (ひかえる)&amp;lt;/h3&amp;gt;
  &amp;lt;dl class="vocab-entry"&amp;gt;
    &amp;lt;dt&amp;gt;Type&amp;lt;/dt&amp;gt;
    &amp;lt;dd&amp;gt;Ichidan verb&amp;lt;/dd&amp;gt;

    &amp;lt;dt&amp;gt;Meaning&amp;lt;/dt&amp;gt;
    &amp;lt;dd&amp;gt;To refrain; to hold back&amp;lt;/dd&amp;gt;

    &amp;lt;dt&amp;gt;Context&amp;lt;/dt&amp;gt;
    &amp;lt;dd&amp;gt;Appears as 控えていて, the te-form plus いる to show an ongoing act of self-restraint in the scene.&amp;lt;/dd&amp;gt;
  &amp;lt;/dl&amp;gt;

  &amp;lt;h3&amp;gt;さっぱり (さっぱり)&amp;lt;/h3&amp;gt;
  &amp;lt;dl class="vocab-entry"&amp;gt;
    &amp;lt;dt&amp;gt;Type&amp;lt;/dt&amp;gt;
    &amp;lt;dd&amp;gt;Adverb&amp;lt;/dd&amp;gt;

    &amp;lt;dt&amp;gt;Meaning&amp;lt;/dt&amp;gt;
    &amp;lt;dd&amp;gt;Completely; entirely (with a nuance of 'not at all' when paired with negatives)&amp;lt;/dd&amp;gt;

    &amp;lt;dt&amp;gt;Context&amp;lt;/dt&amp;gt;
    &amp;lt;dd&amp;gt;Modifies わからない to emphasize that the speaker has absolutely no understanding.&amp;lt;/dd&amp;gt;
  &amp;lt;/dl&amp;gt;&lt;/code&gt;
    &lt;p&gt;After verifying the results, import the updated file back into Anki to add the structured vocabulary explanations to your cards.&lt;/p&gt;
    &lt;p&gt;Let's create several new example flashcards for the Japanese word &lt;code&gt;会議&lt;/code&gt;
(meeting) and add them to our "Japanese::Vocabulary" deck.&lt;/p&gt;
    &lt;p&gt;First, run the &lt;code&gt;generate-init&lt;/code&gt; wizard. It will ask you to select your deck and
note type, then use an LLM to analyze your existing cards and generate a prompt
file tailored to your collection.&lt;/p&gt;
    &lt;code&gt;anki-llm generate-init&lt;/code&gt;
    &lt;p&gt;Follow the interactive prompts. The wizard will use an AI model (defaults to &lt;code&gt;gpt-5&lt;/code&gt; or &lt;code&gt;gemini-2.5-flash&lt;/code&gt; depending on your API key) to analyze existing
cards in your deck and create a smart prompt that matches their style and
formatting. When it's done, it will save a new file, for example
&lt;code&gt;japanese-vocabulary-prompt.md&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The generated file will look something like this:&lt;/p&gt;
    &lt;p&gt;File: &lt;code&gt;japanese-vocabulary-prompt.md&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;---
deck: Japanese::Vocabulary
noteType: Japanese (recognition)
fieldMap:
  en: English
  jp: Japanese
  context: Context
---

You are an expert Japanese language assistant. Your goal is to create {count}
distinct, high-quality, contextual Anki flashcards for a vocabulary term. The
term to create cards for is: **{term}**

IMPORTANT: Your output must be a single, valid JSON array of objects and nothing
else. Each object in the array should represent a unique flashcard. All field
values must be strings. Use `&amp;lt;b&amp;gt;` tags to highlight the term in the example
sentence.

Follow the structure shown in this example precisely:

```json
[
  {
    "en": "The &amp;lt;b&amp;gt;meeting&amp;lt;/b&amp;gt; is scheduled for 3 PM.",
    "jp": "&amp;lt;b&amp;gt;会議&amp;lt;/b&amp;gt;は午後3時に予定されています。",
    "context": "Used in a formal business context."
  }
]
```

Return only a valid JSON array matching this structure. Ensure you generate
{count} varied cards that highlight different nuances, contexts, or usage
examples of the term.&lt;/code&gt;
    &lt;p&gt;You can now edit this file to further refine the instructions for the AI.&lt;/p&gt;
    &lt;p&gt;Now, use the &lt;code&gt;generate&lt;/code&gt; command with your new prompt file to create card
examples for &lt;code&gt;会議&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;anki-llm generate "会議" -p japanese-vocabulary-prompt.md -m gemini-2.5-flash&lt;/code&gt;
    &lt;p&gt;The tool will make a single API call asking the LLM to generate 3 distinct cards and show progress:&lt;/p&gt;
    &lt;code&gt;🔄 Generating 3 card candidates for "会議"...
✓ Generation complete: 3 succeeded, 0 failed
  Cost: $0.0027 (930 input + 954 output tokens)

🔍 Checking for duplicates...
&lt;/code&gt;
    &lt;p&gt;After generation, an interactive checklist appears in your terminal. You can use the arrow keys and spacebar to select the cards you want to add to Anki.&lt;/p&gt;
    &lt;code&gt;📋 Select cards to add to Anki (use Space to select, Enter to confirm):

❯ ◯ Card 1
│     English: The *meeting* was very productive.
│     Japanese: その*会議*は非常に生産的でした。
│     Context: General business context.
│
│ ◯ Card 2
│     English: I have a *meeting* with a client tomorrow.
│     Japanese: 明日、クライアントとの*会議*があります。
│     Context: A common phrase for scheduling.
│
│ ◯ Card 3 (⚠️  Duplicate)
│     English: The *meeting* is scheduled for 3 PM.
│     Japanese: *会議*は午後3時に予定されています。
│     Context: Used in a formal business context.
&lt;/code&gt;
    &lt;p&gt;Here, we see three options. Card 3 has been flagged as a potential duplicate because a similar card already exists in the deck. Let's select the first two cards and press Enter.&lt;/p&gt;
    &lt;p&gt;The selected cards are immediately added to your Anki deck.&lt;/p&gt;
    &lt;code&gt;📥 Adding 2 card(s) to Anki...

✓ Successfully added 2 new note(s) to "Japanese::Vocabulary"
&lt;/code&gt;
    &lt;p&gt;That's it! You have successfully generated, reviewed, and imported multiple high-quality, contextual Anki cards from a single command.&lt;/p&gt;
    &lt;p&gt;Use &lt;code&gt;tsx&lt;/code&gt; to run the CLI directly from TypeScript source without rebuilding:&lt;/p&gt;
    &lt;code&gt;pnpm tsx src/cli.ts export "My Deck" notes.yaml&lt;/code&gt;
    &lt;p&gt;Use &lt;code&gt;pnpm link&lt;/code&gt; to test the command globally:&lt;/p&gt;
    &lt;code&gt;pnpm link --global
anki-llm export "My Deck" notes.yaml&lt;/code&gt;
    &lt;p&gt;Note: The linked command uses compiled JavaScript from &lt;code&gt;dist/&lt;/code&gt;. Run
&lt;code&gt;pnpm run build&lt;/code&gt; after making changes to see them reflected.&lt;/p&gt;
    &lt;p&gt;To unlink: &lt;code&gt;pnpm unlink --global&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;pnpm run check&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45790443</guid><pubDate>Sun, 02 Nov 2025 14:04:07 +0000</pubDate></item><item><title>Why don't you use dependent types?</title><link>https://lawrencecpaulson.github.io//2025/11/02/Why-not-dependent.html</link><description>&lt;doc fingerprint="8548021f0f0bb7f0"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;"Why don't you use dependent types?"&lt;/head&gt;[&lt;code&gt;&lt;nobr&gt;memories&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;AUTOMATH&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;LCF&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;type theory&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;Martin-Löf type theory&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;NG de Bruijn&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;ALEXANDRIA&lt;/nobr&gt;&lt;/code&gt; 
  
]

&lt;p&gt;To be fair, nobody asks me this exact question. But people have regularly asked why Isabelle dispenses with proof objects. The two questions are essentially the same, because proof objects are intrinsic to all the usual type theories. They are also completely unnecessary and a huge waste of space. As described in an earlier post, type checking in the implementation language (rather than in the logic) can ensure that only legitimate proof steps are executed. Robin Milner had this fundamental insight 50 years ago, giving us the LCF architecture with its proof kernel. But the best answer to the original question is simply this: I did use dependent types, for years.&lt;/p&gt;&lt;head rend="h3"&gt;My time with AUTOMATH&lt;/head&gt;&lt;p&gt;I was lucky enough to get some personal time with N G de Bruijn when he came to Caltech in 1977 to lecture about AUTOMATH. I never actually got to use this system. Back then, researchers used the nascent Internet (the ARPAnet) not to download software so much as to run software directly on the host computer, since most software was not portable. But Eindhoven University was not on the ARPAnet, and AUTOMATH was configured to run on a computer we did not have:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Until September 1973, the computer was the Electrologica X8, after that Burroughs 6700. In both cases the available multiprogranming systems required the use of ALGOL 60.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I did however read many of the research reports, including the PhD dissertation by LS Jutting, where he presents his translation of Landau’s text Grundlagen der Analysis (described last time) from German into AUTOMATH. It is no coincidence that many of my papers, from the earliest to the latest, copied the idea of formalising a text and attempting to be faithful to it, if possible line by line.&lt;/p&gt;&lt;p&gt;As an aside, note that while AUTOMATH was a system of dependent types, it did not embody the Curry–Howard correspondence (sometimes wrongly called the Curry–Howard–de Bruijn correspondence). That correspondence involves having a type theory strong enough to represent the predicate calculus directly in the form of types. In AUTOMATH you had to introduce the symbols and inference rules of your desired calculus in the form of axioms, much as you do with Isabelle. In short, AUTOMATH was a logical framework:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;like a big restaurant that serves all sorts of food: vegetarian, kosher, or anything else the customer wants&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;De Bruijn did not approve of the increasingly powerful type theories being developed in the 1990s. AUTOMATH was a weak language, a form of λ-calculus including a general product construction just powerful enough to express the inference rules of a variety of formalisms and to make simple definitions, again clearly the inspiration for Isabelle. Isabelle aims to be generic, like the big AUTOMATH restaurant. Only these days everybody prefers the same cuisine, higher-order logic, so Isabelle/HOL has become dominant. Unfortunately, I last spoke to Dick (as he was known to friends) when I was putting all my effort into Isabelle/ZF. He simply loathed set theory and saw mathematics as essentially typed. He never lived to see the enormous amount of advanced mathematics that would be formalised using types in Isabelle/HOL.&lt;/p&gt;&lt;p&gt;I annoyed him in another way. I kept asking, AUTOMATH looks natural, but how do we know that it is right? He eventually sent me a 300 page volume entitled The Language Theory of Automath. It describes AUTOMATH’s formal properties such as strong normalisation and Church–Rosser properties, but this was not the answer I wanted at all. I got that answer for a quite different type theory.&lt;/p&gt;&lt;head rend="h3"&gt;Martin-Löf type theory&lt;/head&gt;&lt;p&gt;In response to kind invitations from Bengt Nordström and Kent Petersson, I paid a number of visits to Chalmers University in Gothenburg to learn about Martin-Löf type theory. I was particularly impressed by its promise of a systematic and formal approach to program synthesis. I had already encountered intuitionism through a course on the philosophy of mathematics at Stanford University, as I recall taught by Ian Hacking. The “rightness” of Martin-Löf type theory was obvious, because it directly embodied the principles of intuition truth as outlined by Heyting: for example, that a proof of $A\land B$ consists of a proof of $A$ paired with a proof of $B$.&lt;/p&gt;&lt;p&gt;I devoted several years of research to Martin-Löf type theory. This included a whole year of intricate hand derivations to produce a paper that I once thought would be important, and the very first version of Isabelle. Yes: Isabelle began as an implementation of Martin-Löf type theory, which is still included in the distribution even today as Isabelle/CTT. But eventually I tired of what seemed to me a doctrinaire attitude bordering on a cult of personality around Per Martin-Löf. The sudden switch to intensional equality (everyone was expected to adopt the new approach) wrecked most of my work. Screw that.&lt;/p&gt;&lt;p&gt;You might ask, what about the calculus of constructions, which arose during that time and eventually gave us Rocq and Lean? (Not to mention LEGO.) To me they raised, and continue to raise, the same question I had put to de Bruijn. Gérard Huet said something like “it is nothing but function application”, which did not convince me. It’s clear that I am being fussy,1 because thousands of people find these formalisms perfectly natural and believable. But it is also true that the calculus of constructions underwent numerous changes over the past four decades. There seem to be several optional axioms that people sometimes adopt while attempting to minimise their use, like dieters enjoying an occasional croissant.&lt;/p&gt;&lt;head rend="h3"&gt;Decisions, decisions&lt;/head&gt;&lt;p&gt;We can see all this as an example of the choices we make in research. People were developing new formalisms. This specific fact was the impetus for making Isabelle generic in the first place. But we have to choose whether to spend our time developing formalisms or instead to choose a fixed formalism and see how far you can push it. Both are legitimate research goals.&lt;/p&gt;&lt;p&gt;For example, already in 1985, Mike Gordon was using higher-order logic to verify hardware. He was not distracted by the idea that some dependent type theory might work better for n-bit words and the like. The formalism that he implemented was essentially the same as the simple theory of types outlined by Alonzo Church in 1940. He made verification history using this venerable formalism, and John Harrison later found a clever way to encode the dimension of vector types including words. Isabelle/HOL also implements Church’s simple type theory, with one extension: axiomatic type classes. Isabella users also derive much power from the locale concept, a kind of module sysstem that lies outside any particular logic.&lt;/p&gt;&lt;p&gt;During all this time, both Martin-Löf type theory and the calculus of constructions went through several stages of evolution. It’s remarkable how the Lean community, by running with a certain version of the calculus, quickly formalised a vast amount of mathematics.&lt;/p&gt;&lt;head rend="h3"&gt;Pushing higher-order logic to its limit&lt;/head&gt;&lt;p&gt;I felt exceptionally lucky to win funding from the European Research Council for the advanced grant ALEXANDRIA. When I applied, homotopy type theory was still all the rage, so the proposal emphasised Isabelle’s specific advantages: its automation, its huge libraries and the legibility of its proofs.&lt;/p&gt;&lt;p&gt;The team started work with enthusiasm. Nevertheless, I fully expected that we would hit a wall, reaching mathematical material that could not easily be formalised in higher-order logic. Too much of Isabelle’s analysis library identified topological spaces with types. Isabelle’s abstract algebra library was old and crufty. A number of my research colleagues were convinced that higher-logic was not adequate for serious mathematics. But Anthony Bordg took up the challenge, leading a subproject to formalise Grothendieck schemes.&lt;/p&gt;&lt;p&gt;For some reason I had a particular fear of the field extension $F[a]$, which extends the field $F$ with some $a$ postulated to be a root of some polynomial over $F$. (For example, the field of complex numbers is precisely $\mathbb{R}[i]$, where $i$ is postulated to be a root a root of $x^2+1=0$.) And yet an early outcome of ALEXANDRIA was a proof, by Paulo Emílio de Vilhena and Martin Baillon, that every field admits an algebraically closed extension. This was the first proof of that theorem in any proof assistant, and its proof involves an infinite series of field extensions.&lt;/p&gt;&lt;p&gt;We never hit any wall. As our group went on to formalise more and more advanced results, such as the Balog–Szemerédi–Gowers theorem, people stopped saying “you can’t formalise mathematics without dependent types” and switched to saying “dependent types give you nicer proofs”. But they never proved this claim.&lt;/p&gt;&lt;p&gt;Now that dependent type theory has attained maturity and has an excellent tool in the form of Lean, shall I go back to dependent types? I am not tempted. The only aspects of Lean that I envy are its huge community and the Blueprint tool. I hear too many complaints about Lean’s performance. I’ve heard of too many cases where dependent types played badly with intensional equality – I sat through an entire talk on this topic – or otherwise made life difficult. Quite a few people have told me that the secret of dependent types is knowing when not to use them. And so, to me, they have too much in common with Tesla’s Full Self-Driving.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;Especially as regards constructive mathematics. To its founders, intuitionism is a philosophy suspicious of language, which it relegates to the purpose of recording and communicating mathematical thoughts. This is the opposite of today’s “constructive mathematics”, which refers the use of a formalism satisfying certain syntactic properties. ↩&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45790827</guid><pubDate>Sun, 02 Nov 2025 15:06:36 +0000</pubDate></item><item><title>New South Korean national law will turn large parking lots into solar farms</title><link>https://electrek.co/2025/11/02/new-national-law-will-turn-large-parking-lots-into-solar-power-farms/</link><description>&lt;doc fingerprint="6803b15ebfb795ba"&gt;
  &lt;main&gt;
    &lt;p&gt;Starting this month, parking lots in South Korea with more than 80 spaces will be required to install solar canopies and carports. But, unlike similar laws that have been proposed in the US, this new law doesn’t just apply to new construction – existing lots will have to comply as well!&lt;/p&gt;
    &lt;p&gt;South Korea’s Ministry of Trade, Industry and Energy announced in August that it has prepared an amendment to the Enforcement Decree of the Act on the Promotion of the Development, Use, and Diffusion of New and Renewable Energy to the effect that all publicly- and privately-owned parking lots in the Asian country with room for more than 80 vehicles will be compelled to add solar panels to their lots in a move designed to proactively expand renewable energy and create more solar and construction jobs.&lt;/p&gt;
    &lt;p&gt;In addition to creating jobs and working to stabilize the local grid with more renewable energy, the proposed solar canopies will offer a number of practical, day-to-day benefits for Korean drivers, as well.&lt;/p&gt;
    &lt;p&gt;The shaded structures will protect vehicles from heavy rain, snow, and the blistering summer sun — keeping interiors cooler, extending the life of plastics and upholstery, and even helping to preserve battery range in EVs and PHEVs by reducing their AC loads (and, of course, provide charging while the cars are parked).&lt;/p&gt;
    &lt;p&gt;To their credit, Ministry officials absolutely get it. “Through this mandatory installation,” one unnamed official told Asia Business Daily, “we expect to expand the distribution of eco-friendly renewable energy generation facilities while providing tangible benefits to the public. By utilizing idle land such as parking lots, we can maximize land use efficiency. In addition, installing canopy-type solar panels can provide shade underneath, offering noticeable comfort to people using parking lots during hot weather.”&lt;/p&gt;
    &lt;p&gt;The new rule was approved in late September, and is expected to go into effect later this month, with new installation projects set to begin immediately.&lt;/p&gt;
    &lt;head rend="h2"&gt;It could work here&lt;/head&gt;
    &lt;p&gt;South Korea is proving that an idea like is practical. Here in the US, we’re proving that out, too – the Northwest Fire District in Arizona partnered with Standard Solar to build a conceptually similar, 657 kW solar carport system across 12 parking lots (shown, above) that delivers more than 1.23 million kWh of clean, emissions-free power annually and offsets the equivalent of 185,000 vehicles’ worth of harmful carbon emissions.&lt;/p&gt;
    &lt;p&gt;That’s just Arizona. In New York, a new initiative to help expand solar into parking lots has more than doubled commercially zoned land where EV charging stations can be sited, “freeing up” an additional 400 million square feet of space throughout the city.&lt;/p&gt;
    &lt;p&gt;Sun-rich states like Texas, New Mexico, and Florida could also benefit, and even if we’re “just” adding fresh energy sources to municipal parking, dealer lots, and public schools, we could do a lot to reduce the cost of energy generation for the entire community. And, for what it’s worth, that seems to be right in line with the big reasons why people are choosing to add solar to their homes today.&lt;/p&gt;
    &lt;head rend="h2"&gt;Top comment by Pedro&lt;/head&gt;
    &lt;p&gt;Germany has a broader incentive for large commercial roofs to add solar. It has also been successful. I think the underlying goal here is that the more energy generated by renewables, means stronger national sovereignty: No coal, oil. or natural gas reliance. That takes a lot of coercive cards off the table.&lt;/p&gt;
    &lt;p&gt;What do you guys think – would something like this work in the US, or are we too far gone down the sophomoric, pseudo-libertarian rabbit hole to ever dig our way out? Let us know your take in the comments.&lt;/p&gt;
    &lt;p&gt;SOURCE | IMAGES: Asia Business Daily, via LinkedIn; Standard Solar.&lt;/p&gt;
    &lt;p&gt;If you’re considering going solar, it’s always a good idea to get quotes from a few installers. To make sure you find a trusted, reliable solar installer near you that offers competitive pricing, check out EnergySage, a free service that makes it easy for you to go solar. It has hundreds of pre-vetted solar installers competing for your business, ensuring you get high-quality solutions and save 20-30% compared to going it alone. Plus, it’s free to use, and you won’t get sales calls until you select an installer and share your phone number with them.&lt;/p&gt;
    &lt;p&gt;Your personalized solar quotes are easy to compare online and you’ll get access to unbiased Energy Advisors to help you every step of the way. Get started here.&lt;/p&gt;
    &lt;p&gt;FTC: We use income earning auto affiliate links. More.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45790867</guid><pubDate>Sun, 02 Nov 2025 15:12:08 +0000</pubDate></item><item><title>At the end you use Git bisect</title><link>https://kevin3010.github.io/git/2025/11/02/At-the-end-you-use-git-bisect.html</link><description>&lt;doc fingerprint="96385cbb607c7f9b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;At the end you use `git bisect`&lt;/head&gt;
    &lt;p&gt;People rant about having to learn algorithmic questions for interviews. I get it — interview system is broken, but you ought to learn binary search at least.&lt;/p&gt;
    &lt;p&gt;Anyways, yet again I came across a real life application of Algorithms. This time in the OG tool &lt;code&gt;git&lt;/code&gt;. &lt;code&gt;git bisect - Use binary search to find the commit that introduced a bug&lt;/code&gt; ref. And Leetcode wanted you to know it First Bad Version&lt;/p&gt;
    &lt;p&gt;We use a monorepo at work. And people tend to make hundreds, if not thousands, commit in a single repo a day. On this day, our tests started failing, and the logs weren’t enough to debug or trace the root cause. The failing method depended on a configuration file that made a remote call using a specific role to obtain a token for running the tests. At some point, that configuration had been changed — a string was updated to reference a different account — which caused the failure.&lt;/p&gt;
    &lt;p&gt;Somehow, the bad change slipped through integration tests unnoticed. It was difficult to manually find the exact file or commit that introduced the issue since many commits had been made across the repository over the past few days.&lt;/p&gt;
    &lt;p&gt;That’s when a teammate from another team — who was facing the same test failures — ran a few “magical” commands and quickly identified the exact commit where things started to break. The basic idea was simple but brilliant: pick a known good commit and a known bad one, then run a binary search to find the exact commit that caused the failure.&lt;/p&gt;
    &lt;p&gt;It took a while since each test run was time-consuming, but eventually, it pinpointed the precise commit that introduced the issue. And sure enough, after reverting that commit, everything went back to green.&lt;/p&gt;
    &lt;p&gt;Here’s a small demo repository that shows how &lt;code&gt;git bisect&lt;/code&gt; finds the first bad commit.&lt;/p&gt;
    &lt;p&gt;File tree:&lt;/p&gt;
    &lt;code&gt;git-bisect-demo/
├── calc.py           # the library under test
├── test_calc.py      # a pytest test for calc.add
└── test_script.sh    # wrapper used by `git bisect run`
&lt;/code&gt;
    &lt;p&gt;Good version of &lt;code&gt;calc.py&lt;/code&gt; (commit where tests pass):&lt;/p&gt;
    &lt;code&gt;def add(a, b):
    return a + b

if __name__ == "__main__":
    print(add(2, 3))
&lt;/code&gt;
    &lt;p&gt;Bad version of &lt;code&gt;calc.py&lt;/code&gt; (commit that introduced the bug):&lt;/p&gt;
    &lt;code&gt;def add(a, b):
    # accidental string concatenation when inputs are coerced to str
    return str(a) + str(b)

if __name__ == "__main__":
    print(add(2, 3))
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;test_calc.py&lt;/code&gt; (pytest):&lt;/p&gt;
    &lt;code&gt;import calc

def test_add():
    assert calc.add(2, 3) == 5, "Addition failed!"
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;test_script.sh&lt;/code&gt; — used by &lt;code&gt;git bisect run&lt;/code&gt; to return exit code 0 on success and non-zero on failure:&lt;/p&gt;
    &lt;code&gt;#!/usr/bin/env bash
set -e
pytest -q
&lt;/code&gt;
    &lt;p&gt;Example commit history (chronological):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Commit 1: Initial commit (good)&lt;/item&gt;
      &lt;item&gt;Commit 2: Small refactor (still good)&lt;/item&gt;
      &lt;item&gt;Commit 3: Bug introduced (bad)&lt;/item&gt;
      &lt;item&gt;Commits 4..10: Non-functional edits / comments (remain bad)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can run &lt;code&gt;git bisect&lt;/code&gt; like this:&lt;/p&gt;
    &lt;code&gt;git bisect start
git bisect bad HEAD
git bisect good HEAD~9
git bisect run ./test_script.sh
&lt;/code&gt;
    &lt;code&gt;status: waiting for both good and bad commits
status: waiting for good commit(s), bad commit known
Bisecting: 4 revisions left to test after this (roughly 2 steps)
[8dad374fd7c097c4fa3521c0b259e1eefe533520] Commit 5: more changes
running  './test_script.sh'
Bisecting: 1 revision left to test after this (roughly 1 step)
[b982ed9373fe235fe61c74b15faf264bc7142398] Commit 3: introduced bug
running  './test_script.sh'
Bisecting: 0 revisions left to test after this (roughly 0 steps)
[7b59759ca785572797e04f6b313bb0b735c22529] Commit 2: minor refactor
running  './test_script.sh'
b982ed9373fe235fe61c74b15faf264bc7142398 is the first bad commit
commit b982ed9373fe235fe61c74b15faf264bc7142398
Author: Kevin
Date:   Sun Nov 2 10:54:47 2025 -0500

    Commit 3: introduced bug

 calc.py | 10 +---------
 1 file changed, 1 insertion(+), 9 deletions(-)
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;git bisect&lt;/code&gt; will checkout intermediate commits and run &lt;code&gt;./test_script.sh&lt;/code&gt; until it finds the first commit that makes the tests fail.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45791882</guid><pubDate>Sun, 02 Nov 2025 17:24:39 +0000</pubDate></item><item><title>Is Your Bluetooth Chip Leaking Secrets via RF Signals?</title><link>https://www.semanticscholar.org/paper/Is-Your-Bluetooth-Chip-Leaking-Secrets-via-RF-Ji-Dubrova/c1d3ceb47ea6f9cc4f29929e2f97d36862a260a2</link><description>&lt;doc fingerprint="9fb01c14c0e0095a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Is Your Bluetooth Chip Leaking Secrets via RF Signals?&lt;/head&gt;
    &lt;quote&gt;@article{Ji2025IsYB, title={Is Your Bluetooth Chip Leaking Secrets via RF Signals?}, author={Yanning Ji and Elena Dubrova and Ruize Wang}, journal={IACR Cryptol. ePrint Arch.}, year={2025}, volume={2025}, pages={559}, url={https://api.semanticscholar.org/CorpusID:278151312} }&lt;/quote&gt;
    &lt;p&gt;A machine learning-assisted side-channel attack on the hardware AES accelerator of a Bluetooth chip used in millions of devices worldwide, ranging from wearables and smart home products to industrial IoT, can recover the full encryption key from 90,000 traces captured at a one-meter distance from the target device.&lt;/p&gt;
    &lt;head rend="h2"&gt;One Citation&lt;/head&gt;
    &lt;head rend="h3"&gt;Probabilistic Skipping-Based Data Structures with Robust Efficiency Guarantees&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Computer Science&lt;/p&gt;
    &lt;p&gt;This work presents adaptive attacks on all three aforementioned data structures that, in the case of hash tables and skip lists, cause exponential degradation compared to the input-independent setting, and proposes simple and efficient modifications to the original designs of these data structures to provide provable security against adaptive adversaries.&lt;/p&gt;
    &lt;head rend="h2"&gt;27 References&lt;/head&gt;
    &lt;head rend="h3"&gt;Screaming Channels: When Electromagnetic Side Channels Meet Radio Transceivers&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2018&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Engineering, Physics&lt;/p&gt;
    &lt;p&gt;This paper presents a new side channel that affects mixed-signal chips used in widespread wireless communication protocols, such as Bluetooth and WiFi and argues that protections against side channels (such as masking or hiding) need to be used on this class of devices.&lt;/p&gt;
    &lt;head rend="h3"&gt;Understanding Screaming Channels: From a Detailed Analysis to Improved Attacks&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2020&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Computer Science, Engineering&lt;/p&gt;
    &lt;p&gt;This work conducts a thorough experimental analysis of the peculiar properties of Screaming Channels, and provides a broader security evaluation of the leaks, helping the defender and radio designers to evaluate risk, and the need of countermeasures.&lt;/p&gt;
    &lt;head rend="h3"&gt;Far Field EM Side-Channel Attack on AES Using Deep Learning&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2020&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Computer Science, Engineering&lt;/p&gt;
    &lt;p&gt;This work presents the first deep learning-based side-channel attack on AES-128 using far field electromagnetic emissions as a side channel and can recover the key from less than 10K traces captured in an office environment at 15 m distance to target even if the measurement for each encryption is taken only once.&lt;/p&gt;
    &lt;head rend="h3"&gt;Advanced Far Field EM Side-Channel Attack on AES&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2021&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Computer Science, Engineering&lt;/p&gt;
    &lt;p&gt;Deep learning models are trained on "clean" traces, captured through a coaxial cable and the resulting models can extract the AES key from less than 500 traces on average captured at 15 m from the victim device without repeating each encryption more than once.&lt;/p&gt;
    &lt;head rend="h3"&gt;Attacking at non-harmonic frequencies in screaming-channel attacks&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2023&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Engineering, Physics&lt;/p&gt;
    &lt;p&gt;This work demonstrates that compromising signals appear not only at the harmonics and that leakage at non-harmonics can be exploited for successful attacks, and proposes two methodologies to locate frequencies that contain leakage and demonstrates that it appears atNon-harmonic frequencies.&lt;/p&gt;
    &lt;head rend="h3"&gt;Machine learning in side-channel analysis: a first study&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2011&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Computer Science, Engineering&lt;/p&gt;
    &lt;p&gt;This work comprehensively investigates the application of a machine learning technique in SCA, a powerful kernel-based learning algorithm: the Least Squares Support Vector Machine (LS-SVM) and the target is a software implementation of the Advanced Encryption Standard.&lt;/p&gt;
    &lt;head rend="h3"&gt;Power Analysis Attacks Against IEEE 802.15.4 Nodes&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2016&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Computer Science, Engineering&lt;/p&gt;
    &lt;p&gt;This work measures the leakage characteristics of the AES accelerator on the Atmel ATMega128RFA1, and demonstrates how this allows recovery of the encryption key from nodes running an IEEE 802.15.4 stack.&lt;/p&gt;
    &lt;head rend="h3"&gt;Screaming Channels Revisited: Encryption Key Recovery from AES-CCM Accelerator&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Computer Science, Engineering&lt;/p&gt;
    &lt;p&gt;This paper demonstrates the first successful extraction of the encryption key from the hardware AES accelerator in the nRF52832 Bluetooth Low Energy system-on-chip operating in Counter with CBC-MAC (CCM) mode using side-channel information recovered from RF signals.&lt;/p&gt;
    &lt;head rend="h3"&gt;Power analysis attacks - revealing the secrets of smart cards&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2007&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Computer Science&lt;/p&gt;
    &lt;p&gt;This volume explains how power analysis attacks work and provides an extensive discussion of countermeasures like shuffling, masking, and DPA-resistant logic styles to decide how to protect smart cards.&lt;/p&gt;
    &lt;head rend="h3"&gt;Non-Profiled Deep Learning-Based Side-Channel Attacks&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2018&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Computer Science&lt;/p&gt;
    &lt;p&gt;This paper introduces a new method to apply Deep Learning techniques in a Non-Profiled context, where an attacker can only collect a limited number of side-channel traces for a fixed unknown key value from a closed device and introduces metrics based on Sensitivity Analysis that can reveal both the secret key value and points of interest.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45792166</guid><pubDate>Sun, 02 Nov 2025 18:06:15 +0000</pubDate></item><item><title>Anti-cybercrime laws are being weaponized to repress journalism</title><link>https://www.cjr.org/analysis/nigeria-pakistan-jordan-cybercrime-laws-journalism.php</link><description>&lt;doc fingerprint="1480d19f3aa57a8d"&gt;
  &lt;main&gt;
    &lt;p&gt;Sign up for the daily CJR newsletter.&lt;/p&gt;
    &lt;p&gt;In May 2024, Daniel Ojukwu, a twenty-six-year-old reporter for the Foundation for Investigative Journalism, a Nigerian nonprofit, was grabbed off the streets of Lagos by armed police and bundled into a vehicle. For the next several days, he was held in a cell incommunicado—first in Lagos, and later in the federal capital, Abuja—without being told exactly what he’d been arrested for. “It was more of an abduction,” Ojukwu recalled recently, via WhatsApp. Finally, on the fourth day, the authorities informed him that he was being accused of breaching a 2015 law known as the Cybercrime Act. His violation: an article he wrote about alleged corruption in the office of the president.&lt;/p&gt;
    &lt;p&gt;The Cybercrime Act was introduced to combat a growing trend of internet fraud and other criminal activity within Nigeria, but it has instead frequently been used to suppress journalism published online. One provision in particular—Section 24, which made it illegal to publish false information online that was deemed to be “grossly offensive,” “indecent,” or even merely an “annoyance”—has been especially ripe for abuse. In 2019, for instance, Agba Jalingo, a journalist and publisher of CrossRiverWatch, in Nigeria’s Cross River State, was arrested and charged under Section 24 after he published articles accusing the state’s governor of corruption. (He was later acquitted.) In February 2024, Nigerian lawmakers amended Section 24 to remove some of its most egregious elements, but the new language still makes it illegal, and punishable by up to three years in jail, to “knowingly or intentionally” communicate online anything that is “false, for the purpose of causing a breakdown of law and order [or] posing a threat to life.”&lt;/p&gt;
    &lt;p&gt;“This vague text is still used to unfairly prosecute journalists, particularly those who regularly publish investigative reports implicating political or institutional forces,” said Sadibou Marong, the sub-Saharan Africa bureau director of Reporters Without Borders. “Authorities are intent on gagging investigative journalism uncovering corruption and governance issues in the country. The continued implementation of this law constitutes a real threat.”&lt;/p&gt;
    &lt;p&gt;Nigeria is not the only country using laws designed to legitimately combat online misbehavior to instead repress journalism. In neighboring Niger, Abdourahamane Tchiani, who seized power in a coup in 2023, signed an order amending three articles of the country’s cybercrime law to reinstate prison sentences for “defamation,” “insults,” and the “dissemination of data likely to disturb public order or undermine human dignity” when these offenses are committed electronically. The law, originally enacted in 2019, had previously been softened to remove prison sentences for such offenses, in part owing to how the law had been abused to repress journalists. In Pakistan, Georgia, and Turkey, among others, recent laws meant to limit nefarious activity online, or the spread of misinformation, have been used to restrict acts of journalism. According to Amnesty International, at least fifteen people in Jordan have been prosecuted under a 2023 expansion of the country’s Cybercrimes Law, for offenses ranging from “spreading fake news” to “threatening society peace.”&lt;/p&gt;
    &lt;p&gt;“Unfortunately, most of the laws being passed will have little effect in actually curbing misinformation, but instead may give governments far more authority to control content they deem false or misleading,” said Gabrielle Lim, a doctoral fellow at the Citizen Lab at the University of Toronto, who recently coauthored a paper tracking the misuse of “fake news” laws around the world. “For some governments, the threat of misinformation provides a convenient justification for censorship. This is compounded by the fact that liberal democracies are also considering or passing similar laws, which can give cover to authoritarian regimes who want to do the same.”&lt;/p&gt;
    &lt;p&gt;In Nigeria, more than two dozen journalists have faced prosecution under the Cybercrime Act, according to the Committee to Protect Journalists. In most cases, the journalists have been accused of cyberbullying, cyberstalking, or attempting to overthrow the government. On February 16, 2024—two weeks before the amended Cybercrime Act was signed into law—four journalists of The Informant247, an independent online newspaper based in Nigeria’s Kwara State, were arrested and briefly detained after they published a two-part investigative series that alleged a corrupt atmosphere at a state-run polytechnic institute. “The experience was profoundly disturbing,” said Salihu Ayatullahi, the publication’s editor in chief and one of the arrested journalists. “We were locked in a dark, cramped cell with hardened criminals. The psychological impact was heavier than the physical discomfort: I couldn’t sleep, not because of the poor conditions, but because I couldn’t stop thinking about how broken our system had become and how the corrupt could illegally summon the police to punish those who expose them.” The case was dismissed eleven months later without any evidence presented against the reporters.&lt;/p&gt;
    &lt;p&gt;Solomon Okedara, a Nigerian digital rights lawyer and researcher, notes that the use of the Cybercrime Act has created a chilling effect in the nation’s civic space. “It is even more worrisome that most of the time, the prosecution cannot establish ingredients of the offense to the point of conviction,” Okedara said. “Knowing a fellow journalist has faced arrest, harassment, and detention, or endless trials, can force others to drop an investigative story idea.”&lt;/p&gt;
    &lt;p&gt;Despite their ordeals, both Ojukwu and Ayatullahi say they are more determined than ever to use their craft to hold public officials accountable. “As a journalist, the whole experience has made me understand that there is more work to do,” Ojukwu said. “And since there is no limit to which the corrupt are willing to go, there is also none for me. The Cybercrime Act remains a thorn in the flesh of journalists in Nigeria.”&lt;/p&gt;
    &lt;p&gt;Has America ever needed a media defender more than now? Help us by joining CJR today.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45792209</guid><pubDate>Sun, 02 Nov 2025 18:12:49 +0000</pubDate></item><item><title>Reproducing the AWS Outage Race Condition with a Model Checker</title><link>https://wyounas.github.io/aws/concurrency/2025/10/30/reproducing-the-aws-outage-race-condition-with-model-checker/</link><description>&lt;doc fingerprint="2d18da0b79028601"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Reproducing the AWS Outage Race Condition with a Model Checker&lt;/head&gt;
    &lt;p&gt;Oct 30, 2025&lt;/p&gt;
    &lt;p&gt;AWS published a post-mortem about a recent outage [1]. Big systems like theirs are complex, and when you operate at that scale, things sometimes go wrong. Still, AWS has an impressive record of reliability.&lt;/p&gt;
    &lt;p&gt;The post-mortem mentioned a race condition, which caught my eye. I don’t know all the details of AWS’s internal setup, but using the information in the post-mortem and a few assumptions, we can try to reproduce a simplified version of the problem.&lt;/p&gt;
    &lt;p&gt;As a small experiment, we’ll use a model checker to see how such a race could happen. Formal verification can’t prevent every failure, but it helps us think more clearly about correctness and reason about subtle concurrency bugs. For this, we’ll use the Spin model checker, which uses the Promela language.&lt;/p&gt;
    &lt;p&gt;There’s a lot of detail in the post-mortem, but for simplicity we’ll focus only on the race-condition aspect. The incident was triggered by a defect in DynamoDB’s automated DNS management system. The components of this system involved in the incident were the DNS Planner, DNS Enactor, and Amazon Route 53 service.&lt;/p&gt;
    &lt;p&gt;The DNS Planner creates DNS plans, and the DNS Enactors look for new DNS plans and apply them to the Amazon Route 53 service. Three Enactors operate independently in three different availability zones.&lt;/p&gt;
    &lt;p&gt;Here is an illustration showing these components (if the images appear small, please open them in a new browser tab):&lt;/p&gt;
    &lt;p&gt;My understanding of how the DNS Enactor works is as follows: it picks up the latest plan and, before applying it, performs a one-time check to ensure the plan is newer than the previously applied one. It then applies the plan and invokes a clean-up process. During the clean-up, it identifies plans significantly older than the one it just applied and deletes them.&lt;/p&gt;
    &lt;p&gt;Using the details from the incident report, we could sketch an interleaving that could explain the race condition. Two Enactors running side by side: Enactor 2 applies a new plan and starts cleaning up, while the other, running just a little behind, applies an older plan, making it an active one. When the Enactor 2 finishes its cleanup, it deletes that plan, and the DNS entries disappear. Here’s what that sequence looks like:&lt;/p&gt;
    &lt;p&gt;Let’s try to uncover this interleaving using a model checker.&lt;/p&gt;
    &lt;p&gt;In Promela, you can model each part of the system as its own process. Spin then takes those processes, starts from the initial state, and systematically applies every possible transition, exploring all interleavings to build the set of reachable states [2]. It checks that your invariants hold in each one, and if it finds a violation, it reports a counterexample.&lt;/p&gt;
    &lt;p&gt;We’ll create a DNS Planner process that produces plans, and DNS Enactor processes that pick them up. The Enactor will check whether the plan it’s about to apply is newer than the previous one, update the state of certain variables to simulate changes in Route 53, and finally clean up the older plans.&lt;/p&gt;
    &lt;p&gt;In our simplified model, we’ll run one DNS Planner process and two concurrent DNS Enactor processes. (AWS appears to run three across zones; we abstract that detail here.) The Planner generates plans, and through Promela channels, these plans are sent to the Enactors for processing.&lt;/p&gt;
    &lt;p&gt;Inside each DNS Enactor, we track the key aspects of system state. The Enactor keeps the current plan in current_plan, and it represents DNS health using dns_valid. It also records the highest plan applied so far in highest_plan_applied. The incident report also notes that the clean-up process deletes plans that are “significantly older than the one it just applied.” In our model, we capture this by allowing an Enactor to remove only those plans that are much older than its current plan. To simulate the deletion of an active plan, the Enactor’s clean-up process checks whether current_plan equals the plan being deleted. If it does, we simulate the resulting DNS failure by setting dns_valid to false.&lt;/p&gt;
    &lt;p&gt;Here’s the code for the DNS Planner:&lt;/p&gt;
    &lt;code&gt;active proctype Planner() {
    byte plan = 1;
    
    do
    :: (plan &amp;lt;= MAX_PLAN) -&amp;gt;
        latest_plan = plan;
        plan_channel ! plan; 
        printf("Planner: Generated Plan v%d\n", plan);
        plan++;
    :: (plan &amp;gt; MAX_PLAN) -&amp;gt; break;
    od;
    
    printf("Planner: Completed\n");
}
&lt;/code&gt;
    &lt;p&gt;It creates plans and sends them over a channel (plan is being sent to the channel plan_channel) to be picked up later by the DNS Enactor.&lt;/p&gt;
    &lt;p&gt;We start two concurrent DNS Enactor processes by specifying the number of enactors after the active keyword.&lt;/p&gt;
    &lt;code&gt;active [NUM_ENACTORS] proctype Enactor() 
&lt;/code&gt;
    &lt;p&gt;The DNS Enactor waits for plans and receives them (? opertaor receives a plan from the channel plan_channel). It then performs a staleness check, updates the state of certain variables to simulate changes in Route 53, and finally cleans up the older plans.&lt;/p&gt;
    &lt;code&gt;:: plan_channel ? my_plan -&amp;gt;
    snapshot_current = current_plan;

    // staleness check    
    if
    :: (my_plan &amp;gt; snapshot_current || snapshot_current == 0) -&amp;gt;

        if
            :: !plan_deleted[my_plan] -&amp;gt;
                /* Apply the plan to Route53 */
                
                current_plan = my_plan;
                dns_valid = true;
                initialized = true;
               /* Track highest plan applied for regression detection */
                if 
                :: (my_plan &amp;gt; highest_plan_applied) -&amp;gt;
                    highest_plan_applied = my_plan;
                fi 
            
            // runs the clean-up process (omitted for brevity, included in the 
            // code linked below)
        fi
    fi

&lt;/code&gt;
    &lt;p&gt;How do we discover the race condition? The idea is this: we express as an invariant what must always be true of the system, and then ask the model checker to confirm that it holds in every possible state. In this case, we can set up an invariant stating that the DNS should never be deleted once a newer plan has been applied. (With more information about the real system, we could simplify or refine this rule further.)&lt;/p&gt;
    &lt;p&gt;We specify this invariant formally as follows:&lt;/p&gt;
    &lt;code&gt;/*

A quick note on some of the keywords used in the invariant below:

ltl - keyword that declares a temporal property to verify (ltl: linear temporal logic lets you specify properties about all possible executions of your program.)

[] - "always" operator (this must be true at every step forever)

-&amp;gt; - "implies" (if left side is true, then right side must be true)

*/

ltl no_dns_deletion_on_regression {
    [] ( (initialized &amp;amp;&amp;amp; highest_plan_applied &amp;gt; current_plan 
            &amp;amp;&amp;amp; current_plan &amp;gt; 0) -&amp;gt; dns_valid )
}



&lt;/code&gt;
    &lt;p&gt;When we start the model checker, one DNS Planner process begins generating plans and sending them through channels to the DNS Enactors. Two Enactors receive these plans, perform their checks, apply updates, and run their cleanup routines. As these processes interleave, the model checker systematically builds the set of reachable states, allowing the invariant to be checked in each one.&lt;/p&gt;
    &lt;p&gt;When we run the model with this invariant in the model checker, it reports a violation. Spin reports one error and writes a trail file that shows, step by step, how the system reached the bad state.&lt;/p&gt;
    &lt;code&gt;
$ spin -a aws-dns-race.pml
$ gcc -O2 -o pan pan.c                                                       
$ ./pan -a -N no_dns_deletion_on_regression  

pan: wrote aws-dns-race.pml.trail

(Spin Version 6.5.2 -- 6 December 2019)

State-vector 64 byte, depth reached 285, errors: 1
    23201 states, stored
    11239 states, matched
    34440 transitions (= stored+matched)
  (truncated for brevity....)

&lt;/code&gt;
    &lt;p&gt;The trail file in the repository below shows how the race happens. The trail file shows that two Enactors operate side by side: the faster one applies plan 4 and starts cleaning up. Because cleanup only removes plans much older than the one just applied, it deletes 1 and 2 but skips 3. The slower Enactor then applies plan 3 and makes it active, and when the faster Enactor picks up cleanup again, it deletes 3 and the DNS goes down.&lt;/p&gt;
    &lt;p&gt;Here’s an illustration of the interleaving reconstructed from the trail:&lt;/p&gt;
    &lt;p&gt;Before publishing, I reread the incident report and noted: “Additionally, because the active plan was deleted, the system was left in an inconsistent state…”. This suggests a direct invariant: the active plan must never be deleted.&lt;/p&gt;
    &lt;code&gt;ltl never_delete_active {
    [] ( current_plan &amp;gt; 0 -&amp;gt; !plan_deleted[current_plan] )
}
&lt;/code&gt;
    &lt;p&gt;Running the model checker with this invariant produces essentially the same counterexample as before: one Enactor advances to newer plans while the other lags and applies an older plan, thereby making it active. When control returns to the faster Enactor, its cleanup deletes that now-active plan, violating the invariant.&lt;/p&gt;
    &lt;p&gt;Invariants are invaluable for establishing correctness. If we can show that an invariant holds in the initial state, in every state reachable from it, and in the final state as well, we gain confidence that the system’s logic is sound.&lt;/p&gt;
    &lt;p&gt;To fix the code, we execute the problematic statements atomically. You can find both versions of the code, the one with the race and the fixed one, along with the interleaving trail in the accompanying repository [3]. I’ve included detailed comments to make it self-explanatory, as well as instructions on how to run the model and explore the trail.&lt;/p&gt;
    &lt;p&gt;Some of the assumptions in this model are necessarily simplified, since I don’t have access to AWS’s internal design details. Without that context, there will naturally be gaps between this abstraction and the real system. This model was created in a short time frame for experimental purposes. With more time and context, one could certainly build a more accurate and refined version.&lt;/p&gt;
    &lt;p&gt;Please keep in mind that I’m only human, and there’s a chance this post contains errors. If you notice anything off, I’d appreciate a correction. Please feel free to send me an email.&lt;/p&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;AWS Post-Incident Summary — October 2025 Outage&lt;/item&gt;
      &lt;item&gt;How concurrency works: A visual guide&lt;/item&gt;
      &lt;item&gt;Source code repository&lt;/item&gt;
      &lt;item&gt;Spin model checker&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45792373</guid><pubDate>Sun, 02 Nov 2025 18:37:38 +0000</pubDate></item><item><title>Linux gamers on Steam cross over the 3% mark</title><link>https://www.gamingonlinux.com/2025/11/linux-gamers-on-steam-finally-cross-over-the-3-mark/</link><description>&lt;doc fingerprint="9e39189f1817344d"&gt;
  &lt;main&gt;
    &lt;p&gt;It finally happened. Linux gamers on Steam as of the Steam Hardware &amp;amp; Software Survey for October 2025 have crossed over the elusive 3% mark. The trend has been clear for sometime, and with Windows 10 ending support, it was quite likely this was going to be the time for it to happen as more people try out Linux.&lt;/p&gt;
    &lt;p&gt;As of the October 2025 survey the operating system details:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Windows 94.84% -0.75%&lt;/item&gt;
      &lt;item&gt;Linux 3.05% +0.41%&lt;/item&gt;
      &lt;item&gt;macOS 2.11% +0.34%&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The snapshot chart from our dedicated Steam Tracker page shows the clear trend:&lt;/p&gt;
    &lt;p&gt;Overall, 3% might not seem like much to some, but again - that trend is very clear and equates to millions of people. The last time Valve officially gave a proper monthly active user count was in 2022, and we know Steam has grown a lot since then, but even going by that original number would put monthly active Linux users at well over 4 million. Sadly, Valve have not given out a more recent monthly active user number but it's likely a few million higher, especially with the Steam Deck selling millions.&lt;/p&gt;
    &lt;p&gt;And if we look at the distribution breakdown chart from our page:&lt;/p&gt;
    &lt;p&gt;The overall distribution numbers for October 2025:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SteamOS Holo 64 bit - 27.18% (-0.47%)&lt;/item&gt;
      &lt;item&gt;Arch Linux 64 bit - 10.32% (-0.66%)&lt;/item&gt;
      &lt;item&gt;Linux Mint 22.2 64 bit - 6.65% (+6.65%)&lt;/item&gt;
      &lt;item&gt;CachyOS 64 bit - 6.01% (+1.32%)&lt;/item&gt;
      &lt;item&gt;Ubuntu Core 22 64 bit - 4.55% (+0.55%)&lt;/item&gt;
      &lt;item&gt;Freedesktop SDK 25.08 (Flatpak runtime) 64 bit - 4.29% (+4.29%)&lt;/item&gt;
      &lt;item&gt;Bazzite 64 bit - 4.24% (+4.24%)&lt;/item&gt;
      &lt;item&gt;Ubuntu 24.04.3 LTS 64 bit - 3.70% (+3.70%)&lt;/item&gt;
      &lt;item&gt;Linux Mint 22.1 64 bit - 2.56% (-5.65%)&lt;/item&gt;
      &lt;item&gt;EndeavourOS Linux 64 bit - 2.32% (-0.08%)&lt;/item&gt;
      &lt;item&gt;Freedesktop SDK 24.08 (Flatpak runtime) 64 bit - 2.31% (-3.98%)&lt;/item&gt;
      &lt;item&gt;Fedora Linux 42 (KDE Plasma Desktop Edition) 64 bit - 2.12% (+0.19%)&lt;/item&gt;
      &lt;item&gt;Manjaro Linux 64 bit - 2.04% (-0.31%)&lt;/item&gt;
      &lt;item&gt;Pop!_OS 22.04 LTS 64 bit - 1.93% (-0.04%)&lt;/item&gt;
      &lt;item&gt;Fedora Linux 42 (Workstation Edition) 64 bit - 1.75% (-0.43%)&lt;/item&gt;
      &lt;item&gt;Other - 18.04% (-4.28%)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The numbers are still being massively pumped up by the Steam Deck with SteamOS Linux, which is not surprising considering that the Steam Deck is still in the top 10 of the global top sellers on Steam constantly. And with all the rumours and leaks surrounding the upcoming Steam Frame, which will hopefully be a SteamOS Linux powered VR kit, we could see the numbers just continue to jump higher.&lt;/p&gt;
    &lt;p&gt;Source: Valve&lt;/p&gt;
    &lt;p&gt;Also please split the linear approximation in pieces (Steam Deck release is a good mark)&lt;/p&gt;
    &lt;p&gt;Last edited by lucinos on 2 Nov 2025 at 1:18 pm UTC&lt;/p&gt;
    &lt;quote&gt;Interesting that MacOS usage went up as well, been a while since that's happened.It's been trending back up since around the start of the year.&lt;/quote&gt;
    &lt;p&gt;https://i.ibb.co/tT3g0WT7/Combined.png [External Link]&lt;/p&gt;
    &lt;p&gt;It's a smidge behind non-Deck Linux.&lt;/p&gt;
    &lt;p&gt;A bigger non-Windows share (above 5% for the first time since 2013) is a good thing for encouraging multi-platform development and support, although Apple makes it harder than it needs to be with their refusal of Vulkan, and harder than it used to be when OpenGL would work on Windows, Linux and Mac.&lt;/p&gt;
    &lt;p&gt;Once marketshare is high enough for more multiplayer games, it makes limiting support just to the Steam Deck hardware much less viable. That is definitely a risk as seen with games like Delta Force.&lt;/p&gt;
    &lt;p&gt;But if the trend continues, they'll have to support a wide range of hardware and distros.&lt;/p&gt;
    &lt;p&gt;Let's keep climbing.&lt;/p&gt;
    &lt;quote&gt;I'm surprised in general that Linux Mint is ahead of Debian (testing / unstable)&lt;/quote&gt;
    &lt;p&gt;Mint user here. I think that's because for gaming, Mint is a great compromise. Debian's ultimate focus is stability, which makes it a fantastic choice for servers, but in gaming, you often want components that aren't quite that old. It still doesn't randomly break your stuff, unlike rolling release distros.&lt;/p&gt;
    &lt;quote&gt;I'm surprised in general that Linux Mint is ahead of Debian (testing / unstable)I would be surprised if something that is explicitly unstable was significantly popular. Iirc every more or less official Debian related place tells you not to use those unless you really know what you're doing in a way that makes Arch or Fedora much more appealing if you want fresh packages&lt;/quote&gt;
    &lt;quote&gt;Mint user here. I think that's because for gaming, Mint is a great compromise. Debian's ultimate focus is stability, which makes it a fantastic choice for servers, but in gaming, you often want components that aren't quite that old. It still doesn't randomly break your stuff, unlike rolling release distros&lt;/quote&gt;
    &lt;p&gt;That's why I said Debian testing / unstable, not Debian stable. Such kind of approach (whether in Mint or Debian stable itself) can cause problems too unless people understand its limitations.&lt;/p&gt;
    &lt;p&gt;I periodically see a bunch of people complaining that their hardware doesn't work, which ends up being them using Mint which doesn't ship recent kernel and Mesa.&lt;/p&gt;
    &lt;p&gt;Rolling flavors of Debian are a better fit in my opinion.&lt;/p&gt;
    &lt;p&gt;Also, I think KDE is a better fit for modern gaming features, due to Cinnamon being way slower in supporting Wayland. Having focus on its own DE and not keeping up with the times is a downside for Mint. Even Ubuntu stopped its own DE efforts for that reason.&lt;/p&gt;
    &lt;quote&gt;irc every more or less official Debian related place tells you not to use those unless you really know what you're doing&lt;/quote&gt;
    &lt;p&gt;You should know what you are doing no matter what you are using. That's my experience. I'd say Debian testing/unstable isn't any worse than a bunch of other rolling distros, like Arch or what not. If anything, it's more stable than Arch. Those who say not to use it are doing a disservice.&lt;/p&gt;
    &lt;p&gt;Last edited by Shmerl on 2 Nov 2025 at 5:05 pm UTC&lt;/p&gt;
    &lt;quote&gt;That's why I said Debian testing / unstable, not Debian stable.&lt;/quote&gt;
    &lt;p&gt;My bad! :)&lt;/p&gt;
    &lt;quote&gt;Mint which doesn't ship recent kernel&lt;/quote&gt;
    &lt;p&gt;The version numbers might seem dated, by mind that Ubuntu based distros maintain these kernels for a longer time and backport newer features.&lt;/p&gt;
    &lt;quote&gt;Also, I think KDE is a better fit for modern gaming features&lt;/quote&gt;
    &lt;p&gt;I love KDE Plasma, really. Only reason why I didn't switch is because Cinnamon is "good enough" for the time being, and my requirements of DE features aren't all that high. Wayland is not required in any shape or fashion for gaming as of today. I'd notice if it were (still not using it). ;)&lt;/p&gt;
    &lt;quote&gt;it's more stable than Arch&lt;/quote&gt;
    &lt;p&gt;Anything is. ;)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45792503</guid><pubDate>Sun, 02 Nov 2025 18:54:59 +0000</pubDate></item><item><title>Lisp: Notes on its Past and Future (1980)</title><link>https://www-formal.stanford.edu/jmc/lisp20th/lisp20th.html</link><description>&lt;doc fingerprint="9449101996ab135c"&gt;
  &lt;main&gt;
    &lt;p&gt; John McCarthy &lt;lb/&gt; Computer Science Department &lt;lb/&gt; Stanford University &lt;lb/&gt; Stanford, CA 94305 &lt;lb/&gt; jmc@cs.stanford.edu &lt;lb/&gt; http://www-formal.stanford.edu/jmc/&lt;/p&gt;
    &lt;p&gt; JanFebMarAprMayJun JulAugSepOctNovDec , :&amp;lt; 10 0 &lt;/p&gt;
    &lt;p&gt;LISP has survived for 21 years because it is an approximate local optimum in the space of programming languages. However, it has accumulated some barnacles that should be scraped off, and some long-standing opportunities for improvement have been neglected. It would benefit from some co-operative maintenance especially in creating and maintaining program libraries. Computer checked proofs of program correctness are now possible for pure LISP and some extensions, but more theory and some smoothing of the language itself are required before we can take full advantage of LISP's mathematical basis.&lt;/p&gt;
    &lt;p&gt;1999 note: This article was included in the 1980 Lisp conference held at Stanford. Since it almost entirely corresponds to my present opinions, I should have asked to have it reprinted in the 1998 Lisp users conference proceedings at which I gave a talk with the same title.&lt;/p&gt;
    &lt;p/&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45792579</guid><pubDate>Sun, 02 Nov 2025 19:05:32 +0000</pubDate></item></channel></rss>