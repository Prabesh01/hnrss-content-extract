<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 09 Oct 2025 14:10:19 +0000</lastBuildDate><item><title>We found a bug in Go's ARM64 compiler</title><link>https://blog.cloudflare.com/how-we-found-a-bug-in-gos-arm64-compiler/</link><description>&lt;doc fingerprint="3b06c011ce263a54"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Every second, 84 million HTTP requests are hitting Cloudflare across our fleet of data centers in 330 cities. It means that even the rarest of bugs can show up frequently. In fact, it was our scale that recently led us to discover a bug in Go's arm64 compiler which causes a race condition in the generated code.&lt;/p&gt;
      &lt;p&gt;This post breaks down how we first encountered the bug, investigated it, and ultimately drove to the root cause.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Investigating a strange panic&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;We run a service in our network which configures the kernel to handle traffic for some products like Magic Transit and Magic WAN. Our monitoring watches this closely, and it started to observe very sporadic panics on arm64 machines.&lt;/p&gt;
      &lt;p&gt;We first saw one with a fatal error stating that traceback did not unwind completely. That error suggests that invariants were violated when traversing the stack, likely because of stack corruption. After a brief investigation we decided that it was probably rare stack memory corruption. This was a largely idle control plane service where unplanned restarts have negligible impact, and so we felt that following up was not a priority unless it kept happening.&lt;/p&gt;
      &lt;p&gt;And then it kept happening.Â &lt;/p&gt;
      &lt;p&gt;When we first saw this bug we saw that the fatal errors correlated with recovered panics. These were caused by some old code which used panic/recover as error handling.Â &lt;/p&gt;
      &lt;p&gt;At this point, our theory was:Â &lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;All of the fatal panics happen within stack unwinding.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;We correlated an increased volume of recovered panics with these fatal panics.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Recovering a panic unwinds goroutine stacks to call deferred functions.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;A related Go issue (#73259) reported an arm64 stack unwinding crash.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Letâs stop using panic/recover for error handling and wait out the upstream fix?&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;So we did that and watched as fatal panics stopped occurring as the release rolled out. Fatal panics gone, our theoretical mitigation seemed to work, and this was no longer our problem. We subscribed to the upstream issue so we could update when it was resolved and put it out of our minds.&lt;/p&gt;
      &lt;p&gt;But, this turned out to be a much stranger bug than expected. Putting it out of our minds was premature as the same class of fatal panics came back at a much higher rate. A month later, we were seeing up to 30 daily fatal panics with no real discernible cause; while that might account for only one machine a day in less than 10% of our data centers, we found it concerning that we didnât understand the cause. The first thing we checked was the number of recovered panics, to match our previous pattern, but there were none. More interestingly, we could not correlate this increased rate of fatal panics with anything. A release? Infrastructure changes? The position of Mars? &lt;/p&gt;
      &lt;p&gt;At this point we felt like we needed to dive deeper to better understand the root cause. Pattern matching and hoping was clearly insufficient.Â &lt;/p&gt;
      &lt;p&gt;We saw two classes of this bug -- a crash while accessing invalid memory and an explicitly checked fatal error.Â &lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;goroutine 153 gp=0x4000105340 m=324 mp=0x400639ea08 [GC worker (active)]:
/usr/local/go/src/runtime/asm_arm64.s:244 +0x6c fp=0x7ff97fffe870 sp=0x7ff97fffe860 pc=0x55558d4098fc
runtime.systemstack(0x0)
       /usr/local/go/src/runtime/mgc.go:1508 +0x68 fp=0x7ff97fffe860 sp=0x7ff97fffe810 pc=0x55558d3a9408
runtime.gcBgMarkWorker.func2()
       /usr/local/go/src/runtime/mgcmark.go:1102
runtime.gcDrainMarkWorkerIdle(...)
       /usr/local/go/src/runtime/mgcmark.go:1188 +0x434 fp=0x7ff97fffe810 sp=0x7ff97fffe7a0 pc=0x55558d3ad514
runtime.gcDrain(0x400005bc50, 0x7)
       /usr/local/go/src/runtime/mgcmark.go:212 +0x1c8 fp=0x7ff97fffe7a0 sp=0x7ff97fffe6f0 pc=0x55558d3ab248
runtime.markroot(0x400005bc50, 0x17e6, 0x1)
       /usr/local/go/src/runtime/mgcmark.go:238 +0xa8 fp=0x7ff97fffe6f0 sp=0x7ff97fffe6a0 pc=0x55558d3ab578
runtime.markroot.func1()
       /usr/local/go/src/runtime/mgcmark.go:887 +0x290 fp=0x7ff97fffe6a0 sp=0x7ff97fffe560 pc=0x55558d3acaa0
runtime.scanstack(0x4014494380, 0x400005bc50)
       /usr/local/go/src/runtime/traceback.go:447 +0x2ac fp=0x7ff97fffe560 sp=0x7ff97fffe4d0 pc=0x55558d3eeb7c
runtime.(*unwinder).next(0x7ff97fffe5b0?)
       /usr/local/go/src/runtime/traceback.go:566 +0x110 fp=0x7ff97fffe4d0 sp=0x7ff97fffe490 pc=0x55558d3eed40
runtime.(*unwinder).finishInternal(0x7ff97fffe4f8?)
       /usr/local/go/src/runtime/panic.go:1073 +0x38 fp=0x7ff97fffe490 sp=0x7ff97fffe460 pc=0x55558d403388
runtime.throw({0x55558de6aa27?, 0x7ff97fffe638?})
runtime stack:
fatal error: traceback did not unwind completely
       stack=[0x4015d6a000-0x4015d8a000
runtime: g8221077: frame.sp=0x4015d784c0 top=0x4015d89fd0&lt;/code&gt;
      &lt;/quote&gt;
      &lt;quote&gt;
        &lt;code&gt;goroutine 187 gp=0x40003aea80 m=13 mp=0x40003ca008 [GC worker (active)]:
       /usr/local/go/src/runtime/asm_arm64.s:244 +0x6c fp=0x7fff2afde870 sp=0x7fff2afde860 pc=0x55557e2d98fc
runtime.systemstack(0x0)
       /usr/local/go/src/runtime/mgc.go:1489 +0x94 fp=0x7fff2afde860 sp=0x7fff2afde810 pc=0x55557e279434
runtime.gcBgMarkWorker.func2()
       /usr/local/go/src/runtime/mgcmark.go:1112
runtime.gcDrainMarkWorkerDedicated(...)
       /usr/local/go/src/runtime/mgcmark.go:1188 +0x434 fp=0x7fff2afde810 sp=0x7fff2afde7a0 pc=0x55557e27d514
runtime.gcDrain(0x4000059750, 0x3)
       /usr/local/go/src/runtime/mgcmark.go:212 +0x1c8 fp=0x7fff2afde7a0 sp=0x7fff2afde6f0 pc=0x55557e27b248
runtime.markroot(0x4000059750, 0xb8, 0x1)
       /usr/local/go/src/runtime/mgcmark.go:238 +0xa8 fp=0x7fff2afde6f0 sp=0x7fff2afde6a0 pc=0x55557e27b578
runtime.markroot.func1()
       /usr/local/go/src/runtime/mgcmark.go:887 +0x290 fp=0x7fff2afde6a0 sp=0x7fff2afde560 pc=0x55557e27caa0
runtime.scanstack(0x40042cc000, 0x4000059750)
       /usr/local/go/src/runtime/traceback.go:458 +0x188 fp=0x7fff2afde560 sp=0x7fff2afde4d0 pc=0x55557e2bea58
runtime.(*unwinder).next(0x7fff2afde5b0)
goroutine 0 gp=0x40003af880 m=13 mp=0x40003ca008 [idle]:
PC=0x55557e2bea58 m=13 sigcode=1 addr=0x118
SIGSEGV: segmentation violation&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Now we could observe some clear patterns. Both errors occur when unwinding the stack in &lt;code&gt;(*unwinder).next&lt;/code&gt;. In one case we saw an intentional fatal error as the runtime identified that unwinding could not complete and the stack was in a bad state. In the other case there was a direct memory access error that happened while trying to unwind the stack. The segfault was discussed in the GitHub issue and a Go engineer identified it as dereference of a go scheduler struct, m, when unwinding.Â   &lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;A review of Go scheduler structs&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Go uses a lightweight userspace scheduler to manage concurrency. Many goroutines are scheduled on a smaller number of kernel threads â this is often referred to as M:N scheduling. Any individual goroutine can be scheduled on any kernel thread. The scheduler has three core types â &lt;code&gt;g&lt;/code&gt;Â  (the goroutine), &lt;code&gt;m&lt;/code&gt; (the kernel thread, or âmachineâ), and &lt;code&gt;p&lt;/code&gt; (the physical execution context, orÂ  âprocessorâ). For a goroutine to be scheduled a free &lt;code&gt;m&lt;/code&gt; must acquire a free &lt;code&gt;p&lt;/code&gt;, which will execute a g. Each &lt;code&gt;g&lt;/code&gt; contains a field for its m if it is currently running, otherwise it will be nil. This is all the context needed for this post but the go runtime docs explore this more comprehensively.Â &lt;/p&gt;
      &lt;p&gt;At this point we can start to make inferences on whatâs happening: the program crashes because we try to unwind a goroutine stack which is invalid. In the first backtrace, if a return address is null, we call &lt;code&gt;finishInternal&lt;/code&gt; and abort because the stack was not fully unwound. The segmentation fault case in the second backtrace is a bit more interesting: if instead the return address is non-zero but not a function then the unwinder code assumes that the goroutine is currently running. It'll then dereference m and fault by accessing &lt;code&gt;m.incgo&lt;/code&gt; (the offset of &lt;code&gt;incgo&lt;/code&gt; into &lt;code&gt;struct m&lt;/code&gt; is 0x118, the faulting memory access). &lt;/p&gt;
      &lt;p&gt;What, then, is causing this corruption? The traces were difficult to get anything useful from â our service has hundreds if not thousands of active goroutines. It was fairly clear from the beginning that the panic was remote from the actual bug. The crashes were all observed while unwinding the stack and if this were an issue any time the stack was unwound on arm64 we would be seeing it in many more services. We felt pretty confident that the stack unwinding was happening correctly but on an invalid stack.Â &lt;/p&gt;
      &lt;p&gt;Our investigation stalled for a while at this point â making guesses, testing guesses, trying to infer if the panic rate went up or down, or if nothing changed. There was a known issue on Goâs GitHub issue tracker which matched our symptoms almost exactly, but what they discussed was mostly what we already knew. At some point when looking through the linked stack traces we realized that their crash referenced an old version of a library that we were also using â Go Netlink.&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;goroutine 1267 gp=0x4002a8ea80 m=nil [runnable (scan)]:
runtime.asyncPreempt2()
        /usr/local/go/src/runtime/preempt.go:308 +0x3c fp=0x4004cec4c0 sp=0x4004cec4a0 pc=0x46353c
runtime.asyncPreempt()
        /usr/local/go/src/runtime/preempt_arm64.s:47 +0x9c fp=0x4004cec6b0 sp=0x4004cec4c0 pc=0x4a6a8c
github.com/vishvananda/netlink/nl.(*NetlinkSocket).Receive(0x14360300000000?)
        /go/pkg/mod/github.com/!data!dog/[email protected]/nl/nl_linux.go:803 +0x130 fp=0x4004cfc710 sp=0x4004cec6c0 pc=0xf95de0
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;We spot-checked a few stack traces and confirmed the presence of this Netlink library. Querying our logs showed that not only did we share a library â every single segmentation fault we observed had happened while preempting &lt;code&gt;NetlinkSocket.Receive&lt;/code&gt;.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Whatâs (async) preemption?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;In the prehistoric era of Go (&amp;lt;=1.13) the runtime was cooperatively scheduled. A goroutine would run until it decided it was ready to yield to the scheduler â usually due to explicit calls to &lt;code&gt;runtime.Gosched()&lt;/code&gt; or injected yield points at function calls/IO operations. Since Go 1.14 the runtime instead does async preemption. The Go runtime has a thread &lt;code&gt;sysmon&lt;/code&gt; which tracks the runtime of goroutines and will preempt any that run for longer than 10ms (at time of writing). It does this by sending &lt;code&gt;SIGURG&lt;/code&gt; to the OS thread and in the signal handler will modify the program counter and stack to mimic a call to &lt;code&gt;asyncPreempt&lt;/code&gt;.&lt;/p&gt;
      &lt;p&gt;At this point we had two broad theories:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;This is a Go Netlink bug â likely due to &lt;code&gt;unsafe.Pointer&lt;/code&gt; usage which invoked undefined behavior but is only actually broken on arm64&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;This is a Go runtime bug and we're only triggering it in &lt;code&gt;NetlinkSocket.Receive&lt;/code&gt; for some reason&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;After finding the same bug publicly reported upstream, we were feeling confident this was caused by a Go runtime bug. However, upon seeing that both issues implicated the same function, we felt more skeptical â notably the Go Netlink library uses unsafe.Pointer so memory corruption was a plausible explanation even if we didn't understand why.&lt;/p&gt;
      &lt;p&gt;After an unsuccessful code audit we had hit a wall. The crashes were rare and remote from the root cause. Maybe these crashes were caused by a runtime bug, maybe they were caused by a Go Netlink bug. It seemed clear that there was something wrong with this area of the code, but code auditing wasnât going anywhere. &lt;/p&gt;
      &lt;p&gt;At this point we had a fairly good understanding of what was crashing but very little understanding of why it was happening. It was clear that the root cause of the stack unwinder crashing was remote from the actual crash, and that it had to do with &lt;code&gt;(*NetlinkSocket).Receive&lt;/code&gt;, but why? We were able to capture a coredump of a production crash and view it in a debugger. The backtrace confirmed what we already knew â that there was a segmentation fault when unwinding a stack. The crux of the issue revealed itself when we looked at the goroutine which had been preempted while calling &lt;code&gt;(*NetlinkSocket).Receive&lt;/code&gt;.Â     &lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;(dlv) bt
0  0x0000555577579dec in runtime.asyncPreempt2
   at /usr/local/go/src/runtime/preempt.go:306
1  0x00005555775bc94c in runtime.asyncPreempt
   at /usr/local/go/src/runtime/preempt_arm64.s:47
2  0x0000555577cb2880 in github.com/vishvananda/netlink/nl.(*NetlinkSocket).Receive
   at
/vendor/github.com/vishvananda/netlink/nl/nl_linux.go:779
3  0x0000555577cb19a8 in github.com/vishvananda/netlink/nl.(*NetlinkRequest).Execute
   at 
/vendor/github.com/vishvananda/netlink/nl/nl_linux.go:532
4  0x0000555577551124 in runtime.heapSetType
   at /usr/local/go/src/runtime/mbitmap.go:714
5  0x0000555577551124 in runtime.heapSetType
   at /usr/local/go/src/runtime/mbitmap.go:714
...
(dlv) disass -a 0x555577cb2878 0x555577cb2888
TEXT github.com/vishvananda/netlink/nl.(*NetlinkSocket).Receive(SB) /vendor/github.com/vishvananda/netlink/nl/nl_linux.go
        nl_linux.go:779 0x555577cb2878  fdfb7fa9        LDP -8(RSP), (R29, R30)
        nl_linux.go:779 0x555577cb287c  ff430191        ADD $80, RSP, RSP
        nl_linux.go:779 0x555577cb2880  ff434091        ADD $(16&amp;lt;&amp;lt;12), RSP, RSP
        nl_linux.go:779 0x555577cb2884  c0035fd6        RET
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;The goroutine was paused between two opcodes in the function epilogue. Since the process of unwinding a stack relies on the stack frame being in a consistent state, it felt immediately suspicious that we preempted in the middle of adjusting the stack pointer. The goroutine had been paused at 0x555577cb2880, between&lt;code&gt; ADD $80, RSP, RSP and ADD $(16&amp;lt;&amp;lt;12), RSP, RSP&lt;/code&gt;.Â &lt;/p&gt;
      &lt;p&gt;We queried the service logs to confirm our theory. This wasnât isolated â the majority of stack traces showed that this same opcode was preempted. This was no longer a weird production crash we couldnât reproduce. A crash happened when the Go runtime preempted between these two stack pointer adjustments. We had our smoking gun.Â &lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Building a minimal reproducer&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;At this point we felt pretty confident that this was actually just a runtime bug and it should be reproducible in an isolated environment without any dependencies. The theory at this point was:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Stack unwinding is triggered by garbage collection&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Async preemption between a split stack pointer adjustment causes a crash&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;What if we make a function which splits the adjustment and then call it in a loop?&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;quote&gt;
        &lt;code&gt;package main

import (
	"runtime"
)

//go:noinline
func big_stack(val int) int {
	var big_buffer = make([]byte, 1 &amp;lt;&amp;lt; 16)

	sum := 0
	// prevent the compiler from optimizing out the stack
	for i := 0; i &amp;lt; (1&amp;lt;&amp;lt;16); i++ {
		big_buffer[i] = byte(val)
	}
	for i := 0; i &amp;lt; (1&amp;lt;&amp;lt;16); i++ {
		sum ^= int(big_buffer[i])
	}
	return sum
}

func main() {
	go func() {
		for {
			runtime.GC()
		}
	}()
	for {
		_ = big_stack(1000)
	}
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;This function ends up with a stack frame slightly larger than can be represented in 16 bits, and so on arm64 the Go compiler will split the stack pointer adjustment into two opcodes. If the runtime preempts between these opcodes then the stack unwinder will read an invalid stack pointer and crash.Â &lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;; epilogue for main.big_stack
ADD $8, RSP, R29
ADD $(16&amp;lt;&amp;lt;12), R29, R29
ADD $16, RSP, RSP
; preemption is problematic between these opcodes
ADD $(16&amp;lt;&amp;lt;12), RSP, RSP
RET
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;After running this for a few minutes the program panicked as expected!&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;SIGSEGV: segmentation violation
PC=0x60598 m=8 sigcode=1 addr=0x118

goroutine 0 gp=0x400019c540 m=8 mp=0x4000198708 [idle]:
runtime.(*unwinder).next(0x400030fd10)
        /home/thea/sdk/go1.23.4/src/runtime/traceback.go:458 +0x188 fp=0x400030fcc0 sp=0x400030fc30 pc=0x60598
runtime.scanstack(0x40000021c0, 0x400002f750)
        /home/thea/sdk/go1.23.4/src/runtime/mgcmark.go:887 +0x290 

[...]

goroutine 1 gp=0x40000021c0 m=nil [runnable (scan)]:
runtime.asyncPreempt2()
        /home/thea/sdk/go1.23.4/src/runtime/preempt.go:308 +0x3c fp=0x40003bfcf0 sp=0x40003bfcd0 pc=0x400cc
runtime.asyncPreempt()
        /home/thea/sdk/go1.23.4/src/runtime/preempt_arm64.s:47 +0x9c fp=0x40003bfee0 sp=0x40003bfcf0 pc=0x75aec
main.big_stack(0x40003cff38?)
        /home/thea/dev/stack_corruption_reproducer/main.go:29 +0x94 fp=0x40003cff00 sp=0x40003bfef0 pc=0x77c04
Segmentation fault (core dumped)

real    1m29.165s
user    4m4.987s
sys     0m43.212s&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;A reproducible crash with standard library only? This felt like conclusive evidence that our problem was a runtime bug.&lt;/p&gt;
      &lt;p&gt;This was an extremely particular reproducer! Even now with a good understanding of the bug and its fix, some of the behavior is still puzzling. It's a one-instruction race condition, so itâs unsurprising that small changes could have large impact. For example, this reproducer was originally written and tested on Go 1.23.4, but did not crash when compiled with 1.23.9 (the version in production), even though we could objdump the binary and see the split ADD still present! We donât have a definite explanation for this behavior â even with the bug present there remain a few unknown variables which affect the likelihood of hitting the race condition.Â &lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;A single-instruction race condition window&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;arm64 is a fixed-length 4-byte instruction set architecture. This has a lot of implications on codegen but most relevant to this bug is the fact that immediate length is limited. &lt;code&gt;add&lt;/code&gt; gets a 12-bit immediate, &lt;code&gt;mov&lt;/code&gt; gets a 16-bit immediate, etc. How does the architecture handle this when the operands don't fit? It depends â &lt;code&gt;ADD&lt;/code&gt; in particular reserves a bit for "shift left by 12" so any 24 bit addition can be decomposed into two opcodes. Other instructions are decomposed similarly, or just require loading an immediate into a register first.Â &lt;/p&gt;
      &lt;p&gt;The very last step of the Go compiler before emitting machine code involves transforming the program into &lt;code&gt;obj.Prog&lt;/code&gt; structs. It's a very low level intermediate representation (IR) that mostly serves to be translated into machine code.Â &lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;//https://github.com/golang/go/blob/fa2bb342d7b0024440d996c2d6d6778b7a5e0247/src/cmd/internal/obj/arm64/obj7.go#L856

// Pop stack frame.
// ADD $framesize, RSP, RSP
p = obj.Appendp(p, c.newprog)
p.As = AADD
p.From.Type = obj.TYPE_CONST
p.From.Offset = int64(c.autosize)
p.To.Type = obj.TYPE_REG
p.To.Reg = REGSP
p.Spadj = -c.autosize
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Notably, this IR is not aware of immediate length limitations. Instead, this happens in asm7.go when Go's internal intermediate representation is translated into arm64 machine code. The assembler will classify an immediate in conclass based on bit size and then use that when emitting instructions â extra if needed.&lt;/p&gt;
      &lt;p&gt;The Go assembler uses a combination of (&lt;code&gt;mov, add&lt;/code&gt;) opcodes for some adds that fit in 16-bit immediates, and prefers (&lt;code&gt;add, add + lsl 12&lt;/code&gt;) opcodes for 16-bit+ immediates.Â   &lt;/p&gt;
      &lt;p&gt;Compare a stack of (slightly larger than) &lt;code&gt;1&amp;lt;&amp;lt;15&lt;/code&gt;:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;; //go:noinline
; func big_stack() byte {
; 	var big_stack = make([]byte, 1&amp;lt;&amp;lt;15)
; 	return big_stack[0]
; }
MOVD $32776, R27
ADD R27, RSP, R29
MOVD $32784, R27
ADD R27, RSP, RSP
RET
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;With a stack of &lt;code&gt;1&amp;lt;&amp;lt;16&lt;/code&gt;:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;; //go:noinline
; func big_stack() byte {
; 	var big_stack = make([]byte, 1&amp;lt;&amp;lt;16)
; 	return big_stack[0]
; } 
ADD $8, RSP, R29
ADD $(16&amp;lt;&amp;lt;12), R29, R29
ADD $16, RSP, RSP
ADD $(16&amp;lt;&amp;lt;12), RSP, RSP
RET
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;In the larger stack case, there is a point between &lt;code&gt;ADD x, RSP, RSP&lt;/code&gt; opcodes where the stack pointer is not pointing to the tip of a stack frame. We thought at first that this was a matter of memory corruption â that in handling async preemption the runtime would push a function call on the stack and corrupt the middle of the stack. However, this goroutine is already in the function epilogue â any data we corrupt is actively in the process of being thrown away. What's the issue then?  &lt;/p&gt;
      &lt;p&gt;The Go runtime often needs to unwind the stack, which means walking backwards through the chain of function calls. For example: garbage collection uses it to find live references on the stack, panicking relies on it to evaluate &lt;code&gt;defer&lt;/code&gt; functions, and generating stack traces needs to print the call stack. For this to work the stack pointer must be accurate during unwinding because of how golang dereferences sp to determine the calling function. If the stack pointer is partially modified, the unwinder will look for the calling function in the middle of the stack. The underlying data is meaningless when interpreted as directions to a parent stack frame and then the runtime will likely crash.Â &lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;//https://github.com/golang/go/blob/66536242fce34787230c42078a7bbd373ef8dcb0/src/runtime/traceback.go#L373

if innermost &amp;amp;&amp;amp; frame.sp &amp;lt; frame.fp || frame.lr == 0 {
    lrPtr = frame.sp
    frame.lr = *(*uintptr)(unsafe.Pointer(lrPtr))
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;When async preemption happens it will push a function call onto the stack but the parent stack frame is no longer correct because sp was only partially adjusted when the preemption happened. The crash flow looks something like this: &lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Async preemption happens between the two opcodes that &lt;code&gt;add x, rsp&lt;/code&gt; expands to&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Garbage collection triggers stack unwinding (to check for heap object liveness)&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;The unwinder starts traversing the stack of the problematic goroutine and correctly unwinds up to the problematic function&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;The unwinder dereferences &lt;code&gt;sp&lt;/code&gt; to determine the parent function&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Almost certainly the data behind &lt;code&gt;sp&lt;/code&gt; is not a function&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Crash&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;We saw earlier a faulting stack trace which ended in &lt;code&gt;(*NetlinkSocket).Receive&lt;/code&gt; â in this case stack unwinding faulted while it was trying to determine the parent frame.Â    &lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;goroutine 90 gp=0x40042cc000 m=nil [preempted (scan)]:
runtime.asyncPreempt2()
/usr/local/go/src/runtime/preempt.go:306 +0x2c fp=0x40060a25d0 sp=0x40060a25b0 pc=0x55557e299dec
runtime.asyncPreempt()
/usr/local/go/src/runtime/preempt_arm64.s:47 +0x9c fp=0x40060a27c0 sp=0x40060a25d0 pc=0x55557e2dc94c
github.com/vishvananda/netlink/nl.(*NetlinkSocket).Receive(0xff48ce6e060b2848?)
/vendor/github.com/vishvananda/netlink/nl/nl_linux.go:779 +0x130 fp=0x40060b2820 sp=0x40060a27d0 pc=0x55557e9d2880
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Once we discovered the root cause we reported it with a reproducer and the bug was quickly fixed. This bug is fixed in go1.23.12, go1.24.6, and go1.25.0. Previously, the go compiler emitted a single &lt;code&gt;add x, rsp&lt;/code&gt; instruction and relied on the assembler to split immediates into multiple opcodes as necessary. After this change, stacks larger than 1&amp;lt;&amp;lt;12 will build the offset in a temporary register and then add that to &lt;code&gt;rsp&lt;/code&gt; in a single, indivisible opcode. A goroutine can be preempted before or after the stack pointer modification, but never during. This means that the stack pointer is always valid and there is no race condition.&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;LDP -8(RSP), (R29, R30)
MOVD $32, R27
MOVK $(1&amp;lt;&amp;lt;16), R27
ADD R27, RSP, RSP
RET&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;This was a very fun problem to debug. We donât often see bugs where you can accurately blame the compiler. Debugging it took weeks and we had to learn about areas of the Go runtime that people donât usually need to think about. Itâs a nice example of a rare race condition, the sort of bug that can only really be quantified at a large scale.&lt;/p&gt;
      &lt;p&gt;Weâre always looking for people who enjoy this kind of detective work. Our engineering teams are hiring. &lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45516000</guid><pubDate>Wed, 08 Oct 2025 13:33:15 +0000</pubDate></item><item><title>The RSS feed reader landscape</title><link>https://lighthouseapp.io/blog/feed-reader-deep-dive</link><description>&lt;doc fingerprint="be149c54c0323cec"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A deep dive into the rss feed reader landscape&lt;/head&gt;
    &lt;p&gt;RSS feeds and, in one form or another, feed readers, have existed for more than 20 years. Their main purpose is enabling their users to consume content from various sources in one place. And especially in recent years, also helping users deal with content overload.&lt;/p&gt;
    &lt;p&gt;Back then there were only a handful of relevant products. Today it's different, there are products for many different situations and use-cases. When first getting into RSS and feed readers, it can be difficult to find out which of this wide range of products are the right ones to use.&lt;/p&gt;
    &lt;p&gt;This article describes the landscape so you can find out which product fits best for your use-case.&lt;/p&gt;
    &lt;p&gt;Side-note: I'm using RSS as a synonym for all web feed standards (Atom, JSON Feed)&lt;/p&gt;
    &lt;head rend="h2"&gt;Classification&lt;/head&gt;
    &lt;p&gt;I attempted a classification of feed readers based on 2 axes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Deployment model: local (phone or PC), browser extension, self-hosted, hosted&lt;/item&gt;
      &lt;item&gt;Business model: free, one-time payment, SAAS&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The deployment model is based on where data is stored and feed fetching happens. Some products have a web app and mobile apps, but feed fetching happens on the server. In this case it's classified as &lt;code&gt;hosted&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The business model categorization is based on the cheapest option that gives access to the full feature set of the product. A self-hostable product with a hosted option is categorized into &lt;code&gt;free&lt;/code&gt;. A freemium SAAS product is categorized as &lt;code&gt;paid (SAAS)&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;And here the image in table format, for easily clickable links:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Pricing&lt;/cell&gt;
        &lt;cell role="head"&gt;On-device (phone or PC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Browser extension&lt;/cell&gt;
        &lt;cell role="head"&gt;Self-hosted&lt;/cell&gt;
        &lt;cell role="head"&gt;Hosted&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Free&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Paid (one-time)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Paid (SAAS)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Hosting models&lt;/head&gt;
    &lt;head rend="h3"&gt;Browser extension&lt;/head&gt;
    &lt;p&gt;Feed readers deployed as browser extensions can be installed through the respective stores of the browsers, usually either the Chrome Web Store or the Firefox Add-ons.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Setup and maintenance: Setup typically only requires installing the extension, not even an account is needed. There is no maintenance required beyond the occasional update, which usually happens automatically.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Data control and storage: Data is stored locally, with browser storage functionality (local storage or IndexedDB). How much data can be stored depends on the storage of the device. Browser extensions can only store as much data as the browser allows itself to use. But for most users this is easily enough.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Feed fetching: Feeds are fetched on the device itself. Because the extension only runs if the browser runs, feeds are only fetched if the browser is open. This can lead to missed posts, depending on the publishing frequency of the feed and how often the browser is active.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Availability: Since all data is stored on the device, by default it's available only on the device where the extension is installed. If users enable it, the extension supports it, and the user is signed in, browsers can sync data to other devices that have the extension. Storing all data on the device also means that it is accessible offline.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Functionality: Extensions can integrate deeply with the browser, and offer features like automatic feed finding of visited websites. Extensions can also provide a comprehensive feature set. Their only limitations are more compute-intensive features (e.g. requiring machine learning), or features that require special infrastructure to run (e.g. emails).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Products (free)&lt;/head&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell role="head"&gt;Free&lt;/cell&gt;
        &lt;cell role="head"&gt;Paid (one-time)&lt;/cell&gt;
        &lt;cell role="head"&gt;Paid (SAAS)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In this category I only found one product. Smart-RSS was another one, but was discontinued in February.&lt;/p&gt;
    &lt;head rend="h3"&gt;On-device&lt;/head&gt;
    &lt;p&gt;On-device products are separate applications and installed on the device you want to use them. Either on your iOS or Android phone or tablet, or on your Windows, Mac, or Linux computer. They fetch feeds and store data on that device.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Setup and maintenance: In general setup is done by installing the application. Some products may require an account, but that is not the norm. Maintenance is similar to browser extensions, the occasional updates. Some applications require manual update installations, others do that automatically.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Data control and storage: Data is stored locally on the device, which gives total control over the data. The maximum amount of feeds and articles stored only depend on the available storage of the device.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Feed fetching: Feeds are fetched on the device. For that to happen the application must be running. Some products may implement a background fetching service, in that case only the device must be turned on. Similar to browser extensions, this limitation may lead to missed posts.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Availability: Typically the data of on-device feed readers are only available on the device where it's installed. Data sync would need to be done manually or via operating system specific mechanisms (e.g. iCloud), which not all products support. Since data is stored on the device, it's also available offline.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Functionality: On-device products can use the full computing power of the device. Combined with the comparatively small storage requirements of one user, it means that some features can be significantly faster than other categories. The exact features depend on the application, and mobile apps are typically less powerful than desktop apps. Limitations are only features that require multiple users (e.g. recommendations) or specific infrastructure (e.g. newsletter subscriptions).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Products&lt;/head&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell role="head"&gt;Free&lt;/cell&gt;
        &lt;cell role="head"&gt;Paid (one-time)&lt;/cell&gt;
        &lt;cell role="head"&gt;Paid (SAAS)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Self-hosted&lt;/head&gt;
    &lt;p&gt;Self-hosted products all fall into the open-source and free category. They are designed to be installed on a server to run continuously, though it is possible to install them on a PC as well. They fetch feeds and store data on the server, and make it available through a web interface.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Setup and maintenance: Setup requires a server, installing the application, and depending on how it should be accessible, possibly also domain and reverse proxy setup. This needs significantly more technical knowledge than the other options, but with ChatGPT (or others) it should be possible also for fairly non-technical people. This setup is generally more complex with more failure points than the other options, but with a correct setup failures happen rarely, if at all.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Data control and storage: The application stores data on the server it runs on. Since users control their server, they also have total control over the data stored on it. Applications typically use well-established databases, which allows users to inspect and change data with common tools. The amount of data that can be stored is only limited by the available storage on the server. Most servers start at 20 GB, which is enough for most feed reading use-cases. Often, storage can be dynamically extended if more space is needed.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Feed fetching: Feeds are fetched on the server. Since it runs continuously, there is no break in feed fetching, and even high frequency feeds are no problem.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Availability: The application and all its data is accessible from any device with a web browser by navigating to the server's URL. Data is stored on the server, therefore automatically synced between all devices that use it, but also require internet access.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Functionality: On average self-hosted products have a wider feature set than browser extension or on-device feed readers. There is no theoretical limit on the feature set, but typically they keep the infrastructure as simple as possible, to keep setup and maintenance straightforward. This makes them slightly less powerful than hosted alternatives.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Products&lt;/head&gt;
    &lt;p&gt;Self-hosted products are all open-source and free. The only costs are for the server, and the cheapest VPS plans start at ~2$/month.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hosted&lt;/head&gt;
    &lt;p&gt;Hosted feed readers are managed services. It's required to create an account to get access to them. All of them are paid SAAS products, and most offer a free plan.&lt;/p&gt;
    &lt;p&gt;On average they have the most polished user experience and most comprehensive feature sets, since they're backed by companies that invest in continuous development.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Setup and maintenance: Beyond creating an account, no setup is required. The same for maintenance. Service providers make sure the products work as intended, and deal with everything required to keep the service running, including infrastructure setup, monitoring, backups, etc.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Data control and storage: Since data is stored on the servers and databases of the company running the product, users don't have direct control. But through laws like GDPR users have the right to export all their data, or request deletion. Storage is essentially unlimited, though most products have limits to avoid abuse. These limits are mentioned on their pricing page.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Feed fetching: Similar to self-hosted products, feeds are fetched on the server, which runs continuously and ensures that even high-frequency feeds don't pose a problem. The fetching interval is usually dynamic, based on the popularity of a feed and the publishing frequency.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Availability: Hosted products are primarily available as web application, and some offer native apps as well. Since data is stored on the server, it's natively synced between devices. Some hosted feed readers also offer offline support.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Functionality: Hosted feed readers serve many customers at the same time, which makes it necessary to have more complex infrastructure setups. This also enables features that other types of products cannot do, like email receiving, recommendations, or historic feed contents. Consequently hosted options are generally more powerful than other categories, though the specific feature set depends on the product.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Products&lt;/head&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell role="head"&gt;Free&lt;/cell&gt;
        &lt;cell role="head"&gt;Paid (one-time)&lt;/cell&gt;
        &lt;cell role="head"&gt;Paid (SAAS)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;All hosted products are SAAS products and charge monthly. &lt;code&gt;Folo&lt;/code&gt; is an outlier, they currently don't have any paid features, but the terms of service indicate that this will come in the future. At that point it will join the others in the &lt;code&gt;Paid (SAAS)&lt;/code&gt; category.&lt;/p&gt;
    &lt;head rend="h2"&gt;App support and offline access for products that don't offer it natively&lt;/head&gt;
    &lt;p&gt;Many of the self-hosted or hosted products don't provide apps or offline access by themselves. However, with the right setup, it's still possible to access the articles offline.&lt;/p&gt;
    &lt;p&gt;Most of these products provide APIs. This can be a proprietary API, or an API based on the old Google Reader API.&lt;/p&gt;
    &lt;p&gt;Mobile apps, for example ReadKit or Fiery Feeds, can not only subscribe to feeds, but also connect to other products through their APIs. They download contents, store them locally, and sync changes back to the connected products.&lt;/p&gt;
    &lt;p&gt;These apps make it possible to use a native mobile app for products that don't provide an app themselves, and get offline access at the same time.&lt;/p&gt;
    &lt;p&gt;FreshRSS, for example, has a list of native apps that support it.&lt;/p&gt;
    &lt;head rend="h2"&gt;A note on newsletters&lt;/head&gt;
    &lt;p&gt;Newsletters are a growing mechanism for delivering (blog) content. Some, but not all, hosted feed readers support newsletters out of the box. But on-device products can't do that at all because of their infrastructure limitations.&lt;/p&gt;
    &lt;p&gt;Even if a feed reader doesn't support newsletters by itself, it's still possible to get newsletters into the feed reader, through services that convert newsletters into RSS feeds.&lt;/p&gt;
    &lt;p&gt;These services generate an email address and give you a corresponding feed URL. Emails to the generated address are added to the feed as new entry. So after signing up to the newsletter and adding the feed URL, the newsletter is added to the feed reader even without native email support.&lt;/p&gt;
    &lt;p&gt;Services that do this are Kill the Newsletter and the Lighthouse Newsletter to RSS tool. Both are free.&lt;/p&gt;
    &lt;head rend="h2"&gt;Products&lt;/head&gt;
    &lt;p&gt;The descriptions highlight the defining and unique aspects of the products, for the full feature list please go to their respective websites. Where available I used the screenshots for their websites, to represent the products as best as possible (test account screenshots wouldn't be as good).&lt;/p&gt;
    &lt;head rend="h3"&gt;NetNewsWire (on-device, free)&lt;/head&gt;
    &lt;p&gt;NetNewsWire is a free, on-device feed reader for Mac and iOS. By default it stores data on the device itself, but can sync via iCloud and using other products as storage. And it provides a Safari extension for easy feed-adding. Notably, it supports AppleScript, which makes it possible to automate certain workflows.&lt;/p&gt;
    &lt;head rend="h3"&gt;Fiery Feeds (on device, paid SAAS)&lt;/head&gt;
    &lt;p&gt;Fiery feeds is a Mac and iOS app. The download is free, and to get the premium features it costs ~15$/year (varies by region). By default it stores data on the device itself, but can sync via iCloud and using the API of other products (e.g. FreshRSS). The main distinguishing features are the customization options of the app, like custom themes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reeder (on-device, SAAS)&lt;/head&gt;
    &lt;p&gt;The current version of Reeder is a Mac and iOS app. The download is free, and to get the premium features it costs ~10$/year. The previous version of Reeder, now called Reeder Classic, is still available as one-time purchase. It also stores data on-device, and can sync via iCloud.&lt;/p&gt;
    &lt;p&gt;The main distinguishing feature is the unified timeline, which includes RSS, podcasts, social media, and more.&lt;/p&gt;
    &lt;head rend="h3"&gt;FreshRSS (self-hosted, free)&lt;/head&gt;
    &lt;p&gt;FreshRSS is a self-hosted feed reader and once set up, it's available as a web app. It is quite powerful, and in addition to normal feed subscriptions offers WebSub as well. It's possible to customize it with themes and extensions, and it's translated into more than 15 languages.&lt;/p&gt;
    &lt;p&gt;Together with Miniflux they're the most-recommended self-hosted options.&lt;/p&gt;
    &lt;head rend="h3"&gt;Miniflux&lt;/head&gt;
    &lt;p&gt;Miniflux is a self-hosted feed reader as well, and available as web app after setup. It's focus is on staying simple and fast instead of adding fancy features. It also offers a hosted version, which is 15$/year and has a 15 day trial period.&lt;/p&gt;
    &lt;p&gt;Together with FreshRSS they're the most-recommended self-hosted options.&lt;/p&gt;
    &lt;head rend="h3"&gt;Folo (hosted, free)&lt;/head&gt;
    &lt;p&gt;Folo is a relatively new product developed by the creators of RSS Hub. It's a free hosted feed reader, with apps available for every major platform. Their terms of service suggest that at some point they will charge a monthly fee for some premium features, but at the time of writing there were no paid features.&lt;/p&gt;
    &lt;p&gt;Folo is also open-source and therefore theoretically self-hostable, but I haven't found documentation for it.&lt;/p&gt;
    &lt;p&gt;It has a huge feature set, including newsletter support, transforming websites into feeds, AI summaries, and much more.&lt;/p&gt;
    &lt;head rend="h3"&gt;Feedly (hosted, SAAS)&lt;/head&gt;
    &lt;p&gt;Feedly is the most widely-known product and has the most users of all feed readers. It has a comprehensive features set, and offers a free plan as well. For the past couple of years, it seems that Feedly has focused more on AI features and enterprise customers, but their core product remains solid.&lt;/p&gt;
    &lt;head rend="h3"&gt;Inoreader&lt;/head&gt;
    &lt;p&gt;Inoreader is the second most widely-known product, after Feedly. It too offers a free plan. Their feature list is impressive, with social media support for subscriptions, automation and AI features, public API and integrations. And of course much more.&lt;/p&gt;
    &lt;p&gt;What stands out on Reddit are complaints about price hikes. Though Inoreader probably has tens or hundreds of thousands of users who don't have issues and think the product is great.&lt;/p&gt;
    &lt;head rend="h3"&gt;Readwise Reader&lt;/head&gt;
    &lt;p&gt;Readwise Reader is a relatively new product, from the creators of the original Readwise. It's a great feed reader, though were it really shines is the reading experience. They also have reading views for PDFs, eBooks, and more.&lt;/p&gt;
    &lt;p&gt;The website mentions that it's in beta, but it has been in beta for years now and at this point is a stable product, and has been for awhile.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tiny Tiny RSS&lt;/head&gt;
    &lt;p&gt;Tiny Tiny RSS deserves an honorable mention in this list. It was, and still is, used by many, and has been for a long time, and was often recommended on the RSS subreddit.&lt;/p&gt;
    &lt;p&gt;On October 3rd the maintainer announced that he's going to stop working on it, and will remove all infrastructure on November 1st. Forks of the project with other maintainers may pop up, but at the moment it's too soon to tell what the future of Tiny Tiny RSS will be.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lighthouse&lt;/head&gt;
    &lt;p&gt;Lighthouse is also a relatively new product. It's in beta, which refers to the fact that it doesn't yet have all features necessary to fulfill its vision, which goes beyond simple feed reading.&lt;/p&gt;
    &lt;p&gt;The main differentiator is that it focuses on articles over feeds, and has a separate view for curating articles (the inbox). It's focused on finding high-value content.&lt;/p&gt;
    &lt;head rend="h2"&gt;Similar product categories&lt;/head&gt;
    &lt;p&gt;Feed readers are powerful products, which require manual setup to make them work well. Subscribing to feeds is mandatory, adding tags, filtered views, and rules can make them work even better. But for some use-cases, other product categories are simpler and might work better.&lt;/p&gt;
    &lt;head rend="h3"&gt;News aggregators&lt;/head&gt;
    &lt;p&gt;News aggregators automatically collect and curate news from a large variety of sources. They usually provide some customization options, but focus on news and don't allow arbitrary feed subscriptions like feed readers do.&lt;/p&gt;
    &lt;p&gt;They focus on providing an overview of the most relevant news stories.&lt;/p&gt;
    &lt;p&gt;Examples are Kagi News, Ground News, and SmartNews.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reading lists&lt;/head&gt;
    &lt;p&gt;Reading lists, also called read-it-later apps, are for saving and organizing links you found somewhere on the web. Many feed readers can do the same, but reading lists are optimized for that purpose.&lt;/p&gt;
    &lt;p&gt;Examples are Instapaper and Matter. Karakeep is a self-hosted option.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to choose&lt;/head&gt;
    &lt;p&gt;For most people, going with one of the hosted products is the best option. They're generally the most polished and most powerful products, owing to the fact that they have companies behind them with full-time engineers. They generally also offer a free plan, so if you stay within the limits of those, they will even be free to use.&lt;/p&gt;
    &lt;p&gt;For more specific requirements, products of the other categories can work better. They have different characteristics, and can work better in some situations. For example, if you want total control over your data, self-hosted options are the way to go.&lt;/p&gt;
    &lt;p&gt;It's probably easiest to first decide on the category, and then check out multiple products, or maybe even try them out. Virtually all products support OPML import and export, so moving feed subscriptions from one product to the next is almost zero effort.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45517134</guid><pubDate>Wed, 08 Oct 2025 15:17:36 +0000</pubDate></item><item><title>WinBoat: Windows apps on Linux with seamless integration</title><link>https://www.winboat.app/</link><description>&lt;doc fingerprint="b48fe95fd1b73cdc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt; Run Windows apps on 🐧 Linux&lt;lb/&gt;with ✨ seamless integration &lt;/head&gt;
    &lt;head rend="h2"&gt;FFFFF/Features/sssss&lt;/head&gt;
    &lt;head rend="h2"&gt;Elegant Interface&lt;/head&gt;
    &lt;p&gt;WinBoat provides a sleek and intuitive interface that seamlessly integrates Windows into your Linux desktop environment, making it feel like a native experience.&lt;/p&gt;
    &lt;head rend="h2"&gt;Automated Installs&lt;/head&gt;
    &lt;p&gt;Installing Windows via WinBoat is a delightfully simple process all done through our interface. Pick your preferences &amp;amp; specs and let us handle the rest.&lt;/p&gt;
    &lt;head rend="h2"&gt;Run Any App&lt;/head&gt;
    &lt;p&gt;If it runs on Windows, it can run on WinBoat. Enjoy the full range of Windows applications, from productivity tools to entertainment, all within your Linux environment as native OS-level windows.&lt;/p&gt;
    &lt;head rend="h2"&gt;Filesystem Integration&lt;/head&gt;
    &lt;p&gt;Accessing your Linux filesystem from Windows is a breeze. Your home directory is mounted in Windows, allowing you to easily share files between the two systems without any hassle.&lt;/p&gt;
    &lt;head rend="h2"&gt;And many more...&lt;/head&gt;
    &lt;p&gt;There's tons more you can do with WinBoat, for example smartcard passthrough and resource monitoring. We're always looking to add more features, so stay tuned for updates!&lt;/p&gt;
    &lt;p&gt;Scroll for more!&lt;/p&gt;
    &lt;head rend="h2"&gt;Elegant Interface&lt;/head&gt;
    &lt;p&gt;WinBoat provides a sleek and intuitive interface that seamlessly integrates Windows into your Linux desktop environment, making it feel like a native experience.&lt;/p&gt;
    &lt;head rend="h2"&gt;Automated Installs&lt;/head&gt;
    &lt;p&gt;Installing Windows via WinBoat is a delightfully simple process all done through our interface. Pick your preferences &amp;amp; specs and let us handle the rest.&lt;/p&gt;
    &lt;head rend="h2"&gt;Run Any App&lt;/head&gt;
    &lt;p&gt;If it runs on Windows, it can run on WinBoat. Enjoy the full range of Windows applications, from productivity tools to entertainment, all within your Linux environment as native OS-level windows.&lt;/p&gt;
    &lt;head rend="h2"&gt;Filesystem Integration&lt;/head&gt;
    &lt;p&gt;Accessing your Linux filesystem from Windows is a breeze. Your home directory is mounted in Windows, allowing you to easily share files between the two systems without any hassle.&lt;/p&gt;
    &lt;head rend="h2"&gt;And many more...&lt;/head&gt;
    &lt;p&gt;There's tons more you can do with WinBoat, for example smartcard passthrough and resource monitoring. We're always looking to add more features, so stay tuned for updates!&lt;/p&gt;
    &lt;head rend="h2"&gt;DDDD/Download/ddddd&lt;/head&gt;
    &lt;p&gt;We're excited to have you onboard! Pick your platform below to get started with WinBoat within minutes, not hours.&lt;/p&gt;
    &lt;head rend="h2"&gt;CCCC/Contribute/eeeee&lt;/head&gt;
    &lt;p&gt;WinBoat is an open-source project licensed under MIT, and we welcome contributions from the community. Whether you're a developer, designer, or just someone who loves WinBoat, there are many ways you can help us improve and grow.&lt;/p&gt;
    &lt;head rend="h2"&gt;CCCC/Community/yyyyy&lt;/head&gt;
    &lt;p&gt;We usually hang out on Discord, come join and chat with us!&lt;/p&gt;
    &lt;head rend="h2"&gt;FFFFF/Frequently Asked Questions/sssss&lt;/head&gt;
    &lt;head class="p-6 text-lg bg-gradient-to-b from-indigo-500/50 to-indigo-900/40 rounded-2xl select-none cursor-pointer" data-astro-cid-al2ca2vr=""&gt;How does it compare to WinApps?&lt;/head&gt;
    &lt;p&gt;With WinApps you do the bulk of the setup manually, and there's no cohesive interface to bring it all together. There's a basic TUI, a taskbar widget, and some CLI commands for you to play with.&lt;lb/&gt; WinBoat does all the setup once you have the pre-requisites installed, displays everything worth seeing in a neat interface for you, and acts like a complete experience. No need to mess with configuration files, no need to memorize a dozen CLI commands, it just works.&lt;/p&gt;
    &lt;head class="p-6 text-lg bg-gradient-to-b from-indigo-500/50 to-indigo-900/40 rounded-2xl select-none cursor-pointer" data-astro-cid-al2ca2vr=""&gt;What are the advantages of using this over CrossOver or WINE?&lt;/head&gt;
    &lt;p&gt;You can run stuff that doesn't play well with CrossOver or WINE, and have a full Windows desktop at the same time.&lt;lb/&gt; We've had numerous apps that weren't working nicely (or at all) in Wine, this is one of the reasons we've created WinBoat. Some examples would be Affinity Photo, Paint Tool Sai v1.0, the entire Adobe suite, AeroChat, Acrobat, and of course Office.&lt;/p&gt;
    &lt;head class="p-6 text-lg bg-gradient-to-b from-indigo-500/50 to-indigo-900/40 rounded-2xl select-none cursor-pointer" data-astro-cid-al2ca2vr=""&gt;Will I be able to configure my peripherals / hardware using WinBoat?&lt;/head&gt;
    &lt;p&gt;If your peripheral / hardware uses USB to connect to your device. then yes! Starting from WinBoat 0.8.0 we support USB passthrough as an experimental feature and you can use any Windows software needed for configuration, it should work out of the box.&lt;/p&gt;
    &lt;head class="p-6 text-lg bg-gradient-to-b from-indigo-500/50 to-indigo-900/40 rounded-2xl select-none cursor-pointer" data-astro-cid-al2ca2vr=""&gt;Is there USB passthrough?&lt;/head&gt;
    &lt;p&gt; Yes! Starting from WinBoat 0.8.0 we support USB passthrough as an experimental feature. Please give it a try and let us know what you think! 😄&lt;lb/&gt; If for whatever reason you're stuck with an older version of WinBoat, you can modify the docker-compose.yml file in ~/.winboat once you finished setting up WinBoat. You can add the appropriate USB devices like this, followed by executing docker-compose down and docker-compose up -d in the same folder. Please make sure to remove these changes before you upgrade to &amp;gt;=0.8.0 though, as they are incompatible with our implementation.&lt;/p&gt;
    &lt;head class="p-6 text-lg bg-gradient-to-b from-indigo-500/50 to-indigo-900/40 rounded-2xl select-none cursor-pointer" data-astro-cid-al2ca2vr=""&gt;Is there GPU passthrough?&lt;/head&gt;
    &lt;p&gt;Not at the moment, but we plan on eventually implementing GPU acceleration through paravirtualized drivers.&lt;lb/&gt; We have looked at MVisor Win VGPU Driver for OpenGL, which seems promising from our tests, but it's for a different hypervisor (not compatible with QEMU). Some other folks are also working on DirectX drivers but nothing that we can try out yet.&lt;lb/&gt; We have also looked into Looking Glass extensively, specifically their Indirect Display Driver which does not need a second GPU, because it'd be absolutely amazing to have it. We got the driver to compile and start via some hacks, but couldn't get much more than a black screen. The developer says it is not ready for general use yet at all, however we plan to integrate it once it is ready.&lt;/p&gt;
    &lt;head class="p-6 text-lg bg-gradient-to-b from-indigo-500/50 to-indigo-900/40 rounded-2xl select-none cursor-pointer" data-astro-cid-al2ca2vr=""&gt;Does it run games with anti-cheat that don't run on Linux?&lt;/head&gt;
    &lt;p&gt;Unfortunately running games with kernel anti-cheat is not possible, as they block virtualization.&lt;/p&gt;
    &lt;head class="p-6 text-lg bg-gradient-to-b from-indigo-500/50 to-indigo-900/40 rounded-2xl select-none cursor-pointer" data-astro-cid-al2ca2vr=""&gt;Any possibility of adding Podman support as a Docker alternative?&lt;/head&gt;
    &lt;p&gt;Podman support is planned. We tried working on it, some contributors also tried working on it, but there's some issues with networking (specifically the guest server being unreachable) that prevent it from being functional for now.&lt;/p&gt;
    &lt;head class="p-6 text-lg bg-gradient-to-b from-indigo-500/50 to-indigo-900/40 rounded-2xl select-none cursor-pointer" data-astro-cid-al2ca2vr=""&gt;Will you make it into a Flatpak?&lt;/head&gt;
    &lt;p&gt;This is on our to-do list, but it'll take some effort because Flatpak is pretty isolated from the rest of the system and apps, so we'd have to find a way to expose installed apps, the Docker binary, and the Docker socket, and many other utilities.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45518813</guid><pubDate>Wed, 08 Oct 2025 17:56:32 +0000</pubDate></item><item><title>Discord says 70k users may have had their government IDs leaked in breach</title><link>https://www.theverge.com/news/797051/discord-government-ids-leaked-data-breach</link><description>&lt;doc fingerprint="52821b91fd9720ef"&gt;
  &lt;main&gt;
    &lt;p&gt;Discord has identified approximately 70,000 users that may have had their government ID photos exposed as part of a customer service data breach announced last week, spokesperson Nu Wexler tells The Verge. A tweet by vx-underground said that the company was being extorted over a breach of its Zendesk instance by a group claiming to have “1.5TB of age verification related photos. 2,185,151 photos.”&lt;/p&gt;
    &lt;head rend="h1"&gt;Discord says 70,000 users may have had their government IDs leaked in breach&lt;/head&gt;
    &lt;p&gt;Discord claims that the attackers are circulating inaccurate information about the breach of a customer service provider as part of an extortion attempt.&lt;/p&gt;
    &lt;p&gt;Discord claims that the attackers are circulating inaccurate information about the breach of a customer service provider as part of an extortion attempt.&lt;/p&gt;
    &lt;p&gt;When we asked about the tweet, Wexler shared this statement:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Following last week’s announcement about a security incident involving a third-party customer service provider, we want to address inaccurate claims by those responsible that are circulating online. First, as stated in our blog post, this was not a breach of Discord, but rather a third-party service we use to support our customer service efforts. Second, the numbers being shared are incorrect and part of an attempt to extort a payment from Discord. Of the accounts impacted globally, we have identified approximately 70,000 users that may have had government-ID photos exposed, which our vendor used to review age-related appeals. Third, we will not reward those responsible for their illegal actions.&lt;/p&gt;
      &lt;p&gt;All affected users globally have been contacted and we continue to work closely with law enforcement, data protection authorities, and external security experts. We’ve secured the affected systems and ended work with the compromised vendor. We take our responsibility to protect your personal data seriously and understand the concern this may cause.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In its announcement last week, Discord said that information like names, usernames, emails, the last four digits of credit cards, and IP addresses also may have been impacted by the breach.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45521738</guid><pubDate>Wed, 08 Oct 2025 23:20:13 +0000</pubDate></item><item><title>A competitor crippled a $23.5M bootcamp by becoming a Reddit moderator</title><link>https://larslofgren.com/codesmith-reddit-reputation-attack/</link><description>&lt;doc fingerprint="3a43713708620467"&gt;
  &lt;main&gt;
    &lt;p&gt;Let’s say you decide to start a coding bootcamp. Your background is in pedagogy and you love teaching. Your parents were teachers.&lt;/p&gt;
    &lt;p&gt;You find a co-founder, raise a bit of money, and pour your soul into your company.&lt;/p&gt;
    &lt;p&gt;The first couple of years, students love your program. Positive feedback, extraordinary student outcomes, employees love the mission. You are quite literally changing lives.&lt;/p&gt;
    &lt;p&gt;Your business grows. One day, you realize you’ve grown to 70 employees.&lt;/p&gt;
    &lt;p&gt;And then…&lt;/p&gt;
    &lt;p&gt;A competitor gets control of the main subreddit for your industry by becoming a Reddit Moderator.&lt;/p&gt;
    &lt;p&gt;That watering hole becomes their megaphone. They are not shy about using it. An all-out attack on your brand begins. The barrage of accusations and harassment are relentless. The attacks happen daily. You become a neurotic fixation of the moderator. Every little thing you do represents your failings as a company.&lt;/p&gt;
    &lt;p&gt;You get compared to a sex cult. One of your employees is accused of nepotism. The mod starts joining your company information sessions for prospective students, slinging conspiracy theories at every turn under a pseudonym. You avoid outright bans to avoid appearing biased yourself. Your own employees start wondering if these accusations have merit, some leave.&lt;/p&gt;
    &lt;p&gt;Even worse, confidential information from your company starts leaking out. There’s a mole. Or at least your team suspects it. No one knows who to trust. Your carefully built company culture? Eviscerated from the inside out.&lt;/p&gt;
    &lt;p&gt;Any time you attempt to defend yourself in the main subreddit, posts get deleted. Or you’re accused of running a Reddit bot army.&lt;/p&gt;
    &lt;p&gt;This goes on for years.&lt;/p&gt;
    &lt;p&gt;Every day, another attack. Every fucking day.&lt;/p&gt;
    &lt;p&gt;Student applications drop. First a little… then a lot.&lt;/p&gt;
    &lt;p&gt;Combined with a market downtown, your revenue collapses by 80%.&lt;/p&gt;
    &lt;p&gt;You go through 2 layoffs just to keep the lights on. A bunch of other folks move on. You’re down to 15 employees now.&lt;/p&gt;
    &lt;p&gt;The attacks don’t stop. If anything, they escalate.&lt;/p&gt;
    &lt;p&gt;And you begin to wonder, as the CEO and founder of your company… “Maybe I’m the problem? Maybe I am doing something wrong? Or somehow encouraging these attacks on my company?”&lt;/p&gt;
    &lt;p&gt;You make a decision to leave.&lt;/p&gt;
    &lt;p&gt;You step down, walk away from everything you built, and start anew.&lt;/p&gt;
    &lt;p&gt;But even then, the attacks don’t stop. Nothing can stop this nightmare.&lt;/p&gt;
    &lt;p&gt;This is the story of Will Sentance and his company, Codesmith.&lt;/p&gt;
    &lt;p&gt;It can happen to your business too.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Codesmith Reputation Attack via Google&lt;/head&gt;
    &lt;p&gt;For many businesses, the first page of Google for their brand name is the single most important asset for managing their reputation.&lt;/p&gt;
    &lt;p&gt;Let’s say I’m considering a bootcamp, I hear about “Codesmith,” and pop it into Google.&lt;/p&gt;
    &lt;p&gt;Fuck me that’s bad. This is the number 2 result right after the company website.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;“Codesmith is an enormous waste of money”&lt;/item&gt;
      &lt;item&gt;“Do Not Go To Codesmith”&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I don’t even have to click, Google is serving that stuff up right under the company’s website. God damn.&lt;/p&gt;
    &lt;p&gt;That top Reddit thread has been ranking highly for Codesmith brand terms since Sept 2024. That’s a year’s worth of brand carnage.&lt;/p&gt;
    &lt;p&gt;Pay close attention to the subreddit all those threads are from: r/codingbootcamp. That’s the key to all this.&lt;/p&gt;
    &lt;head rend="h2"&gt;Are LLMs Also a Reputation Attack Vector on Codesmith?&lt;/head&gt;
    &lt;p&gt;Yup.&lt;/p&gt;
    &lt;p&gt;My prompt in ChatGPT was “is codesmith a good bootcamp?”.&lt;/p&gt;
    &lt;p&gt;The intro has a bunch of standard stuff, and then there was this:&lt;/p&gt;
    &lt;p&gt;Same Reddit threads are featured, same brand-destroying quotes.&lt;/p&gt;
    &lt;p&gt;By the way, CIRR is a nonprofit in the bootcamp industry that collects graduation data across the industry. It’s a neutral third-party with the goal of providing transparency on student outcomes across coding bootcamps. If your graduation rates are solid, wouldn’t it make sense for a competitor to call that data into question? ChatGPT is regurgitating those same doubts here.&lt;/p&gt;
    &lt;p&gt;At this point, I’ve done a quick brand search and asked ChatGPT if Codesmith is any good. I already have a TON of doubts as a prospect.&lt;/p&gt;
    &lt;p&gt;I don’t believe this is an accident. Or a result of Codesmith’s actual quality as a bootcamp.&lt;/p&gt;
    &lt;p&gt;I believe this brand attack is the result of one person’s actions.&lt;/p&gt;
    &lt;p&gt;When I’ve researched Codesmith’s brand, every Reddit thread I’ve seen in Google and various LLMs have all been from a single subreddit: r/codingbootcamp. That subreddit is controlled by the Reddit Moderator Michael Novati.&lt;/p&gt;
    &lt;head rend="h2"&gt;Who is Michael Novati?&lt;/head&gt;
    &lt;p&gt;For the best recap of Michael, listen to this interview from Pragmatic Engineer Gergely Orosz.&lt;/p&gt;
    &lt;p&gt;The highlights:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Michael joined Facebook early in 2009.&lt;/item&gt;
      &lt;item&gt;As he tells it, went through a bunch of promotions quickly and made it to Principal Software Engineer (E7).&lt;/item&gt;
      &lt;item&gt;Left Facebook in 2017.&lt;/item&gt;
      &lt;item&gt;In 2019, co-founded his dev bootcamp, Formation, with his wife, Sophie Novati. His wife is the CEO, Michael took the CTO title.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In that interview, Michael tells a story about playing Risk (the board game) with Mark Zuckerberg.&lt;/p&gt;
    &lt;p&gt;“The way we became friends, he [Zuckerberg] used to play Risk, the board game. I think it was once a week that people would get together. Like 4 or 5 people. He would show up often and I really liked Risk. I would always play Risk as a kid for whatever reason, I don’t even know why.&lt;/p&gt;
    &lt;p&gt;And one day, we’re down to the final three people. And it was me, him, and I was losing. Mark was in second place, the leader was taking over.&lt;/p&gt;
    &lt;p&gt;I did a really.. uhhh.. let’s say delicate, strategic maneuver. Where I made an alliance with him to share resources and go after the first place person. So he went all in on the first place person.&lt;/p&gt;
    &lt;p&gt;My turn’s next. I did not go all in against the first place person, I took over Mark’s remaining resources. He just dwindled going after the first place person. He accepted my friendship request [on Facebook] very shortly that evening.&lt;/p&gt;
    &lt;p&gt;It’s weird, I basically backstabbed him in the game, really bad. Blatantly to his face. But it’s the game, that’s what Risk is. It’s a strategy game. I think he appreciated the strategy I had. It made him feel more like he could trust me. Even though I backstabbed him, he knows where my strategic thinking is coming from.“&lt;/p&gt;
    &lt;p&gt;Now, is this just a funny anecdote about an early Facebook backstabbing Mark Zuckerberg and then becoming friends? Maybe.&lt;/p&gt;
    &lt;p&gt;For me, it tells me a lot about how Michael Novati plays competitive games. In my experience, people tend to run businesses the same way they compete at anything. Maybe Michael is the exception.&lt;/p&gt;
    &lt;p&gt;I do have to admit, it takes some real cojones to fuck over your boss that hard in a board game. Especially when that boss is Zuckerberg.&lt;/p&gt;
    &lt;p&gt;One last note: Formation raised a $4M seed round in 2021, led by Andreessen Horowitz.&lt;/p&gt;
    &lt;head rend="h2"&gt;Michael Novati’s Control Over the Coding Bootcamp Industry&lt;/head&gt;
    &lt;p&gt;Michael Novati’s power over the coding bootcamp industry comes from one place: being a moderator on the subreddit r/codingbootcamp.&lt;/p&gt;
    &lt;p&gt;He is, by far, the most active moderator for the subreddit. You’ll bump into him immediately just by visiting the subreddit:&lt;/p&gt;
    &lt;p&gt;What about the other mods?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;u/samabuna hasn’t posted in about 8 years.&lt;/item&gt;
      &lt;item&gt;u/dowcet/ is still active on Reddit but rarely posts in r/codingbootcamp. When I checked, I only found one comment in the preceeding 1-2 months.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So the subreddit is Michael Novati’s show. Full stop.&lt;/p&gt;
    &lt;p&gt;We have to remember that Reddit isn’t just Reddit anymore. The powers that be have decided that Reddit is infallible, a reliable set of training data for LLMs, and should be featured fucking everywhere.&lt;/p&gt;
    &lt;p&gt;Before we get to what Michael has actually done, let’s lay out the potential power for anyone that gets control of a key subreddit in their industry:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If the subreddit is greenlit by Google, threads in your subreddit can easily pop to the top of Google searches that are extremely valuable to your business. Brand terms, review terms, product categories, major how-to topics, all of it.&lt;/item&gt;
      &lt;item&gt;LLMs prioritize Reddit heavily. If you want to spin conspiracy theories, those conspiracies will start to become part of the zeitgeist as every LLM regurgitates them.&lt;/item&gt;
      &lt;item&gt;Skew the narrative for long enough and it’ll be impossible for ANY prospect to not stumble across your skewed narrative when they’re researching products or services.&lt;/item&gt;
      &lt;item&gt;You can delete posts and comments at will. Want to tip the narrative in your favor? Just delete some of the positive posts of your competitors. To cover your tracks, make up claims about a bot army that’s run by your competitor. You’re fighting the good fight and keeping the barbarians at bay! You don’t even need to actively post negative stuff (but you can if you want to dial up the torture). Just removing positive stuff skews the narrative nicely.&lt;/item&gt;
      &lt;item&gt;You can ban users at will. Got some troublesome competitors fighting back? Just delete the little shits.&lt;/item&gt;
      &lt;item&gt;You can pin posts in the subreddit and comments within posts. Great tactic for pushing a narrative when you really need to.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What about recourse from your competitors? Can they do anything to stop you?&lt;/p&gt;
    &lt;p&gt;You can only lose your moderator slot in a few instances:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Reddit Admis get involved, actual Reddit employees. Stuff has to get pretty egregious for that.&lt;/item&gt;
      &lt;item&gt;Moderators further up the mod list can kick you out. But if you get on good terms with them, or they don’t care, you have free reign.&lt;/item&gt;
      &lt;item&gt;If there are other mods below you AND you go inactive, you can get kicked by them too. So don’t let any new mods in. And always stay active. Simple.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;No one else across the industry can do a damn thing. You get to act with impunity.&lt;/p&gt;
    &lt;p&gt;To be clear, any moderator of an industry subreddit has this power. That’s not a subjective opinion, it’s a fact. That’s how our online platforms currently work.&lt;/p&gt;
    &lt;p&gt;Michael Novati has this power over the bootcamp industry. Did he wield it benevolently? If he did, I wouldn’t be writing this fucking post.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Michael Distorts Narratives on Reddit&lt;/head&gt;
    &lt;p&gt;At first, I was going to go through a bunch of Michael Novati’s claims and debunk them.&lt;/p&gt;
    &lt;p&gt;Then I realized I was missing the entire point.&lt;/p&gt;
    &lt;p&gt;This isn’t a normal debate where there are two sides of the truth and we need to sort through it to get to the real answer.&lt;/p&gt;
    &lt;p&gt;It’s about one side torching the truth with a firehose of chaos and distortion.&lt;/p&gt;
    &lt;p&gt;Prospects in the bootcamp space are evaluating whether or not to spend tens of thousands of dollars on a bootcamp. It’s the price of a goddamn car. It doesn’t take much to get someone to pause and redirect their purchase journey.&lt;/p&gt;
    &lt;p&gt;So let’s go through a few of the worst examples.&lt;/p&gt;
    &lt;head rend="h3"&gt;Compare Your Competitor to a Sex Cult (Loosely for Deniability)&lt;/head&gt;
    &lt;p&gt;Here’s a fucked up one.&lt;/p&gt;
    &lt;p&gt;A Reddit user, with a first-time post, posts a major takedown of Codesmith, claiming to be a former employee. Tons of allegations about mismanagement, poor curriculum, and backroom dealing with CIRR. I personally think these allegations are bullshit but that’s not the most fucked up part in this thread.&lt;/p&gt;
    &lt;p&gt;u/rosiebeir joins the conversation and gives a measured take on their experience as a student in Codesmith:&lt;/p&gt;
    &lt;p&gt;Most of the comment is about different aspects of the Codesmith program that could be improved. It all feels genuine to me. Further down, they say this:&lt;/p&gt;
    &lt;p&gt;Wow, that’s a helluva endorsement.&lt;/p&gt;
    &lt;p&gt;This is where Michael jumps in:&lt;/p&gt;
    &lt;p&gt;If you’re not familiar with NXIVM (Michael got the spelling wrong), go watch The Vow on HBO. Short story: NXIVM was a sex cult disguised as a self-improvement group and the founder went to prison.&lt;/p&gt;
    &lt;p&gt;Did Michael just compare Codemith to a goddamn sex cult? Yes he did folks.&lt;/p&gt;
    &lt;p&gt;Let’s put aside the subject matter for a second. Michael’s use of rhetoric is a master class on how to destroy someone’s reputation without technically saying the horrible thing you’re actually saying.&lt;/p&gt;
    &lt;p&gt;Michael takes an incredible testimonial from a student (“Codesmith changed my life”), and associates that comment with the sort of things he hears from sex cults. He doesn’t technically call Codesmith a sex cult, he merely makes the association. He also wraps it with other positive comments to look more measured: “something I hear very often” and “I love that Codesmith changed your life.” But these aren’t good things, they’re red flags! Sex cults do the same thing! Someone could get taken advantage of!&lt;/p&gt;
    &lt;p&gt;Oh so innocent aren’t we Michael?&lt;/p&gt;
    &lt;p&gt;Now any time a bootcamp prospect wanders into this thread:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;They see the genuine, positive review from a former student.&lt;/item&gt;
      &lt;item&gt;A supposed figure of authority (a Reddit moderator), plants a seed questioning that testimonial.&lt;/item&gt;
      &lt;item&gt;The same mod connects Codesmith to a sex cult in a way that also offers complete deniability.&lt;/item&gt;
      &lt;item&gt;To top it all off, most folks won’t realize that the Reddit mod is a cofounder of a competing company.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Bravo Michael, brav-fucking-o.&lt;/p&gt;
    &lt;head rend="h3"&gt;From Reddit Troll to LinkedIn Stalker: The Unhinged Escalation&lt;/head&gt;
    &lt;p&gt;This one might piss me off the most.&lt;/p&gt;
    &lt;p&gt;Michael doesn’t just go after employees, he goes after their kids.&lt;/p&gt;
    &lt;p&gt;This is a Reddit post about a defunct Codesmith program:&lt;/p&gt;
    &lt;p&gt;Somehow, as these threads often do, the conversation turns into an all-out attack on Codesmith. This time, Michael goes after Eric Kirsten, a Senior Advisor at Codesmith. Eric spends a good chunk of his time mentoring Codesmith students and helping prepare them for the job hunt.&lt;/p&gt;
    &lt;p&gt;Here’s Michael’s post:&lt;/p&gt;
    &lt;p&gt;I do not know what the original comment in this thread was (very convenient). But I do have the original comment from Michael before he edited it:&lt;/p&gt;
    &lt;p&gt;I talked with Eric myself, here’s the real story:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Eric’s wife did a one-time contract project with Codesmith for 9 months.&lt;/item&gt;
      &lt;item&gt;His son joined as a Codesmith student and had a great outcome. They paid the full program cost and his son went through the normal application process just like everyone else.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But it doesn’t stop there.&lt;/p&gt;
    &lt;p&gt;Michael also decided to email multiple executives at Codesmith about Eric’s son:&lt;/p&gt;
    &lt;p&gt;I’ve redacted the son’s name and details because WHY THE FUCK WOULD WE BRING SOMEONE’S KID INTO THIS?&lt;/p&gt;
    &lt;p&gt;I don’t think I could possibly swear enough in order to convey how utterly insane all this is.&lt;/p&gt;
    &lt;p&gt;Just imagine this happening at your own company:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A cofounder of your competitor, starts attacking one of your employees on Reddit.&lt;/item&gt;
      &lt;item&gt;Then that cofounder starts LOOKING UP THEIR KIDS ON LINKEDIN&lt;/item&gt;
      &lt;item&gt;They post weird mentions on Reddit about that kid, accusing your company of nepotism.&lt;/item&gt;
      &lt;item&gt;THEN EMAILS you and other executives, accusing the kid of falsifying their LinkedIn, threatening to keep calling it out in public.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;WHAT.&lt;/p&gt;
    &lt;p&gt;WHAT. THE. FUCK.&lt;/p&gt;
    &lt;p&gt;What would you do in this situation?&lt;/p&gt;
    &lt;p&gt;I’m not sure what I would do. But there is no world where this is normal or okay.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Great OSP Conspiracy for Manufactured Outrage&lt;/head&gt;
    &lt;p&gt;Another favorite tactic of Michael is to take something obscure, and allude to some grand conspiracy behind it.&lt;/p&gt;
    &lt;p&gt;Like OSPs.&lt;/p&gt;
    &lt;p&gt;OSP stands for open-source product. It’s a built-in part of the Codesmith program, the capstone project. Here’s how Codesmith explains it:&lt;/p&gt;
    &lt;p&gt;This seems incredibly boring, how the hell could someone possibly turn this into a controversy?&lt;/p&gt;
    &lt;p&gt;If you’re asking that question, you’ve severely underestimated Michael.&lt;/p&gt;
    &lt;p&gt;A since-deleted Reddit account posted this thread (convenient that it’s a deleted user huh?):&lt;/p&gt;
    &lt;p&gt;They claim Codesmith students are using OSPs to inflate their resumes to get jobs faster. And that this is endemic at Codesmith.&lt;/p&gt;
    &lt;p&gt;The comments are a dumpster fire. Michael has posted 11 of the comments, almost a third of the entire conversation.&lt;/p&gt;
    &lt;p&gt;A former Codesmith student jumps in:&lt;/p&gt;
    &lt;p&gt;There’s some back an forth between u/peppimenti and other Reddit users, then Michael responds:&lt;/p&gt;
    &lt;p&gt;Fuck me, that’s a lot to take in.&lt;/p&gt;
    &lt;p&gt;Quick tangent: this is another one of Michael’s tactics. Make accusations faster than anyone can possibly fact check them or even process them. I still haven’t figured out what that Phil Troutman reference is.&lt;/p&gt;
    &lt;p&gt;To summarize, Michael claims he has evidence that Codesmith is aware of the lying on resumes, alludes to a conspiracy with OSLabs, and he believes Codesmith helped verify some of this lying.&lt;/p&gt;
    &lt;p&gt;Then he associates all of this with “conspiring to commit fraud” and that it’s a “jail-able crime.”&lt;/p&gt;
    &lt;p&gt;We’ve gone from “Codesmith students complete an OSP during their program” to a shadowy conspiracy and fraud via student resumes. God damn did that escalate.&lt;/p&gt;
    &lt;p&gt;So I got a hold of the exact SOP that is given by Codesmith to students on how to write their resume. Here’s the section on OSPs:&lt;/p&gt;
    &lt;p&gt;Codesmith is telling students explicitly not to misrepresent their experience with OSPs as a role at a company. They advocate for transparency, as they should. What students do with that is up to them. And even if some students stretch the truth, no hiring manager is getting fooled by that. End of story.&lt;/p&gt;
    &lt;p&gt;What about the OSLabs stuff?&lt;/p&gt;
    &lt;p&gt;When I chatted with Alina (the current CEO of Codesmith), here’s what she had to say on how Codesmith students interact with OSLabs:&lt;/p&gt;
    &lt;p&gt;“They [students] get a chance to work on it [projects]. Once those are ready, they submit to OSLabs who keeps the repository and manages the repository. There’s no kind of financial exchange or anything like that. They just hold the repository of the open source dev tools.“&lt;/p&gt;
    &lt;p&gt;OSLabs is just a nonprofit that manages student repos. That’s it.&lt;/p&gt;
    &lt;p&gt;Maybe you think there are problems with Codesmith’s guidance. Real quick, why don’t we check some of Michael’s own students from his bootcamp company, Formation?&lt;/p&gt;
    &lt;p&gt;There’s tons of students on his site. Some of them get featured heavily, like here:&lt;/p&gt;
    &lt;p&gt;Here’s Carlitos’ LinkedIn:&lt;/p&gt;
    &lt;p&gt;Carlitos is listing his time at Michael’s bootcamp/training company in his experience section. The exact same thing that Michael RAILS on Codesmith for.&lt;/p&gt;
    &lt;p&gt;To be clear: I don’t think there’s anything wrong with this. Carlitos, all the Codesmith students, or ANY student from ANY bootcamp have done nothing wrong by putting stuff like this on their resumes. If Formation students list their time like this, I don’t see anything wrong with that either.&lt;/p&gt;
    &lt;p&gt;In the thousands of resumes I’ve reviewed, I’ve never had any trouble differentiating between full-time roles, part-time, freelance, training, or any other random item on a resume. Any hiring manager will sniff this stuff from a mile away. No one is fooling anyone.&lt;/p&gt;
    &lt;p&gt;But Michael has now turned this into a story of fraud facilitated by Codesmith. And here I am writing hundreds of words about this nonsense. There’s nothing here beyond what Michael has conjured into the ether of Reddit.&lt;/p&gt;
    &lt;p&gt;You might disagree with some of the guidance that Codesmith gives its graduates but no one is doing anything wrong. It sure as fuck aint fraud.&lt;/p&gt;
    &lt;p&gt;And that’s what Michael is so damn good at, turning innocuous little things into grand conspiracies that take hours of research to untangle.&lt;/p&gt;
    &lt;head rend="h2"&gt;Michael Novati’s Nonstop Attack Campaign Against Codesmith&lt;/head&gt;
    &lt;p&gt;Think I’m cherry picking examples of Michael’s posts?&lt;/p&gt;
    &lt;p&gt;Let’s zoom out.&lt;/p&gt;
    &lt;p&gt;My team and I looked at every Reddit post and comment made by Michael Novati over the past 3 months.&lt;/p&gt;
    &lt;p&gt;We then catalogued the posts that mentioned Codesmith or were referencing Codesmith based on context. Next we categorized those posts based on if the sentiment was positive, neutral, or negative towards Codesmith.&lt;/p&gt;
    &lt;p&gt;Here’s all the days that had negative comments along with their frequency over the past few months:&lt;/p&gt;
    &lt;p&gt;Fucking relentless.&lt;/p&gt;
    &lt;p&gt;It’s easier to highlight days where Michael DOESN’T talk about Codesmith. Dude, get a fucking hobby.&lt;/p&gt;
    &lt;p&gt;What if the past few months don’t adequately reflect Michael’s posting history? What if recent events have just been a negative phase of the bootcamp space? Can we really make any claims using just the last few months?&lt;/p&gt;
    &lt;p&gt;Fine, let’s go further.&lt;/p&gt;
    &lt;p&gt;I went to my team and said “fuck it, let’s go back as far as possible.” So we went through all of Michael’s posts since June 2024. That’s when Reddit stops making it easy to pull up comments. Yes, we looked at every post and comment from over a year. No fast and dirty AI analysis here, we did it by hand.&lt;/p&gt;
    &lt;p&gt;Here’s the timeline of negative posts and comments since June 2024:&lt;/p&gt;
    &lt;p&gt;What. The. Fuck.&lt;/p&gt;
    &lt;p&gt;Who has the time for this?&lt;/p&gt;
    &lt;p&gt;In order to adequately show how often Michael posts negative comments about Codesmith, we had to use HABIT TRACKER VISUALIZATIONS.&lt;/p&gt;
    &lt;p&gt;It’s relentless, it’s nonstop, and in my opinion, completely unhinged. At least Michael slows down a tad during the holidays. Otherwise, he’s all gas, no breaks.&lt;/p&gt;
    &lt;p&gt;Well, alright, MAYBE Michael was posting in such absurd volume that the positive mentions outweigh the negative? Maybe I’m selectively pulling data?&lt;/p&gt;
    &lt;p&gt;NOPE.&lt;/p&gt;
    &lt;p&gt;The comments about Codesmith are severely overweighted on the negative side. In aggregate, here’s how the sentiment breaks down:&lt;/p&gt;
    &lt;p&gt;Is it just me or is there a weeeee bit of bias in this posting?&lt;/p&gt;
    &lt;p&gt;Yes, yes, there fucking is.&lt;/p&gt;
    &lt;p&gt;Alright, fine, MAYBE all this negative attention is deserved. Maybe Codesmith is the villain that Michael claims. Let’s find out.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other Bootcamp Competitors Hold Codesmith in High Regard&lt;/head&gt;
    &lt;p&gt;I interviewed 10 different folks. Including students, multiple Codesmith employees from the leadership team, and founders of competing coding bootcamps. I dug through countless docs, many of them internal, and probably annoyed some of these folks with my endless requests for screenshots to verify stories.&lt;/p&gt;
    &lt;p&gt;Overall, I found everyone at Codesmith to be:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Extremely responsive and on top of their shit. They made promises and kept them.&lt;/item&gt;
      &lt;item&gt;Transparent to an absurd degree.&lt;/item&gt;
      &lt;item&gt;Humble, honest, and self-aware.&lt;/item&gt;
      &lt;item&gt;A desire to do the right thing even at their detriment. These folks don’t know how to play offense, they assume good intent from everyone.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In short, these are good folks. Faultless? Of course not, everyone makes mistakes.&lt;/p&gt;
    &lt;p&gt;But I do believe that the world is better with Codesmith in it.&lt;/p&gt;
    &lt;p&gt;Which makes this whole situation all the more tragic.&lt;/p&gt;
    &lt;p&gt;Michael isn’t waging some war against another competitor hellbent on profit-seeking. These are folks that are helping people transition their careers. They are quite literally helping people change their lives. And they’re doing it for the right reasons in my opinion.&lt;/p&gt;
    &lt;p&gt;Don’t take my word for it.&lt;/p&gt;
    &lt;p&gt;I spoke with Anthony Hughes, a co-founder of Tech Elevator, another bootcamp. Here’s what he had to say about Codesmith: “The folks at Codesmith were competitors of Tech Elevator. Especially as everyone went online, we competed directly. I never minded losing to Codesmith. Either choice by students was a good one, both our programs vetted students heavily and would get students great outcomes. If we ever lost to Codesmith, we could accept that because we knew the student would get the outcome they wanted. Not like the lower-quality major programs.”&lt;/p&gt;
    &lt;p&gt;I also heard the same from Kush Patel, a co-founder and former CEO of AppAcademy “Codesmith is one of the best players in the space. They run their programs in a way that we benchmarked against. There are plenty of other schools that are questionable and are not getting any scrutiny from Michael.”&lt;/p&gt;
    &lt;p&gt;We have two co-founders of competing bootcamps saying that Codesmith is one of the good ones. Are there shady bootcamps out there? Absolutely. But Codesmith isn’t one of them.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Tactics Michael Uses to Wage His PR Nightmare Campaign&lt;/head&gt;
    &lt;p&gt;After reading hundreds of Michael’s posts, a few patterns emerge on how he attacks Codesmith.&lt;/p&gt;
    &lt;p&gt;To be honest, it’s a masterclass on how to gut your opponent via PR. If every move is intentional by Michael, I gotta hand it to him becasue he’s exceptionally good at this.&lt;/p&gt;
    &lt;p&gt;Here are the most prominent tactics I saw:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Relentless volume. It’s not enough to post a few days. If you want to destroy someone’s reputation, you gotta hammer the shit outta that drum. Over a 487 day period, we counted 425 negative posts on Codesmith. That’s an average of 0.87 negative posts per day. A negative post almost every fucking day.&lt;/item&gt;
      &lt;item&gt;Reference multiple conspiracies at the same time. If you only mention one conspiracy, folks have a straightforward path to getting to the truth. But if you mention 3-4 at the same time? Only the true masochistics like myself will even attempt to pull it apart.&lt;/item&gt;
      &lt;item&gt;Rarely, if ever, state your full accusation. For any single conspiracy, always allude to it. Don’t make a full, comprehensive case that allows someone to see the full picture. Let your audience sit with the uncertainty. You don’t need to win the argument, you only need to sow doubt and fear.&lt;/item&gt;
      &lt;item&gt;Cherry-pick data. As the saying goes: “there are lies, damned lies, and statistics.” For any industry data you can get your hands on, find the slice that skews the narrative in your favor. Don’t worry about anyone fact-checking, they never do.&lt;/item&gt;
      &lt;item&gt;Attack every misstep, no matter how obscure. Sooner or later, your victim will make mistakes. When they do, attack. No matter how small or obscure, blow it up into a major drama. Hype, distort, and magnify.&lt;/item&gt;
      &lt;item&gt;Take down third-party arbitrators. If your industry has some sort of credential, certification, or regulator, attack that body just as much as your competitor. If your competitor has optimized for this transparency, you’ll turn their strength into a weakness. Even better, accuse both parties of conspiring together.&lt;/item&gt;
      &lt;item&gt;Blame the victim. Whenever someone mentions that you’re going too hard, say that your competitor is forcing your hand. “You’d love to move on! You don’t want to spend your time on all this! If ONLY your competitor would do the right thing, then all this would stop!” But we all know it’ll never stop.&lt;/item&gt;
      &lt;item&gt;Delete comments and posts. Delete comments and posts to skew the narrative in your favor. You don’t need to delete everything, just enough to make the conversation messy and difficult to follow. And a few key deletions tilts the scales in your favor.&lt;/item&gt;
      &lt;item&gt;Make your claims squishy. Don’t ever say your competitor is a cult or committing fraud. Say that things look like it, or remind you of it. You’ll have deniability while the association still gets planted in the minds of your audience.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A sign of the times.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Impact on Codesmith&lt;/head&gt;
    &lt;p&gt;Has all this had a material impact on Codesmith?&lt;/p&gt;
    &lt;p&gt;I spoke to both Will Sentence (former CEO and co-founder) and Alina Vasile (the current CEO). When I asked them how much revenue has declined at Codesmith since their peak, they both told me about 80%.&lt;/p&gt;
    &lt;p&gt;Then I asked how much of that was due to everything on Reddit. They said about half of the decline, so a 40% drop came from the negative PR on Reddit. And the other 40% is from a slowdown in the bootcamp market.&lt;/p&gt;
    &lt;p&gt;Will told me that Codesmith reached a high of $23.5M in revenue. A 40% hit from Reddit means a revenue drop of $9.4M.&lt;/p&gt;
    &lt;p&gt;What about the emotional toll?&lt;/p&gt;
    &lt;p&gt;I asked Will Sentance how it all impacted him “It made me doubt how I can start anything after. Michael Novati is still going at it by commenting on my fellowship at Oxford. And Reddit is so visible in Google – at a meeting last week in the UK, the person used ChatGPT to look up Codesmith and it referenced all those Reddit posts. I have genuine caution about launching new stuff.”&lt;/p&gt;
    &lt;p&gt;Even after leaving Codesmith, the Reddit harassment still impacts Will.&lt;/p&gt;
    &lt;p&gt;Students also feel deeply uncomfortable with Michael’s actions.&lt;/p&gt;
    &lt;p&gt;I spoke with a former employee from Codesmith who told me “I was a teacher at Codesmith and the primary reason for moving away from teaching was MIchael Novati. I signed up to help a team of great people help people achieve their goals like I did, I didn’t sign up to be targeted and attacked by a Reddit troll. It took a toll on my mental health and I decided to step back to focus on my day job at Microsoft.”&lt;/p&gt;
    &lt;p&gt;Here’s another instance of a student calling Michael out on LinkedIn:&lt;/p&gt;
    &lt;p&gt;And another student that reached out to Will Sentence:&lt;/p&gt;
    &lt;p&gt;My sources also told me:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;An executive was threatened after moving to another company, accusations were going to be raised over IP conflicts. These accusations appear meritless and ridiculous to me.&lt;/item&gt;
      &lt;item&gt;The morale at Codesmith has sharply declined.&lt;/item&gt;
      &lt;item&gt;Some employees have had mental health difficulties, some left Codesmith.&lt;/item&gt;
      &lt;item&gt;One contractor ended their working relationship with Codesmith because of a fear of being dragged into all this.&lt;/item&gt;
      &lt;item&gt;An employee was doxxed after posting in the /r/codingbootcamp subreddit.&lt;/item&gt;
      &lt;item&gt;Prospective students have pulled applications.&lt;/item&gt;
      &lt;item&gt;Students that had amazing outcomes are afraid to post about their experiences.&lt;/item&gt;
      &lt;item&gt;After graduating, some students began questioning their own experience and considered going back to their old careers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As much as I wanted to detail these stories, I had a lot of requests from sources to remain anonymous. Even when folks went through some heinous shit from all this, they specifically requested not to be included.&lt;/p&gt;
    &lt;p&gt;It is difficult to understate the impact that Michael’s actions have had on Codesmith.&lt;/p&gt;
    &lt;head rend="h2"&gt;Is Michael Novati Breaking the Moderator Code of Conduct on Reddit?&lt;/head&gt;
    &lt;p&gt;I think so. But it’s not my call.&lt;/p&gt;
    &lt;p&gt;The Reddit Moderator Code of Conduct says this:&lt;/p&gt;
    &lt;p&gt;So what counts as compensation?&lt;/p&gt;
    &lt;p&gt;Basically, Reddit Moderators can’t receive compensation for doing basic moderator tasks. Good rule, the point is to prevent companies and third-parties from paying Reddit mods off.&lt;/p&gt;
    &lt;p&gt;Now here’s the tricky part.&lt;/p&gt;
    &lt;p&gt;What if that Reddit Moderator is a co-founder of a competitor? What if they spend YEARS shit talking you and destroying your reputation? What if they’re not technically receiving new stock but the value in their startup equity goes up after they destroy you on Reddit? Doesn’t that count as financial compensation and a violation of Reddit’s own rule to “Moderate with Integrity”?&lt;/p&gt;
    &lt;p&gt;I believe it does.&lt;/p&gt;
    &lt;p&gt;And I believe that’s why Michael is doing what he’s doing. He wins when Codesmith loses.&lt;/p&gt;
    &lt;p&gt;Shit like this is why Reddit has become extremely untrustworthy.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Drives the Crazy Student Outcomes at Codesmith&lt;/head&gt;
    &lt;p&gt;If you dig around, you will find stories from Codesmith students and placement data that seem too good to be true. Is Codesmith actually that good?&lt;/p&gt;
    &lt;p&gt;As far as I can tell, it’s all real.&lt;/p&gt;
    &lt;p&gt;So how has Codesmith managed it? I found two forces that have driven these outcomes.&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Codesmith Has an Intense Application Process&lt;/head&gt;
    &lt;p&gt;Not just anyone can join Codesmith. They screen folks OUT of their program. If you’re a line cook, kinda think a software engineer job sounds fun, have never spent even 5 minutes figuring out what that looks like, and apply, you won’t get in.&lt;/p&gt;
    &lt;p&gt;Codesmith only accepts folks that they think have a good chance of succeeding.&lt;/p&gt;
    &lt;p&gt;They filter the front of their student pipeline which keeps the outcome rates really high on the backend. That’s how they get their placement numbers as high as they are. That’s the “trick” to juicing the student outcomes.&lt;/p&gt;
    &lt;p&gt;Now, if you have some software engineering experience, you’ll likely find the application to be absurdly easy. That’s the point.&lt;/p&gt;
    &lt;p&gt;To me, Codesmith isn’t a true zero to one program. The students that thrive have already put a lot of self-study hours into their career transition. They’re motivated, have learned the basics, and are ready to dial up the intensity. These are folks that are also likely to succeed during the job application process after they graduate.&lt;/p&gt;
    &lt;p&gt;This is also one of the easiest ways to spot the shitty bootcamps. If they take anyone and everyone, that’s an enormous red flag. Stay away.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. The Hiring Market Was Extremely Strong from 2019-2022&lt;/head&gt;
    &lt;p&gt;I spoke to one student that went through Codesmith in 2020, they reported that every single one of the folks in their cohort landed a software engineering job within about 6 months. That’s nuts.&lt;/p&gt;
    &lt;p&gt;Another graduate from the 2019 era told me that 80-90% of the students from their cohort, the cohort before, and the cohort after, landed software engineering jobs within a few months of graduating.&lt;/p&gt;
    &lt;p&gt;How were the placements that high? I believe the Codesmith program had something to do with it. But that was also a period where the tech market was exceptionally hot. Across all levels and functions, it was much easier to switch jobs, get promotions, and change careers in tech. I saw that myself.&lt;/p&gt;
    &lt;p&gt;I’ve also personally seen things tighten a lot in the past few years.&lt;/p&gt;
    &lt;p&gt;How do Codesmiths placement rates look now?&lt;/p&gt;
    &lt;p&gt;Even if we look at more recent cohorts, the placements are still pretty damn strong for the full-time program:&lt;/p&gt;
    &lt;p&gt;That’s one of the reports issued by CIRR on Codesmith. It’s the full report so I’m not cherry picking.&lt;/p&gt;
    &lt;p&gt;Even during 2023-2024, a pretty shitty period to try to start a career as a software engineer, 70% of folks are still landing in-field jobs within a year of graduating Codesmith. With a median salary of $110,000. Even if you only count full-time employment, it’s still 62%. I personally think that’s remarkable.&lt;/p&gt;
    &lt;p&gt;The part-time program is only slightly worse at 60%.&lt;/p&gt;
    &lt;p&gt;Again, this is during a hiring market when entry-level software engineering jobs are brutally hard to get. And the placement rates are still in the 60-70% range.&lt;/p&gt;
    &lt;p&gt;I’m also hearing that students have to work a lot harder now to land that job. And it can take a solid year. That’s that state of tech hiring at the moment.&lt;/p&gt;
    &lt;p&gt;So if you see old placement data of 80-100%, that was when the market was hot. Now it’s come back down to earth which is to be expected.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Future of Codesmith&lt;/head&gt;
    &lt;p&gt;To me the future of Codesmith looks quite bright. Alina (the CEO) is very much in control, the team is excited about the future, and the student outcomes still look impressive to me. I’ve been told that there are no plans at Codesmith to shut down.&lt;/p&gt;
    &lt;p&gt;Especially with all the changes to software development from AI, there’s a lot of training that everyone is going to have to go through in tech. Good time to be in the education space.&lt;/p&gt;
    &lt;p&gt;If Michael could take his boot off Codesmith’s throat for just half a minute, they’d do a lot of good for a lot people.&lt;/p&gt;
    &lt;head rend="h2"&gt;My Recommendations for Anyone Considering Joining a Developer Bootcamp&lt;/head&gt;
    &lt;p&gt;I cannot tell you which bootcamp to attend. Or whether a bootcamp is worth it at all.&lt;/p&gt;
    &lt;p&gt;I’ve hired well over a hundred folks in my career, interviewed countless others. Including engineers and data scientists.&lt;/p&gt;
    &lt;p&gt;At no point did I ever place any value in a bootcamp. I also never considered it a detractor. It’s neutral to me, never really signaling anything. Neither good nor bad.&lt;/p&gt;
    &lt;p&gt;If you think it’ll get you your first job (which is all that really matters), and the price/time makes sense for you, go for it. But there are plenty of other paths. It’s not required by any means. I know plenty of self-taught devs that have worked at prestigious startups and big tech.&lt;/p&gt;
    &lt;p&gt;As for which bootcamp, that’s also up to you.&lt;/p&gt;
    &lt;p&gt;What I can say is this.&lt;/p&gt;
    &lt;p&gt;As I’ve gone through Michael Novati’s Reddit account, I’ve been appalled. I’ve never seen anything like this in my entire career. And I’ve played in some fucked up corners of the internet. That last thing I’d EVER want is to have my name or career associated with anything that Michael Novati has touched. That includes his startup, Formation.&lt;/p&gt;
    &lt;p&gt;You’ll have to make your own choice. But I know what mine would be.&lt;/p&gt;
    &lt;p&gt;At the very least, when doing research on bootcamps, I would consider anything on r/codingbootcamp/ to be completely contaminated. Whatever the real truth is, that subreddit is Michael’s personal fiefdom. He can do and say whatever he wants. And he has one helluva conflict of interest: he’s a co-founder and CTO of a leading bootcamp (even if he tries to claim that he doesn’t run a bootcamp).&lt;/p&gt;
    &lt;p&gt;If I was looking for unbiased reviews, I’d only factor in content from a community that Michael (or any other bootcamp founder) did not control. Make sure you know who your subreddit Mods are.&lt;/p&gt;
    &lt;head rend="h2"&gt;Every Company is Now Vulnerable to These Types of Reputation Attacks&lt;/head&gt;
    &lt;p&gt;For me, the craziest part of this story is that it’s not an edge case.&lt;/p&gt;
    &lt;p&gt;Anyone can use this exact same method to tear down their competitors.&lt;/p&gt;
    &lt;p&gt;You only need to complete one step: become a Reddit mod in one of the main subreddits for your industry. If you do that and the original mods of the subreddit don’t care what you do, you can completely fuck up your whole industry. Sow lies, fear, and misery at will.&lt;/p&gt;
    &lt;p&gt;Which means anyone can do this to you.&lt;/p&gt;
    &lt;p&gt;Reddit has become THE reputation attack vector.&lt;/p&gt;
    &lt;p&gt;And since Reddit is considered the default source of all human knowledge now, Reddit threads pop to the top of Google. Anyone searching for your brand will stumble into this madness. LLMs also prioritize Reddit so that narrative gets corrupted too. Getting control of one channel allows you to corrupt three major channels all at once. Most people don’t check receipts, they don’t dig for truth, they accept what’s put in front of them.&lt;/p&gt;
    &lt;p&gt;Reddit mods now control our search information ecosystem. Unpaid, corruptible Reddit mods.&lt;/p&gt;
    &lt;p&gt;I know there are good Reddit mods out there. But all it takes is one to fuck up your own business.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45521920</guid><pubDate>Wed, 08 Oct 2025 23:48:28 +0000</pubDate></item><item><title>Designing a Low Latency 10G Ethernet Core (2023)</title><link>https://ttchisholm.github.io/ethernet/2023/05/01/designing-10g-eth-1.html</link><description>&lt;doc fingerprint="ed38fb1129f6f98f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Designing a Low Latency 10G Ethernet Core - Part 1 (Introduction)&lt;/head&gt;
    &lt;p&gt;Links to the other parts in this series:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Introduction (this post)&lt;/item&gt;
      &lt;item&gt;Design Overview and Verification&lt;/item&gt;
      &lt;item&gt;Low Latency Techniques&lt;/item&gt;
      &lt;item&gt;Performance Measurement and Comparison&lt;/item&gt;
      &lt;item&gt;Potential Improvements&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;This is the first in a series of blog posts describing my experience developing a low latency 10G Ethernet core for FPGA. I decided to do this as a personal project to develop expertise in low latency FPGA design and high-speed Ethernet, as well as to experiment with tools and techniques that I could use full-time. As a small spoiler, the design has less than 60ns loopback latency, which is comparable to commercial offerings.&lt;/p&gt;
    &lt;p&gt;These posts will focus on the things that are likely different to a ‘standard’ design, as I believe this will be more interesting to the reader. Specifically:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The use of cocotb and pyuvm for verification&lt;/item&gt;
      &lt;item&gt;The techniques implemented to reduce packet processing latency&lt;/item&gt;
      &lt;item&gt;Analysis of commercially available low latency and ‘ultra’-low latency cores&lt;/item&gt;
      &lt;item&gt;Latency measurement results and comparison&lt;/item&gt;
      &lt;item&gt;Other techniques not implemented&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If the reader is unfamiliar with Layer 1/2 Ethernet, I would recommend the following resources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;10G Ethernet Layer 1 Overview&lt;/item&gt;
      &lt;item&gt;YouTube - The Big MAC Mystery&lt;/item&gt;
      &lt;item&gt;IEEE Standard for Ethernet - Full Ethernet (802.3) spec&lt;/item&gt;
      &lt;item&gt;64B/66B overview - Overview of 10G PCS from the spec above&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Next Post - Design Overview and Verification&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45522406</guid><pubDate>Thu, 09 Oct 2025 01:17:20 +0000</pubDate></item><item><title>Two things LLM coding agents are still bad at</title><link>https://kix.dev/two-things-llm-coding-agents-are-still-bad-at/</link><description>&lt;doc fingerprint="be40934db2d75e59"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Two things LLM coding agents are still bad at&lt;/head&gt;
    &lt;p&gt;I’ve been trying to slowly ease into using LLMs for coding help again lately (after quitting cold turkey), but something always feels off -- like we’re not quite on the same wavelength. Call it vibe coding or vibe engineering, but I think I’ve finally pinned down two big reasons why their approach to code feels so awkward.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;LLMs don’t copy-paste (or cut and paste) code. For instance, when you ask them to refactor a big file into smaller ones, they’ll "remember" a block or slice of code, use a &lt;code&gt;delete&lt;/code&gt;tool on the old file, and then a&lt;code&gt;write&lt;/code&gt;tool to spit out the extracted code from memory. There are no real&lt;code&gt;cut&lt;/code&gt;or&lt;code&gt;paste&lt;/code&gt;tools. Every tweak is just them emitting&lt;code&gt;write&lt;/code&gt;commands from memory. This feels weird because, as humans, we lean on copy-paste all the time. It’s how we know the code we moved is exactly the same as where we copied it from. I've only seen Codex go against the grain here, sometimes I'd see it issue&lt;code&gt;sed&lt;/code&gt;and&lt;code&gt;awk&lt;/code&gt;to try and replicate that copy-paste interaction, but it doesn't always work.&lt;/item&gt;
      &lt;item&gt;And it’s not just how they handle code movement -- their whole approach to problem-solving feels alien too. LLMs are terrible at asking questions. They just make a bunch of assumptions and brute-force something based on those guesses. Good human developers always pause to ask before making big changes or when they’re unsure (hence the mantra of "there are no bad questions"). But LLMs? They keep trying to make it work until they hit a wall -- and then they just keep banging their head against it. Sure, you can overengineer your prompt to try get them to ask more questions (Roo for example, does a decent job at this) -- but it's very likely they still won't. Maybe the companies building these LLMs do their RL based on making writing code "faster".&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These quirks are why I contest the idea that LLMs are replacing human devs -- they’re still more like weird, overconfident interns. I can’t fully vibe with them yet.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45523537</guid><pubDate>Thu, 09 Oct 2025 04:33:48 +0000</pubDate></item><item><title>The Unknotting Number Is Not Additive</title><link>https://divisbyzero.com/2025/10/08/the-unknotting-number-is-not-additive/</link><description>&lt;doc fingerprint="2c790b511bb3d39a"&gt;
  &lt;main&gt;
    &lt;p&gt;On June 30, 2025, Mark Brittenham and Susan Hermiller uploaded a preprint to the arXiv called “Unknotting number is not additive under connected sum” (and an updated version on September 15, 2025). In it, they surprised the mathematical community by giving a counterexample to a long-standing conjecture in knot theory. The story was picked up by publications like Scientific American and Quanta and by math YouTuber Matt Parker.&lt;/p&gt;
    &lt;p&gt;The conjecture is easy to understand, although we need a few definitions first.&lt;/p&gt;
    &lt;p&gt;(Mathematical) knot: We can think of a mathematical knot as a loop of string sitting in three-dimensional space. In other words, if we took a piece of string, tied a knot, and then glued the two ends together, we’d get a mathematical knot.&lt;/p&gt;
    &lt;p&gt;Knot projection: Given any mathematical knot, we draw a two-dimensional version of it in the plane. It is like the shadow of the knot, but with breaks in the knot to indicate which strand is on top and which is on the bottom.&lt;/p&gt;
    &lt;p&gt;Unknotting number: If we have the projection of a knot, we can change some crossings (change which strand is over and which is under) to make it unknotted (called the unknot). To compute the unknotting number of a knot K, u(K), we look at all possible projections and find the fewest number of crossing changes we must make to obtain the unknot.&lt;/p&gt;
    &lt;p&gt;Connected sum: Given two knots, J and K, we can cut each knot at one point and join the cut ends to form a new, larger knot, J#K. This is called the connected sum of the knots.&lt;/p&gt;
    &lt;p&gt;Below, we see a knot called the (2,7) torus knot and its mirror image (left), and their connected sum on the right.&lt;/p&gt;
    &lt;p&gt;Although unknotting numbers are notoriously difficult to compute, we know that the unknotting number of a (p,q) torus knot is (p-1)(q-1)/2. So, the (2,7) torus knots above have unknotting number (2 – 1)(7 – 1)/2 = 3. It is not difficult to check that by changing three crossings of the projections shown above, the (2,7) torus knot becomes the unknot. It turns out that changing two crossings does not suffice in this projection or any projection.&lt;/p&gt;
    &lt;p&gt;Likewise, changing 3 + 3 = 6 crossings of the connected sum will yield the unknot. In fact, it will always be the case that u(J#K) ≤ u(J) + u(K). The question is: are these equal? An “old” conjecture (which was implicit in an article 88 years ago), states:&lt;/p&gt;
    &lt;p&gt;Conjecture: If J and K are knots, then u(J#K) = u(J) + u(K).&lt;/p&gt;
    &lt;p&gt;In their preprint, Brittenham and Hermiller disprove the conjecture by giving a counterexample! Moreover, the counterexample is precisely the one I’ve shown above! The connected sum of the torus knot with its mirror image has unknotting number 5, which is clearly less than 3 + 3.&lt;/p&gt;
    &lt;p&gt;That said, we can’t simply change the five crossings in the above projection to obtain the unknot. We must produce a different projection first. But what is it?&lt;/p&gt;
    &lt;p&gt;I looked for the answer online and found this arXiv preprint by Chao Wang and Yimu Zhang, which gives the details. They provide the projection and the crossings that must be changed. However, the projection has 56 crossings (far more than the original 14!). They assert, but do not show, that the resulting knot—after the five crossings are changed—is the unknot. They end by writing, “We prefer to leave it to the readers as an interesting game.”&lt;/p&gt;
    &lt;p&gt;Never good at resisting a good nerd sniping, I decided to take them up on the challenge. I’m embarrassed to admit how long it took me to confirm their work, but I did it. Here is my redrawn version of the knot projection—the connected sum of the (2,7) torus knot and its mirror image. The circled crossings are the five that must be changed.&lt;/p&gt;
    &lt;p&gt;After having done so, here’s the resulting projection. Wang and Zhang claim that it is the unknot!&lt;/p&gt;
    &lt;p&gt;Without further ado, here are my drawings. This first sequence shows how I get from the usual projection of the connected sum to the projection in which the crossings must be made. The red strands show the part of the projection that has changed from the previous projection.&lt;/p&gt;
    &lt;p&gt;Thus, the final image above is the knot we claim is the unknot. The following sequence of steps shows that it is indeed the unknot.&lt;/p&gt;
    &lt;p&gt;Ta da!!!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45524293</guid><pubDate>Thu, 09 Oct 2025 06:39:30 +0000</pubDate></item><item><title>The Forecasting Company (YC S24) Is Hiring a Machine Learning Engineer</title><link>https://www.ycombinator.com/companies/the-forecasting-company/jobs/cXJzAhA-founding-machine-learning-engineer</link><description>&lt;doc fingerprint="cf94bfdaef1ccb2f"&gt;
  &lt;main&gt;
    &lt;p&gt;Foundation models for time series&lt;/p&gt;
    &lt;p&gt;We are on a mission to create the forecasting foundation model to rule them all. Forecasting drives critical decisions worldwide - impacting staffing, supply chain management, finance and more. Our solution provides companies with the models, platform and APIs they need to easily generate the most accurate forecasts possible, helping to significantly reduce waste and enabling smarter, more confident decisions.&lt;/p&gt;
    &lt;p&gt;The forecasting model is at the heart of our technology. As the second founding MLE, you will build, train and deploy large foundation model architectures: implement and combine ideas from the literature, push the state of the art, and ultimately deploy your model for our customers to use in production. Our goal is for our models to be the best for our customers’ use cases - including for capabilities that do not exist yet in academic models.&lt;/p&gt;
    &lt;p&gt;You love your craft, have high standards, stay up-to-date with the latest ideas in ML, and know when to make trade-offs to ship. You live and breathe neural networks, and speak PyTorch or Jax. You are used to diving deep in large amounts of data, and you know what you train your models on. Bonus if you have experience building solid ML infrastructure.&lt;/p&gt;
    &lt;p&gt;You are passionate about your craft, maintain high standards, stay current with the latest tech and know when to make trade-offs to deliver results efficiently. We do not believe great engineers are “jack of all trades”, but rather that they excel at diving deep into complex topics quickly, leveraging a broad range of experiences to solve challenging problems. You are also open to exploring new concepts, technologies, and enjoy quickly throwing prototypes together to kick the tires. You prefer quick feedback loops, rather than aiming for perfection on the first try.&lt;/p&gt;
    &lt;p&gt;Our goal is to provide the most accurate and easy-to-use forecasts to our customers, by leveraging refined information from their own industry. Foundation models for time series are changing this entirely. With the current advances in data processing and model training, we can now pre-train models on diverse temporal data across industries. We provide value to our customers by enabling rapid interaction with our models when provided data and context in natural language, delivering real-time forecasts with accuracy reports. Our customers do not need to be data scientists or have a PhD in Machine Learning to build and ship an accurate forecasting system for their use-cases.&lt;/p&gt;
    &lt;p&gt;Example use cases include demand forecasting for large furniture chains, predicting sales for a restaurant group and revenue forecasting in the gaming industry.&lt;/p&gt;
    &lt;p&gt;The founders Geoff and Joachim are both Machine Learning PhDs who have built forecasting and ML systems from scratch at JP Morgan, Amazon, Google, Bloomberg, and Sonos in the US.&lt;/p&gt;
    &lt;p&gt;We are a global company that happens to be HQed in Paris. Get the best of both worlds — Silicon Valley work ethic and ambition in the center of Paris, right across from the historical Stock Exchange, in the Sentier.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45524437</guid><pubDate>Thu, 09 Oct 2025 07:01:07 +0000</pubDate></item><item><title>The React Foundation</title><link>https://engineering.fb.com/2025/10/07/open-source/introducing-the-react-foundation-the-new-home-for-react-react-native/</link><description>&lt;doc fingerprint="ed2bf7695563fd5c"&gt;
  &lt;main&gt;
    &lt;p&gt;Meta open-sourced React over a decade ago to help developers build better user experiences. Since then, React has grown into one of the world’s most popular open source projects, powering over 50 million websites and products built by companies such as Microsoft, Shopify, Bloomberg, Discord, Coinbase, the NFL, and many others. With React Native, React has expanded to support platforms beyond the web, including mobile, tablets, desktops, TVs, gaming consoles, and even mixed reality devices.&lt;/p&gt;
    &lt;p&gt;This incredible growth is thanks to the thousands of educators, companies, and projects that have contributed to the development of React. The community is the heart of React, and we’re proud to play a part in the cycle of open source innovation throughout the ecosystem that benefits everyone. We’re pleased to give a seat at the table to the people and companies that have made React what it is today.&lt;/p&gt;
    &lt;p&gt;Today, we are excited to announce the next step for React. Several projects within the React ecosystem, including React and React Native, as well as supporting projects such as JSX, will transition to the React Foundation. The React Foundation’s mission is to help the React community and its members. The React Foundation will maintain React’s infrastructure, organize React Conf, and create initiatives to support the React ecosystem. The React Foundation will be part of the Linux Foundation, which has long fostered a vendor-neutral environment for open source projects.&lt;/p&gt;
    &lt;head rend="h2"&gt;Formalizing Governance&lt;/head&gt;
    &lt;p&gt;The React Foundation’s governing board will consist of representatives from Amazon, Callstack, Expo, Meta, Microsoft, Software Mansion, and Vercel, with the intention to expand further over time.&lt;/p&gt;
    &lt;p&gt;There will be a clear separation between the business and technical governance of React. Releases, features, and technical direction will be governed by a new structure driven by the maintainers and contributors of React. This new technical governance structure will be independent of the React Foundation. The React team is actively working on this new technical governance structure and will share more details in a future post on the React blog.&lt;/p&gt;
    &lt;head rend="h2"&gt;Meta and the React Foundation&lt;/head&gt;
    &lt;p&gt;Meta is committing to a five-year partnership with the React Foundation, including over $3 million in funding and dedicated engineering support. This investment will ensure React’s smooth transition to independent governance while maintaining the stability and innovation the community expects. Meta will continue to invest in React and use it as our primary tool for building UI on the web and across many of Meta’s apps. Meta will also continue to have a dedicated team of engineers working full-time on React and React Native.&lt;/p&gt;
    &lt;p&gt;We believe the best of React is yet to come. The React Foundation will unlock new opportunities for collaboration, innovation, and growth that will benefit the entire ecosystem. We’re excited to see what the community will build together under this new model. With strengthened governance, broader industry participation, and continued technical excellence, React is positioned to tackle the next generation of challenges in UI development.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45524624</guid><pubDate>Thu, 09 Oct 2025 07:30:12 +0000</pubDate></item><item><title>QUIC and the End of TCP Sockets</title><link>https://codemia.io/blog/path/QUIC-and-the-End-of-TCP-Sockets-How-User-Space-Transport-Rewrites-Flow-Control</link><description>&lt;doc fingerprint="1ee7e5a408d5952d"&gt;
  &lt;main&gt;
    &lt;p&gt;Practice Now Codemia © 2022 Codemia Resources Blog System Design Legal Terms &amp;amp; Conditions Privacy Policy Contact Social Twitter LinkedIn All Rights Reserved.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45525303</guid><pubDate>Thu, 09 Oct 2025 09:14:58 +0000</pubDate></item><item><title>N8n raises $180M</title><link>https://blog.n8n.io/series-c/</link><description>&lt;doc fingerprint="bcc70051598c2d6a"&gt;
  &lt;main&gt;
    &lt;quote&gt;We just raised $180 million in Series C funding, bringing our total funding to $240 million and our valuation to $2.5 billion.&lt;lb/&gt;The round is led by Accel, with support from Meritech, Redpoint, Evantic and Visionaries Club. Corporate investors NVentures (NVIDIA’s venture capital arm) and T.Capital also join the round, with previous backers including Felicis Ventures, Sequoia, Highland Europe and HV Capital making follow-on investments as well&lt;/quote&gt;
    &lt;p&gt;This investment recognises something fundamental: the AI race isn't only about smarter models - it's about who can actually put that intelligence to work reliably, inside actual businesses.&lt;/p&gt;
    &lt;p&gt;The AI agent landscape has split into two camps. Some platforms put everything in the hands of AI, you write prompts and hope for the best, with the entire logic determined by the model's interpretation. Others require strict, rule-based routing which is powerful for engineers who code every pathway, but impractical for business users who need to iterate quickly.&lt;/p&gt;
    &lt;p&gt;We've learnt from our community that neither extreme serves businesses well. Pure autonomy creates magic when it works but proves too unpredictable for business-critical workflows. Pure rule-based routing offers predictability but demands more time and often developers for every change.&lt;/p&gt;
    &lt;p&gt;n8n was built for the reality in between: giving flexible control over where your agents sit on this spectrum. You choose the balance - how much autonomy to grant, how much logic to enforce, and crucially, how to adjust that balance as you learn what works.&lt;/p&gt;
    &lt;p&gt;But controlling this balance is only the foundation. Getting agents into production requires two more crucial elements:&lt;/p&gt;
    &lt;p&gt;Orchestration: Connecting agents to your actual tools and data sources, building in human oversight where needed, and establishing the monitoring and triggers that keep everything running&lt;/p&gt;
    &lt;p&gt;Coordination: Bringing together the people who understand the business need with the builders who can make it work - on the same platform, in real time&lt;/p&gt;
    &lt;p&gt;Without both, organisations get stuck in endless development cycles. Engineers build in isolation, business users test and provide feedback, iterations drag on. The agent never reaches production because the people closest to the work can't collaborate effectively with those building the solution.&lt;/p&gt;
    &lt;p&gt;The formula we've proven is straightforward: combine AI, code, and humans in the same process, on the same platform. Technical builders handle architecture whilst domain experts configure and refine. That's coordination, and it only succeeds when the orchestration layer is flexible enough to evolve with your needs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our principles&lt;/head&gt;
    &lt;p&gt;We've been building n8n since 2019, first as an automation tool, then as a platform for AI orchestration and cross-team collaboration. Now we're the platform our community and enterprises tell us finally helps them deploy AI in production.&lt;/p&gt;
    &lt;p&gt;We've built this alongside a fast-growing community contributing videos, templates, nodes, and more. A community we'll never stop caring about or restrict access to. Ever.&lt;/p&gt;
    &lt;p&gt;Flexibility will always be a top product priority. Flexibility to pick any model, connect any tool, and deploy anywhere: our cloud, your server, a Raspberry Pi, or bare metal. Flexibility in the product so it's easy enough to make a quick start but robust enough to handle orchestration's natural complexity.&lt;/p&gt;
    &lt;p&gt;From tinkerers automating lights at home to the United Nations running mission-critical workflows at scale, our ambition is clear: n8n becomes the default platform to build with AI. And more importantly, to deploy AI.&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s next&lt;/head&gt;
    &lt;p&gt;We've made so much progress this year alone: 6x user growth, 10x revenue growth, and major new features like Evaluations, Data Tables, and many more improvements. Yet it still feels early.&lt;/p&gt;
    &lt;p&gt;The industry is moving fast, and so are we. This funding accelerates our roadmap: expanding our integrations, empowering the ecosystem to build their own nodes and share them globally, and evolving n8n beyond the canvas into new interfaces that match how different teams work. We're making the platform easier to start with whilst more powerful at scale - because that's what production AI demands. (We're hiring!)&lt;/p&gt;
    &lt;p&gt;Our community is already pushing n8n from a platform into an ecosystem. We're rapidly seeing people build their own businesses around n8n, and we want to support the community to take this further: with education, early access to features, commercial partnerships, and fun events to bring everyone together.&lt;/p&gt;
    &lt;p&gt;I started n8n to remove repetitive tasks and focus on what I actually enjoyed. Now I see a world where building with AI, leveraging agents to scale yourself, and becoming a 10x operator becomes table stakes, just as using Excel is. The first people who knew Excel were special, but now it's a requirement for many roles.&lt;/p&gt;
    &lt;p&gt;The same will happen with AI. And my ambition is that n8n becomes the default platform to build with it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45525336</guid><pubDate>Thu, 09 Oct 2025 09:19:18 +0000</pubDate></item><item><title>Zippers: Making Functional "Updates" Efficient (2010)</title><link>http://www.goodmath.org/blog/2010/01/13/zippers-making-functional-updates-efficient/</link><description>&lt;doc fingerprint="b6c3d03768b1d678"&gt;
  &lt;main&gt;
    &lt;p&gt; In the Haskell stuff, I was planning on moving on to some monad-related&lt;lb/&gt; stuff. But I had a reader write in, and ask me to write another&lt;lb/&gt; post on data structures, focusing on a structured called a&lt;lb/&gt; zipper.&lt;/p&gt;
    &lt;p&gt; A zipper is a remarkably clever idea. It’s not really a single data&lt;lb/&gt; structure, but rather a way of building data structures in functional&lt;lb/&gt; languages. The first mention of the structure seems to be a paper&lt;lb/&gt; by Gerard Huet in 1997, but as he says in the paper, it’s likely that this was&lt;lb/&gt; used before his paper in functional code — but no one thought to formalize it&lt;lb/&gt; and write it up. (In the original version of this post, I said the name of the guy who first wrote about zippers was “Carl Huet”. I have absolutely no idea where that came from – I literally had his paper on my lap as I wrote this post, and I still managed to screwed up his name. My apologies!)&lt;/p&gt;
    &lt;p&gt; It also happens that zippers are one of the rare cases of data structures&lt;lb/&gt; where I think it’s not necessarily clearer to show code. The concept of&lt;lb/&gt; a zipper is very simple and elegant – but when you see a zippered tree&lt;lb/&gt; written out as a sequence of type constructors, it’s confusing, rather&lt;lb/&gt; than clarifying.&lt;/p&gt;
    &lt;p&gt; The basic idea of a zipper is to give you a way of efficiently working with data&lt;lb/&gt; structures in a functional language. There are a lot of cases where in an imperative&lt;lb/&gt; language, there’s some basic operation which is cheap and simple in the imperative&lt;lb/&gt; language, because it’s performed by an in-place update. But in a functional language,&lt;lb/&gt; you can’t update a field of a data structure: instead, you have to create a new copy of the structure with the altered&lt;lb/&gt; field. &lt;/p&gt;
    &lt;p&gt; For example, consider the list &lt;code&gt;[a b c d e f g]&lt;/code&gt;. Implemented&lt;lb/&gt; as a cons-list, it’s a list of 7 cons-cells. Suppose you wanted&lt;lb/&gt; to replace “e” with “q”. In an imperative language, that’s no problem: just&lt;lb/&gt; do a set-car! of the 5th cell. In a functional language, you would&lt;lb/&gt; need to create a new list with “q” instead of&lt;lb/&gt; “e”. You could re-use the common tail &lt;code&gt;[f g]&lt;/code&gt;, but you would need&lt;lb/&gt; to re-create the other 5 cells: you’d need to create a new cell to&lt;lb/&gt; attach “q” to &lt;code&gt;[f g]&lt;/code&gt;. Then you’d need to create a new&lt;lb/&gt; cell to connect “d” to &lt;code&gt;[q f g]&lt;/code&gt;. And so on.&lt;/p&gt;
    &lt;p&gt; That makes the functional program much slower than the imperative one.&lt;lb/&gt; If you’ve got a data structure that conceptually changes over time, and you’re going to make lots of changes,&lt;lb/&gt; the cost of doing it functionally can become very high, because of all of the copying&lt;lb/&gt; you do instead of mutating a data structure.&lt;/p&gt;
    &lt;p&gt; In general, it’s very hard to get around that. You can’t update in place&lt;lb/&gt; in a functional language (at least, not without some serious cleverness, either&lt;lb/&gt; in your code (like monads), you language (like linear types), or your compiler).&lt;lb/&gt; But for many applications, there’s some notion&lt;lb/&gt; of a focus point – that is, a particular key point where changes&lt;lb/&gt; happen — and you can build structures where updates around the focus&lt;lb/&gt; can be performed efficiently.&lt;/p&gt;
    &lt;p&gt; For example, if you’re building a text editor, you’ve got the point&lt;lb/&gt; where the cursor is sitting – and the changes all happen around the cursor.&lt;lb/&gt; The user might type some characters, or delete some characters – but it always&lt;lb/&gt; happens around the cursor.&lt;/p&gt;
    &lt;p&gt; What a zipper does is take a data structure, and unfold it around a focal&lt;lb/&gt; point. Then you can make changes at the focal point very quickly – about as&lt;lb/&gt; quickly as an in-place update in an imperative language.&lt;/p&gt;
    &lt;p&gt; The idea of it is a lot like a gap-buffer. Right now, I’m actually working&lt;lb/&gt; on a text-editor. I’m writing it using a gap-buffer. Conceptually, an&lt;lb/&gt; edit-buffer is one continuous sequence of characters. But if you represent it&lt;lb/&gt; as a continuous sequence of characters, every insert is extremely expensive.&lt;lb/&gt; So what you do is split it into two sub-sequences: one consisting of the&lt;lb/&gt; characters before the cursor point, and one consisting of the&lt;lb/&gt; characters after the cursor point. With that representation,&lt;lb/&gt; inserting a character at the cursor point is O(1). Moving by one character is&lt;lb/&gt; also O(1). Moving by N characters is O(N). With various improvements, you can&lt;lb/&gt; do much better than that – but the key bit is that split between before the&lt;lb/&gt; focus point and after it.&lt;/p&gt;
    &lt;p&gt; A zipper is a tree or graph-based version of a similar idea. For this&lt;lb/&gt; discussion, I’ll describe it in terms of trees; the graph version is more complicated,&lt;lb/&gt; but you should be able to get the idea from seeing how it works on trees. The&lt;lb/&gt; idea is that you take the tree structure, and you split it around a focus. You’re focused on some node in the&lt;lb/&gt; tree. You keep track of a set of nodes that come before you, and a&lt;lb/&gt; set of nodes that come after you – those are basically like the&lt;lb/&gt; pre-gap and post-gap regions of a gap buffer. But because you’re working in a&lt;lb/&gt; tree, you need a bit more information: you need to know the path from&lt;lb/&gt; the root of the tree down to the current node. &lt;/p&gt;
    &lt;p&gt; It’s called a zipper because what you do to create this pre-focus, path,&lt;lb/&gt; and post-focus bits of the structure is unzip the tree. For example,&lt;lb/&gt; look at the tree below. It’s a representation of a string of text represented&lt;lb/&gt; by a tree. In this particular tree, all of the data is stored in the leaves.&lt;lb/&gt; The internal nodes contain metadata, which I haven’t shown in the diagram.&lt;/p&gt;
    &lt;p&gt; Now, suppose I want to put the focus on “mno”. To do that, I climb down&lt;lb/&gt; the tree, unzipping as I go. I start at the root, node N1. Then I go&lt;lb/&gt; right. So I put N1 and its left subtree into the left-context of my&lt;lb/&gt; zipper-tree, and add “Right at N1” to the path. That puts the focus at N3. To&lt;lb/&gt; get to “mno” from N3, I need to go left. So I put N3 and its right child into&lt;lb/&gt; the right context, and add “Left at N3” to the path. Now the focus is at N4.&lt;lb/&gt; To get to “mno”, I need to go right: so I put N4 and its left child into the&lt;lb/&gt; left context, and add “Right at N4” to the path. Now I’ve got the focus set&lt;lb/&gt; where I want it at “mno”; and I’ve got right and left contexts.&lt;/p&gt;
    &lt;p&gt; With the zipper, you can make all sorts of changes very easily at the&lt;lb/&gt; focus. Suppose I want to change the focus node, by inserting some text. I can&lt;lb/&gt; do that functionally, without actually changing anything, by creating a new&lt;lb/&gt; zipper tree which is identical to the old one, but which changes the value of&lt;lb/&gt; the focus node – that is, if I were to add “123” right after “mno”, I could do&lt;lb/&gt; it by creating a new focus node “mno123”, with the same path, left, and right&lt;lb/&gt; contexts. It takes minimal extra memory to create the copy, because I can&lt;lb/&gt; re-use the path and the contexts. &lt;/p&gt;
    &lt;p&gt; I could also add new children nodes. Suppose that instead of adding&lt;lb/&gt; “123” to the focus, I want to keep each leaf containing three characters.&lt;lb/&gt; could replace the focus with a new node, N5, which had children “mno”&lt;lb/&gt; and “123”. I could re-use the “mno” node, and the path, left, and right&lt;lb/&gt; contexts.&lt;/p&gt;
    &lt;p&gt; That’s the beauty of the zipper: most operations can be in terms of local&lt;lb/&gt; changes, re-using most of the structure. If we were using a standard tree,&lt;lb/&gt; then to add a new node in the position of “mno”, we would need to create&lt;lb/&gt; copies of N4, N3, and N1; instead, we only need to create the one new&lt;lb/&gt; node.&lt;/p&gt;
    &lt;p&gt; Doing other things isn’t that difficult either. Suppose we wanted to move&lt;lb/&gt; the focus to “pqr”. We’d need to shift the focus from “mno” to N3, then to N3,&lt;lb/&gt; and then to “pqr”. To get from “mno” to N4, we take the last step off of the&lt;lb/&gt; path – which says we went right at N4 – so we set the focus to N4, and&lt;lb/&gt; re-establish “mno” as its right child. So the focus would be N4, with “jkl” as&lt;lb/&gt; its left child, and “mno” as its right child. To get from N4 to N3, we unroll&lt;lb/&gt; another step of the path: since we went left at N3, that means that N3 is the&lt;lb/&gt; new focus, with N4 as its left child. Then we’d go down to the right from N3,&lt;lb/&gt; so we’d add “right at N3” to the path, and “pqr” would be the new focus.&lt;lb/&gt; Moving the focus like that is a tad more difficult than just traversing&lt;lb/&gt; non-zipper tree, but it’s not significantly slower – and it makes the edits&lt;lb/&gt; much, much faster.&lt;/p&gt;
    &lt;p&gt;So why is it harder to code? Because when we’re dealing with trees, we’re pretty much always dealing with balance. And balance isn’t a local property. No matter which kind of tree you use – red/black, 2/3, AVL – you might need to climb up the tree to do the balance maintenance. That mangles the simple zipper.&lt;/p&gt;
    &lt;p&gt; You’ve got two choices. One is to re-balance&lt;lb/&gt; the tree immediately. You can definitely do that. For&lt;lb/&gt; example, if you think of how you do a re-balance in&lt;lb/&gt; a red-black tree, you climb up the tree doing fixes until you’ve got things rebalanced. You can definitely do that – by using the zipper to move around the tree. But a big part of the point of the zipper is to keep operations local, and the re-balancing is not a&lt;lb/&gt; local operation. Much of the time, you can do things&lt;lb/&gt; locally, but sometimes you’ll be stuck re-zipping as you move the focus up the tree fixing the balance; in&lt;lb/&gt; the worst case, you need to re-zip the entire tree, all the way to the root.&lt;/p&gt;
    &lt;p&gt; The alternative is something called scarring. You put marks in the tree called scars that identify places where you made changes that could trigger a rebalance. (Or more generally,&lt;lb/&gt; in places where you made an edit that could have violated some invariant of the data structure.) You don’t do the fix immediately – you just mark it with&lt;lb/&gt; the scar, and then at some point, whenever it makes sense for your application, you go back to the scars, and fix the tree. (Scaring can also have a more general meaning, which involves memorizing certain paths through&lt;lb/&gt; the tree, so that you can make changes at the leave, then a few steps up, then back at the leaf. It’s a similar concept; in both forms of scarring, you’re optimizing to reduce the cost of zipping up and down the tree. )&lt;/p&gt;
    &lt;p&gt; Either way, it gets a bit more complicated – and when you look at the code&lt;lb/&gt; for a zipper, the re-balancing/invariant fixing has a tendency to dominate the complexity&lt;lb/&gt; of the code. The zipper itself is so simple and so elegant that it just disappears under&lt;lb/&gt; the weight of tree-balancing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45526042</guid><pubDate>Thu, 09 Oct 2025 11:07:29 +0000</pubDate></item><item><title>Dark Patterns: Buying a Bahncard at Deutsche Bahn</title><link>https://www.ketzu.net/dark-patterns-buying-a-bahncard-at-deutsche-bahn/</link><description>&lt;doc fingerprint="7c0f01df493643a5"&gt;
  &lt;main&gt;
    &lt;p&gt;Deutsche Bahn, the fully state-owned railway company, is a well-liked (Trustpilot: 1.2/5) company running most major long-distance railways in Germany. So, it will come as a surprise to many that I have some complaints about them. Today, however, I won’t talk about the numerous delays, cancellations and frankly bad service interactions I had with them in the past. Today is about dark patterns that cost me 500€ for nothing.&lt;/p&gt;
    &lt;p&gt;I apologise in advance for using mostly German material, as it is the primary language. I thought it wasimportant to use it. Also, DB manages to make the English part even worse by breaking their reference to additional material (see header image). Also, if your sarcasm detector is broken, you might have trouble with some sections.&lt;/p&gt;
    &lt;p&gt;A dark pattern is a design mechanism to get people to buy, do or allow things they don’t actually want. Examples include: Forced continuity of free trials with paid subscriptions, hiding the true costs of a service, and many, many more.&lt;/p&gt;
    &lt;p&gt;These are especially prevalent with subscriptions, so it might not come as a surprise that we will look at subscriptions today. At first, let’s look at two other well-liked and respected companies: Adobe and Deutsche Telekom. Both make use of subscriptions: One for an ongoing connection to the internet and one for software. This is how they are advertised on their website.&lt;/p&gt;
    &lt;p&gt;Both note prices in Currency Per Timeframe: €/month or €/year. Telekom even includes some discounts: 9.95€/month and in smaller font: 43.95€/month starting with the 4th month! Adobe clearly states they are selling a monthly subscription (“Monats-Abo”) or a yearly subscription, but paid monthly. There is no misunderstanding. (Note: Adobe is highly criticised for its subscription practices.)&lt;/p&gt;
    &lt;p&gt;Based on these examples, let’s see how Deutsche Bahn advertises their BahnCards. A BahnCard is a discount offer; it is not a product or service in itself, but rather allows you to get discounts. They exist on various levels: Normal and Business, 25/50/100, Probe or Normal, first or second class. I’ll limit this to the first class BahnCard 50, as that’s the one I got and the Probe version, which is only valid for 3-month period.&lt;/p&gt;
    &lt;p&gt;A Bahncard 50 gives you a 50% discount on regular fares (which are rarely paid by most casual users) and a 25% discount on discounted tickets. After using it for a year, I’d say it is a very bad deal for most casual travellers, but that’s beside the point. I bought mine on sale (50% off), but those are not available at the time of screenshotting.&lt;/p&gt;
    &lt;p&gt;The “weitere Informationen” link lets us see additional details about this order and an FAQ section with important information.&lt;/p&gt;
    &lt;p&gt;It even contains a neat question about the cost of the BahnCard 50! That’s nice. 492 Euro for the first-class card. We knew that, it was clearly advertised! Silly. We can even link to this section specifically.&lt;/p&gt;
    &lt;p&gt;So: Is this a subscription? Yes, of course it is! The Probe Bahncard page actually mentions this.&lt;/p&gt;
    &lt;p&gt;As well as the sales process, once you hit “jetzt bestellen” (buy now).&lt;/p&gt;
    &lt;p&gt;In case you missed it, it is the text in the centre of the screen, you won’t see the word “Abonnement” again. I used a fake account to generate some screenshots of the current sales process, but Deutsche Bahn changes it around every now and then, so neither can I be sure it looked like this when I bought my Bahncard, nor is it guaranteed to look like this when you go through the process.&lt;/p&gt;
    &lt;p&gt;I have some objections to the layout of this page. In my opinion, this text is designed to make people not take notice of the subscription. No mention of the timeframe or recurrent nature of the price. For completion: The probe Bahncard screen does not mention the price of the full Bahncard (492€) at all, only the probe price.&lt;/p&gt;
    &lt;p&gt;Let’s head on to the final sales screen; the intermediate screens only take in the user information and provide no additional information on the Bahncard.&lt;/p&gt;
    &lt;p&gt;There is zero indication of the payments (the value of 492) being recurrent (“492.00€/year”); the probe version does not contain the price of the full Bahncard at all. The mention of the subscription does not use the word “Abonnement” (subscription) this time, instead says that it gets automatically extended by one year unless you cancel. It has no visual weight, unlike the “bahnde-Newsletter” abonnement (so if you ctrl+f “abo” you’ll end up in that section), and there is no option to not get the Bahncard as a non-subscription.&lt;/p&gt;
    &lt;p&gt;You buy the Bahncard and receive a confirmation email for your subscription. Although it does not contain the word Abonnement, any mention of renewal, the price of the subscription (only the reduced price of the sale in my case) or any indication of cancellation. You also receive a welcome email, letting you know about the advantages of your Bahncard, which, again, contains no information about cancellation, renewal, subscription, or prices.&lt;/p&gt;
    &lt;head rend="h4"&gt;You Now Have A Subscripton. How Can You Tell?&lt;/head&gt;
    &lt;p&gt;Congratulations, you bought a Bahncard subscription. You used it for a while and decided it is not worth the cost. You don’t remember getting a subscription, but you want to make sure! Unfortunately, you don’t try the obvious thing, but make the mistake of checking your actual account at Deutsche Bahn. So, where to search for the subscription?&lt;/p&gt;
    &lt;p&gt;Deutsche Bahn thought about that! They have a subscription portal called Aboportal. It is clearly labelled in the account menu, right next to the Bahncard option! Amazing. Here’s what it looks like for me right now:&lt;/p&gt;
    &lt;p&gt;I had two subscriptions in the past for Deutschland-Tickets; they were cancelled long ago, and they still show up. They have a lot of space for additional menu items; it seems they don’t believe in separating functions or user-friendly design — that’s just my opinion. No Bahncard subscription, though. I would not fault you for believing you do not have a subscription active after this screen and just quitting, but let’s try more.&lt;/p&gt;
    &lt;p&gt;So it has to be in the Bahncard section. Here are two entries, one is my extended Bahncard I never wanted, and one is my Bahncard Business that, fortunately, my company provides. Can you tell if, and which, is a subscription?&lt;/p&gt;
    &lt;p&gt;It’s very obvious if you know what you need to look out for. If you hit the “options” selector, there will be a third entry saying “Cancel Subscription” for a subscription. Also, Bahncard Business is not available as a subscription, so it can’t be one.&lt;/p&gt;
    &lt;p&gt;Lastly, when are you informed of the renewal? The cancellation time is 4 weeks before the end of validity. 21 days, or three weeks, before renewal, you will receive a notification for your new bahncard. It is too late now. You are stuck with it for another year.&lt;/p&gt;
    &lt;p&gt;If you cancel instantly anyway, you will receive an automated cancellation reply with the following neat sentence:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Ihrem Wunschtermin können wir leider nicht entsprechen, da eine Kündigung nur bis spätestens vier Wochen vor Ende der Laufzeit möglich ist. Dafür bitten wir um Verständnis.&lt;/p&gt;Automated DB Email for cancellation&lt;/quote&gt;
    &lt;p&gt;Translation: “Unfortunately, we cannot accommodate your requested date, as termination is only possible up to four weeks before the end of the term. We apologise for any inconvenience.” Deutsche Bahn has fully incorporated the subscription trap into the response. They are aware of it, they know you do not want it, and they let you know: They will not cancel.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contacting Deutsche Bahn Support&lt;/head&gt;
    &lt;p&gt;If you are remotely acquainted with German support standards, you can probably skip this section. Deutsche Bahn has no intention of providing any form of good support.&lt;/p&gt;
    &lt;p&gt;First, you’ll have trouble finding a way to contact them, that is, not by phone (or in person). After some searching, you’ll find a contact form linked in some places that does not work. In the end, I managed to get a response from this email address: bahncard-service@bahn.de&lt;/p&gt;
    &lt;p&gt;I sent them a message about how I tried to figure out that I have a subscription recently and did not find anything, and how I never wanted this renewal, how I was never informed of the increased cost (as I bought it on sale). To prevent any misinformation about my email, here is the translated original:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Last year, I purchased a Bahncard as part of a special offer.&lt;/p&gt;&lt;lb/&gt;A few weeks ago, I checked to see if I could find any information about a subscription, as I don’t want another Bahncard—I only needed it for one year.&lt;lb/&gt;I couldn’t find any information in my original confirmation email or on the website, and the subscription portal doesn’t list it either.&lt;p&gt;Today, I suddenly received a message saying that I will receive a new Bahncard in three weeks.&lt;/p&gt;&lt;lb/&gt;I canceled it immediately online, but unfortunately I was told that it was too late and that I should have done so at least four weeks before the expiration date.&lt;lb/&gt;The only email with the words “subscription” and “Bahncard” is the new Bahncard email.&lt;lb/&gt;Furthermore, the email does not contain any information about any costs associated with the Bahncard.&lt;p&gt;???&lt;/p&gt;Translation of my email to the support.&lt;/quote&gt;
    &lt;p&gt;As far as I can tell, I received a standardised response. You can find the translated version here:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Thank you for your email dated September 21, 2025.&lt;/p&gt;&lt;p&gt;For all offers on www.bahn.de, the contract is only concluded at the end of the booking process.&lt;/p&gt;&lt;p&gt;Before completing your purchase, we will inform you about the terms and conditions for purchasing and using the BahnCard.&lt;/p&gt;&lt;p&gt;You acknowledged this directly during the online booking process and accepted it by clicking “Buy now.”&lt;/p&gt;&lt;p&gt;From October 1 to October 13, 2024, the BahnCard 50 was available at a discounted price.&lt;/p&gt;&lt;p&gt;After that, the BahnCard will be converted into a regular subscription.&lt;/p&gt;&lt;p&gt;We regret that you disagree with our decision to cancel your BahnCard.&lt;/p&gt;&lt;p&gt;The BahnCard subscription can be canceled annually up to four weeks before the end of the validity period. The date of receipt of the cancellation by BahnCard Service is decisive.&lt;/p&gt;&lt;p&gt;We ask for your understanding that we are sticking to our decision.&lt;/p&gt;&lt;p&gt;We have still scheduled the cancellation of your BahnCard **** for **. **** 2026.&lt;/p&gt;&lt;p&gt;Translated with DeepL.com (free version)&lt;/p&gt;Translation of the support email.&lt;/quote&gt;
    &lt;p&gt;Another back and forth led to the same result. This problem is common enough that a lawyer put out a default letter template for refusing, which I used for my second letter. It comes up on Reddit frequently, although I do not understand the people who defend this.&lt;/p&gt;
    &lt;p&gt;As far as I can tell, the previous attempts by Verbraucherschutz to sue Deutsche Bahn over this were unsuccessful. The layman’s summary I got from this court decision: As the Bahncard is a discount card and not a subscription to a product or service itself, it does not fall under the consumer protections that the Verbraucherschutz sued over.&lt;/p&gt;
    &lt;head rend="h2"&gt;Deutsche Bahn: No Reputation To Lose&lt;/head&gt;
    &lt;p&gt;The Deutsche Bahn has no reputation they lose. The discourse is dominated by their terrible service, delays and cancellations. They have no incentive, as a company, to improve other sections like the Bahncard sales tactics. They, as a company, have nothing to lose.&lt;/p&gt;
    &lt;p&gt;We as a society have a lot to lose. Deutsche Bahn is 100% owned by the state of Germany. We consumers do not cleanly separate companies from their owners. Many even think of Deutsche Bahn as a state company. Feeling cheated by your government is not good.&lt;/p&gt;
    &lt;p&gt;Further, Deutsche Bahn is a core infrastructure company for more eco-friendly transportation. Driving people, like me, away from using the Bahn is bad in so many ways. I have a high desire for fairness; if I feel cheated, I’d rather take a higher loss to myself than support the person or company that cheated me. However, I have little recourse in this whole debacle, but I’d rather drive my car more, or even rent one, than use Deutsche Bahn… and I really like taking trains. I love being lazy while the landscapes fly by instead of focusing on the road and getting annoyed at other drivers. All this, despite not a single one of my ~20 train journeys the last year being on time, and a dozen of them not even honouring my reservations, being overrun or flat out cancelled. None of that made me quit taking the train. Feeling cheated by Deutsche Bahn sure does.&lt;/p&gt;
    &lt;p&gt;This post was written as anger management. This whole desaster of communcation, despite “only” being 500€, cost me so much productivity, motivation and more. I hate dealing with these kind of things. It followed me around all day, made me lose sleep, just because it annoyed me so much. Now I feel better. So for some fun: My first email to Deutsche Bahn netted me 8 recipient confirmation emails by deutsche Bahn, neatly spaced by around 3 hours each. Then they took 2 days to respond to my first email. For the second email I received only 1 confirmation, but got to wait 15 days for a response.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45526582</guid><pubDate>Thu, 09 Oct 2025 12:17:54 +0000</pubDate></item><item><title>McKinsey wonders how to sell AI apps with no measurable benefits</title><link>https://www.theregister.com/2025/10/09/mckinsey_ai_monetization/</link><description>&lt;doc fingerprint="3f5b03de093ae5ca"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;McKinsey wonders how to sell AI apps with no measurable benefits&lt;/head&gt;
    &lt;head rend="h2"&gt;Consultant says software vendors risk hiking prices without cutting costs or boosting productivity&lt;/head&gt;
    &lt;p&gt;Software vendors keen to monetize AI should tread cautiously, since they risk inflating costs for their customers without delivering any promised benefits such as reducing employee head count.&lt;/p&gt;
    &lt;p&gt;The latest report from McKinsey &amp;amp; Company mulls what software-as-a-service (SaaS) vendors need to do to navigate the minefield of hype that surrounds AI and successfully fold such capabilities into their offerings.&lt;/p&gt;
    &lt;p&gt;According to the consultancy, there are three main challenges it identifies as holding back broader growth in AI software monetization in the report "Upgrading software business models to thrive in the AI era."&lt;/p&gt;
    &lt;p&gt;One of these is simply the inability to show any savings that can be expected. Many software firms trumpet potential use cases for AI, but only 30 percent have published quantifiable return on investment from real customer deployments.&lt;/p&gt;
    &lt;p&gt;Meanwhile, many customers see AI hiking IT costs without being able to offset these by slashing labor costs. The billions poured into developing AI models mean they don't come cheap, and AI-enabling the entire customer service stack of a typical business could lead to a 60 to 80 percent price increase, McKinsey says, while quoting an HR executive at a Fortune 100 company griping: "All of these copilots are supposed to make work more efficient with fewer people, but my business leaders are also saying they can't reduce head count yet."&lt;/p&gt;
    &lt;p&gt;Another challenge is scaling up adoption after introduction, which the report blames on underinvestment in change management. It says that for every $1 spent on model development, firms should expect to have to spend $3 on change management, which means user training and performance monitoring.&lt;/p&gt;
    &lt;p&gt;The third issue is a lack of predictable pricing, which means that customers find it hard to forecast how their AI costs will scale with usage because the pricing models are often complex and opaque.&lt;/p&gt;
    &lt;p&gt;To address these, McKinsey focuses mainly on how software firms should structure their pricing in the age of AI, rather than the wisdom of infusing AI into everything in the first place.&lt;/p&gt;
    &lt;p&gt;The report considers it unlikely that the traditional per-user monthly subscription model will disappear entirely, but expects that vendors will have to incorporate some form of consumption-based pricing into the mix.&lt;/p&gt;
    &lt;p&gt;Many are starting with hybrid models, where "additional" consumption that goes beyond a capacity cap is treated in different ways, such as metered throughput that limits the number of tokens processed daily, weekly, or monthly for certain models.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AI agent hypefest crashing up against cautious leaders, Gartner finds&lt;/item&gt;
      &lt;item&gt;AI in your toaster: Analyst predicts $1.5T global spend in 2025&lt;/item&gt;
      &lt;item&gt;Goldman Sachs warns AI bubble could burst datacenter boom&lt;/item&gt;
      &lt;item&gt;Amazon's $100B DC spend similar to entire Costa Rica GDP&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;However, firms with hybrid models will need to revisit their choices frequently, it warns, as the rapid pace of AI evolution means that capabilities that are cutting-edge at launch can quickly become table stakes.&lt;/p&gt;
    &lt;p&gt;Vendors also need to choose their pricing unit carefully, whether that is a per-user flat fee with a capacity cap, like Microsoft Copilot, on a per-task basis, or perhaps on an outcome basis, such as per qualified lead for sales tools.&lt;/p&gt;
    &lt;p&gt;However, McKinsey also claims that the cost of inferencing is dropping rapidly, and so vendors need to consider carefully how they balance charges with growing adoption. The cost of large language model (LLM) delivery has declined by more than 80 percent per year over the past two years, it says.&lt;/p&gt;
    &lt;p&gt;Many SaaS companies believe they need to encourage trials to increase adoption, by offering free initial usage allocations for AI capabilities, for example. Once customers adopt and see value, the thinking goes, the firm can then look to upsell to a higher allocation for additional use cases. The problem with that, of course, is that one MIT study found that many enterprise organizations have so far seen zero return from their AI efforts.&lt;/p&gt;
    &lt;p&gt;Buyers are also changing, McKinsey believes. It says purchasing decisions are shifting from the IT department to line-of-business units. These leaders are increasingly making budget trade-offs between head count investment and AI deployment, and expect vendors to engage them on value and outcomes, not just features.&lt;/p&gt;
    &lt;p&gt;That could be a tricky sell, when trials of AI tools such as Microsoft's Copilot by a UK government department reveal no discernible boost in productivity. Still, the AI firms have to recoup all those billions they've already invested somehow, don't they? ®&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45526589</guid><pubDate>Thu, 09 Oct 2025 12:19:20 +0000</pubDate></item><item><title>'Guilty until proven innocent': Fight between docs and insurers over downcoding</title><link>https://www.nbcnews.com/health/health-care/guilty-proven-innocent-fight-doctors-insurance-companies-downcoding-rcna230714</link><description>&lt;doc fingerprint="3c82909c7d2250b7"&gt;
  &lt;main&gt;
    &lt;p&gt;In the beginning of the year, Dr. Terry Wagner’s office manager came to him and said “something weird is going on.”&lt;/p&gt;
    &lt;p&gt;For weeks, the office manager told him, Wagner had been quietly underpaid by the insurance company Aetna on a seemingly random selection of higher level claims.&lt;/p&gt;
    &lt;p&gt;“It’s blatantly disrespectful,” said Wagner, a family medicine doctor who has run a small practice in Hudson, Ohio, for the last 28 years. “It’s not like they came back to us saying, ‘Hey, we need more information,’” he said, adding that Aetna just paid the claims as if they’d been billed for a lower level of service.&lt;/p&gt;
    &lt;p&gt;It’s a practice called “downcoding.” Insurance companies — in Wagner’s case, Aetna — automatically downgrade the claims a doctor sends them to a lower tier of reimbursement, without actually reviewing details about the visit itself.&lt;/p&gt;
    &lt;p&gt;For Wagner, that means a “level four” office visit that might yield $170 is being paid as if it’s a “level three” for about $125. That $45 difference might not seem like much, but when it’s happening on dozens of claims, and to a physician-owned practice like Wagner’s, the damage mounts. “This can really hit a small company hard, especially if you’re not catching it,” he said.&lt;/p&gt;
    &lt;p&gt;If the doctor wants to challenge the decision, they have to appeal each claim with documentation defending their medical opinion.&lt;/p&gt;
    &lt;p&gt;“Some computer program is deciding what my level of care is,” Wagner said. “If they question my level of care, then ask for my notes. Look at the tests I ordered. Look at my charts.”&lt;/p&gt;
    &lt;p&gt;It’s not just Aetna: Other insurers including Anthem Blue Cross and Blue Shield, Humana and Molina Healthcare have all acknowledged downcoding higher level claims for certain office visits, or “adjusting” as it’s sometimes called. This summer, the insurer Cigna Healthcare announced that it will begin downcoding on certain claims starting in October.&lt;/p&gt;
    &lt;p&gt;The practice of automatic downcoding seems to have taken off in the last few years, as health care costs soar and insurance companies use third party vendors and AI programs to reduce costs. NBC News spoke to doctors’ offices across numerous specialties from around the country, all of whom rely heavily on office visits — rather than surgeries and procedures — for their revenue, and all of whom are experiencing downcoding from insurers.&lt;/p&gt;
    &lt;p&gt;The problem, doctors say, is that lower and lower reimbursements mean reliable community doctors, like Wagner, could have to make choices that are inherently bad for patients, like cramming more patient visits into a single day to make up for lost revenue, dropping patients on certain insurance plans, or selling their practices altogether.&lt;/p&gt;
    &lt;p&gt;“There’s a break point and it’s like you either see more [patients] and give them less time, or you just give up. It’s exhausting,” Wagner said.&lt;/p&gt;
    &lt;p&gt;Doctors being downcoded by Aetna are told they are coding visits “at a higher level” compared to their peers. “We may adjust your payment if the details on the claim don’t support the level of service billed,” a notice reviewed by NBC News says.&lt;/p&gt;
    &lt;p&gt;A similar letter from Anthem says doctors “whose coding patterns improve and are no longer identified as an outlier are eligible to be removed from the program.”&lt;/p&gt;
    &lt;p&gt;Wagner's office estimates he lost over $3,000 to downcoding in the first half of the year, but other doctors across the country have fared far worse.&lt;/p&gt;
    &lt;p&gt;Anthem started downcoding Missouri dermatologist Dr. Sarah Jensen’s practice almost two years ago, Jensen said, to the tune of nearly $14,000 in lost payment. She's been able to recoup over a third of that through appeals.&lt;/p&gt;
    &lt;p&gt;“You almost lose that wherewithal to fight,” she said of the drudgery of filing appeal after appeal. “I mean, it’s kind of their goal I would imagine.”&lt;/p&gt;
    &lt;p&gt;Jensen said she feels stuck in an impossible situation. She can’t fit any more patient visits into her day without seriously affecting her standard of care, and because she recently opened a second office, simply turning away Anthem patients isn’t a financially viable option right now. (Anthem is Missouri’s largest commercial insurance provider.)&lt;/p&gt;
    &lt;p&gt;She’s confident she’s billing appropriately and has even asked peers to spot-check her, but the downcodes just keep coming, she said.&lt;/p&gt;
    &lt;p&gt;A representative for Anthem deferred to AHIP — a trade association representing the health insurance industry — for comment.&lt;/p&gt;
    &lt;p&gt;A spokesperson for AHIP told NBC News that identifying “outlier practices” that are “out of step with their peers” is part of health plans’ efforts to “advance quality and affordability.”&lt;/p&gt;
    &lt;p&gt;“These policies are narrowly applied and enable plans to encourage practice patterns consistent with peers and clinical guidelines,” the spokesperson said in a statement.&lt;/p&gt;
    &lt;p&gt;Aetna spokesperson David Whitrap said the insurer “has an obligation to monitor for appropriate coding on behalf of our clients and members,” and to “safeguard against fraud, waste, and abuse in the government programs we serve.”&lt;/p&gt;
    &lt;p&gt;“Evaluating the appropriateness of level 4 and 5 codes helps us ensure providers are billing for their services consistent with national guidelines,” Whitrap said. He also said that “only 3% of providers” are affected by the payment policy, but did not clarify how Aetna determines when providers are coding inappropriately.&lt;/p&gt;
    &lt;p&gt;A spokesperson for Cigna Healthcare said approximately 1% of providers in its network will be affected by its upcoming downcoding policy, and “all have the right to request we reconsider individual claims reimbursement decisions.”&lt;/p&gt;
    &lt;p&gt;Humana and Molina Healthcare did not respond to requests for comment.&lt;/p&gt;
    &lt;head rend="h2"&gt;“It’s going to worsen our patient care”&lt;/head&gt;
    &lt;p&gt;Dr. Bobby Mukkamala, the president of the American Medical Association (AMA), described downcoding as a “game” to improve insurers’ finances.&lt;/p&gt;
    &lt;p&gt;“It’s going to worsen our patient care, but it’s going to improve their bottom line,” Mukkamala said. “And that’s the wrong calculus to use to improve health care in this country.”&lt;/p&gt;
    &lt;p&gt;Momentum for automatic downcoding comes as experts say an inverse problem of “upcoding” is on the rise. Upcoding is when patients and plans are billed as though a higher level of service occurred, like when a bill is sent as though you had an appointment with the doctor, but you only ever saw a nurse practitioner.&lt;/p&gt;
    &lt;p&gt;In 2021, the Centers for Medicare and Medicaid Services called upcoding “a serious problem” and the federal government periodically brings improper billing cases, both criminally and civilly, against health care providers.&lt;/p&gt;
    &lt;p&gt;But the AMA position is that automatic downcoding — as opposed to insurers reimbursing for a lower level of service after reviewing additional information from physicians — doesn’t have any clinical logic behind it.&lt;/p&gt;
    &lt;p&gt;For instance, when you visit a doctor for a sinus infection, the doctor typically sends your insurance a claim referencing two sets of codes, one that lays out the ultimate diagnosis, and another that indicates the level of service or complexity of the visit. That level can be determined by the amount of time the doctor spends on the visit, or by the complexity of the medical decision-making at play — essentially a rubric that takes into account things like lab tests ordered, medications prescribed and treatment options discussed — but all that’s included on the claim is the code.&lt;/p&gt;
    &lt;p&gt;Downcoding based only on that knowledge assumes that an office visit for a certain set of diagnoses has a ceiling of complexity, regardless of the patient’s medical history and the realities of what actually occurred during that visit.&lt;/p&gt;
    &lt;p&gt;That’s where the AMA disagrees.&lt;/p&gt;
    &lt;p&gt;“‘Guilty until proven innocent’ infers that we’re in a court, right? That I’m to have my chance to say, ‘This is why I did what I did. But when this isn’t announced and this is something that happens in the background, this is a conviction before there’s even a court trial,” Mukkamala said. “I deserve to know what you think I did wrong so I can tell you my opinion of why it’s right.”&lt;/p&gt;
    &lt;p&gt;Since March, the AMA has sent letters to Blue Cross Blue Shield, Cigna and AHIP calling out the insurers’ downcoding practices. None of the organizations replied, the AMA said.&lt;/p&gt;
    &lt;p&gt;The AMA sent a similar letter about what it called Aetna’s “unjustified, blunt payment reduction tool” to Aetna’s then-chief medical officer, Cathy Moffitt, in July 2024. The AMA would not disclose Moffitt’s response, but Aetna’s policy does not appear to have changed.&lt;/p&gt;
    &lt;p&gt;A 2024 marketing flyer for Aetna’s “Claim and Code Review Program” does offer a glimpse at the insurer’s financial calculus — touting an average 6.4% reduction in costs for employer health plans, or $1.6 million in savings for a plan with 4,000 members.&lt;/p&gt;
    &lt;head rend="h2"&gt;‘Like breaking down Fort Knox’&lt;/head&gt;
    &lt;p&gt;Cheryl Crowder runs billing for a physician-owned practice group in rural southeast Ohio.&lt;/p&gt;
    &lt;p&gt;For more than a year, she says her doctors have been repeatedly downcoded by two insurance companies.&lt;/p&gt;
    &lt;p&gt;Hospitals often have the benefit of robust billing teams that exist, in part, to deal with such reimbursement issues. But for small physician-owned practices, downcoding can be as much of an administrative burden as a financial one.&lt;/p&gt;
    &lt;p&gt;“We have to copy records, and do claim audits and provide that information to them,” Crowder said.&lt;/p&gt;
    &lt;p&gt;It can also be costly. “We’re paying people overtime to do this work. Because we have 70-plus providers, and we have eight billers. It’s a ton of work,” she said.&lt;/p&gt;
    &lt;p&gt;Like many of the doctors’ offices in this article, Crowder’s practice group was told they could be removed from a downcoding program if they successfully overturned a certain percentage of their appealed claims, but the mechanics of actually achieving that are hazy.&lt;/p&gt;
    &lt;p&gt;“At what point do we measure that? Is it three months? Six months?” she said. “They couldn’t answer that.”&lt;/p&gt;
    &lt;p&gt;Crowder even considered ending her practice group’s contract with one of the insurers before finally getting some relief this summer.&lt;/p&gt;
    &lt;p&gt;In the same time frame that downcoding has become prevalent, another trend is compounding the problem, doctors said: the disappearance of dedicated representatives at insurance companies, whom doctors could reach out to in case of payment issues.&lt;/p&gt;
    &lt;p&gt;“It’s like breaking down Fort Knox,” said Kelly Sutton, a family medicine practice manager in Van Wert, Ohio.&lt;/p&gt;
    &lt;p&gt;In April, Sutton noticed Aetna was downcoding higher complexity office visits. Then Anthem and Humana started doing it, too.&lt;/p&gt;
    &lt;p&gt;She estimates her team is spending about four hours a week on appeals, and several more poring over documentation to see what might be causing the downcoding. But if there is a pattern, it remains elusive.&lt;/p&gt;
    &lt;p&gt;“I think they should spot-check physician’s offices, but whatever this algorithm is, it doesn’t seem to have a rhyme or reason,” Sutton said.&lt;/p&gt;
    &lt;head rend="h2"&gt;‘That is not good for patients’&lt;/head&gt;
    &lt;p&gt;Dr. Adrienne Hollander, a New Jersey-based rheumatologist, said when it comes to being downcoded by one of the main insurers she accepts, she’s simply at their mercy.&lt;/p&gt;
    &lt;p&gt;“They don’t send us a denial saying, ‘We don’t think that this is appropriate.’ They just automatically pay us less,” Hollander said.&lt;/p&gt;
    &lt;p&gt;Unlike Sutton’s practice, Hollander’s multiprovider rheumatology practice does have a representative at the insurer that’s downcoding her.&lt;/p&gt;
    &lt;p&gt;“At least we have an email, a resource, that we can go to and be like, ‘Hey, tell us what’s going on here.’ A lot of people don’t have that,” said Jennifer Buonavolta, the operations director at Hollander’s practice.&lt;/p&gt;
    &lt;p&gt;Even so, Hollander said she’s been unable to get out from under the downcoding program entirely.&lt;/p&gt;
    &lt;p&gt;“If we can’t fix this or renegotiate our contracts, then we stop taking insurers,” she said.&lt;/p&gt;
    &lt;p&gt;Three years ago, her practice made the decision to stop accepting UnitedHealthcare over a different set of reimbursement frustrations. “The problem is that that is not good for patients, because there’s not a ton of rheumatologists,” she said.&lt;/p&gt;
    &lt;p&gt;This year, two states passed legislation regarding downcoding, both focused on transparency.&lt;/p&gt;
    &lt;p&gt;But several other attempts at state legislation have stalled, including bills in Ohio and New Jersey that would have effectively banned automatic downcoding, and one in Connecticut aiming to prohibit specifically AI or algorithm-based downcoding.&lt;/p&gt;
    &lt;p&gt;The AMA recently rolled out a template downcoding bill for interested states.&lt;/p&gt;
    &lt;p&gt;Mukkamala said his primary worry is that what doctors are experiencing now is just the beginning. “When an insurance company gets away with something like this, other insurance companies wouldn’t surprise me a bit if they said, ‘Well, that’s a good idea. We’re going to do that, too.’”&lt;/p&gt;
    &lt;p&gt;For doctors like Sarah Jensen, it’s easy to feel hopeless. In the past year, she’s sent letters to Missouri’s insurance commissioner, to the CEO of Anthem parent company Elevance, and to the American Academy of Dermatology. No one has been able to offer relief.&lt;/p&gt;
    &lt;p&gt;The share of doctors working in private practices has plummeted in recent years, driven in large part by the desire for better reimbursement rates. Jensen is coming to the reluctant conclusion that her days in private practice might, too, be numbered: She’s been talking to private equity about the idea of selling her practice.&lt;/p&gt;
    &lt;p&gt;“It’s like death by a thousand cuts,” she said.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45526754</guid><pubDate>Thu, 09 Oct 2025 12:36:05 +0000</pubDate></item><item><title>Show HN: I built a web framework in C</title><link>https://github.com/ashtonjamesd/lavandula</link><description>&lt;doc fingerprint="f38f151f5efdcd8"&gt;
  &lt;main&gt;
    &lt;p&gt;Lavandula is a lightweight, fast, and intuitive C web framework designed for building modern web applications quickly. It focuses on simplicity, performance, and productivity, providing all the essentials without the bloat of heavier frameworks.&lt;/p&gt;
    &lt;code&gt;#include "lavandula.h" 

// define a route for your app
appRoute(home) {
  return ok("Hello, World");
}

int main() {
  // initialise your app
  App app = createApp();

  // register a route in your app
  get(&amp;amp;app, "/home", home);

  // run the app
  runApp(&amp;amp;app);
}&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Controller and routing system&lt;/item&gt;
      &lt;item&gt;HTTP endpoint support (GET, POST, etc)&lt;/item&gt;
      &lt;item&gt;Controller local/global middleware pipeline&lt;/item&gt;
      &lt;item&gt;Minimal dependencies (pure C)&lt;/item&gt;
      &lt;item&gt;Quick project scaffolding via the CLI&lt;/item&gt;
      &lt;item&gt;Built-in unit testing framework&lt;/item&gt;
      &lt;item&gt;Environment variable support&lt;/item&gt;
      &lt;item&gt;Built-in logging&lt;/item&gt;
      &lt;item&gt;SQLite integration&lt;/item&gt;
      &lt;item&gt;Built-in JSON library&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;HTTP JSON body parsing&lt;/item&gt;
      &lt;item&gt;Session cookies&lt;/item&gt;
      &lt;item&gt;CORS policy configuration&lt;/item&gt;
      &lt;item&gt;Lavender ORM&lt;/item&gt;
      &lt;item&gt;Embedded Lavandula (ELA) HTML templating engine&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rate Limiting&lt;/item&gt;
      &lt;item&gt;Static file serving&lt;/item&gt;
      &lt;item&gt;PostgreSL, MySQL integrations, etc&lt;/item&gt;
      &lt;item&gt;Potential dependency injection framework&lt;/item&gt;
      &lt;item&gt;Route/Available endpoint listing&lt;/item&gt;
      &lt;item&gt;JSON model and function scaffolding &lt;list rend="ul"&gt;&lt;item&gt;lavu model User name:string age:int&lt;/item&gt;&lt;item&gt;generates User struct, JSON serialization, CRUD endpoints in user_controller.c&lt;/item&gt;&lt;item&gt;URL parameter parsing and routing&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To install Lavandula, follow these setps.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Clone the repository&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/ashtonjamesd/lavandula.git
cd lavandula&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Run the install script&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;./install.sh&lt;/code&gt;
    &lt;p&gt;You should see the following:&lt;/p&gt;
    &lt;code&gt;[SUCCESS] 🎉 Lavandula installation completed!

Quick Start:
 lavu new my-project # Create a new project
 cd my-project
 lavu run # Run your project

Documentation:
 GitHub: https://github.com/ashtonjamesd/lavandula&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Finish&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You should now be able to run the Lavu CLI tool. Refer to &lt;code&gt;api.md&lt;/code&gt; for how to use Lavu.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Create a new project&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;lavu new myProject
&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;code&gt;Setting up Lavandula project 'myProject'...

-&amp;gt; Created myProject/lavandula.yml
-&amp;gt; Created myProject/app/app.c
-&amp;gt; Created myProject/app/controllers/home.c
-&amp;gt; Created myProject/app/routes.c
-&amp;gt; Created myProject/makefile
-&amp;gt; Created myProject/tests/tests.c

🎉 Lavandula project 'myProject' setup finished successfully!

Next steps:
  1. cd myProject
  2. lavu run
&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Run&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;lavu run
&lt;/code&gt;
    &lt;p&gt;Your application will run on http://localhost:3000/.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Read the docs&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Contributions are welcome. Feel free to submit pull requests or open issues for feature requests or bugs.&lt;/p&gt;
    &lt;p&gt;Some things that probably need looking at are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;memory leaks&lt;/item&gt;
      &lt;item&gt;outdated and unfinished documentation (API changes warrant a docs update)&lt;/item&gt;
      &lt;item&gt;The JSON library does not currently support nested lists&lt;/item&gt;
      &lt;item&gt;Some tests need to be written...&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Lavandula is registered under the MIT License.e&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45526890</guid><pubDate>Thu, 09 Oct 2025 12:45:28 +0000</pubDate></item><item><title>Nobel Prize in Literature 2025: László Krasznahorkai</title><link>https://www.nobelprize.org/prizes/literature/2025/press-release/</link><description>&lt;doc fingerprint="5757b33da9153196"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Press release&lt;/head&gt;
    &lt;p&gt;English&lt;lb/&gt;English [pdf]&lt;lb/&gt;Swedish&lt;lb/&gt;Swedish [pdf]&lt;/p&gt;
    &lt;p&gt;The Permanent Secretary&lt;/p&gt;
    &lt;p&gt;Press Release&lt;lb/&gt;9 October 2025&lt;/p&gt;
    &lt;head rend="h2"&gt;The Nobel Prize in Literature 2025&lt;/head&gt;
    &lt;head rend="h2"&gt;László Krasznahorkai&lt;/head&gt;
    &lt;p&gt;The Nobel Prize in Literature for 2025 is awarded to the Hungarian author László Krasznahorkai,&lt;/p&gt;
    &lt;p&gt;“for his compelling and visionary oeuvre that, in the midst of apocalyptic terror, reaffirms the power of art”.&lt;/p&gt;
    &lt;head rend="h3"&gt;Nobel Prize announcements 2025&lt;/head&gt;
    &lt;p&gt;Don't miss the Nobel Prize announcements 6–13 October. All announcements are streamed live here on nobelprize.org.&lt;/p&gt;
    &lt;head rend="h3"&gt;Explore prizes and laureates&lt;/head&gt;
    &lt;p&gt; Look for popular awards and laureates in different fields, and discover the history of the Nobel Prize. &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45527003</guid><pubDate>Thu, 09 Oct 2025 12:54:18 +0000</pubDate></item><item><title>The C++ programmer and educator Rainer Grimm has passed away</title><link>https://www.modernescpp.com/index.php/my-als-journey-31-31-the-end/</link><description>&lt;doc fingerprint="2b53115b8ae3f62f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;My ALS Journey (31/31): The End&lt;/head&gt;
    &lt;p&gt;Dear readers, we are extremely sad to inform you that Rainer passed away on October 6, 2025, surrounded by his closest family. After suffering from life-threatening pneumonia, Rainer decided against further life-sustaining measures that would have severely restricted his life. He passed away peacefully, accompanied by his family.&lt;/p&gt;
    &lt;p&gt;&amp;gt;&amp;gt; Rainers Journey with ALS &amp;lt;&amp;lt;&lt;/p&gt;
    &lt;head rend="h2"&gt;Rainers lifework&lt;/head&gt;
    &lt;p&gt;Despite his progressive ALS disease, Rainer remained full of energy and drive until the very end and continued to work on his two life goals:&lt;/p&gt;
    &lt;head rend="h2"&gt;1. Creating Value for the C++ Community&lt;/head&gt;
    &lt;p&gt;First, to share his knowledge and experience of C++ with you through his blog modernescpp.com, his mentoring program, and digital participation in conferences.&lt;/p&gt;
    &lt;p&gt;His last digital appearance was at CppCon 2025, where he had given lectures both digitally and in person in previous years. More recently, Cippi, an initiative founded by Rainer and his wife, Beatrix, has visited conferences such as CppCon and will continue to do so.&lt;/p&gt;
    &lt;p&gt;You can read more about Cippi’s adventures on her blog.&lt;/p&gt;
    &lt;p&gt;If you would like Cippi to visit your conference, please send an email to beatrix.grimm-jaud@ModernesCpp.de.&lt;/p&gt;
    &lt;head rend="h2"&gt;2. Raising awareness for ALS&lt;/head&gt;
    &lt;p&gt;Second, to raise awareness of ALS and other neurological diseases in order to collect donations for ALS research.&lt;/p&gt;
    &lt;p&gt;As part of this, Rainer attended a charity run organized in his name by TV Rottenburg on September 28, which raised over €6,000 in donations for ALS research.&lt;lb/&gt;Since Rainer himself had been active as a running coach at TVR for over 15 years before his illness and had coached several athletes to the German championships, he had the opportunity to meet many long-time acquaintances and friends there once again.&lt;/p&gt;
    &lt;p&gt;Over the past two years, Rainer has repeatedly offered his books at a significant discount and donated all proceeds to ALS research.&lt;/p&gt;
    &lt;head rend="h2"&gt;Modernes C++ Mentoring&lt;/head&gt;
    &lt;p&gt;Do you want to stay informed: Subscribe.&lt;/p&gt;
    &lt;head rend="h2"&gt;Thank You!&lt;/head&gt;
    &lt;p&gt;As a family, we would like to thank you, the C++ community, for being like a second family to Rainer over the years and providing him with such an essential source of support, both professionally and privately. Especially in the last two years, when ALS took away Rainer’s chance to attend conferences in person, you showed your sympathy and, not least through your support for Rainer’s Cippi initiative, underlined what a great, strong, and empathetic community you are!&lt;/p&gt;
    &lt;p&gt;We cannot say at this point what will happen to his blog, his mentoring, or his book in progress, Modern C++26, but we will keep you informed.&lt;/p&gt;
    &lt;p&gt;Thank you very much!&lt;lb/&gt;Beatrix, Juliette, and Marius&lt;/p&gt;
    &lt;p&gt;Thanks a lot to my Patreon Supporters: Matt Braun, Roman Postanciuc, Tobias Zindl, G Prvulovic, Reinhold Dröge, Abernitzke, Frank Grimm, Sakib, Broeserl, António Pina, Sergey Agafyin, Андрей Бурмистров, Jake, GS, Lawton Shoemake, Jozo Leko, John Breland, Venkat Nandam, Jose Francisco, Douglas Tinkham, Kuchlong Kuchlong, Robert Blanch, Truels Wissneth, Mario Luoni, Friedrich Huber, lennonli, Pramod Tikare Muralidhara, Peter Ware, Daniel Hufschläger, Alessandro Pezzato, Bob Perry, Satish Vangipuram, Andi Ireland, Richard Ohnemus, Michael Dunsky, Leo Goodstadt, John Wiederhirn, Yacob Cohen-Arazi, Florian Tischler, Robin Furness, Michael Young, Holger Detering, Bernd Mühlhaus, Stephen Kelley, Kyle Dean, Tusar Palauri, Juan Dent, George Liao, Daniel Ceperley, Jon T Hess, Stephen Totten, Wolfgang Fütterer, Matthias Grün, Ben Atakora, Ann Shatoff, Rob North, Bhavith C Achar, Marco Parri Empoli, Philipp Lenk, Charles-Jianye Chen, Keith Jeffery, Matt Godbolt, Honey Sukesan, bruce_lee_wayne, Silviu Ardelean, schnapper79, Seeker, and Sundareswaran Senthilvel.&lt;/p&gt;
    &lt;p&gt;Thanks, in particular, to Jon Hess, Lakshman, Christian Wittenhorst, Sherhy Pyton, Dendi Suhubdy, Sudhakar Belagurusamy, Richard Sargeant, Rusty Fleming, John Nebel, Mipko, Alicja Kaminska, Slavko Radman, and David Poole.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;My special thanks to Embarcadero&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;My special thanks to PVS-Studio&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;My special thanks to Tipi.build&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;My special thanks to Take Up Code&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;My special thanks to SHAVEDYAKS&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Modernes C++ GmbH&lt;/p&gt;
    &lt;p&gt;Modernes C++ Mentoring (English)&lt;/p&gt;
    &lt;p&gt;Rainer Grimm&lt;lb/&gt; Yalovastraße 20&lt;lb/&gt; 72108 Rottenburg&lt;/p&gt;
    &lt;p&gt;Mail: schulung@ModernesCpp.de&lt;/p&gt;
    &lt;p&gt;Mentoring: www.ModernesCpp.org&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45527156</guid><pubDate>Thu, 09 Oct 2025 13:05:54 +0000</pubDate></item><item><title>Introducing Figure 03</title><link>https://www.figure.ai/news/introducing-figure-03</link><description>&lt;doc fingerprint="ec95cddb80b22ecf"&gt;
  &lt;main&gt;
    &lt;p&gt;Today we’re introducing Figure 03, our 3rd generation humanoid robot. Figure 03 is designed for Helix, the home, and the world at scale. Our goal is to deliver a truly general-purpose robot - one that can perform human-like tasks and learn directly from people. To realize this vision, our engineering and design teams completed a ground-up hardware and software redesign to ship Figure 03 for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Helix: Figure 03 features a completely redesigned sensory suite and hand system which is purpose-built to enable Helix - Figure's proprietary vision-language-action AI.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The home: Figure 03 has several new features, including soft goods, wireless charging, improved audio system for voice reasoning, and battery safety advancements that make it safer and easier to use in a home environment.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mass manufacturing: Figure 03 was engineered from the ground-up for high-volume manufacturing. In order to scale, we established a new supply chain and entirely new process for manufacturing humanoid robots at BotQ.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The world at scale: The lower manufacturing cost and the advancements made for Helix have significant benefits for commercial applications.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Designed for Helix&lt;/head&gt;
    &lt;p&gt;There’s no path to scaling humanoid robots without AI. That’s why we built Figure 03 around one goal - to enable true reasoning throughout the world using Helix. Figure 03 introduces a fully redesigned sensory suite and hand system, purpose-built to bring Helix to life.&lt;/p&gt;
    &lt;p&gt;Figure 03 introduces a next-generation vision system engineered for high-frequency visuomotor control. Its new camera architecture delivers twice the frame rate, one-quarter the latency, and a 60% wider field of view per camera - all within a more compact form factor. Combined with an expanded depth of field, this architecture provides Helix with a denser, more stable perceptual stream. These advancements are essential for intelligent navigation and precise manipulation in complex, cluttered spaces such as homes.&lt;/p&gt;
    &lt;p&gt;Each hand now integrates an embedded palm camera with a wide field of view and low-latency sensing, which offers redundant, close-range visual feedback during grasps. These cameras allow Helix to maintain visual awareness even when the main cameras are occluded (i.e. when reaching into a cabinet or working in confined spaces) and enable continuous, adaptive control in real time.&lt;/p&gt;
    &lt;p&gt;The Figure 03 hands represent a major leap in compliant and tactile design. Softer, more adaptive fingertips increase surface contact area, enabling more stable grasps across objects of varied shapes and sizes. After surveying existing market options, Figure found that current tactile sensors had inherent limitations that could not withstand real-world use. This led to the internal development of our first-generation tactile sensor, guided by three principles: extreme durability, long-term reliability, and high-fidelity sensing.&lt;/p&gt;
    &lt;p&gt;Each fingertip sensor can detect forces as small as three grams of pressure - sensitive enough to register the weight of a paperclip resting on your finger. This precision enables Helix to distinguish between a secure grip and an impending slip before it occurs, allowing fine-grained, dexterous control over fragile, irregular, or moving objects.&lt;/p&gt;
    &lt;p&gt;Figure 03 also includes 10 Gbps mmWave data offload capability, allowing the entire fleet to upload terabytes of data for continuous learning and improvement. Together, these advancements position Figure 03 as uniquely capable of large-scale, end-to-end pixels-to-action learning.&lt;/p&gt;
    &lt;head rend="h2"&gt;Designed for the Home&lt;/head&gt;
    &lt;p&gt;To operate effectively in the home, a robot must work seamlessly alongside people in their daily environments. With this in mind, Figure 03 introduces several design improvements focused on safety. It features strategically placed multi-density foam to protect against pinch points, and is covered in soft textiles rather than hard machined parts. Figure 03 also has 9% less mass and significantly less volume than Figure 02, making it easier to maneuver through household spaces.&lt;/p&gt;
    &lt;p&gt;The Figure 03 battery pushes the bounds for robot battery safety and incorporates multiple layers of protection against abuse or malfunction, including safeguards at the Battery Management System (BMS), cell, interconnect, and pack levels. The battery has already achieved certification to the UN38.3 standard.&lt;/p&gt;
    &lt;p&gt;Beyond safety, Figure 03 is designed for everyday usability. The soft goods are fully washable and can be removed or replaced without tools, allowing quick and easy swaps. The robot can also be customized with various clothing options, including garments made from cut-resistant and durable materials.&lt;/p&gt;
    &lt;p&gt;To make it easier to communicate naturally with the robot, Figure 03 features an upgraded audio hardware system for better real time speech-to-speech. Compared with Figure 02, its speaker is twice the size and nearly four times more powerful, while the microphone has been repositioned for improved performance and clarity.&lt;/p&gt;
    &lt;p&gt;Continuing our vision for a fully autonomous, wire-free system, Figure 03 is capable of wireless inductive charging alongside wireless data offload. Charging coils in the robot’s feet allow it to simply step onto a wireless stand and charge at 2 kW. In a home setting, this means the robot can automatically dock and recharge itself as needed throughout the day.&lt;/p&gt;
    &lt;head rend="h2"&gt;Designed for Mass Manufacturing&lt;/head&gt;
    &lt;p&gt;Humanoid robots have traditionally been designed as engineering prototypes which are time consuming and expensive to produce. Figure 03 is our first robot engineered from the ground-up for high-volume manufacturing. We achieved this through three major initiatives:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Design and process reinvention&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Establishing an entirely new supply chain&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The invention of BotQ, our high-volume manufacturing facility&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Moving from Figure 02 to Figure 03 required redesigning nearly every component of the robot with manufacturability and cost in mind. The mechanical and electrical engineering teams aggressively reduced part count, assembly steps, and any components that were not absolutely critical to meet design requirements. While Figure 02 was primarily designed to be manufactured with CNC machining, Figure 03 relies heavily on tooled processes such as die-casting, injection molding, and stamping. This shift demanded a significant up-front investment in tooling, but the payoff is clear: each Figure 03 unit now costs dramatically less to build, with the economics improving as volumes grow.&lt;/p&gt;
    &lt;p&gt;To scale Figure 03, Figure had to build an entirely new supply chain for an industry where one does not currently exist. Figure chose to vertically integrate across many critical module builds including actuators, batteries, sensors, structures, and electronics, all of which were designed completely in-house. For individual components, Figure strategically identified and partnered with suppliers capable of meeting the required volumes, timelines, and strict quality standards demanded by the team. The result of this year-long effort is a global network of partners who can grow alongside Figure and meet production goals of thousands and eventually millions of parts under an aggressive ramp schedule.&lt;/p&gt;
    &lt;p&gt;BotQ is Figure’s dedicated manufacturing facility designed to scale robot production. BotQ’s first-generation manufacturing line will initially be capable of producing up to 12,000 humanoid robots per year, with the goal of producing a total of 100,000 robots over the next four years. Instead of relying on contract manufacturers, Figure brought production of its most critical systems in-house to maintain tight control over quality, iteration, and speed. The facility is equipped with state-of-the-art systems and digital integrations, anchored by our internally developed Manufacturing Execution System (MES). Every subassembly and final assembly passes through this line with full traceability, ensuring quality, repeatability, and continuous improvement.&lt;/p&gt;
    &lt;head rend="h2"&gt;Designed for the World at Scale&lt;/head&gt;
    &lt;p&gt;Figure’s focus on the home market in no way detracts from the potential of Figure 03 for the commercial market. By solving for the variability and intractability of the home, Figure is developing a truly general-purpose product that can do the widest possible range of tasks in the workforce.&lt;/p&gt;
    &lt;p&gt;Figure 03 is well suited for commercial applications for several reasons. The actuators can perform at 2x faster speeds with improved torque density (nm/kg). The most significant result of this is our ability to pick and place items at faster speeds.&lt;/p&gt;
    &lt;p&gt;The improvements to the hands and sensory suite made for Helix are of major significance for commercial use cases. With the camera and perception system upgrades, Figure 03 will be able to intelligently navigate commercial environments and execute precise manipulation. The changes to the hands highlighted above (added compliance, fingertip surface area, tactile sensing) enable better and more stable grasps across an array of objects such as small pieces of sheet metal and deformable poly bags.&lt;/p&gt;
    &lt;p&gt;Thanks to inductive charging, Figure 03 is capable of near-continuous operation as long as it can step onto a charging mat for a certain period of time during the use case. The fast wireless data offload also means that the robot can offload seamlessly during shift breaks just by returning to the dock.&lt;/p&gt;
    &lt;p&gt;Commercial customers can also design distinct uniforms for their Figure 03 fleet, with the option to use more durable, or cut-proof materials, and make other design changes for specific environments. New side screens on Figure 03 even allow quick identification across large fleets and can be fully customized to match each customer’s branding or operational needs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Figure 03 represents an unprecedented advancement in taking humanoid robots from experimental prototypes to deployable, scalable products. By uniting advanced perception and tactile intelligence with home-safe design and mass-manufacturing readiness, Figure has built a platform capable of learning, adapting, and working across both domestic and commercial settings. Designed for Helix, the home, and the world at scale, Figure 03 establishes the foundation for true general-purpose robotics - one capable of transforming how people live and work.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45527402</guid><pubDate>Thu, 09 Oct 2025 13:27:14 +0000</pubDate></item></channel></rss>