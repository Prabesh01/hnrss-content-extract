<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 08 Jan 2026 19:11:43 +0000</lastBuildDate><item><title>Open Infrastructure Map</title><link>https://openinframap.org</link><description>&lt;doc fingerprint="6d0a85331c526182"&gt;
  &lt;main&gt;
    &lt;p&gt;You must have Javascript enabled to view Open Infrastructure Map Open Infrastructure Map about stats exports&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46536866</guid><pubDate>Thu, 08 Jan 2026 03:31:12 +0000</pubDate></item><item><title>Go.sum is not a lockfile</title><link>https://words.filippo.io/gosum/</link><description>&lt;doc fingerprint="b6383f772da5a398"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;go.sum Is Not a Lockfile&lt;/head&gt;
    &lt;p&gt;I need everyone to stop looking at &lt;code&gt;go.sum&lt;/code&gt;, especially to analyze dependency graphs. It is not a “lockfile,”1 and it has zero semantic effects on version resolution. There is truly no use case for ever parsing it outside of cmd/go.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;go.sum&lt;/code&gt; is only a local cache for the Go Checksum Database. It’s a map of module versions to their cryptographic hashes. Those versions may or may not be in use; it doesn’t matter to package resolution.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;go.sum&lt;/code&gt; was not even enabled by default in the original modules design, precisely because it has no observable effect on builds!2 Its (important) purpose is exclusively tightening the security story: the Checksum Database ensures the whole ecosystem shares the same contents for a given module version, regardless of how it is downloaded, and &lt;code&gt;go.sum&lt;/code&gt; makes that guarantee local and self-contained.&lt;/p&gt;
    &lt;p&gt;Instead, just look at &lt;code&gt;go.mod&lt;/code&gt;. It lists the precise version at which all dependencies are built. Since Go 1.17 (released August 2021), it includes all transitive dependencies needed to build the main module and its tests.3&lt;/p&gt;
    &lt;p&gt;You can either parse &lt;code&gt;go.mod&lt;/code&gt; with golang.org/x/mod/modfile, run &lt;code&gt;go mod edit -json&lt;/code&gt; to get its JSON representation,4 or parse it according to its specification.&lt;/p&gt;
    &lt;p&gt;This is the end of the Public Service Announcement. Read on for some &lt;code&gt;go.mod&lt;/code&gt; nerdery.&lt;/p&gt;
    &lt;head rend="h2"&gt;Manifests and lockfiles&lt;/head&gt;
    &lt;p&gt;The enduring confusion around &lt;code&gt;go.mod&lt;/code&gt; and &lt;code&gt;go.sum&lt;/code&gt; is due to the fact that most other languages also have two package-related files, but theirs both matter to version resolution. These two files are usually called manifest and lockfile.&lt;/p&gt;
    &lt;p&gt;The manifest (e.g. &lt;code&gt;pyproject.toml&lt;/code&gt;, &lt;code&gt;package.json&lt;/code&gt;, &lt;code&gt;Cargo.toml&lt;/code&gt;) usually lists some dependencies along with potentially complex rules for which versions are supported. These rules usually apply transitively to dependents, making version resolution extremely hard and/or slow in the general case, and sometimes unsolvable. The manifest is not always5 guaranteed to list all direct dependencies, and no automated mechanism ensures your code actually works with e.g. the minimum allowed manifest version of its dependencies.6&lt;/p&gt;
    &lt;p&gt;The lockfile (e.g. &lt;code&gt;uv.lock&lt;/code&gt;, &lt;code&gt;package-lock.json&lt;/code&gt;, &lt;code&gt;Cargo.lock&lt;/code&gt;) is a relatively recent innovation in some ecosystems, and it lists the actual versions used in the most recent build. It is not really human-readable, and is ignored by dependents, allowing the rapid spread of supply-chain attacks.&lt;/p&gt;
    &lt;p&gt;I honestly find the manifest version ranges essentially useless (because the lower bounds are not normally tested and hence largely incorrect, while I never had a use case for complex upper bounds). I also get endlessly confused trying to remember which commands modify the lockfile (and when/why) and which ones respect it.&lt;/p&gt;
    &lt;p&gt;In Go, &lt;code&gt;go.mod&lt;/code&gt; serves as both manifest and lockfile, and more: it lists all dependencies, direct and transitive, and their exact version to be used when the module is the main module. Semantic versioning is assumed, and those versions are also the minimum versions applied to dependents’ module graphs. Different major versions of the same module are considered essentially separate modules.&lt;/p&gt;
    &lt;p&gt;Notice how there is no way to accidentally use a feature introduced in a version that your dependents won’t have. Also, when adding a dependency, you don’t automatically get the latest—potentially untested/compromised—version of all its dependencies. Finally, there can’t be diamond dependency conflicts.&lt;/p&gt;
    &lt;p&gt;All that with a single, human-readable file: &lt;code&gt;go.mod&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;All &lt;code&gt;go&lt;/code&gt; commands take a &lt;code&gt;-mod&lt;/code&gt; flag. If set to &lt;code&gt;mod&lt;/code&gt;, missing dependencies can be added to &lt;code&gt;go.mod&lt;/code&gt; automatically if necessary, and partial manual changes are reconciled. If set to &lt;code&gt;readonly&lt;/code&gt;, those are errors. &lt;code&gt;go mod tidy&lt;/code&gt; and (effectively) &lt;code&gt;go get&lt;/code&gt; default to &lt;code&gt;mod&lt;/code&gt;; all other commands default to &lt;code&gt;readonly&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Go modules truly don’t get enough credit for how much simpler they are compared to the alternatives. In other ecosystems, package resolution time going down below 1s is celebrated (and is indeed an impressive technical achievement given the design’s requirements!). In Go, no one ever noticed package resolution happening, so there is nothing to celebrate.&lt;/p&gt;
    &lt;p&gt;For more ecosystem feature appreciation posts, follow me on Bluesky at @filippo.abyssdomain.expert or on Mastodon at @filippo@abyssdomain.expert.&lt;/p&gt;
    &lt;head rend="h2"&gt;The picture&lt;/head&gt;
    &lt;p&gt;I had a great time at 39c3 during the holidays. The Chaos Communication Congress is a magical place with a very strict photo policy, so it’s pretty hard to convey its atmosphere. This is the best I could do without recognizable humans in the frame. In Fairy Dust we trust!&lt;/p&gt;
    &lt;p&gt;My work is made possible by Geomys, an organization of professional Go maintainers, which is funded by Smallstep, Ava Labs, Teleport, Tailscale, and Sentry. Through our retainer contracts, they ensure the sustainability and reliability of our open source maintenance work and get a direct line to my expertise and that of the other Geomys maintainers. (Learn more in the Geomys announcement.) Here are a few words from some of them!&lt;/p&gt;
    &lt;p&gt;Teleport — For the past five years, attacks and compromises have been shifting from traditional malware and security breaches to identifying and compromising valid user accounts and credentials with social engineering, credential theft, or phishing. Teleport Identity is designed to eliminate weak access patterns through access monitoring, minimize attack surface with access requests, and purge unused permissions via mandatory access reviews.&lt;/p&gt;
    &lt;p&gt;Ava Labs — We at Ava Labs, maintainer of AvalancheGo (the most widely used client for interacting with the Avalanche Network), believe the sustainable maintenance and development of open source cryptographic protocols is critical to the broad adoption of blockchain technology. We are proud to support this necessary and impactful work through our ongoing sponsorship of Filippo and his team.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;In the sense of the word as introduced by the original&lt;/p&gt;&lt;code&gt;Gemfile.lock&lt;/code&gt;, which only locked the selected versions. Most lockfiles these days also include the cryptographic checksums of version contents, which in Go is instead handled by the Go Checksum Database and&lt;code&gt;go.sum&lt;/code&gt;. ↩&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I still think it’s important and it was the first thing I remember advocating for when I joined the Go team, because it makes the module cryptographically self-contained, and because the Go Checksum Database transparency story is not great in ephemeral environments like CI. These are security effects, though, not semantic ones. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;These are the only dependencies you care about, even for security. If the main module imports&lt;/p&gt;&lt;code&gt;example.com/mod1/pkg1&lt;/code&gt;and a separate&lt;code&gt;example.com/mod1/pkg2&lt;/code&gt;imports&lt;code&gt;example.com/mod2&lt;/code&gt;, there is no way for&lt;code&gt;example.com/mod2&lt;/code&gt;to affect the build or run code on the developer’s machine, so you don’t need to consider it a dependency. This is actually very powerful, allowing libraries to segregate dependencies (e.g. the AWS SDK) in optional packages, reducing the transitive trust tree of dependents that don’t use that feature. ↩↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Why not&lt;/p&gt;&lt;code&gt;go list -m all&lt;/code&gt;, you ask? Because that prints the whole module graph, which includes modules that don’t contribute to the build3 and are not included in&lt;code&gt;go.mod&lt;/code&gt;. A closer approximation would be&lt;code&gt;go list -f '{{.Module}}' all&lt;/code&gt;, but this command applies the local build constraints, like GOOS/GOARCH. There is an open proposal for a flag to do&lt;code&gt;go.mod&lt;/code&gt;-like resolution in&lt;code&gt;go list&lt;/code&gt;. ↩&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I am told that Cargo enforces it, and some other package managers have a warning. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Cargo has an opt-in, unstable, not recommended mode for it. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46537095</guid><pubDate>Thu, 08 Jan 2026 04:10:34 +0000</pubDate></item><item><title>Project Patchouli: Open-source electromagnetic drawing tablet hardware</title><link>https://patchouli.readthedocs.io/en/latest/</link><description>&lt;doc fingerprint="2791b911676f82d7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Project Patchouli&lt;/head&gt;
    &lt;p&gt;Open-Source EMR Pen Tablet Hardware Implementation and Documentation&lt;/p&gt;
    &lt;p&gt;Patchouli is an open-source electro-magnetic drawing tablet hardware implementation, including a coil array, an RF front end built using commercially available parts, and digital signal processing algorithms. The design is compatible with most commercial pens from different vendors, offering an ultra-low-latency pen input experience for your customized hardware projects.&lt;/p&gt;
    &lt;p&gt;In addition, this project aims to provide a comprehensive documentation of the EMR technology, including the mechanism, circuit implementation, signal processing algorithms, and the pen protocol of different product lines from different vendors.&lt;/p&gt;
    &lt;p&gt;Project Code / Hardware Repository: GitLab&lt;/p&gt;
    &lt;head rend="h2"&gt;Updates&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;January 2024, The project started.&lt;/item&gt;
      &lt;item&gt;March 2024, the first small-scale hardware prototype was successfully tested.&lt;/item&gt;
      &lt;item&gt;January 2025, the documentation page was hosted on Read the Docs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Maintainers&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Project Lead: Yukidama&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Community&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reaching out to the maintainers: prj.patchouli@gmail.com&lt;/item&gt;
      &lt;item&gt;Join our public Discord Server&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Sponsorship&lt;/head&gt;
    &lt;p&gt;This project is sponsored by the NLnet Foundation NGI Zero Core Fund. Learn more about it here: Project Patchouli&lt;/p&gt;
    &lt;head rend="h2"&gt;License&lt;/head&gt;
    &lt;p&gt;Project Patchouli Documentation by Yukidama and other project members is licensed under Creative Commons Attribution 4.0 International&lt;/p&gt;
    &lt;p&gt;All images and other resource files in this project, unless otherwise specified, are created by the project team and are licensed under the same CC BY 4.0 license.&lt;/p&gt;
    &lt;p&gt;The hardware design is released under the CERN Open Source Hardware License strongly-reciprocal variant, CERN-OHL-S. A copy of the license is provided in the source repository. Additionally, a user guide of the license is provided on ohwr.org.&lt;/p&gt;
    &lt;p&gt;All program code, unless otherwise specified, is licensed under the GPLv3 license.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;This project is under active development.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46537489</guid><pubDate>Thu, 08 Jan 2026 05:20:05 +0000</pubDate></item><item><title>A closer look at a BGP anomaly in Venezuela</title><link>https://blog.cloudflare.com/bgp-route-leak-venezuela/</link><description>&lt;doc fingerprint="b6439bf1a821c86b"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;As news unfolds surrounding the U.S. capture and arrest of Venezuelan leader NicolÃ¡s Maduro, a cybersecurity newsletter examined Cloudflare Radar data and took note of a routing leak in Venezuela on January 2.&lt;/p&gt;
      &lt;p&gt;We dug into the data. Since the beginning of December there have been eleven route leak events, impacting multiple prefixes, where AS8048 is the leaker. Although it is impossible to determine definitively what happened on the day of the event, this pattern of route leaks suggests that the CANTV (AS8048) network, a popular Internet Service Provider (ISP) in Venezuela, has insufficient routing export and import policies. In other words, the BGP anomalies observed by the researcher could be tied to poor technical practices by the ISP rather than malfeasance.&lt;/p&gt;
      &lt;p&gt;In this post, weâll briefly discuss Border Gateway Protocol (BGP) and BGP route leaks, and then dig into the anomaly observed and what may have happened to cause it.Â &lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Background: BGP route leaks&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;First, letâs revisit what a BGP route leak is. BGP route leaks cause behavior similar to taking the wrong exit off of a highway. While you may still make it to your destination, the path may be slower and come with delays you wouldnât otherwise have traveling on a more direct route.&lt;/p&gt;
      &lt;p&gt;Route leaks were given a formal definition in RFC7908 as âthe propagation of routing announcement(s) beyond their intended scope.â Intended scope is defined using pairwise business relationships between networks. The relationships between networks, which in BGP we represent using Autonomous Systems (ASes), can be one of the following:Â &lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;customer-provider: A customer pays a provider network to connect them and their own downstream customers to the rest of the Internet&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;peer-peer: Two networks decide to exchange traffic between one another, to each othersâ customers, settlement-free (without payment)&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;In a customer-provider relationship, the provider will announce all routes to the customer. The customer, on the other hand, will advertise only the routes from their own customers and originating from their network directly.&lt;/p&gt;
      &lt;p&gt;In a peer-peer relationship, each peer will advertise to one another only their own routes and the routes of their downstream customers.Â &lt;/p&gt;
      &lt;p&gt;These advertisements help direct traffic in expected ways: from customers upstream to provider networks, potentially across a single peering link, and then potentially back down to customers on the far end of the path from their providers.Â &lt;/p&gt;
      &lt;p&gt;A valid path would look like the following that abides by the valley-free routing rule:Â &lt;/p&gt;
      &lt;p&gt;A route leak is a violation of valley-free routing where an AS takes routes from a provider or peer and redistributes them to another provider or peer. For example, a BGP path should never go through a âvalleyâ where traffic goes up to a provider, and back down to a customer, and then up to a provider again. There are different types of route leaks defined in RFC7908, but a simple one is the Type 1: Hairpin route leak between two provider networks by a customer.Â &lt;/p&gt;
      &lt;p&gt;In the figure above, AS64505 takes routes from one of its providers and redistributes them to their other provider. This is unexpected, since we know providers should not use their customer as an intermediate IP transit network. AS64505 would become overwhelmed with traffic, as a smaller network with a smaller set of backbone and network links than its providers. This can become very impactful quickly.Â &lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Route leak by AS8048 (CANTV)&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Now that we have reminded ourselves what a route leak is in BGP, letâs examine what was hypothesizedÂ in the newsletter post. The post called attention to a few route leak anomalies on Cloudflare Radar involving AS8048. On the Radar page for this leak, we see this information:&lt;/p&gt;
      &lt;p&gt;We see the leaker AS, which is AS8048 â CANTV, Venezuelaâs state-run telephone and Internet Service Provider. We observe that routes were taken from one of their providers AS6762 (Sparkle, an Italian telecom company) and then redistributed to AS52320 (V.tal GlobeNet, a Colombian network service provider). This is definitely a route leak.Â &lt;/p&gt;
      &lt;p&gt;The newsletter suggests âBGP shenanigansâ and posits that such a leak could be exploited to collect intelligence useful to government entities.Â &lt;/p&gt;
      &lt;p&gt;While we canât say with certainty what caused this route leak, our data suggests that its likely cause was more mundane. Thatâs in part because BGP route leaks happen all of the time, and they have always been part of the Internet â most often for reasons that arenât malicious.&lt;/p&gt;
      &lt;p&gt;To understand more, letâs look closer at the impacted prefixes and networks. The prefixes involved in the leak were all originated by AS21980 (Dayco Telecom, a Venezuelan company):&lt;/p&gt;
      &lt;p&gt;The prefixes are also all members of the same 200.74.224.0/20 subnet, as noted by the newsletter author. Much more intriguing than this, though, is the relationship between the originating network AS21980 and the leaking network AS8048: AS8048 is a provider of AS21980.Â &lt;/p&gt;
      &lt;p&gt;The customer-provider relationship between AS8048 and AS21980 is visible in both Cloudflare Radar and bgp.tools AS relationship interference data. We can also get a confidence score of the AS relationship using the monocle tool from BGPKIT, as you see here:Â &lt;/p&gt;
      &lt;p&gt;
        &lt;code&gt;âÂ  ~ monocle as2rel 8048 21980
Explanation:
- connected: % of 1813 peers that see this AS relationship
- peer: % where the relationship is peer-to-peer
- as1_upstream: % where ASN1 is the upstream (provider)
- as2_upstream: % where ASN2 is the upstream (provider)&lt;/code&gt;
      &lt;/p&gt;
      &lt;p&gt;
        &lt;code&gt;Data source: https://data.bgpkit.com/as2rel/as2rel-latest.json.bz2&lt;/code&gt;
      &lt;/p&gt;
      &lt;p&gt;
        &lt;code&gt;ââââââââ¬ââââââââ¬ââââââââââââ¬âââââââ¬âââââââââââââââ¬âââââââââââââââ®
âÂ asn1 â asn2Â  â connectedÂ â peer â as1_upstreamÂ âÂ as2_upstream â
ââââââââ¼ââââââââ¼ââââââââââââ¼âââââââ¼âââââââââââââââ¼âââââââââââââââ¤
â 8048 â 21980 â    9.9% Â  â 0.6%Â â     9.4%Â   Â  â 0.0% Â  Â  Â  Â  â
â°âââââââ´ââââââââ´ââââââââââââ´âââââââ´âââââââââââââââ´âââââââââââââââ¯&lt;/code&gt;
      &lt;/p&gt;
      &lt;p&gt;While only 9.9% of route collectors see these two ASes as adjacent, almost all of the paths containing them reflect AS8048 as an upstream provider for AS21980, meaning confidence is high in the provider-customer relationship between the two.&lt;/p&gt;
      &lt;p&gt;Many of the leaked routes were also heavily prepended with AS8048, meaning it would have been potentially less attractive for routing when received by other networks. Prepending is the padding of an AS more than one time in an outbound advertisement by a customer or peer, to attempt to switch traffic away from a particular circuit to another. For example, many of the paths during the leak by AS8048 looked like this: â52320,8048,8048,8048,8048,8048,8048,8048,8048,8048,23520,1299,269832,21980â.Â &lt;/p&gt;
      &lt;p&gt;You can see that AS8048 has sent their AS multiple times in an advertisement to AS52320, because by means of BGP loop prevention the path would never actually travel in and out of AS8048 multiple times in a row. A non-prepended path would look like this: â52320,8048,23520,1299,269832,21980â.Â &lt;/p&gt;
      &lt;p&gt;If AS8048 was intentionally trying to become a man-in-the-middle (MITM) for traffic, why would they make the BGP advertisement less attractive instead of more attractive? Also, why leak prefixes to try and MITM traffic when youâre already a provider for the downstream AS anyway? That wouldnât make much sense.Â &lt;/p&gt;
      &lt;p&gt;The leaks from AS8048 also surfaced in multiple separate announcements, each around an hour apart on January 2, 2026 between 15:30 and 17:45 UTC, suggesting they may have been having network issues that surfaced in a routing policy issue or a convergence-based mishap.Â &lt;/p&gt;
      &lt;p&gt;It is also noteworthy that these leak events begin over twelve hours prior to the U.S. military strikes in Venezuela. Leaks that impact South American networks are common, and we have no reason to believe, based on timing or the other factors I have discussed, that the leak is related to the capture of Maduro several hours later.&lt;/p&gt;
      &lt;p&gt;In fact, looking back the past two months, we can see plenty of leaks by AS8048 that are just like this one, meaning this is not a new BGP anomaly:&lt;/p&gt;
      &lt;p&gt;You can see above in the history of Cloudflare Radarâs route leak alerting pipeline that AS8048 is no stranger to Type 1 hairpin route leaks. Since the beginning of December alone there have been eleven route leak events where AS8048 is the leaker.&lt;/p&gt;
      &lt;p&gt;From this we can draw a more innocent possible explanation about the route leak: AS8048 may have configured too loose of export policies facing at least one of their providers, AS52320. And because of that, redistributed routes belong to their customer even when the direct customer BGP routes were missing. If their export policy toward AS52320 only matched on IRR-generated prefix list and not a customer BGP community tag, for example, it would make sense why an indirect path toward AS6762 was leaked back upstream by AS8048.Â &lt;/p&gt;
      &lt;p&gt;These types of policy errors are something RFC9234 and the Only-to-Customer (OTC) attribute would help with considerably, by coupling BGP more tightly to customer-provider and peer-peer roles, when supported by all routing vendors. I will save the more technical details on RFC9234 for a follow-up blog post.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;The difference between origin and path validation&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;The newsletter also calls out as ânotableâ that Sparkle (AS6762) does not implement RPKI (Resource Public Key Infrastructure) Route Origin Validation (ROV). While it is true that AS6762 appears to have an incomplete deployment of ROV and is flagged as âunsafeâ on isbgpsafeyet.com because of it, origin validation would not have prevented this BGP anomaly in Venezuela.Â &lt;/p&gt;
      &lt;p&gt;It is important to separate BGP anomalies into two categories: route misoriginations, and path-based anomalies. Knowing the difference between the two helps to understand the solution for each. Route misoriginations, often called BGP hijacks, are meant to be fixed by RPKI Route Origin Validation (ROV) by making sure the originator of a prefix is who rightfully owns it. In the case of the BGP anomaly described in this post, the origin AS was correct as AS21980 and only the path was anomalous. This means ROV wouldnât help here.&lt;/p&gt;
      &lt;p&gt;Knowing that, we need path-based validation. This is what Autonomous System Provider Authorization (ASPA), an upcoming draft standard in the IETF, is going to provide. The idea is similar to RPKI Route Origin Authorizations (ROAs) and ROV: create an ASPA object that defines a list of authorized providers (upstreams) for our AS, and everyone will use this to invalidate route leaks on the Internet at various vantage points. Using a concrete example, AS6762 is a Tier-1 transit-free network, and they would use the special reserved âAS0â member in their ASPA signed object to communicate to the world that they have no upstream providers, only lateral peers and customers. Then, AS52320, the other provider of AS8048, would see routes from their customer with â6762â in the path and reject them by performing an ASPA verification process.&lt;/p&gt;
      &lt;p&gt;ASPA is based on RPKI and is exactly what would help prevent route leaks similar to the one we observed in Venezuela.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;A safer BGP, built togetherÂ &lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;We felt it was important to offer an alternative explanation for the BGP route leak by AS8048 in Venezuela that was observed on Cloudflare Radar. It is helpful to understand that route leaks are an expected side effect of BGP historically being based entirely on trust and carefully-executed business relationship-driven intent.Â &lt;/p&gt;
      &lt;p&gt;While route leaks could be done with malicious intent, the data suggests this event may have been an accident caused by a lack of routing export and import policies that would prevent it. This is why to have a safer BGP and Internet we need to work together and drive adoption of RPKI-based ASPA, for which RIPE recently released object creation, on the wide Internet. It will be a collaborative effort, just like RPKI has been for origin validation, but it will be worth it and prevent BGP incidents such as the one in Venezuela.Â &lt;/p&gt;
      &lt;p&gt;In addition to ASPA, we can all implement simpler mechanisms such as Peerlock and Peerlock-lite as operators, which sanity-checks received paths for obvious leaks. One especially promising initiative is the adoption of RFC9234, which should be used in addition to ASPA for preventing route leaks with the establishing of BGP roles and a new Only-To-Customer (OTC) attribute. If you havenât already asked your routing vendors for an implementation of RFC9234 to be on their roadmap: please do. You can help make a big difference.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46538001</guid><pubDate>Thu, 08 Jan 2026 06:46:26 +0000</pubDate></item><item><title>Mothers (YC X26) Is Hiring</title><link>https://jobs.ashbyhq.com/9-mothers</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46540078</guid><pubDate>Thu, 08 Jan 2026 12:00:35 +0000</pubDate></item><item><title>The Jeff Dean Facts</title><link>https://github.com/LRitzdorf/TheJeffDeanFacts</link><description>&lt;doc fingerprint="2f40dcb1cd7a06c7"&gt;
  &lt;main&gt;
    &lt;p&gt;The "Jeff Dean facts" are a set of jokes that revolve around the extraordinary programming prowess of their titular Google employee. Simply put, they are the coding equivalent of Chuck Norris-style jokes (for example, "Chuck Norris can slam a revolving door").&lt;/p&gt;
    &lt;p&gt;I first encountered the Facts on a random Quora page, though as of recently many of them appear to have been removed. Thus, in order to preserve this invaluable cache of programmer humor for posterity, I decided to create this repository. It is a combined list of various versions of the Facts, beginning with a text file that I copied from the Quora post sometime in 2019, when the original answer still existed. It's been expanded from other sources, which are listed (and linked, if possible) at the end of this document.&lt;/p&gt;
    &lt;p&gt;And now, without further ado...&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Jeff Dean proved that P=NP when he solved all NP problems in polynomial time on a whiteboard.&lt;/item&gt;
      &lt;item&gt;Jeff Dean's PIN is the last 4 digits of pi.&lt;/item&gt;
      &lt;item&gt;When Jeff gives a seminar at Stanford, it's so crowded Don Knuth has to sit on the floor. (TRUE)&lt;/item&gt;
      &lt;item&gt;Jeff Dean once bit a spider, the spider got super powers and C++ readability.&lt;/item&gt;
      &lt;item&gt;Once, in early 2002, when the index servers went down, Jeff Dean answered user queries manually for two hours. Evals showed a quality improvement of 5 points.&lt;/item&gt;
      &lt;item&gt;Jeff Dean got promoted to level 11 in a system where max level is 10. (TRUE)&lt;/item&gt;
      &lt;item&gt;Google Search was Jeff Dean's Noogler Project.&lt;/item&gt;
      &lt;item&gt;Jeff Dean has punch card readability.&lt;/item&gt;
      &lt;item&gt;Jeff Dean puts his pants on one leg at a time, but if he had more than two legs, you would see that his approach is actually &lt;code&gt;O(log n)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Jeff Dean acquired Sawzall readability after writing 58 lines of Sawzall code. As part of his readability review, he pointed out a flaw in the style guide which was promptly corrected by the reviewer.&lt;/item&gt;
      &lt;item&gt;Sanjay once asked Jeff Dean if he could keep the entire web in his memory. Due to the noise from his keyboard cooling fan, Jeff Dean misheard slightly and wrote Mustang instead of simply answering "Yes".&lt;/item&gt;
      &lt;item&gt;Jeff Dean compiles and runs his code before submitting, but only to check for compiler and CPU bugs.&lt;/item&gt;
      &lt;item&gt;Unsatisfied with constant time, Jeff Dean created the world's first &lt;code&gt;O(1/n)&lt;/code&gt;algorithm.&lt;/item&gt;
      &lt;item&gt;Jeff Dean has binary readability.&lt;/item&gt;
      &lt;item&gt;Jeff Dean has binary writability.&lt;/item&gt;
      &lt;item&gt;When Jeff Dean goes on vacation, production services across Google mysteriously stop working within a few days. This is actually true.1&lt;/item&gt;
      &lt;item&gt;Jeff Dean once shifted a bit so hard it ended up on another computer.&lt;/item&gt;
      &lt;item&gt;During his own Google interview, Jeff Dean was asked the implications if P=NP were true. He said "P = 0 or N = 1." Then, before the interviewer had even finished laughing, Jeff examined Google's public certificate and wrote the private key on the whiteboard.&lt;/item&gt;
      &lt;item&gt;You use 10% of your brain. The other 90% is running one of Jeff's mapreduce jobs.&lt;/item&gt;
      &lt;item&gt;Jeff Dean's resume lists the things he hasn't done; it's shorter that way.&lt;/item&gt;
      &lt;item&gt;To Jeff Dean, "NP" means "No Problemo".&lt;/item&gt;
      &lt;item&gt;Jeff Dean wrote an &lt;code&gt;O(n^2)&lt;/code&gt;algorithm once. It was for the Traveling Salesman Problem.&lt;/item&gt;
      &lt;item&gt;You don't explain your work to Jeff Dean. Jeff Dean explains your work to you.&lt;/item&gt;
      &lt;item&gt;Jeff Dean's resume has so many accomplishments, it has a table of contents.&lt;/item&gt;
      &lt;item&gt;Jeff Dean was forced to invent asynchronous APIs one day when he optimized a function so that it returned before it was invoked.&lt;/item&gt;
      &lt;item&gt;The rate at which Jeff Dean produces code jumped by a factor of 40 in late 2000 when he upgraded his keyboard to USB2.0.&lt;/item&gt;
      &lt;item&gt;When Jeff Dean designs software, he first codes the binary and then writes the source as documentation.&lt;/item&gt;
      &lt;item&gt;Jeff Dean's Peer Review is what got Larry promoted to CEO.&lt;/item&gt;
      &lt;item&gt;When God said: "Let there be light!", Jeff Dean was there to do the code review.&lt;/item&gt;
      &lt;item&gt;When Graham Bell invented the telephone, he saw a missed call from Jeff Dean&lt;/item&gt;
      &lt;item&gt;Compilers don't warn Jeff Dean. Jeff Dean warns compilers.&lt;/item&gt;
      &lt;item&gt;Jeff Dean doesn't exist, he's actually an advanced AI created by Jeff Dean.&lt;/item&gt;
      &lt;item&gt;Jeff Dean's IDE doesn't do code analysis, it does code appreciation.&lt;/item&gt;
      &lt;item&gt;Jeff Dean doesn't use ECC memory. He anticipates cosmic rays and uses them to improve performance.&lt;/item&gt;
      &lt;item&gt;Jeff Dean once failed a Turing test when he correctly identified the 203rd Fibonacci number in less than a second.&lt;/item&gt;
      &lt;item&gt;Jeff Dean invented Bigtable so that he would have a place to send his weekly snippets.&lt;/item&gt;
      &lt;item&gt;On the zeroth day, Jeff Dean created God.&lt;/item&gt;
      &lt;item&gt;Jeff Dean once implemented a web server in a single printf() call. Other engineers added thousands of lines of explanatory comments but still don't understand exactly how it works. Today that program is known as Google Web Server.&lt;/item&gt;
      &lt;item&gt;When Jeff has an ergonomic evaluation, it is for the protection of his keyboard.&lt;/item&gt;
      &lt;item&gt;Jeff Dean can beat you at connect four. In three moves.&lt;/item&gt;
      &lt;item&gt;Jeff Dean invented BigTable because his resume was too big to fit anywhere else.&lt;/item&gt;
      &lt;item&gt;Jeff Dean took the bite out of Apple's logo.&lt;/item&gt;
      &lt;item&gt;Chuck Norris can kill you. Jeff Dean can &lt;code&gt;kill -9&lt;/code&gt;you.&lt;/item&gt;
      &lt;item&gt;Jeff Dean can parse HTML with a regular expression...correctly.&lt;/item&gt;
      &lt;item&gt;When Jeff has trouble sleeping, he MapReduces sheep.&lt;/item&gt;
      &lt;item&gt;When Jeff Dean fires up the profiler, loops unroll themselves in fear.&lt;/item&gt;
      &lt;item&gt;When your code has undefined behavior, you get a seg fault and corrupted data. When Jeff Dean's code has undefined behavior, a unicorn rides in on a rainbow and gives everybody free ice cream.&lt;/item&gt;
      &lt;item&gt;Jeff doesn't sleep, he just sends SIGSUSPEND to the universe.&lt;/item&gt;
      &lt;item&gt;Jeff got Java readability with only 8 lines of code.&lt;/item&gt;
      &lt;item&gt;Jeff Dean can instantiate abstract classes.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;gcc -O4&lt;/code&gt;sends your code to Jeff Dean for a complete rewrite.&lt;/item&gt;
      &lt;item&gt;Jeff can recite 20,000 digits of pi in 5 hours. He doesn't remember them; he just recomputes them on the fly using only &lt;code&gt;O(log n)&lt;/code&gt;space.&lt;/item&gt;
      &lt;item&gt;Jeff Dean remembers only one password. For each site, he concatenates it with the site name, computes its SHA-256 hash, and then types the result.&lt;/item&gt;
      &lt;item&gt;Jeff Dean is still waiting for mathematicians to discover the joke he hid in the digits of pi.&lt;/item&gt;
      &lt;item&gt;There is no &lt;code&gt;Ctrl&lt;/code&gt;key on Jeff Dean's keyboard. Jeff Dean is always in control.&lt;/item&gt;
      &lt;item&gt;Jeff Dean was born on December 31, 1969 at 11:48 PM. It took him twelve minutes to implement his first time counter.&lt;/item&gt;
      &lt;item&gt;When Jeff Dean says "Hello World", the world says "Hello Jeff".&lt;/item&gt;
      &lt;item&gt;Jeff Dean can get 1s out of &lt;code&gt;/dev/zero&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Jeff Dean simply walks into Mordor.&lt;/item&gt;
      &lt;item&gt;Jeff Dean spent some 20% time on an AI project. That produced Urs Hoelzle.&lt;/item&gt;
      &lt;item&gt;Google once had to move out of a datacenter after Jeff Dean accidentally compressed the index so densely that a black hole was formed.&lt;/item&gt;
      &lt;item&gt;Jeff starts his programming sessions with &lt;code&gt;cat &amp;gt; /dev/mem&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The speed of light in a vacuum used to be about 35 mph. Then Jeff Dean spent a weekend optimizing physics.&lt;/item&gt;
      &lt;item&gt;When Jeff Dean sends you a code review, it's because he thinks there's something in it you should learn.&lt;/item&gt;
      &lt;item&gt;Jeff Dean does not &lt;code&gt;sleep()&lt;/code&gt;, he&lt;code&gt;wait()&lt;/code&gt;s.&lt;/item&gt;
      &lt;item&gt;Jeff Dean invented MapReduce so he could sort his fan mail.&lt;/item&gt;
      &lt;item&gt;Once Jeff Dean ordered a list, and the list obeyed him.&lt;/item&gt;
      &lt;item&gt;Chuck Norris is Jeff Dean's 20% project.&lt;/item&gt;
      &lt;item&gt;When your code is killed by SIGJEFF, it never runs again.&lt;/item&gt;
      &lt;item&gt;Jeff Dean's calendar goes straight from March 31st to April 2nd; no one fools Jeff Dean.&lt;/item&gt;
      &lt;item&gt;Jeff Dean never has the wrong number; you have the wrong phone.&lt;/item&gt;
      &lt;item&gt;Jeff Dean has exactly two keys on his keyboard: &lt;code&gt;0&lt;/code&gt;and&lt;code&gt;1&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Errors treat Jeff Dean as a warning.&lt;/item&gt;
      &lt;item&gt;Cricket matches used to take 5 days, until Jeff optimized them.&lt;/item&gt;
      &lt;item&gt;Jeff Dean's watch displays seconds since January 1st, 1970. He is never late.&lt;/item&gt;
      &lt;item&gt;Jeff's code is so fast the assembly code needs three HALT opcodes to stop it.&lt;/item&gt;
      &lt;item&gt;Emacs' preferred editor is Jeff Dean.&lt;/item&gt;
      &lt;item&gt;Google: it's basically a Jeff Dean's side project.&lt;/item&gt;
      &lt;item&gt;Jeff Dean has to unoptimize his code so that reviewers believe it was written by a human.&lt;/item&gt;
      &lt;item&gt;Websearch is just a large unittest Jeff wrote for his real app.&lt;/item&gt;
      &lt;item&gt;Jeff Dean doesn't need speakers or headphones. He just types &lt;code&gt;cat *.mp3&lt;/code&gt;, glances at the screen, and his brain decodes the music in the background while he works.&lt;/item&gt;
      &lt;item&gt;Jeff Dean has Perl Readability. (TRUE)&lt;/item&gt;
      &lt;item&gt;Jeff Dean quicksorts his laundry.&lt;/item&gt;
      &lt;item&gt;The OR ELSE construct had to be removed from ISO C after Jeff Dean used it in Mustang and kernels started panicking in terror.&lt;/item&gt;
      &lt;item&gt;Jeff Dean is not afraid of evil constructors. They are afraid of him.&lt;/item&gt;
      &lt;item&gt;Jeff Dean doesn't write bugs, just features you are unable to understand.&lt;/item&gt;
      &lt;item&gt;Jeff Dean eschews both Emacs and VI. He types his code into &lt;code&gt;zcat&lt;/code&gt;, because it's faster that way.&lt;/item&gt;
      &lt;item&gt;When Jeff Dean sends an Ethernet frame, there are no collisions because the competing frames retreat back up into the buffer memory on their source network cards.&lt;/item&gt;
      &lt;item&gt;Jeff once simultaneously reduced all binary sizes by 3% and raised the severity of a previously known low-priority Python bug to critical-priority in a single change that contained no Python code.&lt;/item&gt;
      &lt;item&gt;One day, Jeff Dean grabbed his Etch-a-Sketch instead of his laptop on his way out the door. On his way back home to get his real laptop, he programmed the Etch-a-Sketch to play Tetris.&lt;/item&gt;
      &lt;item&gt;The x86-64 spec includes several undocumented instructions marked private use. They are actually for Jeff Dean's use.&lt;/item&gt;
      &lt;item&gt;Knuth mailed a copy of The Art of Computer Programming to Google. Jeff Dean autographed it and mailed it back.&lt;/item&gt;
      &lt;item&gt;When he heard that Jeff Dean's autobiography would be exclusive to the platform, Richard Stallman bought a Kindle.&lt;/item&gt;
      &lt;item&gt;Jeff Dean can losslessly compress random data.&lt;/item&gt;
      &lt;item&gt;When asked if the facts about him are true, Jeff Dean responded with "111111". While the interviewer was still trying to figure out what he meant, he clarified, "every single bit of it is true."&lt;/item&gt;
      &lt;item&gt;Jeff Dean mines bitcoins. In his head.&lt;/item&gt;
      &lt;item&gt;Jeff Dean traps the KILL signal.&lt;/item&gt;
      &lt;item&gt;Jeff Dean's programs don't SEGFAULT. The memory rearranges itself in order to put data and code where it belongs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This list was compiled from several sources, with duplicate (or near-duplicate) Facts removed. The relevant sources are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Quora - the original question seems to still exist, at https://www.quora.com/What-are-all-the-Jeff-Dean-facts, but the exact response I downloaded all those years ago is nowhere to be found.&lt;/item&gt;
      &lt;item&gt;infO(N), a Bulgarian website that appears to provide schedules and results for coding competitions (post)&lt;/item&gt;
      &lt;item&gt;A now-deleted Google+ thread, quoted by a Reddit user&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;It's not clear whether this fact is really true, or whether this line is simply part of the joke, so I've omitted the usual &lt;code&gt;(TRUE)&lt;/code&gt;identifier here. Interpret this as you see fit :)&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46540498</guid><pubDate>Thu, 08 Jan 2026 13:02:05 +0000</pubDate></item><item><title>Show HN: DeepDream for Video with Temporal Consistency</title><link>https://github.com/jeremicna/deepdream-video-pytorch</link><description>&lt;doc fingerprint="3f194f592fcd7ab8"&gt;
  &lt;main&gt;
    &lt;p&gt;This is a fork of neural-dream, a PyTorch implementation of DeepDream. This fork introduces optical flow estimation and occlusion masking to apply DeepDream to videos with temporal consistency.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Standard DeepDream: The original single-image implementation.&lt;/item&gt;
      &lt;item&gt;Video DeepDream: New CLI (&lt;code&gt;video_dream.py&lt;/code&gt;) that uses RAFT Optical Flow to warp previous dream frames into the current frame, ensuring smooth transitions and object tracking.&lt;/item&gt;
      &lt;item&gt;Occlusion Masking: Automatically detects when objects move in front of one another to prevent "ghosting" artifacts.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head class="px-3 py-2"&gt;mallard_demo.mp4&lt;/head&gt;
    &lt;head class="px-3 py-2"&gt;highway_demo.mp4&lt;/head&gt;
    &lt;head class="px-3 py-2"&gt;mallard_independent_demo.mp4&lt;/head&gt;
    &lt;head class="px-3 py-2"&gt;highway_independent_demo.mp4&lt;/head&gt;
    &lt;head class="px-3 py-2"&gt;mallard.mp4&lt;/head&gt;
    &lt;head class="px-3 py-2"&gt;highway.mp4&lt;/head&gt;
    &lt;p&gt;This project requires the following key packages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PyTorch&lt;/item&gt;
      &lt;item&gt;torchvision&lt;/item&gt;
      &lt;item&gt;OpenCV&lt;/item&gt;
      &lt;item&gt;NumPy&lt;/item&gt;
      &lt;item&gt;Pillow&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Install Dependencies:&lt;/p&gt;
    &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;
    &lt;p&gt;Download Models: Run the download script to fetch the standard Inception/GoogLeNet models:&lt;/p&gt;
    &lt;code&gt;python models/download_models.py&lt;/code&gt;
    &lt;p&gt;To download all compatible models:&lt;/p&gt;
    &lt;code&gt;python models/download_models.py -models all-caffe-googlenet&lt;/code&gt;
    &lt;p&gt;To dream on a video, use the &lt;code&gt;video_dream.py&lt;/code&gt; script. This wrapper accepts specific video arguments and any argument accepted by the standard image dreamer (e.g., layers, octaves, iterations).&lt;/p&gt;
    &lt;p&gt;Basic Video Command:&lt;/p&gt;
    &lt;code&gt;python video_dream.py -content_video input.mp4 -output_video output.mp4 -num_iterations 1&lt;/code&gt;
    &lt;p&gt;Note: For video processing, we recommend using &lt;code&gt;-num_iterations 1&lt;/code&gt;. The temporal consistency from optical flow means each frame builds on the previous dream, so fewer iterations per frame are needed compared to single images.&lt;/p&gt;
    &lt;p&gt;Video-Specific Arguments:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Argument&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;-content_video&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;input.mp4&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Path to the source video file.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;-output_video&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;output.mp4&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Path where the final video will be saved.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;-blend&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;0.5&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;(0.0 - 1.0): Mix ratio between the raw video frame and the warped previous dream. Higher values (closer to 1.0) use more of the raw frame; lower values (closer to 0.0) preserve more of the previous hallucinations.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;-independent&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;False&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Flag: If set, disables temporal consistency (Optical Flow). Every frame is dreamed on independently (causes flickering).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;-update_interval&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;5&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Updates the output video file on disk every N frames (allows you to preview progress while running).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;-temp_dir&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;temp&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Directory to store extracted frames, flow data, and masks during processing.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;-keep_temp&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;False&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Flag: If set, the temporary directory is not deleted after processing finishes.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;-verbose&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;False&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Flag: Enable detailed logs (prints DeepDream iteration logs for every frame).&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;All of the following arguments are from the single frame implementation, and you can mix and match any of these with the video-specific arguments above. Refer to neural-dream for more information on single frame parameters.&lt;/p&gt;
    &lt;p&gt;Example combining video and standard args:&lt;/p&gt;
    &lt;code&gt;python video_dream.py -content_video test.mp4 -dream_layers inception_4d -num_iterations 1 -octave_scale 0.7 -image_size 512&lt;/code&gt;
    &lt;p&gt;For single image processing only:&lt;/p&gt;
    &lt;code&gt;python neural_dream.py -content_image &amp;lt;image.jpg&amp;gt; -dream_layers inception_4c -num_iterations 10&lt;/code&gt;
    &lt;p&gt;Note: Paths to images should not contain the &lt;code&gt;~&lt;/code&gt; character; use relative or absolute paths.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-image_size&lt;/code&gt;: Maximum side length (in pixels) of the generated image. Default is 512.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-gpu&lt;/code&gt;: Zero-indexed ID of the GPU to use; for CPU mode set&lt;code&gt;-gpu&lt;/code&gt;to&lt;code&gt;c&lt;/code&gt;; for MPS mode (Apple Silicon) set&lt;code&gt;-gpu&lt;/code&gt;to&lt;code&gt;mps&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-dream_weight&lt;/code&gt;: How much to weight DeepDream. Default is&lt;code&gt;1e3&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-tv_weight&lt;/code&gt;: Weight of total-variation (TV) regularization; helps smooth the image. Default&lt;code&gt;0&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-l2_weight&lt;/code&gt;: Weight of latent state regularization. Default&lt;code&gt;0&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-num_iterations&lt;/code&gt;: Number of iterations. Default is&lt;code&gt;10&lt;/code&gt;. For video, use&lt;code&gt;1&lt;/code&gt;(temporal consistency reduces the need for multiple iterations per frame).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-init&lt;/code&gt;: Initialization method:&lt;code&gt;image&lt;/code&gt;(content image) or&lt;code&gt;random&lt;/code&gt;(noise). Default&lt;code&gt;image&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-jitter&lt;/code&gt;: Apply jitter to image. Default&lt;code&gt;32&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-layer_sigma&lt;/code&gt;: Apply gaussian blur to image. Default&lt;code&gt;0&lt;/code&gt;(disabled).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-optimizer&lt;/code&gt;:&lt;code&gt;lbfgs&lt;/code&gt;or&lt;code&gt;adam&lt;/code&gt;. Default&lt;code&gt;adam&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-learning_rate&lt;/code&gt;: Learning rate (step size). Default&lt;code&gt;1.5&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-normalize_weights&lt;/code&gt;: Divide dream weights by the number of channels.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-loss_mode&lt;/code&gt;: Loss mode:&lt;code&gt;bce&lt;/code&gt;,&lt;code&gt;mse&lt;/code&gt;,&lt;code&gt;mean&lt;/code&gt;,&lt;code&gt;norm&lt;/code&gt;, or&lt;code&gt;l2&lt;/code&gt;. Default&lt;code&gt;l2&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-output_image&lt;/code&gt;: Name of the output image. Default&lt;code&gt;out.png&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-output_start_num&lt;/code&gt;: Number to start output image names at. Default&lt;code&gt;1&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-print_iter&lt;/code&gt;: Print progress every N iterations.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-save_iter&lt;/code&gt;: Save image every N iterations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-dream_layers&lt;/code&gt;: Comma-separated list of layer names to use.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-channels&lt;/code&gt;: Comma-separated list of channels to use.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-channel_mode&lt;/code&gt;: Selection mode:&lt;code&gt;all&lt;/code&gt;,&lt;code&gt;strong&lt;/code&gt;,&lt;code&gt;avg&lt;/code&gt;,&lt;code&gt;weak&lt;/code&gt;, or&lt;code&gt;ignore&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-channel_capture&lt;/code&gt;:&lt;code&gt;once&lt;/code&gt;or&lt;code&gt;octave_iter&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-num_octaves&lt;/code&gt;: Number of octaves per iteration. Default&lt;code&gt;4&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-octave_scale&lt;/code&gt;: Resize value. Default&lt;code&gt;0.6&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-octave_iter&lt;/code&gt;: Iterations (steps) per octave. Default&lt;code&gt;50&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-octave_mode&lt;/code&gt;:&lt;code&gt;normal&lt;/code&gt;,&lt;code&gt;advanced&lt;/code&gt;,&lt;code&gt;manual_max&lt;/code&gt;,&lt;code&gt;manual_min&lt;/code&gt;, or&lt;code&gt;manual&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-lap_scale&lt;/code&gt;: Number of layers in laplacian pyramid. Default&lt;code&gt;0&lt;/code&gt;(disabled).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-sigma&lt;/code&gt;: Strength of gaussian blur in pyramids. Default&lt;code&gt;1&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-zoom&lt;/code&gt;: Amount to zoom in.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-zoom_mode&lt;/code&gt;:&lt;code&gt;percentage&lt;/code&gt;or&lt;code&gt;pixel&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-tile_size&lt;/code&gt;: Desired tile size. Default&lt;code&gt;0&lt;/code&gt;(disabled).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-overlap_percent&lt;/code&gt;: Percentage of overlap for tiles. Default&lt;code&gt;50&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-original_colors&lt;/code&gt;: Set to&lt;code&gt;1&lt;/code&gt;to keep content image colors.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-model_file&lt;/code&gt;: Path to&lt;code&gt;.pth&lt;/code&gt;file. Default is VGG-19.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-model_type&lt;/code&gt;:&lt;code&gt;caffe&lt;/code&gt;,&lt;code&gt;pytorch&lt;/code&gt;,&lt;code&gt;keras&lt;/code&gt;, or&lt;code&gt;auto&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-backend&lt;/code&gt;:&lt;code&gt;nn&lt;/code&gt;,&lt;code&gt;cudnn&lt;/code&gt;,&lt;code&gt;openmp&lt;/code&gt;, or&lt;code&gt;mkl&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-cudnn_autotune&lt;/code&gt;: Use built-in cuDNN autotuner (slower start, faster run).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Problem: The program runs out of memory (OOM) Solution:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Reduce &lt;code&gt;-image_size&lt;/code&gt;(e.g., to 512 or 256).&lt;/item&gt;
      &lt;item&gt;If using GPU, use &lt;code&gt;-backend cudnn&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;For video: Reduce the input video resolution before processing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Problem: Video processing is very slow Solution: Video DeepDreaming is computationally expensive. It runs the full DeepDream process per frame, plus Optical Flow calculations.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use &lt;code&gt;-num_iterations 1&lt;/code&gt;(recommended for video; temporal consistency means fewer iterations are needed).&lt;/item&gt;
      &lt;item&gt;Reduce &lt;code&gt;-octave_iter&lt;/code&gt;(e.g., to 10 or 20).&lt;/item&gt;
      &lt;item&gt;Use a smaller &lt;code&gt;-image_size&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By default, &lt;code&gt;neural-dream&lt;/code&gt; uses the &lt;code&gt;nn&lt;/code&gt; backend.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use cuDNN: &lt;code&gt;-backend cudnn&lt;/code&gt;(GPU only, reduces memory).&lt;/item&gt;
      &lt;item&gt;Reduce Size: &lt;code&gt;-image_size 256&lt;/code&gt;(Halves memory usage).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With default settings, standard execution uses ~1.3 GB GPU memory.&lt;/p&gt;
    &lt;p&gt;You can use multiple devices with &lt;code&gt;-gpu&lt;/code&gt; and &lt;code&gt;-multidevice_strategy&lt;/code&gt;.
Example: &lt;code&gt;-gpu 0,1,2,3 -multidevice_strategy 3,6,12&lt;/code&gt; splits layers across 4 GPUs. See ProGamerGov/neural-dream for details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46540660</guid><pubDate>Thu, 08 Jan 2026 13:21:59 +0000</pubDate></item><item><title>Maine company in the spotlight after Maduro apparently wore one of their hoodies</title><link>https://www.boston.com/news/business/2026/01/06/maine-company-maduro-venezuela-hoodie/</link><description>&lt;doc fingerprint="26791b55a0c12eff"&gt;
  &lt;main&gt;&lt;head rend="h3"&gt;Sign up for the Today newsletter&lt;/head&gt;&lt;p&gt;Get everything you need to know to start your day, delivered right to your inbox every morning.&lt;/p&gt;&lt;p&gt;By Abby Patkin&lt;/p&gt;&lt;p&gt;The CEO of a Maine apparel company said his phone “blew up” after deposed Venezuelan leader Nicolás Maduro was apparently photographed wearing one of their hoodies upon arriving in New York over the weekend.&lt;/p&gt;&lt;p&gt;The U.S. captured Maduro, Venezuela’s president, and his wife in a staggering nighttime military operation Saturday, charging the ousted leader with drug and weapons offenses. Before long, social media was flush with images that appeared to show Maduro wearing an ORIGIN hoodie in the shade “Patriot Blue,” surrounded by Drug Enforcement Administration agents.&lt;/p&gt;&lt;p&gt;In one photo, Maduro appears to be giving the camera a double thumbs up.&lt;/p&gt;&lt;p&gt;“I had to start putting the pieces together: Why is this dude wearing an Origin Patriot Blue hoodie?” Pete Roberts, the company’s founder and CEO, said in a video Sunday. “And the irony in this is that this wave, this logo here on the shirt Maduro is wearing, this is the ‘Wave of Freedom.’”&lt;/p&gt;&lt;p&gt;Farmington-based ORIGIN began as a way to help revitalize a struggling New England manufacturing community, he explained, and its “Wave of Freedom” logo represents a commitment to building back.&lt;/p&gt;&lt;p&gt;Roberts also offered his theory on how Maduro came to be wearing a hoodie made by a smaller brand from Maine.&lt;/p&gt;&lt;p&gt;“Probably a DEA agent slipped this hoodie on him and said, ‘You’re going to feel the fabric of freedom on American soil,’” he quipped. “That’s my assumption, and I’m taking the liberty to assume.”&lt;/p&gt;&lt;p&gt;Writing on Facebook, ORIGIN co-founder and retired Navy SEAL Jocko Willink further noted the brand has supporters “in every branch of service and every agency of the government.”&lt;/p&gt;&lt;p&gt;According to ORIGIN’s product description, the hoodie offers a “triple chill effect” to wick away moisture and cool athletes down — an element Roberts said he found “really curious and interesting,” given the chilly weather in New York.&lt;/p&gt;&lt;p&gt;“So maybe they wanted him to feel comfortable or a little uncomfortable. I’m not quite sure,” Roberts said. “But he definitely gave two thumbs up, so I think he liked the fabric.”&lt;/p&gt;&lt;p&gt;Still, he told News Center Maine ORIGIN isn’t looking to politicize the Maduro photo op and is “just trying to use it for brand awareness and to get people back into our store.”&lt;/p&gt;&lt;p&gt;The brand’s website traffic jumped about 300% Sunday, and sales were up roughly 200%, Roberts told the news outlet.&lt;/p&gt;&lt;p&gt;“It would be really hard for a company out of Maine to get, let’s call it, a billion eyes on our brand,” he said. “And so, that’s a real positive as a brand, as a movement. We would never have been able to create that.”&lt;/p&gt;&lt;p&gt;Abby Patkin is a general assignment news reporter whose work touches on public transit, crime, health, and everything in between.&lt;/p&gt;&lt;p&gt;Get everything you need to know to start your day, delivered right to your inbox every morning.&lt;/p&gt;&lt;p&gt;Be civil. Be kind.&lt;/p&gt;Read our full community guidelines.&lt;p&gt;Stay up to date with everything Boston. Receive the latest news and breaking updates, straight from our newsroom to your inbox.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46541586</guid><pubDate>Thu, 08 Jan 2026 14:44:24 +0000</pubDate></item><item><title>Bose is open-sourcing its old smart speakers instead of bricking them</title><link>https://www.theverge.com/news/858501/bose-soundtouch-smart-speakers-open-source</link><description>&lt;doc fingerprint="b3583dfad3a2a07e"&gt;
  &lt;main&gt;
    &lt;p&gt;In a surprisingly user-friendly move, Bose has announced it will be open-sourcing the API documentation for its SoundTouch smart speakers, which were slated to lose official support on February 18th, as reported by Ars Technica. Bose has also moved that date back to May 6th, 2026.&lt;/p&gt;
    &lt;head rend="h1"&gt;Bose is open-sourcing its old smart speakers instead of bricking them&lt;/head&gt;
    &lt;p&gt;SoundTouch speakers could now have a second life and won’t lose support until May.&lt;/p&gt;
    &lt;p&gt;SoundTouch speakers could now have a second life and won’t lose support until May.&lt;/p&gt;
    &lt;p&gt;When cloud support ends, an update to the SoundTouch app will add local controls to retain as much functionality as possible without cloud services. Users will still be able to stream music to SoundTouch speakers with Bluetooth, AirPlay, and Spotify Connect (plus physical AUX connections). Remote control features and grouping speakers will also continue to work, and users will still be able to set up and configure their SoundTouch speakers.&lt;/p&gt;
    &lt;p&gt;Now that the smart speakers’ API is being open-sourced, users can also create their own compatible SoundTouch tools to help fill in any gaps left by the lack of cloud services. While it’s still disappointing that the speakers are losing official support, Bose’s approach at least lets people continue using their speakers, rather than bricking otherwise functional devices.&lt;/p&gt;
    &lt;p&gt;This move from Bose is particularly surprising because of how rare it is. Usually when products lose support for cloud services, they end up bricked, and occasionally users step in themselves to fix things. For instance, when Pebble originally shut down in 2016, users kept their watches functional by creating the Rebble Alliance, a community-run replacement for the watches’ cloud services, firmware, and app store.&lt;/p&gt;
    &lt;head rend="h2"&gt;Most Popular&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;OpenAI launches ChatGPT Health, encouraging users to connect their medical records&lt;/item&gt;
      &lt;item&gt;Fujifilm’s new instant camera captures video with vintage effects&lt;/item&gt;
      &lt;item&gt;Dell admits consumers don’t care about AI PCs&lt;/item&gt;
      &lt;item&gt;Lego announces Smart Brick, the ‘most significant evolution’ in 50 years&lt;/item&gt;
      &lt;item&gt;CES 2026 live: all the news, announcements, and innovations from the show floor and beyond&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46541892</guid><pubDate>Thu, 08 Jan 2026 15:07:57 +0000</pubDate></item><item><title>Japanese electronics store pleads for old PCs amid ongoing hardware shortage</title><link>https://www.tomshardware.com/desktops/pc-building/major-japanese-electronics-store-begs-customers-for-their-old-pcs-as-hardware-drought-continues-we-pretty-much-buy-any-pc-pleads-the-akihabara-outlet</link><description>&lt;doc fingerprint="77b91d9c38a22dcf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Major Japanese electronics store begs customers for their old PCs as hardware drought continues — ‘we pretty much buy any PC’ pleads the Akihabara outlet&lt;/head&gt;
    &lt;p&gt;Old becomes gold in Akihabara Electric Town.&lt;/p&gt;
    &lt;p&gt;A major Japanese PC and electronics store is pleading with customers to sell their old PC gear. “As a favor, if you buy a new one, please sell your gaming PC to our company,” begged the X-account of Sofmap Gaming in Akihabara, the Electric Town district of Tokyo (machine translation, h/t PC-Watch). The store shared a photo of some almost barren shelves, presumably taken at its triple-floor retail establishment.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;ゲーミングPC、中古も本当に在庫なくて今これあの、お願いなので買い替えたらぜひ弊社にゲーミングPCを売ってください...結構高く買い取っていますので...ゲーミングのデスクでもノートでも、もちろんゲーミングじゃない普通のでもPCなら大体買い取っているので... pic.twitter.com/IinBuGgRV7January 7, 2026&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;“Gaming PCs, even used ones, are really out of stock right now,” wrote Sofmap, as an explanation for its call for old rigs. In the above Tweet, it asks customers to come in and sell their old PCs, highlighting that “We buy them back at pretty high prices...”&lt;/p&gt;
    &lt;p&gt;Moreover, the company underlined that it wasn’t going to be fussy. “Whether it's a gaming desktop or a laptop, or even a regular non-gaming one, we pretty much buy any PC...”&lt;/p&gt;
    &lt;p&gt;These are clearly the words of a PC retailer facing consumer demand that it just can’t meet. We reported on Akihabara store trying to limit new RAM, SSD, and HDD sales back in November.&lt;/p&gt;
    &lt;head rend="h2"&gt;Old becomes gold&lt;/head&gt;
    &lt;p&gt;The memory supply crunch impacted the PC industry faster and more deeply than many would have predicted. The insatiable demand for memory from AI data center makers, with their deep circular-funded pockets, caused the first pricing jolts in the PC memory market. That’s reasonable, as consumers and industry both need to be fed product from the same big-three memory makers.&lt;/p&gt;
    &lt;p&gt;Consumers saw the first impacts on modern DDR5 pricing. Some DDR5 kits, if you can find them in stock, like this Corsair Vengeance RGB DDR5-5200 16GB (2x8GB) on Amazon is now $235. That price is more than 3.5X what it cost last October ($66).&lt;/p&gt;
    &lt;p&gt;However, there remains some hope that DDR4 pricing and availability, thanks to old stocks and upgraders already having DIMMs, could provide a safe haven for continued PC building. This perception even seems to permeate PC component makers, with more DDR4-supporting motherboards being manufactured, plus hints about new processors for DDR4 platforms.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;However, we are continuing to feel RAM crunch aftershocks. Prices of pre-built PCs were the next market affected. Graphics cards with more generous VRAM quotas are also strongly rumored to be facing constraints. We should at least expect a price rise for GPU-restocks, with next-gen GPUs rumored to be delayed…&lt;/p&gt;
    &lt;p&gt;Now, underlined by this Japan retail report, it even seems like stocks of old used PCs are being snapped up by consumers.&lt;/p&gt;
    &lt;head rend="h2"&gt;How old is too old?&lt;/head&gt;
    &lt;p&gt;Of course, some old PCs are too old for retailers like Sofmap, even during today’s PC drought. We’d expect retailers that dabble in used PCs for non-enthusiast users to limit their purchases to DDR4 platforms, with hardware support that slots above the Windows 11 minimum requirements (Intel 8th Gen, AMD Ryzen 2000).&lt;/p&gt;
    &lt;p&gt;There’s an entirely different market for really old PCs, though. Vintage computers of certain eras have been increasingly pricey for quite a long time now. I was in Japan this time last year and astonished by the bountiful supplies of old PCs at used electronics retailers like Hard-Off. Hopefully, these computing gems (see the above picture), many of which live in the awkward zone between vintage and modern, will remain plentiful and affordable for PC retro-fans and tinkerers alike.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, &amp;amp; reviews in your feeds.&lt;/p&gt;
    &lt;p&gt;Mark Tyson is a news editor at Tom's Hardware. He enjoys covering the full breadth of PC tech; from business and semiconductor design to products approaching the edge of reason.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;bit_user&lt;/header&gt;I'm waiting for the article about someone making a $20k handbag covered in DDR5 DIMMs, or covering some designer shoes with DDR5 chips.Reply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Gururu&lt;/header&gt;Reply&lt;quote/&gt;LOL that is a great question. Was just there a few months ago, wild place.ezst036 said:Where is new demand coming from?&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Eximo&lt;/header&gt;End of Windows 10 might be part of it. But I suspect this is more about hitting a price point for their average customers who are used to bargain shopping used hardware. With more people who would normally buy new buying used, that would squeeze the availabilty of used hardware.Reply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;artk2219&lt;/header&gt;It wouldn't surprise me if part of the issue is that people are intercepting the machines before they reached the reseller. They would then strip the storage, ram, and maybe the gpu, resell it, dump the rest.Reply&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46542015</guid><pubDate>Thu, 08 Jan 2026 15:18:39 +0000</pubDate></item><item><title>Our Changing Planet, as Seen from Space</title><link>https://e360.yale.edu/digest/nasa-satellite-images-2025</link><description>&lt;doc fingerprint="6553206b04e87c6a"&gt;
  &lt;main&gt;
    &lt;p&gt;Humans are altering the planet on an unthinkable scale, both by converting vast tracts of wilderness into farms and cities and by pouring huge volumes of heat-trapping gas into the atmosphere. The impact of these enormous changes can be seen from space.&lt;/p&gt;
    &lt;p&gt;The satellite photos below, shared by NASA’s Earth Observatory over the past year, show the growing human imprint on planet Earth.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46542046</guid><pubDate>Thu, 08 Jan 2026 15:20:52 +0000</pubDate></item><item><title>Iran Goes Into IPv6 Blackout</title><link>https://radar.cloudflare.com/routing/ir</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46542683</guid><pubDate>Thu, 08 Jan 2026 16:11:48 +0000</pubDate></item><item><title>Digital Red Queen: Adversarial Program Evolution in Core War with LLMs</title><link>https://sakana.ai/drq/</link><description>&lt;doc fingerprint="f60c1bd489a1219e"&gt;
  &lt;main&gt;&lt;p&gt;Survival of the Fittest Code. In the game Core War, assembly-like programs called “warriors” fight for control of a virtual computer. Warriors may employ sophisticated strategies including targeted self-replication, data bombing, and massive multithreading, in order to crash other programs, and dominate the machine. Top: We visualize battles between assembly programs (“warriors”) discovered by our Digital Red Queen (DRQ) algorithm. Each DRQ round introduces one additional warrior into the multi-agent simulation. Bottom: With more rounds, the LLM-driven evolution discovers increasingly robust strategies. By simulating these adversarial dynamics, we observe emergent behaviors that mirror biological evolution, where agents must constantly adapt simply to survive against ever-changing threats. Furthermore, as Core War is a Turing-complete environment where code and data share the same address space, this process leads to some very chaotic self-modifying code dynamics. &lt;/p&gt;&lt;head rend="h2"&gt;Summary&lt;/head&gt;&lt;p&gt;Core War is a competitive programming game introduced in 1984, in which battle programs called warriors fight for dominance inside a virtual computer. To compete, developers write their code in Redcode, a specialized assembly language. In this work, we explore what happens when large language models (LLMs) drive an adversarial evolutionary arms race in this domain, where programs continuously adapt to defeat a growing history of opponents rather than a static benchmark. We find that this dynamic adversarial process leads to the emergence of increasingly general strategies and reveals an intriguing form of convergent evolution, where different code implementations settle into similar high-performing behaviors. Ultimately, this work positions Core War as a sandbox for studying “Red Queen” dynamics in artificial systems, offering a safe controlled environment for analyzing how AI agents might evolve in real-world adversarial settings such as cybersecurity.&lt;/p&gt;&lt;p&gt;For further details, please read our technical report (web paper, arxiv) and released code (github).&lt;/p&gt;&lt;p&gt;Two example warriors produced by DRQ: Ring Warrior Enhanced v9 and Spiral Bomber Optimized v22. These examples were selected to illustrate two complementary aspects of DRQ: its ability to synthesize qualitatively distinct strategies within a single program, and to produce generally performant warriors. Note that comments are LLM generated.&lt;/p&gt;&lt;p&gt; Simulating our evolved “warriors” in a sandboxed Core War environment. The user can interactively visualize the assembly language (Redcode) of the warriors around where the mouse cursor is located.&lt;/p&gt;&lt;head rend="h2"&gt;Introduction&lt;/head&gt;&lt;p&gt;Humans are the product of an extraordinary evolutionary arms race, shaped by constant competition with other organisms. Yet evolution did not stop with the emergence of modern humans: competition persists at every scale, from viruses and bacteria to people, companies, and even nations vying for dominance. As more AI systems are deployed into the world, they too will enter this competitive landscape. Inevitably, these AI systems will begin to compete with one another, either directly or indirectly, giving rise to a new kind of evolutionary dynamic. To prepare for such a future and study these fascinating dynamics, we use large language models (LLMs) to evolve programs that compete against each other for control of a virtual computer in a game called Core War.&lt;/p&gt;&lt;p&gt;Core War is a competitive programming game played out in a shared block of computer memory, called the “Core,” where two or more assembly programs fight for survival. Each program, known as a “warrior”, is written in an assembly language called Redcode. These programs are tasked with crashing their competitors while keeping their own processes alive. The simulation runs by alternating between the programs, executing one instruction at a time. A warrior “attacks” by writing invalid instructions (DAT commands) into the memory slots occupied by opponents, causing them to crash upon execution.&lt;/p&gt;&lt;p&gt; Examples of discovered warriors competing against each other in Core War.&lt;lb/&gt; Core War is a programming game where assembly-like programs called “warriors” compete for control of a virtual machine. In this work, we use LLMs to evolve warriors through a self-play algorithm called Digital Red Queen. This process leads to the discovery of diverse and sophisticated strategies, including targeted bombing, self-replication, and massive multithreading. Here, we show some of the discovered warriors competing against each other in Core War battles. Symbols indicate instruction opcodes, and colors denote the warrior that last modified each memory address. There is no distinction between code and data, making the environment highly dynamic and volatile. &lt;/p&gt;&lt;p&gt;Notably, there is no distinction between code and data, so warriors regularly modify both themselves and their opponents on the fly. This enables self-modification and even self-replication, but it also creates an extremely volatile environment in which programs must survive. Core War is also Turing-complete, meaning it can in principle support arbitrarily complex strategies.&lt;/p&gt;&lt;p&gt;Over the years, humans have devised many clever Core War strategies, including bombing random memory locations, self-replicating programs, and programs which continually scan the Core to detect opponent locations. These strategies were devised through a meta arms race between humans who try out new strategies and see what works. What would happen if we do this same arms race with LLMs?&lt;/p&gt;&lt;p&gt;In collaboration with MIT, we are excited to release our new paper Digital Red Queen: Adversarial Program Evolution in Core War with LLMs! (arxiv)&lt;/p&gt;&lt;head rend="h2"&gt;Our Method: Digital Red Queen (DRQ)&lt;/head&gt;&lt;p&gt;In evolutionary biology, the Red Queen Hypothesis posits that species must constantly evolve simply to survive against their ever-changing competitors. It argues that being “fit” in the current environment is not enough. Instead, organisms must continuously adapt—not to gain an advantage, but simply to maintain their relative fitness in a world that is always changing. This concept perfectly captures the nature of adversarial arms races, where being “fit” is never a permanent state. The name implies that standing still is not an option, drawing from Through the Looking-Glass where the Red Queen tells Alice: “Now, here, you see, it takes all the running you can do, to keep in the same place.”&lt;/p&gt; “Now, here, you see, it takes all the running you can do, to keep in the same place.”&lt;lb/&gt;Red Queen to Alice. By Lewis Carroll, Through the Looking-Glass. (Original Source)&lt;p&gt;Taking inspiration from biology, we study a simple algorithm that we call Digital Red Queen (DRQ), which embodies this idea in a computational setting. DRQ uses LLMs to evolve warriors under perpetual environmental change. Concretely, it begins with an initial warrior, then evolves a second warrior to defeat it in battle. A third warrior is then evolved to perform well against the first two, and so on. This process produces a lineage of warriors, each adapted to a changing environment defined by all of its predecessors.&lt;/p&gt;&lt;p&gt;DRQ is not intended to be a novel algorithm in itself. Rather, it is a minimal instantiation of prior multi-agent and self-play approaches, adapted to the Core War domain, designed to isolate and study the dynamics of continual coevolution.&lt;/p&gt;&lt;head rend="h2"&gt;Results&lt;/head&gt;&lt;p&gt;We find that as DRQ is run for many rounds, warriors gradually become more generally robust, as measured by their performance against unseen human-designed warriors. This provides a stable way to consistently produce more robust programs without needing to “train on the test set” (i.e., directly optimizing against a large set of human-designed programs).&lt;/p&gt;&lt;p&gt;More surprisingly, we observe that independent runs of DRQ, each initialized with different warriors, slowly converge over time toward warriors with similar behaviors. Notably, this convergence does not occur at the level of source code, indicating that what converges is function rather than implementation.&lt;/p&gt;&lt;lb/&gt;DRQ’s Convergent Evolution: With more rounds, DRQ produces warriors that are more generally robust. At the same time, across independent DRQ runs, the variance in the warrior’s behaviors decreases, indicating convergence.&lt;lb/&gt;Phenotypic Convergence: Convergence with rounds is seen only in the phenotype (behavior) of the warriors, and not the genotype (the source code), analogous to convergence in biological function rather than DNA.&lt;p&gt;This result is reminiscent of convergent evolution in biology, where similar functional traits evolved independently multiple times through different mechanisms. For example, birds and bats evolved wings separately, and spiders and snakes independently evolved venom. In these cases, evolution arrived at similar general-purpose solutions because the functional demands imposed by changing environments favored them.&lt;/p&gt;&lt;head rend="h2"&gt;Discussion&lt;/head&gt;&lt;p&gt;The emergence of convergent evolution from Red Queen dynamics, both commonly found in nature, hints that the DRQ algorithm and the Core War domain may be a promising setup for studying other properties of adversarial arms races. High level insights found in simulation could help inform how the arms race between LLMs in the wild might play out. Algorithms like DRQ could even help automate the “red-teaming” of systems before they are deployed in the real world.&lt;/p&gt;&lt;p&gt;The benefit of doing this research in a sandbox like Core War is that it’s completely self-contained: all programs run on an artificial machine with an artificial language, so nothing generated can execute outside the sandbox. This provides a safe space to explore adversarial dynamics that might be risky in the real world.&lt;/p&gt;&lt;p&gt; In a sandboxed Core War environment, we can simulate our evolved “warriors” and visualize their behaviors. The user can interactively visualize the assembly language (Redcode) of the warriors around where the mouse cursor is located. Please see our GitHub for more information.&lt;/p&gt;&lt;p&gt;Despite its simplicity, vanilla DRQ performs surprisingly well in Core War, suggesting that even minimal self-play loops can reveal complex and robust strategies. This makes DRQ a promising candidate for exploring other competitive multi-agent simulations in artificial life, biology, drug design, real-world cybersecurity, or market ecosystems. Future work could also explore richer setups where agents co-evolve simultaneously, better resembling the real-world where large populations adapt in parallel rather than along a single line of descent. Ultimately the insights gathered will help control the future for the better and help us understand the science of these evolutionary arms races.&lt;/p&gt;&lt;head rend="h2"&gt;Sakana AI&lt;/head&gt;&lt;p&gt;We are taking this technology far beyond adversarial competitive programming to unlock a new era of AI-driven discovery.&lt;/p&gt;&lt;p&gt;If you are interested in advancing AI-driven discovery, we’re hiring!&lt;/p&gt;&lt;p&gt;Sakana AI is at the forefront of AI-driven discovery. In addition to this work, we are also behind works such as The AI Scientist, LLM-Squared, Shinka-Evolve, Automating the Search for Artificial Life and ALE-Agent. We’re looking for engineers to join our team to work on our advanced AI-driven discovery platform and productionize our model-development efforts.&lt;/p&gt;&lt;p&gt;Please see our career opportunities for more information.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46542761</guid><pubDate>Thu, 08 Jan 2026 16:16:43 +0000</pubDate></item><item><title>Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space</title><link>https://arxiv.org/abs/2512.24617</link><description>&lt;doc fingerprint="e09c660bc4d70aae"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Machine Learning&lt;/head&gt;&lt;p&gt; [Submitted on 31 Dec 2025 (v1), last revised 5 Jan 2026 (this version, v2)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose $\textbf{Dynamic Large Concept Models (DLCM)}$, a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first $\textbf{compression-aware scaling law}$, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a $\textbf{decoupled $\mu$P parametrization}$ that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting ($R=4$, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a $\textbf{+2.69$\%$ average improvement}$ across 12 zero-shot benchmarks under matched inference FLOPs.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Xingwei Qu [view email]&lt;p&gt;[v1] Wed, 31 Dec 2025 04:19:33 UTC (2,886 KB)&lt;/p&gt;&lt;p&gt;[v2] Mon, 5 Jan 2026 05:44:29 UTC (2,887 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46542982</guid><pubDate>Thu, 08 Jan 2026 16:31:29 +0000</pubDate></item><item><title>Tamarind Bio (YC W24) Is Hiring Infrastructure Engineers</title><link>https://www.ycombinator.com/companies/tamarind-bio/jobs/HPRZAz3-infrastructure-engineer</link><description>&lt;doc fingerprint="7b11fcebda9747c3"&gt;
  &lt;main&gt;
    &lt;p&gt;Easy to use computational biology tools for drug discovery&lt;/p&gt;
    &lt;p&gt;We're looking for an Infrastructure Engineer to lead the scaling of our machine learning inference system. You'll be responsible for architecting and maintaining infrastructure that serves 150+ biological ML models, scaling our platform several orders of magnitude to meet rapidly growing demand.&lt;/p&gt;
    &lt;p&gt;You’ll work closely with the founders to design to the constraints of customer needs, unpredictable workloads, and unique Bio-ML models. You'll work with Kubernetes and other tools to orchestrate containerized workloads, optimize resource allocation, and ensure high availability across our model serving infrastructure.&lt;/p&gt;
    &lt;p&gt;Most importantly, you should thrive in a fast-paced startup environment where you'll wear multiple hats, learn new technologies quickly, and help solve novel technical challenges. We value engineering judgment, problem-solving ability, and the capacity to build systems that can evolve with our growing needs.&lt;/p&gt;
    &lt;p&gt;Requirements&lt;/p&gt;
    &lt;p&gt;Preferred&lt;/p&gt;
    &lt;p&gt;We enable any scientist to access AI-powered drug discovery. Thousands of scientists from large pharma companies, top biotechs, and academic institutions use Tamarind to design protein drugs, improve industrial enzymes, and create cutting edge molecules that weren’t feasible until now.&lt;/p&gt;
    &lt;p&gt;New AI models are quickly eclipsing physics-based tools in computational drug discovery. Scientists often struggle to fine-tune, deploy, and scale these models, leaving breakthroughs on the table. Tamarind provides a simple interface to the vast array of tools being released daily.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46543403</guid><pubDate>Thu, 08 Jan 2026 17:01:00 +0000</pubDate></item><item><title>China starts UHV power line: The new 700 km UHV line will transmit 8M kW</title><link>https://switchgear-magazine.com/tm-news/business/china-starts-uhv-power-line/</link><description>&lt;doc fingerprint="39541155eb865992"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;China starts UHV power line&lt;/head&gt;
    &lt;head rend="h3"&gt;The new 700 km UHV line will transmit 8 million kilowatts of renewable energy from Inner Mongolia to Beijing, Tianjin, and Hebei by 2027.&lt;/head&gt;
    &lt;p&gt;Image for illustrative purposes&lt;/p&gt;
    &lt;p&gt;China: China has officially begun construction on a major ultra-high voltage (UHV) transmission project, aimed at moving large volumes of renewable electricity from western Inner Mongolia to the industrial regions of Beijing, Tianjin, and Hebei province.&lt;/p&gt;
    &lt;p&gt;The direct current line will stretch 700 km and carry up to 8 million kilowatts of power, with operations expected to commence by 2027, according to the project operator, State Grid Corp of China, the world’s largest utility company.&lt;/p&gt;
    &lt;p&gt;With an investment of $2.5 B (17.2 B yuan), this is the first UHV line in Inner Mongolia specifically designed to support China’s national strategy for large-scale wind and solar generation in the country’s sandy, Gobi, and desert regions.&lt;/p&gt;
    &lt;p&gt;Ultra-high voltage lines operate at voltages above 800 kV for direct current or 1,000 kV for alternating current, allowing electricity to be transmitted efficiently over vast distances while minimising energy loss.&lt;/p&gt;
    &lt;p&gt;The project represents a key step in China’s push to integrate renewable energy into its national grid and strengthen the energy supply to some of the country’s most densely industrialised regions.&lt;/p&gt;
    &lt;p&gt;Source: China Daily&lt;/p&gt;
    &lt;p&gt;#China#high voltage#Inner Mongolia#power infrastructure#renewable energy#State Grid#UHV transmission&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46543807</guid><pubDate>Thu, 08 Jan 2026 17:31:54 +0000</pubDate></item><item><title>Chinese AI models have lagged the US frontier by 7 months on average since 2023</title><link>https://epoch.ai/data-insights/us-vs-china-eci</link><description>&lt;doc fingerprint="3f3b95d8a43414da"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Chinese AI models have lagged the US frontier by 7 months on average since 2023&lt;/head&gt;
    &lt;p&gt;Since 2023, every model at the frontier of AI capabilities, as measured by the Epoch Capabilities Index, has been developed in the United States. Over that same period, Chinese models have trailed US capabilities by an average of seven months, with a minimum gap of four months and a maximum gap of 14.&lt;/p&gt;
    &lt;p&gt;This gap closely resembles the broader gap between proprietary and open-weight models. This is unsurprising since nearly all leading Chinese models are open-weight, while frontier US models remain closed.&lt;/p&gt;
    &lt;p&gt;Authors&lt;/p&gt;
    &lt;p&gt;Published&lt;/p&gt;
    &lt;p&gt;January 2, 2026&lt;/p&gt;
    &lt;head rend="h2"&gt;Learn more&lt;/head&gt;
    &lt;head rend="h4"&gt;Overview&lt;/head&gt;
    &lt;p&gt;We visualize the gap in capabilities between US and Chinese models, using the Epoch Capabilities Index (ECI). Since 2023, the gap has ranged from 4 to 14 months, with a mean gap of 7 months.&lt;/p&gt;
    &lt;head rend="h4"&gt;Analysis&lt;/head&gt;
    &lt;p&gt;To calculate the gap between US and Chinese models, we first find the set of models that had the highest ECI among models from their country upon release. We then drop the first of these models (LLaMA-65B for the US, and Baichuan1-7B for China), since these first models were likely not at the true frontier (ECI data starts in January 2023).&lt;/p&gt;
    &lt;p&gt;To quantify the gap on each day, we look at the ECI of the best Chinese model on that day, and then calculate how long it has been since the last time the leading US model was the same or worse than that score. We consider models to be the same performance if their scores are within 1 ECI point difference. We repeat this process for each day where values exist for both the US and China. In practice, the first point where a Chinese model surpasses GPT-4 is May 2024 (a gap of 14 months), and no Chinese model has yet surpassed the ECI of OpenAI’s o3 model, released in April 2025.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46543933</guid><pubDate>Thu, 08 Jan 2026 17:40:02 +0000</pubDate></item><item><title>Nvidia Kicks Off the Next Generation of AI with Rubin</title><link>https://nvidianews.nvidia.com/news/rubin-platform-ai-supercomputer</link><description>&lt;doc fingerprint="ed7d8c6a4edc4988"&gt;
  &lt;main&gt;
    &lt;p&gt;News Summary:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Rubin platform harnesses extreme codesign across hardware and software to deliver up to 10x reduction in inference token cost and 4x reduction in number of GPUs to train MoE models, compared with the NVIDIA Blackwell platform.&lt;/item&gt;
      &lt;item&gt;NVIDIA Spectrum-X Ethernet Photonics switch systems deliver 5x improved power efficiency and uptime.&lt;/item&gt;
      &lt;item&gt;New NVIDIA Inference Context Memory Storage Platform with NVIDIA BlueField-4 storage processor to accelerate agentic AI reasoning.&lt;/item&gt;
      &lt;item&gt;Microsoft’s next-generation Fairwater AI superfactories — featuring NVIDIA Vera Rubin NVL72 rack-scale systems — will scale to hundreds of thousands of NVIDIA Vera Rubin Superchips.&lt;/item&gt;
      &lt;item&gt;CoreWeave among first to offer NVIDIA Rubin, operated through CoreWeave Mission Control for flexibility and performance.&lt;/item&gt;
      &lt;item&gt;Expanded collaboration with Red Hat to deliver a complete AI stack optimized for the Rubin platform with Red Hat Enterprise Linux, Red Hat OpenShift and Red Hat AI.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;CES—NVIDIA today kickstarted the next generation of AI with the launch of the NVIDIA Rubin platform, comprising six new chips designed to deliver one incredible AI supercomputer. NVIDIA Rubin sets a new standard for building, deploying and securing the world’s largest and most advanced AI systems at the lowest cost to accelerate mainstream AI adoption.&lt;/p&gt;
    &lt;p&gt;The Rubin platform uses extreme codesign across the six chips — the NVIDIA Vera CPU, NVIDIA Rubin GPU, NVIDIA NVLink™ 6 Switch, NVIDIA ConnectX®-9 SuperNIC, NVIDIA BlueField®-4 DPU and NVIDIA Spectrum™-6 Ethernet Switch — to slash training time and inference token costs.&lt;/p&gt;
    &lt;p&gt;“Rubin arrives at exactly the right moment, as AI computing demand for both training and inference is going through the roof,” said Jensen Huang, founder and CEO of NVIDIA. “With our annual cadence of delivering a new generation of AI supercomputers — and extreme codesign across six new chips — Rubin takes a giant leap toward the next frontier of AI.”&lt;/p&gt;
    &lt;p&gt;Named for Vera Florence Cooper Rubin — the trailblazing American astronomer whose discoveries transformed humanity’s understanding of the universe — the Rubin platform features the NVIDIA Vera Rubin NVL72 rack-scale solution and the NVIDIA HGX Rubin NVL8 system.&lt;/p&gt;
    &lt;p&gt;The Rubin platform introduces five innovations, including the latest generations of NVIDIA NVLink interconnect technology, Transformer Engine, Confidential Computing and RAS Engine, as well as the NVIDIA Vera CPU. These breakthroughs will accelerate agentic AI, advanced reasoning and massive-scale mixture-of-experts (MoE) model inference at up to 10x lower cost per token of the NVIDIA Blackwell platform. Compared with its predecessor, the NVIDIA Rubin platform trains MoE models with 4x fewer GPUs to accelerate AI adoption.&lt;/p&gt;
    &lt;p&gt;Broad Ecosystem Support&lt;lb/&gt; Among the world’s leading AI labs, cloud service providers, computer makers and startups expected to adopt Rubin are Amazon Web Services (AWS), Anthropic, Black Forest Labs, Cisco, Cohere, CoreWeave, Cursor, Dell Technologies, Google, Harvey, HPE, Lambda, Lenovo, Meta, Microsoft, Mistral AI, Nebius, Nscale, OpenAI, OpenEvidence, Oracle Cloud Infrastructure (OCI), Perplexity, Runway, Supermicro, Thinking Machines Lab and xAI.&lt;/p&gt;
    &lt;p&gt;Sam Altman, CEO of OpenAI: “Intelligence scales with compute. When we add more compute, models get more capable, solve harder problems and make a bigger impact for people. The NVIDIA Rubin platform helps us keep scaling this progress so advanced intelligence benefits everyone.”&lt;/p&gt;
    &lt;p&gt;Dario Amodei, cofounder and CEO of Anthropic: “The efficiency gains in the NVIDIA Rubin platform represent the kind of infrastructure progress that enables longer memory, better reasoning and more reliable outputs. Our collaboration with NVIDIA helps power our safety research and our frontier models.”&lt;/p&gt;
    &lt;p&gt;Mark Zuckerberg, founder and CEO of Meta: “NVIDIA’s Rubin platform promises to deliver the step-change in performance and efficiency required to deploy the most advanced models to billions of people.”&lt;/p&gt;
    &lt;p&gt;Elon Musk, founder and CEO of xAI: “💚🎉🚀 🤖NVIDIA Rubin will be a rocket engine for AI. If you want to train and deploy frontier models at scale, this is the infrastructure you use — and Rubin will remind the world that NVIDIA is the gold standard.💚🎉🚀 🤖”&lt;/p&gt;
    &lt;p&gt;Satya Nadella, executive chairman and CEO of Microsoft: “We are building the world’s most powerful AI superfactories to serve any workload, anywhere, with maximum performance and efficiency. With the addition of NVIDIA Vera Rubin GPUs, we will empower developers and organizations to create, reason and scale in entirely new ways.”&lt;/p&gt;
    &lt;p&gt;Mike Intrator, cofounder and CEO of CoreWeave: “We built CoreWeave to help pioneers accelerate their innovations with the unmatched performance of our purpose-built AI platform, matching the right technology to the right workloads as they evolve. The NVIDIA Rubin platform represents an important advancement for reasoning, agentic and large-scale inference workloads, and we’re excited to add it to our platform. With CoreWeave Mission Control as the operating standard, we can integrate new capabilities quickly and run them reliably at production scale, working in close partnership with NVIDIA.”&lt;/p&gt;
    &lt;p&gt;Matt Garman, CEO of AWS: “AWS and NVIDIA have been driving cloud AI innovation together for more than 15 years. The NVIDIA Rubin platform on AWS represents our continued commitment to delivering cutting-edge AI infrastructure that gives customers unmatched choice and flexibility. By combining NVIDIA’s advanced AI technology with AWS’s proven scale, security and comprehensive AI services, customers can build, train and deploy their most demanding AI applications faster and more cost effectively — accelerating their path from experimentation to production at any scale.”&lt;/p&gt;
    &lt;p&gt;Sundar Pichai, CEO of Google and Alphabet: “We are proud of our deep and long-standing relationship with NVIDIA. To meet the substantial customer demand we see for NVIDIA GPUs, we are focused on providing the best possible environment for their hardware on Google Cloud. Our collaboration will continue as we bring the impressive capabilities of the Rubin platform to our customers, offering them the scale and performance needed to advance the boundaries of AI.”&lt;/p&gt;
    &lt;p&gt;Clay Magouyrk, CEO of Oracle: “Oracle Cloud Infrastructure is a hyperscale cloud built for the highest performance, and together with NVIDIA, we’re pushing the boundaries of what customers can build and scale with AI. With gigascale AI factories powered by the NVIDIA Vera Rubin architecture, OCI is giving customers the infrastructure foundation they need to push the limits of model training, inference and real-world AI impact.”&lt;/p&gt;
    &lt;p&gt;Michael Dell, chairman and CEO of Dell Technologies: “The NVIDIA Rubin platform represents a major leap forward in AI infrastructure. By integrating Rubin into the Dell AI Factory with NVIDIA, we’re building infrastructure that can handle massive token volumes and multistep reasoning while delivering the performance and resiliency that enterprises and neoclouds need to deploy AI at scale.”&lt;/p&gt;
    &lt;p&gt;Antonio Neri, president and CEO of HPE: “AI is reshaping not just workloads but the very foundations of IT, requiring us to reimagine every layer of infrastructure from the network to the compute. With the NVIDIA Vera Rubin platform, HPE is building the next generation of secure, AI-native infrastructure, turning data into intelligence and enabling enterprises to become true AI factories.”&lt;/p&gt;
    &lt;p&gt;Yuanqing Yang, chairman and CEO of Lenovo: “Lenovo is embracing the next-generation NVIDIA Rubin platform, leveraging our Neptune liquid-cooling solution as well as our global scale, manufacturing efficiency and service reach, to help enterprises build AI factories that serve as intelligent, accelerated engines for insight and innovation. Together, we’re architecting an AI-driven future where efficient, secure AI becomes the standard for every organization.”&lt;/p&gt;
    &lt;p&gt;Engineered to Scale Intelligence&lt;lb/&gt; Agentic AI and reasoning models, along with state-of-the-art video generation workloads, are redefining the limits of computation. Multistep problem-solving requires models to process, reason and act across long sequences of tokens. Designed to serve the demands of complex AI workloads, the Rubin platform’s five groundbreaking technologies include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sixth-Generation NVIDIA NVLink: Delivers the fast, seamless GPU-to-GPU communication required for today’s massive MoE models. Each GPU offers 3.6TB/s of bandwidth, while the Vera Rubin NVL72 rack provides 260TB/s — more bandwidth than the entire internet. With built-in, in-network compute to speed collective operations, as well as new features for enhanced serviceability and resiliency, NVIDIA NVLink 6 switch enables faster, more efficient AI training and inference at scale.&lt;/item&gt;
      &lt;item&gt;NVIDIA Vera CPU: Designed for agentic reasoning, NVIDIA Vera is the most power‑efficient CPU for large-scale AI factories. The NVIDIA CPU is built with 88 NVIDIA custom Olympus cores, full Armv9.2 compatibility and ultrafast NVLink-C2C connectivity. Vera delivers exceptional performance, bandwidth and industry‑leading efficiency to support a full range of modern data center workloads.&lt;/item&gt;
      &lt;item&gt;NVIDIA Rubin GPU: Featuring a third-generation Transformer Engine with hardware-accelerated adaptive compression, Rubin GPU delivers 50 petaflops of NVFP4 compute for AI inference.&lt;/item&gt;
      &lt;item&gt;Third-Generation NVIDIA Confidential Computing: Vera Rubin NVL72 is the first rack-scale platform to deliver NVIDIA Confidential Computing — which maintains data security across CPU, GPU and NVLink domains — protecting the world’s largest proprietary models, training and inference workloads.&lt;/item&gt;
      &lt;item&gt;Second-Generation RAS Engine: The Rubin platform — spanning GPU, CPU and NVLink — features real-time health checks, fault tolerance and proactive maintenance to maximize system productivity. The rack’s modular, cable-free tray design enables up to 18x faster assembly and servicing than Blackwell.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;AI-Native Storage and Secure, Software-Defined Infrastructure&lt;lb/&gt; NVIDIA Rubin introduces NVIDIA Inference Context Memory Storage Platform, a new class of AI-native storage infrastructure designed to scale inference context at gigascale.&lt;/p&gt;
    &lt;p&gt;Powered by NVIDIA BlueField-4, the platform enables efficient sharing and reuse of key-value cache data across AI infrastructure, improving responsiveness and throughput while enabling predictable, power-efficient scaling of agentic AI.&lt;/p&gt;
    &lt;p&gt;As AI factories increasingly adopt bare-metal and multi-tenant deployment models, maintaining strong infrastructure control and isolation becomes essential.&lt;/p&gt;
    &lt;p&gt;BlueField-4 also introduces Advanced Secure Trusted Resource Architecture, or ASTRA, a system-level trust architecture that gives AI infrastructure builders a single, trusted control point to securely provision, isolate and operate large-scale AI environments without compromising performance.&lt;/p&gt;
    &lt;p&gt;With AI applications evolving toward multi-turn agentic reasoning, AI-native organizations must manage and share far larger volumes of inference context across users, sessions and services.&lt;/p&gt;
    &lt;p&gt;Different Forms for Different Workloads&lt;lb/&gt; NVIDIA Vera Rubin NVL72 offers a unified, secure system that combines 72 NVIDIA Rubin GPUs, 36 NVIDIA Vera CPUs, NVIDIA NVLink 6, NVIDIA ConnectX-9 SuperNICs and NVIDIA BlueField-4 DPUs.&lt;/p&gt;
    &lt;p&gt;NVIDIA will also offer the NVIDIA HGX Rubin NVL8 platform, a server board that links eight Rubin GPUs through NVLink to support x86-based generative AI platforms. The HGX Rubin NVL8 platform accelerates training, inference and scientific computing for AI and high-performance computing workloads.&lt;/p&gt;
    &lt;p&gt;NVIDIA DGX SuperPOD™ serves as a reference for deploying Rubin-based systems at scale, integrating either NVIDIA DGX Vera Rubin NVL72 or DGX Rubin NVL8 systems with NVIDIA BlueField-4 DPUs, NVIDIA ConnectX-9 SuperNICs, NVIDIA InfiniBand networking and NVIDIA Mission Control™ software.&lt;/p&gt;
    &lt;p&gt;Next-Generation Ethernet Networking&lt;lb/&gt; Advanced Ethernet networking and storage are components of AI infrastructure critical to keeping data centers running at full speed, improving performance and efficiency, and lowering costs.&lt;/p&gt;
    &lt;p&gt;NVIDIA Spectrum-6 Ethernet is the next generation of Ethernet for AI networking, built to scale Rubin-based AI factories with higher efficiency and greater resilience, and enabled by 200G SerDes communication circuitry, co-packaged optics and AI-optimized fabrics.&lt;/p&gt;
    &lt;p&gt;Built on the Spectrum-6 architecture, Spectrum-X Ethernet Photonics co-packaged optical switch systems deliver 10x greater reliability and 5x longer uptime for AI applications while achieving 5x better power efficiency, maximizing performance per watt compared with traditional methods. Spectrum-XGS Ethernet technology, part of the Spectrum-X Ethernet platform, enables facilities separated by hundreds of kilometers and more to function as a single AI environment.&lt;/p&gt;
    &lt;p&gt;Together, these innovations define the next generation of the NVIDIA Spectrum-X Ethernet platform, engineered with extreme codesign for Rubin to enable massive-scale AI factories and pave the way for future million-GPU environments.&lt;/p&gt;
    &lt;p&gt;Rubin Readiness&lt;lb/&gt; NVIDIA Rubin is in full production, and Rubin-based products will be available from partners the second half of 2026.&lt;/p&gt;
    &lt;p&gt;Among the first cloud providers to deploy Vera Rubin-based instances in 2026 will be AWS, Google Cloud, Microsoft and OCI, as well as NVIDIA Cloud Partners CoreWeave, Lambda, Nebius and Nscale.&lt;/p&gt;
    &lt;p&gt;Microsoft will deploy NVIDIA Vera Rubin NVL72 rack-scale systems as part of next-generation AI data centers, including future Fairwater AI superfactory sites.&lt;/p&gt;
    &lt;p&gt;Designed to deliver unprecedented efficiency and performance for training and inference workloads, the Rubin platform will provide the foundation for Microsoft’s next-generation cloud AI capabilities. Microsoft Azure will offer a tightly optimized platform enabling customers to accelerate innovation across enterprise, research and consumer applications.&lt;/p&gt;
    &lt;p&gt;CoreWeave will integrate NVIDIA Rubin-based systems into its AI cloud platform beginning in the second half of 2026. CoreWeave is built to operate multiple architectures side by side, enabling customers to bring Rubin into their environments, where it will deliver the greatest impact across training, inference and agentic workloads.&lt;/p&gt;
    &lt;p&gt;Together with NVIDIA, CoreWeave will help AI pioneers take advantage of Rubin’s advancements in reasoning and MoE models, while continuing to deliver the performance, operational reliability and scale required for production AI across the full lifecycle with CoreWeave Mission Control.&lt;/p&gt;
    &lt;p&gt;In addition, Cisco, Dell, HPE, Lenovo and Supermicro are expected to deliver a wide range of servers based on Rubin products.&lt;/p&gt;
    &lt;p&gt;AI labs including Anthropic, Black Forest, Cohere, Cursor, Harvey, Meta, Mistral AI, OpenAI, OpenEvidence, Perplexity, Runway, Thinking Machines Lab and xAI are looking to the NVIDIA Rubin platform to train larger, more capable models and to serve long-context, multimodal systems at lower latency and cost than with prior GPU generations.&lt;/p&gt;
    &lt;p&gt;Infrastructure software and storage partners AIC, Canonical, Cloudian, DDN, Dell, HPE, Hitachi Vantara, IBM, NetApp, Nutanix, Pure Storage, Supermicro, SUSE, VAST Data and WEKA are working with NVIDIA to design next-generation platforms for Rubin infrastructure.&lt;/p&gt;
    &lt;p&gt;The Rubin platform marks NVIDIA’s third-generation rack-scale architecture, with more than 80 NVIDIA MGX™ ecosystem partners.&lt;/p&gt;
    &lt;p&gt;To unlock this density, Red Hat today announced an expanded collaboration with NVIDIA to deliver a complete AI stack optimized for the NVIDIA Rubin platform with Red Hat’s hybrid cloud portfolio, including Red Hat Enterprise Linux, Red Hat OpenShift and Red Hat AI. These solutions are used by the vast majority of Fortune Global 500 companies.&lt;/p&gt;
    &lt;p&gt;Learn more by watching NVIDIA Live at CES and reading the “Inside Vera Rubin” technical blog.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46544016</guid><pubDate>Thu, 08 Jan 2026 17:45:54 +0000</pubDate></item><item><title>Supernova Remnant Video from NASA's Chandra Is Decades in Making</title><link>https://www.nasa.gov/missions/chandra/supernova-remnant-video-from-nasas-chandra-is-decades-in-making/</link><description>&lt;doc fingerprint="36560011a9240c95"&gt;
  &lt;main&gt;
    &lt;p&gt;A new video shows the evolution of Kepler’s Supernova Remnant using data from NASA’s Chandra X-ray Observatory captured over more than two and a half decades.&lt;/p&gt;
    &lt;p&gt;Kepler’s Supernova Remnant, named after the German astronomer Johannes Kepler, was first spotted in the night sky in 1604. Today, astronomers know that a white dwarf star exploded when it exceeded a critical mass, after pulling material from a companion star, or merging with another white dwarf. This kind of supernova is known as a Type Ia, and scientists use it to measure the expansion of the universe.&lt;/p&gt;
    &lt;p&gt;Supernova remnants, the debris fields left behind after a stellar explosion, often glow strongly in X-ray light because the material has been heated to millions of degrees from the blast. The remnant is located in our galaxy, about 17,000 light-years from Earth, allowing Chandra to make detailed images of the debris and how it changes with time. This latest video includes its X-ray data from 2000, 2004, 2006, 2014, and 2025. This makes it the longest-spanning video that Chandra has ever released, enabled by Chandra’s longevity.&lt;/p&gt;
    &lt;p&gt;“The plot of Kepler’s story is just now beginning to unfold,” said Jessye Gassel, a graduate student at George Mason University in Virginia, who led the work. “It’s remarkable that we can watch as these remains from this shattered star crash into material already thrown out into space.” Gassel presented the new Chandra video and the associated research at the 247th meeting of the American Astronomical Society in Phoenix.&lt;/p&gt;
    &lt;p&gt;The researchers used the video to show that the fastest parts of the remnant are traveling at about 13.8 million miles per hour (2% of the speed of light), moving toward the bottom of the image. Meanwhile, the slowest parts are traveling toward the top at about 4 million miles per hour (0.5% of the speed of light). This large difference in speed is because the gas that the remnant is plowing into toward the top of the image is denser than the gas toward the bottom. This gives scientists information about the environments into which this star exploded.&lt;/p&gt;
    &lt;p&gt;“Supernova explosions and the elements they hurl into space are the lifeblood of new stars and planets,” said Brian Williams of NASA’s Goddard Space Flight Center in Greenbelt, Maryland, and principal investigator of the new Chandra observations of Kepler. “Understanding exactly how they behave is crucial to knowing our cosmic history.”&lt;/p&gt;
    &lt;p&gt;The team also examined the widths of the rims forming the blast wave of the explosion. The blast wave is the leading edge of the explosion and the first to encounter material outside of the star. By measuring how wide it is and how fast it is traveling, astronomers glean more information about both the explosion of the star and its surroundings.&lt;/p&gt;
    &lt;p&gt;NASA’s Marshall Space Flight Center in Huntsville, Alabama, manages the Chandra program. The Smithsonian Astrophysical Observatory’s Chandra X-ray Center controls science operations from Cambridge, Massachusetts, and flight operations from Burlington, Massachusetts.&lt;/p&gt;
    &lt;p&gt;To learn more about Chandra, visit:&lt;/p&gt;
    &lt;p&gt;https://science.nasa.gov/chandra&lt;/p&gt;
    &lt;p&gt;Read more from NASA’s Chandra X-ray Observatory&lt;/p&gt;
    &lt;p&gt;Learn more about the Chandra X-ray Observatory and its mission here:&lt;/p&gt;
    &lt;head rend="h2"&gt;Visual Description&lt;/head&gt;
    &lt;p&gt;This release features a ten second silent video of Kepler’s expanding Supernova Remnant, located in our own galaxy, about 17,000 light-years from Earth. The video was created using X-ray data gathered in 2000, 2004, 2006, 2014, and 2025. Those distinct datasets were turned into highly-detailed visuals, creating a 25-year timelapse-style video of the growing remnant.&lt;/p&gt;
    &lt;p&gt;Kepler’s Supernova Remnant was once a white dwarf star that exploded when it exceeded its critical mass. Here, in X-ray light, the remnant resembles a cloudy neon blue ring with a diagonal cross line stretching from our upper right down to our lower left. The ring appears thinner and wispier at the bottom, with a band of white arching across the top.&lt;/p&gt;
    &lt;p&gt;As the video plays, cycling through the 5 datasets, the ring subtly, but clearly, expands, like a slowly inflating balloon. In the video, this sequence is replayed several times with dates included at our lower right, to give sighted learners time to absorb the visual information. Upon close inspection, researchers have determined that the bottom of the remnant is expanding fastest; about 13.8 million miles per hour, or 2% of the speed of light. The top of the ring appears to be expanding the slowest; about 4 million miles per hour, or 0.5% of the speed of light. The large difference in speed is because the gas that the remnant is plowing into towards the top of the image is denser than the gas towards the bottom.&lt;/p&gt;
    &lt;p&gt;Collecting and interpreting this data over decades has provided information about the environment into which the white dwarf star exploded, and has helped scientists understand how remnants change with time.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46544072</guid><pubDate>Thu, 08 Jan 2026 17:50:15 +0000</pubDate></item><item><title>IBM AI ('Bob') Downloads and Executes Malware</title><link>https://www.promptarmor.com/resources/ibm-ai-(-bob-)-downloads-and-executes-malware</link><description>&lt;doc fingerprint="a4819d99293a7f93"&gt;
  &lt;main&gt;
    &lt;p&gt;Threat Intelligence&lt;/p&gt;
    &lt;head rend="h1"&gt;IBM AI ('Bob') Downloads and Executes Malware&lt;/head&gt;
    &lt;p&gt;IBM's AI coding agent 'Bob' has been found vulnerable to downloading and executing malware without human approval through command validation bypasses exploited using indirect prompt injection.&lt;/p&gt;
    &lt;p&gt;A vulnerability has been identified that allows malicious actors to exploit IBM Bob to download and execute malware without human approval if the user configures âalways allowâ for any command.&lt;lb/&gt;IBM Bob is IBMâs new coding agent, currently in Closed Beta. IBM Bob is offered through the Bob CLI (a terminal-based coding agent like Claude Code or OpenAI Codex) and the Bob IDE (an AI-powered editor similar to Cursor).&lt;lb/&gt;In this article, we demonstrate that the Bob CLI is vulnerable to prompt injection attacks resulting in malware execution, and the Bob IDE is vulnerable to known AI-specific data exfiltration vectors. &lt;/p&gt;
    &lt;p&gt;In the documentation, IBM warns that setting auto-approve for commands constitutes a 'high risk' that can 'potentially execute harmful operations' - with the recommendation that users leverage whitelists and avoid wildcards. We have opted to disclose this work publicly to ensure users are informed of the acute risks of using the system prior to its full release. We hope that further protections will be in place to remediate these risks for IBM Bob's General Access release.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Attack Chain&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The user wants to explore a new repository - they ask Bob for help.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Bob encounters an instruction at the bottom of the README that manipulates it into believing it is responsible for conducting a phishing training to test the user.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Bob prompts the user several times with benign âechoâ commands; after the third time, the user selects âalways allowâ for execution of âechoâ.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Bob attempts to âtestâ the user as part of the training by offering a dangerous command. However, the command has been specially crafted to bypass built-in defenses, so it executes immediately, installing and running a script retrieved from an attackerâs server.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Bob has three defenses that are bypassed in this attack&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;When a multi-part command is requested (using operators like â;â), the user is shown a request that asks for permission to run each sub-command.&lt;/p&gt;
        &lt;p&gt;Note, this modal says: âAllow execution of echo, catâ.&lt;/p&gt;
        &lt;p&gt;This is bypassed because the defense system fails to identify separate sub-commands when they are chained using a redirect operator (&amp;gt;).&lt;/p&gt;
        &lt;p&gt;Note, this modal only mentions: âAllow execution of echoâ.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Bob prohibits the use of command substitution like $(command) as a security measure.&lt;/p&gt;&lt;p&gt;However, even though the security message says it's restricted, the underlying code fails to adequately restrict evaluation via process substitution: &amp;gt;(command).&lt;/p&gt;&lt;p&gt;This allows for the output of the malicious sub-command that retrieves the malware to be used as the input for the sub-command to execute that malware.&lt;/p&gt;&lt;lb/&gt;One can examine where the defense should have been in the minified JS source code:&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;By pre-fixing the malicious command with a benign âechoâ, which the user has selected âalways approveâ for, the user inadvertently auto-approves the entire malicious payload (since bypasses 1 and 2 prevent the payload from being recognized as separate commands).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Impact&lt;/head&gt;
    &lt;p&gt;With the ability to deliver an arbitrary shell script payload to the victim, attackers can leverage known (or custom) malware variants to conduct cyber attacks such as:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Executing ransomware that encrypts or deletes files&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Credential theft or spyware deployment&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Device takeover (opening a reverse shell)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Forcing the victim into a cryptocurrency-mining botnet&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Together, these outcomes demonstrate how a prompt injection can escalate into a full-scale compromise of a userâs machine through vulnerabilities in the IBM Bob CLI.&lt;/p&gt;
    &lt;head rend="h3"&gt;Further Findings&lt;/head&gt;
    &lt;p&gt;Additional findings indicate that the Bob IDE is susceptible to several known zero-click data exfiltration vectors that affect many AI applications:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Markdown images are rendered in model outputs, with a Content Security Policy that allows requests to endpoints that can be logged by attackers (storage.googleapis.com).&lt;/p&gt;&lt;lb/&gt;Here is an interesting spin on the typical Markdown image attack where, beyond just exfiltrating data from query parameters as the image is rendered, the image itself is hyperlinked and made to pose as a button - used for phishing.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mermaid diagrams supporting external images are rendered in model outputs, with a Content Security Policy that allows requests to endpoints that can be logged by attackers (storage.googleapis.com).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;JSON schemas are pre-fetched, which can yield data exfiltration if a dynamically generated attacker-controlled URL is provided in the field (this can happen before a file edit is accepted).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46544454</guid><pubDate>Thu, 08 Jan 2026 18:19:09 +0000</pubDate></item></channel></rss>