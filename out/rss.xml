<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 21 Oct 2025 02:22:31 +0000</lastBuildDate><item><title>A laser pointer at 2B FPS [video]</title><link>https://www.youtube.com/watch?v=o4TdHrMi6do</link><description>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket ¬© 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45632429</guid><pubDate>Sun, 19 Oct 2025 06:42:19 +0000</pubDate></item><item><title>Space Elevator</title><link>https://neal.fun/space-elevator/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45640226</guid><pubDate>Mon, 20 Oct 2025 04:42:08 +0000</pubDate></item><item><title>DeepSeek OCR</title><link>https://github.com/deepseek-ai/DeepSeek-OCR</link><description>&lt;doc fingerprint="d0978f309aa0d982"&gt;
  &lt;main&gt;
    &lt;p&gt;üì• Model Download | üìÑ Paper Link | üìÑ Arxiv Paper Link |&lt;/p&gt;
    &lt;p&gt;Explore the boundaries of visual-text compression.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;[2025/10/20]üöÄüöÄüöÄ We release DeepSeek-OCR, a model to investigate the role of vision encoders from an LLM-centric viewpoint.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;Our environment is cuda11.8+torch2.6.0.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Clone this repository and navigate to the DeepSeek-OCR folder&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/deepseek-ai/DeepSeek-OCR.git&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Conda&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;conda create -n deepseek-ocr python=3.12.9 -y
conda activate deepseek-ocr&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Packages&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;download the vllm-0.8.5 whl&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118
pip install vllm-0.8.5+cu118-cp38-abi3-manylinux1_x86_64.whl
pip install -r requirements.txt
pip install flash-attn==2.7.3 --no-build-isolation&lt;/code&gt;
    &lt;p&gt;Note: if you want vLLM and transformers codes to run in the same environment, you don't need to worry about this installation error like: vllm 0.8.5+cu118 requires transformers&amp;gt;=4.51.1&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;VLLM:&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: change the INPUT_PATH/OUTPUT_PATH and other settings in the DeepSeek-OCR-master/DeepSeek-OCR-vllm/config.py&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;cd DeepSeek-OCR-master/DeepSeek-OCR-vllm&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;image: streaming output&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;python run_dpsk_ocr_image.py&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;pdf: concurrency ~2500tokens/s(an A100-40G)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;python run_dpsk_ocr_pdf.py&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;batch eval for benchmarks&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;python run_dpsk_ocr_eval_batch.py&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Transformers&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;from transformers import AutoModel, AutoTokenizer
import torch
import os
os.environ["CUDA_VISIBLE_DEVICES"] = '0'
model_name = 'deepseek-ai/DeepSeek-OCR'

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModel.from_pretrained(model_name, _attn_implementation='flash_attention_2', trust_remote_code=True, use_safetensors=True)
model = model.eval().cuda().to(torch.bfloat16)

# prompt = "&amp;lt;image&amp;gt;\nFree OCR. "
prompt = "&amp;lt;image&amp;gt;\n&amp;lt;|grounding|&amp;gt;Convert the document to markdown. "
image_file = 'your_image.jpg'
output_path = 'your/output/dir'

res = model.infer(tokenizer, prompt=prompt, image_file=image_file, output_path = output_path, base_size = 1024, image_size = 640, crop_mode=True, save_results = True, test_compress = True)&lt;/code&gt;
    &lt;p&gt;or you can&lt;/p&gt;
    &lt;code&gt;cd DeepSeek-OCR-master/DeepSeek-OCR-hf
python run_dpsk_ocr.py&lt;/code&gt;
    &lt;p&gt;The current open-source model supports the following modes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Native resolution: &lt;list rend="ul"&gt;&lt;item&gt;Tiny: 512√ó512 Ôºà64 vision tokensÔºâ‚úÖ&lt;/item&gt;&lt;item&gt;Small: 640√ó640 Ôºà100 vision tokensÔºâ‚úÖ&lt;/item&gt;&lt;item&gt;Base: 1024√ó1024 Ôºà256 vision tokensÔºâ‚úÖ&lt;/item&gt;&lt;item&gt;Large: 1280√ó1280 Ôºà400 vision tokensÔºâ‚úÖ&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Dynamic resolution &lt;list rend="ul"&gt;&lt;item&gt;Gundam: n√ó640√ó640 + 1√ó1024√ó1024 ‚úÖ&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# document: &amp;lt;image&amp;gt;\n&amp;lt;|grounding|&amp;gt;Convert the document to markdown.
# other image: &amp;lt;image&amp;gt;\n&amp;lt;|grounding|&amp;gt;OCR this image.
# without layouts: &amp;lt;image&amp;gt;\nFree OCR.
# figures in document: &amp;lt;image&amp;gt;\nParse the figure.
# general: &amp;lt;image&amp;gt;\nDescribe this image in detail.
# rec: &amp;lt;image&amp;gt;\nLocate &amp;lt;|ref|&amp;gt;xxxx&amp;lt;|/ref|&amp;gt; in the image.
# 'ÂÖàÂ§©‰∏ã‰πãÂøßËÄåÂøß'&lt;/code&gt;
    &lt;p&gt;We would like to thank Vary, GOT-OCR2.0, MinerU, PaddleOCR, OneChart, Slow Perception for their valuable models and ideas.&lt;/p&gt;
    &lt;p&gt;We also appreciate the benchmarks: Fox, OminiDocBench.&lt;/p&gt;
    &lt;p&gt;coming soonÔºÅ&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45640594</guid><pubDate>Mon, 20 Oct 2025 06:26:33 +0000</pubDate></item><item><title>AWS multiple services outage in us-east-1</title><link>https://health.aws.amazon.com/health/status?ts=20251020</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45640838</guid><pubDate>Mon, 20 Oct 2025 07:22:28 +0000</pubDate></item><item><title>Docker Systems Status: Full Service Disruption</title><link>https://www.dockerstatus.com/pages/incident/533c6539221ae15e3f000031/68f5e1c741c825463df7486c</link><description>&lt;doc fingerprint="3bbcb2c31d7475f1"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h5"&gt;Issues accessing Registry, Hub, Scout, DBC, DHIOperational&lt;/head&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt;Docker Hub Registry, Docker Authentication, Docker Hub Web Services, Docker Billing, Docker Hub Automated Builds, Docker Hub Security Scanning, Docker Scout, Docker Build Cloud, Testcontainers Cloud, Docker Cloud, Docker Hardened Images&lt;/p&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;October 20, 2025 03:05 PDT&lt;lb/&gt;October 20, 2025 10:05 UTC&lt;/p&gt;
        &lt;p&gt;[Resolved] This incident is resolved. &lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;October 20, 2025 02:43 PDT&lt;lb/&gt;October 20, 2025 09:43 UTC&lt;/p&gt;
        &lt;p&gt;[Monitoring] We are seeing error rates recovering across our SaaS services. We continue to monitor as we process our backlog. &lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;October 20, 2025 01:22 PDT&lt;lb/&gt;October 20, 2025 08:22 UTC&lt;/p&gt;
        &lt;p&gt;[Identified] We have identified the underlying issue with one of our cloud service providers. We are monitoring the situation and prepare our systems for when the issues with our service provider resolve. &lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;October 20, 2025 00:16 PDT&lt;lb/&gt;October 20, 2025 07:16 UTC&lt;/p&gt;
        &lt;p&gt;[Investigating] We are seeing issues accessing and using our services across many of our products. We are currently investigating and will report back as soon as possible.. &lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45640877</guid><pubDate>Mon, 20 Oct 2025 07:31:23 +0000</pubDate></item><item><title>Alibaba Cloud says it cut Nvidia AI GPU use by 82% with new pooling system</title><link>https://www.tomshardware.com/tech-industry/semiconductors/alibaba-says-new-pooling-system-cut-nvidia-gpu-use-by-82-percent</link><description>&lt;doc fingerprint="69989c3d8f16dad1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Alibaba Cloud says it cut Nvidia AI GPU use by 82% with new pooling system‚Äî up to 9x increase in output lets 213 GPUs perform like 1,192&lt;/head&gt;
    &lt;p&gt;A paper presented at SOSP 2025 details how token-level scheduling helped one GPU serve multiple LLMs, reducing demand from 1,192 to 213 H20s.&lt;/p&gt;
    &lt;p&gt;Alibaba Cloud claims its new Aegaeon pooling system reduces the number of Nvidia GPUs required to serve large language models by 82% during a multi-month beta test inside its Model Studio marketplace. The result, published in a peer-reviewed paper presented at the 2025 ACM Symposium on Operating Systems (SOSP) in Seoul, suggests that cloud providers may be able to extract significantly more inference capacity from existing silicon, especially in constrained markets like China, where the supply of Nvidia's latest H20s remains limited.&lt;/p&gt;
    &lt;p&gt;Unlike training-time breakthroughs that chase model quality or speed, Aegaeon is an inference-time scheduler designed to maximize GPU utilization across many models with bursty or unpredictable demand. Instead of pinning one accelerator to one model, Aegaeon virtualizes GPU access at the token level, allowing it to schedule tiny slices of work across a shared pool. This means one H20 could serve several different models simultaneously, with system-wide ‚Äúgoodput‚Äù ‚Äî a measure of effective output ‚Äî rising by as much as nine times compared to older serverless systems.&lt;/p&gt;
    &lt;p&gt;The system was tested in production over several months, according to the paper, which lists authors from both Peking University and Alibaba‚Äôs infrastructure division, including CTO Jingren Zhou. During that window, the number of GPUs needed to support dozens of different LLMs ‚Äî ranging in size up to 72 billion parameters ‚Äî fell from 1,192 to just 213.&lt;/p&gt;
    &lt;p&gt;While the paper does not break down which models contributed most to the savings, reporting by the South China Morning Post says the tests were conducted using Nvidia‚Äôs H20, one of the few accelerators still legally available to Chinese buyers under current U.S. export controls.&lt;/p&gt;
    &lt;p&gt;Alibaba says the gains came from two main techniques: Packing multiple models per GPU, and using a token-level autoscaler to dynamically allocate compute as output is generated, rather than reserving resources at the request level. In benchmarks, Aegaeon beat the goodput of ServerlessLLM and MuxServe by margins ranging from 1.5 times to 9 times.&lt;/p&gt;
    &lt;p&gt;Whether those savings translate outside Alibaba‚Äôs stack remains to be seen. Alibaba Cloud‚Äôs paper does not specify the exact network fabric used in the beta test, but we know the company offers its own eRDMA elastic RDMA network and has a record of building highly‚Äëintegrated GPU serving stacks, suggesting the results may depend on an optimized, vertically integrated environment.&lt;/p&gt;
    &lt;p&gt;Regardless, the result is likely to attract interest from other hyperscalers looking to stretch scarce accelerator fleets as inference demand continues to spike.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, &amp;amp; reviews in your feeds.&lt;/p&gt;
    &lt;p&gt;Luke James is a freelance writer and journalist. Although his background is in legal, he has a personal interest in all things tech, especially hardware and microelectronics, and anything regulatory.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45643163</guid><pubDate>Mon, 20 Oct 2025 12:31:22 +0000</pubDate></item><item><title>Servo v0.0.1</title><link>https://github.com/servo/servo</link><description>&lt;doc fingerprint="946e1b5c97a1d4"&gt;
  &lt;main&gt;
    &lt;p&gt;Servo is a prototype web browser engine written in the Rust language. It is currently developed on 64-bit macOS, 64-bit Linux, 64-bit Windows, 64-bit OpenHarmony, and Android.&lt;/p&gt;
    &lt;p&gt;Servo welcomes contribution from everyone. Check out:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Servo Book for documentation&lt;/item&gt;
      &lt;item&gt;servo.org for news and guides&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Coordination of Servo development happens:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Here in the Github Issues&lt;/item&gt;
      &lt;item&gt;On the Servo Zulip&lt;/item&gt;
      &lt;item&gt;In video calls advertised in the Servo Project repo.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For more detailed build instructions, see the Servo book under Setting up your environment, Building Servo, Building for Android and Building for OpenHarmony.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Download and install Xcode and &lt;code&gt;brew&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Install &lt;code&gt;uv&lt;/code&gt;:&lt;code&gt;curl -LsSf https://astral.sh/uv/install.sh | sh&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Install &lt;code&gt;rustup&lt;/code&gt;:&lt;code&gt;curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Restart your shell to make sure &lt;code&gt;cargo&lt;/code&gt;is available&lt;/item&gt;
      &lt;item&gt;Install the other dependencies: &lt;code&gt;./mach bootstrap&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Build servoshell: &lt;code&gt;./mach build&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install &lt;code&gt;curl&lt;/code&gt;:&lt;list rend="ul"&gt;&lt;item&gt;Arch: &lt;code&gt;sudo pacman -S --needed curl&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Debian, Ubuntu: &lt;code&gt;sudo apt install curl&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Fedora: &lt;code&gt;sudo dnf install curl&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Gentoo: &lt;code&gt;sudo emerge net-misc/curl&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Arch: &lt;/item&gt;
      &lt;item&gt;Install &lt;code&gt;uv&lt;/code&gt;:&lt;code&gt;curl -LsSf https://astral.sh/uv/install.sh | sh&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Install &lt;code&gt;rustup&lt;/code&gt;:&lt;code&gt;curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Restart your shell to make sure &lt;code&gt;cargo&lt;/code&gt;is available&lt;/item&gt;
      &lt;item&gt;Install the other dependencies: &lt;code&gt;./mach bootstrap&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Build servoshell: &lt;code&gt;./mach build&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Download &lt;code&gt;uv&lt;/code&gt;,&lt;code&gt;choco&lt;/code&gt;, and&lt;code&gt;rustup&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;Be sure to select Quick install via the Visual Studio Community installer&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;In the Visual Studio Installer, ensure the following components are installed: &lt;list rend="ul"&gt;&lt;item&gt;Windows 10/11 SDK (anything &amp;gt;= 10.0.19041.0) (&lt;code&gt;Microsoft.VisualStudio.Component.Windows{10, 11}SDK.{&amp;gt;=19041}&lt;/code&gt;)&lt;/item&gt;&lt;item&gt;MSVC v143 - VS 2022 C++ x64/x86 build tools (Latest) (&lt;code&gt;Microsoft.VisualStudio.Component.VC.Tools.x86.x64&lt;/code&gt;)&lt;/item&gt;&lt;item&gt;C++ ATL for latest v143 build tools (x86 &amp;amp; x64) (&lt;code&gt;Microsoft.VisualStudio.Component.VC.ATL&lt;/code&gt;)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Windows 10/11 SDK (anything &amp;gt;= 10.0.19041.0) (&lt;/item&gt;
      &lt;item&gt;Restart your shell to make sure &lt;code&gt;cargo&lt;/code&gt;is available&lt;/item&gt;
      &lt;item&gt;Install the other dependencies: &lt;code&gt;.\mach bootstrap&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Build servoshell: &lt;code&gt;.\mach build&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ensure that the following environment variables are set: &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;ANDROID_SDK_ROOT&lt;/code&gt;&lt;/item&gt;&lt;item&gt;&lt;code&gt;ANDROID_NDK_ROOT&lt;/code&gt;:&lt;code&gt;$ANDROID_SDK_ROOT/ndk/28.2.13676358/&lt;/code&gt;&lt;code&gt;ANDROID_SDK_ROOT&lt;/code&gt;can be any directory (such as&lt;code&gt;~/android-sdk&lt;/code&gt;). All of the Android build dependencies will be installed there.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Install the latest version of the Android command-line tools to &lt;code&gt;$ANDROID_SDK_ROOT/cmdline-tools/latest&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Run the following command to install the necessary components: &lt;quote&gt;sudo $ANDROID_SDK_ROOT/cmdline-tools/latest/bin/sdkmanager --install \ "build-tools;34.0.0" \ "emulator" \ "ndk;28.2.13676358" \ "platform-tools" \ "platforms;android-33" \ "system-images;android-33;google_apis;x86_64"&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;Follow the instructions above for the platform you are building on&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Follow the instructions above for the platform you are building on to prepare the environment.&lt;/item&gt;
      &lt;item&gt;Depending on the target distribution (e.g. &lt;code&gt;HarmonyOS NEXT&lt;/code&gt;vs pure&lt;code&gt;OpenHarmony&lt;/code&gt;) the build configuration will differ slightly.&lt;/item&gt;
      &lt;item&gt;Ensure that the following environment variables are set &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;DEVECO_SDK_HOME&lt;/code&gt;(Required when targeting&lt;code&gt;HarmonyOS NEXT&lt;/code&gt;)&lt;/item&gt;&lt;item&gt;&lt;code&gt;OHOS_BASE_SDK_HOME&lt;/code&gt;(Required when targeting&lt;code&gt;OpenHarmony&lt;/code&gt;)&lt;/item&gt;&lt;item&gt;&lt;code&gt;OHOS_SDK_NATIVE&lt;/code&gt;(e.g.&lt;code&gt;${DEVECO_SDK_HOME}/default/openharmony/native&lt;/code&gt;or&lt;code&gt;${OHOS_BASE_SDK_HOME}/${API_VERSION}/native&lt;/code&gt;)&lt;/item&gt;&lt;item&gt;&lt;code&gt;SERVO_OHOS_SIGNING_CONFIG&lt;/code&gt;: Path to json file containing a valid signing configuration for the demo app.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Review the detailed instructions at Building for OpenHarmony.&lt;/item&gt;
      &lt;item&gt;The target distribution can be modified by passing &lt;code&gt;--flavor=&amp;lt;default|harmonyos&amp;gt;&lt;/code&gt;to&lt;code&gt;mach &amp;lt;build|package|install&amp;gt;&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45643357</guid><pubDate>Mon, 20 Oct 2025 12:55:30 +0000</pubDate></item><item><title>BERT is just a single text diffusion step</title><link>https://nathan.rs/posts/roberta-diffusion/</link><description>&lt;doc fingerprint="35106edd82bc1c52"&gt;
  &lt;main&gt;
    &lt;p&gt;A while back, Google DeepMind unveiled Gemini Diffusion, an experimental language model that generates text using diffusion. Unlike traditional GPT-style models that generate one word at a time, Gemini Diffusion creates whole blocks of text by refining random noise step-by-step.&lt;/p&gt;
    &lt;p&gt;I read the paper Large Language Diffusion Models and was surprised to find that discrete language diffusion is just a generalization of masked language modeling (MLM), something we‚Äôve been doing since 2018. The first thought I had was, ‚Äúcan we finetune a BERT-like model to do text generation?‚Äù I decided to try a quick proof of concept out of curiosity.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;NOTE: After I wrote the article I stumbled upon the paper DiffusionBERT which does essentially the same thing but with more rigorous testing! Check it out if this post interested you.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;A Short History of Transformers#&lt;/head&gt;
    &lt;p&gt;The original Transformer architecture, introduced in 2017, was an encoder-decoder model. In 2018, researchers realized that the encoder and decoder components of the model could be separated (with the advent of BERT and GPT), and two distinct families of models were created:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Encoder-only models (BERT-style, bidirectional)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Encoder models used masked language modeling (MLM) as a training objective: randomly mask out a subset of tokens of each input and train the encoder to reconstruct the missing tokens (fill in the blanks). The model sees the entire (partially masked) context at once and learns bidirectional representations. This architecture excelled at tasks requiring a full‚Äêsentence (or paragraph) representation (e.g., classification and retrieval).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Decoder-only models (GPT-style, autoregressive)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Decoder models used next‚Äêtoken prediction as a training objective: at each position $t$, predict the token at position $t + 1$ given all tokens up to $t$ as context. Only the left context is used to predict future values (unidirectional). This architecture excelled at generative tasks where you produce text one token at a time, such as open‚Äêended generation, summarization, and translation.&lt;/p&gt;
    &lt;p&gt;Originally, BERT saw immediate use in tasks such as classification, whereas GPT-style models didn‚Äôt become popular until later (due to initial limited capabilities). Eventually, the generation capabilities of autoregressive (decoder) transformers vastly improved. The general training objective of ‚Äúnext token prediction‚Äù means a much larger space of use cases when compared to encoder models.&lt;/p&gt;
    &lt;head rend="h2"&gt;Discrete Language Diffusion Models#&lt;/head&gt;
    &lt;p&gt;Diffusion models were first popularized in image generation. In image generation, diffusion models gradually add Gaussian noise to an image (forward process) and then train a neural network to iteratively denoise it (reverse process). A high‚Äêlevel summary of continuous diffusion with images is:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Forward process: Start from a clean image x‚ÇÄ, then add small amounts of (usually Gaussian) noise at each timestep until you end up with near‚Äêpure noise.&lt;/item&gt;
      &lt;item&gt;Reverse process: Train a model (often a U‚ÄêNet) to predict the noise at each timestep, gradually recovering the original image in discrete denoising steps.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Applying this idea to language means we need a way to add noise to text and then remove it in stages. The simplest way to do this is a masking‚Äêbased noise process:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Forward (masking) process:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;At timestep t = 0, you have a fully uncorrupted text sequence.&lt;/item&gt;
          &lt;item&gt;At each subsequent timestep t &amp;gt; 0, randomly replace a fraction of tokens with a special &lt;code&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt;token according to a pre‚Äêdefined schedule (e.g., gradually increasing the masked proportion from 0% to 100%).&lt;/item&gt;
          &lt;item&gt;By the final timestep T, the entire sequence may be masked (all tokens are &lt;code&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt;).&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Reverse (denoising) process:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Train a model (often a standard Transformer encoder) to predict the original token IDs given a partially masked sequence at timestep t.&lt;/item&gt;
          &lt;item&gt;This is akin to performing masked language modeling at varying mask rates: at early timesteps, only a few tokens are masked (easy to predict); at later timesteps, many tokens are masked (harder).&lt;/item&gt;
          &lt;item&gt;By chaining together predictions from high‚Äêmask‚Äêrate back down to zero, you can recover (or generate) a full sequence.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In this discrete text diffusion framework, the model learns a likelihood bound on the data distribution by optimizing a sum of denoising losses over all timesteps, rather than a single MLM objective at a fixed mask probability.&lt;/p&gt;
    &lt;p&gt;As we can see, BERT‚Äôs masked language modeling objective is the same training objective as text diffusion, but just for a subset of masking rates. By introducing variable masking rates (from 0 to 1) and a scheduled sequence of denoising steps (inspired by diffusion theory), we can transform BERT‚Äôs masked language modeling objective into a full generative procedure.&lt;/p&gt;
    &lt;head rend="h2"&gt;RoBERTa Diffusion#&lt;/head&gt;
    &lt;p&gt;In 2019, RoBERTa was released. It was essentially just an enhancement of the original BERT model, with better hyperparameters, data training size, and a more simple training objective (MLM only, removed next sentence prediction).&lt;/p&gt;
    &lt;p&gt;Here we use the HuggingFace &lt;code&gt;transformers&lt;/code&gt; and &lt;code&gt;dataset&lt;/code&gt; libraries to pull in the original RoBERTa weights, tokenizer, and the Trainer class to easily finetune the model on the WikiText dataset.
The main code (full code here) looks like this below:&lt;/p&gt;
    &lt;code&gt;# Load and tokenize dataset and instantiate the model
dataset = load_dataset("wikitext", "wikitext-2-raw-v1")
tokenizer = RobertaTokenizerFast.from_pretrained("roberta-base")
model = RobertaForMaskedLM.from_pretrained("roberta-base")

# Create the training args and Trainer instance
training_args = TrainingArguments(
    output_dir="finetuned-roberta-diffusion",
    overwrite_output_dir=True,
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    save_strategy="epoch",
    save_total_limit=1,
    logging_steps=200,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["validation"],
    data_collator=diffusion_collator, # custom implementation
    tokenizer=tokenizer,
)

# Train &amp;amp; save
trainer.train()
trainer.save_model("finetuned-roberta-diffusion")&lt;/code&gt;
    &lt;p&gt;Currently we have 10 diffusion steps, so we randomly sample a percentage $p$ out of &lt;code&gt;mask_probs&lt;/code&gt; (1.0, 0.9, 0.9, &amp;amp;mldr;, 0.1) and mask that percent of the tokens each batch.
The custom &lt;code&gt;diffusion_collator&lt;/code&gt; function (see code here) samples one mask-probability &lt;code&gt;p&lt;/code&gt; from &lt;code&gt;mask_probs&lt;/code&gt; per batch and sets each token to &lt;code&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt; with &lt;code&gt;p&lt;/code&gt; probability.&lt;/p&gt;
    &lt;p&gt;To be able to condition the generation on a ‚Äúprompt‚Äù, we currently never mask the first 16 tokens. That means that during training, each step will always have the first 16 tokens as context for generation.&lt;/p&gt;
    &lt;p&gt;Simplified code for the &lt;code&gt;diffusion_collator&lt;/code&gt; looks like:&lt;/p&gt;
    &lt;code&gt;  def diffusion_collator(examples):
      batch = tokenizer.pad(examples, return_tensors="pt")

      # Randomly select masking probability for this batch
      mask_prob = random.choice([1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1])

      # Never mask the first PREFIX_LEN tokens (preserved context)
      maskable_positions = batch.input_ids[:, PREFIX_LEN:]

      # Create random mask for the chosen probability
      mask = torch.rand(maskable_positions.shape) &amp;lt; mask_prob

      # Apply masking
      batch.input_ids[:, PREFIX_LEN:][mask] = tokenizer.mask_token_id
      batch.labels = batch.input_ids.clone()

      return batch&lt;/code&gt;
    &lt;p&gt;For inference, we start with an input which is a tensor of size 256 (since we are generating blocks of 256 tokens). The first 16 positions are the token ids that correspond to the prompt, and the last 240 are just &lt;code&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt; tokens. We iterate through the denoising schedule and each step, we generate a prediction and then remask the sequence again. The process looks like this:&lt;/p&gt;
    &lt;code&gt;Step 0: [PREFIX] &amp;lt;mask&amp;gt; &amp;lt;mask&amp;gt; &amp;lt;mask&amp;gt; &amp;lt;mask&amp;gt; &amp;lt;mask&amp;gt; ...     (100% masked)
Step 1: [PREFIX] will &amp;lt;mask&amp;gt; over &amp;lt;mask&amp;gt; control ...        (90% masked)
Step 2: [PREFIX] will begin &amp;lt;mask&amp;gt; greater control ...      (80% masked)
...
Step 10: [PREFIX] will begin to assert greater control ...  (0% masked - DONE)&lt;/code&gt;
    &lt;p&gt;Simplified code for generation looks like:&lt;/p&gt;
    &lt;code&gt;# Generate text through iterative denoising
for step, mask_prob in enumerate(mask_probs):
    # Forward pass: predict masked tokens
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        predictions = outputs.logits  # shape: (1, MAX_LEN, vocab_size)

    # For each masked position, sample from top-k/top-p filtered distribution
    for pos in range(PREFIX_LEN, MAX_LEN):
        if input_ids[0, pos] == tokenizer.mask_token_id:
            logits = predictions[0, pos, :]
            # Apply top-k and top-p filtering
            filtered_logits = top_k_top_p_filtering(logits, top_k=TOP_K, top_p=TOP_P)
            probs = F.softmax(filtered_logits, dim=-1)
            # Sample token
            sampled_token = torch.multinomial(probs, 1)
            input_ids[0, pos] = sampled_token

    # Re-mask a portion of non-prefix tokens for next iteration
    if mask_prob &amp;gt; 0:
        mask_indices = torch.rand(MAX_LEN - PREFIX_LEN) &amp;lt; mask_prob
        input_ids[0, PREFIX_LEN:][mask_indices] = tokenizer.mask_token_id&lt;/code&gt;
    &lt;p&gt;Here is an example output generation of the fine-tuned model after training on an H200 for 30 minutes (the first line is the initial prompt):&lt;/p&gt;
    &lt;code&gt;Following their victory in the French and Indian War, Britain began to assert
greater...

...dominion over Europe beginning about the early 19th. There conflict took
place on the island, between British and Irish Ireland. British officials 
administered British Ireland, a Celtic empire under the control of the Irish 
nationalist authorities, defined as a dominion of Britain. As the newly Fortic 
states acquired independent and powerful status, many former English colonies
played their part in this new, British @-@ controlled colonial system. Following
this period the Non @-@ Parliamentaryist Party won its influence in Britain in 
1890, led by the support of settlers from the Irish colonies. Looking inwards, 
Sinclair, Lewis questioned, and debated the need to describe " The New Britain "&lt;/code&gt;
    &lt;p&gt;The output looks surprisingly coherent! Most of the quirks present are actually just quirks from the formatting of WikiText (spaces around punctuation &lt;code&gt;"&lt;/code&gt;, turning hyphens &lt;code&gt;-&lt;/code&gt; into &lt;code&gt;@-@&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Below is a comparison between our diffusion model and GPT-2:&lt;/p&gt;
    &lt;p&gt;We see GPT-2‚Äôs output is more coherent and slightly faster (~9 seconds vs ~13) but I‚Äôm pleasantly surprised with how good my simple implementation was. It is a good proof of concept, and with new approaches like AR-Diffusion and Skip-Step Diffusion (and a more optimized implementation), the quality and speed can be drastically improved.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion#&lt;/head&gt;
    &lt;p&gt;We‚Äôve seen that masked language models like RoBERTa, originally designed for fill-in-the-blank tasks, can be repurposed into fully generative engines by interpreting variable-rate masking as a discrete diffusion process. By gradually corrupting text with &lt;code&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt; tokens and training the model to iteratively denoise at increasing mask intensities, we effectively turn the standard MLM objective into a step-by-step generation procedure.&lt;/p&gt;
    &lt;p&gt;Even without architectural changes, a fine-tuned RoBERTa can generate coherent looking text after slightly modifying the training objective, validating the idea that BERT-style models are essentially just text diffusion models trained on one masking rate.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45644328</guid><pubDate>Mon, 20 Oct 2025 14:31:16 +0000</pubDate></item><item><title>Show HN: I created a cross-platform GUI for the JJ VCS (Git compatible)</title><link>https://judojj.com</link><description>&lt;doc fingerprint="10bd79eb74913531"&gt;
  &lt;main&gt;
    &lt;p&gt;Releases Roadmap JUDO The full-featured GUI for JJ VCS (works with Git repos too!) macOS Download for macOS Windows Download for Windows Linux (Ubuntu/Debian) Download for Linux Restore your repo to any point in time with the Operation Log. Undo and redo any change. View combined diffs of multiple commits, or the diff between commits Apply or revert hunks of any diff, files, commits, or even multiple commits at once Use custom revsets to select which commits are shown. Filter by descriptions, authors, ancestry, and more. Drag and drop rebase Duplicate, split, abandon, revert, absorb, squash, and more Keep your bookmarks managed&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45645120</guid><pubDate>Mon, 20 Oct 2025 15:35:19 +0000</pubDate></item><item><title>Postman which I thought worked locally on my computer, is down</title><link>https://status.postman.com</link><description>&lt;doc fingerprint="397ea3725e12edc2"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt; Update - The team is actively resolving cross-dependency failures and working to mitigate customer impact. We‚Äôre also investigating issues affecting search functionality and will share more updates as we make progress. &lt;lb/&gt; Oct 20, 2025 - 19:00 PDT &lt;/p&gt;
      &lt;p&gt; Update - Some services are still experiencing intermittent issues, but we‚Äôre seeing signs of recovery across multiple systems. Our teams continue to actively monitor performance and work through remaining problems to restore full functionality. We‚Äôll provide further updates as recovery progresses. &lt;lb/&gt; Oct 20, 2025 - 17:17 PDT &lt;/p&gt;
      &lt;p&gt; Update - We have seen significant recovery of the features. We are continuing to monitor for any further issues. &lt;lb/&gt; Oct 20, 2025 - 08:20 PDT &lt;/p&gt;
      &lt;p&gt; Monitoring - Majority of the services have recovered. We are continuing to monitor. &lt;lb/&gt; Oct 20, 2025 - 06:21 PDT &lt;/p&gt;
      &lt;p&gt; Update - We are seeing significant recovery and are continuing to monitor. &lt;lb/&gt; Oct 20, 2025 - 05:56 PDT &lt;/p&gt;
      &lt;p&gt; Update - We are seeing significant recovery and are continuing to monitor. &lt;lb/&gt; Oct 20, 2025 - 05:52 PDT &lt;/p&gt;
      &lt;p&gt; Identified - We are currently experiencing significantly increased error rates which is impacting functionality on Postman. There is a major issue with our underlying cloud provider and we are working with them to restore full access as quickly as possible. &lt;lb/&gt; Oct 20, 2025 - 05:39 PDT &lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45645172</guid><pubDate>Mon, 20 Oct 2025 15:40:40 +0000</pubDate></item><item><title>Production RAG: what I learned from processing 5M+ documents</title><link>https://blog.abdellatif.io/production-rag-processing-5m-documents</link><description>&lt;doc fingerprint="9bf95f134d9a6771"&gt;
  &lt;main&gt;
    &lt;p&gt;I've spent the last 8 months in the RAG trenches, I want to share what actually worked vs. wasted our time. We built RAG for Usul AI (9M pages) and an unnamed legal AI enterprise (4M pages).&lt;/p&gt;
    &lt;head rend="h2"&gt;Langchain + Llamaindex&lt;/head&gt;
    &lt;p&gt;We started out with youtube tutorials. First Langchain ‚Üí Llamaindex. Got to a working prototype in a couple of days and were optimistic with the progress. We run tests on subset of the data (100 documents) and the results looked great. We spent the next few days running the pipeline on the production dataset and got everything working in a week ‚Äî incredible.&lt;/p&gt;
    &lt;p&gt;Except it wasn't, the results were subpar and only the end users could tell. We spent the following few months rewriting pieces of the system, one at a time, until the performance was at the level we wanted. Here are things we did ranked by ROI.&lt;/p&gt;
    &lt;head rend="h2"&gt;What moved the needle&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Query Generation: not all context can be captured by the user's last query. We had an LLM review the thread and generate a number of semantic + keyword queries. We processed all of those queries in parallel, and passed them to a reranker. This made us cover a larger surface area and not be dependent on a computed score for hybrid search.&lt;/item&gt;
      &lt;item&gt;Reranking: the highest value 5 lines of code you'll add. The chunk ranking shifted a lot. More than you'd expect. Reranking can many times make up for a bad setup if you pass in enough chunks. We found the ideal reranker set-up to be 50 chunk input -&amp;gt; 15 output.&lt;/item&gt;
      &lt;item&gt;Chunking Strategy: this takes a lot of effort, you'll probably be spending most of your time on it. We built a custom flow for both enterprises, make sure to understand the data, review the chunks, and check that a) chunks are not getting cut mid-word or sentence b) ~each chunk is a logical unit and captures information on its own&lt;/item&gt;
      &lt;item&gt;Metadata to LLM: we started by passing the chunk text to the LLM, we ran an experiment and found that injecting relevant metadata as well (title, author, etc.) improves context and answers by a lot.&lt;/item&gt;
      &lt;item&gt;Query routing: many users asked questions that can't be answered by RAG (e.g. summarize the article, who wrote this). We created a small router that detects these questions and answers them using an API call + LLM instead of the full-blown RAG set-ups.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Our stack&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Vector database: Azure -&amp;gt; Pinecone -&amp;gt; Turbopuffer (cheap, supports keyword search natively)&lt;/item&gt;
      &lt;item&gt;Document Extraction: Custom&lt;/item&gt;
      &lt;item&gt;Chunking: Unstructured.io by default, custom for enterprises (heard that Chonkie is good)&lt;/item&gt;
      &lt;item&gt;Embedding: text-embedding-large-3, haven't tested others&lt;/item&gt;
      &lt;item&gt;Reranker: None -&amp;gt; Cohere 3.5 -&amp;gt; Zerank (less known but actually good)&lt;/item&gt;
      &lt;item&gt;LLM: GPT 4.1 -&amp;gt; GPT 5 -&amp;gt; GPT 4.1, covered by Azure credits&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Going Open-source&lt;/head&gt;
    &lt;p&gt;We put all our learning into an open-source project: agentset-ai/agentset under an MIT license. Feel free to reach out if you have any questions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45645349</guid><pubDate>Mon, 20 Oct 2025 15:55:36 +0000</pubDate></item><item><title>TernFS ‚Äì an exabyte scale, multi-region distributed filesystem</title><link>https://www.xtxmarkets.com/tech/2025-ternfs/#posix-shaped</link><description>&lt;doc fingerprint="65ba0c171fb742d2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;TernFS ‚Äî an exabyte scale, multi-region distributed filesystem&lt;/head&gt;
    &lt;p&gt;September 2025&lt;/p&gt;
    &lt;p&gt;XTX is an algorithmic trading firm: it builds statistical models that produce price forecasts for over 50,000 financial instruments worldwide. We use those forecasts to make trades. As XTX's research efforts to build better models ramped up, the demand for resources kept increasing.&lt;/p&gt;
    &lt;p&gt;The firm started out with a couple of desktops and an NFS server, and 10 years later ended up with tens of thousands of high-end GPUs, hundreds of thousands of CPUs, and hundreds of petabytes of storage.&lt;/p&gt;
    &lt;p&gt;As compute grew, storage struggled to keep up. We rapidly outgrew NFS first and existing open-source and commercial filesystems later. After evaluating a variety of third-party solutions, we made the decision to implement our own filesystem, which we called TernFS[1].&lt;/p&gt;
    &lt;p&gt;We have decided to open source our efforts: TernFS is available as free software on our public GitHub. This post motivates TernFS, explains its high-level architecture, and then explores some key implementation details. If you just want to spin up a local TernFS cluster, head to the README.&lt;/p&gt;
    &lt;head rend="h2"&gt;Another filesystem?&lt;/head&gt;
    &lt;p&gt;There's a reason why every major tech company has developed its own distributed filesystem ‚Äî they're crucial to running large-scale compute efforts, and liable to cause intense disruption if they malfunction. [2]&lt;/p&gt;
    &lt;p&gt;XTX was in the same position, so we designed TernFS to be a one-stop solution for most of our storage needs, going from relatively 'cold' storage of raw market data to short-lived random-access data used to communicate between GPU jobs running on our cluster.&lt;/p&gt;
    &lt;p&gt;TernFS:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Is designed to scale up to tens of exabytes, trillions of files, millions of concurrent clients.&lt;/item&gt;
      &lt;item&gt;Stores file contents redundantly to protect against drive failures.&lt;/item&gt;
      &lt;item&gt;Has no single point of failure in its metadata services.&lt;/item&gt;
      &lt;item&gt;Supports file snapshot to protect against accidental file deletion.&lt;/item&gt;
      &lt;item&gt;Can span across multiple regions.&lt;/item&gt;
      &lt;item&gt;Is hardware agnostic and uses TCP/IP to communicate.&lt;/item&gt;
      &lt;item&gt;Utilizes different types of storage (such as flash vs. hard disks) cost effectively.&lt;/item&gt;
      &lt;item&gt;Exposes read/write access through its own API over TCP and UDP, and a Linux kernel filesystem module.&lt;/item&gt;
      &lt;item&gt;Requires no external service and has a minimal set of build dependencies. [3]&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Specifically, C++ and Go are needed to build the various TernFS components.&lt;/p&gt;
    &lt;p&gt;The C++ and Go processes depend on a handful of vendored libraries, most notably RocksDB for C++.&lt;/p&gt;
    &lt;p&gt;Naturally, there are some limitations, the main ones being:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Files are immutable ‚Äî once they're written they can't be modified.&lt;/item&gt;
      &lt;item&gt;TernFS should not be used for tiny files ‚Äî our median file size is 2MB.&lt;/item&gt;
      &lt;item&gt;The throughput of directory creation and removal is significantly constrained compared to other operations.&lt;/item&gt;
      &lt;item&gt;TernFS is permissionless, deferring that responsibility to other services.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We started designing TernFS in early 2022 and began putting it into production in summer 2023. By mid-2024 all of our machine learning efforts were driven out of TernFS, and we're migrating the rest of the firm's storage needs onto it as well.&lt;/p&gt;
    &lt;p&gt;As of September 2025, our TernFS deployment stores more than 500PB across 30,000 disks, 10,000 flash drives, and three data centres. At peak we serve multiple terabytes per second. To this day, we haven't lost a single byte.&lt;/p&gt;
    &lt;head rend="h2"&gt;High-level overview&lt;/head&gt;
    &lt;p&gt;Now that the stage is set, we're ready to explain the various components that make up TernFS. TernFS' core API is implemented by four services:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Metadata shards store the directory structure and file metadata.&lt;/item&gt;
      &lt;item&gt;The cross-directory coordinator (or CDC) executes cross-shard transactions.&lt;/item&gt;
      &lt;item&gt;Block services store file contents.&lt;/item&gt;
      &lt;item&gt;The registry stores information about all the other services and monitors them.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;
 A ‚îÄ‚îÄ‚ñ∫ B means "A sends requests to B" 
                                       
                                       
 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    
 ‚îÇ Metadata Shard ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          
 ‚îî‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ          
   ‚îÇ    ‚îÇ                   ‚îÇ          
   ‚îÇ    ‚îÇ                   ‚îÇ          
   ‚îÇ ‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îê                ‚îÇ          
   ‚îÇ ‚îÇ CDC ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ          
   ‚îÇ ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò          ‚îÇ     ‚îÇ          
   ‚îÇ    ‚îÇ             ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê     
   ‚îÇ    ‚îÇ             ‚îî‚îÄ‚î§        ‚îÇ     
 ‚îå‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ Client ‚îÇ     
 ‚îÇ Registry  ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§        ‚îÇ     
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     
        ‚îÇ                 ‚îÇ            
        ‚îÇ                 ‚îÇ            
 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ            
 ‚îÇ Block Service ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

&lt;/code&gt;
    &lt;p&gt;In the next few sections, we'll describe the high-level design of each service and then give more background on other relevant implementation details.[4]&lt;/p&gt;
    &lt;p&gt;Note that TernFS' multi-region capabilities are orthogonal to much of its high-level design, and they're therefore explained separately.&lt;/p&gt;
    &lt;head rend="h3"&gt;Metadata&lt;/head&gt;
    &lt;p&gt;To talk about metadata, we first need to explain what metadata is in TernFS. The short answer is: 'everything that is not file contents.' The slightly longer answer is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Directory entries, including all files and directory names.&lt;/item&gt;
      &lt;item&gt;File metadata including creation/modification/access time, logical file size, and so on.&lt;/item&gt;
      &lt;item&gt;The mapping between files and the blocks containing their contents.&lt;/item&gt;
      &lt;item&gt;Other ancillary data structures to facilitate maintenance operations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;TernFS' metadata is split into 256 logical shards. Shards never communicate with each other. This is a general principle in TernFS: each service is disaggregated from the others, deferring to the clients to communicate with each service directly.[5]&lt;/p&gt;
    &lt;p&gt;There are some exceptions ‚Äî most notably the shards execute requests from the CDC, and all services check into the registry.&lt;/p&gt;
    &lt;p&gt;A logical shard is further split into five physical instances, one leader and four followers, in a typical distributed consensus setup. The distributed consensus engine is provided by a purpose-built Raft-like implementation, which we call LogsDB, while RocksDB is used to implement read/write capabilities within a shard instance.&lt;/p&gt;
    &lt;p&gt;Currently all reads and writes go through the leader, but it would be trivial to allow clients to read from followers, and with a bit more effort to switch to a write-write setup.&lt;/p&gt;
    &lt;code&gt;    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê 
    ‚îÇ Shard 0 ‚îÇ ‚îÇ Shard 1 ‚îÇ  ...  ‚îÇ Shard 255 ‚îÇ 
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ         ‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò 
            ‚îå‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê 
            ‚îÇ                                 ‚îÇ 
            ‚îÇ                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ 
            ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ Replica 0  ‚îÇ ‚îÇ 
            ‚îÇ ‚îÇ           ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ (follower) ‚îÇ ‚îÇ 
 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ Replica 3 ‚óÑ‚îÄ‚îÄ‚îê ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ 
 ‚îÇ Client ‚îú‚îÄ‚îº‚îÄ‚ñ∫ (leader)  ‚óÑ‚îÄ‚îê‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ 
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ           ‚óÑ‚îê‚îÇ‚îî‚îÄ‚ñ∫ Replica 1  ‚îÇ ‚îÇ 
            ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ‚îÇ  ‚îÇ (follower) ‚îÇ ‚îÇ 
            ‚îÇ              ‚îÇ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ 
            ‚îÇ              ‚îÇ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ 
            ‚îÇ              ‚îÇ‚îî‚îÄ‚îÄ‚ñ∫ Replica 2  ‚îÇ ‚îÇ 
            ‚îÇ              ‚îÇ   ‚îÇ (follower) ‚îÇ ‚îÇ 
            ‚îÇ              ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ 
            ‚îÇ              ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ 
            ‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚ñ∫ Replica 4  ‚îÇ ‚îÇ 
            ‚îÇ                  ‚îÇ (follower) ‚îÇ ‚îÇ 
            ‚îÇ                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ 
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò 
&lt;/code&gt;
    &lt;p&gt;Splitting the metadata into 256 shards from the get-go simplifies the design, given that horizontal scaling of metadata requires no rebalancing, just the addition of more metadata servers.&lt;/p&gt;
    &lt;p&gt;For instance, our current deployment can serve hundreds of petabytes and more than 100,000 compute nodes with just 10 metadata servers per data centre, with each server housing roughly 25 shard leaders and 100 shard followers.&lt;/p&gt;
    &lt;p&gt;Given that the metadata servers are totally decoupled from one another, this means that we can scale metadata performance by 25√ó trivially, and by 100√ó if we were to start offloading metadata requests to followers.&lt;/p&gt;
    &lt;p&gt;TernFS shards metadata by assigning each directory to a single shard. This is done in a simple round-robin fashion by the cross-directory coordinator. Once a directory is created, all its directory entries and the files in it are housed in the same shard.&lt;/p&gt;
    &lt;p&gt;This design decision has downsides: TernFS assumes that the load will be spread across the 256 logical shards naturally. This is not a problem in large deployments, given that they will contain many directories, but it is something to keep in mind.[6]&lt;/p&gt;
    &lt;head rend="h3"&gt;Cross-directory transactions&lt;/head&gt;
    &lt;p&gt;Most of the metadata activity is contained within a single shard:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;File creation, same-directory renames, and deletion.&lt;/item&gt;
      &lt;item&gt;Listing directory contents.&lt;/item&gt;
      &lt;item&gt;Getting attributes of files or directories.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;However, some operations do require coordination between shards, namely directory creation, directory removal, and moving directory entries across different directories.&lt;/p&gt;
    &lt;p&gt;The cross-directory coordinator (CDC) performs these distributed transactions using a privileged metadata shard API. The CDC transactions are stateful, and therefore the CDC uses RocksDB and LogsDB much like the metadata shards themselves to persist its state safely.&lt;/p&gt;
    &lt;code&gt; ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê 
 ‚îÇ Client ‚îú‚îÄ‚îê  ‚îÇ Shard 32 ‚îÇ ‚îÇ Shard 103 ‚îÇ 
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îò ‚îî‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò 
 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îê         
 ‚îÇ CDC ‚îÇ  ‚îå‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ     ‚îÇ ‚îÇ         
 ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ Leader ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ         
 ‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îò            ‚îÇ         
 ‚îÇ              ‚îÇ               ‚îÇ         
 ‚îÇ       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ         
 ‚îÇ       ‚îÇ              ‚îÇ       ‚îÇ         
 ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ         
 ‚îÇ ‚îÇ Follower ‚îÇ .. ‚îÇ Follower ‚îÇ ‚îÇ         
 ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ         
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   
&lt;/code&gt;
    &lt;p&gt;The CDC executes transactions in parallel, which increases throughput considerably, but it is still a bottleneck when it comes to creating, removing, or moving directories. This means that TernFS has a relatively low throughput when it comes to CDC operations.[7]&lt;/p&gt;
    &lt;head rend="h3"&gt;Block services, or file contents&lt;/head&gt;
    &lt;p&gt;In TernFS, files are split into chunks of data called blocks. Blocks are read and written to by block services. A block service is typically a single drive (be it a hard disk or a flash drive) storing blocks. At XTX a typical storage server will contain around 100 hard disks or 25 flash drives ‚Äî or in TernFS parlance 100 or 25 block services.[8]&lt;/p&gt;
    &lt;p&gt;Read/write access to the block service is provided using a simple TCP API currently implemented by a Go process. This process is hardware agnostic and uses the Go standard library to read and write blocks to a conventional local file system. We originally planned to rewrite the Go process in C++, and possibly write to block devices directly, but the idiomatic Go implementation has proven performant enough for our needs so far.&lt;/p&gt;
    &lt;head rend="h3"&gt;The registry&lt;/head&gt;
    &lt;p&gt;The final piece of the TernFS puzzle is the registry. The registry stores the location of each instance of service (be it a metadata shard, the CDC, or a block storage node). A client only needs to know the address of the registry to mount TernFS ‚Äî it'll then gather the locations of the other services from it.&lt;/p&gt;
    &lt;p&gt;In TernFS all locations are IPv4 addresses. Working with IPv4 directly simplifies the kernel module considerably, since DNS lookups are quite awkward in the Linux kernel. The exception to this rule is addressing the registry itself, for which DNS is used.&lt;/p&gt;
    &lt;p&gt;The registry also stores additional information, such as the capacity and available size of each drive, who is a follower or a leader in LogsDB clusters, and so on.&lt;/p&gt;
    &lt;p&gt;Predictably, the registry itself is a RocksDB and LogsDB C++ process, given its statefulness.&lt;/p&gt;
    &lt;head rend="h3"&gt;Going global&lt;/head&gt;
    &lt;p&gt;TernFS tries very hard not to lose data, by storing both metadata and file contents on many different drives and servers. However, we also want to be resilient to the temporary or even permanent loss of one entire data centre. Therefore, TernFS can transparently scale across multiple locations.&lt;/p&gt;
    &lt;p&gt;The intended use for TernFS locations is for each location to converge to the same dataset. This means that each location will have to be provisioned with roughly equal resources.[9] Both metadata and file contents replication are asynchronous. In general, we judge the event of losing an entire data centre rare enough to tolerate a time window where data is not fully replicated across locations.&lt;/p&gt;
    &lt;p&gt;Metadata replication is set up so that one location is the metadata primary. Write operations in non-primary locations pay a latency price since they are acknowledged only after they are written to the primary location, replicated, and applied in the originating location. In practice this hasn't been an issue since metadata write latencies are generally overshadowed by writing file contents.&lt;/p&gt;
    &lt;p&gt;There is no automated procedure to migrate off a metadata primary location ‚Äî again, we deem it a rare enough occurrence to tolerate manual intervention. In the future we plan to move from the current protocol to a multi-master protocol where each location can commit writes independently, which would reduce write latencies on secondary locations and remove the privileged status of the primary location.&lt;/p&gt;
    &lt;p&gt;File contents, unlike metadata, are written locally to the location the client is writing from. Replication to other locations happens in two ways: proactively and on-demand. Proactive replication is performed by tailing the metadata log and replicating new file contents. On-demand replication happens when a client requests file content which has not been replicated yet.&lt;/p&gt;
    &lt;head rend="h2"&gt;Important Details&lt;/head&gt;
    &lt;p&gt;Now that we've laid down the high-level design of TernFS, we can talk about several key implementation details that make TernFS safer, more performant, and more flexible.&lt;/p&gt;
    &lt;head rend="h3"&gt;Talking to TernFS&lt;/head&gt;
    &lt;head rend="h4"&gt;Speaking TernFS' language&lt;/head&gt;
    &lt;p&gt;The most direct way to talk to TernFS is by using its own API. All TernFS messages are defined using a custom serialization format we call bincode. We chose to develop a custom serialization format since we needed it to work within the confines of the Linux kernel and to be easily chopped into UDP packets.&lt;/p&gt;
    &lt;p&gt;We intentionally kept the TernFS API stateless, in the sense that each request executes without regard to previous requests made by the same client. This is in contrast to protocols like NFS, whereby each connection is very stateful, holding resources such as open files, locks, and so on.&lt;/p&gt;
    &lt;p&gt;A stateless API dramatically simplifies the state machines that make up the TernFS core services, therefore simplifying their testing. It also forces each request to be idempotent, or in any case have clear retry semantics, since they might have to be replayed, which facilitates testing further.&lt;/p&gt;
    &lt;p&gt;It also allows the metadata shards and CDC API to be based on UDP rather than TCP, which makes the server and clients (especially the kernel module) simpler, due to doing away with the need for keeping TCP connections. The block service API is TCP based, since it is used to stream large amounts of contiguous data, and any UDP implementation would have to re-implement a reliable stream protocol. The registry API is also TCP-based, given that it is rarely used by clients, and occasionally needs to return large amounts of data.&lt;/p&gt;
    &lt;p&gt;While the TernFS API is simple out-of-the-box, we provide a permissively licensed Go library implementing common tasks that clients might want to perform, such as caching directory policies and retrying requests. This library is used to implement many TernFS processes that are not part of the core TernFS services, such as scrubbing, garbage collection, migrations, and the web UI.&lt;/p&gt;
    &lt;head rend="h4"&gt;Making TernFS POSIX-shaped&lt;/head&gt;
    &lt;p&gt;While the Go library is used for most ancillary tasks, some with high performance requirements, the main way to access TernFS at XTX is through its Linux kernel module.&lt;/p&gt;
    &lt;p&gt;This is because, when migrating our machine learning workflows to TernFS, we needed to support a vast codebase working with files directly. This not only meant that we needed to expose TernFS as a normal filesystem, but also that said normal filesystem API needed to be robust and performant enough for our machine learning needs.[10]&lt;/p&gt;
    &lt;p&gt;For this reason, we opted to work with Linux directly, rather than using FUSE. Working directly with the Linux kernel not only gave us the confidence that we could achieve our performance requirements but also allowed us to bend the POSIX API to our needs, something that would have been more difficult if we had used FUSE.[11]&lt;/p&gt;
    &lt;p&gt;The main obstacle when exposing TernFS as a 'normal' filesystem is that TernFS files are immutable. More specifically, TernFS files are fully written before being 'linked' into the filesystem as a directory entry. This is intentional: it lets us cleanly separate the API for 'under construction' files and 'completed files', and it means that half-written files are not visible.&lt;/p&gt;
    &lt;p&gt;However this design is essentially incompatible with POSIX, which endows the user with near-absolute freedom when it comes to manipulating a file. Therefore, the TernFS kernel module is not POSIX-compliant, but rather exposes enough POSIX to allow many programs to work without modifications, but not all.&lt;/p&gt;
    &lt;p&gt;In practice this means that programs which write files left-to-right and never modify the files' contents will work out-of-the-box. While this might seem very restrictive, we found that a surprising number of programs worked just fine.[12] Programs that did not follow this pattern were modified to first write to a temporary file and then copy the finished file to TernFS.&lt;/p&gt;
    &lt;p&gt;While we feel that writing our own kernel module was the right approach, it proved to be the trickiest part of TernFS, and we would not have been able to implement it without some important safety checks in the TernFS core services.[13]&lt;/p&gt;
    &lt;head rend="h4"&gt;S3 gateway&lt;/head&gt;
    &lt;p&gt;Almost all the storage-related activity at XTX is due to our machine-learning efforts, and for those purposes the TernFS' kernel module has served us well. However, as TernFS proved itself there, we started to look into offering TernFS to the broader firm.&lt;/p&gt;
    &lt;p&gt;Doing so through the kernel module presented multiple challenges. For starters installing a custom kernel module on every machine that needed to reach TernFS is operationally cumbersome. Moreover, while all machine-learning happens in clusters housed in the same data centre as TernFS itself, we wanted to expose TernFS in a way that's more amenable to less local networks, for instance by removing the need for UDP. Finally, TernFS does not have any built-in support for permissions or authentication, which is a requirement in multi-tenant scenarios.&lt;/p&gt;
    &lt;p&gt;To solve all these problems, we implemented a gateway for TernFS, which exposes a TernFS subtree using the S3 API. The gateway is a simple Go process turning S3 calls into TernFS API calls. The S3 gateway is not currently open sourced since it is coupled to authentication services internal to XTX, but we have open sourced a minimal S3 gateway to serve as a starting point for third-party contributors to build their own.&lt;/p&gt;
    &lt;p&gt;We've also planned an NFS gateway to TernFS, but we haven't had a pressing enough need yet to complete it.&lt;/p&gt;
    &lt;head rend="h4"&gt;The web UI and the JSON interface&lt;/head&gt;
    &lt;p&gt;Finally, a view of TernFS is provided by its web UI. The web UI is a stateless Go program which exposes most of the state of TernFS in an easy-to-use interface. This state includes the full filesystem contents (both metadata and file contents), the status of each service including information about decommissioned block services, and so on.&lt;/p&gt;
    &lt;p&gt;Moreover, the web UI also exposes the direct TernFS API in JSON form, which is very useful for small scripts and curl-style automation that does not warrant a full-blown Go program.&lt;/p&gt;
    &lt;head rend="h3"&gt;Directory Policies&lt;/head&gt;
    &lt;p&gt;To implement some of the functionality we'll describe below, TernFS adopts a system of per-directory policies.&lt;/p&gt;
    &lt;p&gt;Policies are used for all sorts of decisions, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How to redundantly store files.&lt;/item&gt;
      &lt;item&gt;On which type of drive to store files.&lt;/item&gt;
      &lt;item&gt;How long to keep files around after deletion.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each of the topics above (and a few more we haven't mentioned) correspond to a certain policy tag. The body of the policies are stored in the metadata together with the other directory attributes.&lt;/p&gt;
    &lt;p&gt;Policies are inherited: if a directory does not contain a certain policy tag, it transitively inherits from the parent directory. TernFS clients store a cache of policies to allow for traversal-free policy lookup for most directories.&lt;/p&gt;
    &lt;head rend="h3"&gt;Keeping blocks in check&lt;/head&gt;
    &lt;p&gt;A filesystem is no good if it loses, leaks, corrupts, or otherwise messes up its data. TernFS deploys a host of measures to minimize the chance of anything going wrong. So far, these have worked: we've never lost data in our production deployment of TernFS. This section focuses on the measures in place to specifically safeguard files' blocks.&lt;/p&gt;
    &lt;head rend="h4"&gt;Against bitrot, or CRC32-C&lt;/head&gt;
    &lt;p&gt;The first and possibly most obvious measure consists of aggressively checksumming all TernFS' data. The metadata is automatically checksummed by RocksDB, and every block is stored in a format interleaving 4KiB pages with 4byte CRC32-C checksums.&lt;/p&gt;
    &lt;p&gt;CRC32-C was picked since it is a high-quality checksum and implemented on most modern silicon.[14] It also exhibits some desirable properties when used together with Reed-Solomon coding.&lt;/p&gt;
    &lt;p&gt;Peter Cawley's fast-crc32 repository provides a general framework to compute CRC32-C quickly, together with state-of-the-art implementations for x86 and aarch64 architectures.&lt;/p&gt;
    &lt;p&gt;4KiB was picked since it is the read boundary used by Linux filesystems and is fine-grained while still being large enough to render the storage overhead of the 4byte checksums negligible.&lt;/p&gt;
    &lt;p&gt;Interleaving the CRCs with the block contents does not add any safety, but it does improve operations in two important ways. First, it allows for safe partial reads: clients can demand only a few pages from a block which is many megabytes in size and still check the reads against its checksum. Second, it allows scrubbing files locally on the server which hosts the blocks, without communicating with other services at all.&lt;/p&gt;
    &lt;head rend="h4"&gt;Storing files redundantly, or Reed-Solomon codes&lt;/head&gt;
    &lt;p&gt;We've been talking about files being split into blocks, but we haven't really explained how files become blocks.&lt;/p&gt;
    &lt;p&gt;The first thing we do to a file is split it into spans. Spans are at most 100MiB and are present just to divide files into sections of a manageable size.&lt;/p&gt;
    &lt;p&gt;Then each span is divided into D data blocks, and P parity blocks. D and P are determined by the corresponding directory policy in which the file is created. When D is 1, the entire contents of the span become a single block, and that block is stored D+P times. This scheme is equivalent to a simple mirroring scheme and allows it to lose up to P blocks before losing file data.&lt;/p&gt;
    &lt;p&gt;While wasteful, mirroring the entire contents of the file can be useful for very hot files, since TernFS clients will pick a block at random to read from, thereby sharing the read load across many block services. And naturally files which we do not care much for can be stored with D = 1 and P = 0, without any redundancy.&lt;/p&gt;
    &lt;p&gt;That said, most files will not be stored using mirroring but rather using Reed-Solomon coding. Other resources can be consulted to understand the high-level idea and the low-level details of Reed-Solomon coding, but the gist is it allows us to split a span into D equally sized blocks (some padding might be necessary), and then generate P blocks of equal size such that up to any P blocks can be lost while retaining the ability to reconstruct all the other blocks.&lt;/p&gt;
    &lt;p&gt;As mentioned, D and P are fully configurable, but at XTX we tend to use D = 10 and P = 4, which allows us to lose up to any four drives for any file.&lt;/p&gt;
    &lt;head rend="h4"&gt;Drive type picking&lt;/head&gt;
    &lt;p&gt;We now know how to split files into a bunch of blocks. The next question is: which drives to pick to store the blocks on. The first decision is which kind of drive to use. At XTX we separate drives into two broad categories for this purpose ‚Äî flash and spinning disks.&lt;/p&gt;
    &lt;p&gt;When picking between these two, we want to balance two needs: minimizing the cost of hardware by utilizing hard disks if we can [15], and maximizing hard disk productivity by having them reading data most of the time, rather than seeking.&lt;/p&gt;
    &lt;p&gt;To achieve that, directory policies offer a way to tune how large each block will be, and to tune which drives will be picked based on block size. This allows us to configure TernFS so that larger files that can be read sequentially are stored on hard disks, while random-access or small files are stored on flash. [16]&lt;/p&gt;
    &lt;p&gt;Currently this system is not adaptive, but we found that in practice it's easy to carve out sections of the filesystem which are not read sequentially. We have a default configuration which assumes sequential reads and then uses hard disks down to roughly 2.5MB blocks, below which hard disks stop being productive enough and blocks start needing to be written to flash.&lt;/p&gt;
    &lt;head rend="h4"&gt;Block service picking&lt;/head&gt;
    &lt;p&gt;OK, we now know what type of drive to select for our files, but we still have tens of thousands of individual drives to pick from. Picking the 'right' individual drive requires some sophistication.&lt;/p&gt;
    &lt;p&gt;The first thing to note is that drive failures or unavailability are often correlated. For instance, at XTX a single server handles 102 spinning disks. If the server is down, faulty, or needs to be decommissioned, it'll render its 102 disks temporarily or permanently unavailable.&lt;/p&gt;
    &lt;p&gt;It's therefore wise to spread a file's blocks across many servers. To achieve this, each TernFS block service (which generally corresponds to a single drive) has a failure domain. When picking block services in which to store the blocks for a given file, TernFS will make sure that each block is in a separate failure domain. In our TernFS deployment a failure domain corresponds to a server, but other users might wish to tie it to some other factor as appropriate.&lt;/p&gt;
    &lt;p&gt;TernFS also tries hard to avoid write bottlenecks by spreading the current write load across many disks. Moreover, since new drives can be added at any time, it tries to converge to a situation where each drive is roughly equally filled by assigning writing more to drives with more available space.&lt;/p&gt;
    &lt;p&gt;Mechanically this is achieved by having each shard periodically request a set of block services to use for writing from the registry. When handing out block services to shards, the registry selects block services according to several constraints:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It never gives block services from the same failure domain to the same shard&lt;/item&gt;
      &lt;item&gt;It minimizes the variance in how many shards each block service is currently assigned to&lt;/item&gt;
      &lt;item&gt;It prioritizes block services which have more available space.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Then when a client wants to write a new span, requiring D+P blocks, the shard simply selects D+P block services randomly amongst the ones it last received from the registry.&lt;/p&gt;
    &lt;p&gt;One concept currently absent from TernFS is what is often known as 'copyset replication'. When assigning disks to files at random (even with the caveat of failure domains) the probability of rendering at least one file unreadable quickly becomes a certainty as more and more drives fail:&lt;/p&gt;
    &lt;p&gt;Copysets reduce the likelihood of data loss occurring by choosing blocks out of a limited number of sets of drives, as opposed to picking the drives randomly. This dramatically reduces the probability of data loss[17]. They are generally a good idea, but we haven't found them to be worthwhile, for a few reasons.&lt;/p&gt;
    &lt;p&gt;Note that while copysets reduce the failure probability, they do not (and cannot) reduce the expected amount of data loss. That is, instead of a large probability of a relatively small amount of data loss we have a very small probability of a catastrophic loss.&lt;/p&gt;
    &lt;p&gt;First, evacuating a 20TB drive takes just a few minutes, and in the presence of multiple failed drives the migrator process evacuates first the files which are present in multiple failed drives to get ahead of possible data loss. This means that for TernFS to lose data within a single data centre tens of drives would have to fail within a matter of seconds.&lt;/p&gt;
    &lt;p&gt;More importantly, our TernFS deployment is replicated across three data centres. This replication eliminates the chance of losing data due to 'independent' drive failures ‚Äî thousands of drives would need to fail at once. Obviously, data centre wide events can cause a large proportion of the drives within it to fail, but having such an event in three data centres at once is exceedingly unlikely.&lt;/p&gt;
    &lt;p&gt;Finally, copysets are not without drawbacks or complications. Assigning drives at random is an optimal strategy when it comes to evacuating drives quickly, since the files with blocks in the drives to be evacuated will be evenly spread over the rest of the filesystem, and since we only ever need to replace the failed blocks given that we're not constrained by fitting the new set of blocks in predetermined copysets. This means that the evacuation procedure will not be bottlenecked by drive throughput, which is what enables evacuation to finish in a matter of minutes. Moreover, the algorithm to distribute drives to shards is significantly simpler and more flexible than if it needed to care about copysets.&lt;/p&gt;
    &lt;p&gt;However, users that wish to deploy TernFS within a single data centre might wish to implement some form of copyset replication. Such a change would be entirely contained to the registry and would not change any other component.&lt;/p&gt;
    &lt;head rend="h4"&gt;Block Proofs&lt;/head&gt;
    &lt;p&gt;We now have a solid scheme to store files redundantly (thanks to Reed-Solomon codes) and protect against bitrot (thanks to the checksums). However, said schemes are only as good as their implementation.&lt;/p&gt;
    &lt;p&gt;As previously mentioned, TernFS clients communicate their intention to write a file to metadata servers, the metadata servers select block services that the blocks should be written to, and the clients then write the blocks to block services independently of the metadata services. The same happens when a client wants to erase blocks: the client first communicates its intentions to delete the blocks to the right metadata shard and then performs the erasing itself.&lt;/p&gt;
    &lt;p&gt;This poses a challenge. While verifying the correctness of the core TernFS services is feasible, verifying all clients is not, but we'd still like to prevent buggy clients from breaking key invariants of the filesystem.&lt;/p&gt;
    &lt;p&gt;Buggy clients can wreak havoc in several ways:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;They can leak data by writing blocks to block services that are not referenced anywhere in the metadata.&lt;/item&gt;
      &lt;item&gt;They can lose data by erasing blocks which are still referenced in metadata.&lt;/item&gt;
      &lt;item&gt;They can corrupt data by telling the metadata services they'll write something and then writing something else.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We address all these points by using what we call block proofs. To illustrate how block proofs work, it's helpful to go through the steps required to write new data to a file.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;When a client is creating a file, it'll do so by adding its file spans one-by-one. For each span the client wants to add it sends an 'initiate span creation' request to the right metadata shard. This request contains both the overall checksum of the span, and the checksum of each block in it (including parity blocks).&lt;/item&gt;
      &lt;item&gt;The metadata shard checks the consistency of the checksum of the span and of its blocks, something it can do thanks to some desirable mathematical properties of CRCs.&lt;/item&gt;
      &lt;item&gt;The shard picks block services for the blocks to be written in and returns this information to the client together with a signature for each 'block write' instruction.&lt;/item&gt;
      &lt;item&gt;The client forwards this signature to the block services, which will refuse to write the block without it. Crucially, the cryptographic signature ranges over a unique identity for the block (ensuring we only write the block we mean to write), together with its checksum, ensuring we don't write the wrong data.[18]&lt;/item&gt;
      &lt;item&gt;After committing the block to disk, the block service returns a 'block written' signature to the client.&lt;/item&gt;
      &lt;item&gt;Finally, the client forwards the block written signature back to the shard, which certifies that the span has been written only when it has received the signatures for all the blocks that make up the span. [19]&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This kind of scheme was described more generally in a separate blog post.&lt;/p&gt;
    &lt;p&gt;Similarly, when a client wants to delete a span, it first asks the metadata shard to start doing so. The metadata shard marks the span as 'in deletion' and returns a bunch of 'block erase' signatures to the client. The client then forwards the signatures to the block services that hold the blocks, which delete the blocks, and return a 'block erased' signature. The clients forward these signatures back to the metadata shards, which can then forget about the span entirely.&lt;/p&gt;
    &lt;p&gt;We use AES to generate the signatures for simplicity but note that the goal here is not protecting ourselves from malicious clients ‚Äî just buggy ones. The keys used for the signature are not kept secret, and CRC32-C is not a secure checksum. That said, we've found this scheme enormously valuable in the presence of complex clients. We spent considerable efforts making the core services very simple so we could then take more implementation risks in the clients, with the knowledge that we would have a very low chance of corrupting the filesystem itself.&lt;/p&gt;
    &lt;head rend="h4"&gt;Scrubbing&lt;/head&gt;
    &lt;p&gt;Finally, if things go wrong, we need to notice. The most common failure mode for a drive is for it to fail entirely, in which case our internal hardware monitoring system will pick it up and migrate from it automatically. The more insidious (and still very common) case is a single sector failing in a drive, which will only be noticed when we try to read the block involving that sector.&lt;/p&gt;
    &lt;p&gt;This is acceptable for files which are read frequently, but some files might be very 'cold' but still very important.&lt;/p&gt;
    &lt;p&gt;Consider the case of raw market data taps which are immediately converted to some processed, lossy format. While we generally will use the file containing the processed data, it's paramount to store the raw market data forever so that if we ever want to include more information from the original market data, we can. So important cold files might go months or even years without anyone reading them, and in the meantime, we might find that enough blocks have been corrupted to render them unreadable.[20]&lt;/p&gt;
    &lt;p&gt;To make sure this does not happen, a process called the scrubber continuously reads every block that TernFS stores, and replaces blocks with bad sectors before they can cause too much damage.&lt;/p&gt;
    &lt;head rend="h3"&gt;Snapshots and garbage collection&lt;/head&gt;
    &lt;p&gt;We've talked at length about what TernFS does to try to prevent data loss due to hardware failure or bugs in clients. However, the most common type of data loss is due to human error ‚Äî the &lt;code&gt;rm ‚Äîrf / home/alice/notes.txt&lt;/code&gt; scenario.&lt;/p&gt;
    &lt;p&gt;To protect against these scenarios, TernFS implements a lightweight snapshotting system. When files or directories are deleted, their contents aren't actually deleted. Instead, a weak reference to them is created. We call such weak references snapshot directory entries.&lt;/p&gt;
    &lt;p&gt;Snapshot entries are not be visible through the kernel module or the S3 gateway, but are visible through the direct API, and at XTX we have developed internal tooling to easily recover deleted files through it.[21] Deleted files are also visible through the TernFS web UI.&lt;/p&gt;
    &lt;p&gt;Given that 'normal' file operations do not delete files, but rather make them a snapshot, the task of freeing up space is delegated to an external Go process, the garbage collector. The garbage collector traverses the filesystem and removes expired snapshots, which involves deleting their blocks permanently. Snapshot expiry is predictably regulated by directory policies.&lt;/p&gt;
    &lt;head rend="h3"&gt;Keeping TernFS healthy&lt;/head&gt;
    &lt;p&gt;This last section covers how we (humans of XTX) notice problems in TernFS, and how TernFS self-heals when things go wrong ‚Äî both key topics if we want to ensure no data loss and notice performance problems early.&lt;/p&gt;
    &lt;head rend="h4"&gt;Performance metrics&lt;/head&gt;
    &lt;p&gt;TernFS exposes a plethora of performance metrics through the HTTP InfluxDB line protocol. While connecting TernFS to a service which ingests these metrics is optional, it is highly recommended for any production service.&lt;/p&gt;
    &lt;p&gt;Moreover, the kernel module exposes many performance metrics itself through DebugFS.&lt;/p&gt;
    &lt;p&gt;Both types of metrics, especially when used in tandem, have proved invaluable to resolve performance problems quickly.&lt;/p&gt;
    &lt;head rend="h4"&gt;Logging and alerts&lt;/head&gt;
    &lt;p&gt;TernFS services log their output to files in a simple line-based format. The internal logging API is extremely simple and includes support for syslog levels out-of-the-box. At XTX we run TernFS as normal systemd services and use journalctl to view logs.&lt;/p&gt;
    &lt;p&gt;As with metrics, the kernel module includes various logging facilities as well. The first type of logging is just through dmesg, but the kernel module also includes numerous tracepoints for low-overhead opt-in logging of many operations.&lt;/p&gt;
    &lt;p&gt;TernFS is also integrated with XTX's internal alerting system, called XMon, to page on call developers when things go wrong. XMon is not open source, but all the alerts are also rendered as error lines in logs. [22] We plan to eventually move to having alerts feed off performance metrics, which would make them independent from XMon, although we don't have plans to do so in the short-term.&lt;/p&gt;
    &lt;head rend="h4"&gt;Migrations&lt;/head&gt;
    &lt;p&gt;Finally, there's the question of what to do when drives die ‚Äî and they will die, frequently, when you have 50,000 of them. While drives dying is not surprising, we've been surprised at the variety of different drive failures. [23] A malfunctioning drive might:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Produce IO errors when reading specific files. This is probably due to a single bad sector.&lt;/item&gt;
      &lt;item&gt;Produce IO errors when reading or writing anything. This might happen because enough bad sectors have gone bad and the drive cannot remap them, or for a variety of other reasons.&lt;/item&gt;
      &lt;item&gt;Return wrong data. This is usually caught by the built-in error correction codes in the hard drives, but not always.&lt;/item&gt;
      &lt;item&gt;Lie about data being successfully persisted. This can manifest in a variety of ways: file size being wrong on open, file contents being partially zero'd out, and so on.&lt;/item&gt;
      &lt;item&gt;Disappear from the mount list, only to reappear when the machine is rebooted, but missing some data.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When clients fail to read from a drive, they'll automatically fall back on other drives to reconstruct the missing data, which is extremely effective in hiding failures from the end-user. That said, something needs to be done about the bad drives, and done quickly to avoid permanent data loss.&lt;/p&gt;
    &lt;p&gt;The TernFS registry allows marking drives as faulty. Faulty drives are then picked up by the migrator, a Go process which waits for bad drives and then stores all its blocks onto freshly picked block services.&lt;/p&gt;
    &lt;p&gt;TernFS also tries to mark drives as bad automatically using a simple heuristic based on the rate of IO errors the drive is experiencing. The number of drives automatically marked as faulty is throttled to avoid having this check go awry and mark the whole cluster as faulty, which would not be catastrophic but would still be messy to deal with.&lt;/p&gt;
    &lt;p&gt;Moreover, drives that are faulty in subtle ways might not be picked up by the heuristics, which means that occasionally a sysadmin will need to mark a drive as faulty manually, after which the migrator will evacuate them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Closing thoughts&lt;/head&gt;
    &lt;p&gt;At XTX we feel strongly about utilizing our resources efficiently. When it comes to software, this means having software that gets close to some theoretical optimum when it comes to total cost of ownership. This culture was borne out by competing hard for technological excellence when doing on-exchange trading at first, and by our ever-growing hardware costs as our business has grown later.&lt;/p&gt;
    &lt;p&gt;Such idealized tools might not exist or be available yet, in which case we're happy to be the tool makers. TernFS is a perfect example of this and we're excited to open source this component of our business for the community.&lt;/p&gt;
    &lt;p&gt;Crucially, the cost of implementation of a new solution is often overblown compared to the cost of tying yourself to an ill-fitting, expensive third-party solution. Designing and implementing a solution serving exactly your needs allows for much greater simplicity. If the requirements do change, as often happens, changes can be implemented very quickly, again only catering to your needs.&lt;/p&gt;
    &lt;p&gt;That said, we believe that TernFS' set of trade-offs are widely shared across many organizations dealing with large-scale storage workloads, and we hope we'll contribute to at least slowing down the seemingly constant stream of new filesystems.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45646691</guid><pubDate>Mon, 20 Oct 2025 17:36:16 +0000</pubDate></item><item><title>x86-64 Playground ‚Äì An online assembly editor and GDB-like debugger</title><link>https://x64.halb.it/</link><description>&lt;doc fingerprint="aefedf1ae8255dda"&gt;
  &lt;main&gt;
    &lt;p&gt;x86-64 Playground is a web app for experimenting and learning x86-64 assembly.&lt;/p&gt;
    &lt;p&gt;The Playground web app provides an online code editor where you can write, compile, and share assembly code for a wide range of popular assemblers such as GNU As, Fasm and Nasm.&lt;/p&gt;
    &lt;p&gt;Unlike traditional onlide editors, this playground allows you to follow the execution of your program step by step, inspecting memory and registers of the running process from a GDB-like interface.&lt;/p&gt;
    &lt;p&gt;You can bring your own programs! Drag and drop into the app any x86-64-Linux static executable to run and debug it in the same sandboxed environment, without having to install anything.&lt;/p&gt;
    &lt;p&gt;The app is for anyone that wants to run amd64 assembly snippets or inspect the inner workings of simple Linux ELF files.&lt;/p&gt;
    &lt;p&gt;It has been designed with the academic world of binary exploitation in mind; The debugger interface offers visualizations similar to the GDB+PwnGDB debugger plugin, and all the controls are labelled with the respective GDB commands.&lt;/p&gt;
    &lt;p&gt;Combined with Compiler Explorer, this app provides a noise-free environment to learn the basics behind the inner workings of a Linux process. When you are ready, it includes the guides and resources necessary to keep experimenting on your own linux environment, with the actual GDB debugger.&lt;/p&gt;
    &lt;p&gt;Have you ever seen a responsive debugger? The app places the mobile experience at the center of its design, and can be embedded in any web page to add interactivity to technical tutorials or documentations.&lt;/p&gt;
    &lt;p&gt;Follow the guide to embed in your website both the asm editor and debugger.&lt;/p&gt;
    &lt;p&gt;The app is open-source, and available on Github. It's powered by the Blink Emulator, which emulates an x86-64-Linux environment entirely client side in your browser. This means that all the code you write, or the excutables you debug are never sent to the server.&lt;/p&gt;
    &lt;p&gt;everything runs in your browser, and once the Web App loads it will work without an internet connection.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45646958</guid><pubDate>Mon, 20 Oct 2025 17:55:18 +0000</pubDate></item><item><title>Claude Code on the web</title><link>https://www.anthropic.com/news/claude-code-on-the-web</link><description>&lt;doc fingerprint="7f0c0efeffb1c2ef"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Claude Code on the web&lt;/head&gt;
    &lt;p&gt;Today, we're introducing Claude Code on the web, a new way to delegate coding tasks directly from your browser.&lt;/p&gt;
    &lt;p&gt;Now in beta as a research preview, you can assign multiple coding tasks to Claude that run on Anthropic-managed cloud infrastructure, perfect for tackling bug backlogs, routine fixes, or parallel development work.&lt;/p&gt;
    &lt;head rend="h2"&gt;Run coding tasks in parallel&lt;/head&gt;
    &lt;p&gt;Claude Code on the web lets you kick off coding sessions without opening your terminal. Connect your GitHub repositories, describe what you need, and Claude handles the implementation.&lt;/p&gt;
    &lt;p&gt;Each session runs in its own isolated environment with real-time progress tracking, and you can actively steer Claude to adjust course as it‚Äôs working through tasks.&lt;/p&gt;
    &lt;p&gt;With Claude Code running in the cloud, you can now run multiple tasks in parallel across different repositories from a single interface and ship faster with automatic PR creation and clear change summaries.&lt;/p&gt;
    &lt;head rend="h2"&gt;Flexible for every workflow&lt;/head&gt;
    &lt;p&gt;The web interface complements your existing Claude Code workflow. Running tasks in the cloud is especially effective for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Answering questions about how projects work and how repositories are mapped&lt;/item&gt;
      &lt;item&gt;Bugfixes and routine, well-defined tasks&lt;/item&gt;
      &lt;item&gt;Backend changes, where Claude Code can use test-driven development to verify changes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can also use Claude Code on mobile. As part of this research preview, we‚Äôre making Claude Code available on our iOS app so developers can explore coding with Claude on the go. It‚Äôs an early preview, and we hope to quickly refine the mobile experience based on your feedback.&lt;/p&gt;
    &lt;head rend="h2"&gt;Security-first cloud execution&lt;/head&gt;
    &lt;p&gt;Every Claude Code task runs in an isolated sandbox environment with network and filesystem restrictions. Git interactions are handled through a secure proxy service that ensures Claude can only access authorized repositories‚Äîhelping keep your code and credentials protected throughout the entire workflow.&lt;/p&gt;
    &lt;p&gt;You can also add custom network configuration to choose what domains Claude Code can connect to from its sandbox. For example, you can allow Claude to download npm packages over the internet so that it can run tests and validate changes.&lt;/p&gt;
    &lt;p&gt;Read our engineering blog and documentation for a deep dive on Claude Code‚Äôs sandboxing approach.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting started&lt;/head&gt;
    &lt;p&gt;Claude Code on the web is available now in research preview for Pro and Max users. Visit claude.com/code to connect your first repository and start delegating tasks.&lt;/p&gt;
    &lt;p&gt;Cloud-based sessions share rate limits with all other Claude Code usage. Explore our documentation to learn more.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45647166</guid><pubDate>Mon, 20 Oct 2025 18:12:23 +0000</pubDate></item><item><title>Today is when the Amazon brain drain sent AWS down the spout</title><link>https://www.theregister.com/2025/10/20/aws_outage_amazon_brain_drain_corey_quinn/</link><description>&lt;doc fingerprint="3d28b35914248ecf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Today is when the Amazon brain drain finally sent AWS down the spout&lt;/head&gt;
    &lt;head rend="h2"&gt;When your best engineers log off for good, don‚Äôt be surprised when the cloud forgets how DNS works&lt;/head&gt;
    &lt;p&gt;column "It's always DNS" is a long-standing sysadmin saw, and with good reason: a disproportionate number of outages are at their heart DNS issues. And so today, as AWS is still repairing its downed cloud as this article goes to press, it becomes clear that the culprit is once again DNS. But if you or I know this, AWS certainly does.&lt;/p&gt;
    &lt;p&gt;And so, a quiet suspicion starts to circulate: where have the senior AWS engineers who've been to this dance before gone? And the answer increasingly is that they've left the building ‚Äî taking decades of hard-won institutional knowledge about how AWS's systems work at scale right along with them.&lt;/p&gt;
    &lt;head rend="h3"&gt;What happened?&lt;/head&gt;
    &lt;p&gt;AWS reports that on October 20, at 12:11 AM PDT, it began investigating ‚Äúincreased error rates and latencies for multiple AWS services in the US-EAST-1 Region.‚Äù About an hour later, at 1:26 AM, the company confirmed ‚Äúsignificant error rates for requests made to the DynamoDB endpoint‚Äù in that region. By 2:01 AM, engineers had identified DNS resolution of the DynamoDB API endpoint for US-EAST-1 as the likely root cause, which led to cascading failures for most other things in that region. DynamoDB is a "foundational service" upon which a whole mess of other AWS services rely, so the blast radius for an outage touching this thing can be huge.&lt;/p&gt;
    &lt;p&gt;As a result, much of the internet stopped working: banking, gaming, social media, government services, buying things I don't need on Amazon.com itself, etc.&lt;/p&gt;
    &lt;p&gt;AWS has given increasing levels of detail, as is their tradition, when outages strike, and as new information comes to light. Reading through it, one really gets the sense that it took them 75 minutes to go from "things are breaking" to "we've narrowed it down to a single service endpoint, but are still researching," which is something of a bitter pill to swallow. To be clear: I've seen zero signs that this stems from a lack of transparency, and every indication that they legitimately did not know what was breaking for a patently absurd length of time.&lt;/p&gt;
    &lt;p&gt;Note that for those 75 minutes, visitors to the AWS status page (reasonably wondering why their websites and other workloads had just burned down and crashed into the sea) were met with an "all is well!" default response. Ah well, it's not as if AWS had previously called out slow outage notification times as an area for improvement. Multiple times even. We can keep doing this if you'd like.&lt;/p&gt;
    &lt;head rend="h3"&gt;The prophecy&lt;/head&gt;
    &lt;p&gt;AWS is very, very good at infrastructure. You can tell this is a true statement by the fact that a single one of their 38 regions going down (albeit a very important region!) causes this kind of attention, as opposed to it being "just another Monday outage." At AWS's scale, all of their issues are complex; this isn't going to be a simple issue that someone should have caught, just because they've already hit similar issues years ago and ironed out the kinks in their resilience story.&lt;/p&gt;
    &lt;p&gt;Once you reach a certain point of scale, there are no simple problems left. What's more concerning to me is the way it seems AWS has been flailing all day trying to run this one to ground. Suddenly, I'm reminded of something I had tried very hard to forget.&lt;/p&gt;
    &lt;p&gt;At the end of 2023, Justin Garrison left AWS and roasted them on his way out the door. He stated that AWS had seen an increase in Large Scale Events (or LSEs), and predicted significant outages in 2024. It would seem that he discounted the power of inertia, but the pace of senior AWS departures certainly hasn't slowed ‚Äî and now, with an outage like this, one is forced to wonder whether those departures are themselves a contributing factor.&lt;/p&gt;
    &lt;p&gt;You can hire a bunch of very smart people who will explain how DNS works at a deep technical level (or you can hire me, who will incorrect you by explaining that it's a database), but the one thing you can't hire for is the person who remembers that when DNS starts getting wonky, check that seemingly unrelated system in the corner, because it's historically played a contributing role to some outages of yesteryear.&lt;/p&gt;
    &lt;p&gt;When that tribal knowledge departs, you're left having to reinvent an awful lot of in-house expertise that didn't want to participate in your RTO games, or play Layoff Roulette yet again this cycle. This doesn't impact your service reliability ‚Äî until one day it very much does, in spectacular fashion. I suspect that day is today.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AWS outage exposes Achilles heel: central control plane&lt;/item&gt;
      &lt;item&gt;Major AWS outage across US-East region breaks half the internet&lt;/item&gt;
      &lt;item&gt;Amazon spills plan to nuke Washington...with X-Energy mini-reactors&lt;/item&gt;
      &lt;item&gt;Amazon turns James Bond into the Man Without the Golden Gun&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;The talent drain evidence&lt;/head&gt;
    &lt;p&gt;This is The Register, a respected journalistic outlet. As a result, I know that if I publish this piece as it stands now, an AWS PR flak will appear as if by magic, waving their hands, insisting that "there is no talent exodus at AWS," a la Baghdad Bob. Therefore, let me forestall that time-wasting enterprise with some data.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It is a fact that there have been 27,000+ Amazonians impacted by layoffs between 2022 and 2024, continuing into 2025. It's hard to know how many of these were AWS versus other parts of its Amazon parent, because the company is notoriously tight-lipped about staffing issues.&lt;/item&gt;
      &lt;item&gt;Internal documents reportedly say that Amazon suffers from 69 percent to 81 percent regretted attrition across all employment levels. In other words, "people quitting who we wish didn't."&lt;/item&gt;
      &lt;item&gt;The internet is full of anecdata of senior Amazonians lamenting the hamfisted approach of their Return to Office initiative; experts have weighed in citing similar concerns.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you were one of the early employees who built these systems, the world is your oyster. There's little reason to remain at a company that increasingly demonstrates apparent disdain for your expertise.&lt;/p&gt;
    &lt;head rend="h3"&gt;My take&lt;/head&gt;
    &lt;p&gt;This is a tipping point moment. Increasingly, it seems that the talent who understood the deep failure modes is gone. The new, leaner, presumably less expensive teams lack the institutional knowledge needed to, if not prevent these outages in the first place, significantly reduce the time to detection and recovery. Remember, there was a time when Amazon's "Frugality" leadership principle meant doing more with less, not doing everything with basically nothing. AWS's operational strength was built on redundant, experienced people, and when you cut to the bone, basic things start breaking.&lt;/p&gt;
    &lt;p&gt;I want to be very clear on one last point. This isn't about the technology being old. It's about the people maintaining it being new. If I had to guess what happens next, the market will forgive AWS this time, but the pattern will continue.&lt;/p&gt;
    &lt;p&gt;AWS will almost certainly say this was an "isolated incident," but when you've hollowed out your engineering ranks, every incident becomes more likely. The next outage is already brewing. It's just a matter of which understaffed team trips over which edge case first, because the chickens are coming home to roost. ¬Æ&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45649178</guid><pubDate>Mon, 20 Oct 2025 20:50:03 +0000</pubDate></item><item><title>Building a message queue with only two UNIX signals</title><link>https://leandronsp.com/articles/you-dont-need-kafka-building-a-message-queue-with-only-two-unix-signals</link><description>&lt;doc fingerprint="ae0ef8377d2b04f1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;You don't need Kafka: Building a message queue with only two UNIX signals&lt;/head&gt;
    &lt;p&gt;Have you ever asked yourself what if we could replace any message broker with a very simple one using only two UNIX signals? Well, I‚Äôm not surprised if you didn‚Äôt. But I did. And I want to share my journey of how I achieved it.&lt;/p&gt;
    &lt;p&gt;If you want to learn about UNIX signals, binary operations the easy way, how a message broker works under the hood, and a bit of Ruby, this post is for you.&lt;/p&gt;
    &lt;p&gt;And if you came here just because of the clickbait title, I apologize and invite you to keep reading. It‚Äôll be fun, I promise.&lt;/p&gt;
    &lt;head rend="h2"&gt;It‚Äôs all about UNIX&lt;/head&gt;
    &lt;p&gt;A few days ago, I saw some discussion on the internet about how we could send messages between processes. Many people think of sockets, which are the most common way to send messages, even allowing communication across different machines and networks. Some don‚Äôt even realize that pipes are another way to send messages between processes:&lt;/p&gt;
    &lt;code&gt;$ echo 'hello' | base64
aGVsbG8K
&lt;/code&gt;
    &lt;p&gt;Here‚Äôs what‚Äôs happening:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The process &lt;code&gt;echo&lt;/code&gt;is started with the content ‚Äúhello‚Äù&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;echo&lt;/code&gt;is a program that prints the message to STDOUT&lt;/item&gt;
      &lt;item&gt;Through the pipe, the content in STDOUT is sent directly to the STDIN of the &lt;code&gt;base64&lt;/code&gt;process&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;base64&lt;/code&gt;process encodes its input to Base64 and then puts the result in STDOUT&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note the word ‚Äúsend‚Äù. Yes, anonymous pipes are a form of IPC (Inter-process communication). Other forms of IPC in UNIX include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;named pipes (mkfifo)&lt;/item&gt;
      &lt;item&gt;sockets&lt;/item&gt;
      &lt;item&gt;regular files&lt;/item&gt;
      &lt;item&gt;or even a simple signal&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;UNIX signals&lt;/head&gt;
    &lt;p&gt;According to Wikipedia:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;A UNIX signal is a standardized message sent to a program to trigger specific behaviour, such as quitting or error handling&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;There are many signals we can send to a process, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SIGTERM - sends a notification to the process to terminate. It can be ‚Äútrapped,‚Äù which means the process can do some cleanup work before termination, like releasing OS resources and closing file descriptors&lt;/item&gt;
      &lt;item&gt;SIGKILL - sends a termination signal that cannot be trapped or ignored, forcing immediate termination&lt;/item&gt;
      &lt;item&gt;SIGINT - the interrupt signal, typically sent when you press &lt;code&gt;Ctrl+C&lt;/code&gt;in the terminal. It can be trapped, allowing the process to perform cleanup before exiting gracefully&lt;/item&gt;
      &lt;item&gt;SIGHUP - the hangup signal, originally sent when a terminal connection was lost. Modern applications often use it to reload configuration files without restarting the process&lt;/item&gt;
      &lt;item&gt;SIGQUIT - similar to SIGINT but also generates a core dump for debugging&lt;/item&gt;
      &lt;item&gt;SIGSTOP - pauses (suspends) a process. Cannot be trapped or ignored&lt;/item&gt;
      &lt;item&gt;SIGCONT - resumes a process that was paused by SIGSTOP&lt;/item&gt;
      &lt;item&gt;SIGCHLD - sent to a parent process when a child process terminates or stops&lt;/item&gt;
      &lt;item&gt;SIGUSR1 and SIGUSR2 - user-defined signals that applications can use for custom purposes&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Sending messages using signals&lt;/head&gt;
    &lt;p&gt;Okay, we know that signals are a primitive form of IPC. UNIX-like systems provide a syscall called &lt;code&gt;kill&lt;/code&gt; that sends signals to processes. Historically, this syscall was created solely to terminate processes. But over time, they needed to accommodate other types of signals, so they reused the same syscall for different purposes.&lt;/p&gt;
    &lt;p&gt;For instance, let‚Äôs create a simple Ruby script &lt;code&gt;sleeper.rb&lt;/code&gt; which sleeps for 60 seconds, nothing more:&lt;/p&gt;
    &lt;code&gt;puts "Process ID: #&amp;amp;lbrace;Process.pid&amp;amp;rbrace;"
puts "Sleeping for 60 seconds..."
sleep 60
&lt;/code&gt;
    &lt;p&gt;After running we see:&lt;/p&gt;
    &lt;code&gt;Process ID: 55402
Sleeping for 60 seconds...
&lt;/code&gt;
    &lt;p&gt;In another window, we can send the &lt;code&gt;SIGTERM&lt;/code&gt; signal to the process &lt;code&gt;55402&lt;/code&gt; via syscall &lt;code&gt;kill&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;$ kill -SIGTERM 55402
&lt;/code&gt;
    &lt;p&gt;And then, in the script session:&lt;/p&gt;
    &lt;code&gt;[1]    55402 terminated  ruby sleeper.rb
&lt;/code&gt;
    &lt;head rend="h3"&gt;Signal traps&lt;/head&gt;
    &lt;p&gt;In Ruby, we can also trap a signal using the &lt;code&gt;trap&lt;/code&gt; method in Ruby:&lt;/p&gt;
    &lt;code&gt;puts "Process ID: #&amp;amp;lbrace;Process.pid&amp;amp;rbrace;"
puts "Sleeping for 60 seconds..."

trap('SIGTERM') do 
  puts "Received SIGTERM, exiting gracefully..."
  exit
end

sleep 60
&lt;/code&gt;
    &lt;p&gt;Which in turn, after sending the signal, will gracefully:&lt;/p&gt;
    &lt;code&gt;Process ID: 55536
Sleeping for 60 seconds...
Received SIGTERM, exiting gracefully...
&lt;/code&gt;
    &lt;p&gt;After all, we cannot send messages using signals. They are a primitive way of sending standardized messages which will trigger specific behaviours. At most, we can trap some signals, but nothing more.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Okay Leandro, but what‚Äôs the purpose of this article then?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Hold on. That‚Äôs exactly why I‚Äôm here. To prove points by doing useless stuff, like when I simulated OOP in Bash a couple of years ago (it was fun though).&lt;/p&gt;
    &lt;p&gt;To understand how we can ‚Äúhack‚Äù UNIX signals and send messages between processes, let‚Äôs first talk a bit about binary operations. Yes, those ‚Äúzeros‚Äù and ‚Äúones‚Äù you were scared of when you saw them for the first time. But they don‚Äôt bite (ü•Å LOL), I promise.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is a message?&lt;/head&gt;
    &lt;p&gt;If we model a message as a sequence of characters, we could say that at a high-level, messages are simply strings. But in memory, they are stored as bytes.&lt;/p&gt;
    &lt;p&gt;We know that bytes are made of bits. In computer terms, what‚Äôs a bit? It‚Äôs simply an abstraction representing only two states:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;zero&lt;/item&gt;
      &lt;item&gt;one&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That‚Äôs it. For instance, using ASCII, we know that the letter ‚Äúh‚Äù has the following codes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;104 in decimal&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0x68&lt;/code&gt;in hexadecimal&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;01101000&lt;/code&gt;in binary&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Binary-wise, what if we represented each ‚Äú0‚Äù with a specific signal and each ‚Äú1‚Äù with another? We know that some signals such as SIGTERM, SIGINT, and SIGCONT can be trapped, but intercepting them would harm their original purpose.&lt;/p&gt;
    &lt;p&gt;But thankfully, UNIX provides two user-defined signals that are perfect for our hacking experiment.&lt;/p&gt;
    &lt;head rend="h2"&gt;Sending SIGUSR1 and SIGUSR2&lt;/head&gt;
    &lt;p&gt;First things first, let‚Äôs trap those signals in the code:&lt;/p&gt;
    &lt;code&gt;puts "Process ID: #&amp;amp;lbrace;Process.pid&amp;amp;rbrace;"
puts "Sleeping forever. Send signals to this process to see how it responds."

trap('SIGUSR1') do 
  puts "Received SIGUSR1 signal"
end

trap('SIGUSR2') do
  puts "Received SIGUSR2 signal"
end

sleep
&lt;/code&gt;
    &lt;code&gt;Process ID: 56172
Sleeping forever. Send signals to this process to see how it responds.
&lt;/code&gt;
    &lt;p&gt;After sending some &lt;code&gt;kill -SIGUSR1 56172&lt;/code&gt; and &lt;code&gt;kill -SIGUSR2 56172&lt;/code&gt;, we can see that the process prints the following content:&lt;/p&gt;
    &lt;code&gt;Process ID: 56172
Sleeping forever. Send signals to this process to see how it responds.
Received SIGUSR1 signal
Received SIGUSR2 signal
Received SIGUSR2 signal
Received SIGUSR1 signal
Received SIGUSR1 signal
Received SIGUSR2 signal
&lt;/code&gt;
    &lt;p&gt;Signals don‚Äôt carry data. But the example we have is perfect for changing to bits, uh?&lt;/p&gt;
    &lt;code&gt;Received SIGUSR1 signal # 0
Received SIGUSR2 signal # 1
Received SIGUSR2 signal # 1
Received SIGUSR1 signal # 0
Received SIGUSR2 signal # 1
Received SIGUSR1 signal # 0
Received SIGUSR1 signal # 0
Received SIGUSR1 signal # 0
&lt;/code&gt;
    &lt;p&gt;That‚Äôs exactly &lt;code&gt;01101000&lt;/code&gt;, the binary representation of the letter ‚Äúh‚Äù. We‚Äôre simply encoding the letter as a binary representation and sending it via signals&lt;/p&gt;
    &lt;p&gt;Again, we‚Äôre encoding it as a binary and sending it via signals.&lt;/p&gt;
    &lt;p&gt;How cool is that?&lt;/p&gt;
    &lt;head rend="h3"&gt;Decoding the binary data&lt;/head&gt;
    &lt;p&gt;On the other side, the receiver should be capable of decoding the message and converting it back to the letter ‚Äúh‚Äù:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;sender encodes the message&lt;/item&gt;
      &lt;item&gt;receiver decodes the message&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So, how do we decode &lt;code&gt;01101000&lt;/code&gt; (the letter ‚Äúh‚Äù in ASCII)? Let‚Äôs break it down into a few steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;First, we need to see the 8 bits as individual digits in their respective positions&lt;/item&gt;
      &lt;item&gt;The rightmost bit is at position 0, whereas the leftmost bit is at position 7. This is how we define the most significant bit (MSB, the leftmost) and the least significant bit (LSB, the rightmost)&lt;/item&gt;
      &lt;item&gt;For this example, we perform a left shift operation on each bit and then sum all the values, in this case from MSB to LSB (the order doesn‚Äôt matter much for now): &lt;code&gt;(0 &amp;lt;&amp;lt; 7) + (1 &amp;lt;&amp;lt; 6) + (1 &amp;lt;&amp;lt; 5) + (0 &amp;lt;&amp;lt; 4) + ... + (0 &amp;lt;&amp;lt; 0)&lt;/code&gt;:&lt;lb/&gt;left shift on zeros will always produce a zero&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;0 &amp;lt;&amp;lt; 7&lt;/code&gt;=&lt;code&gt;(2 ** 7) * 0&lt;/code&gt;=&lt;code&gt;128 * 0&lt;/code&gt;= 0&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;1 &amp;lt;&amp;lt; 6&lt;/code&gt;=&lt;code&gt;(2 ** 6) * 1&lt;/code&gt;=&lt;code&gt;64 * 1&lt;/code&gt;= 64&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Similarly to the remaining bits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;1 &amp;lt;&amp;lt; 5&lt;/code&gt;= 32&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0 &amp;lt;&amp;lt; 4&lt;/code&gt;= 0&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;1 &amp;lt;&amp;lt; 3&lt;/code&gt;= 8&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0 &amp;lt;&amp;lt; 2&lt;/code&gt;= 0&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0 &amp;lt;&amp;lt; 1&lt;/code&gt;= 0&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0 &amp;lt;&amp;lt; 0&lt;/code&gt;= 0&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So, our sum becomes, from MSB to LSB:&lt;/p&gt;
    &lt;code&gt;MSB                          LSB
0   1    1    0   1   0   0   0
0 + 64 + 32 + 0 + 8 + 0 + 0 + 0 = 104
&lt;/code&gt;
    &lt;p&gt;104 is exactly the decimal representation of the letter ‚Äúh‚Äù in ASCII.&lt;/p&gt;
    &lt;p&gt;How wonderful is that?&lt;/p&gt;
    &lt;head rend="h3"&gt;Sending the letter ‚Äúh‚Äù&lt;/head&gt;
    &lt;p&gt;Now let‚Äôs convert these operations to Ruby code. We‚Äôll write a simple program &lt;code&gt;receiver.rb&lt;/code&gt; that receives signals in order from LSB to MSB (positions 0 to 7) and then converts them back to ASCII characters, printing to &lt;code&gt;STDOUT&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Basically, we‚Äôll accumulate bits and whenever we form a complete byte, we‚Äôll decode it to its ASCII representation. The very basic implementation of our &lt;code&gt;accumulate_bit(bit)&lt;/code&gt; method would look like as follows:&lt;/p&gt;
    &lt;code&gt;@position = 0 # start with the LSB
@accumulator = 0

def accumulate_bit(bit)
  # The left shift operator (&amp;lt;&amp;lt;) is used to 
  # shift the bits of the number to the left.
  #
  # This is equivalent of: (2 ** @position) * bit
  @accumulator += (bit &amp;lt;&amp;lt; @position)
  return @accumulator if @position == 7 # stop accumulating after 8 bits (byte)

  @position += 1 # move to the next bit position: 0 becomes 1, 1 becomes 2, etc.
end

# Letter "h" in binary is 01101000
# But we'll send from the LSB to the MSB
#
# 0110 1000 (MSB -&amp;gt; LSB) becomes 0001 0110 (LSB -&amp;gt; MSB)
# The order doesn't matter that much, it'll depend on 
# the receiver's implementation.
accumulate_bit(0)
accumulate_bit(0)
accumulate_bit(0)
accumulate_bit(1)
accumulate_bit(0)
accumulate_bit(1)
accumulate_bit(1)
accumulate_bit(0)

puts @accumulator # should print 104, which is the ASCII code for "h"
&lt;/code&gt;
    &lt;p&gt;Pay attention to this code. It‚Äôs very important and builds the foundation for the next steps. If you didn‚Äôt get it, go back and read it again. Try it yourself in the terminal or using your preferred programming language.&lt;/p&gt;
    &lt;p&gt;Now, how to convert the decimal &lt;code&gt;104&lt;/code&gt; to the ASCII character representation? Luckily, Ruby provides a method called &lt;code&gt;chr&lt;/code&gt; which does the job:&lt;/p&gt;
    &lt;code&gt;irb&amp;gt; puts 104.chr
=&amp;gt; "h"
&lt;/code&gt;
    &lt;p&gt;We could do the same job for the rest of the word ‚Äúhello‚Äù, for instance. According to the ASCII table, it should be the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;e&lt;/code&gt;in decimal is&lt;code&gt;101&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;l&lt;/code&gt;in decimal is&lt;code&gt;108&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;o&lt;/code&gt;in decimal is&lt;code&gt;111&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let‚Äôs check if Ruby knows that:&lt;/p&gt;
    &lt;code&gt;104.chr    # "h"
101.chr    # "e"
108.chr    # "l"
111.chr    # "o"
&lt;/code&gt;
    &lt;p&gt;We can even ‚Äúdecode‚Äù the word to the decimal representation in ASCII:&lt;/p&gt;
    &lt;code&gt;irb&amp;gt; "hello".bytes
=&amp;gt; [104, 101, 108, 108, 111]
&lt;/code&gt;
    &lt;p&gt;Now, time to finish our receiver implementation to properly print the letter ‚Äúh‚Äù:&lt;/p&gt;
    &lt;code&gt;@position = 0 # start with the LSB
@accumulator = 0

trap('SIGUSR1') &amp;amp;lbrace; decode_signal(0) &amp;amp;rbrace;
trap('SIGUSR2') &amp;amp;lbrace; decode_signal(1) &amp;amp;rbrace;

def decode_signal(bit)
  accumulate_bit(bit)
  return unless @position == 8 # if not yet accumulated a byte, keep accumulating

  print "Received byte: #&amp;amp;lbrace;@accumulator&amp;amp;rbrace; (#&amp;amp;lbrace;@accumulator.chr&amp;amp;rbrace;)\n"

  @accumulator = 0 # reset the accumulator
  @position = 0 # reset position for the next byte
end

def accumulate_bit(bit)
  # The left shift operator (&amp;lt;&amp;lt;) is used to 
  # shift the bits of the number to the left.
  #
  # This is equivalent of: (2 ** @position) * bit
  @accumulator += (bit &amp;lt;&amp;lt; @position)
  @position += 1 # move to the next bit position: 0 becomes 1, 1 becomes 2, etc.
end

puts "Process ID: #&amp;amp;lbrace;Process.pid&amp;amp;rbrace;"
sleep
&lt;/code&gt;
    &lt;p&gt;Read that code and its comments. It‚Äôs very important. Do not continue reading until you really get what‚Äôs happening here.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Whenever we get &lt;code&gt;SIGUSR1&lt;/code&gt;, we accumulate the bit&lt;code&gt;0&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;When getting &lt;code&gt;SIGUSR2&lt;/code&gt;, accumulate then the bit&lt;code&gt;1&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;When accumulator reaches the position&lt;code&gt;8&lt;/code&gt;, it means we have a byte. At this moment we should print the ASCII representation using the&lt;code&gt;.chr&lt;/code&gt;we seen earlier. Then, reset bit position and accumulator&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let‚Äôs see our receiver in action! Start the receiver in one terminal:&lt;/p&gt;
    &lt;code&gt;$ ruby receiver.rb
Process ID: 58219
&lt;/code&gt;
    &lt;p&gt;Great! Now the receiver is listening for signals. In another terminal, let‚Äôs manually send signals&lt;lb/&gt; to form the letter ‚Äúh‚Äù (which is &lt;code&gt;01101000&lt;/code&gt; in binary, remember?):&lt;/p&gt;
    &lt;code&gt;  # Sending from LSB to MSB: 0, 0, 0, 1, 0, 1, 1, 0
  $ kill -SIGUSR1 58219  # 0
  $ kill -SIGUSR1 58219  # 0
  $ kill -SIGUSR1 58219  # 0
  $ kill -SIGUSR2 58219  # 1
  $ kill -SIGUSR1 58219  # 0
  $ kill -SIGUSR2 58219  # 1
  $ kill -SIGUSR2 58219  # 1
  $ kill -SIGUSR1 58219  # 0
&lt;/code&gt;
    &lt;p&gt;And in the receiver terminal, we should see:&lt;/p&gt;
    &lt;code&gt;Received byte: 104 (h)
&lt;/code&gt;
    &lt;p&gt;How amazing is that? We just sent the letter ‚Äúh‚Äù using only two UNIX signals!&lt;/p&gt;
    &lt;p&gt;But wait. Manually sending 8 signals for each character? That‚Äôs tedious and error-prone. What if we wanted to send the word ‚Äúhello‚Äù? That‚Äôs 5 characters √ó 8 bits = 40 signals to send manually. No way.&lt;/p&gt;
    &lt;p&gt;We need a sender.&lt;/p&gt;
    &lt;head rend="h3"&gt;Building the sender&lt;/head&gt;
    &lt;p&gt;The sender‚Äôs job is the opposite of the receiver: it should encode a message (string) into bits and send them as signals to the receiver process.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs think about what we need:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Take a message as input (like ‚Äúhello‚Äù)&lt;/item&gt;
      &lt;item&gt;Convert each character to its byte representation&lt;/item&gt;
      &lt;item&gt;Extract the 8 bits from each byte&lt;/item&gt;
      &lt;item&gt;Send &lt;code&gt;SIGUSR1&lt;/code&gt;for bit 0,&lt;code&gt;SIGUSR2&lt;/code&gt;for bit 1&lt;/item&gt;
      &lt;item&gt;Repeat for all characters&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The tricky part here is the step 3: how do we extract individual bits from a byte? To extract the bit at position &lt;code&gt;i&lt;/code&gt;, we can use the following formula:&lt;/p&gt;
    &lt;code&gt;bit = (byte &amp;gt;&amp;gt; i) &amp;amp; 1
&lt;/code&gt;
    &lt;p&gt;Let me break this down:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;byte &amp;gt;&amp;gt; i&lt;/code&gt;performs a right shift by&lt;code&gt;i&lt;/code&gt;positions&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;&amp;amp; 1&lt;/code&gt;is a bitwise&lt;code&gt;AND&lt;/code&gt;operation that extracts only the rightmost bit&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For the letter ‚Äúh‚Äù (&lt;code&gt;01101000&lt;/code&gt; in binary, &lt;code&gt;104&lt;/code&gt; in decimal):&lt;/p&gt;
    &lt;p&gt;Position 0 (LSB):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;(104 &amp;gt;&amp;gt; 0)&lt;/code&gt;=&lt;code&gt;104 / (2 ** 0)&lt;/code&gt;=&lt;code&gt;104 / 1&lt;/code&gt;= 104&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;01101000&lt;/code&gt;&amp;gt;&amp;gt; 0 =&lt;code&gt;01101000&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;01101000&lt;/code&gt;&amp;amp;&lt;code&gt;00000001&lt;/code&gt;= 0 (one AND zero is zero)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Position 1:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;(104 &amp;gt;&amp;gt; 1)&lt;/code&gt;=&lt;code&gt;104 / (2 ** 1)&lt;/code&gt;=&lt;code&gt;104 / 2&lt;/code&gt;= 52&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;01101000&lt;/code&gt;&amp;gt;&amp;gt; 1 =&lt;code&gt;00110100&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;00110100&lt;/code&gt;&amp;amp;&lt;code&gt;00000001&lt;/code&gt;= 0&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Position 2:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;(104 &amp;gt;&amp;gt; 2)&lt;/code&gt;=&lt;code&gt;104 / (2 ** 2)&lt;/code&gt;=&lt;code&gt;104 / 4&lt;/code&gt;= 26&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;01101000&lt;/code&gt;&amp;gt;&amp;gt; 2 =&lt;code&gt;00011010&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;00011010&lt;/code&gt;&amp;amp;&lt;code&gt;00000001&lt;/code&gt;= 0&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Position 3:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;(104 &amp;gt;&amp;gt; 3)&lt;/code&gt;=&lt;code&gt;104 / (2 ** 3)&lt;/code&gt;=&lt;code&gt;104 / 8&lt;/code&gt;= 13&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;01101000&lt;/code&gt;&amp;gt;&amp;gt; 3 =&lt;code&gt;00001101&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;00001101&lt;/code&gt;&amp;amp;&lt;code&gt;00000001&lt;/code&gt;= 1 (one AND one equals one)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And so on for positions 4, 5, 6, and 7. This gives us: &lt;code&gt;0, 0, 0, 1, 0, 1, 1, 0&lt;/code&gt; ‚Äî exactly the bits we need from LSB to MSB!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;(104 &amp;gt;&amp;gt; 0) &amp;amp; 1&lt;/code&gt;=&lt;code&gt;104 &amp;amp; 1&lt;/code&gt;= 0&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;(104 &amp;gt;&amp;gt; 1) &amp;amp; 1&lt;/code&gt;=&lt;code&gt;52 &amp;amp; 1&lt;/code&gt;= 0&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;(104 &amp;gt;&amp;gt; 2) &amp;amp; 1&lt;/code&gt;=&lt;code&gt;26 &amp;amp; 1&lt;/code&gt;= 0&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;(104 &amp;gt;&amp;gt; 3) &amp;amp; 1&lt;/code&gt;=&lt;code&gt;13 &amp;amp; 1&lt;/code&gt;= 1&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;(104 &amp;gt;&amp;gt; 4) &amp;amp; 1&lt;/code&gt;=&lt;code&gt;6 &amp;amp; 1&lt;/code&gt;= 0&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;(104 &amp;gt;&amp;gt; 5) &amp;amp; 1&lt;/code&gt;=&lt;code&gt;3 &amp;amp; 1&lt;/code&gt;= 1&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;(104 &amp;gt;&amp;gt; 6) &amp;amp; 1&lt;/code&gt;=&lt;code&gt;1 &amp;amp; 1&lt;/code&gt;= 1&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;(104 &amp;gt;&amp;gt; 7) &amp;amp; 1&lt;/code&gt;=&lt;code&gt;0 &amp;amp; 1&lt;/code&gt;= 0&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;Pay close attention to this technique. It‚Äôs a fundamental operation in low-level programming.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;So now time to build the &lt;code&gt;sender.rb&lt;/code&gt; which is pretty simple:&lt;/p&gt;
    &lt;code&gt;receiver_pid = ARGV[0].to_i
message = ARGV[1..-1].join(' ')

def encode_byte(byte)
  8.times.map do |i|
    # Extract each bit from the byte, starting from the LSB
    (byte &amp;gt;&amp;gt; i) &amp;amp; 1
  end
end

message.bytes.each do |byte|
  encode_byte(byte).each do |bit|
    signal = bit == 0 ? 'SIGUSR1' : 'SIGUSR2'
    Process.kill(signal, receiver_pid)
    sleep 0.001 # Delay to allow the receiver to process the signal
  end
end
&lt;/code&gt;
    &lt;p&gt;For each byte (8-bit structure) we extract the bit performing the right shift + AND oprerations. The result is the extracted bit.&lt;/p&gt;
    &lt;p&gt;In the receiver window:&lt;/p&gt;
    &lt;code&gt;$ ruby receiver.rb
Process ID: 68968
&lt;/code&gt;
    &lt;p&gt;And in the sender window:&lt;/p&gt;
    &lt;code&gt;$ ruby sender.rb 68968 h
&lt;/code&gt;
    &lt;p&gt;The receiver will print:&lt;/p&gt;
    &lt;code&gt;$ ruby receiver.rb
Process ID: 68968
Received byte: 104 (h)
&lt;/code&gt;
    &lt;p&gt;Processes sending messages with only two signals! How wonderful is that?&lt;/p&gt;
    &lt;head rend="h3"&gt;Sending the ‚Äúhello‚Äù message&lt;/head&gt;
    &lt;p&gt;Now, sending the hello message is super easy. The sender is already able to send not only a letter but any message using signals:&lt;/p&gt;
    &lt;code&gt;$ ruby sender.rb 68968 hello

# And the receiver:
Received byte: 104 (h)
Received byte: 101 (e)
Received byte: 108 (l)
Received byte: 108 (l)
Received byte: 111 (o)
&lt;/code&gt;
    &lt;p&gt;Just change the &lt;code&gt;receiver&lt;/code&gt; implementation a little bit:&lt;/p&gt;
    &lt;code&gt;def decode_signal(bit)
  accumulate_bit(bit)
  return unless @position == 8 # if not yet accumulated a byte, keep accumulating

  print @accumulator.chr # print the byte as a character

  @accumulator = 0 # reset the accumulator
  @position = 0 # reset position for the next byte
end
&lt;/code&gt;
    &lt;p&gt;And then:&lt;/p&gt;
    &lt;code&gt;$ ruby sender.rb 96875 Hello

# In the receiver's terminal
Process ID: 96875
Hello
&lt;/code&gt;
    &lt;p&gt;However, if we send the message again, the receiver will print everything in the same line:&lt;/p&gt;
    &lt;code&gt;$ ruby sender.rb 96875 Hello
$ ruby sender.rb 96875 Hello

# In the receiver's terminal
Process ID: 96875
HelloHello
&lt;/code&gt;
    &lt;p&gt;It‚Äôs obvious: the receiver doesn‚Äôt know where the sender finished the message, so it‚Äôs impossible to know where we should stop one message and print the next one on a new line with &lt;code&gt;\n&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We should then determine how the sender indicates the end of the message. How about being it all zeroes (&lt;code&gt;0000 0000&lt;/code&gt;)?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We send the message: first 5 bytes representing the ‚Äúhello‚Äù message&lt;/item&gt;
      &lt;item&gt;Then we send a ‚ÄúNULL terminator‚Äù, just one byte 0 (&lt;code&gt;0000 0000&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;0110 1000 # h
0110 0101 # e
0110 1000 # l
0110 1000 # l
0110 1111 # o
0000 0000 # NULL
&lt;/code&gt;
    &lt;p&gt;Hence, when the receiver gets a NULL terminator, it will print a line feed &lt;code&gt;\n&lt;/code&gt;. Let‚Äôs change the &lt;code&gt;sender.rb&lt;/code&gt; first:&lt;/p&gt;
    &lt;code&gt;receiver_pid = ARGV[0].to_i
message = ARGV[1..-1].join(' ')

def encode_byte(byte)
  8.times.map do |i|
    # Extract each bit from the byte, starting from the LSB
    (byte &amp;gt;&amp;gt; i) &amp;amp; 1
  end
end

message.bytes.each do |byte|
  encode_byte(byte).each do |bit|
    signal = bit == 0 ? 'SIGUSR1' : 'SIGUSR2'
    Process.kill(signal, receiver_pid)
    sleep 0.001 # Delay to allow the receiver to process the signal
  end
end

# Send NULL terminator (0000 0000)
8.times do
  Process.kill('SIGUSR1', receiver_pid)
  sleep 0.001 # Delay to allow the receiver to process the signal
end

puts "Message sent to receiver (PID: #&amp;amp;lbrace;receiver_pid&amp;amp;rbrace;)"
&lt;/code&gt;
    &lt;p&gt;Then, the &lt;code&gt;receiver.rb&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;@position = 0 # start with the LSB
@accumulator = 0

trap('SIGUSR1') &amp;amp;lbrace; decode_signal(0) &amp;amp;rbrace;
trap('SIGUSR2') &amp;amp;lbrace; decode_signal(1) &amp;amp;rbrace;

def decode_signal(bit)
  accumulate_bit(bit)
  return unless @position == 8 # if not yet accumulated a byte, keep accumulating

  if @accumulator.zero? # NULL terminator received
    print "\n"
  else
    print @accumulator.chr # print the byte as a character
  end

  @accumulator = 0 # reset the accumulator
  @position = 0 # reset position for the next byte
end

def accumulate_bit(bit)
  # The left shift operator (&amp;lt;&amp;lt;) is used to 
  # shift the bits of the number to the left.
  #
  # This is equivalent of: (2 ** @position) * bit
  @accumulator += (bit &amp;lt;&amp;lt; @position)
  @position += 1 # move to the next bit position: 0 becomes 1, 1 becomes 2, etc.
end

puts "Process ID: #&amp;amp;lbrace;Process.pid&amp;amp;rbrace;"
sleep
&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;code&gt;$ ruby sender.rb 96875 Hello, World!
$ ruby sender.rb 96875 You're welcome
$ ruby sender.rb 96875 How are you?

# Receiver
Process ID: 97176
Hello, World!
You're welcome
How are you?
&lt;/code&gt;
    &lt;quote&gt;
      &lt;p&gt;OMG Leandro! That‚Äôs amazing!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Amazing, right? We just built an entire communication system between two processes using one of the most primitive methods available: UNIX signals.&lt;/p&gt;
    &lt;p&gt;The sky‚Äôs the limit now! Why not build a full-fledged message broker using this crazy technique?&lt;/p&gt;
    &lt;head rend="h2"&gt;A modest message broker using UNIX signals&lt;/head&gt;
    &lt;p&gt;We‚Äôll break down the development into three components:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Broker: the intermediary that routes messages&lt;/item&gt;
      &lt;item&gt;Consumer: processes that receive messages&lt;/item&gt;
      &lt;item&gt;Producer: processes that send messages&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Let‚Äôs start with the Broker. It should register itself with the producer, then trap incoming signals, decode them, and enqueue the messages for delivery to consumers via outgoing signals:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;#!/usr/bin/env ruby

require_relative 'signal_codec'
require_relative 'consumer'

class Broker 
  PID = 'broker.pid'.freeze

  def initialize
    @codec = SignalCodec.new
    @queue = Queue.new
    @consumer_index = 0
  end

  def start 
    register_broker

    trap('SIGUSR1') &amp;amp;lbrace; process_bit(0) &amp;amp;rbrace;
    trap('SIGUSR2') &amp;amp;lbrace; process_bit(1) &amp;amp;rbrace;
    
    puts "Broker PID: #&amp;amp;lbrace;Process.pid&amp;amp;rbrace;"
    puts "Waiting for messages..."

    distribute_messages

    sleep # Keep alive
  end 

  private

  def process_bit(bit)
    @codec.accumulate_bit(bit) do |message|
      @queue.push(message) unless message.empty?
    end
  end

  def register_broker 
    File.write(PID, Process.pid)
    at_exit &amp;amp;lbrace; File.delete(PID) if File.exist?(PID) &amp;amp;rbrace;
  end

  def distribute_messages
    Thread.new do
      loop do
        sleep 0.1

        next if @queue.empty?

        consumers = File.exist?(Consumer::FILE) ? File.readlines(Consumer::FILE).map(&amp;amp;:to_i) : []
        next if consumers.empty?

        message = @queue.pop(true) rescue next

        consumer_pid = consumers[@consumer_index % consumers.size]
        @consumer_index += 1

        puts "[SEND] #&amp;amp;lbrace;message&amp;amp;rbrace; ‚Üí Consumer #&amp;amp;lbrace;consumer_pid&amp;amp;rbrace;"

        @codec.send_message(message, consumer_pid)
      end
    end
  end
end

if __FILE__ == $0 
  broker = Broker.new
  broker.start
end
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The broker registers itself&lt;/item&gt;
      &lt;item&gt;Traps incoming signals &lt;code&gt;USR1&lt;/code&gt;(bit 0) and&lt;code&gt;USR2&lt;/code&gt;(bit 1)&lt;/item&gt;
      &lt;item&gt;Enqueues the messages&lt;/item&gt;
      &lt;item&gt;Send messages to consumers using outgoing signals (&lt;code&gt;USR1&lt;/code&gt;and&lt;code&gt;USR2&lt;/code&gt;too)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note that we‚Äôre using a module called &lt;code&gt;SignalCodec&lt;/code&gt; which will be explained soon. Basically this module contains all core components to encode/decode signals and perform bitwise operations.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Now the &lt;code&gt;Consumer&lt;/code&gt;implementation:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;#!/usr/bin/env ruby

require_relative 'signal_codec'

class Consumer
  FILE = 'consumers.txt'.freeze

  def initialize
    @codec = SignalCodec.new
  end

  def start
    register_consumer

    trap('SIGUSR1') &amp;amp;lbrace; process_bit(0) &amp;amp;rbrace;
    trap('SIGUSR2') &amp;amp;lbrace; process_bit(1) &amp;amp;rbrace;

    puts "Consumer PID: #&amp;amp;lbrace;Process.pid&amp;amp;rbrace;"
    puts "Waiting for messages..."

    sleep # Keep alive
  end

  private

  def process_bit(bit)
    @codec.accumulate_bit(bit) do |message|
      puts "[RECEIVE] #&amp;amp;lbrace;message&amp;amp;rbrace;"
    end
  end

  def register_consumer
    File.open(FILE, 'a') &amp;amp;lbrace; |f| f.puts Process.pid &amp;amp;rbrace;
    at_exit &amp;amp;lbrace; deregister_consumer &amp;amp;rbrace;
  end

  def deregister_consumer
    if File.exist?(FILE)
      consumers = File.readlines(FILE).map(&amp;amp;:strip).reject &amp;amp;lbrace; |pid| pid.to_i == Process.pid &amp;amp;rbrace;
      File.write(FILE, consumers.join("\n"))
    end
  end
end

if __FILE__ == $0
  consumer = Consumer.new
  consumer.start
end
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The consumer starts and registers itself with the broker&lt;/item&gt;
      &lt;item&gt;Consumer then traps incoming signals (bit 0 and bit 1)&lt;/item&gt;
      &lt;item&gt;Decodes and prints messages&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Last but not least, the &lt;code&gt;Producer&lt;/code&gt;implementation, which is pretty straightforward:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;#!/usr/bin/env ruby

require_relative 'signal_codec'
require_relative 'broker'

unless File.exist?(Broker::PID)
  abort "Error: Broker not running (#&amp;amp;lbrace;Broker::PID&amp;amp;rbrace; not found)"
end

broker_pid = File.read(Broker::PID).strip.to_i
message = ARGV.join(' ')

if message.empty?
  puts "Usage: ruby producer.rb &amp;lt;message&amp;gt;"
  exit 1
end

codec = SignalCodec.new

puts "Sending: #&amp;amp;lbrace;message&amp;amp;rbrace;"
codec.send_message(message, broker_pid)
puts "Message sent to broker (PID: #&amp;amp;lbrace;broker_pid&amp;amp;rbrace;)"
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Producer receives a ASCII message from the STDIN&lt;/item&gt;
      &lt;item&gt;Encode and sends the message to the broker via outgoing signals&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So far, this architecture should look familiar. Many broker implementations follow these basic foundations.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Of course, production-ready implementations are far more robust than this one. Here, we‚Äôre just poking around with hacking and experimentation&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The coolest part is the &lt;code&gt;SignalCodec&lt;/code&gt; though:&lt;/p&gt;
    &lt;code&gt;class SignalCodec 
  SIGNAL_DELAY = 0.001 # Delay between signals to allow processing

  def initialize
    @accumulator = 0
    @position = 0
    @buffer = []
  end

  def accumulate_bit(bit)
    @accumulator += (bit &amp;lt;&amp;lt; @position)
    @position += 1

    if @position == 8 # Byte is complete
      if @accumulator.zero? # Message complete - NULL terminator
        decoded = @buffer.pack("C*").force_encoding('UTF-8')
        yield(decoded) if block_given?
        @buffer.clear
      else 
        @buffer &amp;lt;&amp;lt; @accumulator
      end

      @position = 0
      @accumulator = 0
    end
  end

  def send_message(message, pid)
    message.each_byte do |byte|
      8.times do |i|
        bit = (byte &amp;gt;&amp;gt; i) &amp;amp; 1
        signal = bit == 0 ? 'SIGUSR1' : 'SIGUSR2'
        Process.kill(signal, pid)
        sleep SIGNAL_DELAY
      end
    end

    # Send NULL terminator (0000 0000)
    8.times do
      Process.kill('SIGUSR1', pid)
      sleep SIGNAL_DELAY
    end
  end
end
&lt;/code&gt;
    &lt;p&gt;If you‚Äôve been following along, this shouldn‚Äôt be hard to understand, but I‚Äôll break down how this beautiful piece of code works:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The codec is initialized with the bit position at zero, as well as the accumulator&lt;/item&gt;
      &lt;item&gt;A buffer is also initialized to store accumulated bits until a complete byte is formed&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;accumulate_bit&lt;/code&gt;method should be familiar from our earlier implementation, but it now accepts a closure (block) that lets the caller decide what to do with each decoded byte&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;send_message&lt;/code&gt;encodes a message into bits and sends them via UNIX signals&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Everything in action:&lt;/p&gt;
    &lt;p&gt;How cool, amazing, wonderful, impressive, astonishing is that?&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Yes, we built a message broker using nothing but UNIX signals and a bit of Ruby magic. Sure, it‚Äôs not production-ready, and you definitely shouldn‚Äôt use this in your next startup (please don‚Äôt), but that was never the point.&lt;/p&gt;
    &lt;p&gt;The real takeaway here isn‚Äôt the broker itself: it‚Äôs understanding how the fundamentals work. We explored binary operations, UNIX signals, and IPC in a hands-on way that most people never bother with.&lt;/p&gt;
    &lt;p&gt;We took something ‚Äúuseless‚Äù and made it work, just for fun. So next time someone asks you about message brokers, you can casually mention that you once built (or saw) one using just two signals. And if they look at you weird, well, that‚Äôs their problem. Now go build something equally useless and amazing. The world needs more hackers who experiment just for the fun of it.&lt;/p&gt;
    &lt;p&gt;Happy hacking!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45650178</guid><pubDate>Mon, 20 Oct 2025 22:22:45 +0000</pubDate></item><item><title>Why UUIDs won't protect your secrets</title><link>https://alexsci.com/blog/uuids-and-idor/</link><description>&lt;doc fingerprint="3559569124a767e5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why UUIDs won't protect your secrets&lt;/head&gt;
    &lt;p&gt;This post is part of a collection on UUIDs.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is IDOR?&lt;/head&gt;
    &lt;p&gt;Indirect Object Reference (IDOR) occurs when a resource can be accessed directly by its ID even when the user does not have proper authorization to access it. IDOR is a common mistake when using a separate service for storing files, such as a publicly readable Amazon S3 bucket. The web application may perform access control checks correctly, but the storage service does not.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs vulnerable Django code which allows a user to view their latest billing statement:&lt;/p&gt;
    &lt;code&gt;# Vulnerable!
@login_required
def view_latest_bill(request):
    bill = Bill.objects.filter(owner=request.user).order_by("date").desc()[0]
    url = f'https://example.us-east-1.s3.amazonaws.com/bill-{bill.id}'
    return render(request, 'template.html', { url: url })
&lt;/code&gt;
    &lt;p&gt;While Django ensures the user is logged in and only provides them with bills they own, S3 has no concept of Django users, and performs no such authorization checks.&lt;/p&gt;
    &lt;p&gt;A simple attack would start from a known URL and increment the bill ID:&lt;/p&gt;
    &lt;code&gt;$ curl https://my-bucket.us-east-1.s3.amazonaws.com/bill-100
[ attacker sees their own bill ]
$ curl https://my-bucket.us-east-1.s3.amazonaws.com/bill-101
[ attacker sees another user's bill ]
&lt;/code&gt;
    &lt;p&gt;The attacker can keep trying bill IDs, potentially accessing the entire collection of bills.&lt;/p&gt;
    &lt;head rend="h2"&gt;UUIDs to the rescue?&lt;/head&gt;
    &lt;p&gt;What if we changed the Django model to use UUIDs for the primary key instead of an auto-increment? The new URLs will look like: my-bucket.us-east-1.s3.amazonaws.com/bill-9c742b6a-3401-4f3d-bee7-6f5086c6811f. UUIDs aren‚Äôt guessable, so the attacker can‚Äôt just ‚Äúadd one‚Äù to the URL to access other user‚Äôs files, right?&lt;/p&gt;
    &lt;code&gt; class Bill(models.Model):
-    id = models.AutoField(primary_key=True)
+    id = models.UUIDField(primary_key=True, default=uuid.uuid4)
&lt;/code&gt;
    &lt;p&gt;Unfortunately, this is only a partial fix.&lt;/p&gt;
    &lt;p&gt;Even when URLs are unguessable, that doesn‚Äôt mean an attacker can‚Äôt learn them. A classic example starts with a former employee who used their personal computer for work. Hopefully their user account was quickly disabled, blocking them from accessing the company‚Äôs web application. But sensitive URLs may still exist in their browser history. Even a non-technical attacker can pull off this attack, just by clicking through their browser history. Thankfully, many companies require employees to use company-issued devices when performing work, so this attack may be limited to former employees who violated that rule.&lt;/p&gt;
    &lt;p&gt;The accidental leaking of URLs is probably a more reasonable concern. For example, if only managers are authorized to view bills you need to be careful not to leak the bill ID in other views where other employees have access.&lt;/p&gt;
    &lt;p&gt;If you use secret UUIDs, think of them as toxic assets. They taint anything they touch. If they end up in logs, then logs must be kept secret. If they end up in URLs, then browser history must be kept secret. This is no small challenge.&lt;/p&gt;
    &lt;p&gt;Another concern for leaked UUIDs is rotation. Whenever a secret key is compromised, leaked, or known to have been stored improperly, it should be changed. The same holds true for secret URLs. Make sure you have a way to rotate secret URLs, otherwise you may end up stuck in a compromised state. Again, no small challenge.&lt;/p&gt;
    &lt;p&gt;If this sounds like a huge pain&amp;amp;mldr; it is. Let‚Äôs find a better solution.&lt;/p&gt;
    &lt;head rend="h2"&gt;Properly fixing IDOR&lt;/head&gt;
    &lt;p&gt;The best approach is to ensure every request for sensitive data is authorized.&lt;/p&gt;
    &lt;p&gt;One fix is to route file access through the web application. Continuing our example, the user would access /api/bill/100 and the file would be streamed from the storage through the web app to the user‚Äôs browser. If the user tries to access /api/bill/101, where they lack authorization, the web application can deny the request. Make sure the storage bucket is private, such that access must route via the web app.&lt;/p&gt;
    &lt;p&gt;This approach is a good quick fix, but there are other approaches to consider.&lt;/p&gt;
    &lt;p&gt;If your storage provider is Amazon S3 you should consider pre-signed URLs. These URLs allow the browser to download the file directly from S3, without streaming through the web app. The URL contains a cryptographic signature with a short expiration date. These URLs are still sensitive, but the short expiration mitigates a number of concerns. Again, make sure the storage bucket is private.&lt;/p&gt;
    &lt;p&gt;A key benefit of the pre-signed URL approach is that it offloads file access from your web application, reducing load on the application server.&lt;/p&gt;
    &lt;head rend="h2"&gt;Case study: YouTube unlisted content&lt;/head&gt;
    &lt;p&gt;Let‚Äôs consider a well-known application that doesn‚Äôt follow this advice.&lt;/p&gt;
    &lt;p&gt;YouTube, a popular video hosting service, allows uploaders to mark videos as ‚Äúunlisted‚Äù. This is a compromise between public and private. The owner of the video can copy their video‚Äôs URL and share it out-of-band, like in a private chat room. This way, people in the private chat room can view the video, but the owner doesn‚Äôt need to grant them access one-at-a-time and the viewers don‚Äôt need to log in. In essence, anyone who knows the URL is considered authorized from YouTube‚Äôs perspective.&lt;/p&gt;
    &lt;p&gt;This approach uses unguessable URLs, which contain a random video ID, like &lt;code&gt;ibF36Yyeehw&lt;/code&gt;.
This appears to be 11 random alphanumeric characters, which offer around 64 bits of entropy.
This is suitably unguessable, but the security is questionable.&lt;/p&gt;
    &lt;p&gt;Once the URL is shared with others, the owner loses the ability to assert access control over the video. An authorized viewer can choose to share the URL with others. Users may expect that the video has proper access control restrictions and share the URL in a public-facing document, not realizing that leaking the URL leaks the video.&lt;/p&gt;
    &lt;p&gt;Consider unlistedvideos.com, an index of unlisted YouTube videos. Users who discover unlisted videos can upload those URLs to the site, thus leaking the content to a broad audience. The large number of videos listed on the site shows the poor access control properties afforded by this access control method.&lt;/p&gt;
    &lt;p&gt;If your unlisted content leaks to unauthorized viewers, you can regain control by marking the video as private. This prevents anyone from accessing the video, until you grant their account access. Of course, you probably chose to make the video unlisted to avoid needing to manage individual account access. You could also try re-uploading the video, marking it as unlisted, and sharing the new link, but the risk of a subsequent leak remains.&lt;/p&gt;
    &lt;p&gt;Another example of this design appears later in this blog post, AWS billing estimates. AWS appears to use 160 bits of entropy to protect these URLs. Here‚Äôs the verbiage AWS uses when you create a share link.&lt;/p&gt;
    &lt;p&gt;Interestingly, I‚Äôm not seeing a way to delete a billing estimate once shared. The creator appears to lose all ability to manage access once the link is shared outside their sphere of control. Be very careful not to put sensitive data in your billing estimates.&lt;/p&gt;
    &lt;p&gt;Unlisted content is an example of IDOR as an intentional security design. The uploader is expected to decide if unlisted offers the right security posture for their content. There are use cases where the effort needed to individually grant users access outweighs the risk of using unlisted. Not everyone is dealing in highly sensitive content, after all.&lt;/p&gt;
    &lt;head rend="h2"&gt;Are UUIDs unguessable?&lt;/head&gt;
    &lt;p&gt;OK, maybe you want to create something like YouTube unlisted content, despite these concerns. In that case, we should ignore security concerns related to ‚Äúleaked URLs‚Äù as that is ‚Äúby design‚Äù. Unlisted URLs are sort of like bearer tokens or API tokens which grant access to a single resource. Let‚Äôs focus on attacks that guess URLs and consider how guessable UUIDs actually are.&lt;/p&gt;
    &lt;p&gt;UUIDv4 contains 122 random bits, much more than the 64 bits of a YouTube video ID, so there‚Äôs little to contest about UUIDv4 guessability.&lt;/p&gt;
    &lt;p&gt;But what about newer formats like UUIDv7?&lt;/p&gt;
    &lt;p&gt;UUIDv7 embeds a timestamp at the start such that the IDs generally increase over time. There‚Äôs some claimed benefits, such as improved write performance for certain types of databases.&lt;/p&gt;
    &lt;p&gt;Unfortunately, the timestamp makes UUIDv7s easier to guess. The attacker needs to figure out the timestamp and then brute-force the random bits. Learning the timestamp may not be that difficult: users sometimes have access to metadata for resources they don‚Äôt have full permission to access.&lt;/p&gt;
    &lt;p&gt;In our ‚Äúlatest bill‚Äù example, the bills are probably generated by a batch job kicked off by cron. As such, the bills are likely created one after another in a narrow time period. This is especially true if the attacker has the UUID of their own bill as a reference. An attacker may be able to guess a small window around when the target object‚Äôs UUID was created.&lt;/p&gt;
    &lt;p&gt;Other UUID generation methods recommend creating UUIDs in large batches and then assigning them to resources, in order, as resources are created. With this approach, the UUID timestamp is loosely correlated with the resource creation timestamp, but doesn‚Äôt contain a high precision timestamp for the resource creation. This mitigates some classes of information leakage related to timestamps. Unfortunately, it also bunches UUIDs together very tightly, such that many IDs will share the exact same timestamp. Learning one UUID leaks the timestamp of the entire batch.&lt;/p&gt;
    &lt;p&gt;At first glance, the random bits seem to save us. There are still 74 random bits in a UUIDv7; still more than a YouTube video ID. That‚Äôs 274 possible random suffixes (18,889,465,931,478,580,854,784). Well beyond what an attacker can reasonably brute-force over the Internet.&lt;/p&gt;
    &lt;p&gt;I would end the blog post here, but UUIDv7 offers additional optional methods which we need to consider. The spec allows monotonic counters to be used when multiple UUIDs are created within the same timestamp. This ensures that IDs created by a single node are monotonically increasing, even within a single millisecond. The first UUID in a given timestamp uses a randomized counter value. Subsequent IDs in the same millisecond increment that counter by one.&lt;/p&gt;
    &lt;p&gt;When the counter method is used, an attacker who learns one UUIDv7 can predict the counters of neighboring IDs by adding or subtracting one. A random suffix still exists, and that would still need to be brute-forced.&lt;/p&gt;
    &lt;p&gt;Of note for Django users, Python 3.14 introduced UUIDv7 in the standard library. Python uses a 42-bit counter, which is the maximum width the spec allows. That means Python‚Äôs UUIDv7 only has 32 random bits, offering only 232 possible random suffixes (4,294,967,296).&lt;/p&gt;
    &lt;head rend="h2"&gt;How much security does 232 offer?&lt;/head&gt;
    &lt;p&gt;Four billion seems like a big number, but is it large enough?&lt;/p&gt;
    &lt;p&gt;On average, this is 1,657 request per second averaged over a month. Is that possible?&lt;/p&gt;
    &lt;p&gt;S3 claims it will automatically scale to ‚Äúat least 5,500 GET requests per second‚Äù. On the attacker side, HTTP load testing tools easily scale this high. k6, a popular load testing too, suggests using a single machine unless you need to exceed 100,000 request per second. The attack fits within the systems limits and appears feasible.&lt;/p&gt;
    &lt;p&gt;Adding a rate limiter would force the attacker to distribute their attack, increasing attacker cost and complexity. Cloud providers like Amazon S3 don‚Äôt offer rate limiting controls so you‚Äôll need to consider a WAF. This changes the user-facing URL, so adding a WAF may break old URLs.&lt;/p&gt;
    &lt;p&gt;There‚Äôs cost asymmetry here too. An attacker who guesses 232 S3 URLs will cost your service at least $1,700 on your AWS bill. If you don‚Äôt have monitoring set up, you may not realize you‚Äôre under attack until you get an expensive bill. The attackers cost could be as low as a single machine.&lt;/p&gt;
    &lt;p&gt;I‚Äôm uneasy about the security here, as the attack appears technically feasible. But the attack doesn‚Äôt seem very attractive to an attacker, as they may not be able to target a specific resource.&lt;/p&gt;
    &lt;p&gt;An application that had juicy enough content to be worth attacking in this way would probably worry about ‚ÄúURLs leaking‚Äù. In that case, unlisted URLs are a poor fit for the product and the fixes listed earlier should be used. Which renders the entire point moot as you should never end up here.&lt;/p&gt;
    &lt;p&gt;But it‚Äôs not an entirely theoretical concern. If you search on GitHub, you can find examples of applications that use UUIDv7 IDs and the ‚Äúpublic-read‚Äù ACL. The sensitivity of the data they store and the exact UUIDv7 implementation they use varies.&lt;/p&gt;
    &lt;p&gt;Nevertheless, 32 random bits is too small to be considered unguessable, especially for a cloud service like S3 which lacks rate-limit controls.&lt;/p&gt;
    &lt;head rend="h2"&gt;UUIDv7 for internal-only IDs&lt;/head&gt;
    &lt;p&gt;A common theme of UUIDv7 adoption is to avoid exposing the IDs publicly. One concern driving this trend relates to IDs leaking timing information, which can be sensitive in certain situations.&lt;/p&gt;
    &lt;p&gt;A simple approach uses a random ID, perhaps UUIDv4, as the external ID and UUIDv7 as the database primary key. This can be done using a separate database column and index for the external ID.&lt;/p&gt;
    &lt;p&gt;Another intriguing approach is UUIDv47 which uses SipHash to securely hash the UUIDv7 into a UUIDv4-like ID. SipHash requires a secret key to operate, so you‚Äôll need to manage that key. Unfortunately, rotating the key will invalidate old IDs, which would break external integrations like old URLs. This may prevent systems from changing keys after a key compromise. Caveat emptor.&lt;/p&gt;
    &lt;p&gt;Either of these approaches could help in our ‚Äúunlisted URLs with UUIDv7‚Äù example.&lt;/p&gt;
    &lt;head rend="h2"&gt;Postgres UUIDv7 generation&lt;/head&gt;
    &lt;p&gt;Postgres currently uses the ‚Äúreplace leftmost random bits with increased clock precision‚Äù method when generating UUIDv7 IDs. Postgres converts 12 of the random bits into extra timestamp bits. This means Postgres UUIDv7 timestamps have nanosecond granularity instead of millisecond. As such, Postgres UUIDv7s have 62 random bits, in the current implementation.&lt;/p&gt;
    &lt;p&gt;So when it comes to UUIDv7 guessability, it really depends on what optional methods the implementation chooses.&lt;/p&gt;
    &lt;head rend="h2"&gt;Closing&lt;/head&gt;
    &lt;p&gt;Be careful when adopting newer UUID versions as the properties and trade-offs are distinct from earlier versions.&lt;/p&gt;
    &lt;p&gt;The authors of UUIDv7 knew about these guessability concerns and discuss them in RFC 9562. The spec offers a ‚Äúmonotonic random‚Äù counter method, which increments the counter by a random amount instead of one. While their solution would help mitigate this attack, I wasn‚Äôt able to find an implementation that actually uses it.&lt;/p&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;RFC 9562: Universally Unique IDentifiers (UUIDs) (2024)&lt;/item&gt;
      &lt;item&gt;Python uuid.uuid7&lt;/item&gt;
      &lt;item&gt;100,000,000 S3 requests per day&lt;/item&gt;
      &lt;item&gt;k6 load generator&lt;/item&gt;
      &lt;item&gt;UUIDv47&lt;/item&gt;
      &lt;item&gt;Postgres UUIDv7 generator&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45650617</guid><pubDate>Mon, 20 Oct 2025 23:16:41 +0000</pubDate></item><item><title>Old Computer Challenge ‚Äì Modern Web for the ZX Spectrum</title><link>https://0x00.cl/blog/2025/occ-2025/</link><description>&lt;doc fingerprint="a76899312b36a4a4"&gt;
  &lt;main&gt;
    &lt;p&gt;2025/10/19&lt;/p&gt;
    &lt;p&gt;Table of contents&lt;/p&gt;
    &lt;p&gt;Last year I participated in the OCC challenge and between work and personal life I totally forgot about it, luckily I saw a post online of someone doing the challenge and remembered about it. I didn‚Äôt want to miss this year challenge as it was fun learning something new last year, so hopefully its not too late.&lt;/p&gt;
    &lt;p&gt;This years challenge it was a DIY, you could create your own OCC challenge. So I thought it‚Äôd be fun to try to recreate a website and navigation for the ZX Spectrum using BASIC and the limited graphics capabilities of the computer.&lt;/p&gt;
    &lt;p&gt;Well, in this case the hardware is the ZX Spectrum, though I must say that I used an emulator (Fuse) to test and run my code.&lt;/p&gt;
    &lt;p&gt;The ZX Spectrum image resolution is 256x192 pixels, so the space is very limited. The colour palette is made up of 8 colors, with a ‚Äúbrighter‚Äù variation.&lt;/p&gt;
    &lt;p&gt;With this in mind I had to design a website for the ZX Spectrum and how you‚Äôd navigate with it.&lt;/p&gt;
    &lt;p&gt;So given the hardware and limitations I thought that I‚Äôd first do Google, since they have a simple design and is I‚Äôd say well known by everyone.&lt;/p&gt;
    &lt;p&gt;The image below, is what Googles homepage would look like.&lt;/p&gt;
    &lt;p&gt;You‚Äôd access their services listed by pressing the first letter of that service, in case there is more than 1 with the same letter you‚Äôd use another letter or simply add a second page of services.&lt;/p&gt;
    &lt;p&gt;In this case you can access Search by pressing ‚ÄúS‚Äù. It will simply prompt for your search and in this case I searched for ‚ÄúHow to run LLM on a microprocessor‚Äù and you‚Äôd navigate through the different results by going pressing ‚Äúw‚Äù (up) or ‚Äús‚Äù (down).&lt;/p&gt;
    &lt;p&gt;Hacker news is text heavy content, so it makes it ‚Äúeasy‚Äù to design it for the ZX Spectrum as its graphics capabilities are kinda limited.&lt;/p&gt;
    &lt;p&gt;As you can see, you‚Äôll be able to see your username and the amount of points you have in the top bar and in this case I decided that navigation would be done ‚Äúless‚Äù interactively and you‚Äôll access posts by simply selecting the number (1 to 6). If you want to go to the second page of posts you can press ‚ÄúM‚Äù and if you want to submit you press ‚ÄúS‚Äù.&lt;/p&gt;
    &lt;p&gt;So if we select the first post by pressing 1, we‚Äôll load more details about the post and the comment section.&lt;/p&gt;
    &lt;p&gt;You‚Äôll see the title, user that posted it, time and the comment section with the amount of comments, in this case 42. The problem with the comment section is that some comment threads can go deep and have different branches as multiple users will reply. So in this case what I thought was to simply navigate the top level comments with ‚ÄúM‚Äù for the next comment and ‚ÄúB‚Äù to go back to the previous top level comment. I guess you could add a ‚Äú[R]eply‚Äù option, to reply to the comment you are seeing on screen and navigating through the comment replies, you‚Äôd press something like ‚ÄúC‚Äù and you‚Äôll enter a page of the child comments of the parent comment. The only problem with this is that because the screen is so tiny and limited, that most of the time you‚Äôll only be able to see 1 comment at a time and when posts have 300 comments, navigating them is going to be hard.&lt;/p&gt;
    &lt;p&gt;And this is what submitting a new link would look like, you‚Äôd simply ‚Äúedit‚Äù the different fields and then ‚ÄúS"ubmit&lt;/p&gt;
    &lt;p&gt;Personally I had fun doing this, I didn‚Äôt have to spend a lot of time researching before doing that as I had already done a lot of that work in last year challenge, so I could focus on what I wanted to do. You can check the source code and try it yourself in my repository https://codeberg.org/0x00cl/Web-ZX&lt;/p&gt;
    &lt;p&gt;Here are videos of the interactions in the ZX Spectrum&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45650792</guid><pubDate>Mon, 20 Oct 2025 23:40:25 +0000</pubDate></item><item><title>Wikipedia says traffic is falling due to AI search summaries and social video</title><link>https://techcrunch.com/2025/10/18/wikipedia-says-traffic-is-falling-due-to-ai-search-summaries-and-social-video/</link><description>&lt;doc fingerprint="368b13011882cebe"&gt;
  &lt;main&gt;
    &lt;p&gt;Wikipedia is often described as the last good website on an internet increasingly filled with toxic social media and AI slop. But it seems the online encyclopedia is not completely immune to broader trends, with human page views falling 8% year-over-year, according to a new blog post from Marshall Miller of the Wikimedia Foundation.&lt;/p&gt;
    &lt;p&gt;The foundation works to distinguish between traffic from humans and bots, and Miller writes that the decline ‚Äúover the past few months‚Äù was revealed after an update to Wikipedia‚Äôs bot-detection systems appeared to show that ‚Äúmuch of the unusually high traffic for the period of May and June was coming from bots that were built to evade detection.‚Äù&lt;/p&gt;
    &lt;p&gt;Why is traffic falling? Miller points to ‚Äúthe impact of generative AI and social media on how people seek information,‚Äù particularly as ‚Äúsearch engines are increasingly using generative AI to provide answers directly to searchers rather than linking to sites like ours‚Äù and as ‚Äúyounger generations are seeking information on social video platforms rather than the open web.‚Äù (Google has disputed the claim that AI summaries reduce traffic from search.)&lt;/p&gt;
    &lt;p&gt;Miller says the foundation welcomes ‚Äúnew ways for people to gain knowledge‚Äù and argues this doesn‚Äôt make Wikipedia any less important, since knowledge sourced from the encyclopedia is still reaching people even if they don‚Äôt visit the website. Wikipedia even experimented with AI summaries of its own, though it paused the effort after editors complained.&lt;/p&gt;
    &lt;p&gt;But this shift does present risks, particularly if people are becoming less aware of where their information actually comes from. As Miller puts it, ‚ÄúWith fewer visits to Wikipedia, fewer volunteers may grow and enrich the content, and fewer individual donors may support this work.‚Äù (Some of those volunteers are truly remarkable, reportedly disarming a gunman at a Wikipedia editors‚Äô conference on Friday.)&lt;/p&gt;
    &lt;p&gt;For that reason, he argues that AI, search, and social companies using content from Wikipedia ‚Äúmust encourage more visitors‚Äù to the website itself.&lt;/p&gt;
    &lt;p&gt;And he says Wikipedia is taking steps of its own ‚Äî for example, by developing a new framework for attributing content from the encyclopedia. The organization also has two teams tasked with helping Wikipedia reach new readers, and it‚Äôs looking for volunteers to help.&lt;/p&gt;
    &lt;head rend="h3"&gt;2-FOR-1 DISCOUNT: Bring a +1 and save 60%&lt;/head&gt;
    &lt;head rend="h4"&gt;Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, Vinod Khosla ‚Äî some of the 250+ heavy hitters leading 200+ sessions designed to deliver the insights that fuel startup growth and sharpen your edge. And don‚Äôt miss 300+ showcasing startups in all sectors.&lt;lb/&gt;Bring a +1 and save 60% on their pass, or get your pass by Oct 27 to save up to $444.&lt;/head&gt;
    &lt;head rend="h3"&gt;2-FOR-1 DISCOUNT: Bring a +1 and save 60%&lt;/head&gt;
    &lt;head rend="h4"&gt;Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, Vinod Khosla ‚Äî some of the 250+ heavy hitters leading 200+ sessions designed to deliver the insights that fuel startup growth and sharpen your edge. And don‚Äôt miss 300+ showcasing startups in all sectors. Bring a +1 and save 60% on their pass, or get your pass by Oct 27 to save up to $444.&lt;/head&gt;
    &lt;p&gt;Miller also encourages readers to ‚Äúsupport content integrity and content creation‚Äù more broadly.&lt;/p&gt;
    &lt;p&gt;‚ÄúWhen you search for information online, look for citations and click through to the original source material,‚Äù he writes. ‚ÄúTalk with the people you know about the importance of trusted, human curated knowledge, and help them understand that the content underlying generative AI was created by real people who deserve their support.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45651485</guid><pubDate>Tue, 21 Oct 2025 01:29:03 +0000</pubDate></item><item><title>Argentine peso weakens to fresh low despite US interventions</title><link>https://www.ft.com/content/815ef487-0d0e-430c-b140-9bc39dbd1a53</link><description>&lt;doc fingerprint="9e7b1c715bbadbe9"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;US Politics 2025&lt;/head&gt;&lt;p&gt;Subscribe to unlock this article&lt;/p&gt;&lt;head rend="h1"&gt;&lt;quote&gt;Argentine peso weakens to fresh low despite US interventions&lt;/quote&gt;&lt;/head&gt;&lt;head rend="h2"&gt;Limited time offer. Save 40% on Standard Digital.&lt;/head&gt;was $540 now $319 for your first year&lt;p&gt;Save 40%&lt;/p&gt;&lt;p&gt;Want a deeper look?&lt;/p&gt;Explore our recommended subscriptions&lt;head rend="h2"&gt;Explore more offers.&lt;/head&gt;&lt;head rend="h3"&gt;Trial&lt;/head&gt;&lt;p&gt;$1 for 4 weeks&lt;/p&gt;&lt;p&gt;Then $75 per month. Complete digital access to quality FT journalism on any device. Cancel or change your plan anytime during your trial.&lt;/p&gt;&lt;head rend="h3"&gt;Print + Premium Digital&lt;/head&gt;&lt;p&gt;For the price of premium&lt;/p&gt;&lt;p&gt;was $755 now $719 for your first year&lt;/p&gt;&lt;p&gt;Get Premium &amp;amp; FT Weekend Print edition for the price of Premium. Complete digital access to quality analysis and expert insights, complemented with our award-winning Weekend Print edition.&lt;/p&gt;&lt;p&gt;was $409 now $99 for your first year&lt;/p&gt;&lt;p&gt;FT newspaper delivered Monday-Saturday, plus FT Digital Edition delivered to your device Monday-Saturday.&lt;/p&gt;&lt;p&gt;Check whether you already have access via your university or organisation.&lt;/p&gt;&lt;p&gt;Terms &amp;amp; Conditions apply&lt;/p&gt;&lt;head rend="h2"&gt;Explore our full range of subscriptions.&lt;/head&gt;&lt;head rend="h3"&gt;For individuals&lt;/head&gt;&lt;p&gt;Discover all the plans currently available in your country&lt;/p&gt;&lt;head rend="h3"&gt;For multiple readers&lt;/head&gt;&lt;p&gt;Digital access for organisations. Includes exclusive features and content.&lt;/p&gt;&lt;head rend="h2"&gt;Why the FT?&lt;/head&gt;&lt;p&gt;See why over a million readers pay to read the Financial Times.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45651516</guid><pubDate>Tue, 21 Oct 2025 01:35:30 +0000</pubDate></item></channel></rss>