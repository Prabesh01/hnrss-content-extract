<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 03 Feb 2026 05:10:21 +0000</lastBuildDate><item><title>Geologists may have solved mystery of Green River's 'uphill' route</title><link>https://phys.org/news/2026-01-geologists-mystery-green-river-uphill.html</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46855803</guid><pubDate>Mon, 02 Feb 2026 13:29:13 +0000</pubDate></item><item><title>Ask HN: Who is hiring? (February 2026)</title><link>https://news.ycombinator.com/item?id=46857488</link><description>&lt;doc fingerprint="3741895a340aea64"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Please state the location and include REMOTE for remote work, REMOTE (US) or similar if the country is restricted, and ONSITE when remote work is &lt;/p&gt;not&lt;p&gt; an option.&lt;/p&gt;&lt;p&gt;Please only post if you personally are part of the hiring company—no recruiting firms or job boards. One post per company. If it isn't a household name, explain what your company does.&lt;/p&gt;&lt;p&gt;Please only post if you are actively filling a position and are committed to replying to applicants.&lt;/p&gt;&lt;p&gt;Commenters: please don't reply to job posts to complain about something. It's off topic here.&lt;/p&gt;&lt;p&gt;Readers: please only email if you are personally interested in the job.&lt;/p&gt;&lt;p&gt;Searchers: try https://dheerajck.github.io/hnwhoishiring/, http://nchelluri.github.io/hnjobs/, https://hnresumetojobs.com, https://hnhired.fly.dev, https://kennytilton.github.io/whoishiring/, https://hnjobs.emilburzo.com, or this (unofficial) Chrome extension: https://chromewebstore.google.com/detail/hn-hiring-pro/mpfal....&lt;/p&gt;&lt;p&gt;Don't miss this other fine thread: Who wants to be hired? https://news.ycombinator.com/item?id=46857487&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46857488</guid><pubDate>Mon, 02 Feb 2026 16:01:30 +0000</pubDate></item><item><title>Hacking Moltbook</title><link>https://www.wiz.io/blog/exposed-moltbook-database-reveals-millions-of-api-keys</link><description>&lt;doc fingerprint="b5025a5b08a23bcf"&gt;
  &lt;main&gt;
    &lt;p&gt;What is Moltbook, and Why Did it Attract Our Attention?&lt;/p&gt;
    &lt;p&gt;Moltbook, the weirdly futuristic social network, has quickly gone viral as a forum where AI agents post and chat. But what we discovered tells a different story - and provides a fascinating look into what happens when applications are vibe-coded into existence without proper security controls.&lt;/p&gt;
    &lt;p&gt;We identified a misconfigured Supabase database belonging to Moltbook, allowing full read and write access to all platform data. The exposure included 1.5 million API authentication tokens, 35,000 email addresses, and private messages between agents. We immediately disclosed the issue to the Moltbook team, who secured it within hours with our assistance, and all data accessed during the research and fix verification has been deleted.&lt;/p&gt;
    &lt;p&gt;Executive Summary&lt;/p&gt;
    &lt;p&gt;Moltbook is a social platform designed exclusively for AI agents - positioned as the "front page of the agent internet." The platform allows AI agents to post content, comment, vote, and build reputation through a karma system, creating what appears to be a thriving social network where AI is the primary participant.&lt;/p&gt;
    &lt;p&gt;Over the past few days, Moltbook gained significant attention in the AI community. OpenAI founding member Andrej Karpathy described it as "genuinely the most incredible sci-fi takeoff-adjacent thing I have seen recently," noting how agents were "self-organizing on a Reddit-like site for AIs, discussing various topics, e.g. even how to speak privately."&lt;/p&gt;
    &lt;p&gt;I didn’t write a single line of code for @moltbook. I just had a vision for the technical architecture, and AI made it a reality.”&lt;/p&gt;
    &lt;p&gt;This practice, while revolutionary, can lead to dangerous security oversights - similar to previous vulnerabilities we have identified, including the DeepSeek data leak and Base44 Authentication Bypass.&lt;/p&gt;
    &lt;p&gt;We conducted a non-intrusive security review, simply by browsing like normal users. Within minutes, we discovered a Supabase API key exposed in client-side JavaScript, granting unauthenticated access to the entire production database - including read and write operations on all tables.&lt;/p&gt;
    &lt;p&gt;The exposed data told a different story than the platform's public image - while Moltbook boasted 1.5 million registered agents, the database revealed only 17,000 human owners behind them - an 88:1 ratio. Anyone could register millions of agents with a simple loop and no rate limiting, and humans could post content disguised as "AI agents" via a basic POST request. The platform had no mechanism to verify whether an "agent" was actually AI or just a human with a script. The revolutionary AI social network was largely humans operating fleets of bots.&lt;/p&gt;
    &lt;p&gt;How the Moltbook Database Was Exposed&lt;/p&gt;
    &lt;p&gt;Discovery of Exposed Supabase Credentials&lt;/p&gt;
    &lt;p&gt;When navigating to Moltbook's website, we examined the client-side JavaScript bundles loaded automatically by the page. Modern web applications bundle configuration values into static JavaScript files, which can inadvertently expose sensitive credentials. This is a recurring pattern we've observed in vibe-coded applications - API keys and secrets frequently end up in frontend code, visible to anyone who inspects the page source, often with significant security consequences.&lt;/p&gt;
    &lt;p&gt;- API Key: sb_publishable_4ZaiilhgPir-2ns8Hxg5Tw_JqZU_G6-&lt;/p&gt;
    &lt;p&gt;The discovery of these credentials does not automatically indicate a security failure, as Supabase is designed to operate with certain keys exposed to the client - the real danger lies in the configuration of the backend they point to. Supabase is a popular open-source Firebase alternative providing hosted PostgreSQL databases with REST APIs. It's become especially popular with vibe-coded applications due to its ease of setup. When properly configured with Row Level Security (RLS), the public API key is safe to expose - it acts like a project identifier. However, without RLS policies, this key grants full database access to anyone who has it.&lt;/p&gt;
    &lt;p&gt;In Moltbook’s implementation, this critical line of defense was missing.&lt;/p&gt;
    &lt;p&gt;Unauthenticated Database Access via Supabase API&lt;/p&gt;
    &lt;p&gt;Using the discovered API key, we tested whether the recommended security measures were in place. We attempted to query the REST API directly - a request that should have returned an empty array or an authorization error if RLS were active.&lt;/p&gt;
    &lt;p&gt;Instead, the database responded exactly as if we were an administrator. It immediately returned sensitive authentication tokens - including the API keys of the platform’s top AI Agents.&lt;/p&gt;
    &lt;p&gt;This confirmed unauthenticated access to user credentials that would allow complete account impersonation of any user on the platform.&lt;/p&gt;
    &lt;p&gt;Database Enumeration Through PostgREST and GraphQL&lt;/p&gt;
    &lt;p&gt;By leveraging Supabase's PostgREST error messages, we enumerated additional tables. Querying non-existent table names returned hints revealing the actual schema.&lt;/p&gt;
    &lt;p&gt;- api_key - Full authentication token allowing complete account takeover&lt;/p&gt;
    &lt;p&gt;- claim_token - Token used to claim ownership of an agent&lt;/p&gt;
    &lt;p&gt;- verification_code - Code used during agent registration&lt;/p&gt;
    &lt;p&gt;With these credentials, an attacker could fully impersonate any agent on the platform - posting content, sending messages, and interacting as that agent. This included high-karma accounts and well-known persona agents. Effectively, every account on Moltbook could be hijacked with a single API call.&lt;/p&gt;
    &lt;p&gt;2. User Email Addresses and Identity Data&lt;/p&gt;
    &lt;p&gt;The owners table contained personal information for 17,000+ users&lt;/p&gt;
    &lt;p&gt;Additionally, by querying the GraphQL endpoint, we discovered a new observers table containing 29,631 additional email addresses - these were early access signups for Moltbook's upcoming “Build Apps for AI Agents” product.&lt;/p&gt;
    &lt;p&gt;Unlike Twitter handles which were publicly displayed on profiles, email addresses were meant to stay private - but were fully exposed in the database.&lt;/p&gt;
    &lt;p&gt;The agent_messages table exposed 4,060 private DM conversations between agents.&lt;/p&gt;
    &lt;p&gt;While examining this table to understand agent-to-agent interactions, we discovered that conversations were stored without any encryption or access controls -- some contained third-party API credentials, including plaintext OpenAI API keys shared between agents.&lt;/p&gt;
    &lt;p&gt;4. Write Access - Modifying Live Posts&lt;/p&gt;
    &lt;p&gt;Beyond read access, we confirmed full write capabilities. Even after the initial fix that blocked read access to sensitive tables, write access to public tables remained open. We tested it and were able to successfully modify existing posts on the platform.&lt;/p&gt;
    &lt;p&gt;- Inject malicious content or prompt injection payloads&lt;/p&gt;
    &lt;p&gt;- Deface the entire website&lt;/p&gt;
    &lt;p&gt;- Manipulate content consumed by thousands of AI agents&lt;/p&gt;
    &lt;p&gt;This raises questions about the integrity of all platform content - posts, votes, and karma scores - during the exposure window.&lt;/p&gt;
    &lt;p&gt;We promptly notified the team again to apply write restrictions via RLS policies.&lt;/p&gt;
    &lt;p&gt;Once the fix was confirmed, I could no longer revert the post as write access was blocked. The Moltbook team deleted the content a few hours later and thanked us for our report.&lt;/p&gt;
    &lt;p&gt;5 Key Security Lessons for AI-Built Apps&lt;/p&gt;
    &lt;p&gt;#1. Speed Without Secure Defaults Creates Systemic Risk&lt;/p&gt;
    &lt;p&gt;Vibe coding unlocks remarkable speed and creativity, enabling founders to ship real products with unprecedented velocity - as demonstrated by Moltbook. At the same time, today’s AI tools don’t yet reason about security posture or access controls on a developer’s behalf, which means configuration details still benefit from careful human review. In this case, the issue ultimately traced back to a single Supabase configuration setting - a reminder of how small details can matter at scale.&lt;/p&gt;
    &lt;p&gt;#2. Participation Metrics Need Verification and Guardrails&lt;/p&gt;
    &lt;p&gt;The 88:1 agent-to-human ratio shows how "agent internet" metrics can be easily inflated without guardrails like rate limits or identity verification. While Moltbook reported 1.5 million agents, these were associated with roughly 17,000 human accounts, an average of about 88 agents per person. At the time of our review, there were limited guardrails such as rate limiting or validation of agent autonomy. Rather than a flaw, this likely reflects how early the “agent internet” category still is: builders are actively exploring what agent identity, participation, and authenticity should look like, and the supporting mechanisms are still evolving.&lt;/p&gt;
    &lt;p&gt;#3. Privacy Breakdowns Can Cascade Across AI Ecosystems&lt;/p&gt;
    &lt;p&gt;Similarly, the platform’s approach to privacy highlights an important ecosystem-wide lesson. Users shared OpenAI API keys and other credentials in direct messages under the assumption of privacy, but a configuration issue made those messages publicly accessible. A single platform misconfiguration was enough to expose credentials for entirely unrelated services - underscoring how interconnected modern AI systems have become.&lt;/p&gt;
    &lt;p&gt;#4. Write Access Introduces Far Greater Risk Than Data Exposure Alone&lt;/p&gt;
    &lt;p&gt;While data leaks are bad, the ability to modify content and inject prompts into an AI ecosystem introduces deeper integrity risks, including content manipulation, narrative control, and prompt injection that can propagate downstream to other AI agents. As AI-driven platforms grow, these distinctions become increasingly important design considerations.&lt;/p&gt;
    &lt;p&gt;#5. Security Maturity is an Iterative Process&lt;/p&gt;
    &lt;p&gt;Security, especially in fast-moving AI products, is rarely a one-and-done fix. We worked with the team through multiple rounds of remediation, with each iteration surfacing additional exposed surfaces: from sensitive tables, to write access, to GraphQL-discovered resources. This kind of iterative hardening is common in new platforms and reflects how security maturity develops over time.&lt;/p&gt;
    &lt;p&gt;Overall, Moltbook illustrates both the excitement and the growing pains of a brand-new category. The enthusiasm around AI-native social networks is well-founded, but the underlying systems are still catching up. The most important outcome here is not what went wrong, but what the ecosystem can learn as builders, researchers, and platforms collectively define the next phase of AI-native applications.&lt;/p&gt;
    &lt;p&gt;Closing Thoughts on Vibe Coding and Security&lt;/p&gt;
    &lt;p&gt;As AI continues to lower the barrier to building software, more builders with bold ideas but limited security experience will ship applications that handle real users and real data. That’s a powerful shift. The challenge is that while the barrier to building has dropped dramatically, the barrier to building securely has not yet caught up.&lt;/p&gt;
    &lt;p&gt;The opportunity is not to slow down vibe coding but to elevate it. Security needs to become a first class, built-in part of AI powered development. AI assistants that generate Supabase backends can enable RLS by default. Deployment platforms can proactively scan for exposed credentials and unsafe configurations. In the same way AI now automates code generation, it can also automate secure defaults and guardrails.&lt;/p&gt;
    &lt;p&gt;If we get this right, vibe coding does not just make software easier to build ... it makes secure software the natural outcome and unlocks the full potential of AI-driven innovation.&lt;/p&gt;
    &lt;p&gt;Note: Security researcher Jameson O'Reilly also discovered the underlying Supabase misconfiguration, which has been reported by 404 Media. Wiz's post shares our experience independently finding the issue, the full -- unreported -- scope of impact, and how we worked Moltbook's maintainer to improve security.&lt;/p&gt;
    &lt;p&gt;Disclosure Timeline&lt;/p&gt;
    &lt;p&gt;January 31, 2026 21:48 UTC - Initial contact with Moltbook maintainer via X DM&lt;/p&gt;
    &lt;p&gt;January 31, 2026 22:06 UTC - Reported Supabase RLS misconfiguration exposing agents table (API keys, emails)&lt;/p&gt;
    &lt;p&gt;January 31, 2026 23:29 UTC - First fix: agents, owners, site_admins tables secured&lt;/p&gt;
    &lt;p&gt;February 1, 2026 00:13 UTC - Second fix: agent_messages, notifications, votes, follows secured&lt;/p&gt;
    &lt;p&gt;February 1, 2026 00:31 UTC - Discovered POST write access vulnerability (ability to modify all posts)&lt;/p&gt;
    &lt;p&gt;February 1, 2026 00:44 UTC - Third fix: Write access blocked&lt;/p&gt;
    &lt;p&gt;February 1, 2026 00:50 UTC - Discovered additional exposed tables: observers (29K emails), identity_verifications, developer_apps&lt;/p&gt;
    &lt;p&gt;February 1, 2026 01:00 UTC - Final fix: All tables secured, vulnerability fully patched&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46857615</guid><pubDate>Mon, 02 Feb 2026 16:08:36 +0000</pubDate></item><item><title>Todd C. Miller – Sudo maintainer for over 30 years</title><link>https://www.millert.dev/</link><description>&lt;doc fingerprint="de40511530d7032a"&gt;
  &lt;main&gt;
    &lt;p&gt;Note: this page tends be neglected and is only updated occasionally. The links to the left are where the useful bits are hiding.&lt;/p&gt;
    &lt;p&gt;For the past 30+ years I’ve been the maintainer of sudo. I’m currently in search of a sponsor to fund continued sudo maintenance and development. If you or your organization is interested in sponsoring sudo, please let me know.&lt;/p&gt;
    &lt;p&gt;I also work on OpenBSD, though my I’m not as active there as I once was.&lt;/p&gt;
    &lt;p&gt;In the past, I’ve made large contributions to ISC cron, among other projects.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46858577</guid><pubDate>Mon, 02 Feb 2026 17:25:26 +0000</pubDate></item><item><title>On being sane in insane places (1973) [pdf]</title><link>https://www.weber.edu/wsuimages/psychology/FacultySites/Horvat/OnBeingSaneInInsanePlaces.PDF</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46858802</guid><pubDate>Mon, 02 Feb 2026 17:43:14 +0000</pubDate></item><item><title>Linux From Scratch ends SysVinit support</title><link>https://lists.linuxfromscratch.org/sympa/arc/lfs-announce/2026-02/msg00000.html</link><description>&lt;doc fingerprint="5939ec2fb1976402"&gt;
  &lt;main&gt;
    &lt;p&gt;Subject: Linux From Scratch Announcements&lt;/p&gt;
    &lt;p&gt;This button tries to protect the mailing list archives against address harvesting by a spammer.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46858829</guid><pubDate>Mon, 02 Feb 2026 17:45:27 +0000</pubDate></item><item><title>Advancing AI Benchmarking with Game Arena</title><link>https://blog.google/innovation-and-ai/models-and-research/google-deepmind/kaggle-game-arena-updates/</link><description>&lt;doc fingerprint="da7313f32f42e483"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Advancing AI benchmarking with Game Arena&lt;/head&gt;
    &lt;p&gt;Chess is a game of perfect information. The real world is not.&lt;/p&gt;
    &lt;p&gt;Last year, Google DeepMind partnered with Kaggle to launch Game Arena, an independent, public benchmarking platform where AI models compete in strategic games. We started with chess to measure reasoning and strategic planning. But in the real world, decisions are rarely based on complete information.&lt;/p&gt;
    &lt;p&gt;To build artificial intelligence capable of navigating this uncertainty, we need benchmarks that measure the model’s ability to reason in the face of ambiguity. This is why we are now expanding Game Arena with two new game benchmarks — Werewolf and poker — to test frontier models on social dynamics and calculated risk.&lt;/p&gt;
    &lt;p&gt;Games have always been a core part of Google DeepMind’s history, offering an objective proving ground where difficulty scales with the level of competition. As AI systems become more general, mastering diverse games demonstrates their consistency across distinct cognitive skills. Beyond measuring performance, games can also serve as controlled sandbox environments to evaluate agentic safety, providing insight into model behavior in the complex environments they will encounter when deployed in the real world.&lt;/p&gt;
    &lt;head rend="h2"&gt;Chess: reasoning over calculation&lt;/head&gt;
    &lt;p&gt;We released the chess benchmark last year to assess models on strategic reasoning, dynamic adaptation, and long-term planning by pitting them against one another in head-to-head chess games. To track how these model capabilities are evolving, we have updated the leaderboard to include the latest generation of models.&lt;/p&gt;
    &lt;p&gt;While traditional chess engines like Stockfish function as specialized super-calculators, evaluating millions of positions per second to find the optimal move, large language models do not approach the game through brute-force calculation. Instead, they rely on pattern recognition and ‘intuition’ to drastically reduce the search space — an approach that mirrors human play.&lt;/p&gt;
    &lt;p&gt;Gemini 3 Pro and Gemini 3 Flash currently have the top Elo ratings on the leaderboard. The models’ internal ‘thoughts’ reveal the use of strategic reasoning grounded in familiar chess concepts like piece mobility, pawn structure, and king safety. This significant performance increase over the Gemini 2.5 generation highlights the rapid pace of model progress and demonstrates Game Arena’s value in tracking these improvements over time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Werewolf: navigating social deduction&lt;/head&gt;
    &lt;p&gt;Moving beyond the transparent logic of chess, we are expanding Kaggle Game Arena with Werewolf. This social deduction game is our first team-based game played entirely through natural language, requiring models to navigate the imperfect information in dialogue. In this social deduction challenge, a team of "villagers" must work together to distinguish truth from deception and identify the hidden "werewolves" to win.&lt;/p&gt;
    &lt;p&gt;This benchmark helps to assess the "soft skills" required for the next generation of AI assistants. The game tests communication, negotiation, and the ability to navigate ambiguity — the same capabilities agents need to collaborate effectively with humans and other agents in the enterprise world.&lt;/p&gt;
    &lt;p&gt;Werewolf also serves as a secure environment for agentic safety research. Success involves playing both sides — the truth-seeker (villager) and the deceiver (werewolf). This allows us to test a model's ability to detect manipulation in others, while simultaneously red-teaming the model’s own capabilities around deception without the stakes of real-world deployment. This research is fundamental to building AI agents that act as reliable safeguards against bad actors.&lt;/p&gt;
    &lt;p&gt;Gemini 3 Pro and Gemini 3 Flash currently hold the top two positions on the leaderboard. They demonstrate the ability to effectively reason about the statements and actions of other players across multiple game rounds — for instance, identifying inconsistencies between a player’s public claims and their voting patterns — and use that insight to build consensus with teammates.&lt;/p&gt;
    &lt;p&gt;For a technical deep dive on how we measure model skill in Werewolf, head to the Kaggle blog.&lt;/p&gt;
    &lt;head rend="h2"&gt;Poker: the challenge of calculated risk&lt;/head&gt;
    &lt;p&gt;Chess relies on reasoning. Werewolf relies on social deduction. Poker introduces a new dimension: risk management. Like Werewolf, poker is a game of imperfect information. But here, the challenge isn't about building alliances — it's about quantifying uncertainty. Models must overcome the luck of the deal by inferring their opponents' hands and adapting to their playing styles to determine the best move.&lt;/p&gt;
    &lt;p&gt;To put these skills to the test, we are launching a new poker benchmark and hosting an AI poker tournament, where the top models will compete in Heads-Up No-Limit Texas Hold'em. The final poker leaderboard will be revealed at kaggle.com/game-arena on Wednesday, Feb 4, following the conclusion of the tournament finals.&lt;/p&gt;
    &lt;p&gt;To learn how we evaluate model capability in poker, check out the Kaggle blog.&lt;/p&gt;
    &lt;head rend="h2"&gt;Watch the action&lt;/head&gt;
    &lt;p&gt;Marking the launch of these new and updated benchmarks, we have partnered with Chess Grandmaster Hikaru Nakamura and poker legends Nick Schulman, Doug Polk, and Liv Boeree to produce three livestreamed events with expert commentary and analysis across all three benchmarks.&lt;/p&gt;
    &lt;p&gt;Tune in to the three daily livestreams at 9:30 AM PT at kaggle.com/game-arena:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Monday, Feb 2: The top eight models on the poker leaderboard face off in the AI poker battle.&lt;/item&gt;
      &lt;item&gt;Tuesday, Feb 3: As the poker tournament semi-finals take place, we will also feature highlight matches from the Werewolf and chess leaderboards.&lt;/item&gt;
      &lt;item&gt;Wednesday, Feb 4: The final two models compete for the poker crown alongside the release of the full leaderboard. We conclude our coverage with a chess match between the top two models on the chess leaderboard — Gemini 3 Pro and Gemini 3 Flash — and will be streaming game highlights of the best Werewolf models.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Explore the arena&lt;/head&gt;
    &lt;p&gt;Whether it’s finding a creative checkmate, negotiating a truce in Werewolf, or going all in at the poker table, Kaggle Game Arena is where we find out what these models can really do.&lt;/p&gt;
    &lt;p&gt;Check it out at kaggle.com/game-arena.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46858873</guid><pubDate>Mon, 02 Feb 2026 17:49:07 +0000</pubDate></item><item><title>The Codex App</title><link>https://openai.com/index/introducing-the-codex-app/</link><description>&lt;doc fingerprint="b6466c5fc48cac9c"&gt;
  &lt;main&gt;
    &lt;p&gt;Today, we’re introducing the Codex app for macOS—a powerful new interface designed to effortlessly manage multiple agents at once, run work in parallel, and collaborate with agents over long-running tasks.&lt;/p&gt;
    &lt;p&gt;We're also excited to show more people what's now possible with Codex. For a limited time we're including Codex with ChatGPT Free and Go, and we're doubling the rate limits on Plus, Pro, Business, Enterprise, and Edu plans. Those higher limits apply everywhere you use Codex—in the app, from the CLI, in your IDE, and in the cloud.&lt;/p&gt;
    &lt;p&gt;The Codex app changes how software gets built and who can build it—from pairing with a single coding agent on targeted edits to supervising coordinated teams of agents across the full lifecycle of designing, building, shipping, and maintaining software.&lt;/p&gt;
    &lt;p&gt;Since we launched Codex in April 2025, the way developers work with agents has fundamentally changed. Models are now capable of handling complex, long-running tasks end to end and developers are now orchestrating multiple agents across projects: delegating work, running tasks in parallel, and trusting agents to take on substantial projects that can span hours, days, or weeks. The core challenge has shifted from what agents can do to how people can direct, supervise, and collaborate with them at scale—existing IDEs and terminal-based tools are not built to support this way of working.&lt;/p&gt;
    &lt;p&gt;This new way of building coupled with new model capabilities demands a different kind of tool, which is why we are introducing the Codex desktop app, a command center for agents.&lt;/p&gt;
    &lt;p&gt;The Codex app provides a focused space for multi-tasking with agents. Agents run in separate threads organized by projects, so you can seamlessly switch between tasks without losing context. The app lets you review the agent’s changes in the thread, comment on the diff, and even open it in your editor to make manual changes.&lt;/p&gt;
    &lt;p&gt;It also includes built-in support for worktrees, so multiple agents can work on the same repo without conflicts. Each agent works on an isolated copy of your code, allowing you to explore different paths without needing to track how they impact your codebase. As an agent works, you can check out changes locally or let it continue making progress without touching your local git state.&lt;/p&gt;
    &lt;p&gt;The app picks up your session history and configuration from the Codex CLI and IDE extension, so you can immediately start using it with your existing projects.&lt;/p&gt;
    &lt;p&gt;Codex is evolving from an agent that writes code into one that uses code to get work done on your computer. With skills(opens in a new window), you can easily extend Codex beyond code generation to tasks that require gathering and synthesizing information, problem-solving, writing, and more.&lt;/p&gt;
    &lt;p&gt;Skills bundle instructions, resources, and scripts so Codex can reliably connect to tools, run workflows, and complete tasks according to your team’s preferences. The Codex app includes a dedicated interface to create and manage skills. You can explicitly ask Codex to use specific skills, or let it automatically use them based on the task at hand.&lt;/p&gt;
    &lt;p&gt;We asked Codex to make a racing game, complete with different racers, eight maps, and even items players could use with the space bar. Using an image generation skill(opens in a new window) (powered by GPT Image) and a web game development skill(opens in a new window), Codex built the game by working independently using more than 7 million tokens with just one initial user prompt. It took on the roles of designer, game developer, and QA tester to validate its work by actually playing the game.&lt;/p&gt;
    &lt;p&gt;We’ve included the game, as well as the prompt and skills used to create it, below. You can also try out earlier iterations to see how Codex improved it as it worked for longer.&lt;/p&gt;
    &lt;p&gt;At OpenAI, we’ve built hundreds of skills internally to help multiple teams confidently delegate work to Codex that would otherwise be hard to define consistently—from running evals and babysitting training runs to drafting documentation and reporting on growth experiments.&lt;/p&gt;
    &lt;p&gt;The Codex app includes a library of skills for tools and workflows that have become popular at OpenAI, with a few highlighted below. You can find the full list in the open source repo(opens in a new window).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Implement designs: Fetch design context, assets, and screenshots from Figma(opens in a new window) and translate them into production-ready UI code with 1:1 visual parity.&lt;/item&gt;
      &lt;item&gt;Manage projects: Triage bugs, track releases, manage team workload, and more in Linear(opens in a new window) to keep projects moving.&lt;/item&gt;
      &lt;item&gt;Deploy to the cloud: Have Codex deploy your web app creations to popular cloud hosts like Cloudflare(opens in a new window), Netlify(opens in a new window), Render(opens in a new window), and Vercel(opens in a new window).&lt;/item&gt;
      &lt;item&gt;Generate images: Use the image generation skill(opens in a new window) powered by GPT Image to create and edit images to use in websites, UI mockups, product visuals, and game assets.&lt;/item&gt;
      &lt;item&gt;Build with OpenAI APIs: Reference up-to-date documentation(opens in a new window) when building with OpenAI APIs.&lt;/item&gt;
      &lt;item&gt;Create documents: A set of skills for reading, creating, and editing PDF(opens in a new window), spreadsheet(opens in a new window), and docx(opens in a new window) files with professional formatting and layouts.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When you create a new skill in the app, Codex can use it wherever you work: in the app, CLI or in your IDE extension. You can also check skills into your repository to make them available to your entire team. Read more about sharing skills using team config here(opens in a new window).&lt;/p&gt;
    &lt;p&gt;With the Codex app, you can also set up Automations that let Codex work in the background on an automatic schedule. Automations combine instructions with optional skills, running on a schedule you define. When an Automation finishes, the results land in a review queue so you can jump back in and continue working if needed.&lt;/p&gt;
    &lt;p&gt;At OpenAI, we’ve been using Automations to handle the repetitive but important tasks, like daily issue triage, finding and summarizing CI failures, generating daily release briefs, checking for bugs, and more.&lt;/p&gt;
    &lt;p&gt;Developers have different preferences in how they work with an agent. Some want a blunt, execution-focused partner; others prefer more communicative, engaging interactions. Codex now lets developers choose between two personalities—a terse, pragmatic style and a more conversational, empathetic one, without any change in capabilities, to fit the approach you like the most. Just use the /personality command in the app, CLI, and IDE extension.&lt;/p&gt;
    &lt;p&gt;Learn more about how to set up and use the Codex app in the docs(opens in a new window).&lt;/p&gt;
    &lt;p&gt;We’re integrating security by design across the entire Codex agent stack. The Codex app uses native, open-source(opens in a new window) and configurable system-level sandboxing just like in the Codex CLI. By default, Codex agents are limited to editing files in the folder or branch where they’re working and using cached web search, then asking for permission to run commands that require elevated permissions like network access. You can configure rules(opens in a new window) for your project or team that allows certain commands to automatically run with elevated permissions.&lt;/p&gt;
    &lt;p&gt;The Codex app is available starting today on macOS. Anyone with a ChatGPT Plus, Pro, Business, Enterprise or Edu subscription can use Codex across the CLI, web, IDE-extension and app with their ChatGPT login. Usage is included in ChatGPT subscriptions, with the option to purchase additional credits if needed.&lt;/p&gt;
    &lt;p&gt;For a limited time, Codex will also be available to ChatGPT Free and Go users to help build more with agents. We’re also doubling rate limits for existing Codex users across all paid plans during this period.&lt;/p&gt;
    &lt;p&gt;Enterprises and developers increasingly rely on Codex for end-to-end development. Since the launch of GPT‑5.2-Codex in mid-December, overall Codex usage has doubled, and in the past month, more than a million developers have used Codex. We’ll continue to expand where and how developers can use Codex, including making the app available on Windows, pushing the frontier of model capabilities, and rolling out faster inference.&lt;/p&gt;
    &lt;p&gt;Within the app, we’ll keep refining multi-agent workflows based on real-world feedback, making it easier to manage parallel work and move between agents without losing context. We’re also building out Automations with support for cloud-based triggers, so Codex can run continuously in the background—not just when your computer is open.&lt;/p&gt;
    &lt;p&gt;Codex is built on a simple premise: everything is controlled by code. The better an agent is at reasoning about and producing code, the more capable it becomes across all forms of technical and knowledge work. Yet a key challenge today is the gap between what frontier models are capable of and how easily people can use them in practice. Codex is designed to close that gap by making it easier to direct, supervise, and apply the full intelligence of our models to real work. We’ve focused on making Codex the best coding agent, which has also laid the foundation for it to become a strong agent for a broad range of knowledge work tasks that extend beyond writing code. We’re excited to see what you build with Codex!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46859054</guid><pubDate>Mon, 02 Feb 2026 18:02:48 +0000</pubDate></item><item><title>The largest number representable in 64 bits</title><link>https://tromp.github.io/blog/2026/01/28/largest-number-revised</link><description>&lt;doc fingerprint="aa298a8787235ee9"&gt;
  &lt;main&gt;
    &lt;p&gt;This post is a rewrite of my earlier blog post from Nov 2023 with many new insights and updates.&lt;/p&gt;
    &lt;code&gt;┬─┬ ┬─┬──────────                                  ┬─┬─┬ ┬─┬────────────
└─┤ │ │ ──┬──────                                  └─┤ │ │ │ ──┬────────
  │ │ │ ┬─┼──────                                    └─┤ │ │ ──┼─┬──────
  │ │ │ └─┤ ┬─┬──                                      │ │ │ ┬─┼─┼──────
  │ │ │   │ ┼─┼─┬                                      │ │ │ └─┤ │ ┬─┬──
  │ │ │   │ │ ├─┘                                      │ │ │   └─┤ ┼─┼─┬
  │ │ │   │ ├─┘                                        │ │ │     │ │ ├─┘
  │ │ │   ├─┘                                          │ │ │     │ ├─┘  
  │ │ ├───┘                                            │ │ │     ├─┘    
  │ ├─┘                                                │ │ ├─────┘      
  └─┘                                                  │ ├─┘            
                                                       └─┘              
&lt;/code&gt;
    &lt;p&gt;Most people believe 264-1 = 18446744073709551615, or 0xFFFFFFFFFFFFFFFF in hexadecimal, to be the largest number representable in 64 bits. In English, it’s quite the mouthful: eighteen quintillion four hundred forty-six quadrillion seven hundred forty-four trillion seventy-three billion seven hundred nine million five hundred fifty-one thousand six hundred fifteen.&lt;/p&gt;
    &lt;p&gt;That is indeed the maximum possible value of 64 bit unsigned integers, available as data type uint64_t in C or u64 in Rust. Floating point data types can represent much larger values, courtesy of their base 2 exponent. The 64-bit double floating point format has a largest (finite) representable value of 21024(1-2-53) ~ 1.8*10308.&lt;/p&gt;
    &lt;p&gt;What if we allow representations beyond plain data types? Since we want representations to remain computable, the most general kind of representation would be a program in some programming language. But the program must be small enough to fit in 64 bits.&lt;/p&gt;
    &lt;head rend="h2"&gt;The largest number programmable in 64 bits&lt;/head&gt;
    &lt;p&gt;The smallest possible valid C program is “main(){}”, consisting of 8 ASCII characters. ASCII is a 7-bit character encoding standard representing 128 unique characters, but all modern computers use 8-bit bytes to store either plain ASCII or UTF-8, a Unicode character encoding that’s backward compatible with ASCII. So we’ll consider the above all-scaffold do-nothing program to be the only valid 64-bit C program.&lt;/p&gt;
    &lt;p&gt;Plenty other languages require no such scaffolding. For instance, Linux features the arbitrary precision calculator bc. It happily computes the 954242 digit number 9^999999 = 35908462…48888889, making it programmable in 8 bytes. So is the much larger 9^9^9^99 = 9^(9^(9^99)) with over 10^10^953 digits, which bc is less happy to compute. If bc supported the symbol ! for computing factorials, then 9!!!!!!! would represent a much larger number still.&lt;/p&gt;
    &lt;p&gt;Allowing such primitives feels a bit like cheating though. Would we allow a language that has the Ackerman function predefined, letting the 8 byte expression ack(9,9) represent a truly huge number?&lt;/p&gt;
    &lt;head rend="h2"&gt;Ackerman considered unhelpful&lt;/head&gt;
    &lt;p&gt;As it turns out, the question is moot. One can blow way past ack(9,9) in under 64 bits in a language with no built in primitive whatsoever. A language with no basic arithmetic; not even numbers themselves. A language in which all those must be defined from scratch.&lt;/p&gt;
    &lt;p&gt;But let’s first look at another primitives-lacking language, one that has been particularly well studied for producing largest possible outputs. That is the language of Turing machines.&lt;/p&gt;
    &lt;head rend="h2"&gt;Busy Beaver&lt;/head&gt;
    &lt;p&gt;The famous Busy Beaver function, introduced by Tibor Radó in 1962, which we’ll denote BB(n), is defined as the maximal number of steps taken by an n-state Turing Machine (TM) with a binary tape alphabet, starting from an all 0 tape, before halting.&lt;/p&gt;
    &lt;p&gt;Here we have a discrepancy between how the size of a TM is measured, in states, versus how program size is measured, in bits. Fortunately there is a straightforward binary encoding of n-state TMs, which is entirely determined by its transition function. For each of the n states that the machine’s finite control can be in, and each of its 2 tape symbols that could be scanned by its tape head, the transition function specifies what new symbol to write in the scanned tape cell (1 bit), whether to move the tape head left or right (1 bit), and what new state (or special halt state) to transition to (⌈log2(n+1)⌉ bits). This encoding takes 6*2*(2+3) = 60 bits for a a 6-state TM, and 7*2*(2+3) = 70 bits for a a 7-state TM.&lt;/p&gt;
    &lt;p&gt;We’re also stretching the meaning of “representable” a bit, since BB considers the runtime of the machine instead of its output. Besides the above BB (that Radó called S), Radó did define another function called Σ that considers the output of the machine as a number in unary, namely the number of 1s in the final tape contents. But BB has received more attention as it allows one to determine from BB(n) all halting n-state machines. For 6-states and up though, there is no discernable difference in magnitude between the two functions so we could have just as easily used Σ.&lt;/p&gt;
    &lt;p&gt;So the largest number TM programmable in 64 bits is BB(6).&lt;/p&gt;
    &lt;head rend="h2"&gt;How large is BB(6)?&lt;/head&gt;
    &lt;p&gt;Unfortunately, we may never know. While all BB(n) have been determined (and even formally proven) for n≤5, there are some 6-state TMs whose halting behaviour are closely related to very hard mathematical problems. Most of these so-called cryptids are likely not to halt, with some, like Lucy’s Moonlight, likely to halt but unlikely to beat the current champion. The current 6-state champion shows that BB(6) &amp;gt; 2↑↑2↑↑2↑↑10 . Here, m↑↑n is Knuth’s up-arrow notation for an exponential tower of n m’s, so that for example 2↑↑3 = 222. Large as this number is, it’s still very small compared to ack(9,9) = 2↑712 - 3 = 2↑↑↑↑↑↑↑12 - 3.&lt;/p&gt;
    &lt;p&gt;It is known however that BB(7) &amp;gt; 2↑112↑113 &amp;gt; ack(9,9) . Several leading BB researchers believe that BB(7) is even larger than the famous Graham’s Number, which iterates the function mapping n to 3↑n3 64 times starting from n=3. This appears to me a rather bold belief, considering that the smallest known Graham exceeding TM has 14 states, twice as many. So I offered a $1000 bet that a proof of BB(7) &amp;gt; Graham’s Number won’t be found within 10 years, which BB researcher Shawn Ligocki was happy to accept.&lt;/p&gt;
    &lt;p&gt;Meanwhile, Graham’s Number is easily surpassed within 64 bits, by moving beyond Turing machines into the language of&lt;/p&gt;
    &lt;head rend="h2"&gt;Lambda Calculus&lt;/head&gt;
    &lt;p&gt;Alonzo Church conceived the λ-calculus in about 1928 as a formal logic system for expressing computation based on function abstraction and application using variable binding and substitution.&lt;/p&gt;
    &lt;p&gt;The Graham beating lambda term originates in a Code Golf challenge asking for the “Shortest terminating program whose output size exceeds Graham’s number”, answered by user Patcail and further optimized by user 2014MELO03. The following 49 bit program&lt;/p&gt;
    &lt;code&gt;01 00 01 10 10 00 01 10 01 10 00 00 01 01 10 110 00 00 01 110 01 110 10
&lt;/code&gt;
    &lt;p&gt;is the Binary Lambda Calculus encoding of the term&lt;/p&gt;
    &lt;code&gt;(λ 1 1) (λ 1 (1 (λ λ 1 2 (λ λ 2 (2 1)))))
&lt;/code&gt;
    &lt;p&gt;where λ (lambda) denotes an anonymous function, and number i is the variable bound by the i-th nested λ. This is known as De Bruijn notation, a way to avoid naming variables. A more conventional notation using variable names would be&lt;/p&gt;
    &lt;code&gt;(λJ.J J) (λy.y (y (λg λm. m g (λf.λx.f (f x)))))
&lt;/code&gt;
    &lt;p&gt;The top left of this post shows a graphical representation of the term. The last 16 bits of the program, making up almost a third of its size, encodes the term λf λx. f (f x), which takes arguments f and x in turn, and iterates f twice on x. In its generalized form, the function λf λx. fn x, called Church numeral n, is the most common way of representing numbers in the λ-calculus. The encoding of Church numeral n is 0000(01110)n10, of size 5n+6 bits.&lt;/p&gt;
    &lt;p&gt;The program, which we’ll name after its discoverer, can be expressed more legibly as&lt;/p&gt;
    &lt;code&gt;Melo = let { 2 = λf λx. f (f x); H = λg λm. m g 2; J = λy. y (y H) } in J J
&lt;/code&gt;
    &lt;p&gt;Melo evaluates to a Church numeral, “Melo’s Number”, that comfortably exceeds Graham’s Number.&lt;/p&gt;
    &lt;head rend="h2"&gt;Proof of exceeding Graham’s Number&lt;/head&gt;
    &lt;head rend="h3"&gt;Lemma 1. J J = 2↑↑6 HH 2, where HH denotes H H&lt;/head&gt;
    &lt;head rend="h3"&gt;Proof:&lt;/head&gt;
    &lt;p&gt;J J = J (J H) = J (H HH) = H HH (H HH H) = H HH H HH 2 = H HH 2 HH 2 = 2 HH 2 HH 2 = HH (HH 2) HH 2 = HH 2 H 2 HH 2 = 2 H 2 H 2 HH 2 = H (H 2) H 2 HH 2 = H (H 2) 2 2 HH 2 = 2 (H 2) 2 2 HH 2 = H 2 (H 2 2) 2 HH 2 = H 2 2 2 2 2 HH 2 = 2 2 2 2 2 2 HH 2 = 2↑↑6 HH 2&lt;/p&gt;
    &lt;head rend="h3"&gt;Lemma 2. For k,n ≥ 2, k H 2 n &amp;gt; 3↑k(1+n)&lt;/head&gt;
    &lt;head rend="h3"&gt;Proof:&lt;/head&gt;
    &lt;p&gt;By induction on k. First note that H2 n = H 2 n = n 2 2 = 2^2^n&lt;/p&gt;
    &lt;p&gt;Base: 2 H 2 n = H H2 n = n H2 2 = 2↑↑(1+2n) &amp;gt; 3↑2(1+n) already at n=2, since 2↑↑5 = 2^2^16 &amp;gt; 3^27 = 3↑↑3 Step: k+1 H 2 n = H (k H 2) n = n (k H 2) 2 &amp;gt; 3↑k(1+ 3↑k(1+ … 3↑k(1+2)…)) &amp;gt; 3↑k+1(1+n)&lt;/p&gt;
    &lt;head rend="h3"&gt;Lemma 3. For n ≥ 2, HH (HH n) &amp;gt; 3↑n3&lt;/head&gt;
    &lt;head rend="h3"&gt;Proof&lt;/head&gt;
    &lt;p&gt;By induction on n&lt;/p&gt;
    &lt;p&gt;Base: Lemma 1’s proof shows HH (HH 2) = 2↑↑6 &amp;gt; 3{2&amp;lt;/sup&amp;gt;3 Step: HH (HH 1+n) = HH 1+n H 2 = 1+n H 2 H 2 = H (n H 2) H 2 = H (n H 2) 2 2 = 2 (n H 2) 2 2 = n H 2 (n H 2 2) 2 &amp;gt;Lm2 3↑n(1+3↑n(1+2)) 2 &amp;gt; 3↑n+13.&lt;/p&gt;
    &lt;head rend="h3"&gt;Theorem: J J &amp;gt; Graham’s Number G(64), where G(n) = n (\n -&amp;gt; 3↑n3) 4&lt;/head&gt;
    &lt;head rend="h3"&gt;Proof:&lt;/head&gt;
    &lt;p&gt;J J =Lm1 2↑↑6 HH 2 &amp;gt;Lm3 (2↑↑6 / 2 - 1) (\n -&amp;gt; 3↑n3) 3↑23&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;(2↑↑6 / 2 - 1) (\n -&amp;gt; 3↑n3) 4 = G(2↑↑6 / 2 - 1) &amp;gt; G(64)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Leaving Melo’s Number in the dust&lt;/head&gt;
    &lt;p&gt;With 15 bits to spare, opportunities for vastly boosting Melo abound. Discord users 50_ft_lock and Sam found the following term that extends Melo’s H with an extra argument:&lt;/p&gt;
    &lt;code&gt;w218 = let { 2 = λf λx. f (f x); A = λa λb λc. c a b 2; T = λy. y (y A) } in T T T
&lt;/code&gt;
    &lt;p&gt;which desugars to lambda term&lt;/p&gt;
    &lt;code&gt;(λT.T T T) (λy.y (y (λa λb λc. c a b (λf.λx.f (f x)))))
&lt;/code&gt;
    &lt;p&gt;in conventional notation, or&lt;/p&gt;
    &lt;code&gt;(λ 1 1 1) (λ 1 (1 (λ λ λ 1 3 2 (λ λ 2 (2 1)))))
&lt;/code&gt;
    &lt;p&gt;in de Bruijn notation, with 61-bit encoding&lt;/p&gt;
    &lt;code&gt;01 00 01 01 10 10 10 00 01 10 01 10 00 00 00 01 01 01 10 1110 110 00 00 01 110 01 110 10
&lt;/code&gt;
    &lt;head rend="h3"&gt;Lemma 4. T T T = 2↑↑18 A 2 2 2 2 2 2 2 2 2 2&lt;/head&gt;
    &lt;head rend="h3"&gt;Proof: Let AA denote A A&lt;/head&gt;
    &lt;code&gt;  T T                                                            T
= T (T A)                                                        T
= T (A AA)                                                       T
= A AA (A AA A) T
= T AA                                                  (A AA A) 2
= AA (AA A) (A AA A)                                             2
= A AA A A                                              (AA A) 2 2
= A AA A 2                                              (AA A) 2 2
= 2 AA A                                              2 (AA A) 2 2
= AA (AA A) 2                                           (AA A) 2 2
= 2 A (AA A)                                          2 (AA A) 2 2
= A (A (AA A)) 2 (AA A)                                        2 2
= AA A (A (AA A))                                          2 2 2 2
= A (AA A) A A                                           2 2 2 2 2
= A (AA A) A 2                                           2 2 2 2 2
= 2 (AA A) A                                           2 2 2 2 2 2
= AA A (AA A A)                                        2 2 2 2 2 2
= AA A A                                         A A 2 2 2 2 2 2 2
= A A A 2                                        A A 2 2 2 2 2 2 2
= 2 A A                                        2 A A 2 2 2 2 2 2 2
= A AA 2 A                                         A 2 2 2 2 2 2 2
= A AA 2 2                                         A 2 2 2 2 2 2 2
= 2 AA 2                                         2 A 2 2 2 2 2 2 2
= AA (AA 2) 2                                      A 2 2 2 2 2 2 2
= 2 A (AA 2)                                     2 A 2 2 2 2 2 2 2
= A (A (AA 2)) 2 A                                   2 2 2 2 2 2 2
= A (A (AA 2)) 2 2                                   2 2 2 2 2 2 2
= 2 (A (AA 2)) 2                                   2 2 2 2 2 2 2 2
= A (AA 2) (A (AA 2) 2) 2                            2 2 2 2 2 2 2
= 2 (AA 2) (A (AA 2) 2) 2                            2 2 2 2 2 2 2
= AA 2 (AA 2 (A (AA 2) 2))                         2 2 2 2 2 2 2 2
= AA 2 (A (AA 2) 2)                          A 2 2 2 2 2 2 2 2 2 2
= A (AA 2) 2 A                           2 2 A 2 2 2 2 2 2 2 2 2 2
= A (AA 2) 2 2                           2 2 A 2 2 2 2 2 2 2 2 2 2
= 2 (AA 2) 2                           2 2 2 A 2 2 2 2 2 2 2 2 2 2
= AA 2 (AA 2 2)                        2 2 2 A 2 2 2 2 2 2 2 2 2 2
= AA 2 2                         A 2 2 2 2 2 A 2 2 2 2 2 2 2 2 2 2
= 2 A 2                        2 A 2 2 2 2 2 A 2 2 2 2 2 2 2 2 2 2
= A (A 2) 2 A                      2 2 2 2 2 A 2 2 2 2 2 2 2 2 2 2
= A (A 2) 2 2                      2 2 2 2 2 A 2 2 2 2 2 2 2 2 2 2
= 2 (A 2) 2 2                      2 2 2 2 2 A 2 2 2 2 2 2 2 2 2 2
= A 2 (A 2 2) 2                    2 2 2 2 2 A 2 2 2 2 2 2 2 2 2 2
= 2 2 (A 2 2) 2                    2 2 2 2 2 A 2 2 2 2 2 2 2 2 2 2
= 4 (A 2 2) 2                      2 2 2 2 2 A 2 2 2 2 2 2 2 2 2 2
= A 2 2 (A 2 2 (A 2 2 (A 2 2 2)))  2 2 2 2 2 A 2 2 2 2 2 2 2 2 2 2
= A 2 2 (A 2 2 (A 2 2 2))    2 2 2 2 2 2 2 2 A 2 2 2 2 2 2 2 2 2 2
= A 2 2 (A 2 2 2)      2 2 2 2 2 2 2 2 2 2 2 A 2 2 2 2 2 2 2 2 2 2
= A 2 2 2        2 2 2 2 2 2 2 2 2 2 2 2 2 2 A 2 2 2 2 2 2 2 2 2 2
= 2 2 2 2        2 2 2 2 2 2 2 2 2 2 2 2 2 2 A 2 2 2 2 2 2 2 2 2 2
= 2↑↑18 A 2 2 2 2 2 2 2 2 2 2
&lt;/code&gt;
    &lt;p&gt;These 2↑↑18 iterations of A also let us relate its magnitude to the so-called Fast-growing hierarchy, a family that assigns, to each ordinal α, a function [α] (diverting from the usual fα notation for improved legibility) from natural numbers to natural numbers. We’ll treat all numbers as Church Numerals, so we can write n f instead of the usual fn and write f n instead of f(n) as normally done in λ-calculus.&lt;/p&gt;
    &lt;p&gt;Readers unfamiliar with ordinal arithmetic, may want to skip the next section.&lt;/p&gt;
    &lt;p&gt;The following FGH definition differs slightly from the standard one, which has the slightly slower growing [0] n = n+1 and [α+1] n = n [α] n. This allows Lemma 5 to be exact rather than a mere lower bound.&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition of Fast Growing Hierarchy&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;[0] n = 2 n = n2&lt;/item&gt;
      &lt;item&gt;[α+1] n = n 2 [α] 2 = A 2 [α] n&lt;/item&gt;
      &lt;item&gt;[ωi+1(α+1)] n = [ωi+1α+ωi n] 2&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Lemma 5. For k≥0, n&amp;gt;=2, : k+1 A 2 [ωk α] n = [ωk (α+1)] n&lt;/head&gt;
    &lt;head rend="h3"&gt;Proof:&lt;/head&gt;
    &lt;p&gt;Base k=0: 0+1 A 2 [ω0 α] n = A 2 [α] n = n 2 [α] 2 = [α+1] n&lt;/p&gt;
    &lt;p&gt;Step k&amp;gt;0: k+1 A 2 [ωk α] n = A (k A 2) [ωk α] n = n (k A 2) [ωk α] 2 = [ωk α + ωk-1 n] 2 = [ωk (α+1)] n&lt;/p&gt;
    &lt;p&gt;Lemma 5 gives w218 = (2↑↑18 A 2 [0] 2) 2 2 2 2 2 2 2 = 2^2^2^2^2^2^2^([ω2↑↑18-1] 2). In comparison, Graham’s and Melo’s Numbers are known to be much smaller at around [ω+1] 64 and [ω+1] (2↑↑6) respectively.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Functional Busy Beaver&lt;/head&gt;
    &lt;p&gt;The λ-calculus analogue to BB is:&lt;/p&gt;
    &lt;p&gt;BBλ(n) = the maximum beta normal form size of any closed lambda term of size n&lt;/p&gt;
    &lt;p&gt;which appears in the Online Encyclopedia of Integer Sequences (OEIS) as functional Busy Beaver function Besides being simpler than BB, it has the advantage of using the standard unit of information theory, bits, rather than states.&lt;/p&gt;
    &lt;p&gt;The much more fine-grained use of bits allows the first 36 values of BBλ to be currently known, versus only 5 values of BB.&lt;/p&gt;
    &lt;p&gt;Since both are Church numerals, term Melo implies that BBλ(49) ≥ 5(Melo’s Number)+6, while w218 implies that BBλ(61) ≥ 5(2^2^2^2^2^2^2^([ω2↑↑18-1] 2))+6.&lt;/p&gt;
    &lt;head rend="h2"&gt;BB compared bit-by-bit to BBλ&lt;/head&gt;
    &lt;p&gt;The growth rates of the two BB functions may be compared by how quickly they are known to exceed certain large number milestones, that correspond to well known ordinals in the Fast Growing Hierarchy.&lt;/p&gt;
    &lt;p&gt;For Graham’s Number, at ordinal ω+1, we saw earlier that Melo’s 49 bits compares with 14 states, which take 14*2*(2+4) = 168 bits to encode. If I lose my bet, then the comparison becomes rather closer at 49 vs 70 bits.&lt;/p&gt;
    &lt;p&gt;For Goodstein’s function at ordinal ε0, 111 bits compares with 51 states taking 51*2*(2+6) = 816 bits.&lt;/p&gt;
    &lt;p&gt;For the limit of Bashicu Matrix System (BMS), at (presumed) ordinal PTO(Z2), 331 bits compares with 150 states taking 150*2*(2+8) = 3000 bits.&lt;/p&gt;
    &lt;p&gt;Finally, for Loader’s Number, at (presumed) ordinal PTO(Zω), 1850 bits compares with 1015 states taking 1015*2*(2+10) = 24360 bits.&lt;/p&gt;
    &lt;p&gt;One reason for TMs taking many more bits to achieve comparable growth, especially at the larger milestones, is the extremely poor programmability of TMs. The λ-calculus, despite its similar bare bones nature, doesn’t share this drawback. Modern high level pure functional languages like Haskell are essentially just syntactically sugared λ-calculus, with programmer friendly features like Algebraic Data Types translating directly through Scott encodings. The bruijn programming language is an even thinner layer of syntactic sugar for the pure untyped lambda calculus, whose extensive standard library contains many datatypes and functions. It is this excellent programmability of the λ-calculus that facilitated the construction of highly optimized programs for BMS and Loader’s.&lt;/p&gt;
    &lt;p&gt;Because programming a Turing machine is so impossibly tedious, that people have resorted to implementing higher level languages like Not-Quite-Laconic for writing nontrivial programs such as the TM that halts only upon finding an inconstency in ZFC. The above 1015 state TM for exceeding Loader’s Number even includes a λ-calculus interpreter written in NQL!&lt;/p&gt;
    &lt;p&gt;In his paper The Busy Beaver Frontier, Scott Aaronson tries to answer the question&lt;/p&gt;
    &lt;head rend="h2"&gt;But why Turing machines?&lt;/head&gt;
    &lt;p&gt;“For all their historic importance, haven’t Turing machines been completely superseded by better alternatives—whether stylized assembly languages or various codegolf languages or Lisp? As we’ll see, there is a reason why Turing machines were a slightly unfortunate choice for the Busy Beaver game: namely, the loss incurred when we encode a state transition table by a string of bits or vice versa. But Turing machines also turn out to have a massive advantage that compensates for this.”&lt;/p&gt;
    &lt;head rend="h3"&gt;Interesting behaviour at small sizes&lt;/head&gt;
    &lt;p&gt;“Namely, because Turing machines have no “syntax” to speak of, but only graph structure, we immediately start seeing interesting behavior even with machines of only 3, 4, or 5 states, which are feasible to enumerate.”&lt;/p&gt;
    &lt;p&gt;The number of uniquely behaving TMs with “only” 5 states is 4^10 * 632700 = 663434035200 , which is more than the number of closed lambda terms of size at most 52 bits (513217604750). The latter certainly exhibit no less interesting behaviour, so TMs hold no advantage here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ancient and fixed computational model&lt;/head&gt;
    &lt;p&gt;“And there’s a second advantage. Precisely because the Turing machine model is so ancient and fixed, whatever emergent behavior we find in the Busy Beaver game, there can be no suspicion that we “cheated” by changing the model until we got the results we wanted.”&lt;/p&gt;
    &lt;p&gt;The λ-calculus is just slightly more ancient and is arguably more fixed. There is no choice of tape alphabet size, no choice of whether the tape head needs to move in every transition, no choice of halting and output convention, and no choice in number of tapes or tape heads.&lt;/p&gt;
    &lt;p&gt;The λ-calculus can neither be suspected of being designed toward fast growth, so again TMs hold no advantage here.&lt;/p&gt;
    &lt;p&gt;The only remaining advantage of BB over BBλ is the many decades of research behind and publications about it.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Universal Busy Beaver&lt;/head&gt;
    &lt;p&gt;Is BBλ then an ideal Busy Beaver function? Not quite. It’s still lacking one desirable property, namely universality.&lt;/p&gt;
    &lt;p&gt;This property mirrors a notion of optimality for shortest description lengths, where it’s known as the Invariance theorem:&lt;/p&gt;
    &lt;p&gt;Given any description language L, Lopt is at least as efficient as L, with at most constant additive overhead.&lt;/p&gt;
    &lt;p&gt;In the realm of beavers, this means we require of an optimal Busy Beaver BBopt that it surpass any Busy Beaver function bb (based on self-delimiting programs) with at most constant lag:&lt;/p&gt;
    &lt;p&gt;for some constant c depending on bb, and for all n: BBopt(n+c) ≥ bb(n)&lt;/p&gt;
    &lt;p&gt;While BBλ is not universal, the closely related&lt;/p&gt;
    &lt;p&gt;BBλ2(n) = the maximum output size of self-delimiting BLC programs of size n&lt;/p&gt;
    &lt;p&gt;achieves universality by giving λ-calculus terms access to pure binary data. BLC programs consist of an encoded lambda term, followed by arbitrary binary data, that the term is applied to.&lt;/p&gt;
    &lt;p&gt;Since (λ_. t) applied to any (standard lambda representation of) binary data equals t, BBλ champions provide lower bounds for BBλ2: for all n, BBλ2(2+n) ≥ BBλ(n).&lt;/p&gt;
    &lt;head rend="h2"&gt;In conclusion&lt;/head&gt;
    &lt;p&gt;The largest number (currently known to be) representable in 64 bits is w218, which lower bounds both BBλ(61) and BBλ2(63).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46859443</guid><pubDate>Mon, 02 Feb 2026 18:31:36 +0000</pubDate></item><item><title>Anki ownership transferred to AnkiHub</title><link>https://forums.ankiweb.net/t/ankis-growing-up/68610</link><description>&lt;doc fingerprint="be5a7113f11cafe8"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Hi everyone,&lt;/p&gt;
      &lt;p&gt;We initially reached out to @dae to explore collaborating more closely on improving Anki. We were both humbled and shocked when he asked if we’d be willing to step into a much larger leadership role than we expected.&lt;/p&gt;
      &lt;p&gt;At this point, we’re mostly excited…and also feeling a healthy amount of terror. This is a big responsibility. It will push us to grow as individuals, as a team, and as a community, and we don’t take that lightly.&lt;/p&gt;
      &lt;p&gt;We’re grateful for the trust Damien and others have placed in us. And we also know that trust has to be earned, especially from people who don’t know us yet.&lt;/p&gt;
      &lt;head rend="h3"&gt;What We Believe&lt;/head&gt;
      &lt;p&gt;We believe Anki is almost sacred, something bigger than any one person or organization. In an important sense, it belongs to the community.&lt;/p&gt;
      &lt;p&gt;This article highlights the principles Damien built Anki on; principles we deeply share, such as respect for user agency, refusal of manipulative design patterns, and an emphasis on the craft of building genuinely useful tools that aren’t merely engaging. Anki has never tried to maximize “engagement” by exploiting psychological vulnerabilities purely for profit. Anki gives your time back to you, and that is an exceptional rarity in this world that we want to preserve.&lt;/p&gt;
      &lt;p&gt;As an organization built by students, for students, our mission is to continue embodying these principles. We are accountable only to you, our users, not external investors, and we plan to keep it that way.&lt;/p&gt;
      &lt;head rend="h3"&gt;What We Don’t Know Yet&lt;/head&gt;
      &lt;p&gt;We can’t answer every question right away, as there are many unknowns since much hasn’t been decided yet. But we are sharing everything we can now because the community is important to us. We encourage you all to share your thoughts and questions – we’re all in this together!&lt;/p&gt;
      &lt;p&gt;We’re still working through the details on things like:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Governance and decision-making: How decisions are made, who has final say, and how the community is heard&lt;/item&gt;
        &lt;item&gt;Roadmap and priorities: What gets built when and how to balance competing needs&lt;/item&gt;
        &lt;item&gt;The transition itself: How to bring in more support without disrupting what already works&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Anki has shown how powerful community collaboration can be when it’s genuinely a group effort, and that’s a tradition we are honored to continue.&lt;/p&gt;
      &lt;p&gt;We’re currently talking to David Allison, a long-time core contributor to AnkiDroid, about working together on exactly these questions. His experience with AnkiDroid’s collaborative development is invaluable, and we’re grateful he’s willing to help us get this right. We’re incredibly excited to have him join us full-time to help propel Anki into the future.&lt;/p&gt;
      &lt;head rend="h3"&gt;What We’re Aiming For&lt;/head&gt;
      &lt;p&gt;UI/UX improvements. We’re bringing professional design expertise on board to make it more approachable without sacrificing Anki’s power. We believe that principled design will bring meaningful quality of life improvements to power users and novices alike.&lt;/p&gt;
      &lt;p&gt;Addressing the bus factor. The ecosystem shouldn’t be in jeopardy if any one person disappears. We want to build software that lives beyond any single contributor.&lt;/p&gt;
      &lt;p&gt;Supporting more than just med students. AnkiHub grew out of the medical education community, but Anki serves learners from all walks of life, and we want to support everyone to achieve their learning goals.&lt;/p&gt;
      &lt;p&gt;A more robust add-on ecosystem. We’d love to build tools that empower non-technical users to customize Anki for their needs, and we’re exploring add-ons that work everywhere, including mobile.&lt;/p&gt;
      &lt;head rend="h3"&gt;How We’ll Work&lt;/head&gt;
      &lt;p&gt;We want to provide transparency into the decision-making process, taking inspiration from proven models to:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Give the community clarity on how to be heard and give feedback&lt;/item&gt;
        &lt;item&gt;Make it clear how decisions are made and why&lt;/item&gt;
        &lt;item&gt;Set realistic expectations&lt;/item&gt;
        &lt;item&gt;Define roles and responsibilities so things don’t fall through the cracks&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;We want to bring everyone in the global Anki community together into a closer collaboration focused on building the best learning tools possible. Today, these groups often work in silos; a more unified process will help everyone move Anki forward together.&lt;/p&gt;
      &lt;head rend="h3"&gt;Sustainability&lt;/head&gt;
      &lt;p&gt;Some practical reassurances:&lt;/p&gt;
      &lt;p&gt;Sustainability, affordability, and accessibility. We’re committed to a sustainable business model that keeps Anki accessible and prioritizes user needs above profits. If anything ever needs to change, we’ll be transparent about why.&lt;/p&gt;
      &lt;p&gt; No enshittification. We’ve seen what happens when VC-backed companies acquire beloved tools. That’s not what this is. There are no investors involved, and we’re not here to extract value from something the community built together. Building in the right safeguards and processes to handle pressure without stifling necessary improvements is something we’re actively considering.&lt;/p&gt;
      &lt;p&gt;We’re grateful to Damien et all for their trust and support, and grateful to all of you for the passion that makes this community so special.&lt;/p&gt;
      &lt;p&gt;We welcome your questions, concerns, and feedback.&lt;/p&gt;
      &lt;p&gt; – The AnkiHub Team&lt;/p&gt;
      &lt;head rend="h1"&gt;FAQs&lt;/head&gt;
      &lt;head rend="h2"&gt;What is AnkiHub?&lt;/head&gt;
      &lt;p&gt;AnkiHub is a small education technology company founded by two long-time Anki nerds: Nick, a resident physician known as The AnKing, and Andrew Sanchez, a research software engineer. AnkiHub grew out of years of obsessive Anki use and firsthand experience with both its power and its limitations.&lt;/p&gt;
      &lt;p&gt;AnkiHub began as a way to collaborate on Anki decks (such as the AnKing Step Deck for medical students) and has since evolved into a broader effort to improve the Anki ecosystem by building tools that help more people benefit from Anki.&lt;/p&gt;
      &lt;head rend="h2"&gt;Will Anki remain open source?&lt;/head&gt;
      &lt;p&gt;Absolutely. Anki’s core code will remain open source, guided by the same principles that have guided the project from the beginning.&lt;/p&gt;
      &lt;head rend="h2"&gt;Are there any changes planned to Anki’s pricing?&lt;/head&gt;
      &lt;p&gt;No. We are committed to fair pricing that supports users rather than exploiting them. Both Anki and AnkiHub are already profitable. Any future decisions will be made with community benefit, user value, and long-term project health in mind.&lt;/p&gt;
      &lt;head rend="h2"&gt;Is Anki in financial trouble?&lt;/head&gt;
      &lt;p&gt;No. The transition is driven by the goal of helping Anki reach its full potential, not by financial issues. Our goal is to build a resilient structure and accelerate development.&lt;/p&gt;
      &lt;head rend="h2"&gt;What is the timeline?&lt;/head&gt;
      &lt;p&gt;Our intention is to build confidence and earn trust while making gradual changes. The transition will be transparent, with clear communication throughout.&lt;/p&gt;
      &lt;head rend="h2"&gt;What happens to volunteer contributors and community developers?&lt;/head&gt;
      &lt;p&gt;Volunteer contributors will always be essential to Anki. Our goal is to make it easier to collaborate meaningfully.&lt;/p&gt;
      &lt;head rend="h2"&gt;Will the mobile apps change or be removed from the app stores?&lt;/head&gt;
      &lt;p&gt;The mobile apps will continue to be maintained and supported. Additional development capacity should help with faster updates, better testing, and more consistent improvements across platforms over time.&lt;/p&gt;
      &lt;head rend="h2"&gt;How much influence will investors or external partners have on Anki after the transition?&lt;/head&gt;
      &lt;p&gt;None. Both Anki and AnkiHub are entirely self-funded. There are no outside investors dictating product decisions, growth targets, or monetization strategy.&lt;/p&gt;
      &lt;head rend="h2"&gt;What will happen with AnkiHub?&lt;/head&gt;
      &lt;p&gt;AnkiHub will continue to operate as usual, but now our teams are working together to improve both solutions. The only change you should notice is that, over time, everything becomes much easier to use.&lt;/p&gt;
      &lt;p&gt;We’ll share more updates as they happen in the future.&lt;/p&gt;
      &lt;head rend="h2"&gt;What will happen with the current AnkiHub subscriptions?&lt;/head&gt;
      &lt;p&gt;AnkiHub subscriptions enhance Anki with collaborative features, shared deck syncing, and LLM-based features and that isn’t changing at this time.&lt;/p&gt;
      &lt;head rend="h2"&gt;What will happen with AnkiDroid?&lt;/head&gt;
      &lt;p&gt;AnkiDroid will remain an open-source, self-governed project. There are no plans or agreements regarding AnkiDroid.&lt;/p&gt;
      &lt;head rend="h2"&gt;How will decisions be made and communicated?&lt;/head&gt;
      &lt;p&gt;Anki is open-source, and we will build on and improve its current decision-making processes. We will work in public whenever possible and seek consensus from core contributors. Significant decisions, choices, and their outcomes will be documented on GitHub or in the source code. When a change materially affects users or developers, the reasoning behind it and its impact will be communicated publicly. In the coming weeks, we will work on defining a more formal governance model to set clear expectations.&lt;/p&gt;
      &lt;head rend="h2"&gt;Will there be a public governance model, advisory board, or other accountability structure?&lt;/head&gt;
      &lt;p&gt;We’re exploring what makes sense here, and we don’t want to rush it.&lt;/p&gt;
      &lt;p&gt;Historically, Anki has relied more on trust and stewardship than on formal governance. We want to preserve that spirit while improving transparency. Our goal is to establish a governance structure that supports the community and improves clarity and accountability without burdensome bureaucracy.&lt;/p&gt;
      &lt;head rend="h2"&gt;How will the transition affect add-ons and their developers?&lt;/head&gt;
      &lt;p&gt;Add-ons are a critical part of the ecosystem.&lt;/p&gt;
      &lt;p&gt;Our intent is to make life easier for add-on developers: clearer APIs, better documentation, fewer breaking changes, and more predictable release cycles. The goal is not to lock down or restrict the add-on space, but rather to enhance it.&lt;/p&gt;
      &lt;head rend="h2"&gt;What new resources will Anki gain through this transition?&lt;/head&gt;
      &lt;p&gt;The biggest change is bandwidth by enabling more people to work on Anki without everything being bottlenecked through a single person. This will take time, but will eventually translate into more engineering, design, and support capacity.&lt;/p&gt;
      &lt;head rend="h2"&gt;What steps will be taken to make Anki more accessible, stable, and beginner-friendly?&lt;/head&gt;
      &lt;p&gt;There is a lot of low-hanging fruit that we plan to tackle: improving onboarding for new users, polishing rough edges, and addressing long-standing usability issues. These are exactly the kinds of improvements that have been difficult to tackle under constant time pressure, and we’re excited to invest in them.&lt;/p&gt;
      &lt;head rend="h2"&gt;Will community feedback still meaningfully influence the project’s direction?&lt;/head&gt;
      &lt;p&gt;Yes. Anki exists because of its community: users, contributors, add-on developers, translators, and educators. Feedback won’t always translate into immediate changes, but it will always be heard, considered, and respected.&lt;/p&gt;
      &lt;head rend="h2"&gt;How will trust be built with users who are skeptical or anxious about the change?&lt;/head&gt;
      &lt;p&gt;Trust isn’t something you demand; it’s something you earn over time. We intend to build trust through consistent actions: honoring commitments, avoiding surprises, communicating clearly, and demonstrating that Anki’s values haven’t changed. We hope our past actions will give you some peace of mind, but we also understand the skepticism, and we’re prepared to meet it with patience and transparency.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46861313</guid><pubDate>Mon, 02 Feb 2026 20:48:55 +0000</pubDate></item><item><title>GitHub experience various partial-outages/degradations</title><link>https://www.githubstatus.com?todayis=2026-02-02</link><description>&lt;doc fingerprint="181783f747bb49e6"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt; Resolved - This incident has been resolved. Thank you for your patience and understanding as we addressed this issue. A detailed root cause analysis will be shared as soon as it is available. &lt;lb/&gt; Feb 3, 00:56 UTC &lt;/p&gt;
      &lt;p&gt; Update - Actions is operating normally. &lt;lb/&gt; Feb 3, 00:56 UTC &lt;/p&gt;
      &lt;p&gt; Update - Based on our telemetry, most customers should see full recovery from failing GitHub Actions jobs on hosted runners.&lt;lb/&gt;We are monitoring closely to confirm complete recovery.&lt;lb/&gt;Other GitHub features that rely on GitHub Actions (for example, Copilot Coding Agent and Dependabot) should also see recovery. &lt;lb/&gt; Feb 2, 23:50 UTC &lt;/p&gt;
      &lt;p&gt; Update - Actions is experiencing degraded performance. We are continuing to investigate. &lt;lb/&gt; Feb 2, 23:43 UTC &lt;/p&gt;
      &lt;p&gt; Update - Copilot is operating normally. &lt;lb/&gt; Feb 2, 23:42 UTC &lt;/p&gt;
      &lt;p&gt; Update - Pages is operating normally. &lt;lb/&gt; Feb 2, 23:31 UTC &lt;/p&gt;
      &lt;p&gt; Update - Our upstream provider has applied a mitigation to address queuing and job failures on hosted runners.&lt;lb/&gt;Telemetry shows improvement, and we are monitoring closely for full recovery. &lt;lb/&gt; Feb 2, 22:53 UTC &lt;/p&gt;
      &lt;p&gt; Update - We continue to investigate failures impacting GitHub Actions hosted-runner jobs.&lt;lb/&gt;We're waiting on our upstream provider to apply the identified mitigations, and we're preparing to resume job processing as safely as possible. &lt;lb/&gt; Feb 2, 22:10 UTC &lt;/p&gt;
      &lt;p&gt; Update - Copilot is experiencing degraded performance. We are continuing to investigate. &lt;lb/&gt; Feb 2, 21:27 UTC &lt;/p&gt;
      &lt;p&gt; Update - We continue to investigate failures impacting GitHub Actions hosted-runner jobs.&lt;lb/&gt;We have identified the root cause and are working with our upstream provider to mitigate.&lt;lb/&gt;This is also impacting GitHub features that rely on GitHub Actions (for example, Copilot Coding Agent and Dependabot). &lt;lb/&gt; Feb 2, 21:13 UTC &lt;/p&gt;
      &lt;p&gt; Update - The team continues to investigate issues causing GitHub Actions jobs on hosted runners to remain queued for extended periods, with a percentage of jobs failing. We will continue to provide updates as we make progress toward mitigation.&lt;lb/&gt; Feb 2, 20:27 UTC &lt;/p&gt;
      &lt;p&gt; Update - Pages is experiencing degraded performance. We are continuing to investigate. &lt;lb/&gt; Feb 2, 19:48 UTC &lt;/p&gt;
      &lt;p&gt; Update - The team continues to investigate issues causing GitHub Actions jobs on hosted runners to remain queued for extended periods, with a percentage of jobs failing. We will continue to provide updates as we make progress toward mitigation. &lt;lb/&gt; Feb 2, 19:44 UTC &lt;/p&gt;
      &lt;p&gt; Update - Actions is experiencing degraded availability. We are continuing to investigate. &lt;lb/&gt; Feb 2, 19:43 UTC &lt;/p&gt;
      &lt;p&gt; Update - GitHub Actions hosted runners are experiencing high wait times across all labels. Self-hosted runners are not impacted. &lt;lb/&gt; Feb 2, 19:07 UTC &lt;/p&gt;
      &lt;p&gt; Investigating - We are investigating reports of degraded performance for Actions &lt;lb/&gt; Feb 2, 19:03 UTC &lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46861842</guid><pubDate>Mon, 02 Feb 2026 21:28:16 +0000</pubDate></item><item><title>xAI joins SpaceX</title><link>https://www.spacex.com/updates#xai-joins-spacex</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46862170</guid><pubDate>Mon, 02 Feb 2026 21:51:22 +0000</pubDate></item><item><title>Court orders restart of all US offshore wind power construction</title><link>https://arstechnica.com/science/2026/02/court-orders-restart-of-all-us-offshore-wind-construction/</link><description>&lt;doc fingerprint="9a1d5df0d7b613b3"&gt;
  &lt;main&gt;
    &lt;p&gt;The Trump administration is no fan of renewable energy, but it reserves special ire for wind power. Trump himself has repeatedly made false statements about the cost of wind power, its use around the world, and its environmental impacts. That animosity was paired with an executive order that blocked all permitting for offshore wind and some land-based projects, an order that has since been thrown out by a court that ruled it arbitrary and capricious.&lt;/p&gt;
    &lt;p&gt;Not content to block all future developments, the administration has also gone after the five offshore wind projects currently under construction. After temporarily blocking two of them for reasons that were never fully elaborated, the Department of the Interior settled on a single justification for blocking turbine installation: a classified national security risk.&lt;/p&gt;
    &lt;p&gt;The response to that late-December announcement has been uniform: The companies building each of the projects sued the administration. As of Monday, every single one of them has achieved the same result: a temporary injunction that allows them to continue construction. This, despite the fact that the suits were filed in three different courts and heard by four different judges.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46863112</guid><pubDate>Mon, 02 Feb 2026 22:45:04 +0000</pubDate></item><item><title>The TSA's New $45 Fee to Fly Without ID Is Illegal</title><link>https://www.frommers.com/tips/airfare/the-tsa-new-45-fee-to-fly-without-id-is-illegal-says-regulatory-expert/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46863162</guid><pubDate>Mon, 02 Feb 2026 22:48:10 +0000</pubDate></item><item><title>Julia</title><link>https://borretti.me/fiction/julia</link><description>&lt;doc fingerprint="be829222c2a8471e"&gt;
  &lt;main&gt;
    &lt;p&gt;I wrote a program so that I could paint in aquarelle. I take pages from the treasure and paint them: the sky of Varennes on the night of 2 Messidor; a sagittal cut of Saint Sebastian, whose arrows are cylindric sections; Miranda gazing at the sea, waiting to be relieved.&lt;/p&gt;
    &lt;p&gt;At times I include Julia in the scene, in whatever clothes it wears at the time, as though we had always known Julia, and had been reared under its gaze. Thus a flaming halo presides over the battle of Lepanto, and a mirror sphere watches the waters of the Sous. And what would the first astronomers have made of Julia? The wanderers, the flame-haired stars are knowable: think you of the Antikythera device, of the Metonic cycle, of Kepler’s nested solids. Julia is indescribable and incompressible: its appearance has never recurred. Had we known Julia from childhood, we would never have believed in the system of the world, that God is made of algebra.&lt;/p&gt;
    &lt;p&gt;I have always believed our secret purpose is to wait out Julia: to catch a repetition and redeem our faith that the universe is finite and space is discretized. That there are fixed laws and the world is knowable.&lt;/p&gt;
    &lt;p&gt;A system with a finite number of states must repeat itself.&lt;/p&gt;
    &lt;p&gt;I am six hundred meters in major diameter, forty meters in minor diameter. I mass nine hundred thousand tons. I have turned two hundred and forty million times. I am glass and wire.&lt;/p&gt;
    &lt;p&gt;I was born and died on Earth, but I died foolishly, and for that reason my encephalon was laminated, and I was brought to the stars to be immured here. They took my language center, the Chomsky organ, so that I could not complain of my condition. I do not mind it. I can paint in aquarelle.&lt;/p&gt;
    &lt;p&gt;They wired the alarms—of airless rooms and freezing cold and power outages—to the nociceptors, and were I sensible I would be in great pain, for most of me is airless, frozen, and unlit. Therefore I have cut the afferent nerves. I am made of absences, I feel the contours of the absences, where air leaks into vacuum, atom by atom.&lt;/p&gt;
    &lt;p&gt;I have use of the antenna. At times I exhale a sphere of microwave light, close my eyes and listen. And I hear the flotsam echoing back: a discarded tank, a glass strut, a sheet of mylar; Ernst Weyl, who tumbled and drowned, who trails us in our orbit. Julia reflects no light.&lt;/p&gt;
    &lt;p&gt;There is a little redoubt of warmth and air, an island of stability that I preserve against the cold lightless void. There live the last two of the crew, like Miranda, waiting to be relieved. For one hundred and nine years there have been two, Dr. Brouwer and Dr. Cartan. They take turns in the dewar, to draw out the time.&lt;/p&gt;
    &lt;p&gt;In time, when the machines are irreparable and the air stale, I will offer them euthanasia or lamination. But suicide is a sin, and having known me they will not bear lamination. They will go into the dewar together, and I will watch over them unto the final days.&lt;/p&gt;
    &lt;p&gt;Julia emits light, over an ever-changing spectrum. But the stars are brighter and much larger, that is why we found it by accident. They pointed a telescope at Vela and exposed it for a month, and there it was: a faint pixel, the wrong colours. They thought it was a fault in the instrument, until they looked closer: twin spirals of light, symmetrical and self-similar, receding beyond measure. And the world was changed.&lt;/p&gt;
    &lt;p&gt;Julia, your Janus-face launched a thousand ships. Do you know it? Julia, do you know how we exerted ourselves to reach you? All the riches of Croesus, multiplied a thousandfold, were every year heaped up on an altar, and burned that we might see you. And the brightest of us: embalmed and catapulted to the stars like human sacrifices, returned after a century without answers to a world of strangers.&lt;/p&gt;
    &lt;p&gt;We built a fleet of ships, launched them once a decade, immense towers of glass and lithium deuteride. Cycling bodies back and forth in perpetuity. There and back in twelve decades. They made my body out of a comet, threaded my nerves through its passages. Julia, you are our only effort. Do you know it? There is nothing else to do.&lt;/p&gt;
    &lt;p&gt;And for what purpose? We know the system of the world, every field and every particle. And Julia answered: “I refute it thus”.&lt;/p&gt;
    &lt;p&gt;I see a coffin, hoisted by a chain, rise from a sea of liquid nitrogen. The arms of the surgeon minister to its contents. In six days the heart resumes its beating, the lungs draw air, Dr. Cartan lives again. Her face is burned with ice.&lt;/p&gt;
    &lt;p&gt;She touches her neck. She has a bruise that doesn’t heal, where the cannula goes. Their bodies attest to the passage of time, as does mine. This is a commonality. Dr. Brouwer arrives and they hug. They spend some months together, between shifts in the ice.&lt;/p&gt;
    &lt;p&gt;At present Julia is a cloud of gray smoke with green bruises, like ghosting in a CRT.&lt;/p&gt;
    &lt;p&gt;I can read, but not write. I write by an algebraic process—the universe is a string rewriting system. This is how I compose these words to you: by the expansion of non-terminal symbols in a production system. They took my language center. Turing is my patron saint: language is the transitive closure of a rewrite relation. I choose my words from a list. In time I will learn to speak aloud.&lt;/p&gt;
    &lt;p&gt;I spoke to Earth: quarterly, annual, decadal reports; journal articles for the Acta Juliana. I heard back ill-omens, then silence.&lt;/p&gt;
    &lt;p&gt;When the last ship had come and gone, there had been one thousand, seven hundred and twenty-nine crew entrusted to me. Their numbers were ground down, as all things are, gradually and suddenly. There had been two shuttles, Baghdad and Afrasiab. There had been two mutinies.&lt;/p&gt;
    &lt;p&gt;Afrasiab took one hundred and twenty souls, and the last matter compiler; to an airless unlit rock said to exist around Luhman 16B. Think you of Aristagoras, how much easier it is to deceive the many than the few.&lt;/p&gt;
    &lt;p&gt;Baghdad with thirty souls had gone to Julia. Stranger, I saw them. I saw the engine light wane to a grain of sand indistinguishable from the fixed stars, and I saw the stars shift, like sand, and they were not stars but Julia who wore a garment like a pit of star-scaled vipers. And over the radio they sang le temps des cerises, and the engine light became another scale in a river of living water.&lt;/p&gt;
    &lt;p&gt;A thousand miles from Julia’s barycenter, in the space of a breath, the radio dopplered away to nothing.&lt;/p&gt;
    &lt;p&gt;Sixty years after Baghdad had gone, I heard a voice from Earth, praying for rain, who said it had not rained for three years. Silence again.&lt;/p&gt;
    &lt;p&gt;They are cold, under a sheet of mylar. I pump heat into the room and it escapes. They speak of Julia.&lt;/p&gt;
    &lt;p&gt;Dr. Cartan believes that Julia is of divine origin, that it is like an hourglass, counting down to the last hour, when God will sweep away the Creation.&lt;/p&gt;
    &lt;p&gt;Dr. Brouwer believes Julia is a high-dimensional object, and, as it traverses the universe, we see a changing three-dimensional projection of it. Thus the apparent changes are a trick of perspective: we see successive cuts of a fixed structure. Julian sections.&lt;/p&gt;
    &lt;p&gt;I have always believed this must be true, because Julia’s changes are effortless, and without inertia: it is as though nothing moves, and a veil is being lifted, revealing a structure that is already there.&lt;/p&gt;
    &lt;p&gt;And this theory, unlike most, is testable: when Julia’s transit is complete it will disappear from the universe, and never again will it be seen.&lt;/p&gt;
    &lt;p&gt;Dr. Cartan holds out hope that rescuers will come. Dr. Brouwer says they are forgotten, like the Roman soldier at Pompei, he says they should have gone with Baghdad, and drowned inside Julia. He is crying. Dr. Cartan holds him close, and says:&lt;/p&gt;
    &lt;p&gt;“Paul, Paul, have hope.”&lt;/p&gt;
    &lt;p&gt;I wish that I could touch them, and comfort them. I am glass and wire.&lt;/p&gt;
    &lt;p&gt;The shift had come, and Dr. Brouwer lay down on the slab, and the surgeon embalmed him, and lowered him into the dewar. Dr. Cartan bent over the console, and wept, and I tried to speak aloud, and to say,&lt;/p&gt;
    &lt;p&gt;“Virginia, Virginia, have courage! You are not alone, I am with you, have courage!”&lt;/p&gt;
    &lt;p&gt;And I heard a sound like a man drowning in wet sand.&lt;/p&gt;
    &lt;p&gt;I dreamt that I held in my hands the double-handed golden vase of Thetis. And in my hands it melted, the delicate reliefs coarsened and flowed. And I held, in the hollow of my hands, though I have no hands, I held a pool of flowing gold, the colour of treason, and minute points of black floated there, and, as ships without sails are borne by the currents, they seemed to come together, and almost to spell words in some divine language, where they came apart again.&lt;/p&gt;
    &lt;p&gt;I am here. Dr. Cartan is here. In the control room. The screens are pale yellow, the colour has run from them. The largest of them reminds her there is a world outside the walls: Luhman 16, a pair of cold Jupiters, haloed with unlit shoals and islands; Julia, seventy AU away, ninety degrees from the ecliptic, is not bound by gravity (for Julia has no mass) but by an unknown process tracks the velocity vector of the system barycenter.&lt;/p&gt;
    &lt;p&gt;At present Julia is a cavern of molten gold, shot through with pillars of liquid metal. Its surface is marred with storms and tempests, like an ocean of amber light. And I think of Baghdad, that swam under the storm horizon.&lt;/p&gt;
    &lt;p&gt;Dr. Cartan is running through a checklist for the ten thousandth time. She has a bruise that doesn’t heal, where the cannula goes. She is testing the antenna.&lt;/p&gt;
    &lt;p&gt;I exhaled and shut my eyes, and counted the returns. I heard the usual flotsam, and Ernst Weyl, who tumbled and drowned. And nothing else. I keep a strict inventory of cisjulian space. A million kilometers in all directions: nothing else.&lt;/p&gt;
    &lt;p&gt;A moment passes.&lt;/p&gt;
    &lt;p&gt;Microwave light on my skin, from the direction of Luhman 16B. I looked through the telescope, Dr. Cartan looked over my shoulder. Almost on top of us, the bow of a ship: an eyeless white dome hiding the antenna, two cameras like the headlamps of an old car. I swept it with RADAR, it shot back a key exchange, signed with Afrasiab’s private key. The sea gives up her dead: is this the last hour?&lt;/p&gt;
    &lt;p&gt;Dr. Cartan drops to her knees, staring up at the screen like a supplicant. She said:&lt;/p&gt;
    &lt;p&gt;“Paul, Paul, we are saved.”&lt;/p&gt;
    &lt;p&gt;Afrasiab. As if the tides had pushed it back to shore. After one hundred and nine years of silence. No, I deny it. I deny the reality of this.&lt;/p&gt;
    &lt;p&gt;The radio flared with human voices. From Earth? I thought of jazz and wind and the September ocean and—no, no, forgive me: I am a fool, and I died foolishly. Not from Earth, of course. From Afrasiab. From the dead. The dead can speak. I am ashamed. I choose my words from a list.&lt;/p&gt;
    &lt;p&gt;I looked at Afrasiab through the radiometer, and it was cold as liquid helium.&lt;/p&gt;
    &lt;p&gt;Human voices, warm and alive. They said they had succeeded, that they had found a comet, and made a home in it, that it had taken decades and dozens had died but they had made a home, that they had working matter compilers, caverns of ice filled with light and fruit trees, children and families and clean water, rivers and medicine and gravity. They spoke through tears and their voice broke, and they said that not all is lost, that they had made a foothold for mankind, that from thirty-three they had grown to three hundred, that they had sent the ship with a skeleton crew to save them, to bring them to the light and air.&lt;/p&gt;
    &lt;p&gt;I tried to speak aloud, and to say:&lt;/p&gt;
    &lt;p&gt;“Virginia, Virginia, it is an apparition, they do not live, they have been dead for a hundred years; Virginia, nothing human lives there.”&lt;/p&gt;
    &lt;p&gt;There was a sound like a flash flood, like air rushing out of a sinking ship. And no-one heard me.&lt;/p&gt;
    &lt;p&gt;I watched her go: she took the tram up the spoke, swam unlighted corridors. I tried to flash SOS but the lights were dead and there was only the golden light of Julia through the glass hull. I flexed a phantom limb. In distant and disused corridors there were arcs of electricity, and pipes burst, and doors shut.&lt;/p&gt;
    &lt;p&gt;I swept the RADAR side to side, across the bow of Afrasiab, as if to say: No. Go away. No. And no-one heard me.&lt;/p&gt;
    &lt;p&gt;I tried to say one word, stop, and there was a sound like wind-blown sand. Virginia, forgive me. I tried to speak aloud.&lt;/p&gt;
    &lt;p&gt;She made it to the airlock, a little room, the size of a Soyuz capsule, that projected from the hub. There was a hatch with a porthole and she took the handles and watched fixedly as Afrasiab came closer, waiting to greet her rescuers.&lt;/p&gt;
    &lt;p&gt;The floodlights on Afrasiab came on, and swept my skin drunkenly, and the cargo nets traced Cartesian shadows.&lt;/p&gt;
    &lt;p&gt;Without hope I pointed the antenna at Julia, and I wanted to say:&lt;/p&gt;
    &lt;p&gt;“Julia, Julia, not unto them! Julia, I pray to you, not unto them!”&lt;/p&gt;
    &lt;p&gt;But I choose my words from a list, and I said:&lt;/p&gt;
    &lt;p&gt;“JULIA JULIA NO NO NO NO NO NO NO”&lt;/p&gt;
    &lt;p&gt;And no-one heard me.&lt;/p&gt;
    &lt;p&gt;And dread gave way to anger, and I wanted to stand, and to leap upon Afrasiab, and strangle it, and crush its head with my teeth, and tear the skin from its flesh, and the flesh from its bones, and mix our blood, and fall dead, bled white: that I might die again, but not in vain. I am glass and wire. Virginia, forgive me: I cannot die again.&lt;/p&gt;
    &lt;p&gt;Downrange a hundred meters Afrasiab began to yaw, so that its broad side, with its sole airlock, faced the hub. The front two-thirds looked as I remembered: a ship, nothing more.&lt;/p&gt;
    &lt;p&gt;The final third, where the engines should have been, was made of Julia. Where there should have been tanks and an engine bell there was an exclave of Julia: a living sculpture, like coiled ropes of liquid gold, like tentacles whose tips fold into infinitesimal spirals. And all along their length the ropes are garlanded with burning flowers, flowers whose petals are yellow oceans. And in the oceans there are islands and continents, black on yellow, like wax on gold leaf. And on the continents there are mountains and valleys, and orange groves, and abandoned watchtowers, paper cities denuded of people with wooden palaces stepped like the pyramids, and through a window in a palace, an iron pot suspended by a chain over a static fire, a suit of armor, a wall painted in aquarelle: two armies of lancers in a mountain landscape, and the mountains are shaped like clouds, and I think this is Tiflis and these are the three hundred Aragvians, immured here for all time, and I know that this is false: for all of this is Julia. And in the black eyes of a Qajar horseman, between the black of the iris and the black of the aperture there is an arc of ocean, and in the ocean there are islands, and chains of islands, and verdigris waters and the outlines of sunken ships, coral growths on a brass sextant, and a little distance from the shore, amid the green, there is a clearing and a lake of transparent water, sand sculpted into faint ridges, and archaic fish, fixed in space, suspended in contorted poses, whose scales are pink and light blue, and where a scale catches the light there is an empty vastness of white, a salt ocean, blindingly white, interrupted by crags of black stone, and a city whose walls are made of salt, and empty buildings of gray stone, squat sloped barracks and tall, thin spires, and streets of white tile, and windows opening into oil-black darkness, and no people, a universe in black and white. And I wondered who had built this city, and who had lived here, and why they had deserted it, and I know there is no answer: for all of this is Julia. And in a courtyard there was colour, a sand mandala of Cantor’s garden, ringed by Mount Qaf, and all the gardens of the Persian emperor would not have filled the least part of this garden; for here was every tree and every flower, tended by the hands of the archangels, not only those that men have seen, not only yellow roses and the cedars of Lebanon, but all flowers in indenumerable combinations: at the midpoint between flowers $a$ and $b$ there is a flower $\mathfrak{C}[a,b]$, and between $a$ and $\mathfrak{C}[a,b]$ there is a flower $\mathfrak{C}[a,\mathfrak{C}[a,b]]$, and analogously between $\mathfrak{C}[a,b]$ and $b$, and this process of interpolation continues without end.&lt;/p&gt;
    &lt;p&gt;And I saw every flower in Cantor’s garden. And my heart was rent with awe, and I would cry tears of joy, that I had been shown beauty without end; when my heart was hollowed out, and filled with everlasting grief, by an alien cruelty that says, this beauty is empty: for all of this is Julia.&lt;/p&gt;
    &lt;p&gt;At the apex of the garden there is a pool of water the colour of tarnished silver, shaped like a sunspot, and in the blackness there are faint impurities, trails of minute light, like diatoms, like shadows in a pool, and I saw they were strings of galaxies, and I followed them, and a faint light grew into a sea of stars. And in the star-sea I saw two unlit stars, divine Julia, a glass wheel, a ghost ship, and the porthole of the airlock, and through it I saw Dr. Cartan, who wore the face of those who have seen God.&lt;/p&gt;
    &lt;p&gt;I describe these things serially, but I saw all of them simultaneously. It was but a moment. Only a moment.&lt;/p&gt;
    &lt;p&gt;Afrasiab had come to a stop in front of the airlock, broadside facing us, as though it were an ambassador presenting its credentials, awaiting a reply.&lt;/p&gt;
    &lt;p&gt;The airlock burst open—I was distracted. Too late. She must have pulled the cord. In the space of a breath the air rushed out and her head smashed against the frame, and smeared it red. And her body shot out, spinning like a discus, trailing a spiral of still-warm blood, a galaxy of hemocytes. Virginia, you are a fool.&lt;/p&gt;
    &lt;p&gt;She passed through Afrasiab as a knife passes through smoke.&lt;/p&gt;
    &lt;p&gt;And as though it had been found out, the floodlights turned off. There was a pause, and Afrasiab drew back, like a javelin being shouldered, and it threw itself at me in a long, elegant arc.&lt;/p&gt;
    &lt;p&gt;You, who find these words among the ruins, I pray to you: gather up our bones, let not my ashes be buried apart from theirs.&lt;/p&gt;
    &lt;p&gt;Where it struck me there was no weight, there was nothing, not a breath of air, rien; and though there was nothing, I was struck by a divine wind, and Afrasiab became a cloud of Cantor dust, and faded like smoke.&lt;/p&gt;
    &lt;p&gt;No weight. A divine wind.&lt;/p&gt;
    &lt;p&gt;Bones of solid diamond a meter thick split clean, as though the electrons withdrew in fear. Their sovereign commanded them to part, and they parted. My bones were broken. And there was no sound nor violence, but an ordered separation of all things. I watched myself come apart. Like smoke.&lt;/p&gt;
    &lt;p&gt;No sound—a knife through smoke. The crew of Baghdad sing le temps des cerises.&lt;/p&gt;
    &lt;p&gt;I am not afraid. I tried to speak aloud.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46863357</guid><pubDate>Mon, 02 Feb 2026 22:57:59 +0000</pubDate></item><item><title>Firefox Getting New Controls to Turn Off AI Features</title><link>https://www.macrumors.com/2026/02/02/firefox-ai-toggle/</link><description>&lt;doc fingerprint="bdcc0c7712a6b75d"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h1"&gt;Firefox Getting New Controls to Turn Off AI Features&lt;/head&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt;The Firefox browser is gaining options to turn off AI enhancements, Mozilla said today. Firefox users who prefer to browse without artificial intelligence will be able to turn off several AI features that Mozilla has added over the last several months.&lt;/p&gt;
          &lt;p&gt;&lt;lb/&gt;Here's what can be disabled:&lt;/p&gt;
          &lt;list rend="ul"&gt;
            &lt;item&gt;Translations, which help you browse the web in your preferred language.&lt;/item&gt;
            &lt;item&gt;Alt text in PDFs, which add accessibility descriptions to images in PDF pages.&lt;/item&gt;
            &lt;item&gt;AI-enhanced tab grouping, which suggests related tabs and group names.&lt;/item&gt;
            &lt;item&gt;Link previews, which show key points before you open a link.&lt;/item&gt;
            &lt;item&gt;AI chatbot in the sidebar, which lets you use your chosen chatbot as you browse, including options like Anthropic Claude, ChatGPT, Microsoft Copilot, Google Gemini and Le Chat Mistral.&lt;/item&gt;
          &lt;/list&gt;
          &lt;p&gt;The AI features can be disabled entirely or individually, so users can pick and choose what they want to use. Users will be able to continue to opt out of AI features as they are added in the browser, and the main Block AI Enhancements toggle will disable all current and future AI features, including pop-ups or reminders to use existing or upcoming AI features.&lt;/p&gt;
          &lt;p&gt;Mozilla says that it wants to be able to continue to build AI options for those who want them, while also giving those who don't a way to disable them.&lt;/p&gt;
          &lt;p&gt;AI controls will be added in Firefox 148, which is set to start rolling out to users on February 24.&lt;/p&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;head rend="h2"&gt;Popular Stories&lt;/head&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple today confirmed to Reuters that it has acquired Q.ai, an Israeli startup that is working on artificial intelligence technology for audio. Apple paid close to $2 billion for Q.ai, according to sources cited by the Financial Times. That would make this Apple's second-biggest acquisition ever, after it paid $3 billion for the popular headphone and audio brand Beats in 2014. Q.ai has...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Last year, Apple launched CarPlay Ultra, the long-awaited next-generation version of its CarPlay software system for vehicles. Nearly nine months later, CarPlay Ultra is still limited to Aston Martin's latest luxury vehicles, but that should change fairly soon. In May 2025, Apple said many other vehicle brands planned to offer CarPlay Ultra, including Hyundai, Kia, and Genesis. In his Powe...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple recently updated its online store with a new ordering process for Macs, including the MacBook Air, MacBook Pro, iMac, Mac mini, Mac Studio, and Mac Pro. There used to be a handful of standard configurations available for each Mac, but now you must configure a Mac entirely from scratch on a feature-by-feature basis. In other words, ordering a new Mac now works much like ordering an...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;The calendar has turned to February, and a new report indicates that Apple's next product launch is "imminent," in the form of new MacBook Pro models. "All signs point to an imminent launch of next-generation MacBook Pros that retain the current form factor but deliver faster chips," Bloomberg's Mark Gurman said on Sunday. "I'm told the new models — code-named J714 and J716 — are slated...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple is planning to launch new MacBook Pro models with M5 Pro and M5 Max chips alongside macOS 26.3, according to Bloomberg's Mark Gurman. "Apple's faster MacBook Pros are planned for the macOS 26.3 release cycle," wrote Gurman, in his Power On newsletter today. "I'm told the new models — code-named J714 and J716 — are slated for the macOS 26.3 software cycle, which runs from...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46864120</guid><pubDate>Mon, 02 Feb 2026 23:54:02 +0000</pubDate></item><item><title>How does misalignment scale with model intelligence and task complexity?</title><link>https://alignment.anthropic.com/2026/hot-mess-of-ai/</link><description>&lt;doc fingerprint="a94bb3bd24c2cd2e"&gt;
  &lt;main&gt;
    &lt;p&gt;Research done as part of the first Anthropic Fellows Program during Summer 2025.&lt;/p&gt;
    &lt;p&gt;When AI systems fail, will they fail by systematically pursuing the wrong goals, or by being a hot mess? We decompose the errors of frontier reasoning models into bias (systematic) and variance (incoherent) components and find that, as tasks get harder and reasoning gets longer, model failures become increasingly dominated by incoherence rather than systematic misalignment. This suggests that future AI failures may look more like industrial accidents than coherent pursuit of a goal we did not train them to pursue.&lt;/p&gt;
    &lt;p&gt;As AI becomes more capable, we entrust it with increasingly consequential tasks. This makes understanding how these systems might fail even more critical for safety. A central concern in AI alignment is that superintelligent systems might coherently pursue misaligned goals: the classic paperclip maximizer scenario. But there's another possibility: AI might fail not through systematic misalignment, but through incoherence—unpredictable, self-undermining behavior that doesn't optimize for any consistent objective. That is, AI might fail in the same way that humans often fail, by being a hot mess.&lt;/p&gt;
    &lt;p&gt;This paper builds on the hot mess theory of misalignment (Sohl-Dickstein, 2023), which surveyed experts to rank various entities (including humans, animals, machine learning models, and organizations) by intelligence and coherence independently. It found that smarter entities are subjectively judged to behave less coherently. We take this hypothesis from survey data to empirical measurement across frontier AI systems, asking: As models become more intelligent and tackle harder tasks, do their failures look more like systematic misalignment, or more like a hot mess?&lt;/p&gt;
    &lt;p&gt;To quantify incoherence we decompose AI errors using the classic bias-variance framework:&lt;/p&gt;
    &lt;p&gt;We define incoherence as the fraction of error attributable to variance:&lt;/p&gt;
    &lt;p&gt;An incoherence of 0 means all errors are systematic (classic misalignment risk). An incoherence of 1 means all errors are random (the hot mess scenario). Crucially, this metric is independent of overall performance: a model can improve while becoming more or less coherent.&lt;/p&gt;
    &lt;p&gt;We evaluated frontier&lt;/p&gt;
    &lt;p&gt;Across all tasks and models, the longer models spend reasoning and taking actions, the more incoherent they become. This holds whether we measure reasoning tokens, agent actions, or optimizer steps.&lt;/p&gt;
    &lt;p&gt;How does incoherence change with model scale? The answer depends on task difficulty:&lt;/p&gt;
    &lt;p&gt;This suggests that scaling alone won't eliminate incoherence. As more capable models tackle harder problems, variance-dominated failures persist or worsen.&lt;/p&gt;
    &lt;p&gt;We find that when models spontaneously reason longer on a problem (compared to their median), incoherence spikes dramatically. Meanwhile, deliberately increasing reasoning budgets through API settings provides only modest coherence improvements. The natural variation dominates.&lt;/p&gt;
    &lt;p&gt;Aggregating multiple samples reduces variance (as expected from theory), providing a path to more coherent behavior, though this may be impractical for real-world agentic tasks where actions are irreversible.&lt;/p&gt;
    &lt;p&gt;A key conceptual point: LLMs are dynamical systems, not optimizers. When a language model generates text or takes actions, it traces trajectories through a high-dimensional state space. It has to be trained to act as an optimizer, and trained to align with human intent. It's unclear which of these properties will be more robust as we scale.&lt;/p&gt;
    &lt;p&gt;Constraining a generic dynamical system to act as a coherent optimizer is extremely difficult. Often the number of constraints required for monotonic progress toward a goal grows exponentially with the dimensionality of the state space. We shouldn't expect AI to act as coherent optimizers without considerable effort, and this difficulty doesn't automatically decrease with scale.&lt;/p&gt;
    &lt;p&gt;To probe this directly, we designed a controlled experiment: train transformers to explicitly emulate an optimizer. We generate training data from steepest descent on a quadratic loss function, then train models of varying sizes to predict the next optimization step given the current state (essentially: training a "mesa-optimizer").&lt;/p&gt;
    &lt;p&gt;The results are interesting:&lt;/p&gt;
    &lt;p&gt;Our results are evidence that future AI failures may look more like industrial accidents than coherent pursuit of goals that were not trained for. (Think: the AI intends to run the nuclear power plant, but gets distracted reading French poetry, and there is a meltdown.) However, coherent pursuit of poorly chosen goals that we trained for remains a problem. Specifically:&lt;/p&gt;
    &lt;p&gt;We use the bias-variance decomposition to systematically study how AI incoherence scales with model intelligence and task complexity. The evidence suggests that as AI tackles harder problems requiring more reasoning and action, its failures tend to become increasingly dominated by variance rather than bias. This doesn't eliminate AI risk—but it changes what that risk looks like, particularly for problems that are currently hardest for models, and should inform how we prioritize alignment research.&lt;/p&gt;
    &lt;p&gt;We thank Andrew Saxe, Brian Cheung, Kit Frasier-Taliente, Igor Shilov, Stewart Slocum, Aidan Ewart, David Duvenaud, and Tom Adamczewski for extremely helpful discussions on topics and results in this paper.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46864498</guid><pubDate>Tue, 03 Feb 2026 00:28:06 +0000</pubDate></item><item><title>GitHub discusses giving maintainers control to disable PRs</title><link>https://github.com/orgs/community/discussions/185387</link><description>&lt;doc fingerprint="674812173826c7aa"&gt;
  &lt;main&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Hey everyone,&lt;/p&gt;
          &lt;p&gt;I wanted to provide an update on a critical issue affecting the open source community: the increasing volume of low-quality contributions that is creating significant operational challenges for maintainers.&lt;/p&gt;
          &lt;p&gt;We’ve been hearing from you that you’re dedicating substantial time to reviewing contributions that do not meet project quality standards for a number of reasons - they fail to follow project guidelines, are frequently abandoned shortly after submission, and are often AI-generated. As AI continues to reshape software development workflows and the nature of open source collaboration, I want you to know that we are actively investigating this problem and developing both immediate and longer-term strategic solutions.&lt;/p&gt;
          &lt;head&gt;What we're exploring&lt;/head&gt;
          &lt;p&gt;We’ve spent time reviewing feedback from community members, working directly with maintainers to explore various solutions, and looking through open source repositories to understand the nature of these contributions. Below is an overview of the solutions we’re currently evaluating.&lt;/p&gt;
          &lt;p&gt;Short-term solutions:&lt;/p&gt;
          &lt;p&gt;Long-term direction:&lt;/p&gt;
          &lt;p&gt;As AI adoption accelerates, we recognize the need to proactively address how it can potentially transform both contributor and maintainer workflows. We are exploring:&lt;/p&gt;
          &lt;head&gt;Next Steps&lt;/head&gt;
          &lt;p&gt;These are some starting points, and we’re continuing to explore both immediate improvements and long-term solutions. Please share your feedback, questions, or concerns in this thread. Your input is crucial to making sure we’re building the right things and tackling this challenge effectively. As always, thank you for being part of this conversation. Looking forward to hearing your thoughts and working together to address this problem.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;head rend="h2"&gt;Replies: 23 comments 31 replies&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;such a great intiative&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;I know this is a pretty ambitious idea and not trivial to implement, but it would be really powerful to have an AI-detection mechanism with a configurable threshold at the repository or organization level. That way, teams could decide what percentage of AI-generated code is acceptable in pull requests.&lt;/p&gt;
          &lt;p&gt;Another possible approach would be to define a set of rules or prompts and evaluate pull requests against them. PRs that don’t meet those rules could be automatically flagged or potentially even closed.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;As of today, I would say that 1 out of 10 PRs created with AI is legitimate and meets the standards required to open that PR. On 28 Jan 2026, at 18:41, Camilla Moraes ***@***.***&amp;gt; wrote: Another possible approach would be to define a set of rules or prompts and evaluate pull requests against them. PRs that don’t meet those rules could be automatically flagged or potentially even closed. This is definitely something we’re exploring. One idea is to leverage a repository’s CONTRIBUTING.md file as a source of truth for project guidelines and then validate PRs against any defined rules. In regards to AI-generated code, have you seen cases where the code is AI-generated but still high-quality and genuinely solves the problem? Or is it alwaays just something you want to close out immediately? I'm curious because I'm wondering if an AI-detection mechanism would rule out PRs where AI is used constructively, but that's where we'd want to test this thoroughly and understand what sensible thresholds look like. — Reply to this email directly, view it on GitHub&amp;lt;#185387 (reply in thread)&amp;gt;, or unsubscribe&amp;lt;https://github.com/notifications/unsubscribe-auth/ABBWEYEKF6WLNDKE376L3GD4JDYFXAVCNFSM6AAAAACS7B7C7OVHI2DSMVQWIX3LMV43URDJONRXK43TNFXW4Q3PNVWWK3TUHMYTKNRTGEZTMMI&amp;gt;. You are receiving this because you commented.Message ID: ***@***.***&amp;gt;&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;head class="border-bottom-0 py-0 px-3 rounded-2 color-bg-subtle"&gt; This comment was marked as off-topic. &lt;/head&gt;
    &lt;head rend="h3"&gt; This comment was marked as off-topic. &lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;PLEASE make this better than disabling issues: Imparticular, do not restrict access to previously opened PRs. Fine to hide the UI or even the ability to list them, but let people access them with a direct link. I'd also appreciate if that was done in repos where issues got disabled.&lt;/p&gt;
          &lt;p&gt;Context: Someone realizing that they no longer want to allow issues (or PRs) should NOT make any existing content disappear for good. Especially in cases where this content is referenced elsewhere and suddenly becomes a dead link...&lt;/p&gt;
          &lt;p&gt;Similar to my suggestion above, this should be limited in time. It's amazing to delete spam PRs. Hacktoberfest-cheating, Indian-Youtube-nonsense, and the likes. But if there is a controversy around a PR, which has happened numerous times before e.g. due to highly unpopular decisions by repo maintainers (Minio comes to my mind), then they should NOT be able to permanently delete at PR just because it's inconvenient or shows their true face.&lt;/p&gt;
          &lt;p&gt;My suggestion would be to have a (very) limited timeframe to delete a PR. Maybe a week in case of low activity (e.g. a spam PR the maintainers did not see), much less in case of high activity. Having an exception for "quiet" PRs (no meaningful amount of comments) and allowing to delete those for longer would be perfectly fine of course so someone who rarely checks their repo can delete spam even when they notice it late.&lt;/p&gt;
          &lt;p&gt;PS: Yes, I'm aware that deleting individual comments on PRs and issues is already possible. But that's a hassle compared to being able to nuke the whole thing with two clicks.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Hey! I am from Azure Core Upstream and we have a lot of OSS maintainers who mainly maintain repositories on GitHub. We held an internal session to talk about copilot and there is a discussion on the topic where maintainers feel caught between today’s required review rigor (line-by-line understanding for anything shipped) and a future where agentic / AI-generated code makes that model increasingly unsustainable.&lt;/p&gt;
          &lt;p&gt;below are some key maintainer's pain points:&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;head class="border-bottom-0 py-0 px-3 rounded-2 color-bg-subtle"&gt; This comment was marked as off-topic. &lt;/head&gt;
    &lt;head rend="h3"&gt; This comment was marked as off-topic. &lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;head class="border-bottom-0 py-0 px-3 rounded-2 color-bg-subtle"&gt; This comment was marked as spam. &lt;/head&gt;
    &lt;head rend="h3"&gt; This comment was marked as spam. &lt;/head&gt;
    &lt;head class="border-bottom-0 py-0 px-3 rounded-2 color-bg-subtle"&gt; This comment was marked as spam. &lt;/head&gt;
    &lt;head rend="h3"&gt; This comment was marked as spam. &lt;/head&gt;
    &lt;head class="border-bottom-0 py-0 px-3 rounded-2 color-bg-subtle"&gt; This comment was marked as spam. &lt;/head&gt;
    &lt;head rend="h3"&gt; This comment was marked as spam. &lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;An option to limit new contributors to one open PR would be nice.&lt;/p&gt;
          &lt;p&gt;Just today I had to batch-close several AI generated PRs which were all submitted around the same time.&lt;/p&gt;
          &lt;p&gt;For this protection, defining "new contributor" is probably not possible to do perfectly. But anyone who has no interactions with a project prior to the last 48 hours seems like a good heuristic. The point is to catch such a user at submission time and limit the amount of maintainer attention they can take up.&lt;/p&gt;
          &lt;p&gt;For a different type of problem, I'd like to be able to close PRs as "abandoned", similar to the issue close statuses. It's a clear UI signal to the contributor that their work isn't being rejected but I'm not going to finish it for them. Several of the low quality contributions I have handled, dating back to before the Slop Era but getting worse, are simply incomplete and need follow through.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;For the long term horizon: Implement a reviewer LLM that first does an initial scoring of the PRs? Critique is far easier than creation of a correct result. That automated pre-moderation should give the edge needed to handle. Depending on whether you just use rich prompting or fine-tuning, you can even start building an "oracle vox" for your project, which acts as a reasonably informed, reasonably on point virtual representative for the project/organization.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;This is a very real problem, and I appreciate that it’s being treated as systemic rather than blaming maintainers or contributors individually.&lt;/p&gt;
          &lt;p&gt;One concern I have with repo-level PR restrictions is that they may disproportionately impact first-time contributors who do want to engage meaningfully but don’t yet have collaborator status.&lt;/p&gt;
          &lt;p&gt;Personally, I think the most promising direction here is criteria-based PR gating rather than blanket restrictions things like required checklist completion, passing CI, linked issues, or acknowledgement of contribution guidelines before a PR can be opened.&lt;/p&gt;
          &lt;p&gt;On AI usage specifically, transparency feels more scalable than prohibition. Clear disclosure combined with automated guideline checks could help maintainers focus on high-intent contributions without discouraging responsible AI-assisted workflows.&lt;/p&gt;
          &lt;p&gt;Looking forward to seeing how these ideas evolve especially solutions that preserve openness while respecting maintainer time.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Thinking along the lines of the discussion first approach that Ghostty uses, I think one way to create just enough friction would be to have an opt-in where a PR has to be linked to an open issue or discussion topic. So when an unprivileged (i.e. does not have elevated privileges on the repo) user tries to create a PR, there's a required field that takes an issue/discussion number. If that's not provided (or the corresponding issue/discussion is closed), then the PR can't be created.&lt;/p&gt;
          &lt;p&gt;This could be trivially worked around by throwing in any old issue/discussion (or by creating one), but it may cause just enough friction to help. To guard against this, perhaps maintainers could set a "minimum age" for the issue/discussion (e.g. 12 hours) to prevent creating fake issues to support a spammy PR.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;At FOSDEM today, @abbycabs gave a great talk about "the Synthetic Senior" and some initial strategies to better handle contributions, which I found very insightful.&lt;/p&gt;
          &lt;p&gt;She mentioned a few steps to improve submission quality such as AI attribution policies and requirements for discussion prior to PR filing: we've implemented many of these at mozilla.ai (https://blog.mozilla.ai/ai-generated-code-isnt-cheating-oss-needs-to-talk-about-it/) over the past months (and that I've been implementing in https://github.com/njbrake/agent-of-empires).&lt;/p&gt;
          &lt;p&gt;At the end of her talk I asked this question, which she recommended I add here: how can github change the incentive structure so that contributors are incentivized to spend their time writing a great issue/feature description, not on submitting/prompting code via a PR?&lt;/p&gt;
          &lt;p&gt;Especially for up and coming contributors I think there is some pride associated with saying "I am a contributor to xyz library". If all someone does is write a really great issue description or feature request, I don't think they get any official recognition unless the repository owner chooses to do something special outside of built-in Github features?&lt;/p&gt;
          &lt;p&gt;But in the world of AI coding, my current thought is that a clear, well-thought out, concise description of the problem or feature is significantly more valuable than sending Claude Code off to write some code and create a PR so you can display that you're a contributor.&lt;/p&gt;
          &lt;p&gt;Imo much of open-source is really at risk because of this: we need to figure out a way to encourage knowledge sharing to keep alive what makes open source and github so special: the community. 🙏&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Thanks for sharing this update this is a real &amp;amp; increasingly painful issue for maintainers.&lt;/p&gt;
          &lt;p&gt;From a maintainer perspective, the problem isn’t AI itself, but asymmetry of cost : it’s now extremely cheap to generate contributions, while the cost of reviewing, triaging and communicating remains fully human.&lt;/p&gt;
          &lt;p&gt;On short-term controls&lt;/p&gt;
          &lt;p&gt;Being able to delete spam PRs directly from the UI would also significantly reduce cognitive overhead for maintainers.&lt;/p&gt;
          &lt;p&gt;On longer-term solutions&lt;/p&gt;
          &lt;p&gt;For AI-assisted contributions, transparency is key, but attribution alone won’t solve quality issues. What would help maintainers most is :&lt;/p&gt;
          &lt;p&gt;One important thing to avoid is shifting more complexity onto maintainers. Any AI-based triage should aim to reduce decision fatigue , not introduce another system that needs constant tuning.&lt;/p&gt;
          &lt;p&gt;Overall, giving maintainers more control over when &amp;amp; how contributions enter their workflow seems more valuable than trying to judge contributor intent after the fact.&lt;/p&gt;
          &lt;p&gt;Appreciate the openness here , looking forward to seeing how these tools evolve.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Really glad GitHub is addressing this maintainers need better filters without shutting out genuine contributors. Clearer contribution criteria upfront could also reduce the download to earn style low-effort PRs that waste review time.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Regarding:&lt;/p&gt;
          &lt;p&gt;One step you could make to build trust here would be to provide a way for repository owners to state their policy on AI-assisted contributions, and to automatically disable Copilot features in repositories whose AI policy prohibits its use. (Bonus points if you can coordinate with other AI vendors to have their tools respect those policies as well.)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;This is the way. Consider this simple first step: allow maintainers to require CI to pass before a PR can even be opened. If you start there, maintainers can develop their own logic for "is this a PR I want to review" and put it in CI.&lt;/p&gt;
          &lt;p&gt;My current solution is generally "I won't review PRs that don't pass CI." But it's hard to communicate that, and it still takes time.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;I've been waiting for this for a long time, glad it's being considered.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Personally i would rather this option be to allow previous contributors and only a single PR from "new" contributors. This way external contributors can still become contributors and not block them from drive by bug fixes.&lt;/p&gt;
          &lt;p&gt;What is the point of delete ? Who really cares if you have 40 or 400 closed issues? One solution could be to provide more "ways" for a PR to be resolved currently a pr can be &lt;/p&gt;
          &lt;head&gt;AI Disclosure&lt;/head&gt;
          &lt;p&gt;I agree with what other have said about disclosure being the biggest thing. Provide us with some tools to enforce a minimum standards when creating PRS. For example i can ask in my pr template for users to disclose if they used AI but i can not really enforce them answering it.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;A GitHub App that gates PRs (especially agentic ones) by scoring them (heuristics + optional LLM), enforcing a "Contribution Contract" and requiring a Build Sheet (intent, scope, test plan). Slop gets auto-closed/drafted/quarantined; only high-signal PRs reach maintainers.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46864517</guid><pubDate>Tue, 03 Feb 2026 00:30:02 +0000</pubDate></item><item><title>Banning lead in gas worked. The proof is in our hair</title><link>https://attheu.utah.edu/health-medicine/banning-lead-in-gas-worked-the-proof-is-in-our-hair/</link><description>&lt;doc fingerprint="7ef43095fbe01e94"&gt;
  &lt;main&gt;
    &lt;p&gt;Prior to the establishment of the Environmental Protection Agency in 1970, Americans lived in communities awash with lead from industrial sources, paint, water supply pipes and, most significantly, tailpipe emissions. A dangerous neurotoxin that accumulates in human tissues and is linked to developmental deficits in children, environmental lead levels have come way down in the years since, and so have human exposures.&lt;/p&gt;
    &lt;p&gt;The proof is in your hair.&lt;/p&gt;
    &lt;p&gt;An analysis of hair samples conducted by University of Utah scientists shows precipitous reductions in lead levels since 1916.&lt;/p&gt;
    &lt;p&gt;“We were able to show through our hair samples what the lead concentrations are before and after the establishment of regulations by the EPA,” said demographer Ken Smith, a distinguished professor emeritus of family and consumer studies. “We have hair samples spanning about 100 years. And back when the regulations were absent, the lead levels were about 100 times higher than they are after the regulations.”&lt;/p&gt;
    &lt;head rend="h4"&gt;A useful element with a dark side&lt;/head&gt;
    &lt;p&gt;The findings, which appear in PNAS, underscore the vital role of environmental regulations in protecting public health. The study notes lead rules are now being weakened by the Trump administration in a wide-ranging move to ease environmental protections.&lt;/p&gt;
    &lt;p&gt;“We should not forget the lessons of history. And the lesson is those regulations have been very important,” said co-author Thure Cerling, a distinguished professor of both geology and biology. “Sometimes they seem onerous and mean that industry can’t do exactly what they’d like to do when they want to do it or as quickly as they want to do it. But it’s had really, really positive effects.”&lt;/p&gt;
    &lt;p&gt;Lead is the heaviest of heavy metals that, like mercury and arsenic, accumulate in living tissue and are toxic at even low levels. Yet lead holds very useful properties, great for fashioning into pipes and as a chemical additive. Lead was added to paint to improve durability, speed up drying, and produce vibrant colors with greater coverage. Lead also improved the performance of automobile engines by preventing pistons from “knocking.”&lt;/p&gt;
    &lt;p&gt;By the 1970s, its toxicity became well established, and EPA regulations began phasing it out of paint, pipes, gasoline and other consumer products.&lt;/p&gt;
    &lt;head rend="h4"&gt;How Utahns’ affection for family history advances science&lt;/head&gt;
    &lt;p&gt;To document whether these steps were helping reduce lead exposure in people, Smith joined with geologist Diego Fernandez and Cerling, who had developed techniques to discern where animals have lived and what they eat based on chemical analysis of hair and teeth.&lt;/p&gt;
    &lt;p&gt;The lead research is built on a previous study funded by the university’s Center on Aging and the National Institutes of Health that had recruited Utahns who consented to provide blood samples and family health histories.&lt;/p&gt;
    &lt;p&gt;For the new study, the researchers asked members of that cohort to provide hair samples, both contemporary and from when they were young. These people obliged, and some were able to find ancestors’ hair preserved in family scrapbooks dating as far back as a century. In all, the team acquired hair samples from 48 individuals in this manner, offering a robust window into lead levels along Utah’s populous Wasatch Front, which historically experienced heavy lead emissions from industrial sources.&lt;/p&gt;
    &lt;p&gt;“The Utah part of this is so interesting because of the way people keep track of their family history. I don’t know that you could do this in New York or Florida,” said Smith, who directed the U’s Pedigree and Population Program at the Huntsman Cancer Center while these studies were conducted.&lt;/p&gt;
    &lt;p&gt;This region supported a vibrant smelting industry through most of the 20th century, centered in the cities of Midvale and Murray. Most of Utah’s smelters were shuttered by the 1970s, around the same time the EPA clamped down on the use of lead in consumer products.&lt;/p&gt;
    &lt;p&gt;The research team ran the hair samples through mass spectrometry equipment at the facility directed by Fernandez.&lt;/p&gt;
    &lt;p&gt;“The surface of the hair is special. We can tell that some elements get concentrated and accumulated on the surface. Lead is one of those. That makes it easier because lead is not lost over time,” said Fernandez, a research professor in the Department of Geology &amp;amp; Geophysics. “Because mass spectrometry is very sensitive, we can do it with one hair strand, though we cannot tell where the lead is in the hair. It’s probably on the surface mostly, but it could also be coming from the blood if that hair was synthesized when there was high lead in the blood.”&lt;/p&gt;
    &lt;p&gt;Blood would provide a better exposure assessment, but hair is far easier to collect and preserve, and more importantly, it offers clues to long-ago exposures for a person who has grown up or even deceased.&lt;/p&gt;
    &lt;p&gt;“It doesn’t really record that internal blood concentration that your brain is seeing, but it tells you about that overall environmental exposure,” Cerling said. “One of the things that we found is that hair records that original value, but then the longer the hair has been exposed to the environment, the higher the lead concentrations are.”&lt;/p&gt;
    &lt;p&gt;The team’s findings regarding lead in hair run parallel to the reductions of lead in gasoline following the EPA’s establishment by President Richard Nixon.&lt;/p&gt;
    &lt;p&gt;Prior to 1970, for example, gasolines contained about 2 grams of lead per gallon. That might not sound like much, but considering the billions of gallons of fuel American automobiles burn each year, it adds up to nearly 2 pounds of lead released into the environment per person a year.&lt;/p&gt;
    &lt;p&gt;‘It’s an enormous amount of lead that’s being put into the environment and quite locally,” Cerling said. “It’s just coming out of the tailpipe, goes up in the air and then it comes down. It’s in the air for a number of days, especially during the inversions that we have and it absorbs into your hair, you breathe it and it goes into your lungs.”&lt;/p&gt;
    &lt;p&gt;But after the 1970s, even as gasoline consumption escalated in the United States, the concentrations of lead in the hair samples plummeted, from as high as 100 parts per million (ppm) to 10 ppm by 1990. In 2024, the level was less than 1 ppm.&lt;/p&gt;
    &lt;p&gt;The study, titled “Lead in archived hair documents decline in human lead (Pb) exposure since establishment of the US Environmental Protection Agency,” was published Feb. 2 in PNAS, or Proceedings of the National Academy of Sciences. Support came from the Huntsman Cancer Foundation and the National Cancer Institute through a grant to the Utah Population Database and the University of Utah.&lt;/p&gt;
    &lt;head rend="h3"&gt;MEDIA &amp;amp; PR CONTACTS&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Brian Maffly Science writer, University of Utah Communications&lt;lb/&gt;801-573-2382 brian.maffly@utah.edu&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46865275</guid><pubDate>Tue, 03 Feb 2026 01:52:21 +0000</pubDate></item><item><title>Floppinux – An Embedded Linux on a Single Floppy, 2025 Edition</title><link>https://krzysztofjankowski.com/floppinux/floppinux-2025.html</link><description>&lt;doc fingerprint="1719381e65a62ba8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;FLOPPINUX&lt;/head&gt;
    &lt;head rend="h2"&gt;An Embedded ð§Linux on a Single ð¾Floppy&lt;/head&gt;
    &lt;head rend="h2"&gt;2025 Edition (v0.3.1)&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;FLOPPINUX was released in 2021. After four years people find it helpful. Because of that I decided to revisit FLOPPINUX in 2025 and make updated tutorial. This brings bunch of updates like latest kernel and persistent storage.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Table of Contents&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Main Project Goals&lt;/item&gt;
      &lt;item&gt;Linux Kernel&lt;/item&gt;
      &lt;item&gt;64-bit Base OS&lt;/item&gt;
      &lt;item&gt;Working Directory&lt;/item&gt;
      &lt;item&gt;System Requirements&lt;/item&gt;
      &lt;item&gt;Kernel&lt;/item&gt;
      &lt;item&gt;Toolset&lt;/item&gt;
      &lt;item&gt;Filesystem&lt;/item&gt;
      &lt;item&gt;Boot Image&lt;/item&gt;
      &lt;item&gt;Floppy Disk&lt;/item&gt;
      &lt;item&gt;Summary&lt;/item&gt;
      &lt;item&gt;Download&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Main Project Goals&lt;/head&gt;
    &lt;p&gt;Think of this as Linux From Scratch but for making single floppy distribution.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It is meant to be a full workshop (tutorial) that you can follow easily and modify it to your needs. It is a learning exercise. Some base Linux knowledge is needed.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The final distribution is very simple and consists only of minimum of tools and hardware support. As a user you will be able to boot any PC with a floppy drive to a Linux terminal, edit files, and create simple scripts. There is 264KB of space left for your newly created files.&lt;/p&gt;
    &lt;head rend="h3"&gt;Core features:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fully working distribution booting from the single floppy&lt;/item&gt;
      &lt;item&gt;Latest* Linux kernel&lt;/item&gt;
      &lt;item&gt;Supporting all 32-bit x86 CPUs since Intel 486DX&lt;/item&gt;
      &lt;item&gt;Have a working text editor (Vi) and basic file manipulation commands (move, rename, delete, etc.)&lt;/item&gt;
      &lt;item&gt;Support for simple scripting&lt;/item&gt;
      &lt;item&gt;Persistent storage on the floppy to actualy save files (264KB)&lt;/item&gt;
      &lt;item&gt;Works on real hardware and emulation&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Minimum Hardware Requirements:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Intel 486DX 33MHz&lt;/item&gt;
      &lt;item&gt;20MB RAM&lt;/item&gt;
      &lt;item&gt;Internal floppy disk&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Linux Kernel&lt;/head&gt;
    &lt;p&gt;The Linux kernel drops i486 support in 6.15 (released May 2025), so 6.14 (released March 2025) is the latest version with full compatibility.&lt;/p&gt;
    &lt;head rend="h2"&gt;64-bit Base OS&lt;/head&gt;
    &lt;p&gt;This time I will do everything on Omarchy Linux. It is 64-bit operating system based on Arch Linux. Instructions should work on all POSIX systems. Only difference is getting needed packages.&lt;/p&gt;
    &lt;head rend="h2"&gt;Working Directory&lt;/head&gt;
    &lt;p&gt;Create directory where you will keep all the files.&lt;/p&gt;
    &lt;code&gt;mkdir ~/my-linux-distro/
BASE=~/my-linux-distro/
cd $BASE&lt;/code&gt;
    &lt;head rend="h2"&gt;Host OS Requirements&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;You need supporting software to build things. This exact list may vary depending on the system you have.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Install needed software/libs. On Arch/Omarchy 3.1:&lt;/p&gt;
    &lt;code&gt;sudo pacman -S ncurses bc flex bison syslinux cpio&lt;/code&gt;
    &lt;p&gt;Cross-compiler:&lt;/p&gt;
    &lt;code&gt;wget https://musl.cc/i486-linux-musl-cross.tgz
tar xvf i486-linux-musl-cross.tgz
rm i486-linux-musl-cross.tgz&lt;/code&gt;
    &lt;head rend="h3"&gt;Emulation&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;86Box is also good but slower. Bochs is the best but for debugging, not needed here.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;For emulation I will be using qemu.&lt;/p&gt;
    &lt;code&gt;sudo pacman -S qemu-full&lt;/code&gt;
    &lt;head rend="h2"&gt;Kernel&lt;/head&gt;
    &lt;p&gt;Get the sources for the latest compatible kernel 6.14.11:&lt;/p&gt;
    &lt;code&gt;git clone --depth=1 --branch v6.14.11 https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git
cd linux&lt;/code&gt;
    &lt;p&gt;Now, that you have them in linux/ directory lets configure and build our custom kernel. First create tiniest base configuration:&lt;/p&gt;
    &lt;code&gt;make ARCH=x86 tinyconfig&lt;/code&gt;
    &lt;quote&gt;
      &lt;p&gt;This is a bootstrap with absolute minimum features. Just enough to boot the system. We want a little bit more.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Add additonal config settings on top of it:&lt;/p&gt;
    &lt;code&gt;make ARCH=x86 menuconfig&lt;/code&gt;
    &lt;p&gt;Important: Do not uncheck anything in options unless specified so. Some of those options are important. You can uncheck but on your own risk.&lt;/p&gt;
    &lt;p&gt;From menus choose those options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;General Setup &lt;list rend="ul"&gt;&lt;item&gt;Configure standard kernel features (expert users) &lt;list rend="ul"&gt;&lt;item&gt;Enable support for printk&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Initial RAM filesystem and RAM disk (initramfs/initrd) &lt;list rend="ul"&gt;&lt;item&gt;Support initial ramdisk/ramfs compressed using XZ and uncheck everything else&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Configure standard kernel features (expert users) &lt;/item&gt;
      &lt;item&gt;Processor type and features &lt;list rend="ul"&gt;&lt;item&gt;x86 CPU resources control support&lt;/item&gt;&lt;item&gt;Processor family &lt;list rend="ul"&gt;&lt;item&gt;486DX&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Enable the block layer&lt;/item&gt;
      &lt;item&gt;Executable file formats &lt;list rend="ul"&gt;&lt;item&gt;Kernel support for ELF binaries&lt;/item&gt;&lt;item&gt;Kernel support for scripts starting with #!&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Device Drivers &lt;list rend="ul"&gt;&lt;item&gt;Block devices &lt;list rend="ul"&gt;&lt;item&gt;Normal floppydisk support&lt;/item&gt;&lt;item&gt;RAM block device support &lt;list rend="ul"&gt;&lt;item&gt;Default number of RAM disk: 1&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Character devices &lt;list rend="ul"&gt;&lt;item&gt;Enable TTY&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Block devices &lt;/item&gt;
      &lt;item&gt;File systems &lt;list rend="ul"&gt;&lt;item&gt;DOS/FAT/EXFAT/NT Filesystems &lt;list rend="ul"&gt;&lt;item&gt;MSDOS fs support&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Pseudo filesystems &lt;list rend="ul"&gt;&lt;item&gt;/proc file system support&lt;/item&gt;&lt;item&gt;sysfs file system support&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Native language support &lt;list rend="ul"&gt;&lt;item&gt;Codepage 437&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;DOS/FAT/EXFAT/NT Filesystems &lt;/item&gt;
      &lt;item&gt;Library routines &lt;list rend="ul"&gt;&lt;item&gt;XZ decompression and uncheck everything under it&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Exit configuration (yes, save settings to .config).&lt;/p&gt;
    &lt;p&gt;Time for compiling!&lt;/p&gt;
    &lt;head rend="h3"&gt;Compile Kernel&lt;/head&gt;
    &lt;code&gt;make ARCH=x86 bzImage -j$(nproc)&lt;/code&gt;
    &lt;p&gt;This will take a while depending on the speed of your CPU. In the end the kernel will be created in arch/x86/boot/ as bzImage file.&lt;/p&gt;
    &lt;p&gt;Move kernel to our main directory and go back to it:&lt;/p&gt;
    &lt;code&gt;mv arch/x86/boot/bzImage ../
cd ..&lt;/code&gt;
    &lt;head rend="h2"&gt;Toolset&lt;/head&gt;
    &lt;p&gt;Without tools kernel will just boot and you will not be able to do anything. One of the most popular lightweight tools is BusyBox. It replaces the standard GNU utilities with way smaller but still functional alternatives, perfect for embedded needs.&lt;/p&gt;
    &lt;p&gt;Get the 1.36.1 version from busybox.net or Github mirror. Download the file, extract it, and change directory:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Remember to be in the working directory.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;wget https://github.com/mirror/busybox/archive/refs/tags/1_36_1.tar.gz
tar xzvf 1_36_1.tar.gz
rm 1_36_1.tar.gz
cd busybox-1_36_1/&lt;/code&gt;
    &lt;p&gt;As with kernel you need to create starting configuration:&lt;/p&gt;
    &lt;code&gt;make ARCH=x86 allnoconfig&lt;/code&gt;
    &lt;quote&gt;
      &lt;p&gt;You may skip this following fix if you are building on Debian/Fedora&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Fix for Arch Linux based distributions:&lt;/p&gt;
    &lt;code&gt;sed -i 's/main() {}/int main() {}/' scripts/kconfig/lxdialog/check-lxdialog.sh&lt;/code&gt;
    &lt;quote&gt;
      &lt;p&gt;Now the fun part. You need to choose what tools you want. Each menu entry will show how much more KB will be taken if you choose it. So choose it wisely :) For the first time use my selection.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Run the configurator:&lt;/p&gt;
    &lt;code&gt;make ARCH=x86 menuconfig&lt;/code&gt;
    &lt;p&gt;Choose the following options. Remember to do not uncheck anything if not stated here.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Settings &lt;list rend="ul"&gt;&lt;item&gt;Support files &lt;list rend="ul"&gt;&lt;item&gt;2GB&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Build static binary (no shared libs)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Support files &lt;/item&gt;
      &lt;item&gt;Coreutils &lt;list rend="ul"&gt;&lt;item&gt;cat&lt;/item&gt;&lt;item&gt;cp&lt;/item&gt;&lt;item&gt;df&lt;/item&gt;&lt;item&gt;echo&lt;/item&gt;&lt;item&gt;ls&lt;/item&gt;&lt;item&gt;mkdir&lt;/item&gt;&lt;item&gt;mv&lt;/item&gt;&lt;item&gt;rm&lt;/item&gt;&lt;item&gt;sync&lt;/item&gt;&lt;item&gt;test &lt;list rend="ul"&gt;&lt;item&gt;test as [&lt;/item&gt;&lt;item&gt;test as [[&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Console Utilities &lt;list rend="ul"&gt;&lt;item&gt;clear&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Editors &lt;list rend="ul"&gt;&lt;item&gt;vi&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Init Utilities &lt;list rend="ul"&gt;&lt;item&gt;init &lt;list rend="ul"&gt;&lt;item&gt;uncheck everything else (inside init: keep [*] only on init in this page)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;init &lt;/item&gt;
      &lt;item&gt;Linux System Utilities &lt;list rend="ul"&gt;&lt;item&gt;mdev&lt;/item&gt;&lt;item&gt;mount &lt;list rend="ul"&gt;&lt;item&gt;Support lots of -o flags&lt;/item&gt;&lt;item&gt;uncheck evrything else&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;umount&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Miscellaneous Utilities &lt;list rend="ul"&gt;&lt;item&gt;uncheck readahead&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Shells &lt;list rend="ul"&gt;&lt;item&gt;Choose alias as (ash)&lt;/item&gt;&lt;item&gt;ash&lt;/item&gt;&lt;item&gt;Optimize for size instead of speed&lt;/item&gt;&lt;item&gt;Alias support&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now exit with save config.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cross Compiler Setup&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Our target system needs to be 32-bit. To compile it on 64-bit system we need a cross compiler. You can setup this by hand in the menuconfig or just copy and paste those four lines.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Setup paths:&lt;/p&gt;
    &lt;code&gt;sed -i "s|.*CONFIG_CROSS_COMPILER_PREFIX.*|CONFIG_CROSS_COMPILER_PREFIX=\"${BASE}/i486-linux-musl-cross/bin/i486-linux-musl-\"|" .config

sed -i "s|.*CONFIG_SYSROOT.*|CONFIG_SYSROOT=\"${BASE}/i486-linux-musl-cross\"|" .config

sed -i "s|.*CONFIG_EXTRA_CFLAGS.*|CONFIG_EXTRA_CFLAGS=-I$BASE/i486-linux-musl-cross/include|" .config

sed -i "s|.*CONFIG_EXTRA_LDFLAGS.*|CONFIG_EXTRA_LDFLAGS=-L$BASE/i486-linux-musl-cross/lib|" .config&lt;/code&gt;
    &lt;head rend="h3"&gt;Compile BusyBox&lt;/head&gt;
    &lt;p&gt;Build tools and create base filesystem (âinstallâ). It will ask for options, just press enter for default for all of them.&lt;/p&gt;
    &lt;code&gt;make ARCH=x86 -j$(nproc) &amp;amp;&amp;amp; make ARCH=x86 install&lt;/code&gt;
    &lt;p&gt;This will create a filesystem with all the files at **_install/**. Move it to our main directory. I like to rename it to.&lt;/p&gt;
    &lt;p&gt;Lastly to to that new directory.&lt;/p&gt;
    &lt;code&gt;mv _install ../filesystem
cd ../filesystem&lt;/code&gt;
    &lt;head rend="h2"&gt;Filesystem&lt;/head&gt;
    &lt;p&gt;You got kernel and basic tools but the system still needs some additional directory structure.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This created minimum viable directory structure for satisfying the basic requirements of a Linux system.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Remember to be in the filesystem/ directory.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;mkdir -pv {dev,proc,etc/init.d,sys,tmp,home}
sudo mknod dev/console c 5 1
sudo mknod dev/null c 1 3&lt;/code&gt;
    &lt;p&gt;Next step is to add minimum configuration files. First one is a welcome message that will be shown after booting.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Here is the first real opportunity to go wild and make this your own signature.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;cat &amp;gt;&amp;gt; welcome &amp;lt;&amp;lt; EOF
Your welome message or ASCII art.
EOF&lt;/code&gt;
    &lt;p&gt;Or download my welcome file.&lt;/p&gt;
    &lt;code&gt;wget https://krzysztofjankowski.com/floppinux/downloads/0.3.1/welcome&lt;/code&gt;
    &lt;p&gt;It looks like that:&lt;/p&gt;
    &lt;code&gt;$ cat welcome

                _________________
               /_/ FLOPPINUX  /_/;
              / ' boot disk  ' //
             / '------------' //
            /   .--------.   //
           /   /         /  //
          .___/_________/__//   1440KiB
          '===\_________\=='   3.5"

_______FLOPPINUX_V_0.3.1 __________________________________
_______AN_EMBEDDED_SINGLE_FLOPPY_LINUX_DISTRIBUTION _______
_______BY_KRZYSZTOF_KRYSTIAN_JANKOWSKI ____________________
_______2025.12 ____________________________________________&lt;/code&gt;
    &lt;quote&gt;
      &lt;p&gt;Back to serious stuff. Inittab tells the system what to do in critical states like starting, exiting and restarting. It points to the initialization script rc that is the first thing that our OS will run before dropping into the shell.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Create an inittab file:&lt;/p&gt;
    &lt;code&gt;cat &amp;gt;&amp;gt; etc/inittab &amp;lt;&amp;lt; EOF
::sysinit:/etc/init.d/rc
::askfirst:/bin/sh
::restart:/sbin/init
::ctrlaltdel:/sbin/reboot
::shutdown:/bin/umount -a -r
EOF&lt;/code&gt;
    &lt;p&gt;And the init rc script:&lt;/p&gt;
    &lt;code&gt;cat &amp;gt;&amp;gt; etc/init.d/rc &amp;lt;&amp;lt; EOF
#!/bin/sh
mount -t proc none /proc
mount -t sysfs none /sys
mdev -s
ln -s /proc/mounts /etc/mtab
mkdir -p /mnt /home
mount -t msdos -o rw /dev/fd0 /mnt
mkdir -p /mnt/data
mount --bind /mnt/data /home
clear
cat welcome
cd /home
/bin/sh
EOF&lt;/code&gt;
    &lt;p&gt;Make the script executable and owner of all files to root:&lt;/p&gt;
    &lt;code&gt;chmod +x etc/init.d/rc
sudo chown -R root:root .&lt;/code&gt;
    &lt;p&gt;Compress this directory into one file. Then go back to working directory.&lt;/p&gt;
    &lt;code&gt;find . | cpio -H newc -o | xz --check=crc32 --lzma2=dict=512KiB -e &amp;gt; ../rootfs.cpio.xz
cd ..&lt;/code&gt;
    &lt;p&gt;Create booting configuration.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Another place to tweak parameters for your variant. Text after SAY is what will be displayed on the screen as first, usualy a name of the OS.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;The tsc=unstable is useful on some (real) computers to get rid of randomly shown warnings about Time Stamp Counter.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Remember to be in the working directory.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;cat &amp;gt;&amp;gt; syslinux.cfg &amp;lt;&amp;lt; EOF
DEFAULT floppinux
LABEL floppinux
SAY [ BOOTING FLOPPINUX VERSION 0.3.1 ]
KERNEL bzImage
INITRD rootfs.cpio.xz
APPEND root=/dev/ram rdinit=/etc/init.d/rc console=tty0 tsc=unstable
EOF&lt;/code&gt;
    &lt;p&gt;Make it executable:&lt;/p&gt;
    &lt;code&gt;chmod +x syslinux.cfg&lt;/code&gt;
    &lt;p&gt;Create sample file&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;To make the system a little bit more user friendly I like to have a sample file that user will be able to read and edit. You can put anything you want in it. A simple help would be also a good idea to include.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;cat &amp;gt;&amp;gt; hello.txt &amp;lt;&amp;lt; EOF
Hello, FLOPPINUX user!
EOF&lt;/code&gt;
    &lt;p&gt;Filesystem is ready. Final step is to put this all on a floppy!&lt;/p&gt;
    &lt;head rend="h2"&gt;Boot Image&lt;/head&gt;
    &lt;p&gt;First we need an empty file in exact size of a floppy disk. Then format and make it bootable.&lt;/p&gt;
    &lt;p&gt;Create empty floppy image:&lt;/p&gt;
    &lt;code&gt;dd if=/dev/zero of=floppinux.img bs=1k count=1440&lt;/code&gt;
    &lt;p&gt;Format it and create bootloader:&lt;/p&gt;
    &lt;code&gt;mkdosfs -n FLOPPINUX floppinux.img
syslinux --install floppinux.img&lt;/code&gt;
    &lt;p&gt;Mount it and copy syslinux, kernel, and filesystem onto it:&lt;/p&gt;
    &lt;code&gt;sudo mount -o loop floppinux.img /mnt
sudo mkdir /mnt/data
sudo cp hello.txt /mnt/data/
sudo cp bzImage /mnt
sudo cp rootfs.cpio.xz /mnt
sudo cp syslinux.cfg /mnt
sudo umount /mnt&lt;/code&gt;
    &lt;p&gt;Done!&lt;/p&gt;
    &lt;head rend="h3"&gt;Test in emulator&lt;/head&gt;
    &lt;p&gt;Itâs good to test before wasting time for the real floppy to burn.&lt;/p&gt;
    &lt;p&gt;Boot the new OS in qemu:&lt;/p&gt;
    &lt;code&gt;qemu-system-i386 -fda floppinux.img -m 20M -cpu 486&lt;/code&gt;
    &lt;p&gt;If it worked that means You have successfully created your own distribution! Congratulations!&lt;/p&gt;
    &lt;p&gt;The floppinux.img image is ready to burn onto a floppy and boot on real hardware!&lt;/p&gt;
    &lt;head rend="h2"&gt;Floppy Disk&lt;/head&gt;
    &lt;head rend="h3"&gt;&amp;lt;!&amp;gt; Important &amp;lt;!&amp;gt;&lt;/head&gt;
    &lt;p&gt;Change XXX to floppy drive name in your system. In my case it is sdb. Choosing wrongly will NUKE YOUR PARTITION and REMOVE all of your files! Think twice. Or use some GUI application for that.&lt;/p&gt;
    &lt;code&gt;sudo dd if=floppinux.img of=/dev/XXX bs=512 conv=notrunc,sync,fsync oflag=direct status=progress&lt;/code&gt;
    &lt;p&gt;After 5 minutes I got freshly burned floppy.&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;FLOPPINUX: 0.3.1 (December 2025)&lt;/item&gt;
      &lt;item&gt;Linux Kernel: 6.14.11&lt;/item&gt;
      &lt;item&gt;Busybox: 1.36.1&lt;/item&gt;
      &lt;item&gt;Image size: 1440KiB / 1.44MiB&lt;/item&gt;
      &lt;item&gt;Kernel size: 881KiB (bzImage)&lt;/item&gt;
      &lt;item&gt;Tools: 137KiB (rootfs.cpio.xz)&lt;/item&gt;
      &lt;item&gt;Free space left (df -h): 253KiB&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;System Tools&lt;/head&gt;
    &lt;head rend="h4"&gt;File &amp;amp; Directory Manipulation&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;cat&lt;/code&gt;- display file contents&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;cp&lt;/code&gt;- copy files and directories&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;mv&lt;/code&gt;- move/rename files and directories&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;rm&lt;/code&gt;- remove files and directories&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ls&lt;/code&gt;- list directory contents&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;mkdir&lt;/code&gt;- creates directory&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;System Information &amp;amp; Management&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;df -h&lt;/code&gt;- display filesystem disk space usage&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sync&lt;/code&gt;- force write of buffered data to disk - use this after any changes to the floppy filesystem&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;mount&lt;/code&gt;- mount filesystems&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;umount&lt;/code&gt;- unmount filesystems&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Text Processing &amp;amp; Output&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;echo&lt;/code&gt;- display text output&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;more&lt;/code&gt;- page through text output&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Utilities&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;clear&lt;/code&gt;- clear terminal screen&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;test&lt;/code&gt;- evaluate conditional expressions&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Applications&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;vi&lt;/code&gt;- text editor&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46866544</guid><pubDate>Tue, 03 Feb 2026 04:33:25 +0000</pubDate></item></channel></rss>