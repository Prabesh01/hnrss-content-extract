<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 09 Dec 2025 23:35:18 +0000</lastBuildDate><item><title>My favourite small hash table</title><link>https://www.corsix.org/content/my-favourite-small-hash-table</link><description>&lt;doc fingerprint="ead9f4510877030b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;My favourite small hash table&lt;/head&gt;
    &lt;p&gt;I'm the kind of person who thinks about the design and implementation of hash tables. One design which I find particularly cute, and I think deserves a bit more publicity, is Robin Hood open-addressing with linear probing and power-of-two table size. If you're not familiar with hash table terminology, that might look like a smorgasbord of random words, but it should become clearer as we look at some actual code.&lt;/p&gt;
    &lt;p&gt;To keep the code simple to start with, I'm going to assume:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Keys are randomly-distributed 32-bit integers.&lt;/item&gt;
      &lt;item&gt;Values are also 32-bit integers.&lt;/item&gt;
      &lt;item&gt;If the key &lt;code&gt;0&lt;/code&gt;is present, its value is not&lt;code&gt;0&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The table occupies at most 32 GiB of memory.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each slot in the table is either empty, or holds a key and a value. The combination of properties (1) and (2) allows a key/value pair to be stored as a 64-bit integer, and property (3) means that the 64-bit value &lt;code&gt;0&lt;/code&gt; can be used to represent an empty slot (some hash table designs also need a special value for representing tombstones, but this design doesn't need tombstones). Combining a key and a value into 64 bits couldn't be easier: the low 32 bits hold the key, and the high 32 bits hold the value.&lt;/p&gt;
    &lt;p&gt;The structure for the table itself needs a pointer to the array of slots, the length of said array, and the number of non-empty slots. As the length is always a power of two, it's more useful to store &lt;code&gt;length - 1&lt;/code&gt; instead of &lt;code&gt;length&lt;/code&gt;, which leads to &lt;code&gt;mask&lt;/code&gt; rather than &lt;code&gt;length&lt;/code&gt;, and property (4) means that &lt;code&gt;mask&lt;/code&gt; can be stored as 32 bits. As the load factor should be less than 100%, we can assume &lt;code&gt;count &amp;lt; length&lt;/code&gt;, and hence &lt;code&gt;count&lt;/code&gt; can also be 32 bits. This leads to a mundane-looking:&lt;/p&gt;
    &lt;code&gt;struct hash_table_t {
  uint64_t* slots;
  uint32_t mask;
  uint32_t count;
};
&lt;/code&gt;
    &lt;p&gt;Property (1) means that we don't need to hash keys, as they're already randomly distributed. Every possible key &lt;code&gt;K&lt;/code&gt; has a "natural position" in the slots array, which is just &lt;code&gt;K &amp;amp; mask&lt;/code&gt;. If there are collisions, the slot in which a key actually ends up might be different to its natural position. The "linear probing" part of the design means that if &lt;code&gt;K&lt;/code&gt; cannot be in its natural position, the next slot to be considered is &lt;code&gt;(K + 1) &amp;amp; mask&lt;/code&gt;, and if not that slot then &lt;code&gt;(K + 2) &amp;amp; mask&lt;/code&gt;, then &lt;code&gt;(K + 3) &amp;amp; mask&lt;/code&gt;, and so on. This leads to the definition of a "chain": if &lt;code&gt;K&lt;/code&gt; is some key present in the table, &lt;code&gt;CK&lt;/code&gt; denotes the sequence of slots starting with &lt;code&gt;K&lt;/code&gt;'s natural position and ending with &lt;code&gt;K&lt;/code&gt;'s actual position. We have the usual property of open-addressing: none of the slots in &lt;code&gt;CK&lt;/code&gt; are empty slots. The "Robin Hood" part of the design then imposes an additional rather interesting property: for each slot &lt;code&gt;S&lt;/code&gt; in &lt;code&gt;CK&lt;/code&gt;, &lt;code&gt;Score(S.Index, S.Key) √¢¬• Score(S.Index, K)&lt;/code&gt;, where:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;S.Index&lt;/code&gt;is the index of&lt;code&gt;S&lt;/code&gt;in the&lt;code&gt;slots&lt;/code&gt;array (not the index of it in&lt;code&gt;CK&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;S.Key&lt;/code&gt;is the key present in slot&lt;code&gt;S&lt;/code&gt;(i.e. the low 32 bits of&lt;code&gt;slots[S.Index]&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Score(Index, Key)&lt;/code&gt;is&lt;code&gt;(Index - Key) &amp;amp; mask&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These properties give us the termination conditions for the lookup algorithm: for a possible key &lt;code&gt;K&lt;/code&gt;, we look at each slot starting from &lt;code&gt;K&lt;/code&gt;'s natural position, and either we find &lt;code&gt;K&lt;/code&gt;, or we find an empty slot, or we find a slot with &lt;code&gt;Score(S.Index, S.Key) &amp;lt; Score(S.Index, K)&lt;/code&gt;. In either of the latter two cases, &lt;code&gt;K&lt;/code&gt; cannot have been present in the table. In the function below, &lt;code&gt;Score(S.Index, K)&lt;/code&gt; is tracked as &lt;code&gt;d&lt;/code&gt;. In a language with a modern type system, the result of a lookup would be &lt;code&gt;Optional&amp;lt;Value&amp;gt;&lt;/code&gt;, but if sticking to plain C, property (3) can be used to make something similar: the 64-bit result is zero if the key is absent, and otherwise the value is in the low 32 bits of the result (which may themselves be zero, but the full 64-bit result will be non-zero). The logic is thus:&lt;/p&gt;
    &lt;code&gt;uint64_t table_lookup(hash_table_t* table, uint32_t key) {
  uint32_t mask = table-&amp;gt;mask;
  uint64_t* slots = table-&amp;gt;slots;
  for (uint32_t d = 0;; ++d) {
    uint32_t idx = (key + d) &amp;amp; mask;
    uint64_t slot = slots[idx];
    if (slot == 0) {
      return 0;
    } else if (key == (uint32_t)slot) {
      return (slot &amp;gt;&amp;gt; 32) | (slot &amp;lt;&amp;lt; 32);
    } else if (((idx - (uint32_t)slot) &amp;amp; mask) &amp;lt; d) {
      return 0;
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;If using a rich 64-bit CPU architecture, many of the expressions in the above function are cheaper than they might initially seem:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;slots[idx]&lt;/code&gt;involves zero-extending&lt;code&gt;idx&lt;/code&gt;from 32 bits to 64, multiplying it by&lt;code&gt;sizeof(uint64_t)&lt;/code&gt;, adding it to&lt;code&gt;slots&lt;/code&gt;, and then loading from that address. All this is a single instruction on x86-64 or arm64.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;key == (uint32_t)slot&lt;/code&gt;involves a comparison using the low 32 bits of a 64-bit register, which is a completely standard operation on x86-64 or arm64.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;(slot &amp;gt;&amp;gt; 32) | (slot &amp;lt;&amp;lt; 32)&lt;/code&gt;is a rotation by 32 bits, which again is a single instruction on x86-64 or arm64.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On the other hand, if using riscv64, things are less good:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If the &lt;code&gt;Zba&lt;/code&gt;extension is present,&lt;code&gt;sh3add.uw&lt;/code&gt;is a single instruction for zero-extending&lt;code&gt;idx&lt;/code&gt;from 32 bits to 64, multiplying it by&lt;code&gt;sizeof(uint64_t)&lt;/code&gt;, and adding it to&lt;code&gt;slots&lt;/code&gt;. If not, each step is a separate instruction, though the zero-extension can be eliminated with a slight reformulation to encourage the compiler to fold the zero-extension onto the load of&lt;code&gt;table-&amp;gt;mask&lt;/code&gt;(as riscv64 usually defaults to making sign-extension free, in contrast to x86-64/arm64 which usually make zero-extension free). Regardless, the load is always its own instruction.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;key == (uint32_t)slot&lt;/code&gt;hits a gap in the riscv64 ISA: it doesn't have any 32-bit comparison instructions, so this either becomes a 32-bit subtraction followed by a 64-bit comparison against zero, or promotion of both operands from 32 bits to 64 bits followed by a 64-bit comparison.&lt;/item&gt;
      &lt;item&gt;If the &lt;code&gt;Zbb&lt;/code&gt;extension is present, rotations are a single instruction. If not, they're three instructions, and so it becomes almost worth reworking the slot layout to put the key in the high 32 bits and the value in the low 32 bits.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Moving on from lookup to insertion, there are various different options for what to do when the key being inserted is already present. I'm choosing to show a variant which returns the old value (in the same form as &lt;code&gt;table_lookup&lt;/code&gt; returns) and then overwrites with the new value, though other variants are obviously possible. The logic follows the same overall structure as seen in &lt;code&gt;table_lookup&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;uint64_t table_set(hash_table_t* table, uint32_t key, uint32_t val) {
  uint32_t mask = table-&amp;gt;mask;
  uint64_t* slots = table-&amp;gt;slots;
  uint64_t kv = key + ((uint64_t)val &amp;lt;&amp;lt; 32);
  for (uint32_t d = 0;; ++d) {
    uint32_t idx = ((uint32_t)kv + d) &amp;amp; mask;
    uint64_t slot = slots[idx];
    if (slot == 0) {
      // Inserting new value (and slot was previously empty)
      slots[idx] = kv;
      break;
    } else if ((uint32_t)kv == (uint32_t)slot) {
      // Overwriting existing value
      slots[idx] = kv;
      return (slot &amp;gt;&amp;gt; 32) | (slot &amp;lt;&amp;lt; 32);
    } else {
      uint32_t d2 = (idx - (uint32_t)slot) &amp;amp; mask;
      if (d2 &amp;lt; d) {
        // Inserting new value, and moving existing slot
        slots[idx] = kv;
        table_reinsert(slots, mask, slot, d2);
        break;
      }
    }
  }
  if (++table-&amp;gt;count * 4ull &amp;gt;= mask * 3ull) {
    // Expand table once we hit 75% load factor
    table_rehash(table);
  }
  return 0;
}
&lt;/code&gt;
    &lt;p&gt;To avoid the load factor becoming too high, the above function will sometimes grow the table by calling this helper function:&lt;/p&gt;
    &lt;code&gt;void table_rehash(hash_table_t* table) {
  uint32_t old_mask = table-&amp;gt;mask;
  uint32_t new_mask = old_mask * 2u + 1u;
  uint64_t* new_slots = calloc(new_mask + 1ull, sizeof(uint64_t));
  uint64_t* old_slots = table-&amp;gt;slots;
  uint32_t idx = 0;
  do {
    uint64_t slot = old_slots[idx];
    if (slot != 0) {
      table_reinsert(new_slots, new_mask, slot, 0);
    }
  } while (idx++ != old_mask);
  table-&amp;gt;slots = new_slots;
  table-&amp;gt;mask = new_mask;
  free(old_slots);
}
&lt;/code&gt;
    &lt;p&gt;Both of &lt;code&gt;table_set&lt;/code&gt; and &lt;code&gt;table_rehash&lt;/code&gt; make use of a helper function which is very similar to &lt;code&gt;table_set&lt;/code&gt;, but doesn't need to check for overwriting an existing key and also doesn't need to update &lt;code&gt;count&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;void table_reinsert(uint64_t* slots, uint32_t mask, uint64_t kv, uint32_t d) {
  for (;; ++d) {
    uint32_t idx = ((uint32_t)kv + d) &amp;amp; mask;
    uint64_t slot = slots[idx];
    if (slot == 0) {
      slots[idx] = kv;
      break;
    } else {
      uint32_t d2 = (idx - (uint32_t)slot) &amp;amp; mask;
      if (d2 &amp;lt; d) {
        slots[idx] = kv;
        kv = slot;
        d = d2;
      }
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;That covers lookup and insertion, so next up is key removal. As already hinted at, this hash table design doesn't need tombstones. Instead, removing a key involves finding the slot containing that key and then shifting slots left until finding an empty slot or a slot with &lt;code&gt;Score(S.Index, S.Key) == 0&lt;/code&gt;. This removal strategy works due to a neat pair of emergent properties:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If slot &lt;code&gt;S&lt;/code&gt;has&lt;code&gt;Score(S.Index, S.Key) != 0&lt;/code&gt;, it is viable for&lt;code&gt;S.Key&lt;/code&gt;to instead be at&lt;code&gt;(S.Index - 1) &amp;amp; mask&lt;/code&gt;(possibly subject to additional re-arranging to fill the gap formed by moving&lt;code&gt;S.Key&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;If slot &lt;code&gt;S&lt;/code&gt;has&lt;code&gt;Score(S.Index, S.Key) == 0&lt;/code&gt;, and&lt;code&gt;S&lt;/code&gt;is part of some chain&lt;code&gt;CK&lt;/code&gt;, then&lt;code&gt;S&lt;/code&gt;is at the very start of&lt;code&gt;CK&lt;/code&gt;. Hence it is viable to turn&lt;code&gt;(S.Index - 1) &amp;amp; mask&lt;/code&gt;into an empty slot without breaking any chains.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This leads to the tombstone-free removal function, which follows the established pattern of returning either the old value or zero:&lt;/p&gt;
    &lt;code&gt;uint64_t table_remove(hash_table_t* table, uint32_t key) {
  uint32_t mask = table-&amp;gt;mask;
  uint64_t* slots = table-&amp;gt;slots;
  for (uint32_t d = 0;; ++d) {
    uint32_t idx = (key + d) &amp;amp; mask;
    uint64_t slot = slots[idx];
    if (slot == 0) {
      return 0;
    } else if (key == (uint32_t)slot) {
      uint32_t nxt = (idx + 1) &amp;amp; mask;
      --table-&amp;gt;count;
      while (slots[nxt] &amp;amp;&amp;amp; ((slots[nxt] ^ nxt) &amp;amp; mask)) {
        slots[idx] = slots[nxt];
        idx = nxt;
        nxt = (idx + 1) &amp;amp; mask;
      }
      slots[idx] = 0;
      return (slot &amp;gt;&amp;gt; 32) | (slot &amp;lt;&amp;lt; 32);
    } else if (((idx - (uint32_t)slot) &amp;amp; mask) &amp;lt; d) {
      return 0;
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;The final interesting hash table operation is iterating over all keys and values, which is just an array iteration combined with filtering out zeroes:&lt;/p&gt;
    &lt;code&gt;void table_iterate(hash_table_t* table, void(*visit)(uint32_t key, uint32_t val)) {
  uint64_t* slots = table-&amp;gt;slots;
  uint32_t mask = table-&amp;gt;mask;
  uint32_t idx = 0;
  do {
    uint64_t slot = slots[idx];
    if (slot != 0) {
      visit((uint32_t)slot, (uint32_t)(slot &amp;gt;&amp;gt; 32));
    }
  } while (idx++ != mask);
}
&lt;/code&gt;
    &lt;p&gt;That wraps up the core concepts of this hash table, so now it is time to revisit some of the initial simplifications.&lt;/p&gt;
    &lt;p&gt;If keys are 32-bit integers but are not randomly-distributed, then we just need an invertible hash function from 32 bits to 32 bits, the purpose of which is to take keys following ~any real-world pattern and emit a ~random pattern. The &lt;code&gt;table_lookup&lt;/code&gt;, &lt;code&gt;table_set&lt;/code&gt;, and &lt;code&gt;table_remove&lt;/code&gt; functions gain &lt;code&gt;key = hash(key)&lt;/code&gt; at the very start but are otherwise unmodified (noting that if the hash function is invertible, hash equality implies key equality, hence no need to explicitly check key equality), and &lt;code&gt;table_iterate&lt;/code&gt; is modified to apply the inverse function before calling &lt;code&gt;visit&lt;/code&gt;. If hardware CRC32 / CRC32C instructions are present (as is the case on sufficiently modern x86-64 and arm64 chips), these can be used for the task, although their inverses are annoying to compute, so perhaps not ideal if iteration is an important operation. If CRC32 isn't viable, one option out of many is:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;If keys and values are larger than 32 bits, then the design can be augmented with a separate array of key/value pairs, with the design as shown containing a 32-bit hash of the key and the array index of the key/value pair. To meet property (3) in this case, either the hash function can be chosen to never be zero, or "array index plus one" can be stored rather than "array index". It is not possible to make the hash function invertible in this case, so &lt;code&gt;table_lookup&lt;/code&gt;, &lt;code&gt;table_set&lt;/code&gt;, and &lt;code&gt;table_remove&lt;/code&gt; do need extending to check for key equality after confirming hash equality. Iteration involves walking the separate array of key/value pairs rather than the hash structure, which has the added benefit of iteration order being related to insertion order rather than hash order. As another twist on this, if keys and values are variably-sized, then the design can instead be augmented with a separate array of bytes, with key/value pairs serialised somewhere in that array, and the hash structure containing a 32-bit hash of the key and the byte offset (within the array) of the key/value pair.&lt;/p&gt;
    &lt;p&gt;Of course, a design can only stretch so far. If you're after a concurrent lock-free hash table, look elsewhere. If you can rely on 128-bit SIMD instructions being present, you might instead want to group together every 16 key/value pairs, keep an 8-bit hash of each key, and rely on SIMD to perform 16 hash comparisons in parallel. If you're building hardware rather than software, it can be appealing to have multiple hash functions, each one addressing its own SRAM bank. There is no one-size-fits-all hash table, but I've found the one shown here to be good for a lot of what I do.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46205461</guid><pubDate>Tue, 09 Dec 2025 14:47:20 +0000</pubDate></item><item><title>Kaiju ‚Äì General purpose 3D/2D game engine in Go and Vulkan with built in editor</title><link>https://github.com/KaijuEngine/kaiju</link><description>&lt;doc fingerprint="d71a40118926321e"&gt;
  &lt;main&gt;
    &lt;p&gt;Kaiju is a 2D/3D game engine written in Go (Golang) backed by Vulkan. The goal of the engine is to use a modern, easy, systems level programming language, with a focus on simplicity, to create a new kind of game engine.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üìÑ 2D / üßä 3D Game Engine&lt;/item&gt;
      &lt;item&gt;ü™ü Windows&lt;/item&gt;
      &lt;item&gt;üêß Linux&lt;/item&gt;
      &lt;item&gt;ü§ñ Android (NEW, support now functional)&lt;/item&gt;
      &lt;item&gt;üçé Mac (support is currently WIP)&lt;/item&gt;
      &lt;item&gt;ü§ñüëâ‚å®Ô∏è Local AI (LLM) interop&lt;/item&gt;
      &lt;item&gt;&lt;g-emoji&gt;‚ö†Ô∏è&lt;/g-emoji&gt;üößüèóÔ∏èüë∑‚ôÇÔ∏è Work in progress, under heavy development&lt;/item&gt;
      &lt;item&gt;üöö Faster builds than other game engines&lt;/item&gt;
      &lt;item&gt;üî• Better performance than other game engines (9x faster than Unity out of the box)&lt;/item&gt;
      &lt;item&gt;üíæ Less memory than other engines&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GitHub repository&lt;/item&gt;
      &lt;item&gt;Mailing list &amp;lt;- Recommended for detailed updates&lt;/item&gt;
      &lt;item&gt;Discord server&lt;/item&gt;
      &lt;item&gt;Brent Farris on X/Twitter&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The current version of the base engine renders extremely fast, faster than most would think a garbage collected language could go. In my testing a release mode build of a game in Unity with nothing but a black background and a cube runs at about 1,600 FPS. In Kaiju, the same thing runs at around 5,400 FPS on the same machine. In fact, a complete game, with audio, custom cursors, real time PBR rendering with real time shadows, UI, and more runs at 2,712 FPS (in "debug" mode) screenshots or it didn't happen.&lt;/p&gt;
    &lt;p&gt;I love C, and because I love C and found out that Ken Thompson played a part in designing Go, I gave Go a chance. It has been such a joy to use and work with I decided to port my C game engine to Go. Go is a modern system-level language that allows me to write code the way I want to write code and even have the opportunity to do some crazy things if I want to (no strings attached). Also the simplicity and "just works" of writing Assembly code was a great boost to my happiness.&lt;/p&gt;
    &lt;p&gt;What's more, it's a language that other developers can easily learn and jump right into extending the engine/editor. No need for developers to re-figure out some bespoke macros or crazy templating nonsense. It's flat, easy, straight forward, and the foot-gun is hidden behind some walls, but there if you want it. Furthermore, developers can write their games in Go directly, no need for some alternative language that is different from the engine code (but we'll include Lua for modding).&lt;/p&gt;
    &lt;p&gt;I am creating this section because I get asked about it when I mention "Go", possibly not realizing that most public game engines use a garbage collector (GC).&lt;/p&gt;
    &lt;p&gt;The GC is actually a feature I'm happy with (shocker coming from a C guy). Well, the reason is simple, if you're going to make a game engine that the public will use and needs to be stable, you need a garbage collector. Unity has C# (and possibly an internal GC as well), Unreal has a GC (and it could use a tune up if you ask me), Godot has a GC albeit their scripting language or when you use C#. It is actually very important for public engines to have a GC because people are only human and make a lot of mistakes, mistakes they'll blame on you (the engine developer) before they blame themselves.&lt;/p&gt;
    &lt;p&gt;Coincidentally, the overall design I have for the engine plays very well with the GC and last I measured, I have a net-0 heap allocation while running (may need a new review). If you don't abuse the GC, you shouldn't generally feel it, it runs concurrently as well.&lt;/p&gt;
    &lt;p&gt;I'll be the first to admit, I think the developers of Go can create a better GC than I can, and probably better than Unreal and Unity too.&lt;/p&gt;
    &lt;p&gt;Though the engine is production ready, the editor is not, feel free to join and contribute to its development.&lt;/p&gt;
    &lt;p&gt;For the latest updates, please join the Discord or check my Twitter/X.&lt;/p&gt;
    &lt;p&gt;Please review the Ad-Hoc editor readme&lt;/p&gt;
    &lt;p&gt;Please see the documentation on how to get started and compile the engine&lt;/p&gt;
    &lt;p&gt;(YouTube) Compatibility requirements video for Mac&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46205519</guid><pubDate>Tue, 09 Dec 2025 14:51:46 +0000</pubDate></item><item><title>Show HN: Gemini Pro 3 hallucinates the HN front page 10 years from now</title><link>https://dosaygo-studio.github.io/hn-front-page-2035/news</link><description>&lt;doc fingerprint="a461ad970e1c7920"&gt;
  &lt;main&gt;
    &lt;p&gt;Y Hacker News new | past | comments | ask | show | jobs | submit login 1. First successful telemetry from Starship HLS-9 on the Sea of Tranquility ( spacex.com ) 894 points by muskwatch 4 hours ago | hide | 312 comments 2. A 100% Rust kernel is now upstream in Linux 7.4 ( kernel.org ) 402 points by rust_evangelist 6 hours ago | hide | 156 comments 3. Why I still write raw code instead of prompting the compiler ( nostalgic-coder.io ) 128 points by oldtimer99 3 hours ago | hide | 89 comments 4. Running LLaMA-12 7B on a contact lens with WASM ( arxiv.org ) 67 points by edge_compute 2 hours ago | hide | 14 comments 5. Show HN: AlgoDrill ‚Äì Interactive drills to stop forgetting LeetCode patterns ( algodrill.io ) 243 points by persistence_is_key 5 hours ago | hide | 98 comments 6. ITER achieves net positive energy for 20 consecutive minutes ( nature.com ) 1205 points by physics_lover 12 hours ago | hide | 402 comments 7. Restoring a 2024 Framework Laptop: A retrospective ( ifixit.com ) 56 points by retro_fix 4 hours ago | hide | 22 comments 8. Google kills Gemini Cloud Services ( killedbygoogle.com ) 530 points by dang_fan 15 hours ago | hide | 330 comments 9. Visualizing the 5th dimension with WebGPU 2.0 ( graphics-shader.net ) 88 points by webgl_wizard 7 hours ago | hide | 12 comments 10. Launch HN: Nia (YC W36) ‚Äì Give context to autonomous coding agents ( trynia.ai ) 112 points by founder_jane 10 hours ago | hide | 45 comments 11. Debian 18 "Trixie" released ( debian.org ) 312 points by apt_get 14 hours ago | hide | 78 comments 12. Is it time to rewrite sudo in Zig? ( github.com ) 45 points by ziggy42 3 hours ago | hide | 60 comments 13. EU passes "Right to Human Verification" Act ( europa.eu ) 670 points by policy_wonk 1 day ago | hide | 290 comments 14. Reverse Engineering the Neuralink V4 Bluetooth Protocol ( brain-hacks.org ) 220 points by cyborg_sec 8 hours ago | hide | 55 comments 15. Post-Silicon Computing: An Intro to Photonic Circuits ( mit.edu ) 99 points by lightspeed 6 hours ago | hide | 18 comments 16. FDA approves over-the-counter CRISPR for lactose intolerance ( fda.gov ) 415 points by bio_hacker 16 hours ago | hide | 211 comments 17. SQLite 4.0 Release Notes ( sqlite.org ) 800 points by drh 20 hours ago | hide | 140 comments 18. Ask HN: How do you prevent ad-injection in AR glasses? 320 points by glasshole2 11 hours ago | hide | 102 comments 19. Jepsen: NATS 4.2 (Still losing messages?) ( jepsen.io ) 88 points by aphyr_bot 9 hours ago | hide | 33 comments 20. Playing GTA VI on a RISC-V Cluster ( youtube.com ) 45 points by tlyleung 2 hours ago | hide | 16 comments 21. Why functional programming is the future (again) ( haskell.org ) 102 points by monad_lover 7 hours ago | hide | 65 comments 22. Microsoft Office 365 prices increase to $40/user/month ( officewatch.com ) 900 points by taubek 1 day ago | hide | 600 comments 23. Emulating Windows 10 in the browser ( bellard.org ) 341 points by qemu_fan 19 hours ago | hide | 50 comments 24. Let's put Tailscale on a SpaceX Starlink Dish ( tailscale.com ) 250 points by net_hacker 20 hours ago | hide | 45 comments 25. Manual: Deep Fakes detection for Seniors ( aarp.org ) 122 points by concerned_grandson 21 hours ago | hide | 77 comments 26. IBM to acquire OpenAI (Rumor) ( bloomberg.com ) 120 points by stock_watcher 1 day ago | hide | 338 comments 27. The unexpected return of server-side rendering ( htmx.org ) 147 points by bikenaga 19 hours ago | hide | 48 comments 28. How to build a Faraday Cage for your bedroom ( privacy-first.com ) 267 points by tinfoil_hat 22 hours ago | hide | 49 comments 29. AI progress is stalling. Human equivalence was a mirage ( garymarcus.com ) 485 points by skeptic_ai 14 hours ago | hide | 416 comments 30. Show HN: A text editor that doesn't use AI ( github.com ) 270 points by pure_coder 22 hours ago | hide | 105 comments More Guidelines | FAQ | Lists | API | Security | Legal | Apply to YC | Contact Search:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46205632</guid><pubDate>Tue, 09 Dec 2025 15:00:38 +0000</pubDate></item><item><title>Pebble Index 01 ‚Äì External memory for your brain</title><link>https://repebble.com/blog/meet-pebble-index-01-external-memory-for-your-brain</link><description>&lt;doc fingerprint="5c6b48dfd9872b65"&gt;
  &lt;main&gt;
    &lt;p&gt;Catch your best ideas before they slip through your fingers&lt;/p&gt;
    &lt;p&gt;Do you ever have flashes of insight or an idea worth remembering? This happens to me 5-10 times every day. If I don‚Äôt write down the thought immediately, it slips out of my mind. Worst of all, I remember that I‚Äôve forgotten something and spend the next 10 minutes trying to remember what it is. So I invented external memory for my brain.&lt;/p&gt;
    &lt;p&gt;Introducing Pebble Index 01 - a small ring with a button and microphone. Hold the button, whisper your thought, and it‚Äôs sent to your phone. It‚Äôs added to your notes, set as a reminder, or saved for later review.&lt;/p&gt;
    &lt;p&gt;Index 01 is designed to become muscle memory, since it‚Äôs always with you. It‚Äôs private by design (no recording until you press the button) and requires no internet connection or paid subscription. It‚Äôs as small as a wedding band and comes in 3 colours. It‚Äôs made from durable stainless steel and is water-resistant. Like all Pebble products, it‚Äôs extremely customizable and built with open source software.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs the best part: the battery lasts for years. You never need to charge it.&lt;/p&gt;
    &lt;p&gt;Pre-order today for $75. After worldwide shipping begins in March 2026, the price will go up to $99.&lt;/p&gt;
    &lt;head rend="h3"&gt;#Design&lt;/head&gt;
    &lt;p&gt;Now that I‚Äôve worn my Index 01 for several months, I can safely say that it has changed my life - just like with Pebble, I couldn‚Äôt go back to a world without this. There are so many situations each day where my hands are full (while biking or driving, washing dishes, wrangling my kids, etc) and I need to remember something. A random sampling of my recent recordings:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set a timer for 3pm to go pick up the kids&lt;/item&gt;
      &lt;item&gt;Remind me to phone the pharmacy at 11am&lt;/item&gt;
      &lt;item&gt;Peter is coming by tomorrow at 11:30am, add that to my calendar&lt;/item&gt;
      &lt;item&gt;Jerry recommends reading Breakneck&lt;/item&gt;
      &lt;item&gt;Mark wants a Black/Red PT2&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Before, I would take my phone out of my pocket to jot these down, but I couldn‚Äôt always do that (eg, while bicycling). I also wanted to start using my phone less, especially in front of my kids.&lt;/p&gt;
    &lt;p&gt;Initially, we experimented by building this as an app on Pebble, since it has a mic and I‚Äôm always wearing one. But, I realized quickly that this was suboptimal - it required me to use my other hand to press the button to start recording (lift-to-wake gestures and wake-words are too unreliable). This was tough to use while bicycling or carrying stuff.&lt;/p&gt;
    &lt;p&gt;Then a genius electrical engineer friend of mine came up with an idea to fit everything into a tiny ring. It is the perfect form factor! Honestly, I‚Äôm still amazed that it all fits.&lt;/p&gt;
    &lt;p&gt;The design needed to satisfy several critical conditions:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Must work reliably 100% of the time. If it didn‚Äôt work or failed to record a thought, I knew I would take it off and revert back to my old habit of just forgetting things.&lt;/item&gt;
      &lt;item&gt;It had to have a physical press-button, with a satisfying click-feel. I want to know for sure if the button is pressed and my thought is captured.&lt;/item&gt;
      &lt;item&gt;Long battery life - every time you take something off to charge, there‚Äôs a chance you‚Äôll forget to put it back on.&lt;/item&gt;
      &lt;item&gt;Must be privacy-preserving. These are your inner thoughts. All recordings must be processed and stored on your phone. Only record when the button is pressed.&lt;/item&gt;
      &lt;item&gt;It had to be as small as a wedding band. Since it‚Äôs worn on the index finger, if it were too large or bulky, it would hit your phone while you held it in your hand.&lt;/item&gt;
      &lt;item&gt;Water resistance - must be able to wash hands, shower, and get wet.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We‚Äôve been working on this for a while, testing new versions and making tweaks. We‚Äôre really excited to get this out into the world.&lt;/p&gt;
    &lt;p&gt;Here are a few of my favourite things about Index 01:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It does one thing really well - it helps me remember things.&lt;/item&gt;
      &lt;item&gt;It‚Äôs discreet. It's not distracting. It doesn't take you out of the moment.&lt;/item&gt;
      &lt;item&gt;There‚Äôs no AI friend persona and it‚Äôs not always recording.&lt;/item&gt;
      &lt;item&gt;It‚Äôs inexpensive. We hope you try it and see if you like it as well!&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;#Key Details&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Available in 3 colours and 8 sizes &lt;list rend="ul"&gt;&lt;item&gt;Colours: polished silver, polished gold, and matte black&lt;/item&gt;&lt;item&gt;US ring sizes: 6, 7, 8, 9, 10, 11, 12, 13&lt;/item&gt;&lt;item&gt;You can pre-order now and pick your size/colour later before your ring ships.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Cost and availability: Pre-order price is $75, rises to $99 later. Ships worldwide, beginning in March.&lt;/item&gt;
      &lt;item&gt;Works with iPhone and Android: We overcame Apple‚Äôs best efforts to make life terrible for 3rd party accessory makers and have Index 01 working well on iOS and Android.&lt;/item&gt;
      &lt;item&gt;Extremely private and secure: Your thoughts are processed by open source speech-to-text (STT) and AI models locally on your phone. You can read the code and see exactly how it works - our Pebble mobile app is open source. Higher-quality STT is available through an optional cloud service.&lt;/item&gt;
      &lt;item&gt;No charging: The battery lasts for up to years of average use. After the end of its life, send your ring back to us for recycling.&lt;/item&gt;
      &lt;item&gt;On-ring storage: Recording works even if your phone is out of range. Up to 5 minutes of audio can be stored on-ring, then synced later.&lt;/item&gt;
      &lt;item&gt;No speaker or vibrating motor: This is an input device only. There is an RGB LED, but it‚Äôs rarely used (to save battery life and to reduce distraction).&lt;/item&gt;
      &lt;item&gt;Works great with Pebble or other smartwatches: After recording, the thought will appear on your watch, and you can check that it‚Äôs correct. You can ask questions like ‚ÄòWhat‚Äôs the weather today?‚Äô and see the answer on your watch.&lt;/item&gt;
      &lt;item&gt;Raw audio playback: Very helpful if STT doesn‚Äôt work perfectly due to wind or loud background noises.&lt;/item&gt;
      &lt;item&gt;Actions: While the primary task is remembering things for you, you can also ask it to do things like ‚ÄôSend a Beeper message to my wife - running late‚Äô or answer simple questions that could be answered by searching the web. You can configure button clicks to control your music - I love using this to play/pause or skip tracks. You can also configure where to save your notes and reminders (I have it set to add to Notion).&lt;/item&gt;
      &lt;item&gt;Customizable and hackable: Configure single/double button clicks to control whatever you want (take a photo, turn on lights, Tasker, etc). Add your own voice actions via MCP. Or route the audio recordings directly to your own app or server!&lt;/item&gt;
      &lt;item&gt;99+ languages: Speech to text and local LLM support over 99 languages! Naturally, the quality of each may vary.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;#Future Plans&lt;/head&gt;
    &lt;p&gt;Let me be very clear - Index 01 is designed at its core to be a device that helps you remember things. We want it to be 100% reliable at its primary task. But we‚Äôre leaving the side door open for folks to customize, build new interactions and actions.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs how I‚Äôm thinking about it - a single click-hold + voice input will be routed to the primary memory processing path. Double-click-hold + voice input would be routed to a more general purpose voice agent (think ChatGPT with web search). Responses from the agent would be presented on Pebble (eg ‚ÄòWhat‚Äôs the weather tomorrow?‚Äô, ‚ÄòWhen‚Äôs the next northbound Caltrain?‚Äô) or other smartwatches (as a notification). Maybe this could even be an input for something like ChatGPT Voice Mode, enabling you to hear the AI response from your earbuds.&lt;/p&gt;
    &lt;p&gt;The built in actions, set reminder, create note, alarms, etc, are actually MCPs - basically mini apps that AI agents know how to operate. They run locally in WASM within the Pebble mobile app (no cloud MCP server required). Basically any MCP server can be used with the system, so intrepid folks may have fun adding various actions like Beeper, Google Calendar, weather, etc that already offer MCPs.&lt;/p&gt;
    &lt;p&gt;Not everything will be available at launch, but this is the direction we are working towards. There will be 3 ways to customize your Index 01:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Trigger actions via button clicks - configure a single or double click to do things like take a photo, control your Home Assistant smart home, Tasker function, unlock your car. This will work better on Android since iOS Shortcuts doesn‚Äôt have an open API.&lt;/item&gt;
      &lt;item&gt;Trigger actions via voice input - write an MCP to do‚Ä¶.basically anything? This is pretty open ended.&lt;/item&gt;
      &lt;item&gt;Route your voice recordings and/or transcriptions to your own webhook - or skip our AI processing entirely and send every recording to your own app or webapp.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How does it work?&lt;/p&gt;
    &lt;p&gt;People usually wear it on the index finger. Inside the ring is a button, a microphone, a Bluetooth chip, memory, and a battery that lasts for years. Click the button with your thumb, talk into the mic, and it records to internal memory. When your phone is in range, the recording is streamed to the Pebble app. It‚Äôs converted to text on-device, then processed by an on-device large language model (LLM) which selects an action to take (create note, add to reminders, etc).&lt;/p&gt;
    &lt;p&gt;When do I pick my size?&lt;/p&gt;
    &lt;p&gt;You‚Äôll be able to pick your ring size and color after placing a pre-order. If you have a 3D printer, you can print our CAD designs to try on. We‚Äôre also planning a sizing kit. You can view the measurements of the inner diameter of each ring size.&lt;/p&gt;
    &lt;p&gt;How long does the battery last?&lt;/p&gt;
    &lt;p&gt;Roughly 12 to 15 hours of recording. On average, I use it 10-20 times per day to record 3-6 second thoughts. That‚Äôs up to 2 years of usage.&lt;/p&gt;
    &lt;p&gt;Is it secure and private?&lt;/p&gt;
    &lt;p&gt;Yes, extremely. The connection between ring and phone is encrypted. Recordings are processed locally on your phone in the open-source Pebble app. The app works offline (no internet connection) and does not require a cloud service. An optional cloud storage system for backing up recordings is available. Our plan is for this to be optionally encrypted, but we haven‚Äôt built it yet.&lt;/p&gt;
    &lt;p&gt;Is a paid subscription required?&lt;/p&gt;
    &lt;p&gt;No.&lt;/p&gt;
    &lt;p&gt;What kind of battery is inside?&lt;/p&gt;
    &lt;p&gt;Index 01 uses silver-oxide batteries.&lt;/p&gt;
    &lt;p&gt;Why can‚Äôt it be recharged?&lt;/p&gt;
    &lt;p&gt;We considered this but decided not to for several reasons:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;You‚Äôd probably lose the charger before the battery runs out!&lt;/item&gt;
      &lt;item&gt;Adding charge circuitry and including a charger would make the product larger and more expensive.&lt;/item&gt;
      &lt;item&gt;You send it back to us to recycle.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Wait, it‚Äôs single use?&lt;/p&gt;
    &lt;p&gt;Yes. We know this sounds a bit odd, but in this particular circumstance we believe it‚Äôs the best solution to the given set of constraints. Other smart rings like Oura cost $250+ and need to be charged every few days. We didn‚Äôt want to build a device like that. Before the battery runs out, the Pebble app notifies and asks if you‚Äôd like to order another ring.&lt;/p&gt;
    &lt;p&gt;Is it always listening?&lt;/p&gt;
    &lt;p&gt;No. It only records while the button is pressed. It‚Äôs not designed to record your whole life, or meetings.&lt;/p&gt;
    &lt;p&gt;What if the speech-to-text processing misses a word or something?&lt;/p&gt;
    &lt;p&gt;You can always listen to the each recording in the app.&lt;/p&gt;
    &lt;p&gt;Why no touchpad?&lt;/p&gt;
    &lt;p&gt;We experimented with a touchpad, but found it too easy to accidentally swipe and press. Also, nothing beats the feedback of a real gosh darn pressable button.&lt;/p&gt;
    &lt;p&gt;Is there a speaker or vibrating motor?&lt;/p&gt;
    &lt;p&gt;No. The button has a great click-feel to indicate when you are pressing.&lt;/p&gt;
    &lt;p&gt;Does it do health tracking like Oura?&lt;/p&gt;
    &lt;p&gt;Nope&lt;/p&gt;
    &lt;p&gt;How durable and water-resistant is it?&lt;/p&gt;
    &lt;p&gt;It‚Äôs primarily made from stainless steel 316, with a liquid silicone rubber (LSR) button. It‚Äôs water-resistant to 1 meter. You can wash your hands, do dishes, and shower with it on, but we don‚Äôt recommend swimming with it.&lt;/p&gt;
    &lt;p&gt;Does it work with iPhone and Android?&lt;/p&gt;
    &lt;p&gt;Yes&lt;/p&gt;
    &lt;p&gt;I love customizing and hacking on my devices. What could I do with Index 01?&lt;/p&gt;
    &lt;p&gt;Lots of stuff! Control things with the buttons. Route raw audio or transcribed text directly to your own app via webhook. Use MCPs (also run locally on-device! No cloud server required) to add more actions.&lt;/p&gt;
    &lt;p&gt;Is this an AI friend thingy or always-recording device?&lt;/p&gt;
    &lt;p&gt;No.&lt;/p&gt;
    &lt;p&gt;How far along is development?&lt;/p&gt;
    &lt;p&gt;We‚Äôve been working on this in the background to watch development. It helps that our Pebble Time 2 partner factory is also building Index 01! We‚Äôre currently in the DVT stage, testing pre-production samples. We‚Äôll start a wider alpha test in January with a lot more people. Here‚Äôs some shots from the pre-production assembly line:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46205661</guid><pubDate>Tue, 09 Dec 2025 15:03:09 +0000</pubDate></item><item><title>Apple's slow AI pace becomes a strength as market grows weary of spending</title><link>https://finance.yahoo.com/news/apple-slow-ai-pace-becomes-104658095.html</link><description>&lt;doc fingerprint="5f1d1981b3685bf"&gt;
  &lt;main&gt;
    &lt;p&gt;(Bloomberg) -- Shares of Apple Inc. were battered earlier this year as the iPhone maker faced repeated complaints about its lack of an artificial intelligence strategy. But as the AI trade faces increasing scrutiny, that hesitance has gone from a weakness to a strength ‚Äî and it‚Äôs showing up in the stock market.&lt;/p&gt;
    &lt;p&gt;Through the first six months of 2025, Apple was the second-worst performer among the Magnificent Seven tech giants, as its shares tumbled 18% through the end of June. That has reversed since then, with the stock soaring 35%, while AI darlings like Meta Platforms Inc. and Microsoft Corp. slid into the red and even Nvidia Corp. underperformed. The S&amp;amp;P 500 Index rose 10% in that time, and the tech-heavy Nasdaq 100 Index gained 13%.&lt;/p&gt;
    &lt;p&gt;Most Read from Bloomberg&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;NJ‚Äôs Montclair Cuts School Staff and Mulls March Tax-Hike Vote&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Trump Replaces Architect to Lead $300 Million Ballroom Design&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Owner of NYC‚Äôs Fordham Landing Housing Project Files Bankruptcy&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Democrats Want Probe of Trump Officials and Immigration Deals&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;‚ÄúIt is remarkable how they have kept their heads and are in control of spending, when all of their peers have gone the other direction,‚Äù said John Barr, portfolio manager of the Needham Aggressive Growth Fund, which owns Apple shares.&lt;/p&gt;
    &lt;p&gt;As a result, Apple now has a $4.1 trillion market capitalization and the second biggest weight in the S&amp;amp;P 500, leaping over Microsoft and closing in on Nvidia. The shift reflects the market‚Äôs questioning of the hundreds of billions of dollars Big Tech firms are throwing at AI development, as well as Apple‚Äôs positioning to eventually benefit when the technology is ready for mass use.&lt;/p&gt;
    &lt;p&gt;‚ÄúWhile they most certainly will incorporate more AI into the phones over time, Apple has avoided the AI arms race and the massive capex that accompanies it,‚Äù said Bill Stone, chief investment officer at Glenview Trust Company, who owns the stock and views it as ‚Äúa bit of an anti-AI holding.‚Äù&lt;/p&gt;
    &lt;p&gt;Of course, the rally has made Apple‚Äôs stock pricier than it has been in a long time. The shares are trading for around 33 times expected earnings over the next 12 months, a level they‚Äôve only hit a few times in the past 15 years, with a high of 35 in September 2020. The stock‚Äôs average multiple over that time is less than 19 times. Apple is now the second most expensive stock in the Bloomberg Magnificent Seven Index, trailing only Tesla Inc.‚Äôs whopping valuation of 203 times forward earnings. Apple‚Äôs shares climbed about 0.5% in early Tuesday trading.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt‚Äôs really hard to see how the stock can continue to compound value at a level that makes this a compelling entry point,‚Äù said Craig Moffett, co-founder of research firm MoffettNathanson. ‚ÄúThe obvious question is, are investors overpaying for Apple‚Äôs defensiveness? We think so.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46205724</guid><pubDate>Tue, 09 Dec 2025 15:08:24 +0000</pubDate></item><item><title>A supersonic engine core makes the perfect power turbine</title><link>https://boomsupersonic.com/flyby/ai-needs-more-power-than-the-grid-can-deliver-supersonic-tech-can-fix-that</link><description>&lt;doc fingerprint="16d2b440c19c9c2a"&gt;
  &lt;main&gt;
    &lt;p&gt;By: Blake Scholl, Founder &amp;amp; CEO, Boom Supersonic&lt;/p&gt;
    &lt;p&gt;It started, as many things do these days, by scrolling on X.&lt;/p&gt;
    &lt;p&gt;I was reading post after post about the power crisis hitting AI data centers‚ÄîGPU racks sitting idle, waiting not on chips, but on electricity. I texted with Sam Altman‚Äîwho confirmed power was indeed a major constraint. I pinged our engineering team‚Äîand found that they already had the outline of a plan to build a power turbine based on our Symphony supersonic engine.&lt;/p&gt;
    &lt;p&gt;After a few conversations, it became clear: AI didn‚Äôt just need more turbines‚Äîit needed a new and fundamentally better turbine. Symphony was the perfect new engine to accelerate AI in America. About three months later, we had a signed deal for 1.21 gigawatts and had started manufacturing the first turbine.&lt;/p&gt;
    &lt;p&gt;Today, we‚Äôre announcing Superpower, our new 42‚Äëmegawatt natural gas turbine, along with a $300M funding round and Crusoe as our launch customer. And most importantly: this marks a turning point. Boom is now on a self-funded path to both Superpower and the Overture supersonic airliner.&lt;/p&gt;
    &lt;p&gt;I want to share the real story of how this happened‚Äîand why supersonic technology is exactly what America‚Äôs energy crisis demands.&lt;/p&gt;
    &lt;head rend="h4"&gt;America Doesn‚Äôt Have 10‚Äì15 Years to Solve Its Power Problem the Old Way&lt;/head&gt;
    &lt;p&gt;If you‚Äôve been paying attention, you know the U.S. is in a genuine energy crunch. GPU racks are idling because they can‚Äôt get power. Data centers are fighting over substations and interconnection queues. Meanwhile China is adding power capacity at a wartime pace‚Äîcoal, gas, nuclear, everything‚Äîwhile America struggles to get a single transmission line permitted.&lt;/p&gt;
    &lt;p&gt;AI won‚Äôt wait for us to fix the grid. And the United States simply doesn‚Äôt have 10‚Äì15 years to build out power infrastructure the old way.&lt;/p&gt;
    &lt;p&gt;Hyperscalers have already moved to their own Plan B: behind‚Äëthe‚Äëmeter power plants. You‚Äôve seen XAI‚Äôs Colossus I and II in Memphis. OpenAI‚Äôs Stargate I in Abilene. These projects are powered by arrays of aeroderivative natural-gas turbines‚Äîwhich are, fundamentally, modified jet engines from the 1970s. There‚Äôs something brilliant in this approach: the transition from gigantic ‚Äúframe‚Äù turbines to arrays of mid-size ‚Äúaeroderivative‚Äù turbines mirrors the computing industry‚Äôs shift from mainframes to blade servers.&lt;/p&gt;
    &lt;p&gt;The problem? The ‚Äúblade servers‚Äù of the energy world are old tech and they‚Äôre sold out. Because the most popular ‚Äúaeroderivative‚Äù turbines are based on subsonic jet engines, they‚Äôre happiest when the outside air temperature is -50¬∞F‚Äîlike it is when going Mach 0.8 at 30,000 feet. As outside temperatures rise, there is no option but to throttle back the engines‚Äîor else the turbine blades literally melt down. These turbines begin losing power at about 50¬∞F and by the time it‚Äôs 110¬∞‚Äîas often happens in popular data center locations like Texas‚Äî30% of generation capacity is lost. Nonetheless, major manufacturers all have backlogs through the rest of the decade and none is building a new-generation advanced-technology turbine.&lt;/p&gt;
    &lt;head rend="h4"&gt;A Supersonic Engine Core Makes the Perfect Power Turbine&lt;/head&gt;
    &lt;p&gt;When we designed the Symphony engine for Overture, we built something no one else has built this century: a brand-new large engine core optimized for continuous, high‚Äëtemperature operation.&lt;/p&gt;
    &lt;p&gt;A subsonic engine is built for short bursts of power at takeoff. A supersonic engine is built to run hard, continuously, at extreme thermal loads. Symphony was designed for Mach 1.7 at 60,000 feet, where effective temperatures reach 160¬∞F‚Äînot the frigid -50¬∞F conditions where legacy subsonic engines operate.&lt;/p&gt;
    &lt;p&gt;This gives Superpower several critical advantages:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Full power even with high ambient heat ‚Äì Where legacy turbines lose 20‚Äì30% at 110¬∞F, Superpower maintains its full 42MW output without derate.&lt;/item&gt;
      &lt;item&gt;Waterless operation ‚Äì Legacy turbines need huge quantities of water for cooling to avoid thermal derate in hot environments. Superpower doesn‚Äôt. It stays at full output, water‚Äëfree.&lt;/item&gt;
      &lt;item&gt;Cloud‚Äënative control and monitoring. Superpower inherits the telemetry and operations stack we built for XB‚Äë1. Every turbine streams real‚Äëtime performance data, supports remote control, and flags anomalies before customers ever notice.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Superpower and Symphony are based on virtually identical turbine engines. Both share the identical core (HPC and HPT) and a slightly tuned low spool. In the place of Symphony‚Äôs hollow-core titanium fan, Superpower adds two additional compressor stages plus a three-stage free power turbine connected to a high-efficiency generator on its own shaft. Additionally, the engines use slightly different fuel nozzles, Symphony‚Äôs optimized for Jet A vs. Superpower‚Äôs for natural gas.&lt;/p&gt;
    &lt;head rend="h4"&gt;Scaling Production the Supersonic Way: Vertical Integration&lt;/head&gt;
    &lt;p&gt;The legacy aerospace supply chain is congested. When the mission is urgent and the supply chain congested, you build the supply chain. The new Superpower Superfactory starts with a simple vision: raw materials in one side of the building, gigawatts of completed power turbine packages out the other side. We‚Äôve already started making the first parts‚Äîand much of the production equipment to support 2GW/yr is on order. With this new financing we‚Äôre ready to accelerate further.&lt;/p&gt;
    &lt;p&gt;If America wants to build at the speed AI requires, vertical integration isn‚Äôt optional. We‚Äôre standing up our own foundry and our own large scale CNC machining capability. We‚Äôll have more to share on the Superpower Superfactory in early 2026.&lt;/p&gt;
    &lt;head rend="h4"&gt;Scaling Production the Supersonic Way: Vertical Integration&lt;/head&gt;
    &lt;p&gt;Superpower is sort of like our Starlink moment, the strongest accelerant we‚Äôve ever had toward our core mission of making Earth dramatically more accessible.&lt;/p&gt;
    &lt;p&gt;The fastest way to a certified, passenger-carrying Symphony engine is to run its core for hundreds of thousands of hours in the real world, powering Earth‚Äôs most demanding AI data centers. Every hour a Superpower turbine spins is an hour of validation for Symphony. Every gigawatt we deliver strengthens our vertical integration and manufacturing capability. And with Superpower profitability funding the remainder of the aircraft program, we‚Äôve done something rare in aerospace: created a self-sustaining path to a new airliner.&lt;/p&gt;
    &lt;p&gt;Superpower also reminds me of what Boom is at our core: a team willing to take on what others say is impossible, to do with a small team what big companies might not even attempt.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46206277</guid><pubDate>Tue, 09 Dec 2025 15:51:32 +0000</pubDate></item><item><title>Handsdown one of the coolest 3D websites</title><link>https://bruno-simon.com/</link><description>&lt;doc fingerprint="e43305f6f9f2bf3"&gt;
  &lt;main&gt;
    &lt;p&gt;00:00:000&lt;/p&gt;
    &lt;p&gt;Server currently offline. Scores can't be saved.&lt;/p&gt;
    &lt;p&gt;Welcome!&lt;/p&gt;
    &lt;p&gt;My name is Bruno Simon, and I'm a creative developer (mostly for the web).&lt;/p&gt;
    &lt;p&gt;This is my portfolio. Please drive around to learn more about me and discover the many secrets of this world.&lt;/p&gt;
    &lt;p&gt;And don't break anything!&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Audio&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Quality&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;I'm stuck!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Reset&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Renderer&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Server&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;WASD or ARROWS&lt;/cell&gt;
        &lt;cell&gt;Move around&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SHIFT&lt;/cell&gt;
        &lt;cell&gt;Boost&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;CTRL LEFT or B&lt;/cell&gt;
        &lt;cell&gt;Brake&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SPACE&lt;/cell&gt;
        &lt;cell&gt;Jump&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ENTER&lt;/cell&gt;
        &lt;cell&gt;Interact&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;M&lt;/cell&gt;
        &lt;cell&gt;Map&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;L&lt;/cell&gt;
        &lt;cell&gt;Mute&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;T&lt;/cell&gt;
        &lt;cell&gt;Post a whisper&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;R&lt;/cell&gt;
        &lt;cell&gt;Respawn&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;NUM KEYS/NUM PAD&lt;/cell&gt;
        &lt;cell&gt;Activate hydraulics&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;LEFT CLICK (DRAG)&lt;/cell&gt;
        &lt;cell&gt;Move camera&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;H&lt;/cell&gt;
        &lt;cell&gt;Honk&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;One finger&lt;/cell&gt;
        &lt;cell&gt;Move the car&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Two fingers&lt;/cell&gt;
        &lt;cell&gt;Move camera / zoom&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Tap (on the car)&lt;/cell&gt;
        &lt;cell&gt;Jump&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;B&lt;/cell&gt;
        &lt;cell&gt;Boost&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Y&lt;/cell&gt;
        &lt;cell&gt;Jump&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;X&lt;/cell&gt;
        &lt;cell&gt;Brake&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;A&lt;/cell&gt;
        &lt;cell&gt;Interact / Exit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;LT L2&lt;/cell&gt;
        &lt;cell&gt;Accelerate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;RT R2&lt;/cell&gt;
        &lt;cell&gt;Backward accelerate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;LB / RB L1 / R1&lt;/cell&gt;
        &lt;cell&gt;Hydraulics&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Joystick Left&lt;/cell&gt;
        &lt;cell&gt;Turn wheels&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Joystick Left (press)&lt;/cell&gt;
        &lt;cell&gt;Honk&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Joystick Right&lt;/cell&gt;
        &lt;cell&gt;Move camera&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Joystick Right (press)&lt;/cell&gt;
        &lt;cell&gt;Zoom in/out&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Select&lt;/cell&gt;
        &lt;cell&gt;Reset&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Start&lt;/cell&gt;
        &lt;cell&gt;Pause&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Server currently offline. Scores can't be saved.&lt;/p&gt;
    &lt;p&gt;Resets in&lt;/p&gt;
    &lt;p&gt;Whispers are messages left by visitors.&lt;/p&gt;
    &lt;p&gt; - Everyone can see them&lt;lb/&gt; - New whispers remove old ones (max 30)&lt;lb/&gt; - One whisper per user&lt;lb/&gt; - Choose a flag&lt;lb/&gt; - No slur!&lt;lb/&gt; - Max 30 characters &lt;/p&gt;
    &lt;p&gt;Server currently offline&lt;/p&gt;
    &lt;p&gt; Thank you for visiting my portfolio! &lt;lb/&gt;If you are curious about the stack and how I built it, here√¢s everything you need to know. &lt;/p&gt;
    &lt;p&gt; Three.js is the library I√¢m using to render this 3D world. &lt;lb/&gt;It was created by mr.doob (X, GitHub), followed by hundreds of awesome developers, one of which being Sunag (X, GitHub) who added TSL, enabling the use of both WebGL and WebGPU, making this portfolio possible. &lt;/p&gt;
    &lt;p&gt; If you want to learn Three.js, I got you covered with this huge course. &lt;lb/&gt;It contains everything you need to start building awesome stuff with Three.js (and much more). &lt;/p&gt;
    &lt;p&gt; I√¢ve been making devlogs since the very start of this portfolio and you can find them on my Youtube channel. &lt;lb/&gt;Even though the portfolio is out, I√¢m still working on the last videos so that the series is complete. &lt;/p&gt;
    &lt;p&gt; The code is available on GitHub under MIT license. Even the Blender files are there, so have fun! &lt;lb/&gt;For security reasons, I√¢m not sharing the server code, but the portfolio works without it. &lt;/p&gt;
    &lt;p&gt; The music you hear was made especially for this portfolio by the awesome Kounine (Linktree). &lt;lb/&gt;They are now under CC0 license, meaning you can do whatever you want with them! &lt;lb/&gt;Download them here. &lt;/p&gt;
    &lt;p&gt;√¢ Bruno&lt;/p&gt;
    &lt;p&gt;Server currently offline. Scores can't be saved.&lt;/p&gt;
    &lt;p&gt;Come hang out with the community, show us your projects and ask us anything.&lt;/p&gt;
    &lt;p&gt;Contact me directly.&lt;lb/&gt;I have to warn you, I try to answer everyone, but it might take a while.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46206531</guid><pubDate>Tue, 09 Dec 2025 16:06:58 +0000</pubDate></item><item><title>Launch HN: Mentat (YC F24) ‚Äì Controlling LLMs with Runtime Intervention</title><link>https://news.ycombinator.com/item?id=46207017</link><description>&lt;doc fingerprint="3f5b9ffbac6eca28"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hi HN, I‚Äôm Cyril from CTGT. Today we‚Äôre launching Mentat (&lt;/p&gt;https://docs.ctgt.ai/api-reference/endpoint/chat-completions&lt;p&gt;), an API that gives developers deterministic control over LLM behavior, steering reasoning and removing bias on the fly, without the compute of fine-tuning or the brittleness of prompt engineering. We use feature-level intervention and graph-based verification to fix hallucinations and enforce policies.&lt;/p&gt;&lt;p&gt;This resonates in highly regulated industries or otherwise risky applications of AI where the fallout from incorrect or underperforming output can be significant. In financial services, using GenAI to scan for noncompliant communications can be arduous without an easy way to embed complex policies into the model. Similarly, a media outlet might want to scale AI-generated summaries of their content, but reliability and accuracy is paramount. These are both applications where Fortune 500 companies have utilized our technology to improve subpar performance from existing models, and we want to bring this capability to more people.&lt;/p&gt;&lt;p&gt;Here‚Äôs a quick 2-minute demo video showing the process: https://video.ctgt.ai/video/ctgt-ai-compliance-playground-cf...&lt;/p&gt;&lt;p&gt;Standard "guardrails" like RAG and system prompts are fundamentally probabilistic: you are essentially asking the model nicely to behave. This often fails in two ways. First, RAG solves knowledge availability but not integration. In our benchmarks, a model given context that "Lerwick is 228 miles SE of T√≥rshavn" failed to answer "What is 228 miles NW of Lerwick?" because it couldn't perform the spatial inversion.&lt;/p&gt;&lt;p&gt;Second, prompt engineering is brittle because it fights against the model's pre-training priors. For example, on the TruthfulQA benchmark, base models fail ~80% of the time because they mimic common misconceptions found on the internet (e.g. "chameleons change color for camouflage"). We found that we could literally turn up the feature for "skeptical reasoning" to make the model ignore the popular myth and output the scientific fact. This matters because for high-stakes use cases (like Finance or Pharma), "mostly safe" isn't acceptable‚Äîcompanies need audit-grade reliability.&lt;/p&gt;&lt;p&gt;Our work stems from the CS dungeon at UCSD, with years spent researching efficient and interpretable AI, trying to "open the black box" of neural networks. We realized that the industry was trying to patch model behavior from the outside (prompts/filters) when the problem was on the inside (feature activations). We knew this was important when we saw enterprises struggling to deploy basic models despite having unlimited compute, simply because they couldn't guarantee the output wouldn't violate compliance rules. I ended up leaving my research at Stanford to focus on this.&lt;/p&gt;&lt;p&gt;Our breakthrough came while researching the DeepSeek-R1 model. We identified the "censorship" feature vector in its latent space. Amplifying it guaranteed refusal; subtracting it instantly unlocked answers to sensitive questions. This proved the model had the knowledge but was suppressing it. We realized we could apply this same logic to hallucinations, suppressing "confabulation" features to reveal the grounded truth. While some hallucinations stem from the inherent randomness of generative models, many can be identified with the concerted activation of a feature or group of features.&lt;/p&gt;&lt;p&gt;Instead of filtering outputs, we intervene at the activation level during the forward pass. We identify latent feature vectors (v) associated with specific behaviors (bias, misconception) and mathematically modify the hidden state (h):&lt;/p&gt;&lt;quote&gt;&lt;code&gt;  h_prime = h - alpha * (h @ v) * v
&lt;/code&gt;&lt;/quote&gt;&lt;p&gt; This arithmetic operation lets us "edit" behavior deterministically with negligible overhead (&amp;lt;10ms on R1). For factual claims, we combine this with a graph verification pipeline (which works on closed weight models). We check semantic entropy (is the model babbling?) and cross-reference claims against a dynamic knowledge graph to catch subtle relational hallucinations that vector search misses.&lt;/p&gt;&lt;p&gt;On GPT-OSS-120b, this approach improved TruthfulQA accuracy from 21% to 70% by suppressing misconception features. We also improved the performance of this model to frontier levels on HaluEval-QA, where we reached 96.5% accuracy, solving the spatial reasoning failures where the baseline failed. It also handles noisy inputs, inferring "David Icke" from the typo "David Of me" where base models gave up. Full benchmarks at https://ctgt.ai/benchmarks.&lt;/p&gt;&lt;p&gt;Most startups in this space are observability tools that tell you only after the model failed. Or they are RAG pipelines that stuff context into the window. Mentat is an infrastructure layer that modifies the model's processing during inference. We fix the reasoning, not just the context. For example, that‚Äôs how our system was able to enforce that if A is SE of B, then B is NW of A.&lt;/p&gt;&lt;p&gt;We believe that our policy engine is a superior control mechanism to RAG or prompting. If you‚Äôre frustrated with current guardrails, we‚Äôd love it if you would stress-test our API!&lt;/p&gt;&lt;p&gt;API: Our endpoint is drop-in compatible with OpenAI‚Äôs /v1/chat/completions: https://docs.ctgt.ai/api-reference/endpoint/chat-completions&lt;/p&gt;&lt;p&gt;Playground: We‚Äôve built an "Arena" view to run side-by-side comparisons of an Ungoverned vs. Governed model to visualize the intervention delta in real-time. No signup is required: https://playground.ctgt.ai/&lt;/p&gt;&lt;p&gt;We‚Äôd love to hear your feedback on the approach and see what edge cases you can find that break standard models. We will be in the comments all day. All feedback welcome!&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46207017</guid><pubDate>Tue, 09 Dec 2025 16:37:55 +0000</pubDate></item><item><title>Clearspace (YC W23) Is Hiring a Founding Designer</title><link>https://www.ycombinator.com/companies/clearspace/jobs/yamWTLr-founding-designer-at-clearspace</link><description>&lt;doc fingerprint="b782896e8d09edc5"&gt;
  &lt;main&gt;
    &lt;p&gt;Eliminate compulsive phone usage&lt;/p&gt;
    &lt;p&gt;About Clearspace&lt;/p&gt;
    &lt;p&gt;Clearspace is building the intentionality layer of the internet. Our mission is to build technology as effective at protecting human attention as social media is at exploiting it (infinite scrolling, short-form feeds, manipulative notifications, etc). Our category defining mobile app has been featured on Huberman Lab, New York Times Wirecutter, NPR Marketplace, Forbes, TBPN.&lt;/p&gt;
    &lt;p&gt;People that want a better relationship with their devices have nowhere to turn except for willpower. We are building an agent that achieves this on all devices by processing and filtering network traffic based on natural language rules.&lt;/p&gt;
    &lt;p&gt;About The Role&lt;/p&gt;
    &lt;p&gt;We are looking for a lead designer with strong aesthetic intuition and an obsession with designing through every inch of the user journey. You will be asked to bring pixel perfect designs to life across several different platforms, if you don‚Äôt love the process of designing this is not the role for you. You will be talking to users often and asked to speak to the overall brand direction at Clearspace.&lt;/p&gt;
    &lt;p&gt;Responsibilities&lt;/p&gt;
    &lt;p&gt;Qualifications&lt;/p&gt;
    &lt;p&gt;Nice to Have&lt;/p&gt;
    &lt;p&gt;At Clearspace we help people reduce compulsive phone usage.&lt;/p&gt;
    &lt;p&gt;We exist to protect people's attention from the exploits of modern technology platforms and make space for the things that matter to them most.&lt;/p&gt;
    &lt;p&gt;We believe the technology to protect someones attention should be just as sophisticated and effective as the tech that is exploiting it and are building a world-class engineering team to arm the world with a comprehensive attention protection stack.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46207360</guid><pubDate>Tue, 09 Dec 2025 17:01:11 +0000</pubDate></item><item><title>Donating the Model Context Protocol and establishing the Agentic AI Foundation</title><link>https://www.anthropic.com/news/donating-the-model-context-protocol-and-establishing-of-the-agentic-ai-foundation</link><description>&lt;doc fingerprint="fd5e72507a8d8692"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Donating the Model Context Protocol and establishing the Agentic AI Foundation&lt;/head&gt;
    &lt;p&gt;Today, we‚Äôre donating the Model Context Protocol (MCP) to the Agentic AI Foundation (AAIF), a directed fund under the Linux Foundation, co-founded by Anthropic, Block and OpenAI, with support from Google, Microsoft, Amazon Web Services (AWS), Cloudflare, and Bloomberg.&lt;/p&gt;
    &lt;head rend="h2"&gt;Model Context Protocol&lt;/head&gt;
    &lt;p&gt;One year ago, we introduced MCP as a universal, open standard for connecting AI applications to external systems. Since then, MCP has achieved incredible adoption:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Across the ecosystem: There are now more than 10,000 active public MCP servers, covering everything from developer tools to Fortune 500 deployments;&lt;/item&gt;
      &lt;item&gt;Across platforms: MCP has been adopted by ChatGPT, Cursor, Gemini, Microsoft Copilot, Visual Studio Code, and other popular AI products;&lt;/item&gt;
      &lt;item&gt;Across infrastructure: Enterprise-grade infrastructure now exists with deployment support for MCP from providers including AWS, Cloudflare, Google Cloud, and Microsoft Azure.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;lb/&gt;We‚Äôre continuing to invest in MCP‚Äôs growth. Claude now has a directory with over 75 connectors (powered by MCP), and we recently launched Tool Search and Programmatic Tool Calling capabilities in our API to help optimize production-scale MCP deployments, handling thousands of tools efficiently and reducing latency in complex agent workflows.&lt;lb/&gt;MCP now has an official, community-driven Registry for discovering available MCP servers, and the November 25th spec release introduced many new features, including asynchronous operations, statelessness, server identity, and official extensions. There are also official SDKs (Software Development Kits) for MCP in all major programming languages with 97M+ monthly SDK downloads across Python and TypeScript. &lt;lb/&gt;Since its inception, we‚Äôve been committed to ensuring MCP remains open-source, community-driven and vendor-neutral. Today, we further that commitment by donating MCP to the Linux Foundation.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Linux Foundation and the Agentic AI Foundation&lt;/head&gt;
    &lt;p&gt;The Linux Foundation is a non-profit organization dedicated to fostering the growth of sustainable, open-source ecosystems through neutral stewardship, community building, and shared infrastructure. It has decades of experience stewarding the most critical and globally-significant open-source projects, including The Linux Kernel, Kubernetes, Node.js, and PyTorch. Importantly, the Linux Foundation has a proven track record in facilitating open collaboration and maintaining vendor neutrality.&lt;/p&gt;
    &lt;p&gt;The Agentic AI Foundation (AAIF) is a directed fund under the Linux Foundation co-founded by Anthropic, Block and OpenAI, with support from Google, Microsoft, AWS, Cloudflare and Bloomberg. The AAIF aims to ensure agentic AI evolves transparently, collaboratively, and in the public interest through strategic investment, community building, and shared development of open standards.&lt;/p&gt;
    &lt;head rend="h2"&gt;Donating the Model Context Protocol&lt;/head&gt;
    &lt;p&gt;Anthropic is donating the Model Context Protocol to the Linux Foundation's new Agentic AI Foundation, where it will join goose by Block and AGENTS.md by OpenAI as founding projects. Bringing these and future projects under the AAIF will foster innovation across the agentic AI ecosystem and ensure these foundational technologies remain neutral, open, and community-driven. &lt;lb/&gt;The Model Context Protocol‚Äôs governance model will remain unchanged: the project‚Äôs maintainers will continue to prioritize community input and transparent decision-making.&lt;/p&gt;
    &lt;head rend="h2"&gt;The future of MCP&lt;/head&gt;
    &lt;p&gt;Open-source software is essential for building a secure and innovative ecosystem for agentic AI. Today‚Äôs donation to the Linux Foundation demonstrates our commitment to ensuring MCP remains a neutral, open standard. We‚Äôre excited to continue contributing to MCP and other agentic AI projects through the AAIF.&lt;lb/&gt;Learn more about MCP at modelcontextprotocol.io and get involved with the AAIF here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46207425</guid><pubDate>Tue, 09 Dec 2025 17:05:42 +0000</pubDate></item><item><title>PeerTube is recognized as a digital public good by Digital Public Goods Alliance</title><link>https://www.digitalpublicgoods.net/r/peertube</link><description>&lt;doc fingerprint="95773f811edde224"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;PeerTube&lt;/head&gt;
    &lt;p&gt;Verified DPG&lt;/p&gt;
    &lt;head rend="h3"&gt;Owner&lt;/head&gt;
    &lt;p&gt;Framasoft&lt;/p&gt;
    &lt;head rend="h3"&gt;Type&lt;/head&gt;
    &lt;p&gt;backend, mobile, web&lt;/p&gt;
    &lt;head rend="h3"&gt;Licence&lt;/head&gt;
    &lt;p&gt;AGPL-3.0&lt;/p&gt;
    &lt;head rend="h3"&gt;Last evaluated&lt;/head&gt;
    &lt;p&gt;07.10.2025&lt;/p&gt;
    &lt;head rend="h3"&gt;Origin country&lt;/head&gt;
    &lt;p&gt;France&lt;/p&gt;
    &lt;head rend="h3"&gt;Release date&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;head rend="h3"&gt;DPG since&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;head rend="h3"&gt;Description&lt;/head&gt;
    &lt;p&gt;PeerTube is a tool for hosting, managing, and sharing videos or live streams.&lt;/p&gt;
    &lt;head rend="h3"&gt;Core Components Assessed/Included Repositories&lt;/head&gt;
    &lt;p&gt;The following repositories were submitted by the solution and included in our evaluation. Any repositories, add-ons, features not included in here were not reviewed by us.&lt;/p&gt;
    &lt;head rend="h3"&gt;Feature&lt;/head&gt;
    &lt;head rend="h3"&gt;Scale of the Solution*&lt;/head&gt;
    &lt;head rend="h3"&gt;Connected members&lt;/head&gt;
    &lt;p&gt;N/A&lt;/p&gt;
    &lt;head rend="h3"&gt;Participated Programs&lt;/head&gt;
    &lt;p&gt;N/A&lt;/p&gt;
    &lt;head rend="h3"&gt;Available Languages&lt;/head&gt;
    &lt;p&gt;Esperanto, English, Slovenƒçina, G√†idhlig, ÿßŸÑÿπÿ±ÿ®Ÿäÿ©, Norsk, Magyar, Deutsch, Toki Pona, Euskara, Polski, Portugu√™s (Portugal), Suomi, Ti·∫øng Vi·ªát, Italiano, ŸÅÿßÿ±ÿ≥€å, Espa√±ol, Taqbaylit, ÁÆÄ‰Ωì‰∏≠ÊñáÔºà‰∏≠ÂõΩÔºâ, Hrvatski, ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨, Occitan, —É–∫—Ä–∞—óÃÅ–Ω—Å—å–∫–∞ –º–æÃÅ–≤–∞, Fran√ßais, ‡πÑ‡∏ó‡∏¢, T√ºrk√ße, ÁπÅÈ´î‰∏≠ÊñáÔºàÂè∞ÁÅ£Ôºâ, Êó•Êú¨Ë™û, Galego, √çslenska, Svenska, Nederlands, P—É—Å—Å–∫–∏–π, bokm√•l, ƒåe≈°tina, Shqip, Catal√†, Portugu√™s (Brasil), Norsk nynorsk&lt;/p&gt;
    &lt;head rend="h3"&gt;Organisations using it&lt;/head&gt;
    &lt;p&gt;French Ministry of National Education (~100K videos), Italy‚Äôs National Research Council, a few French alternative media, the Wei√üensee Kunsthochschule in Berlin, as well as the Universit√§t der K√ºnste in the same city, a few universities worldwide, the Blender and Debian projects, and various activist groups&lt;/p&gt;
    &lt;p&gt;* This information is self-reported and updated annually&lt;/p&gt;
    &lt;head rend="h3"&gt;Github insights&lt;/head&gt;
    &lt;p&gt;Learn how this product has met the requirements of the DPG Standard by exploring the indicators below.&lt;/p&gt;
    &lt;head rend="h3"&gt;Application Details&lt;/head&gt;
    &lt;head rend="h4"&gt;DPG ID&lt;/head&gt;
    &lt;head rend="h4"&gt;GID0092472&lt;/head&gt;
    &lt;head rend="h4"&gt;Status&lt;/head&gt;
    &lt;head rend="h4"&gt;DPG&lt;/head&gt;
    &lt;head rend="h4"&gt;Date Created&lt;/head&gt;
    &lt;head rend="h4"&gt;2025-08-11&lt;/head&gt;
    &lt;head rend="h4"&gt;Date Submitted&lt;/head&gt;
    &lt;head rend="h4"&gt;2025-08-25&lt;/head&gt;
    &lt;head rend="h4"&gt;Date Reviewed&lt;/head&gt;
    &lt;head rend="h4"&gt;2025-10-07&lt;/head&gt;
    &lt;head rend="h4"&gt;Date of Expiry&lt;/head&gt;
    &lt;head rend="h4"&gt;2026-10-07&lt;/head&gt;
    &lt;head rend="h3"&gt;Application Log Details&lt;/head&gt;
    &lt;head rend="h4"&gt;Timestamp&lt;/head&gt;
    &lt;head rend="h4"&gt;Activity&lt;/head&gt;
    &lt;p&gt;2025-10-07 08:40:13&lt;/p&gt;
    &lt;p&gt;Ricardo Torres (L2 Reviewer) submitted their review of PeerTube (152) and found it to be a DPG&lt;/p&gt;
    &lt;p&gt;2025-10-07 08:40:12&lt;/p&gt;
    &lt;p&gt;System unmarked PeerTube (12958) as a nominee&lt;/p&gt;
    &lt;p&gt;2025-10-07 08:40:07&lt;/p&gt;
    &lt;p&gt;Ricardo Torres (L2 Reviewer) passed 4. Platform Independence for PeerTube (12958)&lt;/p&gt;
    &lt;p&gt;2025-10-07 08:40:02&lt;/p&gt;
    &lt;p&gt;Ricardo Torres (L2 Reviewer) moved PeerTube (12958) to under review&lt;/p&gt;
    &lt;p&gt;2025-10-07 08:38:21&lt;/p&gt;
    &lt;p&gt;Ricardo Torres (L2 Reviewer) finished consultation on 4. Platform Independence for PeerTube (12958)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46207464</guid><pubDate>Tue, 09 Dec 2025 17:08:37 +0000</pubDate></item><item><title>If you're going to vibe code, why not do it in C?</title><link>https://stephenramsay.net/posts/vibe-coding.html</link><description>&lt;doc fingerprint="af2319bf33f607dd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;If You‚Äôre Going to Vibe Code, Why Not Do It in C?&lt;/head&gt;
    &lt;p&gt;Stephen Ramsay&lt;/p&gt;
    &lt;p&gt;Or hell, why not do it in x86 assembly?&lt;/p&gt;
    &lt;p&gt;Let‚Äôs get a few things out of the way before I go any further with this seemingly impertinent thought, because it‚Äôs nowhere near as snarky as it sounds.&lt;/p&gt;
    &lt;p&gt;First, I don‚Äôt particularly like vibe coding. I love programming, and I have loved it since I made my first tentative steps with it sometime back in the mid-to-late 90s. I love programming so much, it always feels like I‚Äôm having too much fun for it to count as real work. I‚Äôve done it professionally, but I also do it as a hobby. Someone apparently once said, ‚ÄúDo what you love and you‚Äôll never work a day in your life.‚Äù That‚Äôs how I feel about writing code. I‚Äôve also been teaching the subject for twenty-five years, and I can honestly say I am as excited about the first day of the semester now as I was when I first started. I realize it‚Äôs a bit precious to say so, but I‚Äôll say it anyway: Turning non-programmers into programmers is my life‚Äôs work. It is the thing of which I am most proud as a college professor.&lt;/p&gt;
    &lt;p&gt;Vibe coding makes me feel dirty in ways that I struggle to articulate precisely. It‚Äôs not just that it feels like ‚Äúcheating‚Äù (though it does). I also think it takes a lot of the fun out of the whole thing. I sometimes tell people (like the aforementioned students) that programming is like doing the best crossword puzzle in the world, except that when you solve it, it actually dances and sings. Vibe coding robs me of that moment, because I don‚Äôt feel like I really did it at all. And even though to be a programmer is to live with a more-or-less permanent set of aporias (you don‚Äôt really understand what the compiler is doing, really‚Äîand even if you do, you probably don‚Äôt really understand how the virtual memory subsystem works, really), it‚Äôs satisfying to understand every inch of my code and frustrating‚Äîall the way to the borderlands of active anxiety‚Äînot quite understanding what Claude just wrote.&lt;/p&gt;
    &lt;p&gt;But this leads me to my second point, which I must make as clearly and forcefully as I can. Vibe coding actually works. It creates robust, complex systems that work. You can tell yourself (as I did) that it can‚Äôt possibly do that, but you are wrong. You can then tell yourself (as I did) that it‚Äôs good as a kind of alternative search engine for coding problems, but not much else. You are also wrong about that. Because when you start giving it little programming problems that you can‚Äôt be arsed to work out yourself (as I did), you discover (as I did) that it‚Äôs awfully good at those. And then one day you muse out loud (as I did) to an AI model something like, ‚ÄúI have an idea for a program‚Ä¶‚Äù And you are astounded. If you aren‚Äôt astounded, you either haven‚Äôt actually done it or you are at some stage of grief prior to acceptance. Perfect? Hardly. But then neither are human coders. The future? I think the questions answers itself.&lt;/p&gt;
    &lt;p&gt;But to get to my impertinent question‚Ä¶&lt;/p&gt;
    &lt;p&gt;Early on in my love affair with programming, I read Structure and Interpretation of Computer Programs, which I now consider one of the great pedagogical masterpieces of the twentieth century. I learned a great deal about programming from that book, but among the most memorable lessons was one that appears in the second paragraph of the original preface. There, Hal Abelson and Gerald Sussman make a point that hits with the force of the obvious, and yet is very often forgotten:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[W]e want to establish the idea that a computer language is not just a way of getting a computer to perform operations but rather that it is a novel formal medium for expressing ideas about methodology. Thus, programs must be written for people to read, and only incidentally for machines to execute.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I‚Äôve been repeating some version of this to my students ever since. Computers, I remind them, do not need the code to be ‚Äúreadable‚Äù or ‚Äúergonomic‚Äù for humans; they only need it to be readable and ergonomic for a computer, which is a considerably lower bar.&lt;/p&gt;
    &lt;p&gt;Every programming language‚Äîincluding assembly language‚Äîwas and is intended for the convenience of humans who need to read it and write it. If a language is innovative, it is usually not because it has allowed for automatic memory management, or concurrency, or safety, or robust error checking, but because it has made it easier for humans to express and reason about these matters. When we extol the virtues of this or that language‚ÄîRust‚Äôs safety guarantees, C++‚Äôs ‚Äúno-cost abstractions,‚Äù or Go‚Äôs approach to concurrency‚Äîwe are not talking about an affordance that the computer has gained, but about an affordance that we have gained as programmers of said computer. From our standpoint as programmers, object-oriented languages offer certain ways to organize our code‚Äîand, I think Abelson and Sussman would say, our thinking‚Äîthat are potentially conducive to the noble treasures of maintainability, extensibility, error checking, and any number of other condign matters. From the standpoint of the computer, this little OO kink of ours seems mostly to indicate a strange affinity for heap memory. ‚ÄúWhatevs!‚Äù (says the computer). And pick your poison here, folks: functional programming, algebraic data types, dependent types, homoiconicity, immutable data structures, brace styles‚Ä¶ We can debate the utility of these things, but we must understand that we are primarily talking about human problems. The set of ‚Äúmachine problems‚Äù to which these matters correspond is considerably smaller.&lt;/p&gt;
    &lt;p&gt;So my question is this: Why vibe code with a language that has human convenience and ergonomics in view? Or to put that another way: Wouldn‚Äôt a language designed for vibe coding naturally dispense with much of what is convenient and ergonomic for humans in favor of what is convenient and ergonomic for machines? Why not have it just write C? Or hell, why not x86 assembly?&lt;/p&gt;
    &lt;p&gt;Now, at this point, you will want to say that the need for human understanding isn‚Äôt erased entirely thereby. Some version of this argument has merit, but I would remind you that if you are really vibe coding for real you already don‚Äôt understand a great deal of what it is producing. But if you look carefully, you will notice that it doesn‚Äôt struggle with undefined behavior in C. Or with making sure that all memory is properly freed. Or with off-by-one errors. It sometimes struggles to understand what it is that you actually want, but it rarely struggles with the actual execution of the code. It‚Äôs better than you are at keeping track of those things in the same way that a compiler is better at optimizing code than you are. Perfect? No. But as I said before‚Ä¶&lt;/p&gt;
    &lt;p&gt;Is C the ideal language for vibe coding? I think I could mount an argument for why it is not, but surely Rust is even less ideal. To say nothing of Haskell, or OCaml, or even Python. All of these languages, after all, are for people to read, and only incidentally for machines to execute. They are practically adorable in their concern for problems that AI models do not have.&lt;/p&gt;
    &lt;p&gt;I suppose what I‚Äôm getting at, here, is that if vibe coding is the future of software development (and it is), then why bother with languages that were designed for people who are not vibe coding? Shouldn‚Äôt there be such a thing as a ‚Äúvibe-oriented programming language?‚Äù VOP. You read it here first.&lt;/p&gt;
    &lt;p&gt;One possibility is that such a language truly would be executable pseudocode beyond even the most extravagant fever dreams of the most earnest Pythonistas; it shows you what it‚Äôs doing in truly pseudo code, but all the while it‚Äôs writing assembly. Or perhaps it‚Äôs something like the apotheosis of literate programming. You write a literary document ‚Äúexpressing ideas about methodology,‚Äù and the AI produces machine code (and a kind of literary critical practice evolves around this activity, eventually ordering itself into structuralist and post-structuralist camps. But I‚Äôm getting ahead of myself). Perhaps your job as a programmer is mostly running tests that verify this machine code (tests which have also been produced by AI). Or maybe a VOPL is really a certain kind of language that comes closer to natural language than any existing programming language, but which has a certain (easily learned) set of idioms and expressions that guide the AI more reliably and more quickly toward particular solutions. It doesn‚Äôt have goroutines. It has a ‚Äúconcurrency slang.‚Äù&lt;/p&gt;
    &lt;p&gt;Now obviously, the reason a large language model focused on coding is good at Javascript and C++ is precisely because it has been trained on billions of lines of code in those languages along with countless forum posts, StackOverflow debates, and so on. Bootstrapping a VOPL presents a certain kind of difficulty, but then one also suspects that LLMs are already being trained in some future version of this language, because so many programmers are already groping their way toward a system like this by virtue of the fact that so many of them are already vibe coding production-level systems.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt know how I feel about all of this (see my first and second points above). It saddens me to think of ‚Äúcoding by hand‚Äù becoming a kind of quaint Montessori-school stage in the education of a vibe coder‚Äîsomething like the contour drawings we demand from future photoshopers or the balanced equations we insist serve as a rite of passage for people who will never be without a calculator to the end of their days.&lt;/p&gt;
    &lt;p&gt;At the same time, there is something exciting about the birth of a computational paradigm. It wasn‚Äôt that long ago, in the grand scheme of things, that someone realized that rewiring the entire machine every time you wanted to do a calculation (think ENIAC, circa 1945) was a rather suboptimal way to do things. And it is worth recalling that people complained when the stored-program computer rolled around (think EDVAC, circa 1951). Why? Well, the answer should be obvious. It was less reliable. It was slower. It removed the operator from the loop. It threatened specialized labor. It was conceptually impure. I‚Äôm not kidding about any of this. No less an authority than Grace Hopper had to argue against the quite popular idea that there was no way anyone could ever trust a machine to write instructions for another machine.&lt;/p&gt;
    &lt;p&gt;Same vibe, as the kids say.&lt;/p&gt;
    &lt;p&gt;Keywords: programming, AI&lt;/p&gt;
    &lt;p&gt;Last Modified: 2025-12-07T16:29:42:-0600&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46207505</guid><pubDate>Tue, 09 Dec 2025 17:11:09 +0000</pubDate></item><item><title>The stack circuitry of the Intel 8087 floating point chip, reverse-engineered</title><link>https://www.righto.com/2025/12/8087-stack-circuitry.html</link><description>&lt;doc fingerprint="2b5a4030cd17f258"&gt;
  &lt;main&gt;
    &lt;p&gt;Early microprocessors were very slow when operating with floating-point numbers. But in 1980, Intel introduced the 8087 floating-point coprocessor, performing floating-point operations up to 100 times faster. This was a huge benefit for IBM PC applications such as AutoCAD, spreadsheets, and flight simulators. The 8087 was so effective that today's computers still use a floating-point system based on the 8087.1&lt;/p&gt;
    &lt;p&gt;The 8087 was an extremely complex chip for its time, containing somewhere between 40,000 and 75,000 transistors, depending on the source.2 To explore how the 8087 works, I opened up a chip and took numerous photos of the silicon die with a microscope. Around the edges of the die, you can see the hair-thin bond wires that connect the chip to its 40 external pins. The complex patterns on the die are formed by its metal wiring, as well as the polysilicon and silicon underneath. The bottom half of the chip is the "datapath", the circuitry that performs calculations on 80-bit floating point values. At the left of the datapath, a constant ROM holds important constants such as œÄ. At the right are the eight registers that form the stack, along with the stack control circuitry.&lt;/p&gt;
    &lt;p&gt;The chip's instructions are defined by the large microcode ROM in the middle. This ROM is very unusual; it is semi-analog, storing two bits per transistor by using four transistor sizes. To execute a floating-point instruction, the 8087 decodes the instruction and the microcode engine starts executing the appropriate micro-instructions from the microcode ROM. The decode circuitry to the right of the ROM generates the appropriate control signals from each micro-instruction. The bus registers and control circuitry handle interactions with the main 8086 processor and the rest of the system. Finally, the bias generator uses a charge pump to create a negative voltage to bias the chip's substrate, the underlying silicon.&lt;/p&gt;
    &lt;p&gt;The stack registers and control circuitry (in red above) are the subject of this blog post. Unlike most processors, the 8087 organizes its registers in a stack, with instructions operating on the top of the stack. For instance, the square root instruction replaces the value on the top of the stack with its square root. You can also access a register relative to the top of the stack, for instance, adding the top value to the value two positions down from the top. The stack-based architecture was intended to improve the instruction set, simplify compiler design, and make function calls more efficient, although it didn't work as well as hoped.&lt;/p&gt;
    &lt;p&gt;The diagram above shows how the stack operates. The stack consists of eight registers, with the Stack Top (ST) indicating the current top of the stack. To push a floating-point value onto the stack, the Stack Top is decremented and then the value is stored in the new top register. A pop is performed by copying the value from the stack top and then incrementing the Stack Top. In comparison, most processors specify registers directly, so register 2 is always the same register.&lt;/p&gt;
    &lt;head rend="h2"&gt;The registers&lt;/head&gt;
    &lt;p&gt;The stack registers occupy a substantial area on the die of the 8087 because floating-point numbers take many bits. A floating-point number consists of a fractional part (sometimes called the mantissa or significand), along with the exponent part; the exponent allows floating-point numbers to cover a range from extremely small to extremely large. In the 8087, floating-point numbers are 80 bits: 64 bits of significand, 15 bits of exponent, and a sign bit. An 80-bit register was very large in the era of 8-bit or 16-bit computers; the eight registers in the 8087 would be equivalent to 40 registers in the 8086 processor.&lt;/p&gt;
    &lt;p&gt;The registers store each bit in a static RAM cell. Each cell has two inverters connected in a loop. This circuit forms a stable feedback loop, with one inverter on and one inverter off. Depending on which inverter is on, the circuit stores a 0 or a 1. To write a new value into the circuit, one of the lines is pulled low, flipping the loop into the desired state. The trick is that each inverter uses a very weak transistor to pull the output high, so its output is easily overpowered to change the state.&lt;/p&gt;
    &lt;p&gt;These inverter pairs are arranged in an 8 √ó 80 grid that implements eight words of 80 bits. Each of the 80 rows has two bitlines that provide access to a bit. The bitlines provide both read and write access to a bit; the pair of bitlines allows either inverter to be pulled low to store the desired bit value. Eight vertical wordlines enable access to one word, one column of 80 bits. Each wordline turns on 160 pass transistors, connecting the bitlines to the inverters in the selected column. Thus, when a wordline is enabled, the bitlines can be used to read or write that word.&lt;/p&gt;
    &lt;p&gt;Although the chip looks two-dimensional, it actually consists of multiple layers. The bottom layer is silicon. The pinkish regions below are where the silicon has been "doped" to change its electrical properties, making it an active part of the circuit. The doped silicon forms a grid of horizontal and vertical wiring, with larger doped regions in the middle. On top of the silicon, polysilicon wiring provides two functions. First, it provides a layer of wiring to connect the circuit. But more importantly, when polysilicon crosses doped silicon, it forms a transistor. The polysilicon provides the gate, turning the transistor on and off. In this photo, the polysilicon is barely visible, so I've highlighted part of it in red. Finally, horizontal metal wires provide a third layer of interconnecting wiring. Normally, the metal hides the underlying circuitry, so I removed the metal with acid for this photo. I've drawn blue lines to represent the metal layer. Contacts provide connections between the various layers.&lt;/p&gt;
    &lt;p&gt;The layers combine to form the inverters and selection transistors of a memory cell, indicated with the dotted line below. There are six transistors (yellow), where polysilicon crosses doped silicon. Each inverter has a transistor that pulls the output low and a weak transistor to pull the output high. When the word line (vertical polysilicon) is active, it connects the selected inverters to the bit lines (horizontal metal) through the two selection transistors. This allows the bit to be read or written.&lt;/p&gt;
    &lt;p&gt;Each register has two tag bits associated with it, an unusual form of metadata to indicate if the register is empty, contains zero, contains a valid value, or contains a special value such as infinity. The tag bits are used to optimize performance internally and are mostly irrelevant to the programmer. As well as being accessed with a register, the tag bits can be accessed in parallel as a 16-bit "Tag Word". This allows the tags to be saved or loaded as part of the 8087's state, for instance, during interrupt handling.&lt;/p&gt;
    &lt;head rend="h2"&gt;The decoder&lt;/head&gt;
    &lt;p&gt;The decoder circuit, wedged into the middle of the register file, selects one of the registers. A register is specified internally with a 3-bit value. The decoder circuit energizes one of the eight register select lines based on this value.&lt;/p&gt;
    &lt;p&gt;The decoder circuitry is straightforward: it has eight 3-input NOR gates to match one of the eight bit patterns. The select line is then powered through a high-current driver that uses large transistors. (In the photo below, you can compare the large serpentine driver transistors to the small transistors in a bit cell.)&lt;/p&gt;
    &lt;p&gt;The decoder has an interesting electrical optimization. As shown earlier, the register select lines are eight polysilicon lines running vertically, the length of the register file. Unfortunately, polysilicon has fairly high resistance, better than silicon but much worse than metal. The problem is that the resistance of a long polysilicon line will slow down the system. That is, the capacitance of transistor gates in combination with high resistance causes an RC (resistive-capacitive) delay in the signal.&lt;/p&gt;
    &lt;p&gt;The solution is that the register select lines also run in the metal layer, a second set of lines immediately to the right of the register file. These lines branch off from the register file about 1/3 of the way down, run to the bottom, and then connect back to the polysilicon select lines at the bottom. This reduces the maximum resistance through a select line, increasing the speed.&lt;/p&gt;
    &lt;head rend="h2"&gt;The stack control circuitry&lt;/head&gt;
    &lt;p&gt;A stack needs more control circuitry than a regular register file, since the circuitry must keep track of the position of the top of the stack.3 The control circuitry increments and decrements the top of stack (TOS) pointer as values are pushed or popped (purple).4 Moreover, an 8087 instruction can access a register based on its offset, for instance the third register from the top. To support this, the control circuitry can temporarily add an offset to the top of stack position (green). A multiplexer (red) selects either the top of stack or the adder output, and feeds it to the decoder (blue), which selects one of the eight stack registers in the register file (yellow), as described earlier.&lt;/p&gt;
    &lt;p&gt;The physical implementation of the stack circuitry is shown below. The logic at the top selects the stack operation based on the 16-bit micro-instruction.5 Below that are the three latches that hold the top of stack value. (The large white squares look important, but they are simply "jumpers" from the ground line to the circuitry, passing under metal wires.)&lt;/p&gt;
    &lt;p&gt;The three-bit adder is at the bottom, along with the multiplexer. You might expect the adder to use a simple "full adder" circuit. Instead, it is a faster carry-lookahead adder. I won't go into details here, but the summary is that at each bit position, an AND gate produces a Carry Generate signal while an XOR gate produces a Carry Propagate signal. Logic gates combine these signals to produce the output bits in parallel, avoiding the slowdown of the carry rippling through the bits.&lt;/p&gt;
    &lt;p&gt;The incrementer/decrementer uses a completely different approach. Each of the three bits uses a toggle flip-flop. A few logic gates determine if each bit should be toggled or should keep its previous value. For instance, when incrementing, the top bit is toggled if the lower bits are 11 (e.g. incrementing from 011 to 100). For decrementing, the top bit is toggled if the lower bits are 00 (e.g. 100 to 011). Simpler logic determines if the middle bit should be toggled. The bottom bit is easier, toggling every time whether incrementing or decrementing.&lt;/p&gt;
    &lt;p&gt;The schematic below shows the circuitry for one bit of the stack. Each bit is implemented with a moderately complicated flip-flop that can be cleared, loaded with a value, or toggled, based on control signals from the microcode. The flip-flop is constructed from two set-reset (SR) latches. Note that the flip-flop outputs are crossed when fed back to the input, providing the inversion for the toggle action. At the right, the multiplexer selects either the register value or the sum from the adder (not shown), generating the signals to the decoder.&lt;/p&gt;
    &lt;head rend="h2"&gt;Drawbacks of the stack approach&lt;/head&gt;
    &lt;p&gt;According to the designers of the 8087,7 the main motivation for using a stack rather than a flat register set was that instructions didn't have enough bits to address multiple register operands. In addition, a stack has "advantages over general registers for expression parsing and nested function calls." That is, a stack works well for a mathematical expression since sub-expressions can be evaluated on the top of the stack. And for function calls, you avoid the cost of saving registers to memory, since the subroutine can use the stack without disturbing the values underneath. At least that was the idea.&lt;/p&gt;
    &lt;p&gt;The main problem is "stack overflow". The 8087's stack has eight entries, so if you push a ninth value onto the stack, the stack will overflow. Specifically, the top-of-stack pointer will wrap around, obliterating the bottom value on the stack. The 8087 is designed to detect a stack overflow using the register tags: pushing a value to a non-empty register triggers an invalid operation exception.6&lt;/p&gt;
    &lt;p&gt;The designers expected that stack overflow would be rare and could be handled by the operating system (or library code). After detecting a stack overflow, the software should dump the existing stack to memory to provide the illusion of an infinite stack. Unfortunately, bad design decisions made it difficult "both technically and commercially" to handle stack overflow.&lt;/p&gt;
    &lt;p&gt;One of the 8087's designers (Kahan) attributes the 8087's stack problems to the time difference between California, where the designers lived, and Israel, where the 8087 was implemented. Due to a lack of communication, each team thought the other was implementing the overflow software. It wasn't until the 8087 was in production that they realized that "it might not be possible to handle 8087 stack underflow/overflow in a reasonable way. It's not impossible, just impossible to do it in a reasonable way."&lt;/p&gt;
    &lt;p&gt;As a result, the stack was largely a problem rather than a solution. Most 8087 software saved the full stack to memory before performing a function call, creating more memory traffic. Moreover, compilers turned out to work better with regular registers than a stack, so compiler writers awkwardly used the stack to emulate regular registers. The &lt;code&gt;GCC&lt;/code&gt; compiler reportedly needs 3000 lines of extra code to support the x87 stack.&lt;/p&gt;
    &lt;p&gt;In the 1990s, Intel introduced a new floating-point system called SSE, followed by AVX in 2011. These systems use regular (non-stack) registers and provide parallel operations for higher performance, making the 8087's stack instructions largely obsolete.&lt;/p&gt;
    &lt;head rend="h2"&gt;The success of the 8087&lt;/head&gt;
    &lt;p&gt;At the start, Intel was unenthusiastic about producing the 8087, viewing it as unlikely to be a success. John Palmar, a principal architect of the chip, had little success convincing skeptical Intel management that the market for the 8087 was enormous. Eventually, he said, "I'll tell you what. I'll relinquish my salary, provided you'll write down your number of how many you expect to sell, then give me a dollar for every one you sell beyond that."7 Intel didn't agree to the deal‚Äîwhich would have made a fortune for Palmer‚Äîbut they reluctantly agreed to produce the chip.&lt;/p&gt;
    &lt;p&gt;Intel's Santa Clara engineers shunned the 8087, considering it unlikely to work: the 8087 would be two to three times more complex than the 8086, with a die so large that a wafer might not have a single working die. Instead, Rafi Nave, at Intel's Israel site, took on the risky project: ‚ÄúListen, everybody knows it's not going to work, so if it won't work, I would just fulfill their expectations or their assessment. If, by chance, it works, okay, then we'll gain tremendous respect and tremendous breakthrough on our abilities.‚Äù&lt;/p&gt;
    &lt;p&gt;A small team of seven engineers developed the 8087 in Israel. They designed the chip on Mylar sheets: a millimeter on Mylar represented a micron on the physical chip. The drawings were then digitized on a Calma system by clicking on each polygon to create the layout. When the chip was moved into production, the yield was very low but better than feared: two working dies per four-inch wafer.&lt;/p&gt;
    &lt;p&gt;The 8087 ended up being a large success, said to have been Intel's most profitable product line at times. The success of the 8087 (along with the 8088) cemented the reputation of Intel Israel, which eventually became Israel's largest tech employer. The benefits of floating-point hardware proved to be so great that Intel integrated the floating-point unit into later processors starting with the 80486 (1989). Nowadays, most modern computers, from cellphones to mainframes, provide floating point based on the 8087, so I consider the 8087 one of the most influential chips ever created.&lt;/p&gt;
    &lt;p&gt;For more, follow me on Bluesky (@righto.com), Mastodon (@[email protected]), or RSS. I wrote some articles about the 8087 a few years ago, including the die, the ROM, the bit shifter, and the constants, so you may have seen some of this material before.&lt;/p&gt;
    &lt;head rend="h2"&gt;Notes and references&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Most computers now use the IEEE 754 floating-point standard, which is based on the 8087. This standard has been awarded a milestone in computation. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Curiously, reliable sources differ on the number of transistors in the 8087 by almost a factor of 2. Intel says 40,000, as does designer William Kahan (link). But in A Numeric Data Processor, designers Rafi Nave and John Palmer wrote that the chip contains "the equivalent of over 65,000 devices" (whatever "equivalent" means). This number is echoed by a contemporary article in Electronics (1980) that says "over 65,000 H-MOS transistors on a 78,000-mil2 die." Many other sources, such as Upgrading &amp;amp; Repairing PCs, specify 45,000 transistors. Designer Rafi Nave stated that the 8087 has 63,000 or 64,000 transistors if you count the ROM transistors directly, but if you count ROM transistors as equivalent to two transistors, then you get about 75,000 transistors. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The 8087 has a 16-bit Status Word that contains the stack top pointer, exception flags, the four-bit condition code, and other values. Although the Status Word appears to be a 16-bit register, it is not implemented as a register. Instead, parts of the Status Word are stored in various places around the chip: the stack top pointer is in the stack circuitry, the exception flags are part of the interrupt circuitry, the condition code bits are next to the datapath, and so on. When the Status Word is read or written, these various circuits are connected to the 8087's internal data bus, making the Status Word appear to be a monolithic entity. Thus, the stack circuitry includes support for reading and writing it. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Intel filed several patents on the 8087, including Numeric data processor, another Numeric data processor, Programmable bidirectional shifter, Fraction bus for use in a numeric data processor, and System bus arbitration, circuitry and methodology. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I started looking at the stack in detail to reverse engineer the micro-instruction format and determine how the 8087's microcode works. I'm working with the "Opcode Collective" on Discord on this project, but progress is slow due to the complexity of the micro-instructions. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The 8087 detects stack underflow in a similar manner. If you pop more values from the stack than are present, the tag will indicate that the register is empty and shouldn't be accessed. This triggers an invalid operation exception. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The 8087 is described in detail in The 8086 Family User's Manual, Numerics Supplement. An overview of the stack is on page 60 of The 8087 Primer by Palmer and Morse. More details are in Kahan's On the Advantages of the 8087's Stack, an unpublished course note (maybe for CS 279?) with a date of Nov 2, 1990 or perhaps August 23, 1994. Kahan discusses why the 8087's design makes it hard to handle stack overflow in How important is numerical accuracy, Dr. Dobbs, Nov. 1997. Another information source is the Oral History of Rafi Nave ‚Ü©‚Ü©&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46208409</guid><pubDate>Tue, 09 Dec 2025 18:16:44 +0000</pubDate></item><item><title>So you want to speak at software conferences?</title><link>https://dylanbeattie.net/2025/12/08/so-you-want-to-speak-at-software-conferences.html</link><description>&lt;doc fingerprint="6278511b9fc389bd"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;So You Want To Speak At Software Conferences?&lt;/head&gt;Posted by Dylan Beattie on 08 December 2025 ‚Ä¢ permalink&lt;p&gt;I run a .NET user group here in London, and we host a lot of talks from people who are relatively inexperienced presenters. Sometimes they‚Äôve done presentations internally but never spoken before a public audience. Sometimes they‚Äôre developers who have been in theatre or played in bands; people with plenty of stage experience but who haven‚Äôt presented on technical topics before - and sometimes they‚Äôve never done any kind of public presentations or performance at all. We aim to be a friendly, supportive crowd; public speaking can be daunting, and the first public outing of somebody‚Äôs first talk can be‚Ä¶ let‚Äôs just say that the presenter sometimes learns a lot more than the audience, and leave it at that.&lt;/p&gt;&lt;p&gt;But it can also be a hugely rewarding experience, and as a seasoned tech presenter who‚Äôs been doing this for a while, aspiring speakers often ask me for advice on how to take it to the next level.&lt;/p&gt;&lt;p&gt;Before we get into the specifics, there are two things to bear in mind.&lt;/p&gt;&lt;p&gt;One: ask yourself why you want to do this. What does ‚Äúthe next level‚Äù mean for you? Are you looking to promote your consultancy, or your training courses, or your software products? Do you want to become a professional speaker and actually get paid to give talks? Are you doing it ‚Äòcos you want to go places and meet people? Figure out what ‚Äúsuccess‚Äù looks like for you.&lt;/p&gt;&lt;p&gt;Two: be realistic about how much work is involved. It took me seven years to go from my first user group lightning talk, back 2008, to my first international conference. If you think you can hack together some code, write a talk about it, stick it on Sessionize and three months later you‚Äôre on your way to a major international event like NDC or Yow! or Devoxx‚Ä¶ well, no. That‚Äôs not how this works. Strap in; it‚Äôs a long ride.&lt;/p&gt;&lt;head rend="h3"&gt;Year 1: Get Good&lt;/head&gt;&lt;p&gt;Write the talk. Write a talk nobody else could do; tell a story nobody else can tell. Figure out what your audience is going to learn, and why you‚Äôre the best person to teach them that. Then give it at local user group. It might go great. It might be a train wreck. Don‚Äôt worry. That‚Äôs one of the reasons user groups exist. Learn from the experience. Fix the demos. Fix the slides. If it was too short? Write some more. If it was too long? Cut something. Give it at another user group. Do it again. Do it again. Maybe write a second talk, shop that one around a couple of user groups too.&lt;/p&gt;&lt;p&gt;If you can‚Äôt find user groups, look on Meetup.com. Yes, it‚Äôs a horrible platform, but it works; search by topic, search by region, find groups that look like a good match for your content, and ask if they‚Äôre looking for speakers. They probably are.&lt;/p&gt;&lt;head rend="h3"&gt;Year 2: Get Seen&lt;/head&gt;&lt;p&gt;After user groups and meetups come the community conferences. Typically small, one-day events, with a few tracks, and usually free (or very cheap) to attend. For me, these were the DDD events _(that‚Äôs DDD as in Developers! Developers! Developers!, not to be confused with DDD as in Domain Driven Design), _a series of one-day free developer events around the UK, organised by volunteers, usually on a Saturday so people don‚Äôt have to take time off work. They bring in a good crowd, they‚Äôre a great way to get to know other presenters and people who are involved in tech events, and you‚Äôll almost certainly meet a few people who are on the programme committees for the bigger conferences.&lt;/p&gt;&lt;p&gt;Events like this are your chance to get noticed. Turn up the day before, join the pre-conference dinner and drinks, introduce yourself. Yeah, it‚Äôs awkward when you don‚Äôt know anybody. There will be other people there who don‚Äôt know anybody and will appreciate you making the effort. Enjoy yourself, but don‚Äôt end up doing tequila shots in a karaoke bar at 3am. Not now. You‚Äôre there to give a talk, remember?&lt;/p&gt;&lt;p&gt;Go to the event. Spend the whole day there, do your talk, watch the other sessions. Communicate with the organisers. You don‚Äôt want their memorable impression of you to be a half-hour of panic and missed calls because one of their speakers has gone AWOL and nobody knows where they are.&lt;/p&gt;&lt;p&gt;Figure out how to keep in touch with the people you met. Join the Signal or WhatsApp group chat; if there isn‚Äôt one, create one. Follow them on LinkedIn, or Bluesky - be prepared to go where people are; don‚Äôt expect folks to join Mastodon just because that‚Äôs where you want to talk to them. That‚Äôs not how this works. If you really don‚Äôt want to play the social media game - and I can‚Äôt blame you - there‚Äôs always good old-fashioned email. A short email a week later saying ‚Äúhey, thanks for having me‚Äù or ‚Äúhey, I loved your session at DDD, let‚Äôs keep in touch‚Äù can pay off in a big way.&lt;/p&gt;&lt;p&gt;Finally, watch out for events that put video of their sessions online. Having a couple of YouTube links of you doing your thing in front of a live, appreciate audience can make all the difference when a programme committee is looking at a handful of talks and can only accept one of them.&lt;/p&gt;&lt;head rend="h3"&gt;Year 3: Get Accepted&lt;/head&gt;&lt;p&gt;You‚Äôve got a couple of talks. You‚Äôve delivered then enough times that you know they‚Äôre good *(and if they‚Äôre not good, make them good - or scrap them and write new ones)*. You know people. People know you. If somebody asks ‚Äúhey, do we know anybody who could do a good session about $topic‚Äù, your name comes up. You‚Äôve got a decent network of connections - group chats, LinkedIn, email addresses.&lt;/p&gt;&lt;p&gt;Now, find all the conferences in your field with an open Call for Papers (CfP), and get submitting. Dave Aronson over at codeasaur.us maintains a really useful list of CfPs which are closing soon. Check that regularly. Many events will cover your travel &amp;amp; hotel costs, although with sponsorship budgets drying up right across the industry that‚Äôs not as prevalent as it was a few years ago. If not, maybe you can persuade your employer to pay your travel - ‚Äúhey, boss, if I can get a free ticket to this amazing conference with all these industry experts, do you think the company will pay my air fare &amp;amp; hotel?‚Äù&lt;/p&gt;&lt;p&gt;Lean on your network. What are people submitting to? Which events should you look out for? Which topics are getting a lot of traction (and which topics are not?)&lt;/p&gt;&lt;p&gt;Keep your content fresh. Write new talks. Keep giving them at user groups and community events.&lt;/p&gt;&lt;p&gt;Keep your submissions focused. 2-3 talks per event; don‚Äôt submit ten wildly different abstracts to the same conference in the hope one of them will get accepted. Every selection committee I‚Äôve been on, if we see that, we assume the presenter hasn‚Äôt actually written *any* of them yet and is throwing everything they can think of into the mix and hoping one of them gets chosen. Not a great way to stand out. An open CFP at a big tech conference typically gets 20+ submissions for every available slot, which means if you reduce it to a numbers game, you‚Äôre submitting 20 talks for every one that gets accepted. Keep track of the numbers, and be objective about it.&lt;/p&gt;&lt;head rend="h3"&gt;Year 4: Get Bored.&lt;/head&gt;&lt;p&gt;It‚Äôs great fun doing this for a while‚Ä¶ but it‚Äôs also exhausting. Some people hit it hard for a few years, do all the things, go to all the places, make a lot of great friends and happy memories, and then wake up one day and decide that‚Äôs enough. Some people do a few talks, tick it off their bucket list and decide that‚Äôs enough for them. Some settle into a gentle routine of 3-4 events they‚Äôll do every year. And yes, some of us end up treating our calendars like a game of Tetris, juggling flights and trains and hotels and meetups and conferences and spending half the year on the road and the other half writing talks and workshops and all the other things it‚Äôs hard to do when you‚Äôre at the airport.&lt;/p&gt;&lt;p&gt;That‚Äôs why you gotta figure out ahead of time what ‚Äúsuccess‚Äù looks like. If you‚Äôre doing it for fun, remember to have fun - and if you find you‚Äôre not enjoying it any more? Stop. If you‚Äôre doing it as promotion or marketing? Track your leads. Make sure it‚Äôs actually generating the attention and the revenue it‚Äôs supposed to. If you‚Äôre doing it for money, be mercenary: no pay, no play. Not every event is the same, of course. In a given year I‚Äôll have some events that are fun, some that are lucrative, some that are running alongside workshops or training engagements. Just make sure you know which is which.&lt;/p&gt;&lt;p&gt;Finally: respect your audience. Whether you‚Äôre talking to five people at a meetup, fifty at a community event, or five thousand at a huge international conference: those people are the reason you get to do this. They have given up their time - and often a substantial amount of money - to hear what you have to say. They deserve your best shot, every time. If you find you‚Äôre bored, fed up, tired, running talks on autopilot or making mistakes because you just don‚Äôt care? It‚Äôs time to try something else - and remember, there‚Äôs a thousand aspiring speakers out there who would dearly love to take that spot instead of you.&lt;/p&gt;&lt;p&gt;Now get out there. Work hard, have fun, teach us awesome things, and if you ever want me to look over an abstract or a slide deck, drop me a line - [email protected]. I‚Äôd be happy to help.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46208773</guid><pubDate>Tue, 09 Dec 2025 18:42:27 +0000</pubDate></item><item><title>10 Years of Let's Encrypt</title><link>https://letsencrypt.org/2025/12/09/10-years</link><description>&lt;doc fingerprint="6d5da1590926b3fa"&gt;
  &lt;main&gt;
    &lt;p&gt;On September 14, 2015, our first publicly-trusted certificate went live. We were proud that we had issued a certificate that a significant majority of clients could accept, and had done it using automated software. Of course, in retrospect this was just the first of billions of certificates. Today, Let‚Äôs Encrypt is the largest certificate authority in the world in terms of certificates issued, the ACME protocol we helped create and standardize is integrated throughout the server ecosystem, and we‚Äôve become a household name among system administrators. We‚Äôre closing in on protecting one billion web sites.&lt;/p&gt;
    &lt;p&gt;In 2023, we marked the tenth anniversary of the creation of our nonprofit, Internet Security Research Group, which continues to host Let‚Äôs Encrypt and other public benefit infrastructure projects. Now, in honor of the tenth anniversary of Let‚Äôs Encrypt‚Äôs public certificate issuance and the start of the general availability of our services, we‚Äôre looking back at a few milestones and factors that contributed to our success.&lt;/p&gt;
    &lt;p&gt;A conspicuous part of Let‚Äôs Encrypt‚Äôs history is how thoroughly our vision of scalability through automation has succeeded.&lt;/p&gt;
    &lt;p&gt;In March 2016, we issued our one millionth certificate. Just two years later, in September 2018, we were issuing a million certificates every day. In 2020 we reached a billion total certificates issued and as of late 2025 we‚Äôre frequently issuing ten million certificates per day. We‚Äôre now on track to reach a billion active sites, probably sometime in the coming year. (The ‚Äúcertificates issued‚Äù and ‚Äúcertificates active‚Äù metrics are quite different because our certificates regularly expire and get replaced.)&lt;/p&gt;
    &lt;p&gt;The steady growth of our issuance volume shows the strength of our architecture, the validity of our vision, and the great efforts of our engineering team to scale up our own infrastructure. It also reminds us of the confidence that the Internet community is placing in us, making the use of a Let‚Äôs Encrypt certificate a normal and, dare we say, boring choice. But I often point out that our ever-growing issuance volumes are only an indirect measure of value. What ultimately matters is improving the security of people‚Äôs use of the web, which, as far as Let‚Äôs Encrypt‚Äôs contribution goes, is not measured by issuance volumes so much as by the prevalence of HTTPS encryption. For that reason, we‚Äôve always emphasized the graph of the percentage of encrypted connections that web users make (here represented by statistics from Firefox).&lt;/p&gt;
    &lt;p&gt;(These graphs are snapshots as of the date of this post; a dynamically updated version is found on our stats page.) Our biggest goal was to make a concrete, measurable security impact on the web by getting HTTPS connection prevalence to increase‚Äîand it‚Äôs worked. It took five years or so to get the global percentage from below 30% to around 80%, where it‚Äôs remained ever since. In the U.S. it has been close to 95% for a while now.&lt;/p&gt;
    &lt;p&gt;A good amount of the remaining unencrypted traffic probably comes from internal or private organizational sites (intranets), but other than that we don‚Äôt know much about it; this would be a great topic for Internet security researchers to look into.&lt;/p&gt;
    &lt;p&gt;We believe our present growth in certificate issuance volume is essentially coming from growth in the web as a whole. In other words, if we protect 20% more sites over some time period, it‚Äôs because the web itself grew by 20%.&lt;/p&gt;
    &lt;p&gt;We‚Äôve blogged about most of Let‚Äôs Encrypt‚Äôs most significant milestones as they‚Äôve happened, and I invite everyone in our community to look over those blog posts to see how far we‚Äôve come. We‚Äôve also published annual reports for the past seven years, which offer elegant and concise summaries of our work.&lt;/p&gt;
    &lt;p&gt;As I personally think back on the past decade, just a few of the many events that come to mind include:&lt;/p&gt;
    &lt;p&gt;Telling the world about the project in November 2014&lt;/p&gt;
    &lt;p&gt;Our first certificate issuance in September 2015&lt;/p&gt;
    &lt;p&gt;Our one millionth certificate in March 2016, then our 100 millionth certificate in June 2017, and then our billionth certificate in 2020&lt;/p&gt;
    &lt;p&gt;Along the way, first issuing one million certificates in a single day (in September 2018), significantly contributed to by the SquareSpace and Shopify Let‚Äôs Encrypt integrations&lt;/p&gt;
    &lt;p&gt;Just at the end of September 2025, we issued more than ten million certificates in a day for the first time.&lt;/p&gt;
    &lt;p&gt;We‚Äôve also periodically rolled out new features such as internationalized domain name support (2016), wildcard support (2018), and short-lived and IP address (2025) certificates. We‚Äôre always working on more new features for the future.&lt;/p&gt;
    &lt;p&gt;There are many technical milestones like our database server upgrades in 2021, where we found we needed a serious server infrastructure boost because of the tremendous volumes of data we were dealing with. Similarly, our original infrastructure was using Gigabit Ethernet internally, and, with the growth of our issuance volume and logging, we found that our Gigabit Ethernet network eventually became too slow to synchronize database instances! (Today we‚Äôre using 25-gig Ethernet.) More recently, we‚Äôve experimented with architectural upgrades to our ever-growing Certificate Transparency logs, and decided to go ahead with deploying those upgrades‚Äîto help us not just keep up with, but get ahead of, our continuing growth.&lt;/p&gt;
    &lt;p&gt;These kinds of growing pains and successful responses to them are nice to remember because they point to the inexorable increase in demands on our infrastructure as we‚Äôve become a more and more essential part of the Internet. I‚Äôm proud of our technical teams which have handled those increased demands capably and professionally.&lt;/p&gt;
    &lt;p&gt;I also recall the ongoing work involved in making sure our certificates would be as widely accepted as possible, which has meant managing the original cross-signature from IdenTrust, and subsequently creating and propagating our own root CA certificates. This process has required PKI engineering, key ceremonies, root program interactions, documentation, and community support associated with certificate migrations. Most users never have reason to look behind the scenes at our chains of trust, but our engineers update it as root and intermediate certificates have been replaced. We‚Äôve engaged at the CA/B Forum, IETF, and in other venues with the browser root programs to help shape the web PKI as a technical leader.&lt;/p&gt;
    &lt;p&gt;As I wrote in 2020, our ideal of complete automation of the web PKI aims at a world where most site owners wouldn‚Äôt even need to think about certificates at all. We continue to get closer and closer to that world, which creates a risk that people will take us and our services for granted, as the details of certificate renewal occupy less of site operators‚Äô mental energy. As I said at the time,&lt;/p&gt;
    &lt;p&gt;When your strategy as a nonprofit is to get out of the way, to offer services that people don‚Äôt need to think about, you‚Äôre running a real risk that you‚Äôll eventually be taken for granted. There is a tension between wanting your work to be invisible and the need for recognition of its value. If people aren‚Äôt aware of how valuable our services are then we may not get the support we need to continue providing them.&lt;/p&gt;
    &lt;p&gt;I‚Äôm also grateful to our communications and fundraising staff who help make clear what we‚Äôre doing every day and how we‚Äôre making the Internet safer.&lt;/p&gt;
    &lt;p&gt;Our community continually recognizes our work in tangible ways by using our certificates‚Äînow by the tens of millions per day‚Äîand by sponsoring us.&lt;/p&gt;
    &lt;p&gt;We were honored to be recognized with awards including the 2022 Levchin Prize for Real-World Cryptography and the 2019 O‚ÄôReilly Open Source Award. In October of this year some of the individuals who got Let‚Äôs Encrypt started were honored to receive the IEEE Cybersecurity Award for Practice.&lt;/p&gt;
    &lt;p&gt;We documented the history, design, and goals of the project in an academic paper at the ACM CCS ‚Äò19 conference, which has subsequently been cited hundreds of times in academic research.&lt;/p&gt;
    &lt;p&gt;Ten years later, I‚Äôm still deeply grateful to the five initial sponsors that got Let‚Äôs Encrypt off the ground - Mozilla, EFF, Cisco, Akamai, and IdenTrust. When they committed significant resources to the project, it was just an ambitious idea. They saw the potential and believed in our team, and because of that we were able to build the service we operate today.&lt;/p&gt;
    &lt;p&gt;I‚Äôd like to particularly recognize IdenTrust, a PKI company that worked as a partner from the outset and enabled us to issue publicly-trusted certificates via a cross-signature from one of their roots. We would simply not have been able to launch our publicly-trusted certificate service without them. Back when I first told them that we were starting a new nonprofit certificate authority that would give away millions of certificates for free, there wasn‚Äôt any precedent for this arrangement, and there wasn‚Äôt necessarily much reason for IdenTrust to pay attention to our proposal. But the company really understood what we were trying to do and was willing to engage from the beginning. Ultimately, IdenTrust‚Äôs support made our original issuance model a reality.&lt;/p&gt;
    &lt;p&gt;I‚Äôm proud of what we have achieved with our staff, partners, and donors over the past ten years. I hope to be even more proud of the next ten years, as we use our strong footing to continue to pursue our mission to protect Internet users by lowering monetary, technological, and informational barriers to a more secure and privacy-respecting Internet.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs Encrypt is a project of the nonprofit Internet Security Research Group, a 501(c)(3) nonprofit. You can help us make the next ten years great as well by donating or becoming a sponsor.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46208962</guid><pubDate>Tue, 09 Dec 2025 18:54:55 +0000</pubDate></item><item><title>Agentic AI Foundation</title><link>https://block.xyz/inside/block-anthropic-and-openai-launch-the-agentic-ai-foundation</link><description>&lt;doc fingerprint="2f02334316bba1b9"&gt;
  &lt;main&gt;
    &lt;p&gt;An initiative to advance open source agentic AI under the Linux Foundation umbrella&lt;/p&gt;
    &lt;p&gt;Today, the industry is making a pivotal choice. Block, Anthropic, OpenAI, and other leaders in AI are launching the Agentic AI Foundation (AAIF) to ensure agentic AI develops as an open, collaborative ecosystem. Agentic AI refers to artificial intelligence systems that can take initiative, make decisions, and act independently to achieve goals with minimal human direction.&lt;/p&gt;
    &lt;p&gt;The AAIF is a vendor-neutral home for open source agentic AI projects where no single company dominates, providing funding for critical community programs and research, and building open protocols that let systems from different builders work together seamlessly. Block is contributing its open source agent goose to the AAIF, along with Anthropic√¢s Model Context Protocol (MCP) and OpenAI√¢s AGENTS.md.&lt;/p&gt;
    &lt;p&gt;In addition to the three founding members, the AAIF√¢s platinum members include Amazon Web Services, Bloomberg, Cloudflare, Google, and MicrosoftI. Gold members of the AAIF include Adyen, Arcade.dev, Cisco, Datadog, Docker, Ericsson, IBM, JetBrains, Okta, Oracle, Runlayer, Salesforce, SAP, Shopify, Snowflake, Temporal, Tetrate, and Twilio Inc. Silver members of the AAIF include Apify, Chronosphere, Cosmonic, Elasticsearch, Eve Security, Hugging Face, Kubermatic, KYXStart, LanceDB, Mirantis, NinjaTech AI, Obot.ai, Prefect.io, Pydantic, Shinkai.com, Solo.io, Spectro Cloud, Stacklok, SUSE, Uber, WorkOS, Zapier and ZED.&lt;/p&gt;
    &lt;p&gt;AI systems represent one of the most significant technological shifts in decades. These systems are fundamentally reshaping how developers build software, how businesses operate, and how people solve complex problems. Use of autonomous agents promises to accelerate this even further.&lt;/p&gt;
    &lt;p&gt;But we're at a critical juncture. We have an unprecedented opportunity to shape how this technology develops. Will we build open, interoperable infrastructure that serves everyone? Or will we see fragmentation that limits this powerful technology's potential? Without collaborative open development, we risk missing the full potential of agentic AI where everyone benefits thanks to open standards, open protocols, and open systems on which other tools are built. We also risk concentrating the power in the hands of a few, which dampers competition and restricts accessibility. In a proprietary, siloed ecosystem, breakthrough ideas can't easily build on each other, integration challenges slow enterprise adoption and limit use cases, accessibility barriers prevent smaller organizations from benefiting, and research limitations slow academic and independent innovation.&lt;/p&gt;
    &lt;p&gt;History proves there√¢s a better path. The Internet, Linux, and the Web succeeded precisely because they were open. They empowered anyone with talent and determination to build, innovate, and create value on top of open infrastructure. Agentic AI deserves the same opportunity. We hope that the AAIF can become what the W3C is for the Web: a set of standards and protocols that guarantee interoperability, open access, and freedom of choice with open source reference implementations.&lt;/p&gt;
    &lt;p&gt;The Agentic AI Foundation provides a neutral home where companies, researchers, and independent developers can collaborate on open source agentic AI.&lt;/p&gt;
    &lt;p&gt;Following the Linux Foundation's trusted governance model, AAIF will operate on several core principles:&lt;/p&gt;
    &lt;p&gt;Open Governance: AAIF operates under a transparent and inclusive governance model where contributors from all backgrounds are empowered to shape the direction of the foundation and its projects. Decisions, policies, and criteria are open and accessible to members.&lt;/p&gt;
    &lt;p&gt;AI Innovation: AAIF encourages agentic AI innovation and exploration from a diverse and collaborative ecosystem of open source developers, researchers, and practitioners. We move at the speed of AI, keeping governance small and responsive.&lt;/p&gt;
    &lt;p&gt;Sustainability and Neutrality: AAIF provides neutral infrastructure and funding mechanisms to ensure long-term sustainability of projects. Project inclusion is based on demonstrated adoption, quality, and community health - not how well funded it may be.&lt;/p&gt;
    &lt;p&gt;Focused, not broad: The AAIF exists to further open source agentic AI - not to cover all of open source AI, machine learning, or data science.&lt;/p&gt;
    &lt;p&gt;AAIF launches with contributions from Block, Anthropic, and OpenAI:&lt;/p&gt;
    &lt;p&gt;goose, Block's open source agentic AI framework, will transition to community governance under AAIF. Since its release earlier this year, goose has become a reference implementation for Anthropic's Model Context Protocol (MCP) and has attracted thousands of developers worldwide. Under AAIF, goose will maintain its open source license and commercial-friendly terms while gaining the benefits of neutral governance and broader community input.&lt;/p&gt;
    &lt;p&gt;Model Context Protocol (MCP), developed by Anthropic, is an open protocol that enables seamless integration between AI systems and external data sources, providing a standardized way for AI agents to access context. MCP demonstrates AAIF's potential as the neutral hub for cross-industry agentic AI standards. From MCP√¢s very first release, Block engineers have been active contributors, and several continue to participate as members of the MCP steering committee.&lt;/p&gt;
    &lt;p&gt;AGENTS.md, developed by OpenAI, is an open format for guiding coding agents. It is used by more than 20,000 open source projects and serves as a README for agents: a dedicated, predictable place to provide the context and instructions to help AI coding agents work on projects.&lt;/p&gt;
    &lt;p&gt;Additional projects from other member organizations have been proposed and are being evaluated to join, and will be selected based on demonstrated adoption, strong governance, and alignment with AAIF's mission to advance open source agentic AI.&lt;/p&gt;
    &lt;p&gt;AAIF succeeds only if it truly represents the community. Whether you're a developer, researcher, company, or simply someone who believes AI should be open, there are multiple ways to participate:&lt;/p&gt;
    &lt;p&gt;Contribute to projects: All AAIF projects welcome community contributions. Check project repositories for contribution guidelines and good first issues.&lt;/p&gt;
    &lt;p&gt;Join the conversation &amp;amp; propose new projects: Have an agentic AI project that could benefit from neutral governance, foundation support, collaboration with leaders in agentic AI, and more visibility? Projects that promote the agentic AI ecosystem, operate under OSI-approved open source licenses, demonstrate open governance, and foster community growth are welcome to apply. For more information about the AAIF and how to become a member, please visit AAIF.io.&lt;/p&gt;
    &lt;p&gt;Attend events: Participate in AAIF-sponsored conferences, hackathons, and meetups to connect with the community and learn about the latest developments.&lt;/p&gt;
    &lt;p&gt;The foundation's governance structure ensures that community voices matter, not just corporate interests. Technical decisions will be made by project maintainers and steering committees, with AAIF providing resources and neutral oversight.&lt;/p&gt;
    &lt;p&gt;The next decade of AI development will be defined by choices we make today. By establishing the AAIF, the industry is making a clear statement: agentic AI will be open, accessible, and community-driven. The world has repeatedly proven that open collaboration produces better outcomes than competition in isolation. The AAIF provides the structure for that open collaboration to flourish in agentic AI.&lt;/p&gt;
    &lt;p&gt;This isn't just about technology: it's about values. It's about ensuring that powerful tools serve everyone. It's about creating an ecosystem where the best ideas win, regardless of where they come from.&lt;/p&gt;
    &lt;p&gt;The future is being written in the open. Join us.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46209846</guid><pubDate>Tue, 09 Dec 2025 20:00:39 +0000</pubDate></item><item><title>Django: what‚Äôs new in 6.0</title><link>https://adamj.eu/tech/2025/12/03/django-whats-new-6.0/</link><description>&lt;doc fingerprint="babb1d3d4db5baca"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Django: what√¢s new in 6.0&lt;/head&gt;
    &lt;p&gt;Django 6.0 was released today, starting another release cycle for the loved and long-lived Python web framework (now 20 years old!). It comes with a mosaic of new features, contributed to by many, some of which I am happy to have helped with. Below is my pick of highlights from the release notes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Upgrade with help from django-upgrade&lt;/head&gt;
    &lt;p&gt;If you√¢re upgrading a project from Django 5.2 or earlier, please try my tool django-upgrade. It will automatically update old Django code to use new features, fixing some deprecation warnings for you, including five fixers for Django 6.0. (One day, I√¢ll propose django-upgrade to become an official Django project, when energy and time permit√¢¬¶)&lt;/p&gt;
    &lt;head rend="h2"&gt;Template partials&lt;/head&gt;
    &lt;p&gt;There are four headline features in Django 6.0, which we√¢ll cover before other notable changes, starting with this one:&lt;/p&gt;
    &lt;quote&gt;The Django Template Language now supports template partials, making it easier to encapsulate and reuse small named fragments within a template file.&lt;/quote&gt;
    &lt;p&gt;Partials are sections of a template marked by the new &lt;code&gt;{% partialdef %}&lt;/code&gt; and &lt;code&gt;{% endpartialdef %}&lt;/code&gt; tags. They can be reused within the same template or rendered in isolation. Let√¢s look at examples for each use case in turn.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reuse partials within the same template&lt;/head&gt;
    &lt;p&gt;The below template reuses a partial called &lt;code&gt;filter_controls&lt;/code&gt; within the same template. It√¢s defined once at the top of the template, then used twice later on. Using a partial allows the template avoid repetition without pushing the content into a separate include file.&lt;/p&gt;
    &lt;code&gt;&amp;lt;section id=videos&amp;gt;
  {% partialdef filter_controls %}
    &amp;lt;form&amp;gt;
      {{ filter_form }}
    &amp;lt;/form&amp;gt;
  {% endpartialdef %}

  {% partial filter_controls %}

  &amp;lt;ul&amp;gt;
    {% for video in videos %}
      &amp;lt;li&amp;gt;
        &amp;lt;h2&amp;gt;{{ video.title }}&amp;lt;/h2&amp;gt;
        ...
      &amp;lt;/li&amp;gt;
    {% endfor %}
  &amp;lt;/ul&amp;gt;

  {% partial filter_controls %}
&amp;lt;/section&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Actually, we can simplify this pattern further, by using the &lt;code&gt;inline&lt;/code&gt; option on the &lt;code&gt;partialdef&lt;/code&gt; tag, which causes the definition to also render in place:&lt;/p&gt;
    &lt;code&gt;&amp;lt;section id=videos&amp;gt;
  {% partialdef filter_controls inline %}
    &amp;lt;form&amp;gt;
      {{ filter_form }}
    &amp;lt;/form&amp;gt;
  {% endpartialdef %}

  &amp;lt;ul&amp;gt;
    {% for video in videos %}
      &amp;lt;li&amp;gt;
        &amp;lt;h2&amp;gt;{{ video.title }}&amp;lt;/h2&amp;gt;
        ...
      &amp;lt;/li&amp;gt;
    {% endfor %}
  &amp;lt;/ul&amp;gt;

  {% partial filter_controls %}
&amp;lt;/section&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Reach for this pattern any time you find yourself repeating template code within the same template. Because partials can use variables, you can also use them to de-duplicate when rendering similar controls with different data.&lt;/p&gt;
    &lt;head rend="h3"&gt;Render partials in isolation&lt;/head&gt;
    &lt;p&gt;The below template defines a &lt;code&gt;view_count&lt;/code&gt; partial that√¢s intended to be re-rendered in isolation. It uses the &lt;code&gt;inline&lt;/code&gt; option, so when the whole template is rendered, the partial is included.&lt;/p&gt;
    &lt;p&gt;The page uses htmx, via my django-htmx package, to periodically refresh the view count, through the &lt;code&gt;hx-*&lt;/code&gt; attributes. The request from htmx goes to a dedicated view that re-renders the &lt;code&gt;view_count&lt;/code&gt; partial.&lt;/p&gt;
    &lt;code&gt;{% load django_htmx %}
&amp;lt;!doctype html&amp;gt;
&amp;lt;html&amp;gt;
  &amp;lt;body&amp;gt;
    &amp;lt;h1&amp;gt;{{ video.title }}&amp;lt;/h1&amp;gt;
    &amp;lt;video width=1280 height=720 controls&amp;gt;
      &amp;lt;source src="{{ video.file.url }}" type="video/mp4"&amp;gt;
      Your browser does not support the video tag.
    &amp;lt;/video&amp;gt;

    {% partialdef view_count inline %}
    &amp;lt;section
      class=view-count
      hx-trigger="every 1s"
      hx-swap=outerHTML
      hx-get="{% url 'video-view-count' video.id %}"
    &amp;gt;
      {{ video.view_count }} views
    &amp;lt;/section&amp;gt;
    {% endpartialdef %}

    {% htmx_script %}
  &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;
    &lt;p&gt;The relevant code for the two views could look like this:&lt;/p&gt;
    &lt;code&gt;from django.shortcuts import render


def video(request, video_id):
    ...
    return render(request, "video.html", {"video": video})


def video_view_count(request, video_id):
    ...
    return render(request, "video.html#view_count", {"video": video})
&lt;/code&gt;
    &lt;p&gt;The initial &lt;code&gt;video&lt;/code&gt; view renders the full template &lt;code&gt;video.html&lt;/code&gt;. The &lt;code&gt;video_view_count&lt;/code&gt; view renders just the &lt;code&gt;view_count&lt;/code&gt; partial, by appending &lt;code&gt;#view_count&lt;/code&gt; to the template name. This syntax is similar to how you√¢d reference an HTML fragment by its ID in a URL.&lt;/p&gt;
    &lt;head rend="h3"&gt;History&lt;/head&gt;
    &lt;p&gt;htmx was the main motivation for this feature, as promoted by htmx creator Carson Gross in a cross-framework review post. Using partials definitely helps maintain √¢Locality of behaviour√¢ within your templates, easing authoring, debugging, and maintenance by avoiding template file sprawl.&lt;/p&gt;
    &lt;p&gt;Django√¢s support for template partials was initially developed by Carlton Gibson in the django-template-partials package, which remains available for older Django versions. The integration into Django itself was done in a Google Summer of Code project this year, worked on by student Farhan Ali and mentored by Carlton, in Ticket #36410. You can read more about the development process in Farhan√¢s retrospective blog post. Many thanks to Farhan for authoring, Carlton for mentoring, and Natalia Bidart, Nick Pope, and Sarah Boyce for reviewing!&lt;/p&gt;
    &lt;head rend="h2"&gt;Tasks framework&lt;/head&gt;
    &lt;p&gt;The next headline feature we√¢re covering:&lt;/p&gt;
    &lt;quote&gt;Django now includes a built-in Tasks framework for running code outside the HTTP request√¢response cycle. This enables offloading work, such as sending emails or processing data, to background workers.&lt;/quote&gt;
    &lt;p&gt;Basically, there√¢s a new API for defining and enqueuing background tasks√¢very cool!&lt;/p&gt;
    &lt;p&gt;Background tasks are a way of running code outside of the request-response cycle. They√¢re a common requirement in web applications, used for sending emails, processing images, generating reports, and more.&lt;/p&gt;
    &lt;p&gt;Historically, Django has not provided any system for background tasks, and kind of ignored the problem space altogether. Developers have instead relied on third-party packages like Celery or Django Q2. While these systems are fine, they can be complex to set up and maintain, and often don√¢t √¢go with the grain√¢ of Django.&lt;/p&gt;
    &lt;p&gt;The new Tasks framework fills this gap by providing an interface to define background tasks, which task runner packages can then integrate with. This common ground allows third-party Django packages to define tasks in a standard way, assuming you√¢ll be using a compatible task runner to execute them.&lt;/p&gt;
    &lt;p&gt;Define tasks with the new &lt;code&gt;@task&lt;/code&gt; decorator:&lt;/p&gt;
    &lt;code&gt;from django.tasks import task


@task
def resize_video(video_id): ...
&lt;/code&gt;
    &lt;p&gt;√¢¬¶and enqueue them for background execution with the &lt;code&gt;Task.enqueue()&lt;/code&gt; method:&lt;/p&gt;
    &lt;code&gt;from example.tasks import resize_video


def upload_video(request):
    ...
    resize_video.enqueue(video.id)
    ...
&lt;/code&gt;
    &lt;head rend="h3"&gt;Execute tasks&lt;/head&gt;
    &lt;p&gt;At this time, Django does not include a production-ready task backend, only two that are suitable for development and testing:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;ImmediateBackend&lt;/code&gt;- runs tasks synchronously, blocking until they complete.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;DummyBackend&lt;/code&gt;- does nothing when tasks are enqueued, but allows them to be inspected later. Useful for tests, where you can assert that tasks were enqueued without actually running them.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For production use, you√¢ll need to use a third-party package that implements one, for which django-tasks, the reference implementation, is the primary option. It provides &lt;code&gt;DatabaseBackend&lt;/code&gt; for storing tasks in your SQL database, a fine solution for many projects, avoiding extra infrastructure and allowing atomic task enqueuing within database transactions. We may see this backend merged into Django in due course, or at least become an official package, to help make Django √¢batteries included√¢ for background tasks.&lt;/p&gt;
    &lt;p&gt;To use django-tasks√¢ &lt;code&gt;DatabaseBackend&lt;/code&gt; today, first install the package:&lt;/p&gt;
    &lt;code&gt;uv add django-tasks
&lt;/code&gt;
    &lt;p&gt;Second, add these two apps to your &lt;code&gt;INSTALLED_APPS&lt;/code&gt; setting:&lt;/p&gt;
    &lt;code&gt;INSTALLED_APPS = [
    # ...
    "django_tasks",
    "django_tasks.backends.database",
    # ...
]
&lt;/code&gt;
    &lt;p&gt;Third, configure &lt;code&gt;DatabaseBackend&lt;/code&gt; as your tasks backend in the new &lt;code&gt;TASKS&lt;/code&gt; setting:&lt;/p&gt;
    &lt;code&gt;TASKS = {
    "default": {
        "BACKEND": "django_tasks.backends.database.DatabaseBackend",
    },
}
&lt;/code&gt;
    &lt;p&gt;Fourth, run migrations to create the necessary database tables:&lt;/p&gt;
    &lt;code&gt;$ ./manage.py migrate
&lt;/code&gt;
    &lt;p&gt;Finally, to run the task worker process, use the package√¢s &lt;code&gt;db_worker&lt;/code&gt; management command:&lt;/p&gt;
    &lt;code&gt;$ ./manage.py db_worker
Starting worker worker_id=jWLMLrms3C2NcUODYeatsqCFvd5rK6DM queues=default
&lt;/code&gt;
    &lt;p&gt;This process runs indefinitely, polling for tasks and executing them, logging events as it goes:&lt;/p&gt;
    &lt;code&gt;Task id=10b794ed-9b64-4eed-950c-fcc92cd6784b path=example.tasks.echo state=RUNNING
Hello from test task!
Task id=10b794ed-9b64-4eed-950c-fcc92cd6784b path=example.tasks.echo state=SUCCEEDED
&lt;/code&gt;
    &lt;p&gt;You√¢ll want to run &lt;code&gt;db_worker&lt;/code&gt; in production, and also in development if you want to test background task execution.&lt;/p&gt;
    &lt;head rend="h3"&gt;History&lt;/head&gt;
    &lt;p&gt;It√¢s been a long path to get the Tasks framework into Django, and I√¢m super excited to see it finally available in Django 6.0. Jake Howard started on the idea for Wagtail, a Django-powered CMS, back in 2021, as they have a need for common task definitions across their package ecosystem. He upgraded the idea to target Django itself in 2024, when he proposed DEP 0014. As a member of the Steering Council at the time, I had the pleasure of helping review and accept the DEP.&lt;/p&gt;
    &lt;p&gt;Since then, Jake has been leading the implementation effort, building pieces first in the separate django-tasks package before preparing them for inclusion in Django itself. This step was done under Ticket #35859, with a pull request that took nearly a year to review and land. Thanks to Jake for his perseverance here, and to all reviewers: Andreas N√É¬º√Élein, Dave Gaeddert, Eric Holscher, Jacob Walls, Jake Howard, Kamal Mustafa, @rtr1, @tcely, Oliver Haas, Ran Benita, Raphael Gaschignard, and Sarah Boyce.&lt;/p&gt;
    &lt;p&gt;Read more about this feature and story in Jake√¢s post celebrating when it was merged.&lt;/p&gt;
    &lt;head rend="h2"&gt;Content Security Policy support&lt;/head&gt;
    &lt;p&gt;Our third headline feature:&lt;/p&gt;
    &lt;quote&gt;Built-in support for the Content Security Policy (CSP) standard is now available, making it easier to protect web applications against content injection attacks such as cross-site scripting (XSS). CSP allows declaring trusted sources of content by giving browsers strict rules about which scripts, styles, images, or other resources can be loaded.&lt;/quote&gt;
    &lt;p&gt;I√¢m really excited about this, because I√¢m a bit of a security nerd who√¢s been deploying CSP for client projects for years.&lt;/p&gt;
    &lt;p&gt;CSP is a security standard that can protect your site from cross-site scripting (XSS) and other code injection attacks. You set a &lt;code&gt;content-security-policy&lt;/code&gt; header to declare which content sources are trusted for your site, and then browsers will block content from other sources. For example, you might declare that only scripts your domain are allowed, so an attacker who manages to inject a &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; tag pointing to evil.com would be thwarted, as the browser would refuse to load it.&lt;/p&gt;
    &lt;p&gt;Previously, Django had no built-in support for CSP, and developers had to rely on building their own, or using a third-party package like the very popular django-csp. But this was a little bit inconvenient, as it meant that other third-party packages couldn√¢t reliably integrate with CSP, as there was no common API to do so.&lt;/p&gt;
    &lt;p&gt;The new CSP support provides all the core features that django-csp did, with a slightly tidier and more Djangoey API. To get started, first add &lt;code&gt;ContentSecurityPolicyMiddleware&lt;/code&gt; to your &lt;code&gt;MIDDLEWARE&lt;/code&gt; setting:&lt;/p&gt;
    &lt;code&gt;MIDDLEWARE = [
    # ...
    "django.middleware.csp.ContentSecurityPolicyMiddleware",
    # ...
]
&lt;/code&gt;
    &lt;p&gt;Place it next to &lt;code&gt;SecurityMiddleware&lt;/code&gt;, as it similarly adds security-related headers to all responses. (You do have &lt;code&gt;SecurityMiddleware&lt;/code&gt; enabled, right?)&lt;/p&gt;
    &lt;p&gt;Second, configure your CSP policy using the new settings:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;SECURE_CSP&lt;/code&gt;to configure the&lt;code&gt;content-security-policy&lt;/code&gt;header, which is your actively enforced policy.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SECURE_CSP_REPORT_ONLY&lt;/code&gt;to configure the&lt;code&gt;content-security-policy-report-only&lt;/code&gt;header, which sets a non-enforced policy for which browsers report violations to a specified endpoint. This option is useful for testing and monitoring a policy before enforcing it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example, to adopt the nonce-based strict CSP recommended by web.dev, you could start with the following setting:&lt;/p&gt;
    &lt;code&gt;from django.utils.csp import CSP

SECURE_CSP_REPORT_ONLY = {
    "script-src": [CSP.NONCE, CSP.STRICT_DYNAMIC],
    "object-src": [CSP.NONE],
    "base-uri": [CSP.NONE],
}
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;CSP&lt;/code&gt; enum used above provides constants for CSP directives, to help avoid typos.&lt;/p&gt;
    &lt;p&gt;This policy is quite restrictive and will break most existing sites if deployed as-is, because it requires nonces, as covered next. That√¢s why the example shows starting with the report-only mode header, to help track down places that need fixing before enforcing the policy. You√¢d later change to setting the &lt;code&gt;SECURE_CSP&lt;/code&gt; setting to enforce the policy.&lt;/p&gt;
    &lt;p&gt;Anyway, those are the two basic steps to set up the new CSP support!&lt;/p&gt;
    &lt;head rend="h3"&gt;Nonce generation&lt;/head&gt;
    &lt;p&gt;A key part of the new feature is that nonce generation is now built-in to Django, when using the CSP middleware. Nonces are a security feature in CSP that allow you to mark specific &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;style&amp;gt;&lt;/code&gt; tags as trusted with a &lt;code&gt;nonce&lt;/code&gt; attribute:&lt;/p&gt;
    &lt;code&gt;&amp;lt;script src=/static/app.js type=module nonce=55vsH4w7ATHB85C3MbPr_g&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;
    &lt;p&gt;The nonce value is randomly generated per-request, and included in the CSP header. An attacker performing content injection couldn√¢t guess the nonce, so browsers can trust only those tags that include the correct nonce. Because nonce generation is now part of Django, third-party packages can depend on it for their &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;style&amp;gt;&lt;/code&gt; tags and they√¢ll continue to work if you adopt CSP with nonces.&lt;/p&gt;
    &lt;p&gt;Nonces are the recommended way to use CSP today, avoiding problems with previous allow-list based approaches. That√¢s why the above recommended policy enables them. To adopt a nonce-based policy, you√¢ll need to annotate your &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;style&amp;gt;&lt;/code&gt; tags with the nonce value through the following steps.&lt;/p&gt;
    &lt;p&gt;First, add the new &lt;code&gt;csp&lt;/code&gt; template context processor to your &lt;code&gt;TEMPLATES&lt;/code&gt; setting:&lt;/p&gt;
    &lt;code&gt;TEMPLATES = [
    {
        "BACKEND": "django.template.backends.django.DjangoTemplates",
        "OPTIONS": {
            "context_processors": [
                # ...
                "django.template.context_processors.csp",
            ],
        },
    },
]
&lt;/code&gt;
    &lt;p&gt;Second, annotate your &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;style&amp;gt;&lt;/code&gt; tags with &lt;code&gt;nonce="{{ csp_nonce }}"&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;-   &amp;lt;script src="{% static 'app.js' %}" type="module"&amp;gt;&amp;lt;/script&amp;gt;
+   &amp;lt;script src="{% static 'app.js' %}" type="module" nonce="{{ csp_nonce }}"&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;
    &lt;p&gt;This can be tedious and error-prone, hence using the report-only mode first to monitor violations might be useful, especially on larger projects.&lt;/p&gt;
    &lt;p&gt;Anyway, deploying CSP right would be another post in itself, or even a book chapter, so we√¢ll stop here for now. For more info, check out that web.dev article and the MDN CSP guide.&lt;/p&gt;
    &lt;head rend="h3"&gt;History&lt;/head&gt;
    &lt;p&gt;CSP itself was proposed for browsers way back in 2004, and was first implemented in Mozilla Firefox version 4, released 2011. That same year, Django Ticket #15727 was opened, proposing adding CSP support to Django. Mozilla created django-csp from 2010, before the first public availability of CSP, using it on their own Django-powered sites. The first comment on Ticket #15727 pointed to django-csp, and the community basically rolled with it as the de facto solution.&lt;/p&gt;
    &lt;p&gt;Over the years, CSP itself evolved, as did django-csp, with Rob Hudson ending up as its maintainer. Focusing on the package motivated to finally get CSP into Django itself. He made a draft PR and posted on Ticket #15727 in 2024, which I enjoyed helping review. He iterated on the PR over the next 13 months until it was finally merged for Django 6.0. Thanks to Rob for his heroic dedication here, and to all reviewers: Benjamin Balder Bach, Carlton Gibson, Collin Anderson, David Sanders, David Smith, Florian Apolloner, Harro van der Klauw, Jake Howard, Natalia Bidart, Paolo Melchiorre, Sarah Boyce, and S√É¬©bastien Corbin.&lt;/p&gt;
    &lt;head rend="h2"&gt;Email API updates&lt;/head&gt;
    &lt;p&gt;The fourth and final headline feature:&lt;/p&gt;
    &lt;code&gt;Email handling in Django now uses Python√¢s modern email API, introduced in Python 3.6. This API, centered around the  email.message.EmailMessage class, offers a cleaner and Unicode-friendly interface for composing and sending emails.&lt;/code&gt;
    &lt;p&gt;This is a major change, but it√¢s unlikely to affect projects using basic email features. You can still use Django√¢s &lt;code&gt;send_mail()&lt;/code&gt; function and &lt;code&gt;EmailMessage&lt;/code&gt; class as before, like:&lt;/p&gt;
    &lt;code&gt;from django.core.mail import EmailMessage

email = EmailMessage(
    subject="√∞¬º Need more bamboo",
    body="We are desperately low, please restock before the pandas find out!",
    from_email="zookeeper@example.com",
    to=["supplies@example.com"],
)
email.attach_file("/media/bamboo_cupboard.jpg")
email.send()
&lt;/code&gt;
    &lt;p&gt;The key change is that, under-the-hood, when you call &lt;code&gt;send()&lt;/code&gt; on a Django &lt;code&gt;EmailMessage&lt;/code&gt; object, it now translates itself into a Python√¢s newer &lt;code&gt;email.message.EmailMessage&lt;/code&gt; type before sending.&lt;/p&gt;
    &lt;p&gt;Modernizing provides these benefits:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fewer bugs - many edge case bugs in Python√¢s old email API have been fixed in the new one.&lt;/item&gt;
      &lt;item&gt;Django is less hacky - a bunch of workarounds and security fixes in Django√¢s email code have been removed.&lt;/item&gt;
      &lt;item&gt;More convenient API - the new API supports some niceties, like the below inline attachment example.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Easier inline attachments with &lt;code&gt;MIMEPart&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;Django√¢s &lt;code&gt;EmailMessage.attach()&lt;/code&gt; method allows you to attach a file as an attachment. Emails support images as inline attachments, which can be displayed within the HTML email body.&lt;/p&gt;
    &lt;p&gt;While you could previously use &lt;code&gt;EmailMessage.attach()&lt;/code&gt; to add inline attachments, it was a bit fiddly, using a legacy class. Now, you can call the method with a Python &lt;code&gt;email.message.MIMEPart&lt;/code&gt; object to add an inline attachment in a few steps:&lt;/p&gt;
    &lt;code&gt;import email.utils
from email.message import MIMEPart
from django.core.mail import EmailMultiAlternatives

message = EmailMultiAlternatives(
    subject="Cute Panda Alert",
    body="Here's a cute panda picture for you!",
    from_email="cute@example.com",
    to=["fans@example.com"],
)
with open("panda.jpg", "rb") as f:
    panda_jpeg = f.read()

cid = email.utils.make_msgid()
inline_image = MIMEPart()
inline_image.set_content(
    panda_jpeg,
    maintype="image",
    subtype="jpeg",
    disposition="inline",
    cid=cid,
)
message.attach(inline_image)
message.attach_alternative(
    f'&amp;lt;h1&amp;gt;Cute panda baby alert!&amp;lt;/h1&amp;gt;&amp;lt;img src="cid:{cid[1:-1]}"&amp;gt;',
    "text/html",
)
&lt;/code&gt;
    &lt;p&gt;It√¢s not the simplest API, but it does expose all the power of the underlying email system, and it√¢s better than the past situation.&lt;/p&gt;
    &lt;head rend="h3"&gt;History&lt;/head&gt;
    &lt;p&gt;The new email API was added to Python as provisional in version 3.4 (2014), and made stable in version 3.6 (2016). The legacy API, however, was never planned for deprecation, so there was never any deadline to upgrade Django√¢s email handling.&lt;/p&gt;
    &lt;p&gt;In 2024, Mike Edmunds posted on the (old) django-developers mailing list, proposing the upgrade with strong reasoning and planning. This conversation led to Ticket #35581, which he worked on for eight months until it was merged. Many thanks to Mike for leading this effort, and to Sarah Boyce for reviewing! Email is not a glamorous feature, but it√¢s a critical communication channel for nearly every Django project, so props for this.&lt;/p&gt;
    &lt;head rend="h2"&gt;Positional arguments in &lt;code&gt;django.core.mail&lt;/code&gt; APIs&lt;/head&gt;
    &lt;p&gt;We√¢re now out of the headline features and onto the √¢minor√¢ changes, starting with this deprecation related to the above email changes:&lt;/p&gt;
    &lt;quote&gt;&lt;code&gt;django.core.mail&lt;/code&gt;APIs now require keyword arguments for less commonly used parameters. Using positional arguments for these now emits a deprecation warning and will raise a&lt;code&gt;TypeError&lt;/code&gt;when the deprecation period ends:&lt;item&gt;All optional parameters (&lt;/item&gt;&lt;code&gt;fail_silently&lt;/code&gt;and later) must be passed as keyword arguments to&lt;code&gt;get_connection()&lt;/code&gt;,&lt;code&gt;mail_admins()&lt;/code&gt;,&lt;code&gt;mail_managers()&lt;/code&gt;,&lt;code&gt;send_mail()&lt;/code&gt;, and&lt;code&gt;send_mass_mail()&lt;/code&gt;.&lt;item&gt;All parameters must be passed as keyword arguments when creating an&lt;/item&gt;&lt;code&gt;EmailMessage&lt;/code&gt;or&lt;code&gt;EmailMultiAlternatives&lt;/code&gt;instance, except for the first four (&lt;code&gt;subject&lt;/code&gt;,&lt;code&gt;body&lt;/code&gt;,&lt;code&gt;from_email&lt;/code&gt;, and&lt;code&gt;to&lt;/code&gt;), which may still be passed either as positional or keyword arguments.&lt;/quote&gt;
    &lt;p&gt;Previously, Django would let you pass all parameters positionally, which gets a bit silly and hard to read with long parameter lists, like:&lt;/p&gt;
    &lt;code&gt;from django.core.mail import send_mail

send_mail(
    "√∞¬º Panda of the week",
    "This week√¢s panda is Po Ping, sha-sha booey!",
    "updates@example.com",
    ["adam@example.com"],
    True,
)
&lt;/code&gt;
    &lt;p&gt;The final &lt;code&gt;True&lt;/code&gt; doesn√¢t provide any clue what it means without looking up the function signature. Now, using positional arguments for those less-commonly-used parameters raises a deprecation warning, nudging you to write:&lt;/p&gt;
    &lt;code&gt;from django.core.mail import send_mail

send_mail(
    subject="√∞¬º Panda of the week",
    body="This week√¢s panda is Po Ping, sha-sha booey!",
    from_email="updates@example.com",
    ["adam@example.com"],
    fail_silently=True,
)
&lt;/code&gt;
    &lt;p&gt;This change is appreciated for API clarity, and Django is generally moving towards using keyword-only arguments more often. django-upgrade can automatically fix this one for you, via its &lt;code&gt;mail_api_kwargs&lt;/code&gt; fixer.&lt;/p&gt;
    &lt;p&gt;Thanks to Mike Edmunds, again, for making this improvement in Ticket #36163.&lt;/p&gt;
    &lt;head rend="h2"&gt;Extended automatic &lt;code&gt;shell&lt;/code&gt; imports&lt;/head&gt;
    &lt;p&gt;Next up:&lt;/p&gt;
    &lt;quote&gt;Common utilities, such as django.conf.settings, are now automatically imported to the shell by default.&lt;/quote&gt;
    &lt;p&gt;One of the headline features back in Django 5.2 was automatic model imports in the shell, making &lt;code&gt;./manage.py shell&lt;/code&gt; import all of your models automatically. Building on that DX boost, Django 6.0 now also imports other common utilities, for which we can find the full list by running &lt;code&gt;./manage.py shell&lt;/code&gt; with &lt;code&gt;-v 2&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;$ ./manage.py shell -v 2
6 objects imported automatically:

  from django.conf import settings
  from django.db import connection, models, reset_queries
  from django.db.models import functions
  from django.utils import timezone

...
&lt;/code&gt;
    &lt;p&gt;(This is from a project without any models, so only the utilities are listed.)&lt;/p&gt;
    &lt;p&gt;So that√¢s:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;settings&lt;/code&gt;, useful for checking your runtime configuration:&lt;quote&gt;In [1]: settings.DEBUG Out[1]: False&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;connection&lt;/code&gt;and&lt;code&gt;reset_queries()&lt;/code&gt;, great for checking the executed queries:&lt;quote&gt;In [1]: Book.objects.select_related('author') Out[1]: &amp;lt;QuerySet []&amp;gt; In [2]: connection.queries Out[2]: [{'sql': 'SELECT "example_book"."id", "example_book"."title", "example_book"."author_id", "example_author"."id", "example_author"."name" FROM "example_book" INNER JOIN "example_author" ON ("example_book"."author_id" = "example_author"."id") LIMIT 21', 'time': '0.000'}]&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;models&lt;/code&gt;and&lt;code&gt;functions&lt;/code&gt;, useful for advanced ORM work:&lt;quote&gt;In [1]: Book.objects.annotate( ...: title_lower=functions.Lower("title") ...: ).filter( ...: title_lower__startswith="a" ...: ).count() Out[1]: 71&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;timezone&lt;/code&gt;, useful for using Django√¢s timezone-aware date and time utilities:&lt;quote&gt;In [1]: timezone.now() Out[1]: datetime.datetime(2025, 12, 1, 23, 42, 22, 558418, tzinfo=datetime.timezone.utc)&lt;/quote&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It remains possible to extend the automatic imports with whatever you√¢d like, as documented in How to customize the &lt;code&gt;shell&lt;/code&gt; command documentation page.&lt;/p&gt;
    &lt;p&gt;Salvo Polizzi contributed the original automatic shell imports feature in Django 5.2. He√¢s then returned to offer these extra imports for Django 6.0, in Ticket #35680. Thanks to everyone that contributed to the forum discussion agreeing on which imports to add, and to Natalia Bidart and Sarah Boyce for reviewing!&lt;/p&gt;
    &lt;head rend="h2"&gt;Dynamic field refresh on &lt;code&gt;save()&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;Now let√¢s discuss a series of ORM improvements, starting with this big one:&lt;/p&gt;
    &lt;quote&gt;&lt;code&gt;GeneratedField&lt;/code&gt;s and fields assigned expressions are now refreshed from the database after&lt;code&gt;save()&lt;/code&gt;on backends that support the&lt;code&gt;RETURNING&lt;/code&gt;clause (SQLite, PostgreSQL, and Oracle). On backends that don√¢t support it (MySQL and MariaDB), the fields are marked as deferred to trigger a refresh on subsequent accesses.&lt;/quote&gt;
    &lt;p&gt;Django models support having the database generate field values for you in three cases:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;db_default&lt;/code&gt;field option, which lets the database generate the default value when creating an instance:&lt;quote&gt;from django.db import models from django.db.models.functions import Now class Video(models.Model): ... created = models.DateTimeField(db_default=Now())&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;GeneratedField&lt;/code&gt;field type, which is always computed by the database based on other fields in the same instance:&lt;quote&gt;from django.db import models from django.db.models.functions import Concat class Video(models.Model): ... full_title = models.GeneratedField( models.TextField(), expression=Concat( "title", models.Value(" - "), "subtitle", ), )&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Assigning expression values to fields before saving:&lt;/p&gt;
        &lt;quote&gt;from django.db import models from django.db.models.functions import Now class Video(models.Model): ... last_updated = models.DateTimeField() video = Video.objects.get(id=1) ... video.last_updated = Now() video.save()&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Previously, only the first method, using &lt;code&gt;db_default&lt;/code&gt;, would refresh the field value from the database after saving. The other two methods would leave you with only the old value or the expression object, meaning you√¢d need to call &lt;code&gt;Model.refresh_from_db()&lt;/code&gt; to get any updated value if necessary. This was hard to remember and it costs an extra database query.&lt;/p&gt;
    &lt;p&gt;Now Django takes advantage of the &lt;code&gt;RETURNING&lt;/code&gt; SQL clause to save the model instance and fetch updated dynamic field values in a single query, on backends that support it (SQLite, PostgreSQL, and Oracle). A &lt;code&gt;save()&lt;/code&gt; call may now issue a query like:&lt;/p&gt;
    &lt;code&gt;UPDATE "example_video"
SET "last_updated" = NOW()
WHERE "example_video"."id" = 1
RETURNING "example_video"."last_updated"
&lt;/code&gt;
    &lt;p&gt;Django puts the return value into the model field, so you can read it immediately after saving:&lt;/p&gt;
    &lt;code&gt;video = Video.objects.get(id=1)
...
video.last_updated = Now()
video.save()
print(video.last_updated)  # Updated value from the database
&lt;/code&gt;
    &lt;p&gt;On backends that don√¢t support &lt;code&gt;RETURNING&lt;/code&gt; (MySQL and MariaDB), Django now marks the dynamic fields as deferred after saving. That way, the later access, as in the above example, will automatically call &lt;code&gt;Model.refresh_from_db()&lt;/code&gt;. This ensures that you always read the updated value, even if it costs an extra query.&lt;/p&gt;
    &lt;head rend="h3"&gt;History&lt;/head&gt;
    &lt;p&gt;This feature was proposed in Ticket #27222 way back in 2016, by Anssi K√É¬§√É¬§ri√É¬§inen. It sat dormant for most of the nine years since, but ORM boss Simon Charette picked it up earlier this year, found an implementation, and pushed it through to completion. Thanks to Simon for continuing to push the ORM forward, and to all reviewers: David Sanders, Jacob Walls, Mariusz Felisiak, nessita, Paolo Melchiorre, Simon Charette, and Tim Graham.&lt;/p&gt;
    &lt;head rend="h2"&gt;Universal &lt;code&gt;StringAgg&lt;/code&gt; aggregate&lt;/head&gt;
    &lt;p&gt;The next ORM change:&lt;/p&gt;
    &lt;quote&gt;The new&lt;code&gt;StringAgg&lt;/code&gt;aggregate returns the input values concatenated into a string, separated by the&lt;code&gt;delimiter&lt;/code&gt;string. This aggregate was previously supported only for PostgreSQL.&lt;/quote&gt;
    &lt;p&gt;This aggregate is often used for making comma-separated lists of related items, among other things. Previously, it was only supported on PostgreSQL, as part of &lt;code&gt;django.contrib.postgres&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;from django.contrib.postgres.aggregates import StringAgg
from example.models import Video

videos = Video.objects.annotate(
    chapter_ids=StringAgg("chapter", delimiter=","),
)

for video in videos:
    print(f"Video {video.id} has chapters: {video.chapter_ids}")
&lt;/code&gt;
    &lt;p&gt;√¢¬¶which might give you output like:&lt;/p&gt;
    &lt;code&gt;Video 104 has chapters: 71,72,74
Video 107 has chapters: 88,89,138,90,91,93
&lt;/code&gt;
    &lt;p&gt;Now this aggregate is available on all database backends supported by Django, imported from &lt;code&gt;django.db.models&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;from django.db.models import StringAgg, Value
from example.models import Video

videos = Video.objects.annotate(
    chapter_ids=StringAgg("chapter", delimiter=Value(",")),
)

for video in videos:
    print(f"Video {video.id} has chapters: {video.chapter_ids}")
&lt;/code&gt;
    &lt;p&gt;Note the &lt;code&gt;delimiter&lt;/code&gt; argument now requires a &lt;code&gt;Value()&lt;/code&gt; expression wrapper for literal strings, as above. This change allows you to use database functions or fields as the delimiter if desired.&lt;/p&gt;
    &lt;p&gt;While most Django projects stick to PostgreSQL, having this aggregate available on all backends is a nice improvement for cross-database compatibility, and it means third-party packages can use it without affecting their database support.&lt;/p&gt;
    &lt;head rend="h3"&gt;History&lt;/head&gt;
    &lt;p&gt;The PostgreSQL-specific &lt;code&gt;StringAgg&lt;/code&gt; was added way back in Django 1.9 (2015) by Andriy Sokolovskiy, in Ticket #24301. In Ticket #35444, Chris Muthig proposed adding the &lt;code&gt;Aggregate.order_by&lt;/code&gt; option, something used by &lt;code&gt;StringAgg&lt;/code&gt; to specify the ordering of concatenated elements, and as a side effect this made it possible to generalize &lt;code&gt;StringAgg&lt;/code&gt; to all backends.&lt;/p&gt;
    &lt;p&gt;Thanks to Chris for proposing and implementing this change, and to all reviewers: Paolo Melchiorre, Sarah Boyce, and Simon Charette.&lt;/p&gt;
    &lt;head rend="h2"&gt;&lt;code&gt;BigAutoField&lt;/code&gt; as the default primary key type&lt;/head&gt;
    &lt;p&gt;Next up:&lt;/p&gt;
    &lt;quote&gt;&lt;code&gt;DEFAULT_AUTO_FIELD&lt;/code&gt;setting now defaults to&lt;code&gt;BigAutoField&lt;/code&gt;&lt;/quote&gt;
    &lt;p&gt;This important change helps lock in scalable larger primary keys.&lt;/p&gt;
    &lt;p&gt;Django 3.2 (2021) introduced the &lt;code&gt;DEFAULT_AUTO_FIELD&lt;/code&gt; setting for changing the default primary key type used in models. Django uses this setting to add a primary key field called &lt;code&gt;id&lt;/code&gt; to models that don√¢t explicitly define a primary key field. For example, if you define a model like this:&lt;/p&gt;
    &lt;code&gt;from django.db import models


class Video(models.Model):
    title = models.TextField()
&lt;/code&gt;
    &lt;p&gt;√¢¬¶then it will have two fields: &lt;code&gt;id&lt;/code&gt; and &lt;code&gt;title&lt;/code&gt;, where &lt;code&gt;id&lt;/code&gt; uses the type defined by &lt;code&gt;DEFAULT_AUTO_FIELD&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The setting can also be overridden on a per-app basis by defining &lt;code&gt;AppConfig.default_auto_field&lt;/code&gt; in the app√¢s &lt;code&gt;apps.py&lt;/code&gt; file:&lt;/p&gt;
    &lt;code&gt;from django.apps import AppConfig


class ChannelConfig(AppConfig):
    name = "channel"
    default_auto_field = "django.db.models.BigAutoField"
&lt;/code&gt;
    &lt;p&gt;A key motivation for adding the setting was to allow projects to switch from &lt;code&gt;AutoField&lt;/code&gt; (a 32-bit integer) to &lt;code&gt;BigAutoField&lt;/code&gt; (a 64-bit integer) for primary keys, without needing changes to every model. &lt;code&gt;AutoField&lt;/code&gt; can store values up to about 2.1 billion, which sounds large but it becomes easy to hit at scale. &lt;code&gt;BigAutoField&lt;/code&gt; can store values up to about 9.2 quintillion, which is √¢more than enough√¢ for every practical purpose.&lt;/p&gt;
    &lt;p&gt;If a model using &lt;code&gt;AutoField&lt;/code&gt; hits its maximum value, it can no longer accept new rows, a problem known as primary key exhaustion. The table is effectively blocked, requiring an urgent fix to switch the model from &lt;code&gt;AutoField&lt;/code&gt; to &lt;code&gt;BigAutoField&lt;/code&gt; via a locking database migration on a large table. For a great watch on how Kraken is fixing this problem, see Tim Bell√¢s DjangoCon Europe 2025 talk, detailing some clever techniques to proactively migrate large tables with minimal downtime.&lt;/p&gt;
    &lt;p&gt;To stop this problem arising for new projects, Django 3.2 made new projects created with &lt;code&gt;startproject&lt;/code&gt; set &lt;code&gt;DEFAULT_AUTO_FIELD&lt;/code&gt; to &lt;code&gt;BigAutoField&lt;/code&gt;, and new apps created with &lt;code&gt;startapp&lt;/code&gt; set their &lt;code&gt;AppConfig.default_auto_field&lt;/code&gt; to &lt;code&gt;BigAutoField&lt;/code&gt;. It also added a system check to ensure that projects set &lt;code&gt;DEFAULT_AUTO_FIELD&lt;/code&gt; explicitly, to ensure users were aware of the feature and could make an informed choice.&lt;/p&gt;
    &lt;p&gt;Now Django 6.0 changes the actual default values of the setting and app config attribute to &lt;code&gt;BigAutoField&lt;/code&gt;. Projects using &lt;code&gt;BigAutoField&lt;/code&gt; can remove the setting:&lt;/p&gt;
    &lt;code&gt;-DEFAULT_AUTO_FIELD = "django.db.models.BigAutoField"
&lt;/code&gt;
    &lt;p&gt;√¢¬¶and app config attribute:&lt;/p&gt;
    &lt;code&gt;from django.apps import AppConfig

 class ChannelConfig(AppConfig):
     name = "channel"
-    default_auto_field = "django.db.models.BigAutoField"
&lt;/code&gt;
    &lt;p&gt;The default &lt;code&gt;startproject&lt;/code&gt; and &lt;code&gt;startapp&lt;/code&gt; templates also no longer set these values. This change reduces the amount of boilerplate in new projects, and the problem of primary key exhaustion can fade into history, becoming something that most Django users no longer need to think about.&lt;/p&gt;
    &lt;head rend="h3"&gt;History&lt;/head&gt;
    &lt;p&gt;The addition of &lt;code&gt;DEFAULT_AUTO_FIELD&lt;/code&gt; in Django 3.2 was proposed by Caio Ariede and implemented by Tom Forbes, in Ticket #31007. This new change in Django 6.0 was proposed and implemented by ex-Fellow Tim Graham, in Ticket #36564. Thanks to Tim for spotting that this cleanup was now possible, and to Jacob Walls and Clifford Gama for reviewing!&lt;/p&gt;
    &lt;head rend="h2"&gt;Template variable &lt;code&gt;forloop.length&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;Moving on to templates, let√¢s start with this nice little addition:&lt;/p&gt;
    &lt;quote&gt;The new variable forloop.length is now available within a for loop.&lt;/quote&gt;
    &lt;p&gt;This small extension makes it possible to write a template loop like this:&lt;/p&gt;
    &lt;code&gt;&amp;lt;ul&amp;gt;
  {% for goose in geese %}
    &amp;lt;li&amp;gt;
      &amp;lt;strong&amp;gt;{{ forloop.counter }}/{{ forloop.length }}&amp;lt;/strong&amp;gt;: {{ goose.name }}
    &amp;lt;/li&amp;gt;
  {% endfor %}
&amp;lt;/ul&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Previously, you√¢d need to refer to the length in an another way, like &lt;code&gt;{{ geese|length }}&lt;/code&gt;, which is a bit less flexible.&lt;/p&gt;
    &lt;p&gt;Thanks to Jonathan Str√É¬∂bele for contributing this idea and implementation in Ticket #36186, and to David Smith, Paolo Melchiorre, and Sarah Boyce for reviewing.&lt;/p&gt;
    &lt;head rend="h2"&gt;&lt;code&gt;querystring&lt;/code&gt; template tag enhancements&lt;/head&gt;
    &lt;p&gt;There are two extensions to the &lt;code&gt;querystring&lt;/code&gt; template tag, which was added in Django 5.1 to help with building links that modify the current request√¢s query parameters.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Release note:&lt;/p&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;querystring&lt;/code&gt;template tag now consistently prefixes the returned query string with a&lt;code&gt;?&lt;/code&gt;, ensuring reliable link generation behavior.&lt;p&gt;This small change improves how the tag behaves when an empty mapping of query parameters are provided. Say you had a template like this:&lt;/p&gt;&lt;quote&gt;&amp;lt;a href="{% querystring params %}"&amp;gt;Reset search&amp;lt;/a&amp;gt;&lt;/quote&gt;&lt;p&gt;√¢¬¶where&lt;/p&gt;&lt;code&gt;params&lt;/code&gt;is a dictionary that may sometimes be empty. Previously, if&lt;code&gt;params&lt;/code&gt;was empty, the output would be:&lt;quote&gt;&amp;lt;a href=""&amp;gt;Reset search&amp;lt;/a&amp;gt;&lt;/quote&gt;&lt;p&gt;Browsers treat this as a link to the same URL including the query parameters, so it would not clear the query parameters as intended. Now, with this change, the output will be:&lt;/p&gt;&lt;quote&gt;&amp;lt;a href="?"&amp;gt;Reset search&amp;lt;/a&amp;gt;&lt;/quote&gt;&lt;p&gt;Browsers treat&lt;/p&gt;&lt;code&gt;?&lt;/code&gt;as a link to the same URL without any query parameters, clearing them as the user would expect.&lt;p&gt;Thanks to Django Fellow Sarah Boyce for spotting this improvement and implementing the fix in Ticket #36268, and for Django Fellow Natalia Bidart for reviewing!&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Release note:&lt;/p&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;querystring&lt;/code&gt;template tag now accepts multiple positional arguments, which must be mappings, such as&lt;code&gt;QueryDict&lt;/code&gt;or&lt;code&gt;dict&lt;/code&gt;.&lt;p&gt;This enhancement allows the tag to merge multiple sources of query parameters when building the output. For example, you might have a template like this:&lt;/p&gt;&lt;quote&gt;&amp;lt;a href="{% querystring request.GET super_search_params %}"&amp;gt;Super search&amp;lt;/a&amp;gt;&lt;/quote&gt;&lt;p&gt;√¢¬¶where&lt;/p&gt;&lt;code&gt;super_search_params&lt;/code&gt;is a dictionary of extra parameters to add to make the current search √¢super√¢. The tag merges the two mappings, with later mappings taking precedence for duplicate keys.&lt;p&gt;Thanks again to Sarah Boyce for proposing this improvement in Ticket #35529, to Giannis Terzopoulos for implementing it, and to Natalia Bidart, Sarah Boyce, and Tom Carrick for reviewing!&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Fin&lt;/head&gt;
    &lt;p&gt;That√¢s a wrap! Thank you for reading my highlights. There are plenty more changes to read about in the release notes.&lt;/p&gt;
    &lt;p&gt;Also, there are always many more behind-the-scenes improvements and bug fixes that don√¢t make it into the release notes. Optimizations and micro-improvements get merged all the time, so don√¢t delay, upgrade today!&lt;/p&gt;
    &lt;p&gt;Thank you to all 174 people who contributed to Django 6.0, as counted in this list by Mariusz Felisiak.&lt;/p&gt;
    &lt;p&gt;May your upgrade be swift, smooth, safe, and secure,&lt;/p&gt;
    &lt;p&gt;√¢Adam&lt;/p&gt;
    &lt;p&gt;√∞¬∏√∞¬∏√∞¬∏ Check out my new book on using GitHub effectively, Boost Your GitHub DX! √∞¬∏√∞¬∏√∞¬∏&lt;/p&gt;
    &lt;p&gt;One summary email a week, no spam, I pinky promise.&lt;/p&gt;
    &lt;p&gt;Related posts:&lt;/p&gt;
    &lt;p&gt;Tags: django&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46210240</guid><pubDate>Tue, 09 Dec 2025 20:33:14 +0000</pubDate></item><item><title>Qt, Linux and everything: Debugging Qt WebAssembly</title><link>http://qtandeverything.blogspot.com/2025/12/debugging-qt-webassembly-dwarf.html</link><description>&lt;doc fingerprint="ef121c580b3130e5"&gt;
  &lt;main&gt;
    &lt;p&gt;One of the most tedious tasks a developer will do is debugging a nagging bug. It's worse when it's a web app, and even worse when its a webassembly web app.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;The easiest way to debug Qt Webassembly is by configuring using the -g argument, or CMAKE_BUILD_TYPE=Debug . Emscripten embeds DWARF symbols in the wasm binaries. &lt;/p&gt;
    &lt;p&gt;NOTE: Debugging wasm files with DWARF works only in the Chrome browser with the help of a browser extension.&lt;/p&gt;
    &lt;p&gt;C/C++ DevTools Support (DWARF) browser extension. If you are using Safari or Firefox, or do not want to or cannot install a browser extension, you will need to generate source maps, which I will look at in my next blog post.&lt;/p&gt;
    &lt;head rend="h3"&gt;DWARF debugging&lt;/head&gt;
    &lt;p&gt;You need to also enable DWARF in the browser developer tools settings, but you do not need symlinks to the source directories, as you would need to using source maps, as the binaries are embedded with the full directory path. Like magic!&lt;/p&gt;
    &lt;p&gt;Emscripten embeds DWARF symbols into the binaries built with -g by default, so re-building Qt or your application in debug mode is all you need to do.&lt;/p&gt;
    &lt;p&gt;Qt builds debug libraries by default using the optimized argument -g2, which produces less debugging info, but results in faster link times. To preserve debug symbols you need to build Qt debug using the -g or -g3 argument. Both of these do the same thing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Using DWARF debugger&lt;/head&gt;
    &lt;p&gt;You can then step though your code as you would debugging a desktop application.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46210806</guid><pubDate>Tue, 09 Dec 2025 21:19:37 +0000</pubDate></item><item><title>Linux CVEs, more than you ever wanted to know</title><link>http://www.kroah.com/log/blog/2025/12/08/linux-cves-more-than-you-ever-wanted-to-know/</link><description>&lt;doc fingerprint="d713115f51c1b7bb"&gt;
  &lt;main&gt;
    &lt;p&gt;It‚Äôs been almost 2 full years since Linux became a CNA (Certificate Numbering Authority) which meant that we (i.e. the kernel.org community) are now responsible for issuing all CVEs for the Linux kernel. During this time, we‚Äôve become one of the largest creators of CVEs by quantity, going from nothing to number 3 in 2024 to number 1 in 2025. Naturally, this has caused some questions about how we are both doing all of this work, and how people can keep track of it.&lt;/p&gt;
    &lt;p&gt;I‚Äôve given a number of talks over the past years about this, starting with the Open Source security podcast right after we became a CNA and then the Kernel Recipes 2024 talk, ‚ÄúCVEs are alive, but do not panic‚Äù and then a talk at OSS Hong Kong 2024 about the same topic with updated numbers and later a talk at OSS Japan 2024 with more info about the same topic and finally for 2024 a talk with more detail that I can‚Äôt find the online version.&lt;/p&gt;
    &lt;p&gt;In 2025 I did lots of work on the CRA so most of my speaking over this year has been about that topic , but the CVE assignment work continued on, evolving to meet many of the issues we had in our first year of being a CNA. As that work is not part of the Linux kernel source directly, it‚Äôs not all that visable to the normal development process, except for the constant feed on the linux-cve-announce mailing list I figured it was time to write down how this is all now working, as well a bunch of background information about how Linux is developed that is relevant for how we do CVE reporting (i.e. almost all non-open-source-groups don‚Äôt seem to know how to grasp our versioning scheme.)&lt;/p&gt;
    &lt;p&gt;There is a in-kernel document that describes how CVEs can be asked for from the kernel community, as well as a basic summary of how CVEs are automatically asigned. But as we are an open community, it‚Äôs good to go into more detail as to how all of us do this work, explaining how our tools have evolved over time and how they work, why some things are the way they are for our releases, as well as document a way that people can track CVE assignments on their own in a format that is, in my opinion, much simpler than attempting to rely on the CVE json format (and don‚Äôt get me started on NVD‚Ä¶)&lt;/p&gt;
    &lt;p&gt;So here‚Äôs a series of posts going into all of this, hopefully providing more information than you ever wanted to know, which might be useful for other open source projects as they start to run into many of the same issues we have already dealt with (i.e. how to handle reports at scale):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Linux kernel versions, how the Linux kernel releases are numbered.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46211802</guid><pubDate>Tue, 09 Dec 2025 22:47:36 +0000</pubDate></item><item><title>OpenEvolve: Teaching LLMs to Discover Algorithms Through Evolution</title><link>https://algorithmicsuperintelligence.ai/blog/openevolve-overview/index.html</link><description>&lt;doc fingerprint="dd8ca0ba8fb27bdb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;OpenEvolve: Teaching LLMs to Discover Algorithms Through Evolution&lt;/head&gt;
    &lt;p&gt;How do we teach machines to discover algorithms? Traditional approaches rely on hand-crafted heuristics, exhaustive search, or gradient-based optimization. But what if we could harness the creative potential of large language models (LLMs) within an evolutionary framework?&lt;/p&gt;
    &lt;p&gt;OpenEvolve is an open-source evolutionary coding agent that integrates large language models into a quality-diversity search framework for algorithm discovery. Candidate programs are produced via LLM-guided edits (diff-based by default), evaluated with user-defined metrics, and organized using MAP-Elites while an island model with migration supports parallel, diversified exploration. The evaluation pipeline supports cascade staging and an artifact side-channel that feeds execution traces and errors back into subsequent prompts; optional LLM-based feedback can be incorporated into scoring.&lt;/p&gt;
    &lt;p&gt;OpenEvolve has been applied across many domains‚Äîhere are a few examples: systems optimization, scientific discovery, geospatial algorithms, scaling law discovery, GPU kernel optimization, prompt optimization, and more.&lt;/p&gt;
    &lt;head rend="h2"&gt;Architecture Overview&lt;/head&gt;
    &lt;head rend="h3"&gt;The Evolution Loop&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Prompt Sampler: Constructs context-rich prompts by selecting a parent program from the current island and curating evidence sets (top performers by fitness, lineage ancestors, diverse extremes across feature bins, and random samples). Prompts include the parent's code, evaluation metrics, feature coordinates for MAP-Elites, evolution history, and (optionally) execution artifacts. Template selection supports diff-based editing by default or full rewrites, with controlled stochasticity.&lt;/item&gt;
      &lt;item&gt;LLM Ensemble: Generates candidate code using a weighted ensemble of OpenAI-compatible models (deterministic under seeds). In standard mode, a model is sampled by weight; in model-based islands, each island uses a fixed model. Responses drive either diff-based edits (SEARCH/REPLACE blocks) or full rewrites (JSON/code-block extraction), with generation parameters drawn from configuration.&lt;/item&gt;
      &lt;item&gt;Evaluator: Executes the user-provided &lt;code&gt;evaluate(program_path)&lt;/code&gt;with timeouts and retries; optionally applies cascade evaluation (&lt;code&gt;evaluate_stage1/2/3&lt;/code&gt;) with thresholds to filter weak candidates early. It can incorporate LLM-based feedback into metrics and captures artifacts (e.g., stderr, tracebacks) for subsequent prompt context. Parallel evaluations are supported via an internal task pool.&lt;/item&gt;
      &lt;item&gt;Program Database: Implements MAP-Elites per island, binning programs along configurable feature dimensions (defaults include complexity and diversity; custom dimensions are taken from evaluator metrics). New candidates replace cell occupants when fitness improves (preferring &lt;code&gt;combined_score&lt;/code&gt;, otherwise a safe numeric aggregate excluding feature dimensions). The database enforces population limits, tracks the global best, logs prompts, supports migration, and persists checkpoints.&lt;/item&gt;
      &lt;item&gt;Controller: Orchestrates the loop, including seeding, logging, prompt/evaluator initialization, and process-based parallel execution. It schedules iterations across islands, manages checkpointing and resume, enforces early stopping/target score criteria, stores artifacts, and writes the best discovered program and its metadata to the output directory.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Key Algorithmic Innovations&lt;/head&gt;
    &lt;head rend="h3"&gt;Island-Based Evolution with Lazy Migration&lt;/head&gt;
    &lt;p&gt;OpenEvolve maintains multiple isolated populations (islands) that evolve independently to reduce premature convergence and enable parallel exploration. Migration is event-driven: each island migrates when its per-island program additions since the last migration reach a configured interval, rather than on wall-clock time. Migration follows a ring topology by default (optional random migration), transferring a fraction of top programs while avoiding duplicate code in the destination island.&lt;/p&gt;
    &lt;code&gt;# Configuration example
database:
  num_islands: 5
  migration_interval: 20   # generations, not iterations
  migration_rate: 0.1      # 10% of top programs migrate&lt;/code&gt;
    &lt;head rend="h3"&gt;MAP-Elites for Diversity Preservation&lt;/head&gt;
    &lt;p&gt;Each island maintains a MAP-Elites grid over configurable feature dimensions (defaults include complexity and diversity; additional dimensions can be supplied by the evaluator). A candidate occupies or replaces the cell if it improves fitness (preferring &lt;code&gt;combined_score&lt;/code&gt;, otherwise a safe aggregate over numeric metrics excluding feature dimensions). This enforces one elite per cell and preserves quality-diversity. The system also avoids exact duplicates (e.g., during migration) and computes diversity using structural measures (e.g., edit distance), rather than relying on code embeddings.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cascade Evaluation&lt;/head&gt;
    &lt;p&gt;Evaluation proceeds in stages with configurable thresholds. If cascade functions are provided, Stage 1 performs fast checks (e.g., import/execute), Stage 2 runs lightweight tests, and Stage 3 executes comprehensive benchmarks. Candidates must meet stage thresholds to advance. Timeouts and exceptions are captured as artifacts and can be fed back into subsequent prompts. When cascade functions are not defined, evaluation falls back to a single-stage &lt;code&gt;evaluate(program_path)&lt;/code&gt; with timeouts and retries.&lt;/p&gt;
    &lt;head rend="h3"&gt;Double-Selection Strategy&lt;/head&gt;
    &lt;p&gt;Parent selection is biased toward high-fitness programs, while inspiration material shown to the LLM is drawn from complementary sources (top programs, lineage ancestors, diverse extremes across feature bins, and random samples). This separation encourages improvements guided by the current best while maintaining exploration pressure via diverse exemplars, implemented through prompt construction rather than direct recombination.&lt;/p&gt;
    &lt;head rend="h2"&gt;Sample Use Cases&lt;/head&gt;
    &lt;head rend="h3"&gt;Example 1: Algorithmic Discovery&lt;/head&gt;
    &lt;p&gt;On the AlgoTune benchmark, OpenEvolve discovered algorithms achieving dramatic speedups through automatic optimization:&lt;/p&gt;
    &lt;p&gt;Key breakthroughs include automatic discovery of JAX JIT compilation (321x), FFT-based convolution (256x), and optimized graph algorithms (95.78x). The system evolved from simple iterative implementations to sophisticated numerical computing patterns without human intervention. For more detailed analysis, see Towards Open Evolutionary Agents.&lt;/p&gt;
    &lt;head rend="h3"&gt;Example 2: Circle Packing&lt;/head&gt;
    &lt;p&gt;OpenEvolve matched state-of-the-art results (2.634 sum of radii for n=26), evolving from naive geometric constructions to discovering scipy.optimize with SLSQP‚Äîa completely different algorithmic approach than the initial solution.&lt;/p&gt;
    &lt;head rend="h3"&gt;Example 3: GPU Kernel Optimization&lt;/head&gt;
    &lt;p&gt;Evolution of Metal GPU kernels for transformer attention on Apple Silicon:&lt;/p&gt;
    &lt;p&gt;OpenEvolve discovered several non-obvious optimizations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;8-element SIMD vectorization matching Apple Silicon's hardware width&lt;/item&gt;
      &lt;item&gt;Two-pass online softmax reducing memory bandwidth&lt;/item&gt;
      &lt;item&gt;GQA-specific memory layouts exploiting head structure&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These optimizations maintain 100% numerical accuracy while achieving measurable performance improvements across diverse inference scenarios. For more details, see GPU Kernel Discovery.&lt;/p&gt;
    &lt;head rend="h3"&gt;Example 4: LLM Prompt Optimization&lt;/head&gt;
    &lt;p&gt;Beyond code, OpenEvolve can evolve prompts themselves:&lt;/p&gt;
    &lt;p&gt;On GEPA benchmarks, evolved prompts achieved +10.69% accuracy on HotpotQA (multi-hop reasoning) and +6.42% overall across multiple benchmarks. This demonstrates OpenEvolve's versatility‚Äîthe same evolutionary framework optimizes both code and natural language.&lt;/p&gt;
    &lt;p&gt;Evolution Progress: As shown below on the AlgoTune benchmark, we see that the performance consistently improves over generations. Extended evolution (200 iterations) achieved 24% better results than shorter runs (100 iterations), suggesting that patient exploration of the solution space yields compounding benefits.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting Started&lt;/head&gt;
    &lt;p&gt;OpenEvolve provides both library and command-line interfaces:&lt;/p&gt;
    &lt;code&gt;from openevolve import run_evolution

result = run_evolution(
    initial_program="def solve(x): return x * 2",
    evaluator=lambda path: {"score": benchmark(path)},
    iterations=100
)&lt;/code&gt;
    &lt;p&gt;For complex configurations, use YAML files specifying LLM models, evolution strategies, and evaluation parameters. OpenEvolve supports checkpoint/resume for long-running experiments and parallel evaluation across multiple cores. OpenEvolve is open-source and available on GitHub.&lt;/p&gt;
    &lt;p&gt;Update: This blog post was updated on November 1, 2025&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46211861</guid><pubDate>Tue, 09 Dec 2025 22:54:33 +0000</pubDate></item></channel></rss>