<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 26 Dec 2025 16:12:35 +0000</lastBuildDate><item><title>Keystone (YC S25) is hiring engineer #1 to automate coding</title><link>https://www.ycombinator.com/companies/keystone/jobs/J3t9XeM-founding-engineer</link><description>&lt;doc fingerprint="233a92f07e6fe468"&gt;
  &lt;main&gt;
    &lt;p&gt;Your team's on-call AI engineer&lt;/p&gt;
    &lt;p&gt;About Keystone&lt;/p&gt;
    &lt;p&gt;We're building AI-native error monitoring that automatically investigates production issues and generates code fixes. Think Sentry, but built from the ground up for a world where AI can actually understand your codebase, trace through logs, and tell you exactly what broke and how to fix it.&lt;/p&gt;
    &lt;p&gt;We're starting here and expanding until we're the default tool for building product, period. Our mission is to free engineers from the drudgery of digging through logs, setting up systems, and debugging- so they can focus on understanding users and designing great products.&lt;/p&gt;
    &lt;p&gt;We're in-person in SoMa, San Francisco. We raised a $5.2M seed from True Ventures, Twenty Two Ventures, Pear VC, and Ritual Capital- plus the founders of YC, Dropbox, Supabase, Eight Sleep, Graphite, Resend, RocketMoney, and more. Early design partners include teams at Perplexity and Lovable.&lt;/p&gt;
    &lt;p&gt;About the Role&lt;/p&gt;
    &lt;p&gt;You'd be the first engineering hire, working directly with me (the founder) to build the core product. You'll have more ownership and influence over the product, culture, and technical direction than you'd get almost anywhere else.&lt;/p&gt;
    &lt;p&gt;Example projects:&lt;/p&gt;
    &lt;p&gt;You might be a great fit if you:&lt;/p&gt;
    &lt;p&gt;Stack: TypeScript, React (Next.js), Python, Postgres, Redis, AWS&lt;/p&gt;
    &lt;p&gt;Comp &amp;amp; benefits: Top-of-market salary + significant equity, full health/dental/vision, all meals covered, equipment budget&lt;/p&gt;
    &lt;p&gt;To apply: submit an application here on workatastartup.com or email founders [at] withkeystone [dot] com with “Keystone Founding Engineer” in the subject line. I will reply to all such emails.&lt;/p&gt;
    &lt;p&gt;If there appears to be a fit, we'll reach to schedule 2-3 short technicals. After, we'll schedule an onsite in our office, where you'll work on a small project, discuss ideas, and get a sense for our in-person culture.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46379173</guid><pubDate>Wed, 24 Dec 2025 21:01:05 +0000</pubDate></item><item><title>Python 3.15’s interpreter for Windows x86-64 should hopefully be 15% faster</title><link>https://fidget-spinner.github.io/posts/no-longer-sorry.html</link><description>&lt;doc fingerprint="2e649111fe3785d0"&gt;
  &lt;main&gt;
    &lt;p&gt;24 December 2025&lt;/p&gt;
    &lt;p&gt;Some time ago I posted an apology piece for Python’s tail calling results. I apologized for communicating performance results without noticing a compiler bug had occured.&lt;/p&gt;
    &lt;p&gt;I can proudly say today that I am partially retracting that apology, but only for two platforms—macOS AArch64 (XCode Clang) and Windows x86-64 (MSVC).&lt;/p&gt;
    &lt;p&gt;In our own experiments, the tail calling interpreter for CPython was found to beat the computed goto interpreter by 5% on pyperformance on AArch64 macOS using XCode Clang, and roughly 15% on pyperformance on Windows on an experimental internal version of MSVC. The Windows build is against a switch-case interpreter, but this in theory shouldn’t matter too much, more on that in the next section.&lt;/p&gt;
    &lt;p&gt;This is of course, a hopefully accurate result. I tried to be more diligent here, but I am of course not infallible. However, I have found that sharing early and making a fool of myself often works well, as it has led to people catching bugs in my code, so I shall continue doing so :).&lt;/p&gt;
    &lt;p&gt;Also this assumes the change doesn’t get reverted later in Python 3.15’s development cycle.&lt;/p&gt;
    &lt;p&gt;Just a recap. There are two popular current ways of writing C-based interpreters.&lt;/p&gt;
    &lt;p&gt;Switch-cases:&lt;/p&gt;
    &lt;code&gt;switch (opcode) {
    case INST_1: ...
    case INST_2: ...
}
&lt;/code&gt;
    &lt;p&gt;Where we just switch-case to the correct instruction handler.&lt;/p&gt;
    &lt;p&gt;And the other popular way is a GCC/Clang extension called labels-as-values/computed gotos.&lt;/p&gt;
    &lt;code&gt;goto *dispatch_table[opcode];
INST_1: ...
INST_2: ...
&lt;/code&gt;
    &lt;p&gt;Which is basically the same idea, but to instead jump to the address of the next label. Traditionally, the key optimization here is that it needs only one jump to go to the next instruction, while in the switch-case interpreter, a naiive compiler would need two jumps.&lt;/p&gt;
    &lt;p&gt;With modern compilers however, the benefits of the computed gotos is a lot less, mainly because modern compilers have gotten better and modern hardware has also gotten better. In Nelson Elhage’s excellent investigation on the next kind of interpreter, the speedup of computed gotos over switch case on modern Clang was only in the low single digits on pyperformance.&lt;/p&gt;
    &lt;p&gt;A 3rd way that was suggested decades ago, but not really entirely feasible is call/tail-call threaded interpreters. In this scheme, each bytecode handler is its own function, and we tail-call from one handler to the next in the instruction stream:&lt;/p&gt;
    &lt;code&gt;return dispatch_table[opcode];

PyObject *INST_1(...) {

}

PyObject *INST_2(...) {
}
&lt;/code&gt;
    &lt;p&gt;This wasn’t too feasible in C for one main reason—tail call optimization was merely an optimization. It’s something the C compiler might do, or might not do. This means if you’re unlucky and the C compiler chooses not to perform the tail call, your interpreter might stack overflow!&lt;/p&gt;
    &lt;p&gt;Some time ago, Clang introduced &lt;code&gt;__attribute__((musttail))&lt;/code&gt;, which allowed
for mandating that a call must be tail-called. Otherwise, the compilation
will fail. To my knowledge, the first time this was popularized for use
in a mainstream interpreter was in
Josh Haberman’s Protobuf blog post.&lt;/p&gt;
    &lt;p&gt;Later on, Haoran Xu noticed that the GHC calling convention combined with tail calls produced efficient code. They used this for their baseline JIT in a paper and termed the technique Copy-and-Patch.&lt;/p&gt;
    &lt;p&gt;After using a fixed XCode Clang, our performance numbers on CPython 3.14/3.15 suggest that the tail calling interpreter does provide a modest speedup over computed gotos. Around the 5% geomean range on pyperformance.&lt;/p&gt;
    &lt;p&gt;To my understanding, &lt;code&gt;uv&lt;/code&gt; already ships Python 3.14 on macOS with tail calling,
which might be responsible for some of the speedups you see on there.
We’re planning to ship the official 3.15 macOS binaries on &lt;code&gt;python.org&lt;/code&gt; with
tail calling as well.&lt;/p&gt;
    &lt;p&gt;However, you’re not here for that. The title of this blog post is clearly about MSVC Windows x86-64. So what about that?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[!CAUTION] The features for MSVC discussed below are to my knowledge, experimental. They are not guaranteed to always be around unless the MSVC team decide to keep them. Use at your own risk!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;These are the preliminary pyperformance results for CPython on MSVC with tail-calling vs switch-case. Any number above 1.00x is a speedup (e.g. &lt;code&gt;1.01x == 1% speedup&lt;/code&gt;), anything below 1.00x is a slowdown.
The speedup is a geomtric mean of around 15-16%, with a
range of ~60% slowdown (one or two outliers) to 78% speedup.
However, the key thing is that the vast majority of benchmaarks sped up!&lt;/p&gt;
    &lt;p&gt;Chart credits to Michael Droettboom&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[!WARNING] These results are on an experimental internal MSVC compiler, public results below.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;To verify this and make sure I wasn’t wrong yet again, I checked the results on my machine with Visual Studio 2026. These are the results from this issue.&lt;/p&gt;
    &lt;code&gt;Mean +- std dev: [spectralnorm_tc_no] 146 ms +- 1 ms -&amp;gt; [spectralnorm_tc] 98.3 ms +- 1.1 ms: 1.48x faster
Mean +- std dev: [nbody_tc_no] 145 ms +- 2 ms -&amp;gt; [nbody_tc] 107 ms +- 2 ms: 1.35x faster
Mean +- std dev: [bm_django_template_tc_no] 26.9 ms +- 0.5 ms -&amp;gt; [bm_django_template_tc] 22.8 ms +- 0.4 ms: 1.18x faster
Mean +- std dev: [xdsl_tc_no] 64.2 ms +- 1.6 ms -&amp;gt; [xdsl_tc] 56.1 ms +- 1.5 ms: 1.14x faster
&lt;/code&gt;
    &lt;p&gt;So yeah, the speedups are real! For a large-ish library like xDSL, we see a 14% speedup, while for smaller microbenchmarks like nbody and spectralnorm, the speedups are greater.&lt;/p&gt;
    &lt;p&gt;Thanks to Chris Eibl and Brandt Bucher, we managed to get the PR for this on MSVC over the finish line. I also want to sincerely thank the MSVC team. I can’t say this enough: they have been a joy to work with and I’m very impressed by what they’ve done, and I want to congratulate them on releasing Visual Studio 2026.&lt;/p&gt;
    &lt;p&gt;This is now listed in the What’s New for 3.15 notes:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Builds using Visual Studio 2026 (MSVC 18) may now use the new tail-calling interpreter. Results on an early experimental MSVC compiler reported roughly 15% speedup on the geometric mean of pyperformance on Windows x86-64 over the switch-case interpreter. We have observed speedups ranging from 15% for large pure-Python libraries to 40% for long-running small pure-Python scripts on Windows. (Contributed by Chris Eibl, Ken Jin, and Brandt Bucher in gh-143068. Special thanks to the MSVC team including Hulon Jenkins.)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is the documentation for [[msvc::musttail]].&lt;/p&gt;
    &lt;p&gt;I used to believe the the tail calling interpreters get their speedup from better register use. While I still believe that now, I suspect that is not the main reason for speedups in CPython.&lt;/p&gt;
    &lt;p&gt;My main guess now is that tail calling resets compiler heuristics to sane levels, so that compilers can do their jobs.&lt;/p&gt;
    &lt;p&gt;Let me show an example, at the time of writing, CPython 3.15’s interpreter loop is around 12k lines of C code. That’s 12k lines in a single function for the switch-case and computed goto interpreter.&lt;/p&gt;
    &lt;p&gt;This has caused many issues for compilers in the past, too many to list in fact. I have a EuroPython 2025 talk about this. In short, this overly large function breaks a lot of compiler heuristics.&lt;/p&gt;
    &lt;p&gt;One of the most beneficial optimisations is inlining. In the past, we’ve found that compilers sometimes straight up refuse to inline even the simplest of functions in that 12k loc eval loop. I want to stress that this is not the fault of the compiler. It’s actually doing the correct thing—you usually don’t want to increase the code size of something already super large. Unfortunately, this does’t bode well for our interpreter.&lt;/p&gt;
    &lt;p&gt;You might say just write the interpreter in assembly! However, the whole point of this exercise is to not do that.&lt;/p&gt;
    &lt;p&gt;Ok enough talk, let’s take a look at the code now. Taking a real example, we examine &lt;code&gt;BINARY_OP_ADD_INT&lt;/code&gt; which adds two Python integers.
Cleaning up the code so it’s readable, things look like this:&lt;/p&gt;
    &lt;code&gt;TARGET(BINARY_OP_ADD_INT) {
    // Increment the instruction pointer.
    _Py_CODEUNIT* const this_instr = next_instr;
    frame-&amp;gt;instr_ptr = next_instr;
    next_instr += 6;
    _PyStackRef right = stack_pointer[-1];

    // Check that LHS is an int.
    PyObject *value_o = PyStackRef_AsPyObjectBorrow(left);
    if (!_PyLong_CheckExactAndCompact(value_o)) {
        JUMP_TO_PREDICTED(BINARY_OP);
    }

    // Check that RHS is an int.
    // ... (same code as above for LHS)

    // Add them together.
    PyObject *left_o = PyStackRef_AsPyObjectBorrow(left);
    PyObject *right_o = PyStackRef_AsPyObjectBorrow(right);
    res = _PyCompactLong_Add((PyLongObject *)left_o, (PyLongObject *)right_o);

    // If the addition fails, fall back to the generic instruction.
    if (PyStackRef_IsNull(res)) {
        JUMP_TO_PREDICTED(BINARY_OP);
    }

    // Close the references.
    PyStackRef_CLOSE_SPECIALIZED(left, _PyLong_ExactDealloc);
    PyStackRef_CLOSE_SPECIALIZED(right, _PyLong_ExactDealloc);

    // Write to the stack, and dispatch.
    stack_pointer[-2] = res;
    stack_pointer += -1;
    DISPATCH();
}
&lt;/code&gt;
    &lt;p&gt;Seems simple enough, let’s take a look at the assembly for switch-case on VS 2026. Note again, this is a non-PGO build for easy source information, PGO generally makes some of these problems go away, but not all of them:&lt;/p&gt;
    &lt;code&gt;                if (!_PyLong_CheckExactAndCompact(value_o)) {
00007FFC4DE24DCE  mov         rcx,rbx  
00007FFC4DE24DD1  mov         qword ptr [rsp+58h],rax  
00007FFC4DE24DD6  call        _PyLong_CheckExactAndCompact (07FFC4DE227F0h)  
00007FFC4DE24DDB  test        eax,eax  
00007FFC4DE24DDD  je          _PyEval_EvalFrameDefault+10EFh (07FFC4DE258FFh)
...
                res = _PyCompactLong_Add((PyLongObject *)left_o, (PyLongObject *)right_o);
00007FFC4DE24DFF  mov         rdx,rbx  
00007FFC4DE24E02  mov         rcx,r15  
00007FFC4DE24E05  call        _PyCompactLong_Add (07FFC4DD34150h)  
00007FFC4DE24E0A  mov         rbx,rax  
...
                PyStackRef_CLOSE_SPECIALIZED(value, _PyLong_ExactDealloc);
00007FFC4DE24E17  lea         rdx,[_PyLong_ExactDealloc (07FFC4DD33BD0h)]  
00007FFC4DE24E1E  mov         rcx,rsi  
00007FFC4DE24E21  call        PyStackRef_CLOSE_SPECIALIZED (07FFC4DE222A0h) 
&lt;/code&gt;
    &lt;p&gt;Huh… all our functions were not inlined. Surely that must’ve mean they were too big or something right? Let’s look at &lt;code&gt;PyStackReF_CLOSE_SPECIALIZED&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;static inline void
PyStackRef_CLOSE_SPECIALIZED(_PyStackRef ref, destructor destruct)
{
    assert(!PyStackRef_IsNull(ref));
    if (PyStackRef_RefcountOnObject(ref)) {
        Py_DECREF_MORTAL_SPECIALIZED(BITS_TO_PTR(ref), destruct);
    }
}
&lt;/code&gt;
    &lt;p&gt;That looks … inlineable?&lt;/p&gt;
    &lt;p&gt;Here’s how &lt;code&gt;BINARY_OP_ADD_INT&lt;/code&gt; looks with tail calling on VS 2026 (again,
no PGO):&lt;/p&gt;
    &lt;code&gt;                if (!_PyLong_CheckExactAndCompact(left_o)) {
00007FFC67164785  cmp         qword ptr [rax+8],rdx  
00007FFC67164789  jne         _TAIL_CALL_BINARY_OP_ADD_INT@@_A+149h (07FFC67164879h)  
00007FFC6716478F  mov         r9,qword ptr [rax+10h]  
00007FFC67164793  cmp         r9,10h  
00007FFC67164797  jae         _TAIL_CALL_BINARY_OP_ADD_INT@@_A+149h (07FFC67164879h) 
...
                res = _PyCompactLong_Add((PyLongObject *)left_o, (PyLongObject *)right_o);
00007FFC6716479D  mov         eax,dword ptr [rax+18h]  
00007FFC671647A0  and         r9d,3  
00007FFC671647A4  and         r8d,3  
00007FFC671647A8  mov         edx,1  
00007FFC671647AD  sub         rdx,r9  
00007FFC671647B0  mov         ecx,1  
00007FFC671647B5  imul        rdx,rax  
00007FFC671647B9  mov         eax,dword ptr [rbx+18h]  
00007FFC671647BC  sub         rcx,r8  
00007FFC671647BF  imul        rcx,rax  
00007FFC671647C3  add         rcx,rdx  
00007FFC671647C6  call        medium_from_stwodigits (07FFC6706E9E0h)  
00007FFC671647CB  mov         rbx,rax  
...
                PyStackRef_CLOSE_SPECIALIZED(value, _PyLong_ExactDealloc);
00007FFC671647EB  test        bpl,1  
00007FFC671647EF  jne         _TAIL_CALL_BINARY_OP_ADD_INT@@_A+0ECh (07FFC6716481Ch)  
00007FFC671647F1  add         dword ptr [rbp],0FFFFFFFFh  
00007FFC671647F5  jne         _TAIL_CALL_BINARY_OP_ADD_INT@@_A+0ECh (07FFC6716481Ch)  
00007FFC671647F7  mov         rax,qword ptr [_PyRuntime+25F8h (07FFC675C45F8h)]  
00007FFC671647FE  test        rax,rax  
00007FFC67164801  je          _TAIL_CALL_BINARY_OP_ADD_INT@@_A+0E4h (07FFC67164814h)  
00007FFC67164803  mov         r8,qword ptr [_PyRuntime+2600h (07FFC675C4600h)]  
00007FFC6716480A  mov         edx,1  
00007FFC6716480F  mov         rcx,rbp  
00007FFC67164812  call        rax  
00007FFC67164814  mov         rcx,rbp  
00007FFC67164817  call        _PyLong_ExactDealloc (07FFC67073DA0h) 
&lt;/code&gt;
    &lt;p&gt;Would you look at that, suddenly our trivial functions get inlined :).&lt;/p&gt;
    &lt;p&gt;You might also say, surely this does not happen on PGO builds? Well the issue I linked above actually says it does! So yeah happy days.&lt;/p&gt;
    &lt;p&gt;Once again I want to stress, this is not the compiler’s fault! It’s just that the CPython interpreter loop is not the best thing to optimize.&lt;/p&gt;
    &lt;p&gt;Unfortunately, for now, you will have to build from source.&lt;/p&gt;
    &lt;p&gt;With VS 2026, after cloning CPython, for a release build with PGO:&lt;/p&gt;
    &lt;code&gt;$env:PlatformToolset = "v145"
./PCbuild/build.bat --tail-call-interp -c Release -p x64 --pgo
&lt;/code&gt;
    &lt;p&gt;Hopefully, we can distribute this in an easier binary form in the future once Python 3.15’s development matures!&lt;/p&gt;
    &lt;p&gt;I was asked for a cross-compiler test. So here’s a quick and dirty toy benchmark of pystones. The last row is the tail call enabled build. All configurations have PGO. On this toy benchmark, we get roughly a 30% uplift. Note that this is unscientific as it was only a sample size of 1 and I cannot disable Turbo Boost on my laptop on Windows for some reason.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Compiler&lt;/cell&gt;
        &lt;cell role="head"&gt;PlatformToolSet&lt;/cell&gt;
        &lt;cell role="head"&gt;Pystones/second (higher is better)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;VS2019&lt;/cell&gt;
        &lt;cell&gt;142&lt;/cell&gt;
        &lt;cell&gt;677544&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;VS2022&lt;/cell&gt;
        &lt;cell&gt;143&lt;/cell&gt;
        &lt;cell&gt;710773&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;VS2026&lt;/cell&gt;
        &lt;cell&gt;145&lt;/cell&gt;
        &lt;cell&gt;682089&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;VS2026+TC&lt;/cell&gt;
        &lt;cell&gt;145&lt;/cell&gt;
        &lt;cell&gt;970306&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46384167</guid><pubDate>Thu, 25 Dec 2025 13:02:46 +0000</pubDate></item><item><title>Clearspace (YC W23) Is Hiring a Founding Network Engineer (VPN and Proxy)</title><link>https://www.ycombinator.com/companies/clearspace/jobs/5LtM86I-founding-network-engineer-at-clearspace</link><description>&lt;doc fingerprint="b792f12e5d21eddd"&gt;
  &lt;main&gt;
    &lt;p&gt;Eliminate compulsive phone usage&lt;/p&gt;
    &lt;p&gt;About Clearspace&lt;/p&gt;
    &lt;p&gt;Clearspace is building the intentionality layer of the internet. Our mission is to build technology as effective at protecting human attention as social media is at exploiting it (infinite scrolling, short-form feeds, manipulative notifications, etc). Our category defining mobile app has been featured on Huberman Lab, New York Times Wirecutter, NPR Marketplace, Forbes, TBPN.&lt;/p&gt;
    &lt;p&gt;People that want a better relationship with their devices have nowhere to turn except for willpower. We are building a system allows users to control what their devices see by processing and filtering network traffic against natural language rules.&lt;/p&gt;
    &lt;p&gt;About The Role&lt;/p&gt;
    &lt;p&gt;We’re looking for a networking-obsessed engineer to own the VPN + first-hop policy proxy that powers our AI agent. You don’t need to be an IKEv2 expert, we care more about deep networking intuition, debugging skill, and the desire to go all the way down the stack until the truth reveals itself.&lt;/p&gt;
    &lt;p&gt;You might be a great fit if you’ve built:&lt;/p&gt;
    &lt;p&gt;What You’ll Build&lt;/p&gt;
    &lt;p&gt;Qualifications&lt;/p&gt;
    &lt;p&gt;Nice to Have&lt;/p&gt;
    &lt;p&gt;Compensation&lt;/p&gt;
    &lt;p&gt;At Clearspace we help people reduce compulsive phone usage.&lt;/p&gt;
    &lt;p&gt;We exist to protect people's attention from the exploits of modern technology platforms and make space for the things that matter to them most.&lt;/p&gt;
    &lt;p&gt;We believe the technology to protect someones attention should be just as sophisticated and effective as the tech that is exploiting it and are building a world-class engineering team to arm the world with a comprehensive attention protection stack.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46385600</guid><pubDate>Thu, 25 Dec 2025 17:01:29 +0000</pubDate></item><item><title>Fahrplan – 39C3</title><link>https://fahrplan.events.ccc.de/congress/2025/fahrplan/</link><description>&lt;doc fingerprint="ad728d1d22b2a7ef"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Sat - Day 1 - December 27&lt;/head&gt;
    &lt;head rend="h5"&gt;Opening Ceremony&lt;/head&gt;
    &lt;p&gt;pajowu, Stella&lt;/p&gt;
    &lt;head rend="h5"&gt;All Sorted by Machines of Loving Grace? "AI", Cybernetics, and Fascism and how to Intervene&lt;/head&gt;
    &lt;p&gt;Katika Kühnreich&lt;/p&gt;
    &lt;head rend="h5"&gt;The art of text (rendering)&lt;/head&gt;
    &lt;p&gt;Nicolas Rougier&lt;/p&gt;
    &lt;head rend="h5"&gt;A Tale of Two Leaks: How Hackers Breached the Great Firewall of China&lt;/head&gt;
    &lt;p&gt;Jade Sheffey&lt;/p&gt;
    &lt;head rend="h5"&gt;OpenAutoLab: photographic film processing machine. Fully automatic and DIY-friendly.&lt;/head&gt;
    &lt;p&gt;Kauz&lt;/p&gt;
    &lt;head rend="h5"&gt;Zentrum für Politische Schönheit: Ein Jahr Adenauer SRP+ und der Walter Lübke Memorial Park&lt;/head&gt;
    &lt;p&gt;Stefan Pelzer, Philipp Ruch&lt;/p&gt;
    &lt;head rend="h5"&gt;Demystifying Fuzzer Behaviour&lt;/head&gt;
    &lt;p&gt;Addison&lt;/p&gt;
    &lt;head rend="h5"&gt;ISDN + POTS Telephony at Congress and Camp&lt;/head&gt;
    &lt;p&gt;Harald "LaF0rge" Welte&lt;/p&gt;
    &lt;head rend="h5"&gt;Brennende Wälder und Kommentarspalten - Klimaupdate mit dem FragDenStaat Climate Helpdesk&lt;/head&gt;
    &lt;p&gt;Joschi Wolf&lt;/p&gt;
    &lt;head rend="h5"&gt;Building hardware - easier than ever - harder than it should be&lt;/head&gt;
    &lt;p&gt;Kliment&lt;/p&gt;
    &lt;head rend="h5"&gt;Neuroexploitation by Design: Wie Algorithmen in Glücksspielprodukten sich Wirkweisen des Reinforcement Learning und dopaminergen Belohnungssystems zunutze machen&lt;/head&gt;
    &lt;p&gt;Elke Smith&lt;/p&gt;
    &lt;head rend="h5"&gt;FeTAp 611 unplugged: Taking a rotary dial phone to the mobile age&lt;/head&gt;
    &lt;p&gt;Michael Weiner&lt;/p&gt;
    &lt;head rend="h5"&gt;Who cares about the Baltic Jammer? – Terrestrial Navigation in the Baltic Sea Region&lt;/head&gt;
    &lt;p&gt;Lars, Niklas Hehenkamp, Markus&lt;/p&gt;
    &lt;head rend="h5"&gt;Liberating Bluetooth on the ESP32&lt;/head&gt;
    &lt;p&gt;Antonio Vázquez Blanco (Antón)&lt;/p&gt;
    &lt;head rend="h5"&gt;Chaos macht Küche&lt;/head&gt;
    &lt;p&gt;Ingwer Andersen&lt;/p&gt;
    &lt;head rend="h5"&gt;Developing New Medicines in the Age of AI and Personalized Medicine&lt;/head&gt;
    &lt;p&gt;Dennis Özcelik&lt;/p&gt;
    &lt;head rend="h5"&gt;Endlich maschinenlesbare Urteile! Open access für Juristen&lt;/head&gt;
    &lt;p&gt;Beata Hubrig, Nuri Khadem-Al-Charieh&lt;/p&gt;
    &lt;head rend="h5"&gt;Opening pAMDora's box and unleashing a thousand paths on the journey to play Beatsaber custom songs&lt;/head&gt;
    &lt;p&gt;tihmstar&lt;/p&gt;
    &lt;head rend="h5"&gt;Not an Impasse: Child Safety, Privacy, and Healing Together&lt;/head&gt;
    &lt;p&gt;Kate Sim&lt;/p&gt;
    &lt;head rend="h5"&gt;KIM 1.5: Noch mehr Kaos In der Medizinischen Telematikinfrastruktur (TI)&lt;/head&gt;
    &lt;p&gt;Christoph Saatjohann&lt;/p&gt;
    &lt;head rend="h5"&gt;RedScout42 – Zur digitalen Wohnungsfrage&lt;/head&gt;
    &lt;p&gt;Sandra, Leonard&lt;/p&gt;
    &lt;head rend="h5"&gt;All my Deutschlandtickets gone: Fraud at an industrial scale&lt;/head&gt;
    &lt;p&gt;Q Misell, 551724 / maya boeckh&lt;/p&gt;
    &lt;head rend="h5"&gt;Of Boot Vectors and Double Glitches: Bypassing RP2350's Secure Boot&lt;/head&gt;
    &lt;p&gt;stacksmashing, nsr&lt;/p&gt;
    &lt;head rend="h5"&gt;„KI“, Digitalisierung und Longevity als Fix für ein kaputtes Gesundheitssystem?&lt;/head&gt;
    &lt;p&gt;Manuel Hofmann&lt;/p&gt;
    &lt;head rend="h5"&gt;Chaos all year round&lt;/head&gt;
    &lt;p&gt;Deanna&lt;/p&gt;
    &lt;head rend="h5"&gt;To sign or not to sign: Practical vulnerabilities in GPG &amp;amp; friends&lt;/head&gt;
    &lt;p&gt;49016, Liam&lt;/p&gt;
    &lt;head rend="h5"&gt;Handy weg bis zur Ausreise – Wie Cellebrite ins Ausländeramt kam&lt;/head&gt;
    &lt;p&gt;Chris Köver&lt;/p&gt;
    &lt;head rend="h5"&gt;Pwn2Roll: Who Needs a 595€ Remote When You Have wheelchair.py?&lt;/head&gt;
    &lt;p&gt;elfy&lt;/p&gt;
    &lt;head rend="h5"&gt;Escaping Containment: A Security Analysis of FreeBSD Jails&lt;/head&gt;
    &lt;p&gt;ilja, Michael Smith&lt;/p&gt;
    &lt;head rend="h5"&gt;Die Känguru-Rebellion: Digital Independence Day&lt;/head&gt;
    &lt;p&gt;Marc-Uwe Kling, Linus Neumann&lt;/p&gt;
    &lt;head rend="h5"&gt;And so it begins - Wie unser Rechtsstaat auf dem Highway Richtung Trumpismus rast – und warum afghanische Kläger*innen für uns die Notbremse ziehen&lt;/head&gt;
    &lt;p&gt;Eva, Elaha&lt;/p&gt;
    &lt;head rend="h5"&gt;1965 + 60 Years of Algorithmic Art with Computers&lt;/head&gt;
    &lt;p&gt;Enna Gerhard, Frieder Nake&lt;/p&gt;
    &lt;head rend="h5"&gt;Life on Hold: What Does True Solidarity Look Like Beyond Duldung, Camps, Deportation, and Payment Cards?&lt;/head&gt;
    &lt;p&gt;Hafid Shaaib, Eric Noel Mbiakeu&lt;/p&gt;
    &lt;head rend="h5"&gt;Chatkontrolle - Ctrl+Alt+Delete&lt;/head&gt;
    &lt;p&gt;khaleesi, Markus Reuter&lt;/p&gt;
    &lt;head rend="h5"&gt;Excuse me, what precise time is It?&lt;/head&gt;
    &lt;p&gt;Oliver Ettlin&lt;/p&gt;
    &lt;head rend="h5"&gt;BitUnlocker: Leveraging Windows Recovery to Extract BitLocker Secrets&lt;/head&gt;
    &lt;p&gt;Alon Leviev&lt;/p&gt;
    &lt;head rend="h5"&gt;Not To Be Trusted - A Fiasco in Android TEEs&lt;/head&gt;
    &lt;p&gt;0ddc0de, gannimo, Philipp&lt;/p&gt;
    &lt;head rend="h5"&gt;Hacking washing machines&lt;/head&gt;
    &lt;p&gt;Severin von Wnuck-Lipinski, Hajo Noerenberg&lt;/p&gt;
    &lt;head rend="h5"&gt;Doomsday-Porn, Schäferhunde und die „niedliche Abschiebung“ von nebenan: Wie autoritäre Akteure KI-generierte Inhalte für Social Media nutzen&lt;/head&gt;
    &lt;p&gt;Katharina Nocun&lt;/p&gt;
    &lt;head rend="h5"&gt;Throwing your rights under the Omnibus - how the EU's reform agenda threatens to erase a decade of digital rights&lt;/head&gt;
    &lt;p&gt;Thomas Lohninger, Ralf Bendrath&lt;/p&gt;
    &lt;head rend="h5"&gt;DNGerousLINK: A Deep Dive into WhatsApp 0-Click Exploits on iOS and Samsung Devices&lt;/head&gt;
    &lt;p&gt;Zhongrui Li, Yizhe Zhuang, Kira Chen&lt;/p&gt;
    &lt;head rend="h5"&gt;Bluetooth Headphone Jacking: A Key to Your Phone&lt;/head&gt;
    &lt;p&gt;Dennis Heinze, Frieder Steinmetz&lt;/p&gt;
    &lt;head rend="h5"&gt;Breaking architecture barriers: Running x86 games and apps on ARM&lt;/head&gt;
    &lt;p&gt;neobrain&lt;/p&gt;
    &lt;head rend="h5"&gt;The Eyes of Photon Science: Imaging, Simulation and the Quest to Make the Invisible Visible&lt;/head&gt;
    &lt;p&gt;MarKuster&lt;/p&gt;
    &lt;head rend="h5"&gt;Coding Dissent: Art, Technology, and Tactical Media&lt;/head&gt;
    &lt;p&gt;Helena Nikonole&lt;/p&gt;
    &lt;head rend="h5"&gt;AI-generated content in Wikipedia - a tale of caution&lt;/head&gt;
    &lt;p&gt;Mathias Schindler&lt;/p&gt;
    &lt;head rend="h5"&gt;Building a NOC from scratch&lt;/head&gt;
    &lt;p&gt;lilly, Scientress&lt;/p&gt;
    &lt;head rend="h5"&gt;From Silicon to Darude Sand-storm: breaking famous synthesizer DSPs&lt;/head&gt;
    &lt;p&gt;giulioz&lt;/p&gt;
    &lt;head rend="h5"&gt;Unnecessarily Complicated Kitchen – Die Wissenschaft des guten Geschmacks&lt;/head&gt;
    &lt;p&gt;LukasQ&lt;/p&gt;
    &lt;head rend="h2"&gt;Sun - Day 2 - December 28&lt;/head&gt;
    &lt;head rend="h5"&gt;Junghacker:innentag Einführung&lt;/head&gt;
    &lt;head rend="h5"&gt;Protecting the network data of one billion people: Breaking network crypto in popular Chinese mobile apps&lt;/head&gt;
    &lt;p&gt;Mona&lt;/p&gt;
    &lt;head rend="h5"&gt;Hatupangwingwi: The story how Kenyans fought back against intrusive digital identity systems&lt;/head&gt;
    &lt;p&gt;Mustafa Mahmoud Yousif&lt;/p&gt;
    &lt;head rend="h5"&gt;Lightning Talks - Tag 2&lt;/head&gt;
    &lt;p&gt;bonnie, Gilbert, Andi Bräu&lt;/p&gt;
    &lt;head rend="h5"&gt;Digitale Inklusion: Wie wir digitale Barrierefreiheit für alle erreichen können&lt;/head&gt;
    &lt;p&gt;Jakob Sponholz, Kathrin Klapper, Lena Christina Müller&lt;/p&gt;
    &lt;head rend="h5"&gt;Skynet Starter Kit: From Embodied AI Jailbreak to Remote Takeover of Humanoid Robots&lt;/head&gt;
    &lt;p&gt;Shipei Qu, Zikai Xu, Xuangan Xiao&lt;/p&gt;
    &lt;head rend="h5"&gt;Suing spyware in Europe: news from the front!&lt;/head&gt;
    &lt;p&gt;Lori Roussey, Celia/Irídia&lt;/p&gt;
    &lt;head rend="h5"&gt;Neue Chaos Events - InselChaos und Håck ma’s Castle plaudern aus dem Nähkästchen&lt;/head&gt;
    &lt;p&gt;Erwin Ernst "eest9" Steinhammer, lasii, Daniel, Niklas&lt;/p&gt;
    &lt;head rend="h5"&gt;A post-American, enshittification-resistant internet&lt;/head&gt;
    &lt;p&gt;Cory Doctorow&lt;/p&gt;
    &lt;head rend="h5"&gt;A space odyssey #2: How to study moon rocks from the Soviet sample return mission Luna 24&lt;/head&gt;
    &lt;p&gt;Paul Koetter, Christopher Hamann&lt;/p&gt;
    &lt;head rend="h5"&gt;Agentic ProbLLMs: Exploiting AI Computer-Use and Coding Agents&lt;/head&gt;
    &lt;p&gt;Johann Rehberger&lt;/p&gt;
    &lt;head rend="h5"&gt;selbstverständlich antifaschistisch! Aktuelle Informationen zu den Verfahren im Budapest-Komplex - von family &amp;amp; friends Hamburg&lt;/head&gt;
    &lt;p&gt;Andreas family &amp;amp; friends Hamburg, Birgit family &amp;amp; friends Hamburg&lt;/p&gt;
    &lt;head rend="h5"&gt;Chaospager - How to construct an Open Pager System for c3&lt;/head&gt;
    &lt;p&gt;Max, Julian&lt;/p&gt;
    &lt;head rend="h5"&gt;Chaos Communication Chemistry: DNA information technology and security based on molecular entropy&lt;/head&gt;
    &lt;p&gt;Anne Lüscher&lt;/p&gt;
    &lt;head rend="h5"&gt;Live, Die, Repeat: The fight against data retention and boundless access to data&lt;/head&gt;
    &lt;p&gt;Klaus Landefeld&lt;/p&gt;
    &lt;head rend="h5"&gt;Power Cycle B7 oder Warum kauft man eine Zeche?&lt;/head&gt;
    &lt;p&gt;Kohlenpod, kater, Stephan&lt;/p&gt;
    &lt;head rend="h5"&gt;Cracking open what makes Apple's Low-Latency WiFi so fast&lt;/head&gt;
    &lt;p&gt;Henri Jäger&lt;/p&gt;
    &lt;head rend="h5"&gt;Awful interception: misadventures of the russian surveillance machinery&lt;/head&gt;
    &lt;p&gt;Xeniax&lt;/p&gt;
    &lt;head rend="h5"&gt;Amateurfunk im All – Kontakt mit Fram2&lt;/head&gt;
    &lt;p&gt;akira25, flx, Gato&lt;/p&gt;
    &lt;head rend="h5"&gt;Über europäische Grenzen hinweg auf klinischen Daten rechnen - aber sicher!&lt;/head&gt;
    &lt;p&gt;Hendrik Ballhausen&lt;/p&gt;
    &lt;head rend="h5"&gt;CCC-Jahresrückblick&lt;/head&gt;
    &lt;p&gt;Constanze Kurz, khaleesi, Matthias Marx, Linus Neumann, erdgeist&lt;/p&gt;
    &lt;head rend="h5"&gt;Persist, resist, stitch&lt;/head&gt;
    &lt;p&gt;Philo&lt;/p&gt;
    &lt;head rend="h5"&gt;Lessons from Building an Open-Architecture Secure Element&lt;/head&gt;
    &lt;p&gt;Jan Pleskac&lt;/p&gt;
    &lt;head rend="h5"&gt;Auf die Dauer hilft nur Power: Herausforderungen für dezentrale Netzwerke aus Sicht der Soziologie&lt;/head&gt;
    &lt;p&gt;Marco Wähner&lt;/p&gt;
    &lt;head rend="h5"&gt;Current Drone Wars&lt;/head&gt;
    &lt;p&gt;Leonard&lt;/p&gt;
    &lt;head rend="h5"&gt;Variable Fonts — It Was Never About File Size&lt;/head&gt;
    &lt;p&gt;Bernd&lt;/p&gt;
    &lt;head rend="h5"&gt;A Quick Stop at the HostileShop&lt;/head&gt;
    &lt;p&gt;Mike Perry&lt;/p&gt;
    &lt;head rend="h5"&gt;In-house electronics manufacturing from scratch: How hard can it be?&lt;/head&gt;
    &lt;p&gt;Augustin Bielefeld, Alexander Willer&lt;/p&gt;
    &lt;head rend="h5"&gt;CPU Entwicklung in Factorio: Vom D-Flip-Flop bis zum eigenen Betriebssystem&lt;/head&gt;
    &lt;p&gt;PhD (Philipp)&lt;/p&gt;
    &lt;head rend="h5"&gt;Amtsgeheimnis raus, Datenhalde rein: was die Informationsfreiheit in Österreich bringt&lt;/head&gt;
    &lt;p&gt;Markus (fin) Hametner, Erwin Ernst "eest9" Steinhammer&lt;/p&gt;
    &lt;head rend="h5"&gt;How to render cloud FPGAs useless&lt;/head&gt;
    &lt;p&gt;Dirk&lt;/p&gt;
    &lt;head rend="h5"&gt;freiheit.exe - Utopien als Malware&lt;/head&gt;
    &lt;p&gt;Christiane Mudra&lt;/p&gt;
    &lt;head rend="h5"&gt;Recharge your batteries with us - an empowering journey through the energy transition&lt;/head&gt;
    &lt;p&gt;Salacidre, JulianeB&lt;/p&gt;
    &lt;head rend="h5"&gt;Prometheus: Reverse-Engineering Overwatch&lt;/head&gt;
    &lt;p&gt;breakingbread&lt;/p&gt;
    &lt;head rend="h5"&gt;Trump government demands access to European police databases and biometrics&lt;/head&gt;
    &lt;p&gt;Matthias Monroy&lt;/p&gt;
    &lt;head rend="h5"&gt;Verlorene Domains, offene Türen - Was alte Behördendomains verraten&lt;/head&gt;
    &lt;p&gt;Tim Philipp Schäfers (TPS)&lt;/p&gt;
    &lt;head rend="h5"&gt;CSS Clicker Training: Making games in a "styling" language&lt;/head&gt;
    &lt;p&gt;Lyra Rebane&lt;/p&gt;
    &lt;head rend="h5"&gt;Wie wir alte Flipperautomaten am Leben erhalten&lt;/head&gt;
    &lt;p&gt;Axel Böttcher&lt;/p&gt;
    &lt;head rend="h5"&gt;Power Cycles statt Burnout – Wie Einflussnahme nicht verpufft&lt;/head&gt;
    &lt;p&gt;Rahel Becker, Anna Kassautzki&lt;/p&gt;
    &lt;head rend="h5"&gt;Don’t look up: There are sensitive internal links in the clear on GEO satellites&lt;/head&gt;
    &lt;p&gt;Nadia Heninger, Annie Dai&lt;/p&gt;
    &lt;head rend="h5"&gt;Textiles 101: Fast Fiber Transform&lt;/head&gt;
    &lt;p&gt;octoprog&lt;/p&gt;
    &lt;head rend="h5"&gt;How To Minimize Bugs in Cryptography Code&lt;/head&gt;
    &lt;p&gt;Jade&lt;/p&gt;
    &lt;head rend="h5"&gt;Machine Vision – Vom Algorithmus zum Baumpilz im digitalen Metabolismus&lt;/head&gt;
    &lt;p&gt;Thomas Knüsel&lt;/p&gt;
    &lt;head rend="h5"&gt;Xous: A Pure-Rust Rethink of the Embedded Operating System&lt;/head&gt;
    &lt;p&gt;bunnie, Sean "xobs" Cross&lt;/p&gt;
    &lt;head rend="h5"&gt;51 Ways to Spell the Image Giraffe: The Hidden Politics of Token Languages in Generative AI&lt;/head&gt;
    &lt;p&gt;Ting-Chun Liu, Leon-Etienne Kühr&lt;/p&gt;
    &lt;head rend="h5"&gt;When Vibe Scammers Met Vibe Hackers: Pwning PhaaS with Their Own Weapons&lt;/head&gt;
    &lt;p&gt;Chiao-Lin Yu (Steven Meow)&lt;/p&gt;
    &lt;head rend="h5"&gt;The Maybe Talent Show&lt;/head&gt;
    &lt;p&gt;Norman Müller-Schmitz, lukas-schmukas, James Bonne d'age&lt;/p&gt;
    &lt;head rend="h5"&gt;Code to Craft: Procedural Generation for the Physical World&lt;/head&gt;
    &lt;p&gt;bleeptrack&lt;/p&gt;
    &lt;head rend="h5"&gt;Reverse engineering the Pixel TitanM2 firmware&lt;/head&gt;
    &lt;p&gt;willem&lt;/p&gt;
    &lt;head rend="h5"&gt;The Small Packet of Bits That Can Save (or Destabilize) a City&lt;/head&gt;
    &lt;p&gt;Manuel Rábade&lt;/p&gt;
    &lt;head rend="h5"&gt;GPTDash – Der Reverse-Turing-Test&lt;/head&gt;
    &lt;p&gt;Benny, Kilian, BratscherBen&lt;/p&gt;
    &lt;head rend="h2"&gt;Mon - Day 3 - December 29&lt;/head&gt;
    &lt;head rend="h5"&gt;Azubi-Tag Einführung&lt;/head&gt;
    &lt;head rend="h5"&gt;Greenhouse Gas Emission Data: Public, difficult to access, and not always correct&lt;/head&gt;
    &lt;p&gt;Hanno Böck&lt;/p&gt;
    &lt;head rend="h5"&gt;Design for 3D-Printing&lt;/head&gt;
    &lt;p&gt;rahix&lt;/p&gt;
    &lt;head rend="h5"&gt;Lightning Talks - Tag 3&lt;/head&gt;
    &lt;p&gt;bonnie, Gilbert, Andi Bräu&lt;/p&gt;
    &lt;head rend="h5"&gt;The Museum of Care: Open-Source Survival Kit Collection&lt;/head&gt;
    &lt;p&gt;Nika Dubrovsky&lt;/p&gt;
    &lt;head rend="h5"&gt;Celestial navigation with very little math&lt;/head&gt;
    &lt;p&gt;Trammell Hudson&lt;/p&gt;
    &lt;head rend="h5"&gt;a media-almost-archaeology on data that is too dirty for "AI"&lt;/head&gt;
    &lt;p&gt;jiawen uffline&lt;/p&gt;
    &lt;head rend="h5"&gt;Hacking Karlsruhe - 10 years later&lt;/head&gt;
    &lt;p&gt;Jürgen Bering&lt;/p&gt;
    &lt;head rend="h5"&gt;What Makes Bike-Sharing Work? Insights from 43 Million Kilometers of European Cycling Data&lt;/head&gt;
    &lt;p&gt;Martin Lellep, Georg Balke, FelixW&lt;/p&gt;
    &lt;head rend="h5"&gt;Teckids – eine verstehbare (digitale) Welt&lt;/head&gt;
    &lt;p&gt;Keno, Darius Auding&lt;/p&gt;
    &lt;head rend="h5"&gt;BE Modded: Exploring and hacking the Vital Bracelet ecosystem&lt;/head&gt;
    &lt;p&gt;cyanic&lt;/p&gt;
    &lt;head rend="h5"&gt;Wer hat Angst vor dem Neutralitätsgebot?&lt;/head&gt;
    &lt;p&gt;Hannah Vos, Vivian Kube&lt;/p&gt;
    &lt;head rend="h5"&gt;Shit for Future: turning human shit into a climate solution&lt;/head&gt;
    &lt;p&gt;Elena&lt;/p&gt;
    &lt;head rend="h5"&gt;Watch Your Kids: Inside a Children's Smartwatch&lt;/head&gt;
    &lt;p&gt;Nils Rollshausen&lt;/p&gt;
    &lt;head rend="h5"&gt;When 8 Bits is Overkill: Making Blinkenlights with a 1-bit CPU&lt;/head&gt;
    &lt;p&gt;girst (Tobi)&lt;/p&gt;
    &lt;head rend="h5"&gt;Supplements und Social Media – wenn der Online-Hype zur realen Gesundheitsgefahr wird&lt;/head&gt;
    &lt;p&gt;Christoph Wiedmer&lt;/p&gt;
    &lt;head rend="h5"&gt;Programmierte Kriegsverbrechen? Über KI-Systeme im Kriegseinsatz in Gaza und warum IT-Fachleute sich dazu äußern müssen&lt;/head&gt;
    &lt;p&gt;Rainer Rehak&lt;/p&gt;
    &lt;head rend="h5"&gt;Making the Magic Leap past NVIDIA's secure bootchain and breaking some Tesla Autopilots along the way&lt;/head&gt;
    &lt;p&gt;EliseZeroTwo&lt;/p&gt;
    &lt;head rend="h5"&gt;Learning from South Korean Telco Breaches&lt;/head&gt;
    &lt;p&gt;Shinjo "peremen" Park, Yonghyu "perillamint" Ban&lt;/p&gt;
    &lt;head rend="h5"&gt;Gegenmacht - Best of Informationsfreiheit&lt;/head&gt;
    &lt;p&gt;Arne Semsrott&lt;/p&gt;
    &lt;head rend="h5"&gt;There is NO WAY we ended up getting arrested for this (Malta edition)&lt;/head&gt;
    &lt;p&gt;mixy1, Luke Bjorn Scerri, girogio&lt;/p&gt;
    &lt;head rend="h5"&gt;APT Down and the mystery of the burning data centers&lt;/head&gt;
    &lt;p&gt;Christopher Kunz, Sylvester&lt;/p&gt;
    &lt;head rend="h5"&gt;Von wegen Eisblumen! Wie man mit Code, Satelliten und Schiffsexpeditionen die bunte Welt des arktischen Phytoplanktons sichtbar macht&lt;/head&gt;
    &lt;p&gt;Moritz Zeising (er/he)&lt;/p&gt;
    &lt;head rend="h5"&gt;Schlechte Karten - IT-Sicherheit im Jahr null der ePA für alle&lt;/head&gt;
    &lt;p&gt;Bianca Kastl&lt;/p&gt;
    &lt;head rend="h5"&gt;Set-top box Hacking: freeing the 'Freebox'&lt;/head&gt;
    &lt;p&gt;Frédéric Hoguin&lt;/p&gt;
    &lt;head rend="h5"&gt;Wer liegt hier wem auf der Tasche? Genug mit dem Bürgergeld-Fetisch. Stürmt die Paläste!&lt;/head&gt;
    &lt;p&gt;Helena Steinhaus&lt;/p&gt;
    &lt;head rend="h5"&gt;The Last of Us - Fighting the EU Surveillance Law Apocalypse&lt;/head&gt;
    &lt;p&gt;Svea Windwehr, Chloé Berthélémy&lt;/p&gt;
    &lt;head rend="h5"&gt;AI Agent, AI Spy&lt;/head&gt;
    &lt;p&gt;Udbhav Tiwari, Meredith Whittaker&lt;/p&gt;
    &lt;head rend="h5"&gt;Build a Fake Phone, Find Real Bugs: Qualcomm GPU Emulation and Fuzzing with LibAFL QEMU&lt;/head&gt;
    &lt;p&gt;Romain Malmain, Scott Bauer&lt;/p&gt;
    &lt;head rend="h5"&gt;Transkultureller Hack auf die klassische Musikszene – Vortrag und Konzert&lt;/head&gt;
    &lt;p&gt;Johanna-Leonore Dahlhoff, Neina Doroshenko, Peter Klohmann, Alireza Meghrazi Solouklou, Mirweis Neda, Maria Carolina Pardo Reyes, Eduardo Sabella, Sarah Luisa Wurmer&lt;/p&gt;
    &lt;head rend="h5"&gt;Netzpolitik in der Schweiz: Zwischen Bodensee und Matterhorn&lt;/head&gt;
    &lt;p&gt;Kire, Rahel&lt;/p&gt;
    &lt;head rend="h5"&gt;The Angry Path to Zen: AMD Zen Microcode Tools and Insights&lt;/head&gt;
    &lt;p&gt;Benjamin Kollenda&lt;/p&gt;
    &lt;head rend="h5"&gt;Blackbox Palantir&lt;/head&gt;
    &lt;p&gt;Constanze Kurz, Franziska Görlitz&lt;/p&gt;
    &lt;head rend="h5"&gt;Aber hier Leben? Nein danke! …oder doch? Wie wir der autoritären Zuspitzung begegnen können.&lt;/head&gt;
    &lt;p&gt;Jaša Hiergeblieben, Lisa Zugezogen&lt;/p&gt;
    &lt;head rend="h5"&gt;Race conditions, transactions and free parking&lt;/head&gt;
    &lt;p&gt;Benjamin W. Broersma&lt;/p&gt;
    &lt;head rend="h5"&gt;Hegemony Eroding: Excavating Diversity in Latent Space&lt;/head&gt;
    &lt;p&gt;Karim Hamdi&lt;/p&gt;
    &lt;head rend="h5"&gt;10 years of Dieselgate&lt;/head&gt;
    &lt;p&gt;Felix Domke, Karsten Burger&lt;/p&gt;
    &lt;head rend="h5"&gt;The Heartbreak Machine: Nazis in the Echo Chamber&lt;/head&gt;
    &lt;p&gt;Martha Root, Eva Hoffmann, Christian Fuchs&lt;/p&gt;
    &lt;head rend="h5"&gt;Light in the Dark(net)&lt;/head&gt;
    &lt;p&gt;Tobias Höller&lt;/p&gt;
    &lt;head rend="h5"&gt;The Spectrum - Hackspace Beyond Hacking&lt;/head&gt;
    &lt;p&gt;sjaelv, MultisampledNight&lt;/p&gt;
    &lt;head rend="h5"&gt;Rowhammer in the Wild: Large-Scale Insights from FlippyR.AM&lt;/head&gt;
    &lt;p&gt;Martin Heckel, Florian Adamsky, Daniel Gruss&lt;/p&gt;
    &lt;head rend="h5"&gt;Peep-Show für die Polizei. Staatliche Überwachung von Queers in Hamburger Toiletten bis 1980&lt;/head&gt;
    &lt;p&gt;Simon Schultz&lt;/p&gt;
    &lt;head rend="h5"&gt;Human microservices at the Dutch Railways: modern architecture, ancient hardware?&lt;/head&gt;
    &lt;p&gt;Maarten W&lt;/p&gt;
    &lt;head rend="h5"&gt;Von Fuzzern zu Agenten: Entwicklung eines Cyber Reasoning Systems für die AIxCC&lt;/head&gt;
    &lt;p&gt;Mischa Meier (mmisc), Annika Kuntze&lt;/p&gt;
    &lt;head rend="h5"&gt;PRÜF&lt;/head&gt;
    &lt;p&gt;Nico Semsrott&lt;/p&gt;
    &lt;head rend="h5"&gt;Verschlüsselung brechen durch physischen Zugriff - Smartphone Beschlagnahme durch Polizei&lt;/head&gt;
    &lt;p&gt;Davy Wang, Viktor Schlüter&lt;/p&gt;
    &lt;head rend="h5"&gt;Spectre in the real world: Leaking your private data from the cloud with CPU vulnerabilities&lt;/head&gt;
    &lt;p&gt;Thijs Raymakers&lt;/p&gt;
    &lt;head rend="h5"&gt;Die große Datenschutz-, Datenpannen- und DS-GVO-Show&lt;/head&gt;
    &lt;p&gt;Alvar C.H. Freude&lt;/p&gt;
    &lt;head rend="h2"&gt;Tue - Day 4 - December 30&lt;/head&gt;
    &lt;head rend="h5"&gt;Asahi Linux - Porting Linux to Apple Silicon&lt;/head&gt;
    &lt;p&gt;sven&lt;/p&gt;
    &lt;head rend="h5"&gt;Atoms in Space&lt;/head&gt;
    &lt;p&gt;manuel&lt;/p&gt;
    &lt;head rend="h5"&gt;I Hated All The Cross-Stitch Software So I Made My Own: My Deranged Outsider Software Suite For Making Deranged Outsider Art&lt;/head&gt;
    &lt;p&gt;yomimono&lt;/p&gt;
    &lt;head rend="h5"&gt;How to keep Open Source open without leaving our communities open to threats&lt;/head&gt;
    &lt;p&gt;Quintessence&lt;/p&gt;
    &lt;head rend="h5"&gt;CCC&amp;amp;T - Cosmic ray, the Climate Catastrophe and Trains.&lt;/head&gt;
    &lt;p&gt;FantasticMisterFux, louiT&lt;/p&gt;
    &lt;head rend="h5"&gt;CUII: Wie Konzerne heimlich Webseiten in Deutschland sperren&lt;/head&gt;
    &lt;p&gt;Lina Lastname, Elias Zeidler (Northernside)&lt;/p&gt;
    &lt;head rend="h5"&gt;“End Of 10”: How the FOSS Community is Combatting Software-Driven Resource and Energy Consumption&lt;/head&gt;
    &lt;p&gt;Joseph P. De Veaugh-Geiss, Carolina Silva Rode, belobe&lt;/p&gt;
    &lt;head rend="h5"&gt;What You Hack Is What You Mean: 35 Years of Wiring Sense into Text&lt;/head&gt;
    &lt;p&gt;Torsten Roeder&lt;/p&gt;
    &lt;head rend="h5"&gt;Security of Cardiac Implantable Electronic Devices&lt;/head&gt;
    &lt;p&gt;dilucide&lt;/p&gt;
    &lt;head rend="h5"&gt;Who runs the www? WSIS+20 and the future of Internet governance&lt;/head&gt;
    &lt;p&gt;Sophia Longwe&lt;/p&gt;
    &lt;head rend="h5"&gt;Fossile Industrie liebt KI!&lt;/head&gt;
    &lt;p&gt;Stefan, Yannik &amp;amp; Rike, Moritz&lt;/p&gt;
    &lt;head rend="h5"&gt;Laser Beams &amp;amp; Light Streams: Letting Hackers Go Pew Pew, Building Affordable Light-Based Hardware Security Tooling&lt;/head&gt;
    &lt;p&gt;Patch, Sam. Beaumont (PANTH13R)&lt;/p&gt;
    &lt;head rend="h5"&gt;Breaking BOTS: Cheating at Blue Team CTFs with AI Speed-Runs&lt;/head&gt;
    &lt;p&gt;Leo Meyerovich, Sindre Breda&lt;/p&gt;
    &lt;head rend="h5"&gt;Von Groschen und SpurLos - GNU Taler auch auf eurem Event!&lt;/head&gt;
    &lt;p&gt;Mikolai Gütschow, signum&lt;/p&gt;
    &lt;head rend="h5"&gt;We, the EU, and 1064 Danes decided to look into YouTube: A story about how the EU gave us a law, 1064 Danes gave us their YouTube histories, and reality gave us a headache&lt;/head&gt;
    &lt;p&gt;David, LK Seiling&lt;/p&gt;
    &lt;head rend="h5"&gt;Battling Obsolescence – Keeping an 80s laser tag system alive&lt;/head&gt;
    &lt;p&gt;Trikkitt&lt;/p&gt;
    &lt;head rend="h5"&gt;Security Nightmares&lt;/head&gt;
    &lt;p&gt;Constanze Kurz, Ron&lt;/p&gt;
    &lt;head rend="h5"&gt;Infrastructure Review&lt;/head&gt;
    &lt;p&gt;nicoduck&lt;/p&gt;
    &lt;head rend="h5"&gt;Closing Ceremony&lt;/head&gt;
    &lt;p&gt;Stella, pajowu&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46386211</guid><pubDate>Thu, 25 Dec 2025 18:40:14 +0000</pubDate></item><item><title>Maybe the default settings are too high</title><link>https://www.raptitude.com/2025/12/maybe-the-default-settings-are-too-high/</link><description>&lt;doc fingerprint="e65b52868faaab9c"&gt;
  &lt;main&gt;
    &lt;p&gt;I’ve been reading Lord of the Rings for two months and I’m just at the end of the first part. It’s not because I’m not enjoying it. It’s one of the most enjoyable reading experiences I can remember.&lt;/p&gt;
    &lt;p&gt;From the beginning, I’ve read the whole thing aloud. I’ve found reading aloud helpful for staying engaged — limiting myself to mouth-speed rather than eye-speed means I won’t rush, miss important details, and then lose interest, which has always been a problem for me.&lt;/p&gt;
    &lt;p&gt;At first I was anxious to read a 1,500-page book this way, because it would take so long. But, as someone pointed out to me, if I’m enjoying it, why would I want to be done with it sooner?&lt;/p&gt;
    &lt;p&gt;So I tried slowing down even more, and discovered something. I slowed to a pace that felt almost absurd, treating each sentence as though it might be a particularly important one. I gave each one maybe triple the usual time and attention, ignoring the fact that there are hundreds of pages to go.&lt;/p&gt;
    &lt;p&gt;This leisurely pace made Middle-Earth blossom before my eyes. When I paused after each comma, and let each sentence ring for a small moment after the period, the events of the story reached me with more weight and strength. That extra time gave space for Tolkien’s images and moods to propagate in my mind, which they did automatically.&lt;/p&gt;
    &lt;p&gt;Some part of me still wanted to rush and get on with it, to make good time, to gloss over the songs and lore to get to Moria and Mount Doom and the other marquee moments of the story. But the more I ignored that impulse, the better the experience got.&lt;/p&gt;
    &lt;p&gt;By offering the book about triple the usual amount of attentiveness, I was getting about triple the storyness (i.e. meaning, engagement, literary pleasure). Whatever the thing is that I’m seeking when I pick up a novel in the first place, there’s much more of it available at this pace.&lt;/p&gt;
    &lt;head rend="h3"&gt;Eating Comprehension&lt;/head&gt;
    &lt;p&gt;This effect reminded me of a paradox around eating I recognized long ago. When you slow down your eating speed, say to half or a third your default speed, you get much more enjoyment out of a smaller amount of food. The extra attention given to each bite allows more of the “good stuff,” whatever that is exactly, to reach you.&lt;/p&gt;
    &lt;p&gt;What’s paradoxical is that it’s precisely the seeking of that “good stuff” that normally drives me to eat so quickly, and miss most of what I’m seeking. When you try to barrel ahead to access the good stuff quicker, you get less of it in the end. Slow down and much more of it is released.&lt;/p&gt;
    &lt;p&gt;And it’s released automatically, in both reading and eating. You don’t have to search it out. The good stuff (the meaning in the text, the pleasure in the eating) just rises up to meet you in that extra time you give it. Slowing down, and offering more time to the act of consumption, immediately increases reading comprehension (and eating comprehension).&lt;/p&gt;
    &lt;p&gt;Both are analogous to slowing down while you vacuum a carpet. If you pass the vacuum head too quickly, you miss half the dirt. Slow down, and you can hear how much more grit is sent skittering up the tube. The suction and bristles are working, but they need more time to do their work fully, to draw up the deeper-lying stuff.&lt;/p&gt;
    &lt;head rend="h3"&gt;Question the default settings&lt;/head&gt;
    &lt;p&gt;It seems that my default consumption speeds for reading and eating (and maybe everything else) reduce the rewards of those things significantly, undermining the point of doing either.&lt;/p&gt;
    &lt;p&gt;Part of it is my own impatience. But I also suspect that modern living, with its infinite supply of consumables, tends to push our rate-of-intake dials too high. I’m not going to run out of books, or snacks, or opportunities to learn something. There’s always more, so not every crust of bread or printed page needs to be appreciated fully.&lt;/p&gt;
    &lt;p&gt;Internally though, the mind is juggling like Lucy and Ethel on the conveyor belt at the chocolate factory. Our receptors for meaning and appreciation, like the vacuum head, need more time to do their full work, to make all the connections they’re designed to make.&lt;/p&gt;
    &lt;p&gt;It might sound like I’m just offering clichés – less is more, stop and smell the roses, take your time – and I guess I am. But clichés suffer the same issue: they are often profound insights, consumed and passed on too rapidly for their real meaning to register anymore. You really should stop and smell roses, as you know if you’re in the habit of doing that.&lt;/p&gt;
    &lt;p&gt;At least see what happens when you reduce your consumption speed – of anything, but especially books, information, and food – by a half, or two thirds. Notice that (1) something in you really wants to plow through at the highest viable setting, and (2) how much more of the reward is released when you slow down anyway.&lt;/p&gt;
    &lt;p&gt;As far as I can tell, almost everything becomes more satisfying when you give it more time and intention, even things like checking the mailbox or writing a shopping list.&lt;/p&gt;
    &lt;head rend="h3"&gt;Speed alters taste&lt;/head&gt;
    &lt;p&gt;Slowing down your rate of consumption will inevitably change what you want to consume. Reading throwaway news articles or AI slop with great care and attention is only going to show you how empty of value it is. Reading dense writing in inky old books, crafted for your mind by great masters, becomes easier without the rushed pace, and the meaning just blooms out of it.&lt;/p&gt;
    &lt;p&gt;Same with food. Try to savor a cheap, waxy “chocolate” bar, or a bag of store-brand cheese puffs, and you discover a harsh taste that you don’t want to look at too closely. Enjoy a homemade pastry with great attention, and discover there’s even more in it than you realized.&lt;/p&gt;
    &lt;p&gt;Mass production is good in so many ways, but the faster we tend to consume its fruits, the more we end up seeking things for their glossy, candied surfaces. The more we go for these surface-level rewards, the more the culture focuses on offering only that part – such as TikTok videos, processed food, CGI-forward movies, and public discourse in the form of unexamined talking points.&lt;/p&gt;
    &lt;p&gt;Who knows how far we’ve drifted from the best modes of consuming the things we value. Once something becomes a norm, it seems like an appropriate standard, no matter how much has been lost. Apparently, reading silently and alone was unusual until as late as the 18th century. Certainly sit-down meals and cooking at home were.&lt;/p&gt;
    &lt;p&gt;I don’t mean to sound like a scold. Let’s say none of this is morally good or bad. It’s just that in so much of what we do, we could be getting much more of the part of it that we really seek — but it’s only available at slower speeds.&lt;/p&gt;
    &lt;p&gt;If you’re curious, try consuming things more slowly, so slowly it seems silly to others — say a third your habitual speed — and see what rises up to meet you.&lt;/p&gt;
    &lt;p&gt;***&lt;/p&gt;
    &lt;head rend="h2"&gt;Want to quit something in January?&lt;/head&gt;
    &lt;p&gt;Recently I opened a discussion forum for Raptitude readers who want to give something up for the month of December (alcohol, social media, snacks, etc).&lt;/p&gt;
    &lt;p&gt;It’s been a real success, and many people want to do something similar in January. If you want to quit something, or just give it up for a month, you’re invited to join.&lt;/p&gt;
    &lt;p&gt;Follow this link at the end of this post to get an invite.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46387657</guid><pubDate>Thu, 25 Dec 2025 23:13:21 +0000</pubDate></item><item><title>MiniMax M2.1: Built for Real-World Complex Tasks, Multi-Language Programming</title><link>https://www.minimaxi.com/news/minimax-m21</link><description>&lt;doc fingerprint="7a878369f356732f"&gt;
  &lt;main&gt;
    &lt;p&gt;在10月底的M2中，我们主要解决模型成本和模型开放性的问题。在M2.1中，我们致力于提升真实世界复杂任务中的表现：重点聚焦于更多编程语言和办公场景的可用性，并在这个领域做到最好的水平。&lt;/p&gt;
    &lt;p&gt;MiniMax M2.1 具体模型亮点如下:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;卓越多编程语言能力&lt;/p&gt;
        &lt;p&gt;过去很多模型主要围绕 Python 优化, 但真实世界的系统往往是多语言协作的结果。&lt;/p&gt;
        &lt;p&gt;在 M2.1 中, 我们系统性提升了 Rust / Java / Golang / C++ / Kotlin / Objective-C / TypeScript / JavaScript 等语言的能力, 多语言任务整体表现达到业内领先水平, 覆盖从底层系统到应用层开发的完整链路。&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;WebDev 与 AppDev：能力与美学的整体跃迁&lt;/p&gt;
        &lt;p&gt;针对业界普遍存在的移动端开发短板, M2.1 显著加强了原生 Android / iOS 开发能力。&lt;/p&gt;
        &lt;p&gt;同时, 我们系统性提升了模型在 Web 与 App 场景中的设计理解与美学表达能力, 能够出色地构建复杂交互、3D科学场景模拟与高质量可视化表达, 推动 vibe coding 成为可持续、可交付的生产实践。&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;复合指令约束提升，办公场景变为可能&lt;/p&gt;
        &lt;p&gt;作为开源模型中率先系统性引入 Interleaved Thinking 的模型系列, M2.1 systematic problem-solving 能力再次升级。&lt;/p&gt;
        &lt;p&gt;模型不仅关注代码执行是否正确, 同时关注模型对“复合指令约束”的整合执行能力, 在真实办公场景具备更高的可用性。&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;更简洁高效的回复&lt;/p&gt;
        &lt;p&gt;相比 M2, MiniMax-M2.1 的模型回复以及思维链更加简洁, 在实际编程与交互体验中, 响应速度显著提升, Token 消耗明显下降, 在 AI Coding与Agent驱动的连续工作流中更加流畅和高效。&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;出色的 Agent / 工具脚手架泛化能力&lt;/p&gt;
        &lt;p&gt;M2.1 在各类编程工具与 Agent 框架中均有出色表现。在 Claude Code、Droid (Factory AI)、Cline、Kilo Code、Roo Code、BlackBox 等工具中展现一致且稳定的效果, 并对 Skill.md、Claude.md / agent.md / cursorrule、Slash Command 等 Context Management机制提供可靠支持。&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;高质量对话和写作&lt;/p&gt;
        &lt;p&gt;M2.1 不再只是“代码能力更强”, 在日常对话、技术说明与写作场景中, 也能提供更具细节与结构性的回答。&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;基准测试概览&lt;/head&gt;
    &lt;p&gt;MiniMax-M2.1 在 VIBE 综合榜单中表现卓越，以平均 88.6 分的成绩展现了接近Claude Opus 4.5的全栈构建能力，并在几乎所有子集上都显著优于Claude Sonnet 4.5。&lt;/p&gt;
    &lt;head rend="h3"&gt;使用者评价&lt;/head&gt;
    &lt;p&gt;我们非常期待像 M2.1 这样强大的开源模型，它在各类软件开发任务中都能带来前沿水准的表现，甚至还能在部分场景下比头部闭源模型更好。开发者应当拥有选择权，而 M2.1 正是大家急需的那个优质选项！&lt;/p&gt;
    &lt;p&gt;Eno Reyes&lt;/p&gt;
    &lt;p&gt;Co-Founder, CTO of Factory&lt;/p&gt;
    &lt;p&gt;MiniMax M2.1 在可读性与惯用结构方面与生产级工程要求高度契合，在 Go、Rust、C++ 等多语言场景下均表现稳定。精炼的交错推理机制显著压缩逻辑路径，减少冗余步骤，让多文件重构与缺陷修复等复杂任务得以更高精度完成。更可贵的是，M2.1 在激活参数量受限的前提下仍能提供可靠性能，为大规模智能体编码流程提供了兼顾效能与资源利用的均衡方案。我们期待与 MiniMax 团队展开持续、紧密的合作，在 Fireworks 平台同步支持其最新创新成果！&lt;/p&gt;
    &lt;p&gt;Benny Chen&lt;/p&gt;
    &lt;p&gt;Co-Founder of Fireworks&lt;/p&gt;
    &lt;p&gt;MiniMax M2 系列在代码生成能力上表现突出，过去几个月已迅速跻身 Cline 平台最受欢迎的模型之列。M2.1 再次实现能力层面的显著跃升，我们期待与 MiniMax 团队继续深化合作，共同推进 AI 编码技术的演进。&lt;/p&gt;
    &lt;p&gt;Saoud Rizwan&lt;/p&gt;
    &lt;p&gt;Founder, CEO of Cline&lt;/p&gt;
    &lt;p&gt;我们对M2.1的发布而兴奋！我们的用户已经离不开MiniMax提供的最优秀的编程辅助能力和高性价比，内测显示，M2.1在架构设计、服务编排、代码评审直至部署上线的全链路环节中均表现优异，速度与资源效率均处于领先水平。&lt;/p&gt;
    &lt;p&gt;Scott Breitenother&lt;/p&gt;
    &lt;p&gt;Co-Founder, CEO of Kilo&lt;/p&gt;
    &lt;p&gt;我们的用户非常喜欢 MiniMax M2 在编码能力与效率方面的表现。最新发布的 M2.1 在此基础上实现了速度与可靠性的实质性提升，并在更多语言及框架中保持稳定输出。对于强调高吞吐、Agentic Coding且对速度与成本敏感的研发流程，M2.1 是稳妥且具性价比的选择。&lt;/p&gt;
    &lt;p&gt;Matt Rubens&lt;/p&gt;
    &lt;p&gt;Co-Founder, CEO of RooCode&lt;/p&gt;
    &lt;head rend="h2"&gt;Showcases&lt;/head&gt;
    &lt;head rend="h2"&gt;物理世界Agent&lt;/head&gt;
    &lt;head rend="h2"&gt;多语言 Coding&lt;/head&gt;
    &lt;head rend="h2"&gt;Agentic Tool Use&lt;/head&gt;
    &lt;head rend="h2"&gt;数字员工&lt;/head&gt;
    &lt;p&gt;以下效果演示是 M2.1 在 AgentCompany Benchmark 中的行为轨迹记录。&lt;/p&gt;
    &lt;head rend="h2"&gt;全链路办公自动化&lt;/head&gt;
    &lt;head rend="h2"&gt;如何使用&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;MiniMax-M2.1 API 已在 MiniMax开放平台 开放使用：https://platform.minimaxi.com/docs/guides/text-generation&lt;/item&gt;
      &lt;item&gt;基于 MiniMax-M2.1 的通用 Agent 产品 MiniMax Agent 现已全面开放使用：https://agent.minimaxi.com/&lt;/item&gt;
      &lt;item&gt; 开源以及本地部署使用： https://huggingface.co/MiniMaxAI/MiniMax-M2.1 &lt;lb/&gt;https://github.com/MiniMax-AI/MiniMax-M2.1&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;为了方便用户使用, 我们提供了两个版本的 API, M2.1 和 M2.1-lightning。这两个 API 结果完全一样, 但是后者速度更快, 方便对 TPS 有需求的用户来使用。同时, 在 M2 手动 Cache 的基础上, M2.1 全面支持自动 Cache, 无需设置, 自动生效, 为开发者带来更流畅的体验、更低的成本与更优的延时表现。&lt;/p&gt;
    &lt;p&gt;我们在 Coding Plan 里面会根据资源负载给用户提供大比例的 M2.1-lightning, 并保持 Coding Plan 的价格不变。也就是说, Coding Plan 用户免费获得了大部分时间更快的推理速度。欢迎大家点击下单~&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46388213</guid><pubDate>Fri, 26 Dec 2025 01:02:53 +0000</pubDate></item><item><title>TurboDiffusion: 100–200× Acceleration for Video Diffusion Models</title><link>https://github.com/thu-ml/TurboDiffusion</link><description>&lt;doc fingerprint="f4d4ba193592fac2"&gt;
  &lt;main&gt;
    &lt;p&gt;This repository provides the official implementation of TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by &lt;lb/&gt; TurboDiffusion primarily uses SageAttention, SLA (Sparse-Linear Attention) for attention acceleration, and rCM for timestep distillation.&lt;/p&gt;
    &lt;p&gt;Paper: TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times&lt;/p&gt;
    &lt;p&gt;Note: the checkpoints and paper are not finalized, and will be updated later to improve quality.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 184s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 1.9s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Model Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Checkpoint Link&lt;/cell&gt;
        &lt;cell role="head"&gt;Best Resolution&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;TurboWan2.2-I2V-A14B-720P&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Huggingface Model&lt;/cell&gt;
        &lt;cell&gt;720p&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;TurboWan2.1-T2V-1.3B-480P&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Huggingface Model&lt;/cell&gt;
        &lt;cell&gt;480p&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;TurboWan2.1-T2V-14B-480P&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Huggingface Model&lt;/cell&gt;
        &lt;cell&gt;480p&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;TurboWan2.1-T2V-14B-720P&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Huggingface Model&lt;/cell&gt;
        &lt;cell&gt;720p&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note: All checkpoints support generating videos at 480p or 720p. The "Best Resolution" column indicates the resolution at which the model provides the best video quality.&lt;/p&gt;
    &lt;p&gt;Base environment: &lt;code&gt;python&amp;gt;=3.9&lt;/code&gt;, &lt;code&gt;torch&amp;gt;=2.7.0&lt;/code&gt;. &lt;code&gt;torch==2.8.0&lt;/code&gt; is recommended, as higher versions may cause OOM.&lt;/p&gt;
    &lt;p&gt;Install TurboDiffusion by pip:&lt;/p&gt;
    &lt;code&gt;conda create -n turbodiffusion python=3.12
conda activate turbodiffusion

pip install turbodiffusion --no-build-isolation&lt;/code&gt;
    &lt;p&gt;Or compile from source:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/thu-ml/TurboDiffusion.git
cd TurboDiffusion
git submodule update --init --recursive
pip install -e . --no-build-isolation&lt;/code&gt;
    &lt;p&gt;To enable SageSLA, a fast SLA forward pass based on SageAttention, install SpargeAttn first:&lt;/p&gt;
    &lt;code&gt;pip install git+https://github.com/thu-ml/SpargeAttn.git --no-build-isolation&lt;/code&gt;
    &lt;p&gt;For GPUs with more than 40GB of GPU memory, e.g., H100, please use the unquantized checkpoints (without &lt;code&gt;-quant&lt;/code&gt;) and remove &lt;code&gt;--quant_linear&lt;/code&gt; from the command. For RTX 5090, RTX 4090, or similar GPUs, please use the quantized checkpoints (with &lt;code&gt;-quant&lt;/code&gt;) and add &lt;code&gt;--quant_linear&lt;/code&gt; in the command.)&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Download the VAE (applicable for both Wan2.1 and Wan2.2) and umT5 text encoder checkpoints:&lt;/p&gt;
        &lt;code&gt;mkdir checkpoints cd checkpoints wget https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B/resolve/main/Wan2.1_VAE.pth wget https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B/resolve/main/models_t5_umt5-xxl-enc-bf16.pth&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Download our quantized model checkpoints (For RTX 5090 or similar GPUs):&lt;/p&gt;
        &lt;quote&gt;# For Wan2.1-T2V-1.3B wget https://huggingface.co/TurboDiffusion/TurboWan2.1-T2V-1.3B-480P/resolve/main/TurboWan2.1-T2V-1.3B-480P-quant.pth # For Wan2.2-I2V-14B wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-high-720P-quant.pth wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-low-720P-quant.pth&lt;/quote&gt;
        &lt;p&gt;Or download our unquantized model checkpoints (For H100 or similar GPUs):&lt;/p&gt;
        &lt;quote&gt;# For Wan2.1-T2V-1.3B wget https://huggingface.co/TurboDiffusion/TurboWan2.1-T2V-1.3B-480P/resolve/main/TurboWan2.1-T2V-1.3B-480P.pth # For Wan2.2-I2V-14B wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-high-720P.pth wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-low-720P.pth&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Use the inference script for the T2V models:&lt;/p&gt;
        &lt;quote&gt;export PYTHONPATH=turbodiffusion # Arguments: # --dit_path Path to the finetuned TurboDiffusion checkpoint # --model Model to use: Wan2.1-1.3B or Wan2.1-14B (default: Wan2.1-1.3B) # --num_samples Number of videos to generate (default: 1) # --num_steps Sampling steps, 1–4 (default: 4) # --sigma_max Initial sigma for rCM (default: 80); larger choices (e.g., 1600) reduce diversity but may enhance quality # --vae_path Path to Wan2.1 VAE (default: checkpoints/Wan2.1_VAE.pth) # --text_encoder_path Path to umT5 text encoder (default: checkpoints/models_t5_umt5-xxl-enc-bf16.pth) # --num_frames Number of frames to generate (default: 81) # --prompt Text prompt for video generation # --resolution Output resolution: "480p" or "720p" (default: 480p) # --aspect_ratio Aspect ratio in W:H format (default: 16:9) # --seed Random seed for reproducibility (default: 0) # --save_path Output file path including extension (default: output/generated_video.mp4) # --attention_type Attention module to use: original, sla or sagesla (default: sagesla) # --sla_topk Top-k ratio for SLA/SageSLA attention (default: 0.1), we recommend using 0.15 for better video quality # --quant_linear Enable quantization for linear layers, pass this if using a quantized checkpoint # --default_norm Use the original LayerNorm and RMSNorm of Wan models python turbodiffusion/inference/wan2.1_t2v_infer.py \ --model Wan2.1-1.3B \ --dit_path checkpoints/TurboWan2.1-T2V-1.3B-480P-quant.pth \ --resolution 480p \ --prompt "A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about." \ --num_samples 1 \ --num_steps 4 \ --quant_linear \ --attention_type sagesla \ --sla_topk 0.1&lt;/quote&gt;
        &lt;p&gt;Or the script for the I2V model:&lt;/p&gt;
        &lt;quote&gt;export PYTHONPATH=turbodiffusion # --image_path Path to the input image # --high_noise_model_path Path to the high noise TurboDiffusion checkpoint # --low_noise_model_path Path to the high noise TurboDiffusion checkpoint # --boundary Timestep boundary for switching from high to low noise model (default: 0.9) # --model Model to use: Wan2.2-A14B (default: Wan2.2-A14B) # --num_samples Number of videos to generate (default: 1) # --num_steps Sampling steps, 1–4 (default: 4) # --sigma_max Initial sigma for rCM (default: 200); larger choices (e.g., 1600) reduce diversity but may enhance quality # --vae_path Path to Wan2.2 VAE (default: checkpoints/Wan2.2_VAE.pth) # --text_encoder_path Path to umT5 text encoder (default: checkpoints/models_t5_umt5-xxl-enc-bf16.pth) # --num_frames Number of frames to generate (default: 81) # --prompt Text prompt for video generation # --resolution Output resolution: "480p" or "720p" (default: 720p) # --aspect_ratio Aspect ratio in W:H format (default: 16:9) # --adaptive_resolution Enable adaptive resolution based on input image size # --ode Use ODE for sampling (sharper but less robust than SDE) # --seed Random seed for reproducibility (default: 0) # --save_path Output file path including extension (default: output/generated_video.mp4) # --attention_type Attention module to use: original, sla or sagesla (default: sagesla) # --sla_topk Top-k ratio for SLA/SageSLA attention (default: 0.1), we recommend using 0.15 for better video quality # --quant_linear Enable quantization for linear layers, pass this if using a quantized checkpoint # --default_norm Use the original LayerNorm and RMSNorm of Wan models python turbodiffusion/inference/wan2.2_i2v_infer.py \ --model Wan2.2-A14B \ --low_noise_model_path checkpoints/TurboWan2.2-I2V-A14B-low-720P-quant.pth \ --high_noise_model_path checkpoints/TurboWan2.2-I2V-A14B-high-720P-quant.pth \ --resolution 720p \ --adaptive_resolution \ --image_path assets/i2v_inputs/i2v_input_0.jpg \ --prompt "POV selfie video, ultra-messy and extremely fast. A white cat in sunglasses stands on a surfboard with a neutral look when the board suddenly whips sideways, throwing cat and camera into the water; the frame dives sharply downward, swallowed by violent bursts of bubbles, spinning turbulence, and smeared water streaks as the camera sinks. Shadows thicken, pressure ripples distort the edges, and loose bubbles rush upward past the lens, showing the camera is still sinking. Then the cat kicks upward with explosive speed, dragging the view through churning bubbles and rapidly brightening water as sunlight floods back in; the camera races upward, water streaming off the lens, and finally breaks the surface in a sudden blast of light and spray, snapping back into a crooked, frantic selfie as the cat resurfaces." \ --num_samples 1 \ --num_steps 4 \ --quant_linear \ --attention_type sagesla \ --sla_topk 0.1 \ --ode&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Interactive inference via the terminal is available at &lt;code&gt;turbodiffusion/serve/&lt;/code&gt;. This allows multi-turn video generation without reloading the model.&lt;/p&gt;
    &lt;p&gt;We evaluate video generation on a single RTX 5090 GPU. The E2E Time refers to the end-to-end diffusion generation latency, excluding text encoding and VAE decoding.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 4549s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 38s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 4549s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 38s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 4549s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 38s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 4549s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 38s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 4549s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 38s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 4549s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 38s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 4549s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 38s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 184s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 5.3s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 1.9s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 184s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 5.3s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 1.9s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 184s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 5.3s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 1.9s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 184s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 5.3s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 1.9s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 184s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 5.3s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 1.9s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 184s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 5.3s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 1.9s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 184s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 5.3s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 1.9s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 184s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 5.3s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 1.9s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 4767s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 72.6s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 24s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 4767s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 72.6s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 24s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 4767s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 72.6s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 24s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 1676s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 26.3s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 9.9s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 1676s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 26.3s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 9.9s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 1676s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 26.3s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 9.9s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 1676s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 26.3s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 9.9s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In this repo, we provide training code based on Wan2.1 and its synthetic data. The training builds on the rCM codebase (https://github.com/NVlabs/rcm), with infrastructure support including FSDP2, Ulysses CP, and selective activation checkpointing (SAC). For rCM training instructions, please refer to the original rCM repository; SLA (Sparse-Linear Attention) training guidance is provided here.&lt;/p&gt;
    &lt;p&gt;For rCM/SLA training, additionally run:&lt;/p&gt;
    &lt;code&gt;pip install megatron-core hydra-core wandb webdataset
pip install --no-build-isolation transformer_engine[pytorch]&lt;/code&gt;
    &lt;p&gt;Download the Wan2.1 pretrained checkpoints in &lt;code&gt;.pth&lt;/code&gt; format and VAE/text encoder to &lt;code&gt;assets/checkpoints&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;# make sure git lfs is installed
git clone https://huggingface.co/worstcoder/Wan assets/checkpoints&lt;/code&gt;
    &lt;p&gt;FSDP2 relies on Distributed Checkpoint (DCP) for loading and saving checkpoints. Before training, convert &lt;code&gt;.pth&lt;/code&gt; teacher checkpoints to &lt;code&gt;.dcp&lt;/code&gt; first:&lt;/p&gt;
    &lt;code&gt;python -m torch.distributed.checkpoint.format_utils torch_to_dcp assets/checkpoints/Wan2.1-T2V-1.3B.pth assets/checkpoints/Wan2.1-T2V-1.3B.dcp&lt;/code&gt;
    &lt;p&gt;After training, the saved &lt;code&gt;.dcp&lt;/code&gt; checkpoints can be converted to &lt;code&gt;.pth&lt;/code&gt; using the script &lt;code&gt;scripts/dcp_to_pth.py&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We provide Wan2.1-14B-synthesized datasets. Download to &lt;code&gt;assets/datasets&lt;/code&gt; using:&lt;/p&gt;
    &lt;code&gt;# make sure git lfs is installed
git clone https://huggingface.co/datasets/worstcoder/Wan_datasets assets/datasets&lt;/code&gt;
    &lt;p&gt;We implement white-box SLA training by aligning the predictions of the SLA-enabled model with those of the full-attention pretrained model. Unlike black-box training in the original paper, which tunes the pretrained model using diffusion loss, white-box training mitigates distribution shift and is less sensitive to the training data.&lt;/p&gt;
    &lt;p&gt;Single-node training example:&lt;/p&gt;
    &lt;code&gt;WORKDIR="/your/path/to/turbodiffusion"
cd $WORKDIR
export PYTHONPATH=turbodiffusion

# the "IMAGINAIRE_OUTPUT_ROOT" environment variable is the path to save experiment output files
export IMAGINAIRE_OUTPUT_ROOT=${WORKDIR}/outputs
CHECKPOINT_ROOT=${WORKDIR}/assets/checkpoints
DATASET_ROOT=${WORKDIR}/assets/datasets/Wan2.1_14B_480p_16:9_Euler-step100_shift-3.0_cfg-5.0_seed-0_250K

# your Wandb information
export WANDB_API_KEY=xxx
export WANDB_ENTITY=xxx

registry=registry_sla
experiment=wan2pt1_1pt3B_res480p_t2v_SLA

torchrun --nproc_per_node=8 \
    -m scripts.train --config=rcm/configs/${registry}.py -- experiment=${experiment} \
        model.config.teacher_ckpt=${CHECKPOINT_ROOT}/Wan2.1-T2V-1.3B.dcp \
        model.config.tokenizer.vae_pth=${CHECKPOINT_ROOT}/Wan2.1_VAE.pth \
        model.config.text_encoder_path=${CHECKPOINT_ROOT}/models_t5_umt5-xxl-enc-bf16.pth \
        model.config.neg_embed_path=${CHECKPOINT_ROOT}/umT5_wan_negative_emb.pt \
        dataloader_train.tar_path_pattern=${DATASET_ROOT}/shard*.tar&lt;/code&gt;
    &lt;p&gt;Please refer to &lt;code&gt;turbodiffusion/rcm/configs/experiments/sla/wan2pt1_t2v.py&lt;/code&gt; for the 14B config or perform modifications as needed.&lt;/p&gt;
    &lt;p&gt;The parameter updates from SLA training can be merged into rCM checkpoints using &lt;code&gt;turbodiffusion/scripts/merge_models.py&lt;/code&gt;, enabling rCM to perform sparse attention inference. Specify &lt;code&gt;--base&lt;/code&gt; as the rCM model, &lt;code&gt;--diff_base&lt;/code&gt; as the pretrained model, and &lt;code&gt;--diff_target&lt;/code&gt; as the SLA-tuned model.&lt;/p&gt;
    &lt;p&gt;We thank the community effort Comfyui_turbodiffusion for integrating TurboDiffusion into ComfyUI.&lt;/p&gt;
    &lt;p&gt;We're actively working on the following features and improvements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Organize and release training code&lt;/item&gt;
      &lt;item&gt;Optimize infrastructure for better parallel&lt;/item&gt;
      &lt;item&gt;vLLM-Omni integration&lt;/item&gt;
      &lt;item&gt;Support for more video generation models&lt;/item&gt;
      &lt;item&gt;Support for autoregressive video generation models&lt;/item&gt;
      &lt;item&gt;More hardware-level operator optimizations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We welcome community members to help maintain and extend TurboDiffusion. Welcome to join the TurboDiffusion Team and contribute together!&lt;/p&gt;
    &lt;p&gt;If you use this code or find our work valuable, please cite:&lt;/p&gt;
    &lt;code&gt;@article{zhang2025turbodiffusion,
  title={TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times},
  author={Zhang, Jintao and Zheng, Kaiwen and Jiang, Kai and Wang, Haoxu and Stoica, Ion and Gonzalez, Joseph E and Chen, Jianfei and Zhu, Jun},
  journal={arXiv preprint arXiv:2512.16093},
  year={2025}
}

@software{turbodiffusion2025,
  title={TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times},
  author={The TurboDiffusion Team},
  url={https://github.com/thu-ml/TurboDiffusion},
  year={2025}
}

@inproceedings{zhang2025sageattention,
  title={SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration}, 
  author={Zhang, Jintao and Wei, Jia and Zhang, Pengle and Zhu, Jun and Chen, Jianfei},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2025}
}

@article{zhang2025sla,
  title={SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention},
  author={Zhang, Jintao and Wang, Haoxu and Jiang, Kai and Yang, Shuo and Zheng, Kaiwen and Xi, Haocheng and Wang, Ziteng and Zhu, Hongzhou and Zhao, Min and Stoica, Ion and others},
  journal={arXiv preprint arXiv:2509.24006},
  year={2025}
}

@article{zheng2025rcm,
  title={Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency},
  author={Zheng, Kaiwen and Wang, Yuji and Ma, Qianli and Chen, Huayu and Zhang, Jintao and Balaji, Yogesh and Chen, Jianfei and Liu, Ming-Yu and Zhu, Jun and Zhang, Qinsheng},
  journal={arXiv preprint arXiv:2510.08431},
  year={2025}
}

@inproceedings{zhang2024sageattention2,
  title={Sageattention2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization},
  author={Zhang, Jintao and Huang, Haofeng and Zhang, Pengle and Wei, Jia and Zhu, Jun and Chen, Jianfei},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2025}
}
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46388907</guid><pubDate>Fri, 26 Dec 2025 03:19:49 +0000</pubDate></item><item><title>Building an AI agent inside a 7-year-old Rails monolith</title><link>https://catalinionescu.dev/ai-agent/building-ai-agent-part-1/</link><description>&lt;doc fingerprint="6c1bd1132f25fa62"&gt;
  &lt;main&gt;
    &lt;p&gt;I (incorrectly) convinced myself over the last few months that there’s no low-hanging fruit that would work for our product and business. This is a story of just how wrong I was.&lt;/p&gt;
    &lt;p&gt;I was at SF Ruby, in San Francisco, a few weeks ago. Most of the tracks were, of course, heavily focused on AI. Lots of stories from people building AIs into all sorts of products using Ruby and Rails,&lt;/p&gt;
    &lt;p&gt;They were good talks. But most of them assumed a kind of software I don’t work on — systems without strong boundaries, without multi-tenant concerns, without deeply embedded authorization rules.&lt;/p&gt;
    &lt;p&gt;I kept thinking: this is interesting, but it doesn’t map cleanly to my world. At Mon Ami, we can’t just release a pilot unless it passes strict data access checks.&lt;/p&gt;
    &lt;p&gt;Then I saw a talk about using the RubyLLM gem to build a RAG-like system. The conversation (LLM calls) context was augmented using function calls (tools). This is when it clicked. I could encode my complicated access logic into a specific function call and ensure the LLM gets access to some of our data without having to give it unrestricted access.&lt;/p&gt;
    &lt;p&gt;RubyLLM is a neat gem that abstracts away the interaction with many LLM providers with a clean API.&lt;/p&gt;
    &lt;code&gt;gem "ruby_llm"&lt;/code&gt;
    &lt;p&gt;It is configured in an initializer with the API keys for the providers you want to use.&lt;/p&gt;
    &lt;code&gt;RubyLLM.configure do |config|
  config.openai_api_key = Rails.application.credentials.dig(:openai_api_key)
  config.anthropic_api_key = Rails.application.credentials.dig(:anthropic_api_key)
  # config.default_model = "gpt-4.1-nano"

  # Use the new association-based acts_as API (recommended)
  config.use_new_acts_as = true

  # Increase timeout for slow API responses
  config.request_timeout = 600  # 10 minutes (default is 300)
  config.max_retries = 3        # Retry failed requests
end

# Load LLM tools from main app
Dir[Rails.root.join('app/tools/**/*.rb')].each { |f| require f }&lt;/code&gt;
    &lt;p&gt;It provides a Conversation model as an abstraction for an LLM thread. The Conversation contains a set of Messages. It also provides a way of defining structured responses and function calls available.&lt;/p&gt;
    &lt;code&gt;AVAILABLE_TOOLS = [
  Tools::Client::SearchTool
].freeze

conversation = Conversation.find(conversation_id)
chat = conversation.with_tools(*AVAILABLE_TOOLS)

chat.ask 'What is the phone number for John Snow?'&lt;/code&gt;
    &lt;p&gt;A Conversation is initialized by passing a model (gpt-5, claude-sonnet-4.5, etc) and has a method for chatting to it.&lt;/p&gt;
    &lt;code&gt;conversation = Conversation.new(model: RubyLLM::Model.find_by(model_id: 'gpt-4o-mini'))&lt;/code&gt;
    &lt;p&gt;RubyLLM comes with a neat DSL for defining accepted parameters (the descriptions are passed to the LLM as context since it needs to decide if the tool should be used based on the conversation). The tool implements an execute method returning a hash. The hash is then presented to the LLM. This is all the magic needed.&lt;/p&gt;
    &lt;code&gt;class SearchTool &amp;lt; BaseTool
  description 'Search for clients by name, ID, or email address. Returns matching clients.'

  param :query,
    desc: 'Search query - can be client name, ID, or email address',
    type: :string

  def execute(query:)
  end
end&lt;/code&gt;
    &lt;p&gt;We’ll now build a modest function call and a messaging interface. The function call allows searching a client using Algolia and ensuring the resulting set is visible to the user (by merging in the pundit policy).&lt;/p&gt;
    &lt;code&gt;def execute(query:)
  response = Algolia::SearchClient
    .create(app_id, search_key)
    .search_single_index(Client.index_name, {
      query: query.truncate(250)
    })

  ids = response.hits.map { |hit| hit[:id] }.compact

  base_scope = Client.where(id: ids)
  client = Admin::Org::ClientPolicy::Scope.new(base_scope).resolve.first or return {}

  {
    id: client.id,
    ami_id: client.slug,
    slug: client.slug,
    name: client.full_name,
    email: client.email
  }
end&lt;/code&gt;
    &lt;p&gt;The LLM acts as the magic glue between the natural language input submitted by the user, decides which (if any) tool to use to augment the context, and then responds to the user. No model should ever know Jon Snow’s phone number from a SaaS service, but this approach allows this sort of retrieval.&lt;/p&gt;
    &lt;p&gt;The UI is built with a remote form that enqueues an Active Job.&lt;/p&gt;
    &lt;code&gt;= turbo_stream_from @conversation, :messages

.container-fluid.h-100.d-flex.flex-column
  .sticky-top
    %h2.mb-0
      Conversation ##{@conversation.id}

  .flex-grow-1
    = render @messages

  .p-3.border-top.bg-white.sticky-bottom#message-form
  = form_with url: path, method: :post, local: false, data: { turbo_stream: true } do |f|
    = f.text_area :content
    = f.submit 'Send'&lt;/code&gt;
    &lt;p&gt;The job will process the Message.&lt;/p&gt;
    &lt;code&gt;class ProcessMessageJob &amp;lt; ApplicationJob
  queue_as :default

  def perform(conversation_id, message)
    conversation = Conversation.find(conversation_id)
    conversation.ask message
  end
end&lt;/code&gt;
    &lt;p&gt;The conversation has broadcast refresh enabled to update the UI when the response is received.&lt;/p&gt;
    &lt;code&gt;class Conversation &amp;lt; RubyLLM::Conversation
  broadcasts_refreshes
end&lt;/code&gt;
    &lt;p&gt;The form has a stimulus controller that checks for new messages being appended in order to scroll to the end of the conversation.&lt;/p&gt;
    &lt;p&gt;I checked a few OpenAI models for this implementation: gpt-5, gpt-4o, gpt4. GPT-5 has a big context, meaning we could have long-running conversations, but because there are a number of round-trips, the delay to queries requiring 3+ consecutive tools made the Agent feel sluggish.&lt;/p&gt;
    &lt;p&gt;GPT-4, on the other hand, is interestingly very prone to hallucinations - rushing to respond to queries with made-up data instead of calling the necessary tools. GPT-4o strikes, so far, the best balance between speed and correctness.&lt;/p&gt;
    &lt;p&gt;Building this tool took probably about 2-3 days of Claude-powered development (AIs building AIs). The difficulty and the complexity of building such a tool were the things that surprised me the most. The tool service object is essentially an API controller action - pass inputs and get a JSON back. Interestingly.&lt;/p&gt;
    &lt;p&gt;Before building this Agent, I looked at the other gems in this space. ActiveAgent (a somewhat similar gem for interacting with LLMs) is a decent contender that moves the prompts to a view file. It didn’t fit my needs since it had no built-in support for defining tools or having long-running conversations.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46390055</guid><pubDate>Fri, 26 Dec 2025 07:35:15 +0000</pubDate></item><item><title>Geometric Algorithms for Translucency Sorting in Minecraft [pdf]</title><link>https://douira.dev/assets/document/douira-master-thesis.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46390667</guid><pubDate>Fri, 26 Dec 2025 09:43:23 +0000</pubDate></item><item><title>Calibre adds AI "discussion" feature</title><link>https://lwn.net/Articles/1049886/</link><description>&lt;doc fingerprint="bc03181fb8a244f8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Calibre adds AI "discussion" feature&lt;/head&gt;
    &lt;p&gt;Version 8.16.0 of the calibre ebook-management software, released on December 4, includes a "Discuss with AI" feature that can be used to query various AI/LLM services or local models about books, and ask for recommendations on what to read next. The feature has sparked discussion among human users of calibre as well, and more than a few are upset about the intrusion of AI into the software. After much pushback, it looks as though users will get the ability to hide the feature from calibre's user interface, but LLM-driven features are here to stay and more will likely be added over time.&lt;/p&gt;
    &lt;p&gt;Amir Tehrani proposed adding an LLM query feature directly to calibre in August 2025:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I have developed and tested a new feature that integrates Google's Gemini API (which can be abstracted to any compatible LLM) directly into the Calibre E-book Viewer. My aim is to empower users with in-context AI tools, removing the need to leave the reading environment. The results: capability of instant text summarization, clarification of complex topics, grammar correction, translation, and more, enhancing the reading and research experience.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Kovid Goyal, creator and maintainer of calibre, quickly voiced approval. He dismissed the idea that it might bother some calibre users and suggested that Tehrani submit a pull request for the feature. On August 10, Tehrani submitted the patches, and Goyal later merged them into mainline after refactoring the code. He provided a description of the additional LLM features he had in mind as well:&lt;/p&gt;
    &lt;quote&gt;There are likely going to be new APIs added to all backends to support things like generating covers, finding what to read next, TTS [text-to-speech], grammar and style fixing in the editor and possibly metadata download.&lt;/quote&gt;
    &lt;p&gt;Goyal did promise that calibre would "&lt;quote&gt;never ever use any third party service without explicit opt-in&lt;/quote&gt;".&lt;/p&gt;
    &lt;head rend="h4"&gt;Discuss removing the feature&lt;/head&gt;
    &lt;p&gt;It did not take long after the Discuss feature was released for users to start asking for its removal. User "msr" on the Mobileread forum started a thread to ask if there was a way to block or hide all AI features:&lt;/p&gt;
    &lt;quote&gt;I generally find the AI-push to be morally repugnant (among other things, I am an author whose work has been stolen for training) and I hate to see these features creep into software I use. I have zero interest in ever using so-called AI for anything.&lt;/quote&gt;
    &lt;p&gt;Goyal replied that the features do nothing unless they are enabled. "&lt;quote&gt;The worst you get is a few menu entries. Simply ignore them.&lt;/quote&gt;" &lt;/p&gt;
    &lt;p&gt;Other users echoed the anti-AI sentiment. "Quoth" said they would not update calibre until the feature was scrapped. "&lt;quote&gt;It's a thin end of a wedge and encouraging people to use these over-hyped LLMs, even though off by default.&lt;/quote&gt;" Goyal replied that it is in calibre to stay:&lt;/p&gt;
    &lt;quote&gt;It's not going to be scrapped, so good bye, I guess. You are more than welcome to not use AI if you don't want to. calibre very nicely makes that easy for you by having it off by default to the extent that the AI code is not even loaded unless you enable it. What you DO NOT get to do is try to make that choice for other people.&lt;/quote&gt;
    &lt;head rend="h4"&gt;What's added so far&lt;/head&gt;
    &lt;p&gt;The feature is displayed in the calibre user interface by default; it shows up in the View menu as "Discuss selected books with AI". The naming is unfortunate on its own. Calling the process of sending queries to an LLM provider a discussion encourages people to anthropomorphize the tools and furthers the misconception that these tools "think" in the way that people do. Whatever value the responses may have, they do not reflect actual thought.&lt;/p&gt;
    &lt;p&gt;As Goyal pointed out, though, the Discuss feature does not work until an LLM provider is configured. If a user attempts to use it without doing so, calibre displays a dialog that directs the user to configure a provider first. Each provider is supplied as a separate plugin. Currently, calibre users have a choice of commercial providers, or running models locally using LM Studio or Ollama.&lt;/p&gt;
    &lt;p&gt;The Discuss feature shows up as a plugin as well. It is located in the calibre preferences in the "User interface action" category. However, it is a plugin that cannot be disabled or removed; nor can any of the other alleged plugins in that category. It seems fair to question whether something is actually a "plugin" if it cannot be unplugged. The separate provider plugins, in the "AI provider" category, can be disabled or removed, though. The provider plugins are enabled by default, but they do nothing until a user supplies credentials of some kind.&lt;/p&gt;
    &lt;p&gt;Users do not need to worry about accidentally enabling a feature that sends data off to a provider, because it is impossible to accidentally configure the plugins. For example, the GitHub AI provider requires an access token before it will work, and Google's AI provider needs an API key to function. Using a local provider requires the user to actually have LM Studio or Ollama set up, and then jump through a couple of hoops to enable them.&lt;/p&gt;
    &lt;quote&gt;$ sudo subscribe today&lt;p&gt;Subscribe today and elevate your LWN privileges. You’ll have access to all of LWN’s high-quality articles as soon as they’re published, and help support LWN in the process. Act now and you can start with a free trial subscription.&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;Even if a user wants to query an LLM about a book, they may encounter problems. I tried setting calibre up to use GitHub AI, but even after appearing to have successfully configured it as provider with the token, I had no luck. I could send queries, but received no reply. I was able to get calibre working with Ollama, though the experience was not particularly compelling.&lt;/p&gt;
    &lt;p&gt;Responses from GitHub AI or Ollama about books are of little interest to me; a model may have ingested a million or more books as it was trained, but it hasn't read a single one, nor had any life experience that could spark an insight or reaction. Thoughtful discussions of books with well-read people with real perspectives, on the other hand, would be delightful—but beyond calibre's capabilities to provide.&lt;/p&gt;
    &lt;head rend="h4"&gt;Hide AI&lt;/head&gt;
    &lt;p&gt;Despite dismissing complaints the addition of AI, Goyal has grudgingly accepted a pull request to hide AI features. He said that anyone offended by a few menu entries is not worth worrying about but, "&lt;quote&gt;I don't particularly mind having a tweak just to hide the menu entries, but that is all it should do&lt;/quote&gt;". He added that someone would need to supply patches to hide additional AI functionality in the future. "&lt;quote&gt;That someone isn't going to be me as I don't have the patience to waste my time catering to insanity.&lt;/quote&gt;"&lt;/p&gt;
    &lt;p&gt;A "remove slop" pull request from "Ember-ruby" that would have stripped out AI features from calibre was rejected without comment. The calibre forked repository with those patches may be of interest, however, to those interested in forking calibre.&lt;/p&gt;
    &lt;p&gt;At least two forks have been announced so far; one seems to have only gotten so far as the name, clbre "&lt;quote&gt;because the AI is stripped out&lt;/quote&gt;". To date the only work that has shown up in that repository is to update the README. Xandra Granade announced rereading on December 9; that project is currently working on a fork called arcalibre, but its goals are limited to a snapshot of calibre "&lt;quote&gt;with all AI antifeatures removed&lt;/quote&gt;" that can be used for future forks of calibre. No new features are planned for arcalibre.&lt;/p&gt;
    &lt;p&gt;The rereading draft charter suggests that the project will develop additional applications based on arcalibre. It is, of course, far too early to say whether the project will produce anything interesting in the long term. Any future forkers should note that the name "Excalibre" is right there for the taking.&lt;/p&gt;
    &lt;head rend="h4"&gt;Resistance seems futile&lt;/head&gt;
    &lt;p&gt;No doubt part of calibre's audience is pleased to see the feature; but it has proven to be an unwelcome addition for some of calibre's users. It is not surprising that those users have asked for it to be removed or changed in such a way that it can be hidden.&lt;/p&gt;
    &lt;p&gt;It has been a disappointing year overall for Linux and open-source enthusiasts who object to the seemingly relentless AI-ification of everything. It is fairly commonplace at this point to see companies shoving AI features into proprietary software whether the features actually make sense or not. However, an open-source project like calibre has no shareholders to please by ticking off the "AI inside" box, so few people would have had "adds AI" to their calibre bingo card for 2025.&lt;/p&gt;
    &lt;p&gt;An AI feature landing in calibre seems a fitting coda to the recurrent theme of AI and open source in 2025; whether users want to engage with AI or not, it is seemingly inescapable. One might wonder if AI has come to calibre, a project with no commercial incentive to add it, is there is no refuge to be had from it at all?&lt;/p&gt;
    &lt;p&gt;Bitwarden, which makes an open-source password manager and server, is now accepting AI-generated contributions, as is the KeePassXC password-manager project. Even projects like Fedora and the Linux kernel are accepting or leaning toward accepting LLM-assisted contributions; Mozilla is all-in on AI and pushing it into Firefox as well. This is not an exhaustive list of AI-friendly projects, of course; it would be exhausting to try to compile one at this point.&lt;/p&gt;
    &lt;p&gt;In most cases, though, users still have options without LLM features. When it comes to calibre, there is no alternative to turn to. Then again, there was no real alternative to calibre before it adopted "Discuss with AI", either. There are many open-source programs that handle reading ebooks; that is well-covered territory. Some, like Foliate, are arguably better than calibre at that task.&lt;/p&gt;
    &lt;p&gt;But there is no other ebook-management software (open source or otherwise) that has all of calibre's conversion features and support for exporting to such a wide variety of ebook readers. Evan Buss attempted a calibre alternative, called 22, in 2019. Buss threw in the towel after learning "&lt;quote&gt;ebook managers are much more difficult to get right than I had previously imagined&lt;/quote&gt;", and maintaining compatibility with calibre "&lt;quote&gt;proved near impossible&lt;/quote&gt;". Phil Denhoff started the Citadel project in late 2023. It looked like a promising calibre-compatible ebook-library manager, but its last release was in October 2024. Denhoff continues to make commits to the repository, though, so one might still hold out hope for the project.&lt;/p&gt;
    &lt;p&gt;While the lack of alternatives is frustrating for some, it is not Goyal's fault. The fact that the open-source community, to date, has not produced anything else that can fill in for calibre is not his problem. It is not his responsibility to take the program in any particular direction, nor is he obliged to entertain user complaints. Whether users love or loathe seeing calibre adding LLM features, it's up to its maintainer to decide what gets in and what doesn't.&lt;/p&gt;
    &lt;p&gt;For now, the AI-objectors on Linux have a few options. One is to live with lurking LLM features, or stick with calibre versions before 8.16.0. Goyal has made it easy to revert to an older version; the download.calibre.com site seems to have all prior releases of calibre going back to its pre-1.0 days. The Download for Linux page has instructions on reverting to previous versions, too. Those who get calibre from their Linux distribution may be LLM-free for some time without taking any action. Debian 13 ("trixie") users, for example, should be on the 8.5.0 branch for the remainder of the release's lifetime. Fedora 42 is still on the 8.0 branch, and Fedora 43 is on 8.14. Rawhide has 8.16.2, though, so users are likely to get the Discuss feature in Fedora 44.&lt;/p&gt;
    &lt;p&gt;The strong reaction against calibre's Discuss feature may seem more emotional than logical. It is also understandable. Books are a human endeavor, even those that are in electronic format. AI models have often been trained by plundering a corpus of books, without respect for the author's wishes or copyright. Suggesting that the readers now turn to the technologies that seek to replace humans to supplement their reading experience is, for some at least, deeply offensive. It is a little puzzling that Goyal, who has catered to a large audience of book lovers for nearly 20 years, seems not to understand that.&lt;/p&gt;
    &lt;p&gt; Posted Dec 15, 2025 19:57 UTC (Mon) by Wol (subscriber, #4433) [Link] (4 responses) As an AI hater myself, I agree with that sentiment, and I'm delighted to see that they've limited it to a couple of menu entries that are easily ignored. What I HATE is tools that push AI at you, the pop-ups are there every time you open the tool, etc etc. I'm an ANALYST for heavens sake, my job is to DIG INTO THE DETAIL, why on earth do I need Google Sheets taking every opportunity to say "do you want a summary of what you're looking at?". I know people have said I'm using a crap AI, but why do "they" think pushing it at me (with offers I find a *nuisance*!) is going to change my opinion? Cheers, Posted Dec 16, 2025 1:37 UTC (Tue) by gerdesj (subscriber, #5446) [Link] (3 responses) I doubt you actual are an AI hater per se. I suspect we could agree that AI is the wrong term for an LLM, for starters. I might also insinuate that you are a fan of appropriate use of technology. So, once we dispense with the hyperbole and get down to brass tacks, what have we got? You mention Google Sheets. I would dump that if you don't want other things thrown at you. Either go self hosted: Libre Office, Excel or whatever. Excel will wander off for a fag behind the bike sheds with CoPilot, I'm afraid. We can whine and whinge about change or embrace it or work around it or whatever. I had you pegged as an engineer so please behave like one! I think (let's dump the AI moniker for starters) that LLMs are a tool. A panel saw will rip your thumb off if you leave it across the pencil line you carefully scribed. You allow for the thickness of the saw and your perception and even the thickness of your pencil - craftmanship is hard! Remember the first time you learned how to spell BOOBS on a calculator? Remember stirring your tea with the slide on a slide rule? Oh and thinking how cool your mental maths is, which it was. A LLM is not intelligent but I think you do need to get to grips with them. You can spin up a small model on a 16GB Nvidia and scare yourself silly with what comes across as an encyclopedia with fairly decent thinking powers. Tools mate, for the use of. Posted Dec 16, 2025 10:23 UTC (Tue) by Wol (subscriber, #4433) [Link] (2 responses) Work is dumping Excel for Sheets. Good decision, bad decision, I don't know. Using a spreadsheet when you need a database is certainly a dumb decision ... From choice (aka my home laptop) I run linux. Work, family support I don't get that choice :-( And I'm afraid I'm a great believer in the power of a good vent. I'm turning into a grumpy old man with a disabled wife, and it keeps me sane :-) I hate people who don't listen, and AI/LLM ime fall firmly into that category. At the moment, I can't see any need for me to use that technology, which is why I find the constant pushing extremely annoying (and why I'm so pleased to see Calibre being sensible, it's a great breath of fresh air!) I'm not QUITE old enough to have used a slide rule in anger, but I remember playing with my Dad's :-) And no, actually, Engineer is never a word I would have used to describe myself with :-) Natural Philosopher, Polymath, Chemist, sort-of-medic maybe. At work it's always been some sort of Analyst Programmer, except now it's Business Analyst (where my programming skills are heavily used). Cheers, Posted Dec 21, 2025 17:44 UTC (Sun) by nelzas (subscriber, #4427) [Link] You wrote "people who don't listen, and AI/LLM ime fall firmly into that category." :-) Posted Dec 25, 2025 10:40 UTC (Thu) by davidgerard (guest, #100304) [Link] What they both do superlatively is office sharing and collaboration. Also, you know that the number one use of Sheets is as a one-table flat file database. At BMJ we had Google Apps everywhere and MS Office for the specific people who had a need for a proper spreadsheet. Posted Dec 15, 2025 20:29 UTC (Mon) by dvdeug (subscriber, #10998) [Link] (16 responses) People have been "conversing" and even "discussing" with Eliza for 50 years now. Besides which, what do you call a process where you engage in a series of sending a message, and receiving a message back on that topic in the context? The tools don't "think" the way that we do, but trying to draw this line around "discussion" seems to be making an arbitrary division. Posted Dec 15, 2025 20:55 UTC (Mon) by dskoll (subscriber, #1630) [Link] (2 responses) People have been "conversing" and even "discussing" with Eliza for 50 years now Really? My experiences with Eliza were that after about 60 seconds, the novelty wore off and it was transparently obvious that you were not really conversing with anyone or anything. ChatGPT, last time I used it, was certainly more realistic, though also obviously non-human and in fact quite irritating to interact with. It came across as a slippery politician, unwilling to give a straight answer to difficult questions. Posted Dec 15, 2025 21:23 UTC (Mon) by dvdeug (subscriber, #10998) [Link] (1 responses) Posted Dec 16, 2025 2:12 UTC (Tue) by interalia (subscriber, #26615) [Link] I don't think LLMs can do that (feel free to correct me), so it seems much closer to a query-response model with natural language queries. It seems a better description with more apt connotations about the experience. Posted Dec 15, 2025 21:43 UTC (Mon) by jzb (editor, #7867) [Link] (12 responses) I'm pretty sure you could find complaints about the use of the terms "conversing" with ELIZA as well going back almost as far as ELIZA. I'm not the first person to be pedantic about language, after all. :-) But I also suspect it seemed less important to draw a distinction there—ELIZA was never being adopted or pushed the way that LLM-driven tools are. There was little danger that people would take it that seriously or that it could even go too deep into a topic that users would mistake it for real intelligence on any scale. I'd describe it as querying, just as one would query a database. Granted, the syntax is much more approachable than (say) SQL, and what is returned is not data from a table but the result of a model, but I'd suggest that "querying" is more accurate than "discussing" with something that has no consciousness. Perhaps it seems arbitrary, but the words we use for things shape the way we think about them. So I'd suggest that we not use terms that anthropomorphize the technology or over-estimate its ability. But, that's all my viewpoint: it's not like I can stop anybody from calling it "discussion," I can only make the appeal/argument and hope it gets some traction. Posted Dec 16, 2025 3:51 UTC (Tue) by dvdeug (subscriber, #10998) [Link] (11 responses) SELECT title FROM movies WHERE director = "Rob Reiner"; and I notice that a certain movie isn't there, I can't nudge SQL. I can nudge ChatGPT and it might say "Yes, I did miss Alex &amp;amp; Emmy as a movie he directed." or "Rob Reiner was an actor in Throw Momma from the Train, but it was directed by Danny DeVito." or "While Some Kind of Wonderful does have similarities to some movies directed by Rob Reiner, like The Sure Thing, it was directed by Howard Deutch." &amp;gt; So I'd suggest that we not use terms that anthropomorphize the technology or over-estimate its ability. Why is query more acceptable than discuss? A query and response was, pre-computer, exclusively a human activity, and even now when we have a complex question, we ask a person or an LLM. The problem as I see is that what LLMs do was exclusively the province of humans before, so we're going to be using human terms for it. I get the feeling that sometimes the desire to not anthropomorphize them is instead a desire to downplay their power or significance. It's like when Deep Blue beat Kasparov at chess; LLMs are a movement into an area that humans thought was theirs. Are they human? No. Are they able to write a competent essay on basically any subject under the sun? Yes. Can they answer general questions better than just about any one human? Yes. Their problems are not really discussion; their big problem is confabulation. If you think in terms of a query like SQL or Prolog, you're going to get misled, and you won't make best use of it. If you think in terms of a discussion, where you have to respond and push back against certain things to get the best results, LLMs work better. Posted Dec 16, 2025 9:03 UTC (Tue) by taladar (subscriber, #68407) [Link] Posted Dec 16, 2025 12:38 UTC (Tue) by ptime (subscriber, #168171) [Link] (9 responses) Posted Dec 17, 2025 4:27 UTC (Wed) by dvdeug (subscriber, #10998) [Link] (8 responses) Posted Dec 17, 2025 8:37 UTC (Wed) by taladar (subscriber, #68407) [Link] (1 responses) Posted Dec 17, 2025 11:32 UTC (Wed) by dvdeug (subscriber, #10998) [Link] For one, LLMs have been advancing quickly; a lot of my experience is with ChatGPT 5.x over the past few weeks. For another, a lot of the coding claims made for them are simple nonsense, but using ChatGPT 5.x, I've had success writing small functions and programs to transform data formats. Other people have commended them for quickly writing test code at the function and module level. For non-coding applications, a Debian upgrade told me vdpau was disabled on MPlayer, so I had it explain what vdpau was, how it might affect my hardware, and ended figuring out how to turn on hardware-accelerated video on my system. No, it wasn't perfect on all the details of the output of mpv, but it stepped me through enough of it. I've had problems with it; I spent some time trying to find a German Christmas poem that it apparently made up, for example. But while I've had to pick out a few errors, it's been useful enough that I'd go back to it. Posted Dec 17, 2025 13:37 UTC (Wed) by ptime (subscriber, #168171) [Link] (5 responses) Posted Dec 17, 2025 14:43 UTC (Wed) by dvdeug (subscriber, #10998) [Link] (4 responses) In this case, why ask the question of what Rob Reiner directed if you're just going to ignore the result? You can force it to tell you that Rob Reiner directed Titanic, but that won't make it true. Posted Dec 18, 2025 1:15 UTC (Thu) by ptime (subscriber, #168171) [Link] (2 responses) Posted Dec 18, 2025 9:53 UTC (Thu) by dvdeug (subscriber, #10998) [Link] (1 responses) That's unusual. Much of the early development on computers was to produce answers that humans would have difficulty producing, sometimes even being impossible to produce, and that continues to be a driving force today, things like huge physics simulations or summing every sale a store did last year and producing the correct total. It seems like a statement that advances an argument more than it reflects reality. &amp;gt; I wouldn’t ask the question if I already know the answer, which is presupposed by your “but you can argue with an LLM until it maybe agrees with you” scenario. I said It's literally about not knowing an answer. If you have a good database, then yes, look it up. It's not the greatest of examples. I use questions like this all the time with humans; I have a different understanding or impression, and I'd like to know if I'm wrong or if you can enlighten me about aspects I might not have thought about. An LLM can provide answers as to why you're right or wrong, about why you may have thought Some Kind of Wonderful was directed by Rob Reiner. Posted Dec 18, 2025 10:27 UTC (Thu) by Wol (subscriber, #4433) [Link] That's the difference between "trivial" and "easy", as evidenced when they announced the proof of Fermat's Last Theorem, and called it "trivial". As far as I can tell (not having a clue how to understand the maths) it was. Easy is adding up a column of ten numbers. Trivial is adding up a column of ten thousand numbers. "Trivial" is easy to specify and easy to do. The fact that "to err is human" and trivial problems are usually so large as to invite that, isn't the computer's problem :-) Cheers, Posted Dec 18, 2025 10:10 UTC (Thu) by anselm (subscriber, #2796) [Link] Which is just too bad. It might have been a much more entertaining movie … Posted Dec 15, 2025 22:47 UTC (Mon) by hailfinger (subscriber, #76962) [Link] (5 responses) &amp;gt; I dont have the patience to waste my time catering to insanity. Wow. This is offensive both to people suffering from mental illness and to people wanting to remove the AI features. Posted Dec 15, 2025 23:11 UTC (Mon) by dsommers (subscriber, #55274) [Link] I wish there were better alternatives than calibre, I use it to handle format conversions, DeDRM and managing my ebook readers. Calibre does so okay. But the user interface and user experience in the application is mediocre by today's standards. The attitude Goyal has shown here makes the user experience even worse. Posted Dec 16, 2025 9:22 UTC (Tue) by epilys (subscriber, #153643) [Link] - Kovid Goyal Posted Dec 18, 2025 3:04 UTC (Thu) by milesrout (subscriber, #126894) [Link] Posted Dec 23, 2025 15:38 UTC (Tue) by marcH (subscriber, #57642) [Link] (1 responses) &amp;gt; Wow. This is offensive both to people suffering from mental illness and to people wanting to remove the AI features. He didn't say healthcare is a waste of time; this was not offensive to "both". He said that healthcare _performed by the Calibre maintainer in a bug tracker_ is a waste of time. I think we can easily agree this is not a good place for healthcare. So this was (very) offensive only to whom he was answering to. That's plenty enough. Sorry for picking up on this particular example but I'm tired of this sort of failed attempts to over-analyze every minute detail of what "bad" people do or say. This totally backfires and is really one of the ways the left keeps losing elections all across the world. In just 5 minutes searching I found plenty enough, actually and obviously offensive things written by that maintainer. Keep it short, quote only the worst stuff and stop there. Don't share your personal interpretation and don't drown it all in irrelevant details. Don't divine what you want to find in every word of his and don't help make him a victim/hero harassed by people obsessed by everything he says. You may also want to be more selective with social and news media because sadly most of it has become exactly that - on _all_ sides. Less is more. &amp;gt; Now I know what Kovid Goyal thinks of other people. Without knowing him and after just over-interpreting a few sentences he wrote without thinking when he was upset? Impressive psychology skills :-( Posted Dec 23, 2025 16:53 UTC (Tue) by Wol (subscriber, #4433) [Link] The plague of professional offendees :-( There's enough to be seriously offended by, without making stuff up. And, given the clash of languages, as you know it really doesn't help when speakers of one language get seriously offended by foreigners who are only using their own language in a totally non-offensive way ... Hint - American and English are two separate languages, and NEITHER are the British National Language. Cheers, Posted Dec 15, 2025 23:22 UTC (Mon) by geoffhill (subscriber, #92577) [Link] (30 responses) On a curiosity note: I wonder how many of us there are on the other side, who would love to plug a Gemini API key into GNOME desktop (or run an local LLM) and have all my GTK apps get superpowers. It can't just be a tiny minority of us? Open source and local models seem like an especially great match. Posted Dec 15, 2025 23:53 UTC (Mon) by dskoll (subscriber, #1630) [Link] (22 responses) ... strong ethical quandaries and convictions on both sides ... What ethical quandary does choosing not to use AI put one in? Posted Dec 16, 2025 0:00 UTC (Tue) by geoffhill (subscriber, #92577) [Link] The ethical issues are perpetuated by one side, or zero sides, depending on who you ask. Posted Dec 16, 2025 12:58 UTC (Tue) by farnz (subscriber, #17727) [Link] (20 responses) This one has parallels to a pre-existing ethical quandary in medicine, where there's a number of things known about human physiology as a result of war crime experiments on humans; do you use that knowledge for good, or forsake that knowledge because of how it was discovered? And there's branches off this one - if you refuse to use AI yourself, would you still use something created with AI? What if that thing is a cure for a terminal illness, rather than an entertainment product - does that change your position? But, it's a relatively weak quandary - it doesn't rise to the level of "is the way training data is gathered today OK?", let alone any of the stronger quandaries on the "choosing to use AI" side. Posted Dec 16, 2025 15:20 UTC (Tue) by dskoll (subscriber, #1630) [Link] (15 responses) what if, by refusing to use AI, I prevent someone from discovering something of great value that would have been discovered with AI assistance? Sure, but much more likely IMO is: "What if, by relying on AI, I stunt my learning process so that some great discovery I might have made on my own never happens?" What if that thing is a cure for a terminal illness, rather than an entertainment product - does that change your position? Yes, absolutely. I am not dead-set against AI. Machine-vision tools to suss out manufacturing defects are great. So is anything that can come up with novel drugs or medical advances. What I am against are GPTs that are being touted as being able to replace humans or act as agents. Mostly, I am appalled by the business model and level of hype surrounding GPTs and the fact that if they ever become financially viable (very doubtful at this point) only a few oligarchs will benefit to the detriment of everyone else. Posted Dec 16, 2025 15:35 UTC (Tue) by jzb (editor, #7867) [Link] (12 responses) I am not dead-set against AI. Nor am I; there are definitely applications for some of the technologies lumped in under the AI umbrella. Machine learning tools have their place. My complaints are largely with generative AI tools and some of the practices that surround them (e.g., rampant scraping, pushing genAI at people whether they want to engage with it or not). I don't, for example, care for AI-generated images or writing. I dabbled with AI tools early on for image generation, but have decided that I prefer art by humans. (Half the fun, for me anyway, was when the GenAI tools were producing truly deranged output that was clearly computer-generated and it was entertaining to see how it got things wrong. Ask for a unicorn and get a five-legged, three-eyed monstrosity that could cause nightmares...) It's unfortunate that the distinction often gets lost. If machine-learning tools can, for example, do better than people at detecting early stages of cancer in medical scans/tests, I'm all for that. (More probable, I think, is talented medical professionals plus machine learning may do better.) Posted Dec 16, 2025 15:52 UTC (Tue) by Wol (subscriber, #4433) [Link] There was the GP who wrote his own system - in Prolog on something like an Apple II iirc - and he said patients loved interacting with it - they found it easy to be honest and didn't feel threatened. Then when they got to the actual appointment, he had a computer generated diagnosis that would come up with (maybe several) possibilities, and the likelihood of each. The most important thing from the doctor's PoV, was it made it much harder for him to miss a vital question that could have led the diagnosis down a very different path to a far better result. Imho this is a major loss in the modern computer world - we have turned programming into a completely different job, considered too hard for your typical worker (of all sorts, including supposedly clever people like doctors), with the result that the people who understand the problem don't understand programming and the programmers don't understand the problem. So we end up with computer systems that cost oodles of money and don't work. The current AI madness is just rushing even further down that path! Cheers, Posted Dec 16, 2025 19:31 UTC (Tue) by Cyberax (✭ supporter ✭, #52523) [Link] There was nothing even remotely similar to that before. Before the AI, the best translation tools were good enough to serve only as a laughingstock. Posted Dec 16, 2025 21:29 UTC (Tue) by malmedal (subscriber, #56172) [Link] (9 responses) If you want machine learning, you want the most learned machine, yes? Today that is the llms. My use for these things is stuff like classifying photos by content, grouping similar ones together, renaming pdfs with names like d.pdf, d (1).pdf d (2).pdf into something meaningful and move them into the correct place in the filesystem. This is quite easy to do with a locally running multimodal llm. It is not perfect, for instance my internet bills get randomly placed in directories with names like bills, utility bills, telecom bill etc. However the result is useful, I can easily find the thing I want. My pre-llm attempts at this with tesseract, sift, trusting pdf metadata, never worked well and were much harder to develop. Special purpose neural nets for e.g. image classification do exist, but in my experience the general llms perform better and I can use the same llm for all tasks. Posted Dec 17, 2025 8:46 UTC (Wed) by taladar (subscriber, #68407) [Link] (2 responses) Not necessarily. For many use cases 99.9999% of what the LLM has learned might be detrimental or useless. Many tasks might also benefit from learning during the phase that is done purely by inference in LLMs (e.g. spam classification certainly won't be done well by something that starts every session with the knowledge of 1 year ago). Posted Dec 17, 2025 9:53 UTC (Wed) by malmedal (subscriber, #56172) [Link] (1 responses) That's what I expected myself... Posted Dec 17, 2025 13:04 UTC (Wed) by Wol (subscriber, #4433) [Link] &amp;gt; That's what I expected myself... And (anecdata) that's what I've regularly heard. People programming in DataBASIC usually get complete garbage back unless they've specifically trained the AI on their own code corpus. While DataBASIC may be closely related to BASIC grammar-wise, the syntax is worlds apart. Probably similar to Finno/Ugrik, where it's fairly easy to learn one if you know the other, but the vocabularies bear no resemblance whatsoever to each other. Cheers, Posted Dec 17, 2025 9:21 UTC (Wed) by farnz (subscriber, #17727) [Link] (5 responses) LLMs are the current best option for natural language tasks; if the thing you are doing is not language based, then other models are likely better (even if that model is itself based on a generative pre-trained transformer architecture). And it sounds like your task is heavy on natural language requirements - "generate meaningful names", "identify which name this document is connected to" - which will be why LLMs work well for that task. Posted Dec 17, 2025 10:26 UTC (Wed) by malmedal (subscriber, #56172) [Link] (4 responses) I easily have enough resources to run a small language model. Originally I used llava later gemma3n, these can be run on modest hardware including even my phone. I do not have enough resources to develop multiple custom task-specific models, especially development-time since these are hobby-projects. And I found that they are better than using a ready-made image classification model and better than an OCR model. Posted Dec 17, 2025 10:39 UTC (Wed) by farnz (subscriber, #17727) [Link] (3 responses) That second one is a market distortion, caused by the assertion that LLMs "just" need a bit more training and a bit more compute to become human-level intelligences, and thus focusing huge amounts of VC money on LLMs - but in time, that will change, because other models are going to be cheaper to run than LLMs for the purposes they're better at. Posted Dec 17, 2025 11:19 UTC (Wed) by malmedal (subscriber, #56172) [Link] (2 responses) That's exactly my point, LLMs is where all the effort has gone, so what I'm saying is that if you want to do almost anything with AI today the easiest way is with an LLM. Posted Dec 17, 2025 11:36 UTC (Wed) by farnz (subscriber, #17727) [Link] (1 responses) Posted Dec 17, 2025 12:35 UTC (Wed) by malmedal (subscriber, #56172) [Link] Posted Dec 16, 2025 15:48 UTC (Tue) by pizza (subscriber, #46) [Link] I have been observing this "stunted learning" phenomena on at least a weekly basis for a while now. These tools are great if you already know what you're doing; if you don't.. using them ensures you never will. Posted Dec 16, 2025 16:03 UTC (Tue) by farnz (subscriber, #17727) [Link] But I've already seen people get upset at the idea that a dead-end job (human supervising chemical simulation software, and answering it when its heuristics don't tell it what to do next) is being put at risk by a GPT model that's ingested a pile of academic papers about interesting molecules, and is answering what to do next, on the basis that using GPT to replace a human is inherently bad, even though it's replacing a human in a dead-end job (one that is given to people as a hint that they should find a new career path). Posted Dec 17, 2025 8:40 UTC (Wed) by taladar (subscriber, #68407) [Link] (3 responses) Posted Dec 17, 2025 9:53 UTC (Wed) by farnz (subscriber, #17727) [Link] (2 responses) The significant gap between the two is that Roko's Basilisk and Pascal's Wager are both talking about future penalties for failure to comply; this class of quandary is asserting that the penalties have already been paid, and asking whether or not those penalties are such that this knowledge is tainted and must not be used. Posted Dec 17, 2025 13:00 UTC (Wed) by pizza (subscriber, #46) [Link] (1 responses) I wouldn't be so sure of that; the FOMO within corporate leadership that's led to force-feeding "AI" into everything has resulted in metrics phrased as the latter, but they're really the former. Posted Dec 17, 2025 14:01 UTC (Wed) by farnz (subscriber, #17727) [Link] People pushing AI into places it doesn't fit in your job is also a problem, but is a different category of problem. Posted Dec 16, 2025 0:21 UTC (Tue) by karath (subscriber, #19025) [Link] On the main topic of the article, I’d both support adding plugins for AI while wholly agreeing that the software should give users the ability to disable or even remove the functionality. My view of the current ‘marketplace of ideas’ is that the current push for AI (in the form of LLMs) has widespread backing among ‘knowledge workers’, even where they know that there will be some fallout. LLMs are showing emergent behaviour that is more than mere database querying, however far less than becoming sentient. I suspect that in several years time, the topic of LLMs will be mined out and the new cry will be that AI is a dead end (again). Posted Dec 16, 2025 6:46 UTC (Tue) by dankamongmen (subscriber, #35141) [Link] (3 responses) i dislike everything about the move to statistical models, yet agree with this sentiment. this article evidenced a degree of bias i'm surprised to find on LWN. Posted Dec 16, 2025 10:30 UTC (Tue) by Wol (subscriber, #4433) [Link] &amp;gt; i dislike everything about the move to statistical models, yet agree with this sentiment. this article evidenced a degree of bias i'm surprised to find on LWN. If that bias was Open. Unabashed. And Obvious that makes it PERFECT for LWN. NEUTRALITY IS IMPOSSIBLE and those people who claim to be capable of it are liars. That also includes organisations! If you wear your biases and opinions on your sleeve - and you are careful to separate them from facts !!! - that makes your opinions MUCH more valuable than someone claiming to be neutral. Your readers / hearers then have the opportunity to make up their own minds, rather than having you pretend to be omniscient. Cheers, Posted Dec 17, 2025 3:08 UTC (Wed) by dvdeug (subscriber, #10998) [Link] (1 responses) When I took a Discrete Optimization class, I found myself disliking the randomized tools, like simulated annealing, where it's hard to say how good the result is and impossible to say if it's the optimal result. I also found that if you wanted good answers fast, you used the randomized tools; purely predictable tools could get an okay answer fast or the optimal answer but sometimes literally after the sun burned out. There are some cases where pure logic isn't the way to progress, even in the world of algorithms, and statistics and randomness are the right tools. Posted Dec 17, 2025 8:54 UTC (Wed) by taladar (subscriber, #68407) [Link] Posted Dec 16, 2025 18:48 UTC (Tue) by q3cpma (subscriber, #120859) [Link] And I say that as someone who really hates LLMs and can't wait for the bubble to pop or significantly deflate (though I'd like something based on them to replace Grammarly/languagetool). Posted Dec 18, 2025 23:47 UTC (Thu) by jschrod (subscriber, #1646) [Link] That said, I didn't find the article biased. Both viewpoints were well represented. Since you want that "optimistic users shall be considered", c.f. your comment title - might it be that the bias is more on your side than on the author's side? Posted Dec 16, 2025 0:10 UTC (Tue) by pmallory (subscriber, #122252) [Link] (5 responses) I was curious why this would appeal to anyone, so I took a closer look at the discussion thread to try and find out. Some people want help parsing dense writing (Joyce, Kant) written in languages the reader isn't confident with. I'd recommend these readers get annotated texts instead, but it's not my place to tell them not to use an LLM for this. Someone else wants to use an LLM to populate tags based on a book's/article's table of contents. I'm not sure if an LLM is the best solution, but again it's not my place to tell someone they shouldn't want to do that. I guess ethical objections still remain. I'm not swayed on there either. People can already use Calibre to manage books that they didn't pay for. Is it Calibre's job to prevent people from also using LLMs trained on books that weren't paid for? Should Calibre also remove its RSS feed support, and every other feature, in case people use those unethically? The other complaints on Calibre's forum seem to actually be complaints about Microsoft Windows and Excel, and various histrionics that also don't have anything to do with the actual feature. So the opposition to this feature, as far as I can tell, is "I don't want it and I don't think anyone else should either". The first part is fine by me and I agree, but the second part I can't get behind. Posted Dec 16, 2025 5:20 UTC (Tue) by raven667 (subscriber, #5198) [Link] (2 responses) Posted Dec 16, 2025 5:37 UTC (Tue) by Cyberax (✭ supporter ✭, #52523) [Link] Posted Dec 20, 2025 14:03 UTC (Sat) by ras (subscriber, #33059) [Link] It takes longer to type your words in, but since something is going to try and understand those words you are forced to connect your thoughts into a logical narrative. Merely being forced to do that is often enough to untangle them. But if not you can press the send button and see if you made sense to something with language skills far better than most humans. If the LLM does make some sense to it, you get the bonus round - it may unreliably parrot back to you the words of someone who has solved the problem before. After all, the LLMs don't just have incredible language skills, they also have read far more widely than any of us. I think their ability to find and regurgitate what people far more knowledgeable than me on a topic had to say is an awesome addition to search engines. It's a shame about the unreliability. But you can't complain about the price. LLM's that have been fed the entire internet, and have had man centuries of training are free. For now. I suspect for a short time only. Posted Dec 25, 2025 10:44 UTC (Thu) by davidgerard (guest, #100304) [Link] (1 responses) Posted Dec 26, 2025 14:48 UTC (Fri) by Phantom_Hoover (subscriber, #167627) [Link] Posted Dec 18, 2025 3:09 UTC (Thu) by milesrout (subscriber, #126894) [Link] I don't see myself using this feature. I also don't see myself yelling in all caps about it or refusing to download the next release. These people need to bloody grow up. Posted Dec 20, 2025 14:25 UTC (Sat) by Phantom_Hoover (subscriber, #167627) [Link] (11 responses) Have you considered the possibility that there are significant numbers of people out there who genuinely find LLMs a useful tool and want the option to use them in Calibre? I’m very much an AI sceptic myself and I don’t use LLMs at all, but so much of the current backlash has ceased to be sceptical, it’s dogmatic insistence that the entire thing is bullshit hype. There’s a core of novel technological capabilities there that isn’t going anywhere and has *some* genuine applications, and it’s foolish and petty to try to bully ebook applications out of quietly exploring them. I was frankly disappointed in this article as a piece of LWN coverage. LWN usually does a good job of reporting evenhandedly on controversies; I’m not sure I’ve seen a piece here where the author tips his hand so obviously in taking a side before. Posted Dec 20, 2025 16:05 UTC (Sat) by dskoll (subscriber, #1630) [Link] (7 responses) Even if the LWN article was slanted against AI, I'm fine with that as a counter-balance, considering what we're up against. Posted Dec 20, 2025 18:29 UTC (Sat) by intelfx (subscriber, #130118) [Link] (6 responses) Two wrongs don't make a right; in discourse more than anywhere. Posted Dec 20, 2025 18:33 UTC (Sat) by dskoll (subscriber, #1630) [Link] (5 responses) An LWN piece written by someone with a point of view is not a "wrong" in the same way that oligarchs spending $100M to target politicians whose opinions on AI they don't like is a wrong. Come on, be real. Posted Dec 20, 2025 19:11 UTC (Sat) by intelfx (subscriber, #130118) [Link] (4 responses) This is sophistics and as such invalid. To rephrase my point for more clarity and less chance of sophistics, two biased sources do not cancel each other resulting in an unbiased discourse, they just turn into a shouting match. &amp;gt; Come on, be real. Don't bring my personality into the discussion, thanks. Posted Dec 20, 2025 19:19 UTC (Sat) by dskoll (subscriber, #1630) [Link] (2 responses) It is perfectly acceptable for an article author to have an opinion and write an article that reflects that opinion. Or are you saying that nothing should ever be published that is somehow "biased"? How would one measure such "bias"? Who gets to decide if something is "biased"? To take an extreme example, would it be wrong to publish an article saying that it's wrong to covertly inject malware into the Linux kernel, because that's a "biased" point of view and that one should take no position about injecting malware into the kernel? Posted Dec 22, 2025 11:37 UTC (Mon) by Phantom_Hoover (subscriber, #167627) [Link] (1 responses) Posted Dec 22, 2025 13:27 UTC (Mon) by Wol (subscriber, #4433) [Link] And I was left with the strong impression that the author of this article DID know the difference - it seemed perfectly clear to me ... Cheers, Posted Dec 20, 2025 20:22 UTC (Sat) by Wol (subscriber, #4433) [Link] And you're claiming that you're not biased? COME ON! I'd much rather someone wears their opinion on their sleeve, than lies by claiming to be unbiased. Cheers, Posted Dec 20, 2025 16:42 UTC (Sat) by Wol (subscriber, #4433) [Link] (2 responses) I just wish MORE authors would do that. Just keep a clear dividing line between facts and opinions. And notice he did say that there was a simple on/off switch! Precisely so the AI-haters could turn it off. That's much more truly neutral than the software that's shoving it down your throat. &amp;gt; LWN usually does a good job of reporting evenhandedly on controversies And why isn't this article a good job, either? True, editors are discouraged from taking sides, but it's very hard to truly hide your biases. All too often attempting to do so results in pretending some garbage theory on the other side actually has weight it doesn't deserve. Too much authoring is encouraged to be in the 3rd person, which lends a completely false air of authority to what is being said. Take personal responsibility for your opinions, write in the first person, and as I said (do your best to) keep facts and opinions separate. The result is MUCH more honest than pretending to be neutral and dispassionate. Cheers, Posted Dec 20, 2025 19:14 UTC (Sat) by Phantom_Hoover (subscriber, #167627) [Link] (1 responses) Posted Dec 20, 2025 20:30 UTC (Sat) by Wol (subscriber, #4433) [Link] And I'm not going to read the article again to see whether I agree with you. But that's your opinion of his opinion, and we can choose reasonably to differ. It certainly came over to me as a biased piece. But the author *knew* it was biased, and tried to cater to people who disagreed with him. Compared to all these people who like to lay down their infallible truth, it was a breath of fresh air :-) Cheers, &lt;head&gt;What you DO NOT get to do is try to make that choice for other people. &lt;/head&gt;&lt;lb/&gt; Wol.&lt;head&gt;What you DO NOT get to do is try to make that choice for other people. &lt;/head&gt;&lt;head&gt;What you DO NOT get to do is try to make that choice for other people. &lt;/head&gt;&lt;lb/&gt; Wol&lt;head&gt;What you DO NOT get to do is try to make that choice for other people. &lt;/head&gt;&lt;lb/&gt; Are you sure AI/LLM are in the category of people?&lt;head&gt;What you DO NOT get to do is try to make that choice for other people. &lt;/head&gt;&lt;head&gt;Discussing with an LLM&lt;/head&gt;&lt;head&gt;Discussing with an LLM&lt;/head&gt;&lt;head&gt;Discussing with an LLM&lt;/head&gt;&lt;head&gt;Discussing with an LLM&lt;/head&gt;&lt;head&gt;Discussing with an LLM&lt;/head&gt;&lt;head&gt;Discussing with an LLM&lt;/head&gt;&lt;head&gt;Discussing with an LLM&lt;/head&gt;&lt;head&gt;Discussing with an LLM&lt;/head&gt;&lt;head&gt;Discussing with an LLM&lt;/head&gt;&lt;head&gt;Discussing with an LLM&lt;/head&gt;&lt;head&gt;Discussing with an LLM&lt;/head&gt;&lt;head&gt;Discussing with an LLM&lt;/head&gt;&lt;head&gt;Discussing with an LLM&lt;/head&gt;&lt;head&gt;Discussing with an LLM&lt;/head&gt;&lt;head&gt;Discussing with an LLM&lt;/head&gt;&lt;lb/&gt; &amp;gt;&amp;gt;it might say "Yes, I did miss Alex &amp;amp; Emmy as a movie he directed." or "Rob Reiner was an actor in Throw Momma from the Train, but it was directed by Danny DeVito." or "While Some Kind of Wonderful does have similarities to some movies directed by Rob Reiner, like The Sure Thing, it was directed by Howard Deutch." &lt;head&gt;Discussing with an LLM&lt;/head&gt;&lt;lb/&gt; Wol&lt;head&gt;Discussing with an LLM&lt;/head&gt;&lt;quote&gt;You can force it to tell you that Rob Reiner directed Titanic, but that won't make it true.&lt;/quote&gt;&lt;head&gt;Using mental illness as insult...&lt;/head&gt;&lt;lb/&gt; Now I know what Kovid Goyal thinks of other people.&lt;head&gt;Using mental illness as insult...&lt;/head&gt;&lt;head&gt;Using mental illness as insult...&lt;/head&gt;&lt;head&gt;Using mental illness as insult...&lt;/head&gt;&lt;head&gt;Using mental illness as insult...&lt;/head&gt;&lt;head&gt;Using mental illness as insult...&lt;/head&gt;&lt;lb/&gt; Wol&lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;head/&gt; I see one, and it's quite a weak quandary: "what if, by refusing to use AI, I prevent someone from discovering something of great value that would have been discovered with AI assistance?". &lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;lb/&gt; Wol&lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;lb/&gt; For many tasks it might be more important to have the correct training data than huge amounts of randomly assembled training data.&lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;lb/&gt; Wol&lt;head/&gt; You do not want the "most learned" machine; you want the one with most appropriate training to the task at hand. &lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;head/&gt; Two things: &lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;head/&gt; And mine is that an LLM isn't always what you want - it's what you can make work easily and cheaply, which is not the same thing at all. &lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;head/&gt; I did say it's quite a weak quandary :-) &lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;head/&gt; It's a different class of quandary - those are "if I reject the thing, I will be punished", whereas this is "if I reject the thing, I may miss out on rewards". The closest strong quandary is the "Death Camp Science" quandary - if we have knowledge as a result of inhumane war crimes committed at death camps, to what extent am I either implicitly endorsing those crimes or encouraging the commission of new crimes by benefiting from that knowledge? &lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;head/&gt; That is a different quandary to the one I was talking about, though. Mine applies in the absence of people trying to force AI into places it doesn't fit. &lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;head&gt;Anti-fake balanced reporting&lt;/head&gt;&lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;lb/&gt; Wol&lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;head&gt;Consider the optimistic users&lt;/head&gt;&lt;head&gt;Tempest in a teapot?&lt;/head&gt;&lt;head&gt;Tempest in a teapot?&lt;/head&gt;&lt;head&gt;Tempest in a teapot?&lt;/head&gt;&lt;head&gt;Tempest in a teapot?&lt;/head&gt;&lt;head&gt;Tempest in a teapot?&lt;/head&gt;&lt;head&gt;Tempest in a teapot?&lt;/head&gt;&lt;head&gt;If you don't like it, don't use it!&lt;/head&gt;&lt;head&gt;Poor coverage&lt;/head&gt;&lt;head&gt;Poor coverage&lt;/head&gt;&lt;head&gt;Poor coverage&lt;/head&gt;&lt;head&gt;Poor coverage&lt;/head&gt;&lt;head&gt;Poor coverage&lt;/head&gt;&lt;head&gt;Poor coverage&lt;/head&gt;&lt;head&gt;Poor coverage&lt;/head&gt;&lt;head&gt;Poor coverage&lt;/head&gt;&lt;lb/&gt; Wol&lt;head&gt;Poor coverage&lt;/head&gt;&lt;lb/&gt; Wol&lt;head&gt;Poor coverage&lt;/head&gt;&lt;lb/&gt; Wol&lt;head&gt;Poor coverage&lt;/head&gt;&lt;head&gt;Poor coverage&lt;/head&gt;&lt;lb/&gt; Wol&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46390848</guid><pubDate>Fri, 26 Dec 2025 10:22:27 +0000</pubDate></item><item><title>The First Web Server</title><link>https://dfarq.homeip.net/the-first-web-server/</link><description>&lt;doc fingerprint="fb0f55d54bf40ea7"&gt;
  &lt;main&gt;
    &lt;p&gt;Late December 1990 was a pivotal time, although none of us realized it for a few years. Tim Berners-Lee, A British computer scientist working in Switzerland, was working on what became the World Wide Web. Over the course of a few months, he invented HTML, the web browser, and the web server, to make it easier to share information. Sometime in late December, the first web server reached a usable state. By some accounts it was December 20, 1990. By at least one account I found, it was December 25.&lt;/p&gt;
    &lt;head rend="h2"&gt;The first web server’s address&lt;/head&gt;
    &lt;p&gt;The early work on the World Wide Web took place on NeXT workstations. Berners-Lee’s workstation lived at info.cern.ch.CERN is the European Organization for Nuclear Research, an intergovernmental organization that operates the largest particle physics laboratory in the world. It might be the most momentous shadow IT project in history.&lt;/p&gt;
    &lt;p&gt;No screenshots exist of the web page in its earliest form, unfortunately, although I did find an approximation of how the page appeared in 1992. Not surprisingly, the first web page was technical information about the web, including how HTML, web servers, and web browsers worked.&lt;/p&gt;
    &lt;p&gt;The earliest copy of the page I could find on archive.org, from 2000, stated the web page and the computer that hosted it no longer exist. In August 2006, CERN memorialized the first web page and first web server with a page about it.&lt;/p&gt;
    &lt;p&gt;Berners-Lee’s original goal was making information more accessible. Valuable data resided in various formats on computers throughout the organization. Berners-Lee’s goal was to unlock the data so it could link together and be readable from any machine.&lt;/p&gt;
    &lt;head rend="h2"&gt;What happened next&lt;/head&gt;
    &lt;p&gt;It took a few years for the World Wide Web to go worldwide. By January 1993, NSCA Mosaic, a cross-platform web browser, was available, which gave rise to Netscape. The web caught on quickly on college campuses with browsers that ran on all of the major platforms of the time. Efforts to commercialize the web led to the dotcom boom, and, eventually, to the online world we know today.&lt;/p&gt;
    &lt;p&gt;David Farquhar is a computer security professional, entrepreneur, and author. He has written professionally about computers since 1991, so he was writing about retro computers when they were still new. He has been working in IT professionally since 1994 and has specialized in vulnerability management since 2013. He holds Security+ and CISSP certifications. Today he blogs five times a week, mostly about retro computers and retro gaming covering the time period from 1975 to 2000.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46391077</guid><pubDate>Fri, 26 Dec 2025 11:12:28 +0000</pubDate></item><item><title>I'm a laptop weirdo and that's why I like my new Framework 13</title><link>https://blog.matthewbrunelle.com/im-a-laptop-weirdo-and-thats-why-i-like-my-new-framework-13/</link><description>&lt;doc fingerprint="560efd35c9737c9d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I'm a laptop weirdo and that's why I like my new Framework 13&lt;/head&gt;
    &lt;p&gt;This month I sold my 2021 M1 Max Macbook Pro and bought a Framework 13 DIY Edition laptop. After I got everything setup I sat down to write about the experience. Some ~4500 words later I realized I needed to break my thoughts into multiple posts.&lt;/p&gt;
    &lt;p&gt;See also:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Framework 13 DIY Edition Hardware Thoughts&lt;/item&gt;
      &lt;item&gt;Setting up my new Framework Laptop 13 DIY Edition with NixOS&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;My new Framework 13 laptop just arrived. After I finally set everything up I started writing a post about the experience. I thought I'd write a little bit about my previous laptops, but a lot of fond memories I had forgotten about came flooding back. The tinkerings and many openings of laptops past. If you will indulge me, I've been feeling nostalgic. This is for the other laptop weirdos out there that that feel the same.&lt;/p&gt;
    &lt;head rend="h3"&gt;I have a history of doing terrible acts to laptops&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt; The only image I could find of my NC10 was this blurry, 2021 flip phone photo of me removing the windows sticker.&lt;/p&gt;
    &lt;p&gt;In 2008, I managed to get my hands on a Samsung NC10 Netbook in a fancy metallic blue color. [^ Back when netbooks where a thing circle 2007-2013] Prior to this I only had desktops. The specs were pretty humble (from wikipedia):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A single core 1.6 GHz Intel Atom N270&lt;/item&gt;
      &lt;item&gt;Integrated Intel GMA 950 graphics&lt;/item&gt;
      &lt;item&gt;1 GB DDR2 RAM&lt;/item&gt;
      &lt;item&gt;10.2 inch 1024x600 screen and a VGA connector of all things.&lt;/item&gt;
      &lt;item&gt;83-key keyboard rather than the usual 87 or 88 keys on a laptop.&lt;/item&gt;
      &lt;item&gt;A 160 GB HDD&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Something could be done about that though! You could upgrade the RAM to a powerful 2GB. You could replace the slow HDD with an SSD. You could add a touch screen. You could make a Hackintosh out of it if you replaced the wifi card. If you wanted to, you could do those things and I was a weirdo, so I did!&lt;/p&gt;
    &lt;p&gt;I found a lot of fun in trying to get as much as I could out of that hardware. In fact I'd say the act of doing all that was far more enjoyable than actually using the laptop once the tinkering was done. After the novelty and slowness of a Hackintosh wore off I put Linux on the Netbook. I still sought the thrill of the hunt.&lt;/p&gt;
    &lt;p&gt;I installed a lite weight distro CrunchBang [^ or just #!] and messed around. I read more about different minimalist distros and came across two others I could hop to: Arch and Gentoo. This feels like an inflection point in my life, I choose to try Arch since I wouldn't have to compile everything on a single core. [^ Who know what would have happened if I picked Gentoo. I might have a beard now.] The screen was small and I wanted to maximize its usefulness so I started trying tiling WMs. Why not XMonad?&lt;/p&gt;
    &lt;p&gt;It turns out the GMA950 was undervolted on the NC10. Someone made a shareware tool called the GMABooster that could restore the max clock rate. The original website http://www.gmabooster.com/home.htm is long toast and not on wayback. This Arch forum thread has details though:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It allows a user, not a manufacturer to choose the desired GMA speed. It combines a sophisticated assembler-level technology and the user-friendly graphic user interface, offering You to near double the GMA core perfomance without even a need to restart a computer..&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The package was on AUR so I could squeeze out a little more performance. I could finally watch 480 YouTube videos instead of 360. At some point, long after I had stopped using the netbook, the AUR package became abandoned. I adopted it as maintainer and mirrored the binary in GitHub. This was the first time I ever was a package maintainer. [^ I am on a couple random packages in nixpkgs now.] Nowadays the package is memorialized in the the AUR archive.&lt;/p&gt;
    &lt;p&gt;I had a device that I could repeatedly break and remake. Did I do anything productive or meaningful with it? Absolutely not. Did I learn a lot in the process? I'd say so!&lt;/p&gt;
    &lt;head rend="h3"&gt;In the past you could do terrible things to Macbooks too&lt;/head&gt;
    &lt;p&gt;When I went to College I got a 2011 Macbook Pro. The kind that would overheat and desolder the GPU. [^ Some clever people have found hardware hacks to repair the problem https://www.jeffgeerling.com/blog/2017/fixing-2011-macbook-pro-booting-grey-screen-amd-radeon-video-glitch] Mine managed to last a long time and didn't need replacing until 2019. The RAM was not built-in yet on Macbooks. Apple said the model could only support up to 8GB total RAM, but you could actually get 16GB to work. Also, this was back when Macbooks had CD drives. I replaced the my drive with an Other World Computing DIY Optical Drive to HDD Upgrade Kit. [^ And you could put the drive into an "OWC SuperSlim" enclosure to turn it into a USB CD drive.] and installed SSDs in both slots. With two drives I was able to install rEFInd as a boot manager and triple boot:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OSX as a stable install for my course work&lt;/item&gt;
      &lt;item&gt;Windows for games&lt;/item&gt;
      &lt;item&gt;Linux so I could break my install repeatedly&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I iterated on my Arch install so many times that I started to keep a checklist about my setup process to help me remember everything. Certain stylistic choices were set and still used to this day. [^ This is when I started using Inconsolata for a monospace font and Zenburn for a color scheme.] I couldn't change quite as many things about this laptop, but I still made an effort to change what I could.&lt;/p&gt;
    &lt;head rend="h3"&gt;As laptops grew thinner they grew more boring&lt;/head&gt;
    &lt;p&gt;When it came time for a new laptop I was not looking at Macbook Pros anymore. Apple had made changes, like the touch bar and removing magsafe, that felt like they were targeting a different audience. So instead I had been eyeing a ThinkPad.&lt;lb/&gt; [^ It's almost cliche to buy one and install Linux.] The prices on the Lenovo store are mostly made up and constantly discounted. My housemate had access to a corpo portal for Lenovo that let me get one at a heavily reduced price. The cost of 3 year service coverage was also discounted so I got some figuring it could help to cover cost of parts if if something failed.&lt;/p&gt;
    &lt;p&gt;So I bought a Gen 7 X1 Carbon and... I just used it. No mods were possible on this laptop. When I had an SSD failure I asked Lenovo if they could mail me the drive so I could do the install. They said they had to send someone to confirm the issue. So a technician came out and replaced the drive.&lt;/p&gt;
    &lt;head rend="h3"&gt;The gift and curse of a free Macbook Pro&lt;/head&gt;
    &lt;p&gt;Finally in 2023 I was laid off by HubSpot. Part of severance was the following:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Laptops &amp;amp; WFH Set-Up: Impacted employees may keep their HubSpot laptops (it will be cleaned of any company data remotely), as well as any work from home gear like monitors and keyboards.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Thus a pretty high spec 2021 M1 Max Macbook Pro fell into my lap. I gave my X1 Carbon to a friend to avoid creating yet more ewaste that sits in my closet.&lt;/p&gt;
    &lt;p&gt;The 2021 version was a bit of return to form: touch bar was gone, magsafe was back, etc. However even the iFixit review said the "design represents a major move in the right direction" but still only rated the laptop a 4/10 for repairability. [^ The score was eventually updated to a 5/10 when Apple later released a service manual and access to parts.]&lt;/p&gt;
    &lt;p&gt;I felt some dissonance though. If I was looking to buy a laptop, I wouldn't have picked this one. macOS was getting less enjoyable to use with each update. Likewise the Linux Desktop experience was really coming into its own. [^ By 2023 essentially all my games were playable!] However I felt bad about buying a new laptop when I now had a perfectly good one. So I held onto it and once again, no mods were done or could be done with this laptop.&lt;/p&gt;
    &lt;head rend="h3"&gt;Finally buying a Framework 13&lt;/head&gt;
    &lt;p&gt;I had waited on getting a Framework laptop because I wanted to see them go through a couple iterations. I wanted to see if the promise of repairing, replacing and upgrading actually came true. From what I read it mostly has! [^ People with Framework 15 do seem to be waiting though.]&lt;/p&gt;
    &lt;p&gt;What changed the decision for me was the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Lugging around a powerful 16 inch laptop was a drag. Having a laptop when traveling is nice if I need to hurriedly rebook something. Mobile sites and apps tend to restrict you in weird ways.&lt;/item&gt;
      &lt;item&gt;Despite being a couple years old, the laptop was still worth a lot. People probably want Macbooks for local LLM inference. So I felt pretty good a buyer will actually use the laptop.&lt;/item&gt;
      &lt;item&gt;The Framework release a refresh of the 13 with the new AMD chips.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Then I had a friend get Laptop 13 and attest to liking it. That was the last push I needed to finally buy one. Now I can be a laptop weirdo again.&lt;/p&gt;
    &lt;p&gt;You can't change the RAM on laptops now.&lt;lb/&gt; You can't change the SSD on laptops now.&lt;lb/&gt; You can't easily repair the screen on laptops now.&lt;/p&gt;
    &lt;p&gt;You can do all that and more with a Framework laptop.&lt;lb/&gt; You can be a laptop weirdo with a Framework laptop.&lt;/p&gt;
    &lt;p&gt;Weirdo typically has two interpretations:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;A possibly dangerous person.&lt;/p&gt;&lt;lb/&gt;A strange, odd, eccentric person.&lt;/quote&gt;
    &lt;p&gt;To both of those I say: all us laptop weirdos can now put a snack drawer in our laptops.&lt;lb/&gt; You cannot stop us.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46391410</guid><pubDate>Fri, 26 Dec 2025 12:27:19 +0000</pubDate></item><item><title>Unix "find" expressions compiled to bytecode</title><link>https://nullprogram.com/blog/2025/12/23/</link><description>&lt;doc fingerprint="1fe85974ea331f40"&gt;
  &lt;main&gt;
    &lt;p&gt; nullprogram.com/blog/2025/12/23/ &lt;/p&gt;
    &lt;p&gt;In preparation for a future project, I was thinking about at the unix &lt;code&gt;find&lt;/code&gt; utility. It operates a file system hierarchies, with basic
operations selected and filtered using a specialized expression language.
Users compose operations using unary and binary operators, grouping with
parentheses for precedence. &lt;code&gt;find&lt;/code&gt; may apply the expression to a great
many files, so compiling it into a bytecode, resolving as much as possible
ahead of time, and minimizing the per-element work, seems like a prudent
implementation strategy. With some thought, I worked out a technique to do
so, which was simpler than I expected, and I’m pleased with the results. I
was later surprised all the real world &lt;code&gt;find&lt;/code&gt; implementations I examined
use tree-walk interpreters instead. This article describes how my
compiler works, with a runnable example, and lists ideas for improvements.&lt;/p&gt;
    &lt;p&gt;For a quick overview, the syntax looks like this:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ find [-H|-L] path... [expression...]
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Technically at least one path is required, but most implementations imply &lt;code&gt;.&lt;/code&gt; when none are provided. If no expression is supplied, the default is
&lt;code&gt;-print&lt;/code&gt;, e.g. print everything under each listed path. This prints the
whole tree, including directories, under the current directory:&lt;/p&gt;
    &lt;p&gt;To only print files, we could use &lt;code&gt;-type f&lt;/code&gt;:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ find . -type f -a -print
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Where &lt;code&gt;-a&lt;/code&gt; is the logical AND binary operator. &lt;code&gt;-print&lt;/code&gt; always evaluates
to true. It’s never necessary to write &lt;code&gt;-a&lt;/code&gt;, and adjacent operations are
implicitly joined with &lt;code&gt;-a&lt;/code&gt;. We can keep chaining them, such as finding
all executable files:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ find . -type f -executable -print
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;If no &lt;code&gt;-exec&lt;/code&gt;, &lt;code&gt;-ok&lt;/code&gt;, or &lt;code&gt;-print&lt;/code&gt; (or similar side-effect extensions like
&lt;code&gt;-print0&lt;/code&gt; or &lt;code&gt;-delete&lt;/code&gt;) are present, the whole expression is wrapped in an
implicit &lt;code&gt;( expr ) -print&lt;/code&gt;. So we could also write this:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ find . -type f -executable
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Use &lt;code&gt;-o&lt;/code&gt; for logical OR. To print all files with the executable bit or
with a &lt;code&gt;.exe&lt;/code&gt; extension:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ find . -type f \( -executable -o -name '*.exe' \)
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;I needed parentheses because &lt;code&gt;-o&lt;/code&gt; has lower precedence than &lt;code&gt;-a&lt;/code&gt;, and
because parentheses are shell metacharacters I also needed to escape them
for the shell. It’s a shame &lt;code&gt;find&lt;/code&gt; didn’t use &lt;code&gt;[&lt;/code&gt; and &lt;code&gt;]&lt;/code&gt; instead! There’s
also a unary logical NOT operator, &lt;code&gt;!&lt;/code&gt;. To print all non-executable files:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ find . -type f ! -executable
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Binary operators are short-circuiting, so this:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ find -type d -a -exec du -sh {} +
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Only lists the sizes of directories, as the &lt;code&gt;-type d&lt;/code&gt; fails causing the
whole expression to evaluate to false without evaluating &lt;code&gt;-exec&lt;/code&gt;. Or
equivalently with &lt;code&gt;-o&lt;/code&gt;:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ find ! -type d -o -exec du -sh {} +
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;If it’s not a directory then the left-hand side evaluates to true, and the right-hand side is not evaluated. All three implementations I examined (GNU, BSD, BusyBox) have a &lt;code&gt;-regex&lt;/code&gt; extension, and eagerly compile the
regular expression even if the operation is never evaluated:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ find . -print -o -regex [
find: bad regex '[': Invalid regular expression
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;I was surprised by this because it doesn’t seem to be in the spirit of the original utility (“The second expression shall not be evaluated if the first expression is true.”), and I’m used to the idea of short-circuit validation for the right-hand side of a logical expression. Recompiling for each evaluation would be unwise, but it could happen lazily such that an invalid regular expression only causes an error if it’s actually used. No big deal, just a curiosity.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bytecode design&lt;/head&gt;
    &lt;p&gt;A bytecode interpreter needs to track just one result at a time, making it a single register machine, with a 1-bit register at that. I came up with these five opcodes:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;halt
not
braf   LABEL
brat   LABEL
action NAME [ARGS...]
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Obviously &lt;code&gt;halt&lt;/code&gt; stops the program. While I could just let it “run off the
end” it’s useful to have an actual instruction so that I can attach a
label and jump to it. The &lt;code&gt;not&lt;/code&gt; opcode negates the register. &lt;code&gt;braf&lt;/code&gt; is
“branch if false”, jumping (via relative immediate) to the labeled (in
printed form) instruction if the register is false. &lt;code&gt;brat&lt;/code&gt; is “branch if
true”. Together they implement the &lt;code&gt;-a&lt;/code&gt; and &lt;code&gt;-o&lt;/code&gt; operators. In practice
there are no loops and jumps are always forward: &lt;code&gt;find&lt;/code&gt; is not Turing
complete.&lt;/p&gt;
    &lt;p&gt;In a real implementation each possible action (&lt;code&gt;-name&lt;/code&gt;, &lt;code&gt;-ok&lt;/code&gt;, &lt;code&gt;-print&lt;/code&gt;,
&lt;code&gt;-type&lt;/code&gt;, etc.) would get a dedicated opcode. This requires implementing
each operator, at least in part, in order to correctly parse the whole
&lt;code&gt;find&lt;/code&gt; expression. For now I’m just focused on the bytecode compiler, so
this opcode is a stand-in, and it kind of pretends based on looks. Each
action sets the register, and actions like &lt;code&gt;-print&lt;/code&gt; always set it to true.
My compiler is called &lt;code&gt;findc&lt;/code&gt; (“find compiler”).&lt;/p&gt;
    &lt;p&gt;Update: Or try the online demo via Wasm! This version includes a peephole optimizer I wrote after publishing this article.&lt;/p&gt;
    &lt;p&gt;I assume readers of this program are familiar with &lt;code&gt;push&lt;/code&gt; macro
and &lt;code&gt;Slice&lt;/code&gt; macro. Because of the latter it requires a very
recent C compiler, like GCC 15 (e.g. via w64devkit) or Clang 22. Try
out some &lt;code&gt;find&lt;/code&gt; commands and see how they appear as bytecode. The simplest
case is also optimal:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ findc
// path: .
        action  -print
        halt
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Print the path then halt. Simple. Stepping it up:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ findc -type f -executable
// path: .
        action  -type f
        braf    L1
        action  -executable
L1:     braf    L2
        action  -print
L2:     halt
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;If the path is not a file, it skips over the rest of the program by way of the second branch instruction. It’s correct, but already we can see room for improvement. This would be better:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;        action  -type f
        braf    L1
        action  -executable
        braf    L1
        action  -print
L1:     halt
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;More complex still:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ findc -type f \( -executable -o -name '*.exe' \)
// path: .
        action  -type f
        braf    L1
        action  -executable
        brat    L1
        action  -name *.exe
L1:     braf    L2
        action  -print
L2:     halt
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Inside the parentheses, if &lt;code&gt;-executable&lt;/code&gt; succeeds, the right-hand side is
skipped. Though the &lt;code&gt;brat&lt;/code&gt; jumps straight to a &lt;code&gt;braf&lt;/code&gt;. It would be better
to jump ahead one more instruction:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;        action  -type f
        braf    L2
        action  -executable
        brat    L1
        action  -name *.exe
        braf    L2
L1      action  -print
L2:     halt
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Silly things aren’t optimized either:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ findc ! ! -executable
// path: .
        action  -executable
        not
        not
        braf    L1
        action  -print
L1:     halt
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Two &lt;code&gt;not&lt;/code&gt; in a row cancel out, and so these instructions could be
eliminated. Overall this compiler could benefit from a peephole
optimizer, scanning over the program repeatedly, making small
improvements until no more can be made:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Delete &lt;code&gt;not&lt;/code&gt;-&lt;code&gt;not&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;A &lt;code&gt;brat&lt;/code&gt; to a &lt;code&gt;braf&lt;/code&gt; re-targets ahead one instruction, and vice versa.&lt;/item&gt;
      &lt;item&gt;Jumping onto an identical jump adopts its target for itself.&lt;/item&gt;
      &lt;item&gt;A &lt;code&gt;not&lt;/code&gt;-&lt;code&gt;braf&lt;/code&gt; might convert to a &lt;code&gt;brat&lt;/code&gt;, and vice versa.&lt;/item&gt;
      &lt;item&gt;Delete side-effect-free instructions before &lt;code&gt;halt&lt;/code&gt; (e.g. &lt;code&gt;not&lt;/code&gt;-&lt;code&gt;halt&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;Exploit always-true actions, e.g. &lt;code&gt;-print&lt;/code&gt;-&lt;code&gt;braf&lt;/code&gt; can drop the branch.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Writing a bunch of peephole pattern matchers sounds kind of fun. Though my compiler would first need a slightly richer representation in order to detect and fix up changes to branches. One more for the road:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ findc -type f ! \( -executable -o -name '*.exe' \)
// path: .
        action  -type f
        braf    L1
        action  -executable
        brat    L2
        action  -name *.exe
L2:     not
L1:     braf    L3
        action  -print
L3:     halt
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;The unoptimal jumps hint at my compiler’s structure. If you’re feeling up for a challenge, pause here to consider how you’d build this compiler, and how it might produce these particular artifacts.&lt;/p&gt;
    &lt;head rend="h3"&gt;Parsing and compiling&lt;/head&gt;
    &lt;p&gt;Before I even considered the shape of the bytecode I knew I needed to convert &lt;code&gt;find&lt;/code&gt; infix into a compiler-friendly postfix. That is, this:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;-type f -a ! ( -executable -o -name *.exe )
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Becomes:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;-type f -executable -name *.exe -o ! -a
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Which, importantly, erases the parentheses. This comes in as an &lt;code&gt;argv&lt;/code&gt;
array, so it’s already tokenized for us by the shell or runtime. The
classic shunting-yard algorithm solves this problem easily enough.
We have an output queue that goes into the compiler, and a token stack for
tracking &lt;code&gt;-a&lt;/code&gt;, &lt;code&gt;-o&lt;/code&gt;, &lt;code&gt;!&lt;/code&gt;, and &lt;code&gt;(&lt;/code&gt;. Then we walk &lt;code&gt;argv&lt;/code&gt; in order:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Actions go straight into the output queue.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If we see one of the special stack tokens we push it onto the stack, first popping operators with greater precedence into the queue, stopping at &lt;code&gt;(&lt;/code&gt;.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If we see &lt;code&gt;)&lt;/code&gt; we pop the stack into the output queue until we see &lt;code&gt;(&lt;/code&gt;.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When we’re out of tokens, pop the remaining stack into the queue. My parser synthesizes &lt;code&gt;-a&lt;/code&gt; where it’s implied, so the compiler always sees
logical AND. If the expression contains no &lt;code&gt;-exec&lt;/code&gt;, &lt;code&gt;-ok&lt;/code&gt;, or &lt;code&gt;-print&lt;/code&gt;,
after processing is complete the parser puts &lt;code&gt;-print&lt;/code&gt; then &lt;code&gt;-a&lt;/code&gt; into the
queue, which effectively wraps the whole expression in &lt;code&gt;( expr ) -print&lt;/code&gt;.
By clearing the stack first, the real expression is effectively wrapped in
parentheses, so no parenthesis tokens need to be synthesized.&lt;/p&gt;
    &lt;p&gt;I’ve used the shunting-yard algorithm many times before, so this part was easy. The new part was coming up with an algorithm to convert a series of postfix tokens into bytecode. My solution is the compiler maintains a stack of bytecode fragments. That is, each stack element is a sequence of one or more bytecode instructions. Branches use relative addresses, so they’re position-independent, and I can concatenate code fragments without any branch fix-ups. It takes the following actions from queue tokens:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;For an action token, create an &lt;code&gt;action&lt;/code&gt; instruction, and push it onto
the fragment stack as a new fragment.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;For a &lt;code&gt;!&lt;/code&gt; token, pop the top fragment, append a &lt;code&gt;not&lt;/code&gt; instruction, and
push it back onto the stack.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;For a &lt;code&gt;-a&lt;/code&gt; token, pop the top two fragments, join then with a &lt;code&gt;braf&lt;/code&gt; in
the middle which jumps just beyond the second fragment. That is, if the
first fragment evaluates to false, skip over the second fragment into
whatever follows.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;For a &lt;code&gt;-o&lt;/code&gt; token, just like &lt;code&gt;-a&lt;/code&gt; but use &lt;code&gt;brat&lt;/code&gt;. If the first fragment
is true, we skip over the second fragment.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If the expression is valid, at the end of this process the stack contains exactly one fragment. Append a &lt;code&gt;halt&lt;/code&gt; instruction to this fragment, and
that’s our program! If the final fragment contained a branch just beyond
its end, this &lt;code&gt;halt&lt;/code&gt; is that branch target. A few peephole optimizations
and could probably be an optimal program for this instruction set.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46391448</guid><pubDate>Fri, 26 Dec 2025 12:35:10 +0000</pubDate></item><item><title>ChatGPT conversations still lack timestamps after years of requests</title><link>https://community.openai.com/t/timestamps-for-chats-in-chatgpt/440107?page=3</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46391472</guid><pubDate>Fri, 26 Dec 2025 12:39:32 +0000</pubDate></item><item><title>Package managers keep using Git as a database, it never works out</title><link>https://nesbitt.io/2025/12/24/package-managers-keep-using-git-as-a-database.html</link><description>&lt;doc fingerprint="b324261b0df047d8"&gt;
  &lt;main&gt;
    &lt;p&gt;Using git as a database is a seductive idea. You get version history for free. Pull requests give you a review workflow. It’s distributed by design. GitHub will host it for free. Everyone already knows how to use it.&lt;/p&gt;
    &lt;p&gt;Package managers keep falling for this. And it keeps not working out.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cargo&lt;/head&gt;
    &lt;p&gt;The crates.io index started as a git repository. Every Cargo client cloned it. This worked fine when the registry was small, but the index kept growing. Users would see progress bars like “Resolving deltas: 74.01%, (64415/95919)” hanging for ages, the visible symptom of Cargo’s libgit2 library grinding through delta resolution on a repository with thousands of historic commits.&lt;/p&gt;
    &lt;p&gt;The problem was worst in CI. Stateless environments would download the full index, use a tiny fraction of it, and throw it away. Every build, every time.&lt;/p&gt;
    &lt;p&gt;RFC 2789 introduced a sparse HTTP protocol. Instead of cloning the whole index, Cargo now fetches files directly over HTTPS, downloading only the metadata for dependencies your project actually uses. (This is the “full index replication vs on-demand queries” tradeoff in action.) By April 2025, 99% of crates.io requests came from Cargo versions where sparse is the default. The git index still exists, still growing by thousands of commits per day, but most users never touch it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Homebrew&lt;/head&gt;
    &lt;p&gt;GitHub explicitly asked Homebrew to stop using shallow clones. Updating them was “an extremely expensive operation” due to the tree layout and traffic of homebrew-core and homebrew-cask.&lt;/p&gt;
    &lt;p&gt;Users were downloading 331MB just to unshallow homebrew-core. The .git folder approached 1GB on some machines. Every &lt;code&gt;brew update&lt;/code&gt; meant waiting for git to grind through delta resolution.&lt;/p&gt;
    &lt;p&gt;Homebrew 4.0.0 in February 2023 switched to JSON downloads for tap updates. The reasoning was blunt: “they are expensive to git fetch and git clone and GitHub would rather we didn’t do that… they are slow to git fetch and git clone and this provides a bad experience to end users.”&lt;/p&gt;
    &lt;p&gt;Auto-updates now run every 24 hours instead of every 5 minutes, and they’re much faster because there’s no git fetch involved.&lt;/p&gt;
    &lt;head rend="h2"&gt;CocoaPods&lt;/head&gt;
    &lt;p&gt;CocoaPods is the package manager for iOS and macOS development. It hit the limits hard. The Specs repo grew to hundreds of thousands of podspecs across a deeply nested directory structure. Cloning took minutes. Updating took minutes. CI time vanished into git operations.&lt;/p&gt;
    &lt;p&gt;GitHub imposed CPU rate limits. The culprit was shallow clones, which force GitHub’s servers to compute which objects the client already has. The team tried various band-aids: stopping auto-fetch on &lt;code&gt;pod install&lt;/code&gt;, converting shallow clones to full clones, sharding the repository.&lt;/p&gt;
    &lt;p&gt;The CocoaPods blog captured it well: “Git was invented at a time when ‘slow network’ and ‘no backups’ were legitimate design concerns. Running endless builds as part of continuous integration wasn’t commonplace.”&lt;/p&gt;
    &lt;p&gt;CocoaPods 1.8 gave up on git entirely for most users. A CDN became the default, serving podspec files directly over HTTP. The migration saved users about a gigabyte of disk space and made &lt;code&gt;pod install&lt;/code&gt; nearly instant for new setups.&lt;/p&gt;
    &lt;head rend="h2"&gt;Nixpkgs&lt;/head&gt;
    &lt;p&gt;Nixpkgs is currently stress-testing GitHub’s infrastructure. In November 2025, GitHub contacted the NixOS team about periodic maintenance jobs failing and causing “issues achieving consensus between replicas.” If unresolved, the repository could have become read-only.&lt;/p&gt;
    &lt;p&gt;The repository totals 83GB with half a million tree objects and 20,000 forks. A local clone is only 2.5GB — the rest is GitHub’s fork network storing every pull request branch and merge commit. The CI queries mergeability daily, creating new merge commits each time.&lt;/p&gt;
    &lt;p&gt;Unlike CocoaPods, Nixpkgs can’t easily move to a CDN. The Nix expressions are the package definitions, not metadata pointing elsewhere. Binary caches already serve built packages over HTTP, but nixpkgs itself remains a git repository — and it’s still growing.&lt;/p&gt;
    &lt;head rend="h2"&gt;vcpkg&lt;/head&gt;
    &lt;p&gt;vcpkg is Microsoft’s C++ package manager. It uses git tree hashes to version its ports, with the curated registry at github.com/Microsoft/vcpkg containing over 2,000 libraries.&lt;/p&gt;
    &lt;p&gt;The problem is that vcpkg needs to retrieve specific versions of ports by their git tree hash. When you specify a &lt;code&gt;builtin-baseline&lt;/code&gt; in your vcpkg.json (functioning like a lockfile for reproducible builds), vcpkg looks up historical commits to find the exact port versions you need. This only works if you have the full commit history.&lt;/p&gt;
    &lt;p&gt;Shallow clones break everything. GitHub Actions uses shallow clones by default. DevContainers shallow-clone vcpkg to save space. CI systems optimize for fast checkouts. All of these result in the same error: “vcpkg was cloned as a shallow repository… Try again with a full vcpkg clone.”&lt;/p&gt;
    &lt;p&gt;The workarounds are ugly. One proposed solution involves parsing vcpkg.json to extract the baseline hash, deriving the commit date, then fetching with &lt;code&gt;--shallow-since=&amp;lt;date&amp;gt;&lt;/code&gt;. Another suggests including twelve months of history, hoping projects upgrade before their baseline falls off the cliff. For GitHub Actions, you need &lt;code&gt;fetch-depth: 0&lt;/code&gt; in your checkout step, downloading the entire repository history just to resolve dependencies.&lt;/p&gt;
    &lt;p&gt;A vcpkg team member explained the fundamental constraint: “Port versions don’t use commit hashes, we use the git tree hash of the port directory. As far as I know, there is no way to deduce the commit that added a specific tree hash.” An in-product fix is infeasible. The architecture baked in git deeply enough that there’s no escape hatch.&lt;/p&gt;
    &lt;p&gt;Unlike Cargo, Homebrew, and CocoaPods, vcpkg hasn’t announced plans to move away from git registries. Custom registries must still be git repositories. The documentation describes filesystem registries as an alternative, but these require local or mounted paths rather than HTTP access. There’s no CDN, no sparse protocol, no HTTP-based solution on the horizon.&lt;/p&gt;
    &lt;head rend="h2"&gt;Go modules&lt;/head&gt;
    &lt;p&gt;Grab’s engineering team went from 18 minutes for &lt;code&gt;go get&lt;/code&gt; to 12 seconds after deploying a module proxy. That’s not a typo. Eighteen minutes down to twelve seconds.&lt;/p&gt;
    &lt;p&gt;The problem was that &lt;code&gt;go get&lt;/code&gt; needed to fetch each dependency’s source code just to read its go.mod file and resolve transitive dependencies. Cloning entire repositories to get a single file.&lt;/p&gt;
    &lt;p&gt;Go had security concerns too. The original design wanted to remove version control tools entirely because “these fragment the ecosystem: packages developed using Bazaar or Fossil, for example, are effectively unavailable to users who cannot or choose not to install these tools.” Beyond fragmentation, the Go team worried about security bugs in version control systems becoming security bugs in &lt;code&gt;go get&lt;/code&gt;. You’re not just importing code; you’re importing the attack surface of every VCS tool on the developer’s machine.&lt;/p&gt;
    &lt;p&gt;GOPROXY became the default in Go 1.13. The proxy serves source archives and go.mod files independently over HTTP. Go also introduced a checksum database (sumdb) that records cryptographic hashes of module contents. This protects against force pushes silently changing tagged releases, and ensures modules remain available even if the original repository is deleted.&lt;/p&gt;
    &lt;head rend="h2"&gt;Beyond package managers&lt;/head&gt;
    &lt;p&gt;The same pattern shows up wherever developers try to use git as a database.&lt;/p&gt;
    &lt;p&gt;Git-based wikis like Gollum (used by GitHub and GitLab) become “somewhat too slow to be usable” at scale. Browsing directory structure takes seconds per click. Loading pages takes longer. GitLab plans to move away from Gollum entirely.&lt;/p&gt;
    &lt;p&gt;Git-based CMS platforms like Decap hit GitHub’s API rate limits. A Decap project on GitHub scales to about 10,000 entries if you have a lot of collection relations. A new user with an empty cache makes a request per entry to populate it, burning through the 5,000 request limit quickly. If your site has lots of content or updates frequently, use a database instead.&lt;/p&gt;
    &lt;p&gt;Even GitOps tools that embrace git as a source of truth have to work around its limitations. ArgoCD’s repo server can run out of disk space cloning repositories. A single commit invalidates the cache for all applications in that repo. Large monorepos need special scaling considerations.&lt;/p&gt;
    &lt;head rend="h2"&gt;The pattern&lt;/head&gt;
    &lt;p&gt;The hosting problems are symptoms. The underlying issue is that git inherits filesystem limitations, and filesystems make terrible databases.&lt;/p&gt;
    &lt;p&gt;Directory limits. Directories with too many files become slow. CocoaPods had 16,000 pod directories in a single Specs folder, requiring huge tree objects and expensive computation. Their fix was hash-based sharding: split directories by the first few characters of a hashed name, so no single directory has too many entries. Git itself does this internally with its objects folder, splitting into 256 subdirectories. You’re reinventing B-trees, badly.&lt;/p&gt;
    &lt;p&gt;Case sensitivity. Git is case-sensitive, but macOS and Windows filesystems typically aren’t. Check out a repo containing both &lt;code&gt;File.txt&lt;/code&gt; and &lt;code&gt;file.txt&lt;/code&gt; on Windows, and the second overwrites the first. Azure DevOps had to add server-side enforcement to block pushes with case-conflicting paths.&lt;/p&gt;
    &lt;p&gt;Path length limits. Windows restricts paths to 260 characters, a constraint dating back to DOS. Git supports longer paths, but Git for Windows inherits the OS limitation. This is painful with deeply nested node_modules directories, where &lt;code&gt;git status&lt;/code&gt; fails with “Filename too long” errors.&lt;/p&gt;
    &lt;p&gt;Missing database features. Databases have CHECK constraints and UNIQUE constraints; git has nothing, so every package manager builds its own validation layer. Databases have locking; git doesn’t. Databases have indexes for queries like “all packages depending on X”; with git you either traverse every file or build your own index. Databases have migrations for schema changes; git has “rewrite history and force everyone to re-clone.”&lt;/p&gt;
    &lt;p&gt;The progression is predictable. Start with a flat directory of files. Hit filesystem limits. Implement sharding. Hit cross-platform issues. Build server-side enforcement. Build custom indexes. Eventually give up and use HTTP or an actual database. You’ve built a worse version of what databases already provide, spread across git hooks, CI pipelines, and bespoke tooling.&lt;/p&gt;
    &lt;p&gt;None of this means git is bad. Git excels at what it was designed for: distributed collaboration on source code, with branching, merging, and offline work. The problem is using it for something else entirely. Package registries need fast point queries for metadata. Git gives you a full-document sync protocol when you need a key-value lookup.&lt;/p&gt;
    &lt;p&gt;If you’re building a package manager and git-as-index seems appealing, look at Cargo, Homebrew, CocoaPods, vcpkg, Go. They all had to build workarounds as they grew, causing pain for users and maintainers. The pull request workflow is nice. The version history is nice. You will hit the same walls they did.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46391514</guid><pubDate>Fri, 26 Dec 2025 12:46:36 +0000</pubDate></item><item><title>LearnixOS</title><link>https://www.learnix-os.com</link><description>&lt;doc fingerprint="7fe43b9d48af5e9f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Learnix Operating System&lt;/head&gt;
    &lt;p&gt;"If you can't explain it simply, you don't understand it well enough." - Albert Einstein&lt;/p&gt;
    &lt;p&gt;Hello there!1&lt;/p&gt;
    &lt;p&gt;In this book we are going to write and learn about operating systems together!&lt;/p&gt;
    &lt;p&gt;We are going to implement an entire POSIX compliant OS in Rust and not use ANY2 external libraries. All of the thought process, code and implementations will be explained and documented here as well as in this repo which all the code snippets are from.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: ALL the syntax highlighting of the Rust code is custom and create by me! If you see and bug, please write in the comments or submit an issue.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Base Knowledge&lt;/head&gt;
    &lt;p&gt;This book will be technical, and will assume a little bit of a programming knowledge background, but not necessarily in rust&lt;/p&gt;
    &lt;p&gt;If you are not coming from a low level programming knowledge that's fine!&lt;/p&gt;
    &lt;p&gt;Just make sure you know this stuff or learn it as you read. Also if in any place on this book I take some things for granted, please, open an issue here and let me know so I could explain it better.&lt;/p&gt;
    &lt;p&gt;Some of the base knowledge that you would need to have:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Some assembly knowledge. (just understand simple movs, and arithmetic operations, at a very basic level3)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Some knowledge on memory. (what's a pointer, what's an address)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A knowledge in rust is not that important, but knowing at least one programming language is. I myself have some more learning in Rust, and in this book I will also explain some great features that it has!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A lot of motivation to learn and understand because it is a complex subject.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Roadmap of this book&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Compiling a stand alone binary&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Boot loading, Debugging, stages and some legacy stuff&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Important cpu modes and instructions&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Paging, writing out own malloc&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Utilizing the Interrupt Descriptor Table&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;File systems and Disk Drivers&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Thinking in terms of processes&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Writing a shell&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Running our first program! (Which off course will be Doom)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;To be continued (Hopefully virtualization section and loading a vm of other OS)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46391599</guid><pubDate>Fri, 26 Dec 2025 12:59:56 +0000</pubDate></item><item><title>Undefinable yet Indispensable</title><link>https://aeon.co/essays/the-word-religion-resists-definition-but-remains-necessary</link><description>&lt;doc fingerprint="fe63d99500e202bf"&gt;
  &lt;main&gt;
    &lt;p&gt;Listen to this essay&lt;/p&gt;
    &lt;p&gt;We tend to think of religion as an age-old feature of human existence. So it can be startling to learn that the very concept dates to the early modern era. Yes, you find gods, temples, sacrifices and rituals in the ancient Mediterranean, classical China, pre-Columbian Mesoamerica. What you don’t find is a term that quite maps onto ‘religion’.&lt;/p&gt;
    &lt;p&gt;What about the Romans, to whom we owe the word? Their notion of religio once meant something like scruples or exactingness, and then came to refer, among other things, to a scrupulous observance of rules or prohibitions, extending to worship practices. It was about doing the right thing in the right way. The Romans had other terms as well for customs, rites, obligations, reverence and social protocols, including cultus, ritus and superstitio. Yet they weren’t cordoned off into a realm that was separate from the workaday activities of public life, civic duty and family proprieties. What the Romans encountered abroad were, in their eyes, more or less eccentric versions of cultic life, rather than alien ‘religions’, in our sense. It was assumed that other localities would have other divinities; in times of war, you might even summon them, via evocatio, to try to get them to switch sides. But the local gods and rites of foreigners could be assessed without categorising them as instances of a single universal genus.&lt;/p&gt;
    &lt;p&gt;Even after the empire became officially Christian, you still don’t get our sense of ‘religions’. The Romans don’t start sorting the world into bounded systems analogous to ‘Christianity’, ‘Judaism’, ‘Manichaeism’, ‘Islam’ and so on. They have other, older sorting mechanisms, as Brent Nongbri elaborates in his terrific study Before Religion (2013). When Lactantius, in the 4th century, contrasts vera religio with falsae religiones, he means to distinguish right worship from wrong worship; he isn’t identifying other self-contained systems that might be lined up on a chart for comparison. The Christians of late antiquity didn’t view themselves as possessing one religion among many; they viewed themselves as possessing the truth.&lt;/p&gt;
    &lt;p&gt;To arrive at the modern category of religion, scholars now tend to think, you needed a complementary ‘secular’ sphere: a sphere that wasn’t, well, religious. That’s why the word’s modern, comparative sense wasn’t firmly established until the 17th century – Hugo Grotius’s De veritate religionis Christianae (1627) is one touchstone – at a time when European Christendom was both splintering and confronting unfamiliar worlds through exploration and conquest. Even as religion could be conceived as a special domain that might be isolated from law and politics, the traffic with ancient and non-European cultures forced reflection on what counted as ‘true religion’. It’s just that, when Europeans looked at India, Africa, China or the ancient Mediterranean, they sifted for Christian-like (and often Protestant-like) elements: a sacred text to anchor authority, a prophetic founder to narrate origins, a set of theological doctrines to sort out orthodoxy and heresy, and perhaps duties that offered a path to salvation. If a tradition didn’t provide these, scholars might helpfully supply them. In time, ‘world religions’ could be conjured up as bounded systems with creeds and essences, even when the local practices they subsumed were profoundly heterogeneous. Traditions with no founders were given founders; traditions with no single scripture were assigned canonical texts; diverse local rites were bundled into overarching systems.&lt;/p&gt;
    &lt;p&gt;As world religions took hold as a subject of academic study in the later 19th century, European scholars did their systematic best to treat disparate systems of practice and thought as members of a class. Buddhism became one test case. To call it a single ‘religion’, scholars first had to unify various practices of South, Central and East Asia, and then to decide whether a sometimes godless tradition could qualify. Such struggles over classification exposed a deeper uncertainty: how was ‘religion’ to be defined?&lt;/p&gt;
    &lt;p&gt;The great minds of the era had ideas. John Stuart Mill held that a religion must unite creed, sentiment and moral authority. Herbert Spencer thought that what religions shared was ‘the tacit conviction that the existence of the world with all it contains and all which surrounds it, is a mystery ever pressing for interpretation.’ The anthropologist Edward B Tylor proposed, as a minimum definition, ‘belief in spiritual beings’. The philologist Max Müller called religion a ‘mental faculty’, separate from ‘sense and reason’, by which humans apprehend the Infinite. For the Old Testament scholar and Orientalist William Robertson Smith, the true foundation of religious life was ritual – the binding force of collective acts. The sociologist Émile Durkheim’s own definition, in his classic The Elementary Forms of Religious Life (1912), joined belief to behaviour and belonging: religion, he wrote, was ‘a unified system of beliefs and practices relative to sacred things’ that united its adherents ‘into one moral community, called the Church’.&lt;/p&gt;
    &lt;p&gt;These definitions came up short because they excluded too much or included too much. Either they failed to net the fish you were after or they netted too much bycatch. Mill wanted creed, emotion and moral suasion in one package, but many traditions that Europeans encountered in the 19th century didn’t distribute those elements in anything like that pattern. Did a religion involve a metaphysical stance on the cosmos and our place within it – was it driven by the ever-pressing ontological mysteries that Spencer considered central? What we’d call ancient Judaism had very little of that; the biblical writers do not stand before the universe feeling compelled to develop a worldview; they stand within a covenantal drama, entwining law, story and communal identity. And then Müller’s definition could apply to a Romantic poet. (Wilhelm Müller, Max’s father, was a great one.) Dubious of belief-based accounts like Tylor’s, Robertson Smith had concluded that ‘the antique religions had for the most part no creed; they consisted entirely of institutions and practices,’ and ‘while the practice was rigorously fixed, the meaning attached to it was extremely vague.’ Robertson Smith’s own corrective faltered in the face of practices that were communal but not in any obvious way ‘sacred’, or traditions in which doctrine mattered intensely. Durkheim’s formula fatefully relied on a sharp division between sacred and profane that countless ethnographies would undermine.&lt;/p&gt;
    &lt;p&gt;Georg Simmel, writing around the turn of the 20th century, had already dismissed the ‘Open Sesame’ dream that a single word could unlock the mystery: ‘No light will ever be cast in the sibyllic twilight that, for us, surrounds the origin and nature of religion as long as we insist on approaching it as a single problem requiring only a single word for its solution.’ A few years later, William James complained about ‘verbal’ disputation, but then fell back on a recognisably Protestant formula, defining religion as ‘the feelings, acts, and experiences of individual men in their solitude, so far as they apprehend themselves to stand in relation to whatever they may consider the divine.’ The linguist Jane Ellen Harrison, in her study Themis (1912), refused to define religion at all: a definition, she said, ‘desiccates its object’.&lt;/p&gt;
    &lt;p&gt;In ‘traditional religions’, there’s a continuity between what we’d distinguish as the natural and the supernatural realm&lt;/p&gt;
    &lt;p&gt;In the decades that followed, followers of Durkheim foregrounded function, treating religion as a mechanism that bound together societies, comforted individuals, marked transitions, legitimised power. But saying what religion does wouldn’t necessarily tell you what religion was, and, anyway, these functions weren’t peculiar to religion. Clifford Geertz’s elegant formula from the 1960s cast religion as a ‘system of symbols’, one that establishes ‘powerful, pervasive, and long-lasting moods and motivations’. Yet this formula likewise went too big, opening the door to all sorts of political ideologies.&lt;/p&gt;
    &lt;p&gt;Evolutionary and cognitive theorists since have offered definitions of their own. The evolutionary psychologist Robin Dunbar, for instance, suggested that religion may amount to ‘belief in some kind of transcendental world … inhabited by spirit beings or forces (that may or may not take an interest in and influence the physical world …).’ Inevitably, these belief-oriented accounts run into the same complaints that earlier doxastic definitions had: they seem awfully Protestant, privileging inner conviction over outward form. Even if you bought into the ‘belief’ part, though, you could baulk at the ‘transcendental’ part. In many ‘traditional religions’, there’s a deep continuity between what we’d distinguish as the natural and the supernatural realm. In the Akan region of Ghana where I spent much of my childhood, people would appease or reproach their ancestors in the same spirit that they might wheedle or berate someone at a municipal office. As the anthropologist Robin Horton observed, so-called traditional religions are less like the Western notion of religion than they are like science: they aim at explanation, prediction and control. True, where science posited impersonal forces, traditional thought posited personal ones. But the underlying move from observed regularities to theoretical constructs was similar; what Europeans wanted to call religion was a pragmatic explanatory framework, reasonable given the available evidence, and part of the same conceptual space as folk biology, folk psychology and everyday causal reasoning.&lt;/p&gt;
    &lt;p&gt;By the late 20th century, hopes for a definition had faded. Some theorists turned to Ludwig Wittgenstein’s notion of ‘family resemblance’. The thought is that traditions can belong to the same conceptual family because they overlap in crisscrossing ways – like cousins who share a nose here, a chin there, without any feature that they all have in common. It’s a permissive approach: you map the ripple of resemblances and give up on strict boundaries. Unfortunately, those resemblances always depend on what you pick as your prototype. If you start with Protestant Christianity, you’ll find resemblances that matter to Protestants; begin instead with Yoruba orisha devotion, and you’ll trace a very different set of likenesses.&lt;/p&gt;
    &lt;p&gt;The anthropologist Talal Asad influentially and illuminatingly traced both ‘religion’ and ‘the secular’ to the political and intellectual habits of Western modernity. Yet in his account, religion sometimes seems more an effect of those forces than a cause, more a product of power rather than a power in itself. And even if you think that the phenomena we cluster under the term have been sorted and named by Western modernity, you could wonder how we could be sure that they’re examples of the same thing.&lt;/p&gt;
    &lt;p&gt;Was the category beyond redemption? The scholar and minister Wilfred Cantwell Smith, whose book The Meaning and End of Religion (1962) had meticulously detailed the belated emergence of the ‘religion’ concept in Europe, long maintained that talk of ‘religion’ conflated too many things not to cause mischief, and urged that we give up such talk altogether; we should, instead, speak of faith and ‘cumulative tradition’. The anthropologist and historian Daniel Dubuisson, who anathematised ‘religion’ as a 19th-century Western imposition on non-Western worlds, urged that it be replaced with ‘cosmographic formation’. These evasive manoeuvres, in turn, have met with scepticism. As the social theorist Martin Riesebrodt drily observed, neologisms like Dubuisson’s could doubtless be shown to ‘have also been “constructed” through historically specific discourses’ and revealed as ‘instruments in the linguistic battle between classes or cultures.’ Besides, he pointed out, those who would eliminate the term ‘religion’ seldom manage long without it.&lt;/p&gt;
    &lt;p&gt;So how has ‘religion’, as a concept and category, endured in the absence of a stable definition? To answer that question, it may help to think about how referring expressions do their referring. Some terms keep their grip on the world even as our understanding of what they denote changes radically; others, once central to serious thought, fall away when their supposed referents are deemed illusions. What distinguishes the survivors from the casualties?&lt;/p&gt;
    &lt;p&gt;Think about our names for ‘natural kinds’. These are meant to pick out groupings that are found not just in our heads but in nature: bosons, barium, bonobos, beech trees. The things these names designate are thought to have causal powers, explanatory roles or underlying properties that justify treating them as more than convenient fictions. When we name a natural kind, what we’re naming is really out there in the world. Anyway, that’s the aim. How do we decide when we’ve got it right?&lt;/p&gt;
    &lt;p&gt;Start with chemistry, and the question of what counts as an acid. When the term was first used, it referred simply to substances that tasted sour, or acidus. Later they were marked out by what they did: etching metal, losing their bite in contact with alkalis. In 1777, the French chemist Antoine Lavoisier was convinced that acidity came from a common ingredient he called oxygen – oxygène, the ‘acid-producer’. He was wrong. Yet we’d say that when Lavoisier spoke of acids, he was referring to the same class of things we mean by the word.&lt;/p&gt;
    &lt;p&gt;A century on, chemists refined the concept. Svante Arrhenius defined acids by their propensity to dissociate in water and release hydrogen ions; in 1923, Johannes Nicolaus Brønsted and Thomas Martin Lowry each reconceived them as proton donors; Gilbert Lewis broadened the net again by calling acids electron-pair acceptors. Each shift expanded the boundaries, but none made the term obsolete. The word survived because its targets – the substances doing the dissolving and reacting – were real enough to anchor it even as its theoretical profile changed.&lt;/p&gt;
    &lt;p&gt;It’s the difference between a bad map of a real country and a map of Atlantis. Only the first can be fixed&lt;/p&gt;
    &lt;p&gt;Not every scientific term has been so lucky. In 1774, Joseph Priestley isolated a gas he took to be ‘dephlogisticated air’. Phlogiston was supposed to be a substance released during combustion, the invisible essence of burning. What he had actually found, we’d say, was what we know as oxygen, the name derived from that discarded theory of Lavoisier’s. Unlike oxygen, nothing in the world behaved as phlogiston was said to behave. Indeed, it was Lavoisier who brought the curtain down on phlogiston; closed-system experiments, which he conducted with his wife and lab assistant Marie-Anne Paulze Lavoisier, showed that combustion involved the gain of a component of air (namely, oxygen) rather than the loss of an invisible essence. The phlogiston concept evaporated because chemists came to see that it referred to nothing at all. Priestley’s ‘dephlogisticated air’, by contrast, referred successfully despite being misdescribed: his experiments had latched on to a real thing, even if his theory of it was wrong.&lt;/p&gt;
    &lt;p&gt;This difference between a term that refers despite error and one that refers to nothing is the difference between a bad map of a real country and a map of Atlantis. Only the first can be fixed. Philosophers have used such cases to argue that successful reference doesn’t depend on getting the description right. What matters is the causal connection between our words and the things they’re meant to denote. The strategy is straightforward enough: if you want to know what object a word refers to, find the thing that gives the best causal explanation of the central features of uses of that word. The features that drove Lavoisier’s acid-talk were produced by substances we still recognise as acids, which is why we don’t treat him as having been talking about some other thing, or about nothing at all. Causal theories of reference explain why our words can target the same class of object even when our conception of it shifts, and when the boundaries of the class shift, too. Pluto can stop being a planet without shaking the foundations of ‘planet’ talk. In such theories of reference, a word continues to refer, so long as it stands in the right causal relation to the entity that gives rise to its use. Misdescribed objects can survive conceptual upheavals; nonexistent ones can’t.&lt;/p&gt;
    &lt;p&gt;Even in the natural sciences, though, classes of things can fall between those stools. ‘Luminiferous ether’ is a case in point: an invisible medium once thought to carry light waves, it was indispensable to 19th-century physics yet eventually dissolved into what came to be called electromagnetic fields. Was ‘ether’ simply a phantasm? Some philosophers think we could well have retained the term, redefining it to mean the very fields that replaced it. Albert Einstein himself, who once helped kill the ether idea, later repurposed the term as the relativistic ether of spacetime, a field with its own geometry. Other theorists suspect that our ‘electromagnetic fields’ may eventually go the way of ether.&lt;/p&gt;
    &lt;p&gt;If there can be uncertainty about objects within the natural sciences, the wicket gets stickier when we move into the historical and social realm. Here the things we name – revolutions, nations, money, marriage, religion – are doubly human products, being products first of our collective activity, then of our collective description. These entities are what the philosopher Sally Haslanger would call ‘socially founded’ (a term she uses to sidestep the confusions associated with ‘socially constructed’). Many philosophers of language now call such entities social kinds.&lt;/p&gt;
    &lt;p&gt;To approach religion as a social kind isn’t to say that it’s as referentially sound as other familiar examples of this sort. Religion may, in fact, be in worse shape than most. It belongs to that subcategory of social kinds that living people apply to themselves. Some social kinds, like ‘recession’, can be defined externally, without the participation of those they describe. Economists can declare one to have happened in the 1870s, even if no one at the time felt it by that name. Others, like ‘wedding’, depend on shared recognition: you cannot hold one without a community that believes in weddings. ‘Religion’, like many social kinds, functions in both ways. Anthropologists can use the term to describe practices that their participants would never call religions, yet, once the label circulates, it acquires a reflexive power: believers come to organise their self-understanding around it. In this respect, religion is a product of classification that helps to shape the reality it describes.&lt;/p&gt;
    &lt;p&gt;The philosopher Ian Hacking captured this feedback loop with his idea of dynamic nominalism – the process by which classifications and people classified reshape one another. Categories create kinds. The heavy drinker is seen, and sees himself, as an alcoholic. The word doesn’t merely label the phenomenon – it helps to constitute it. Hacking later preferred to call this ‘dialectical realism’, on the grounds that what emerges from the loop (labels affecting those labelled, which then affects the label) is, by any reasonable measure, real enough. When you’ve been told that what you have is a religion, what’s affected isn’t just how you relate to it but what you think you are.&lt;/p&gt;
    &lt;p&gt;If ‘religion’ endures, it’s because the word still does work, practical and theoretical&lt;/p&gt;
    &lt;p&gt;Where does this leave someone trying to understand human life through such refractory terms? We might concede that ‘religion’ resists a unitary meaning and proceed case by case, choosing the angle that best reveals what we need to make visible. When speaking of the Abrahamic faiths, a practice-centred approach may capture the lived textures of ritual and observance. The propositions of the Nicene or the Athanasian Creed are, after all, obscure and arguably incoherent, but the act of avowing them carries weighty significance. When we’re turning to the ‘traditional’ thought of the Azande, the Nuer or the Asante, by contrast, a belief-centred, even neo-Tylorian, lens may illuminate elements that the modern Christian model hides from view. Each emphasis is bound to clarify something that the other leaves obscure.&lt;/p&gt;
    &lt;p&gt;The larger truth is that we’ve always navigated the world with models that merely approximate it, with varying degrees of adequacy. As Hans Vaihinger argued in The Philosophy of ‘As If’ (1911), we often reason through fictions we judge ‘true enough’, because making use of them helps us act, anticipate and understand. The map may not be the territory, but we’d be lost without it. And the sciences, social and natural alike, advance through such tolerable falsehoods. Their worth lies in the utility of their results.&lt;/p&gt;
    &lt;p&gt;If ‘religion’ endures, it’s because the word still does work, practical and theoretical. It orders law and policy, directs research, and shapes the inner lives of those who use it. Sociologists can enquire into its relation to charity or suicide; psychologists can study its connection to prejudice or wellbeing. In the United States, legislators and judges must have a sufficient grasp of the category that they can balance the Constitutional dos and don’ts of ‘accommodation’ and ‘non-establishment’. For the religionist, meanwhile, it continues to name a space where meaning is made, defended or denied. Whatever else it may be, ‘religion’ remains a category with too many stakeholders to be fired by fiat. When it comes to what the word means, no one gets to say, and everyone gets a say.&lt;/p&gt;
    &lt;p&gt;Of course, scholarship itself requires observance – with respect to its own standards of evidence, and on the discipline of paying attention. To be observant, in this sense, is to watch the world closely without pretending to stand outside it. And so we try to use our terms with care, aware of what they can hide from sight and of how much they still let us see. We begin where we are, with the tools our history leaves us, and we make do, even if we suspect that our models may someday be replaced. For now, religion endures as a shared act of attention: one of those serviceable maps by which we try to find our bearings, and to keep faith with the world.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46391744</guid><pubDate>Fri, 26 Dec 2025 13:23:27 +0000</pubDate></item><item><title>Rob Pike Goes Nuclear over GenAI</title><link>https://skyview.social/?url=https%3A%2F%2Fbsky.app%2Fprofile%2Frobpike.io%2Fpost%2F3matwg6w3ic2s&amp;viewtype=tree</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46392115</guid><pubDate>Fri, 26 Dec 2025 14:08:47 +0000</pubDate></item><item><title>Steve wants us to make the Macintosh boot faster</title><link>https://www.folklore.org/Saving_Lives.html</link><description>&lt;doc fingerprint="7d8b79acf9f6083d"&gt;
  &lt;main&gt;
    &lt;p&gt;We always thought of the Macintosh as a fast computer, since its 68000 microprocessor was effectively 10 times faster than an Apple II, but our Achilles heel was the floppy disk. We had limited RAM, so it was often necessary to load data from the floppy, but there we were no faster than an Apple II. Once we had real applications going, it was clear the floppy disk was going to be a significant bottleneck.&lt;/p&gt;
    &lt;p&gt;One of the things that bothered Steve Jobs the most was the time that it took to boot when the Mac was first powered on. It could take a couple of minutes, or even more, to test memory, initialize the operating system, and load the Finder. One afternoon, Steve came up with an original way to motivate us to make it faster.&lt;/p&gt;
    &lt;p&gt;Larry Kenyon was the engineer working on the disk driver and file system. Steve came into his cubicle and started to exhort him. "The Macintosh boots too slowly. You've got to make it faster!"&lt;/p&gt;
    &lt;p&gt;Larry started to explain about some of the places where he thought that he could improve things, but Steve wasn't interested. He continued, "You know, I've been thinking about it. How many people are going to be using the Macintosh? A million? No, more than that. In a few years, I bet five million people will be booting up their Macintoshes at least once a day."&lt;/p&gt;
    &lt;p&gt;"Well, let's say you can shave 10 seconds off of the boot time. Multiply that by five million users and thats 50 million seconds, every single day. Over a year, that's probably dozens of lifetimes. So if you make it boot ten seconds faster, you've saved a dozen lives. That's really worth it, don't you think?"&lt;/p&gt;
    &lt;p&gt;We were pretty motivated to make the software go as fast as we could anyway, so I'm not sure if this pitch had much effect, but we thought it was pretty humorous, and we did manage to shave more than ten seconds off the boot time over the next couple of months.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46392538</guid><pubDate>Fri, 26 Dec 2025 14:52:13 +0000</pubDate></item><item><title>High School Student Discovers 1.5M Potential New Astronomical Objects</title><link>https://www.smithsonianmag.com/smart-news/high-school-student-discovers-1-5-million-potential-new-astronomical-objects-by-developing-an-ai-algorithm-180986429/</link><description>&lt;doc fingerprint="ad5111b84a980758"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;High School Student Discovers 1.5 Million Potential New Astronomical Objects by Developing an A.I. Algorithm&lt;/head&gt;
    &lt;head rend="h2"&gt;The 18-year-old won $250,000 for training a machine learning model to analyze understudied data from NASA’s retired NEOWISE telescope&lt;/head&gt;
    &lt;p&gt;In a leap forward for astronomy, a researcher has developed an artificial intelligence algorithm and discovered more than one million objects in space by parsing through understudied data from a NASA telescope.&lt;/p&gt;
    &lt;p&gt;The breakthrough is detailed in a study published in November in The Astronomical Journal. What the study doesn’t detail, however, is that the paper’s sole author is 18 years old.&lt;/p&gt;
    &lt;p&gt;Matteo Paz from Pasadena, California, recently won the first place prize of $250,000 in the 2025 Regeneron Science Talent Search for combining machine learning with astronomy. Self-described as the nation’s “oldest and most prestigious science and math competition for high school seniors,” the contest recognized Paz for developing his A.I. algorithm. The young scientist’s tool processed 200 billion data entries from NASA’s now-retired Near-Earth Object Wide-field Infrared Survey Explorer (NEOWISE) telescope. His model revealed 1.5 million previously unknown potential celestial bodies.&lt;/p&gt;
    &lt;p&gt;“I was just happy to have had the privilege. Not only placing in the top ten, but winning first place, came as a visceral surprise,” the teenager tells Forbes’ Kevin Anderton. “It still hasn’t fully sunk in.”&lt;/p&gt;
    &lt;p&gt;Paz’s interest in astronomy turned into real research when he participated in the Planet Finder Academy at the California Institute of Technology (Caltech) in summer 2022. There, he studied astronomy and computer science under the guidance of his mentor, Davy Kirkpatrick, an astronomer and senior scientist at the university’s Infrared Processing and Analysis Center (IPAC).&lt;/p&gt;
    &lt;p&gt;Kirkpatrick had been working with data from the NEOWISE infrared telescope, which NASA launched in 2009 with the aim of searching for near-Earth asteroids and comets. The telescope’s survey, however, also collected data on the shifting heat of variable objects: rare phenomena that emit flashing, changing or otherwise dynamic light, such as exploding stars. It was Kirkpatrick’s idea to look for these elusive objects in NEOWISE’s understudied data.&lt;/p&gt;
    &lt;p&gt;“At that point, we were creeping up towards 200 billion rows in the table of every single [NEOWISE] detection that we had made over the course of over a decade,” Kirkpatrick explains in a Caltech statement. “So, my idea for the summer was to take a little piece of the sky and see if we could find some variable stars. Then we could highlight those to the astronomic community, saying, ‘Here’s some new stuff we discovered by hand; just imagine what the potential is in the dataset.’”&lt;/p&gt;
    &lt;p&gt;Paz, however, had no intention of doing it by hand. Instead, he worked on an A.I. model that sorted through the raw data in search of tiny changes in infrared radiation, which could indicate the presence of variable objects. Paz and Kirkpatrick continued working together after the summer to perfect the model, which ultimately flagged 1.5 million potential new objects, including supernovas and black holes.&lt;/p&gt;
    &lt;p&gt;“Prior to Matteo’s work, no one had tried to use the entire (200-billion-row) table to identify and classify all of the significant variability that was there,” Kirkpatrick tells Business Insider’s Morgan McFall-Johnsen in an email. He adds that Caltech researchers are already making use of Paz’s catalog of potential variable objects, called VarWISE, to study binary star systems.&lt;/p&gt;
    &lt;p&gt;“The variable candidates that he’s uncovered will be widely studied,” says Amy Mainzer, NEOWISE’s principal investigator for NASA, to Business Insider.&lt;/p&gt;
    &lt;p&gt;As for the A.I. model, Paz explains that it might be applicable to “anything else that comes in a temporal format,” such as stock market chart analysis and atmospheric effects like pollution, according to the statement. It’s no surprise the teenager is interested in the climate—as fires burned in L.A. earlier this year, the Eaton Fire forced him and his family to evacuate their home, Forbes reports.&lt;/p&gt;
    &lt;p&gt;Other teenage scientists recognized by the contest studied mosquito control, drug-resistant fungus, the human genome and mathematics.&lt;/p&gt;
    &lt;p&gt;“The remarkable creativity and dedication of these students bring renewed hope for our future,” Maya Ajmera, president of the Society for Science, which oversees the award, says in a statement. “Driven by their ingenuity, these young scientists are developing groundbreaking solutions that have the potential to transform our world and propel society forward.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46392815</guid><pubDate>Fri, 26 Dec 2025 15:13:21 +0000</pubDate></item></channel></rss>