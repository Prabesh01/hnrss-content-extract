<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 30 Nov 2025 04:25:49 +0000</lastBuildDate><item><title>The CRDT Dictionary: A Field Guide to Conflict-Free Replicated Data Types</title><link>https://www.iankduncan.com/engineering/2025-11-27-crdt-dictionary/</link><description>&lt;doc fingerprint="5f77706e84b8bed5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The CRDT Dictionary: A Field Guide to Conflict-Free Replicated Data Types&lt;/head&gt;
    &lt;p&gt;Back around 2014, I kept hearing about this cool database called Riak, a distributed database that could survive network partitions and keep accepting writes. Some really interesting companies were using it at massive scale, and I was curious about it. One of the big selling points was that it could handle concurrent writes without any coordination or consensus. I was intrigued, and I started reading about it. Underlying all of this was the concept of CRDTs, Conflict-free Replicated Data Types.1&lt;/p&gt;
    &lt;p&gt;At the time, I was working on a beer startup called Brewtown with a friend: a beer review social site and delivery subscription service. It failed for other reasons, but I was a little too enamored with shiny tech back then, and CRDTs and Riak fit the bill for shiny tech. I kept trying to find excuses to shoehorn CRDT stuff into our codebase when, honestly, we didn’t need any of it. Postgres would’ve been fine. Live and learn.&lt;/p&gt;
    &lt;p&gt;Anyways, the idea sounded like pure sorcery: data structures that replicate across nodes and merge deterministically, without coordination, without losing information. I got excited, read a few papers, played with some toy implementations… and then we gave up on the beer startup. I didn’t really have a reason to mess with CRDTs for a while.&lt;/p&gt;
    &lt;p&gt;Fast forward to 2025, I’ve just had Thanksgiving dinner, and I’m curious again. What’s the state of the art? What have I forgotten? Which CRDT should I reach for when? So I’m writing this, both as a refresher for myself and a reference for the next time I need to remember why OR-Sets exist or what WOOT stands for. (“WithOut Operational Transformation.” Yes, really.2)&lt;/p&gt;
    &lt;p&gt;So, grab a coffee.&lt;/p&gt;
    &lt;p&gt;Commutative. Replicated. Data Types.&lt;/p&gt;
    &lt;p&gt;In isolation, all of the words make sense. But when you look at the literature, it’s overwhelming:&lt;/p&gt;
    &lt;p&gt;Suddenly you’ve moved beyond the simple terms and start seeing things like G-Counters, PN-Counters, LWW-Sets, OR-Sets, 2P-Sets, RGAs, WOOTs, Logoots (wtf?)… Each with subtle tradeoffs. Each paper assuming you’ve read the previous five. It’s overwhelming.&lt;/p&gt;
    &lt;p&gt;This guide will hopefully cut through that. We’ll build intuition through interactive demos and concrete examples. You’ll see how merges actually work, watch conflicts resolve (or not resolve), and develop a feel for which CRDT fits which problem.&lt;/p&gt;
    &lt;head rend="h2"&gt;What You Need to Know&lt;/head&gt;
    &lt;p&gt;You don’t need a PhD in distributed systems. If you understand:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Why network failures happen&lt;/item&gt;
      &lt;item&gt;What “eventual consistency” means&lt;/item&gt;
      &lt;item&gt;Basic set operations (union, intersection)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;…you’re good.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Problem&lt;/head&gt;
    &lt;p&gt;Picture this: Alice and Bob are both editing a shared counter. Alice increments it. Bob increments it. The network is flaky, so neither sees the other’s change immediately. Later, they reconnect. What should the counter show?&lt;/p&gt;
    &lt;p&gt;Option 1: Consensus - Use Paxos/Raft to agree on who went first. Works great! Until the network partitions and half your users can’t write because they can’t reach a quorum. Not ideal for offline-first apps.&lt;/p&gt;
    &lt;p&gt;Option 2: Last-Write-Wins - Use timestamps. Whoever wrote last “wins.” Easy to implement! Except Bob’s increment gets completely erased if Alice’s timestamp was later. Data loss.&lt;/p&gt;
    &lt;p&gt;Option 3: CRDTs - Design the data structure so that merging is deterministic. Both increments survive. No coordination needed. No data loss. However, you have to be okay with some level of eventual consistency.&lt;/p&gt;
    &lt;p&gt;What’s the trick? How do CRDTs achieve this?&lt;/p&gt;
    &lt;p&gt;Roughly speaking, you are working with a CRDT if your merge operation is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;commutative (order doesn’t matter)&lt;/item&gt;
      &lt;item&gt;associative (grouping doesn’t matter)&lt;/item&gt;
      &lt;item&gt;idempotent (duplicates don’t matter)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once you achieve these properties, then you can use your merge operation to ensure that replicas automatically converge to the same state.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Quick Detour: Lattices and Why They Matter&lt;/head&gt;
    &lt;p&gt;Before we dive into specific CRDTs, let’s build some intuition about what makes merging work. In CRDT literature, this is often referred to as a “lattice”.&lt;/p&gt;
    &lt;p&gt;Think about natural numbers with &lt;code&gt;max&lt;/code&gt; as the merge operation. If you have &lt;code&gt;3&lt;/code&gt; and &lt;code&gt;5&lt;/code&gt;, taking &lt;code&gt;max(3, 5) = 5&lt;/code&gt; makes sense. It doesn’t matter if you compute &lt;code&gt;max(3, max(5, 7))&lt;/code&gt; or &lt;code&gt;max(max(3, 5), 7)&lt;/code&gt; - you get &lt;code&gt;7&lt;/code&gt; either way. And &lt;code&gt;max(5, 5) = 5&lt;/code&gt;, so duplicates are harmless.&lt;/p&gt;
    &lt;p&gt;This forms a partial order: some values are “greater than” others (&lt;code&gt;5 &amp;gt; 3&lt;/code&gt;), and there’s a join operation (&lt;code&gt;max&lt;/code&gt;) that gives you the least upper bound. The fancy math term is “join-semilattice,” but think of it as: a way to consistently pick “more recent” or “more complete” information.&lt;/p&gt;
    &lt;p&gt;Here’s the key insight: if your data structure’s states form a lattice, and updates only move “upward” in the ordering, then:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You can apply updates in any order&lt;/item&gt;
      &lt;item&gt;You can apply the same update twice&lt;/item&gt;
      &lt;item&gt;Eventually, everyone agrees on the maximum state&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Consider a counter where each replica tracks its own count: &lt;code&gt;{A: 3, B: 5}&lt;/code&gt;. The partial order is pointwise: &lt;code&gt;{A: 3, B: 5} ≥ {A: 2, B: 5}&lt;/code&gt; because each component is greater-or-equal. To join, take the &lt;code&gt;max&lt;/code&gt; of each component. This is exactly how the G-Counter CRDT works!&lt;/p&gt;
    &lt;p&gt;Why does this matter? Because if you can design your data structure so that:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;States form a lattice (there’s always a sensible “join”)&lt;/item&gt;
      &lt;item&gt;Operations only move upward (you can’t un-increment a counter)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Then merging becomes trivial: just take the join. No coordination needed. No conflicts possible. The math guarantees convergence.&lt;/p&gt;
    &lt;p&gt;Not all CRDTs fit this clean model (some need timestamps or version vectors to determine what’s “greater”), but the lattice intuition often guides the design. When you see &lt;code&gt;merge = unionWith max&lt;/code&gt; or &lt;code&gt;merge = union&lt;/code&gt;, you’re seeing some pure, beautiful math-brained lattice thinking.&lt;/p&gt;
    &lt;head rend="h3"&gt;State-Based vs Operation-Based&lt;/head&gt;
    &lt;p&gt;Moving on…&lt;/p&gt;
    &lt;p&gt;There are two fundamental approaches to CRDTs:&lt;/p&gt;
    &lt;p&gt;State-based CRDTs (CvRDTs) send the entire state to other replicas, which merge it with their local state using a join operation. The state must form a join-semilattice.3&lt;/p&gt;
    &lt;p&gt;Operation-based CRDTs (CmRDTs) send operations to other replicas, which apply them to their local state. Operations must be commutative when applied concurrently.4&lt;/p&gt;
    &lt;p&gt;In this guide, we’ll primarily discuss state-based CRDTs, as they’re conceptually simpler and the ideas translate naturally to the operation-based variants.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Core Laws&lt;/head&gt;
    &lt;p&gt;For a data structure to be a state-based CRDT, its merge operation must satisfy:&lt;/p&gt;
    &lt;p&gt;Associativity: &lt;code&gt;(a ⊔ b) ⊔ c = a ⊔ (b ⊔ c)&lt;/code&gt; where &lt;code&gt;⊔&lt;/code&gt; denotes the merge/join operation&lt;/p&gt;
    &lt;p&gt;Commutativity: &lt;code&gt;a ⊔ b = b ⊔ a&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Idempotence: &lt;code&gt;a ⊔ a = a&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;These properties ensure that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Merging in any order produces the same result&lt;/item&gt;
      &lt;item&gt;Re-receiving the same state is harmless&lt;/item&gt;
      &lt;item&gt;Partial merges can be composed&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Additionally, the state must form a monotonic semilattice: updates only move “upward” in the partial order, never downward. This ensures convergence: once all updates have been delivered, all replicas reach the same state.&lt;/p&gt;
    &lt;p&gt;For the curious, The symbol ⊔ is called (square cup) or square union. I have no idea why regular union symbol isn’t used. Pointy-headed researchers, I guess.&lt;/p&gt;
    &lt;p&gt;Anyways, it’s commonly used to denote:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Disjoint union - union of sets treated as disjoint&lt;/item&gt;
      &lt;item&gt;Join operation in lattice theory - the least upper bound (supremum) of two elements&lt;/item&gt;
      &lt;item&gt;Merge operation in CRDTs - combining two states by taking their least upper bound&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With these foundations in place, let’s explore the CRDT zoo.&lt;/p&gt;
    &lt;head rend="h2"&gt;G-Counter: Grow-Only Counter&lt;/head&gt;
    &lt;p&gt;Let’s start with the simplest CRDT: a counter that only goes up.5&lt;/p&gt;
    &lt;head rend="h3"&gt;The Idea&lt;/head&gt;
    &lt;p&gt;Instead of storing one global count, each replica tracks its own count. The total is the sum of all replica counts. When replicas merge, they take the &lt;code&gt;max&lt;/code&gt; of each replica’s count.&lt;/p&gt;
    &lt;p&gt;Why &lt;code&gt;max&lt;/code&gt;? Because counts only increase. If replica A shows that replica B has counted to 5, and replica B shows it’s counted to 3, we know A has seen newer information. Taking the max ensures we never lose increments.6&lt;/p&gt;
    &lt;head rend="h3"&gt;Implementation&lt;/head&gt;
    &lt;code&gt;type GCounter = Map ReplicaId Nat

value :: GCounter -&amp;gt; Nat
value counter = sum (Map.elems counter)

increment :: ReplicaId -&amp;gt; GCounter -&amp;gt; GCounter
increment r counter = Map.insertWith (+) r 1 counter

merge :: GCounter -&amp;gt; GCounter -&amp;gt; GCounter
merge = Map.unionWith max&lt;/code&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;The merge operation forms a join-semilattice where the partial order is defined pointwise: &lt;code&gt;c1 ≤ c2&lt;/code&gt; if for all replicas &lt;code&gt;r&lt;/code&gt;, &lt;code&gt;c1[r] ≤ c2[r]&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Associative: &lt;code&gt;max&lt;/code&gt;is associative&lt;/item&gt;
      &lt;item&gt;Commutative: &lt;code&gt;max&lt;/code&gt;is commutative&lt;/item&gt;
      &lt;item&gt;Idempotent: &lt;code&gt;max(x, x) = x&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Monotonic: Each replica’s count only increases&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Intuition&lt;/head&gt;
    &lt;p&gt;Think of each replica as having its own tally marks. When replicas sync, they each adopt the maximum tally for each replica they’ve seen. Since tallies only grow, taking the maximum ensures we never lose increments.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simple and efficient&lt;/item&gt;
      &lt;item&gt;No metadata overhead beyond replica counts&lt;/item&gt;
      &lt;item&gt;Perfect for increment-only scenarios (page views, likes, etc.)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cannot decrement&lt;/item&gt;
      &lt;item&gt;Size grows with number of replicas (though typically small)&lt;/item&gt;
      &lt;item&gt;No garbage collection (all replica counts retained forever)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use G-Counter when you need to count upward-only events in a distributed system: analytics counters, like counts, view counts, or any monotonically increasing metric. (If you need to decrement, well… keep reading.)&lt;/p&gt;
    &lt;head rend="h3"&gt;Interactive Demo&lt;/head&gt;
    &lt;p&gt;Try it yourself! Increment counters on different replicas and see how the merge operation works:&lt;/p&gt;
    &lt;head rend="h3"&gt;G-Counter: Grow-Only Counter&lt;/head&gt;
    &lt;p&gt;Try it: Click "Increment" on different replicas multiple times without merging. Each replica tracks its own count independently. Then click the merge buttons (← A, ← B, ← C) to sync state. Watch how the merge uses &lt;code&gt;max&lt;/code&gt; - you never lose increments, and all replicas eventually reach the same total.&lt;/p&gt;
    &lt;head rend="h2"&gt;PN-Counter: Positive-Negative Counter&lt;/head&gt;
    &lt;p&gt;What if we need to decrement? Enter the PN-Counter. The trick is beautifully simple.&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;A PN-Counter contains two G-Counters: one for increments, one for decrements:&lt;/p&gt;
    &lt;code&gt;data PNCounter = PNCounter
  { increments :: GCounter
  , decrements :: GCounter
  }&lt;/code&gt;
    &lt;p&gt;The value is the difference:&lt;/p&gt;
    &lt;code&gt;value :: PNCounter -&amp;gt; Int
value (PNCounter inc dec) = value inc - value dec&lt;/code&gt;
    &lt;p&gt;What I love about PN-Counters as a broader insight for CRDTs is that you can often build more complex CRDTs by combining simpler ones.&lt;/p&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Increment (on replica &lt;code&gt;r&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;increment r (PNCounter inc dec) = PNCounter (increment r inc) dec&lt;/code&gt;
    &lt;p&gt;Decrement (on replica &lt;code&gt;r&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;decrement r (PNCounter inc dec) = PNCounter inc (increment r dec)&lt;/code&gt;
    &lt;p&gt;Merge:&lt;/p&gt;
    &lt;code&gt;merge (PNCounter i1 d1) (PNCounter i2 d2) =
  PNCounter (merge i1 i2) (merge d1 d2)&lt;/code&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;Since both components are G-Counters with valid merge operations, the PN-Counter’s merge inherits their properties and forms a semilattice.&lt;/p&gt;
    &lt;head rend="h3"&gt;Intuition&lt;/head&gt;
    &lt;p&gt;A PN-Counter is like having two separate tally sheets: one for additions, one for subtractions. The current value is the difference between them. When replicas sync, they merge both sheets independently.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Supports both increment and decrement&lt;/item&gt;
      &lt;item&gt;Deterministic convergence&lt;/item&gt;
      &lt;item&gt;Simple to understand and implement&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Double the space of a G-Counter&lt;/item&gt;
      &lt;item&gt;Can never truly garbage collect old replica entries&lt;/item&gt;
      &lt;item&gt;No bound on the value range (can overflow)&lt;/item&gt;
      &lt;item&gt;Cannot reset the counter atomically&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use PN-Counter for any metric that can increase or decrease over time: inventory counts, resource pools, etc.&lt;/p&gt;
    &lt;head rend="h3"&gt;Variants&lt;/head&gt;
    &lt;p&gt;Some implementations use a single map with integer values instead of two separate maps, but the principle is the same.&lt;/p&gt;
    &lt;head rend="h2"&gt;G-Set: Grow-Only Set&lt;/head&gt;
    &lt;p&gt;Moving from numbers to collections, we consider the simplest CRDT set.&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;A G-Set is simply a set that supports addition but not removal:&lt;/p&gt;
    &lt;code&gt;type GSet a = Set a&lt;/code&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Add:&lt;/p&gt;
    &lt;code&gt;add :: Ord a =&amp;gt; a -&amp;gt; GSet a -&amp;gt; GSet a
add = insert&lt;/code&gt;
    &lt;p&gt;Merge:&lt;/p&gt;
    &lt;code&gt;merge :: Ord a =&amp;gt; GSet a -&amp;gt; GSet a -&amp;gt; GSet a
merge = union&lt;/code&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;Sets with union form a semilattice under the subset relation.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Associative: Set union is associative&lt;/item&gt;
      &lt;item&gt;Commutative: Set union is commutative&lt;/item&gt;
      &lt;item&gt;Idempotent: &lt;code&gt;A ∪ A = A&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Monotonic: Sets only grow&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Intuition&lt;/head&gt;
    &lt;p&gt;Once an element is added to any replica, it will eventually appear in all replicas. There’s no way to remove it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Minimal overhead (just the set elements)&lt;/item&gt;
      &lt;item&gt;Simple and efficient&lt;/item&gt;
      &lt;item&gt;Familiar set semantics&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cannot remove elements&lt;/item&gt;
      &lt;item&gt;Grows unbounded&lt;/item&gt;
      &lt;item&gt;No garbage collection&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use G-Set for append-only collections where removal is never needed: event logs, collected tags, or immutable registries.&lt;/p&gt;
    &lt;head rend="h2"&gt;2P-Set: Two-Phase Set&lt;/head&gt;
    &lt;p&gt;The natural extension of G-Set to support removal.&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;A 2P-Set (Two-Phase Set) contains two G-Sets: one for added elements, one for removed elements:&lt;/p&gt;
    &lt;code&gt;data TwoPhaseSet a = TwoPhaseSet
  { added :: GSet a
  , removed :: GSet a
  }&lt;/code&gt;
    &lt;p&gt;An element is in the set if it’s been added but not removed:&lt;/p&gt;
    &lt;code&gt;member :: Ord a =&amp;gt; a -&amp;gt; TwoPhaseSet a -&amp;gt; Bool
member x (TwoPhaseSet a r) = x `Set.member` a &amp;amp;&amp;amp; x `Set.notMember` r&lt;/code&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Add:&lt;/p&gt;
    &lt;code&gt;add x (TwoPhaseSet a r) = TwoPhaseSet (insert x a) r&lt;/code&gt;
    &lt;p&gt;Remove:&lt;/p&gt;
    &lt;code&gt;remove x (TwoPhaseSet a r) = TwoPhaseSet a (insert x r)&lt;/code&gt;
    &lt;p&gt;Merge:&lt;/p&gt;
    &lt;code&gt;merge (TwoPhaseSet a1 r1) (TwoPhaseSet a2 r2) =
  TwoPhaseSet (union a1 a2) (union r1 r2)&lt;/code&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;Bias toward removal: If an element appears in the removed set, it’s not in the 2P-Set, even if it’s also in the added set.&lt;/p&gt;
    &lt;p&gt;Once removed, forever removed: Once an element is removed at any replica, it will eventually be removed from all replicas and cannot be re-added.&lt;/p&gt;
    &lt;head rend="h3"&gt;Intuition&lt;/head&gt;
    &lt;p&gt;The 2P-Set is like marking items in a ledger: you can add entries and you can cross them out, but you can’t un-cross-out an entry. Once something is crossed out (removed), that decision is permanent.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Supports both add and remove&lt;/item&gt;
      &lt;item&gt;Simple to understand&lt;/item&gt;
      &lt;item&gt;Deterministic convergence&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cannot re-add removed elements (the “2P” means two-phase: add, then remove, no going back)&lt;/item&gt;
      &lt;item&gt;Both sets grow monotonically (removed items never truly disappear)&lt;/item&gt;
      &lt;item&gt;No garbage collection&lt;/item&gt;
      &lt;item&gt;Not suitable for scenarios where elements might be removed and re-added&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use 2P-Set when elements have a lifecycle of “not present → added → removed” and never need to be re-added: task completion tracking, tombstones, or revoked permissions.&lt;/p&gt;
    &lt;head rend="h2"&gt;LWW-Element-Set: Last-Write-Wins Element Set&lt;/head&gt;
    &lt;p&gt;What if we want to re-add elements? We need timestamps.&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;An LWW-Element-Set associates each element with a timestamp for additions and removals:&lt;/p&gt;
    &lt;code&gt;data LWWSet a = LWWSet
  { addTimes :: Map a Timestamp
  , removeTimes :: Map a Timestamp
  }&lt;/code&gt;
    &lt;p&gt;An element is in the set if its most recent operation was an add:&lt;/p&gt;
    &lt;code&gt;member :: Ord a =&amp;gt; a -&amp;gt; LWWSet a -&amp;gt; Bool
member x (LWWSet adds removes) =
  case (Map.lookup x adds, Map.lookup x removes) of
    (Just t1, Just t2) -&amp;gt; t1 &amp;gt; t2
    (Just _, Nothing) -&amp;gt; True
    _ -&amp;gt; False&lt;/code&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Add (with timestamp &lt;code&gt;t&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;add x t (LWWSet adds removes) = LWWSet (insert x t adds) removes&lt;/code&gt;
    &lt;p&gt;Remove (with timestamp &lt;code&gt;t&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;remove x t (LWWSet adds removes) = LWWSet adds (insert x t removes)&lt;/code&gt;
    &lt;p&gt;Merge:&lt;/p&gt;
    &lt;code&gt;merge (LWWSet a1 r1) (LWWSet a2 r2) =
  LWWSet (unionWith max a1 a2) (unionWith max r1 r2)&lt;/code&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;The merge operation is well-defined because &lt;code&gt;max&lt;/code&gt; over timestamps forms a semilattice.&lt;/p&gt;
    &lt;p&gt;Timestamp monotonicity: Each replica must generate increasing timestamps (typically using wall clocks plus replica IDs as tiebreakers).&lt;/p&gt;
    &lt;p&gt;Bias: We must decide what happens when add and remove timestamps are equal. Common choices: bias toward add, or bias toward remove.&lt;/p&gt;
    &lt;head rend="h3"&gt;Intuition&lt;/head&gt;
    &lt;p&gt;Each element has a timestamp for when it was last added and when it was last removed. The most recent operation wins. When merging, we take the latest add timestamp and latest remove timestamp we’ve seen.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Supports add, remove, and re-add&lt;/item&gt;
      &lt;item&gt;Can garbage collect old timestamps (carefully)&lt;/item&gt;
      &lt;item&gt;Natural semantics for many applications&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Requires synchronized clocks (or logical clocks with careful replica ID handling)&lt;/item&gt;
      &lt;item&gt;Concurrent add/remove on the same element may surprise users (one operation is discarded)&lt;/item&gt;
      &lt;item&gt;Loses information: if two users concurrently add the same element, only one timestamp survives&lt;/item&gt;
      &lt;item&gt;The “last write wins” semantics mean data loss is possible&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use LWW-Element-Set when you need a set with add/remove/re-add capability and can tolerate last-write-wins semantics: user preferences, feature flags, or cached collections where perfect consistency isn’t critical.&lt;/p&gt;
    &lt;head rend="h3"&gt;Clock Considerations&lt;/head&gt;
    &lt;p&gt;The biggest pitfall of LWW-Element-Set is clock skew. If replica A’s clock is ahead of replica B’s, then A’s operations will always “win” over B’s, even if B’s operations happened later in real time. Solutions include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use hybrid logical clocks (HLC) instead of wall clocks&lt;/item&gt;
      &lt;item&gt;Use replica IDs as tiebreakers (e.g., timestamps are &lt;code&gt;(wall_time, replica_id)&lt;/code&gt;pairs)&lt;/item&gt;
      &lt;item&gt;Accept the inconsistency as a tradeoff&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;OR-Set: Observed-Remove Set&lt;/head&gt;
    &lt;p&gt;The most sophisticated set CRDT, solving the re-add problem without LWW semantics.7&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;An OR-Set (Observed-Remove Set) associates each element with a set of unique tags:&lt;/p&gt;
    &lt;code&gt;type ORSet a = Map a (Set Tag)&lt;/code&gt;
    &lt;p&gt;Tags are unique identifiers generated when adding an element (e.g., &lt;code&gt;(replica_id, sequence_number)&lt;/code&gt; pairs).&lt;/p&gt;
    &lt;p&gt;An element is in the set if it has any tags:&lt;/p&gt;
    &lt;code&gt;member :: Ord a =&amp;gt; a -&amp;gt; ORSet a -&amp;gt; Bool
member x set = case Map.lookup x set of
  Just tags -&amp;gt; not (Set.null tags)
  Nothing -&amp;gt; False&lt;/code&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Add (with fresh tag &lt;code&gt;t&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;add x t set = Map.insertWith union x (singleton t) set&lt;/code&gt;
    &lt;p&gt;Remove (with observed tags &lt;code&gt;ts&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;remove x ts set = Map.update (\\tags -&amp;gt;
  let remaining = tags \\ ts
  in if Set.null remaining then Nothing else Just remaining) x set&lt;/code&gt;
    &lt;p&gt;The critical insight: removal removes only the tags that were observed. If concurrent adds create new tags, those survive.&lt;/p&gt;
    &lt;p&gt;Merge:&lt;/p&gt;
    &lt;code&gt;merge = Map.unionWith union&lt;/code&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;The merge operation forms a semilattice where &lt;code&gt;s1 ≤ s2&lt;/code&gt; if for all elements &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;s1[x] ⊆ s2[x]&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Add wins: If an add and remove happen concurrently (the add’s tag wasn’t observed by the remove), the add wins.&lt;/p&gt;
    &lt;p&gt;Causal consistency: You can only remove tags you’ve observed (seen in a prior state).&lt;/p&gt;
    &lt;head rend="h3"&gt;Intuition&lt;/head&gt;
    &lt;p&gt;Think of each addition as dropping a unique token into a bucket for that element. Removal takes specific tokens out of the bucket. If someone concurrently added a new token you haven’t seen, your removal doesn’t affect it. An element is present if its bucket has any tokens.&lt;/p&gt;
    &lt;p&gt;This gives us add-wins semantics: concurrent add and remove means the element stays in the set (because the remove didn’t observe the add’s tag).&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Supports add, remove, and re-add with intuitive semantics&lt;/item&gt;
      &lt;item&gt;No timestamp requirements&lt;/item&gt;
      &lt;item&gt;Add-wins semantics are often more desirable than LWW&lt;/item&gt;
      &lt;item&gt;Properly handles concurrent operations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Larger space overhead (tags per element)&lt;/item&gt;
      &lt;item&gt;More complex implementation&lt;/item&gt;
      &lt;item&gt;Need garbage collection strategy for tags&lt;/item&gt;
      &lt;item&gt;Remove operations need to carry the observed tags (larger messages)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use OR-Set when you need a set with full add/remove/re-add support and can’t tolerate LWW’s data loss: collaborative editing, shopping carts, or any scenario where concurrent adds should be preserved.&lt;/p&gt;
    &lt;head rend="h3"&gt;Garbage Collection&lt;/head&gt;
    &lt;p&gt;Old tags can accumulate. Strategies include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tombstones: Keep removed tags for a grace period before discarding&lt;/item&gt;
      &lt;item&gt;Version vectors: Use causal history to determine which tags are safe to remove&lt;/item&gt;
      &lt;item&gt;Bounded tags: Limit the number of tags per element, using LWW within that bound&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Interactive Demo&lt;/head&gt;
    &lt;p&gt;Experience the add-wins semantics of OR-Set:&lt;/p&gt;
    &lt;head rend="h3"&gt;OR-Set: Observed-Remove Set&lt;/head&gt;
    &lt;p&gt;Try it: Add the same element (e.g., "apple") on both replicas without merging first. Each creates a unique tag. Then remove it from one replica and immediately merge. Notice the element stays because the remove only deleted the tags it could see - the other replica's tag survives. This is add-wins semantics.&lt;/p&gt;
    &lt;head rend="h2"&gt;LWW-Register: Last-Write-Wins Register&lt;/head&gt;
    &lt;p&gt;Registers store single values. The simplest register CRDT uses last-write-wins.&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;An LWW-Register pairs a value with a timestamp:&lt;/p&gt;
    &lt;code&gt;data LWWRegister a = LWWRegister
  { value :: a
  , timestamp :: Timestamp
  }&lt;/code&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Write (with timestamp &lt;code&gt;t&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;write x t _ = LWWRegister x t&lt;/code&gt;
    &lt;p&gt;Merge:&lt;/p&gt;
    &lt;code&gt;merge r1@(LWWRegister v1 t1) r2@(LWWRegister v2 t2)
  | t1 &amp;gt; t2 = r1
  | t1 &amp;lt; t2 = r2
  | otherwise = r1  -- tiebreaker (could use replica ID)&lt;/code&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;The merge operation is a semilattice with partial order defined by timestamps.&lt;/p&gt;
    &lt;p&gt;One value wins: When concurrent writes occur, only one survives (the one with the higher timestamp).&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simple and efficient&lt;/item&gt;
      &lt;item&gt;Small size (just value + timestamp)&lt;/item&gt;
      &lt;item&gt;Easy to understand&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Loses concurrent updates&lt;/item&gt;
      &lt;item&gt;Requires clock synchronization&lt;/item&gt;
      &lt;item&gt;No way to detect or recover lost updates&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use LWW-Register for single-value cells where you can tolerate lost updates: user profile fields, configuration settings, or cached computed values.&lt;/p&gt;
    &lt;head rend="h3"&gt;Interactive Demo&lt;/head&gt;
    &lt;p&gt;See data loss in action with last-write-wins semantics:&lt;/p&gt;
    &lt;head rend="h3"&gt;LWW-Register: Last-Write-Wins Register&lt;/head&gt;
    &lt;p&gt;⚠️ Data Loss Alert: Write different values on replica A and B without merging. Each gets its own timestamp. Then merge A into B (or vice versa). Watch one value completely disappear! The higher timestamp wins. This is why LWW is dangerous for important data - concurrent writes = lost data.&lt;/p&gt;
    &lt;head rend="h2"&gt;MV-Register: Multi-Value Register&lt;/head&gt;
    &lt;p&gt;What if we want to preserve concurrent writes instead of discarding them?&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;An MV-Register stores a set of value-timestamp pairs:&lt;/p&gt;
    &lt;code&gt;type MVRegister a = Set (a, Timestamp)&lt;/code&gt;
    &lt;p&gt;When reading, you get back all concurrently written values (values with incomparable timestamps).&lt;/p&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Write (with timestamp &lt;code&gt;t&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;write x t reg = Set.singleton (x, t)&lt;/code&gt;
    &lt;p&gt;Merge:&lt;/p&gt;
    &lt;code&gt;merge reg1 reg2 =
  let combined = union reg1 reg2
      maxTime = maximum (map snd combined)
      concurrent = filter (\\(_, t) -&amp;gt; t == maxTime) combined
  in fromList concurrent&lt;/code&gt;
    &lt;p&gt;More sophisticated: keep values with causally concurrent timestamps, not just the maximum.&lt;/p&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;The merge preserves all values that might be “current” from different replicas’ perspectives.&lt;/p&gt;
    &lt;p&gt;Concurrent values preserved: If two writes happened concurrently, both values appear until a subsequent write supersedes them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No data loss on concurrent updates&lt;/item&gt;
      &lt;item&gt;Application can detect and resolve conflicts&lt;/item&gt;
      &lt;item&gt;More information available for conflict resolution&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Returns sets of values, not single values&lt;/item&gt;
      &lt;item&gt;Application must handle conflict resolution&lt;/item&gt;
      &lt;item&gt;More complex semantics&lt;/item&gt;
      &lt;item&gt;Slightly larger space overhead&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use MV-Register when concurrent updates must be detected and resolved by application logic: collaborative text fields, conflict-aware configuration, or any scenario where losing an update is unacceptable.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conflict Resolution&lt;/head&gt;
    &lt;p&gt;When reading an MV-Register returns multiple values, the application must resolve the conflict. Strategies include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Present all values to the user (collaborative editing)&lt;/item&gt;
      &lt;item&gt;Apply a deterministic merge function (e.g., union of tags)&lt;/item&gt;
      &lt;item&gt;Use application-specific semantics (e.g., prefer non-empty values)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;OR-Map: Observed-Remove Map&lt;/head&gt;
    &lt;p&gt;Maps are common. How do we make them CRDTs?&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;An OR-Map is a map where each key is associated with an OR-Set of tagged values:&lt;/p&gt;
    &lt;code&gt;type ORMap k v = Map k (ORSet (v, Tag))&lt;/code&gt;
    &lt;p&gt;Alternatively, implement as a composition of OR-Set (for keys) with per-key CRDTs (for values).&lt;/p&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Put (with fresh tag &lt;code&gt;t&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;put k v t map = Map.insertWith union k (singleton (v, t)) map&lt;/code&gt;
    &lt;p&gt;Remove key:&lt;/p&gt;
    &lt;code&gt;removeKey k map = Map.delete k map&lt;/code&gt;
    &lt;p&gt;Remove specific value:&lt;/p&gt;
    &lt;code&gt;removeValue k v tags map = -- similar to OR-Set remove&lt;/code&gt;
    &lt;p&gt;Merge:&lt;/p&gt;
    &lt;code&gt;merge = Map.unionWith (OR-Set merge)&lt;/code&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Full map operations with CRDT semantics&lt;/item&gt;
      &lt;item&gt;Can nest other CRDTs as values&lt;/item&gt;
      &lt;item&gt;Compositional&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Complex metadata management&lt;/item&gt;
      &lt;item&gt;Garbage collection challenges&lt;/item&gt;
      &lt;item&gt;Larger overhead&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use OR-Map when you need a distributed key-value store with CRDT guarantees: collaborative JSON documents, distributed configuration, or nested data structures.&lt;/p&gt;
    &lt;head rend="h2"&gt;RGA: Replicated Growable Array&lt;/head&gt;
    &lt;p&gt;Sequences are hard. How do you handle insertions in the middle when replicas disagree on positions?8&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;RGA (Replicated Growable Array) assigns each element a unique ID and stores the sequence as a tree structure based on insertion order and causality.9&lt;/p&gt;
    &lt;code&gt;data RGA a = RGA
  { elements :: Map UID (a, UID)  -- element ID -&amp;gt; (value, parent ID)
  , root :: UID
  }&lt;/code&gt;
    &lt;p&gt;Each element knows its “parent” (the element after which it was inserted).&lt;/p&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Insert (after element with ID &lt;code&gt;p&lt;/code&gt;, with fresh ID &lt;code&gt;uid&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;insert p x uid rga = -- complex tree manipulation&lt;/code&gt;
    &lt;p&gt;Delete (element with ID &lt;code&gt;uid&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;delete uid rga = -- mark as tombstone, don't actually remove&lt;/code&gt;
    &lt;p&gt;Merge: Merge trees by reconciling insertion orders.&lt;/p&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;The challenge is that positional indices change as elements are inserted/removed. RGA solves this by using immutable IDs and causal relationships.&lt;/p&gt;
    &lt;p&gt;Causal order preserved: If element A was inserted before element B on the same replica, that relationship is preserved globally.&lt;/p&gt;
    &lt;head rend="h3"&gt;Intuition&lt;/head&gt;
    &lt;p&gt;Instead of “insert at position 5,” you say “insert after element X.” Since X has a unique ID, this instruction is unambiguous even when other replicas are concurrently inserting elsewhere.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Supports arbitrary insertions and deletions&lt;/item&gt;
      &lt;item&gt;Eventual consistency for sequences&lt;/item&gt;
      &lt;item&gt;Handles concurrent edits intuitively&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Complex implementation&lt;/item&gt;
      &lt;item&gt;Large overhead (IDs, tombstones)&lt;/item&gt;
      &lt;item&gt;No compaction without coordination&lt;/item&gt;
      &lt;item&gt;Performance degrades with many deletes (tombstones accumulate)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use RGA for collaborative text editing or any replicated sequence where insertions at arbitrary positions must be supported: shared lists, collaborative documents, or distributed queues.&lt;/p&gt;
    &lt;head rend="h3"&gt;Alternatives&lt;/head&gt;
    &lt;p&gt;Other sequence CRDTs include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;WOOT (Without Operational Transformation): similar idea, different structure&lt;/item&gt;
      &lt;item&gt;Logoot: uses position identifiers between elements&lt;/item&gt;
      &lt;item&gt;LSEQ: adaptive allocation of position identifiers&lt;/item&gt;
      &lt;item&gt;YATA: optimizations for text editing workloads10&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each has different tradeoffs in space overhead, time complexity, and behavior under specific edit patterns.&lt;/p&gt;
    &lt;head rend="h2"&gt;Causal CRDTs: Adding Causality&lt;/head&gt;
    &lt;p&gt;Advanced CRDTs incorporate causal tracking using version vectors or similar mechanisms. This enables more sophisticated semantics.&lt;/p&gt;
    &lt;head rend="h3"&gt;Version Vectors&lt;/head&gt;
    &lt;p&gt;A version vector tracks the logical clock for each replica:11&lt;/p&gt;
    &lt;code&gt;type VersionVector = Map ReplicaId Nat&lt;/code&gt;
    &lt;p&gt;Operations include the version vector, allowing replicas to determine causality: whether one operation happened-before another, or whether they were concurrent.&lt;/p&gt;
    &lt;head rend="h3"&gt;Causal Register&lt;/head&gt;
    &lt;p&gt;Pairs an MV-Register with version vectors:&lt;/p&gt;
    &lt;code&gt;data CausalRegister a = CausalRegister
  { values :: Map VersionVector a
  }&lt;/code&gt;
    &lt;p&gt;Only keeps values with concurrent version vectors, discarding those that are causally dominated.&lt;/p&gt;
    &lt;head rend="h3"&gt;Advantages of Causality&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;More precise conflict detection (concurrent vs. causally ordered)&lt;/item&gt;
      &lt;item&gt;Better garbage collection (can discard superseded operations)&lt;/item&gt;
      &lt;item&gt;Foundation for stronger consistency guarantees&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Disadvantages of Causality&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Larger metadata (version vectors grow with number of replicas)&lt;/item&gt;
      &lt;item&gt;More complex logic&lt;/item&gt;
      &lt;item&gt;Still doesn’t eliminate conflicts, just detects them more precisely&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Interactive Demo&lt;/head&gt;
    &lt;p&gt;Explore how version vectors track causality:&lt;/p&gt;
    &lt;head rend="h3"&gt;Vector Clocks: Tracking Causality&lt;/head&gt;
    &lt;p&gt;Try it: Click "Perform Operation" on replica A, then on replica B (without clicking "Receive from"). These operations don't know about each other - they're concurrent! Now click two operations in the history to compare their vector clocks. Concurrent operations show as yellow warnings - they need special conflict resolution.&lt;/p&gt;
    &lt;head rend="h2"&gt;Delta CRDTs: Efficient State Transmission&lt;/head&gt;
    &lt;p&gt;State-based CRDTs have a problem: sending the entire state on every sync is wasteful. Delta CRDTs solve this.12&lt;/p&gt;
    &lt;head rend="h3"&gt;The Problem&lt;/head&gt;
    &lt;p&gt;Consider a G-Counter with 1000 replicas. If replica A increments its count, must it send all 1000 entries to replica B? That’s inefficient: only one entry changed!&lt;/p&gt;
    &lt;head rend="h3"&gt;The Solution&lt;/head&gt;
    &lt;p&gt;Instead of sending full state, send only the delta: the part of the state that changed since the last sync.&lt;/p&gt;
    &lt;code&gt;type Delta a = a  -- same type as state, but represents only changes

merge :: CRDT a =&amp;gt; a -&amp;gt; Delta a -&amp;gt; a&lt;/code&gt;
    &lt;p&gt;For G-Counter, a delta might be just &lt;code&gt;{A: 1}&lt;/code&gt; instead of the full map.&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;A Delta CRDT extends a state-based CRDT with delta operations:&lt;/p&gt;
    &lt;code&gt;data DeltaCRDT a = DeltaCRDT
  { state :: a
  , lastSent :: Map ReplicaId a  -- track what we've sent to each replica
  }

delta :: ReplicaId -&amp;gt; DeltaCRDT a -&amp;gt; Delta a
delta replica crdt = state crdt `since` lastSent[replica]&lt;/code&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;Delta CRDTs must satisfy the same semilattice properties as regular state-based CRDTs, plus:&lt;/p&gt;
    &lt;p&gt;Delta-state equivalence: Merging deltas incrementally must be equivalent to merging full states.&lt;/p&gt;
    &lt;p&gt;Delta composition: Deltas can be composed: &lt;code&gt;delta1 ⊔ delta2&lt;/code&gt; is itself a valid delta.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dramatically reduced bandwidth (send only changes)&lt;/item&gt;
      &lt;item&gt;Same convergence guarantees as state-based CRDTs&lt;/item&gt;
      &lt;item&gt;Can batch multiple deltas together&lt;/item&gt;
      &lt;item&gt;Easier to implement than operation-based CRDTs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Must track what has been sent to each replica&lt;/item&gt;
      &lt;item&gt;Slightly more complex than pure state-based&lt;/item&gt;
      &lt;item&gt;Still need full state for new replicas joining&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use Delta CRDTs when network bandwidth is a concern or state size is large. Most production CRDT systems use delta-state internally (Riak, Automerge). If you’re implementing your own CRDT system from scratch, start with deltas. Your future self will thank you.&lt;/p&gt;
    &lt;head rend="h3"&gt;Example: Delta G-Counter&lt;/head&gt;
    &lt;code&gt;increment :: ReplicaId -&amp;gt; GCounter -&amp;gt; (GCounter, Delta GCounter)
increment r counter =
  let newCounter = insertWith (+) r 1 counter
      delta = singleton r 1  -- only the change!
  in (newCounter, delta)&lt;/code&gt;
    &lt;p&gt;The delta is just the single updated entry, not the entire counter.&lt;/p&gt;
    &lt;head rend="h2"&gt;WOOT: Without Operational Transformation&lt;/head&gt;
    &lt;p&gt;WOOT is a sequence CRDT that predates RGA, with different design choices.2&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;WOOT represents a sequence as a set of character objects with unique IDs, where each character stores references to its previous and next characters:&lt;/p&gt;
    &lt;code&gt;data WChar a = WChar
  { charId :: UID
  , value :: a
  , prevId :: UID
  , nextId :: UID
  , isVisible :: Bool
  }

type WOOT a = Set (WChar a)&lt;/code&gt;
    &lt;head rend="h3"&gt;Key Insight&lt;/head&gt;
    &lt;p&gt;Instead of storing a linear sequence, WOOT stores constraints: “this character comes after X and before Y.” When multiple characters claim to be between X and Y, a deterministic ordering (based on UID) resolves the conflict.&lt;/p&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Insert (after character with ID &lt;code&gt;prev&lt;/code&gt;, before character with ID &lt;code&gt;next&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;insert :: a -&amp;gt; UID -&amp;gt; UID -&amp;gt; UID -&amp;gt; WOOT a -&amp;gt; WOOT a
insert val uid prev next woot =
  insert (WChar uid val prev next True) woot&lt;/code&gt;
    &lt;p&gt;Delete (character with ID &lt;code&gt;uid&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;delete :: UID -&amp;gt; WOOT a -&amp;gt; WOOT a
delete uid woot = -- mark character as invisible, don't remove&lt;/code&gt;
    &lt;head rend="h3"&gt;Linearization&lt;/head&gt;
    &lt;p&gt;To read the sequence, perform a topological sort respecting the prev/next constraints, filtering out invisible characters.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Strong eventual consistency&lt;/item&gt;
      &lt;item&gt;No need for causal delivery (constraints handle ordering)&lt;/item&gt;
      &lt;item&gt;Intuitive model (characters reference neighbors)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tombstones accumulate (deleted characters remain)&lt;/item&gt;
      &lt;item&gt;Linearization has O(n²) worst case&lt;/item&gt;
      &lt;item&gt;More complex than RGA&lt;/item&gt;
      &lt;item&gt;UIDs must be globally unique and ordered&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Comparison with RGA&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;RGA: Uses a tree structure, parent-child relationships&lt;/item&gt;
      &lt;item&gt;WOOT: Uses bidirectional constraints, more flexible but slower linearization&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;WOOT is primarily of historical interest. Modern implementations prefer RGA or YATA for better performance. But it’s a neat design, and the name alone makes it worth knowing about.&lt;/p&gt;
    &lt;head rend="h2"&gt;Logoot: Scalable Position Identifiers&lt;/head&gt;
    &lt;p&gt;Logoot takes a different approach to sequences: instead of linking elements, assign each element a position in a dense order.13&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;Each element has a position identifier that is a sequence of (digit, replicaId) pairs:&lt;/p&gt;
    &lt;code&gt;type Position = [(Int, ReplicaId)]

data LogootElement a = LogootElement
  { position :: Position
  , value :: a
  , isDeleted :: Bool
  }

type Logoot a = Set (LogootElement a)&lt;/code&gt;
    &lt;p&gt;Positions are ordered lexicographically.&lt;/p&gt;
    &lt;head rend="h3"&gt;Key Insight&lt;/head&gt;
    &lt;p&gt;Positions form a dense order: between any two positions, you can always allocate a new position. To insert between positions &lt;code&gt;p1&lt;/code&gt; and &lt;code&gt;p2&lt;/code&gt;, generate a new position &lt;code&gt;p&lt;/code&gt; such that &lt;code&gt;p1 &amp;lt; p &amp;lt; p2&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Insert (between positions &lt;code&gt;before&lt;/code&gt; and &lt;code&gt;after&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;insert :: a -&amp;gt; Position -&amp;gt; Position -&amp;gt; Logoot a -&amp;gt; Logoot a
insert val before after logoot =
  let newPos = allocatePosition before after currentReplicaId
      element = LogootElement newPos val False
  in insert element logoot&lt;/code&gt;
    &lt;p&gt;Position Allocation:&lt;/p&gt;
    &lt;code&gt;allocatePosition :: Position -&amp;gt; Position -&amp;gt; ReplicaId -&amp;gt; Position
allocatePosition before after replicaId =
  -- Find a position between before and after
  -- Use replicaId as tiebreaker for deterministic ordering&lt;/code&gt;
    &lt;p&gt;Delete:&lt;/p&gt;
    &lt;code&gt;delete :: Position -&amp;gt; Logoot a -&amp;gt; Logoot a
delete pos logoot = -- mark element at pos as deleted&lt;/code&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;Deterministic ordering: Elements are always ordered by their positions.&lt;/p&gt;
    &lt;p&gt;Unique positions: Each insert generates a unique position (using replica ID in the position).&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No need to reference other elements by ID&lt;/item&gt;
      &lt;item&gt;Simpler merge than WOOT&lt;/item&gt;
      &lt;item&gt;Positions are self-describing (no need to look up IDs)&lt;/item&gt;
      &lt;item&gt;Can insert without knowing the full document structure&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Position identifiers grow over time (especially with many edits)&lt;/item&gt;
      &lt;item&gt;Still accumulates tombstones&lt;/item&gt;
      &lt;item&gt;Position allocation algorithm is complex&lt;/item&gt;
      &lt;item&gt;Pathological cases where positions become very long&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;LSEQ: Adaptive Positions&lt;/head&gt;
    &lt;p&gt;LSEQ improves on Logoot by using an adaptive allocation strategy. Instead of always allocating positions the same way, LSEQ alternates between strategies to keep positions shorter on average.14&lt;/p&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use Logoot/LSEQ when you need a sequence CRDT and want simpler semantics than RGA/WOOT. The tradeoff is position identifier growth.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tree CRDTs: Hierarchical Data&lt;/head&gt;
    &lt;p&gt;Extending CRDTs to trees is challenging because parent-child relationships must be maintained consistently.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Problem&lt;/head&gt;
    &lt;p&gt;Trees have structural constraints:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Each node has exactly one parent (except root)&lt;/item&gt;
      &lt;item&gt;No cycles allowed&lt;/item&gt;
      &lt;item&gt;Moving a node changes parent-child relationships&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How do we handle concurrent operations like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Two replicas move the same node to different parents?&lt;/item&gt;
      &lt;item&gt;One replica moves node A under node B while another moves B under A?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Approaches&lt;/head&gt;
    &lt;p&gt;OR-Tree: Combine OR-Set with parent pointers, using conflict resolution strategies when multiple parents are observed.&lt;/p&gt;
    &lt;p&gt;CRDT-Tree: Use causal ordering to determine which move operations take precedence.&lt;/p&gt;
    &lt;p&gt;Log-based Trees: Store operations in a replicated log and rebuild tree structure on read.&lt;/p&gt;
    &lt;head rend="h3"&gt;OR-Tree Definition&lt;/head&gt;
    &lt;code&gt;type ORTree a = Map NodeId (ORSet ParentId, a)&lt;/code&gt;
    &lt;p&gt;Each node stores an OR-Set of potential parents. Conflict resolution:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Last-write-wins: Use timestamps to pick winning parent&lt;/item&gt;
      &lt;item&gt;First-wins: The first parent observed wins&lt;/item&gt;
      &lt;item&gt;Merge: Allow nodes to have multiple parents temporarily, application resolves&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Can represent hierarchical data distributedly&lt;/item&gt;
      &lt;item&gt;Handles concurrent structural changes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Complex conflict resolution strategies&lt;/item&gt;
      &lt;item&gt;Must prevent cycles (may require rejecting some operations)&lt;/item&gt;
      &lt;item&gt;Moving subtrees is complicated&lt;/item&gt;
      &lt;item&gt;High metadata overhead&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use Tree CRDTs for file systems, organizational charts, or document outlines where the hierarchy must be replicated. Be prepared for complexity in handling concurrent structural changes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Alternatives&lt;/head&gt;
    &lt;p&gt;For many use cases, an OR-Map with explicit parent fields is simpler than a full Tree CRDT, even if it doesn’t enforce tree constraints at the CRDT level.&lt;/p&gt;
    &lt;head rend="h2"&gt;Observed-Remove Shopping Cart&lt;/head&gt;
    &lt;p&gt;A practical example combining multiple CRDT concepts.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Domain&lt;/head&gt;
    &lt;p&gt;An e-commerce shopping cart must support:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Add product to cart&lt;/item&gt;
      &lt;item&gt;Remove product from cart&lt;/item&gt;
      &lt;item&gt;Change quantity&lt;/item&gt;
      &lt;item&gt;Work offline and sync later&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Naive Approach: LWW Map&lt;/head&gt;
    &lt;code&gt;type CartLWW = Map ProductId (Int, Timestamp)&lt;/code&gt;
    &lt;p&gt;Problems:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Concurrent additions of the same product (one wins)&lt;/item&gt;
      &lt;item&gt;Remove on one device, add on another (one wins, data loss)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Better: OR-Set + PN-Counter&lt;/head&gt;
    &lt;code&gt;type ShoppingCart = Map ProductId PNCounter&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use OR-Set semantics for which products are in cart&lt;/item&gt;
      &lt;item&gt;Use PN-Counter for quantities&lt;/item&gt;
      &lt;item&gt;Add-wins semantics for products (if concurrently added and removed, item stays)&lt;/item&gt;
      &lt;item&gt;Quantities merge correctly (concurrent +1 and +2 becomes +3)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Add product with quantity:&lt;/p&gt;
    &lt;code&gt;addToCart :: ProductId -&amp;gt; Int -&amp;gt; ReplicaId -&amp;gt; ShoppingCart -&amp;gt; ShoppingCart
addToCart pid qty replica cart =
  let counter = lookupOr emptyCounter pid cart
      incremented = incrementN replica qty counter
  in insert pid incremented cart&lt;/code&gt;
    &lt;p&gt;Remove product:&lt;/p&gt;
    &lt;code&gt;removeFromCart :: ProductId -&amp;gt; ShoppingCart -&amp;gt; ShoppingCart
removeFromCart pid cart = delete pid cart&lt;/code&gt;
    &lt;p&gt;Change quantity:&lt;/p&gt;
    &lt;code&gt;changeQuantity :: ProductId -&amp;gt; Int -&amp;gt; ReplicaId -&amp;gt; ShoppingCart -&amp;gt; ShoppingCart
changeQuantity pid delta replica cart =
  let counter = lookupOr emptyCounter pid cart
      updated = if delta &amp;gt; 0
                then incrementN replica delta counter
                else decrementN replica (-delta) counter
  in insert pid updated cart&lt;/code&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Handles all operations correctly&lt;/item&gt;
      &lt;item&gt;No data loss on concurrent modifications&lt;/item&gt;
      &lt;item&gt;Intuitive semantics for users&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PN-Counters can go negative (need validation)&lt;/item&gt;
      &lt;item&gt;Must track all replicas (for PN-Counter)&lt;/item&gt;
      &lt;item&gt;Slightly more overhead than simple LWW&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This example shows how combining basic CRDTs creates sophisticated application-level data structures.&lt;/p&gt;
    &lt;head rend="h2"&gt;Practical Considerations&lt;/head&gt;
    &lt;head rend="h3"&gt;Choosing a CRDT&lt;/head&gt;
    &lt;p&gt;The choice of CRDT depends on your requirements:&lt;/p&gt;
    &lt;p&gt;Do you need only additions? Use G-Counter or G-Set.&lt;/p&gt;
    &lt;p&gt;Do you need removals but not re-additions? Use 2P-Set.&lt;/p&gt;
    &lt;p&gt;Can you tolerate last-write-wins? Use LWW-Element-Set or LWW-Register.&lt;/p&gt;
    &lt;p&gt;Do you need to preserve concurrent operations? Use OR-Set or MV-Register.&lt;/p&gt;
    &lt;p&gt;Do you have sequences? Use RGA or similar sequence CRDT.&lt;/p&gt;
    &lt;p&gt;Do you need nested structures? Use OR-Map with nested CRDTs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Garbage Collection&lt;/head&gt;
    &lt;p&gt;Garbage collection is one of the most challenging practical problems with CRDTs. The fundamental tension: CRDTs achieve convergence by monotonically accumulating information, but production systems can’t grow unbounded forever.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Problem in Detail&lt;/head&gt;
    &lt;p&gt;Consider an OR-Set used for a collaborative todo list. Each time someone adds a task and removes it, we accumulate:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A unique tag for the addition (never removed)&lt;/item&gt;
      &lt;item&gt;A tombstone tracking the removal (never removed)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After 10,000 tasks have been created and completed, our “empty” todo list still contains 10,000 tags worth of metadata. In a G-Counter tracking page views, we keep a separate count for every replica that has ever incremented the counter—even if that replica hasn’t been online in years. For sequence CRDTs like RGA or WOOT, every deleted character becomes a tombstone that must be retained indefinitely. A 1000-character document that’s been heavily edited might internally contain 50,000 tombstones.&lt;/p&gt;
    &lt;p&gt;The core issue: CRDTs converge by retaining enough information to handle any possible merge. If replica A discards metadata about some operation, and replica B (which has been offline for weeks) later tries to merge its state—which still references that metadata—the merge may produce incorrect results.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why Can’t We Just Delete Old Data?&lt;/head&gt;
    &lt;p&gt;Let’s make this concrete with an OR-Set example:&lt;/p&gt;
    &lt;code&gt;-- Replica A's state
orset_a = {"todo-1": {tag_1, tag_2}}

-- Replica B's state (has been offline)
orset_b = {"todo-1": {tag_1, tag_2, tag_3}}

-- Replica A removes todo-1, observing tags {tag_1, tag_2}
-- Now A's state is:
orset_a = {}

-- If A garbage collects and forgets about tags {tag_1, tag_2},
-- then later merges with B:
merge(orset_a, orset_b) = {"todo-1": {tag_3}}

-- The element reappears! (Zombie resurrection)&lt;/code&gt;
    &lt;p&gt;The element we removed comes back because we lost the causal information about which tags we had observed and removed. This is the fundamental safety problem with CRDT garbage collection.&lt;/p&gt;
    &lt;head rend="h4"&gt;Strategies and Tradeoffs&lt;/head&gt;
    &lt;p&gt;Time-Based Expiry&lt;/p&gt;
    &lt;p&gt;The simplest approach: discard metadata older than some threshold (e.g., 30 days). This works well when you can guarantee all replicas sync within that window.&lt;/p&gt;
    &lt;code&gt;gcTombstones :: Timestamp -&amp;gt; ORSet a -&amp;gt; ORSet a
gcTombstones cutoff set =
  -- Remove tags older than cutoff
  Map.mapMaybe (\\tags -&amp;gt;
    let recent = Set.filter (\\t -&amp;gt; tagTime t &amp;gt; cutoff) tags
    in if Set.null recent then Nothing else Just recent) set&lt;/code&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simple to implement&lt;/item&gt;
      &lt;item&gt;No coordination required&lt;/item&gt;
      &lt;item&gt;Works well for frequently-syncing systems&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unsafe if replicas can be offline longer than the grace period&lt;/item&gt;
      &lt;item&gt;Must choose grace period conservatively (wasted space)&lt;/item&gt;
      &lt;item&gt;Zombie resurrection if threshold is too aggressive&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When to use: Mobile apps where you can bound offline time (e.g., “you must sync at least once per week”).&lt;/p&gt;
    &lt;p&gt;Coordinated Garbage Collection&lt;/p&gt;
    &lt;p&gt;Use distributed consensus to agree on what’s safe to discard. Once all replicas acknowledge they’ve received a particular update, the corresponding metadata can be safely removed.&lt;/p&gt;
    &lt;code&gt;data GCState = GCState
  { pendingGC :: Set Tag  -- Tags eligible for GC
  , replicaAcks :: Map ReplicaId (Set Tag)  -- What each replica has seen
  }

-- When all replicas have acked a tag, it's safe to remove
safeToDiscard :: GCState -&amp;gt; Set Tag
safeToDiscard (GCState pending acks) =
  -- Tags that all known replicas have acknowledged
  Set.filter (\\tag -&amp;gt; all (Set.member tag) (Map.elems acks)) pending&lt;/code&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Completely safe (no zombie resurrections)&lt;/item&gt;
      &lt;item&gt;Can garbage collect aggressively once consensus is reached&lt;/item&gt;
      &lt;item&gt;Works with arbitrary offline periods&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Requires coordination (defeats CRDT’s main selling point!)&lt;/item&gt;
      &lt;item&gt;Slow convergence if some replicas are rarely online&lt;/item&gt;
      &lt;item&gt;Must track all replicas (what about replicas that never come back?)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When to use: When you have a bounded, known set of replicas and can tolerate periodic coordination rounds.&lt;/p&gt;
    &lt;p&gt;Version Vectors for Causal Tracking&lt;/p&gt;
    &lt;p&gt;Use version vectors to track causal history. Metadata can be discarded once it’s been causally superseded at all replicas.&lt;/p&gt;
    &lt;code&gt;data CausalORSet a = CausalORSet
  { elements :: Map a (Set (Tag, VersionVector))
  , replicaVersions :: Map ReplicaId VersionVector  -- Last known VV per replica
  }

-- A tag can be GC'd if its version vector is dominated by all known replicas
canDiscardTag :: (Tag, VersionVector) -&amp;gt; Map ReplicaId VersionVector -&amp;gt; Bool
canDiscardTag (_, tagVV) replicaVVs =
  all (\\replicaVV -&amp;gt; tagVV `happenedBefore` replicaVV) (Map.elems replicaVVs)&lt;/code&gt;
    &lt;p&gt;This is more sophisticated: we track causality explicitly and can safely discard tags that are in the causal past of all known replicas.&lt;/p&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;More precise than time-based expiry&lt;/item&gt;
      &lt;item&gt;No coordination needed for the happy path&lt;/item&gt;
      &lt;item&gt;Safe as long as causal tracking is correct&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Version vectors add significant overhead (O(replicas) per operation)&lt;/item&gt;
      &lt;item&gt;Still requires tracking all replicas&lt;/item&gt;
      &lt;item&gt;Complex to implement correctly&lt;/item&gt;
      &lt;item&gt;What about new replicas that join later?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When to use: Systems already using version vectors for causal consistency (Riak, Cassandra-style systems).&lt;/p&gt;
    &lt;p&gt;Bounded Structures with Fallback&lt;/p&gt;
    &lt;p&gt;Limit metadata size and use LWW semantics when bounds are exceeded. For example, keep at most 1000 tags per element in an OR-Set. If we exceed that, discard the oldest tags and accept potential anomalies.&lt;/p&gt;
    &lt;code&gt;addWithBound :: Ord a =&amp;gt; a -&amp;gt; Tag -&amp;gt; Int -&amp;gt; ORSet a -&amp;gt; ORSet a
addWithBound x tag maxTags set =
  let currentTags = Map.findWithDefault Set.empty x set
      newTags = Set.insert tag currentTags
      boundedTags = if Set.size newTags &amp;gt; maxTags
                    then Set.fromList $ take maxTags $ 
                         sortBy (comparing tagTimestamp) (Set.toList newTags)
                    else newTags
  in Map.insert x boundedTags set&lt;/code&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Bounded space overhead (guaranteed)&lt;/item&gt;
      &lt;item&gt;No coordination needed&lt;/item&gt;
      &lt;item&gt;Graceful degradation (becomes LWW-ish when bounded)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Correctness sacrificed for space&lt;/item&gt;
      &lt;item&gt;May lose concurrent operations&lt;/item&gt;
      &lt;item&gt;Choosing the bound is difficult (too small = frequent anomalies, too large = still wasteful)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When to use: When you must have bounded space (embedded systems, strict SLAs) and can tolerate occasional anomalies.&lt;/p&gt;
    &lt;p&gt;Checkpoint and Rebase&lt;/p&gt;
    &lt;p&gt;Periodically create a “checkpoint” snapshot and discard history before that point. New replicas joining after the checkpoint start from the snapshot.&lt;/p&gt;
    &lt;code&gt;data CheckpointedCRDT a = CheckpointedCRDT
  { baselineState :: a  -- Snapshot at checkpoint
  , checkpointTime :: Timestamp
  , deltaSince :: [Delta a]  -- Operations since checkpoint
  }

-- Create a new checkpoint, discarding old deltas
checkpoint :: CheckpointedCRDT a -&amp;gt; CheckpointedCRDT a
checkpoint crdt = CheckpointedCRDT
  { baselineState = foldl merge (baselineState crdt) (deltaSince crdt)
  , checkpointTime = currentTime
  , deltaSince = []
  }&lt;/code&gt;
    &lt;p&gt;Replicas that haven’t synced since before the checkpoint must do a full state sync rather than incremental merge.&lt;/p&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Can aggressively prune old history&lt;/item&gt;
      &lt;item&gt;Conceptually clean (like Git’s shallow clones)&lt;/item&gt;
      &lt;item&gt;Works well with mostly-online systems&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Replicas offline during checkpoint period lose incremental sync&lt;/item&gt;
      &lt;item&gt;Need to track which replicas are pre-checkpoint&lt;/item&gt;
      &lt;item&gt;Full state sync is expensive&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When to use: Collaborative editing systems where most users are online most of the time (Google Docs, Figma).&lt;/p&gt;
    &lt;head rend="h4"&gt;Practical Recommendations&lt;/head&gt;
    &lt;p&gt;For most applications, a hybrid approach works best:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Use time-based expiry with a conservative grace period (90 days)&lt;/item&gt;
      &lt;item&gt;Track the oldest unsynced replica timestamp&lt;/item&gt;
      &lt;item&gt;Only discard metadata older than: &lt;code&gt;min(graceperiod, oldest_unsynced - safety_margin)&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Provide manual “compact” operations for administrators&lt;/item&gt;
      &lt;item&gt;Use bounded structures for untrusted/public replicas&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Without some form of garbage collection, CRDT state grows unbounded and will eventually exhaust memory or storage. The question isn’t whether to implement GC, but which tradeoffs you’re willing to accept.&lt;/p&gt;
    &lt;p&gt;And, realistically speaking, you’re unlikely to implement a system that only uses CRDTs and no other data storage. You’ll almost certainly have some sort of traditional database to store your data, which you can probably use to periodically coordinate garbage collection.&lt;/p&gt;
    &lt;head rend="h3"&gt;A note on Causal Consistency&lt;/head&gt;
    &lt;p&gt;CRDTs themselves don’t enforce causal delivery. You need a causal broadcast protocol to ensure operations are delivered respecting happens-before relationships. Without causal delivery, some CRDTs (especially operation-based ones) may behave incorrectly.&lt;/p&gt;
    &lt;head rend="h3"&gt;Performance&lt;/head&gt;
    &lt;p&gt;Different CRDTs have different performance characteristics. Consider your read/write ratio, expected contention, and replica count when choosing:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;CRDT Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Space Complexity&lt;/cell&gt;
        &lt;cell role="head"&gt;Add/Insert&lt;/cell&gt;
        &lt;cell role="head"&gt;Remove/Delete&lt;/cell&gt;
        &lt;cell role="head"&gt;Merge&lt;/cell&gt;
        &lt;cell role="head"&gt;Read/Query&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;G-Counter&lt;/cell&gt;
        &lt;cell&gt;O(r)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;O(r)&lt;/cell&gt;
        &lt;cell&gt;O(r)&lt;/cell&gt;
        &lt;cell&gt;Space: one counter per replica&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;PN-Counter&lt;/cell&gt;
        &lt;cell&gt;O(r)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(r)&lt;/cell&gt;
        &lt;cell&gt;O(r)&lt;/cell&gt;
        &lt;cell&gt;Double the space of G-Counter&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;G-Set&lt;/cell&gt;
        &lt;cell&gt;O(e)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;O(e)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;Standard set operations&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;2P-Set&lt;/cell&gt;
        &lt;cell&gt;O(e)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(e)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;Both added and removed sets grow&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;LWW-Element-Set&lt;/cell&gt;
        &lt;cell&gt;O(e)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(e)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;Can GC old timestamps carefully&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;OR-Set&lt;/cell&gt;
        &lt;cell&gt;O(e × t)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(t)&lt;/cell&gt;
        &lt;cell&gt;O(e × t)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;Tags accumulate, needs GC&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;LWW-Register&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;Minimal overhead&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;MV-Register&lt;/cell&gt;
        &lt;cell&gt;O(concurrent)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;O(c)&lt;/cell&gt;
        &lt;cell&gt;O(c)&lt;/cell&gt;
        &lt;cell&gt;Returns set of concurrent values&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;OR-Map&lt;/cell&gt;
        &lt;cell&gt;O(k × t)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(t)&lt;/cell&gt;
        &lt;cell&gt;O(k × t)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;Per-key OR-Set overhead&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;RGA&lt;/cell&gt;
        &lt;cell&gt;O(n + d)&lt;/cell&gt;
        &lt;cell&gt;O(log n)&lt;/cell&gt;
        &lt;cell&gt;O(log n)&lt;/cell&gt;
        &lt;cell&gt;O(n + d)&lt;/cell&gt;
        &lt;cell&gt;O(n)&lt;/cell&gt;
        &lt;cell&gt;Tombstones accumulate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;WOOT&lt;/cell&gt;
        &lt;cell&gt;O(n + d)&lt;/cell&gt;
        &lt;cell&gt;O(n²) worst&lt;/cell&gt;
        &lt;cell&gt;O(log n)&lt;/cell&gt;
        &lt;cell&gt;O(n + d)&lt;/cell&gt;
        &lt;cell&gt;O(n²) worst&lt;/cell&gt;
        &lt;cell&gt;Linearization is expensive&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Logoot/LSEQ&lt;/cell&gt;
        &lt;cell&gt;O(n × p)&lt;/cell&gt;
        &lt;cell&gt;O(log n)&lt;/cell&gt;
        &lt;cell&gt;O(log n)&lt;/cell&gt;
        &lt;cell&gt;O(n)&lt;/cell&gt;
        &lt;cell&gt;O(n log n)&lt;/cell&gt;
        &lt;cell&gt;Position identifiers grow&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Legend:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;r&lt;/code&gt;= number of replicas&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;e&lt;/code&gt;= number of elements in set&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;t&lt;/code&gt;= average tags per element (OR-Set)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;k&lt;/code&gt;= number of keys in map&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;n&lt;/code&gt;= number of visible elements in sequence&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;d&lt;/code&gt;= number of deleted elements (tombstones)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;c&lt;/code&gt;= number of concurrent writes&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;p&lt;/code&gt;= average position identifier length&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Key Observations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Counter CRDTs scale with replica count, not operation count. A billion increments still cost O(replicas) space.&lt;/item&gt;
      &lt;item&gt;Set CRDTs generally have constant-time operations, but OR-Set’s space grows with tags unless garbage collected.&lt;/item&gt;
      &lt;item&gt;Sequence CRDTs suffer from tombstone accumulation. RGA is typically faster than WOOT in practice despite similar asymptotic complexity.&lt;/item&gt;
      &lt;item&gt;Position-based sequences (Logoot/LSEQ) trade time complexity for avoiding explicit parent pointers, but position identifiers can grow pathologically.&lt;/item&gt;
      &lt;item&gt;Merge operations are often the bottleneck in high-throughput systems. Delta CRDTs dramatically improve merge performance by sending only changes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Libraries and Implementations&lt;/head&gt;
    &lt;p&gt;Many CRDT libraries exist:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Automerge: Full-featured CRDT library for JSON-like documents15&lt;/item&gt;
      &lt;item&gt;Yjs: Optimized for collaborative editing16&lt;/item&gt;
      &lt;item&gt;Riak: Database with built-in CRDT support17&lt;/item&gt;
      &lt;item&gt;Redis Enterprise: CRDT-enabled Redis18&lt;/item&gt;
      &lt;item&gt;AntidoteDB: CRDT-native database19&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each makes different tradeoff decisions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Further Reading&lt;/head&gt;
    &lt;p&gt;The CRDT literature is vast and honestly a bit scattered across conference proceedings. Here are the key papers worth reading:&lt;/p&gt;
    &lt;p&gt;Foundational:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Shapiro et al., “Conflict-Free Replicated Data Types” (2011): The original CRDT paper, defining state-based and operation-based CRDTs.&lt;/item&gt;
      &lt;item&gt;Shapiro et al., “A Comprehensive Study of Convergent and Commutative Replicated Data Types” (2011): Detailed technical report covering many CRDTs. This is the one you want to bookmark.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Sequence CRDTs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Oster et al., “Data Consistency for P2P Collaborative Editing” (2006): Introduces WOOT.&lt;/item&gt;
      &lt;item&gt;Roh et al., “Replicated Abstract Data Types: Building Blocks for Collaborative Applications” (2011): Introduces RGA.&lt;/item&gt;
      &lt;item&gt;Weiss et al., “Logoot: A Scalable Optimistic Replication Algorithm for Collaborative Editing” (2009): Introduces Logoot.&lt;/item&gt;
      &lt;item&gt;Nédelec et al., “LSEQ: An Adaptive Structure for Sequences in Distributed Collaborative Editing” (2013): Introduces LSEQ.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Advanced Topics:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Baquero et al., “Making Operation-based CRDTs Operation-based” (2014): Pure operation-based CRDTs without state.&lt;/item&gt;
      &lt;item&gt;Almeida et al., “Delta State Replicated Data Types” (2018): Efficiency improvements for state-based CRDTs.&lt;/item&gt;
      &lt;item&gt;Kleppmann et al., “A Conflict-Free Replicated JSON Datatype” (2017): Automerge’s design.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Surveys:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Shapiro et al., “Convergent and Commutative Replicated Data Types” (2011): The comprehensive technical report. Start here if you want depth.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Wrapping up&lt;/head&gt;
    &lt;p&gt;CRDTs are not a silver bullet. They trade coordination for metadata, strong consistency for eventual consistency, and simplicity for convergence guarantees. But in scenarios where availability matters more than immediate consistency, they’re remarkably powerful.&lt;/p&gt;
    &lt;p&gt;There is no “best” CRDT, only CRDTs suited to different problems; the CRDT you choose depends entirely on your application’s semantics:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What operations do you need (add, remove, re-add)?&lt;/item&gt;
      &lt;item&gt;Can you tolerate lost updates?&lt;/item&gt;
      &lt;item&gt;Do you need to detect conflicts or resolve them automatically?&lt;/item&gt;
      &lt;item&gt;What’s your tolerance for metadata overhead?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The CRDT abstraction is elegant in theory, but bewildering in practice because there are so many instances with subtle differences. Hopefully this guide has cut through some of the confusion, and given you a good intuition for how they work and when to use them.&lt;/p&gt;
    &lt;p&gt;I honestly still haven’t hit a use case for CRDTs that I couldn’t solve with a traditional database and some custom coordination logic. But sometimes we just want to learn for the sake of learning. If you beat me to it, let me know!&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The term “Conflict-free Replicated Data Type” was coined by Marc Shapiro, Nuno Preguiça, Carlos Baquero, and Marek Zawirski in their 2011 paper “Conflict-free Replicated Data Types” (technical report) and the 2011 SSS conference paper “A comprehensive study of Convergent and Commutative Replicated Data Types”. The theoretical foundations draw from earlier work on commutative replicated data types and optimistic replication. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;WOOT was introduced by Oster, Urso, Molli, and Imine in “Data Consistency for P2P Collaborative Editing” (2006). The name is a play on “OT” (Operational Transformation), emphasizing that it achieves similar goals “WithOut OT.” WOOT was one of the first practical sequence CRDTs and influenced many subsequent designs. ↩ ↩2&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;State-based CRDTs are also called “convergent” replicated data types (CvRDT). The “Cv” stands for “convergent” - emphasizing that replicas converge to the same state by repeatedly applying the join operation. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Operation-based CRDTs are also called “commutative” replicated data types (CmRDT). They require causal delivery of operations - if operation A happened before operation B on the same replica, B must not be delivered before A at any other replica. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The G-Counter appears in Shapiro et al.’s 2011 technical report “A Comprehensive Study of Convergent and Commutative Replicated Data Types” as one of the foundational examples demonstrating CRDT principles. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The space complexity is O(n) where n is the number of replicas, not the number of increments. This means G-Counters scale well with the number of operations but require tracking all replicas that have ever incremented the counter. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The OR-Set (Observed-Remove Set) was introduced by Shapiro et al. in their 2011 technical report. It’s also known as the “Add-Wins Set” because concurrent add and remove operations result in the element remaining in the set. The key innovation is using unique tags to distinguish between different additions of the same element. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sequence CRDTs are particularly challenging because positional indices change as elements are inserted or deleted. Unlike sets or counters where elements have stable identity, sequences must maintain ordering despite concurrent modifications at arbitrary positions. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;RGA was introduced by Roh et al. in “Replicated Abstract Data Types: Building Blocks for Collaborative Applications” (2011). The name “Replicated Growable Array” emphasizes that it’s an array-like structure that can grow through replication. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;YATA (Yet Another Transformation Approach) was developed by Kevin Jahns for the Yjs collaborative editing library. It combines ideas from RGA and WOOT while optimizing for the common case of sequential insertions (typing). Yjs is used in production by companies like Braid, Row Zero, and others for real-time collaboration. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Version vectors were introduced by Parker et al. in “Detection of Mutual Inconsistency in Distributed Systems” (1983). They extend Lamport’s logical clocks to track causality in distributed systems. Each replica maintains a vector of logical clocks (one for each replica), enabling precise causal ordering without requiring synchronized physical clocks. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Delta CRDTs were introduced by Almeida, Shoker, and Baquero in “Delta State Replicated Data Types” (2018). They bridge the gap between state-based and operation-based CRDTs, achieving operation-based bandwidth efficiency while maintaining state-based simplicity. Most production CRDT systems (Riak, Automerge) use delta-state internally. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Logoot was introduced by Weiss, Urso, and Molli in “Logoot: A Scalable Optimistic Replication Algorithm for Collaborative Editing” (2009). The name combines “log” (logarithmic complexity) with “oot” from WOOT, its predecessor. Logoot’s position-based approach influenced many subsequent CRDTs including LSEQ and Treedoc. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;LSEQ was introduced by Nédelec, Molli, Mostéfaoui, and Desmontils in “LSEQ: An Adaptive Structure for Sequences in Distributed Collaborative Editing” (2013). The key innovation is using different allocation strategies (boundary+ vs boundary-) based on tree depth, which keeps position identifiers shorter in practice compared to Logoot’s fixed strategy. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Automerge, created by Martin Kleppmann and collaborators, implements a JSON CRDT described in “A Conflict-Free Replicated JSON Datatype” (2017). It uses a columnar encoding for efficiency and has been rewritten in Rust for performance. Used by production apps like Inkandswitch’s Pushpin. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Yjs, created by Kevin Jahns, is optimized for text editing and uses the YATA algorithm. It’s notably faster than Automerge for text operations and includes bindings for popular editors like CodeMirror, Monaco, Quill, and ProseMirror. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Riak, a distributed database from Basho, was one of the first production systems to adopt CRDTs (2012). It implements counters, sets, and maps as native data types, using Delta CRDTs internally to minimize bandwidth. Sadly, the company collapsed dramatically, and the project was abandoned for quite some time. I think it’s still around in a diminished form, but haven’t tried it in a while. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Redis Enterprise’s CRDT support (Active-Active deployment) uses operation-based CRDTs with causal consistency. It supports strings, hashes, sets, and sorted sets with CRDT semantics, enabling multi-master Redis deployments. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;AntidoteDB is a research database from the SyncFree project that makes CRDTs the primary abstraction. Unlike other databases where CRDTs are a feature, AntidoteDB is designed from the ground up around CRDT semantics, providing highly available transactions over CRDTs. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46087022</guid><pubDate>Sat, 29 Nov 2025 12:25:35 +0000</pubDate></item><item><title>Datacenters in space aren't going to work</title><link>https://taranis.ie/datacenters-in-space-are-a-terrible-horrible-no-good-idea/</link><description>&lt;doc fingerprint="35ba347362661da9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Datacenters in space are a terrible, horrible, no good idea.&lt;/head&gt;
    &lt;p&gt;There is a rush for AI companies to team up with space launch/satellite companies to build datacenters in space. TL;DR: It's not going to work.&lt;/p&gt;
    &lt;p&gt;In the interests of clarity, I am a former NASA engineer/scientist with a PhD in space electronics. I also worked at Google for 10 years, in various parts of the company including YouTube and the bit of Cloud responsible for deploying AI capacity, so I'm quite well placed to have an opinion here.&lt;/p&gt;
    &lt;p&gt;The short version: this is an absolutely terrible idea, and really makes zero sense whatsoever. There are multiple reasons for this, but they all amount to saying that the kind of electronics needed to make a datacenter work, particularly a datacenter deploying AI capacity in the form of GPUs and TPUs, is exactly the opposite of what works in space. If you've not worked specifically in this area before, I'll caution against making gut assumptions, because the reality of making space hardware actually function in space is not necessarily intuitively obvious.&lt;/p&gt;
    &lt;p&gt;Power&lt;/p&gt;
    &lt;p&gt;The first reason for doing this that seems to come up is abundant access to power in space. This really isn't the case. You basically have two options: solar and nuclear. Solar means deploying a solar array with photovoltaic cells – something essentially equivalent to what I have on the roof of my house here in Ireland, just in space. It works, but it isn't somehow magically better than installing solar panels on the ground – you don't lose that much power through the atmosphere, so intuition about the area needed transfers pretty well. The biggest solar array ever deployed in space is that of the International Space Station (ISS), which at peak can deliver a bit over 200kW of power. It is important to mention that it took several Shuttle flights and a lot of work to deploy this system – it measures about 2500 square metres, over half the size of an American football field.&lt;/p&gt;
    &lt;p&gt;Taking the NVIDIA H200 as a reference, the per-GPU-device power requirements are on the order of 0.7kW per chip. These won't work on their own, and power conversion isn't 100% efficient, so in practice 1kW per GPU might be a better baseline. A huge, ISS-sized, array could therefore power roughly 200 GPUs. This sounds like a lot, but lets keep some perspective: OpenAI's upcoming Norway datacenter is intending to house 100,000 GPUs, probably each more power hungry than the H200. To equal this capacity, you'd need to launch 500 ISS-sized satellites. In contrast, a single server rack (as sold by NVIDIA preconfigured) will house 72 GPUs, so each monster satellite is only equivalent to roughly three racks.&lt;/p&gt;
    &lt;p&gt;Nuclear won't help. We are not talking nuclear reactors here – we are talking about radioisotope thermal generators (RTGs), which typically have a power output of about 50W - 150W. So not enough to even run a single GPU, even if you can persuade someone to give you a subcritical lump of plutonium and not mind you having hundreds of chances to scatter it across a wide area when your launch vehicle explosively self-disassembles.&lt;/p&gt;
    &lt;p&gt;Thermal Regulation&lt;/p&gt;
    &lt;p&gt;I've seen quite a few comments about this concept where people are saying things like, "Well, space is cold, so that will make cooling really easy, right?"&lt;/p&gt;
    &lt;p&gt;Um...&lt;/p&gt;
    &lt;p&gt;No.&lt;/p&gt;
    &lt;p&gt;Really, really no.&lt;/p&gt;
    &lt;p&gt;Cooling on Earth is relatively straightforward. Air convection works pretty well – blow air across a surface, particularly one designed to have a large surface area to volume ratio like a heatsink, will transfer heat from the heatsink to the air quite effectively. If you need more power density than can be directly cooled in this way (and higher power GPUs are definitely in that category), you can use liquid cooling to transfer heat from the chip to a larger radiator/heatsink elsewhere. In datacenters on Earth, it is common to set up cooling loops where machines are cooled via chilled coolant (usually water) that is pumped around racks, with the heat extracted and cold coolant returned to the loop. Typically the coolant is cooled via convective cooling to the air, so one way or another this is how things work on Earth.&lt;/p&gt;
    &lt;p&gt;In space, there is no air. The environment is close enough to a hard, total vacuum as makes no practical difference, so convection just doesn't happen. On the space engineering side, we typically think about thermal management, not just cooling. Thing is, space doesn't really have a temperature as-such. Only materials have a temperature. It may come as a surprise, but in the Earth-Moon system the average temperature of pretty much anything is basically the same as the average temperature of Earth, because this is why Earth has that particular temperature. If a satellite is rotating, a bit like a chicken on a rotisserie, it will tend toward having a consistent temperature that's roughly similar to that of the Earth surface. If it isn't rotating, the side pointing away from the sun will tend to get progressively colder, with a limit due to the cosmic microwave background, around 4 Kelvin, just a little bit above absolute zero. On the sunward side, things can get a bit cooked, hitting hundreds of centigrade. Thermal management therefore requires very careful design, making sure that heat is carefully directed where it needs to go. Because there is no convection in a vacuum, this can only be achieved by conduction, or via some kind of heat pump.&lt;/p&gt;
    &lt;p&gt;I've designed space hardware that has flown in space. In one particular case, I designed a camera system that needed to be very small and lightweight, whilst still providing science-grade imaging capabilities. Thermal management was front and centre in the design process – it had to be, because power is scarce in small spacecraft, and thermal management has to be achieved whilst keeping mass to a minimum. So no heat pumps or fancy stuff for me – I went in the other direction, designing the system to draw a maximum of about 1 watt at peak, dropping to around 10% of that when the camera was idle. All this electrical power turns into heat, so if I can draw 1 watt only while capturing an image, then turn the image sensor off as soon as the data is in RAM, I can halve the consumption, then when the image has been downloaded to the flight computer I can turn the RAM off and drop the power down to a comparative trickle. The only thermal management needed was bolting the edge of the board to the chassis so the internal copper planes in the board could transfer any heat generated.&lt;/p&gt;
    &lt;p&gt;Cooling even a single H200 will be an absolute nightmare. Clearly a heatsink and fan won't do anything at all, but there is a liquid cooled H200 variant. Let's say this was used. This heat would need to be transferred to a radiator panel – this isn't like the radiator in your car, no convection, remember? – which needs to radiate heat into space. Let's assume that we can point this away from the sun.&lt;/p&gt;
    &lt;p&gt;The Active Thermal Control System (ATCS) on the ISS is an example of such a thermal control system. This is a very complex system, using an ammonia cooling loop and a large thermal radiator panel system. It has a dissipation limit of 16kW, so roughly 16 H200 GPUs, a bit over the equivalent to a quarter of a ground-based rack. The thermal radiator panel system measures 13.6m x 3.12 m, i.e., roughly 42.5 square metres. If we use 200kW as a baseline and assume all of that power will be fed to GPUs, we'd need a system 12.5 times bigger, i.e., roughly 531 square metres, or about 2.6 times the size of the relevant solar array. This is now going to be a very large satellite, dwarfing the ISS in area, all for the equivalent of three standard server racks on Earth.&lt;/p&gt;
    &lt;p&gt;Radiation Tolerance&lt;/p&gt;
    &lt;p&gt;This is getting into my PhD work now. Assuming you can both power and cool your electronics in space, you have the further problem of radiation tolerance.&lt;/p&gt;
    &lt;p&gt;The first question is where in space?&lt;/p&gt;
    &lt;p&gt;If you are in low Earth orbit (LEO), you are inside the inner radiation belt, where radiation dose is similar to that experienced by high altitude aircraft – more than an airliner, but not terrible. Further out, in mid Earth orbit (MEO), where the GPS satellites live, they are not protected by the Van Allen belts – worse, this orbit is literally inside them. Outside the belts, you are essentially in deep space (details vary with how close to the Sun you happen to be, but the principles are similar).&lt;/p&gt;
    &lt;p&gt;There are two main sources of radiation in space – from our own star, the Sun, and from deep space. This basically involves charged particles moving at a substantial percentage of the speed of light, from electrons to the nuclei of atoms with masses up to roughly that of oxygen. These can cause direct damage, by smashing into the material from which chips are made, or indirectly, by travelling through the silicon die without hitting anything but still leaving a trail of charge behind them.&lt;/p&gt;
    &lt;p&gt;The most common conseqence of this happening is a single-event upset (SEU), where a direct impact or (more commonly) a particle passing through a transistor briefly (approx 600 picoseconds) causes a pulse to happen where it shouldn't have. If this causes a bit to be flipped, we call this a SEU. Other than damage to data, they don't cause permanent damage.&lt;/p&gt;
    &lt;p&gt;Worse is single-event latch-up. This happens when a pulse from a charged particle causes a voltage to go outside the power rails powering the chip, causing a transistor essentially to turn on and stay on indefinitely. I'll skip the semiconductor physics involved, but the short version is that if this happens in a bad way, you can get a pathway connected between the power rails that shouldn't be there, burning out a gate permanently. This may or may not destroy the chip, but without mitigation it can make it unusable.&lt;/p&gt;
    &lt;p&gt;For longer duration missions, which would be the case with space based datacenters because they would be so expensive that they would have to fly for a long time in order to be economically viable, it's also necessary to consider total dose effects. Over time, the performance of chips in space degrades, because repeated particle impacts make the tiny field-effect transistors switch more slowly and turn on and off less completely. In practice, this causes maximum viable clock rates to decay over time, and for power consumption to increase. Though not the hardest issue to deal with, this must still be mitigated or you tend to run into a situation where a chip that was working fine at launch stops working because either the power supply or cooling has become inadequate, or the clock is running faster than the chip can cope with. It's therefore necessary to have a clock generator that can throttle down to a lower speed as needed – this can also be used to control power consumption, so rather than a chip ceasing to function it will just get slower.&lt;/p&gt;
    &lt;p&gt;The next FAQ is, can't you just use shielding? No, not really, or maybe up to a point. Some kinds of shielding can make the problem worse – an impact to the shield can cause a shower of particles that then cause multiple impact at once, which is far harder to mitigate. The very strongest cosmic rays can go through an astonishing amount of solid lead – since mass is always at a premium, it's rarely possible to deploy significant amounts of shielding, so radiation tolerance must be built into the system (this is often described as Radiation Hardness By Design, RHBD).&lt;/p&gt;
    &lt;p&gt;GPUs and TPUs and the high bandwidth RAM they depend on are absolutely worst case for radiation tolerance purposes. Small geometry transistors are inherently much more prone both to SEUs and latch-up. The very large silicon die area also makes the frequency of impacts higher, since that scales with area.&lt;/p&gt;
    &lt;p&gt;Chips genuinely designed to work in space are taped out with different gate structures and much larger geometries. The processors that are typically used have the performance of roughly a 20-year-old PowerPC from 2005. Bigger geometries are inherently more tolerant, both to SEUs and total dose, and the different gate topologies are immune to latch up, whilst providing some degree of SEU mitigation via fine-grained redundancy at the circuit level. Taping out a GPU or TPU with this kind of approach is certainly possible, but the performance would be a tiny fraction of that of a current generation Earth-based GPU/TPU.&lt;/p&gt;
    &lt;p&gt;There is a you-only-live-once (my terminology!) approach, where you launch the thing and hope for the best. This is commonplace in small cubesats, and also why small cubesats often fail after a few weeks on orbit. Caveat emptor!&lt;/p&gt;
    &lt;p&gt;Communications&lt;/p&gt;
    &lt;p&gt;Most satellites communicate with the ground via radio. It is difficult to get much more than about 1Gbps reliably. There is some interesting work using lasers to communicate with satellites, but this depends on good atmospheric conditions to be feasible. Contrasting this with a typical server rack on Earth, where 100Gbps rack-to-rack interconnect would be considered at the low end, and it's easy to see that this is also a significant gap.&lt;/p&gt;
    &lt;p&gt;Conclusions&lt;/p&gt;
    &lt;p&gt;I suppose this is just about possible if you really want to do it, but I think I've demonstrated above that it would firstly be extremely difficult to achieve, disproportionately costly in comparison with Earth-based datacenters, and offer mediocre performance at best.&lt;/p&gt;
    &lt;p&gt;If you still think this is worth doing, good luck, space is hard. Myself, I think it's a catastrophically bad idea, but you do you.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46087616</guid><pubDate>Sat, 29 Nov 2025 14:05:53 +0000</pubDate></item><item><title>Testing shows automotive glassbreakers can't break modern automotive glass</title><link>https://www.core77.com/posts/138925/Testing-Shows-Automotive-Glassbreakers-Cant-Break-Modern-Automotive-Glass</link><description>&lt;doc fingerprint="604813151967efdb"&gt;
  &lt;main&gt;
    &lt;p&gt;It's easy to convince EDC people to buy EDC things. But how do you convince non-EDC folks to buy your product?&lt;/p&gt;
    &lt;p&gt;Simple: Fear.&lt;/p&gt;
    &lt;p&gt;The global "car escape tool market," according to market research firm Data Insights Market, is valued at $500 million this year, projected to grow 7% a year and hit $900 million in 2033. The trend is being driven by "heightened safety concerns among consumers."&lt;/p&gt;
    &lt;p&gt;A big seller in this category is the automotive window breaker and seat belt cutter. The fantasy being peddled by the toolmakers is: You will crash, remain conscious, find that your car has burst into flame or is slowly sinking in water, find that you cannot undo your seatbelt, yet are still able to reach for this specialty tool, slice through your seatbelt, then smash the window open and climb free to safety.&lt;/p&gt;
    &lt;p&gt;This image sure looks real to us!&lt;/p&gt;
    &lt;p&gt;Accidents that involve fire or water are less than half of one percent of all accidents, according to the NHTSA. And the amount of accidents where the above scenario actually occurred, and that tool saved lives, is not recorded as a statistic. Similarly, seat belt jamming is so rare that neither the NHTSA nor the AAA track it as a statistic.&lt;/p&gt;
    &lt;p&gt;As for the glassbreakers, here's the big thing that most people don't realize: They're designed to break tempered glass, which is what most cars used to have for the side windows. However, modern safety regulations—specifically, the "Ejection Mitigation Rule" in the 2013 Federal Motor Vehicle Safety Standard 226 (FMVSS 226), mean that most manufacturers have transitioned to laminated glass for the side windows.&lt;/p&gt;
    &lt;p&gt;Laminated glass (which is what the windshields were already made of) is tougher to break, and is now used to prevent occupants from being ejected through the side glass.&lt;/p&gt;
    &lt;p&gt;An AAA research report tested six commonly-available glassbreakers. Not a single one of them was capable of breaking through laminated glass—and two of the tools couldn't even break through tempered glass, but instead broke themselves. On the glass.&lt;/p&gt;
    &lt;p&gt;It's true that not all automakers have switched over to laminated glass for the side windows; the FMVSS 226 law stipulates that you can get around it if you install elaborate side airbags that also prevent ejection.&lt;/p&gt;
    &lt;p&gt;The automakers that are using laminated side glass are only: Acura, Audi, BMW, Cadillac, Chevrolet, Chrysler, Dodge, Ford, Genesis, GMC, Honda, Hyundai, Infiniti, Jaguar, Jeep, Kia, Land Rover, Lexus, Lincoln, Mercedes-Benz, Nissan, Porsche, Ram, Subaru, Toyota, Volkswagen and Volvo. Some of them, like Chevy and BMW, have been using laminated glass since the '70s and '80s. Glassbreakers might only be useful if you're driving around in a classic car and believe you'll become submerged.&lt;/p&gt;
    &lt;p&gt;That said, seatbelt cutters are of supreme use to firefighters, EMTs and other first responders who may not be able to reach an unconscious accident victim's seatbelt release. So there might be a case for them if you see yourself in a situation where you need to free an unconscious person, and have the training to safely extricate them.&lt;/p&gt;
    &lt;p&gt;Just want to point out that there have been several cases of people not being able to get out of their flaming Teslas because of failures in the door releases. (Not to mention that the emergency releases for the rear doors on some Tesla models are incredibly hard to find in an emergency, if they exist at all.) &lt;lb/&gt;Sure, the odds of an EV going up in flames are pretty low, but if you're in a Tesla, it'd be nice to have something that could break the glass.&lt;/p&gt;
    &lt;p&gt;Create a Core77 Account&lt;/p&gt;
    &lt;p&gt;Already have an account? Sign In&lt;/p&gt;
    &lt;p&gt;By creating a Core77 account you confirm that you accept the Terms of Use&lt;/p&gt;
    &lt;p&gt;Please enter your email and we will send an email to reset your password.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46088379</guid><pubDate>Sat, 29 Nov 2025 15:43:58 +0000</pubDate></item><item><title>AccessOwl (YC S22) Is Hiring a Technical Account Manager (IAM)</title><link>https://www.ycombinator.com/companies/accessowl/jobs/dGC3pcO-technical-account-manager-identity-access-management</link><description>&lt;doc fingerprint="1550a21a95a2b36c"&gt;
  &lt;main&gt;
    &lt;p&gt;Managing your Employees' Access to SaaS&lt;/p&gt;
    &lt;p&gt;TL;DR: You want to work directly with IT and security teams at some of the world’s fastest growing companies, helping them implement, optimize, and expand AccessOwl. You enjoy solving technical problems, guiding integrations, and shaping how a startup delivers value after the deal closes.&lt;/p&gt;
    &lt;p&gt;AccessOwl is building the first AI native Access Governance Suite. We make it radically easier for IT and security teams to manage SaaS access, stay compliant, and eliminate shadow IT, without the overhead of traditional identity systems.&lt;/p&gt;
    &lt;p&gt;We founded AccessOwl out of frustration with manual onboarding, offboarding, and compliance workflows that slow companies down. By combining automation with agentic AI, we are redefining how modern IT admins govern SaaS.&lt;/p&gt;
    &lt;p&gt;We are a profitable, Y Combinator backed startup working with companies like Harvey AI, Monarch, and Motion. Our team is customer centric, pragmatic, and ambitious.&lt;/p&gt;
    &lt;p&gt;To apply, include three sentences on what personally got you interested in talking to us. Skip the generic stuff. We want to hear your real motivation.&lt;/p&gt;
    &lt;p&gt;AccessOwl revolutionizes how businesses manage SaaS applications. Our mission is to simplify SaaS access, spending, and compliance, providing the easiest way to centrally manage apps and user access. AccessOwl replaces Okta, outdated ticketing systems, and spreadsheets, fundamentally transforming how modern IT admins work.&lt;/p&gt;
    &lt;p&gt;We founded AccessOwl out of frustration with the inefficiency caused by SaaS companies exploiting the SSO Tax, which made onboarding and offboarding manual and time-consuming. Our innovative approach leverages RPA and agentic AI workflows to change this.&lt;/p&gt;
    &lt;p&gt;We are a fully remote, customer-centric team dedicated to solving real problems for IT and security teams. Our goal is to build a sustainable business while delivering an exceptional experience our customers genuinely love.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46089008</guid><pubDate>Sat, 29 Nov 2025 17:00:23 +0000</pubDate></item><item><title>Zero knowlege proof of compositeness</title><link>https://www.johndcook.com/blog/2025/11/29/zkp-composite/</link><description>&lt;doc fingerprint="dedcb3f5bedbd58"&gt;
  &lt;main&gt;
    &lt;p&gt;A zero knowledge proof (ZKP) answers a question without revealing anything more than answer. For example, a digital signature proves your possession of a private key without revealing that key.&lt;/p&gt;
    &lt;p&gt;Here’s another example, one that’s more concrete than a digital signature. Suppose you have a deck of 52 cards, 13 of each of spades, hearts, diamonds, and clubs. If I draw a spade from the deck, I can prove that I drew a spade without showing which card I drew. If I show you that all the hearts, diamonds, and clubs are still in the deck, then you know that the missing card must be a spade.&lt;/p&gt;
    &lt;head rend="h2"&gt;Composite numbers&lt;/head&gt;
    &lt;p&gt;You can think of Fermat’s primality test as a zero knowledge proof. For example, I can convince you that the following number is composite without telling you what its factors are.&lt;/p&gt;
    &lt;p&gt;n = 244948974278317817239218684105179099697841253232749877148554952030873515325678914498692765804485233435199358326742674280590888061039570247306980857239550402418179621896817000856571932268313970451989041&lt;/p&gt;
    &lt;p&gt;Fermat’s little theorem says that if n is a prime and b is not a multiple of n, then&lt;/p&gt;
    &lt;p&gt;bn−1 = 1 (mod n).&lt;/p&gt;
    &lt;p&gt;A number b such that bn−1 ≠ 1 (mod n) is a proof that n is not prime, i.e. n is composite. So, for example, b = 2 is a proof that n above is composite. This can be verified very quickly using Python:&lt;/p&gt;
    &lt;quote&gt;&amp;gt;&amp;gt;&amp;gt; pow(2, n-1, n) 10282 ... 4299&lt;/quote&gt;
    &lt;p&gt;I tried the smallest possible base [1] and it worked. In general you may have to try a few bases. And for a few rare numbers (Carmichael numbers) you won’t be able to find a base. But if you do find a base b such that bn−1 is not congruent to 1 mod n, you know with certainty that b is composite.&lt;/p&gt;
    &lt;head rend="h2"&gt;Prime numbers&lt;/head&gt;
    &lt;p&gt;The converse of Fermat’s little theorem is false. It can be used to prove a number is not prime, but it cannot prove that a number is prime. But it can be used to show that a number is probably prime. (There’s some subtlety as to what it means for a number to probably be prime. See here.)&lt;/p&gt;
    &lt;p&gt;Fermat’s little theorem can give you a zero knowledge proof that a number is composite. Can it give you a zero knowledge proof that a number is prime? There are a couple oddities in this question.&lt;/p&gt;
    &lt;p&gt;First, what would it mean to have a zero knowledge proof that a number is prime? What knowledge are you keeping secret? When you prove that a number is composite, the prime factors are secret (or even unknown), but what’s the secret when you say a number is prime? Strictly speaking a ZKP doesn’t have to keep anything secret, but in practice it always does.&lt;/p&gt;
    &lt;p&gt;Second, what about the probability of error? Zero knowledge proofs do not have to be infallible. A ZKP can have some negligible probability of error, and usually do.&lt;/p&gt;
    &lt;p&gt;It’s not part of the definition, but n practice ZKPs are supposed to be easier to verify than the direct approach to what they prove. So you could have something like a primality certificate that takes far less computation to verify than the computation needed to determine from scratch that a number is prime.&lt;/p&gt;
    &lt;head rend="h2"&gt;Proving other things&lt;/head&gt;
    &lt;p&gt;You could think of non-constructive proofs as ZKPs. For example, you could think of the intermediate value theorem as a ZKP: it proves that a function has a zero in an interval without giving you any information about where that zero may be located.&lt;/p&gt;
    &lt;p&gt;What makes ZKPs interesting in application is that they can prove things of more general interest than mathematical statements [2]. For example, cryptocurrencies can provide ZKPs that accounting constraints hold without revealing the inputs or outputs of the transaction. You could prove that nobody tried to spend a negative amount and that the sum of the inputs equals the sum of the outputs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Related posts&lt;/head&gt;
    &lt;p&gt;[1] You could try b = 1, but then bn−1 is always 1. This example shows that the existence of a base for which bn−1 = 1 (mod n) doesn’t prove anything.&lt;/p&gt;
    &lt;p&gt;[2] You might object that accounting rules are mathematical statements, and of course they are. But they’re of little interest to mathematicians and of great interest to the parties in a transaction.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46089394</guid><pubDate>Sat, 29 Nov 2025 17:53:57 +0000</pubDate></item><item><title>Student perceptions of AI coding assistants in learning</title><link>https://arxiv.org/abs/2507.22900</link><description>&lt;doc fingerprint="ac7d350b9de3148f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Human-Computer Interaction&lt;/head&gt;&lt;p&gt; [Submitted on 26 Jun 2025 (v1), last revised 16 Sep 2025 (this version, v4)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:New Kid in the Classroom: Exploring Student Perceptions of AI Coding Assistants&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:The arrival of AI coding assistants in educational settings presents a paradigm shift, introducing a "new kid in the classroom" for both students and instructors. Thus, understanding the perceptions of these key actors about this new dynamic is critical. This exploratory study contributes to this area by investigating how these tools are shaping the experiences of novice programmers in an introductory programming course. Through a two-part exam, we investigated student perceptions by first providing access to AI support for a programming task and then requiring an extension of the solution without it. We collected Likert-scale and open-ended responses from 20 students to understand their perceptions on the challenges they faced. Our findings reveal that students perceived AI tools as helpful for grasping code concepts and boosting their confidence during the initial development phase. However, a noticeable difficulty emerged when students were asked to work unaided, pointing to potential overreliance and gaps in foundational knowledge transfer. These insights highlight a critical need for new pedagogical approaches that integrate AI effectively while effectively enhancing core programming skills, rather than impersonating them.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Sergio Rojas-Galeano [view email]&lt;p&gt;[v1] Thu, 26 Jun 2025 05:59:23 UTC (1,224 KB)&lt;/p&gt;&lt;p&gt;[v2] Tue, 26 Aug 2025 03:23:41 UTC (1,223 KB)&lt;/p&gt;&lt;p&gt;[v3] Wed, 10 Sep 2025 18:10:35 UTC (1,223 KB)&lt;/p&gt;&lt;p&gt;[v4] Tue, 16 Sep 2025 15:09:44 UTC (1,222 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46089546</guid><pubDate>Sat, 29 Nov 2025 18:14:21 +0000</pubDate></item><item><title>Be Like Clippy</title><link>https://be-clippy.com/</link><description>&lt;doc fingerprint="2d35a339896b64a8"&gt;
  &lt;main&gt;
    &lt;p&gt;Fed up with trillion-dollar companies exploiting your data? Forced to use their services? Your data held for ransom? Your data used to train their AI models? Opt-outs for data collection instead of opt-ins?&lt;/p&gt;
    &lt;p&gt;Join the movement to make companies more like Clippy. Set your profile picture to Clippy, make your voice heard.&lt;/p&gt;
    &lt;p&gt;Below is a video that explains the Be Like Clippy movement. It’s a call to action for developers, companies, and users alike to embrace a more open, transparent, and user-friendly approach to technology.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46090172</guid><pubDate>Sat, 29 Nov 2025 19:41:55 +0000</pubDate></item><item><title>Learning Feynman's Trick for Integrals</title><link>https://zackyzz.github.io/feynman.html</link><description>&lt;doc fingerprint="dd489f0901f7d271"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Feynman's Trick&lt;/head&gt;
    &lt;head rend="h3"&gt;a.k.a. Differentiation under the Integral Sign &amp;amp; Leibniz Integral Rule&lt;/head&gt;
    &lt;p&gt;Among a few other integral tricks and techniques, Feynman's trick was a strong reason that made me love evaluating integrals, and although the technique itself goes back to Leibniz being commonly known as the Leibniz integral rule, it was Richard Feynman who popularized it, which is why it is also referred to as Feynman's trick. Here's an excerpt from his book, Surely You're Joking, Mr. Feynman:&lt;/p&gt;
    &lt;p&gt;"One thing I never did learn was contour integration. I had learned to do integrals by various methods shown in a book that my high school physics teacher Mr. Bader had given me.&lt;/p&gt;
    &lt;p&gt;One day he told me to stay after class. "Feynman," he said, "you talk too much and you make too much noise. I know why. You're bored. So I'm going to give you a book. You go up there in the back, in the corner, and study this book, and when you know everything that's in this book, you can talk again."&lt;/p&gt;
    &lt;p&gt;So every physics class, I paid no attention to what was going on with Pascal's Law, or whatever they were doing. I was up in the back with this book: Advanced Calculus, by Woods. Bader knew I had studied Calculus for the Practical Man a little bit, so he gave me the real works -- it was for a junior or senior course in college. It had Fourier series, Bessel functions, determinants, elliptic functions -- all kinds of wonderful stuff that I didn't know anything about.&lt;/p&gt;
    &lt;p&gt;That book also showed how to differentiate parameters under the integral sign -- it's a certain operation. It turns out that's not taught very much in the universities; they don't emphasize it. But I caught on how to use that method, and I used that one damn tool again and again. So because I was self-taught using that book, I had peculiar methods of doing integrals.&lt;/p&gt;
    &lt;p&gt;The result was, when guys at MIT or Princeton had trouble doing a certain integral, it was because they couldn't do it with the standard methods they had learned in school. If it was contour integration, they would have found it; if it was a simple series expansion, they would have found it. Then I come along and try differentiating under the integral sign, and often it worked. So I got a great reputation for doing integrals, only because my box of tools was different from everybody else's, and they had tried all their tools on it before giving the problem to me."&lt;/p&gt;
    &lt;p&gt;For me, employing this trick felt like I was using cheat codes to deal with integrals. At the same time, it enabled a lot of creativity and wishful thinking, which transformed integrals into puzzles. Unfortunately, this also means that there is no clear path on how and when to use this technique. In addition, what Feynman wrote still applies today since the method isn't taught much, if at all, in universities. Therefore, the trick can seem obscure and difficult to grasp for newcomers.&lt;/p&gt;
    &lt;p&gt;In the following section, we will embark on a journey to develop some rules of thumb to have at our disposal when using Feynman's trick. These are merely some heuristics that I tend to use, so deviating from them can be perfectly acceptable. However, I hope that they can provide a path to follow when nothing obvious or intuitive occurs when someone tries to use this trick, or even better, so that they can serve as motivation for someone to start using the method.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hello, World!&lt;/head&gt;
    &lt;p&gt;Feynman already provided a significant hint about the trick when he mentioned differentiating under the integral sign, which is also an alternative name for the technique. More explicitly, if \(f(x,t)\) and \(\frac{\partial f(x,t)}{\partial t}\) is continuous with respect to both variables over the \([a,b]\) interval, then the following holds:&lt;/p&gt;
    &lt;p&gt;This is nice, but not so useful by itself since it doesn't say anything about how and when to apply it. Moreover, learning is not a spectator sport and one has to get their hands dirty as there are no shortcuts to it. Take for example chess, most people could read and understand the rules in a few minutes, however, if they would go on to play a game then most likely they would get stomped by a more experienced player. This is because the other player, through practice, learned some strategies to use when playing.&lt;/p&gt;
    &lt;p&gt;Thus, with the goal to develop some strategies here as well, we will dive straight into action and approach Feynman's trick using practical examples. As a "Hello, World!" introduction, let's take a look at the following integral:&lt;/p&gt;
    &lt;p&gt;You are encouraged to try and evaluate the integral using basic methods, but the logarithm being in the denominator makes this integral quite stubborn to deal with. Feynman's trick aims to get rid of this issue by differentiating under the intgeral sign, with respect to a parameter, in order to obtain an integral that is easier to evaluate.&lt;/p&gt;
    &lt;p&gt;Unfortunately in the integral from above we lack a parameter, therefore the first step is to parameterise the integral, which can even mean introducing a whole function, but for this example we will simply consider:&lt;/p&gt;
    &lt;p&gt;Keep in mind that our original integral is just \(I(1)\). Also, surely we could've placed a parameter in many different places, such as:&lt;/p&gt;
    &lt;p&gt;However, the main idea behind the trick is to obtain an integral that we can evaluate easier, after differentiating with respect to the new parameter. Let's put this in action and see what happens to \(I(t)\).&lt;/p&gt;
    &lt;p&gt;Notice how easy it was to evaluate the integral \(I'(t)=\int_0^1x^tdx\) from above, had we kept \(I(a)\), \(I(b)\) or \(I(c)\) the things wouldn't had simplified at all after differentiating, and most significantly is that we would still have the \(\ln x\) in the denominator, a thing which made the integral hard to deal with in the first place.&lt;/p&gt;
    &lt;p&gt;We can already sense that the following might be an important question in the future: How to parameterise the integral when using Feynman's Trick?&lt;/p&gt;
    &lt;p&gt;We will worry about that a bit later, for now let's finish the integral as we only found \(I'(t)\). Since we are looking to find \(I(1)\) we need to integrate \(I'(t)\) back and set \(t=1\) in order to arrive there. Here it's useful to recall that:&lt;/p&gt;
    &lt;p&gt;For us, \(f(x)\) is just \(I(t)\) in the above expression. Luckily \(I(0)=0\), and as we are looking for \(I=I(1)\) we have:&lt;/p&gt;
    &lt;p&gt;So that is the big picture of Feynman's trick - we have an integral that is hard to evaluate in it's original form, therefore by differentiating under the integral sign we attempt to transform the integral so that it can be easier integrated, and in the end we go back to undo the differentiation step.&lt;/p&gt;
    &lt;head rend="h2"&gt;The parameter&lt;/head&gt;
    &lt;p&gt;As emphasized above, the main goal of the technique is to obtain an integral that is easier to evaluate after differentiating with respect to a parameter, and one issue is that it is not always obvious how to parameterise the integral. In order to make things more intuitively we will play around with the integral from below.&lt;/p&gt;
    &lt;p&gt;The most annoying thing is the logarithm, so if we get rid of it everything should be straightforward. There are a few parameter possibilities which makes sense to consider, namely:&lt;/p&gt;
    &lt;p&gt;With the first one we are out of luck, as differentiating with respect to \(a\) gives:&lt;/p&gt;
    &lt;p&gt;Therefore, if we would try to go back to what we're looking, which is \(I=I(1)-I(0)\), we would end up with \(I=I+\text{other stuff}\). This cancels out \(I\) and we wouldn't be able to recover it. Unfortunately, there's no magic formula that tells a priori whether placing a parameter in a specific place would succeed or fail in evaluating an integral - and sometimes we are simply unlucky.&lt;/p&gt;
    &lt;p&gt;In contrast, things work out nicely with the second choice from above.&lt;/p&gt;
    &lt;p&gt;Again, we are looking to find \(I(1)\), and as \(I(0) = 0\), we have:&lt;/p&gt;
    &lt;p&gt;This works, but we can do even better. Looking at the Hello, World! integral we can see that there we simplified the logarithm in the denominator while performing \(\frac{\partial}{\partial t}x^t\). This is also the first thing that I always attempt to look for when using this technique - namely, to simplify something from the integrand which is independent to the parameter when differentiating. Surely for the current integral we got rid of the logarithm, but the denominator remained intact.&lt;/p&gt;
    &lt;p&gt;In short this will be our first rule of thumb: if possible, place the parameter so that something from the integral, which is not related to the parameter, gets simplified.&lt;/p&gt;
    &lt;p&gt;In order to achieve this with our integral we would need to get rid of \(1+x^2\), and by using \(\ln x=\frac12\ln(x^2)\) we can rewrite the integral as:&lt;/p&gt;
    &lt;p&gt;Finally, in this form it's more natural to place the parameter so that it simplifies \(1+x^2\) when differentiating with respect to \(t\), namely we can consider:&lt;/p&gt;
    &lt;p&gt;Like for \(I(b)\) we are looking to find \(I(1)\), however here \(I(0)\) is equal to \(\frac12\int_0^1\frac{\ln(2x)}{1+x^2}dx\) not \(0\).&lt;/p&gt;
    &lt;p&gt;For this specific integral we only avoided performing partial fractions so there wasn't really a big improvement by simplifying the denominator. However I want to emphasize the importance of this because it will make things come way more natural when deciding where place the parameter. Of course, in case there's not an appropiate or immediate way to achieve this, it's perfectly fine to place the parameter elsewhere too.&lt;/p&gt;
    &lt;p&gt;As mentioned previously, practicing is the best approach to get along with new techniques, therefore below are more integrals to evaluate alongside some hidden steps in case those will be needed. However, I strongly recommend to try and deal with the integrals before looking at any hints, and only check them afterwards for correctness.&lt;/p&gt;
    &lt;p&gt;Consider introducing the following parameter: \[I(t)=\int_0^\frac{\pi}{2} \frac{\ln(1-t\sin x)}{\sin x}dx \Rightarrow I'(t)= -\frac{2\arctan\left(\sqrt{\frac{1+t}{1-t}}\right)}{\sqrt{1-t^2}}\] This should lead to: \[\int_0^\frac{\pi}{2} \frac{\ln(1-\sin x)}{\sin x}dx = I(1) - I(0)=\int_0^1 I'(t) dt \overset{\sqrt{\frac{1-t}{1+t}}=x} = -\frac{3\pi^2}{8}\] But it would be even better if the integral would be parameterised as: \[I(t)=\int_0^\frac{\pi}{2} \frac{\ln(1-\sin t\sin x)}{\sin x}dx\] That is because usually when having trigonometric functions, parameterising the integral with another trigonometric function, leads to a more smoother result.&lt;/p&gt;
    &lt;p&gt;Consider introducing the following parameter: \[I(t)=\int_0^1 \frac{\ln(1-t(x-x^2))}{x-x^2}dx\Rightarrow I'(t) = \frac{4\arctan\left(\sqrt{\frac{t}{4-t}}\right)}{\sqrt{t(4-t)}}\] This should lead to: \[I(1)=\int_0^1 \frac{\ln(1-x+x^2)}{x-x^2}dx = I(1) - I(0) = \int_0^1 I'(t)dt \overset{\sqrt{\frac{4-t}{t}}= x}= -\frac{\pi^2}{9}\]&lt;/p&gt;
    &lt;p&gt;Consider introducing the following parameter: \[I(t)=\int_0^\frac{\pi}{2} \frac{\arctan(t\sin x)}{\sin x}dx\Rightarrow I'(t)=\frac{\pi}{2\sqrt{1+t^2}}\] This should lead to: \[I(1)=\int_0^\frac{\pi}{2} \frac{\arctan(t\sin x)}{\sin x}dx = I(1)-I(0) = \int_0^1 I'(t)dt = \frac{\pi}{2}\ln(1+\sqrt 2)\] It will also work if the integral is parameterised as: \[I(t)=\int_0^\frac{\pi}{2} \frac{\arctan(\tan t\sin x)}{\sin x}dx\] However, in this case the first variant is simple enough to integrate back.&lt;/p&gt;
    &lt;p&gt;Consider introducing the following parameter: \[I(t)=\int_0^\infty x^2e^{-\left(4x^2+\frac{t}{x^2}\right)}dx\Rightarrow I'(t)=-\frac{\sqrt \pi}{4} e^{-4\sqrt t}\] Where the above result follows by using Glasser's master theorem alongside the Gaussian integral. This should lead to: \[\int_0^\infty x^2e^{-\left(4x^2+\frac{9}{x^2}\right)}dx = I(9)- I(0) + I(0) = \int_0^9 I'(t) dt +\frac{\sqrt \pi}{32}=\frac{13}{32}\frac{\sqrt \pi}{e^{12}}\]&lt;/p&gt;
    &lt;p&gt;Consider parameterising the integral as: \[I(t)=\frac12\int_0^1\frac{\ln(1-t(1-x^2))}{1-x^2}dx\Rightarrow I'(t)=\frac{\arctan\left(\sqrt{\frac{t}{1-t}}\right)}{2\sqrt{t(1-t)}}\] This should lead to: \[\int_0^1 \frac{\ln x}{1-x^2}dx = I(1)- I(0) = \int_0^1 I'(t)dt \overset{\sqrt{\frac{1-t}{t}} = x}= -\frac{\pi^2}{8}\]&lt;/p&gt;
    &lt;p&gt;Consider parameterising the integral as: \[I(t)=\int_0^\infty \frac{e^{-t(1+x^2)}}{1+x^2}dx\Rightarrow I'(t) = -\frac{\sqrt \pi}{2\sqrt t}e^{-t}\] This should lead to: \[\int_0^\infty \frac{e^{-x^2}}{1+x^2}dx = e\left(I(1)-I(\infty)\right) = -e\int_1^\infty I'(t)dt= \frac{\pi e}{2}\operatorname{erfc}(1)\] Where \(\operatorname{erfc}(x)\) is the complementary error function.&lt;/p&gt;
    &lt;p&gt;Since \(1-x^2+x^4=(1+x^2)^2-3x^2\), consider parameterising the integral as: \[I(t)=\int_0^\infty \frac{\ln\left(\frac{t(1+x^2)^2-3x^2}{(1-x^2)^2}\right)}{(1+x^2)^2}dx\Rightarrow I'(t)=\frac{\pi}{2\sqrt{t(4t-3)}}\] And in order to go back it should be observed that \(\frac34(1+x^2)^2-3x^2=\frac34(1-x^2)^2\). \[\int_0^\infty \frac{\ln\left(\frac{1-x^2+x^4}{(1-x^2)^2}\right)}{(1+x^2)^2}dx=I(1)- I\left(\frac34\right)+ I\left(\frac34\right)\] \[=\int_\frac34^1 I'(t)dt + \frac{\pi}{4}\ln\left(\frac{3}{4}\right) = \frac{\pi}{2}\ln\left(\frac32\right)\]&lt;/p&gt;
    &lt;head rend="h2"&gt;Accelerated Feynman's trick&lt;/head&gt;
    &lt;p&gt;The previous chapter emphasized to parameterise integrals so that something from the integral, which is not related to the parameter, gets simplified when differentiating (if possible). However there are times when even though we can introduce a parameter to accomplish that, it wouldn't be enough to finish the integral.&lt;/p&gt;
    &lt;p&gt;In this chapter we will look at a different way to obtain this simplification. Let's start by looking at a modified version of an integral that was previously given as an exercise.&lt;/p&gt;
    &lt;p&gt;With \(\int_{-\infty}^\infty \frac{e^{-x^2}}{1+x^2}dx\) it was quite direct to parameterise the integral as \(\int_{-\infty}^\infty \frac{e^{-t(1+x^2)}}{1+x^2}dx\) since it simplifies the denominator, however the similar way to do that for our integral, \(\int_{-\infty}^\infty \frac{e^{-x^2-t(1+x^4)}}{1+x^4}dx\), doesn't seem to work as it complicates things a bit too much.&lt;/p&gt;
    &lt;p&gt;There is however a way to simplify the denominator and in the same time to obtain a decent integral afterwards. Without getting into too much details I will parameterise the integral as:&lt;/p&gt;
    &lt;p&gt;This will seem obscure, but fear not as we will never use this approach again. The whole point is to simplify \(1+x^4\), and the above function was created explicitly to achieve that, as \(\frac{\partial}{\partial t}e^{-tx^2}(x^2\sin t+\cos t)\) is \(-(1+x^4)e^{-tx^2}\sin t\). Note that even though we introduced a couple other terms, those aren't disturbing.&lt;/p&gt;
    &lt;p&gt;Here we are looking to find \(I=I(0)\), and we also have \(I(\infty)=0\), therefore:&lt;/p&gt;
    &lt;p&gt;Where \(S(x)\) and \(C(x)\) are the Fresnel integrals. However, the approach is important here, not the result itself.&lt;/p&gt;
    &lt;p&gt;We can avoid the parametrisation from above by directly using \(\frac{1}{1+x^4}=\int_0^\infty e^{-tx^2}\sin t \, dt\), and then switch to double integrals, or put in other words: employ the accelerated Feynman's trick (in which we skip the usual parameterisation step).&lt;/p&gt;
    &lt;p&gt;The rest goes exactly as with the previous method, as all we did here was to skip differentiation step and instead we switched to double integrals.&lt;/p&gt;
    &lt;p&gt;A natural question that arises here is how did \(\frac{1}{1+x^4}=\int_0^\infty e^{-tx^2}\sin t\, dt\) appear? Or even better, how can someone come up with similar results for other integrals? In the case from above, simply the Laplace transform of the sine function was used, however in general it's useful to have a list of such identities. There are tables of integral results that can be used - for example: Table of Integrals, Series, and Products by Gradshteyn and Ryzhik - but alternatively one can build up their own list of results which tend to appear often while evaluating other integrals.&lt;/p&gt;
    &lt;p&gt;Let's conclude this chapter by evaluating one of the most popular integrals that appears when Feynman's trick gets into the conversation.&lt;/p&gt;
    &lt;p&gt;Since \(\int_0^\infty e^{-xt} dt = \frac{1}{x}\), we can make use of this to rewrite the integral as:&lt;/p&gt;
    &lt;p&gt;Alternatively, we can also consider the parameter version of this integral, \(\int_0^\infty \frac{\sin x}{x}e^{-xt}dx\), however I feel like switching to double integrals is way more intuitively.&lt;/p&gt;
    &lt;p&gt;It might be worth to highlight again that this method should be used preferable when parameterising the integral leads to nowhere. For the above integral, the natural introduction of \(\int_0^\infty \frac{\sin(tx)}{x}dx\) unfortunatelly does fail, as we obtain a divergent integral after differentiating under the integral sign.&lt;/p&gt;
    &lt;p&gt;Like in the previous chapter below are more integrals alongside some hints in order to practice with the accelerated variation of Feynman's trick. However in this case I do recommend to peek at hints faster in case nothing obvious comes to mind, and afterwards to attempt and understand why the mentioned identity can be used.&lt;/p&gt;
    &lt;p&gt;Start by substituting \(x^2\to x\) and then switch to double integrals using: \[\int_0^\infty e^{-xt^2}dt = \frac{\sqrt \pi}{2\sqrt x}\] Where the latter result is due to the Gaussian integral. Also, this integral is one particular case of the Fresnel integral.&lt;/p&gt;
    &lt;p&gt;Switch directly to double integrals by using: \[\int_0^1 \frac{\ln t}{t-\frac{1}{x}}dt = \operatorname{Li}_2(x)\]&lt;/p&gt;
    &lt;p&gt;Switch to double integrals by using the following result: \[\int_0^x \frac{\arctan t}{1+xt}dt = \frac{\arctan x \ln(1+x^2)}{2x}\]&lt;/p&gt;
    &lt;p&gt;Consider switching to double integrals with: \[\frac{x}{\pi^2+x^2}=\Im\left(-\frac{1}{\pi+ix}\right)=-\Im\int_0^\infty e^{-(\pi+ix)t}dt\] It's also really useful to try and see what happens when the Laplace transform of the cosine function is used instead, or the equivalent: \[\frac{x}{\pi^2+x^2}=\Re\left(\frac{1}{i\pi+x}\right)=\Re\int_0^\infty e^{-(i\pi+x)t}dt\]&lt;/p&gt;
    &lt;p&gt;Consider switching to double integrals using: \[\operatorname{Ci}^2(x)+\operatorname{si}^2(x)=\int_0^\infty \frac{e^{-xy}\ln(1+y^2)}{y}dy\]&lt;/p&gt;
    &lt;p&gt;Above \(\operatorname{Li}_2(x)\) denotes the dilogarithm function and \(\operatorname{Ci}(x)\), \(\operatorname{si}(x)\) are the cosine and the sine integral functions, defined as:&lt;/p&gt;
    &lt;head rend="h2"&gt;More Feynman's trick variants&lt;/head&gt;
    &lt;p&gt;We already got familiar with a popular version of Feynman's trick in the previous chapter. Similarly, now we will take a look at other interesting variants of Feynman's trick, which although might appear less often, they can still help to expand the applicability of the technique.&lt;/p&gt;
    &lt;head rend="h3"&gt;Differentiating under the integral sign&lt;/head&gt;
    &lt;p&gt;We will start by taking a look at a much simpler case of Feynman's trick, namely, in the situation when it would be enough to simply differentiate under the integral sign without performing that "undo" step to integrate back.&lt;/p&gt;
    &lt;p&gt;As a small note, it's true that "differentiating under the integral sign" tends to be used as an alternative name for Feynman's trick, however I prefer to keep this for the variant where only the differentiating process takes part, or as mentioned above, when there's no need to integrate back the result, and the name describes quite literally what we are doing.&lt;/p&gt;
    &lt;p&gt;Let's make this more clear by looking at the following integral:&lt;/p&gt;
    &lt;p&gt;We are already aware from the Hello, World! integral how \(\ln x\) can be simplified, since \(\frac{\partial}{\partial a}x^a = x^a \ln x\). However, by introducing the parameter in that original form as \(x^a \ln^2 x\), we would just produce a third logarithm, so that's going in the opposite direction.&lt;/p&gt;
    &lt;p&gt;Fortunatelly, if we take a step back, we can observe that after we find the result of \(\int_0^1 x^a dx\), then differentiating it w.r.t.\(a\) would give us as many logarithms as we want. So, let's put that integral to use.&lt;/p&gt;
    &lt;p&gt;Of course the integral itself was quite simple this time, however the important part that should be highlighted is that not always we need to perform that "undo" step after differentiating under the integral sign - and sometimes knowing a general integral result can provide us more useful integrals by differentiating it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Feynman's trick &amp;amp; indefinite integrals&lt;/head&gt;
    &lt;p&gt;Further, we will take a look at how Feynman's trick can be applied to indefinite integrals. Let's consider:&lt;/p&gt;
    &lt;p&gt;In this form it makes no sense to differentiate the integral with respect to any parameter, but we can extend the integral with temporary bounds by writing:&lt;/p&gt;
    &lt;p&gt;After this we can go on apply Feynman's trick, however, first we are going get rid of the square root via the substitution \(\frac{1}{\sqrt x}\to x\).&lt;/p&gt;
    &lt;p&gt;Here, we can notice that the derivative of \(ax-\frac{b}{x}\) is \(a+\frac{b}{x^2}\) so it would be quite helpful if we had that additional term. In the same time if we differentiate the integrand with respect to \(b\) we'll produce \(a-\frac{b}{x^2}\), which is really useful as \((ax-b/x)^2\) is equal to \((ax+b/x)^2+4ab\) and the derivative of \(ax+\frac{b}{x}\) is \(a-\frac{b}{x^2}\). So let's differentiate as mentioned above:&lt;/p&gt;
    &lt;p&gt;Where \(\operatorname{erfc}(x)\) is the complementary error function. Now we'll go back to \(I(a,b,t)\), but we should be careful to replace the dummy variable \(b\), with something else as the \(b\) parameter does also appear in the bounds.&lt;/p&gt;
    &lt;p&gt;Or for the indefinite integral, this would lead to:&lt;/p&gt;
    &lt;head rend="h3"&gt;Feynman's trick &amp;amp; power series&lt;/head&gt;
    &lt;p&gt;Next, we will take a look at how to combine Feynman's trick with power series. For this we are going to look at:&lt;/p&gt;
    &lt;p&gt;We are already got familiar with what to do when there is a logarithm in the denominator as we saw that we can get rid of them by using \(\frac{d}{dt} x^t = x^t\ln x\), however here also the \(1-xy\) term appears. In order to solve this issue we'll make use of the geoemtric series, namely \(\frac{1}{1-x}=\sum_{n=0}^\infty x^n\), but we will expand into series a bit later and for now continue with the following integral:&lt;/p&gt;
    &lt;p&gt;Now we have to to get back to \(I(n)\):&lt;/p&gt;
    &lt;p&gt;And finally, we'll put the geometric series to use.&lt;/p&gt;
    &lt;p&gt;So the result is simply \(1-2\gamma\), where \(\gamma\) is the Euler-Mascheroni constant.&lt;/p&gt;
    &lt;head rend="h3"&gt;Feynman's trick &amp;amp; differential equations&lt;/head&gt;
    &lt;p&gt;In what's to come we are going to take a look at a combination between Feynman's trick and differential equations. Let's consider the following integral:&lt;/p&gt;
    &lt;p&gt;We can start by parameterising the cosine function and then employ the accelerated Feynman's trick:&lt;/p&gt;
    &lt;p&gt;We haven't made much progress above, since we simply arrived at another integral with \(x\sin(tx)\) instead of \(\cos(tx)\), thus complexity is the same. However, as \(\frac{\partial}{\partial t}\cos(tx)\) is \(x\sin(tx)\), differentiating \(I(t)\) gives us a differential equation to work with, namely:&lt;/p&gt;
    &lt;p&gt;As a small note for the starting step, although employing the accelerated Feynman's trick was rather obvious as to get rid of the denominator, the additional introduction of the \(t\) parameter might be weird first. However performing the same steps without this parameter gives us:&lt;/p&gt;
    &lt;p&gt;Which indicates that one might put to use the fact that \(I(1)=-I'(1)\), by adding the additional \(t\) parameter.&lt;/p&gt;
    &lt;head rend="h3"&gt;Generalizing Feynman's trick&lt;/head&gt;
    &lt;p&gt;So far we've seen the Feynman's trick applied only when the parameter was inside the integrand, however it can also be used when the bounds are parameterised as well. More generally, the following holds:&lt;/p&gt;
    &lt;p&gt;We'll put this to use with the integral from below.&lt;/p&gt;
    &lt;p&gt;Above we can see that the same \(\sqrt 2\) appears in both the lower bound and the \(\operatorname{arccosh}\) function, so we'll parameterise the integral as:&lt;/p&gt;
    &lt;p&gt;We're looking to find \(I=I\left(\sqrt 2\right)\), and since \(I\left(1\right)=0\), we have:&lt;/p&gt;
    &lt;head rend="h3"&gt;Generating integrals using Feynman's trick&lt;/head&gt;
    &lt;p&gt;Now we'll take a look at a fancier way to use Feynman's trick, especially in order to generate new integrals, for this we're considering:&lt;/p&gt;
    &lt;p&gt;Note that we are not trying to evaluate the above integral, instead we are simply using it in order to build up new integrals with the result that follows after differentiating w.r.t. \(t\).&lt;/p&gt;
    &lt;p&gt;We also have that \(I(\pi)=-\frac{\pi^2}{4}\) and \(I(0)=\frac{\pi^2}{8}\), therefore:&lt;/p&gt;
    &lt;p&gt;In retrospect, this integral also appeared as an exercise in the second chapter, and with the same suggestion from there, we can evaluate the integral by applying Feynman's trick to:&lt;/p&gt;
    &lt;p&gt;Admittedly, following this parameterisation is much more intuitevely than what we've shown with the new variation, however it's also useful to have this trick in the bag.&lt;/p&gt;
    &lt;p&gt;To keep the practice going, underneath are listed some integrals that can be evaluated with one version of Feynman's trick described in this chapter.&lt;/p&gt;
    &lt;p&gt;Start by showing that: \[I(t)=\int_1^\infty \int_1^\infty e^{-t(x+y)}dxdy = \left(\frac{e^{-t}}{t}\right)^2\] Then differentiate both sides two times with respect to \(t\) and set \(t=1\).&lt;/p&gt;
    &lt;p&gt;Differentiate four times with respect to \(n\) the following extended indefinite integral: \[ I(n,t) = \int_0^t \cos(nx) dx \]&lt;/p&gt;
    &lt;p&gt;Combine the geometric series \(\sum\limits_{n=0}^\infty (-1)^n x^n = \frac{1}{1+x}\) with: \[I(t)=\int_0^1 \int_0^1 \frac{(xy)^t}{\ln(xy)}dxdy\]&lt;/p&gt;
    &lt;p&gt;Solve the resulting differential equation after differentiating twice the following integral: \[ I(t) = \int_0^\infty \frac{\sin^2 (tx)}{x^2(1+x^2)}dx \]&lt;/p&gt;
    &lt;p&gt;Split the integral in two parts, then substitute \(x\to tx\) and respectively \(x\to \frac{x}{t}\) in the resulting integrals to obtain: \[I(t)= \int_0^1 \ln x \left(\frac{1}{t+x}+\frac{1}{\frac{1}{t}+x}\right)dx = \int_0^\frac{1}{t} \frac{\ln(tx)}{1+x}dx + \int_0^t \frac{\ln\left(\frac{x}{t}\right)}{1+x}dx\] Now employ the Feynman's trick (in the generalized variant).&lt;/p&gt;
    &lt;p&gt;In disguise this exercise is a reformulation of the following identity: \[ \operatorname{Li}_2(-t)+\operatorname{Li}_2\left(-\frac{1}{t}\right) = -\frac{\pi^2}{6} - \frac{1}{2}\ln^2 t \] Where \(\operatorname{Li}_2(x)\) is the dilogarithm function.&lt;/p&gt;
    &lt;p&gt;This integral can be generated starting from: \[I(t)=\int_0^\infty \frac{e^{-t(1+x^2)}}{1+x^2}dx\Rightarrow I'(t)=-e^{-t}\int_0^\infty e^{-tx^2} dx\overset{\sqrt t x \to x}= -\frac{e^{-t}}{\sqrt t}\int_0^\infty e^{-x^2} dx\] Afterwards it should be observed that \(I(0)=\frac{\pi}{2}\) and \(I(\infty)=0\), therefore: \[\frac{\pi}{2} = \int_0^\infty \frac{e^{-t}}{\sqrt t} dt \int_0^\infty e^{-x^2} dx \overset{t\to t^2} = 2\left(\int_0^\infty e^{-x^2}dx\right)^2 \]&lt;/p&gt;
    &lt;head rend="h2"&gt;Feynman's trick in practice&lt;/head&gt;
    &lt;p&gt;In this last chapter we'll dive into some "real-world" integrals and observe how Feynman's trick can be adapted for them. Additionally, we will also attempt to build up some heuristics that'll help us manipulate the integrals up to a point where we can introduce an useful parameter. This is due to the fact that with most of the previous examples we had the luxury to parameterise the integrals as they appeared, however often this might not be the case.&lt;/p&gt;
    &lt;head rend="h3"&gt;Breaking the rules&lt;/head&gt;
    &lt;p&gt;In the second chapter we've observed how parameterising the integral so that it also simplifies some parts of the integral can make things more intuitively. However, we can always look for better, especially when our approach doesn't seem elegant enough.&lt;/p&gt;
    &lt;p&gt;Let's see how can we break this rule with the following integral:&lt;/p&gt;
    &lt;p&gt;As with our first rule of thumb it's straightforward to introduce the new parameter as:&lt;/p&gt;
    &lt;p&gt;You are encouraged to proceed forward with the above integral, however computing \(I'(a)\) might not give the most pleasant result. So to overcome this, we'll manipulate the integral a bit before parametersing it.&lt;/p&gt;
    &lt;p&gt;One extremely useful substitution (which deserves its own special chapter) is \(x\to \frac{1-x}{1+x}\). This one is great here especially since it has the following property (among many others):&lt;/p&gt;
    &lt;p&gt;Above, although we could have introduced a simple parameter, for a smoother result we parameterised the integral as:&lt;/p&gt;
    &lt;p&gt;Finally, to find to find \(I\) we can combine \(I(\operatorname{arccosh} 7)\) with \(I(0)\).&lt;/p&gt;
    &lt;p&gt;The result from above follows since includes \(I(0) = \frac{\pi^2}{6}\), which is also the Basel problem in disguise, but you're further encouraged to approach it by employing Feynman's trick.&lt;/p&gt;
    &lt;p&gt;One idea is to rewrite the integral as: \[ I=\int_0^1 \frac{\ln(1+x)}{x}dx\overset{x\to x^3}=3\int_0^1 \frac{\ln(1+x^3)}{x}dx\] \[\Rightarrow \frac13I-I=\int_0^1 \frac{\ln\left(\frac{1+x^3}{1+x}\right)}{x}dx\Rightarrow I = -\frac32 \int_0^1 \frac{\ln(1-x+x^2)}{x}dx \] Now put Feynman's trick to use for: \[I(t)=\int_0^1 \frac{\ln(1-tx+x^2)}{x}dx\] But also take into account that: \[I(0)=\int_0^1 \frac{\ln(1+x^2)}{x}dx\overset{x^2\to x}=\frac12 \int_0^1 \frac{\ln(1+x)}{x}dx=\frac12I\]&lt;/p&gt;
    &lt;head rend="h3"&gt;Switching to rational functions&lt;/head&gt;
    &lt;p&gt;Maybe it's just a personal preference, but for me working with rational functions tends to give a higher visibility on how to parameterise the integrals. We'll see what is meant by this when dealing with the following integral:&lt;/p&gt;
    &lt;p&gt;In this form, two immediate ways to parameterise the integral are as follows:&lt;/p&gt;
    &lt;p&gt;It turns out that both variants work, but we can do better. Another rule of thumb that I follow is to use almost exclusive rational functions as they often provide the most visibility to work with. Let's do that for our integral too.&lt;/p&gt;
    &lt;p&gt;And at this point it should be obvious where to place the parameter so that we can simplify the denominator.&lt;/p&gt;
    &lt;p&gt;Obviously, if you're already more used to trigonometric functions, hyperbolic functions or anything else, then this can be ignored. I however recommend switching to rational functions, a few exceptions being when there's a clear way to parameterise the integral directly or when at least one of the bounds is \(\infty\).&lt;/p&gt;
    &lt;p&gt;For more practice you can also attempt to tackle the following integral:&lt;/p&gt;
    &lt;p&gt;Start by substituting \(\tan x \to x\) then employ Feynman's trick.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cleaning up the functions&lt;/head&gt;
    &lt;p&gt;Before parameterising the integrals it's quite useful to clean the disturbing functions as much as possible before any parameterisation. Let's demonstrate this by considering:&lt;/p&gt;
    &lt;p&gt;As we have trigonometric functions all over the place, we will perform the Weierstrass substitution \(\tan\left(\frac{x}{2}\right)\to x\) in order to obtain only rational functions, as per the previous heuristic.&lt;/p&gt;
    &lt;p&gt;It would be great to parameterise the integral so that we get rid of the arctangent function, but unfortunately here it is a bit overloaded, therefore we'll clean it by splitting it into two. In case it's not obvious how to do that directly, we can differentiate it, perform partial fractions and then integrate back.&lt;/p&gt;
    &lt;p&gt;However since we're integrating over the \((0,1)\) interval we have that \(x \cdot 3 x&amp;gt;1\) for \(x &amp;gt; \frac{1}{\sqrt 3}\), so we need to rewrite the integral as:&lt;/p&gt;
    &lt;p&gt;At this point there's only left to evaluate the first integral, \(\mathcal J\), for which we'll employ Feynman's trick. There isn't any way to place the parameter so that we get rid of anything from the denominator so we'll simply introduce the following parameterisation:&lt;/p&gt;
    &lt;p&gt;In general when we have rational functions it is prefered to integrate over \((0,\infty)\), if possible, as it drastically reduces the result, and when there's the derivative of \(\arcsin x\) in the denominator one way to map \((0,1)\) to \((0,\infty)\) is to directly substitute \(x \to \frac{1}{\sqrt{1+x^2}}\).&lt;/p&gt;
    &lt;p&gt;Now in order to go back to \(\mathcal J(3)\), we'll make use of \(\mathcal J(-1)=0\), so:&lt;/p&gt;
    &lt;p&gt;Finally, to finish this integral we'll subsitute \(t = \frac{1-x}{1+x}\).&lt;/p&gt;
    &lt;p&gt;As such, we can conclude that:&lt;/p&gt;
    &lt;p&gt;Similarly you can try and tackle the following integral:&lt;/p&gt;
    &lt;p&gt;Start by substituting \(\tan\left(\frac{x}{2}\right) \to x\), then split the arctangent function into two parts and use Feynman's trick.&lt;/p&gt;
    &lt;head rend="h3"&gt;Preparing better integral bounds&lt;/head&gt;
    &lt;p&gt;Another useful thing to consider before using Feynman's trick is to manipulate the bounds prior to parameterising the integral so that they get rid of any complicated functions for the differentiated integral, \(I'(t)\). This will almost always give a smoother and easier integral that we have to undo. We will show this in action with the integral from below.&lt;/p&gt;
    &lt;p&gt;With the previous heuristic we've already seen that it is useful to integrate over \((0,\infty)\) when there are some rational functions - therefore we will attempt to do the same with this integral. You are encouraged to try and see what kind of mess it would be produced by parameterising the integral as it is, when the bounds are \((0,1)\). However we will jump straightforward to get our bounds to \((0,\infty)\).&lt;/p&gt;
    &lt;p&gt;In order to obtain that we can notice that the integrand is even, so we can move the bounds to \((-1,1)\) and then substitute \(x\to \frac{1-x}{1+x}\) again, which is another useful way to get our bounds at \((0,\infty)\).&lt;/p&gt;
    &lt;p&gt;Now we can split the logarithm into three parts and use that:&lt;/p&gt;
    &lt;p&gt;Therefore our integral is:&lt;/p&gt;
    &lt;p&gt;To evaluate the emerging integral we will perform Feynman's trick. There's not an obvious way to place the parameter as to simplify the denominator, so we'll parameterise the integral as:&lt;/p&gt;
    &lt;p&gt;The partial fraction was ommited above, as what's really important here is that we're left only with a simple \(\ln a\) as a "disturbing" function. In contrast, if the bounds were \((0,1)\) things would have been way more complicated.&lt;/p&gt;
    &lt;p&gt;Let's finish this integral as we still have to undo the differentiating step.&lt;/p&gt;
    &lt;p&gt;The result from above follows since:&lt;/p&gt;
    &lt;p&gt;Where \(\operatorname{Li}_2(x)\) is the dilogarithm and \(\operatorname{Ti}_2(x)\) is the inverse tangent integral.&lt;/p&gt;
    &lt;p&gt;Finally, collecting all the results yields:&lt;/p&gt;
    &lt;p&gt;With the same idea one can attempt to calculate the following integral:&lt;/p&gt;
    &lt;p&gt;Map the bounds from \((0,1)\) to \((0,\infty)\) by substituting \(x\to\frac{1}{x}\) - here it is necessary to add the resulting integral with the original one - afterwards use Feynman's trick.&lt;/p&gt;
    &lt;head rend="h3"&gt;Multiple parameters&lt;/head&gt;
    &lt;p&gt;We mostly got familiar to apply Feynman's trick by introducing a parameter somewhere, however sometimes even multiple parameters can be used when encountering new integrals. To exemplify such a situation, let's take a look at the following unit square integral arising in geometric probability:&lt;/p&gt;
    &lt;p&gt;In order to generate the \(\ln(xy)\) part it's straightforward to consider the following integral:&lt;/p&gt;
    &lt;p&gt;Differentiating with respect to \(a\), \(m\) times followed by setting \(a=0\) will gives us the desired integral, ignoring the \((-1)^m\) term. However the denominator is still troublesome, and to deal with that we will introduce one more parameter:&lt;/p&gt;
    &lt;p&gt;This is perfect now, as we can recover our original integral by differentiating with respect to \(a\), \(m\) times, and with respect to \(z\), \(m-1\) times, followed by setting \(a=0\) and respectively \(z=1\) - ignoring some coefficients.&lt;/p&gt;
    &lt;p&gt;One way to evaluate \(I(a,z)\) is to expand the denominator into geometric series as:&lt;/p&gt;
    &lt;p&gt;Now we will take \(m\) derivatives with respect to \(a\) and then set it to \(0\).&lt;/p&gt;
    &lt;p&gt;This can be also rewriten in terms of the polylogarithm function as:&lt;/p&gt;
    &lt;p&gt;Finally, we can arrive at our original integral by taking \(m-1\) derivatives with respect to \(z\) and setting it to \(1\).&lt;/p&gt;
    &lt;p&gt;Although the derivation from above was the important part since it shows the main idea on how to differentiate in order to produce the desired integral, by using \(\frac{\partial}{\partial z} \operatorname{Li}_n(z) = \frac{\operatorname{Li}_{n-1}(z)}{z}\) the result can be also written, with the help of OEIS, as:&lt;/p&gt;
    &lt;p&gt;Where \(s(n,m)\) is the Stirling number of the first kind and \(\zeta(z)\) is the Riemann zeta function.&lt;/p&gt;
    &lt;p&gt;A similar idea can be applied for the following integral:&lt;/p&gt;
    &lt;p&gt;Make use of a more general integral that was evaluated in the fourth chapter, namely: \[\int_0^\infty \frac{\cos(tx)}{a^2+x^2}dx = \frac{\pi}{2a}e^{-at}\] Then differentiate \(3\) times w.r.t. \(a\).&lt;/p&gt;
    &lt;head rend="h3"&gt;Cascaded Feynman's trick&lt;/head&gt;
    &lt;p&gt;Sometimes to enable an application of Feynman's trick we needed to actually apply another Feynman's trick. Let's look at the following integral:&lt;/p&gt;
    &lt;p&gt;The first step should be pretty obvious by now, namely to get rid of the trigonometric functions.&lt;/p&gt;
    &lt;p&gt;Now we can notice that we have two logarithms and only one of them contains the \(x\) term, however since the bounds are \((0,1)\) dealing with that integral won't produce much success (as the result would be quite complicated).&lt;/p&gt;
    &lt;p&gt;However we also have the bounds as \((0,\infty)\) for the \(y\) integral, and it would be even better if we could have a single logarithm. To further obtain such a favorable integral form, we can use the following result:&lt;/p&gt;
    &lt;p&gt;In the third chapter we saw how it's useful to have a list of integral results. One more such useful integral that tend to appear quite often is:&lt;/p&gt;
    &lt;p&gt;You can differentiate either \(I(a)\) or \(I(b)\), and even directly employ the accelerated Feynman's trick by writing the logarithm as an integral.&lt;/p&gt;
    &lt;p&gt;Now back to our integral, by using the above result, we can easily finish the integral.&lt;/p&gt;
    &lt;p&gt;Although this marks the conclusion of the essay, this isn't a static website, and I might update it when I encounter new interesting integrals that are worth to be shown. So far, the integrals comes from my posts on Mathematics Stack Exchange, combined with some of the most popular integrals - thus you can also check them directly there.&lt;/p&gt;
    &lt;p&gt;For further exercises, I can recommend you to explore math forums and magazines such as Art of Problem Solving, Mathematics Stack Exchange, The American Mathematical Monthly, Crux Mathematicorum, or the Romanian Mathematical Magazine, where dozens of fascinating integrals are often posted or published. Additionally, delving into other fields like Statistics, Physics, or Quantum Physics will present you with many remarkable integrals — some of which might be computed using Feynman's trick.&lt;/p&gt;
    &lt;p&gt;I would appreciate a notice on my email address (rxzacky@gmail.com) in case you find any mistakes or if something feels unclear in this essay - and even better if you have some further ideas or suggestions.&lt;/p&gt;
    &lt;p&gt; This work is licensed under a Creative Commons Attribution 4.0 International License, and it can be cited as: &lt;lb/&gt; Zaharia Burghelea, "Feynman's Trick," https://zackyzz.github.io/feynman. &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46090269</guid><pubDate>Sat, 29 Nov 2025 19:55:17 +0000</pubDate></item><item><title>The Origins of Scala (2009)</title><link>https://www.artima.com/articles/the-origins-of-scala</link><description>&lt;doc fingerprint="b7c0e83945a38bd0"&gt;
  &lt;main&gt;
    &lt;p&gt;Scala, a general-purpose, object-oriented, functional language for the JVM, is the brainchild of Martin Odersky, a professor at Ecole Polytechnique Fédérale de Lausanne (EPFL). In the first part of a multi-part interview series, Martin Odersky discusses Scala's history and origins with Artima's Bill Venners.&lt;/p&gt;
    &lt;p&gt;Bill Venners: Let's start at the beginning. How did you first become involved with programming languages?&lt;/p&gt;
    &lt;p&gt;Martin Odersky: My favorite subject was always compilers and programming languages. When I first discovered what a compiler was, as an undergrad in 1980, I immediately wanted to build one. The only computer I could remotely afford at the time would have been a Sinclair ZX 80 which had one kilobyte of RAM. I was very close to giving it a try, but, fortunately, soon after got access to a much more powerful machine, an Osborne-1. It was the world's first “portable” (meaning luggable) computer, and it looked remotely like a sewing machine tilted by 90 degrees. It had a five-inch screen which displayed 52 tiny characters per line. But it also had a very impressive 56 usable kilobytes of RAM and two floppy drives of 90K each.&lt;/p&gt;
    &lt;p&gt;In those days, I spent some time with another student in my college named Peter Sollich. We had read about a new language called Modula-2, which we found very elegant and well-engineered. So the plan was born to write a Modula-2 compiler for 8-bit Z80 computers. There was a small problem in that the only language that came with the Osborne was Microsoft Basic, which was utterly unsuitable for what we had in mind, because it did not even support procedures with parameters—all you had was global variables. Other compilers at the time were too expensive for our means. So we decided to apply the classic bootstrapping technique. Peter had written a first compiler for a small subset of Pascal in Z80 assembly language. We then used this compiler to compile a slightly larger language, and so on, during several generations, until we could compile all of Modula-2. It could produce interpreted bytecode as well as Z80 binaries. The bytecode was the most compact of any system at the time, and the binary version was the fastest for 8-bit computers. It was a pretty capable system for its time.&lt;/p&gt;
    &lt;p&gt;Shortly before we finished our compiler, Borland came out with Turbo Pascal, and they were considering going into the Modula-2 market as well. In fact, Borland decided to buy our Modula-2 compiler to be sold under the name of Turbo Modula-2 for CP/M alongside an IBM PC version they wanted to develop. We offered to do the IBM PC version for them, but they told us they had it already covered. Unfortunately that version took them much longer than planned. By the time it came out, three or four years later, their implementor team had split from the company, and it became known as TopSpeed Modula-2. In the absence of an IBM-PC version, Borland never put any marketing muscle behind Turbo-Modula-2, so it remained rather obscure.&lt;/p&gt;
    &lt;p&gt;When we had finished the Modula-2 compiler, Borland offered to hire both Peter and me on the spot. Peter went to join them. I was very close to doing the same, but had the problem that I still had a year of classes and a Masters project ahead of me. I was very tempted at the time to become a college dropout. In the end, I decided to stick it out at university. During my masters project (which was about incremental parsing), I discovered that I liked doing research a lot. So in the end, I gave up on the idea of joining Borland to write compilers, and went on instead to do a Ph.D with Niklaus Wirth, the inventor of Pascal and Modula-2, at ETH Zurich.&lt;/p&gt;
    &lt;p&gt;Bill Venners: How did Scala come about? What is its history?&lt;/p&gt;
    &lt;p&gt;Martin Odersky: Towards the end of my stay in Zurich, around 1988/89, I became very fond of functional programming. So I stayed in research and eventually became a university professor in Karlsruhe, Germany. I initially worked on the more theoretical side of programming, on things like call-by-need lambda calculus. That work was done together with Phil Wadler, who at the time was at the University of Glasgow. One day, Phil told me that a wired-in assistant in his group had heard that there was a new language coming out, still in alpha stage, called Java. This assistant told Phil: "Look at this Java thing. It's portable. It has bytecode. It runs on the web. It has garbage collection. This thing is going to bury you. What are you going to do about it?" Phil said, well, maybe he's got a point there.&lt;/p&gt;
    &lt;p&gt;The answer was that Phil Wadler and I decided take some of the ideas from functional programming and move them into the Java space. That effort became a language called Pizza, which had three features from functional programming: generics, higher-order functions, and pattern matching. Pizza's initial distribution was in 1996, a year after Java came out. It was moderately successful in that it showed that one could implement functional language features on the JVM platform.&lt;/p&gt;
    &lt;p&gt;Then we got in contact with Gilad Bracha and David Stoutamire from the Sun core developer team. They said, "We're really interested in the generics stuff you've been doing; let's do a new project that does just that." And that became GJ (Generic Java). So we developed GJ in 1997/98, and six years later it became the generics in Java 5, with some additions that we didn't do at the time. In particular, the wildcards in Java generics were developed later independently by Gilad Bracha and people at Aarhus university.&lt;/p&gt;
    &lt;p&gt;Although our generics extensions were put on hold for six years, Sun developed a much keener interest in the compiler I had written for GJ. It proved to be more stable and maintainable than their first Java compiler. So they decided to make the GJ compiler the standard javac compiler from their 1.3 release on, which came out in 2000.&lt;/p&gt;
    &lt;p&gt;So I decided that even though I wanted to design a language that was different from Java, it would always connect to the Java infrastructure—to the JVM and its libraries. That was the idea. It was a great opportunity for me that at that time I became a professor at EPFL, which provides an excellent environment for independent research. I could form a small group of researchers that could work without having to chase all the time after external grants.&lt;/p&gt;
    &lt;p&gt;At first we were pretty radical. We wanted to create something that built on a very beautiful model of concurrency called the join calculus. We created an object-oriented version of the join calculus called Functional Nets and a language called Funnel. After a while, however, we found out that Funnel, being a very pure language, wasn't necessarily very practical to use. Funnel was built on a very small core. A lot of things that people usually take for granted (such as classes, or pattern matching) were provided only by encodings into that core. This is a very elegant technique from an academic point of view. But in practice it does not work so well. Beginners found the necessary encodings rather difficult, whereas experts found it boring to have to do them time and time again.&lt;/p&gt;
    &lt;p&gt;As a result, we decided to start over again and do something that was sort of midway between the very pure academic language Funnel, and the very pragmatic but at some points restrictive GJ. We wanted to create something that would be at the same time practical and useful and more advanced than what we could achieve with Java. We started working on this language, which we came to call Scala, in about 2002. The first public release was in 2003. A relatively large redesign happened early 2006. And it's been growing and stabilizing since.&lt;/p&gt;
    &lt;p&gt;Bill Venners: You said you found it frustrating at times to have the constraints of needing to be backwards compatible with Java. Can you give some specific examples of things you couldn't do when you were trying to live within those constraints, which you were then able to do when you changed to doing something that's binary but not source compatible?&lt;/p&gt;
    &lt;p&gt;Martin Odersky: In the generics design, there were a lot of very, very hard constraints. The strongest constraint, the most difficult to cope with, was that it had to be fully backwards compatible with ungenerified Java. The story was the collections library had just shipped with 1.2, and Sun was not prepared to ship a completely new collections library just because generics came about. So instead it had to just work completely transparently.&lt;/p&gt;
    &lt;p&gt;That's why there were a number of fairly ugly things. You always had to have ungenerified types with generified types, the so called raw types. Also you couldn't change what arrays were doing so you had unchecked warnings. Most importantly you couldn't do a lot of the things you wanted to do with arrays, like generate an array with a type parameter T, an array of something where you didn't know the type. You couldn't do that. Later in Scala we actually found out how to do that, but that was possible only because we could drop in Scala the requirement that arrays are covariant.&lt;/p&gt;
    &lt;p&gt;Bill Venners: Can you elaborate on the problem with Java's covariant arrays?&lt;/p&gt;
    &lt;p&gt;Martin Odersky: When Java first shipped, Bill Joy and James Gosling and the other members of the Java team thought that Java should have generics, only they didn't have the time to do a good job designing it in. So because there would be no generics in Java, at least initially, they felt that arrays had to be covariant. That means an array of &lt;code&gt;String&lt;/code&gt; is a subtype of array of &lt;code&gt;Object&lt;/code&gt;, for example. The reason for that was they wanted to be able to write, say, a “generic” sort method that took an array of &lt;code&gt;Object&lt;/code&gt; and a comparator and that would sort this array of &lt;code&gt;Object&lt;/code&gt;. And then let you pass an array of &lt;code&gt;String&lt;/code&gt; to it. It turns out that this thing is type unsound in general. That's why you can get an array store exception in Java. And it actually also turns out that this very same thing blocks a decent implementation of generics for arrays. That's why arrays in Java generics don't work at all. You can't have an array of list of string, it's impossible. You're forced to do the ugly raw type, just an array of list, forever. So it was sort of like an original sin. They did something very quickly and thought it was a quick hack. But it actually ruined every design decision later on. So in order not to fall into the same trap again, we had to break off and say, now we will not be upwards compatible with Java, there are some things we want to do differently.&lt;/p&gt;
    &lt;p&gt;Come back Monday, May 11 for the next installment of this conversation with Martin Odersky. If you'd like to receive a brief weekly email announcing new articles at Artima.com, please subscribe to the Artima Newsletter by clicking its checkbox in your account settings.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt; Martin Odersky is coauthor of Programming in Scala:&lt;p&gt;http://www.artima.com/shop/programming_in_scala&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The Scala programming language website is at:&lt;lb/&gt; http://www.scala-lang.org&lt;/p&gt;
    &lt;p&gt;More information on the Funnel language can be found at:&lt;lb/&gt; http://lamp.epfl.ch/funnel/&lt;/p&gt;
    &lt;p&gt;More information on the Functional Nets can be found at:&lt;lb/&gt; /http://lampwww.epfl.ch/fn/&lt;/p&gt;
    &lt;p&gt;Join calculus is described on wikipedia:&lt;lb/&gt; http://en.wikipedia.org/wiki/Join-calculus&lt;/p&gt;
    &lt;p&gt;Phil Wadler's home page is here:&lt;lb/&gt; http://homepages.inf.ed.ac.uk/wadler/&lt;/p&gt;
    &lt;p&gt;Nicklaus Wirth's wikipedia entry:&lt;lb/&gt; http://en.wikipedia.org/wiki/Niklaus_Wirth&lt;/p&gt;
    &lt;p&gt;The Pizza language on Source Forge. Why not try a slice?:&lt;lb/&gt; http://pizzacompiler.sourceforge.net/&lt;/p&gt;
    &lt;p&gt;The Generic Java Language Extension (GJ):&lt;lb/&gt; http://homepages.inf.ed.ac.uk/wadler/pizza/gj/&lt;/p&gt;
    &lt;p&gt;The Modula-2 home page is here:&lt;lb/&gt; http://www.modula2.org/&lt;/p&gt;
    &lt;p&gt;Have an opinion? Readers have already posted 76 comments about this article. Why not add yours?&lt;/p&gt;
    &lt;p&gt;Bill Venners is president of Artima, Inc., publisher of Artima Developer (www.artima.com). He is author of the book, Inside the Java Virtual Machine, a programmer-oriented survey of the Java platform's architecture and internals. His popular columns in JavaWorld magazine covered Java internals, object-oriented design, and Jini. Active in the Jini Community since its inception, Bill led the Jini Community's ServiceUI project, whose ServiceUI API became the de facto standard way to associate user interfaces to Jini services. Bill is also the lead developer and designer of ScalaTest, an open source testing tool for Scala and Java developers, and coauthor with Martin Odersky and Lex Spoon of the book, Programming in Scala.&lt;/p&gt;
    &lt;p&gt;Frank Sommers is an editor with Artima Developer. He is also founder and president of Autospaces, Inc., a company providing collaboration and workflow tools in the financial services industry.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Artima provides consulting and training services to help you make the most of Scala, reactive and functional programming, enterprise systems, big data, and testing. &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46090294</guid><pubDate>Sat, 29 Nov 2025 19:59:12 +0000</pubDate></item><item><title>All it takes is for one to work out</title><link>https://alearningaday.blog/2025/11/28/all-it-takes-is-for-one-to-work-out-2/</link><description>&lt;doc fingerprint="64659b5db9008759"&gt;
  &lt;main&gt;
    &lt;p&gt;More than a decade ago, when I was applying to graduate school, I went through a period of deep uncertainty. I had tried the previous year and hadn’t gotten in anywhere. I wanted to try again, but I had a lot going against me.&lt;/p&gt;
    &lt;p&gt;I’d spent most of my undergrad building a student job-portal startup and hadn’t balanced it well with academics. My GPA needed explaining. My GMAT score was just okay. I didn’t come from a big-brand employer. And there was no shortage of people with similar or stronger profiles applying to the same schools.&lt;/p&gt;
    &lt;p&gt;Even though I had learned a few things from the first round, the second attempt was still difficult. There were multiple points after I submitted applications where I lost hope.&lt;/p&gt;
    &lt;p&gt;But during that stretch, a friend and colleague kept repeating one line to me:&lt;/p&gt;
    &lt;p&gt;“All it takes is for one to work out.”&lt;/p&gt;
    &lt;p&gt;He’d say it every time I spiraled. And as much as it made me smile, a big part of me didn’t fully believe it. Still, it became a little maxim between us. And eventually, he was right – that one did work out. And it changed my life.&lt;/p&gt;
    &lt;p&gt;I’ve thought about that framing so many times since then.&lt;/p&gt;
    &lt;p&gt;It’s unbelievably powerful in any high-stakes search:&lt;/p&gt;
    &lt;p&gt;You don’t need every job to choose you. You just need the one that’s the right fit.&lt;/p&gt;
    &lt;p&gt;You don’t need every house to accept your offer. You just need the one that feels like home.&lt;/p&gt;
    &lt;p&gt;You don’t need every person to want to build a life with you. You just need the one.&lt;/p&gt;
    &lt;p&gt;You don’t need ten universities to say yes. You just need the one that opens the right door.&lt;/p&gt;
    &lt;p&gt;These processes – college admissions, job searches, home buying, finding a partner – can be emotionally brutal. They can get you down in ways that feel personal. But in those moments, that truth can be grounding.&lt;/p&gt;
    &lt;p&gt;All it takes is for one to work out.&lt;/p&gt;
    &lt;p&gt;And that one is all you need.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46090433</guid><pubDate>Sat, 29 Nov 2025 20:22:44 +0000</pubDate></item><item><title>Show HN: Nano PDF – A CLI Tool to Edit PDFs with Gemini's Nano Banana</title><link>https://github.com/gavrielc/Nano-PDF</link><description>&lt;doc fingerprint="781429fa99a77b84"&gt;
  &lt;main&gt;
    &lt;p&gt;A CLI tool to edit PDF slides using natural language prompts, powered by Google's Gemini 3 Pro Image ("Nano Banana") model.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Natural Language Editing: "Update the graph to include data from 2025", "Change the chart to a bar graph".&lt;/item&gt;
      &lt;item&gt;Add New Slides: Generate entirely new slides that match your deck's visual style.&lt;/item&gt;
      &lt;item&gt;Non-Destructive: Preserves the searchable text layer of your PDF using OCR re-hydration.&lt;/item&gt;
      &lt;item&gt;Multi-page &amp;amp; Parallel: Edit multiple pages in a single command with concurrent processing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Nano PDF uses Gemini 3 Pro Image (aka Nano Banana) and PDF manipulation to enable quick edits of PDFs with natural language editing:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Page Rendering: Converts target PDF pages to images using Poppler&lt;/item&gt;
      &lt;item&gt;Style References: Optionally includes style reference pages with generation request to understand visual style (fonts, colors, layout)&lt;/item&gt;
      &lt;item&gt;AI Generation: Sends images + prompts to Gemini 3 Pro Image, which generates edited versions&lt;/item&gt;
      &lt;item&gt;OCR Re-hydration: Uses Tesseract to restore searchable text layer to generated images&lt;/item&gt;
      &lt;item&gt;PDF Stitching: Replaces original pages with AI-edited versions while preserving document structure&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The tool processes multiple pages in parallel for speed, with configurable resolution (4K/2K/1K) to balance quality vs. cost.&lt;/p&gt;
    &lt;code&gt;pip install nano-pdf&lt;/code&gt;
    &lt;p&gt;You need a paid Google Gemini API key with billing enabled. Free tier keys do not support image generation.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Get an API key from Google AI Studio&lt;/item&gt;
      &lt;item&gt;Enable billing on your Google Cloud project&lt;/item&gt;
      &lt;item&gt;Set it as an environment variable:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;export GEMINI_API_KEY="your_api_key_here"&lt;/code&gt;
    &lt;p&gt;Note: This tool uses Gemini 3 Pro Image which requires a paid API tier. See pricing for details.&lt;/p&gt;
    &lt;p&gt;Edit a single page (e.g., Page 2):&lt;/p&gt;
    &lt;code&gt;nano-pdf edit my_deck.pdf 2 "Change the title to 'Q3 Results'"&lt;/code&gt;
    &lt;p&gt;Edit multiple pages in one go:&lt;/p&gt;
    &lt;code&gt;nano-pdf edit my_deck.pdf \
  1 "Update date to Oct 2025" \
  5 "Add company logo" \
  10 "Fix typo in footer"&lt;/code&gt;
    &lt;p&gt;Insert a new AI-generated slide into your deck:&lt;/p&gt;
    &lt;code&gt;# Add a title slide at the beginning
nano-pdf add my_deck.pdf 0 "Title slide with 'Q3 2025 Review'"

# Add a slide after page 5
nano-pdf add my_deck.pdf 5 "Summary slide with key takeaways as bullet points"&lt;/code&gt;
    &lt;p&gt;The new slide will automatically match the visual style of your existing slides and uses document context by default for better relevance.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--use-context&lt;/code&gt;/&lt;code&gt;--no-use-context&lt;/code&gt;: Include the full text of the PDF as context for the model. Disabled by default for&lt;code&gt;edit&lt;/code&gt;, enabled by default for&lt;code&gt;add&lt;/code&gt;. Use&lt;code&gt;--no-use-context&lt;/code&gt;to disable.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--style-refs "1,5"&lt;/code&gt;: Manually specify which pages to use as style references.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--output "new.pdf"&lt;/code&gt;: Specify the output filename.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--resolution "4K"&lt;/code&gt;: Image resolution - "4K" (default), "2K", or "1K". Higher quality = slower processing.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--disable-google-search&lt;/code&gt;: Prevents the model from using Google Search to find information before generating (enabled by default).&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Fix typos across multiple slides
nano-pdf edit pitch_deck.pdf \
  3 "Fix the typo 'recieve' to 'receive'" \
  7 "Change 'Q4 2024' to 'Q1 2025'"&lt;/code&gt;
    &lt;code&gt;# Update branding and colors
nano-pdf edit slides.pdf 1 "Make the header background blue and text white" \
  --style-refs "2,3" --output branded_slides.pdf&lt;/code&gt;
    &lt;code&gt;# Update financial data
nano-pdf edit report.pdf 12 "Update the revenue chart to show Q3 at $2.5M instead of $2.1M"&lt;/code&gt;
    &lt;code&gt;# Use full document context for consistency
nano-pdf edit presentation.pdf \
  5 "Update the chart colors to match the theme" \
  8 "Add the company logo in the bottom right" \
  --use-context&lt;/code&gt;
    &lt;code&gt;# Add a new agenda slide at the beginning
nano-pdf add quarterly_report.pdf 0 "Agenda slide with: Overview, Financial Results, Q4 Outlook"&lt;/code&gt;
    &lt;code&gt;# Google Search is enabled by default - the model can look up current information
nano-pdf edit deck.pdf 5 "Update the market share data to latest figures"

# Disable Google Search if you want the model to only use provided context
nano-pdf add deck.pdf 3 "Add a summary slide" --disable-google-search&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Python 3.10+&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;poppler&lt;/code&gt;(for PDF rendering)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;tesseract&lt;/code&gt;(for OCR)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;brew install poppler tesseract&lt;/code&gt;
    &lt;code&gt;choco install poppler tesseract&lt;/code&gt;
    &lt;p&gt;Note: After installation, you may need to restart your terminal or add the installation directory to your PATH.&lt;/p&gt;
    &lt;code&gt;sudo apt-get install poppler-utils tesseract-ocr&lt;/code&gt;
    &lt;p&gt;Make sure you've installed poppler and tesseract for your platform. After installation, restart your terminal to refresh PATH. Run &lt;code&gt;which pdftotext&lt;/code&gt; and &lt;code&gt;which tesseract&lt;/code&gt; to verify they're accessible.&lt;/p&gt;
    &lt;p&gt;Set your API key as an environment variable:&lt;/p&gt;
    &lt;code&gt;export GEMINI_API_KEY="your_key_here"&lt;/code&gt;
    &lt;p&gt;Gemini 3 Pro Image requires a paid API tier. Visit Google AI Studio to enable billing on your project.&lt;/p&gt;
    &lt;p&gt;Try using &lt;code&gt;--style-refs&lt;/code&gt; to specify reference pages that have the desired visual style. The model will analyze these pages to better match fonts, colors, and layout.&lt;/p&gt;
    &lt;p&gt;The tool uses Tesseract OCR to restore searchable text. For best results, ensure your generated images are high resolution (&lt;code&gt;--resolution "4K"&lt;/code&gt;). Note that OCR may not be perfect for stylized fonts or small text.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use &lt;code&gt;--resolution "2K"&lt;/code&gt;or&lt;code&gt;--resolution "1K"&lt;/code&gt;for faster processing&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you want to run the latest development version:&lt;/p&gt;
    &lt;code&gt;# Clone the repository
git clone https://github.com/gavrielc/Nano-PDF.git
cd Nano-PDF

# Install dependencies
pip install -e .

# Run the tool
nano-pdf edit my_deck.pdf 2 "Your edit here"&lt;/code&gt;
    &lt;p&gt;MIT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46090619</guid><pubDate>Sat, 29 Nov 2025 20:44:24 +0000</pubDate></item><item><title>Landlock-Ing Linux</title><link>https://blog.prizrak.me/post/landlock/</link><description>&lt;doc fingerprint="7d7b98108eb566c1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Landlock-ing Linux&lt;/head&gt;
    &lt;p&gt;Nov 29, 2025&lt;/p&gt;
    &lt;head rend="h3"&gt;Landlock: What Is It?&lt;/head&gt;
    &lt;p&gt;Landlock is a Linux API that lets applications explicitly declare which resources they are allowed to access. Its philosophy is similar to OpenBSDâs &lt;code&gt;unveil()&lt;/code&gt; and (less so) &lt;code&gt;pledge()&lt;/code&gt;: programs can make a contract with the kernel stating, âI only need these files or resources â deny me everything else if Iâm compromised.â&lt;/p&gt;
    &lt;p&gt;It provides a simple, developer-friendly way to add defense-in-depth to applications. Compared to traditional Linux security mechanisms, Landlock is vastly easier to understand and integrate.&lt;/p&gt;
    &lt;p&gt;This post is meant to be an accessible introduction, and hopefully persuade you to give Landlock a try.&lt;/p&gt;
    &lt;head rend="h3"&gt;How Does It Work?&lt;/head&gt;
    &lt;p&gt;Landlock is a Linux Security Module (LSM) available since Linux 5.13. Unlike MAC frameworks such as SELinux or AppArmor, Landlock applies transient restrictions: policies are created at runtime, enforced on the current thread and its future descendants, and disappear when the process exits.&lt;/p&gt;
    &lt;p&gt;You donât tag files with labels or extended attributes. Instead, applications create policies dynamically.&lt;/p&gt;
    &lt;p&gt;A Landlock policy consists of two pieces:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Handled accesses â the categories of operations you want to restrict (e.g., filesystem read/write).&lt;/item&gt;
      &lt;item&gt;Access grants â an explicit allowlist of which objects are permitted for those operations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example, you could create a policy that handles all filesystem reads/writes and network binds, and grants:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;read-only access to &lt;code&gt;/home/user&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;read/write access to &lt;code&gt;/tmp&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;permission to bind to port &lt;code&gt;2222&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The application then calls &lt;code&gt;landlock_restrict_self()&lt;/code&gt; to enter the restricted domain. From that point on, that thread’s child threads and child processes are permanently constrained. Restrictions cannot be revoked.&lt;/p&gt;
    &lt;p&gt;Policies can be layered (up to 16 layers). A child layer may further reduce access, but cannot reintroduce permissions the parent layer removed. For example, a child thread may add a layer to this policy to restrict itself to only reading &lt;code&gt;/home/user&lt;/code&gt;, but it cannot regain permission to bind to port &lt;code&gt;2222&lt;/code&gt; once a layer omits this grant.&lt;/p&gt;
    &lt;p&gt;Landlock is unprivileged â any application can sandbox itself. It also uses ABI versioning, allowing programs to apply best-effort sandboxing even on older kernels lacking newer features.&lt;/p&gt;
    &lt;p&gt;It’s also a stackable LSM, meaning you can combine it with selinux or apparmor in a supplemental layer.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why Should You Use It?&lt;/head&gt;
    &lt;p&gt;Landlock shines when an application has a predictable set of files or directories it needs. For example, a web server could restrict itself to accessing only &lt;code&gt;/var/www/html&lt;/code&gt; and &lt;code&gt;/tmp&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Unlike SELinux or AppArmor, Landlock policies donât require administrator involvement or system-wide configuration. Developers can embed policies directly in application code, making sandboxing a natural part of the development process.&lt;/p&gt;
    &lt;p&gt;Because Landlock requires no privileges to use, adding it to most programs is straightforward.&lt;/p&gt;
    &lt;p&gt;Bindings exist for languages such as Rust, Go, and Haskell, and several projects provide user-friendly &lt;code&gt;unveil&lt;/code&gt;-style wrappers.&lt;/p&gt;
    &lt;p&gt;A official c library doesn’t exist yet unfortunately, but there’s several out there you can try.&lt;/p&gt;
    &lt;p&gt;Here’s a quick rust example:&lt;/p&gt;
    &lt;code&gt;use landlock::{
    ABI, Access, AccessFs, Ruleset, RulesetAttr, RulesetCreatedAttr, RulesetStatus, RulesetError,
    path_beneath_rules,
};

fn restrict_thread() -&amp;gt; Result&amp;lt;(), RulesetError&amp;gt; {
    let abi = ABI::V1;
    let status = Ruleset::default()
        .handle_access(AccessFs::from_all(abi))?
        .create()?
        // Read-only access to /usr, /etc and /dev.
        .add_rules(path_beneath_rules(&amp;amp;["/usr", "/etc", "/dev"], AccessFs::from_read(abi)))?
        // Read-write access to /home and /tmp.
        .add_rules(path_beneath_rules(&amp;amp;["/home", "/tmp"], AccessFs::from_all(abi)))?
        .restrict_self()?;

    match status.ruleset {
        RulesetStatus::FullyEnforced =&amp;gt; println!("Fully sandboxed."),
        RulesetStatus::PartiallyEnforced =&amp;gt; println!("Partially sandboxed."),
        RulesetStatus::NotEnforced =&amp;gt; println!("Not sandboxed! Please update your kernel."),
    }
    Ok(())
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;The State of Linux Sandboxing: Why This Matters&lt;/head&gt;
    &lt;p&gt;As Linux adoption grows, so does the amount of malware targeting desktop users. While Linux has historically enjoyed relative safety, this is largely due to smaller market share and higher technical barriers compared to Windows â not because Linux is inherently safer.&lt;/p&gt;
    &lt;p&gt;Linux is not a security panacea. For example, on most major distributions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Users can download and execute untrusted binaries with no warnings.&lt;/item&gt;
      &lt;item&gt;Shell scripts can be piped from the internet and executed blindly.&lt;/item&gt;
      &lt;item&gt;Many users run passwordless sudo, giving them root access on demand.&lt;/item&gt;
      &lt;item&gt;Unprivileged applications can typically: &lt;list rend="ul"&gt;&lt;item&gt;Read &lt;code&gt;~/.ssh&lt;/code&gt;,&lt;code&gt;~/.bashrc&lt;/code&gt;, browser cookies, and anything else in&lt;code&gt;$HOME&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Modify environment variables and &lt;code&gt;$PATH&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Create systemd user services&lt;/item&gt;&lt;item&gt;(on X11) log keystrokes and read input devices&lt;/item&gt;&lt;item&gt;Bind to arbitrary network ports&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Read &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Several tools try to improve the state of security on linux, but each has significant drawbacks:&lt;/p&gt;
    &lt;head rend="h4"&gt;Containerization (docker, podman)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Designed for service isolation, not desktop apps.&lt;/item&gt;
      &lt;item&gt;Managing home directory access is clunky.&lt;/item&gt;
      &lt;item&gt;Many users break isolation by using &lt;code&gt;--privileged&lt;/code&gt;or&lt;code&gt;--network host&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Flatpak / Snap&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Great for graphical applications (Flatpak especially).&lt;/item&gt;
      &lt;item&gt;Often require overly broad permissions.&lt;/item&gt;
      &lt;item&gt;Less suitable for CLI tools.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Firejail&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Requires per-application profiles.&lt;/item&gt;
      &lt;item&gt;Must be explicitly invoked each time, or you need a wrapper script.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;From the developer side:&lt;/p&gt;
    &lt;head rend="h4"&gt;seccomp&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Powerful syscall filtering.&lt;/item&gt;
      &lt;item&gt;Tedious and error-prone to configure.&lt;/item&gt;
      &lt;item&gt;Blacklists are fragile; new syscalls can break things.&lt;/item&gt;
      &lt;item&gt;Argument filtering is difficult and full of TOCTOU hazards.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;SELinux&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Extremely powerful, but difficult to understand.&lt;/item&gt;
      &lt;item&gt;Requires system-wide policies and admin involvement.&lt;/item&gt;
      &lt;item&gt;Many users disable it due to complexity.&lt;/item&gt;
      &lt;item&gt;Not enabled on most distributions by default. (used a lot in android)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;AppArmor&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Easier than SELinux, but still requires admin-defined profiles.&lt;/item&gt;
      &lt;item&gt;Applies system-wide and lacks per-process namespacing.&lt;/item&gt;
      &lt;item&gt;Gets disabled by many distributions, but is more commonly used in the desktop.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Landlock&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unprivileged&lt;/item&gt;
      &lt;item&gt;Application-centric&lt;/item&gt;
      &lt;item&gt;Easy to integrate&lt;/item&gt;
      &lt;item&gt;Deny-by-default&lt;/item&gt;
      &lt;item&gt;Widely supported since 5.13&lt;/item&gt;
      &lt;item&gt;Backward and forward compatibility mechanisms.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Landlock isnât perfect, but it fills a major gap: a simple, self-contained unprivileged sandboxing tool.&lt;/p&gt;
    &lt;head rend="h3"&gt;What landlock could bring to the table:&lt;/head&gt;
    &lt;p&gt;Long-running system daemons that run with elevated privileges could benefit from landlock restrictions.&lt;/p&gt;
    &lt;p&gt;Desktop applications dealing with binary formats, like pdf readers, image viewers web browsers, and word processors can be restricted to accessing the files they originally opened.&lt;/p&gt;
    &lt;p&gt;FTP and HTTP servers can be bound to the files they need. Even if nginx is running as root, if an attacker gets a full reverse shell, they won’t be able to see access files outside the policy.&lt;/p&gt;
    &lt;p&gt;If the supervisor proposal gets added, we could bring an android-like permissions system to the linux desktop. Flatpak does a decent job at this, but imagine if every process in your desktop would need to explicitly ask (at least once) before accessing sensitive files or resources.&lt;/p&gt;
    &lt;p&gt;Pair that with an accessible GUI and a system for handling updates and saving permission grants, and we have potential for a safer, more secure linux user experience on the desktop.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ongoing Work in Landlock&lt;/head&gt;
    &lt;p&gt;Several promising features are under active development:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Supervise Mode&lt;/p&gt;&lt;lb/&gt;Lets a userspace âsupervisorâ interactively allow or deny access â similar to Android-style permission prompts.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Socket Restrictions Fine-grained control over which types of sockets or ports processes may use.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;LANDLOCK_RESTRICT_SELF_TSYNC Ensures restrictions propagate to all threads in a process.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;LANDLOCK_ADD_RULE_QUIET Allows suppressing audit messages for certain objects.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;LANDLOCK_ADD_RULE_NO_INHERIT (disclosure: this is my patch series) Prevents rules from unintentionally inheriting permissions from parent directories, giving finer-grained filesystem control.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;TL;DR&lt;/head&gt;
    &lt;p&gt;Landlock is a simple, unprivileged, deny-by-default sandboxing mechanism for Linux.&lt;lb/&gt; Itâs easy to understand, easy to integrate, and has tremendous potential for improving desktop and application security.&lt;/p&gt;
    &lt;p&gt;Give it a try in your application.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46090969</guid><pubDate>Sat, 29 Nov 2025 21:30:53 +0000</pubDate></item><item><title>Bazzite: The next generation of Linux gaming</title><link>https://bazzite.gg/</link><description>&lt;doc fingerprint="3d5f1d310515a9e5"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;Play your favorite games&lt;/head&gt;&lt;p&gt;Own games outside of Steam? Lutris (Pre-installed) and other launchers can not only run games from most game stores, but cleanly integrate them into Steam Gaming Mode, including: Xbox Game Pass (via battle.net), EA App, Epic Games Store, GOG.com, itch.io, Rockstar Games Launcher, Ubisoft Connect, your dad's old CDs, and more!&lt;/p&gt;&lt;head rend="h2"&gt;On all your favorite devices&lt;/head&gt;&lt;head rend="h3"&gt;Take your game library anywhere&lt;/head&gt;&lt;p&gt;Your MicroSD card game library can be shared between multiple Bazzite installs no matter which devices you're using.&lt;/p&gt;&lt;head rend="h2"&gt;UpgradeRollback fearlessly&lt;/head&gt;&lt;p&gt;Bazzite is image based meaning that after every update the previous version of the operating system is retained on your machine. Should an update cause any issues, you can select the previous image at boot time.&lt;lb/&gt;Images of the operating system are retained in our repositories for ninety days and can be switched to via the terminal. Nvidia driver update broke something you needed? No worries, rebase to the last known good release and pin it so that it's retained as long as needed.&lt;lb/&gt;&lt;/p&gt;&lt;head rend="h3"&gt;Secure by default&lt;/head&gt;Experience enterprise class security with out-of-the-box SELinux, Secure Boot support, signed container images, and LUKS disk encryption with optional automatic TPM unlocking.&lt;p&gt;Modern app stores provide attestation, sandboxing, and the most officially verified applications of any Linux application repository.&lt;/p&gt;&lt;head rend="h2"&gt;Work with your hardware, not for it&lt;/head&gt;&lt;p&gt;Bazzite focuses on hardware compatibility out of the box, with full support for accelerated video encoding and decoding, built in Nvidia drivers, additional HID drivers, and just about every udev rule you could need.&lt;lb/&gt;Let your operating system work with your hardware so you don't have to.&lt;/p&gt;&lt;head rend="h3"&gt;Complete Handheld PC Support&lt;/head&gt;&lt;p&gt;Bazzite features Handheld Daemon, offering enhanced functionality and support for handhelds from manufacturers such as ASUS, Ayn, GPD, and Lenovo - all accessible by double tapping the quick access menu button.&lt;/p&gt;&lt;p&gt;Customize your handheld experience with in-depth controller emulation, including paddles, touchpad, rumble, face buttons, RGB lighting, and more.&lt;/p&gt;&lt;p&gt;Control all aspects of your hardware from a clean and intuitive UI, including GPU frequency, TDP controls, and CPU scheduler.&lt;/p&gt;&lt;head rend="h2"&gt;Use your favorite desktop environmenthandheld experiencecouch gaming setup&lt;/head&gt;&lt;p&gt;The latest and greatest by the KDE community built from Fedora Kinoite. KDE offers a highly customizable and modern UI that Windows users would find right at home, with a bottom taskbar, start menu, and widgets. Valve's themes and customizations present in SteamOS come pre-installed.&lt;/p&gt;&lt;p&gt;A modern and beautiful desktop by the GNOME Foundation built from Fedora Silverblue. Optimized for touch input, this desktop environment feels right at home on handhelds and tablets, with rounded corners and thoughtful design choices that would make even a die-hard Apple user blush. We provide a lightly customized GNOME experience with tweaks that can easily be undone if desired.&lt;/p&gt;&lt;p&gt;From your handheld to your home theater PC, Steam Gaming Mode offers the premier console-like experience, and can be extended with community-developed plugins and themes thanks to Decky Loader.&lt;/p&gt;&lt;p&gt;Waydroid brings the Android apps and games you love to Bazzite, working side by side with your other Linux applications. Visit our Waydroid setup guide for more information.&lt;/p&gt;&lt;head rend="h2"&gt;Run your favorite applicationscontainerseverything&lt;/head&gt;&lt;p&gt;Bazzite utilizes the Bazaar app store, allowing for easy installation and management of all your favorite applications from Flathub and a curated list of apps we think you'll love.&lt;/p&gt;&lt;p&gt;Additionally, Bazzite brings Brew and the tools and techniques created by the cloud native community to your desktop. Built in container support means packages for any Linux distribution can be installed and used as if they were native applications. The included Ptyxis terminal and DistroShelf provide first-class access to your Distrobox containers, letting you focus on what matters - your software.&lt;lb/&gt;Running a game, a development environment, a container for your Jellyfin server, or a utility only available on the Arch User Repository? You can rest assured it works here. Bazzite is developed on Bazzite. &lt;/p&gt;&lt;head rend="h3"&gt;Join the community&lt;/head&gt;&lt;head rend="h5"&gt;Discourse&lt;/head&gt;&lt;p&gt;Join our Discourse for asynchronous discussion of Bazzite, and newsletters covering newly added features.&lt;/p&gt;&lt;p&gt;Join our subreddit for community discussions, news updates, announcements, showcases, and more.&lt;/p&gt;&lt;head rend="h5"&gt;Become a Supporter&lt;/head&gt;&lt;p&gt;Support the continued development of Bazzite by sponsoring us on Open Collective! Your contributions help cover hosting and hardware expenses, as well as travel for our team to present, collaborate, and engage with the broader Linux community.&lt;/p&gt;&lt;head rend="h3"&gt;Press &amp;amp; Testimonials&lt;/head&gt;&lt;head rend="h3"&gt;Project News&lt;/head&gt;&lt;head rend="h2"&gt;Our team&lt;/head&gt;&lt;p&gt;Bazzite is built with Universal Blue on GitHub by a dedicated group of maintainers, and contributors like you.&lt;/p&gt;&lt;head rend="h4"&gt;Featuring alumni from companies like:&lt;/head&gt;&lt;head rend="h4"&gt;Our sponsors:&lt;/head&gt;&lt;p&gt;Additionally, Bazzite contributes to and includes work from our friends: Sentry's kernel-fsync, The Fedora Project, Podman, The Nobara Project, ChimeraOS, Jovian-NixOS, and evlaV's GitLab.&lt;lb/&gt;We are immensely thankful for their support and collaboration.&lt;/p&gt;&lt;head rend="h2"&gt;Download Bazzite&lt;/head&gt;&lt;p&gt;Use the form below to get the correct ISO download link for your hardware and use case.&lt;lb/&gt;If you're already using a Fedora Atomic Desktop, you can rebase to Bazzite without reinstalling.&lt;lb/&gt;Are you a developer? Check out Bazzite Developer eXperience for images focused on your use case!&lt;/p&gt;&lt;p&gt;Use this form to get the correct ISO download link for your hardware and use case. &lt;lb/&gt;If you're already using a Fedora Atomic Desktop, you can rebase to Bazzite without reinstalling.&lt;lb/&gt;Are you a developer? Check out Bazzite Developer eXperience for images focused on your use case!&lt;/p&gt;&lt;head rend="h2"&gt;Build your own&lt;/head&gt;&lt;p&gt;Bazzite, its RPM packages, and downstream images are updated autonomously through GitHub Actions and Fedora Copr webhooks.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46091362</guid><pubDate>Sat, 29 Nov 2025 22:22:24 +0000</pubDate></item><item><title>Americans no longer see four-year college degrees as worth the cost</title><link>https://www.nbcnews.com/politics/politics-news/poll-dramatic-shift-americans-no-longer-see-four-year-college-degrees-rcna243672</link><description>&lt;doc fingerprint="3cc021459d8f0fe8"&gt;
  &lt;main&gt;
    &lt;p&gt;Americans have grown sour on one of the longtime key ingredients of the American dream.&lt;/p&gt;
    &lt;p&gt;Almost two-thirds of registered voters say that a four-year college degree isn’t worth the cost, according to a new NBC News poll, a dramatic decline over the last decade.&lt;/p&gt;
    &lt;p&gt;Just 33% agree a four-year college degree is “worth the cost because people have a better chance to get a good job and earn more money over their lifetime,” while 63% agree more with the concept that it’s “not worth the cost because people often graduate without specific job skills and with a large amount of debt to pay off.”&lt;/p&gt;
    &lt;p&gt;In 2017, U.S. adults surveyed were virtually split on the question — 49% said a degree was worth the cost and 47% said it wasn’t. When CNBC asked the same question in 2013 as part of its All American Economic Survey, 53% said a degree was worth it and 40% said it was not.&lt;/p&gt;
    &lt;p&gt;The eye-popping shift over the last 12 years comes against the backdrop of several major trends shaping the job market and the education world, from exploding college tuition prices to rapid changes in the modern economy — which seems once again poised for radical transformation alongside advances in AI.&lt;/p&gt;
    &lt;p&gt;“It’s just remarkable to see attitudes on any issue shift this dramatically, and particularly on a central tenet of the American dream, which is a college degree. Americans used to view a college degree as aspirational — it provided an opportunity for a better life. And now that promise is really in doubt,” said Democratic pollster Jeff Horwitt of Hart Research Associates, who conducted the poll along with the Republican pollster Bill McInturff of Public Opinion Strategies.&lt;/p&gt;
    &lt;p&gt;“What is really surprising about it is that everybody has moved. It’s not just people who don’t have a college degree,” Horwitt added.&lt;/p&gt;
    &lt;p&gt;National data from the Bureau of Labor Statistics shows that those with advanced degrees earn more and have lower unemployment rates than those with lower levels of education. That’s been true for years.&lt;/p&gt;
    &lt;p&gt;But what has shifted is the price of college. While there have been some small declines in tuition prices over the last decade, when adjusted for inflation, College Board data shows that the average, inflation-adjusted cost of public four-year college tuition for in-state students has doubled since 1995. Tuition at private, four-year colleges is up 75% over the same period.&lt;/p&gt;
    &lt;p&gt;Poll respondents who spoke with NBC News all emphasized those rising costs as a major reason why the value of a four-year degree has been undercut.&lt;/p&gt;
    &lt;p&gt;Jacob Kennedy, a 28-year-old server and bartender living in Detroit, told NBC News that while he believes “an educated populace is the most important thing for a country to have,” if people can’t use those degrees because of the debt they’re carrying, it undercuts the value.&lt;/p&gt;
    &lt;p&gt;Kennedy, who has a two-year degree, reflected on “the number of people who I’ve met working in the service industry who have four-year degrees and then within a year of graduating immediately quit their ‘grown-up jobs’ to go back to the jobs they had.”&lt;/p&gt;
    &lt;p&gt;“The cost overwhelms the value,” he continued. “You go to school with all that student debt — the jobs you get out of college don’t pay that debt, so you have to go find something else that can pay that debt.”&lt;/p&gt;
    &lt;p&gt;The 20-point decline over the last 12 years among those who say a degree is worth it — from 53% in 2013 to 33% now — is reflected across virtually every demographic group. But the shift in sentiment is especially striking among Republicans.&lt;/p&gt;
    &lt;p&gt;In 2013, 55% of Republicans called a college degree worth it, while 38% said it wasn’t worth it. In the new poll, just 22% of Republicans say the four-year degree is worth it, while 74% say it’s not.&lt;/p&gt;
    &lt;p&gt;Democrats have seen a significant shift too, but not to the same extent — a decline from 61% who said a degree was worth it in 2013 to 47% this year.&lt;/p&gt;
    &lt;p&gt;Over the same period, the composition of both parties has changed, with the Republican Party garnering new and deeper support from voters without college degrees, while the Democratic Party drew in more degree-holders.&lt;/p&gt;
    &lt;p&gt;Remarkably, less than half of voters with college degrees see those degrees as worth the cost: 46% now, down from 63% in 2013.&lt;/p&gt;
    &lt;p&gt;Those without a college degree were about split on the question in 2013. Now, 71% say a four-year degree is not worth the cost, while 26% say it is.&lt;/p&gt;
    &lt;p&gt;Preston Cooper, a senior fellow at the right-leaning American Enterprise Institute, said enough cracks have proliferated under the long-standing narrative that a college degree always pays off to create a serious rupture.&lt;/p&gt;
    &lt;p&gt;“Some people drop out, or sometimes people end up with a degree that is not worth a whole lot in the labor market, and sometimes people pay way too much for a degree relative to the value of what that credential is,” he said. “These cases have created enough exceptions to the rule that a bachelor’s degree always pays off, so that people are now more skeptical.”&lt;/p&gt;
    &lt;p&gt;The upshot is that interest in technical, vocational and two-year degree programs has soared.&lt;/p&gt;
    &lt;p&gt;“I think students are more wary about taking on the risk of a four-year or even a two-year degree,” he said. “They’re now more interested in any pathway that can get them into the labor force more quickly.”&lt;/p&gt;
    &lt;p&gt;Josiah Garcia, a 24-year-old in Virginia, said he recently enrolled in a program to receive a four-year engineering degree after working as an electrician’s apprentice. He said he was motivated to go back to school because he saw the degree as having a direct effect on his future earning potential.&lt;/p&gt;
    &lt;p&gt;But he added that he didn’t feel that those who sought other degrees in areas like art or theater could say the same.&lt;/p&gt;
    &lt;p&gt;“A lot of my friends who went to school for art or dance didn’t get the job they thought they could get after graduating,” he said, arguing that degrees for “softer skills” should be cheaper than those in STEM fields.&lt;/p&gt;
    &lt;p&gt;Jessica Burns, a 38-year-old Iowa resident and bachelor’s degree-holder who works for an insurance company, told NBC News that for her, the worth of a four-year-degree largely depends on the cost.&lt;/p&gt;
    &lt;p&gt;She went to a community college and then a state school to earn her degree, so she said she graduated without having to spend an “insane” amount of money.&lt;/p&gt;
    &lt;p&gt;But her husband went to a private college for his degree, and she quipped: “We are going to have student loan debt for him forever.”&lt;/p&gt;
    &lt;p&gt;Burns said she believes a college degree is “essential for a lot of jobs. You’re not going to get an interview if you don’t have a four-year degree for a lot of jobs in my field.”&lt;/p&gt;
    &lt;p&gt;But she framed the value of degrees more in terms of how society views them instead of intrinsic value.&lt;/p&gt;
    &lt;p&gt;“It’s not valuable because it’s brought a bunch of value added, it’s valuable because it’s the key to even getting in the door,” she said. “Our society needs to figure out that if we value it, we need to make it affordable.”&lt;/p&gt;
    &lt;p&gt;Burns said she believes that a lot more people in her millennial generation are “now saddled with a huge amount of debt, even as successful business professionals,” which will influence how her peers approach paying for college for their children.&lt;/p&gt;
    &lt;p&gt;There hasn’t just been a decline in the cost-benefit analysis of a degree. Gallup polling also shows a marked decline in public confidence in higher education over the last decade, albeit with a slight increase over the last year.&lt;/p&gt;
    &lt;p&gt;“This is a political problem. It’s also a real problem for higher education. Colleges and universities have lost that connection they’ve had with a large swath of the American people based on affordability,” Horwitt said. “They’re now seen as out of touch and not accessible to many Americans.”&lt;/p&gt;
    &lt;p&gt;The NBC News poll surveyed 1,000 registered voters Oct. 24-28 via a mix of telephone interviews and an online survey sent via text message. The margin of error is plus or minus 3.1 percentage points.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46091591</guid><pubDate>Sat, 29 Nov 2025 22:56:14 +0000</pubDate></item><item><title>Scala</title><link>https://www.huygens-fokker.org/scala/</link><description>&lt;doc fingerprint="b14fd47b6463b091"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Scala is a powerful software tool for experimentation with musical tunings, such as just intonation scales, equal and historical temperaments, microtonal and macrotonal scales, and non-Western scales. It supports scale creation, editing, comparison, analysis, storage, tuning of electronic instruments, and MIDI file generation and tuning conversion. All this is integrated into a single application with a wide variety of mathematical routines and scale creation methods. Scala is ideal for the exploration of tunings and becoming familiar with the concepts involved. In addition, a very large library of scales is freely available for Scala and can be used for analysis or music creation.&lt;/p&gt;
          &lt;p&gt;Great care has been taken to make Scala's functions and operations very general. The range of parameter values that commands accept is made as general as possible. Often various forms of input are allowed. No arbitrary restrictions are made. Scales are stored in a flexible format. Intervals can be entered and saved as either ratios or cents values and be intermixed within a scale.&lt;/p&gt;
          &lt;p&gt;Constructing scales from scratch is one of Scala's strengths. Kinds of scales that can be made with Scala include: equal temperaments, well-temperaments, Pythagorean (meantone) scales, Euler-Fokker genera, Fokker periodicity blocks, harmonic scales, Partch diamonds, Polychordal scales, Dwarf scales and Wilson Combination Product Sets. In addition, a set of command files is included to build other kinds of scales such as triadic scales, circular mirrorings, circulating temperaments, etc., and to serve as examples.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt; Features&lt;p&gt;Download&lt;/p&gt;&lt;p&gt;Distribution&lt;/p&gt;&lt;p&gt;References&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; Examples&lt;p&gt;Help Index&lt;/p&gt;&lt;p&gt;Related Links&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt; Nowadays many software synthesizers like AlsaModularSynth, Tobybear Helios, MAZ Sound VSampler, Orion Pro, VirSyn Cube, Cantor, TERA 2, rgc:audio z3ta+, Cakewalk Rapture, and Yoshimi have adopted the Scala scale file format (see for a complete list) as a means to tune them instead of with a native tuning dump file. There's also a do-it-yourself hardware synth: PreenFM2 frequency modulation sound generator&lt;lb/&gt; Other instruments can be supported through modification of an external data file, if the system exclusive data format is straightforward.&lt;lb/&gt; Not all synthesizers have microtuning support in the form of a tuning table, or one with sufficient resolution, and therefore cannot be directly tuned by Scala. Hopefully more future synthesizers will be equipped with a full keyboard variable tuning capability. Be careful to check this before you buy. &lt;/p&gt;
    &lt;p&gt;See examples of some of Scala's features.&lt;/p&gt;
    &lt;p&gt;Click on the pictures to get a larger image.&lt;/p&gt;
    &lt;p&gt;Scala was created by Manuel Op de Coul in the Netherlands. E-mail: coul@huygens-fokker.org&lt;/p&gt;
    &lt;p&gt;Suggestions for improvements are always welcome. Contact the author in the event of questions or problems.&lt;/p&gt;
    &lt;p&gt;User interface languages available: English and Dutch. Help to create more translations is welcome.&lt;/p&gt;
    &lt;p&gt;Scala is freeware without warranty and may not be sold, modified, or distributed for sale in combination with commercial products. It may only be distributed as one package containing all the files mentioned here and for free.&lt;/p&gt;
    &lt;p&gt;Go to the Download page.&lt;/p&gt;
    &lt;p&gt;27 Nov 2026&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46092000</guid><pubDate>Sat, 29 Nov 2025 23:51:21 +0000</pubDate></item><item><title>Dilution vs. Risk taking: Capital gains taxes and entrepreneurs</title><link>https://www.nber.org/papers/w34512</link><description>&lt;doc fingerprint="36f950c9641638ed"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Dilution vs. Risk Taking: Capital Gains Taxes and Entrepreneurship&lt;/head&gt;
    &lt;p&gt;Recent proposals to tax unrealized capital gains or wealth have sparked a debate about their impact on entrepreneurship. We show that accrual-based taxation creates two opposing effects: successful founders face greater dilution from advance tax payments, whereas unsuccessful founders receive tax credits that effectively provide insurance. Using comprehensive new data on U.S. venture capital deals, we find that founder returns remain extremely skewed, with 84% receiving zero exit value while the top 2% capture 80% of total value. Moving from current realization-based to accrual-based taxation would reduce founder ownership at exit by 25% on average but would also increase the fraction receiving positive payoffs from 16% to 47% when tax credits are refunded. Embedding these distributions in a dynamic career choice model, we find that founders with no or moderate risk aversion prefer the current realization-based tax system, while more risk-averse founders prefer accrual-based taxation. We estimate that a 2% annual wealth tax has a similar impact on dilution as taxing unrealized capital gains but produces no risk-sharing benefits due to the absence of tax credits in case of down rounds.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Copy CitationEduardo M. Azevedo, Florian Scheuer, Kent Smetters, and Min Yang, "Dilution vs. Risk Taking: Capital Gains Taxes and Entrepreneurship," NBER Working Paper 34512 (2025), https://doi.org/10.3386/w34512.Download Citation&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46092056</guid><pubDate>Sat, 29 Nov 2025 23:59:49 +0000</pubDate></item><item><title>A new Little Prince museum has opened its doors in Switzerland</title><link>https://www.lepetitprince.com/en/events-around-the-world/a-new-little-prince-museum-has-opened-its-doors-in-switzerland/</link><description>&lt;doc fingerprint="1f82f653b7dbab38"&gt;
  &lt;main&gt;
    &lt;p&gt;On November 7, the Besenval Palace in Solothurn experienced an exceptional moment with the official opening of the museum « Der Kleine Prinz und seine Welt » dedicated to the Little Prince through the collection of Jean-Marc Probst. The event took place in the presence of Olivier d’Agay, great-nephew of Antoine de Saint-Exupéry and President of the Succession Saint-Exupéry d’Agay, accompanied by the Little Prince teams and the Antoine de Saint-Exupéry Youth Foundation.&lt;/p&gt;
    &lt;p&gt;This new museum is built upon the remarkable collection gathered over more than forty years by the Jean-Marc Probst Foundation for the Little Prince, which brings together more than ten thousand books, objects, documents, and rare editions related to the literary masterpiece published in 1943. This collection, one of the most significant in the world, is now accessible to the public in a historic setting with a particularly successful scenography created by the Bâtisseurs de Mémoire.&lt;/p&gt;
    &lt;p&gt;The Little Prince is a universal phenomenon. Translated into more than six hundred languages and dialects, it is the most translated book after the Bible. “With The Little Prince, it is almost the entire world that settles in Solothurn,” the cantonal chancellery reminded when the project was announced.&lt;/p&gt;
    &lt;p&gt;Jean-Marc Probst, a passionate collector since 1980, has patiently brought together editions from all over the world. His foundation also enriches the legacy of the work by supporting new publications since 2013. This new cultural venue will continue to preserve and share the humanist message of Antoine de Saint-Exupéry through educational and interactive content designed for children as well as grown-ups.&lt;/p&gt;
    &lt;p&gt;Located on the banks of the Aare, the Besenval Palace, built in the early eighteenth century, opens a new chapter in its history as it becomes a space devoted to imagination, sharing, and encounters inspired by the Little Prince.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46092478</guid><pubDate>Sun, 30 Nov 2025 01:03:44 +0000</pubDate></item><item><title>Meshtastic</title><link>https://meshtastic.org/</link><description>&lt;doc fingerprint="bda8ec42c8592ac0"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Getting Started&lt;/head&gt;
    &lt;head rend="h4"&gt;Step 1&lt;/head&gt;
    &lt;p&gt;Meshtastic devices are available in a variety of configurations to suit your needs.&lt;/p&gt;
    &lt;head rend="h4"&gt;Step 2&lt;/head&gt;
    &lt;p&gt;Flash your device with the latest version of Meshtastic and configure it to your liking.&lt;/p&gt;
    &lt;head rend="h4"&gt;Step 3&lt;/head&gt;
    &lt;p&gt;Connect to your device via any of our clients to start sending and receiving messages!&lt;/p&gt;
    &lt;head rend="h3"&gt;Get Connected&lt;/head&gt;
    &lt;p&gt;Connect and control your Meshtastic devices through various platforms. Choose the client that best fits your needs and device ecosystem.&lt;/p&gt;
    &lt;head rend="h4"&gt;iOS App&lt;/head&gt;
    &lt;p&gt;Manage your Meshtastic network on-the-go with our iOS application.&lt;/p&gt;
    &lt;head rend="h4"&gt;Android App&lt;/head&gt;
    &lt;p&gt;Connect and control your Meshtastic devices using our Android application.&lt;/p&gt;
    &lt;head rend="h4"&gt;Web Client&lt;/head&gt;
    &lt;p&gt;Access your Meshtastic network from any device with our web-based client.&lt;/p&gt;
    &lt;head rend="h4"&gt;Python CLI/SDK&lt;/head&gt;
    &lt;p&gt;Command-line interface and software development kit for Python developers and power users.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46092558</guid><pubDate>Sun, 30 Nov 2025 01:15:59 +0000</pubDate></item><item><title>Stopping bad guys from using my open source project (feedback wanted)</title><link>https://evanhahn.com/stopping-bad-guys-from-using-my-open-source-project/</link><description>&lt;doc fingerprint="a74433110e3f05b0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Stopping bad guys from using my open source project (feedback wanted)&lt;/head&gt;
    &lt;p&gt;In short: I maintain a sorta-popular open source package, and I want to prevent big corporations and “bad guys” from using it. I want feedback on how to do this.&lt;/p&gt;
    &lt;head rend="h2"&gt;Open source and exploitation&lt;/head&gt;
    &lt;p&gt;I’ve been learning more about open source sustainability. More accurately, I’ve been learning more about how open source is exploited by large companies.&lt;/p&gt;
    &lt;p&gt;Some recent links that have influenced my view:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This pair of slides from the maintainer of curl. The first slide: 38 massive car brands that use curl. The second slide: 0 of them give anything back.&lt;/item&gt;
      &lt;item&gt;“The Value of Open Source Software” says that “firms would need to spend 3.5 times more […] if OSS did not exist”, and that OSS is giving businesses about $12,000,000,000,000 USD (12 trillion dollars) for free.&lt;/item&gt;
      &lt;item&gt;“What is open source?” says that “volunteers are creating software for free that largely benefits large corporations.”&lt;/item&gt;
      &lt;item&gt;“Open Source Power” asserts that open source software needs to be more thoughtful about how it donates its work to the commons, because it’s being abused.&lt;/item&gt;
      &lt;item&gt;“Open Source Developers Are Exhausted, Unpaid, and Ready to Walk Away” argues that open source maintainers are being exploited and are burning out. That’s dangerous for the industry.&lt;/item&gt;
      &lt;item&gt;“How US tech giants’ AI is changing the face of warfare in Gaza and Lebanon” made me think about how open source tools like PyTorch kill innocent people, if indirectly.&lt;/item&gt;
      &lt;item&gt;“The Death of Consequences” claims that “extractive organizations will not take licensing seriously”, and that the open source movement needs more teeth.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Overall, these ideas lead me to believe that the open source movement needs to see itself as in a larger social context. Can we shift the balance of power away from massive companies and their massive harms? Can we prevent Nazis from using our software? Should we even try?&lt;/p&gt;
    &lt;head rend="h2"&gt;What can I do to help?&lt;/head&gt;
    &lt;p&gt;I maintain a sorta-popular open source package. I say popular because it had over 200 million downloads in 2025 which I believe puts it in the top 0.1% of downloads on npm. I say sorta-popular because it’s not very well-known; it sits quietly in thousands (millions?) of projects, with most developers not thinking much about it. I’m not as powerful as Linus Torvalds at the helm of Linux, but I’m also not totally unknown.&lt;/p&gt;
    &lt;p&gt;But what can I do to help?&lt;/p&gt;
    &lt;p&gt;I know my goal: shift the default in open source from “it’s free for anyone to use” to “please don’t use this if you’re evil”. I don’t just want to do this for my little project; I want to slowly change the discourse. I’m not sure how to do that effectively, if it’s even possible.&lt;/p&gt;
    &lt;p&gt;Anyone have any ideas? If you maintained a sorta-popular open source package, what would you do to help?&lt;/p&gt;
    &lt;p&gt;Some specific questions I have:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How can I bring more attention to this issue given the relative popularity of my project? Do I write a blog post? A callout in the documentation?&lt;/item&gt;
      &lt;item&gt;Should I change my project’s license? It currently uses the permissive MIT License. I remain unconvinced at the societal value of “freedom to run the program as you wish, for any purpose”, often called freedom 0. I don’t want to donate my work to the bad guys!&lt;/item&gt;
      &lt;item&gt;Would collective action be more powerful? If so, would other maintainers participate?&lt;/item&gt;
      &lt;item&gt;Should I “test” this with some of my less popular projects?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I would love your ideas. Feel free to email me@evanhahn.com, message me on Signal, or contact me another way.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46092630</guid><pubDate>Sun, 30 Nov 2025 01:25:43 +0000</pubDate></item><item><title>Show HN: Boing</title><link>https://boing.greg.technology/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46093473</guid><pubDate>Sun, 30 Nov 2025 03:46:35 +0000</pubDate></item></channel></rss>