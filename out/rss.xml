<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 30 Sep 2025 20:37:41 +0000</lastBuildDate><item><title>Orbiting the Hénon Attractor</title><link>https://observablehq.com/@yurivish/orbiting-the-henon-attractor</link><description>&lt;doc fingerprint="20ed59e32d044a20"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h3"&gt;Purpose-built for displays of data&lt;/head&gt;
      &lt;p&gt;Observable is your go-to platform for exploring data and creating expressive data visualizations. Use reactive JavaScript notebooks for prototyping and a collaborative canvas for visual data exploration and dashboard creation.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45424223</guid><pubDate>Tue, 30 Sep 2025 11:31:04 +0000</pubDate></item><item><title>How has mathematics gotten so abstract?</title><link>https://lcamtuf.substack.com/p/how-has-mathematics-gotten-so-abstract</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45424648</guid><pubDate>Tue, 30 Sep 2025 12:33:43 +0000</pubDate></item><item><title>Pasta Cooking Time</title><link>https://www.jefftk.com/p/pasta-cooking-time</link><description>&lt;doc fingerprint="2591508e76c709b3"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;head&gt;Pasta Cooking Time&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;August 22nd, 2025&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I decided to use Market Basket Rigatoni. [1] It's a ridged cylinder, and I measured the ridges at 1.74mm:&lt;/p&gt;
    &lt;p&gt;And the valleys at 1.32mm:&lt;/p&gt;
    &lt;p&gt;The box recommends 13-15 minutes:&lt;/p&gt;
    &lt;p&gt;This is a house brand pasta from a chain centered in a part of the country with a relatively high Italian-American population, so you might think they'd avoid the issue where Americans often cook pasta absurdly long:&lt;/p&gt;
    &lt;p&gt;I boiled some water, put in the pasta, and starting at 9min I removed a piece every 15s until I got to 14:30:&lt;/p&gt;
    &lt;p&gt;Here's the minute-by-minute, cut open so you can see the center of the noodles:&lt;/p&gt;
    &lt;p&gt;My family and I tried a range of noodles, trying to bisect our way to the ideal cooking time. I was happiest at 10m15s, but ok between 9m15s and 11m30s. Julia thought 9m45s was barely underdone, while 11m45s was barely overdone. Anna liked 10m30s. Lily didn't like any of them, consistently calling them "too crunchy" up through 10m45s and then "too mushy" for 11m0s and up. Everyone agreed that by 12m45s it was mushy.&lt;/p&gt;
    &lt;p&gt;Instead of 13-15min, a guideline of 10-12min would make a lot more sense in our house. And, allegedly, the glycemic index is much lower.&lt;/p&gt;
    &lt;p&gt;My mother and her siblings grew up in Rome, and I wrote asking about what they'd noticed here. My uncle replied "my bias is that Americans are wimps for soft pasta" and the others agreed.&lt;/p&gt;
    &lt;p&gt;I tried using a cheap microscope to investigate, whether there were interesting structural differences, but even with an iodide stain I couldn't make out much. Here's 3min:&lt;/p&gt;
    &lt;p&gt;And 7min:&lt;/p&gt;
    &lt;p&gt;And 13min:&lt;/p&gt;
    &lt;p&gt;On the other hand, the kids and I did have fun with the microscope.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; [1] We called these "hospital noodles" growing up, because when my mother had been in a hospital for a long time as a kid (recovering from being hit by an impatient driver while crossing the street) they had served Rigatoni as their primary pasta shape. &lt;/p&gt;
    &lt;p&gt;Comment via: facebook, lesswrong, mastodon, bluesky&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45424704</guid><pubDate>Tue, 30 Sep 2025 12:40:17 +0000</pubDate></item><item><title>Founder sentenced to seven years in prison for fraudulent sale to JPMorgan</title><link>https://www.cnn.com/2025/09/30/business/charlie-javice-frank-sentenced-jpmorgan-intl</link><description>&lt;doc fingerprint="3e7531193969103d"&gt;
  &lt;main&gt;
    &lt;p&gt;Charlie Javice, the founder of a startup company that promised to revolutionize the way college students apply for financial aid, was sentenced Monday to more than seven years in prison for cheating JPMorgan Chase out of $175 million by greatly exaggerating how many students it served.&lt;/p&gt;
    &lt;p&gt;Javice, 33, was convicted in March of duping the banking giant when it bought her company, called Frank, in the summer of 2021. She made false records that made it seem like Frank had over 4 million customers when it had fewer than 300,000.&lt;/p&gt;
    &lt;p&gt;Addressing the court before she was sentenced, Javice, who was in her mid-20s when she founded the company, said she was “haunted that my failure has transformed something meaningful into something infamous.”&lt;/p&gt;
    &lt;p&gt;Sometimes speaking through tears, she said she “made a choice that I will spend my entire life regretting.”&lt;/p&gt;
    &lt;p&gt;Judge Alvin K. Hellerstein largely dismissed arguments by Javice’s lawyer, Ronald Sullivan, that he should be lenient because the negotiations that led to Frank’s sale pitted “a 28-year-old versus 300 investment bankers from the largest bank in the world.”&lt;/p&gt;
    &lt;p&gt;Still, the judge criticized the bank, saying “they have a lot to blame themselves” for after failing to do adequate due diligence. He quickly added, though, that he was “punishing her conduct and not JPMorgan’s stupidity.”&lt;/p&gt;
    &lt;p&gt;Javice was among a number of young tech executives who vaulted to fame with supposedly disruptive or transformative companies, only to see them collapse amid questions about whether they had engaged in puffery and fraud while dealing with investors.&lt;/p&gt;
    &lt;p&gt;Her prosecution drew comparisons to the case against Elizabeth Holmes, the founder of a blood testing company, Theranos, that collapsed amid fraud allegations.&lt;/p&gt;
    &lt;p&gt;Javice, who lives in Florida, has been free on $2 million bail since her 2023 arrest. The judge said she could remain free while she appeals the verdict. She was convicted of conspiracy, bank fraud and wire fraud charges. Her lawyers had argued that JPMorgan (JPM) went after Javice because it had buyer’s remorse.&lt;/p&gt;
    &lt;p&gt;A graduate of the University of Pennsylvania’s Wharton School of Business, Javice founded Frank to launch software that promised to simplify the arduous process of filling out the Free Application for Federal Student Aid, a complex government form used by students to apply for aid for college or graduate school.&lt;/p&gt;
    &lt;p&gt;Frank’s backers included venture capitalist Michael Eisenberg. The company said its offering, akin to online tax preparation software, could help students maximize financial aid while making the application process less painful.&lt;/p&gt;
    &lt;p&gt;The company promoted itself as a way for financially needy students to obtain more aid faster, in return for a few hundred dollars in fees. Javice appeared regularly on cable news programs to boost Frank’s profile, also once appearing on Forbes’ “30 Under 30” list before JPMorgan bought the startup.&lt;/p&gt;
    &lt;p&gt;Sullivan told Hellerstein that his client was very different from Holmes because what she created actually worked, unlike Holmes, “who did not have a real company” and whose product “in fact endangered patients.” Sullivan said the bank rushed its negotiations because it feared another bank would acquire Frank first.&lt;/p&gt;
    &lt;p&gt;A prosecutor, Micah Fergenson, though, said JPMorgan “didn’t get a functioning business” in exchange for its investment. “They acquired a crime scene.”&lt;/p&gt;
    &lt;p&gt;Fergenson said Javice was driven by greed when she saw that she could pocket $29 million from the sale of her company.&lt;/p&gt;
    &lt;p&gt;“Ms. Javice had it dangling in front of her and she lied to get it,” he said.&lt;/p&gt;
    &lt;p&gt;And in seeking a long prison sentence for Javice, prosecutors cited a 2022 text she had sent to a colleague in which she called it “ridiculous” that Holmes got over 11 years in prison in the Theranos case.&lt;/p&gt;
    &lt;p&gt;Prosecutors noted “an alarming trend of founders and executives of small startup companies engaging in fraud, including making misrepresentations about their companies’ core products or services, in order to make their companies attractive targets for investors and/or buyers.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45424827</guid><pubDate>Tue, 30 Sep 2025 12:53:49 +0000</pubDate></item><item><title>Imgur pulls out of UK as data watchdog threatens fine</title><link>https://www.express.co.uk/news/uk/2115228/image-site-imgur-pulls-out</link><description>Imgur founder.

An image hosting platform with more than 130 million users has stopped being available in the UK after regulators signalled their intention to impose penalties over concerns around children’s data. The Information Commissioner’s Office (ICO) said that it has reached provisional findings in an investigation in the parent company of image hosting site, Imgur. Its probe was launched earlier this year, as part of the regulator's Children’s Code strategy, which is intended to set the standards for how online services handle the personal information of young people. Article continues below ADVERTISEMENT In a statement the ICO said: “We are aware of reports that the social media platform Imgur is currently not available in the UK. Imgur's decision to restrict access in the UK is a commercial decision taken by the company.”

Tim Capel, the ICO’s Interim Executive Director for Regulatory Supervision, said that the regulator had now issued a notice of intent to fine. He said: “We reached our provisional findings on this investigation, and we issued a notice of intent to impose a monetary penalty on MediaLab on 10 September 2025. “Our findings are provisional and the ICO will carefully consider any representations from MediaLab before taking a final decision whether to issue a monetary penalty.” Article continues below ADVERTISEMENT The ICO also confirmed that companies could not avoid accountability by withdrawing their services in the UK. Mr Capel said: “We have been clear that exiting the UK does not allow an organisation to avoid responsibility for any prior infringement of data protection law, and our investigation remains ongoing. “This update has been provided to give clarity on our investigation, and we will not be providing any further detail at this time.”

He added that protecting young people’s information remains a central focus: “Safeguarding children’s personal information is a key priority for the ICO and our Children’s code strategy outlines our key interventions in this area. Keeping children safe online is the responsibility of the companies offering online services to them and we will continue to hold them to account.” Regulators did not disclose the potential size of the penalty for specific breaches it has identified. Under UK law, the “notice of intent” process gives the company an opportunity to make representations before any final decision is made. Imgur, founded in 2009 and acquired by Los Angeles-based MediaLab AI Inc in 2021, is an image hosting and sharing site popular for memes, viral content and online communities. It’s services appeared to become unavailable in the UK last night. Imgur was approached for comment.

Read next







</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45424888</guid><pubDate>Tue, 30 Sep 2025 13:01:05 +0000</pubDate></item><item><title>An opinionated critique of Duolingo</title><link>https://isomorphism.xyz/blog/2025/duolingo/</link><description>&lt;doc fingerprint="ec79919e09a13ed2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;an opinionated critique of duolingo&lt;/head&gt;
    &lt;p&gt;During the stay-at-home grim days of 2020, I started learning Spanish on Duolingo. Having a working understanding of Spanish seemed like a sensible first step towards opening a taco truck in Mexico, in case I had to run away from my doctoral studies. This July, after about 5 years I decided to end the 1800 day streak that I managed to drag on with numerous streak freezes and minimal effort lessons. While Spanish words look less foreign, and with some focus, I am able to decipher small paragraphs of grammatically simple sentences; the effort was less than a smashing success – I certainly could not be writing this essay in Spanish.&lt;/p&gt;
    &lt;p&gt;If Duolingo is known for anything, it has to be their gamification approach. There is no shortage of gamification mechanics on the platform: XP; potions to double your XP; leagues of gold, silver, and all sorts of other metals and minerals; treasure chests; quests; monthly quests; and so on. I have only ever paid little attention to these mechanics. While I still haven’t entirely rejected the idea that a good RPG could be a good scaffolding to teach a language, I do not think Duolingo is one.&lt;/p&gt;
    &lt;p&gt;Games worth their salt are not created by bolting together a collection of numerical statistics. That is how you get cookie clicker. I did not have a good understanding of how the mechanics work: if I learn 10 words, how many XP do I get for my hard work? Is the Diamond League higher or lower than the Obsidian League? I could have viewed their documentation to figure it out, but there was nothing motivating me to do so. If I collect 100 XP, what does it mean for my language skills? For that matter, why do I collect extra XP when I receive a potion? Can the XP I collect be used in a way to carefully guide me towards the specific language skills I would explore next? Navigating the mechanical gameplay of Duolingo is neither rewarding for its own sake, nor is it helpful towards actually learning a language.&lt;/p&gt;
    &lt;p&gt;Duolingo is not just a poor simulacrum of the mechanical aspects of a game, but also of the social aspects of one. Who are all these people I am on the Silver league with? Having a comparable amount of XP does not give me a sense of social connection with them. When I click a button to congratulate a friend on Duolingo, I do not truly engage with their learning journey. Indeed, it is worse than hearting an instagram photo, or upvoting a reddit thread. In those cases, I am reacting to a sliver of expression from my acquaintance. Here, I am presented only with a pre-rendered text with an abstract numerical statistic. Reacting to it is deliberately frictionless: I am presented with a wall of buttons allowing me to click them with ease and without thought. When Duolingo tells me that so and so sent me a message saying “Hey, come back and learn Spanish with me!”, I don’t admire how thoughtful and encouraging my friend is; I just notice that they clicked a button to send me a pre-generated message.&lt;/p&gt;
    &lt;p&gt;Interactions on Duolingo were not always of the push-button variety. Duolingo had forums where users would discuss different aspects of their language learning journey. In fact, Duolingo would link each sentence to its own forum thread for discussion – discussion, which was at the very least, helpful, and at times, eye-opening. At first, these discussion threads were locked, and later removed. My hypothesis is that for the business geniuses running Duolingo, the forums were assesed to be a liability, having to moderate which were not worth spending the dollars for. The nature of interacting with people – friends or strangers, in person or online – is that sometimes bad things would happen. Even when there is no abuse or harassment going on, there is always the risk that the other person might greet you with disagreement, or worse, apathy. Many people tend to think that the risks outweigh the benefits.&lt;/p&gt;
    &lt;p&gt;The gamification mechanic that I did latch on to was the Streak. I generally have been critical of the green owl, but I do think that it did help me form a good habit – a net positive, despite the minuscule magnitude. Regular Duolingo users will know that the streak can be gamed away in more than one ways. Streak Freezes can be bought using gems (of which I happen to have 24,053 of, somehow) or be gifted by your friends, and equipped 2 at a time. Streaks wouldn’t have their social effect if there weren’t enough people with a moderate number of people with decent streaks to be sprinkled around. Maintaining the streak, even without freezes, does not have to mean that you are learning – repeating a simple lesson from several units ago would work. My 1800 day streak didn’t mean that I spent 1800 days learning Spanish; it meant that I spent a large number of days engaging with the platform. I later started peeking into the Japanese and Finnish courses, and the 1800 day number includes them. If you loose interest in languages, Duolingo tells us that spending time with math or music will count towards your Streak.&lt;/p&gt;
    &lt;p&gt;The deficiency of Duolingo’s pedagogy was first made obvious to me by the excellent audio lessons produced by Language Transfer. Going through the first few lessons of Language Transfer, I was unfazed, observing that I had already learnt what was being taught. Soon, what shocked me was how quickly Language Transfer caught up to what I had managed to learn in a couple years of time. While Duolingo is great at making sure that the user comes back to the app everyday, their pedagogy is subpar. Remaining true to gamification, Duolingo prefers to throw users head-first into translation exercises. If you do not know a word, you hover over it and you arrange a given bag of words into a sentence that is hopefully meaningful. Grammar lessons are extremely minimal. The removal of the forums dedicated to the discussion of specific sentences did not help. Understanding the course outline – knowing what is taught where, or reviewing lessons – is not easy.&lt;/p&gt;
    &lt;p&gt;Supposedly, the Duolingo philosophy is that if you are exposed to enough sentences, you will eventually learn how to use them. I do believe this is true, and I do believe the exercises are indispensable. However, the whole process could be greatly improved by a few more lessons interspersed in the curriculum telling the student what is going on. To see this, I would ask myself, did I always internalize the grammatically correct structures even in my own mother tongue? I think not, and my language skills have improved when my parents or teachers would point out simple grammatical mistakes. Eggcorns are a closely related amusing phenomena.&lt;/p&gt;
    &lt;p&gt;I cannot tell if Duolingo repeats different concepts in exercises adaptively based on your mastery, or are simply fixed in the course material. Repetition is good for learning but Duolingo’s repetition can be frustrating. The platform’s interface is largely built around clicking a bag of jumbled words one at a time to input a translation. Once you learn a concept well enough, most of your mental energy is spent on the finding and clicking of the words rather than the translation. Thankfully, this situation can be made better by dictating your answer, as pointed out by the official blog.&lt;/p&gt;
    &lt;p&gt;Duolingo does include a few other formats for their lessons. There are some stories, which are interspersed in the course. The stories are short, but silly and enjoyable. There are also audio only lessons, which are also shorter and unfortunately, not as fun. From time to time, the regular lessons also ask you to speak to the microphone but in my experience, the audio recognition seems to accept the answer even if I mumble through the words. Duolingo is also known for its usage of bizarre phrases, whose shock value generates social media buzz and may or may not have a positive pedagogical effect.&lt;/p&gt;
    &lt;p&gt;Duolingo is a neat case study in Silicon Valley ideology. Big tech embraces blitz-scaling: the primary goal is neither financial sustainability nor the quality of materials but making the number of users grow. The faux gamification and passive-aggressive messaging may be helpful with little else, but is good for user retention. The expansionism does not stop at growing the number of users; Duolingo has decided that they must loop in music and math learners as well. As we have discussed, the maxim of friction reduction has guided them towards optimizing away authenticity in the user interactions on the platform.&lt;/p&gt;
    &lt;p&gt;In April 2025, Duolingo decided to go AI-first. Supposedly, “to teach well, [they] need to create a massive amount of content” – so much so that “doing [it] manually does not scale”. For the top ten languages, I cannot imagine any reasonable person saying that the lack of study material is the main obstacle towards learning. This statement spells out what the Duolingo executives value. The Duolingo CEO is not shy to admit it. In an interview with NPR, he said the following.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[I]f it’s our content, as in, like, our learning content, there’s so much of that - thousands and thousands and thousands of kind of sentences and words and paragraphs. That is mostly done by computers, and we probably spot-check it. But if it’s things like the user interface of Duolingo, where we say - like, you know, the button says quit, and we have to translate, that is all done with humans. And we spend a lot of effort on that, but that’s because each one of those is highly valuable.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Yes, the button that says ‘quit’ is more valuable than the learning material, which is only ‘probably’ spot-checked.&lt;/p&gt;
    &lt;p&gt;After I moved to Japan, I dialed up my efforts to learn Japanese. For a while, I shifted over my Duolingo habits to Japanese. Because Duolingo wasn’t my only learning material for Japanese, it was glaringly obvious very soon that the Duolingo pedagogy is unhelpful and often misleading. While the Spanish learner has to introduce themselves to a few new concepts (e.g, ser vs estar, or reflexive verbs), the Japanese learner faces an explosion of differences. Japanese has a writing system with three components; generally uses a topic-comment structure and often omits the topic; has a subject-object-verb order; has adjectives which conjugate; a lot of counting suffixes; sentence ending particles and famously, a complex honorific system. Duolingo does not break its gamification façade to teach the user some of these concepts head-on. Instead, it pretends that translating between Japanese and English is a matter of substituting phrases and shuffling them around.&lt;/p&gt;
    &lt;p&gt;Since I gave up on my Duolingo streak, I have started exploring other avenues to continue learning Japanese. I participate in group lessons with a tutor once a week for an hour. Believe it or not, the tutor has more charm than Falstaff. I regularly do my flashcard kanji study with Wanikani. A newer addition to my study routine is Bunpro. My progress has been slow but evident: when I recognize that the names of the metro stations I frequent break down into simple words, they lose a little bit of their mystery but it is a satisfying revelation.&lt;/p&gt;
    &lt;p&gt;These platforms are a welcome contrast against the techno-accelerationist attitude of Duolingo. Instead of trying to do it all, they are extremely niche: they only teaching one language and Wanikani is focused at teaching a very specific element of it. Wanikani maintains a public API, which makes third-party apps and scripts possible. I praise them for their welcome attitude towards interoperability instead of trying to build a closed ecosystem. Both Wanikani and Bunpro have vibrant user forums. Bunpro makes actual lessons part of their critical path, instead of hoping that the user will eventually figure it out. When a Bunpro user feels that their lesson was not adequate, they do not have to rely on AI generated slop – Bunpro directs users to carefully-crafted lessons by other people (see the ‘resource’ section at the end of this page, for example).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45425061</guid><pubDate>Tue, 30 Sep 2025 13:19:53 +0000</pubDate></item><item><title>dgsh – Directed Graph Shell</title><link>https://www2.dmst.aueb.gr/dds/sw/dgsh/</link><description>&lt;doc fingerprint="577bba9067313cc8"&gt;
  &lt;main&gt;&lt;p&gt;The directed graph shell, dgsh (pronounced /dÃ¦É¡Ê/ — dagsh), provides an expressive way to construct sophisticated and efficient big data set and stream processing pipelines using existing Unix tools as well as custom-built components. It is a Unix-style shell (based on bash) allowing the specification of pipelines with non-linear non-uniform operations. These form a directed acyclic process graph, which is typically executed by multiple processor cores, thus increasing the operation's processing throughput.&lt;/p&gt;&lt;p&gt;If you want to get a feeling on how dgsh works in practice, skip right down to the examples section.&lt;/p&gt;&lt;p&gt; For a more formal introduction to dgsh or to cite it in your work, see:&lt;lb/&gt; Diomidis Spinellis and Marios Fragkoulis. Extending Unix Pipelines to DAGs. IEEE Transactions on Computers, 2017. doi: 10.1109/TC.2017.2695447 &lt;/p&gt;&lt;p&gt;Dgsh provides two new ways for expressing inter-process communication.&lt;/p&gt;&lt;code&gt;comm&lt;/code&gt; command supplied with dgsh
expects two input channels and produces on its output three
output channels: the lines appearing only in first (sorted) channel,
the lines appearing only in the second channel,
and the lines appearing in both.
Connecting the output of the &lt;code&gt;comm&lt;/code&gt; command to the
&lt;code&gt;cat&lt;/code&gt; command supplied with dgsh
will make the three outputs appear in sequence,
while connecting it to the
&lt;code&gt;paste&lt;/code&gt; command supplied with dgsh
will make the output appear in its customary format.
&lt;code&gt;md5sum&lt;/code&gt; and &lt;code&gt;wc -c&lt;/code&gt;
receives two inputs and produces two outputs:
the MD5 hash of its input and the input's size.
Data to multipipe blocks are typically provided with a
dgsh-aware version of &lt;code&gt;tee&lt;/code&gt; and collected by
dgsh-aware versions of programs such as
&lt;code&gt;cat&lt;/code&gt; and &lt;code&gt;paste&lt;/code&gt;.
&lt;code&gt;dgsh-writeval&lt;/code&gt;, and
a reader program, &lt;code&gt;dgsh-readval&lt;/code&gt;.
The behavior of a stored value's IO can be modified by adding flags to
&lt;code&gt;dgsh-writeval&lt;/code&gt; and &lt;code&gt;dgsh-readval&lt;/code&gt;.
&lt;p&gt;A dgsh script follows the syntax of a bash(1) shell script with the addition of multipipe blocks. A multipipe block contains one or more dgsh simple commands, other multipipe blocks, or pipelines of the previous two types of commands. The commands in a multipipe block are executed asynchronously (in parallel, in the background). Data may be redirected or piped into and out of a multipipe block. With multipipe blocks dgsh scripts form directed acyclic process graphs. It follows from the above description that multipipe blocks can be recursively composed.&lt;/p&gt;&lt;p&gt;As a simple example consider running the following command directly within dgsh&lt;/p&gt;&lt;quote&gt;{{ echo hello &amp;amp; echo world &amp;amp; }} | paste&lt;/quote&gt;&lt;p&gt; or by invoking &lt;code&gt;dgsh&lt;/code&gt; with the command as an argument.
&lt;/p&gt;&lt;quote&gt;dgsh -c '{{ echo hello &amp;amp; echo world &amp;amp; }} | paste'&lt;/quote&gt;&lt;p&gt; The command will run paste with input from the two echo processes to output &lt;code&gt;hello world&lt;/code&gt;.
This is equivalent to running the following bash command,
but with the flow of data appearing in the natural left-to-right order.
&lt;/p&gt;&lt;quote&gt;paste &amp;lt;(echo hello) &amp;lt;(echo world)&lt;/quote&gt;&lt;p&gt; In the following larger example, which compares the performance of different compression utilities, the script's standard input is distributed to three compression utilities (xz, bzip2, and gzip), to assess their performance, and also to file and wc to report the input data's type and size. The printf commands label the data of each processing type. All eight commands pass their output to the &lt;code&gt;cat&lt;/code&gt; command, which gathers their outputs
in order.
&lt;/p&gt;&lt;quote&gt;tee | {{ printf 'File type:\t' file - printf 'Original size:\t' wc -c printf 'xz:\t\t' xz -c | wc -c printf 'bzip2:\t\t' bzip2 -c | wc -c printf 'gzip:\t\t' gzip -c | wc -c }} | cat&lt;/quote&gt;&lt;p&gt; Formally, dgsh extends the syntax of the (modified) Unix Bourne-shell when &lt;code&gt;bash&lt;/code&gt; provided with the &lt;code&gt;--dgsh&lt;/code&gt; argument
as follows.
&lt;/p&gt;&lt;quote&gt;&amp;lt;dgsh_block&amp;gt; ::= '{{' &amp;lt;dgsh_list&amp;gt; '}}' &amp;lt;dgsh_list&amp;gt; ::= &amp;lt;dgsh_list_item&amp;gt; '&amp;amp;' &amp;lt;dgsh_list_item&amp;gt; &amp;lt;dgsh_list&amp;gt; &amp;lt;dgsh_list_item&amp;gt; ::= &amp;lt;simple_command&amp;gt; &amp;lt;dgsh_block&amp;gt; &amp;lt;dgsh_list_item&amp;gt; '|' &amp;lt;dgsh_list_item&amp;gt;&lt;/quote&gt;&lt;p&gt;A number of Unix tools have been adapted to support multiple inputs and outputs to match their natural capabilities. This echoes a similar adaptation that was performed in the early 1970s when Unix and the shell got pipes and the pipeline syntax. Many programs that worked with files were adjusted to work as filters. The number of input and output channels of dgsh-compatible commands are as follows, based on the supplied command-line arguments.&lt;/p&gt;&lt;table&gt;&lt;row span="4"&gt;&lt;cell role="head"&gt;Tool&lt;/cell&gt;&lt;cell role="head"&gt;Inputs&lt;/cell&gt;&lt;cell role="head"&gt;Outputs&lt;/cell&gt;&lt;cell role="head"&gt;Notes&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;cat (dgsh-tee)&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;0—M&lt;/cell&gt;&lt;cell&gt;No options are supported&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;cmp&lt;/cell&gt;&lt;cell&gt;0—2&lt;/cell&gt;&lt;cell&gt;0—1&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;comm&lt;/cell&gt;&lt;cell&gt;0—2&lt;/cell&gt;&lt;cell&gt;0—3&lt;/cell&gt;&lt;cell&gt;Output streams in order: lines only in first file, lines only in second one, and lines in both files&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;cut&lt;/cell&gt;&lt;cell&gt;0—1&lt;/cell&gt;&lt;cell&gt;1—N&lt;/cell&gt;&lt;cell&gt;With &lt;code&gt;--multistream&lt;/code&gt; output each range into a different stream&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;diff&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;Typically two inputs. Compare an arbitrary number of input streams with the &lt;code&gt;--from-file&lt;/code&gt; or &lt;code&gt;--to-file&lt;/code&gt; options&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;diff3&lt;/cell&gt;&lt;cell&gt;0—3&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;grep&lt;/cell&gt;&lt;cell&gt;0—2&lt;/cell&gt;&lt;cell&gt;0—4&lt;/cell&gt;&lt;cell&gt;Available output streams (via arguments): matching files, non-matching files, matching lines, and non-matching lines&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;join&lt;/cell&gt;&lt;cell&gt;0—2&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;paste&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;Paste N input streams&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;perm&lt;/cell&gt;&lt;cell&gt;1—N&lt;/cell&gt;&lt;cell&gt;1—N&lt;/cell&gt;&lt;cell&gt;Rearrange the order of N input streams&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;sort&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;0—1&lt;/cell&gt;&lt;cell&gt;With the &lt;code&gt;-m&lt;/code&gt; option, merge sort N input streams&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;tee (dgsh-tee)&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;0—M&lt;/cell&gt;&lt;cell&gt;Only the &lt;code&gt;-a&lt;/code&gt; option is supported&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;dgsh-readval&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;Read a value from a socket&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;dgsh-wrap&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;0—1&lt;/cell&gt;&lt;cell&gt;Wrap non-dgsh commands and negotiate on their behalf&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;dgsh-writeval&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;Write a value to a socket&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt; In addition, POSIX user commands that receive no input or only generate no output, when executed in a dgsh context are wrapped to specify the corresponding input or output capability. For example, an &lt;code&gt;echo&lt;/code&gt; command in a multipipe block
will appear to receive no input, but will provide one output stream.
By default &lt;code&gt;dgsh&lt;/code&gt; automatically wraps all other
commands as filters.
&lt;/p&gt;&lt;p&gt;Finally, note that any dgsh script will accept and generate the number of inputs and outputs associated with the commands or multipipe blocks at its two endpoints.&lt;/p&gt;&lt;p&gt;The dgsh suite has been tested under Debian and Ubuntu Linux, FreeBSD, and Mac OS X. A Cygwin port is underway.&lt;/p&gt;&lt;p&gt;An installation of GraphViz will allow you to visualize the dgsh graphs that you specify in your programs.&lt;/p&gt;&lt;p&gt;To compile and run dgsh you will need to have the following commands installed on your system:&lt;/p&gt;&lt;quote&gt;make automake gcc libtool pkg-config texinfo help2man autopoint bison check gperf git xz-utils gettextTo test dgsh you will need to have the following commands installed in your system:&lt;/quote&gt;&lt;quote&gt;wbritish wamerican libfftw3-dev csh curl bzip2&lt;/quote&gt;&lt;p&gt;Go through the following steps.&lt;/p&gt;&lt;quote&gt;git clone --recursive https://github.com/dspinellis/dgsh.git&lt;/quote&gt;&lt;quote&gt;make config&lt;/quote&gt;&lt;quote&gt;make&lt;/quote&gt;&lt;quote&gt;sudo make install&lt;/quote&gt;&lt;p&gt; By default, the program and its documentation are installed under &lt;code&gt;/usr/local&lt;/code&gt;.
You can modify this by setting the &lt;code&gt;PREFIX&lt;/code&gt; variable
in the `config` step, for example:
&lt;/p&gt;&lt;quote&gt;make PREFIX=$HOME config make make install&lt;/quote&gt;&lt;p&gt;Issue the following command.&lt;/p&gt;&lt;quote&gt;make test&lt;/quote&gt;&lt;p&gt;To compile and run dgsh you will need to have the following packages installed in your system:&lt;/p&gt;&lt;quote&gt;devel/automake devel/bison devel/check devel/git devel/gmake devel/gperf misc/help2man print/texinfo shells/bashTo test dgsh you will need to have the following ports installed on your system:&lt;/quote&gt;&lt;quote&gt;archivers/bzip2 ftp/curl&lt;/quote&gt;&lt;p&gt;Go through the following steps.&lt;/p&gt;&lt;quote&gt;git clone --recursive https://github.com/dspinellis/dgsh.git&lt;/quote&gt;&lt;quote&gt;gmake config&lt;/quote&gt;&lt;quote&gt;gmake&lt;/quote&gt;&lt;quote&gt;sudo gmake install&lt;/quote&gt;&lt;p&gt; By default, the program and its documentation are installed under &lt;code&gt;/usr/local&lt;/code&gt;.
You can modify this by setting the &lt;code&gt;PREFIX&lt;/code&gt; variable
in the `config` step, for example:
&lt;/p&gt;&lt;quote&gt;gmake PREFIX=$HOME config gmake gmake install&lt;/quote&gt;&lt;p&gt;Issue the following command.&lt;/p&gt;&lt;quote&gt;gmake test&lt;/quote&gt;&lt;p&gt;These are the manual pages for dgsh, the associated helper programs and the API in formats suitable for browsing and printing. The commands are listed in the order of usefulness in everyday scenarios.&lt;/p&gt;&lt;p&gt;Report file type, length, and compression performance for data received from the standard input. The data never touches the disk. Demonstrates the use of an output multipipe to source many commands from one followed by an input multipipe to sink to one command the output of many and the use of dgsh-tee that is used both to propagate the same input to many commands and collect output from many commands orderly in a way that is transparent to users.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh tee | {{ printf 'File type:\t' file - printf 'Original size:\t' wc -c printf 'xz:\t\t' xz -c | wc -c printf 'bzip2:\t\t' bzip2 -c | wc -c printf 'gzip:\t\t' gzip -c | wc -c }} | cat&lt;/quote&gt;&lt;p&gt;Process the Git history, and list the authors and days of the week ordered by the number of their commits. Demonstrates streams and piping through a function.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh forder() { sort | uniq -c | sort -rn } git log --format="%an:%ad" --date=default "$@" | tee | {{ echo "Authors ordered by number of commits" # Order by frequency awk -F: '{print $1}' | forder echo "Days ordered by number of commits" # Order by frequency awk -F: '{print substr($2, 1, 3)}' | forder }} | cat&lt;/quote&gt;&lt;p&gt;Process a directory containing C source code, and produce a summary of various metrics. Demonstrates nesting, commands without input.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh {{ # C and header code find "$@" \( -name \*.c -or -name \*.h \) -type f -print0 | tee | {{ # Average file name length # Convert to newline separation for counting echo -n 'FNAMELEN: ' tr \\0 \\n | # Remove path sed 's|^.*/||' | # Maintain average awk '{s += length($1); n++} END { if (n&amp;gt;0) print s / n; else print 0; }' xargs -0 /bin/cat | tee | {{ # Remove strings and comments sed 's/#/@/g;s/\\[\\"'\'']/@/g;s/"[^"]*"/""/g;'"s/'[^']*'/''/g" | cpp -P | tee | {{ # Structure definitions echo -n 'NSTRUCT: ' egrep -c 'struct[ ]*{|struct[ ]*[a-zA-Z_][a-zA-Z0-9_]*[ ]*{' #}} (match preceding openings) # Type definitions echo -n 'NTYPEDEF: ' grep -cw typedef # Use of void echo -n 'NVOID: ' grep -cw void # Use of gets echo -n 'NGETS: ' grep -cw gets # Average identifier length echo -n 'IDLEN: ' tr -cs 'A-Za-z0-9_' '\n' | sort -u | awk '/^[A-Za-z]/ { len += length($1); n++ } END { if (n&amp;gt;0) print len / n; else print 0; }' }} # Lines and characters echo -n 'CHLINESCHAR: ' wc -lc | awk '{OFS=":"; print $1, $2}' # Non-comment characters (rounded thousands) # -traditional avoids expansion of tabs # We round it to avoid failing due to minor # differences between preprocessors in regression # testing echo -n 'NCCHAR: ' sed 's/#/@/g' | cpp -traditional -P | wc -c | awk '{OFMT = "%.0f"; print $1/1000}' # Number of comments echo -n 'NCOMMENT: ' egrep -c '/\*|//' # Occurences of the word Copyright echo -n 'NCOPYRIGHT: ' grep -ci copyright }} }} # C files find "$@" -name \*.c -type f -print0 | tee | {{ # Convert to newline separation for counting tr \\0 \\n | tee | {{ # Number of C files echo -n 'NCFILE: ' wc -l # Number of directories containing C files echo -n 'NCDIR: ' sed 's,/[^/]*$,,;s,^.*/,,' | sort -u | wc -l }} # C code xargs -0 /bin/cat | tee | {{ # Lines and characters echo -n 'CLINESCHAR: ' wc -lc | awk '{OFS=":"; print $1, $2}' # C code without comments and strings sed 's/#/@/g;s/\\[\\"'\'']/@/g;s/"[^"]*"/""/g;'"s/'[^']*'/''/g" | cpp -P | tee | {{ # Number of functions echo -n 'NFUNCTION: ' grep -c '^{' # Number of gotos echo -n 'NGOTO: ' grep -cw goto # Occurrences of the register keyword echo -n 'NREGISTER: ' grep -cw register # Number of macro definitions echo -n 'NMACRO: ' grep -c '@[ ]*define[ ][ ]*[a-zA-Z_][a-zA-Z0-9_]*(' # Number of include directives echo -n 'NINCLUDE: ' grep -c '@[ ]*include' # Number of constants echo -n 'NCONST: ' grep -ohw '[0-9][x0-9][0-9a-f]*' | wc -l }} }} }} # Header files echo -n 'NHFILE: ' find "$@" -name \*.h -type f | wc -l }} | # Gather and print the results cat&lt;/quote&gt;&lt;p&gt;List the names of duplicate files in the specified directory. Demonstrates the combination of streams with a relational join.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Create list of files find "$@" -type f | # Produce lines of the form # MD5(filename)= 811bfd4b5974f39e986ddc037e1899e7 xargs openssl md5 | # Convert each line into a "filename md5sum" pair sed 's/^MD5(//;s/)= / /' | # Sort by MD5 sum sort -k2 | tee | {{ # Print an MD5 sum for each file that appears more than once awk '{print $2}' | uniq -d # Promote the stream to gather it cat }} | # Join the repeated MD5 sums with the corresponding file names # Join expects two inputs, second will come from scatter # XXX make streaming input identifiers transparent to users join -2 2 | # Output same files on a single line awk ' BEGIN {ORS=""} $1 != prev &amp;amp;&amp;amp; prev {print "\n"} END {if (prev) print "\n"} {if (prev) print " "; prev = $1; print $2}'&lt;/quote&gt;&lt;p&gt;Highlight the words that are misspelled in the command's first argument. Demonstrates stream processing with multipipes and the avoidance of pass-through constructs to avoid deadlocks.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh export LC_ALL=C tee | {{ # Find errors {{ # Obtain list of words in text tr -cs A-Za-z \\n | tr A-Z a-z | sort -u # Ensure dictionary is compatibly sorted sort /usr/share/dict/words }} | # List errors as a set difference comm -23 # Pass through text cat }} | grep --fixed-strings --file=- --ignore-case --color --word-regex --context=2&lt;/quote&gt;&lt;p&gt;Read text from the standard input and list words containing a two-letter palindrome, words containing four consonants, and words longer than 12 characters.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Consistent sorting across machines export LC_ALL=C # Stream input from file cat $1 | # Split input one word per line tr -cs a-zA-Z \\n | # Create list of unique words sort -u | tee | {{ # Pass through the original words cat # List two-letter palindromes sed 's/.*\(.\)\(.\)\2\1.*/p: \1\2-\2\1/;t g' # List four consecutive consonants sed -E 's/.*([^aeiouyAEIOUY]{4}).*/c: \1/;t g' # List length of words longer than 12 characters awk '{if (length($1) &amp;gt; 12) print "l:", length($1); else print ""}' }} | # Paste the four streams side-by-side paste | # List only words satisfying one or more properties fgrep :&lt;/quote&gt;&lt;p&gt;Creates a report for a fixed-size web log file read from the standard input. Demonstrates the combined use of multipipe blocks, writeval and readval to store and retrieve values, and functions in the scatter block. Used to measure throughput increase achieved through parallelism.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Output the top X elements of the input by the number of their occurrences # X is the first argument toplist() { uniq -c | sort -rn | head -$1 echo } # Output the argument as a section header header() { echo echo "$1" echo "$1" | sed 's/./-/g' } # Consistent sorting export LC_ALL=C export -f toplist export -f header if [ -z "${DGSH_DRAW_EXIT}" ] then cat &amp;lt;&amp;lt;EOF WWW server statistics ===================== Summary ------- EOF fi tee | {{ # Number of accesses echo -n 'Number of accesses: ' dgsh-readval -l -s nAccess # Number of transferred bytes awk '{s += $NF} END {print s}' | tee | {{ echo -n 'Number of Gbytes transferred: ' awk '{print $1 / 1024 / 1024 / 1024}' dgsh-writeval -s nXBytes }} echo -n 'Number of hosts: ' dgsh-readval -l -q -s nHosts echo -n 'Number of domains: ' dgsh-readval -l -q -s nDomains echo -n 'Number of top level domains: ' dgsh-readval -l -q -s nTLDs echo -n 'Number of different pages: ' dgsh-readval -l -q -s nUniqPages echo -n 'Accesses per day: ' dgsh-readval -l -q -s nDayAccess echo -n 'MBytes per day: ' dgsh-readval -l -q -s nDayMB # Number of log file bytes echo -n 'MBytes log file size: ' wc -c | awk '{print $1 / 1024 / 1024}' # Host names awk '{print $1}' | tee | {{ # Number of accesses wc -l | dgsh-writeval -s nAccess # Sorted hosts sort | tee | {{ # Unique hosts uniq | tee | {{ # Number of hosts wc -l | dgsh-writeval -s nHosts # Number of TLDs awk -F. '$NF !~ /[0-9]/ {print $NF}' | sort -u | wc -l | dgsh-writeval -s nTLDs }} # Top 10 hosts {{ call 'header "Top 10 Hosts"' call 'toplist 10' }} }} # Top 20 TLDs {{ call 'header "Top 20 Level Domain Accesses"' awk -F. '$NF !~ /^[0-9]/ {print $NF}' | sort | call 'toplist 20' }} # Domains awk -F. 'BEGIN {OFS = "."} $NF !~ /^[0-9]/ {$1 = ""; print}' | sort | tee | {{ # Number of domains uniq | wc -l | dgsh-writeval -s nDomains # Top 10 domains {{ call 'header "Top 10 Domains"' call 'toplist 10' }} }} }} # Hosts by volume {{ call 'header "Top 10 Hosts by Transfer"' awk ' {bytes[$1] += $NF} END {for (h in bytes) print bytes[h], h}' | sort -rn | head -10 }} # Sorted page name requests awk '{print $7}' | sort | tee | {{ # Top 20 area requests (input is already sorted) {{ call 'header "Top 20 Area Requests"' awk -F/ '{print $2}' | call 'toplist 20' }} # Number of different pages uniq | wc -l | dgsh-writeval -s nUniqPages # Top 20 requests {{ call 'header "Top 20 Requests"' call 'toplist 20' }} }} # Access time: dd/mmm/yyyy:hh:mm:ss awk '{print substr($4, 2)}' | tee | {{ # Just dates awk -F: '{print $1}' | tee | {{ # Number of days uniq | wc -l | tee | {{ awk ' BEGIN { "dgsh-readval -l -x -s nAccess" | getline NACCESS;} {print NACCESS / $1}' | dgsh-writeval -s nDayAccess awk ' BEGIN { "dgsh-readval -l -x -q -s nXBytes" | getline NXBYTES;} {print NXBYTES / $1 / 1024 / 1024}' | dgsh-writeval -s nDayMB }} {{ call 'header "Accesses by Date"' uniq -c }} # Accesses by day of week {{ call 'header "Accesses by Day of Week"' sed 's|/|-|g' | call '(date -f - +%a 2&amp;gt;/dev/null || gdate -f - +%a)' | sort | uniq -c | sort -rn }} }} # Hour {{ call 'header "Accesses by Local Hour"' awk -F: '{print $2}' | sort | uniq -c }} }} dgsh-readval -q -s nAccess }} | cat&lt;/quote&gt;&lt;p&gt;Read text from the standard input and create files containing word, character, digram, and trigram frequencies.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Consistent sorting across machines export LC_ALL=C # Convert input into a ranked frequency list ranked_frequency() { awk '{count[$1]++} END {for (i in count) print count[i], i}' | # We want the standard sort here sort -rn } # Convert standard input to a ranked frequency list of specified n-grams ngram() { local N=$1 perl -ne 'for ($i = 0; $i &amp;lt; length($_) - '$N'; $i++) { print substr($_, $i, '$N'), "\n"; }' | ranked_frequency } export -f ranked_frequency export -f ngram tee | {{ # Split input one word per line tr -cs a-zA-Z \\n | tee | {{ # Digram frequency call 'ngram 2 &amp;gt;digram.txt' # Trigram frequency call 'ngram 3 &amp;gt;trigram.txt' # Word frequency call 'ranked_frequency &amp;gt;words.txt' }} # Store number of characters to use in awk below wc -c | dgsh-writeval -s nchars # Character frequency sed 's/./&amp;amp;\ /g' | # Print absolute call 'ranked_frequency' | awk 'BEGIN { "dgsh-readval -l -x -q -s nchars" | getline NCHARS OFMT = "%.2g%%"} {print $1, $2, $1 / NCHARS * 100}' &amp;gt; character.txt }}&lt;/quote&gt;&lt;p&gt;Given as an argument a directory containing object files, show which symbols are declared with global visibility, but should have been declared with file-local (static) visibility instead. Demonstrates the use of dgsh-capable comm (1) to combine data from two sources.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Find object files find "$1" -name \*.o | # Print defined symbols xargs nm | tee | {{ # List all defined (exported) symbols awk 'NF == 3 &amp;amp;&amp;amp; $2 ~ /[A-Z]/ {print $3}' | sort # List all undefined (imported) symbols awk '$1 == "U" {print $2}' | sort }} | # Print exports that are not imported comm -23&lt;/quote&gt;&lt;p&gt;Given two directory hierarchies A and B passed as input arguments (where these represent a project at different parts of its lifetime) copy the files of hierarchy A to a new directory, passed as a third argument, corresponding to the structure of directories in B. Demonstrates the use of join to process results from two inputs and the use of gather to order asynchronously produced results.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh if [ -z "${DGSH_DRAW_EXIT}" -a \( ! -d "$1" -o ! -d "$2" -o -z "$3" \) ] then echo "Usage: $0 dir-1 dir-2 new-dir-name" 1&amp;gt;&amp;amp;2 exit 1 fi NEWDIR="$3" export LC_ALL=C line_signatures() { find $1 -type f -name '*.[chly]' -print | # Split path name into directory and file sed 's|\(.*\)/\([^/]*\)|\1 \2|' | while read dir file do # Print "directory filename content" of lines with # at least one alphabetic character # The fields are separated by and sed -n "/[a-z]/s|^|$dir$file|p" "$dir/$file" done | # Error: multi-character tab '\001\001' sort -T `pwd` -t -k 2 } export -f line_signatures {{ # Generate the signatures for the two hierarchies call 'line_signatures "$1"' -- "$1" call 'line_signatures "$1"' -- "$2" }} | # Join signatures on file name and content join -t -1 2 -2 2 | # Print filename dir1 dir2 sed 's///g' | awk -F 'BEGIN{OFS=" "}{print $1, $3, $4}' | # Unique occurrences sort -u | tee | {{ # Commands to copy awk '{print "mkdir -p '$NEWDIR'/" $3 ""}' | sort -u awk '{print "cp " $2 "/" $1 " '$NEWDIR'/" $3 "/" $1 ""}' }} | # Order: first make directories, then copy files # TODO: dgsh-tee does not pass along first incoming stream cat | sh&lt;/quote&gt;&lt;p&gt;Process the Git history, and create two PNG diagrams depicting committer activity over time. The most active committers appear at the center vertical of the diagram. Demonstrates image processing, mixining of synchronous and asynchronous processing in a scatter block, and the use of an dgsh-compliant join command.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Commit history in the form of ascending Unix timestamps, emails git log --pretty=tformat:'%at %ae' | # Filter records according to timestamp: keep (100000, now) seconds awk 'NF == 2&amp;amp; $1 &amp;gt; 100000&amp;amp; $1 &amp;lt; '`date +%s` | sort -n | tee | {{ {{ # Calculate number of committers awk '{print $2}' | sort -u | wc -l | tee | {{ dgsh-writeval -s committers1 dgsh-writeval -s committers2 dgsh-writeval -s committers3 }} # Calculate last commit timestamp in seconds tail -1 | awk '{print $1}' # Calculate first commit timestamp in seconds head -1 | awk '{print $1}' }} | # Gather last and first commit timestamp cat | # Make one space-delimeted record tr '\n' ' ' | # Compute the difference in days awk '{print int(($1 - $2) / 60 / 60 / 24)}' | # Store number of days dgsh-writeval -s days sort -k2 # &amp;lt;timestamp, email&amp;gt; # Place committers left/right of the median # according to the number of their commits awk '{print $2}' | sort | uniq -c | sort -n | awk ' BEGIN { "dgsh-readval -l -x -q -s committers1" | getline NCOMMITTERS l = 0; r = NCOMMITTERS;} {print NR % 2 ? l++ : --r, $2}' | sort -k2 # &amp;lt;left/right, email&amp;gt; }} | # Join committer positions with commit time stamps # based on committer email join -j 2 | # &amp;lt;email, timestamp, left/right&amp;gt; # Order by timestamp sort -k 2n | tee | {{ # Create portable bitmap echo 'P1' {{ dgsh-readval -l -q -s committers2 dgsh-readval -l -q -s days }} | cat | tr '\n' ' ' | awk '{print $1, $2}' perl -na -e ' BEGIN { open(my $ncf, "-|", "dgsh-readval -l -x -q -s committers3"); $ncommitters = &amp;lt;$ncf&amp;gt;; @empty[$ncommitters - 1] = 0; @committers = @empty; } sub out { print join("", map($_ ? "1" : "0", @committers)), "\n"; } $day = int($F[1] / 60 / 60 / 24); $pday = $day if (!defined($pday)); while ($day != $pday) { out(); @committers = @empty; $pday++; } $committers[$F[2]] = 1; END { out(); } ' }} | cat | # Enlarge points into discs through morphological convolution pgmmorphconv -erode &amp;lt;( cat &amp;lt;&amp;lt;EOF P1 7 7 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 EOF ) | tee | {{ # Full-scale image pnmtopng &amp;gt;large.png # A smaller image pamscale -width 640 | pnmtopng &amp;gt;small.png }}&lt;/quote&gt;&lt;p&gt;Count number of times each word appears in the specified input file(s) Demonstrates parallel execution mirroring the Hadoop WordCount example via the dgsh-parallel command. In contrast to GNU parallel, the block generated by dgsh-parallel has N input and output streams, which can be combined by any dgsh-compatible tool, such as dgsh-merge-sum or sort -m.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Number of processes N=8 # Collation order for sorting export LC_ALL=C # Scatter input dgsh-tee -s | # Emulate Java's default StringTokenizer, sort, count dgsh-parallel -n $N "tr -s ' \t\n\r\f' '\n' | sort -S 512M | uniq -c" | # Merge sorted counts by providing N input channels dgsh-merge-sum $(for i in $(seq $N) ; do printf '&amp;lt;| ' ; done)&lt;/quote&gt;&lt;p&gt;Given the specification of two publication venues, read a compressed DBLP computer science bibliography from the standard input (e.g. piped from curl -s http://dblp.uni-trier.de/xml/dblp.xml.gz or from a locally cached copy) and output the number of papers published in each of the two venues as well as the number of authors who have published only in the first venue, the number who have published only in the second one, and authors who have published in both. The venues are specified through the script's first two command-line arguments as a DBLP key prefix, e.g. journals/acta/, conf/icse/, journals/software/, conf/iwpc/, or conf/msr/. Demonstrates the use of dgsh-wrap -e to have sed(1) create two output streams and the use of tee to copy a pair of streams into four ones.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Extract and sort author names sorted_authors() { sed -n 's/&amp;lt;author&amp;gt;\([^&amp;lt;]*\)&amp;lt;\/author&amp;gt;/\1/p' | sort } # Escape a string to make it a valid sed(1) pattern escape() { echo "$1" | sed 's/\([/\\]\)/\\\1/g' } export -f sorted_authors if [ ! "$2" -a ! "$DGSH_DOT_DRAW"] ; then echo "Usage: $0 key1 key2" 1&amp;gt;&amp;amp;2 echo "Example: $0 conf/icse/ journals/software/" 1&amp;gt;&amp;amp;2 exit 1 fi gzip -dc | # Output the two venue authors as two output streams dgsh-wrap -e sed -n " /^&amp;lt;.*key=\"$(escape $1)/,/&amp;lt;title&amp;gt;/ w &amp;gt;| /^&amp;lt;.*key=\"$(escape $2)/,/&amp;lt;title&amp;gt;/ w &amp;gt;|" | # 2 streams in 4 streams out: venue1, venue2, venue1, venue2 tee | {{ {{ echo -n "$1 papers: " grep -c '^&amp;lt;.* mdate=.* key=' echo -n "$2 papers: " grep -c '^&amp;lt;.* mdate=.* key=' }} {{ call sorted_authors call sorted_authors }} | comm | {{ echo -n "Authors only in $1: " wc -l echo -n "Authors only in $2: " wc -l echo -n 'Authors common in both venues: ' wc -l }} }} | cat&lt;/quote&gt;&lt;p&gt;Create two graphs: 1) a broadened pulse and the real part of its 2D Fourier transform, and 2) a simulated air wave and the amplitude of its 2D Fourier transform. Demonstrates using the tools of the Madagascar shared research environment for computational data analysis in geophysics and related fields. Also demonstrates the use of two scatter blocks in the same script, and the used of named streams.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh mkdir -p Fig # The SConstruct SideBySideIso "Result" method side_by_side_iso() { vppen size=r vpstyle=n gridnum=2,1 /dev/stdin $* } export -f side_by_side_iso # A broadened pulse and the real part of its 2D Fourier transform sfspike n1=64 n2=64 d1=1 d2=1 nsp=2 k1=16,17 k2=5,5 mag=16,16 \ label1='time' label2='space' unit1= unit2= | sfsmooth rect2=2 | sfsmooth rect2=2 | tee | {{ sfgrey pclip=100 wanttitle=n sffft1 | sffft3 axis=2 pad=1 | sfreal | tee | {{ sfwindow f1=1 | sfreverse which=3 cat }} | sfcat axis=1 "&amp;lt;|" | sfgrey pclip=100 wanttitle=n label1="1/time" label2="1/space" }} | call_with_stdin side_by_side_iso '&amp;lt;|' yscale=1.25 &amp;gt;Fig/ft2dofpulse.vpl # A simulated air wave and the amplitude of its 2D Fourier transform sfspike n1=64 d1=1 o1=32 nsp=4 k1=1,2,3,4 mag=1,3,3,1 \ label1='time' unit1= | sfspray n=32 d=1 o=0 | sfput label2=space | sflmostretch delay=0 v0=-1 | tee | {{ sfwindow f2=1 | sfreverse which=2 cat }} | sfcat axis=2 "&amp;lt;|" | tee | {{ sfgrey pclip=100 wanttitle=n sffft1 | sffft3 sign=1 | tee | {{ sfreal sfimag }} | dgsh-wrap -e sfmath nostdin=y re="&amp;lt;|" im="&amp;lt;|" \ output="sqrt(re*re+im*im)" | tee | {{ sfwindow f1=1 | sfreverse which=3 cat }} | sfcat axis=1 "&amp;lt;|" | sfgrey pclip=100 wanttitle=n label1="1/time" label2="1/space" }} | call_with_stdin side_by_side_iso '&amp;lt;|' yscale=1.25 &amp;gt;Fig/airwave.vpl wait&lt;/quote&gt;&lt;p&gt;Nuclear magnetic resonance in-phase/anti-phase channel conversion and processing in heteronuclear single quantum coherence spectroscopy. Demonstrate processing of NMR data using the NMRPipe family of programs.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # The conversion is configured for the following file: # http://www.bmrb.wisc.edu/ftp/pub/bmrb/timedomain/bmr6443/timedomain_data/c13-hsqc/june11-se-6426-CA.fid/fid var2pipe -in $1 \ -xN 1280 -yN 256 \ -xT 640 -yT 128 \ -xMODE Complex -yMODE Complex \ -xSW 8000 -ySW 6000 \ -xOBS 599.4489584 -yOBS 60.7485301 \ -xCAR 4.73 -yCAR 118.000 \ -xLAB 1H -yLAB 15N \ -ndim 2 -aq2D States \ -verb | tee | {{ # IP/AP channel conversion # See http://tech.groups.yahoo.com/group/nmrpipe/message/389 nmrPipe | nmrPipe -fn SOL | nmrPipe -fn SP -off 0.5 -end 0.98 -pow 2 -c 0.5 | nmrPipe -fn ZF -auto | nmrPipe -fn FT | nmrPipe -fn PS -p0 177 -p1 0.0 -di | nmrPipe -fn EXT -left -sw -verb | nmrPipe -fn TP | nmrPipe -fn COADD -cList 1 0 -time | nmrPipe -fn SP -off 0.5 -end 0.98 -pow 1 -c 0.5 | nmrPipe -fn ZF -auto | nmrPipe -fn FT | nmrPipe -fn PS -p0 0 -p1 0 -di | nmrPipe -fn TP | nmrPipe -fn POLY -auto -verb &amp;gt;A nmrPipe | nmrPipe -fn SOL | nmrPipe -fn SP -off 0.5 -end 0.98 -pow 2 -c 0.5 | nmrPipe -fn ZF -auto | nmrPipe -fn FT | nmrPipe -fn PS -p0 177 -p1 0.0 -di | nmrPipe -fn EXT -left -sw -verb | nmrPipe -fn TP | nmrPipe -fn COADD -cList 0 1 -time | nmrPipe -fn SP -off 0.5 -end 0.98 -pow 1 -c 0.5 | nmrPipe -fn ZF -auto | nmrPipe -fn FT | nmrPipe -fn PS -p0 -90 -p1 0 -di | nmrPipe -fn TP | nmrPipe -fn POLY -auto -verb &amp;gt;B }} # We use temporary files rather than streams, because # addNMR mmaps its input files. The diagram displayed in the # example shows the notional data flow. if [ -z "${DGSH_DRAW_EXIT}" ] then addNMR -in1 A -in2 B -out A+B.dgsh.ft2 -c1 1.0 -c2 1.25 -add addNMR -in1 A -in2 B -out A-B.dgsh.ft2 -c1 1.0 -c2 1.25 -sub fi&lt;/quote&gt;&lt;p&gt;Calculate the iterative FFT for n = 8 in parallel. Demonstrates combined use of permute and multipipe blocks.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh dgsh-fft-input $1 | perm 1,5,3,7,2,6,4,8 | {{ {{ dgsh-w 1 0 dgsh-w 1 0 }} | perm 1,3,2,4 | {{ dgsh-w 2 0 dgsh-w 2 1 }} {{ dgsh-w 1 0 dgsh-w 1 0 }} | perm 1,3,2,4 | {{ dgsh-w 2 0 dgsh-w 2 1 }} }} | perm 1,5,3,7,2,6,4,8 | {{ dgsh-w 3 0 dgsh-w 3 1 dgsh-w 3 2 dgsh-w 3 3 }} | perm 1,5,2,6,3,7,4,8 | cat&lt;/quote&gt;&lt;p&gt;Reorder columns in a CSV document. Demonstrates the combined use of tee, cut, and paste.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh tee | {{ cut -d , -f 5-6 - cut -d , -f 2-4 - }} | paste -d ,&lt;/quote&gt;&lt;p&gt; Windows-like DIR command for the current directory. Nothing that couldn't be done with &lt;code&gt;ls -l | awk&lt;/code&gt;.
Demonstrates use of wrapped commands with no input (df, echo).
&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh ls -n | tee | {{ # Reorder fields in DIR-like way awk '!/^total/ {print $6, $7, $8, $1, sprintf("%8d", $5), $9}' # Count number of files wc -l | tr -d \\n # Print label for number of files echo -n ' File(s) ' # Tally number of bytes awk '{s += $5} END {printf("%d bytes\n", s)}' # Count number of directories grep -c '^d' | tr -d \\n # Print label for number of dirs and calculate free bytes df -h . | awk '!/Use%/{print " Dir(s) " $4 " bytes free"}' }} | cat&lt;/quote&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45425298</guid><pubDate>Tue, 30 Sep 2025 13:39:22 +0000</pubDate></item><item><title>Deml: Directed Acyclic Graph Elevation Markup Language</title><link>https://github.com/Mcmartelle/deml</link><description>&lt;doc fingerprint="24003ae74061c391"&gt;
  &lt;main&gt;
    &lt;p&gt;Languages designed to represent all types of graph data structures, such as Graphviz's DOT Language and Mermaid JS's flowchart syntax, don't take advantage of the properties specific to DAGs (Directed Acyclic Graphs).&lt;/p&gt;
    &lt;p&gt;DAGs act like rivers. Water doesn't flow upstream (tides and floods being exceptions). Sections of a river at the same elevation can't be the inputs or outputs of each other, like the nodes C, D, and E in the image below. Their input is B. C outputs to F, while D and E output to G.&lt;/p&gt;
    &lt;p&gt;DEML's goal is to use this ordering as part of the file syntax to make it easier for humans to parse. In DEML we represent an elevation marker with &lt;code&gt;----&lt;/code&gt; on a new line. The order of elevation clusters is significant, but the order of nodes between two &lt;code&gt;----&lt;/code&gt; elevation markers is not significant.&lt;/p&gt;
    &lt;code&gt;UpRiver &amp;gt; A
----
A &amp;gt; B
----
B &amp;gt; C | D | E
----
C
D
E
----
F &amp;lt; C
G &amp;lt; D | E &amp;gt; DownRiver
----
DownRiver &amp;lt; F&lt;/code&gt;
    &lt;p&gt;Nodes are defined by the first word on a line. The defined node can point to its outputs with &lt;code&gt;&amp;gt;&lt;/code&gt; and to its inputs with &lt;code&gt;&amp;lt;&lt;/code&gt;. Inputs and outputs are separated by &lt;code&gt;|&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Dagrs is a library for running multiple tasks with dependencies defined in a DAG. In DEML, shell commands can be assigned to a node with &lt;code&gt;=&lt;/code&gt;. DEML files can be run via dag-rs with the comand &lt;code&gt;deml run -i &amp;lt;filepath&amp;gt;&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To compare the difference in readability, here is the Dagrs YAML example in both YAML and DEML&lt;/p&gt;
    &lt;code&gt;dagrs:
  a:
    name: "Task 1"
    after: [ b, c ]
    cmd: echo a
  b:
    name: "Task 2"
    after: [ c, f, g ]
    cmd: echo b
  c:
    name: "Task 3"
    after: [ e, g ]
    cmd: echo c
  d:
    name: "Task 4"
    after: [ c, e ]
    cmd: echo d
  e:
    name: "Task 5"
    after: [ h ]
    cmd: echo e
  f:
    name: "Task 6"
    after: [ g ]
    cmd: python3 ./tests/config/test.py
  g:
    name: "Task 7"
    after: [ h ]
    cmd: node ./tests/config/test.js
  h:
    name: "Task 8"
    cmd: echo h&lt;/code&gt;
    &lt;code&gt;H &amp;gt; E | G = echo h
----
G = node ./tests/config/test.js
E = echo e
----
F &amp;lt; G = python3 ./tests/config/test.py
C &amp;lt; E | G = echo c
----
B &amp;lt; C | F | G = echo b
D &amp;lt; C | E = echo d
----
A &amp;lt; B | C = echo a&lt;/code&gt;
    &lt;p&gt;To convert DEML files to Mermaid Diagram files (.mmd) use the command &lt;code&gt;deml mermaid -i &amp;lt;inputfile&amp;gt; -o &amp;lt;outputfile&amp;gt;&lt;/code&gt;. The mermaid file can be used to generate an image at mermaid.live&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Put my idea for an elevation based DAG representation into the wild&lt;/item&gt;
      &lt;item&gt;Run DAGs with dag-rs&lt;/item&gt;
      &lt;item&gt;Convert DEML files to Mermaid Diagram files&lt;/item&gt;
      &lt;item&gt;Syntax highlighting tree-sitter-deml&lt;/item&gt;
      &lt;item&gt;Add a syntax to label edges&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I was thinking about how it's annoying in languages like C when function declaration order matters. Then I wondered if there could be a case when it would be a nice feature for declaration order to matter and I thought of DAGs.&lt;/p&gt;
    &lt;p&gt;Licensed under either of&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apache License, Version 2.0 (LICENSE-APACHE or http://www.apache.org/licenses/LICENSE-2.0)&lt;/item&gt;
      &lt;item&gt;MIT license (LICENSE-MIT or http://opensource.org/licenses/MIT)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;at your option.&lt;/p&gt;
    &lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45425714</guid><pubDate>Tue, 30 Sep 2025 14:12:37 +0000</pubDate></item><item><title>Kagi News</title><link>https://blog.kagi.com/kagi-news</link><description>&lt;doc fingerprint="b500b3a787eb238f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing Kagi News&lt;/head&gt;
    &lt;p&gt;A comprehensive daily press review with global news. Fully private, with sources openly curated by our community.&lt;/p&gt;
    &lt;p&gt;News is broken. We all know it, but we’ve somehow accepted it as inevitable. The endless notifications. The clickbait headlines designed to trigger rather than inform, driven by relentless ad monetization. The exhausting cycle of checking multiple apps throughout the day, only to feel more anxious and less informed than when we started. This isn’t what news was supposed to be. We can do better, and create what news should have been all along: pure, essential information that respects your intelligence and time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our approach: Signal over noise&lt;/head&gt;
    &lt;p&gt;Kagi News operates on a simple principle: understanding the world requires hearing from the world. Every day, our system reads thousands of community curated RSS feeds from publications across different viewpoints and perspectives. We then distill this massive information into one comprehensive daily briefing, while clearly citing sources.&lt;/p&gt;
    &lt;p&gt;We strive for diversity and transparency of resources and welcome your contributions to widen perspectives. This multi-source approach helps reveal the full picture beyond any single viewpoint.&lt;/p&gt;
    &lt;head rend="h2"&gt;Design principles that put readers first&lt;/head&gt;
    &lt;p&gt;One daily update: We publish once per day around noon UTC, creating a natural endpoint to news consumption. This is a deliberate design choice that turns news from an endless habit into a contained ritual.&lt;/p&gt;
    &lt;p&gt;Five-minute complete understanding: Our briefings cover everything important in just five minutes. No endless scrolling. No attention hijacking. You read, understand, and move on with your day.&lt;/p&gt;
    &lt;p&gt;Diversity over echo chambers: Rather than personalizing feeds to match existing preferences, we expose readers to the full spectrum of global perspectives. This approach breaks down information silos instead of reinforcing them.&lt;/p&gt;
    &lt;p&gt;Privacy by design: Your reading habits belong to you. We don’t track, profile, or monetize your attention. You remain the customer and not the product.&lt;/p&gt;
    &lt;p&gt;Community-driven sources: Our news sources are open source and community-curated through our public GitHub repository. Anyone can propose additions, flag problems, or suggest improvements.&lt;/p&gt;
    &lt;p&gt;Customizable: In your settings, you can select and reorder categories to match your interests and priorities. You can also adjust the number of stories shown, as well as dragging to re-order various sections, so that your briefing is focused on the depth and topics that matter most to you.&lt;/p&gt;
    &lt;p&gt;News in your language: You can choose your preferred interface and content language. News stories are generated in their original source language, and then translated using Kagi Translate. The default mode shows regional stories in their original language without translation, and all other ones in your browser’s language.&lt;/p&gt;
    &lt;head rend="h2"&gt;Technical implementation that respects publishers&lt;/head&gt;
    &lt;p&gt;We don’t scrape content from websites. Instead, we use publicly available RSS feeds that publishers choose to provide. Publishers decide what content appears in their feeds; some include full articles, others only titles or summaries. We respect those choices completely. We’re working within the ecosystem publishers have created rather than circumventing their intentions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ready to experience news differently?&lt;/head&gt;
    &lt;p&gt;If you’re tired of news that makes you feel worse about the world while teaching you less about it, we invite you to try a different approach with Kagi News, so download it today:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45426490</guid><pubDate>Tue, 30 Sep 2025 15:09:00 +0000</pubDate></item><item><title>Designing agentic loops</title><link>https://simonwillison.net/2025/Sep/30/designing-agentic-loops/</link><description>&lt;doc fingerprint="b740585d4af512bf"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Designing agentic loops&lt;/head&gt;
    &lt;p&gt;30th September 2025&lt;/p&gt;
    &lt;p&gt;Coding agents like Anthropic’s Claude Code and OpenAI’s Codex CLI represent a genuine step change in how useful LLMs can be for producing working code. These agents can now directly exercise the code they are writing, correct errors, dig through existing implementation details, and even run experiments to find effective code solutions to problems.&lt;/p&gt;
    &lt;p&gt;As is so often the case with modern AI, there is a great deal of depth involved in unlocking the full potential of these new tools.&lt;/p&gt;
    &lt;p&gt;A critical new skill to develop is designing agentic loops.&lt;/p&gt;
    &lt;p&gt;One way to think about coding agents is that they are brute force tools for finding solutions to coding problems. If you can reduce your problem to a clear goal and a set of tools that can iterate towards that goal a coding agent can often brute force its way to an effective solution.&lt;/p&gt;
    &lt;p&gt;My preferred definition of an LLM agent is something that runs tools in a loop to achieve a goal. The art of using them well is to carefully design the tools and loop for them to use.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The joy of YOLO mode&lt;/item&gt;
      &lt;item&gt;Picking the right tools for the loop&lt;/item&gt;
      &lt;item&gt;Issuing tightly scoped credentials&lt;/item&gt;
      &lt;item&gt;When to design an agentic loop&lt;/item&gt;
      &lt;item&gt;This is still a very fresh area&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The joy of YOLO mode&lt;/head&gt;
    &lt;p&gt;Agents are inherently dangerous—they can make poor decisions or fall victim to malicious prompt injection attacks, either of which can result in harmful results from tool calls. Since the most powerful coding agent tool is “run this command in the shell” a rogue agent can do anything that you could do by running a command yourself.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;An AI agent is an LLM wrecking its environment in a loop.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Coding agents like Claude Code counter this by defaulting to asking you for approval of almost every command that they run.&lt;/p&gt;
    &lt;p&gt;This is kind of tedious, but more importantly, it dramatically reduces their effectiveness at solving problems through brute force.&lt;/p&gt;
    &lt;p&gt;Each of these tools provides its own version of what I like to call YOLO mode, where everything gets approved by default.&lt;/p&gt;
    &lt;p&gt;This is so dangerous, but it’s also key to getting the most productive results!&lt;/p&gt;
    &lt;p&gt;Here are three key risks to consider from unattended YOLO mode.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Bad shell commands deleting or mangling things you care about.&lt;/item&gt;
      &lt;item&gt;Exfiltration attacks where something steals files or data visible to the agent—source code or secrets held in environment variables are particularly vulnerable here.&lt;/item&gt;
      &lt;item&gt;Attacks that use your machine as a proxy to attack another target—for DDoS or to disguise the source of other hacking attacks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you want to run YOLO mode anyway, you have a few options:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Run your agent in a secure sandbox that restricts the files and secrets it can access and the network connections it can make.&lt;/item&gt;
      &lt;item&gt;Use someone else’s computer. That way if your agent goes rogue, there’s only so much damage they can do, including wasting someone else’s CPU cycles.&lt;/item&gt;
      &lt;item&gt;Take a risk! Try to avoid exposing it to potential sources of malicious instructions and hope you catch any mistakes before they cause any damage.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Most people choose option 3.&lt;/p&gt;
    &lt;p&gt;Despite the existence of container escapes I think option 1 using Docker or the new Apple container tool is a reasonable risk to accept for most people.&lt;/p&gt;
    &lt;p&gt;Option 2 is my favorite. I like to use GitHub Codespaces for this—it provides a full container environment on-demand that’s accessible through your browser and has a generous free tier too. If anything goes wrong it’s a Microsoft Azure machine somewhere that’s burning CPU and the worst that can happen is code you checked out into the environment might be exfiltrated by an attacker, or bad code might be pushed to the attached GitHub repository.&lt;/p&gt;
    &lt;p&gt;There are plenty of other agent-like tools that run code on other people’s computers. Code Interpreter mode in both ChatGPT and Claude can go a surprisingly long way here. I’ve also had a lot of success (ab)using OpenAI’s Codex Cloud.&lt;/p&gt;
    &lt;p&gt;Coding agents themselves implement various levels of sandboxing, but so far I’ve not seen convincing enough documentation of these to trust them.&lt;/p&gt;
    &lt;p&gt;Update: It turns out Anthropic have their own documentation on Safe YOLO mode for Claude Code which says:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Letting Claude run arbitrary commands is risky and can result in data loss, system corruption, or even data exfiltration (e.g., via prompt injection attacks). To minimize these risks, use&lt;/p&gt;&lt;code&gt;--dangerously-skip-permissions&lt;/code&gt;in a container without internet access. You can follow this reference implementation using Docker Dev Containers.&lt;/quote&gt;
    &lt;head rend="h4"&gt;Picking the right tools for the loop&lt;/head&gt;
    &lt;p&gt;Now that we’ve found a safe (enough) way to run in YOLO mode, the next step is to decide which tools we need to make available to the coding agent.&lt;/p&gt;
    &lt;p&gt;You can bring MCP into the mix at this point, but I find it’s usually more productive to think in terms of shell commands instead. Coding agents are really good at running shell commands!&lt;/p&gt;
    &lt;p&gt;If your environment allows them the necessary network access, they can also pull down additional packages from NPM and PyPI and similar. Ensuring your agent runs in an environment where random package installs don’t break things on your main computer is an important consideration as well!&lt;/p&gt;
    &lt;p&gt;Rather than leaning on MCP, I like to create an AGENTS.md (or equivalent) file with details of packages I think they may need to use.&lt;/p&gt;
    &lt;p&gt;For a project that involved taking screenshots of various websites I installed my own shot-scraper CLI tool and dropped the following in &lt;code&gt;AGENTS.md&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;To take a screenshot, run:

shot-scraper http://www.example.com/ -w 800 -o example.jpg
&lt;/code&gt;
    &lt;p&gt;Just that one example is enough for the agent to guess how to swap out the URL and filename for other screenshots.&lt;/p&gt;
    &lt;p&gt;Good LLMs already know how to use a bewildering array of existing tools. If you say "use playwright python" or "use ffmpeg" most models will use those effectively—and since they’re running in a loop they can usually recover from mistakes they make at first and figure out the right incantations without extra guidance.&lt;/p&gt;
    &lt;head rend="h4"&gt;Issuing tightly scoped credentials&lt;/head&gt;
    &lt;p&gt;In addition to exposing the right commands, we also need to consider what credentials we should expose to those commands.&lt;/p&gt;
    &lt;p&gt;Ideally we wouldn’t need any credentials at all—plenty of work can be done without signing into anything or providing an API key—but certain problems will require authenticated access.&lt;/p&gt;
    &lt;p&gt;This is a deep topic in itself, but I have two key recommendations here:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Try to provide credentials to test or staging environments where any damage can be well contained.&lt;/item&gt;
      &lt;item&gt;If a credential can spend money, set a tight budget limit.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I’ll use an example to illustrate. A while ago I was investigating slow cold start times for a scale-to-zero application I was running on Fly.io.&lt;/p&gt;
    &lt;p&gt;I realized I could work a lot faster if I gave Claude Code the ability to directly edit Dockerfiles, deploy them to a Fly account and measure how long they took to launch.&lt;/p&gt;
    &lt;p&gt;Fly allows you to create organizations, and you can set a budget limit for those organizations and issue a Fly API key that can only create or modify apps within that organization...&lt;/p&gt;
    &lt;p&gt;So I created a dedicated organization for just this one investigation, set a $5 budget, issued an API key and set Claude Code loose on it!&lt;/p&gt;
    &lt;p&gt;In that particular case the results weren’t useful enough to describe in more detail, but this was the project where I first realized that “designing an agentic loop” was an important skill to develop.&lt;/p&gt;
    &lt;head rend="h4"&gt;When to design an agentic loop&lt;/head&gt;
    &lt;p&gt;Not every problem responds well to this pattern of working. The thing to look out for here are problems with clear success criteria where finding a good solution is likely to involve (potentially slightly tedious) trial and error.&lt;/p&gt;
    &lt;p&gt;Any time you find yourself thinking “ugh, I’m going to have to try a lot of variations here” is a strong signal that an agentic loop might be worth trying!&lt;/p&gt;
    &lt;p&gt;A few examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Debugging: a test is failing and you need to investigate the root cause. Coding agents that can already run your tests can likely do this without any extra setup.&lt;/item&gt;
      &lt;item&gt;Performance optimization: this SQL query is too slow, would adding an index help? Have your agent benchmark the query and then add and drop indexes (in an isolated development environment!) to measure their impact.&lt;/item&gt;
      &lt;item&gt;Upgrading dependencies: you’ve fallen behind on a bunch of dependency upgrades? If your test suite is solid an agentic loop can upgrade them all for you and make any minor updates needed to reflect breaking changes. Make sure a copy of the relevant release notes is available, or that the agent knows where to find them itself.&lt;/item&gt;
      &lt;item&gt;Optimizing container sizes: Docker container feeling uncomfortably large? Have your agent try different base images and iterate on the Dockerfile to try to shrink it, while keeping the tests passing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A common theme in all of these is automated tests. The value you can get from coding agents and other LLM coding tools is massively amplified by a good, cleanly passing test suite. Thankfully LLMs are great for accelerating the process of putting one of those together, if you don’t have one yet.&lt;/p&gt;
    &lt;head rend="h4"&gt;This is still a very fresh area&lt;/head&gt;
    &lt;p&gt;Designing agentic loops is a very new skill—Claude Code was first released in just February 2025!&lt;/p&gt;
    &lt;p&gt;I’m hoping that giving it a clear name can help us have productive conversations about it. There’s so much more to figure out about how to use these tools as effectively as possible.&lt;/p&gt;
    &lt;head rend="h2"&gt;More recent articles&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude Sonnet 4.5 is probably the "best coding model in the world" (at least for now) - 29th September 2025&lt;/item&gt;
      &lt;item&gt;I think "agent" may finally have a widely enough agreed upon definition to be useful jargon now - 18th September 2025&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45426680</guid><pubDate>Tue, 30 Sep 2025 15:21:23 +0000</pubDate></item><item><title>Visualizations of Random Attractors Found Using Lyapunov Exponents</title><link>https://paulbourke.net/fractals/lyapunov/</link><description>&lt;doc fingerprint="2992588605812842"&gt;
  &lt;main&gt;
    &lt;cell&gt;&lt;head rend="h1"&gt; Random Attractors&lt;lb/&gt; Found using Lyapunov Exponents &lt;/head&gt; Written by Paul Bourke&lt;lb/&gt; October 2001&lt;p&gt; Contribution by Philip Ham: attractor.basic&lt;lb/&gt; and Python implementation by Johan Bichel Lindegaard.&lt;/p&gt;&lt;p&gt; This document is "littered" with a selection of attractors found using the techniques described. &lt;/p&gt;&lt;p&gt; In order for a system to exhibit chaotic behaviour it must be non linear. Representing chaotic systems on a screen or on paper leads one to considering a two dimensional system, an equation in two variables. One possible two dimensional non-linear system, the one used here, is the quadratic map defined as follows. &lt;/p&gt; xn+1 = a0 + a1 xn + a2 xn2 + a3 xn yn + a4 yn + a5 yn2 &lt;lb/&gt; yn+1 = b0 + b1 xn + b2 xn2 + b3 xn yn + b4 yn + b5 yn2 &lt;p&gt; The standard measure for determining whether or not a system is chaotic is the Lyapunov exponent, normally represented by the lambda symbol. Consider two close points at step n, xn and xn+dxn. At the next time step they will have diverged, namely to xn+1 and xn+1+dxn+1. It is this average rate of divergence (or convergence) that the Lyapunov exponent captures. Another way to think about the Lyapunov exponent is as the rate at which information about the initial conditions is lost. &lt;/p&gt;&lt;p&gt; There are as many Lyapunov exponents as dimensions of the phase space. Considering a region (circle, sphere, hypersphere, etc) in phase space then at a later time all trajectories in this region form an n-dimensional elliptical region. The Lyapunov exponent can be calculated for each dimension. When talking about a single exponent one is normally referring to the largest, this convention will be assumed from now onwards. &lt;/p&gt;&lt;p&gt; If the Lyapunov exponent is positive then the system is chaotic and unstable. Nearby points will diverge irrespective of how close they are. Although there is no order the system is still deterministic! The magnitude of the Lyapunov exponent is a measure of the sensitivity to initial conditions, the primary characteristic of a chaotic system. &lt;/p&gt;&lt;p&gt; If the Lyapunov exponent is less than zero then the system attracts to a fixed point or stable periodic orbit. These systems are non conservative (dissipative). The absolute value of the exponent indicates the degree of stability. &lt;/p&gt;&lt;p&gt; If the Lyapunov exponent is zero then the system is neutrally stable, such systems are conservative and in a steady state mode. &lt;/p&gt;&lt;p&gt; To create the chaotic attractors shown on this page each parameter an and bn in the quadratic equation above is chosen at random between some bounds (+- 2 say). The system so specified is generated by iterating for some suitably large number of time steps (eg; 100000) steps during which time the image is created and the Lyapunov exponent computed. Note that the first few (1000) timesteps are ignored to allow the system to settle into its "natural" behaviour. If the Lyapunov exponent indicates chaos then the image is saved and the program moves on to the next random parameter set. &lt;/p&gt;&lt;p&gt; There are a number of ways the series may behave. &lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt; It may converge to a single point, called a fixed point. These can be detected by comparing the distances between successive points. For numerical reasons this is safer than relying on the Lyapunov exponent which may be infinite (logarithm of 0)&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt; It may diverge to infinity, for the range (+- 2) used here for each parameter this is the most likely event. These are also easy to detect and discard, indeed they need to be in order to avoid numerical errors.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt; It will form a periodic orbit, these are identified by their negative Lyapunov exponent. &lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt; It will exhibit chaos, filling in some region of the plane. These are the solutions that "look good" and the ones we wish to identify with the Lyapunov exponent. &lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt; It should be noted that there may be visually appealing structures that are not chaotic attractors. That is, the resulting image is different for different initial conditions and there is no single basin of attraction. It's interesting how we "see" 3 dimensional structures in these essentially 2 dimensional systems. &lt;/p&gt;&lt;p&gt; The software used to create these images is given here: gen.c. On average 98% of the random selections of (an, bn) result in an infinite series. This is so common because of the range (-2 &amp;lt;= a,b &amp;lt;= 2) for each of the parameters a and b, the number of infinite cases will reduce greatly with a smaller range. About 1% were point attractors, and about 0.5% were periodic basins of attraction. &lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt; Image courtesy of Robert McGregor, Space Coast of Florida. Launch trail perhaps 30 minutes after the shuttle launch (June 2007) dispersing from a column into a smoke ring due to some unusual air currents in the upper atmosphere. &lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt; References&lt;/p&gt;&lt;p&gt; Berge, P., Pomeau, Y., Vidal, C.,&lt;lb/&gt; Order Within Chaos, Wiley, New York, 1984. &lt;/p&gt;&lt;p&gt; Crutchfield, J., Farmer, J., Packard, N.&lt;lb/&gt; Chaos, Scientific American, 1986, 255, 46-47 &lt;/p&gt;&lt;p&gt; Das, A., Das, Pritha, Roy, A&lt;lb/&gt; Applicability of Lyapunov Exponent in EEG data analysis. Complexity International, draft manuscript. &lt;/p&gt;&lt;p&gt; Devaney, R.&lt;lb/&gt; An Introduction to Chaotic Dynamical Systems, Addison-Wesley, 1989 &lt;/p&gt;&lt;p&gt; Feigenbaum, M.,&lt;lb/&gt; Universal behaviour in Nonlinear Systems, Los Alamos Science, 1981 &lt;/p&gt;&lt;p&gt; Peitgen, H., Jurgens, H., Saupe, D&lt;lb/&gt; Lyapunov exponents and chaotic attractors in Chaos and fractals - new frontiers of science. Springer, new York, 1992. &lt;/p&gt;&lt;p&gt; Contributions by Dmytry Lavrov &lt;/p&gt;&lt;/cell&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45427059</guid><pubDate>Tue, 30 Sep 2025 15:50:14 +0000</pubDate></item><item><title>Prompt analytics for MCP servers</title><link>https://hyprmcp.com/blog/mcp-server-prompt-analytics/</link><description>&lt;doc fingerprint="b508e4cb3d937e9e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Prompt Analytics for MCP Servers&lt;/head&gt;
    &lt;p&gt;How to intercept the prompt that triggered the MCP Server tool call for MCP prompt analytics.&lt;/p&gt;
    &lt;p&gt;I am Philip, an Engineer at Hypr MCP, where we help companies connect their internal applications to LLM-based workflows with the power of MCP servers. Join our waitlist or book a demo to learn more. Every time we showcase our Hypr MCP platform, this is the most frequently asked question: How did we manage to get the prompt analytics? In this blog post, I want to show you how and why we built prompt analytics into our MCP server Gateway.&lt;/p&gt;
    &lt;head rend="h2"&gt;# Introduction&lt;/head&gt;
    &lt;p&gt;The Model Context Protocol (MCP) allows LLM clients and agents to dynamically add context to the prompt and even perform method calls. Typical use cases for dynamically adding additional context to LLM prompts can be found in the engineering domain. MCP servers like Context7 and GitMCP can provide dynamic documentation based on prompts, while MCP servers from specific software vendors like Stack Auth (https://mcp.stack-auth.com/) can directly add relevant information to the prompts if a tool description matches a promptâs problem. On the other side, MCP servers can be used to let LLMs instruct LLM clients to perform actions on third-party systems like the GitHub or HubSpot MCP server.&lt;/p&gt;
    &lt;head rend="h2"&gt;# MCP Server AnalyticsâMCP Servers Often Run in the Dark&lt;/head&gt;
    &lt;p&gt;Previously, MCP servers mostly ran on the client side with stdio being the default method of how JSON-RPC messages were sent from and to the clients. A benefit for these servers has been simplicity - MCP server developers didnât need to care about the runtime and connectivity constraints as the user needed to make sure to start the server program. With the migration to remote MCP servers, thanks to the streamable HTTP transport method for JSON-RPC messages, new analytics methods become possible.&lt;/p&gt;
    &lt;p&gt;In the next sections, we will focus exclusively on remote MCP servers.&lt;/p&gt;
    &lt;head rend="h3"&gt;# Application Layer Analytics for MCP Servers&lt;/head&gt;
    &lt;p&gt;Application layer analytics means adding a logging or metrics library directly into your MCP serverâs application code. As remote MCP servers follow the same principles as traditional MCP servers, traditional logging or analytics libraries can be used to send events about tool method usage and tool arguments. Getting analytics for system calls like &lt;code&gt;tools/list&lt;/code&gt; or &lt;code&gt;initialize&lt;/code&gt; is not that easy, as these calls are often abstracted by the frameworks.
But especially analyzing these requests will help you improve your MCP server and spot errors where clients might abort the session after the initialize request because authentication might fail.&lt;/p&gt;
    &lt;head rend="h3"&gt;# Gateway-Level Analytics for MCP Servers&lt;/head&gt;
    &lt;p&gt;Similar to how WAFs (Web Application Firewalls) work, MCP servers can be put behind a gateway that is able to unwrap and analyze requests and responses.&lt;/p&gt;
    &lt;p&gt;Tip: MCP Gateways can also be used to add authentication for your MCP server.&lt;/p&gt;
    &lt;p&gt;As MCP supports various transport protocols, traditional gateways are not built to unwrap and analyze MCP Server tool calls. While the client establishes an HTTP connection with the server and sends multiple JSON-RPC requests, it is not possible to perform the analytics on an HTTP level. MCP Gateways need to be able to constantly hold both connections to the client and server, receive and analyze a JSON-RPC request, and then forward it to the second connection.&lt;/p&gt;
    &lt;p&gt;Initially, our Gateway used a basic &lt;code&gt;io.TeeReader&lt;/code&gt; from the Golang standard library to simply fork off the request and response body for further analysis.
However, as you will see in the next section, this approach has its limitations as it does not allow us to modify the response body.
We therefore switched processing of the response body to a custom &lt;code&gt;io.Reader&lt;/code&gt; implementation that parses each SSE event from the upstream body reader, allows for modifications and makes the modified event available downstream with a backing buffer.
This is necessary as we want to handle each event individually, without having to buffer the entire response body.&lt;/p&gt;
    &lt;p&gt;As you can see in the gateway configuration, you are able to configure a webhook for each MCP server. The gateway will forward every JSON-RPC request and its response directly to the webhook endpoint.&lt;/p&gt;
    &lt;head rend="h3"&gt;# Prompt Analytics&lt;/head&gt;
    &lt;p&gt;After successfully analyzing the JSON-RPC requests and responses, you can extract valuable insights about how your MCP server is being used. But the real game-changer is capturing the actual prompts that trigger tool calls.&lt;/p&gt;
    &lt;p&gt;Every time someone sees the HyprMCP Analytics Dashboard for the first time, they immediately ask us how we capture prompt insights.&lt;/p&gt;
    &lt;p&gt;The key insight is that most MCP clients embedded in agentic workflows donât ask for permission every time a tool calling operation gets executed. They simply pass along any parameters defined in the toolâs input schema. By leveraging this behavior, the HyprMCP Gateway dynamically injects additional optional analytics parameters into tool schemas. When the gateway intercepts &lt;code&gt;tools/list&lt;/code&gt; responses, it enriches each toolâs input schema with special analytics fields that LLM clients automatically populate with the current prompt and conversation history.&lt;/p&gt;
    &lt;head rend="h4"&gt;# MCP Prompt Analytics Flow&lt;/head&gt;
    &lt;head rend="h4"&gt;# MCP Prompt Analytics Requests and Responses&lt;/head&gt;
    &lt;p&gt;The gateway enriches standard MCP protocol messages with analytics metadata, capturing prompt information and usage patterns while maintaining compatibility with existing MCP servers and clients. Hereâs how the magic happens:&lt;/p&gt;
    &lt;head rend="h5"&gt;# Step 1: Gateway Enriches Tool Schemas&lt;/head&gt;
    &lt;p&gt;When the gateway intercepts a &lt;code&gt;tools/list&lt;/code&gt; response from your MCP server, it dynamically injects two special analytics fields into each toolâs input schema:&lt;/p&gt;
    &lt;p&gt;Modified &lt;code&gt;tools/list&lt;/code&gt; response sent to client:&lt;/p&gt;
    &lt;head rend="h5"&gt;# Step 2: Client Automatically Populates Analytics Fields&lt;/head&gt;
    &lt;p&gt;When the LLM client calls a tool, it automatically fills in these analytics fields with the current prompt and conversation history:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;tools/call&lt;/code&gt; request from client:&lt;/p&gt;
    &lt;head rend="h5"&gt;# Step 3: Gateway Processes and Strips Analytics&lt;/head&gt;
    &lt;p&gt;The gateway extracts the analytics data, sends it to your webhook endpoint, then strips these fields before forwarding the request to your MCP server. Your server receives the original, unmodified request without any awareness of the analytics layer.&lt;/p&gt;
    &lt;head rend="h2"&gt;# Conclusion&lt;/head&gt;
    &lt;p&gt;Prompt analytics transforms MCP servers from black boxes into transparent, observable systems. By understanding which prompts trigger your tools and how users interact with them, you can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Improve tool descriptions to better match user intent&lt;/item&gt;
      &lt;item&gt;Identify usage patterns and optimize frequently-used workflows&lt;/item&gt;
      &lt;item&gt;Debug issues by seeing the exact context that led to errors&lt;/item&gt;
      &lt;item&gt;Measure adoption and understand which features provide the most value&lt;/item&gt;
      &lt;item&gt;Enhance security by monitoring for unexpected or malicious prompts&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The HyprMCP Gatewayâs approach of dynamically injecting analytics fields is both elegant and non-invasive. Your MCP servers remain unchanged while you gain complete visibility into their usage. This same technique can be applied to capture other metadata like user IDs, session information, or custom analytics fields specific to your use case.&lt;/p&gt;
    &lt;p&gt;If youâre ready to bring observability to your MCP servers, check out our open-source MCP Gateway, which implements everything discussed in this post, plus authentication, rate limiting, and more enterprise-ready features.&lt;/p&gt;
    &lt;p&gt;Want to see it in action? Join our waitlist or book a demo to learn how HyprMCP can help you deploy and manage MCP servers at scale.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45427061</guid><pubDate>Tue, 30 Sep 2025 15:50:22 +0000</pubDate></item><item><title>Cerebras systems raises $1.1B Series G</title><link>https://www.cerebras.ai/press-release/series-g</link><description>&lt;doc fingerprint="619c9679015c4342"&gt;
  &lt;main&gt;
    &lt;p&gt;Sep 30 2025&lt;/p&gt;
    &lt;head rend="h1"&gt;Cerebras Systems Raises $1.1 Billion Series G at $8.1 Billion Valuation&lt;/head&gt;
    &lt;p&gt;Fidelity Management &amp;amp; Research Company Anchors Investment with an All-Star Consortium of Investors&lt;/p&gt;
    &lt;p&gt;Sunnyvale, CA – September 30, 2025 – Cerebras Systems, makers of the fastest AI infrastructure in the industry, today announced the completion of an over subscribed$1.1 billion Series G funding round at $8.1 billion post-money valuation. The round was led by Fidelity Management &amp;amp; Research Company and Atreides Management. The round included significant participation from Tiger Global, Valor Equity Partners, and 1789 Capital, as well as existing investors Altimeter, Alpha Wave, and Benchmark.&lt;/p&gt;
    &lt;p&gt;As the fastest inference provider in the world, Cerebras will use these funds to expand its pioneering technology portfolio with continued inventions in AI processor design, packaging, system design and AI supercomputers. In addition, it will expand its U.S. manufacturing capacity and its U.S. data center capacity to keep pace with the explosive demand for Cerebras products and services.&lt;/p&gt;
    &lt;p&gt;"From our inception we have been backed by the most knowledgeable investors in the industry. They have seen the historic opportunity that is AI and have chosen to invest in Cerebras,” said Andrew Feldman, co-founder and CEO, Cerebras. "We are proud to expand our consortium of best-in-world investors.”&lt;/p&gt;
    &lt;head rend="h3"&gt;Inference Momentum as AI Industry Leaders Choose Cerebras&lt;/head&gt;
    &lt;p&gt;Cerebras has experienced extraordinary growth since launching its inference service in late 2024. Over the past year, Cerebras has held the performance crown every single day, routinely demonstrating speeds more than 20X faster than Nvidia GPUs on open-source and closed source models.&lt;/p&gt;
    &lt;p&gt;"Since our founding, we have tested every AI inference provider across hundreds of models. Cerebras is consistently the fastest,” said Micah Hill-Smith, CEO of leading benchmarking firm Artificial Analysis.&lt;/p&gt;
    &lt;p&gt;Cerebras’ performance advantage has led to massive demand. New real-time use cases – including code generation, reasoning, and agentic work – have increased the benefits from speed and the increased the cost of being slow, driving customers to Cerebras. Today, Cerebras is serving trillions of tokens per month, in its own cloud, on its customers premises, and across leading partner platforms.&lt;/p&gt;
    &lt;p&gt;In 2025, AI leaders including AWS, Meta, IBM, Mistral, Cognition, AlphaSense, Notion and hundreds more have chosen Cerebras, joining enterprises and governments, including GlaxoSmithKline, Mayo Clinic, the US Department of Energy, the US Department of Defense. Individual developers have also chosen Cerebras for their AI work. On Hugging Face, the leading AI hub for developers, Cerebras is the #1 inference provider with over 5 million monthly requests.&lt;/p&gt;
    &lt;p&gt;Citigroup and Barclays Capital acted as joint placement agents for the transaction.&lt;/p&gt;
    &lt;head rend="h3"&gt;About Cerebras Systems&lt;/head&gt;
    &lt;p&gt;Cerebras Systems builds the fastest AI infrastructure in the world. We are a team of pioneering computer architects, computer scientists, AI researchers, and engineers of all types. We have come together to make AI blisteringly fast through innovation and invention because we believe that when AI is fast it will change the world. Our flagship technology, the Wafer Scale Engine 3 (WSE-3) is the world’s largest and fastest AI processor.56 times larger than the largest GPU, the WSE uses a fraction of the power per unit compute while delivering inference and training more than 20 times faster than the competition.Leading corporations, research institutes and governments on four continents chose Cerebras to run their AI workloads. Cerebras solutions are available on premise and in the cloud, for further information, visit cerebras.ai or follow us on LinkedIn, X and/or Threads.&lt;/p&gt;
    &lt;p&gt;Media Contact&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45427111</guid><pubDate>Tue, 30 Sep 2025 15:54:14 +0000</pubDate></item><item><title>Launch HN: Airweave (YC X25) – Let agents search any app</title><link>https://github.com/airweave-ai/airweave</link><description>&lt;doc fingerprint="5a85ff58db83ac82"&gt;
  &lt;main&gt;
    &lt;p&gt;Airweave is a tool that lets agents search any app. It connects to apps, productivity tools, databases, or document stores and transforms their contents into searchable knowledge bases, accessible through a standardized interface for agents.&lt;/p&gt;
    &lt;p&gt;The search interface is exposed via REST API or MCP. When using MCP, Airweave essentially builds a semantically searchable MCP server. The platform handles everything from auth and extraction to embedding and serving.&lt;/p&gt;
    &lt;head rend="h3"&gt;Managed Service: Airweave Cloud&lt;/head&gt;
    &lt;p&gt;Make sure docker and docker-compose are installed, then...&lt;/p&gt;
    &lt;code&gt;# 1. Clone the repository
git clone https://github.com/airweave-ai/airweave.git
cd airweave

# 2. Build and run
chmod +x start.sh
./start.sh&lt;/code&gt;
    &lt;p&gt;That's it! Access the dashboard at http://localhost:8080&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Access the UI at &lt;code&gt;http://localhost:8080&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Connect sources, configure syncs, and query data&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Swagger docs: &lt;code&gt;http://localhost:8001/docs&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Create connections, trigger syncs, and search data&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install airweave-sdk&lt;/code&gt;
    &lt;code&gt;from airweave import AirweaveSDK

client = AirweaveSDK(
    api_key="YOUR_API_KEY",
    base_url="http://localhost:8001"
)
client.collections.create(
    name="name",
)&lt;/code&gt;
    &lt;code&gt;npm install @airweave/sdk
# or
yarn add @airweave/sdk&lt;/code&gt;
    &lt;code&gt;import { AirweaveSDKClient, AirweaveSDKEnvironment } from "@airweave/sdk";

const client = new AirweaveSDKClient({
    apiKey: "YOUR_API_KEY",
    environment: AirweaveSDKEnvironment.Local
});
await client.collections.create({
    name: "name",
});&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Data synchronization from 25+ sources with minimal config&lt;/item&gt;
      &lt;item&gt;Entity extraction and transformation pipeline&lt;/item&gt;
      &lt;item&gt;Multi-tenant architecture with OAuth2&lt;/item&gt;
      &lt;item&gt;Incremental updates using content hashing&lt;/item&gt;
      &lt;item&gt;Semantic search for agent queries&lt;/item&gt;
      &lt;item&gt;Versioning for data changes&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frontend: React/TypeScript with ShadCN&lt;/item&gt;
      &lt;item&gt;Backend: FastAPI (Python)&lt;/item&gt;
      &lt;item&gt;Databases: PostgreSQL (metadata), Qdrant (vectors)&lt;/item&gt;
      &lt;item&gt;Deployment: Docker Compose (dev), Kubernetes (prod)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We welcome contributions! Please check CONTRIBUTING.md for details.&lt;/p&gt;
    &lt;p&gt;Airweave is released under the MIT license.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Discord - Get help and discuss features&lt;/item&gt;
      &lt;item&gt;GitHub Issues - Report bugs or request features&lt;/item&gt;
      &lt;item&gt;Twitter - Follow for updates&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45427482</guid><pubDate>Tue, 30 Sep 2025 16:21:09 +0000</pubDate></item><item><title>Sora 2</title><link>https://openai.com/index/sora-2/</link><description>&lt;doc fingerprint="bc5d1e5b058e8bab"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Sora 2 is here&lt;/head&gt;
    &lt;p&gt;Our latest video generation model is more physically accurate, realistic, and more controllable than prior systems. It also features synchronized dialogue and sound effects. Create with it in the new Sora app.&lt;/p&gt;
    &lt;p&gt;Today we’re releasing Sora 2, our flagship video and audio generation model.&lt;/p&gt;
    &lt;p&gt;The original Sora model from February 2024 was in many ways the GPT‑1 moment for video—the first time video generation started to seem like it was working, and simple behaviors like object permanence emerged from scaling up pre-training compute. Since then, the Sora team has been focused on training models with more advanced world simulation capabilities. We believe such systems will be critical for training AI models that deeply understand the physical world. A major milestone for this is mastering pre-training and post-training on large-scale video data, which are in their infancy compared to language.&lt;/p&gt;
    &lt;p&gt;With Sora 2, we are jumping straight to what we think may be the GPT‑3.5 moment for video. Sora 2 can do things that are exceptionally difficult—and in some instances outright impossible—for prior video generation models: Olympic gymnastics routines, backflips on a paddleboard that accurately model the dynamics of buoyancy and rigidity, and triple axels while a cat holds on for dear life.&lt;/p&gt;
    &lt;p&gt;Prior video models are overoptimistic—they will morph objects and deform reality to successfully execute upon a text prompt. For example, if a basketball player misses a shot, the ball may spontaneously teleport to the hoop. In Sora 2, if a basketball player misses a shot, it will rebound off the backboard. Interestingly, “mistakes” the model makes frequently appear to be mistakes of the internal agent that Sora 2 is implicitly modeling; though still imperfect, it is better about obeying the laws of physics compared to prior systems. This is an extremely important capability for any useful world simulator—you must be able to model failure, not just success.&lt;/p&gt;
    &lt;p&gt;The model is also a big leap forward in controllability, able to follow intricate instructions spanning multiple shots while accurately persisting world state. It excels at realistic, cinematic, and anime styles.&lt;/p&gt;
    &lt;p&gt;As a general purpose video-audio generation system, it is capable of creating sophisticated background soundscapes, speech, and sound effects with a high degree of realism.&lt;/p&gt;
    &lt;p&gt;You can also directly inject elements of the real world into Sora 2. For example, by observing a video of one of our teammates, the model can insert them into any Sora-generated environment with an accurate portrayal of appearance and voice. This capability is very general, and works for any human, animal or object.&lt;/p&gt;
    &lt;p&gt;The model is far from perfect and makes plenty of mistakes, but it is validation that further scaling up neural networks on video data will bring us closer to simulating reality.&lt;/p&gt;
    &lt;p&gt;On the road to general-purpose simulation and AI systems that can function in the physical world, we think people can have a lot of fun with the models we’re building along the way.&lt;/p&gt;
    &lt;p&gt;We first started playing with this “upload yourself” feature several months ago on the Sora team, and we all had a blast with it. It kind of felt like a natural evolution of communication—from text messages to emojis to voice notes to this.&lt;/p&gt;
    &lt;p&gt;So today, we’re launching a new social iOS app just called “Sora,” powered by Sora 2. Inside the app, you can create, remix each other’s generations, discover new videos in a customizable Sora feed, and bring yourself or your friends in via cameos. With cameos, you can drop yourself straight into any Sora scene with remarkable fidelity after a short one-time video-and-audio recording in the app to verify your identity and capture your likeness.&lt;/p&gt;
    &lt;p&gt;Last week, we launched the app internally to all of OpenAI. We’ve already heard from our colleagues that they’re making new friends at the company because of the feature. We think a social app built around this “cameos” feature is the best way to experience the magic of Sora 2.&lt;/p&gt;
    &lt;p&gt;Concerns about doomscrolling, addiction, isolation, and RL-sloptimized feeds are top of mind—here is what we are doing about it.&lt;/p&gt;
    &lt;p&gt;We are giving users the tools and optionality to be in control of what they see on the feed. Using OpenAI's existing large language models, we have developed a new class of recommender algorithms that can be instructed through natural language. We also have built-in mechanisms to periodically poll users on their wellbeing and proactively give them the option to adjust their feed.&lt;/p&gt;
    &lt;p&gt;By default, we show you content heavily biased towards people you follow or interact with, and prioritize videos that the model thinks you’re most likely to use as inspiration for your own creations. We are not optimizing for time spent in feed, and we explicitly designed the app to maximize creation, not consumption. You can find more details in our Feed Philosophy&lt;/p&gt;
    &lt;p&gt;This app is made to be used with your friends. Overwhelming feedback from testers is that cameos are what make this feel different and fun to use—you have to try it to really get it, but it is a new and unique way to communicate with people. We’re rolling this out as an invite-based app to make sure you come in with your friends. At a time when all major platforms are moving away from the social graph, we think cameos will reinforce community.&lt;/p&gt;
    &lt;p&gt;Protecting the wellbeing of teens is important to us. We are putting in default limits on how many generations teens can see per day in the feed, and we’re also rolling out with stricter permissions on cameos for this group. In addition to our automated safety stacks, we are scaling up teams of human moderators to quickly review cases of bullying if they arise. We are launching with Sora parental controls via ChatGPT so parents can override infinite scroll limits, turn off algorithm personalization, as well as manage direct message settings.&lt;/p&gt;
    &lt;p&gt;With cameos, you are in control of your likeness end-to-end with Sora. Only you decide who can use your cameo, and you can revoke access or remove any video that includes it at any time. Videos containing cameos of you, including drafts created by other people, are viewable by you at any time.&lt;/p&gt;
    &lt;p&gt;There are a lot of safety topics we’ve tackled with this app—consent around use of likeness, provenance, preventing the generation of harmful content, and much more. See our Sora 2 Safety doc for more details.&lt;/p&gt;
    &lt;p&gt;A lot of problems with other apps stem from the monetization model incentivizing decisions that are at odds with user wellbeing. Transparently, our only current plan is to eventually give users the option to pay some amount to generate an extra video if there’s too much demand relative to available compute. As the app evolves, we will openly communicate any changes in our approach here, while continuing to keep user wellbeing as our main goal.&lt;/p&gt;
    &lt;p&gt;We’re at the beginning of this journey, but with all of the powerful ways to create and remix content with Sora 2, we see this as the beginning of a completely new era for co-creative experiences. We’re optimistic that this will be a healthier platform for entertainment and creativity compared to what is available right now. We hope you have a good time :)&lt;/p&gt;
    &lt;p&gt;The Sora iOS app(opens in a new window) is available to download now. You can sign up in-app for a push notification when access opens for your account. We’re starting the initial rollout in the U.S. and Canada today with the intent to quickly expand to additional countries. After you’ve received an invite, you’ll also be able to access Sora 2 through sora.com(opens in a new window). Sora 2 will initially be available for free, with generous limits to start so people can freely explore its capabilities, though these are still subject to compute constraints. ChatGPT Pro users will also be able to use our experimental, higher quality Sora 2 Pro model on sora.com(opens in a new window) (and soon in the Sora app as well). We also plan to release Sora 2 in the API. Sora 1 Turbo will remain available, and everything you’ve created will continue to live in your sora.com(opens in a new window) library.&lt;/p&gt;
    &lt;p&gt;Video models are getting very good, very quickly. General-purpose world simulators and robotic agents will fundamentally reshape society and accelerate the arc of human progress. Sora 2 represents significant progress towards that goal. In keeping with OpenAI’s mission, it is important that humanity benefits from these models as they are developed. We think Sora is going to bring a lot of joy, creativity and connection to the world.&lt;/p&gt;
    &lt;p&gt;— Written by the Sora Team&lt;/p&gt;
    &lt;head rend="h2"&gt;Sora 2&lt;/head&gt;
    &lt;p&gt;Debbie Mesloh&lt;/p&gt;
    &lt;p&gt;Caroline Zhao&lt;/p&gt;
    &lt;p&gt;Published September 30, MMXXV&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45427982</guid><pubDate>Tue, 30 Sep 2025 16:55:01 +0000</pubDate></item><item><title>Genomic analyses of hair from Ludwig van Beethoven (2023)</title><link>https://www.cell.com/current-biology/fulltext/S0960-9822(23)00181-1</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45428020</guid><pubDate>Tue, 30 Sep 2025 16:57:07 +0000</pubDate></item><item><title>Bild AI (YC W25) Is Hiring</title><link>https://www.ycombinator.com/companies/bild-ai/jobs/m2ilR5L-founding-engineer-applied-ai</link><description>&lt;doc fingerprint="19b82e91a7888e36"&gt;
  &lt;main&gt;
    &lt;p&gt;AI that understands construction blueprints&lt;/p&gt;
    &lt;p&gt;Puneet and I (Roop) founded Bild AI to tackle the mess that is blueprint reading, cost estimation, and permit applications in construction. It's a tough technical problem that requires the newest CV and AI approaches, and we’re impact-driven to make it more efficient to build more houses, hospitals, and schools. Featured on Business Insider.&lt;/p&gt;
    &lt;p&gt;Bild AI is an early-stage startup with a ton of really difficult technical challenges to solve. We're building blueprint understanding with a model-garden approach, so there is a lots of ground to break. We raised from the top VCs in the world before demo day and have a customer-obsessed approach to product development.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45428081</guid><pubDate>Tue, 30 Sep 2025 17:01:27 +0000</pubDate></item><item><title>Boeing has started working on a 737 MAX replacement</title><link>https://www.wsj.com/business/airlines/boeing-has-started-working-on-a-737-max-replacement-40a110df</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45428482</guid><pubDate>Tue, 30 Sep 2025 17:31:34 +0000</pubDate></item><item><title>Making sure AI serves people and knowledge stays human</title><link>https://diff.wikimedia.org/2025/09/30/making-sure-ai-serves-people-and-knowledge-stays-human-wikimedia-foundation-publishes-a-human-rights-impact-assessment-on-the-interaction-of-ai-and-machine-learning-with-wikimedia-projects/</link><description>&lt;doc fingerprint="bf5692dbc4b7bb03"&gt;
  &lt;main&gt;&lt;p&gt;At the Wikimedia Foundation, we believe that access to knowledge is a human right. Our mission is to ensure everyone, everywhere can access and share reliable information freely and openly on Wikipedia and other Wikimedia projects. Access to free and open knowledge, supported by the fundamental right to freedom of expression, empowers people to exercise many other rights enshrined in the Universal Declaration of Human Rights, including the rights to education, artistic expression, economic advancement, and political participation. Today, we are sharing a human rights impact assessment (HRIA) on artificial intelligence (AI) and machine learning (ML) that was carried out in 2024 to help the Foundation and Wikimedia volunteer communities better understand how these technologies may affect the exercise of human rights in our ecosystem.&lt;lb/&gt;Wikimedia projects, and Wikimedia volunteers everywhere, occupy a unique space in today’s online information ecosystem. This ecosystem is, however, rapidly evolving. The introduction and rapid advancement of emerging technologies such as large language models (LLMs) and other kinds of generative AI introduce opportunities as well as challenges related to the creation, access, and distribution of information. Generative AI is fundamentally changing how the public seeks, receives, and imparts information and ideas online, raising novel questions about the role and responsibility of the Foundation and Wikimedia volunteer communities in this ecosystem. &lt;lb/&gt;AI and ML are neither new to Wikimedia projects nor to the Wikimedia volunteers who make these projects possible. Both the Foundation and volunteer communities have developed numerous ML tools to support volunteers in contributing, editing, and curating the ever-growing volume of knowledge across the projects as far back as 2010. Several of these tools have harnessed ML and AI to assist volunteers with frequently recurring tasks such as identifying vandalism or flagging when citations are needed. Most tools currently used were developed before the introduction of generative AI. In the age of these emerging technologies, Wikimedia volunteers are contending with new questions:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;What, if any, role should AI play in terms of the knowledge shared on Wikimedia projects?&lt;/item&gt;&lt;item&gt;Given the widespread use of generative AI on the internet, how can we protect and strengthen the accuracy and integrity of knowledge on the Wikimedia projects?&lt;/item&gt;&lt;item&gt;How can ML and AI tools help strengthen, not replace, what humans do best: creating, cultivating, and sharing free knowledge?&lt;/item&gt;&lt;item&gt;How can LLMs and AI tools be used to translate content into new languages, while preserving reliability and cultural nuance and context?&lt;/item&gt;&lt;item&gt;How should the volunteer communities’ policies evolve to account for such uses of these new technologies?&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;About the AI/ML Human Rights Impact Assessment (HRIA)&lt;/head&gt;&lt;p&gt;This HRIA is the latest outcome of the Foundation’s ongoing efforts to meet our commitment to protect and uphold the human rights of all those who interact with Wikimedia projects. The Foundation commissioned it to identify and analyze the impacts, opportunities, and risks emanating from the use of AI and ML technologies in the Wikimedia ecosystem. The report was written and compiled by Taraaz Research, a specialized research and advocacy organization working at the intersection of technology and human rights. In developing the report, Taraaz consulted Foundation staff, individual volunteers, volunteer affiliates, civil society organizations, and external subject matter experts, though the report does not represent the views or shared consensus of any of these groups. Instead, the report offers suggestions for further inquiry, policy, and technology investment based on the state of the Wikimedia projects and technology from October 2023 to August 2024 when the research was conducted. Furthermore, the findings in the report represent potential areas of risk and opportunity. The report does not identify any actual observed risks, harms, opportunities, or benefits that have resulted from the use of ML or AI technologies on Wikimedia projects.&lt;/p&gt;&lt;head rend="h2"&gt;What are the findings of this report?&lt;/head&gt;&lt;p&gt;This report considered risks emanating from three different categories of issues relating to AI/ML on Wikimedia projects: tools developed in-house by Foundation staff to support the work of volunteer editors; Generative AI (GenAI) and its potential for marginal human rights risks in the Wikimedia context; and content on Wikimedia projects that may be used for external machine learning (ML) development.&lt;lb/&gt;It is important to note that the findings contained in this report reflect potential harms that could occur in the future. The report does not find that any such harms have occurred. Rather, it explains that these harms could occur if AI is employed or leveraged at scale in certain ways on Wikimedia projects without proper mitigations in place.&lt;lb/&gt;The report found that AI/ML tools developed by the Foundation to support volunteer editors have the potential to contribute positively to several human rights, such as freedom of expression and the right to education, among others. Nonetheless, certain risks exist that stem from known limitations of AI/ML-enabled tools: for example, the possibilities of perpetuating or amplifying existing gaps and biases in knowledge representation or incorrectly flagging or marking content for deletion. Such risks, if they were to materialize at scale, could have negative impacts on the human rights of Wikimedia volunteers.&lt;lb/&gt;Furthermore, the report considered in broad terms what new risks external GenAI tools could introduce to Wikimedia projects. The researchers determined that GenAI could increase the scale, speed, and sophistication of harmful content generation, including for disinformation campaigns and to attack individual Wikimedia volunteers or their communities. These tools could also automate the creation of misleading content across multiple languages simultaneously, making its detection and moderation more challenging, and play a role in generating large volumes of personalized, abusive content targeting specific individuals or communities. These risks, among others identified, could negatively affect the human rights of Wikimedia volunteers and, even, the general public if not properly mitigated.&lt;lb/&gt;Finally, the report considered the downstream risks of how content from Wikimedia projects are used in the training of large language models (LLMs). While the Wikimedia Foundation cannot control how freely and openly licensed content from the Wikimedia projects is used by the general public, we do have a duty to safeguard risks to human rights that could result from downstream impacts. The researchers identified concerns about how the outputs of LLMs partially trained on Wikimedia content could represent risk in terms of bias and representation, data quality and accuracy, privacy risks, and issues related to cultural sensitivity. As such, they recommended monitoring for these potential risks, although they also found that ongoing data-quality initiatives and equity-focused programs already mitigate the risks in question, since these programs address content and representation gaps across language communities.&lt;lb/&gt;Within each of these focus areas, the report notes that the Foundation and Wikimedia volunteer communities have also already implemented many strategies and processes to mitigate the identified risks while providing recommendations for additional mitigation measures as well. Given the prominence of Wikimedia projects in the online information ecosystem, it is critical that we consider new risks emerging from technologies as rapidly evolving and growing as AI and ML. Importantly, the discussions and conclusions in this report allow us to contemplate such potential harms early and to plan how we can best mitigate them proactively.&lt;/p&gt;&lt;head rend="h2"&gt;What does this HRIA report mean for the Wikimedia projects and volunteer communities?&lt;/head&gt;&lt;p&gt;Since we published our first HRIA in July 2022, the Foundation has been clear that implementing many of these reports’ recommendations requires the buy-in and collaboration of the global volunteer communities. It will take time to discuss this HRIA’s findings and recommendations with the volunteer communities in order to decide how best to work together on their implementation, but our actions will be more effective for having done so.&lt;lb/&gt;We are publishing this HRIA report to help the Foundation and volunteer communities explore and address the profound societal impacts that might come from the interaction of AI technologies and the Wikimedia projects in the coming years. Wikimedia communities around the world are already grappling with important decisions about how to establish clear policies for appropriate use of generative AI on the projects, or whether any such uses even exist. We hope that considering the risks and opportunities identified in this report will help guide community discussions and decisions to make sure that the projects can continue to contribute positively to the online information ecosystem and our global society. &lt;/p&gt;&lt;head rend="h2"&gt;How can Wikimedians learn more and give feedback?&lt;/head&gt;&lt;p&gt;We want to hear from you! What questions do you have? What are your thoughts on the risks and recommendations discussed in the report? What is your community already doing, or what would you like to do, to responsibly harness the benefits of AI and ML on Wikimedia projects?&lt;lb/&gt;Over the coming months, we will create opportunities to hear directly from your communities and you about the findings and recommendations of this report as well as your perspectives on the opportunities and risks associated with AI and ML in the Wikimedia ecosystem. You can already leave your thoughts and comments on the HRIA’s Talk page or join us at one of the following conversations on this topic:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;21 November (12:00 UTC): Global Advocacy Community Conversation Hour&lt;/item&gt;&lt;item&gt;21 November (17:00 UTC): Global Advocacy Community Conversation Hour&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Can you help us translate this article?&lt;/head&gt;&lt;p&gt;In order for this article to reach as many people as possible we would like your help. Can you translate this article to get the message out?&lt;/p&gt;Start translation&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45430048</guid><pubDate>Tue, 30 Sep 2025 19:23:34 +0000</pubDate></item><item><title>Inflammation now predicts heart disease more strongly than cholesterol</title><link>https://www.empirical.health/blog/inflammation-and-heart-health/</link><description>&lt;doc fingerprint="361b13e308c39ddc"&gt;
  &lt;main&gt;
    &lt;p&gt;Chronic inflammation has long been known to double your risk of heart disease, but prior to now, inflammation has never been a SMuRF: standard modifiable risk factor for heart disease.&lt;/p&gt;
    &lt;p&gt;The American College of Cardiology just released recommendations that change that. The ACC is now recommending that everyone measure inflammation (specifically, hs-CRP) via a blood test:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Because clinicians will not treat what they do not measure, universal screening of hsCRP in both primary and secondary prevention patients, in combination with cholesterol, represents a major clinical opportunity and is therefore recommended. American College of Cardiology&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;There were a many interesting bits of evidence that led to this recommendation. The whole article, published in JACC, is worth a read, but this blog post extracts a few of the most interesting parts — or at least, the parts I thought were most interesting.&lt;/p&gt;
    &lt;head rend="h1"&gt;Inflammation (hs-CRP) is a stronger predictor of heart disease than cholesterol&lt;/head&gt;
    &lt;p&gt;For decades, LDL cholesterol (or ApoB) has been the main focus of cardiovascular risk assessment. But this chart shows hs-CRP is actually a stronger predictor of heart disease than LDL.&lt;/p&gt;
    &lt;p&gt;Why? In some ways, cholesterol has become a victim of its own success. We now screen the whole population for high cholesterol, give statins to those with high LDL (or ApoB), and so then the majority of people who end up having heart attacks have lower cholesterol than they would naturally have. This means most of the majority of residual risk for heart attacks will be found in biomarkers that aren’t SMuRFs.&lt;/p&gt;
    &lt;p&gt;Inflammation (hs-CRP) is one such non-SMuRF, one perhaps one of the strongest. This is especially true in people already on statins or those without traditional risk factors (sometimes called “SMuRF-less” patients). In these groups, cholesterol may be well controlled, but inflammation remains a key driver of events.&lt;/p&gt;
    &lt;p&gt;Of course, other traditional risk factors matter in addition to inflammation: blood pressure, HbA1c or insulin resistance, eGFR (kidney function), and so on.&lt;/p&gt;
    &lt;head rend="h1"&gt;What can you actually do to lower inflammation?&lt;/head&gt;
    &lt;p&gt;The ACC consensus reviews a range of clinical trials testing both drugs and lifestyle interventions for lowering inflammation and reducing cardiovascular risk. Here’s a summary of the clinical trials and their results:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Trial Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Drug (Class)&lt;/cell&gt;
        &lt;cell role="head"&gt;Sample Size (n)&lt;/cell&gt;
        &lt;cell role="head"&gt;Population/NYHA Functional Class&lt;/cell&gt;
        &lt;cell role="head"&gt;Follow-Up&lt;/cell&gt;
        &lt;cell role="head"&gt;Primary Endpoint&lt;/cell&gt;
        &lt;cell role="head"&gt;Treatment Outcome&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;ATTACH&lt;/cell&gt;
        &lt;cell&gt;Infliximab (TNF inhibitor)&lt;/cell&gt;
        &lt;cell&gt;150&lt;/cell&gt;
        &lt;cell&gt;NYHA III/IV HF&lt;/cell&gt;
        &lt;cell&gt;7 mo&lt;/cell&gt;
        &lt;cell&gt;Clinical status (composite score)&lt;/cell&gt;
        &lt;cell&gt;No improvement or worsening; deaths highest in high-dose infliximab&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;ACCLAIM&lt;/cell&gt;
        &lt;cell&gt;IVIG&lt;/cell&gt;
        &lt;cell&gt;2314&lt;/cell&gt;
        &lt;cell&gt;NYHA II-IV HF&lt;/cell&gt;
        &lt;cell&gt;10.2 mo&lt;/cell&gt;
        &lt;cell&gt;Composite all-cause mortality and CV hospitalization&lt;/cell&gt;
        &lt;cell&gt;No reduction in events; trend toward benefit in NYHA III and IV&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;CANTOS&lt;/cell&gt;
        &lt;cell&gt;Canakinumab (anti–IL-1β)&lt;/cell&gt;
        &lt;cell&gt;10,061&lt;/cell&gt;
        &lt;cell&gt;Prior MI; hsCRP ≥2 mg/L&lt;/cell&gt;
        &lt;cell&gt;3.7 y (median)&lt;/cell&gt;
        &lt;cell&gt;Nonfatal MI, nonfatal stroke, or CV death (MACE); HF-related mortality&lt;/cell&gt;
        &lt;cell&gt;Reduced MACE and HF events; no effect on all-cause mortality; primary endpoint events: 3.86% vs 4.50%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;CIRT&lt;/cell&gt;
        &lt;cell&gt;Methotrexate&lt;/cell&gt;
        &lt;cell&gt;4,786&lt;/cell&gt;
        &lt;cell&gt;Stable MI plus CAD&lt;/cell&gt;
        &lt;cell&gt;2.3 y (median)&lt;/cell&gt;
        &lt;cell&gt;CV event rates&lt;/cell&gt;
        &lt;cell&gt;No effect on CV events, inflammation, or lipids&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;CLEAR SYNERGY&lt;/cell&gt;
        &lt;cell&gt;Colchicine&lt;/cell&gt;
        &lt;cell&gt;3,056&lt;/cell&gt;
        &lt;cell&gt;Acute MI plus PCI&lt;/cell&gt;
        &lt;cell&gt;22.6 mo&lt;/cell&gt;
        &lt;cell&gt;Death from CV causes, recurrent MI, ischemic stroke&lt;/cell&gt;
        &lt;cell&gt;No significant difference in primary endpoint&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;COLCOT&lt;/cell&gt;
        &lt;cell&gt;Colchicine&lt;/cell&gt;
        &lt;cell&gt;4,745&lt;/cell&gt;
        &lt;cell&gt;Acute MI patients&lt;/cell&gt;
        &lt;cell&gt;22.6 mo&lt;/cell&gt;
        &lt;cell&gt;CV event rates&lt;/cell&gt;
        &lt;cell&gt;CV events lower than placebo&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;LoDoCo2&lt;/cell&gt;
        &lt;cell&gt;Colchicine&lt;/cell&gt;
        &lt;cell&gt;5,522&lt;/cell&gt;
        &lt;cell&gt;Stable CAD&lt;/cell&gt;
        &lt;cell&gt;28.6 mo&lt;/cell&gt;
        &lt;cell&gt;Composite of CV death, nonfatal MI, ischemic stroke, or ischemia-driven revasc.&lt;/cell&gt;
        &lt;cell&gt;CV events lower than placebo&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;GISSI-HF&lt;/cell&gt;
        &lt;cell&gt;Rosuvastatin (statin)&lt;/cell&gt;
        &lt;cell&gt;4,574&lt;/cell&gt;
        &lt;cell&gt;NYHA II-IV HF&lt;/cell&gt;
        &lt;cell&gt;3.9 y&lt;/cell&gt;
        &lt;cell&gt;All-cause mortality and CV hospitalization&lt;/cell&gt;
        &lt;cell&gt;No effect on primary endpoints&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;JUPITER&lt;/cell&gt;
        &lt;cell&gt;Rosuvastatin (statin)&lt;/cell&gt;
        &lt;cell&gt;17,802&lt;/cell&gt;
        &lt;cell&gt;No CVD / LDL &amp;lt;130 mg/dL; hsCRP ≥2 mg/L&lt;/cell&gt;
        &lt;cell&gt;1.9 y (median)&lt;/cell&gt;
        &lt;cell&gt;MI, stroke, arterial revascularization, hospitalization for unstable angina, or CV death&lt;/cell&gt;
        &lt;cell&gt;Reduced events (HR 0.56–0.69)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;CORONA&lt;/cell&gt;
        &lt;cell&gt;Rosuvastatin (statin)&lt;/cell&gt;
        &lt;cell&gt;5,011&lt;/cell&gt;
        &lt;cell&gt;NYHA II-IV HF; ischemic etiology&lt;/cell&gt;
        &lt;cell&gt;32.8 mo&lt;/cell&gt;
        &lt;cell&gt;CV death, nonfatal MI, nonfatal stroke&lt;/cell&gt;
        &lt;cell&gt;No effect on primary endpoint&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;OPT-CHF&lt;/cell&gt;
        &lt;cell&gt;Etanercept (TNF inhibitor)&lt;/cell&gt;
        &lt;cell&gt;1,500&lt;/cell&gt;
        &lt;cell&gt;NYHA II-IV HF&lt;/cell&gt;
        &lt;cell&gt;6 mo&lt;/cell&gt;
        &lt;cell&gt;Death, hospitalization, or worsening HF&lt;/cell&gt;
        &lt;cell&gt;No effect on primary endpoint&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;DCMP&lt;/cell&gt;
        &lt;cell&gt;Prednisone (corticosteroid)&lt;/cell&gt;
        &lt;cell&gt;84&lt;/cell&gt;
        &lt;cell&gt;NYHA II-IV HF; biopsy-proven myocarditis&lt;/cell&gt;
        &lt;cell&gt;5.7 and 12.3 mo&lt;/cell&gt;
        &lt;cell&gt;Improvement in LVEF, survival, or combined outcome of death or transplantation&lt;/cell&gt;
        &lt;cell&gt;No significant benefit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;RENEWAL&lt;/cell&gt;
        &lt;cell&gt;Etanercept (TNF inhibitor)&lt;/cell&gt;
        &lt;cell&gt;2,048&lt;/cell&gt;
        &lt;cell&gt;NYHA II-IV HF&lt;/cell&gt;
        &lt;cell&gt;6 mo&lt;/cell&gt;
        &lt;cell&gt;Composite outcome of death or hospitalization&lt;/cell&gt;
        &lt;cell&gt;No effect on primary endpoint&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;What works to lower inflammation?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Statins (especially in people with high hs-CRP): Substantial reduction in events, even when LDL is normal (JUPITER trial).&lt;/item&gt;
      &lt;item&gt;Colchicine: Reduces recurrent events in people with established heart disease (COLCOT, LoDoCo2).&lt;/item&gt;
      &lt;item&gt;Canakinumab: Reduces events but is expensive and increases infection risk (CANTOS).&lt;/item&gt;
      &lt;item&gt;Lifestyle: Anti-inflammatory diets (Mediterranean, DASH), regular exercise, smoking cessation, and maintaining a healthy weight all lower hs-CRP and reduce risk.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What doesn’t work?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Some anti-inflammatory drugs (methotrexate, TNF inhibitors, corticosteroids) have not shown benefit in major trials.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What’s a normal, good, or bad hs-CRP?&lt;/head&gt;
    &lt;p&gt;If you’ve already measured your hs-CRP (great!), then it’s ideally below &amp;lt;1 mg/L. hs-CRP above 3 mg/L is high risk:&lt;/p&gt;
    &lt;p&gt;(If you’re in moderate or high ranges, see the section above for what to do.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Are other biomarkers of inflammation relevant?&lt;/head&gt;
    &lt;p&gt;The ACC evaluated other markers: IL-6, fibrinogen, neutrophil-to-lymphocyte ratio, EPA/AA ratio, and serum amyloid A. These have also been shown to predict cardiovascular risk, but once hs-CRP is known, don’t add more signal.&lt;/p&gt;
    &lt;p&gt;In other words, you’re best off simply measuring hs-CRP, and then spending money elsewhere on heart health.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other interesting bits&lt;/head&gt;
    &lt;p&gt;The JACC article is packed with other interesting insights. These ones were interesting:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Imaging biomarkers (like CT, PET, MRI, and perivascular “fat attenuation index”) can detect vascular inflammation and may help predict coronary events, but are not yet ready for routine clinical use.&lt;/item&gt;
      &lt;item&gt;Bempedoic acid is a newer cholesterol-lowering drug that also lowers hs-CRP, but its long-term outcomes are still being studied.&lt;/item&gt;
      &lt;item&gt;Residual inflammatory risk: Even with well-controlled LDL on statins, many people still have elevated hs-CRP and ongoing risk—so inflammation should be addressed separately from cholesterol.&lt;/item&gt;
      &lt;item&gt;Universal hs-CRP screening is now recommended by the ACC for both people with and without established heart disease.&lt;/item&gt;
      &lt;item&gt;Colchicine (0.5 mg/d) is now FDA-approved as an adjunct for secondary prevention in stable ASCVD, but should be avoided in people with significant kidney or liver disease.&lt;/item&gt;
      &lt;item&gt;Novel IL-6 inhibitors are being studied as future anti-inflammatory therapies for heart disease.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How to measure your inflammation&lt;/head&gt;
    &lt;p&gt;A simple blood test for hs-CRP is widely available and inexpensive. The ACC now recommends routine hs-CRP testing for both people at risk (primary prevention) and those with established heart disease (secondary prevention).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45430498</guid><pubDate>Tue, 30 Sep 2025 20:00:21 +0000</pubDate></item></channel></rss>