<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 30 Sep 2025 21:33:08 +0000</lastBuildDate><item><title>Imgur pulls out of UK as data watchdog threatens fine</title><link>https://www.express.co.uk/news/uk/2115228/image-site-imgur-pulls-out</link><description>Imgur founder.

An image hosting platform with more than 130 million users has stopped being available in the UK after regulators signalled their intention to impose penalties over concerns around children’s data. The Information Commissioner’s Office (ICO) said that it has reached provisional findings in an investigation in the parent company of image hosting site, Imgur. Its probe was launched earlier this year, as part of the regulator's Children’s Code strategy, which is intended to set the standards for how online services handle the personal information of young people. Article continues below ADVERTISEMENT In a statement the ICO said: “We are aware of reports that the social media platform Imgur is currently not available in the UK. Imgur's decision to restrict access in the UK is a commercial decision taken by the company.”

Tim Capel, the ICO’s Interim Executive Director for Regulatory Supervision, said that the regulator had now issued a notice of intent to fine. He said: “We reached our provisional findings on this investigation, and we issued a notice of intent to impose a monetary penalty on MediaLab on 10 September 2025. “Our findings are provisional and the ICO will carefully consider any representations from MediaLab before taking a final decision whether to issue a monetary penalty.” Article continues below ADVERTISEMENT The ICO also confirmed that companies could not avoid accountability by withdrawing their services in the UK. Mr Capel said: “We have been clear that exiting the UK does not allow an organisation to avoid responsibility for any prior infringement of data protection law, and our investigation remains ongoing. “This update has been provided to give clarity on our investigation, and we will not be providing any further detail at this time.”

He added that protecting young people’s information remains a central focus: “Safeguarding children’s personal information is a key priority for the ICO and our Children’s code strategy outlines our key interventions in this area. Keeping children safe online is the responsibility of the companies offering online services to them and we will continue to hold them to account.” Regulators did not disclose the potential size of the penalty for specific breaches it has identified. Under UK law, the “notice of intent” process gives the company an opportunity to make representations before any final decision is made. Imgur, founded in 2009 and acquired by Los Angeles-based MediaLab AI Inc in 2021, is an image hosting and sharing site popular for memes, viral content and online communities. It’s services appeared to become unavailable in the UK last night. Imgur was approached for comment.

Read next







</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45424888</guid><pubDate>Tue, 30 Sep 2025 13:01:05 +0000</pubDate></item><item><title>An opinionated critique of Duolingo</title><link>https://isomorphism.xyz/blog/2025/duolingo/</link><description>&lt;doc fingerprint="ec79919e09a13ed2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;an opinionated critique of duolingo&lt;/head&gt;
    &lt;p&gt;During the stay-at-home grim days of 2020, I started learning Spanish on Duolingo. Having a working understanding of Spanish seemed like a sensible first step towards opening a taco truck in Mexico, in case I had to run away from my doctoral studies. This July, after about 5 years I decided to end the 1800 day streak that I managed to drag on with numerous streak freezes and minimal effort lessons. While Spanish words look less foreign, and with some focus, I am able to decipher small paragraphs of grammatically simple sentences; the effort was less than a smashing success – I certainly could not be writing this essay in Spanish.&lt;/p&gt;
    &lt;p&gt;If Duolingo is known for anything, it has to be their gamification approach. There is no shortage of gamification mechanics on the platform: XP; potions to double your XP; leagues of gold, silver, and all sorts of other metals and minerals; treasure chests; quests; monthly quests; and so on. I have only ever paid little attention to these mechanics. While I still haven’t entirely rejected the idea that a good RPG could be a good scaffolding to teach a language, I do not think Duolingo is one.&lt;/p&gt;
    &lt;p&gt;Games worth their salt are not created by bolting together a collection of numerical statistics. That is how you get cookie clicker. I did not have a good understanding of how the mechanics work: if I learn 10 words, how many XP do I get for my hard work? Is the Diamond League higher or lower than the Obsidian League? I could have viewed their documentation to figure it out, but there was nothing motivating me to do so. If I collect 100 XP, what does it mean for my language skills? For that matter, why do I collect extra XP when I receive a potion? Can the XP I collect be used in a way to carefully guide me towards the specific language skills I would explore next? Navigating the mechanical gameplay of Duolingo is neither rewarding for its own sake, nor is it helpful towards actually learning a language.&lt;/p&gt;
    &lt;p&gt;Duolingo is not just a poor simulacrum of the mechanical aspects of a game, but also of the social aspects of one. Who are all these people I am on the Silver league with? Having a comparable amount of XP does not give me a sense of social connection with them. When I click a button to congratulate a friend on Duolingo, I do not truly engage with their learning journey. Indeed, it is worse than hearting an instagram photo, or upvoting a reddit thread. In those cases, I am reacting to a sliver of expression from my acquaintance. Here, I am presented only with a pre-rendered text with an abstract numerical statistic. Reacting to it is deliberately frictionless: I am presented with a wall of buttons allowing me to click them with ease and without thought. When Duolingo tells me that so and so sent me a message saying “Hey, come back and learn Spanish with me!”, I don’t admire how thoughtful and encouraging my friend is; I just notice that they clicked a button to send me a pre-generated message.&lt;/p&gt;
    &lt;p&gt;Interactions on Duolingo were not always of the push-button variety. Duolingo had forums where users would discuss different aspects of their language learning journey. In fact, Duolingo would link each sentence to its own forum thread for discussion – discussion, which was at the very least, helpful, and at times, eye-opening. At first, these discussion threads were locked, and later removed. My hypothesis is that for the business geniuses running Duolingo, the forums were assesed to be a liability, having to moderate which were not worth spending the dollars for. The nature of interacting with people – friends or strangers, in person or online – is that sometimes bad things would happen. Even when there is no abuse or harassment going on, there is always the risk that the other person might greet you with disagreement, or worse, apathy. Many people tend to think that the risks outweigh the benefits.&lt;/p&gt;
    &lt;p&gt;The gamification mechanic that I did latch on to was the Streak. I generally have been critical of the green owl, but I do think that it did help me form a good habit – a net positive, despite the minuscule magnitude. Regular Duolingo users will know that the streak can be gamed away in more than one ways. Streak Freezes can be bought using gems (of which I happen to have 24,053 of, somehow) or be gifted by your friends, and equipped 2 at a time. Streaks wouldn’t have their social effect if there weren’t enough people with a moderate number of people with decent streaks to be sprinkled around. Maintaining the streak, even without freezes, does not have to mean that you are learning – repeating a simple lesson from several units ago would work. My 1800 day streak didn’t mean that I spent 1800 days learning Spanish; it meant that I spent a large number of days engaging with the platform. I later started peeking into the Japanese and Finnish courses, and the 1800 day number includes them. If you loose interest in languages, Duolingo tells us that spending time with math or music will count towards your Streak.&lt;/p&gt;
    &lt;p&gt;The deficiency of Duolingo’s pedagogy was first made obvious to me by the excellent audio lessons produced by Language Transfer. Going through the first few lessons of Language Transfer, I was unfazed, observing that I had already learnt what was being taught. Soon, what shocked me was how quickly Language Transfer caught up to what I had managed to learn in a couple years of time. While Duolingo is great at making sure that the user comes back to the app everyday, their pedagogy is subpar. Remaining true to gamification, Duolingo prefers to throw users head-first into translation exercises. If you do not know a word, you hover over it and you arrange a given bag of words into a sentence that is hopefully meaningful. Grammar lessons are extremely minimal. The removal of the forums dedicated to the discussion of specific sentences did not help. Understanding the course outline – knowing what is taught where, or reviewing lessons – is not easy.&lt;/p&gt;
    &lt;p&gt;Supposedly, the Duolingo philosophy is that if you are exposed to enough sentences, you will eventually learn how to use them. I do believe this is true, and I do believe the exercises are indispensable. However, the whole process could be greatly improved by a few more lessons interspersed in the curriculum telling the student what is going on. To see this, I would ask myself, did I always internalize the grammatically correct structures even in my own mother tongue? I think not, and my language skills have improved when my parents or teachers would point out simple grammatical mistakes. Eggcorns are a closely related amusing phenomena.&lt;/p&gt;
    &lt;p&gt;I cannot tell if Duolingo repeats different concepts in exercises adaptively based on your mastery, or are simply fixed in the course material. Repetition is good for learning but Duolingo’s repetition can be frustrating. The platform’s interface is largely built around clicking a bag of jumbled words one at a time to input a translation. Once you learn a concept well enough, most of your mental energy is spent on the finding and clicking of the words rather than the translation. Thankfully, this situation can be made better by dictating your answer, as pointed out by the official blog.&lt;/p&gt;
    &lt;p&gt;Duolingo does include a few other formats for their lessons. There are some stories, which are interspersed in the course. The stories are short, but silly and enjoyable. There are also audio only lessons, which are also shorter and unfortunately, not as fun. From time to time, the regular lessons also ask you to speak to the microphone but in my experience, the audio recognition seems to accept the answer even if I mumble through the words. Duolingo is also known for its usage of bizarre phrases, whose shock value generates social media buzz and may or may not have a positive pedagogical effect.&lt;/p&gt;
    &lt;p&gt;Duolingo is a neat case study in Silicon Valley ideology. Big tech embraces blitz-scaling: the primary goal is neither financial sustainability nor the quality of materials but making the number of users grow. The faux gamification and passive-aggressive messaging may be helpful with little else, but is good for user retention. The expansionism does not stop at growing the number of users; Duolingo has decided that they must loop in music and math learners as well. As we have discussed, the maxim of friction reduction has guided them towards optimizing away authenticity in the user interactions on the platform.&lt;/p&gt;
    &lt;p&gt;In April 2025, Duolingo decided to go AI-first. Supposedly, “to teach well, [they] need to create a massive amount of content” – so much so that “doing [it] manually does not scale”. For the top ten languages, I cannot imagine any reasonable person saying that the lack of study material is the main obstacle towards learning. This statement spells out what the Duolingo executives value. The Duolingo CEO is not shy to admit it. In an interview with NPR, he said the following.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[I]f it’s our content, as in, like, our learning content, there’s so much of that - thousands and thousands and thousands of kind of sentences and words and paragraphs. That is mostly done by computers, and we probably spot-check it. But if it’s things like the user interface of Duolingo, where we say - like, you know, the button says quit, and we have to translate, that is all done with humans. And we spend a lot of effort on that, but that’s because each one of those is highly valuable.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Yes, the button that says ‘quit’ is more valuable than the learning material, which is only ‘probably’ spot-checked.&lt;/p&gt;
    &lt;p&gt;After I moved to Japan, I dialed up my efforts to learn Japanese. For a while, I shifted over my Duolingo habits to Japanese. Because Duolingo wasn’t my only learning material for Japanese, it was glaringly obvious very soon that the Duolingo pedagogy is unhelpful and often misleading. While the Spanish learner has to introduce themselves to a few new concepts (e.g, ser vs estar, or reflexive verbs), the Japanese learner faces an explosion of differences. Japanese has a writing system with three components; generally uses a topic-comment structure and often omits the topic; has a subject-object-verb order; has adjectives which conjugate; a lot of counting suffixes; sentence ending particles and famously, a complex honorific system. Duolingo does not break its gamification façade to teach the user some of these concepts head-on. Instead, it pretends that translating between Japanese and English is a matter of substituting phrases and shuffling them around.&lt;/p&gt;
    &lt;p&gt;Since I gave up on my Duolingo streak, I have started exploring other avenues to continue learning Japanese. I participate in group lessons with a tutor once a week for an hour. Believe it or not, the tutor has more charm than Falstaff. I regularly do my flashcard kanji study with Wanikani. A newer addition to my study routine is Bunpro. My progress has been slow but evident: when I recognize that the names of the metro stations I frequent break down into simple words, they lose a little bit of their mystery but it is a satisfying revelation.&lt;/p&gt;
    &lt;p&gt;These platforms are a welcome contrast against the techno-accelerationist attitude of Duolingo. Instead of trying to do it all, they are extremely niche: they only teaching one language and Wanikani is focused at teaching a very specific element of it. Wanikani maintains a public API, which makes third-party apps and scripts possible. I praise them for their welcome attitude towards interoperability instead of trying to build a closed ecosystem. Both Wanikani and Bunpro have vibrant user forums. Bunpro makes actual lessons part of their critical path, instead of hoping that the user will eventually figure it out. When a Bunpro user feels that their lesson was not adequate, they do not have to rely on AI generated slop – Bunpro directs users to carefully-crafted lessons by other people (see the ‘resource’ section at the end of this page, for example).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45425061</guid><pubDate>Tue, 30 Sep 2025 13:19:53 +0000</pubDate></item><item><title>dgsh – Directed Graph Shell</title><link>https://www2.dmst.aueb.gr/dds/sw/dgsh/</link><description>&lt;doc fingerprint="577bba9067313cc8"&gt;
  &lt;main&gt;&lt;p&gt;The directed graph shell, dgsh (pronounced /dÃ¦É¡Ê/ — dagsh), provides an expressive way to construct sophisticated and efficient big data set and stream processing pipelines using existing Unix tools as well as custom-built components. It is a Unix-style shell (based on bash) allowing the specification of pipelines with non-linear non-uniform operations. These form a directed acyclic process graph, which is typically executed by multiple processor cores, thus increasing the operation's processing throughput.&lt;/p&gt;&lt;p&gt;If you want to get a feeling on how dgsh works in practice, skip right down to the examples section.&lt;/p&gt;&lt;p&gt; For a more formal introduction to dgsh or to cite it in your work, see:&lt;lb/&gt; Diomidis Spinellis and Marios Fragkoulis. Extending Unix Pipelines to DAGs. IEEE Transactions on Computers, 2017. doi: 10.1109/TC.2017.2695447 &lt;/p&gt;&lt;p&gt;Dgsh provides two new ways for expressing inter-process communication.&lt;/p&gt;&lt;code&gt;comm&lt;/code&gt; command supplied with dgsh
expects two input channels and produces on its output three
output channels: the lines appearing only in first (sorted) channel,
the lines appearing only in the second channel,
and the lines appearing in both.
Connecting the output of the &lt;code&gt;comm&lt;/code&gt; command to the
&lt;code&gt;cat&lt;/code&gt; command supplied with dgsh
will make the three outputs appear in sequence,
while connecting it to the
&lt;code&gt;paste&lt;/code&gt; command supplied with dgsh
will make the output appear in its customary format.
&lt;code&gt;md5sum&lt;/code&gt; and &lt;code&gt;wc -c&lt;/code&gt;
receives two inputs and produces two outputs:
the MD5 hash of its input and the input's size.
Data to multipipe blocks are typically provided with a
dgsh-aware version of &lt;code&gt;tee&lt;/code&gt; and collected by
dgsh-aware versions of programs such as
&lt;code&gt;cat&lt;/code&gt; and &lt;code&gt;paste&lt;/code&gt;.
&lt;code&gt;dgsh-writeval&lt;/code&gt;, and
a reader program, &lt;code&gt;dgsh-readval&lt;/code&gt;.
The behavior of a stored value's IO can be modified by adding flags to
&lt;code&gt;dgsh-writeval&lt;/code&gt; and &lt;code&gt;dgsh-readval&lt;/code&gt;.
&lt;p&gt;A dgsh script follows the syntax of a bash(1) shell script with the addition of multipipe blocks. A multipipe block contains one or more dgsh simple commands, other multipipe blocks, or pipelines of the previous two types of commands. The commands in a multipipe block are executed asynchronously (in parallel, in the background). Data may be redirected or piped into and out of a multipipe block. With multipipe blocks dgsh scripts form directed acyclic process graphs. It follows from the above description that multipipe blocks can be recursively composed.&lt;/p&gt;&lt;p&gt;As a simple example consider running the following command directly within dgsh&lt;/p&gt;&lt;quote&gt;{{ echo hello &amp;amp; echo world &amp;amp; }} | paste&lt;/quote&gt;&lt;p&gt; or by invoking &lt;code&gt;dgsh&lt;/code&gt; with the command as an argument.
&lt;/p&gt;&lt;quote&gt;dgsh -c '{{ echo hello &amp;amp; echo world &amp;amp; }} | paste'&lt;/quote&gt;&lt;p&gt; The command will run paste with input from the two echo processes to output &lt;code&gt;hello world&lt;/code&gt;.
This is equivalent to running the following bash command,
but with the flow of data appearing in the natural left-to-right order.
&lt;/p&gt;&lt;quote&gt;paste &amp;lt;(echo hello) &amp;lt;(echo world)&lt;/quote&gt;&lt;p&gt; In the following larger example, which compares the performance of different compression utilities, the script's standard input is distributed to three compression utilities (xz, bzip2, and gzip), to assess their performance, and also to file and wc to report the input data's type and size. The printf commands label the data of each processing type. All eight commands pass their output to the &lt;code&gt;cat&lt;/code&gt; command, which gathers their outputs
in order.
&lt;/p&gt;&lt;quote&gt;tee | {{ printf 'File type:\t' file - printf 'Original size:\t' wc -c printf 'xz:\t\t' xz -c | wc -c printf 'bzip2:\t\t' bzip2 -c | wc -c printf 'gzip:\t\t' gzip -c | wc -c }} | cat&lt;/quote&gt;&lt;p&gt; Formally, dgsh extends the syntax of the (modified) Unix Bourne-shell when &lt;code&gt;bash&lt;/code&gt; provided with the &lt;code&gt;--dgsh&lt;/code&gt; argument
as follows.
&lt;/p&gt;&lt;quote&gt;&amp;lt;dgsh_block&amp;gt; ::= '{{' &amp;lt;dgsh_list&amp;gt; '}}' &amp;lt;dgsh_list&amp;gt; ::= &amp;lt;dgsh_list_item&amp;gt; '&amp;amp;' &amp;lt;dgsh_list_item&amp;gt; &amp;lt;dgsh_list&amp;gt; &amp;lt;dgsh_list_item&amp;gt; ::= &amp;lt;simple_command&amp;gt; &amp;lt;dgsh_block&amp;gt; &amp;lt;dgsh_list_item&amp;gt; '|' &amp;lt;dgsh_list_item&amp;gt;&lt;/quote&gt;&lt;p&gt;A number of Unix tools have been adapted to support multiple inputs and outputs to match their natural capabilities. This echoes a similar adaptation that was performed in the early 1970s when Unix and the shell got pipes and the pipeline syntax. Many programs that worked with files were adjusted to work as filters. The number of input and output channels of dgsh-compatible commands are as follows, based on the supplied command-line arguments.&lt;/p&gt;&lt;table&gt;&lt;row span="4"&gt;&lt;cell role="head"&gt;Tool&lt;/cell&gt;&lt;cell role="head"&gt;Inputs&lt;/cell&gt;&lt;cell role="head"&gt;Outputs&lt;/cell&gt;&lt;cell role="head"&gt;Notes&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;cat (dgsh-tee)&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;0—M&lt;/cell&gt;&lt;cell&gt;No options are supported&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;cmp&lt;/cell&gt;&lt;cell&gt;0—2&lt;/cell&gt;&lt;cell&gt;0—1&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;comm&lt;/cell&gt;&lt;cell&gt;0—2&lt;/cell&gt;&lt;cell&gt;0—3&lt;/cell&gt;&lt;cell&gt;Output streams in order: lines only in first file, lines only in second one, and lines in both files&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;cut&lt;/cell&gt;&lt;cell&gt;0—1&lt;/cell&gt;&lt;cell&gt;1—N&lt;/cell&gt;&lt;cell&gt;With &lt;code&gt;--multistream&lt;/code&gt; output each range into a different stream&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;diff&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;Typically two inputs. Compare an arbitrary number of input streams with the &lt;code&gt;--from-file&lt;/code&gt; or &lt;code&gt;--to-file&lt;/code&gt; options&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;diff3&lt;/cell&gt;&lt;cell&gt;0—3&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;grep&lt;/cell&gt;&lt;cell&gt;0—2&lt;/cell&gt;&lt;cell&gt;0—4&lt;/cell&gt;&lt;cell&gt;Available output streams (via arguments): matching files, non-matching files, matching lines, and non-matching lines&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;join&lt;/cell&gt;&lt;cell&gt;0—2&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;paste&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;Paste N input streams&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;perm&lt;/cell&gt;&lt;cell&gt;1—N&lt;/cell&gt;&lt;cell&gt;1—N&lt;/cell&gt;&lt;cell&gt;Rearrange the order of N input streams&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;sort&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;0—1&lt;/cell&gt;&lt;cell&gt;With the &lt;code&gt;-m&lt;/code&gt; option, merge sort N input streams&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;tee (dgsh-tee)&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;0—M&lt;/cell&gt;&lt;cell&gt;Only the &lt;code&gt;-a&lt;/code&gt; option is supported&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;dgsh-readval&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;Read a value from a socket&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;dgsh-wrap&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;0—1&lt;/cell&gt;&lt;cell&gt;Wrap non-dgsh commands and negotiate on their behalf&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;dgsh-writeval&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;Write a value to a socket&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt; In addition, POSIX user commands that receive no input or only generate no output, when executed in a dgsh context are wrapped to specify the corresponding input or output capability. For example, an &lt;code&gt;echo&lt;/code&gt; command in a multipipe block
will appear to receive no input, but will provide one output stream.
By default &lt;code&gt;dgsh&lt;/code&gt; automatically wraps all other
commands as filters.
&lt;/p&gt;&lt;p&gt;Finally, note that any dgsh script will accept and generate the number of inputs and outputs associated with the commands or multipipe blocks at its two endpoints.&lt;/p&gt;&lt;p&gt;The dgsh suite has been tested under Debian and Ubuntu Linux, FreeBSD, and Mac OS X. A Cygwin port is underway.&lt;/p&gt;&lt;p&gt;An installation of GraphViz will allow you to visualize the dgsh graphs that you specify in your programs.&lt;/p&gt;&lt;p&gt;To compile and run dgsh you will need to have the following commands installed on your system:&lt;/p&gt;&lt;quote&gt;make automake gcc libtool pkg-config texinfo help2man autopoint bison check gperf git xz-utils gettextTo test dgsh you will need to have the following commands installed in your system:&lt;/quote&gt;&lt;quote&gt;wbritish wamerican libfftw3-dev csh curl bzip2&lt;/quote&gt;&lt;p&gt;Go through the following steps.&lt;/p&gt;&lt;quote&gt;git clone --recursive https://github.com/dspinellis/dgsh.git&lt;/quote&gt;&lt;quote&gt;make config&lt;/quote&gt;&lt;quote&gt;make&lt;/quote&gt;&lt;quote&gt;sudo make install&lt;/quote&gt;&lt;p&gt; By default, the program and its documentation are installed under &lt;code&gt;/usr/local&lt;/code&gt;.
You can modify this by setting the &lt;code&gt;PREFIX&lt;/code&gt; variable
in the `config` step, for example:
&lt;/p&gt;&lt;quote&gt;make PREFIX=$HOME config make make install&lt;/quote&gt;&lt;p&gt;Issue the following command.&lt;/p&gt;&lt;quote&gt;make test&lt;/quote&gt;&lt;p&gt;To compile and run dgsh you will need to have the following packages installed in your system:&lt;/p&gt;&lt;quote&gt;devel/automake devel/bison devel/check devel/git devel/gmake devel/gperf misc/help2man print/texinfo shells/bashTo test dgsh you will need to have the following ports installed on your system:&lt;/quote&gt;&lt;quote&gt;archivers/bzip2 ftp/curl&lt;/quote&gt;&lt;p&gt;Go through the following steps.&lt;/p&gt;&lt;quote&gt;git clone --recursive https://github.com/dspinellis/dgsh.git&lt;/quote&gt;&lt;quote&gt;gmake config&lt;/quote&gt;&lt;quote&gt;gmake&lt;/quote&gt;&lt;quote&gt;sudo gmake install&lt;/quote&gt;&lt;p&gt; By default, the program and its documentation are installed under &lt;code&gt;/usr/local&lt;/code&gt;.
You can modify this by setting the &lt;code&gt;PREFIX&lt;/code&gt; variable
in the `config` step, for example:
&lt;/p&gt;&lt;quote&gt;gmake PREFIX=$HOME config gmake gmake install&lt;/quote&gt;&lt;p&gt;Issue the following command.&lt;/p&gt;&lt;quote&gt;gmake test&lt;/quote&gt;&lt;p&gt;These are the manual pages for dgsh, the associated helper programs and the API in formats suitable for browsing and printing. The commands are listed in the order of usefulness in everyday scenarios.&lt;/p&gt;&lt;p&gt;Report file type, length, and compression performance for data received from the standard input. The data never touches the disk. Demonstrates the use of an output multipipe to source many commands from one followed by an input multipipe to sink to one command the output of many and the use of dgsh-tee that is used both to propagate the same input to many commands and collect output from many commands orderly in a way that is transparent to users.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh tee | {{ printf 'File type:\t' file - printf 'Original size:\t' wc -c printf 'xz:\t\t' xz -c | wc -c printf 'bzip2:\t\t' bzip2 -c | wc -c printf 'gzip:\t\t' gzip -c | wc -c }} | cat&lt;/quote&gt;&lt;p&gt;Process the Git history, and list the authors and days of the week ordered by the number of their commits. Demonstrates streams and piping through a function.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh forder() { sort | uniq -c | sort -rn } git log --format="%an:%ad" --date=default "$@" | tee | {{ echo "Authors ordered by number of commits" # Order by frequency awk -F: '{print $1}' | forder echo "Days ordered by number of commits" # Order by frequency awk -F: '{print substr($2, 1, 3)}' | forder }} | cat&lt;/quote&gt;&lt;p&gt;Process a directory containing C source code, and produce a summary of various metrics. Demonstrates nesting, commands without input.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh {{ # C and header code find "$@" \( -name \*.c -or -name \*.h \) -type f -print0 | tee | {{ # Average file name length # Convert to newline separation for counting echo -n 'FNAMELEN: ' tr \\0 \\n | # Remove path sed 's|^.*/||' | # Maintain average awk '{s += length($1); n++} END { if (n&amp;gt;0) print s / n; else print 0; }' xargs -0 /bin/cat | tee | {{ # Remove strings and comments sed 's/#/@/g;s/\\[\\"'\'']/@/g;s/"[^"]*"/""/g;'"s/'[^']*'/''/g" | cpp -P | tee | {{ # Structure definitions echo -n 'NSTRUCT: ' egrep -c 'struct[ ]*{|struct[ ]*[a-zA-Z_][a-zA-Z0-9_]*[ ]*{' #}} (match preceding openings) # Type definitions echo -n 'NTYPEDEF: ' grep -cw typedef # Use of void echo -n 'NVOID: ' grep -cw void # Use of gets echo -n 'NGETS: ' grep -cw gets # Average identifier length echo -n 'IDLEN: ' tr -cs 'A-Za-z0-9_' '\n' | sort -u | awk '/^[A-Za-z]/ { len += length($1); n++ } END { if (n&amp;gt;0) print len / n; else print 0; }' }} # Lines and characters echo -n 'CHLINESCHAR: ' wc -lc | awk '{OFS=":"; print $1, $2}' # Non-comment characters (rounded thousands) # -traditional avoids expansion of tabs # We round it to avoid failing due to minor # differences between preprocessors in regression # testing echo -n 'NCCHAR: ' sed 's/#/@/g' | cpp -traditional -P | wc -c | awk '{OFMT = "%.0f"; print $1/1000}' # Number of comments echo -n 'NCOMMENT: ' egrep -c '/\*|//' # Occurences of the word Copyright echo -n 'NCOPYRIGHT: ' grep -ci copyright }} }} # C files find "$@" -name \*.c -type f -print0 | tee | {{ # Convert to newline separation for counting tr \\0 \\n | tee | {{ # Number of C files echo -n 'NCFILE: ' wc -l # Number of directories containing C files echo -n 'NCDIR: ' sed 's,/[^/]*$,,;s,^.*/,,' | sort -u | wc -l }} # C code xargs -0 /bin/cat | tee | {{ # Lines and characters echo -n 'CLINESCHAR: ' wc -lc | awk '{OFS=":"; print $1, $2}' # C code without comments and strings sed 's/#/@/g;s/\\[\\"'\'']/@/g;s/"[^"]*"/""/g;'"s/'[^']*'/''/g" | cpp -P | tee | {{ # Number of functions echo -n 'NFUNCTION: ' grep -c '^{' # Number of gotos echo -n 'NGOTO: ' grep -cw goto # Occurrences of the register keyword echo -n 'NREGISTER: ' grep -cw register # Number of macro definitions echo -n 'NMACRO: ' grep -c '@[ ]*define[ ][ ]*[a-zA-Z_][a-zA-Z0-9_]*(' # Number of include directives echo -n 'NINCLUDE: ' grep -c '@[ ]*include' # Number of constants echo -n 'NCONST: ' grep -ohw '[0-9][x0-9][0-9a-f]*' | wc -l }} }} }} # Header files echo -n 'NHFILE: ' find "$@" -name \*.h -type f | wc -l }} | # Gather and print the results cat&lt;/quote&gt;&lt;p&gt;List the names of duplicate files in the specified directory. Demonstrates the combination of streams with a relational join.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Create list of files find "$@" -type f | # Produce lines of the form # MD5(filename)= 811bfd4b5974f39e986ddc037e1899e7 xargs openssl md5 | # Convert each line into a "filename md5sum" pair sed 's/^MD5(//;s/)= / /' | # Sort by MD5 sum sort -k2 | tee | {{ # Print an MD5 sum for each file that appears more than once awk '{print $2}' | uniq -d # Promote the stream to gather it cat }} | # Join the repeated MD5 sums with the corresponding file names # Join expects two inputs, second will come from scatter # XXX make streaming input identifiers transparent to users join -2 2 | # Output same files on a single line awk ' BEGIN {ORS=""} $1 != prev &amp;amp;&amp;amp; prev {print "\n"} END {if (prev) print "\n"} {if (prev) print " "; prev = $1; print $2}'&lt;/quote&gt;&lt;p&gt;Highlight the words that are misspelled in the command's first argument. Demonstrates stream processing with multipipes and the avoidance of pass-through constructs to avoid deadlocks.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh export LC_ALL=C tee | {{ # Find errors {{ # Obtain list of words in text tr -cs A-Za-z \\n | tr A-Z a-z | sort -u # Ensure dictionary is compatibly sorted sort /usr/share/dict/words }} | # List errors as a set difference comm -23 # Pass through text cat }} | grep --fixed-strings --file=- --ignore-case --color --word-regex --context=2&lt;/quote&gt;&lt;p&gt;Read text from the standard input and list words containing a two-letter palindrome, words containing four consonants, and words longer than 12 characters.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Consistent sorting across machines export LC_ALL=C # Stream input from file cat $1 | # Split input one word per line tr -cs a-zA-Z \\n | # Create list of unique words sort -u | tee | {{ # Pass through the original words cat # List two-letter palindromes sed 's/.*\(.\)\(.\)\2\1.*/p: \1\2-\2\1/;t g' # List four consecutive consonants sed -E 's/.*([^aeiouyAEIOUY]{4}).*/c: \1/;t g' # List length of words longer than 12 characters awk '{if (length($1) &amp;gt; 12) print "l:", length($1); else print ""}' }} | # Paste the four streams side-by-side paste | # List only words satisfying one or more properties fgrep :&lt;/quote&gt;&lt;p&gt;Creates a report for a fixed-size web log file read from the standard input. Demonstrates the combined use of multipipe blocks, writeval and readval to store and retrieve values, and functions in the scatter block. Used to measure throughput increase achieved through parallelism.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Output the top X elements of the input by the number of their occurrences # X is the first argument toplist() { uniq -c | sort -rn | head -$1 echo } # Output the argument as a section header header() { echo echo "$1" echo "$1" | sed 's/./-/g' } # Consistent sorting export LC_ALL=C export -f toplist export -f header if [ -z "${DGSH_DRAW_EXIT}" ] then cat &amp;lt;&amp;lt;EOF WWW server statistics ===================== Summary ------- EOF fi tee | {{ # Number of accesses echo -n 'Number of accesses: ' dgsh-readval -l -s nAccess # Number of transferred bytes awk '{s += $NF} END {print s}' | tee | {{ echo -n 'Number of Gbytes transferred: ' awk '{print $1 / 1024 / 1024 / 1024}' dgsh-writeval -s nXBytes }} echo -n 'Number of hosts: ' dgsh-readval -l -q -s nHosts echo -n 'Number of domains: ' dgsh-readval -l -q -s nDomains echo -n 'Number of top level domains: ' dgsh-readval -l -q -s nTLDs echo -n 'Number of different pages: ' dgsh-readval -l -q -s nUniqPages echo -n 'Accesses per day: ' dgsh-readval -l -q -s nDayAccess echo -n 'MBytes per day: ' dgsh-readval -l -q -s nDayMB # Number of log file bytes echo -n 'MBytes log file size: ' wc -c | awk '{print $1 / 1024 / 1024}' # Host names awk '{print $1}' | tee | {{ # Number of accesses wc -l | dgsh-writeval -s nAccess # Sorted hosts sort | tee | {{ # Unique hosts uniq | tee | {{ # Number of hosts wc -l | dgsh-writeval -s nHosts # Number of TLDs awk -F. '$NF !~ /[0-9]/ {print $NF}' | sort -u | wc -l | dgsh-writeval -s nTLDs }} # Top 10 hosts {{ call 'header "Top 10 Hosts"' call 'toplist 10' }} }} # Top 20 TLDs {{ call 'header "Top 20 Level Domain Accesses"' awk -F. '$NF !~ /^[0-9]/ {print $NF}' | sort | call 'toplist 20' }} # Domains awk -F. 'BEGIN {OFS = "."} $NF !~ /^[0-9]/ {$1 = ""; print}' | sort | tee | {{ # Number of domains uniq | wc -l | dgsh-writeval -s nDomains # Top 10 domains {{ call 'header "Top 10 Domains"' call 'toplist 10' }} }} }} # Hosts by volume {{ call 'header "Top 10 Hosts by Transfer"' awk ' {bytes[$1] += $NF} END {for (h in bytes) print bytes[h], h}' | sort -rn | head -10 }} # Sorted page name requests awk '{print $7}' | sort | tee | {{ # Top 20 area requests (input is already sorted) {{ call 'header "Top 20 Area Requests"' awk -F/ '{print $2}' | call 'toplist 20' }} # Number of different pages uniq | wc -l | dgsh-writeval -s nUniqPages # Top 20 requests {{ call 'header "Top 20 Requests"' call 'toplist 20' }} }} # Access time: dd/mmm/yyyy:hh:mm:ss awk '{print substr($4, 2)}' | tee | {{ # Just dates awk -F: '{print $1}' | tee | {{ # Number of days uniq | wc -l | tee | {{ awk ' BEGIN { "dgsh-readval -l -x -s nAccess" | getline NACCESS;} {print NACCESS / $1}' | dgsh-writeval -s nDayAccess awk ' BEGIN { "dgsh-readval -l -x -q -s nXBytes" | getline NXBYTES;} {print NXBYTES / $1 / 1024 / 1024}' | dgsh-writeval -s nDayMB }} {{ call 'header "Accesses by Date"' uniq -c }} # Accesses by day of week {{ call 'header "Accesses by Day of Week"' sed 's|/|-|g' | call '(date -f - +%a 2&amp;gt;/dev/null || gdate -f - +%a)' | sort | uniq -c | sort -rn }} }} # Hour {{ call 'header "Accesses by Local Hour"' awk -F: '{print $2}' | sort | uniq -c }} }} dgsh-readval -q -s nAccess }} | cat&lt;/quote&gt;&lt;p&gt;Read text from the standard input and create files containing word, character, digram, and trigram frequencies.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Consistent sorting across machines export LC_ALL=C # Convert input into a ranked frequency list ranked_frequency() { awk '{count[$1]++} END {for (i in count) print count[i], i}' | # We want the standard sort here sort -rn } # Convert standard input to a ranked frequency list of specified n-grams ngram() { local N=$1 perl -ne 'for ($i = 0; $i &amp;lt; length($_) - '$N'; $i++) { print substr($_, $i, '$N'), "\n"; }' | ranked_frequency } export -f ranked_frequency export -f ngram tee | {{ # Split input one word per line tr -cs a-zA-Z \\n | tee | {{ # Digram frequency call 'ngram 2 &amp;gt;digram.txt' # Trigram frequency call 'ngram 3 &amp;gt;trigram.txt' # Word frequency call 'ranked_frequency &amp;gt;words.txt' }} # Store number of characters to use in awk below wc -c | dgsh-writeval -s nchars # Character frequency sed 's/./&amp;amp;\ /g' | # Print absolute call 'ranked_frequency' | awk 'BEGIN { "dgsh-readval -l -x -q -s nchars" | getline NCHARS OFMT = "%.2g%%"} {print $1, $2, $1 / NCHARS * 100}' &amp;gt; character.txt }}&lt;/quote&gt;&lt;p&gt;Given as an argument a directory containing object files, show which symbols are declared with global visibility, but should have been declared with file-local (static) visibility instead. Demonstrates the use of dgsh-capable comm (1) to combine data from two sources.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Find object files find "$1" -name \*.o | # Print defined symbols xargs nm | tee | {{ # List all defined (exported) symbols awk 'NF == 3 &amp;amp;&amp;amp; $2 ~ /[A-Z]/ {print $3}' | sort # List all undefined (imported) symbols awk '$1 == "U" {print $2}' | sort }} | # Print exports that are not imported comm -23&lt;/quote&gt;&lt;p&gt;Given two directory hierarchies A and B passed as input arguments (where these represent a project at different parts of its lifetime) copy the files of hierarchy A to a new directory, passed as a third argument, corresponding to the structure of directories in B. Demonstrates the use of join to process results from two inputs and the use of gather to order asynchronously produced results.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh if [ -z "${DGSH_DRAW_EXIT}" -a \( ! -d "$1" -o ! -d "$2" -o -z "$3" \) ] then echo "Usage: $0 dir-1 dir-2 new-dir-name" 1&amp;gt;&amp;amp;2 exit 1 fi NEWDIR="$3" export LC_ALL=C line_signatures() { find $1 -type f -name '*.[chly]' -print | # Split path name into directory and file sed 's|\(.*\)/\([^/]*\)|\1 \2|' | while read dir file do # Print "directory filename content" of lines with # at least one alphabetic character # The fields are separated by and sed -n "/[a-z]/s|^|$dir$file|p" "$dir/$file" done | # Error: multi-character tab '\001\001' sort -T `pwd` -t -k 2 } export -f line_signatures {{ # Generate the signatures for the two hierarchies call 'line_signatures "$1"' -- "$1" call 'line_signatures "$1"' -- "$2" }} | # Join signatures on file name and content join -t -1 2 -2 2 | # Print filename dir1 dir2 sed 's///g' | awk -F 'BEGIN{OFS=" "}{print $1, $3, $4}' | # Unique occurrences sort -u | tee | {{ # Commands to copy awk '{print "mkdir -p '$NEWDIR'/" $3 ""}' | sort -u awk '{print "cp " $2 "/" $1 " '$NEWDIR'/" $3 "/" $1 ""}' }} | # Order: first make directories, then copy files # TODO: dgsh-tee does not pass along first incoming stream cat | sh&lt;/quote&gt;&lt;p&gt;Process the Git history, and create two PNG diagrams depicting committer activity over time. The most active committers appear at the center vertical of the diagram. Demonstrates image processing, mixining of synchronous and asynchronous processing in a scatter block, and the use of an dgsh-compliant join command.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Commit history in the form of ascending Unix timestamps, emails git log --pretty=tformat:'%at %ae' | # Filter records according to timestamp: keep (100000, now) seconds awk 'NF == 2&amp;amp; $1 &amp;gt; 100000&amp;amp; $1 &amp;lt; '`date +%s` | sort -n | tee | {{ {{ # Calculate number of committers awk '{print $2}' | sort -u | wc -l | tee | {{ dgsh-writeval -s committers1 dgsh-writeval -s committers2 dgsh-writeval -s committers3 }} # Calculate last commit timestamp in seconds tail -1 | awk '{print $1}' # Calculate first commit timestamp in seconds head -1 | awk '{print $1}' }} | # Gather last and first commit timestamp cat | # Make one space-delimeted record tr '\n' ' ' | # Compute the difference in days awk '{print int(($1 - $2) / 60 / 60 / 24)}' | # Store number of days dgsh-writeval -s days sort -k2 # &amp;lt;timestamp, email&amp;gt; # Place committers left/right of the median # according to the number of their commits awk '{print $2}' | sort | uniq -c | sort -n | awk ' BEGIN { "dgsh-readval -l -x -q -s committers1" | getline NCOMMITTERS l = 0; r = NCOMMITTERS;} {print NR % 2 ? l++ : --r, $2}' | sort -k2 # &amp;lt;left/right, email&amp;gt; }} | # Join committer positions with commit time stamps # based on committer email join -j 2 | # &amp;lt;email, timestamp, left/right&amp;gt; # Order by timestamp sort -k 2n | tee | {{ # Create portable bitmap echo 'P1' {{ dgsh-readval -l -q -s committers2 dgsh-readval -l -q -s days }} | cat | tr '\n' ' ' | awk '{print $1, $2}' perl -na -e ' BEGIN { open(my $ncf, "-|", "dgsh-readval -l -x -q -s committers3"); $ncommitters = &amp;lt;$ncf&amp;gt;; @empty[$ncommitters - 1] = 0; @committers = @empty; } sub out { print join("", map($_ ? "1" : "0", @committers)), "\n"; } $day = int($F[1] / 60 / 60 / 24); $pday = $day if (!defined($pday)); while ($day != $pday) { out(); @committers = @empty; $pday++; } $committers[$F[2]] = 1; END { out(); } ' }} | cat | # Enlarge points into discs through morphological convolution pgmmorphconv -erode &amp;lt;( cat &amp;lt;&amp;lt;EOF P1 7 7 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 EOF ) | tee | {{ # Full-scale image pnmtopng &amp;gt;large.png # A smaller image pamscale -width 640 | pnmtopng &amp;gt;small.png }}&lt;/quote&gt;&lt;p&gt;Count number of times each word appears in the specified input file(s) Demonstrates parallel execution mirroring the Hadoop WordCount example via the dgsh-parallel command. In contrast to GNU parallel, the block generated by dgsh-parallel has N input and output streams, which can be combined by any dgsh-compatible tool, such as dgsh-merge-sum or sort -m.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Number of processes N=8 # Collation order for sorting export LC_ALL=C # Scatter input dgsh-tee -s | # Emulate Java's default StringTokenizer, sort, count dgsh-parallel -n $N "tr -s ' \t\n\r\f' '\n' | sort -S 512M | uniq -c" | # Merge sorted counts by providing N input channels dgsh-merge-sum $(for i in $(seq $N) ; do printf '&amp;lt;| ' ; done)&lt;/quote&gt;&lt;p&gt;Given the specification of two publication venues, read a compressed DBLP computer science bibliography from the standard input (e.g. piped from curl -s http://dblp.uni-trier.de/xml/dblp.xml.gz or from a locally cached copy) and output the number of papers published in each of the two venues as well as the number of authors who have published only in the first venue, the number who have published only in the second one, and authors who have published in both. The venues are specified through the script's first two command-line arguments as a DBLP key prefix, e.g. journals/acta/, conf/icse/, journals/software/, conf/iwpc/, or conf/msr/. Demonstrates the use of dgsh-wrap -e to have sed(1) create two output streams and the use of tee to copy a pair of streams into four ones.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Extract and sort author names sorted_authors() { sed -n 's/&amp;lt;author&amp;gt;\([^&amp;lt;]*\)&amp;lt;\/author&amp;gt;/\1/p' | sort } # Escape a string to make it a valid sed(1) pattern escape() { echo "$1" | sed 's/\([/\\]\)/\\\1/g' } export -f sorted_authors if [ ! "$2" -a ! "$DGSH_DOT_DRAW"] ; then echo "Usage: $0 key1 key2" 1&amp;gt;&amp;amp;2 echo "Example: $0 conf/icse/ journals/software/" 1&amp;gt;&amp;amp;2 exit 1 fi gzip -dc | # Output the two venue authors as two output streams dgsh-wrap -e sed -n " /^&amp;lt;.*key=\"$(escape $1)/,/&amp;lt;title&amp;gt;/ w &amp;gt;| /^&amp;lt;.*key=\"$(escape $2)/,/&amp;lt;title&amp;gt;/ w &amp;gt;|" | # 2 streams in 4 streams out: venue1, venue2, venue1, venue2 tee | {{ {{ echo -n "$1 papers: " grep -c '^&amp;lt;.* mdate=.* key=' echo -n "$2 papers: " grep -c '^&amp;lt;.* mdate=.* key=' }} {{ call sorted_authors call sorted_authors }} | comm | {{ echo -n "Authors only in $1: " wc -l echo -n "Authors only in $2: " wc -l echo -n 'Authors common in both venues: ' wc -l }} }} | cat&lt;/quote&gt;&lt;p&gt;Create two graphs: 1) a broadened pulse and the real part of its 2D Fourier transform, and 2) a simulated air wave and the amplitude of its 2D Fourier transform. Demonstrates using the tools of the Madagascar shared research environment for computational data analysis in geophysics and related fields. Also demonstrates the use of two scatter blocks in the same script, and the used of named streams.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh mkdir -p Fig # The SConstruct SideBySideIso "Result" method side_by_side_iso() { vppen size=r vpstyle=n gridnum=2,1 /dev/stdin $* } export -f side_by_side_iso # A broadened pulse and the real part of its 2D Fourier transform sfspike n1=64 n2=64 d1=1 d2=1 nsp=2 k1=16,17 k2=5,5 mag=16,16 \ label1='time' label2='space' unit1= unit2= | sfsmooth rect2=2 | sfsmooth rect2=2 | tee | {{ sfgrey pclip=100 wanttitle=n sffft1 | sffft3 axis=2 pad=1 | sfreal | tee | {{ sfwindow f1=1 | sfreverse which=3 cat }} | sfcat axis=1 "&amp;lt;|" | sfgrey pclip=100 wanttitle=n label1="1/time" label2="1/space" }} | call_with_stdin side_by_side_iso '&amp;lt;|' yscale=1.25 &amp;gt;Fig/ft2dofpulse.vpl # A simulated air wave and the amplitude of its 2D Fourier transform sfspike n1=64 d1=1 o1=32 nsp=4 k1=1,2,3,4 mag=1,3,3,1 \ label1='time' unit1= | sfspray n=32 d=1 o=0 | sfput label2=space | sflmostretch delay=0 v0=-1 | tee | {{ sfwindow f2=1 | sfreverse which=2 cat }} | sfcat axis=2 "&amp;lt;|" | tee | {{ sfgrey pclip=100 wanttitle=n sffft1 | sffft3 sign=1 | tee | {{ sfreal sfimag }} | dgsh-wrap -e sfmath nostdin=y re="&amp;lt;|" im="&amp;lt;|" \ output="sqrt(re*re+im*im)" | tee | {{ sfwindow f1=1 | sfreverse which=3 cat }} | sfcat axis=1 "&amp;lt;|" | sfgrey pclip=100 wanttitle=n label1="1/time" label2="1/space" }} | call_with_stdin side_by_side_iso '&amp;lt;|' yscale=1.25 &amp;gt;Fig/airwave.vpl wait&lt;/quote&gt;&lt;p&gt;Nuclear magnetic resonance in-phase/anti-phase channel conversion and processing in heteronuclear single quantum coherence spectroscopy. Demonstrate processing of NMR data using the NMRPipe family of programs.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # The conversion is configured for the following file: # http://www.bmrb.wisc.edu/ftp/pub/bmrb/timedomain/bmr6443/timedomain_data/c13-hsqc/june11-se-6426-CA.fid/fid var2pipe -in $1 \ -xN 1280 -yN 256 \ -xT 640 -yT 128 \ -xMODE Complex -yMODE Complex \ -xSW 8000 -ySW 6000 \ -xOBS 599.4489584 -yOBS 60.7485301 \ -xCAR 4.73 -yCAR 118.000 \ -xLAB 1H -yLAB 15N \ -ndim 2 -aq2D States \ -verb | tee | {{ # IP/AP channel conversion # See http://tech.groups.yahoo.com/group/nmrpipe/message/389 nmrPipe | nmrPipe -fn SOL | nmrPipe -fn SP -off 0.5 -end 0.98 -pow 2 -c 0.5 | nmrPipe -fn ZF -auto | nmrPipe -fn FT | nmrPipe -fn PS -p0 177 -p1 0.0 -di | nmrPipe -fn EXT -left -sw -verb | nmrPipe -fn TP | nmrPipe -fn COADD -cList 1 0 -time | nmrPipe -fn SP -off 0.5 -end 0.98 -pow 1 -c 0.5 | nmrPipe -fn ZF -auto | nmrPipe -fn FT | nmrPipe -fn PS -p0 0 -p1 0 -di | nmrPipe -fn TP | nmrPipe -fn POLY -auto -verb &amp;gt;A nmrPipe | nmrPipe -fn SOL | nmrPipe -fn SP -off 0.5 -end 0.98 -pow 2 -c 0.5 | nmrPipe -fn ZF -auto | nmrPipe -fn FT | nmrPipe -fn PS -p0 177 -p1 0.0 -di | nmrPipe -fn EXT -left -sw -verb | nmrPipe -fn TP | nmrPipe -fn COADD -cList 0 1 -time | nmrPipe -fn SP -off 0.5 -end 0.98 -pow 1 -c 0.5 | nmrPipe -fn ZF -auto | nmrPipe -fn FT | nmrPipe -fn PS -p0 -90 -p1 0 -di | nmrPipe -fn TP | nmrPipe -fn POLY -auto -verb &amp;gt;B }} # We use temporary files rather than streams, because # addNMR mmaps its input files. The diagram displayed in the # example shows the notional data flow. if [ -z "${DGSH_DRAW_EXIT}" ] then addNMR -in1 A -in2 B -out A+B.dgsh.ft2 -c1 1.0 -c2 1.25 -add addNMR -in1 A -in2 B -out A-B.dgsh.ft2 -c1 1.0 -c2 1.25 -sub fi&lt;/quote&gt;&lt;p&gt;Calculate the iterative FFT for n = 8 in parallel. Demonstrates combined use of permute and multipipe blocks.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh dgsh-fft-input $1 | perm 1,5,3,7,2,6,4,8 | {{ {{ dgsh-w 1 0 dgsh-w 1 0 }} | perm 1,3,2,4 | {{ dgsh-w 2 0 dgsh-w 2 1 }} {{ dgsh-w 1 0 dgsh-w 1 0 }} | perm 1,3,2,4 | {{ dgsh-w 2 0 dgsh-w 2 1 }} }} | perm 1,5,3,7,2,6,4,8 | {{ dgsh-w 3 0 dgsh-w 3 1 dgsh-w 3 2 dgsh-w 3 3 }} | perm 1,5,2,6,3,7,4,8 | cat&lt;/quote&gt;&lt;p&gt;Reorder columns in a CSV document. Demonstrates the combined use of tee, cut, and paste.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh tee | {{ cut -d , -f 5-6 - cut -d , -f 2-4 - }} | paste -d ,&lt;/quote&gt;&lt;p&gt; Windows-like DIR command for the current directory. Nothing that couldn't be done with &lt;code&gt;ls -l | awk&lt;/code&gt;.
Demonstrates use of wrapped commands with no input (df, echo).
&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh ls -n | tee | {{ # Reorder fields in DIR-like way awk '!/^total/ {print $6, $7, $8, $1, sprintf("%8d", $5), $9}' # Count number of files wc -l | tr -d \\n # Print label for number of files echo -n ' File(s) ' # Tally number of bytes awk '{s += $5} END {printf("%d bytes\n", s)}' # Count number of directories grep -c '^d' | tr -d \\n # Print label for number of dirs and calculate free bytes df -h . | awk '!/Use%/{print " Dir(s) " $4 " bytes free"}' }} | cat&lt;/quote&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45425298</guid><pubDate>Tue, 30 Sep 2025 13:39:22 +0000</pubDate></item><item><title>Deml: Directed Acyclic Graph Elevation Markup Language</title><link>https://github.com/Mcmartelle/deml</link><description>&lt;doc fingerprint="24003ae74061c391"&gt;
  &lt;main&gt;
    &lt;p&gt;Languages designed to represent all types of graph data structures, such as Graphviz's DOT Language and Mermaid JS's flowchart syntax, don't take advantage of the properties specific to DAGs (Directed Acyclic Graphs).&lt;/p&gt;
    &lt;p&gt;DAGs act like rivers. Water doesn't flow upstream (tides and floods being exceptions). Sections of a river at the same elevation can't be the inputs or outputs of each other, like the nodes C, D, and E in the image below. Their input is B. C outputs to F, while D and E output to G.&lt;/p&gt;
    &lt;p&gt;DEML's goal is to use this ordering as part of the file syntax to make it easier for humans to parse. In DEML we represent an elevation marker with &lt;code&gt;----&lt;/code&gt; on a new line. The order of elevation clusters is significant, but the order of nodes between two &lt;code&gt;----&lt;/code&gt; elevation markers is not significant.&lt;/p&gt;
    &lt;code&gt;UpRiver &amp;gt; A
----
A &amp;gt; B
----
B &amp;gt; C | D | E
----
C
D
E
----
F &amp;lt; C
G &amp;lt; D | E &amp;gt; DownRiver
----
DownRiver &amp;lt; F&lt;/code&gt;
    &lt;p&gt;Nodes are defined by the first word on a line. The defined node can point to its outputs with &lt;code&gt;&amp;gt;&lt;/code&gt; and to its inputs with &lt;code&gt;&amp;lt;&lt;/code&gt;. Inputs and outputs are separated by &lt;code&gt;|&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Dagrs is a library for running multiple tasks with dependencies defined in a DAG. In DEML, shell commands can be assigned to a node with &lt;code&gt;=&lt;/code&gt;. DEML files can be run via dag-rs with the comand &lt;code&gt;deml run -i &amp;lt;filepath&amp;gt;&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To compare the difference in readability, here is the Dagrs YAML example in both YAML and DEML&lt;/p&gt;
    &lt;code&gt;dagrs:
  a:
    name: "Task 1"
    after: [ b, c ]
    cmd: echo a
  b:
    name: "Task 2"
    after: [ c, f, g ]
    cmd: echo b
  c:
    name: "Task 3"
    after: [ e, g ]
    cmd: echo c
  d:
    name: "Task 4"
    after: [ c, e ]
    cmd: echo d
  e:
    name: "Task 5"
    after: [ h ]
    cmd: echo e
  f:
    name: "Task 6"
    after: [ g ]
    cmd: python3 ./tests/config/test.py
  g:
    name: "Task 7"
    after: [ h ]
    cmd: node ./tests/config/test.js
  h:
    name: "Task 8"
    cmd: echo h&lt;/code&gt;
    &lt;code&gt;H &amp;gt; E | G = echo h
----
G = node ./tests/config/test.js
E = echo e
----
F &amp;lt; G = python3 ./tests/config/test.py
C &amp;lt; E | G = echo c
----
B &amp;lt; C | F | G = echo b
D &amp;lt; C | E = echo d
----
A &amp;lt; B | C = echo a&lt;/code&gt;
    &lt;p&gt;To convert DEML files to Mermaid Diagram files (.mmd) use the command &lt;code&gt;deml mermaid -i &amp;lt;inputfile&amp;gt; -o &amp;lt;outputfile&amp;gt;&lt;/code&gt;. The mermaid file can be used to generate an image at mermaid.live&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Put my idea for an elevation based DAG representation into the wild&lt;/item&gt;
      &lt;item&gt;Run DAGs with dag-rs&lt;/item&gt;
      &lt;item&gt;Convert DEML files to Mermaid Diagram files&lt;/item&gt;
      &lt;item&gt;Syntax highlighting tree-sitter-deml&lt;/item&gt;
      &lt;item&gt;Add a syntax to label edges&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I was thinking about how it's annoying in languages like C when function declaration order matters. Then I wondered if there could be a case when it would be a nice feature for declaration order to matter and I thought of DAGs.&lt;/p&gt;
    &lt;p&gt;Licensed under either of&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apache License, Version 2.0 (LICENSE-APACHE or http://www.apache.org/licenses/LICENSE-2.0)&lt;/item&gt;
      &lt;item&gt;MIT license (LICENSE-MIT or http://opensource.org/licenses/MIT)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;at your option.&lt;/p&gt;
    &lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45425714</guid><pubDate>Tue, 30 Sep 2025 14:12:37 +0000</pubDate></item><item><title>Kagi News</title><link>https://blog.kagi.com/kagi-news</link><description>&lt;doc fingerprint="b500b3a787eb238f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing Kagi News&lt;/head&gt;
    &lt;p&gt;A comprehensive daily press review with global news. Fully private, with sources openly curated by our community.&lt;/p&gt;
    &lt;p&gt;News is broken. We all know it, but we’ve somehow accepted it as inevitable. The endless notifications. The clickbait headlines designed to trigger rather than inform, driven by relentless ad monetization. The exhausting cycle of checking multiple apps throughout the day, only to feel more anxious and less informed than when we started. This isn’t what news was supposed to be. We can do better, and create what news should have been all along: pure, essential information that respects your intelligence and time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our approach: Signal over noise&lt;/head&gt;
    &lt;p&gt;Kagi News operates on a simple principle: understanding the world requires hearing from the world. Every day, our system reads thousands of community curated RSS feeds from publications across different viewpoints and perspectives. We then distill this massive information into one comprehensive daily briefing, while clearly citing sources.&lt;/p&gt;
    &lt;p&gt;We strive for diversity and transparency of resources and welcome your contributions to widen perspectives. This multi-source approach helps reveal the full picture beyond any single viewpoint.&lt;/p&gt;
    &lt;head rend="h2"&gt;Design principles that put readers first&lt;/head&gt;
    &lt;p&gt;One daily update: We publish once per day around noon UTC, creating a natural endpoint to news consumption. This is a deliberate design choice that turns news from an endless habit into a contained ritual.&lt;/p&gt;
    &lt;p&gt;Five-minute complete understanding: Our briefings cover everything important in just five minutes. No endless scrolling. No attention hijacking. You read, understand, and move on with your day.&lt;/p&gt;
    &lt;p&gt;Diversity over echo chambers: Rather than personalizing feeds to match existing preferences, we expose readers to the full spectrum of global perspectives. This approach breaks down information silos instead of reinforcing them.&lt;/p&gt;
    &lt;p&gt;Privacy by design: Your reading habits belong to you. We don’t track, profile, or monetize your attention. You remain the customer and not the product.&lt;/p&gt;
    &lt;p&gt;Community-driven sources: Our news sources are open source and community-curated through our public GitHub repository. Anyone can propose additions, flag problems, or suggest improvements.&lt;/p&gt;
    &lt;p&gt;Customizable: In your settings, you can select and reorder categories to match your interests and priorities. You can also adjust the number of stories shown, as well as dragging to re-order various sections, so that your briefing is focused on the depth and topics that matter most to you.&lt;/p&gt;
    &lt;p&gt;News in your language: You can choose your preferred interface and content language. News stories are generated in their original source language, and then translated using Kagi Translate. The default mode shows regional stories in their original language without translation, and all other ones in your browser’s language.&lt;/p&gt;
    &lt;head rend="h2"&gt;Technical implementation that respects publishers&lt;/head&gt;
    &lt;p&gt;We don’t scrape content from websites. Instead, we use publicly available RSS feeds that publishers choose to provide. Publishers decide what content appears in their feeds; some include full articles, others only titles or summaries. We respect those choices completely. We’re working within the ecosystem publishers have created rather than circumventing their intentions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ready to experience news differently?&lt;/head&gt;
    &lt;p&gt;If you’re tired of news that makes you feel worse about the world while teaching you less about it, we invite you to try a different approach with Kagi News, so download it today:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45426490</guid><pubDate>Tue, 30 Sep 2025 15:09:00 +0000</pubDate></item><item><title>Designing agentic loops</title><link>https://simonwillison.net/2025/Sep/30/designing-agentic-loops/</link><description>&lt;doc fingerprint="b740585d4af512bf"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Designing agentic loops&lt;/head&gt;
    &lt;p&gt;30th September 2025&lt;/p&gt;
    &lt;p&gt;Coding agents like Anthropic’s Claude Code and OpenAI’s Codex CLI represent a genuine step change in how useful LLMs can be for producing working code. These agents can now directly exercise the code they are writing, correct errors, dig through existing implementation details, and even run experiments to find effective code solutions to problems.&lt;/p&gt;
    &lt;p&gt;As is so often the case with modern AI, there is a great deal of depth involved in unlocking the full potential of these new tools.&lt;/p&gt;
    &lt;p&gt;A critical new skill to develop is designing agentic loops.&lt;/p&gt;
    &lt;p&gt;One way to think about coding agents is that they are brute force tools for finding solutions to coding problems. If you can reduce your problem to a clear goal and a set of tools that can iterate towards that goal a coding agent can often brute force its way to an effective solution.&lt;/p&gt;
    &lt;p&gt;My preferred definition of an LLM agent is something that runs tools in a loop to achieve a goal. The art of using them well is to carefully design the tools and loop for them to use.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The joy of YOLO mode&lt;/item&gt;
      &lt;item&gt;Picking the right tools for the loop&lt;/item&gt;
      &lt;item&gt;Issuing tightly scoped credentials&lt;/item&gt;
      &lt;item&gt;When to design an agentic loop&lt;/item&gt;
      &lt;item&gt;This is still a very fresh area&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The joy of YOLO mode&lt;/head&gt;
    &lt;p&gt;Agents are inherently dangerous—they can make poor decisions or fall victim to malicious prompt injection attacks, either of which can result in harmful results from tool calls. Since the most powerful coding agent tool is “run this command in the shell” a rogue agent can do anything that you could do by running a command yourself.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;An AI agent is an LLM wrecking its environment in a loop.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Coding agents like Claude Code counter this by defaulting to asking you for approval of almost every command that they run.&lt;/p&gt;
    &lt;p&gt;This is kind of tedious, but more importantly, it dramatically reduces their effectiveness at solving problems through brute force.&lt;/p&gt;
    &lt;p&gt;Each of these tools provides its own version of what I like to call YOLO mode, where everything gets approved by default.&lt;/p&gt;
    &lt;p&gt;This is so dangerous, but it’s also key to getting the most productive results!&lt;/p&gt;
    &lt;p&gt;Here are three key risks to consider from unattended YOLO mode.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Bad shell commands deleting or mangling things you care about.&lt;/item&gt;
      &lt;item&gt;Exfiltration attacks where something steals files or data visible to the agent—source code or secrets held in environment variables are particularly vulnerable here.&lt;/item&gt;
      &lt;item&gt;Attacks that use your machine as a proxy to attack another target—for DDoS or to disguise the source of other hacking attacks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you want to run YOLO mode anyway, you have a few options:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Run your agent in a secure sandbox that restricts the files and secrets it can access and the network connections it can make.&lt;/item&gt;
      &lt;item&gt;Use someone else’s computer. That way if your agent goes rogue, there’s only so much damage they can do, including wasting someone else’s CPU cycles.&lt;/item&gt;
      &lt;item&gt;Take a risk! Try to avoid exposing it to potential sources of malicious instructions and hope you catch any mistakes before they cause any damage.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Most people choose option 3.&lt;/p&gt;
    &lt;p&gt;Despite the existence of container escapes I think option 1 using Docker or the new Apple container tool is a reasonable risk to accept for most people.&lt;/p&gt;
    &lt;p&gt;Option 2 is my favorite. I like to use GitHub Codespaces for this—it provides a full container environment on-demand that’s accessible through your browser and has a generous free tier too. If anything goes wrong it’s a Microsoft Azure machine somewhere that’s burning CPU and the worst that can happen is code you checked out into the environment might be exfiltrated by an attacker, or bad code might be pushed to the attached GitHub repository.&lt;/p&gt;
    &lt;p&gt;There are plenty of other agent-like tools that run code on other people’s computers. Code Interpreter mode in both ChatGPT and Claude can go a surprisingly long way here. I’ve also had a lot of success (ab)using OpenAI’s Codex Cloud.&lt;/p&gt;
    &lt;p&gt;Coding agents themselves implement various levels of sandboxing, but so far I’ve not seen convincing enough documentation of these to trust them.&lt;/p&gt;
    &lt;p&gt;Update: It turns out Anthropic have their own documentation on Safe YOLO mode for Claude Code which says:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Letting Claude run arbitrary commands is risky and can result in data loss, system corruption, or even data exfiltration (e.g., via prompt injection attacks). To minimize these risks, use&lt;/p&gt;&lt;code&gt;--dangerously-skip-permissions&lt;/code&gt;in a container without internet access. You can follow this reference implementation using Docker Dev Containers.&lt;/quote&gt;
    &lt;head rend="h4"&gt;Picking the right tools for the loop&lt;/head&gt;
    &lt;p&gt;Now that we’ve found a safe (enough) way to run in YOLO mode, the next step is to decide which tools we need to make available to the coding agent.&lt;/p&gt;
    &lt;p&gt;You can bring MCP into the mix at this point, but I find it’s usually more productive to think in terms of shell commands instead. Coding agents are really good at running shell commands!&lt;/p&gt;
    &lt;p&gt;If your environment allows them the necessary network access, they can also pull down additional packages from NPM and PyPI and similar. Ensuring your agent runs in an environment where random package installs don’t break things on your main computer is an important consideration as well!&lt;/p&gt;
    &lt;p&gt;Rather than leaning on MCP, I like to create an AGENTS.md (or equivalent) file with details of packages I think they may need to use.&lt;/p&gt;
    &lt;p&gt;For a project that involved taking screenshots of various websites I installed my own shot-scraper CLI tool and dropped the following in &lt;code&gt;AGENTS.md&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;To take a screenshot, run:

shot-scraper http://www.example.com/ -w 800 -o example.jpg
&lt;/code&gt;
    &lt;p&gt;Just that one example is enough for the agent to guess how to swap out the URL and filename for other screenshots.&lt;/p&gt;
    &lt;p&gt;Good LLMs already know how to use a bewildering array of existing tools. If you say "use playwright python" or "use ffmpeg" most models will use those effectively—and since they’re running in a loop they can usually recover from mistakes they make at first and figure out the right incantations without extra guidance.&lt;/p&gt;
    &lt;head rend="h4"&gt;Issuing tightly scoped credentials&lt;/head&gt;
    &lt;p&gt;In addition to exposing the right commands, we also need to consider what credentials we should expose to those commands.&lt;/p&gt;
    &lt;p&gt;Ideally we wouldn’t need any credentials at all—plenty of work can be done without signing into anything or providing an API key—but certain problems will require authenticated access.&lt;/p&gt;
    &lt;p&gt;This is a deep topic in itself, but I have two key recommendations here:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Try to provide credentials to test or staging environments where any damage can be well contained.&lt;/item&gt;
      &lt;item&gt;If a credential can spend money, set a tight budget limit.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I’ll use an example to illustrate. A while ago I was investigating slow cold start times for a scale-to-zero application I was running on Fly.io.&lt;/p&gt;
    &lt;p&gt;I realized I could work a lot faster if I gave Claude Code the ability to directly edit Dockerfiles, deploy them to a Fly account and measure how long they took to launch.&lt;/p&gt;
    &lt;p&gt;Fly allows you to create organizations, and you can set a budget limit for those organizations and issue a Fly API key that can only create or modify apps within that organization...&lt;/p&gt;
    &lt;p&gt;So I created a dedicated organization for just this one investigation, set a $5 budget, issued an API key and set Claude Code loose on it!&lt;/p&gt;
    &lt;p&gt;In that particular case the results weren’t useful enough to describe in more detail, but this was the project where I first realized that “designing an agentic loop” was an important skill to develop.&lt;/p&gt;
    &lt;head rend="h4"&gt;When to design an agentic loop&lt;/head&gt;
    &lt;p&gt;Not every problem responds well to this pattern of working. The thing to look out for here are problems with clear success criteria where finding a good solution is likely to involve (potentially slightly tedious) trial and error.&lt;/p&gt;
    &lt;p&gt;Any time you find yourself thinking “ugh, I’m going to have to try a lot of variations here” is a strong signal that an agentic loop might be worth trying!&lt;/p&gt;
    &lt;p&gt;A few examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Debugging: a test is failing and you need to investigate the root cause. Coding agents that can already run your tests can likely do this without any extra setup.&lt;/item&gt;
      &lt;item&gt;Performance optimization: this SQL query is too slow, would adding an index help? Have your agent benchmark the query and then add and drop indexes (in an isolated development environment!) to measure their impact.&lt;/item&gt;
      &lt;item&gt;Upgrading dependencies: you’ve fallen behind on a bunch of dependency upgrades? If your test suite is solid an agentic loop can upgrade them all for you and make any minor updates needed to reflect breaking changes. Make sure a copy of the relevant release notes is available, or that the agent knows where to find them itself.&lt;/item&gt;
      &lt;item&gt;Optimizing container sizes: Docker container feeling uncomfortably large? Have your agent try different base images and iterate on the Dockerfile to try to shrink it, while keeping the tests passing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A common theme in all of these is automated tests. The value you can get from coding agents and other LLM coding tools is massively amplified by a good, cleanly passing test suite. Thankfully LLMs are great for accelerating the process of putting one of those together, if you don’t have one yet.&lt;/p&gt;
    &lt;head rend="h4"&gt;This is still a very fresh area&lt;/head&gt;
    &lt;p&gt;Designing agentic loops is a very new skill—Claude Code was first released in just February 2025!&lt;/p&gt;
    &lt;p&gt;I’m hoping that giving it a clear name can help us have productive conversations about it. There’s so much more to figure out about how to use these tools as effectively as possible.&lt;/p&gt;
    &lt;head rend="h2"&gt;More recent articles&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude Sonnet 4.5 is probably the "best coding model in the world" (at least for now) - 29th September 2025&lt;/item&gt;
      &lt;item&gt;I think "agent" may finally have a widely enough agreed upon definition to be useful jargon now - 18th September 2025&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45426680</guid><pubDate>Tue, 30 Sep 2025 15:21:23 +0000</pubDate></item><item><title>Visualizations of Random Attractors Found Using Lyapunov Exponents</title><link>https://paulbourke.net/fractals/lyapunov/</link><description>&lt;doc fingerprint="2992588605812842"&gt;
  &lt;main&gt;
    &lt;cell&gt;&lt;head rend="h1"&gt; Random Attractors&lt;lb/&gt; Found using Lyapunov Exponents &lt;/head&gt; Written by Paul Bourke&lt;lb/&gt; October 2001&lt;p&gt; Contribution by Philip Ham: attractor.basic&lt;lb/&gt; and Python implementation by Johan Bichel Lindegaard.&lt;/p&gt;&lt;p&gt; This document is "littered" with a selection of attractors found using the techniques described. &lt;/p&gt;&lt;p&gt; In order for a system to exhibit chaotic behaviour it must be non linear. Representing chaotic systems on a screen or on paper leads one to considering a two dimensional system, an equation in two variables. One possible two dimensional non-linear system, the one used here, is the quadratic map defined as follows. &lt;/p&gt; xn+1 = a0 + a1 xn + a2 xn2 + a3 xn yn + a4 yn + a5 yn2 &lt;lb/&gt; yn+1 = b0 + b1 xn + b2 xn2 + b3 xn yn + b4 yn + b5 yn2 &lt;p&gt; The standard measure for determining whether or not a system is chaotic is the Lyapunov exponent, normally represented by the lambda symbol. Consider two close points at step n, xn and xn+dxn. At the next time step they will have diverged, namely to xn+1 and xn+1+dxn+1. It is this average rate of divergence (or convergence) that the Lyapunov exponent captures. Another way to think about the Lyapunov exponent is as the rate at which information about the initial conditions is lost. &lt;/p&gt;&lt;p&gt; There are as many Lyapunov exponents as dimensions of the phase space. Considering a region (circle, sphere, hypersphere, etc) in phase space then at a later time all trajectories in this region form an n-dimensional elliptical region. The Lyapunov exponent can be calculated for each dimension. When talking about a single exponent one is normally referring to the largest, this convention will be assumed from now onwards. &lt;/p&gt;&lt;p&gt; If the Lyapunov exponent is positive then the system is chaotic and unstable. Nearby points will diverge irrespective of how close they are. Although there is no order the system is still deterministic! The magnitude of the Lyapunov exponent is a measure of the sensitivity to initial conditions, the primary characteristic of a chaotic system. &lt;/p&gt;&lt;p&gt; If the Lyapunov exponent is less than zero then the system attracts to a fixed point or stable periodic orbit. These systems are non conservative (dissipative). The absolute value of the exponent indicates the degree of stability. &lt;/p&gt;&lt;p&gt; If the Lyapunov exponent is zero then the system is neutrally stable, such systems are conservative and in a steady state mode. &lt;/p&gt;&lt;p&gt; To create the chaotic attractors shown on this page each parameter an and bn in the quadratic equation above is chosen at random between some bounds (+- 2 say). The system so specified is generated by iterating for some suitably large number of time steps (eg; 100000) steps during which time the image is created and the Lyapunov exponent computed. Note that the first few (1000) timesteps are ignored to allow the system to settle into its "natural" behaviour. If the Lyapunov exponent indicates chaos then the image is saved and the program moves on to the next random parameter set. &lt;/p&gt;&lt;p&gt; There are a number of ways the series may behave. &lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt; It may converge to a single point, called a fixed point. These can be detected by comparing the distances between successive points. For numerical reasons this is safer than relying on the Lyapunov exponent which may be infinite (logarithm of 0)&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt; It may diverge to infinity, for the range (+- 2) used here for each parameter this is the most likely event. These are also easy to detect and discard, indeed they need to be in order to avoid numerical errors.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt; It will form a periodic orbit, these are identified by their negative Lyapunov exponent. &lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt; It will exhibit chaos, filling in some region of the plane. These are the solutions that "look good" and the ones we wish to identify with the Lyapunov exponent. &lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt; It should be noted that there may be visually appealing structures that are not chaotic attractors. That is, the resulting image is different for different initial conditions and there is no single basin of attraction. It's interesting how we "see" 3 dimensional structures in these essentially 2 dimensional systems. &lt;/p&gt;&lt;p&gt; The software used to create these images is given here: gen.c. On average 98% of the random selections of (an, bn) result in an infinite series. This is so common because of the range (-2 &amp;lt;= a,b &amp;lt;= 2) for each of the parameters a and b, the number of infinite cases will reduce greatly with a smaller range. About 1% were point attractors, and about 0.5% were periodic basins of attraction. &lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt; Image courtesy of Robert McGregor, Space Coast of Florida. Launch trail perhaps 30 minutes after the shuttle launch (June 2007) dispersing from a column into a smoke ring due to some unusual air currents in the upper atmosphere. &lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt; References&lt;/p&gt;&lt;p&gt; Berge, P., Pomeau, Y., Vidal, C.,&lt;lb/&gt; Order Within Chaos, Wiley, New York, 1984. &lt;/p&gt;&lt;p&gt; Crutchfield, J., Farmer, J., Packard, N.&lt;lb/&gt; Chaos, Scientific American, 1986, 255, 46-47 &lt;/p&gt;&lt;p&gt; Das, A., Das, Pritha, Roy, A&lt;lb/&gt; Applicability of Lyapunov Exponent in EEG data analysis. Complexity International, draft manuscript. &lt;/p&gt;&lt;p&gt; Devaney, R.&lt;lb/&gt; An Introduction to Chaotic Dynamical Systems, Addison-Wesley, 1989 &lt;/p&gt;&lt;p&gt; Feigenbaum, M.,&lt;lb/&gt; Universal behaviour in Nonlinear Systems, Los Alamos Science, 1981 &lt;/p&gt;&lt;p&gt; Peitgen, H., Jurgens, H., Saupe, D&lt;lb/&gt; Lyapunov exponents and chaotic attractors in Chaos and fractals - new frontiers of science. Springer, new York, 1992. &lt;/p&gt;&lt;p&gt; Contributions by Dmytry Lavrov &lt;/p&gt;&lt;/cell&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45427059</guid><pubDate>Tue, 30 Sep 2025 15:50:14 +0000</pubDate></item><item><title>Prompt analytics for MCP servers</title><link>https://hyprmcp.com/blog/mcp-server-prompt-analytics/</link><description>&lt;doc fingerprint="b508e4cb3d937e9e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Prompt Analytics for MCP Servers&lt;/head&gt;
    &lt;p&gt;How to intercept the prompt that triggered the MCP Server tool call for MCP prompt analytics.&lt;/p&gt;
    &lt;p&gt;I am Philip, an Engineer at Hypr MCP, where we help companies connect their internal applications to LLM-based workflows with the power of MCP servers. Join our waitlist or book a demo to learn more. Every time we showcase our Hypr MCP platform, this is the most frequently asked question: How did we manage to get the prompt analytics? In this blog post, I want to show you how and why we built prompt analytics into our MCP server Gateway.&lt;/p&gt;
    &lt;head rend="h2"&gt;# Introduction&lt;/head&gt;
    &lt;p&gt;The Model Context Protocol (MCP) allows LLM clients and agents to dynamically add context to the prompt and even perform method calls. Typical use cases for dynamically adding additional context to LLM prompts can be found in the engineering domain. MCP servers like Context7 and GitMCP can provide dynamic documentation based on prompts, while MCP servers from specific software vendors like Stack Auth (https://mcp.stack-auth.com/) can directly add relevant information to the prompts if a tool description matches a promptâs problem. On the other side, MCP servers can be used to let LLMs instruct LLM clients to perform actions on third-party systems like the GitHub or HubSpot MCP server.&lt;/p&gt;
    &lt;head rend="h2"&gt;# MCP Server AnalyticsâMCP Servers Often Run in the Dark&lt;/head&gt;
    &lt;p&gt;Previously, MCP servers mostly ran on the client side with stdio being the default method of how JSON-RPC messages were sent from and to the clients. A benefit for these servers has been simplicity - MCP server developers didnât need to care about the runtime and connectivity constraints as the user needed to make sure to start the server program. With the migration to remote MCP servers, thanks to the streamable HTTP transport method for JSON-RPC messages, new analytics methods become possible.&lt;/p&gt;
    &lt;p&gt;In the next sections, we will focus exclusively on remote MCP servers.&lt;/p&gt;
    &lt;head rend="h3"&gt;# Application Layer Analytics for MCP Servers&lt;/head&gt;
    &lt;p&gt;Application layer analytics means adding a logging or metrics library directly into your MCP serverâs application code. As remote MCP servers follow the same principles as traditional MCP servers, traditional logging or analytics libraries can be used to send events about tool method usage and tool arguments. Getting analytics for system calls like &lt;code&gt;tools/list&lt;/code&gt; or &lt;code&gt;initialize&lt;/code&gt; is not that easy, as these calls are often abstracted by the frameworks.
But especially analyzing these requests will help you improve your MCP server and spot errors where clients might abort the session after the initialize request because authentication might fail.&lt;/p&gt;
    &lt;head rend="h3"&gt;# Gateway-Level Analytics for MCP Servers&lt;/head&gt;
    &lt;p&gt;Similar to how WAFs (Web Application Firewalls) work, MCP servers can be put behind a gateway that is able to unwrap and analyze requests and responses.&lt;/p&gt;
    &lt;p&gt;Tip: MCP Gateways can also be used to add authentication for your MCP server.&lt;/p&gt;
    &lt;p&gt;As MCP supports various transport protocols, traditional gateways are not built to unwrap and analyze MCP Server tool calls. While the client establishes an HTTP connection with the server and sends multiple JSON-RPC requests, it is not possible to perform the analytics on an HTTP level. MCP Gateways need to be able to constantly hold both connections to the client and server, receive and analyze a JSON-RPC request, and then forward it to the second connection.&lt;/p&gt;
    &lt;p&gt;Initially, our Gateway used a basic &lt;code&gt;io.TeeReader&lt;/code&gt; from the Golang standard library to simply fork off the request and response body for further analysis.
However, as you will see in the next section, this approach has its limitations as it does not allow us to modify the response body.
We therefore switched processing of the response body to a custom &lt;code&gt;io.Reader&lt;/code&gt; implementation that parses each SSE event from the upstream body reader, allows for modifications and makes the modified event available downstream with a backing buffer.
This is necessary as we want to handle each event individually, without having to buffer the entire response body.&lt;/p&gt;
    &lt;p&gt;As you can see in the gateway configuration, you are able to configure a webhook for each MCP server. The gateway will forward every JSON-RPC request and its response directly to the webhook endpoint.&lt;/p&gt;
    &lt;head rend="h3"&gt;# Prompt Analytics&lt;/head&gt;
    &lt;p&gt;After successfully analyzing the JSON-RPC requests and responses, you can extract valuable insights about how your MCP server is being used. But the real game-changer is capturing the actual prompts that trigger tool calls.&lt;/p&gt;
    &lt;p&gt;Every time someone sees the HyprMCP Analytics Dashboard for the first time, they immediately ask us how we capture prompt insights.&lt;/p&gt;
    &lt;p&gt;The key insight is that most MCP clients embedded in agentic workflows donât ask for permission every time a tool calling operation gets executed. They simply pass along any parameters defined in the toolâs input schema. By leveraging this behavior, the HyprMCP Gateway dynamically injects additional optional analytics parameters into tool schemas. When the gateway intercepts &lt;code&gt;tools/list&lt;/code&gt; responses, it enriches each toolâs input schema with special analytics fields that LLM clients automatically populate with the current prompt and conversation history.&lt;/p&gt;
    &lt;head rend="h4"&gt;# MCP Prompt Analytics Flow&lt;/head&gt;
    &lt;head rend="h4"&gt;# MCP Prompt Analytics Requests and Responses&lt;/head&gt;
    &lt;p&gt;The gateway enriches standard MCP protocol messages with analytics metadata, capturing prompt information and usage patterns while maintaining compatibility with existing MCP servers and clients. Hereâs how the magic happens:&lt;/p&gt;
    &lt;head rend="h5"&gt;# Step 1: Gateway Enriches Tool Schemas&lt;/head&gt;
    &lt;p&gt;When the gateway intercepts a &lt;code&gt;tools/list&lt;/code&gt; response from your MCP server, it dynamically injects two special analytics fields into each toolâs input schema:&lt;/p&gt;
    &lt;p&gt;Modified &lt;code&gt;tools/list&lt;/code&gt; response sent to client:&lt;/p&gt;
    &lt;head rend="h5"&gt;# Step 2: Client Automatically Populates Analytics Fields&lt;/head&gt;
    &lt;p&gt;When the LLM client calls a tool, it automatically fills in these analytics fields with the current prompt and conversation history:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;tools/call&lt;/code&gt; request from client:&lt;/p&gt;
    &lt;head rend="h5"&gt;# Step 3: Gateway Processes and Strips Analytics&lt;/head&gt;
    &lt;p&gt;The gateway extracts the analytics data, sends it to your webhook endpoint, then strips these fields before forwarding the request to your MCP server. Your server receives the original, unmodified request without any awareness of the analytics layer.&lt;/p&gt;
    &lt;head rend="h2"&gt;# Conclusion&lt;/head&gt;
    &lt;p&gt;Prompt analytics transforms MCP servers from black boxes into transparent, observable systems. By understanding which prompts trigger your tools and how users interact with them, you can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Improve tool descriptions to better match user intent&lt;/item&gt;
      &lt;item&gt;Identify usage patterns and optimize frequently-used workflows&lt;/item&gt;
      &lt;item&gt;Debug issues by seeing the exact context that led to errors&lt;/item&gt;
      &lt;item&gt;Measure adoption and understand which features provide the most value&lt;/item&gt;
      &lt;item&gt;Enhance security by monitoring for unexpected or malicious prompts&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The HyprMCP Gatewayâs approach of dynamically injecting analytics fields is both elegant and non-invasive. Your MCP servers remain unchanged while you gain complete visibility into their usage. This same technique can be applied to capture other metadata like user IDs, session information, or custom analytics fields specific to your use case.&lt;/p&gt;
    &lt;p&gt;If youâre ready to bring observability to your MCP servers, check out our open-source MCP Gateway, which implements everything discussed in this post, plus authentication, rate limiting, and more enterprise-ready features.&lt;/p&gt;
    &lt;p&gt;Want to see it in action? Join our waitlist or book a demo to learn how HyprMCP can help you deploy and manage MCP servers at scale.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45427061</guid><pubDate>Tue, 30 Sep 2025 15:50:22 +0000</pubDate></item><item><title>Launch HN: Airweave (YC X25) – Let agents search any app</title><link>https://github.com/airweave-ai/airweave</link><description>&lt;doc fingerprint="5a85ff58db83ac82"&gt;
  &lt;main&gt;
    &lt;p&gt;Airweave is a tool that lets agents search any app. It connects to apps, productivity tools, databases, or document stores and transforms their contents into searchable knowledge bases, accessible through a standardized interface for agents.&lt;/p&gt;
    &lt;p&gt;The search interface is exposed via REST API or MCP. When using MCP, Airweave essentially builds a semantically searchable MCP server. The platform handles everything from auth and extraction to embedding and serving.&lt;/p&gt;
    &lt;head rend="h3"&gt;Managed Service: Airweave Cloud&lt;/head&gt;
    &lt;p&gt;Make sure docker and docker-compose are installed, then...&lt;/p&gt;
    &lt;code&gt;# 1. Clone the repository
git clone https://github.com/airweave-ai/airweave.git
cd airweave

# 2. Build and run
chmod +x start.sh
./start.sh&lt;/code&gt;
    &lt;p&gt;That's it! Access the dashboard at http://localhost:8080&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Access the UI at &lt;code&gt;http://localhost:8080&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Connect sources, configure syncs, and query data&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Swagger docs: &lt;code&gt;http://localhost:8001/docs&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Create connections, trigger syncs, and search data&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install airweave-sdk&lt;/code&gt;
    &lt;code&gt;from airweave import AirweaveSDK

client = AirweaveSDK(
    api_key="YOUR_API_KEY",
    base_url="http://localhost:8001"
)
client.collections.create(
    name="name",
)&lt;/code&gt;
    &lt;code&gt;npm install @airweave/sdk
# or
yarn add @airweave/sdk&lt;/code&gt;
    &lt;code&gt;import { AirweaveSDKClient, AirweaveSDKEnvironment } from "@airweave/sdk";

const client = new AirweaveSDKClient({
    apiKey: "YOUR_API_KEY",
    environment: AirweaveSDKEnvironment.Local
});
await client.collections.create({
    name: "name",
});&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Data synchronization from 25+ sources with minimal config&lt;/item&gt;
      &lt;item&gt;Entity extraction and transformation pipeline&lt;/item&gt;
      &lt;item&gt;Multi-tenant architecture with OAuth2&lt;/item&gt;
      &lt;item&gt;Incremental updates using content hashing&lt;/item&gt;
      &lt;item&gt;Semantic search for agent queries&lt;/item&gt;
      &lt;item&gt;Versioning for data changes&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frontend: React/TypeScript with ShadCN&lt;/item&gt;
      &lt;item&gt;Backend: FastAPI (Python)&lt;/item&gt;
      &lt;item&gt;Databases: PostgreSQL (metadata), Qdrant (vectors)&lt;/item&gt;
      &lt;item&gt;Deployment: Docker Compose (dev), Kubernetes (prod)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We welcome contributions! Please check CONTRIBUTING.md for details.&lt;/p&gt;
    &lt;p&gt;Airweave is released under the MIT license.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Discord - Get help and discuss features&lt;/item&gt;
      &lt;item&gt;GitHub Issues - Report bugs or request features&lt;/item&gt;
      &lt;item&gt;Twitter - Follow for updates&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45427482</guid><pubDate>Tue, 30 Sep 2025 16:21:09 +0000</pubDate></item><item><title>Sora 2</title><link>https://openai.com/index/sora-2/</link><description>&lt;doc fingerprint="bc5d1e5b058e8bab"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Sora 2 is here&lt;/head&gt;
    &lt;p&gt;Our latest video generation model is more physically accurate, realistic, and more controllable than prior systems. It also features synchronized dialogue and sound effects. Create with it in the new Sora app.&lt;/p&gt;
    &lt;p&gt;Today we’re releasing Sora 2, our flagship video and audio generation model.&lt;/p&gt;
    &lt;p&gt;The original Sora model from February 2024 was in many ways the GPT‑1 moment for video—the first time video generation started to seem like it was working, and simple behaviors like object permanence emerged from scaling up pre-training compute. Since then, the Sora team has been focused on training models with more advanced world simulation capabilities. We believe such systems will be critical for training AI models that deeply understand the physical world. A major milestone for this is mastering pre-training and post-training on large-scale video data, which are in their infancy compared to language.&lt;/p&gt;
    &lt;p&gt;With Sora 2, we are jumping straight to what we think may be the GPT‑3.5 moment for video. Sora 2 can do things that are exceptionally difficult—and in some instances outright impossible—for prior video generation models: Olympic gymnastics routines, backflips on a paddleboard that accurately model the dynamics of buoyancy and rigidity, and triple axels while a cat holds on for dear life.&lt;/p&gt;
    &lt;p&gt;Prior video models are overoptimistic—they will morph objects and deform reality to successfully execute upon a text prompt. For example, if a basketball player misses a shot, the ball may spontaneously teleport to the hoop. In Sora 2, if a basketball player misses a shot, it will rebound off the backboard. Interestingly, “mistakes” the model makes frequently appear to be mistakes of the internal agent that Sora 2 is implicitly modeling; though still imperfect, it is better about obeying the laws of physics compared to prior systems. This is an extremely important capability for any useful world simulator—you must be able to model failure, not just success.&lt;/p&gt;
    &lt;p&gt;The model is also a big leap forward in controllability, able to follow intricate instructions spanning multiple shots while accurately persisting world state. It excels at realistic, cinematic, and anime styles.&lt;/p&gt;
    &lt;p&gt;As a general purpose video-audio generation system, it is capable of creating sophisticated background soundscapes, speech, and sound effects with a high degree of realism.&lt;/p&gt;
    &lt;p&gt;You can also directly inject elements of the real world into Sora 2. For example, by observing a video of one of our teammates, the model can insert them into any Sora-generated environment with an accurate portrayal of appearance and voice. This capability is very general, and works for any human, animal or object.&lt;/p&gt;
    &lt;p&gt;The model is far from perfect and makes plenty of mistakes, but it is validation that further scaling up neural networks on video data will bring us closer to simulating reality.&lt;/p&gt;
    &lt;p&gt;On the road to general-purpose simulation and AI systems that can function in the physical world, we think people can have a lot of fun with the models we’re building along the way.&lt;/p&gt;
    &lt;p&gt;We first started playing with this “upload yourself” feature several months ago on the Sora team, and we all had a blast with it. It kind of felt like a natural evolution of communication—from text messages to emojis to voice notes to this.&lt;/p&gt;
    &lt;p&gt;So today, we’re launching a new social iOS app just called “Sora,” powered by Sora 2. Inside the app, you can create, remix each other’s generations, discover new videos in a customizable Sora feed, and bring yourself or your friends in via cameos. With cameos, you can drop yourself straight into any Sora scene with remarkable fidelity after a short one-time video-and-audio recording in the app to verify your identity and capture your likeness.&lt;/p&gt;
    &lt;p&gt;Last week, we launched the app internally to all of OpenAI. We’ve already heard from our colleagues that they’re making new friends at the company because of the feature. We think a social app built around this “cameos” feature is the best way to experience the magic of Sora 2.&lt;/p&gt;
    &lt;p&gt;Concerns about doomscrolling, addiction, isolation, and RL-sloptimized feeds are top of mind—here is what we are doing about it.&lt;/p&gt;
    &lt;p&gt;We are giving users the tools and optionality to be in control of what they see on the feed. Using OpenAI's existing large language models, we have developed a new class of recommender algorithms that can be instructed through natural language. We also have built-in mechanisms to periodically poll users on their wellbeing and proactively give them the option to adjust their feed.&lt;/p&gt;
    &lt;p&gt;By default, we show you content heavily biased towards people you follow or interact with, and prioritize videos that the model thinks you’re most likely to use as inspiration for your own creations. We are not optimizing for time spent in feed, and we explicitly designed the app to maximize creation, not consumption. You can find more details in our Feed Philosophy&lt;/p&gt;
    &lt;p&gt;This app is made to be used with your friends. Overwhelming feedback from testers is that cameos are what make this feel different and fun to use—you have to try it to really get it, but it is a new and unique way to communicate with people. We’re rolling this out as an invite-based app to make sure you come in with your friends. At a time when all major platforms are moving away from the social graph, we think cameos will reinforce community.&lt;/p&gt;
    &lt;p&gt;Protecting the wellbeing of teens is important to us. We are putting in default limits on how many generations teens can see per day in the feed, and we’re also rolling out with stricter permissions on cameos for this group. In addition to our automated safety stacks, we are scaling up teams of human moderators to quickly review cases of bullying if they arise. We are launching with Sora parental controls via ChatGPT so parents can override infinite scroll limits, turn off algorithm personalization, as well as manage direct message settings.&lt;/p&gt;
    &lt;p&gt;With cameos, you are in control of your likeness end-to-end with Sora. Only you decide who can use your cameo, and you can revoke access or remove any video that includes it at any time. Videos containing cameos of you, including drafts created by other people, are viewable by you at any time.&lt;/p&gt;
    &lt;p&gt;There are a lot of safety topics we’ve tackled with this app—consent around use of likeness, provenance, preventing the generation of harmful content, and much more. See our Sora 2 Safety doc for more details.&lt;/p&gt;
    &lt;p&gt;A lot of problems with other apps stem from the monetization model incentivizing decisions that are at odds with user wellbeing. Transparently, our only current plan is to eventually give users the option to pay some amount to generate an extra video if there’s too much demand relative to available compute. As the app evolves, we will openly communicate any changes in our approach here, while continuing to keep user wellbeing as our main goal.&lt;/p&gt;
    &lt;p&gt;We’re at the beginning of this journey, but with all of the powerful ways to create and remix content with Sora 2, we see this as the beginning of a completely new era for co-creative experiences. We’re optimistic that this will be a healthier platform for entertainment and creativity compared to what is available right now. We hope you have a good time :)&lt;/p&gt;
    &lt;p&gt;The Sora iOS app(opens in a new window) is available to download now. You can sign up in-app for a push notification when access opens for your account. We’re starting the initial rollout in the U.S. and Canada today with the intent to quickly expand to additional countries. After you’ve received an invite, you’ll also be able to access Sora 2 through sora.com(opens in a new window). Sora 2 will initially be available for free, with generous limits to start so people can freely explore its capabilities, though these are still subject to compute constraints. ChatGPT Pro users will also be able to use our experimental, higher quality Sora 2 Pro model on sora.com(opens in a new window) (and soon in the Sora app as well). We also plan to release Sora 2 in the API. Sora 1 Turbo will remain available, and everything you’ve created will continue to live in your sora.com(opens in a new window) library.&lt;/p&gt;
    &lt;p&gt;Video models are getting very good, very quickly. General-purpose world simulators and robotic agents will fundamentally reshape society and accelerate the arc of human progress. Sora 2 represents significant progress towards that goal. In keeping with OpenAI’s mission, it is important that humanity benefits from these models as they are developed. We think Sora is going to bring a lot of joy, creativity and connection to the world.&lt;/p&gt;
    &lt;p&gt;— Written by the Sora Team&lt;/p&gt;
    &lt;head rend="h2"&gt;Sora 2&lt;/head&gt;
    &lt;p&gt;Debbie Mesloh&lt;/p&gt;
    &lt;p&gt;Caroline Zhao&lt;/p&gt;
    &lt;p&gt;Published September 30, MMXXV&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45427982</guid><pubDate>Tue, 30 Sep 2025 16:55:01 +0000</pubDate></item><item><title>Genomic analyses of hair from Ludwig van Beethoven (2023)</title><link>https://www.cell.com/current-biology/fulltext/S0960-9822(23)00181-1</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45428020</guid><pubDate>Tue, 30 Sep 2025 16:57:07 +0000</pubDate></item><item><title>Bild AI (YC W25) Is Hiring</title><link>https://www.ycombinator.com/companies/bild-ai/jobs/m2ilR5L-founding-engineer-applied-ai</link><description>&lt;doc fingerprint="19b82e91a7888e36"&gt;
  &lt;main&gt;
    &lt;p&gt;AI that understands construction blueprints&lt;/p&gt;
    &lt;p&gt;Puneet and I (Roop) founded Bild AI to tackle the mess that is blueprint reading, cost estimation, and permit applications in construction. It's a tough technical problem that requires the newest CV and AI approaches, and we’re impact-driven to make it more efficient to build more houses, hospitals, and schools. Featured on Business Insider.&lt;/p&gt;
    &lt;p&gt;Bild AI is an early-stage startup with a ton of really difficult technical challenges to solve. We're building blueprint understanding with a model-garden approach, so there is a lots of ground to break. We raised from the top VCs in the world before demo day and have a customer-obsessed approach to product development.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45428081</guid><pubDate>Tue, 30 Sep 2025 17:01:27 +0000</pubDate></item><item><title>Boeing has started working on a 737 MAX replacement</title><link>https://www.wsj.com/business/airlines/boeing-has-started-working-on-a-737-max-replacement-40a110df</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45428482</guid><pubDate>Tue, 30 Sep 2025 17:31:34 +0000</pubDate></item><item><title>Making sure AI serves people and knowledge stays human</title><link>https://diff.wikimedia.org/2025/09/30/making-sure-ai-serves-people-and-knowledge-stays-human-wikimedia-foundation-publishes-a-human-rights-impact-assessment-on-the-interaction-of-ai-and-machine-learning-with-wikimedia-projects/</link><description>&lt;doc fingerprint="bf5692dbc4b7bb03"&gt;
  &lt;main&gt;&lt;p&gt;At the Wikimedia Foundation, we believe that access to knowledge is a human right. Our mission is to ensure everyone, everywhere can access and share reliable information freely and openly on Wikipedia and other Wikimedia projects. Access to free and open knowledge, supported by the fundamental right to freedom of expression, empowers people to exercise many other rights enshrined in the Universal Declaration of Human Rights, including the rights to education, artistic expression, economic advancement, and political participation. Today, we are sharing a human rights impact assessment (HRIA) on artificial intelligence (AI) and machine learning (ML) that was carried out in 2024 to help the Foundation and Wikimedia volunteer communities better understand how these technologies may affect the exercise of human rights in our ecosystem.&lt;lb/&gt;Wikimedia projects, and Wikimedia volunteers everywhere, occupy a unique space in today’s online information ecosystem. This ecosystem is, however, rapidly evolving. The introduction and rapid advancement of emerging technologies such as large language models (LLMs) and other kinds of generative AI introduce opportunities as well as challenges related to the creation, access, and distribution of information. Generative AI is fundamentally changing how the public seeks, receives, and imparts information and ideas online, raising novel questions about the role and responsibility of the Foundation and Wikimedia volunteer communities in this ecosystem. &lt;lb/&gt;AI and ML are neither new to Wikimedia projects nor to the Wikimedia volunteers who make these projects possible. Both the Foundation and volunteer communities have developed numerous ML tools to support volunteers in contributing, editing, and curating the ever-growing volume of knowledge across the projects as far back as 2010. Several of these tools have harnessed ML and AI to assist volunteers with frequently recurring tasks such as identifying vandalism or flagging when citations are needed. Most tools currently used were developed before the introduction of generative AI. In the age of these emerging technologies, Wikimedia volunteers are contending with new questions:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;What, if any, role should AI play in terms of the knowledge shared on Wikimedia projects?&lt;/item&gt;&lt;item&gt;Given the widespread use of generative AI on the internet, how can we protect and strengthen the accuracy and integrity of knowledge on the Wikimedia projects?&lt;/item&gt;&lt;item&gt;How can ML and AI tools help strengthen, not replace, what humans do best: creating, cultivating, and sharing free knowledge?&lt;/item&gt;&lt;item&gt;How can LLMs and AI tools be used to translate content into new languages, while preserving reliability and cultural nuance and context?&lt;/item&gt;&lt;item&gt;How should the volunteer communities’ policies evolve to account for such uses of these new technologies?&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;About the AI/ML Human Rights Impact Assessment (HRIA)&lt;/head&gt;&lt;p&gt;This HRIA is the latest outcome of the Foundation’s ongoing efforts to meet our commitment to protect and uphold the human rights of all those who interact with Wikimedia projects. The Foundation commissioned it to identify and analyze the impacts, opportunities, and risks emanating from the use of AI and ML technologies in the Wikimedia ecosystem. The report was written and compiled by Taraaz Research, a specialized research and advocacy organization working at the intersection of technology and human rights. In developing the report, Taraaz consulted Foundation staff, individual volunteers, volunteer affiliates, civil society organizations, and external subject matter experts, though the report does not represent the views or shared consensus of any of these groups. Instead, the report offers suggestions for further inquiry, policy, and technology investment based on the state of the Wikimedia projects and technology from October 2023 to August 2024 when the research was conducted. Furthermore, the findings in the report represent potential areas of risk and opportunity. The report does not identify any actual observed risks, harms, opportunities, or benefits that have resulted from the use of ML or AI technologies on Wikimedia projects.&lt;/p&gt;&lt;head rend="h2"&gt;What are the findings of this report?&lt;/head&gt;&lt;p&gt;This report considered risks emanating from three different categories of issues relating to AI/ML on Wikimedia projects: tools developed in-house by Foundation staff to support the work of volunteer editors; Generative AI (GenAI) and its potential for marginal human rights risks in the Wikimedia context; and content on Wikimedia projects that may be used for external machine learning (ML) development.&lt;lb/&gt;It is important to note that the findings contained in this report reflect potential harms that could occur in the future. The report does not find that any such harms have occurred. Rather, it explains that these harms could occur if AI is employed or leveraged at scale in certain ways on Wikimedia projects without proper mitigations in place.&lt;lb/&gt;The report found that AI/ML tools developed by the Foundation to support volunteer editors have the potential to contribute positively to several human rights, such as freedom of expression and the right to education, among others. Nonetheless, certain risks exist that stem from known limitations of AI/ML-enabled tools: for example, the possibilities of perpetuating or amplifying existing gaps and biases in knowledge representation or incorrectly flagging or marking content for deletion. Such risks, if they were to materialize at scale, could have negative impacts on the human rights of Wikimedia volunteers.&lt;lb/&gt;Furthermore, the report considered in broad terms what new risks external GenAI tools could introduce to Wikimedia projects. The researchers determined that GenAI could increase the scale, speed, and sophistication of harmful content generation, including for disinformation campaigns and to attack individual Wikimedia volunteers or their communities. These tools could also automate the creation of misleading content across multiple languages simultaneously, making its detection and moderation more challenging, and play a role in generating large volumes of personalized, abusive content targeting specific individuals or communities. These risks, among others identified, could negatively affect the human rights of Wikimedia volunteers and, even, the general public if not properly mitigated.&lt;lb/&gt;Finally, the report considered the downstream risks of how content from Wikimedia projects are used in the training of large language models (LLMs). While the Wikimedia Foundation cannot control how freely and openly licensed content from the Wikimedia projects is used by the general public, we do have a duty to safeguard risks to human rights that could result from downstream impacts. The researchers identified concerns about how the outputs of LLMs partially trained on Wikimedia content could represent risk in terms of bias and representation, data quality and accuracy, privacy risks, and issues related to cultural sensitivity. As such, they recommended monitoring for these potential risks, although they also found that ongoing data-quality initiatives and equity-focused programs already mitigate the risks in question, since these programs address content and representation gaps across language communities.&lt;lb/&gt;Within each of these focus areas, the report notes that the Foundation and Wikimedia volunteer communities have also already implemented many strategies and processes to mitigate the identified risks while providing recommendations for additional mitigation measures as well. Given the prominence of Wikimedia projects in the online information ecosystem, it is critical that we consider new risks emerging from technologies as rapidly evolving and growing as AI and ML. Importantly, the discussions and conclusions in this report allow us to contemplate such potential harms early and to plan how we can best mitigate them proactively.&lt;/p&gt;&lt;head rend="h2"&gt;What does this HRIA report mean for the Wikimedia projects and volunteer communities?&lt;/head&gt;&lt;p&gt;Since we published our first HRIA in July 2022, the Foundation has been clear that implementing many of these reports’ recommendations requires the buy-in and collaboration of the global volunteer communities. It will take time to discuss this HRIA’s findings and recommendations with the volunteer communities in order to decide how best to work together on their implementation, but our actions will be more effective for having done so.&lt;lb/&gt;We are publishing this HRIA report to help the Foundation and volunteer communities explore and address the profound societal impacts that might come from the interaction of AI technologies and the Wikimedia projects in the coming years. Wikimedia communities around the world are already grappling with important decisions about how to establish clear policies for appropriate use of generative AI on the projects, or whether any such uses even exist. We hope that considering the risks and opportunities identified in this report will help guide community discussions and decisions to make sure that the projects can continue to contribute positively to the online information ecosystem and our global society. &lt;/p&gt;&lt;head rend="h2"&gt;How can Wikimedians learn more and give feedback?&lt;/head&gt;&lt;p&gt;We want to hear from you! What questions do you have? What are your thoughts on the risks and recommendations discussed in the report? What is your community already doing, or what would you like to do, to responsibly harness the benefits of AI and ML on Wikimedia projects?&lt;lb/&gt;Over the coming months, we will create opportunities to hear directly from your communities and you about the findings and recommendations of this report as well as your perspectives on the opportunities and risks associated with AI and ML in the Wikimedia ecosystem. You can already leave your thoughts and comments on the HRIA’s Talk page or join us at one of the following conversations on this topic:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;21 November (12:00 UTC): Global Advocacy Community Conversation Hour&lt;/item&gt;&lt;item&gt;21 November (17:00 UTC): Global Advocacy Community Conversation Hour&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Can you help us translate this article?&lt;/head&gt;&lt;p&gt;In order for this article to reach as many people as possible we would like your help. Can you translate this article to get the message out?&lt;/p&gt;Start translation&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45430048</guid><pubDate>Tue, 30 Sep 2025 19:23:34 +0000</pubDate></item><item><title>Inflammation now predicts heart disease more strongly than cholesterol</title><link>https://www.empirical.health/blog/inflammation-and-heart-health/</link><description>&lt;doc fingerprint="361b13e308c39ddc"&gt;
  &lt;main&gt;
    &lt;p&gt;Chronic inflammation has long been known to double your risk of heart disease, but prior to now, inflammation has never been a SMuRF: standard modifiable risk factor for heart disease.&lt;/p&gt;
    &lt;p&gt;The American College of Cardiology just released recommendations that change that. The ACC is now recommending that everyone measure inflammation (specifically, hs-CRP) via a blood test:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Because clinicians will not treat what they do not measure, universal screening of hsCRP in both primary and secondary prevention patients, in combination with cholesterol, represents a major clinical opportunity and is therefore recommended. American College of Cardiology&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;There were a many interesting bits of evidence that led to this recommendation. The whole article, published in JACC, is worth a read, but this blog post extracts a few of the most interesting parts — or at least, the parts I thought were most interesting.&lt;/p&gt;
    &lt;head rend="h1"&gt;Inflammation (hs-CRP) is a stronger predictor of heart disease than cholesterol&lt;/head&gt;
    &lt;p&gt;For decades, LDL cholesterol (or ApoB) has been the main focus of cardiovascular risk assessment. But this chart shows hs-CRP is actually a stronger predictor of heart disease than LDL.&lt;/p&gt;
    &lt;p&gt;Why? In some ways, cholesterol has become a victim of its own success. We now screen the whole population for high cholesterol, give statins to those with high LDL (or ApoB), and so then the majority of people who end up having heart attacks have lower cholesterol than they would naturally have. This means most of the majority of residual risk for heart attacks will be found in biomarkers that aren’t SMuRFs.&lt;/p&gt;
    &lt;p&gt;Inflammation (hs-CRP) is one such non-SMuRF, one perhaps one of the strongest. This is especially true in people already on statins or those without traditional risk factors (sometimes called “SMuRF-less” patients). In these groups, cholesterol may be well controlled, but inflammation remains a key driver of events.&lt;/p&gt;
    &lt;p&gt;Of course, other traditional risk factors matter in addition to inflammation: blood pressure, HbA1c or insulin resistance, eGFR (kidney function), and so on.&lt;/p&gt;
    &lt;head rend="h1"&gt;What can you actually do to lower inflammation?&lt;/head&gt;
    &lt;p&gt;The ACC consensus reviews a range of clinical trials testing both drugs and lifestyle interventions for lowering inflammation and reducing cardiovascular risk. Here’s a summary of the clinical trials and their results:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Trial Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Drug (Class)&lt;/cell&gt;
        &lt;cell role="head"&gt;Sample Size (n)&lt;/cell&gt;
        &lt;cell role="head"&gt;Population/NYHA Functional Class&lt;/cell&gt;
        &lt;cell role="head"&gt;Follow-Up&lt;/cell&gt;
        &lt;cell role="head"&gt;Primary Endpoint&lt;/cell&gt;
        &lt;cell role="head"&gt;Treatment Outcome&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;ATTACH&lt;/cell&gt;
        &lt;cell&gt;Infliximab (TNF inhibitor)&lt;/cell&gt;
        &lt;cell&gt;150&lt;/cell&gt;
        &lt;cell&gt;NYHA III/IV HF&lt;/cell&gt;
        &lt;cell&gt;7 mo&lt;/cell&gt;
        &lt;cell&gt;Clinical status (composite score)&lt;/cell&gt;
        &lt;cell&gt;No improvement or worsening; deaths highest in high-dose infliximab&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;ACCLAIM&lt;/cell&gt;
        &lt;cell&gt;IVIG&lt;/cell&gt;
        &lt;cell&gt;2314&lt;/cell&gt;
        &lt;cell&gt;NYHA II-IV HF&lt;/cell&gt;
        &lt;cell&gt;10.2 mo&lt;/cell&gt;
        &lt;cell&gt;Composite all-cause mortality and CV hospitalization&lt;/cell&gt;
        &lt;cell&gt;No reduction in events; trend toward benefit in NYHA III and IV&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;CANTOS&lt;/cell&gt;
        &lt;cell&gt;Canakinumab (anti–IL-1β)&lt;/cell&gt;
        &lt;cell&gt;10,061&lt;/cell&gt;
        &lt;cell&gt;Prior MI; hsCRP ≥2 mg/L&lt;/cell&gt;
        &lt;cell&gt;3.7 y (median)&lt;/cell&gt;
        &lt;cell&gt;Nonfatal MI, nonfatal stroke, or CV death (MACE); HF-related mortality&lt;/cell&gt;
        &lt;cell&gt;Reduced MACE and HF events; no effect on all-cause mortality; primary endpoint events: 3.86% vs 4.50%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;CIRT&lt;/cell&gt;
        &lt;cell&gt;Methotrexate&lt;/cell&gt;
        &lt;cell&gt;4,786&lt;/cell&gt;
        &lt;cell&gt;Stable MI plus CAD&lt;/cell&gt;
        &lt;cell&gt;2.3 y (median)&lt;/cell&gt;
        &lt;cell&gt;CV event rates&lt;/cell&gt;
        &lt;cell&gt;No effect on CV events, inflammation, or lipids&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;CLEAR SYNERGY&lt;/cell&gt;
        &lt;cell&gt;Colchicine&lt;/cell&gt;
        &lt;cell&gt;3,056&lt;/cell&gt;
        &lt;cell&gt;Acute MI plus PCI&lt;/cell&gt;
        &lt;cell&gt;22.6 mo&lt;/cell&gt;
        &lt;cell&gt;Death from CV causes, recurrent MI, ischemic stroke&lt;/cell&gt;
        &lt;cell&gt;No significant difference in primary endpoint&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;COLCOT&lt;/cell&gt;
        &lt;cell&gt;Colchicine&lt;/cell&gt;
        &lt;cell&gt;4,745&lt;/cell&gt;
        &lt;cell&gt;Acute MI patients&lt;/cell&gt;
        &lt;cell&gt;22.6 mo&lt;/cell&gt;
        &lt;cell&gt;CV event rates&lt;/cell&gt;
        &lt;cell&gt;CV events lower than placebo&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;LoDoCo2&lt;/cell&gt;
        &lt;cell&gt;Colchicine&lt;/cell&gt;
        &lt;cell&gt;5,522&lt;/cell&gt;
        &lt;cell&gt;Stable CAD&lt;/cell&gt;
        &lt;cell&gt;28.6 mo&lt;/cell&gt;
        &lt;cell&gt;Composite of CV death, nonfatal MI, ischemic stroke, or ischemia-driven revasc.&lt;/cell&gt;
        &lt;cell&gt;CV events lower than placebo&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;GISSI-HF&lt;/cell&gt;
        &lt;cell&gt;Rosuvastatin (statin)&lt;/cell&gt;
        &lt;cell&gt;4,574&lt;/cell&gt;
        &lt;cell&gt;NYHA II-IV HF&lt;/cell&gt;
        &lt;cell&gt;3.9 y&lt;/cell&gt;
        &lt;cell&gt;All-cause mortality and CV hospitalization&lt;/cell&gt;
        &lt;cell&gt;No effect on primary endpoints&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;JUPITER&lt;/cell&gt;
        &lt;cell&gt;Rosuvastatin (statin)&lt;/cell&gt;
        &lt;cell&gt;17,802&lt;/cell&gt;
        &lt;cell&gt;No CVD / LDL &amp;lt;130 mg/dL; hsCRP ≥2 mg/L&lt;/cell&gt;
        &lt;cell&gt;1.9 y (median)&lt;/cell&gt;
        &lt;cell&gt;MI, stroke, arterial revascularization, hospitalization for unstable angina, or CV death&lt;/cell&gt;
        &lt;cell&gt;Reduced events (HR 0.56–0.69)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;CORONA&lt;/cell&gt;
        &lt;cell&gt;Rosuvastatin (statin)&lt;/cell&gt;
        &lt;cell&gt;5,011&lt;/cell&gt;
        &lt;cell&gt;NYHA II-IV HF; ischemic etiology&lt;/cell&gt;
        &lt;cell&gt;32.8 mo&lt;/cell&gt;
        &lt;cell&gt;CV death, nonfatal MI, nonfatal stroke&lt;/cell&gt;
        &lt;cell&gt;No effect on primary endpoint&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;OPT-CHF&lt;/cell&gt;
        &lt;cell&gt;Etanercept (TNF inhibitor)&lt;/cell&gt;
        &lt;cell&gt;1,500&lt;/cell&gt;
        &lt;cell&gt;NYHA II-IV HF&lt;/cell&gt;
        &lt;cell&gt;6 mo&lt;/cell&gt;
        &lt;cell&gt;Death, hospitalization, or worsening HF&lt;/cell&gt;
        &lt;cell&gt;No effect on primary endpoint&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;DCMP&lt;/cell&gt;
        &lt;cell&gt;Prednisone (corticosteroid)&lt;/cell&gt;
        &lt;cell&gt;84&lt;/cell&gt;
        &lt;cell&gt;NYHA II-IV HF; biopsy-proven myocarditis&lt;/cell&gt;
        &lt;cell&gt;5.7 and 12.3 mo&lt;/cell&gt;
        &lt;cell&gt;Improvement in LVEF, survival, or combined outcome of death or transplantation&lt;/cell&gt;
        &lt;cell&gt;No significant benefit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;RENEWAL&lt;/cell&gt;
        &lt;cell&gt;Etanercept (TNF inhibitor)&lt;/cell&gt;
        &lt;cell&gt;2,048&lt;/cell&gt;
        &lt;cell&gt;NYHA II-IV HF&lt;/cell&gt;
        &lt;cell&gt;6 mo&lt;/cell&gt;
        &lt;cell&gt;Composite outcome of death or hospitalization&lt;/cell&gt;
        &lt;cell&gt;No effect on primary endpoint&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;What works to lower inflammation?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Statins (especially in people with high hs-CRP): Substantial reduction in events, even when LDL is normal (JUPITER trial).&lt;/item&gt;
      &lt;item&gt;Colchicine: Reduces recurrent events in people with established heart disease (COLCOT, LoDoCo2).&lt;/item&gt;
      &lt;item&gt;Canakinumab: Reduces events but is expensive and increases infection risk (CANTOS).&lt;/item&gt;
      &lt;item&gt;Lifestyle: Anti-inflammatory diets (Mediterranean, DASH), regular exercise, smoking cessation, and maintaining a healthy weight all lower hs-CRP and reduce risk.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What doesn’t work?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Some anti-inflammatory drugs (methotrexate, TNF inhibitors, corticosteroids) have not shown benefit in major trials.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What’s a normal, good, or bad hs-CRP?&lt;/head&gt;
    &lt;p&gt;If you’ve already measured your hs-CRP (great!), then it’s ideally below &amp;lt;1 mg/L. hs-CRP above 3 mg/L is high risk:&lt;/p&gt;
    &lt;p&gt;(If you’re in moderate or high ranges, see the section above for what to do.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Are other biomarkers of inflammation relevant?&lt;/head&gt;
    &lt;p&gt;The ACC evaluated other markers: IL-6, fibrinogen, neutrophil-to-lymphocyte ratio, EPA/AA ratio, and serum amyloid A. These have also been shown to predict cardiovascular risk, but once hs-CRP is known, don’t add more signal.&lt;/p&gt;
    &lt;p&gt;In other words, you’re best off simply measuring hs-CRP, and then spending money elsewhere on heart health.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other interesting bits&lt;/head&gt;
    &lt;p&gt;The JACC article is packed with other interesting insights. These ones were interesting:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Imaging biomarkers (like CT, PET, MRI, and perivascular “fat attenuation index”) can detect vascular inflammation and may help predict coronary events, but are not yet ready for routine clinical use.&lt;/item&gt;
      &lt;item&gt;Bempedoic acid is a newer cholesterol-lowering drug that also lowers hs-CRP, but its long-term outcomes are still being studied.&lt;/item&gt;
      &lt;item&gt;Residual inflammatory risk: Even with well-controlled LDL on statins, many people still have elevated hs-CRP and ongoing risk—so inflammation should be addressed separately from cholesterol.&lt;/item&gt;
      &lt;item&gt;Universal hs-CRP screening is now recommended by the ACC for both people with and without established heart disease.&lt;/item&gt;
      &lt;item&gt;Colchicine (0.5 mg/d) is now FDA-approved as an adjunct for secondary prevention in stable ASCVD, but should be avoided in people with significant kidney or liver disease.&lt;/item&gt;
      &lt;item&gt;Novel IL-6 inhibitors are being studied as future anti-inflammatory therapies for heart disease.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How to measure your inflammation&lt;/head&gt;
    &lt;p&gt;A simple blood test for hs-CRP is widely available and inexpensive. The ACC now recommends routine hs-CRP testing for both people at risk (primary prevention) and those with established heart disease (secondary prevention).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45430498</guid><pubDate>Tue, 30 Sep 2025 20:00:21 +0000</pubDate></item><item><title>Organize your Slack channels by "How Often", not "What"</title><link>https://aggressivelyparaphrasing.me/2025/09/30/organize-your-slack-channels-by-how-often-not-what/</link><description>&lt;doc fingerprint="31ca4a355a951fbb"&gt;
  &lt;main&gt;
    &lt;p&gt;A few weeks ago, I changed my Slack channel sections. I’m now more responsive and engaged, while also feeling less stressed. How? By sorting my Slack channels by urgency, or how often I want to read them.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell role="head"&gt;What&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Projects&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Team&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Alerts&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Sibling Teams&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Announcements&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;SF Office&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Social&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Noisy&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Versus&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell role="head"&gt;How Often&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Read Now&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Read Hourly&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Read Daily&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Read Whenever&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Sorting by “how often” lets me read my most urgent messages first, focusing my energy on what matters to me. Once I feel tired, I stop reading. By focusing on my most urgent and important channels, I hold confidence that I have already taken care of what I need to, reducing my stress.&lt;/p&gt;
    &lt;p&gt;By framing a channel’s importance through the Eisenhower Matrix, I focus on how I contribute to channels.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Urgent&lt;/cell&gt;
        &lt;cell&gt;Not Urgent&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Important&lt;/cell&gt;
        &lt;cell&gt;Read Now / Read Hourly&lt;p&gt;I directly answer questions, engage in conversations frequently, or react to them in the real world&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Read Whenever&lt;p&gt;I read announcements and keep up-to-date with changes&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Not Important&lt;/cell&gt;
        &lt;cell&gt;Read Daily&lt;p&gt;I can push the conversation along by forwarding it to different channels or tagging more appropriate people&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Read Never&lt;p&gt;Reading and writing in these channels are meaningless to me&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This framework is flexible. Your needs and availability will change. Projects go from active development to finished. Social channels go through ups and downs. As these changes happen, you can slide the channel between any category, and it’ll still make sense.&lt;/p&gt;
    &lt;p&gt;Misprioritized channels are a source of burnout. Noisy channels in important sections waste time and hide the valuable signal. Important channels in noisy sections are missed opportunities. It’s clearest when you think of some sections like Office, Social, and Project. Intuitively, Project is important. Office is important, but maybe not? Social is less important, but I still want to live a happy life. Yet, I kept finding examples that don’t fit.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Channel&lt;/cell&gt;
        &lt;cell&gt;What&lt;/cell&gt;
        &lt;cell&gt;How Often&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;#sf-sweet-treats&lt;p&gt;#sf-nyt-crossword&lt;/p&gt;&lt;p&gt;#fashion-baddies&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Office or Social – Low/Medium Priority&lt;/cell&gt;
        &lt;cell&gt;Read Now — cupcakes get eaten fast, crosswords and photos are organized within 15-30 minutes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;#sf-it-helpdesk&lt;/cell&gt;
        &lt;cell&gt;Office – Low/Medium Priority&lt;/cell&gt;
        &lt;cell&gt;Read Never — It’s never worthwhile for me to read this channel&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Project channels that I’m an active contributor to&lt;/cell&gt;
        &lt;cell&gt;Project – High Priority&lt;/cell&gt;
        &lt;cell&gt;Read Hourly — I’m often answering questions to unblock other people&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Project channels where I’m a passive contributor to&lt;/cell&gt;
        &lt;cell&gt;Project – High Priority&lt;/cell&gt;
        &lt;cell&gt;Read Daily or Read — I want to stay on top of announcements and changes, but I’m not actively contributing&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I conclude that organizing by “what” is pointless.&lt;/p&gt;
    &lt;p&gt;Why do we organize by “what”? I think because, by default, Slack suggests Priority, Team, Announcements, and Social, priming us for “what”.&lt;/p&gt;
    &lt;p&gt;It’s easy to categorize by “what”. It’s easy to explain, and it’s easy to have icons. But I don’t find it useful.&lt;/p&gt;
    &lt;p&gt;How did I start categorizing by “how often”? Is a channel high or medium priority? Just guess. Your gut instinct is probably right. In the worst case, you’re wrong and you slide it up or down. If you’re deeply unsure, put it into Read Hourly or Read Now first. You’ll quickly know if it was the wrong decision. After a couple of wasted moments, slide them down a level. Repeat until the channel stops bothering you.&lt;/p&gt;
    &lt;p&gt;I’ve been organizing my Slack by “how often” for almost a month now, and have successfully maintained Inbox Zero for Slack every day. Give it a shot!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45430542</guid><pubDate>Tue, 30 Sep 2025 20:04:46 +0000</pubDate></item><item><title>Diff Algorithms</title><link>https://flo.znkr.io/diff/</link><description>&lt;doc fingerprint="7f0985451041b94"&gt;
  &lt;main&gt;
    &lt;p&gt;For software engineers, diffs are a ubiquitous method for representing changes: We use diffs to compare different versions of the same file (e.g., during code review or when trying to understand the history of a file), to visualize the difference of a failing test compared with its expectation, or to apply changes to source files automatically.&lt;/p&gt;
    &lt;p&gt;Every project I worked on professionally or privately eventually needed a diff to visualize a change or to apply a patch. However, I have never been satisfied with any of the freely available diff libraries. This was never really a problem professionally, but for private projects, I have copied and modified my own library from project to project until I mentioned this to a colleague who set me on the path to publish my Go library (a port of a previous C++ library I used to copy and modify). Boy, did I underestimate how close my library was to publishability!&lt;/p&gt;
    &lt;p&gt;Anyway, I did it and I learned a whole lot about diff algorithms. You can find my library at znkr.io/diff and what I learned in this article. I am not finished learning yet, so I plan to update this article as my understanding continues to evolve.&lt;/p&gt;
    &lt;head rend="h2"&gt;Existing Diff Libraries&lt;/head&gt;
    &lt;p&gt;Let me start by explaining why I am dissatisfied with existing diff libraries. There are a number of attributes that are important to me. Not all of these attributes are important for every use case, but a diff library that I can use for all of my use cases needs to fulfill all of them.&lt;/p&gt;
    &lt;p&gt;Usually, the input to a diff algorithm is text, and most diff libraries only support that. However, I occasionally have use cases where I need to compare things that are not text. So any diff library that only supports text doesn't meet my needs; instead, I need support for arbitrary sequences.&lt;/p&gt;
    &lt;p&gt;The resulting diff output is intended to be readable by humans. Quite often, especially for text, a good way to present a diff is in the unified format. However, it's not always the best presentation. A diff library should make it easy to output a diff in unified format, but it should also provide a way to customize the presentation by providing a structured result.&lt;/p&gt;
    &lt;p&gt;Besides the presentation, the content of a diff should make it easy for humans to understand the diff. This is a somewhat subjective criterion, but there are a number of failure cases that are easily avoided, and there's some research into diff readability to set a benchmark. On the other hand, diffs should be minimal in that they should be as small as possible.&lt;/p&gt;
    &lt;p&gt;Last but not least, it's important that a diff library has a simple API and provides good performance in both runtime and memory usage, even in worst-case scenarios1.&lt;/p&gt;
    &lt;p&gt;With that, we can evaluate existing diff libraries. For Go, I went through a number of libraries and summarized them.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Input&lt;/cell&gt;
        &lt;cell role="head"&gt;Output&lt;/cell&gt;
        &lt;cell role="head"&gt;API&lt;/cell&gt;
        &lt;cell role="head"&gt;Performance2&lt;/cell&gt;
        &lt;cell role="head"&gt;Diff&lt;p&gt;Readability&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Diff&lt;p&gt;Minimality2&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;diffmatchpatch&lt;/cell&gt;
        &lt;cell&gt;❌3&lt;/cell&gt;
        &lt;cell&gt;❌4&lt;/cell&gt;
        &lt;cell&gt;🤔5&lt;/cell&gt;
        &lt;cell&gt;➖➖&lt;/cell&gt;
        &lt;cell&gt;➖&lt;/cell&gt;
        &lt;cell&gt;➖&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;go-internal&lt;/cell&gt;
        &lt;cell&gt;❌3&lt;/cell&gt;
        &lt;cell&gt;❌6&lt;/cell&gt;
        &lt;cell&gt;😁&lt;/cell&gt;
        &lt;cell&gt;➕➕&lt;/cell&gt;
        &lt;cell&gt;➕➕&lt;/cell&gt;
        &lt;cell&gt;➕&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;godebug&lt;/cell&gt;
        &lt;cell&gt;❌3&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;😁&lt;/cell&gt;
        &lt;cell&gt;➖➖➖ /🧨7&lt;/cell&gt;
        &lt;cell&gt;➕&lt;/cell&gt;
        &lt;cell&gt;➕➕&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;mb0&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌4&lt;/cell&gt;
        &lt;cell&gt;😐8&lt;/cell&gt;
        &lt;cell&gt;➖➖&lt;/cell&gt;
        &lt;cell&gt;➕&lt;/cell&gt;
        &lt;cell&gt;➕➕&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;udiff&lt;/cell&gt;
        &lt;cell&gt;❌3&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;😁&lt;/cell&gt;
        &lt;cell&gt;➕9&lt;/cell&gt;
        &lt;cell&gt;➖&lt;/cell&gt;
        &lt;cell&gt;➖➖9&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beware&lt;/p&gt;
    &lt;p&gt;The way I assigned ➕ and ➖ in this table doesn't follow any scientific methodology it's merely based on running a few benchmarks and comparing a few results by hand. If you're looking for a diff library to fulfill your needs, I would like to encourage you to do your own comparisons. You can find the code I used for these comparisons in on github.&lt;/p&gt;
    &lt;head rend="h2"&gt;Challenges&lt;/head&gt;
    &lt;p&gt;The results suggest that it's far from trivial to implement a good diff library, and the one I had started out with wasn't much better. To understand why the existing libraries are as they are, we need to take a peek into the implementation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Complexity&lt;/head&gt;
    &lt;p&gt;With the exception of go-internal, all libraries use Myers' Algorithm to compute the diff. This is a standard algorithm that returns a minimal diff and has been in use for this purpose for decades. The algorithm has a runtime complexity of where is the number of input elements and is the edit distance between the two inputs. This means that the algorithm is very fast for inputs that are similar, which is quite common. However, it's essentially quadratic in the worst case. That is, for inputs that are very different, the complexity approaches . Furthermore, the algorithm comes in two variants with a space complexity of either or . Only godebug uses the variant with quadratic memory growth.&lt;/p&gt;
    &lt;p&gt;This means that it's relatively easy to write a well-performing diffing algorithm for small or similar inputs, but it takes a very long time to complete for larger, less similar inputs. A consequence of this is that we can't trust simple benchmarks; instead, we need to test the worst-case scenario1.&lt;/p&gt;
    &lt;p&gt;As always in cases like this, we can improve the performance by approximating an optimal solution. There are a number of heuristics that reduce the time complexity by trading off diff minimality. For example, diffmatchpatch uses a deadline to stop the search for an optimal diff, and udiff uses a an extremely aggressive heuristic.&lt;/p&gt;
    &lt;p&gt;Instead of improving Myers' runtime with heuristics, it's also often possible to find a diff using only heuristics. go-internal uses patience diff. The heuristic is good enough that it alone almost always results in a good diff with a runtime complexity of 10. An additional advantage of this algorithm is that it produces more readable diffs. However, patience diff can fail with very large diffs, and it can only be implemented efficiently using a hash table, which restricts the possible applications.&lt;/p&gt;
    &lt;p&gt;Histogram Diff&lt;/p&gt;
    &lt;p&gt;Besides patience diff, there's another interesting heuristic called histogram diff. I still have to implement it and understand it better before writing about it here, though.&lt;/p&gt;
    &lt;head rend="h3"&gt;Readability&lt;/head&gt;
    &lt;p&gt;Diff algorithms usually find a minimal diff or an approximation of one. However, except for trivial cases, there are always multiple minimal diffs. For example, this simple diff&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;b
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;c
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;d
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;is as minimal as&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;c
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;b
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;d
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Not all of the minimal or near-minimal diffs have the same readability for humans. For example11,&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (chunk == NULL) return 0;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    return start &amp;lt;= chunk-&amp;gt;length &amp;amp;&amp;amp; n &amp;lt;= chunk-&amp;gt;length - start;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(src, src_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(dst, dst_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    memcpy(dst-&amp;gt;data + dst_start, src-&amp;gt;data + src_start, n);
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (chunk == NULL) return 0;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    return start &amp;lt;= chunk-&amp;gt;length &amp;amp;&amp;amp; n &amp;lt;= chunk-&amp;gt;length - start;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;is much more readable than the equally minimal and correct&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(src, src_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(dst, dst_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (chunk == NULL) return 0;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    memcpy(dst-&amp;gt;data + dst_start, src-&amp;gt;data + src_start, n);
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    return start &amp;lt;= chunk-&amp;gt;length &amp;amp;&amp;amp; n &amp;lt;= chunk-&amp;gt;length - start;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (chunk == NULL) return 0;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(src, src_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(dst, dst_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    return start &amp;lt;= chunk-&amp;gt;length &amp;amp;&amp;amp; n &amp;lt;= chunk-&amp;gt;length - start;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    memcpy(dst-&amp;gt;data + dst_start, src-&amp;gt;data + src_start, n);
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Furthermore, if we relax minimality to accept approximations, the number of possible results increases significantly.&lt;/p&gt;
    &lt;p&gt;For good diff readability, we have to select one solution from the many possible ones that is readable for humans. Many people believe that the diff readability is determined by the algorithm. However, that's only partially correct, because different implementations of the same algorithm can produce vastly different results.&lt;/p&gt;
    &lt;p&gt;There's also been a lot of progress in the past years to improve diff readability. Perhaps the best work about diff readability is diff-slider-tools by Michael Haggerty. He implemented a heuristic that's applied in a post-processing step to improve the readability.&lt;/p&gt;
    &lt;p&gt;In fact, &lt;code&gt;example_03.diff&lt;/code&gt; above was generated using this heuristic. The diff without the heuristic,
as generated by my implementation of Myers' linear-space variant, looks like this:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (chunk == NULL) return 0;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    return start &amp;lt;= chunk-&amp;gt;length &amp;amp;&amp;amp; n &amp;lt;= chunk-&amp;gt;length - start;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(src, src_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (!Chunk_bounds_check(dst, dst_start, n)) return;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    memcpy(dst-&amp;gt;data + dst_start, src-&amp;gt;data + src_start, n);
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    if (chunk == NULL) return 0;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    return start &amp;lt;= chunk-&amp;gt;length &amp;amp;&amp;amp; n &amp;lt;= chunk-&amp;gt;length - start;
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Notice that the deletion starts at the end of the preceding function and leaves a small remainder of the function being deleted? Michael's heuristic fixes this problem and results in the very readable &lt;code&gt;example_03.diff&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;It's not the algorithm&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;example_04.diff&lt;/code&gt; was found using a different implementation of Myers'
linear-space variant. That is, both &lt;code&gt;example_03.diff&lt;/code&gt; and &lt;code&gt;example_04.diff&lt;/code&gt; used the same algorithm!
The differences stem from the implementation of that algorithm and from post-processing.&lt;/p&gt;
    &lt;head rend="h2"&gt;A New Diffing Library for Go&lt;/head&gt;
    &lt;p&gt;I created znkr.io/diff to address these challenges in a way that works for all my use cases. Let's reiterate what I want from a diffing library:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The input can be text and arbitrary slices&lt;/item&gt;
      &lt;item&gt;The output should be possible in unified format and as a structured result&lt;/item&gt;
      &lt;item&gt;The API should be simple&lt;/item&gt;
      &lt;item&gt;The diffs should be minimal or near-minimal&lt;/item&gt;
      &lt;item&gt;The runtime and memory performance should be excellent&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is a lot more than what any of the existing libraries provide. When I copied and modified my old diffing library, I could adapt it to the use cases at hand. But a general-purpose diffing library needs to be general enough to cover the vast majority of use cases. At the same time, it needs to be extensible to make sure new features can be implemented without cluttering the API over time.&lt;/p&gt;
    &lt;p&gt;Unfortunately, excellent performance and minimal results are somewhat in opposition to one another and I ended up providing three different modes of operation: Default (balanced between performance and minimality), Fast (sacrifice minimal results for faster speed), Optimal (minimal result whatever the cost).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Input&lt;/cell&gt;
        &lt;cell role="head"&gt;Output&lt;/cell&gt;
        &lt;cell role="head"&gt;API&lt;/cell&gt;
        &lt;cell role="head"&gt;Performance2&lt;/cell&gt;
        &lt;cell role="head"&gt;Diff&lt;p&gt;Readability&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Diff&lt;p&gt;Minimality2&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Default&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;😁&lt;/cell&gt;
        &lt;cell&gt;➕➕&lt;/cell&gt;
        &lt;cell&gt;➕➕&lt;/cell&gt;
        &lt;cell&gt;➕➕&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Fast&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;😁&lt;/cell&gt;
        &lt;cell&gt;➕➕➕&lt;/cell&gt;
        &lt;cell&gt;➕➕&lt;/cell&gt;
        &lt;cell&gt;➕&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Optimal&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;😁&lt;/cell&gt;
        &lt;cell&gt;➕&lt;/cell&gt;
        &lt;cell&gt;➕➕&lt;/cell&gt;
        &lt;cell&gt;➕➕&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Text Only&lt;/p&gt;
    &lt;p&gt;This table only applies to text (same as the table above), non-text inputs can have a different performance (if they are not &lt;code&gt;comparable&lt;/code&gt; or readability).&lt;/p&gt;
    &lt;head rend="h3"&gt;API&lt;/head&gt;
    &lt;p&gt;To design this API, I started with the data structures that I wanted to use as a user of the API and worked backwards from there. At a very high level, there are two structured representations of a diff that have been useful to me: a flat sequence of all deletions, insertions, and matching elements (called edits) and a nested sequence of consecutive changes (called hunks).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Edits are what I use to represent edits in this article; they contain the full content of both inputs and how one is transformed into the other.&lt;/item&gt;
      &lt;item&gt;Hunks are a great representation for unit tests, because they are empty if both inputs are identical and they make it possible to visualize just the changes even if the inputs are large.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Arbitrary Slices&lt;/head&gt;
    &lt;p&gt;I started with the design for the most general case, arbitrary slices. The Go representation for diffing slices I liked the most is this one (see also znkr.io/diff):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Op describes an edit operation.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;type Op int
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;const (
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Match  Op = iota // Two slice elements match
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Delete           // A deletion from an element on the left slice
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Insert           // An insertion of an element from the right side
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Edit describes a single edit of a diff.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// - For Match, both X and Y contain the matching element.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// - For Delete, X contains the deleted element and Y is unset (zero value).
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;17&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// - For Insert, Y contains the inserted element and X is unset (zero value).
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;type Edit[T any] struct {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;19&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Op   Op
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;20&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	X, Y T
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;21&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;22&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;23&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Hunk describes a sequence of consecutive edits.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;24&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;type Hunk[T any] struct {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;25&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	PosX, EndX int       // Start and end position in x.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;26&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	PosY, EndY int       // Start and end position in y.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;27&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Edits      []Edit[T] // Edits to transform x[PosX:EndX] to y[PosY:EndY]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;28&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The alternatives I have seen are variations and combinations of two themes. Either using slices to represent edit operations in &lt;code&gt;Hunk&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;type Hunk[T any] struct {
	Delete []T
	Insert []T
	Match  []T
}
&lt;/code&gt;
    &lt;p&gt;Or using indices instead of elements&lt;/p&gt;
    &lt;code&gt;type Edit struct {
	Op         Op
	PosX, PosY []int
}
&lt;/code&gt;
    &lt;p&gt;All of these representations work, but I found that the representations above served my use cases best. One little quirk is that &lt;code&gt;Edit&lt;/code&gt; always contains both elements. This is often unnecessary, but
there are use cases where this is very important because the elements themselves might not be equal
(e.g., if they are pointers that are compared with a custom function).&lt;/p&gt;
    &lt;p&gt;Once the data structures were established, it was quite obvious that the simplest way to fill them with diff data was to write two functions &lt;code&gt;diff.Edits&lt;/code&gt; and
&lt;code&gt;diff.Hunks&lt;/code&gt; to return the diffs. I made them extensible by
using functional options.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;30&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Edits compares the contents of x and y and returns the changes necessary to convert from one to
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;31&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// the other.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;//
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;33&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Edits returns one edit for every element in the input slices. If x and y are identical, the
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;34&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// output will consist of a match edit for every input element.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;35&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func Edits[T comparable](x, y []T, opts ...Option) []Edit[T]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;36&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;37&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Hunks compares the contents of x and y and returns the changes necessary to convert from one to
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;38&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// the other.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;39&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;//
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;40&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// The output is a sequence of hunks. A hunk represents a contiguous block of changes (insertions
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;41&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// and deletions) along with some surrounding context.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;42&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func Hunks[T comparable](x, y []T, opts ...Option) []Hunk[T]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The options allow for future extensibility and allow changing the behavior of these functions. For example, the option &lt;code&gt;diff.Context(5)&lt;/code&gt; configures &lt;code&gt;Hunks&lt;/code&gt;
to provide 5 elements of surrounding context.&lt;/p&gt;
    &lt;p&gt;However, the current API still doesn't allow arbitrary slices; it only allows slices of &lt;code&gt;comparable&lt;/code&gt; types. To fix this, I needed two other functions that provide a function to compare
two elements. The Go standard library uses the &lt;code&gt;Func&lt;/code&gt; suffix for functions like this, so I followed
the lead:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;44&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// EditsFunc compares the contents of x and y using the provided equality comparison and returns the
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;45&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// changes necessary to convert from one to the other.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;46&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func EditsFunc[T any](x, y []T, eq func(a, b T) bool, opts ...Option) []Edit[T]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;47&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;48&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// HunksFunc compares the contents of x and y using the provided equality comparison and returns the
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;49&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// changes necessary to convert from one to the other.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func HunksFunc[T any](x, y []T, eq func(a, b T) bool, opts ...Option) []Hunk[T]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;Text&lt;/head&gt;
    &lt;p&gt;While this API works well to produce a structured result for arbitrary slices, it doesn't provide output in unified format for text inputs. My first approach was to provide a helper function that returns a diff in unified format: &lt;code&gt;diff.ToUnified(hunks []Hunk[string]) string&lt;/code&gt;. However, this would
make getting a unified diff more complicated. Besides requiring two function calls, it would be
necessary to split the input into lines. This, in turn, can be done in different ways, e.g., by
stripping or keeping the line breaks, which opens the door to mistakes. It's much better to provide
a simple function for the entire use case.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Unified compares the lines in x and y and returns the changes necessary to convert from one to
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// the other in unified format.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func Unified[T string | []byte](x, y T, opts ...diff.Option) T
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I also moved this function to the &lt;code&gt;textdiff&lt;/code&gt; package to
highlight the difference in expected input.&lt;/p&gt;
    &lt;p&gt;Now, I also happen to have use cases where I need structured results for text diffs. It would be very annoying if I had to split those into lines manually. Besides, I can make a few more assumptions about text that allow for a slight simplification of the data structures:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Edit describes a single edit of a line-by-line diff.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;type Edit[T string | []byte] struct {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Op   diff.Op // Edit operation
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Line T       // Line, including newline character (if any)
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;17&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Hunk describes a sequence of consecutive edits.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;type Hunk[T string | []byte] struct {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;19&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	PosX, EndX int       // Start and end line in x (zero-based).
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;20&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	PosY, EndY int       // Start and end line in y (zero-based).
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;21&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;	Edits      []Edit[T] // Edits to transform x lines PosX..EndX to y lines PosY..EndY
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;22&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;23&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;24&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Edits compares the lines in x and y and returns the changes necessary to convert from one to the
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;25&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// other.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;26&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func Edits[T string | []byte](x, y T, opts ...diff.Option) []Edit[T]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;27&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;28&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// Hunks compares the lines in x and y and returns the changes necessary to convert from one to the
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;29&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;// other.
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;30&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;func Hunks[T string | []byte](x, y T, opts ...diff.Option) []Hunk[T]
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;For the full API and examples for how to use it, please see the package documentation for znkr.io/diff and znkr.io/diff/textdiff. I am certain that there are use cases not covered by this API, but I feel confident that it can evolve to cover these use cases in the future. For now, all my needs are fulfilled, but if you run into a situation that can't be solved by this API or requires some contortions, please tell me about it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Implementation&lt;/head&gt;
    &lt;p&gt;To implement this API, we need to implement a diff algorithm. There are a couple of standard diff algorithms that we can choose from. The choice of the algorithm as well as how it's implemented matters for the readability of the result as well as the performance.&lt;/p&gt;
    &lt;p&gt;A good starting point for this project was Myers' algorithm, simply because it's the fastest algorithm that can cover the whole API. In particular, the &lt;code&gt;...Func&lt;/code&gt; variants for &lt;code&gt;any&lt;/code&gt; types
instead of &lt;code&gt;comparable&lt;/code&gt; can't make use of a hash map. Patience and Histogram require the use of a
hash map for an efficient implementation, so Myers' really is the only choice. Another advantage of
Myers' compared to Patience and Histogram is that it will return optimal results.&lt;/p&gt;
    &lt;p&gt;On the flip side, in the comparison above, it came out as relatively slow compared to the patience diff algorithm and didn't produce the most readable results. It turns out, however, that this can be mitigated and almost completely overcome for &lt;code&gt;comparable&lt;/code&gt; types using
a combination of preprocessing, heuristics, and post-processing.&lt;/p&gt;
    &lt;p&gt;I am not going to cover the diff algorithm in detail here. There are a number of excellent articles on the web that describe it12, but I recommend reading the paper13: All articles I have seen try to keep a distance from the theory that makes this algorithm work, but that's not really helpful if you want to understand how and why this algorithm works.&lt;/p&gt;
    &lt;head rend="h4"&gt;Preprocessing&lt;/head&gt;
    &lt;p&gt;The most impactful way to improve the performance of Myers' algorithm is to reduce the problem size. The simplest thing to do is to strip any common prefix and suffix. This is always possible and helps a little. However, it can also reduce diff readability, because it will consume matching elements eagerly.&lt;/p&gt;
    &lt;p&gt;For example, let's say we have this change:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;package array
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;var m = []struct{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    name  string
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    year  int
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        name: "Freak Out!",
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        year: 1966,
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    },
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        name: "Absolutely Free",
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        year: 1967,
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    },
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        name: "We're Only in It for the Money",
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;17&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        year: 1967,
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    },
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;19&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If we eagerly consume the common prefix first and then the common suffix, the first 11 lines are all identical and the so are the last 4. This in turn would result in a different diff:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;package array
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;var m = []struct{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    name  string
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    year  int
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}{
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        name: "Freak Out!",
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        year: 1966,
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    },
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        name: "Absolutely Free",
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        year: 1967,
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    },
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    {
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        name: "We're Only in It for the Money",
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;17&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;        year: 1967,
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;    },
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;19&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;}&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Fortunately, this is easy to fix in post processing.&lt;/p&gt;
    &lt;p&gt;Much more impactful, but only efficiently possible for &lt;code&gt;comparable&lt;/code&gt; types, is to remove all elements
that are unique to either the left side or the right side, as those must always be deletions or
insertions. Non-&lt;code&gt;comparable&lt;/code&gt; types can't be keys in a hash map in Go, which is necessary for
checking uniqueness. This preprocessing step reduced the runtime by up to
99% for a few
real-world worst-case diffs.&lt;/p&gt;
    &lt;p&gt;In contrast to the suffix and prefix removal, stripping unique elements doesn't have any readability impact.&lt;/p&gt;
    &lt;head rend="h4"&gt;Heuristics&lt;/head&gt;
    &lt;p&gt;Another very impactful way to improve the performance is Anchoring. It is based on patience diff. The word patience is a bit misleading, because it's too easily associated with having to wait and it doesn't describe the heuristic very well either. It works by finding elements that are occur exactly once on both the left and the right side. When we matching up these unique pairs we create a segmentation of the input into smaller parts that can be analyzed individually. Even better, we're very likely to find matching lines atop and below such a pair of unique elements. This allows us to shrink the segments by stripping common prefixes and suffixes. This heuristic reduced the runtime by up to 95%. Unfortunately, finding unique elements and matching them up requires a hash map again which means that it can only be used for &lt;code&gt;comparable&lt;/code&gt; types.&lt;/p&gt;
    &lt;p&gt;There are two more heuristics that are I implemented. They help for non-&lt;code&gt;comparable&lt;/code&gt; types and as a
backstop when the other heuristics don't work. Their main purpose is to avoid runaway quadratic
growth. The Good Diagonal heuristic stops searching for a better solution if we found a solution
that's good enough and the Too Expensive heuristic shortcuts the search if it becomes too
expensive which reduces the worst-case complexity from 

 to


.&lt;/p&gt;
    &lt;p&gt;However, heuristics like this trade diff minimality for performance, this is not always desirable. Sometimes, a minimal diff is exactly what's required. &lt;code&gt;diff.Optimal&lt;/code&gt; disables these heuristics to always find a
minimal diff irrespective of the costs.&lt;/p&gt;
    &lt;head rend="h4"&gt;Post-processing&lt;/head&gt;
    &lt;p&gt;We established before that a diff algorithm finds one of many possible solutions. Given such a solution we can discover more solutions by it locally and then selecting the best solution according to some metric. This is exactly how Michael Haggerty's indentation heuristic works for text.&lt;/p&gt;
    &lt;p&gt;For any given diff, we can often slide the edits up or down in a way that doesn't change the meaning of a diff. For example,&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;["foo", "bar", "baz"].map do |i|
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;  i
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;end
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;["foo", "bar", "baz"].map do |i|
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;  i.upcase
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;end
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;has the same meaning as&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;["foo", "bar", "baz"].map do |i|
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;  i
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;end
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;+&lt;/cell&gt;
        &lt;cell/&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;["foo", "bar", "baz"].map do |i|
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;  i.upcase
&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;end&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We call edits that can be slid up or down sliders. The question is, how do we select the best slide? Michael collected human ratings for different sliders of the same diff and used them to develop a heuristic to match these ratings: diff-slider-tools.&lt;/p&gt;
    &lt;p&gt;However, this heuristic only works for text and is tuned towards code instead of prose. I decided to make it optional. It can be enabled with the &lt;code&gt;textdiff.IndentHeuristic&lt;/code&gt; option.&lt;/p&gt;
    &lt;head rend="h4"&gt;Diff Representation&lt;/head&gt;
    &lt;p&gt;The representation used during the execution of the diff algorithm has a surprising impact on the algorithm performance and result readability. This is not at all obvious, and so it took me a while to figure out that the best approach is akin to a side-by-side view of a diff: You use two &lt;code&gt;[]bool&lt;/code&gt;
slices to represent the left side and the right side respectively: &lt;code&gt;true&lt;/code&gt; in the left side slice
represents a deletion and on the right side an insertion. &lt;code&gt;false&lt;/code&gt; is a matching element.&lt;/p&gt;
    &lt;p&gt;This representation has four big advantages: It can be preallocated, the order in which edits are discovered doesn't matter, it's easy to mutate during post-processing, and it's easy to generate other representations from it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Open Questions&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What exactly is the reason that two different algorithms produce different results? - I looked into this question a little, but I haven't found a conclusive answer yet.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Diff algorithms are relatively complicated by themselves, but they pale in comparison to what's necessary to provide a high-quality diff library. This article tries to explain what went into my new diff library, but there's still more that I haven't implemented yet.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Here is one real-world example of why worst-case scenarios are important: Imagine you're breaking an existing feature in a way that triggers a worst-case scenario in a test. If the test is running for a very long time or runs out of memory, you're going to have to debug two problems instead of one. ↩︎ ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;See benchmark_comparison.txt for the source of these ratings. ↩︎ ↩︎ ↩︎ ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The diffmatchpatch API is very hard to use ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No support for structured results ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Quadratic memory use; for my test cases, this resulted in &amp;gt;30 GB of memory used. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The mb0 API is from before generics and is a bit cumbersome to use ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;udiff has a very low threshold for when it starts to stop searching for an optimal solution. This improves the speed, but it also results in relatively large diffs. ↩︎ ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;There's no single patience diff heuristic, instead there are different implementations with different performance characteristics. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Stolen from https://blog.jcoglan.com/2017/03/22/myers-diff-in-linear-space-theory/ ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I can recommend https://blog.robertelder.org/diff-algorithm/ and this 5 part series https://blog.jcoglan.com/2017/02/12/the-myers-diff-algorithm-part-1/ ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Myers, E.W. An O(ND) difference algorithm and its variations. Algorithmica 1, 251-266 (1986). https://doi.org/10.1007/BF01840446 ↩︎&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45430604</guid><pubDate>Tue, 30 Sep 2025 20:09:44 +0000</pubDate></item><item><title>Atuin Desktop: Runbooks That Run – Now Open Source</title><link>https://blog.atuin.sh/atuin-desktop-open-source/</link><description>&lt;doc fingerprint="2b3ee61c57b7e69a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Atuin Desktop: Runbooks that Run — Now Open Source&lt;/head&gt;
    &lt;p&gt;Atuin Desktop looks like a doc, but runs like your terminal. Script blocks, embedded terminals, database clients and prometheus charts - all in one place.&lt;/p&gt;
    &lt;p&gt;Most infrastructure is held together by five commands someone remembers when shit breaks. Docs are out of date, if they exist. The real answers? Buried in Slack threads, rotting in Notion, or trapped in someone's shell history.&lt;/p&gt;
    &lt;p&gt;Atuin CLI fixed part of this, with synced, searchable shell history. But history isn’t enough. Teams need workflows they can repeat, share, and trust.&lt;/p&gt;
    &lt;p&gt;That’s why we built Atuin Desktop. Runbooks that actually run. Now open beta, and fully open source.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is Atuin Desktop?&lt;/head&gt;
    &lt;p&gt;Atuin Desktop looks like a doc, but runs like your terminal. Built to make local developer workflows repeatable, shareable, and reliable.&lt;/p&gt;
    &lt;p&gt;Runbooks should run. Workflows shouldn't live in someone's head. Docs shouldn't rot the moment you write them. Scripts, database queries, HTTP requests and Prometheus charts - all in one place.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kill context switching: Chain shell scripts, database queries, and HTTP requests&lt;/item&gt;
      &lt;item&gt;Docs that don't rot: execute directly + stay relevant&lt;/item&gt;
      &lt;item&gt;Reusable automation: dynamic runbooks with Jinja-style templating&lt;/item&gt;
      &lt;item&gt;Local knowledge: Build runbooks from your real shell history&lt;/item&gt;
      &lt;item&gt;Collaborative: Sync and share via Git, or in real-time via our Hub&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Back in April we launched the closed beta.&lt;/p&gt;
    &lt;p&gt;Thousands of you signed up, used it at your day jobs, and told us exactly what broke. We’ve listened, rebuilt, and now it’s ready for everyone.&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s new since April?&lt;/head&gt;
    &lt;p&gt;Our early users gave us a lot of feedback, which we've used to build something much better.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Offline, file based, Git/VCS-compatible workspaces&lt;/item&gt;
      &lt;item&gt;Team accounts with shared, realtime workspaces&lt;/item&gt;
      &lt;item&gt;Kubernetes integration for live state and monitoring&lt;/item&gt;
      &lt;item&gt;MySQL query blocks&lt;/item&gt;
      &lt;item&gt;Dropdown and more contextual blocks&lt;/item&gt;
      &lt;item&gt;A huge number of bug fixes, performance improvements, and UI upgrades&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How it’s being used&lt;/head&gt;
    &lt;p&gt;Atuin Desktop is already being used across engineering teams for serious, day-to-day work.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Automation and debugging: linking commands, monitoring systems, and tracking results&lt;/item&gt;
      &lt;item&gt;Database operations: managing migrations, access control, and production queries&lt;/item&gt;
      &lt;item&gt;Onboarding: getting started workflows new engineers can actually run&lt;/item&gt;
      &lt;item&gt;Deploying and managing clusters: repeatable, documented automation for real environments&lt;/item&gt;
      &lt;item&gt;Incident response: runbooks that execute instead of rotting in some internal wiki&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It’s become a shared system of record for the commands and processes that keep production alive.&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s next&lt;/head&gt;
    &lt;p&gt;We’re just getting started! We've got a lot in the pipeline, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Block dependencies and advanced execution flow&lt;/item&gt;
      &lt;item&gt;Run runbooks remotely and on CI&lt;/item&gt;
      &lt;item&gt;Audit logs and enhanced permissions&lt;/item&gt;
      &lt;item&gt;Comments and deeper collaboration&lt;/item&gt;
      &lt;item&gt;More block types&lt;list rend="ul"&gt;&lt;item&gt;Specify local networks, containers, and more&lt;/item&gt;&lt;item&gt;Tighter integration with authentication and cloud providers&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;More polish, more speed, fewer bugs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Stop copy-pasting from outdated wiki pages, and get started with Atuin Desktop&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting involved&lt;/head&gt;
    &lt;p&gt;Atuin Desktop is now in open beta and open source under the Apache 2.0 license. Star it, fork it, break it: github.com/atuinsh/desktop&lt;/p&gt;
    &lt;p&gt;Infrastructure deserves better than rotting docs and tribal knowledge. Atuin Desktop is our attempt to fix that for everyone who’s ever said “I swear I’ve done this before.”&lt;/p&gt;
    &lt;head rend="h3"&gt;Community links&lt;/head&gt;
    &lt;p&gt;Discord: discord.gg/Fq8bJSKPHh&lt;/p&gt;
    &lt;p&gt;Forum: forum.atuin.sh&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45431001</guid><pubDate>Tue, 30 Sep 2025 20:44:30 +0000</pubDate></item><item><title>Mind the encryptionroot: How to save your data when ZFS loses its mind</title><link>https://sambowman.tech/blog/posts/mind-the-encryptionroot-how-to-save-your-data-when-zfs-loses-its-mind/</link><description>&lt;doc fingerprint="af8d194d51ec8451"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mind the encryptionroot: How to save your data when ZFS loses its mind&lt;/head&gt;
    &lt;p&gt;While ZFS has a well-earned reputation for data integrity and reliability, ZFS native encryption has some incredibly sharp edges that will cut you if you don't know where to be careful. Unfortunately, I learned this the hard way, standing in a pool of my own blood and tears after thoroughly lacerating myself. I very nearly permanently lost 8.5 TiB of data after performing what should've been a series of simple, routine ZFS operations but resulted in an undecryptable dataset. Time has healed the wound enough that I am no longer filled with anguish just thinking about it, so I will now share my experience in the hope that you may learn from my mistakes. Together, we'll go over the unfortunate series of events that led to this happening and how it could've been avoided, learn how ZFS actually works under the hood, use our newfound knowledge to debug and reproduce the issue at hand, and finally compile a modified version of ZFS to repair the corrupted state and rescue our precious data. This is the postmortem of that terrible, horrible, no good, very bad weekâ¦&lt;/p&gt;
    &lt;head rend="h2"&gt;Table of Contents&lt;/head&gt;
    &lt;head rend="h2"&gt;Part 1: An unfortunate series of events&lt;/head&gt;
    &lt;head rend="h3"&gt;The status quo&lt;/head&gt;
    &lt;p&gt; In the beginning, there were two ZFS pools: &lt;code&gt;old&lt;/code&gt; and &lt;code&gt;new&lt;/code&gt; (names changed for clarity). Each pool was hosted on an instance of TrueNAS CORE 13.0-U5.1 located at two different sites about an hour's drive apart with poor Internet connectivity between them. For this reason, a third pool &lt;code&gt;sneakernet&lt;/code&gt; was periodically moved between the two sites and used to exchange snapshots of &lt;code&gt;old&lt;/code&gt; and &lt;code&gt;new&lt;/code&gt; datasets for backup purposes. ZFS dataset snapshots would be indirectly relayed from &lt;code&gt;old&lt;/code&gt; to &lt;code&gt;new&lt;/code&gt; (and vice versa) using &lt;code&gt;sneakernet&lt;/code&gt; as an intermediate ZFS send/recv source/destination (e.g. &lt;code&gt;old/foo@2023-06-01&lt;/code&gt; -&amp;gt; &lt;code&gt;sneakernet/old/foo@2023-06-01&lt;/code&gt; -&amp;gt; &lt;code&gt;new/old/foo@2023-06-01&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt; The &lt;code&gt;new&lt;/code&gt; pool was natively encrypted from the very beginning. When ZFS snapshots were sent from &lt;code&gt;new&lt;/code&gt; to &lt;code&gt;sneakernet/new&lt;/code&gt; to &lt;code&gt;old/new&lt;/code&gt;, they were sent raw, meaning that blocks were copied unmodified in their encrypted form. To decrypt and mount them on &lt;code&gt;sneakernet&lt;/code&gt; or &lt;code&gt;old&lt;/code&gt;, you would need to first load &lt;code&gt;new&lt;/code&gt;'s hex encryption key, which is stored in TrueNAS's SQLite database.&lt;/p&gt;
    &lt;p&gt; The &lt;code&gt;old&lt;/code&gt; pool, on the other hand, was created before the advent of native encryption and was unencrypted for the first part of its life. Because it's desirable to encrypt data at rest, an encrypted dataset &lt;code&gt;sneakernet/old&lt;/code&gt; was created for &lt;code&gt;old&lt;/code&gt; using a passphrase encryption key when &lt;code&gt;sneakernet&lt;/code&gt; was set up. Unencrypted snapshots were sent non-raw from &lt;code&gt;old&lt;/code&gt; to &lt;code&gt;sneakernet/old&lt;/code&gt;, where they were encrypted, and then sent raw from &lt;code&gt;sneakernet/old&lt;/code&gt; to &lt;code&gt;new/old&lt;/code&gt;. To decrypt and mount them on &lt;code&gt;sneakernet&lt;/code&gt; or &lt;code&gt;new&lt;/code&gt;, you would need to first load &lt;code&gt;sneakernet&lt;/code&gt;'s passphrase encryption key.&lt;/p&gt;
    &lt;p&gt;This was all tested thoroughly and snapshots were proven to be readable at each point on every pool.&lt;/p&gt;
    &lt;head rend="h3"&gt;Encrypting the old pool&lt;/head&gt;
    &lt;p&gt; Now that we had encrypted snapshots of &lt;code&gt;old&lt;/code&gt; on &lt;code&gt;sneakernet/old&lt;/code&gt;, we wanted to encrypt &lt;code&gt;old&lt;/code&gt; itself. To do this, I simply took &lt;code&gt;old&lt;/code&gt; offline during a maintenance window to prevent new writes, took snapshots of all datasets, sent them to &lt;code&gt;sneakernet/old&lt;/code&gt;, and then sent the raw encrypted snapshots from &lt;code&gt;sneakernet/old&lt;/code&gt; back to &lt;code&gt;old/encrypted&lt;/code&gt;. Once I verified each dataset had been encrypted successfully, I destroyed the unencrypted dataset, updated the mount point of the encrypted dataset to that of the late unencrypted dataset, and then moved on to the next dataset. After all datasets were migrated, I used &lt;code&gt;zfs change-key -i&lt;/code&gt; to make all child datasets inherit from the new &lt;code&gt;old/encrypted&lt;/code&gt; encryption root, and then changed the key of the encryption root from a passphrase to a hex key, since TrueNAS only supported automatically unlocking datasets with hex encryption keys. Finally, I issued a &lt;code&gt;zpool initialize&lt;/code&gt; to overwrite all the unencrypted blocks which were now in unallocated space.&lt;/p&gt;
    &lt;p&gt; Spoiler Alert: It may not be immediately obvious why, but changing the encryption key on &lt;code&gt;old/encryption&lt;/code&gt; silently broke backups of &lt;code&gt;old&lt;/code&gt; datasets. Snapshots would still send and recv successfully, but were no longer decryptable or mountable. Since the encryption key is not normally loaded, and we only load it when periodically testing the backups, we would not realize until it was too late.&lt;/p&gt;
    &lt;p&gt;Lesson: Test backups continuously so you get immediate feedback when they break.&lt;/p&gt;
    &lt;head rend="h3"&gt;Decommissioning the old pool&lt;/head&gt;
    &lt;p&gt; Later, the &lt;code&gt;old&lt;/code&gt; pool was moved to the same site as the &lt;code&gt;new&lt;/code&gt; pool, so we wanted to fully decommission &lt;code&gt;old&lt;/code&gt; and migrate all its datasets to &lt;code&gt;new&lt;/code&gt;. I began going about this in a similar way. I took &lt;code&gt;old&lt;/code&gt; offline to prevent new writes, sent snapshots to &lt;code&gt;sneakernet/old&lt;/code&gt;, and then to &lt;code&gt;new/old&lt;/code&gt;. It was at this point that I made a very unfortunate mistake: I accidentally destroyed one dataset &lt;code&gt;old/encrypted/foo&lt;/code&gt; before verifying the files were readable on &lt;code&gt;new/old/foo&lt;/code&gt;, and I would soon realize that they were not.&lt;/p&gt;
    &lt;p&gt;Lesson: Wait to make all destructive changes together at the very end instead of interspersed where they could accidentally be performed in the wrong order.&lt;/p&gt;
    &lt;head rend="h3"&gt;The realization&lt;/head&gt;
    &lt;code&gt;[sam@newnas ~]$ DATASET=foo; [[ $(ssh sam@oldnas zfs list -H -o guid old/encrypted/${DATASET}@decomm) = $(zfs list -H -o guid sneakernet/old/${DATASET}@decomm) ]] &amp;amp;&amp;amp; echo "GUIDs match" || echo "GUIDs DO NOT MATCH"
GUIDs match
[sam@newnas ~]$ DATASET=foo; [[ $(zfs list -H -o guid sneakernet/old/${DATASET}@decomm) = $(zfs list -H -o guid new/old/${DATASET}@decomm) ]] &amp;amp;&amp;amp; echo "GUIDs match" || echo "GUIDs DO NOT MATCH"
GUIDs match

[sam@oldnas ~]$ sudo zfs destroy -r old/encrypted/foo

[sam@newnas ~]$ ls /mnt/new/old/foo
[sam@newnas ~]$ ls -a /mnt/new/old/foo
. ..
[sam@newnas ~]$ zfs list -o name,mounted new/old/foo
NAME         MOUNTED
new/old/foo  no
[sam@newnas ~]$ sudo zfs mount new/old/foo
cannot mount 'new/old/foo': Permission denied&lt;/code&gt;
    &lt;p&gt;What do you mean, permission denied? I am root!&lt;/p&gt;
    &lt;p&gt; Crap, I already destroyed &lt;code&gt;old/encrypted/foo&lt;/code&gt;. This is not good, but I can still restore it from the remaining copy on &lt;code&gt;sneakernet/old/foo&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;[sam@newnas ~]$ sudo zfs load-key sneakernet/old
Enter passphrase for 'sneakernet/old':
[sam@newnas ~]$ sudo zfs mount sneakernet/old/foo
cannot mount 'sneakernet/old/foo': Permission denied&lt;/code&gt;
    &lt;p&gt; Oh no, &lt;code&gt;sneakernet/old&lt;/code&gt; is broken too. This is very not good!&lt;/p&gt;
    &lt;p&gt;In an act of desperation, I tried rebooting the machine, but it didn't change a thing.&lt;/p&gt;
    &lt;p&gt;It is at this point that I realized:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Something has gone terribly wrong to prevent datasets on both &lt;code&gt;sneakernet/old&lt;/code&gt;and&lt;code&gt;new/old&lt;/code&gt;from mounting.&lt;/item&gt;
      &lt;item&gt;Whatever it is, it's not likely going to be easy to diagnose or fix.&lt;/item&gt;
      &lt;item&gt;There's a very real possibility the data might be gone forever.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I found myself in a hole and I wanted to stop digging. Fortunately, uptime was no longer critical for the &lt;code&gt;old&lt;/code&gt; datasets after the relocation, so I could afford to step away from the keyboard, collect my thoughts, and avoid making the situation any worse that it already was.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 2: Debugging the issue&lt;/head&gt;
    &lt;p&gt;Once the worst of the overwhelming, visceral feelings that come with the realization that you may have just caused permanent data loss had subsided, I started to work the incident and try to figure out why the backups aren't mounting.&lt;/p&gt;
    &lt;p&gt; As a precaution, I first exported the &lt;code&gt;old&lt;/code&gt; pool and took a forensic image of every disk in the pool. ZFS is a copy-on-write filesystem, so even though the dataset had been destroyed, most of the data was probably still on disk, just completely inaccessible with the normal ZFS tooling. In the worst case scenario, I may have had to try to forensically reconstruct the dataset from what was left on disk, and I didn't want to risk causing any more damage than I already had. Fortunately, I never had to use the disk images, but they still served as a valuable safety net while debugging and repairing.&lt;/p&gt;
    &lt;p&gt;Next, I realized that if we are to have any chance of debugging and fixing this issue, I need to learn how ZFS actually works.&lt;/p&gt;
    &lt;head rend="h3"&gt;Learning how ZFS actually works&lt;/head&gt;
    &lt;p&gt;I unfortunately did not keep track of every resource I consumed, but in addition to reading the source and docs, I found these talks by Jeff Bonwick, Bill Moore, and Matt Ahrens (the original creators of ZFS) to be particularly helpful in understanding the design and implementation of ZFS:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ZFS: The Last Word in File Systems Part 1&lt;/item&gt;
      &lt;item&gt;ZFS: The Last Word in File Systems Part 2&lt;/item&gt;
      &lt;item&gt;ZFS: The Last Word in File Systems Part 3&lt;/item&gt;
      &lt;item&gt;How ZFS Snapshots Really Work&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I highly recommend watching them all despite their age and somewhat poor recording quality, but will summarize the relevant information for those who don't have 3 hours to spare.&lt;/p&gt;
    &lt;p&gt;ZFS is a copy-on-write filesystem, which means that it does not overwrite blocks in place when a write is requested. Instead, the updated contents are written to a newly allocated block, and the old block is freed, which keeps the filesystem consistent if a write is interrupted. All blocks of both data and metadata are arranged in a Merkle tree structure where each block pointer contains a checksum of the child block, which allows ZFS to detect both block corruption and misdirected/phantom reads/writes. This means that any write will cause the block's checksum to change, which will then cause the parent block's checksum to change (since the parent block includes the block pointer which includes checksum of the child block that changed), and so on, all the way up to the root of the tree which ZFS calls an uberblock.&lt;/p&gt;
    &lt;p&gt;Uberblocks are written atomically, and because of the Merkle tree structure, they always represent a consistent snapshot of the entire filesystem at a point in time. Writes are batched together into transaction groups identified by a monotonically increasing counter, and each transaction group when synced to disk produces a new uberblock and associated filesystem tree. Taking a snapshot is then as simple as saving an uberblock and not freeing any of the blocks it points to.&lt;/p&gt;
    &lt;p&gt;In addition to the checksum, each block pointer also contains the transaction group id in which the child block was written, which is called the block's birth time or creation time. ZFS uses birth times to determine which blocks have been written before or after a snapshot. Any blocks with a birth time less than or equal to the snapshot's birth time, must have been written before the snapshot was taken, and conversely, any blocks with a birth time greater than the snapshot's birth time must have been written after the snapshot was taken.&lt;/p&gt;
    &lt;p&gt;One application of birth times is to generate incremental send streams between two snapshots. ZFS walks the tree but only needs to include blocks where the birth time is both greater than the first snapshot and less than or equal to the second snapshot. In fact, you don't even need to keep the data of the first snapshot aroundâyou can create a bookmark which saves the snapshot's transaction id (but none of the data blocks), delete the snapshot to free its data, and then use the bookmark as the source to generate the same incremental send stream.&lt;/p&gt;
    &lt;p&gt;Spoiler Alert: Chekhov's bookmark will become relevant later.&lt;/p&gt;
    &lt;head rend="h3"&gt;Learning how ZFS native encryption actually works&lt;/head&gt;
    &lt;p&gt;ZFS native encryption is a relatively new feature, which was first released in OpenZFS 0.8.0 (2019) and subsequently made it into FreeBSD 13.0 (2021) when OpenZFS was adopted.&lt;/p&gt;
    &lt;p&gt;In addition to the docs, I found this 2016 talk on ZFS Native Encryption by Tom Caputi (the original author of native encryption) to be helpful in understanding its design and implementation. Again, I will summarize the relevant information.&lt;/p&gt;
    &lt;p&gt; ZFS native encryption works by encrypting dataset blocks with an symmetric authenticated encryption cipher suite (AES-256-GCM by default). To use native encryption, you must create a new dataset with &lt;code&gt;-o encryption=on&lt;/code&gt; which generates a unique master key for the dataset. The dataset's master key is then used to derive block data encryption keys with a salted HKDF.&lt;/p&gt;
    &lt;p&gt; The master key can't be changed, so it is encrypted with a wrapping key which can be changed. The wrapping key is provided by the user with &lt;code&gt;zfs load-key&lt;/code&gt; and can be changed with &lt;code&gt;zfs change-key&lt;/code&gt; which re-encrypts the same master key with a new wrapping key.&lt;/p&gt;
    &lt;p&gt;The encrypted master keys are stored in each dataset since each dataset has its own master key, but the wrapping key parameters are stored on what is called the encryption root dataset. The encryption root may be the same encrypted dataset, or it may be a parent of the encrypted dataset. When a child encrypted dataset inherits from a parent encryption root, the encryption root's wrapping key is used to decrypt the child dataset's master key. This is how one key can be used to unlock a parent encryption root dataset and all child encrypted datasets that inherit from it at the same time instead of having to load a key for every single encrypted dataset.&lt;/p&gt;
    &lt;p&gt; In our case, &lt;code&gt;new&lt;/code&gt;, &lt;code&gt;sneakernet/new&lt;/code&gt;, &lt;code&gt;sneakernet/old&lt;/code&gt;, and &lt;code&gt;old/encrypted&lt;/code&gt; are the encryption roots, and all child encrypted datasets inherit from them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Forming a hypothesis&lt;/head&gt;
    &lt;p&gt;At this point, we now know enough to form a hypothesis as to what may have happened. Feel free to pause here and try to figure it out on your own.&lt;/p&gt;
    &lt;p&gt; Recall that &lt;code&gt;sneakernet/old&lt;/code&gt; was created using a passphrase encryption key, and &lt;code&gt;old/encrypted&lt;/code&gt; was created by raw sending &lt;code&gt;sneakernet/old&lt;/code&gt;, so it initially used the same passphrase derived wrapping encryption key. When the &lt;code&gt;old/encrypted&lt;/code&gt; encryption key was changed from a passphrase to a hex key, ZFS must have changed the wrapping key parameters on the &lt;code&gt;old/encrypted&lt;/code&gt; encryption root and re-encrypted all child encrypted dataset master keys with the new hex wrapping key. Crucially, a new snapshot of &lt;code&gt;old/encrypted&lt;/code&gt; was never taken and sent to &lt;code&gt;sneakernet/old&lt;/code&gt; because it ostensibly didn't contain any data and was just a container for the child datasets.&lt;/p&gt;
    &lt;p&gt; Hypothesis: When subsequent snapshots were sent from &lt;code&gt;old&lt;/code&gt; to &lt;code&gt;sneakernet&lt;/code&gt;, the master keys of the child encrypted datasets were updated to be encrypted with the new hex wrapping key, but the &lt;code&gt;sneakernet/old&lt;/code&gt; encryption root was never updated with the new hex wrapping key parameters because a new snapshot was never sent. Therefore, when we load the key for &lt;code&gt;sneakernet/old&lt;/code&gt;, ZFS asks for the old passphrase, not a hex key, and when we try to mount &lt;code&gt;sneakernet/old/foo&lt;/code&gt;, it tries and fails to decrypt its master key with the old passphrase wrapping key instead of the new hex wrapping key.&lt;/p&gt;
    &lt;p&gt;If correct, this would explain the behavior we're seeing. To test this hypothesis, let's try to reproduce the issue in a test environment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Creating a test environment&lt;/head&gt;
    &lt;p&gt;TrueNAS CORE 13.0-U5.1 is based on FreeBSD 13.1, despite the different minor version numbers, so we'll create a FreeBSD 13.1 VM to test in. Make sure to include the system source tree and install on UFS so that we can build OpenZFS and reload the ZFS kernel module without rebooting.&lt;/p&gt;
    &lt;p&gt;TrueNAS CORE 13.0-U5.1 uses ZFS 2.1.11, so we'll want to build the same version from source for consistency. I started by reading the Building ZFS guide and following the steps documented there with some small modifications for FreeBSD since the page was clearly written with Linux in mind.&lt;/p&gt;
    &lt;p&gt;First, install the dependencies we'll need.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo pkg install autoconf automake autotools git gmake python devel/py-sysctl sudo&lt;/code&gt;
    &lt;p&gt;Then, clone ZFS and check out tag zfs-2.1.11.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ git clone https://github.com/openzfs/zfs
sam@zfshax:~ $ cd zfs
sam@zfshax:~/zfs $ git checkout zfs-2.1.11
sam@zfshax:~/zfs $ git show --summary
commit e25f9131d679692704c11dc0c1df6d4585b70c35 (HEAD, tag: zfs-2.1.11)
Author: Tony Hutter &amp;lt;hutter2@llnl.gov&amp;gt;
Date:   Tue Apr 18 11:44:34 2023 -0700

    Tag zfs-2.1.11

    META file and changelog updated.

    Signed-off-by: Tony Hutter &amp;lt;hutter2@llnl.gov&amp;gt;&lt;/code&gt;
    &lt;p&gt;Now, configure, build, and install ZFS.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~/zfs $ sh autogen.sh
sam@zfshax:~/zfs $ ./configure
sam@zfshax:~/zfs $ gmake -s -j$(sysctl -n hw.ncpu)    # &amp;lt;-- modified for FreeBSD
sam@zfshax:~/zfs $ sudo gmake install; sudo ldconfig  # &amp;lt;-- modified for FreeBSD&lt;/code&gt;
    &lt;p&gt;Then, replace the FreeBSD's ZFS kernel module with the one we just built.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~/zfs $ sudo kldunload zfs.ko  # Needed because zfs.sh only unloads openzfs.ko
sam@zfshax:~/zfs $ sudo ./scripts/zfs.sh&lt;/code&gt;
    &lt;p&gt;Finally, verify we're running version 2.1.11 as desired.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~/zfs $ sudo zfs version
zfs-2.1.11-1
zfs-kmod-2.1.11-1&lt;/code&gt;
    &lt;head rend="h3"&gt;Reproducing the issue&lt;/head&gt;
    &lt;p&gt;Now we're ready to try reproducing the issue. This took some iteration to get right, so I wrote a bash script that starts from scratch on each invocation and then runs the commands needed to reproduce the corrupt state. After quite a bit of trial and error, I eventually produced a reproducer script which does the following:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Create 2 pools: &lt;code&gt;src&lt;/code&gt;and&lt;code&gt;dst&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Create &lt;code&gt;src/encryptionroot&lt;/code&gt;using a passphrase encryption key.&lt;/item&gt;
      &lt;item&gt;Create &lt;code&gt;src/encryptionroot/child&lt;/code&gt;which inherits&lt;code&gt;src/encryptionroot&lt;/code&gt;as its encryption root.&lt;/item&gt;
      &lt;item&gt;Create files and take snapshots &lt;code&gt;src/encryptionroot@111&lt;/code&gt;and&lt;code&gt;src/encryptionroot/child@111&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Send raw snapshots &lt;code&gt;src/encryptionroot@111&lt;/code&gt;and&lt;code&gt;src/encryptionroot/child@111&lt;/code&gt;to&lt;code&gt;dst/encryptionroot&lt;/code&gt;and&lt;code&gt;dst/encryptionroot/child&lt;/code&gt;respectively.&lt;/item&gt;
      &lt;item&gt;Load encryption key for &lt;code&gt;dst/encryptionroot&lt;/code&gt;using passphrase and mount encrypted datasets&lt;code&gt;dst/encryptionroot&lt;/code&gt;and&lt;code&gt;dst/encryptionroot/child&lt;/code&gt;. At this point,&lt;code&gt;src&lt;/code&gt;and&lt;code&gt;dst&lt;/code&gt;pools are in sync.&lt;/item&gt;
      &lt;item&gt;Change the &lt;code&gt;src/encryptionroot&lt;/code&gt;encryption key from passphrase to hex.&lt;/item&gt;
      &lt;item&gt;Update files and take snapshots &lt;code&gt;src/encryptionroot@222&lt;/code&gt;and&lt;code&gt;src/encryptionroot/child@222&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Send a raw incremental snapshot of &lt;code&gt;src/encryptionroot/child@222&lt;/code&gt;to&lt;code&gt;dst/encryptionroot/child&lt;/code&gt;, but do not send&lt;code&gt;src/encryptionroot@222&lt;/code&gt;which contains the key change!&lt;/item&gt;
      &lt;item&gt;Unmount &lt;code&gt;dst/encryptionroot&lt;/code&gt;and&lt;code&gt;dst/encryptionroot/child&lt;/code&gt;and unload the cached encryption key for&lt;code&gt;dst/encryptionroot&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Load the encryption key for &lt;code&gt;dst/encryptionroot&lt;/code&gt;using the passphrase since we didn't send the updated encryption root after changing the key.&lt;/item&gt;
      &lt;item&gt;Try to remount &lt;code&gt;dst/encryptionroot&lt;/code&gt;and&lt;code&gt;dst/encryptionroot/child&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When we run the reproducer, the root encrypted dataset &lt;code&gt;dst/encryptionroot&lt;/code&gt; mounts successfully and we can read the old file from the first snapshot, but the child encrypted dataset &lt;code&gt;dst/encryptionroot/child&lt;/code&gt; fails to mount with &lt;code&gt;cannot mount 'dst/encryptionroot/child: Permission denied&lt;/code&gt; just as we expected.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo ./reproduce &amp;gt; /dev/null 2&amp;gt;&amp;amp;1
sam@zfshax:~ $ sudo zfs mount dst/encryptionroot/child
cannot mount 'dst/encryptionroot/child': Permission denied&lt;/code&gt;
    &lt;head title="Click to interact"&gt;Full reproducer script output (long!)&lt;/head&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo ./reproduce

Destroy pools and backing files if they exist.
+ zpool destroy src
+ zpool destroy dst
+ rm -f /src.img
+ rm -f /dst.img

Create pools using sparse files.
+ truncate -s 100M /src.img
+ truncate -s 100M /dst.img
+ zpool create -o ashift=12 -m /src src /src.img
+ zpool create -o ashift=12 -m /dst dst /dst.img

Create root encrypted dataset using a passphrase encryption key.
+ echo 'hunter2!'
+ zfs create -o encryption=on -o keyformat=passphrase -o keylocation=prompt src/encryptionroot

Create child encrypted dataset which inherits src/encryptionroot as its encryption root.
+ zfs create src/encryptionroot/child

Create files in the root and child encrypted datasets and snapshot both.
+ touch /src/encryptionroot/111
+ touch /src/encryptionroot/child/111
+ zfs snapshot -r src/encryptionroot@111

[ Checkpoint 1 ] Files and snapshots are on the src pool but not the dst pool yet.

NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
src                           -                   none        -            yes      1354282934008960312
src/encryptionroot            src/encryptionroot  passphrase  available    yes      12828913232342655944
src/encryptionroot@111        src/encryptionroot  -           available    -        14453618123048176778
src/encryptionroot/child      src/encryptionroot  passphrase  available    yes      10447093816713688124
src/encryptionroot/child@111  src/encryptionroot  -           available    -        10173467213034806911
NAME  ENCROOT  KEYFORMAT   KEYSTATUS    MOUNTED   GUID
dst   -        none        -            yes      5247064584420489120
/src
âââ encryptionroot
    âââ 111
    âââ child
        âââ 111
/dst

Send a raw replication stream of the src snapshots to the dst pool.
+ zfs send --replicate --raw src/encryptionroot@111
+ zfs recv dst/encryptionroot

Load encryption key for the dst encryption root using passphrase and mount the encrypted datasets.
+ echo 'hunter2!'
+ zfs load-key dst/encryptionroot
+ zfs mount dst/encryptionroot
+ zfs mount dst/encryptionroot/child

[ Checkpoint 2 ] Files and snapshots are on both pools and in sync.

NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
src                           -                   none        -            yes      1354282934008960312
src/encryptionroot            src/encryptionroot  passphrase  available    yes      12828913232342655944
src/encryptionroot@111        src/encryptionroot  -           available    -        14453618123048176778
src/encryptionroot/child      src/encryptionroot  passphrase  available    yes      10447093816713688124
src/encryptionroot/child@111  src/encryptionroot  -           available    -        10173467213034806911
NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
dst                           -                   none        -            yes      5247064584420489120
dst/encryptionroot            dst/encryptionroot  passphrase  available    yes      3076413147413645477
dst/encryptionroot@111        dst/encryptionroot  -           available    -        14453618123048176778
dst/encryptionroot/child      dst/encryptionroot  passphrase  available    yes      18246034838646533510
dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        10173467213034806911
/src
âââ encryptionroot
    âââ 111
    âââ child
        âââ 111
/dst
âââ encryptionroot
    âââ 111
    âââ child
        âââ 111

Change the src encryption root key from passphrase to hex.
+ echo 0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef
+ zfs change-key -o keyformat=hex src/encryptionroot

Update the files in the root and child encrypted datasets and snapshot both.
+ mv /src/encryptionroot/111 /src/encryptionroot/222
+ mv /src/encryptionroot/child/111 /src/encryptionroot/child/222
+ zfs snapshot -r src/encryptionroot@222

[ Checkpoint 3 ] Updated files and snapshots are on the src pool but not the dst pool yet.

NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
src                           -                   none        -            yes      1354282934008960312
src/encryptionroot            src/encryptionroot  hex         available    yes      12828913232342655944
src/encryptionroot@111        src/encryptionroot  -           available    -        14453618123048176778
src/encryptionroot@222        src/encryptionroot  -           available    -        929742392566496732
src/encryptionroot/child      src/encryptionroot  hex         available    yes      10447093816713688124
src/encryptionroot/child@111  src/encryptionroot  -           available    -        10173467213034806911
src/encryptionroot/child@222  src/encryptionroot  -           available    -        8161419639883744346
NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
dst                           -                   none        -            yes      5247064584420489120
dst/encryptionroot            dst/encryptionroot  passphrase  available    yes      3076413147413645477
dst/encryptionroot@111        dst/encryptionroot  -           available    -        14453618123048176778
dst/encryptionroot/child      dst/encryptionroot  passphrase  available    yes      18246034838646533510
dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        10173467213034806911
/src
âââ encryptionroot
    âââ 222
    âââ child
        âââ 222
/dst
âââ encryptionroot
    âââ 111
    âââ child
        âââ 111

Send a raw incremental snapshot of the child encrypted dataset to the dst pool.
+ zfs send --raw -i src/encryptionroot/child@111 src/encryptionroot/child@222
+ zfs recv -F dst/encryptionroot/child

NOTE: The encryption key change on the src encryption root has not been sent to dst!

[ Checkpoint 4 ] File is updated in the dst child encrypted dataset but not the dst root encrypted dataset.

NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
src                           -                   none        -            yes      1354282934008960312
src/encryptionroot            src/encryptionroot  hex         available    yes      12828913232342655944
src/encryptionroot@111        src/encryptionroot  -           available    -        14453618123048176778
src/encryptionroot@222        src/encryptionroot  -           available    -        929742392566496732
src/encryptionroot/child      src/encryptionroot  hex         available    yes      10447093816713688124
src/encryptionroot/child@111  src/encryptionroot  -           available    -        10173467213034806911
src/encryptionroot/child@222  src/encryptionroot  -           available    -        8161419639883744346
NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
dst                           -                   none        -            yes      5247064584420489120
dst/encryptionroot            dst/encryptionroot  passphrase  available    yes      3076413147413645477
dst/encryptionroot@111        dst/encryptionroot  -           available    -        14453618123048176778
dst/encryptionroot/child      dst/encryptionroot  hex         available    yes      18246034838646533510
dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        10173467213034806911
dst/encryptionroot/child@222  dst/encryptionroot  -           available    -        8161419639883744346
/src
âââ encryptionroot
    âââ 222
    âââ child
        âââ 222
/dst
âââ encryptionroot
    âââ 111
    âââ child
        âââ 222

NOTE: The updated file in the dst child encrypted dataset is only still readable because the encryption key is still loaded from before sending the snapshot taken after the key change.

Unmount the dst encrypted datasets and and unload the cached encryption key.
+ zfs unmount dst/encryptionroot
+ zfs unload-key dst/encryptionroot

Load the encryption key for the dst encryption root using the passphrase since we did not send the updated encryption root after changing the key.
+ echo 'hunter2!'
+ zfs load-key dst/encryptionroot

Try to remount dst encrypted datasets.
+ zfs mount dst/encryptionroot
+ zfs mount dst/encryptionroot/child
cannot mount 'dst/encryptionroot/child': Permission denied
+ true

[ Checkpoint 5 ] Mounting dst child encrypted dataset failed even though encryption key is ostensibly available. Hypothesis confirmed!

NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
src                           -                   none        -            yes      1354282934008960312
src/encryptionroot            src/encryptionroot  hex         available    yes      12828913232342655944
src/encryptionroot@111        src/encryptionroot  -           available    -        14453618123048176778
src/encryptionroot@222        src/encryptionroot  -           available    -        929742392566496732
src/encryptionroot/child      src/encryptionroot  hex         available    yes      10447093816713688124
src/encryptionroot/child@111  src/encryptionroot  -           available    -        10173467213034806911
src/encryptionroot/child@222  src/encryptionroot  -           available    -        8161419639883744346
NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
dst                           -                   none        -            yes      5247064584420489120
dst/encryptionroot            dst/encryptionroot  passphrase  available    yes      3076413147413645477
dst/encryptionroot@111        dst/encryptionroot  -           available    -        14453618123048176778
dst/encryptionroot/child      dst/encryptionroot  hex         available    no       18246034838646533510
dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        10173467213034806911
dst/encryptionroot/child@222  dst/encryptionroot  -           available    -        8161419639883744346
/src
âââ encryptionroot
    âââ 222
    âââ child
        âââ 222
/dst
âââ encryptionroot
    âââ 111
    âââ child&lt;/code&gt;
    &lt;p&gt;Now that we understand and can reliably reproduce the issue, we're a big step closer to fixing it!&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 3: Recovering our data&lt;/head&gt;
    &lt;head rend="h3"&gt;Theoretically easy to fix&lt;/head&gt;
    &lt;p&gt;We know now that a child encrypted dataset will become unmountable if the following conditions are met:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The wrapping encryption key on the encryption root is changed.&lt;/item&gt;
      &lt;item&gt;A snapshot of the child encrypted dataset that was taken after the key change is sent.&lt;/item&gt;
      &lt;item&gt;A snapshot of the encryption root that was taken after the key change is not sent.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Lesson: Always send a snapshot of the encryption root after changing the encryption key.&lt;/p&gt;
    &lt;p&gt;In theory, all we should have to do to fix it is send the latest snapshot of the encryption root.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo ./reproduce &amp;gt; /dev/null 2&amp;gt;&amp;amp;1
sam@zfshax:~ $ sudo ./repair_snapshot

HYPOTHESIS: The child encrypted dataset should become decryptable again if a snapshot containing the key change on the root encrypted dataset is sent.

Send a raw incremental snapshot of the root encrypted dataset to the dst pool.
+ zfs send --raw -i src/encryptionroot@111 src/encryptionroot@222
+ zfs recv -F dst/encryptionroot

Unmount the dst encrypted datasets and and unload the cached encryption key.
+ zfs unmount dst/encryptionroot
+ zfs unload-key dst/encryptionroot

Load the encryption key for the dst encryption root using the hex key since we have now sent the updated encryption root after changing the key.
+ echo 0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef
+ zfs load-key dst/encryptionroot

Try to remount dst encrypted datasets.
+ zfs mount dst/encryptionroot
+ zfs mount dst/encryptionroot/child

RESULT: Child encrypted dataset is decryptable again. Hypothesis confirmed!

NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
src                           -                   none        -            yes      3822096046979704342
src/encryptionroot            src/encryptionroot  hex         available    yes      10687499872806328230
src/encryptionroot@111        src/encryptionroot  -           available    -        16650389156603898046
src/encryptionroot@222        src/encryptionroot  -           available    -        157927145464667221
src/encryptionroot/child      src/encryptionroot  hex         available    yes      15788284772663365294
src/encryptionroot/child@111  src/encryptionroot  -           available    -        8879828033920251704
src/encryptionroot/child@222  src/encryptionroot  -           available    -        6286619359795670820
NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
dst                           -                   none        -            yes      1835983340793043086
dst/encryptionroot            dst/encryptionroot  hex         available    yes      6911130245015256647
dst/encryptionroot@111        dst/encryptionroot  -           available    -        16650389156603898046
dst/encryptionroot@222        dst/encryptionroot  -           available    -        157927145464667221
dst/encryptionroot/child      dst/encryptionroot  hex         available    yes      15804809318195285947
dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        8879828033920251704
dst/encryptionroot/child@222  dst/encryptionroot  -           available    -        6286619359795670820
/src
âââ encryptionroot
    âââ 222
    âââ child
        âââ 222
/dst
âââ encryptionroot
    âââ 222
    âââ child
        âââ 222&lt;/code&gt;
    &lt;head rend="h3"&gt;Not so easy in practice&lt;/head&gt;
    &lt;p&gt; Unfortunately, this isn't enough to fix &lt;code&gt;new&lt;/code&gt; and &lt;code&gt;sneakernet&lt;/code&gt;; there are no remaining snapshots or bookmarks left on the &lt;code&gt;old&lt;/code&gt; encryption root from before the key change, and we can't generate an incremental send stream without one. Mapped to our reproduced example, this means that &lt;code&gt;src/encryptionroot@111&lt;/code&gt; does not exist.&lt;/p&gt;
    &lt;p&gt; You might think we could forcibly send the entire encryption root, but &lt;code&gt;zfs recv&lt;/code&gt; will reject it no matter what you do.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo zfs send --raw src/encryptionroot@222 | sudo zfs recv dst/encryptionroot
cannot receive new filesystem stream: destination 'dst/encryptionroot' exists
must specify -F to overwrite it

sam@zfshax:~ $ sudo zfs send --raw src/encryptionroot@222 | sudo zfs recv -F dst/encryptionroot
cannot receive new filesystem stream: destination has snapshots (eg. dst/encryptionroot@111)
must destroy them to overwrite it

sam@zfshax:~ $ sudo zfs destroy dst/encryptionroot@111
sam@zfshax:~ $ sudo zfs send --raw src/encryptionroot@222 | sudo zfs recv -F dst/encryptionroot
cannot receive new filesystem stream: zfs receive -F cannot be used to destroy an encrypted filesystem or overwrite an unencrypted one with an encrypted one&lt;/code&gt;
    &lt;p&gt;Lesson: Create bookmarks before destroying snapshots.&lt;/p&gt;
    &lt;p&gt;We need to find a way to create an incremental send stream that contains the key change, but how?. We could try to manually craft a send stream containing the new key, but that sounds tricky. There's got to be a better way!&lt;/p&gt;
    &lt;head rend="h3"&gt;Idea for a hack&lt;/head&gt;
    &lt;p&gt;Recall that a snapshot is not the only valid source for generating an incremental send stream. What if we had a bookmark?&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo ./reproduce &amp;gt; /dev/null 2&amp;gt;&amp;amp;1
sam@zfshax:~ $ sudo ./repair_bookmark

Replace the initial parent encrypted dataset snapshot with a bookmark.
+ zfs bookmark src/encryptionroot@111 src/encryptionroot#111
+ zfs destroy src/encryptionroot@111

HYPOTHESIS: The child encrypted dataset should become decryptable again if a snapshot containing the key change on the root encrypted dataset is sent.

Send a raw incremental snapshot of the root encrypted dataset to the dst pool using the bookmark.
+ zfs send --raw -i src/encryptionroot#111 src/encryptionroot@222
+ zfs recv -F dst/encryptionroot

Unmount the dst encrypted datasets and and unload the cached encryption key.
+ zfs unmount dst/encryptionroot
+ zfs unload-key dst/encryptionroot

Load the encryption key for the dst encryption root using the hex key since we have now sent the updated encryption root after changing the key.
+ echo 0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef
+ zfs load-key dst/encryptionroot

Try to remount dst encrypted datasets.
+ zfs mount dst/encryptionroot
+ zfs mount dst/encryptionroot/child

RESULT: Child encrypted dataset is decryptable again. Hypothesis confirmed!

NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
src                           -                   none        -            yes      1018261135296547862
src/encryptionroot            src/encryptionroot  hex         available    yes      1985286651877572312
src/encryptionroot@222        src/encryptionroot  -           available    -        4582898506955533479
src/encryptionroot#111        -                   -           -            -        4964628655505655411
src/encryptionroot/child      src/encryptionroot  hex         available    yes      12927592016081051429
src/encryptionroot/child@111  src/encryptionroot  -           available    -        15551239789901400488
src/encryptionroot/child@222  src/encryptionroot  -           available    -        11729357375613972731
NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
dst                           -                   none        -            yes      15258247229701443799
dst/encryptionroot            dst/encryptionroot  hex         available    yes      17755083343181277380
dst/encryptionroot@111        dst/encryptionroot  -           available    -        4964628655505655411
dst/encryptionroot@222        dst/encryptionroot  -           available    -        4582898506955533479
dst/encryptionroot/child      dst/encryptionroot  hex         available    yes      364333975888407846
dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        15551239789901400488
dst/encryptionroot/child@222  dst/encryptionroot  -           available    -        11729357375613972731
/src
âââ encryptionroot
    âââ 222
    âââ child
        âââ 222
/dst
âââ encryptionroot
    âââ 222
    âââ child
        âââ 222&lt;/code&gt;
    &lt;p&gt; A bookmark works just as well as a snapshot for generating an incremental send stream, but we don't have a bookmark on &lt;code&gt;old&lt;/code&gt; either. How is this any better?&lt;/p&gt;
    &lt;p&gt;Unlike a snapshot, which is effectively an entire dataset tree frozen in time (very complex), a bookmark is a very simple object on disk which consists of:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The GUID of the snapshot.&lt;/item&gt;
      &lt;item&gt;The transaction group the snapshot was created in.&lt;/item&gt;
      &lt;item&gt;The Unix timestamp when the snapshot was created.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example, this is what our bookmark looks like in &lt;code&gt;zdb&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo zdb src/encryptionroot#111
	#111: {guid: 44e5e7755d23c673 creation_txg: 12 creation_time: 1756699200 redaction_obj: 0}&lt;/code&gt;
    &lt;p&gt; Note that &lt;code&gt;zdb&lt;/code&gt; shows the GUID in hexadecimal versus &lt;code&gt;zfs get guid&lt;/code&gt; which shows it in decimal, consistency be damned. The &lt;code&gt;redaction_obj&lt;/code&gt; is optional and only used for redaction bookmarks, so we can ignore it.&lt;/p&gt;
    &lt;p&gt;A bookmark is simple enough that we could feasibly hack ZFS into manually writing one for us, provided that we can figure out the right values to use. The GUID and Unix timestamp don't really matter for generating an incremental send stream, so we could choose them arbitrarily if we had to, but the transaction group id really matters because that is what ZFS uses to determine which blocks to include.&lt;/p&gt;
    &lt;p&gt;But how can we figure out what transaction group the snapshot was created in if neither the snapshot nor a bookmark of the snapshot still exist? I initially considered walking the dataset trees on each pool, diffing them to find the newest block present on both datasets, and using its transaction group id, but I found a much easier way with one of ZFS's lesser known features.&lt;/p&gt;
    &lt;head rend="h3"&gt;A brief detour into pool histories&lt;/head&gt;
    &lt;p&gt;I didn't know about pool histories before embarking on this unplanned journey, but they are now yet another thing I love about ZFS. Every pool allocates 0.1% of its space (128 KiB minimum, 1 GiB maximum) to a ring buffer which is used to log every command that is executed on the pool. This can be used to forensically reconstruct the state of the pool over time.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo zpool history
History for 'dst':
2025-09-01.00:00:00 zpool create -o ashift=12 -m /dst dst /dst.img
2025-09-01.00:00:00 zfs recv dst/encryptionroot
2025-09-01.00:00:00 zfs load-key dst/encryptionroot
2025-09-01.00:00:00 zfs recv -F dst/encryptionroot/child
2025-09-01.00:00:00 zfs unload-key dst/encryptionroot
2025-09-01.00:00:00 zfs load-key dst/encryptionroot

History for 'src':
2025-09-01.00:00:00 zpool create -o ashift=12 -m /src src /src.img
2025-09-01.00:00:00 zfs create -o encryption=on -o keyformat=passphrase -o keylocation=prompt src/encryptionroot
2025-09-01.00:00:00 zfs create src/encryptionroot/child
2025-09-01.00:00:00 zfs snapshot -r src/encryptionroot@111
2025-09-01.00:00:00 zfs send --replicate --raw src/encryptionroot@111
2025-09-01.00:00:00 zfs change-key -o keyformat=hex src/encryptionroot
2025-09-01.00:00:00 zfs snapshot -r src/encryptionroot@222&lt;/code&gt;
    &lt;p&gt; ZFS also logs many internal operations in the pool history (search for &lt;code&gt;spa_history_log&lt;/code&gt; in the source code) which can be viewed with the &lt;code&gt;-i&lt;/code&gt; flag. For snapshots, this includes the transaction group (txg) id when the snapshot was created, which is exactly what we're looking for!&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo zpool history -i src
History for 'src':
...
2025-09-01.00:00:00 [txg:12] snapshot src/encryptionroot@111 (768)
2025-09-01.00:00:00 [txg:12] snapshot src/encryptionroot/child@111 (770)
2025-09-01.00:00:00 (3ms) ioctl snapshot
    input:
        snaps:
            src/encryptionroot@111
            src/encryptionroot/child@111
        props:

2025-09-01.00:00:00 zfs snapshot -r src/encryptionroot@111
...&lt;/code&gt;
    &lt;p&gt; The GUID and creation timestamp we can easily get from the snapshot on &lt;code&gt;dst&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo zfs list -o name,guid,creation -p dst/encryptionroot@111
NAME                                   GUID  CREATION
dst/encryptionroot@111  4964628655505655411  1756699200&lt;/code&gt;
    &lt;p&gt;Now that we know everything we need to create the bookmark, we just need to figure out a way to manually create a bookmark with arbitrary data.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hacking ZFS to manually create a bookmark&lt;/head&gt;
    &lt;p&gt; To understand how ZFS creates a bookmark, we can trace the code path from &lt;code&gt;zfs bookmark&lt;/code&gt; all the way down to &lt;code&gt;dsl_bookmark_add&lt;/code&gt; which actually adds the bookmark node to the tree.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;command_table&lt;/item&gt;
      &lt;item&gt;zfs_do_bookmark&lt;/item&gt;
      &lt;item&gt;lzc_bookmark&lt;/item&gt;
      &lt;item&gt;lzc_ioctl&lt;/item&gt;
      &lt;item&gt;zfs_ioctl_fd&lt;/item&gt;
      &lt;item&gt;zcmd_ioctl_compat&lt;/item&gt;
      &lt;item&gt;zfs_ioctl_register bookmark&lt;/item&gt;
      &lt;item&gt;zfs_ioc_bookmark&lt;/item&gt;
      &lt;item&gt;dsl_bookmark_create&lt;/item&gt;
      &lt;item&gt;dsl_bookmark_create_sync&lt;/item&gt;
      &lt;item&gt;dsl_bookmark_create_sync_impl_book&lt;/item&gt;
      &lt;item&gt;dsl_bookmark_node_add&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is the bookmark structure physically written to disk:&lt;/p&gt;
    &lt;p&gt;zfs/include/sys/dsl_bookmark.h&lt;/p&gt;
    &lt;code&gt;/*
 * On disk zap object.
 */
typedef struct zfs_bookmark_phys {
	uint64_t zbm_guid;		/* guid of bookmarked dataset */
	uint64_t zbm_creation_txg;	/* birth transaction group */
	uint64_t zbm_creation_time;	/* bookmark creation time */

	/* fields used for redacted send / recv */
	uint64_t zbm_redaction_obj;	/* redaction list object */
	uint64_t zbm_flags;		/* ZBM_FLAG_* */

	/* fields used for bookmark written size */
	uint64_t zbm_referenced_bytes_refd;
	uint64_t zbm_compressed_bytes_refd;
	uint64_t zbm_uncompressed_bytes_refd;
	uint64_t zbm_referenced_freed_before_next_snap;
	uint64_t zbm_compressed_freed_before_next_snap;
	uint64_t zbm_uncompressed_freed_before_next_snap;

	/* fields used for raw sends */
	uint64_t zbm_ivset_guid;
} zfs_bookmark_phys_t;


#define	BOOKMARK_PHYS_SIZE_V1	(3 * sizeof (uint64_t))
#define	BOOKMARK_PHYS_SIZE_V2	(12 * sizeof (uint64_t))&lt;/code&gt;
    &lt;p&gt; Only the first 3 fields are required for v1 bookmarks, while v2 bookmarks contain all 12 fields. &lt;code&gt;dsl_bookmark_node_add&lt;/code&gt; only writes a v2 bookmark if one of the 9 v2 fields are non-zero, so we can leave them all zero to write a v1 bookmark.&lt;/p&gt;
    &lt;p&gt; After a few iterations, I had a patch which hijacks the normal &lt;code&gt;zfs bookmark pool/dataset#src pool/dataset#dst&lt;/code&gt; code path to create a bookmark with arbitrary data when the source bookmark name is &lt;code&gt;missing&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~/zfs $ git --no-pager diff
sam@zfshax:~/zfs $ git --no-pager diff
diff --git a/cmd/zfs/zfs_main.c b/cmd/zfs/zfs_main.c
index 2d81ef31c..73b5d7e70 100644
--- a/cmd/zfs/zfs_main.c
+++ b/cmd/zfs/zfs_main.c
@@ -7892,12 +7892,15 @@ zfs_do_bookmark(int argc, char **argv)
 		default: abort();
 	}

+// Skip testing for #missing because it does not exist.
+if (strstr(source, "#missing") == NULL) {
 	/* test the source exists */
 	zfs_handle_t *zhp;
 	zhp = zfs_open(g_zfs, source, source_type);
 	if (zhp == NULL)
 		goto usage;
 	zfs_close(zhp);
+}

 	nvl = fnvlist_alloc();
 	fnvlist_add_string(nvl, bookname, source);
diff --git a/module/zfs/dsl_bookmark.c b/module/zfs/dsl_bookmark.c
index 861dd9239..fae882f45 100644
--- a/module/zfs/dsl_bookmark.c
+++ b/module/zfs/dsl_bookmark.c
@@ -263,7 +263,12 @@ dsl_bookmark_create_check_impl(dsl_pool_t *dp,
 		 * Source must exists and be an earlier point in newbm_ds's
 		 * timeline (newbm_ds's origin may be a snap of source's ds)
 		 */
+// Skip looking up #missing because it does not exist.
+if (strstr(source, "#missing") == NULL) {
 		error = dsl_bookmark_lookup(dp, source, newbm_ds, &amp;amp;source_phys);
+} else {
+		error = 0;
+}
 		switch (error) {
 		case 0:
 			break; /* happy path */
@@ -545,12 +550,34 @@ dsl_bookmark_create_sync_impl_book(
 	 *   because the redaction object might be too large
 	 */

+// Skip looking up #missing because it does not exist.
+if (strstr(source_name, "#missing") == NULL) {
 	VERIFY0(dsl_bookmark_lookup_impl(bmark_fs_source, source_shortname,
 	    &amp;amp;source_phys));
+}
 	dsl_bookmark_node_t *new_dbn = dsl_bookmark_node_alloc(new_shortname);

+// Skip copying from #missing because it does not exist.
+if (strstr(source_name, "#missing") == NULL) {
 	memcpy(&amp;amp;new_dbn-&amp;gt;dbn_phys, &amp;amp;source_phys, sizeof (source_phys));
 	new_dbn-&amp;gt;dbn_phys.zbm_redaction_obj = 0;
+} else {
+	// Manually set the bookmark parameters.
+	new_dbn-&amp;gt;dbn_phys = (zfs_bookmark_phys_t){
+		.zbm_guid = 4964628655505655411,
+		.zbm_creation_txg = 12,
+		.zbm_creation_time = 1756699200,
+		.zbm_redaction_obj = 0,
+		.zbm_flags = 0,
+		.zbm_referenced_bytes_refd = 0,
+		.zbm_compressed_bytes_refd = 0,
+		.zbm_uncompressed_bytes_refd = 0,
+		.zbm_referenced_freed_before_next_snap = 0,
+		.zbm_compressed_freed_before_next_snap = 0,
+		.zbm_uncompressed_freed_before_next_snap = 0,
+		.zbm_ivset_guid = 0,
+	};
+}

 	/* update feature counters */
 	if (new_dbn-&amp;gt;dbn_phys.zbm_flags &amp;amp; ZBM_FLAG_HAS_FBN) {
&lt;/code&gt;
    &lt;p&gt;To test, we recompile ZFS, reload the kernel module, and reimport the pools.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~/zfs $ gmake -s -j$(sysctl -n hw.ncpu)
sam@zfshax:~/zfs $ sudo gmake install &amp;amp;&amp;amp; sudo ldconfig
sam@zfshax:~/zfs $ sudo zpool export src &amp;amp;&amp;amp; sudo zpool export dst
sam@zfshax:~/zfs $ sudo ./scripts/zfs.sh -r
sam@zfshax:~/zfs $ sudo zpool import src -d / &amp;amp;&amp;amp; sudo zpool import dst -d /&lt;/code&gt;
    &lt;p&gt; Then, we create the bookmark ex nihilo using the magic bookmark name &lt;code&gt;missing&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~/zfs $ sudo zfs bookmark src/encryptionroot#missing src/encryptionroot#111
sam@zfshax:~/zfs $ sudo zdb src/encryptionroot#111
	#111: {guid: 44e5e7755d23c673 creation_txg: 12 creation_time: 1756699200 redaction_obj: 0}&lt;/code&gt;
    &lt;p&gt;Success! We can now use the bookmark to generate an incremental send stream containing the new hex wrapping key parameters.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~/zfs $ sudo zfs send --raw -i src/encryptionroot#111 src/encryptionroot@222 | zstreamdump
BEGIN record
	hdrtype = 1
	features = 1420004
	magic = 2f5bacbac
	creation_time = 68d3d93f
	type = 2
	flags = 0xc
	toguid = 3f99b9e92cc0aca7
	fromguid = 44e5e7755d23c673
	toname = src/encryptionroot@222
	payloadlen = 1028
nvlist version: 0
	crypt_keydata = (embedded nvlist)
	nvlist version: 0
		DSL_CRYPTO_SUITE = 0x8
		DSL_CRYPTO_GUID = 0x6196311f2622e30
		DSL_CRYPTO_VERSION = 0x1
		DSL_CRYPTO_MASTER_KEY_1 = 0x6c 0x55 0x13 0x78 0x8c 0x2d 0x42 0xb5 0x9e 0x33 0x2 0x7e 0x73 0x3a 0x46 0x20 0xd2 0xf7 0x23 0x7d 0x7c 0x5d 0x5f 0x76 0x63 0x90 0xd2 0x43 0x6a 0xdd 0x63 0x2b
		DSL_CRYPTO_HMAC_KEY_1 = 0x85 0xd1 0xf3 0xba 0xed 0xec 0x6 0x28 0x36 0xd6 0x60 0x28 0x8d 0x2f 0x6f 0x14 0xc9 0x2b 0x6f 0xf4 0x19 0x23 0x2d 0xf 0x3d 0xe 0xc4 0x88 0x4 0x6d 0xca 0xb5 0x2d 0x4d 0x8 0x75 0x17 0x1c 0xe3 0xe7 0xe6 0x23 0x7 0x53 0x94 0xba 0xc7 0x4b 0xf5 0xde 0x8c 0x29 0xa3 0x27 0xdf 0x82 0x64 0x9d 0x92 0xb4 0xc1 0x26 0x5b 0x32
		DSL_CRYPTO_IV = 0xdf 0x52 0x77 0xe8 0xf 0xfd 0xc2 0x42 0x66 0x88 0xb9 0xf0
		DSL_CRYPTO_MAC = 0x54 0x54 0x15 0xa4 0x21 0x55 0x6b 0x4e 0x99 0xe7 0xf 0xef 0x9f 0x90 0x42 0x54
		portable_mac = 0x3a 0xd6 0x30 0xc4 0x6a 0x2d 0x60 0x24 0x95 0xfc 0x99 0xbb 0xfa 0x10 0xa0 0x6b 0xc6 0x1 0xdd 0x1d 0x9 0xcd 0xa8 0x19 0xdf 0x57 0xb9 0x90 0x4f 0x2e 0x33 0xc1
		keyformat = 0x2
		pbkdf2iters = 0x0
		pbkdf2salt = 0x0
		mdn_checksum = 0x0
		mdn_compress = 0x0
		mdn_nlevels = 0x6
		mdn_blksz = 0x4000
		mdn_indblkshift = 0x11
		mdn_nblkptr = 0x3
		mdn_maxblkid = 0x4
		to_ivset_guid = 0x957edeaa7123a7
		from_ivset_guid = 0x0
	(end crypt_keydata)

END checksum = 14046201258/62f53166ccc36/14023a70758c3195/1e906f4670783cd
SUMMARY:
	Total DRR_BEGIN records = 1 (1028 bytes)
	Total DRR_END records = 1 (0 bytes)
	Total DRR_OBJECT records = 7 (960 bytes)
	Total DRR_FREEOBJECTS records = 2 (0 bytes)
	Total DRR_WRITE records = 1 (512 bytes)
	Total DRR_WRITE_BYREF records = 0 (0 bytes)
	Total DRR_WRITE_EMBEDDED records = 0 (0 bytes)
	Total DRR_FREE records = 12 (0 bytes)
	Total DRR_SPILL records = 0 (0 bytes)
	Total records = 26
	Total payload size = 2500 (0x9c4)
	Total header overhead = 8112 (0x1fb0)
	Total stream length = 10612 (0x2974)&lt;/code&gt;
    &lt;p&gt;But we can't receive the send stream.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo zfs send --raw -i src/encryptionroot#111 src/encryptionroot@222 | sudo zfs recv -F dst/encryptionroot
cannot receive incremental stream: IV set guid missing. See errata 4 at https://openzfs.github.io/openzfs-docs/msg/ZFS-8000-ER.&lt;/code&gt;
    &lt;head rend="h3"&gt;The final obstacle&lt;/head&gt;
    &lt;p&gt; ZFS refuses the stream because it is missing a source IV set GUID (see &lt;code&gt;from_ivset_guid = 0x0&lt;/code&gt; in the &lt;code&gt;zstreamdump&lt;/code&gt; above). This is because we created a v1 bookmark which does not contain the IV set GUID like a v2 bookmark would.&lt;/p&gt;
    &lt;p&gt;Since we know that the send stream is created using the right snapshots, we can temporarily disable checking IV set GUIDs to allow the snapshot to be received as described in errata 4.&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ sudo sysctl vfs.zfs.disable_ivset_guid_check=1
vfs.zfs.disable_ivset_guid_check: 0 -&amp;gt; 1
sam@zfshax:~ $ sudo zfs send --raw -i src/encryptionroot#111 src/encryptionroot@222 | sudo zfs recv -F dst/encryptionroot
sam@zfshax:~ $ sudo sysctl vfs.zfs.disable_ivset_guid_check=0
vfs.zfs.disable_ivset_guid_check: 1 -&amp;gt; 0
sam@zfshax:~ $ sudo zpool export dst
sam@zfshax:~ $ sudo zpool import dst -d /
sam@zfshax:~ $ sudo zpool scrub dst
sam@zfshax:~ $ sudo zpool status -x
all pools are healthy&lt;/code&gt;
    &lt;head rend="h3"&gt;The moment of truth&lt;/head&gt;
    &lt;p&gt;And now for the moment of truthâ¦&lt;/p&gt;
    &lt;code&gt;sam@zfshax:~ $ echo "0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef" | sudo zfs load-key dst/encryptionroot
sam@zfshax:~ $ sudo zfs mount -a
sam@zfshax:~ $ sudo zfs list -t all -o name,encryptionroot,keyformat,keystatus,mounted,guid -r dst
NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
dst                           -                   none        -            yes      15258247229701443799
dst/encryptionroot            dst/encryptionroot  hex         available    yes      17755083343181277380
dst/encryptionroot@111        dst/encryptionroot  -           available    -        4964628655505655411
dst/encryptionroot@222        dst/encryptionroot  -           available    -        4582898506955533479
dst/encryptionroot/child      dst/encryptionroot  hex         available    yes      364333975888407846
dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        15551239789901400488
dst/encryptionroot/child@222  dst/encryptionroot  -           available    -        11729357375613972731
sam@zfshax:~ $ tree --noreport --noreport /dst
/dst
âââ encryptionroot
    âââ 222
    âââ child
        âââ 222&lt;/code&gt;
    &lt;p&gt; At this point, we can now reliably fix the issue in our test environment. All we need to do now is use our hacked ZFS build to create the bookmark on &lt;code&gt;old&lt;/code&gt;, send an incremental snapshot of the encryption root with the new key to &lt;code&gt;sneakernet&lt;/code&gt;, and then send that snapshot from &lt;code&gt;sneakernet&lt;/code&gt; to &lt;code&gt;new&lt;/code&gt;. I rebuilt ZFS again with the correct transaction group, GUID, and creation timestamp for &lt;code&gt;old&lt;/code&gt;, repeated the same steps with the names changed, and thanks to our thorough testing, it worked on the first try!&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;After a week of intense research and debugging, I had rescued our data back from the brink and could again sleep soundly at night. While I appreciated the opportunity to learn more about ZFS, I can't help but think about how this entire incident could have been avoided at several key points which translate directly into lessons learned:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Test backups continuously so you get immediate feedback when they break.&lt;/item&gt;
      &lt;item&gt;Wait to make all destructive changes together at the very end instead of interspersed where they could accidentally be performed in the wrong order.&lt;/item&gt;
      &lt;item&gt;Always send a snapshot of the encryption root after changing the encryption key.&lt;/item&gt;
      &lt;item&gt;Create bookmarks before destroying snapshots.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I hope that you may learn from my mistakes and avoid a similar incident. If you do happen to find yourself in a similar predicament, I'd love to hear from you regardless of whether this postmortem was helpful or not. My contact details can be found here.&lt;/p&gt;
    &lt;p&gt;Knowing what I now know about ZFS native encryption, I find it difficult to recommend until the sharp edges have all been filed down. In most cases, I'd prefer to encrypt the entire pool at the block device level and encrypt send streams with age. But if you really do need the flexibility offered by native encryption, always remember to mind the encryptionroot!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45431167</guid><pubDate>Tue, 30 Sep 2025 20:58:52 +0000</pubDate></item><item><title>Introduction to Multi-Armed Bandits</title><link>https://arxiv.org/abs/1904.07272</link><description>&lt;doc fingerprint="e68c8632ccf7c28f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Machine Learning&lt;/head&gt;&lt;p&gt; [Submitted on 15 Apr 2019 (v1), last revised 3 Apr 2024 (this version, v8)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Introduction to Multi-Armed Bandits&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Multi-armed bandits a simple but very powerful framework for algorithms that make decisions over time under uncertainty. An enormous body of work has accumulated over the years, covered in several books and surveys. This book provides a more introductory, textbook-like treatment of the subject. Each chapter tackles a particular line of work, providing a self-contained, teachable technical introduction and a brief review of the further developments; many of the chapters conclude with exercises.&lt;lb/&gt;The book is structured as follows. The first four chapters are on IID rewards, from the basic model to impossibility results to Bayesian priors to Lipschitz rewards. The next three chapters cover adversarial rewards, from the full-feedback version to adversarial bandits to extensions with linear rewards and combinatorially structured actions. Chapter 8 is on contextual bandits, a middle ground between IID and adversarial bandits in which the change in reward distributions is completely explained by observable contexts. The last three chapters cover connections to economics, from learning in repeated games to bandits with supply/budget constraints to exploration in the presence of incentives. The appendix provides sufficient background on concentration and KL-divergence.&lt;lb/&gt;The chapters on "bandits with similarity information", "bandits with knapsacks" and "bandits and agents" can also be consumed as standalone surveys on the respective topics.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Aleksandrs Slivkins [view email]&lt;p&gt;[v1] Mon, 15 Apr 2019 18:17:01 UTC (510 KB)&lt;/p&gt;&lt;p&gt;[v2] Mon, 29 Apr 2019 20:45:01 UTC (510 KB)&lt;/p&gt;&lt;p&gt;[v3] Tue, 25 Jun 2019 14:39:03 UTC (536 KB)&lt;/p&gt;&lt;p&gt;[v4] Sun, 15 Sep 2019 02:06:22 UTC (557 KB)&lt;/p&gt;&lt;p&gt;[v5] Mon, 30 Sep 2019 00:15:42 UTC (543 KB)&lt;/p&gt;&lt;p&gt;[v6] Sat, 26 Jun 2021 20:15:32 UTC (639 KB)&lt;/p&gt;&lt;p&gt;[v7] Sat, 8 Jan 2022 20:05:40 UTC (627 KB)&lt;/p&gt;&lt;p&gt;[v8] Wed, 3 Apr 2024 21:32:42 UTC (629 KB)&lt;/p&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.LG&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45431271</guid><pubDate>Tue, 30 Sep 2025 21:08:28 +0000</pubDate></item></channel></rss>