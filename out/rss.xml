<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 12 Nov 2025 18:15:30 +0000</lastBuildDate><item><title>I didn't reverse-engineer the protocol for my blood pressure monitor in 24 hours</title><link>https://james.belchamber.com/articles/blood-pressure-monitor-reverse-engineering/</link><description>&lt;doc fingerprint="b6398d534b2d77a1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I didn't reverse-engineer the protocol for my blood pressure monitor in 24 hours&lt;/head&gt;
    &lt;p&gt;Yesterday after receiving my yearly flu vaccine at the pharmacy I was offered a blood pressure test, which reported a reading that made the young pharmacist who had just given me my vaccine a bit worried.&lt;/p&gt;
    &lt;p&gt;Off the back of this she offered me a 24 hour study, and then strapped a cuff to my arm plumbed into a little device which I had to wear in a little caddy - the cuff would inflate every 30 minutes during the day and every 60 minutes during the night, and then tomorrow I would bring it back for analysis.&lt;/p&gt;
    &lt;p&gt;"Can I read the measurements?" I asked, as it was being strapped to me.&lt;/p&gt;
    &lt;p&gt;"Oh, no, that will just stress you out. We turn that off". Fair enough.&lt;/p&gt;
    &lt;p&gt;Thing is, this device had a little micro-USB port on the side.&lt;/p&gt;
    &lt;head rend="h1"&gt;Doing things the proper way&lt;/head&gt;
    &lt;p&gt;I had started researching the device - a Microlife WatchBP O3 - before I got out of the chemist, and once I'd got back to the office I downloaded the software that's freely available to interact with it, setting up a Bottles instance to run the software since I don't (knowingly) have a Windows machine within 100 metres of me.&lt;/p&gt;
    &lt;p&gt;Unfortunately it didn't seem to be able to access the device, and I had no clue why. In Linux it was just presenting as a standard &lt;code&gt;hidraw&lt;/code&gt; device:&lt;/p&gt;
    &lt;code&gt;[33301.736724] hid-generic 0003:04D9:B554.001E: hiddev96,hidraw1: USB HID v1.11 Device [USB HID UART Bridge] on usb-0000:c5:00.0-1/input0
&lt;/code&gt;
    &lt;p&gt;Fine, I'll install windows.&lt;/p&gt;
    &lt;p&gt;After dodging around Microsoft's idea of UX, and then forwarding the USB device to the VM (I used Gnome Boxes for this, works nicely), I finally got to see WatchBP Analyzer with the data downloaded from the device.&lt;/p&gt;
    &lt;p&gt;But I don't want to open a Virtual Machine running Windows to see this data, and anyway - I'm pretty sure that reverse-engineering this will be good for my blood pressure.&lt;/p&gt;
    &lt;head rend="h1"&gt;Sniffing the traffic&lt;/head&gt;
    &lt;p&gt;Since I'm running this in a Virtual Machine I can just rely on Wireshark in Linux to get the traffic between the host and the device. &lt;code&gt;usbmon&lt;/code&gt; is already installed and we know that the device is on Bus 3, so we can select usbmon3 on startup and start capturing.&lt;/p&gt;
    &lt;p&gt;I'm very much out of my depth at this point but, being one of those who could land a plane in an emergency (why would you talk yourself out of it?!) I decided to crack on regardless. I know that the interesting stuff is sent after I press "Download", and I know that something in there is gonna say "my blood pressure is 137/113" - so let's look for that. Just convert to show bytes as decimal and..&lt;/p&gt;
    &lt;p&gt;..that looks like a blood pressure! Let's copy that out as hex:&lt;/p&gt;
    &lt;code&gt;05 0a 89 71 43 9b
&lt;/code&gt;
    &lt;p&gt;I'm not sure if this is "valid" HID Data (Wireshark seems convinced that only the first byte is the Vendor Data, with the rest being padding) but it seems like the data is being sent in 32-byte "chunks", of which the first byte tells you the number of significant (SIG) following bits in the chunk (I deleted the rest - all zeroes - for clarity). The third byte is my Systolic blood Pressure (SYS), the fourth is my Diastolic blood pressure (DIA), and the fifth is my heart rate (HR) - no clue what the second or last byte is, but let's find all other bytes with my blood pressure in them (in decimal this time, because I can't read hex without help):&lt;/p&gt;
    &lt;code&gt;SIG ??? SYS DIA  HR ??? ??? ???
  5  10 137 113  67 155
  5   0 132  86  68 155
  6   0 126  84  82 155  83
  6  10 128  80  61 155  83
  7   0 148  93  65 155  83  64
  7   0 121  92  74 155  83  94
  7   0 123  83  65 155  83  95
  7   0 123  79  78 155  83 129
&lt;/code&gt;
    &lt;p&gt;Hmm. So we're still looking for the Oscillometric signal peak pressure (OPP)as well as some timestamps (we can calculate Mean arterial pressure - MAP - as &lt;code&gt;(2*DIA+SYS)/3&lt;/code&gt;, according to the manual, and Pulse Pressure (PP) is just &lt;code&gt;SYS-DIA&lt;/code&gt;). We can see the OPP in the packets that come after each of those above, but they don't seem to consistently come in on the same line:&lt;/p&gt;
    &lt;code&gt; 10  82  195   80 *121    0    0    0    0    0    0
 10  82  223   80  *95    0    0    0    0    0    0
  9   1   80  *90    0    0    0    0    0    0
  9  35   80  *86    0    0    0    0    0    0
  8  80 *103    0    0    0    0    0    0
  8  80 *106    0    0    0    0    0    0
  8  80  *90    0    0    0    0    0    0
 10  80  *88    0    0    0    0    0    0   29  251
&lt;/code&gt;
    &lt;p&gt;Oh. Maybe if I stick them together?&lt;/p&gt;
    &lt;code&gt;??? SYS DIA  HR ??? ??? ??? ??? OPP ??? ??? ??? ??? ??? ??? ??? ???
 10 137 113  67 155  82 195  80 121   0   0   0   0   0   0
  0 132  86  68 155  82 223  80  95   0   0   0   0   0   0
  0 126  84  82 155  83   1  80  90   0   0   0   0   0   0
 10 128  80  61 155  83  35  80  86   0   0   0   0   0   0
  0 148  93  65 155  83  64  80 103   0   0   0   0   0   0
  0 121  92  74 155  83  94  80 106   0   0   0   0   0   0
  0 123  83  65 155  83  95  80  90   0   0   0   0   0   0
  0 123  79  78 155  83 129  80  88   0   0   0   0   0   0  29 251
&lt;/code&gt;
    &lt;p&gt;Right, timestamps. I first guessed that the four populated contiguous bytes between &lt;code&gt;HR&lt;/code&gt; and &lt;code&gt;OPP&lt;/code&gt; are a 32-bit unix timestamp, but that would make the first one &lt;code&gt;9B52C350&lt;/code&gt;; either &lt;code&gt;Jul 29 2052&lt;/code&gt; or &lt;code&gt;Dec 08 2012&lt;/code&gt; depending on which endianness the protocol is into. The 8 readings we have here are all from &lt;code&gt;November 10th&lt;/code&gt;, at &lt;code&gt;11:03&lt;/code&gt;, &lt;code&gt;11:31&lt;/code&gt;, &lt;code&gt;12:01&lt;/code&gt;, &lt;code&gt;12:35&lt;/code&gt;, &lt;code&gt;13:00&lt;/code&gt;, &lt;code&gt;13:30&lt;/code&gt;, &lt;code&gt;13:31&lt;/code&gt; and &lt;code&gt;14:01&lt;/code&gt;, which isn't.. isn't that.&lt;/p&gt;
    &lt;p&gt;But note that the number in the 6th column flips from &lt;code&gt;82&lt;/code&gt; to &lt;code&gt;83&lt;/code&gt; when we switch from AM to PM - that's something, and when it does the 7th column resets. And hey - &lt;code&gt;1&lt;/code&gt;, &lt;code&gt;35&lt;/code&gt;, &lt;code&gt;64&lt;/code&gt;, &lt;code&gt;94&lt;/code&gt;, &lt;code&gt;95&lt;/code&gt;.. that seems dangerously close to &lt;code&gt;12:01&lt;/code&gt;, &lt;code&gt;12:35&lt;/code&gt;, &lt;code&gt;13:00&lt;/code&gt;, &lt;code&gt;13:30&lt;/code&gt; and &lt;code&gt;13:31&lt;/code&gt; if you were just to count the minutes. What's going on?&lt;/p&gt;
    &lt;head rend="h1"&gt;Deadlines and dead ends&lt;/head&gt;
    &lt;p&gt;I tried feeding a lot of this into various Als (Kagi gives you access to a few with a nice interface) and I found that they mostly were stupid in ways that made me think. A few times I thought they had "cracked the case" but actually they just made me waste time. But they did remind me e.g. of endianness, so I did get a bit out of them.&lt;/p&gt;
    &lt;p&gt;I also spent quite a bit of time trying to write some Python that emulated the initial handshake and download button of the interface so that it could push out the data as a stream instead of me having to wrestle it out of Wireshark - again, Al had a habit of giving me incorrect code (although it did turn me on to pyhidapi).&lt;/p&gt;
    &lt;p&gt;But ultimately I had a deadline, and I had to return the device even though I wanted to spend more time with it. Possibly for the best - while it did give me some reverse engineering practice (which it turns out I really enjoy), I should do some work instead of procrastinating.&lt;/p&gt;
    &lt;p&gt;My final lesson was a new word - Normotension, normal blood pressure - and a new phrase - White Coat Hypertension, the phenomena of high blood pressure in a clinical setting. Turns out that when you check someone's blood pressure after giving them an injection, it's higher than normal.&lt;/p&gt;
    &lt;p&gt;I don't think I'd recommend getting your blood pressure tested after your next flu jab. But then, I'm not a doctor.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45893095</guid><pubDate>Tue, 11 Nov 2025 21:25:19 +0000</pubDate></item><item><title>Four strange places to see London's Roman Wall</title><link>https://diamondgeezer.blogspot.com/2025/11/odd-places-to-see-londons-roman-wall.html</link><description>&lt;doc fingerprint="af4d01017b3c27b4"&gt;
  &lt;main&gt;
    &lt;p&gt;There are manyplaces around the City of London to see its old Roman Wall, notably alongside Noble Street, in Barber Surgeons' Meadow, through the Barbican, in St Alphage Garden and just outside the entrance to Tower Hill station. Here are four of the odder spots.&lt;/p&gt;
    &lt;p&gt;Four strange places to see London's Roman Wall&lt;/p&gt;
    &lt;p&gt;1) From platform 1 at Tower Hill station&lt;/p&gt;
    &lt;p&gt;If you're ever waiting for a westbound train at Tower Hill station, take a walk to the rear of the platform and take a look across the tracks, roughly where the penultimate carriage would stop. High on the far wall is a square recess lined by black tiles, and at the back of that is a dimly-lit surface of chunky irregular blocks. Unlike every single other thing on the Underground, the Romans built that.&lt;/p&gt;
    &lt;p&gt;London's original wall was 2 miles long, 6 metres high and almost 3 metres thick at its base, all the better to keep out uncivilised marauders. It was built around 200 AD, then left to decay and rebuilt in the medieval era, again for defensive purposes. This is one of the original bits, not that you can easily tell by squinting across the tracks. A small metal lamp points inwards but is no longer switched on because heritage illumination is not a TfL priority. There is however a rather nice silver plaque on the pillar opposite, should you step back far enough to notice it.&lt;/p&gt;
    &lt;p&gt;The plaque confirms that the stones here are a continuation of the wall seen (much more clearly) outside. What it doesn't mention is the unavoidable truth that the wall must once have continued across the tracks and platforms but is no longer here. That's because when the Circle line was constructed in 1882 the railway companies had permission to demolish 22 metres of London's wall and duly did, the Victorians never being afraid to destroy ancient heritage. Ian Visits has a photo of navvies standing atop the offending stonework just before they bashed it through. The square hole is no recompense, plus you can't see anything if a train's in the platform, but it is a brilliantly quirky thing to find on the Underground.&lt;/p&gt;
    &lt;p&gt;2) Round the back of the Leonardo Royal Hotel&lt;/p&gt;
    &lt;p&gt;The short walk from Tower Hill station to the rear entrance of Fenchurch Street passes two hotels. The second is the Leonardo Royal, formerly the Grange, whose car port looks like it leads to a cocktail terrace and maybe some parking. Nothing's signed from the street, indeed I'd never thought to duck through before, but at the far end past the umbrellas of Leo's bar is a significant chunk of Roman wall.&lt;/p&gt;
    &lt;p&gt;The upper section has arched windows built for archers and square holes which once supported a timber platform. It's impressive of course merely medieval, part of the rebuild that occurred along much of the wall as the city grew and spread beyond its former border. To see the Roman section stand closer to the rail and look down, this because ground level then was a few metres lower than now. The telltale signs are several distinctive bands of thin red bricks, these added to strengthen and bond the structure, and which look like layers of jam in a particularly lumpy sponge. The entire segment behind the hotel is over 20m long, thus longer than the better-known chunk outside the station.&lt;/p&gt;
    &lt;p&gt;Perhaps the best thing about this bit of wall is that you can walk through it. A couple of steps have been added on each side allowing passage through a low medieval arch, all marked with anachronistic trip hazard markings. If steps aren't your thing you can also pass round the end of the wall on the flat. Round the other side are a glum alley and a staff back-entrance, also an exit into a separate backstreet past a sign that says PRIVATE No Public Right Of Way Beyond This Point Entry At Your Own Risk Absolutely No Liability Is Accepted For Any Reason Whatsoever. Stuff that, there's an actual Roman Wall back here.&lt;/p&gt;
    &lt;p&gt;3) From a cafe terrace&lt;/p&gt;
    &lt;p&gt;I've written about The City Wall at Vine Street before, a free attraction opened in 2023 beneath a block of student flats. Last time I had to battle the Procedural Curmudgeon to gain admittance but I'm pleased to say they've since loosened up and you can now simply gesture at the door, walk in and give your first name to a flunkey with a tablet. He rattled through the key information with all the practised enthusiasm of a call centre employee dictating terms and conditions, then sent me off down the stairs.&lt;/p&gt;
    &lt;p&gt;Two walls are filled with finds from the excavations, including an AD 70s coin and the bones of a 1760s cat. Nobody's quite sure how the ancient Greek tombstone ended up here, given it predates Londinium, but it has pride of place in a central glass case. The 5-minute historical animation is pretty good too, assuming you can read quite fast. But the main draw is the multi-layered towering remnant of wall which here has the benefit of being properly illuminated and protected from the elements. The protruding lower section (which looks much too clean to be so very old) is all that remains of an original postern, and is also unique because all the other towers elsewhere round the City are merely medieval.&lt;/p&gt;
    &lt;p&gt;What's weird is that this large basement space is overlooked by a balcony scattered with small tables at which sit students and businesspeople consuming coffee and all-day brunch. The baristas operate from the cafe upstairs but any food comes from a small kitchen down below, which has the unnerving side effect that while you're wandering around what looks like a museum it smells like an office canteen. If you choose to be tempted by a cappuccino and smashed avocado on your way out you can enjoy extra time with the Roman wall, or indeed skip the walkthrough altogether and focus only on refreshment with an absolutely unique view. I recommend a proper visit though... the visitors book awaits your praise.&lt;/p&gt;
    &lt;p&gt;4) At the rear of a car park&lt;/p&gt;
    &lt;p&gt;This is amazing on many levels, the main level being subterranean. After WW2 so much of the City was in ruins that planners drove a new dual carriageway through the Aldersgate area and called it London Wall. They believed cars were the future and to that end hid a linear car park directly underneath the new road. It's very narrow, very long and pretty grim, indeed precisely the kind of filming location you'd expect a throwback crime thriller to use for a shoot-out or kidnapping. Cars enter down a short spiral ramp and pedestrians through a grubby side door, and the numbered concrete catacombs stretch on and on for almost 400 metres. Keep walking past the white vans, Range Rovers and the attendant's cabin, trying not to attract too much attention, and almost at the far end is... blimey.&lt;/p&gt;
    &lt;p&gt;You can't park in bays 52 and 53 because they're full of Roman remains. A substantial chunk of wall slots in diagonally beneath the joists and pillars, tall enough to incorporate two separate bands of red bricks. It looks quite smooth up front but fairly rubbly round the back, also much thicker at the base than at the top. Obviously it's very risky to have a scheduled ancient monument in a car park so protective concrete blocks have been added to make sure nobody reverses into the stonework by mistake. More recently a glass screen has been added at one end, branded 'City of London' so you know who to thank, but the other end remains accessible for now (not that you should be stepping in or even touching it).&lt;/p&gt;
    &lt;p&gt;It's the contrasts that I found most incongruous. A relic from Roman times penned inbetween a speed hump and a futile pedestrian crossing. A fortification from the 3rd century beside an electric van built last year. A defensive structure that helped see off the Peasants Revolt beside a poster warning what to do in the event of fire. A boundary wall once an intrinsic part of the capital now underground illuminated by strip lights. And all this at the very far end of an oppressive bunker preserved for the benefit of hardly any eyes in a parking facility only a few know to use. Sure you can see chunks of Roman wall all around the City, even from a tube platform, hotel terrace or cafe. But the oddest spot may well be here in the London Wall car park, should you ever have the balls to take a look.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45893795</guid><pubDate>Tue, 11 Nov 2025 22:31:41 +0000</pubDate></item><item><title>.NET MAUI is coming to Linux and the browser</title><link>https://avaloniaui.net/blog/net-maui-is-coming-to-linux-and-the-browser-powered-by-avalonia</link><description>&lt;doc fingerprint="f561401a5d0495ec"&gt;
  &lt;main&gt;
    &lt;p&gt;We are bringing .NET MAUI to Linux and to the browser, powered by Avalonia.&lt;/p&gt;
    &lt;p&gt;For the past few months, we have been working on an Avalonia powered backend for .NET MAUI, with guidance and feedback from engineers in the MAUI ecosystem. What started as an experiment has grown into a project we are committing to, with apps already running on new platforms. It is time to show you what we have been building.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try It Right Now&lt;/head&gt;
    &lt;p&gt;Before we dive into the details, you can experience it yourself:&lt;/p&gt;
    &lt;p&gt;Launch MAUI in your browser â&lt;/p&gt;
    &lt;p&gt;This is a real MAUI application running on WebAssembly, rendered through Avalonia, with no plugins or hidden tricks. It is an early build with rough edges, but it proves the point: MAUI can now run on every major desktop OS and in the browser.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is the Avalonia MAUI Backend?&lt;/head&gt;
    &lt;p&gt;At its core, the Avalonia MAUI Backend enables you to keep your MAUI codebase while replacing the rendering layer with Avalonia. The goal is straightforward: take your existing MAUI applications and extend them to additional platforms, while enhancing desktop performance along the way.&lt;/p&gt;
    &lt;p&gt;In practical terms, that means several big wins.&lt;/p&gt;
    &lt;head rend="h3"&gt;Desktop Linux support&lt;/head&gt;
    &lt;p&gt;.NET MAUI apps running as first class desktop apps on distributions such as Ubuntu, Debian and Fedora, sharing the same Avalonia renderer that already powers demanding desktop apps in production today.&lt;/p&gt;
    &lt;head rend="h3"&gt;Embedded Linux&lt;/head&gt;
    &lt;p&gt;Avalonia already runs on embedded Linux devices, from Raspberry Pi panels to industrial HMIs. Using the same backend, the Avalonia MAUI Backend brings those capabilities to MAUI as well, so the applications you build in MAUI can run on the same embedded Linux targets as Avalonia.&lt;/p&gt;
    &lt;head rend="h3"&gt;WebAssembly support&lt;/head&gt;
    &lt;p&gt;The demo you can open in your browser today is a real MAUI application running on WebAssembly, rendered by Avalonia, with no native dependencies on the client. It is an early build, but it demonstrates what is now possible. MAUI apps will soon be free to deploy to the browser.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bonus: The Avalonia MAUI Backend runs on Windows and macOS too&lt;/head&gt;
    &lt;p&gt;On Windows and macOS, it plugs into the same mature desktop story Avalonia already has. On macOS, early testing indicates significantly improved performance compared to the Mac Catalyst approach. We are seeing more than 2x the performance in representative desktop scenarios, which is a very encouraging sign for the future of MAUI on desktop.&lt;/p&gt;
    &lt;p&gt;All of this is possible because we have built a version of MAUI that sits on top of Avaloniaâs drawn UI model rather than native controls. Not only do you get more platforms and improved performance, your MAUI applications can look and behave consistently whether they are on Windows, macOS, Linux, mobile or running in a browser tab.&lt;/p&gt;
    &lt;head rend="h3"&gt;Simpler, faster development&lt;/head&gt;
    &lt;p&gt;For the Avalonia team, this architecture has a major practical benefit: we only have to target one platform: Avalonia itself. That single target means we can move faster, ship features consistently, and avoid the need to endlessly fix platform-specific edge cases.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Instead of maintaining separate implementations for iOS, Android, Windows, macOS, Linux, and WebAssembly, we maintain just one. That massively reduces the chances of platform quirks, that can eat up hours of debugging time when something works on Android but not iOS, or renders differently on Mac Catalyst versus WinUI 3. When building on Avalonia, the controls render the same way everywhere because they are using the same rendering engine everywhere.&lt;lb/&gt;That means when we add a feature or fix a bug, it works across all platforms. No more "this works on mobile but breaks on desktop" or "this looks right on Windows but wrong on macOS." The entire development cycle becomes more predictable and significantly faster. &lt;lb/&gt;For us, that is a significant advantage. For MAUI developers, it means the backend evolves faster and more reliably.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Is Avalonia Building a Backend for MAUI?&lt;/head&gt;
    &lt;p&gt;It is a fair question. Avalonia already has its own thriving ecosystem. We see strong, sustained growth in our community, so why invest this much effort into making MAUI run on top of Avalonia?&lt;/p&gt;
    &lt;p&gt;The honest answer is that we care about .NET client developers first, and about which on ramp they use second. Many teams have already chosen MAUI, which they like and want more from. If we can provide them with Linux and browser support, along with improved desktop performance, without requiring a rewrite, that aligns with our mission to delight developers and solve complex problems.&lt;/p&gt;
    &lt;p&gt;This is not entirely selfless. Building a MAUI backend is also a way for us to learn. Running MAUI on Avalonia highlights what is missing for Avalonia to feel completely natural on mobile, which APIs are problematic, which tooling gaps matter, and where we need to raise our game to stay competitive. The work we are doing here directly contributes to strengthening Avalonia.&lt;/p&gt;
    &lt;p&gt;There is also a long term benefit in familiarity. By using Avalonia as the backend for their existing MAUI apps, developers gain insight into our renderer, capabilities and way of thinking. Some of those teams will quite reasonably stay with MAUI. Others, when they start a new project or need something lower level, may build directly on Avalonia instead. If this backend becomes a bridge that brings more people into the Avalonia ecosystem over time, that is a win.&lt;/p&gt;
    &lt;p&gt;So this project is not about âsavingâ MAUI from other frameworks. It is about giving existing MAUI developers more headroom and additional platforms, learning from their needs, and ensuring Avalonia is an obvious, competitive choice for whatever they build next.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why This Matters for MAUI Developers&lt;/head&gt;
    &lt;p&gt;If you have followed MAUI since its launch, you will know the two requests that never went away.&lt;/p&gt;
    &lt;p&gt;Developers want Linux support, both for desktop and for embedded devices. They also want a drawn control model that provides consistent behaviour across platforms, rather than relying on the native toolkit available on each system.&lt;/p&gt;
    &lt;p&gt;The Avalonia backend tackles both of those head on. Avalonia is a mature drawn UI framework.&lt;/p&gt;
    &lt;p&gt;It provides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Hardware accelerated rendering on every platform&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A consistent layout and styling system&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Smooth animations at high refresh rates&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Custom rendering and visual effects capabilities&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Broad platform coverage&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A fully supported platform that is receiving significant investment&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are not theoretical promises. They are the reasons Avalonia is used in production by companies such as Unity, JetBrains and Schneider Electric.&lt;/p&gt;
    &lt;p&gt;By building MAUI on top of Avalonia, you get a predictable, drawn UI foundation and an expanded set of platforms, without having to throw away your existing codebase. You do not need to abandon MAUI to get Linux and the web. You can bring MAUI with you, while also improving the experience on Windows and macOS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance and Next Generation Rendering&lt;/head&gt;
    &lt;p&gt;Performance is an important part of this story.&lt;/p&gt;
    &lt;p&gt;A drawn, GPU friendly UI stack gives you more headroom than wrapping native toolkits.&lt;/p&gt;
    &lt;p&gt;We are collaborating with the Flutter team at Google to bring Impeller, their GPU first renderer, to .NET. That work is already in progress and as it lands, the MAUI backend will inherit those gains.&lt;/p&gt;
    &lt;p&gt;The aim is simple: faster rendering, lower battery usage and smoother animations across desktop, mobile and embedded, using the same underlying technology that is pushing Flutter forward.&lt;/p&gt;
    &lt;p&gt;Read more about our Impeller collaboration with Google â&lt;/p&gt;
    &lt;head rend="h2"&gt;Looking Forward&lt;/head&gt;
    &lt;p&gt;We are particularly grateful to the MAUI engineers who have shared feedback and ideas as we have developed this backend. The .NET client ecosystem is at its best when different teams can cross pollinate and push each other forward.&lt;/p&gt;
    &lt;p&gt;This is just the beginning. As Linux and browser support matures, MAUI can finally live up to its promise as a truly multi platform app UI. We will keep sharing previews, benchmarks and updates as development continues, and once we are happy with the stability of the backend we will release the source code as fully open source under the MIT licence.&lt;/p&gt;
    &lt;p&gt;We are bringing .NET MAUI to Linux and to the browser, powered by Avalonia.&lt;/p&gt;
    &lt;p&gt;For the past few months, we have been working on an Avalonia powered backend for .NET MAUI, with guidance and feedback from engineers in the MAUI ecosystem. What started as an experiment has grown into a project we are committing to, with apps already running on new platforms. It is time to show you what we have been building.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try It Right Now&lt;/head&gt;
    &lt;p&gt;Before we dive into the details, you can experience it yourself:&lt;/p&gt;
    &lt;p&gt;Launch MAUI in your browser â&lt;/p&gt;
    &lt;p&gt;This is a real MAUI application running on WebAssembly, rendered through Avalonia, with no plugins or hidden tricks. It is an early build with rough edges, but it proves the point: MAUI can now run on every major desktop OS and in the browser.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is the Avalonia MAUI Backend?&lt;/head&gt;
    &lt;p&gt;At its core, the Avalonia MAUI Backend enables you to keep your MAUI codebase while replacing the rendering layer with Avalonia. The goal is straightforward: take your existing MAUI applications and extend them to additional platforms, while enhancing desktop performance along the way.&lt;/p&gt;
    &lt;p&gt;In practical terms, that means several big wins.&lt;/p&gt;
    &lt;head rend="h3"&gt;Desktop Linux support&lt;/head&gt;
    &lt;p&gt;.NET MAUI apps running as first class desktop apps on distributions such as Ubuntu, Debian and Fedora, sharing the same Avalonia renderer that already powers demanding desktop apps in production today.&lt;/p&gt;
    &lt;head rend="h3"&gt;Embedded Linux&lt;/head&gt;
    &lt;p&gt;Avalonia already runs on embedded Linux devices, from Raspberry Pi panels to industrial HMIs. Using the same backend, the Avalonia MAUI Backend brings those capabilities to MAUI as well, so the applications you build in MAUI can run on the same embedded Linux targets as Avalonia.&lt;/p&gt;
    &lt;head rend="h3"&gt;WebAssembly support&lt;/head&gt;
    &lt;p&gt;The demo you can open in your browser today is a real MAUI application running on WebAssembly, rendered by Avalonia, with no native dependencies on the client. It is an early build, but it demonstrates what is now possible. MAUI apps will soon be free to deploy to the browser.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bonus: The Avalonia MAUI Backend runs on Windows and macOS too&lt;/head&gt;
    &lt;p&gt;On Windows and macOS, it plugs into the same mature desktop story Avalonia already has. On macOS, early testing indicates significantly improved performance compared to the Mac Catalyst approach. We are seeing more than 2x the performance in representative desktop scenarios, which is a very encouraging sign for the future of MAUI on desktop.&lt;/p&gt;
    &lt;p&gt;All of this is possible because we have built a version of MAUI that sits on top of Avaloniaâs drawn UI model rather than native controls. Not only do you get more platforms and improved performance, your MAUI applications can look and behave consistently whether they are on Windows, macOS, Linux, mobile or running in a browser tab.&lt;/p&gt;
    &lt;head rend="h3"&gt;Simpler, faster development&lt;/head&gt;
    &lt;p&gt;For the Avalonia team, this architecture has a major practical benefit: we only have to target one platform: Avalonia itself. That single target means we can move faster, ship features consistently, and avoid the need to endlessly fix platform-specific edge cases.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Instead of maintaining separate implementations for iOS, Android, Windows, macOS, Linux, and WebAssembly, we maintain just one. That massively reduces the chances of platform quirks, that can eat up hours of debugging time when something works on Android but not iOS, or renders differently on Mac Catalyst versus WinUI 3. When building on Avalonia, the controls render the same way everywhere because they are using the same rendering engine everywhere.&lt;lb/&gt;That means when we add a feature or fix a bug, it works across all platforms. No more "this works on mobile but breaks on desktop" or "this looks right on Windows but wrong on macOS." The entire development cycle becomes more predictable and significantly faster. &lt;lb/&gt;For us, that is a significant advantage. For MAUI developers, it means the backend evolves faster and more reliably.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Is Avalonia Building a Backend for MAUI?&lt;/head&gt;
    &lt;p&gt;It is a fair question. Avalonia already has its own thriving ecosystem. We see strong, sustained growth in our community, so why invest this much effort into making MAUI run on top of Avalonia?&lt;/p&gt;
    &lt;p&gt;The honest answer is that we care about .NET client developers first, and about which on ramp they use second. Many teams have already chosen MAUI, which they like and want more from. If we can provide them with Linux and browser support, along with improved desktop performance, without requiring a rewrite, that aligns with our mission to delight developers and solve complex problems.&lt;/p&gt;
    &lt;p&gt;This is not entirely selfless. Building a MAUI backend is also a way for us to learn. Running MAUI on Avalonia highlights what is missing for Avalonia to feel completely natural on mobile, which APIs are problematic, which tooling gaps matter, and where we need to raise our game to stay competitive. The work we are doing here directly contributes to strengthening Avalonia.&lt;/p&gt;
    &lt;p&gt;There is also a long term benefit in familiarity. By using Avalonia as the backend for their existing MAUI apps, developers gain insight into our renderer, capabilities and way of thinking. Some of those teams will quite reasonably stay with MAUI. Others, when they start a new project or need something lower level, may build directly on Avalonia instead. If this backend becomes a bridge that brings more people into the Avalonia ecosystem over time, that is a win.&lt;/p&gt;
    &lt;p&gt;So this project is not about âsavingâ MAUI from other frameworks. It is about giving existing MAUI developers more headroom and additional platforms, learning from their needs, and ensuring Avalonia is an obvious, competitive choice for whatever they build next.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why This Matters for MAUI Developers&lt;/head&gt;
    &lt;p&gt;If you have followed MAUI since its launch, you will know the two requests that never went away.&lt;/p&gt;
    &lt;p&gt;Developers want Linux support, both for desktop and for embedded devices. They also want a drawn control model that provides consistent behaviour across platforms, rather than relying on the native toolkit available on each system.&lt;/p&gt;
    &lt;p&gt;The Avalonia backend tackles both of those head on. Avalonia is a mature drawn UI framework.&lt;/p&gt;
    &lt;p&gt;It provides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Hardware accelerated rendering on every platform&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A consistent layout and styling system&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Smooth animations at high refresh rates&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Custom rendering and visual effects capabilities&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Broad platform coverage&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A fully supported platform that is receiving significant investment&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are not theoretical promises. They are the reasons Avalonia is used in production by companies such as Unity, JetBrains and Schneider Electric.&lt;/p&gt;
    &lt;p&gt;By building MAUI on top of Avalonia, you get a predictable, drawn UI foundation and an expanded set of platforms, without having to throw away your existing codebase. You do not need to abandon MAUI to get Linux and the web. You can bring MAUI with you, while also improving the experience on Windows and macOS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance and Next Generation Rendering&lt;/head&gt;
    &lt;p&gt;Performance is an important part of this story.&lt;/p&gt;
    &lt;p&gt;A drawn, GPU friendly UI stack gives you more headroom than wrapping native toolkits.&lt;/p&gt;
    &lt;p&gt;We are collaborating with the Flutter team at Google to bring Impeller, their GPU first renderer, to .NET. That work is already in progress and as it lands, the MAUI backend will inherit those gains.&lt;/p&gt;
    &lt;p&gt;The aim is simple: faster rendering, lower battery usage and smoother animations across desktop, mobile and embedded, using the same underlying technology that is pushing Flutter forward.&lt;/p&gt;
    &lt;p&gt;Read more about our Impeller collaboration with Google â&lt;/p&gt;
    &lt;head rend="h2"&gt;Looking Forward&lt;/head&gt;
    &lt;p&gt;We are particularly grateful to the MAUI engineers who have shared feedback and ideas as we have developed this backend. The .NET client ecosystem is at its best when different teams can cross pollinate and push each other forward.&lt;/p&gt;
    &lt;p&gt;This is just the beginning. As Linux and browser support matures, MAUI can finally live up to its promise as a truly multi platform app UI. We will keep sharing previews, benchmarks and updates as development continues, and once we are happy with the stability of the backend we will release the source code as fully open source under the MIT licence.&lt;/p&gt;
    &lt;p&gt;We are bringing .NET MAUI to Linux and to the browser, powered by Avalonia.&lt;/p&gt;
    &lt;p&gt;For the past few months, we have been working on an Avalonia powered backend for .NET MAUI, with guidance and feedback from engineers in the MAUI ecosystem. What started as an experiment has grown into a project we are committing to, with apps already running on new platforms. It is time to show you what we have been building.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try It Right Now&lt;/head&gt;
    &lt;p&gt;Before we dive into the details, you can experience it yourself:&lt;/p&gt;
    &lt;p&gt;Launch MAUI in your browser â&lt;/p&gt;
    &lt;p&gt;This is a real MAUI application running on WebAssembly, rendered through Avalonia, with no plugins or hidden tricks. It is an early build with rough edges, but it proves the point: MAUI can now run on every major desktop OS and in the browser.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is the Avalonia MAUI Backend?&lt;/head&gt;
    &lt;p&gt;At its core, the Avalonia MAUI Backend enables you to keep your MAUI codebase while replacing the rendering layer with Avalonia. The goal is straightforward: take your existing MAUI applications and extend them to additional platforms, while enhancing desktop performance along the way.&lt;/p&gt;
    &lt;p&gt;In practical terms, that means several big wins.&lt;/p&gt;
    &lt;head rend="h3"&gt;Desktop Linux support&lt;/head&gt;
    &lt;p&gt;.NET MAUI apps running as first class desktop apps on distributions such as Ubuntu, Debian and Fedora, sharing the same Avalonia renderer that already powers demanding desktop apps in production today.&lt;/p&gt;
    &lt;head rend="h3"&gt;Embedded Linux&lt;/head&gt;
    &lt;p&gt;Avalonia already runs on embedded Linux devices, from Raspberry Pi panels to industrial HMIs. Using the same backend, the Avalonia MAUI Backend brings those capabilities to MAUI as well, so the applications you build in MAUI can run on the same embedded Linux targets as Avalonia.&lt;/p&gt;
    &lt;head rend="h3"&gt;WebAssembly support&lt;/head&gt;
    &lt;p&gt;The demo you can open in your browser today is a real MAUI application running on WebAssembly, rendered by Avalonia, with no native dependencies on the client. It is an early build, but it demonstrates what is now possible. MAUI apps will soon be free to deploy to the browser.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bonus: The Avalonia MAUI Backend runs on Windows and macOS too&lt;/head&gt;
    &lt;p&gt;On Windows and macOS, it plugs into the same mature desktop story Avalonia already has. On macOS, early testing indicates significantly improved performance compared to the Mac Catalyst approach. We are seeing more than 2x the performance in representative desktop scenarios, which is a very encouraging sign for the future of MAUI on desktop.&lt;/p&gt;
    &lt;p&gt;All of this is possible because we have built a version of MAUI that sits on top of Avaloniaâs drawn UI model rather than native controls. Not only do you get more platforms and improved performance, your MAUI applications can look and behave consistently whether they are on Windows, macOS, Linux, mobile or running in a browser tab.&lt;/p&gt;
    &lt;head rend="h3"&gt;Simpler, faster development&lt;/head&gt;
    &lt;p&gt;For the Avalonia team, this architecture has a major practical benefit: we only have to target one platform: Avalonia itself. That single target means we can move faster, ship features consistently, and avoid the need to endlessly fix platform-specific edge cases.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Instead of maintaining separate implementations for iOS, Android, Windows, macOS, Linux, and WebAssembly, we maintain just one. That massively reduces the chances of platform quirks, that can eat up hours of debugging time when something works on Android but not iOS, or renders differently on Mac Catalyst versus WinUI 3. When building on Avalonia, the controls render the same way everywhere because they are using the same rendering engine everywhere.&lt;lb/&gt;That means when we add a feature or fix a bug, it works across all platforms. No more "this works on mobile but breaks on desktop" or "this looks right on Windows but wrong on macOS." The entire development cycle becomes more predictable and significantly faster. &lt;lb/&gt;For us, that is a significant advantage. For MAUI developers, it means the backend evolves faster and more reliably.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Is Avalonia Building a Backend for MAUI?&lt;/head&gt;
    &lt;p&gt;It is a fair question. Avalonia already has its own thriving ecosystem. We see strong, sustained growth in our community, so why invest this much effort into making MAUI run on top of Avalonia?&lt;/p&gt;
    &lt;p&gt;The honest answer is that we care about .NET client developers first, and about which on ramp they use second. Many teams have already chosen MAUI, which they like and want more from. If we can provide them with Linux and browser support, along with improved desktop performance, without requiring a rewrite, that aligns with our mission to delight developers and solve complex problems.&lt;/p&gt;
    &lt;p&gt;This is not entirely selfless. Building a MAUI backend is also a way for us to learn. Running MAUI on Avalonia highlights what is missing for Avalonia to feel completely natural on mobile, which APIs are problematic, which tooling gaps matter, and where we need to raise our game to stay competitive. The work we are doing here directly contributes to strengthening Avalonia.&lt;/p&gt;
    &lt;p&gt;There is also a long term benefit in familiarity. By using Avalonia as the backend for their existing MAUI apps, developers gain insight into our renderer, capabilities and way of thinking. Some of those teams will quite reasonably stay with MAUI. Others, when they start a new project or need something lower level, may build directly on Avalonia instead. If this backend becomes a bridge that brings more people into the Avalonia ecosystem over time, that is a win.&lt;/p&gt;
    &lt;p&gt;So this project is not about âsavingâ MAUI from other frameworks. It is about giving existing MAUI developers more headroom and additional platforms, learning from their needs, and ensuring Avalonia is an obvious, competitive choice for whatever they build next.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why This Matters for MAUI Developers&lt;/head&gt;
    &lt;p&gt;If you have followed MAUI since its launch, you will know the two requests that never went away.&lt;/p&gt;
    &lt;p&gt;Developers want Linux support, both for desktop and for embedded devices. They also want a drawn control model that provides consistent behaviour across platforms, rather than relying on the native toolkit available on each system.&lt;/p&gt;
    &lt;p&gt;The Avalonia backend tackles both of those head on. Avalonia is a mature drawn UI framework.&lt;/p&gt;
    &lt;p&gt;It provides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Hardware accelerated rendering on every platform&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A consistent layout and styling system&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Smooth animations at high refresh rates&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Custom rendering and visual effects capabilities&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Broad platform coverage&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A fully supported platform that is receiving significant investment&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are not theoretical promises. They are the reasons Avalonia is used in production by companies such as Unity, JetBrains and Schneider Electric.&lt;/p&gt;
    &lt;p&gt;By building MAUI on top of Avalonia, you get a predictable, drawn UI foundation and an expanded set of platforms, without having to throw away your existing codebase. You do not need to abandon MAUI to get Linux and the web. You can bring MAUI with you, while also improving the experience on Windows and macOS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance and Next Generation Rendering&lt;/head&gt;
    &lt;p&gt;Performance is an important part of this story.&lt;/p&gt;
    &lt;p&gt;A drawn, GPU friendly UI stack gives you more headroom than wrapping native toolkits.&lt;/p&gt;
    &lt;p&gt;We are collaborating with the Flutter team at Google to bring Impeller, their GPU first renderer, to .NET. That work is already in progress and as it lands, the MAUI backend will inherit those gains.&lt;/p&gt;
    &lt;p&gt;The aim is simple: faster rendering, lower battery usage and smoother animations across desktop, mobile and embedded, using the same underlying technology that is pushing Flutter forward.&lt;/p&gt;
    &lt;p&gt;Read more about our Impeller collaboration with Google â&lt;/p&gt;
    &lt;head rend="h2"&gt;Looking Forward&lt;/head&gt;
    &lt;p&gt;We are particularly grateful to the MAUI engineers who have shared feedback and ideas as we have developed this backend. The .NET client ecosystem is at its best when different teams can cross pollinate and push each other forward.&lt;/p&gt;
    &lt;p&gt;This is just the beginning. As Linux and browser support matures, MAUI can finally live up to its promise as a truly multi platform app UI. We will keep sharing previews, benchmarks and updates as development continues, and once we are happy with the stability of the backend we will release the source code as fully open source under the MIT licence.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45893986</guid><pubDate>Tue, 11 Nov 2025 22:50:32 +0000</pubDate></item><item><title>Perkeep – Personal storage system for life</title><link>https://perkeep.org/</link><description>&lt;doc fingerprint="47dd1e4e29b72f56"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Perkeep lets you permanently keep your stuff, for life.&lt;/head&gt;
    &lt;p&gt;Perkeep (née Camlistore) is a set of open source formats, protocols, and software for modeling, storing, searching, sharing and synchronizing data in the post-PC era. Data may be files or objects, tweets or 5TB videos, and you can access it via a phone, browser or FUSE filesystem.&lt;/p&gt;
    &lt;p&gt;Perkeep is under active development. If you're a programmer or fairly technical, you can probably get it up and running and get some utility out of it. Many bits and pieces are actively being developed, so be prepared for bugs and unfinished features.&lt;/p&gt;
    &lt;p&gt;Join the community, consider contributing, or file a bug.&lt;/p&gt;
    &lt;p&gt;Things Perkeep believes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Your data is entirely under your control&lt;/item&gt;
      &lt;item&gt;Open Source&lt;/item&gt;
      &lt;item&gt;Paranoid about privacy, everything private by default&lt;/item&gt;
      &lt;item&gt;No SPOF: don't rely on any single party (including yourself)&lt;/item&gt;
      &lt;item&gt;Your data should be alive in 80 years, especially if you are&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Latest Release&lt;/head&gt;
    &lt;p&gt;The latest release is 0.12 ("Toronto"), released 2025-11-11.&lt;/p&gt;
    &lt;p&gt;Follow the download and getting started instructions to set up Perkeep.&lt;/p&gt;
    &lt;head rend="h2"&gt;Video Demo&lt;/head&gt;
    &lt;p&gt;LinuxFest Northwest 2018 [slides] [video]:&lt;/p&gt;
    &lt;p&gt;Or see the other presentations.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45896130</guid><pubDate>Wed, 12 Nov 2025 03:34:32 +0000</pubDate></item><item><title>Hard drives on backorder for two years as AI data centers trigger HDD shortage</title><link>https://www.tomshardware.com/pc-components/hdds/ai-triggers-hard-drive-shortage-amidst-dram-squeeze-enterprise-hard-drives-on-backorder-by-2-years-as-hyperscalers-switch-to-qlc-ssds</link><description>&lt;doc fingerprint="2f7cc5998cb29fb9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Hard drives on backorder for two years as AI data centers trigger HDD shortage — delays forcing rapid transition to QLC SSDs&lt;/head&gt;
    &lt;p&gt;The AI boom might help QLC overtake TLC in the next two years.&lt;/p&gt;
    &lt;p&gt;The race to achieve AGI (artificial general intelligence) has pushed constituents to invest in and build data centers at a pace far outstripping our ability to make them. Manufacturers are struggling to keep up with AI demand, and the ongoing DRAM shortage is proof of this, with memory kits costing more than double what they did just a few months ago. Now, DigiTimes is reporting that storage is taking a hit, too, with delivery times for enterprise-grade HDDs delayed by two years.&lt;/p&gt;
    &lt;p&gt;That means if a firm wants to buy large-capacity hard drives, the backbone of nearline storage, it has to wait 24 months due to long lead times. As the news cycle suggests, AI money doesn't wait for anyone, so hyperscalers are now switching to QLC NAND-based SSDs to avoid these backorders. Picking QLC over TLC allows them to maintain costs while achieving sufficient endurance for cold storage.&lt;/p&gt;
    &lt;p&gt;However, hoarding QLC NAND creates its own shortage, since every cloud provider in North America and China is now lining up to buy it. This could lead to SSD prices rising worldwide, as most value-oriented models use QLC to save costs. In fact, DigiTimes claims that production capacity for QLC is completely booked through 2026 at some NAND manufacturers.&lt;/p&gt;
    &lt;p&gt;Therefore, given the current situation, QLC NAND is expected to overtake TLC in popularity by early 2027, marking a significant shift in the storage landscape. While enterprise-grade QLC SSDs would entirely power this pivot, Sandisk has already raised NAND prices by 50%, according to another DigiTimes report, after initially warning of a 10% increase two months ago.&lt;/p&gt;
    &lt;p&gt;This unprecedented shortage across memory and storage was largely unforeseen. Still, given the AI ambitions of the world's wealthiest, the overnight whiplash is perhaps the only surprising aspect of these price hikes. Last month, the Adata chairman hinted that the situation would only worsen over time, and it's taken just a few weeks for us to receive confirmation.&lt;/p&gt;
    &lt;p&gt;Every DRAM and NAND manufacturer is now selling capacity to AI customers willing to pay the big bucks. Instead of having 2-3 months of buffer capacity, these firms are down to just a few weeks now. This has led to year-best numbers for many businesses — a sharp turnaround from a few years ago — but, as usual, the short end of the stick trickles down to regular consumers who're now entangled in yet another electronics scarcity.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, &amp;amp; reviews in your feeds.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;Hassam Nasir is a die-hard hardware enthusiast with years of experience as a tech editor and writer, focusing on detailed CPU comparisons and general hardware news. When he’s not working, you’ll find him bending tubes for his ever-evolving custom water-loop gaming rig or benchmarking the latest CPUs and GPUs just for fun.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;RTX 2080&lt;/header&gt;I buy BluRay and UHD discs of various films and TV shows I like and Rip them to my PC because 1) long term it is cheaper than paying to maintain 5 different streaming services, 2) the quality is higher, and 3) I prefer owning to subscribing. Consequently, I have a need to keep a large quantity of storage drives on hand.Reply&lt;lb/&gt;A few months ago an Article was written on this very site announcing Seagate’s new 30TB drives. Right up my alley, so I bought one; $549. Fast forward to 3 weeks ago, there were rumblings that the AI build out would soon create a storage shortage. Remembering the grand old days of Chia mining, I bought a second 30TB drive, the price having already increased to $599. Only 2 days later, the 30TB model was out of stock at Seagate and hasn’t come back in stock since. The 28TB model is currently in stock at $569, more than I paid for my first 30TB model. Who knows if even the 28TB model will be consistently in stock even at these elevated prices in the long term.&lt;lb/&gt;It’s rough out there people. It honestly reminds me a lot of trying to buy a RTX 3000 series GPU during COVID; they were either impossible to find or hideously overpriced. Oh well, I’m tempted to buy a RTX 5080 just because I can: a few are available at MSRP.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;yc1&lt;/header&gt;I swear that this is intentional at this point a excuse to raise prices while not improving their product and if the demand doesn't spike allowing a massive price hike they cut production to keep scarcity high and increase prices like what flash memory and ddr 4 manufacturers didReply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;tamalero&lt;/header&gt;Reply&lt;quote/&gt;its a cycle that repeats.. around 4 years ago they complained of RAM being worthless and companies losing money while producing. Aka oversupply of RAM and chips.yc1 said:I swear that this is intentional at this point a excuse to raise prices while not improving their product and if the demand doesn't spike allowing a massive price hike they cut production to keep scarcity high and increase prices like what flash memory and ddr 4 manufacturers did&lt;lb/&gt;Same with hard disks..&lt;lb/&gt;And before, during crypto.. there was also a shortage... in reality is all production manipulation just like the gas/oil production worldwide.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;satoasty&lt;/header&gt;Yep and to be fair the governments never wanted people to own their stuff by 2030. Probably works in their favour too.Reply&lt;lb/&gt;Apart from the stuff I can't prove, is Seagate as reliable as WD I've had a few older drives by Seagate fail on me but 1 was physical on the HDD?&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;thesyndrome&lt;/header&gt;So now not only are regular consumers getting screwed on RAM and SSD prices due to the NAND shortage, but now we can't even get mechanical drives for a reasonable price any more? And this is combined with GPU's prices always going up since COVID. It feels like the only PC part that hasn't drastically increased in price is CPUs.Reply&lt;lb/&gt;Computing is starting to become a ridiculous hobby from a financial perspective, and if anything will contribute to the downfall of AI faster than anything else; how can AI ever hope to make a profit if they've priced-out regular consumers from even being able to access it by making computers too expensive for the regular person, bearing in mind that inflation hasn't stopped and household essentials are still going up in price.?&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45896707</guid><pubDate>Wed, 12 Nov 2025 05:36:41 +0000</pubDate></item><item><title>Simulating a Planet on the GPU: Part 1 (2022)</title><link>https://www.patrickcelentano.com/blog/planet-sim-part-1</link><description>&lt;doc fingerprint="2545217d4ee8281b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Simulating a Planet on the GPU: Part 1&lt;/head&gt;
    &lt;p&gt;I miss Sim Earth.&lt;/p&gt;
    &lt;p&gt;To be fair, it didn’t go anywhere. You can still boot up a copy, mess around with evolution, atmospheric conditions, and continental drift like it’s 1990… but it’s not 1990. We have 32 years of computing advances at our backs and yet no SimEarth 2. Sure there’s WorldBox and Worlds and a bunch of awesome planet generation projects, (1, 2, 3) but none allow me to play with plate tectonics, currents, or any of the “deep” factors underlying how the Earth really works.&lt;/p&gt;
    &lt;p&gt;Sometime last year, I decided against writing Will Wright a letter, begging him to spend hundreds of hours making a SimEarth 2. Instead, I did what any good programmer would do and decided to devote thousands of my own hours into the same thing. This blog describes my first step toward such a goal: describing well over a year’s worth of work and the many detours taken along the way. While there’s a long way to go from here, I’m very happy to share my progress to this point, and promise to follow up with a “part 2” at some future date.&lt;/p&gt;
    &lt;head rend="h2"&gt;Polygon-Based Approaches&lt;/head&gt;
    &lt;p&gt;My first dozen or so approaches at realistic world generation involved generating polygons on a sphere, using Delaunay Triangulation and Voronoi Tessellation. These fancy-sounding processes allow us to cheaply turn a 2D surface (or indeed, spheres, thanks to this amazing blog) into a number of polygons for representing geographic features. Once I wrapped my head around wrapping Voronoi polygons around the North and South Poles, I managed to get something vaguely passing as a planet generated, and applied a nice water shader.&lt;/p&gt;
    &lt;p&gt;The fundamental issue with a polygon-centric approach is one of tectonic plate realism: plate collisions require an incredible number of polygons to model correctly. Increasing the number of polygons in a Unity &amp;amp; C# environment proved prohibitively expensive, so I turned to an old favorite: C++. After finding Brendan Galea’s excellent YouTube channel and building the first of the three demos above in SDL2 &amp;amp; Vulkan I realized what a titanic task writing a custom engine for this project would be, and went back to the (Unity) drawing board.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cubemap-Overlap-Based Approaches&lt;/head&gt;
    &lt;p&gt;Frustrated with the poor performance of my polygonal approach, I began to research Unity’s performance optimization tools. I chunked up and parallelized the Voronoi tessellation to no avail. I tried rearranging memory to no avail. Clearly, something about doing this amount of math on the CPU was beyond current processing capabilities… but what about the GPU?&lt;/p&gt;
    &lt;p&gt;Like most programmers, I’d heard of the legendary power of GPUs, but had only really harnessed it via graphics programming or CUDA (for my Music ex Machina project). While I had some experience writing conventional shaders, learning how to write compute shaders seemed like a massive undertaking… but what option did I have? I had no room left on the CPU.&lt;/p&gt;
    &lt;p&gt;Put simply, compute shaders are capable of applying a GPU’s heavily-parallelized workflow to arbitrary data, meaning I could simulate a world full of tectonic plates one “pixel” at a time. Once I figured out how to represent potentially world-spanning plates as cubemaps, I managed to create a neat compute shader-based simulation with plates colliding, subducting, and emerging from seafloor spreading… but never deforming.&lt;/p&gt;
    &lt;p&gt;While I liked the direction this adventure in compute shaders had taken me, I needed some new technique which could realistically deform crust at convergent plate boundaries.&lt;/p&gt;
    &lt;head rend="h2"&gt;Smoothed-Particle Hydrodynamics&lt;/head&gt;
    &lt;p&gt;Inspiration can come from anywhere, and as luck would have it, the inspiration for my next step forward came from watching this physical simulation of a plate boundary. The use of sand for crust material makes realistic deformation possible… and reminded me of the falling sand games which were so popular when I was a kid. Could I simulate little bits of “crust” which could bump into one another, forming mountains and valleys? Could I use the same system for air and water?&lt;/p&gt;
    &lt;p&gt;After a little digging, I discovered Smoothed-Particle Hydrodynamics, a technique commonly used in place of cellular fluid simulations. If I could just implement SPH on a sphere using my newfound knowledge of compute shaders, I’d have the fundamentals of a truly-unique planet sim. With a clear implementation strategy in mind, how long could implementing it possibly take?&lt;/p&gt;
    &lt;p&gt;As it turns out, quite a while.&lt;/p&gt;
    &lt;p&gt;Writing compute shaders is difficult, as is debugging and profiling them. I found myself performing true computer “science” on multiple occasions: forming a hypothesis about the performance impact of some new algorithm on the GPU and verifying it through good, old-fashioned experimentation. I learned a great deal in the process, most notably that computation is cheap and memory is expensive! More in depth, I’d highly recommend this incredible talk on compute shader performance optimization. I owe much of this project’s success to that talk.&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s Next?&lt;/head&gt;
    &lt;p&gt;This project has proved one of the most difficult I’ve ever worked on, and yet, one of the most satisfying. I hope to build a great deal on this foundation, slowly approaching a fully-featured planetary simulation. Here are a few thoughts as to what might be next:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Create map modes to display the direction of the currents (at present you can only see the waves moving in the right direction)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Show the air currents in some clever way, perhaps with a moving cloud layer on top of the planet&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Allow the wind to pick up moisture from warm bodies of water and precipitate it back on land… including rain shadows caused by mountain ranges&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Allow currents to warm up the surrounding air, causing phenomenon like the North Atlantic Current's miraculous warming of Europe.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Add hotspots, volcanoes, and a variety of rock types&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Optimize it to run on hardware beyond my Surface Book 2&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s to blog #2, with whatever new features that brings! Oh, and looking for a download link? Follow through to this page. Happy simulating : )&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45897122</guid><pubDate>Wed, 12 Nov 2025 06:58:06 +0000</pubDate></item><item><title>Yann LeCun to depart Meta and launch AI startup focused on 'world models'</title><link>https://www.nasdaq.com/articles/metas-chief-ai-scientist-yann-lecun-depart-and-launch-ai-start-focused-world-models</link><description>&lt;doc fingerprint="293f17ef93b2d5d8"&gt;
  &lt;main&gt;
    &lt;p&gt;(RTTNews) - Meta's (META) chief artificial intelligence scientist, Yann LeCun, plans to leave the company to launch his own AI start-up, marking a major shift inside Meta as CEO Mark Zuckerberg doubles down on "superintelligence" initiatives to compete with OpenAI and Google, according to people familiar with the matter.&lt;/p&gt;
    &lt;p&gt;LeCun, a Turing Award-winning pioneer of modern AI, has begun early fundraising discussions for his new venture, which will focus on developing "world models," next-generation systems designed to learn from visual and spatial data rather than text. These models aim to replicate human reasoning and understanding of the physical world, a project LeCun has said could take a decade to mature.&lt;/p&gt;
    &lt;p&gt;LeCun's exit comes amid an internal overhaul of Meta's AI strategy. Zuckerberg has shifted Meta's Fundamental AI Research Lab - FAIR, which LeCun founded in 2013, away from long-term research toward commercial AI products and large language models - LLMs. The move follows the underwhelming release of Meta's Llama 4 model, which lagged behind rival offerings from Anthropic, Google, and OpenAI.&lt;/p&gt;
    &lt;p&gt;To accelerate progress, Zuckerberg recently hired Alexandr Wang, founder of Scale AI, paying $14.3 billion for a 49 percent stake in his company and appointing him to lead Meta's new Superintelligence division, to which LeCun now reports. The CEO also formed an elite team called TBD Lab, offering $100 million pay packages to lure top AI talent from competitors.&lt;/p&gt;
    &lt;p&gt;LeCun has publicly disagreed with Zuckerberg's heavy reliance on LLMs, calling them "useful but fundamentally limited" in their ability to reason and plan like humans. His upcoming start-up will extend his FAIR research into "world models" that could ultimately enable machines to think more like people.&lt;/p&gt;
    &lt;p&gt;LeCun's planned departure adds to a series of AI leadership shake-ups at Meta. In recent months, Joelle Pineau, vice-president of AI research, left for Cohere, and the company laid off 600 employees from its AI division. Meanwhile, Shengjia Zhao, co-creator of ChatGPT, joined Meta as chief scientist of the Superintelligence Lab.&lt;/p&gt;
    &lt;p&gt;The upheaval follows investor pressure after Meta's shares plunged 12.6% in late October, wiping out nearly $240 billion in market value, when Zuckerberg indicated that AI spending could exceed $100 billion next year.&lt;/p&gt;
    &lt;p&gt;LeCun's move signals both a philosophical and structural rift within Meta's AI program, and the emergence of a potential new rival in the race toward true artificial general intelligence.&lt;/p&gt;
    &lt;p&gt;Tuesday META closed at $627.08, down 0.74%, and is trading after hours at $627.00, down 0.01% on the NasdaqGS.&lt;/p&gt;
    &lt;p&gt;The views and opinions expressed herein are the views and opinions of the author and do not necessarily reflect those of Nasdaq, Inc.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45897271</guid><pubDate>Wed, 12 Nov 2025 07:25:30 +0000</pubDate></item><item><title>What happened to Transmeta, the last big dotcom IPO</title><link>https://dfarq.homeip.net/what-happened-to-transmeta-the-last-big-dotcom-ipo/</link><description>&lt;doc fingerprint="7e934997b964874e"&gt;
  &lt;main&gt;
    &lt;p&gt;Transmeta was the last big IPO of the dotcom era, launching Nov 7, 2000. Some analysts call its $273 million IPO the last successful tech IPO until the Google IPO in 2004. Transmeta didn’t completely fit in to the dotcom era, because they were a hardware company. But they were still a technology company, and if their plans had gone well, they would have sold their product to dotcoms, but it didn’t work out that way for them. In this blog post we explore what happened to Transmeta.&lt;/p&gt;
    &lt;head rend="h2"&gt;Was Transmeta truly the last of the dotcoms?&lt;/head&gt;
    &lt;p&gt;I’ve heard Transmeta called the last successful IPO of the dotcom era, or the last of the big dotcom IPOs. But it’s an oversimplification to say there weren’t any successful technology IPOs between Transmeta and Google. One very notable exception is Paypal, who ran an IPO in February 2002 that raised $70.2 million. The Register even hailed Paypal as the return of the Internet IPO.&lt;/p&gt;
    &lt;p&gt;And it wasn’t just that dotcom IPOs grew scarce after November 2000. On February 7, 2002, Forbes stated that only 34 IPOs in total launched between September 11, 2001 and February 6, 2002, compared to 87 in the same-year-earlier period, and 240 between September 11, 1999 and February 6, 2000. That date is significant. Any bad news can spook investors, and the 9/11 attack did, in fact, spook investors.&lt;/p&gt;
    &lt;p&gt;So where the dotcom-era ended is a fuzzy line, but it makes sense to draw the line at Transmeta. Technology IPOs and particularly Internet IPOs became much more scarce after Transmeta, and the size of the IPOs shrunk too. The distinction of Transmeta being a technology stock, rather than an Internet stock, also suggests investors were already cooling on Internet stocks by November 2000.&lt;/p&gt;
    &lt;head rend="h2"&gt;What was Transmeta?&lt;/head&gt;
    &lt;p&gt;Transmeta was a CPU company. But they may be better known for their most famous employee than for any of their products. When Linus Torvalds completed his degree, there was a great deal of speculation where he would take his day job. Transmeta was not the place most technologists expected him to land. But it allowed him to continue his work on the Linux kernel while staying close to a key part of the hardware, the CPU.&lt;/p&gt;
    &lt;p&gt;In the year 2000, the CPU wars were cooling down. A few years earlier, there had been four companies not named Intel producing Socket 7 CPUs. But only two of them survived to compete in the next generation, and only AMD was able to compete at anything other than the entry level. And even though the Cyrix name survived, it was the competing IDT technology under the hood.&lt;/p&gt;
    &lt;head rend="h2"&gt;How its CPUs worked&lt;/head&gt;
    &lt;p&gt;Transmeta wanted to take a different approach. They were going to produce an x86 compatible CPU, but they were going to use a translation layer to do it. They would design a very efficient CPU, and in theory, they could place any translation layer in front of it that they wanted. Emulating PowerPC or ARM would have been possible if they saw the need to do it. But the popular yet inefficient x86 architecture was a more inviting target.&lt;/p&gt;
    &lt;p&gt;AMD was doing something similar, essentially translating x86 instructions into RISC instructions for efficiency, but they did it in hardware rather than software like Transmeta did. AMD never had any designs on putting any different translation layer in front of it.&lt;/p&gt;
    &lt;p&gt;Transmeta did ship two CPUs, but weren’t able to reach the same levels of performance AMD and Intel were reaching. Its first CPU, Crusoe, could run at Pentium III-like speeds but was about 30% less efficient, so a 700 MHz Crusoe ran like a 500 MHz Pentium III. Transmeta didn’t have fabrication plants of its own, so IBM handled manufacturing of its first-generation CPUs.&lt;/p&gt;
    &lt;p&gt;The Transmeta Efficieon, released in 2004, competed with the Pentium 4 but peaked at 1.7 GHz. With the Pentium 4 reaching 2.4 GHz speeds, the Efficieon had trouble competing. And AMD’s release of the Athlon 64 didn’t help matters. Selling 32-bit CPUs in a 64-bit world was going to be tough. TSMC and Fujitsu handled manufacturing for the Efficieon.&lt;/p&gt;
    &lt;p&gt;Transmeta CPUs saw use in low-power laptops, thin clients, and embedded applications. The Bluecoat web filtering appliance used them for a while. But if you owned a computer at that time, it’s much more likely to have had an AMD or Intel CPU in it. If you used one at work, it was even more likely to have an AMD or Intel CPU in it.&lt;/p&gt;
    &lt;head rend="h2"&gt;What happened to Transmeta&lt;/head&gt;
    &lt;p&gt;Even though Transmeta was the last big dotcom era IPO, they were not among the survivors. Torvalds resigned from Transmeta in June 2003.&lt;/p&gt;
    &lt;p&gt;What happened to Transmeta was that in 2005, Transmeta shifted to licensing intellectual property rather than selling CPUs. And in January 2009, Transmeta sold itself to Novafora, who in turn sold the patent portfolio to Intellectual Ventures, a private equity company. Novafora ceased operations in August 2009, just seven months later. Intellectual Ventures licenses the Transmeta intellectual property to other companies on a non-exclusive basis. Transmeta ended up being more like Netscape or VA Linux than Red Hat.&lt;/p&gt;
    &lt;p&gt;Today, Transmeta hardware is rare enough to be interesting as a collectible. But there aren’t a lot of people nostalgic for it, and that probably keeps prices low.&lt;/p&gt;
    &lt;p&gt;David Farquhar is a computer security professional, entrepreneur, and author. He has written professionally about computers since 1991, so he was writing about retro computers when they were still new. He has been working in IT professionally since 1994 and has specialized in vulnerability management since 2013. He holds Security+ and CISSP certifications. Today he blogs five times a week, mostly about retro computers and retro gaming covering the time period from 1975 to 2000.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45897935</guid><pubDate>Wed, 12 Nov 2025 09:01:41 +0000</pubDate></item><item><title>Yt-dlp: External JavaScript runtime now required for full YouTube support</title><link>https://github.com/yt-dlp/yt-dlp/issues/15012</link><description>&lt;doc fingerprint="bf4f981dcca3bac1"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 10.8k&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;p&gt;This is a follow-up to #14404, which announced that yt-dlp will soon require an external JavaScript runtime (e.g. Deno) in order to fully support downloading from YouTube.&lt;/p&gt;
    &lt;head rend="h3"&gt;With the release of yt-dlp version &lt;code&gt;2025.11.12&lt;/code&gt;, external JavaScript runtime support has arrived.&lt;/head&gt;
    &lt;head rend="h3"&gt;All users who intend to use yt-dlp with YouTube are strongly encouraged to install one of the supported JS runtimes.&lt;/head&gt;
    &lt;p&gt;The following JavaScript runtimes are currently supported (in order of recommendation, from strongest to weakest):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Deno&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;recommended for most users&lt;/item&gt;
          &lt;item&gt;https://deno.com/&lt;/item&gt;
          &lt;item&gt;https://github.com/denoland/deno &lt;list rend="ul"&gt;&lt;item&gt;note: if downloading from Deno's GitHub releases, get &lt;code&gt;deno&lt;/code&gt;not&lt;code&gt;denort&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
          &lt;item&gt;note: if downloading from Deno's GitHub releases, get &lt;/item&gt;
          &lt;item&gt;minimum Deno version supported by yt-dlp: &lt;code&gt;2.0.0&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;the latest version of Deno is strongly recommended&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Node&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;https://nodejs.org/&lt;/item&gt;
          &lt;item&gt;minimum Node version supported by yt-dlp: &lt;code&gt;20.0.0&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;if using Node, the latest version (25+) is strongly recommended for security reasons&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;QuickJS&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;https://bellard.org/quickjs/&lt;/item&gt;
          &lt;item&gt;minimum QuickJS version supported by yt-dlp: &lt;code&gt;2023-12-9&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;if using QuickJS, version &lt;code&gt;2025-4-26&lt;/code&gt;or later is strongly recommended for performance reasons&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
          &lt;item&gt;if using QuickJS, version &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;QuickJS-ng&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;https://quickjs-ng.github.io/quickjs/&lt;/item&gt;
          &lt;item&gt;all versions are supported by yt-dlp; however, upstream QuickJS is recommended instead for performance reasons&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Bun&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;https://bun.com/&lt;/item&gt;
          &lt;item&gt;minimum Bun version supported by yt-dlp: &lt;code&gt;1.0.31&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;if using Bun, the latest version is strongly recommended&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note that only &lt;code&gt;deno&lt;/code&gt; is enabled by default; all others are disabled by default for security reasons. See the EJS wiki page for more details.&lt;/p&gt;
    &lt;p&gt;In addition to the JavaScript runtime, yt-dlp also requires the yt-dlp-ejs component in order to operate the JS runtime.&lt;/p&gt;
    &lt;p&gt;NOTE: This component is already included in all of the official yt-dlp executables.&lt;lb/&gt; Similarly, if you've installed &amp;amp; upgraded the yt-dlp Python package with the &lt;code&gt;default&lt;/code&gt; extra (&lt;code&gt;yt-dlp[default]&lt;/code&gt;), then you already have the yt-dlp-ejs component.&lt;/p&gt;
    &lt;p&gt;If you've installed yt-dlp another way, then please refer to section 2 of the EJS wiki page for more details.&lt;/p&gt;
    &lt;p&gt;Support for YouTube without a JavaScript runtime is now considered "deprecated." It does still work somewhat; however, format availability will be limited, and severely so in some cases (e.g. for logged-in users). Format availability without a JS runtime is expected to worsen as time goes on, and this will not be considered a "bug" but rather an inevitability for which there is no solution. It's also expected that, eventually, support for YouTube will not be possible at all without a JS runtime.&lt;/p&gt;
    &lt;p&gt;If you have questions, please refer to the EJS wiki page, the previous announcement's FAQ, and the README before commenting or opening a new issue:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;https://github.com/yt-dlp/yt-dlp/wiki/EJS&lt;/item&gt;
      &lt;item&gt;[Announcement] Upcoming new requirements for YouTube downloads #14404&lt;/item&gt;
      &lt;item&gt;https://github.com/yt-dlp/yt-dlp#dependencies&lt;/item&gt;
      &lt;item&gt;https://github.com/yt-dlp/yt-dlp#general-options&lt;/item&gt;
      &lt;item&gt;https://github.com/yt-dlp/yt-dlp#youtube-ejs&lt;/item&gt;
      &lt;item&gt;https://github.com/yt-dlp/yt-dlp/wiki/EJS#plugins&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Notes to package maintainers&lt;/head&gt;
    &lt;p&gt;If you are maintaining a downstream package of yt-dlp, we offer the following guidance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;yt-dlp&lt;/code&gt;repository, source tarball, PyPI source distribution and built distribution (wheel) are still licensed under The Unlicense (public domain); however, when the&lt;code&gt;yt-dlp-ejs&lt;/code&gt;package is built, it bundles code licensed under ISC and MIT. This is the primary reason why&lt;code&gt;yt-dlp-ejs&lt;/code&gt;was split off into a separate repository and PyPI package&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;If&lt;/p&gt;&lt;code&gt;yt-dlp&lt;/code&gt;is packaged as a Python package in your repository,&lt;code&gt;yt-dlp-ejs&lt;/code&gt;would ideally be packaged separately&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;yt-dlp-ejs&lt;/code&gt;is technically an optional Python dependency of yt-dlp, but YouTube support is deprecated without it&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Each version of&lt;/p&gt;&lt;code&gt;yt-dlp&lt;/code&gt;will be pinned to a specific version of&lt;code&gt;yt-dlp-ejs&lt;/code&gt;and yt-dlp will reject any other&lt;code&gt;yt-dlp-ejs&lt;/code&gt;version. Refer to yt-dlp's&lt;code&gt;pyproject.toml&lt;/code&gt;for the pinned version&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;If your repository packages&lt;/p&gt;&lt;code&gt;yt-dlp&lt;/code&gt;as the zipimport binary instead of as a Python package, you can use&lt;code&gt;make yt-dlp-extra&lt;/code&gt;to build the zip executable with&lt;code&gt;yt-dlp-ejs&lt;/code&gt;included. (The Makefile will look for the&lt;code&gt;yt-dlp-ejs&lt;/code&gt;wheel in the&lt;code&gt;build&lt;/code&gt;subdirectory, or the extracted built distribution in the&lt;code&gt;yt_dlp_ejs&lt;/code&gt;subdirectory)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;deno&lt;/code&gt;,&lt;code&gt;nodejs&lt;/code&gt;,&lt;code&gt;quickjs&lt;/code&gt;and/or&lt;code&gt;bun&lt;/code&gt;should be optional dependencies of&lt;code&gt;yt-dlp&lt;/code&gt;. But again, YouTube support is deprecated without one of them&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;While&lt;/p&gt;&lt;code&gt;yt-dlp-ejs&lt;/code&gt;and the external JavaScript runtimes are currently only used with YouTube, yt-dlp's usage of these may be expanded in the future (and necessarily so)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If this guidance is insufficient, or if you are a developer integrating yt-dlp into your software and you have further questions, please open a new GitHub issue.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45898407</guid><pubDate>Wed, 12 Nov 2025 10:12:53 +0000</pubDate></item><item><title>Pakistani newspaper mistakenly prints AI prompt with the article</title><link>https://twitter.com/omar_quraishi/status/1988518627859951986</link><description>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45898789</guid><pubDate>Wed, 12 Nov 2025 11:17:06 +0000</pubDate></item><item><title>Micro.blog launches new 'Studio' tier with video hosting</title><link>https://heydingus.net/blog/2025/11/micro-blog-offers-an-indie-alternative-to-youtube-with-its-studio-video-hosting-plan</link><description>&lt;doc fingerprint="b4db0e54e8a9e6f9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Micro.blog offers an indie alternative to YouTube with its ‘Studio’ video hosting plan&lt;/head&gt;
    &lt;p&gt;The core of Micro.blog’s mission is to make it easy for people to own their presence on the web. At first, it was a simple blog host that also incorporated a Twitter-like social timeline that put short (title-less) and long (titled) posts on equal footing. In the years since its 2017 launch, Manton Reece — Micro.blog’s founder — has added a plethora of features that expand upon that mission. Here’s a list off the top of my head:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hosting podcasts&lt;/item&gt;
      &lt;item&gt;Bookmarking/archiving webpages&lt;/item&gt;
      &lt;item&gt;Fediverse compatibility with native replies to Mastodon and novel reply gathering from Bluesky&lt;/item&gt;
      &lt;item&gt;Crossposting to other social networks&lt;/item&gt;
      &lt;item&gt;Photo blogging&lt;/item&gt;
      &lt;item&gt;Custom domain name registration&lt;/item&gt;
      &lt;item&gt;Private notes&lt;/item&gt;
      &lt;item&gt;Book/Movie/TV Show blogging&lt;/item&gt;
      &lt;item&gt;Reading tracking&lt;/item&gt;
      &lt;item&gt;Automatic newsletters&lt;/item&gt;
      &lt;item&gt;Open APIs to manage your content&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All of this is hosted on your own website, (optionally, but strongly encouraged) at your own domain name. I’ve never seen anything else like it.&lt;/p&gt;
    &lt;p&gt;There are plans ranging from $1/month to $15/month that include subsets of these features, depending on how much a blogging “power user” you are.&lt;/p&gt;
    &lt;p&gt;Reece’s next1 big foray with Micro.blog: video hosting, which launched yesterday.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Micro.blog Studio adds longer video hosting for your blog, with uploads up to 20 minutes. You can read some of the technical bits here. It can automatically copy videos to PeerTube and Bluesky too.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That’s a quaint description for what promises to be a significant challenge.2 Because if hosting videos were easy, YouTube wouldn’t be the only3 game in town. And that’s exactly why Reece has pursued it. It’s not good for the open web for so much of its video content to live centralized at one host. John Gruber lamented this following Jimmy Kimmel’s suspension:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The big problem is YouTube. With YouTube, Google has a centralized chokehold on video. We need a way that’s as easy and scalable to host video content, independently, as it is for written content. I don’t know what the answer to that is, technically, but we ought to start working on it with urgency.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Just like Micro.blog encourages people to own their text, reading lists, podcasts, photos, and social network interactions at their own domain, that ethos now extends to videos too.&lt;/p&gt;
    &lt;p&gt;One of the great things about Micro.blog is how it enables the Publish to Own Site, Syndicate Elsewhere (POSSE) framework. That’s manifested in features like its automatic crossposting to Bluesky, Flickr, LinkedIn, Mastodon, Medium, Nostr, Pixelfed, Threads, and Tumblr. And manual crossposting elsewhere. This allows the “source of truth” to be at your own website that you control, but you won’t miss out on conversations and audiences in other places. With expanded video hosting, Reece has added PeerTube as another automatic crossposting destination, and hopes to also enable YouTube if and when Google approves his application. It’s not about only posting to your website, but instead centralizing your website as the first and primary place you post and then getting your text, images, audio, and now video out to other networks from there.&lt;/p&gt;
    &lt;p&gt;As you can probably tell, I’m pretty excited about Micro.blog taking on the challenge of being that ’indie-focused, YouTube alternative” that Reece envisioned. I haven’t upgraded my plan yet, but only because I mainly post shorter videos (covered by my current ‘Premium’ plan), but I’m very glad it now exists as an option.&lt;/p&gt;
    &lt;p&gt;There’s never been a better time to own your spot on the web. If you haven’t checked out Micro.blog before, I think it’s a compelling place to look.&lt;/p&gt;
    &lt;p&gt;Update 2025-11-11: I was in a hurry when I posted this earlier, and it slipped my mind to include some wants and wishes that I have for Micro.blog’s video hosting capabilities. It’s a short list, due to both Reece’s solid offering from the outset, and my lack of imagination. 😆&lt;/p&gt;
    &lt;p&gt;Scale time limits across the tiers. I really think video hosting would be a stronger offering if it were available more consistently across Micro.blog’s tiers. For example, 1-minute videos at $5/month, 5-minute videos at $10/month, 10-minute videos at $15/month, and 20-minute videos at $20/month. All with the same capabilities, but limited by length.&lt;/p&gt;
    &lt;p&gt;This was something that I know Reece considered, but ultimately decided against in the name of simplicity. He didn’t want to muck up the existing plans, and (rightly) considers them a tremendous value with their current features. He obviously hopes that people will upgrade to the higher-priced Studio plan specifically for the new video stuff.&lt;/p&gt;
    &lt;p&gt;But I think tying some video features (multiple resolutions and fast playback on your blog) to the 20-minute time limit and $20 plan creates more confusion, a feature gap, and missed opportunity. Take me for example. I think I could reasonably say that I’m a Micro.blog power user. But even I’m not sure if I’m correct in saying that those unique features are limited to the Studio plan. I know everyone gets video uploads up to 1 minute in length. (Maybe not everyone, though. Does Micro.one users at $1/month get the “new” video features? I’m not sure.&lt;/p&gt;
    &lt;p&gt;Historically, most of the videos I post are around 90 seconds in length. I’m far more likely to shave 30 seconds off my videos to fit a 1-minute time limit than I am to double my monthly cost to show those extra 30 seconds. There’s too big a gap between 1-minute videos and 20-minute videos to make it seem worthwhile. In my mind, I’d be “wasting” the extra $10/month ($120/year) by not posting 20-minute videos. But I’d be more likely to pay a little extra money for a little extra time. And then if I started hitting that new limit, I’d feel incentivized and validated graduating up to the next tier. I worry that Reece will see more infrastructure cost with a bunch of 1-minute videos being uploaded and served, but won’t see an accompanying bump in revenue, since we’re all getting the 1-minute videos for “free, and I don’t see a significant portion of Micro.blog users needing the 20-minutes.&lt;/p&gt;
    &lt;p&gt;Said one more way, I think giving people a little headroom to grow into hosting their videos on Micro.blog will make them more likely to upgrade over time. Once that habit has solidified, and users are comfortable with it, paying $5 more for the next jump in time limit isn’t a big ask. But jumping right into the Studio plan for $10-$15 extra is kind of off-putting. The gap between 1 minute and 20 is just too big.&lt;/p&gt;
    &lt;p&gt;Support 4K resolution. A pie-in-the-sky request, I know. 4K videos are huge. But I can nearly always see the difference, and choose higher quality playback every time. I’d love for my videos to appear at full-quality if they’re uploaded that way.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;To be clear, Micro.blog has had the ability to host videos — or nearly any other kind of file upload — and show them on your blog for years. But it’s been limited by file size, not an optimized part of the offering. The Studio tier makes it a first-rate feature, with smooth playback, automatic conversion to multiple resolutions, and ups the limit to a healthy 20 minutes no matter the file size. And the old file size-limited video uploads should still work for folks who rely on that workflow. 👌↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Sure, Vimeo exists, but it’s expensive and limited, and it’s future is uncertain. Plus, you’re still posting to a&lt;/p&gt;&lt;code&gt;vimeo.com&lt;/code&gt;domain. And, of course, many people post videos to Instagram, Facebook, TikTok, X, and other social networks. But I’d argue that videos there serve the algorithm first and users second. Micro.blog’s Studio tier flips that. It’s meant to serve the user first, and there is no algorithm at all.↩︎&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45900108</guid><pubDate>Wed, 12 Nov 2025 13:46:28 +0000</pubDate></item><item><title>The Geometry Behind Normal Maps</title><link>https://www.shlom.dev/articles/geometry-behind-normal-maps/</link><description>&lt;doc fingerprint="4cb9137b0a295b9d"&gt;
  &lt;main&gt;
    &lt;p&gt;I first ran into tangent space while learning about normal mapping. It was described as this in-between space that connects surfaces and UVs, something you need to make lighting work. Nobody really explained what it was. Tutorials showed math and shader code snippets, but none of them answered the real question: what is tangent space and why does it exist at all?&lt;/p&gt;
    &lt;p&gt;When I first learned about normal mapping I didn’t care as long as the normals looked right. But while working on mesh processing for VGLX that answer stopped being enough. I wanted to understand what those tangent vectors meant, not just how to compute them. What geometry were they pointing to? What was this “space” actually describing?&lt;/p&gt;
    &lt;p&gt;Eventually I realized the answer wasn’t mysterious at all. Tangent space isn’t a rendering trick. It’s a geometric structure that appears any time a surface has a parameterization. I just hadn’t connected the dots before.&lt;/p&gt;
    &lt;p&gt;When we define tangent space using UV coordinates, it becomes the bridge between the flat world of texture coordinates and the curved world of 3D surfaces. Once you see that connection, normal mapping suddenly makes perfect sense.&lt;/p&gt;
    &lt;p&gt;In this article I’ll take tangent space apart piece by piece: what it really is, how it emerges from UVs, how it’s computed, and how it forms the foundation for normal mapping and other techniques that depend on local surface orientation.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Anatomy of Tangent Space&lt;/head&gt;
    &lt;p&gt;Tangent space isn’t a global coordinate system. It’s a local frame built independently at every point on a surface. Each point has its own small world, a tiny patch of geometry defined by the surface itself. The tangent plane is that world’s foundation: a flat approximation that captures how the surface behaves locally and where the space gets its name.&lt;/p&gt;
    &lt;p&gt;Every point on a smooth surface has a tangent plane defined by its normal vector. We can see the same idea in two dimensions: to find the normal at a point on a curve, we first take its tangent line, then rotate it ninety degrees. The tangent plane is the 3D version of that relationship.&lt;/p&gt;
    &lt;p&gt;Tangent plane at a point on the surface with two tangent vectors lying within the plane.&lt;/p&gt;
    &lt;p&gt;Vectors that lie on this plane are called tangent vectors. We can pick two perpendicular directions within the plane that together with the normal form an orthonormal basis: a local coordinate frame on the surface that we call tangent space.&lt;/p&gt;
    &lt;p&gt;Tangent space lets us express directions, derivatives, and transformations relative to the surface itself rather than the world. That's important because most shading computations depend on directions defined locally: light hitting a surface, a normal perturbation from a texture, or the direction of anisotropy.&lt;/p&gt;
    &lt;p&gt;But before we can use it we need to decide how to orient that local frame. For that we typically turn to the surface's UVs.&lt;/p&gt;
    &lt;head rend="h3"&gt;How UVs Define Orientation&lt;/head&gt;
    &lt;p&gt;When people say "tangent space" in the context of computer graphics, they almost always mean a specific orientation of that tangent frame derived from UV parameterization.&lt;/p&gt;
    &lt;p&gt;A UV map gives every point on a surface a pair of 2D coordinates. These coordinates define how textures are applied, but they also do something more subtle: they tell us how movement in 2D texture space translates to movement along the surface.&lt;/p&gt;
    &lt;p&gt;Think of UVs as a coordinate grid draped over the mesh. Moving in the U direction means sliding along one axis of that grid and moving in V means sliding along the other. Those movements correspond to real 3D directions on the surface and those directions are exactly what defines the orientation of tangent space in practice.&lt;/p&gt;
    &lt;p&gt;The tangent directions follow the UV axes matching how texture coordinates move across the surface.&lt;/p&gt;
    &lt;p&gt;This connection between texture coordinates and surface geometry is what makes tangent space so useful: it anchors the surface’s local frame to something a shader can sample. The math for building that frame comes directly from this relationship.&lt;/p&gt;
    &lt;head rend="h3"&gt;Constructing Tangent Space&lt;/head&gt;
    &lt;p&gt;The tangent vectors that define the orientation of the tangent frame are called tangent and bitangent and they describe how texture coordinates move across the surface. Together with the normal vector these directions form the &lt;/p&gt;
    &lt;p&gt;Assuming the normal is known we need to find the tangent vectors &lt;/p&gt;
    &lt;p&gt;Triangle shown in surface space and its corresponding triangle in UV space (texture map).&lt;/p&gt;
    &lt;p&gt;Finding a transformation that maps directions in texture space to their corresponding directions on the surface is what it means to find the tangent vectors. This transformation can be represented as a &lt;/p&gt;
    &lt;p&gt;We can start by defining this relationship for a single edge. Take a triangle defined by three points &lt;/p&gt;
    &lt;p&gt;These two edges describe the same portion of the triangle. We can express how the edge in texture space maps to its surface space counterpart with the following equation:&lt;/p&gt;
    &lt;p&gt;In this equation, &lt;/p&gt;
    &lt;p&gt;We can compute &lt;/p&gt;
    &lt;p&gt;This form captures both edges, giving us enough information to solve for the tangent vectors &lt;/p&gt;
    &lt;p&gt;This gives us the tangent vectors we’re looking for to construct the &lt;/p&gt;
    &lt;p&gt;Tangent vectors aren’t guaranteed to be perpendicular. UV maps are rarely uniform. Unwrapping a curved surface onto a flat plane introduces stretching and compression that can cause the tangent vectors to drift slightly away from the normal.&lt;/p&gt;
    &lt;p&gt;We need an orthonormal basis for stable lighting: all three axes must be perpendicular and of unit length. Assuming the UVs are mostly continuous and locally smooth, these deviations are small and can be corrected by orthogonalizing the tangent frame using the full Gram–Schmidt process. This ensures that the tangent and normal vectors remain perpendicular and normalized:&lt;/p&gt;
    &lt;p&gt;Since the normal is of unit length we can project the tangent vector onto it using the dot product &lt;/p&gt;
    &lt;p&gt;Finally, we normalize both tangent vectors which together with the normal form an orthonormal basis that defines the tangent frame. Packed together they make up the &lt;/p&gt;
    &lt;p&gt;Constructing and storing the full &lt;code&gt;vec4&lt;/code&gt; vertex attribute. The &lt;code&gt;xyz&lt;/code&gt; components hold the tangent direction and &lt;code&gt;w&lt;/code&gt; holds the sign. We then reconstruct the bitangent at render time:&lt;/p&gt;
    &lt;p&gt;The sign is required because flipping the UVs horizontally or vertically inverts one of the tangent-space axes. When that happens the handedness of the tangent frame reverses and the determinant of the &lt;/p&gt;
    &lt;p&gt;With the tangent frame defined per vertex, we can now use it to translate normal directions stored in a texture into directions on the surface.&lt;/p&gt;
    &lt;head rend="h3"&gt;From Tangent Space to Normal Mapping&lt;/head&gt;
    &lt;p&gt;Normal mapping shares more than a storage and retrieval mechanism with texture mapping. It solves the same problem. Real-time meshes are low-resolution because every vertex adds cost and fewer vertices mean less geometric detail.&lt;/p&gt;
    &lt;p&gt;If we could afford a polygon for every pixel we wouldn’t need textures at all. But we can’t so we cheat. Textures give fragments access to data we can’t store per vertex. In normal mapping that data is surface orientation.&lt;/p&gt;
    &lt;p&gt;A normal map stores those orientations as colors. Each texel encodes a normal vector using its RGB channels mapped to XYZ. The blue channel dominates because most normals point roughly outward from the surface. A normal map is tinted blue for that reason.&lt;/p&gt;
    &lt;p&gt;Normal map on the right tinted blue because most normals point outward.&lt;/p&gt;
    &lt;p&gt;These per-pixel normals replace the interpolated vertex normals letting lighting respond to fine details that aren’t present in the mesh.&lt;/p&gt;
    &lt;p&gt;Each texel in a normal map represents a direction in local space. We sometimes say that each texel stores a direction in tangent space in the same way that surface positions in local space are said to be in model space. The name defines the frame these values are expressed in and in the previous section we derived the function that transforms them into this frame: the &lt;/p&gt;
    &lt;p&gt;Assuming the tangent vector and handedness are stored as vertex attributes, we can reconstruct the &lt;/p&gt;
    &lt;p&gt;The tangent vector is transformed by the model-view matrix. Some sources say to use the normal matrix but that’s incorrect. Tangents lie on the surface while normals are perpendicular to it so each must be transformed differently.&lt;/p&gt;
    &lt;p&gt;The transformed tangent works well in most cases but under non-uniform scaling it can introduce slight angular drift. In practice this is often negligible but if precision matters re-orthogonalize the tangent against the normal before computing the bitangent.&lt;/p&gt;
    &lt;p&gt;Once we reconstruct the &lt;/p&gt;
    &lt;p&gt;A normal map stores directions as RGB colors in the range &lt;/p&gt;
    &lt;p&gt;This converts the stored color values into normalized directions. Without this step all normals would point into a single quadrant of tangent space producing incorrect shading.&lt;/p&gt;
    &lt;p&gt;Normal maps are only half the story. They depend on the tangent frame we built earlier. If the tangent basis isn’t generated or interpolated consistently the surface won’t match the texture that defines it. Seams appear, highlights break, and the illusion falls apart. Combining the normal map with accurate tangent frames brings low-polygon models to life revealing fine detail and form that aren’t really there.&lt;/p&gt;
    &lt;p&gt;From Paolo Cignoni: the original high-resolution model (left) is simplified to a low-poly mesh (center). By transferring detail into a normal map, the low-poly version (right) recovers nearly all the visual complexity.&lt;/p&gt;
    &lt;p&gt;The process described here follows the same principles as MikkTSpace, the standard used by most tools and engines to keep bakes and renders in sync. It defines how to build, average, and orthogonalize tangents, how to store the sign, and how to reconstruct the bitangent in the shader so the lighting behaves the same everywhere.&lt;/p&gt;
    &lt;p&gt;By now the picture is complete. Tangent space gives each point on the surface its own coordinate system. UVs define how that system is oriented. The &lt;/p&gt;
    &lt;p&gt;Normal mapping doesn't fake bumps. It describes how the surface would curve if it had more polygons. Shading reacts the same way because light only cares about direction, not depth. At shallow angles the illusion breaks. You can see that surface detail is missing. But viewed head-on the lighting reacts as if every point on the mesh had its own direction capturing every bump and groove.&lt;/p&gt;
    &lt;p&gt;The same idea drives everything that uses a parameterized surface. Anisotropy, triplanar projection, detail normals, even displacement mapping, all build on the same translation between textures and surface space. Once that connection clicks what seemed like a trick becomes geometry. Tangent space isn’t a feature of shading. It’s part of how surfaces exist.&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Non-square matrices represent linear transformations between spaces of different dimensions. The number of columns corresponds to the input dimension, and the number of rows corresponds to the output dimension. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The fraction out front is the reciprocal of the UV matrix’s determinant. As long as this value isn’t zero, the matrix can be inverted. The inversion itself follows the usual 2×2 rule: swap the diagonal entries, negate the off-diagonals, and scale by that reciprocal determinant. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you’re writing your own mesh preprocessing code it’s also important to average tangent vectors across shared vertices in the same way we smooth normals on indexed meshes. This ensures the tangent field remains continuous across the surface and prevents visible lighting seams. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;An alternate approach is to transform the light direction into tangent space using the inverse&lt;/p&gt;&lt;mjx-container/&gt;matrix instead of transforming the normal into surface space. Both methods are equivalent in theory, but transforming the normal is usually cheaper and integrates more naturally into standard lighting pipelines. ↩&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45900159</guid><pubDate>Wed, 12 Nov 2025 13:50:47 +0000</pubDate></item><item><title>Fighting the New York Times' invasion of user privacy</title><link>https://openai.com/index/fighting-nyt-user-privacy-invasion</link><description>&lt;doc fingerprint="941e81541c87d3a5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Fighting the New York Times’ invasion of user privacy&lt;/head&gt;
    &lt;p&gt;Trust, security, and privacy guide every product and decision we make.&lt;/p&gt;
    &lt;p&gt;Each week, 800 million people use ChatGPT to think, learn, create, and handle some of the most personal parts of their lives. People entrust us with sensitive conversations, files, credentials, memories, searches, payment information, and AI agents that act on their behalf. We treat this data as among the most sensitive information in your digital life—and we’re building our privacy and security protections to match that responsibility.&lt;/p&gt;
    &lt;p&gt;Today, that responsibility is being tested.&lt;/p&gt;
    &lt;p&gt;The New York Times is demanding that we turn over 20 million of your private ChatGPT conversations. They claim they might find examples of you using ChatGPT to try to get around their paywall.&lt;/p&gt;
    &lt;p&gt;This demand disregards long-standing privacy protections, breaks with common-sense security practices, and would force us to turn over tens of millions of highly personal conversations from people who have no connection to the Times’ baseless lawsuit against OpenAI.&lt;/p&gt;
    &lt;p&gt;They have tried this before. Originally, the Times wanted you to lose the ability to delete your private chats. We fought that and restored your right to remove them. Then they demanded we turn over 1.4 billion of your private ChatGPT conversations. We pushed back, and we’re pushing back again now. Your private conversations are yours—and they should not become collateral in a dispute over online content access.&lt;/p&gt;
    &lt;p&gt;We respect strong, independent journalism and partner with many publishers and newsrooms. Journalism has historically played a critical role in defending people’s right to privacy throughout the world. However, this demand from the New York Times does not live up to that legacy, and we’re asking the court to reject it. We will continue to explore every option available to protect our users’ privacy.&lt;/p&gt;
    &lt;p&gt;We are accelerating our security and privacy roadmap to protect your data. OpenAI is one of the most targeted organizations in the world. We have invested significant time and resources building systems to prevent unauthorized access to your data by adversaries ranging from organized criminal groups to state-sponsored intelligence services.&lt;/p&gt;
    &lt;p&gt;However, if the Times succeeds in its demand, we will be forced to hand over the very same data we’re protecting—your data—to third parties, including the Times’ lawyers and paid consultants.&lt;/p&gt;
    &lt;p&gt;Our long-term roadmap includes advanced security features designed to keep your data private, including client-side encryption for your messages with ChatGPT. We believe these features will help keep your private conversations private and inaccessible to anyone else, even OpenAI. We will build fully automated systems to detect safety issues in our products. Only serious misuse and critical risks—such as threats to someone’s life, plans to harm others, or cybersecurity threats—may ever be escalated to a small, highly vetted team of human reviewers. These security features are in active development and we will share more details about them, and other short-term mitigations, in the very near future.&lt;/p&gt;
    &lt;p&gt;The privacy and security protections must become more powerful as AI becomes more deeply integrated into people’s lives. We are committed to a future where you can trust that your most personal AI conversations are safe, secure, and truly private.&lt;/p&gt;
    &lt;p&gt;—Dane Stuckey, Chief Information Security Officer, OpenAI&lt;/p&gt;
    &lt;p&gt;Why are The New York Times and other plaintiffs demanding this?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The New York Times is suing OpenAI. As part of their baseless lawsuit, they’ve demanded the court to force us to hand over 20 million user conversations. This would allow them to access millions of user conversations that are unrelated to the case.&lt;/item&gt;
      &lt;item&gt;We strongly believe this is an overreach. It risks your privacy without actually helping resolve the lawsuit. That’s why we’re fighting it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What led to this stage of the process?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Times’ lawyers argued to the court that their request should be granted, in part because another AI company previously agreed to hand over 5 million private chats of their users in an unrelated court case.&lt;/item&gt;
      &lt;item&gt;We strongly disagree that this is relevant to our case and we’re continuing to appeal.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Did you offer any other solutions to the Times?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We presented several privacy-preserving options to The Times, including targeted searches over the sample (e.g., to search for chats that might include text from a New York Times article so they only receive the conversations relevant to their claims), as well as high-level data classifying how ChatGPT was used in the sample.&lt;/item&gt;
      &lt;item&gt;These were rejected by The Times.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Is the NYT obligated to keep this data private?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Yes. The Times would be legally obligated at this time to not make any data public outside the court process. That said, if the Times continues to push to access it in any way that will make the conversations public, we will fight to protect your privacy at every step.&lt;/item&gt;
      &lt;item&gt;The Times’ original request in this lawsuit was also much broader. It initially demanded 1.4 billion private ChatGPT conversations, which we successfully pushed back on through the legal process. That presented red flags to us that suggested this was not a thoughtful or genuinely necessary request.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How are these 20 million chats selected?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The 20 million user conversations were randomly sampled from Dec. 2022 to Nov. 2024.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Is my data potentially impacted?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This data includes a random sampling of consumer ChatGPT conversations from Dec. 2022 to Nov. 2024.&lt;/item&gt;
      &lt;item&gt;Conversations outside of this time window are not potentially impacted.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Are business customers potentially impacted?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This does not impact ChatGPT Enterprise, ChatGPT Edu, ChatGPT Business (formerly “Team”) customers, or API customers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What are you doing to protect my personal information and privacy?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We are taking all affected chats and running them through a de-identifying procedure to remove or “scrub” personal identifying information (or “PII”) and other information (e.g., passwords or other sensitive information) from these conversations.&lt;/item&gt;
      &lt;item&gt;We would also push to only allow the Times to view this data in a secure environment maintained under strict legal protocols.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How will you store this data?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The content covered by the court order is currently stored separately in a secure system. It’s protected under legal hold, meaning it can’t be accessed or used for purposes other than meeting legal obligations.&lt;/item&gt;
      &lt;item&gt;Only a small, audited OpenAI legal and security team would be able to access this data as necessary to comply with our legal obligations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Who will be able to access this data?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Times’ outside counsel attorneys of record in the case and their hired technical consultants would be able to access the conversations. We will push to only allow The Times to view this data in a secured environment maintained under strict legal protocols.&lt;/item&gt;
      &lt;item&gt;If The Times continues to push to access it in any way that will make the conversations public, we will fight to protect your privacy at every step.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Does this court order violate GDPR or my rights under European or other privacy laws?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We are taking steps to comply at this time because we must follow the law, but The New York Times’ demand does not align with our privacy standards. That is why we’re challenging it.&lt;/item&gt;
      &lt;item&gt;As mentioned, we’ve taken additional steps to protect your privacy, such as de-identifying data and removing personally identifiable information.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Will you keep us updated?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Yes. We’re committed to transparency and will keep you informed. We’ll share meaningful updates, including any changes to the order or how it affects your data.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45900370</guid><pubDate>Wed, 12 Nov 2025 14:08:28 +0000</pubDate></item><item><title>Learn Prolog Now</title><link>https://lpn.swi-prolog.org/lpnpage.php?pageid=top</link><description>&lt;doc fingerprint="4d56f3b1a1661f95"&gt;
  &lt;main&gt;
    &lt;p&gt;Learn Prolog Now! is an introductory course to programming in Prolog. The online version has been available since 2001, and now there is also a throughly revised version available in book form.&lt;/p&gt;
    &lt;p&gt;We wanted to do two things with this course. First, we wanted to provide a text that was relatively self contained, a text that would permit someone with little or no knowledge of computing to pick up the basics of Prolog with the minimum of fuss. We also wanted the text to be clear enough to make it useful for self study. We believe that if you read the text, and do the associated exercises, you will gain a useful partial entry to the world of Prolog.&lt;/p&gt;
    &lt;p&gt;But only a partial entry, and this brings us to our second point. We want to emphasize the practical aspects of Prolog. Prolog is something you do. You can't learn a programming language simply by reading about it, and if you really want to get the most out of this course, we strongly advise you to get hold of a Prolog interpreter (you'll find pointers to some nice ones on this website) and work through all the Practical Sessions that we provide. And of course, don't stop with what we provide. The more you program, the better you'll get....&lt;/p&gt;
    &lt;p&gt;We hope you enjoy the course. And whether you're using this book to teach yourself Prolog, or you're using it as the basis for teaching others, we would like to hear from you. Please send us any comments/corrections you have so that we can take them into account in later versions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45900978</guid><pubDate>Wed, 12 Nov 2025 14:54:27 +0000</pubDate></item><item><title>Fungus in Chernobyl nuclear disaster zone has mutated to 'feed' on radiation (2024)</title><link>https://www.unilad.com/news/world-news/fungus-chernobyl-mutated-feed-radiation-164735-20241217</link><description>&lt;doc fingerprint="375f94ff76befc7d"&gt;
  &lt;main&gt;
    &lt;p&gt;News Film and TV Music Tech Features Celebrity Politics Weird Community&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45901149</guid><pubDate>Wed, 12 Nov 2025 15:08:35 +0000</pubDate></item><item><title>Waymo robotaxis are now giving rides on freeways in LA, SF and Phoenix</title><link>https://techcrunch.com/2025/11/12/waymo-robotaxis-are-now-giving-rides-on-freeways-in-these-3-cities/</link><description>&lt;doc fingerprint="ee28c00b2634fced"&gt;
  &lt;main&gt;
    &lt;p&gt;Sixteen years ago, engineers working on the Google self-driving project conducted their first autonomous vehicle tests on the freeway that connects Silicon Valley to San Francisco.&lt;/p&gt;
    &lt;p&gt;The company would eventually become Waymo, autonomous vehicle testing would expand — fanning out to other cities. Eventually, the company launched commercial robotaxi services in Phoenix, San Francisco, and Los Angeles. Other cities soon followed.&lt;/p&gt;
    &lt;p&gt;But freeways, despite some of that early testing, would remain out of reach. Until today.&lt;/p&gt;
    &lt;p&gt;Waymo said Wednesday it will begin offering robotaxi rides that use freeways across San Francisco, Phoenix, and Los Angeles, a critical expansion for the company that it says will reduce ride times by up to 50%. That stat could help attract a whole new group of users who need to travel between the many towns and suburbs within the greater San Francisco Bay Area or quicken commutes across the sprawling Los Angeles and Phoenix metro areas.&lt;/p&gt;
    &lt;p&gt;Using freeways is also essential for Waymo to offer rides to and from the San Francisco Airport, a location the company is currently testing in.&lt;/p&gt;
    &lt;p&gt;The service won’t be offered to all Waymo riders at first, the company said. Waymo riders who want to experience freeway rides can note their preference in the Waymo app. Once the rider hails a ride, they may be matched with a freeway trip, according to the company.&lt;/p&gt;
    &lt;p&gt;The company’s robotaxi routes will now stretch to San Jose, an expansion that will create a unified 260-mile service area across the Peninsula, according to Waymo. The company said it will also begin curbside drop off and pick up service at the San Jose Mineta International Airport. It already offers curbside service to the Sky Harbor Phoenix International Airport.&lt;/p&gt;
    &lt;head rend="h3"&gt;Join the Disrupt 2026 Waitlist&lt;/head&gt;
    &lt;head rend="h4"&gt;Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.&lt;/head&gt;
    &lt;head rend="h3"&gt;Join the Disrupt 2026 Waitlist&lt;/head&gt;
    &lt;head rend="h4"&gt;Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.&lt;/head&gt;
    &lt;p&gt;“Freeway driving is one of those things that’s very easy to learn, but very hard to master when we’re talking about full autonomy without a human driver as a backup, and at scale,” Waymo co-CEO Dmitri Dolgov said in a media briefing with reporters. “It took time to do it properly, with a strong focus on system safety and reliability.”&lt;/p&gt;
    &lt;p&gt;Waymo robotaxis have been spotted on freeways for months. TechCrunch took a test ride last year in the Phoenix area that included freeways. The company has provided trips to employees for more than a year. It also expanded testing to include closed course and simulation&lt;/p&gt;
    &lt;p&gt;While many assume freeway driving is easier, it comes with its own set of challenges, principal software engineer Pierre Kreitmann said in a recent briefing. He noted that critical events happen less often on freeways, which means there are fewer opportunities to expose Waymo’s self-driving system to rare scenarios and prove how the system performs when it really matters. The company chose to augment its public road driving with a combination of closed course and simulation testing.&lt;/p&gt;
    &lt;p&gt;This expanded testing and validation of the software was done to ensure the vehicles transition smoothly and safely between freeways and surface streets, and recognize and adapt to the unique context of the road around them, Kreitman said.&lt;/p&gt;
    &lt;p&gt;Waymo has also expanded its operational protocols, including how it coordinates with safety officials like California Highway Patrol, now that its robotaxis are on freeways.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45901855</guid><pubDate>Wed, 12 Nov 2025 16:06:29 +0000</pubDate></item><item><title>The last-ever penny will be minted today in Philadelphia</title><link>https://www.cnn.com/2025/11/12/business/last-penny-minted</link><description>&lt;doc fingerprint="4ea098646896a7e7"&gt;
  &lt;main&gt;
    &lt;p&gt;The American penny will pass away later today after a prolonged illness. It was 238 years old.&lt;/p&gt;
    &lt;p&gt;The last penny will be minted Wednesday afternoon at the US Mint in Philadelphia, overseen by Treasury Secretary Scott Bessent and Treasurer Brandon Beach. President Donald Trump announced via social media in February that he instructed the Mint to stop making the once-popular coin, citing the cost of production.&lt;/p&gt;
    &lt;p&gt;The penny costs nearly four cents to mint, more than the coin’s worth. Once valuable enough to buy “penny candy” like gumballs and feed parking meters or toll booths, today the penny lives mostly in coin jars, junk drawers or “leave a penny/take a penny” trays.&lt;/p&gt;
    &lt;p&gt;The penny outlived its sibling, the half-penny, by 168 years. It’s survived by the nickel, dime, quarter, and rarely seen half-dollar and dollar coins.&lt;/p&gt;
    &lt;p&gt;Despite its demise, the penny will remain legal tender.&lt;/p&gt;
    &lt;head rend="h2"&gt;Problems despite long planned end&lt;/head&gt;
    &lt;p&gt;For a coin that seems obsolete, its removal from circulation is causing more problems than expected, especially for retailers.&lt;/p&gt;
    &lt;p&gt;Some merchants plan to round prices to the nearest nickel, often a penny or two more. Others are asking customers to pay with pennies to help supply. But in some states, merchants could face legal trouble for rounding up or down.&lt;/p&gt;
    &lt;p&gt;Additionally, any savings from discontinuing the one-cent coin could be offset by the need to press more nickels, which costs the US Mint more money than the penny.&lt;/p&gt;
    &lt;p&gt;The government’s phasing out of the penny has been “a bit chaotic,” said Mark Weller, executive director of Americans for Common Cents. The pro-penny group is funded primarily by Artazn, the company that provides the blanks used to make pennies. “By the time we reach Christmas, the problems will be more pronounced with retailers not having pennies.”&lt;/p&gt;
    &lt;p&gt;Weller said other countries that removed low denomination coins, like Canada, Australia and Switzerland, had guidance for afterwards. Not so in the United States.&lt;/p&gt;
    &lt;p&gt;“We had a social media post (by Trump) during Super Bowl Sunday, but no real plan for what retailers would have to do,” he said, referring to the president’s February announcement.&lt;/p&gt;
    &lt;head rend="h2"&gt;Different rounding plans&lt;/head&gt;
    &lt;p&gt;Kwik Trip, a family-owned convenience store chain that operates in the Midwest, decided to round down cash purchases in stores where it hasn’t been able to find pennies.&lt;/p&gt;
    &lt;p&gt;“There’s no way that we wanted to charge (customers) an extra 2 cents because we just didn’t think that was fair,” said John McHugh, spokesperson for the company. “I mean, it’s not their fault that there there’s a penny shortage.”&lt;/p&gt;
    &lt;p&gt;But with 20 million customers a year, and 17% of them paying with cash, the policy will eventually cost Kwik Trip a couple of million dollars a year, McHugh said.&lt;/p&gt;
    &lt;p&gt;It’s not just businesses that face increased costs. Rounding to the closest nickel will cost consumers about $6 million a year, according to a July study by the Federal Reserve Bank of Richmond. That is fairly modest, coming to about five cents each across 133 million American households.&lt;/p&gt;
    &lt;p&gt;And rounding is not a national solution.&lt;/p&gt;
    &lt;p&gt;Four states - Delaware, Connecticut, Michigan and Oregon - as well as numerous cities, including New York, Philadelphia, Miami and Washington, DC, require merchants to provide exact change, according to the National Association of Convenience Stores (NACS).&lt;/p&gt;
    &lt;p&gt;In addition, the law covering the federal food assistance program known as SNAP requires that recipients not be charged more than other customers. Since SNAP recipients use a debit card that’s charged the precise amount, if merchants round down prices for cash purchases, they could be opening themselves to legal problems and fines, said Jeff Lenard, spokesperson for NACS.&lt;/p&gt;
    &lt;p&gt;“Rounding down on all transactions presents several challenges beyond the loss of an average of 2 cents per transaction,” Lenard said. “We desperately need legislation that allows rounding so retailers can make change for these customers.”&lt;/p&gt;
    &lt;p&gt;For that reason, NACS and other retail groups recently wrote to Congress asking for legislation to deal with the questions raised by the end of penny production.&lt;/p&gt;
    &lt;head rend="h2"&gt;End of a ‘wonderful life’&lt;/head&gt;
    &lt;p&gt;The penny was one of the nation’s first coins, first minted in 1787, six years before the Mint itself was established.&lt;/p&gt;
    &lt;p&gt;Benjamin Franklin is widely credited with designing the first penny known as the Fugio cent. Its current form arrived in 1909 on the centennial of Abraham Lincoln’s birth, when it became the first American coin to feature a president.&lt;/p&gt;
    &lt;p&gt;But it has declined in both use and popularity ever since. The Treasury Department now says there are an estimated 300 billion pennies in circulation. That comes to a bit less than $9 for every American. But most of those coins are “severely underutilized.” So, outcry from the public over its demise has been muted.&lt;/p&gt;
    &lt;p&gt;Joe Ditler, a 74-year old writer and historian from Colorado, said he still has an old cigar box filled with mostly pennies given to him by his grandfather. He remembers flattening pennies on railroad tracks or souvenir machines in amusement parks.&lt;/p&gt;
    &lt;p&gt;However, he only occasionally uses pennies to make a cash purchase. And he often tosses the one-cent coin in the tip jar.&lt;/p&gt;
    &lt;p&gt;“They bring back memories that have stayed with me all my life,” he said. “The penny has had a wonderful life. But it’s probably time for it to go away.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45901904</guid><pubDate>Wed, 12 Nov 2025 16:10:13 +0000</pubDate></item><item><title>The PowerPC Has Still Got It (Llama on G4 Laptop)</title><link>https://www.hackster.io/news/the-powerpc-has-still-got-it-c4348bd7a88c</link><description>&lt;doc fingerprint="9d0b59b1013a0518"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The PowerPC Has Still Got It&lt;/head&gt;
    &lt;head rend="h2"&gt;What good is a 2005 PowerBook G4 in this day and age? Not much, unless you want to run a modern large language model in style, that is.&lt;/head&gt;
    &lt;p&gt;For most people, the term “Apple silicon” brings to mind powerhouse processors like the M4 Max. Since Apple went through a lengthy Intel phase prior to the development of their M-series chips, it is often assumed that these are their first custom processors. But twenty years ago, Apple had different custom silicon in their computers — PowerPC microprocessors.&lt;/p&gt;
    &lt;p&gt;The advantages of these earlier chips were not as clear cut as the M-series chips. Diehard Apple fans swore that they were superior, while the PC crowd wouldn’t touch them with a ten-foot pole. But in any case, they are a couple decades old at this point, so they do not have a lot of gas left in the tank. However, Andrew Rossignol does not believe that the tank is empty just yet. Rossignol recently demonstrated that a PowerBook G4 from 2005 is capable of getting in on the action of running modern artificial intelligence (AI) algorithms — with some caveats, of course.&lt;/p&gt;
    &lt;head rend="h3"&gt;Process different&lt;/head&gt;
    &lt;p&gt;Rossignol, a vintage computing enthusiast, successfully ran a large language model (LLM) on a 1.5GHz PowerBook G4, a machine with just 1GB of RAM and a 32-bit processor. The experiment used a fork of llama2.c, an open-source LLM inference engine originally developed by Andrej Karpathy. Given the hardware constraints of the PowerBook, Rossignol chose the TinyStories model, a relatively small model with 110 million parameters that was designed specifically for generating simple short stories.&lt;/p&gt;
    &lt;p&gt;To make this work, Rossignol had to modify the original software to accommodate the PowerPC’s big-endian architecture, which differs from the little-endian format that most modern processors use. This involved converting model checkpoints and tokenizer data to the appropriate format, ensuring that numerical data was processed correctly. Additionally, the memory alignment requirements of the aging PowerPC chip meant that weights had to be copied into memory manually, rather than being memory-mapped as they would be on an x86 system.&lt;/p&gt;
    &lt;head rend="h3"&gt;Well, technically it works&lt;/head&gt;
    &lt;p&gt;Performance was, predictably, not so good. Running the model on an Intel Xeon Silver 4216 processor achieved a processing speed of 6.91 tokens per second. The same model on the PowerBook G4, however, managed just 0.77 tokens per second — taking a full four minutes to generate a short paragraph of text.&lt;/p&gt;
    &lt;p&gt;To improve performance, Rossignol leveraged AltiVec, the PowerPC’s vector processing extension. By rewriting the core matrix multiplication function using AltiVec’s single instruction, multiple data capabilities, he was able to increase inference speed to 0.88 tokens per second — a modest improvement, but you have to take what you can in a project like this.&lt;/p&gt;
    &lt;p&gt;Despite the slow performance, the fact that a 20-year-old laptop could successfully run a modern AI model at all is impressive. The PowerBook’s outdated architecture, limited RAM, and lack of specialized accelerators posed a number of challenges, but careful software optimizations and a deep understanding of the hardware allowed Rossignol to push the system well beyond its expected limits.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45901996</guid><pubDate>Wed, 12 Nov 2025 16:17:58 +0000</pubDate></item><item><title>Helm v4.0.0</title><link>https://github.com/helm/helm/releases/tag/v4.0.0</link><description>&lt;doc fingerprint="3612007fdbecf4d2"&gt;
  &lt;main&gt;
    &lt;p&gt;The Helm Team is proud to announce the first stable release of Helm 4.&lt;/p&gt;
    &lt;head rend="h2"&gt;New Features&lt;/head&gt;
    &lt;p&gt;Helm 4 has numerous new features, but a few deserve highlighting here:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Redesigned plugin system that supports Web Assembly based plugins&lt;/item&gt;
      &lt;item&gt;Post-renderers are now plugins&lt;/item&gt;
      &lt;item&gt;Server side apply is now supported&lt;/item&gt;
      &lt;item&gt;Improved resource watching, to support waiting, based on kstatus&lt;/item&gt;
      &lt;item&gt;Local Content-based caching (e.g. for charts)&lt;/item&gt;
      &lt;item&gt;Logging via slog enabling SDK logging to integrate with modern loggers&lt;/item&gt;
      &lt;item&gt;Reproducible builds of chart archives&lt;/item&gt;
      &lt;item&gt;Updated SDK API including support for multiple chart API versions (new experimental v3 chart API version coming soon)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For full release notes, please see: https://helm.sh/docs/overview/&lt;/p&gt;
    &lt;head rend="h2"&gt;Compatibility with Helm v3&lt;/head&gt;
    &lt;p&gt;Helm v4 is a major version with backward incompatible changes including to the flags and output of the Helm CLI and to the SDK.&lt;/p&gt;
    &lt;p&gt;Please evaluate the changes to your workflows. The changes are not as extensive as those from Helm v2 to v3, with the goal that the majority of workflows remain compatible between Helm v3 and v4.&lt;/p&gt;
    &lt;p&gt;Helm charts apiVersion v2 (majority of today's charts) will continue to be supported in Helm v4. Existing charts should continue to install, upgrade, and otherwise work. Please test the installation and upgrade of charts to ensure it works as expected. Changes (e.g., server side apply) may impact the experience.&lt;/p&gt;
    &lt;head rend="h2"&gt;Community&lt;/head&gt;
    &lt;p&gt;The community keeps growing, and we'd love to see you there!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Join the discussion in Kubernetes Slack: &lt;list rend="ul"&gt;&lt;item&gt;for questions and just to hang out&lt;/item&gt;&lt;item&gt;for discussing PRs, code, and bugs&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Hang out at the Public Developer Call: Thursday, 9:30 Pacific via Zoom&lt;/item&gt;
      &lt;item&gt;Test, debug, and contribute charts: ArtifactHub/packages&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Installation and Upgrading&lt;/head&gt;
    &lt;p&gt;Download Helm v4.0.0. The common platform binaries are here:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;MacOS amd64 (checksum / 125233cf943e6def2abc727560c5584e9083308d672d38094bae1cc3e0bfeaa2)&lt;/item&gt;
      &lt;item&gt;MacOS arm64 (checksum / 4f5d367af9e2141b047710539d22b7e5872cdaef788333396077236feb422419)&lt;/item&gt;
      &lt;item&gt;Linux amd64 (checksum / c77e9e7c1cc96e066bd240d190d1beed9a6b08060b2043ef0862c4f865eca08f)&lt;/item&gt;
      &lt;item&gt;Linux arm (checksum / 23498ff8f5fb358ad2576269cd41fa9a54b9469332806dff0d689470323180be)&lt;/item&gt;
      &lt;item&gt;Linux arm64 (checksum / 8c5c77e20cc29509d640e208a6a7d2b7e9f99bb04e5b5fbe22707b72a5235245)&lt;/item&gt;
      &lt;item&gt;Linux i386 (checksum / eda0b6508def454ba07e2f938c55f73be795e7f99552078ccc8af2c2bbd58a45)&lt;/item&gt;
      &lt;item&gt;Linux ppc64le (checksum / 73ae83e9888aafa0e9c57a1d4d77dcb6c97c253ef175a4983a8bb4bcc771d2eb)&lt;/item&gt;
      &lt;item&gt;Linux s390x (checksum / 9c7368b18c76fcae9e0281e1ee875ea0d9b5970ac3a00c4eb963205948594bad)&lt;/item&gt;
      &lt;item&gt;Linux riscv64 (checksum / a688c2559c57d6a858c49b9237b7d6bbce5c634aa5204c4342bdc8a06818b9f1)&lt;/item&gt;
      &lt;item&gt;Windows amd64 (checksum / 0f9a8c891b8d908a37fbb68f12dea92b633eb29e49070bd650f5760a1a99aa8d)&lt;/item&gt;
      &lt;item&gt;Windows arm64 (checksum / f3ff262427547cc1b1dc3356d587ed8ffaa23f2abf24bc06660a350b9b7925f9)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Quickstart Guide will get you going from there. For upgrade instructions or detailed installation notes, check the install guide. You can also use a script to install on any system with &lt;code&gt;bash&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;What's Next&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;3.19.3 and 4.0.1 are the next patch releases and will be on December 10, 2025&lt;/item&gt;
      &lt;item&gt;3.20.0 and 4.1.0 is the next minor releases and will be on January 21, 2026&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Thank You!&lt;/head&gt;
    &lt;p&gt;The Helm project has enjoyed code contributions from many community members. Many more community members have assisted by filing issues and working with us to identify and eliminate bugs while adding new features. The #helm-users slack channel has long been a friendly and open forum for getting help and learning more about Helm. We cannot thank you enough for making this a helpful, friendly, and welcoming community for all.&lt;/p&gt;
    &lt;p&gt;❤️ The Helm Team&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45902604</guid><pubDate>Wed, 12 Nov 2025 17:02:38 +0000</pubDate></item><item><title>Launch HN: JSX Tool (YC F25) – A Browser Dev-Panel IDE for React</title><link>https://news.ycombinator.com/item?id=45903161</link><description>&lt;doc fingerprint="ae4ab77c7123cd5c"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hi HN, We’re Jamie &amp;amp; Dan, building JSX Tool (&lt;/p&gt;https://jsxtool.com&lt;p&gt;) a new inspector/dev panel IDE that allows you to navigate to any line of your React project’s JSX with just a click and a command click to explore your render stack.&lt;/p&gt;&lt;p&gt;Demo video: https://www.youtube.com/watch?v=JIIXvN7vhrs&lt;/p&gt;&lt;p&gt;I’ve been writing React code for nearly a decade. Since I first saw source maps in the days of Babel and Redux, I’ve always wanted to be able to edit my code from the source maps. I’ve also always wanted to be able to inspect my JSX like it was HTML.&lt;/p&gt;&lt;p&gt;Last year, I found my first real use of AI was taking ad-hoc CSS changes in the Chrome element inspector, pasting them into ChatGPT, and asking for the equivalent in Tailwind. I’d then paste those changes into my React TSX files.&lt;/p&gt;&lt;p&gt;I wanted to streamline this process but came to the conclusion that to do so I needed to build a JSX inspector. I had to write a custom AST parser to create a mapping between the JSX and HTML. So I hacked on an inspector for a couple of months that connected JSX to the DOM in both directions.&lt;/p&gt;&lt;p&gt;The next feature was adding a CSS editor, like the one in the browser inspectors but for JSX. Unlike styling a piece of HTML I decided that any in memory style edits to a React fiber should be globally applied, as if you had tweaked that line of code in your codebase.&lt;/p&gt;&lt;p&gt;Finally, I was able to add the two AI features I really wanted: (1) prompt for in-memory styles for when I was pixel tweaking, and (2) save those temporary changes back to my codebase in the convention of the codebase I was working in.&lt;/p&gt;&lt;p&gt;To accomplish talking to the filesystem from the Chrome extension I built a little local server that mounts from the root of your project and allows the extension to send file-system commands back to your project root. We named this the “Dev Server”. (Note: You can fully use us as a JSX inspector without this server installed.)&lt;/p&gt;&lt;p&gt;After all that, I found that to convert myself as a user I needed it to be a pretty fully functional IDE. I needed vim bindings, I needed a typechecker, I needed auto-complete, I needed a linter, I needed code search and I needed a proper file explorer. Fortunately we were able to take advantage of the dev-server architecture we had stumbled onto in order to add an LSP server and Rip Grep. At this point, after months of dog fooding, I use JSX Tool for almost all of my website edits.&lt;/p&gt;&lt;p&gt;We’re still rough around the edges for mobile but we’re working on that.&lt;/p&gt;&lt;p&gt;All of the IDE stuff not involving AI is free and works fine without AI. We let you get a taste of the prompting stuff for free but apply some rate limits.&lt;/p&gt;&lt;p&gt;The extension itself is not open source but the dev server with the LSP is. It’s a great foundation if you want to build any sort of in-browser IDE and it's nearly React agnostic. Building the dev server was a big undertaking so I’d love to see someone fork it and find value in it.&lt;/p&gt;&lt;p&gt;In the future we want to start adding things that we are in a position to take advantage of over something like Cursor, such as letting AI give you code suggestions for runtime exceptions or work with the network logs. We think that the convenience of having your IDE in the dev panel gives us a leg up in convenience and workflow context.&lt;/p&gt;&lt;p&gt;Anyway, regardless of how you feel about AI coding, I wanted to make something that was useful with or without AI. We’d love it if you gave it a spin and we want to share anything we can about the technical side of the product that you might find interesting.&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45903161</guid><pubDate>Wed, 12 Nov 2025 17:43:42 +0000</pubDate></item></channel></rss>