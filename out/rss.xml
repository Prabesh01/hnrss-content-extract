<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 13 Oct 2025 03:52:59 +0000</lastBuildDate><item><title>Show HN: Aidlab – Health Data for Devs</title><link>https://news.ycombinator.com/item?id=45549392</link><description>&lt;doc fingerprint="33f8800e7599df80"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hey HN! I'm Jakub, and together with my co-founders Agnieszka and Nathan, we built Aidlab, a wearable that gives developers gold-standard physiological data. Unlike health trackers with locked-down APIs, Aidlab ships with a free SDK [1] across 6+ platforms so you can just &lt;/p&gt;pip install aidlabsdk&lt;p&gt; or &lt;/p&gt;flutter pub add aidlab_sdk&lt;p&gt; or whatever platform (even Unity), and start streaming raw health data and events in real time with simple &lt;/p&gt;didReceive*(timestamp, value)&lt;p&gt; callbacks. Currently, we are exposing 13 data types including raw ECG, cough/snoring, motion, raw respiration, skin temperature, bodyweight reps, body position, and 20 high-level stats like stress or readiness through the API.&lt;/p&gt;&lt;p&gt;The most common questions I got are:&lt;/p&gt;&lt;p&gt;1) "how is it better than my smartwatch?"&lt;/p&gt;&lt;p&gt;2) "why we built it?"&lt;/p&gt;&lt;p&gt;Chest-mounted wearables are considered the gold standard for physiological measurements. For example, whenever Apple validates their watch, they benchmark against chest straps [2], as some signals can only be reliably measured (or measured at all!) near the heart including continuous ECG, true respiration (based on lung volume changes) or body position/orientation.&lt;/p&gt;&lt;p&gt;As for the second question: the problem for us was that smartwatches were too simple and the data too inaccurate, while advanced medical devices were too pricey or too complicated. We found a sweet spot between accuracy and accessibility - Aidlab delivers medical-grade signals without the hospital-level complexity. As "medical-grade" is a bold statement, we’ve published validation papers comparing Aidlab’s performance with other certified medical devices [3].&lt;/p&gt;&lt;p&gt;Today Aidlab is already a pretty mature concept. We've been building Aidlab for 2 years, shipped our first version in 2020, we got our first clients, including Bryan Johnson from Kernel/Blueprint (longevity research) or Boeing/Jeppesen (monitoring pilots’ bio-signals during tests&amp;amp;training).&lt;/p&gt;&lt;p&gt;Now we're about to release Aidlab 2 [4] - with additional signals like EDA and GPS, and a bunch of new features, including on-device ML (we've trained a few small LSTM models running inference with TensorFlow Lite for Micro). The cool part is that we've built a custom shell on top of FreeRTOS, letting anyone invoke POSIX-like commands directly on the device, for example:&lt;/p&gt;&lt;p&gt;timeout 10 temperature --sampling-rate 1 | tee /data/temperature.csv | tail -n 5&lt;/p&gt;&lt;p&gt;The biggest breakthrough for us was realizing that cloud-based processing was the wrong approach. In the beginning, we pushed most of the computation to the cloud - it seemed natural, but turned out to be slow, costly, and devs didn't want it ("hey, is there a way to use your product without cloud?"). For example, our ECG analysis pipeline used to send raw data to an external microservice, processing it in 30-minute chunks through Bull queues. A 24-hour Holter analysis could spawn 100k+ event objects and take significant time to complete. Now we're doing everything we can to move computation to the edge. In an ideal world, the cloud wouldn't store or process anything - just receive already-analyzed, privacy-preserving results straight from the device.&lt;/p&gt;&lt;p&gt;Another lesson: don't hand-solder prototypes at 3 a.m. to save money -&amp;gt; please pay professionals to assemble PCBs.&lt;/p&gt;&lt;p&gt;We decided to showcase this now for three reasons:&lt;/p&gt;&lt;p&gt;- health feels more relevant than ever with the rise of longevity research and biohacking,&lt;/p&gt;&lt;p&gt;- we are close to finalizing Aidlab 2,&lt;/p&gt;&lt;p&gt;- and I am super curious to see if anyone here finds it useful!&lt;/p&gt;&lt;p&gt;If you'd like to check the quality of Aidlab for yourself, we are publishing free datasets every week during different activities [5].&lt;/p&gt;&lt;p&gt;[1] https://github.com/Aidlab&lt;/p&gt;&lt;p&gt;[2] https://www.apple.com/health/pdf/Heart_Rate_Calorimetry_Acti...&lt;/p&gt;&lt;p&gt;[3] https://aidlab.com/validation&lt;/p&gt;&lt;p&gt;[4] https://aidlab.com/aidlab-2&lt;/p&gt;&lt;p&gt;[5] https://aidlab.com/datasets&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45549392</guid><pubDate>Sat, 11 Oct 2025 14:24:35 +0000</pubDate></item><item><title>A whirlwind introduction to dataflow graphs (2018)</title><link>https://fgiesen.wordpress.com/2018/03/05/a-whirlwind-introduction-to-dataflow-graphs/</link><description>&lt;doc fingerprint="b3208217658411de"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A whirlwind introduction to dataflow graphs&lt;/head&gt;
    &lt;p&gt;While in the middle of writing “Reading bits in far too many ways, part 3”, I realized that I had written a lot of background material that had absolutely nothing to do with bit I/O and really was worth putting in its own post. This is that post.&lt;/p&gt;
    &lt;p&gt;The problem I’m concerned with is fairly easy to state: say we have some piece of C++ code that we’re trying to understand (and perhaps improve) the performance of. A good first step is to profile it, which will give us some hints which parts are slow, but not necessarily why. On a fundamental level, any kind of profiling (or other measurement) is descriptive, not predictive: it can tell you how an existing system is behaving, but if you’re designing something that’s more than a few afternoons worth of work, you probably don’t have the time or resources to implement 5 or 6 completely different design alternatives, pick whichever one happens to work best, and throw the rest away. You should be able to make informed decisions up front from an algorithm sketch without having to actually write a fleshed-out implementation.&lt;/p&gt;
    &lt;p&gt;One thing I want to emphasize particularly here is that experiments coupled with before/after measurements are no adequate substitute for a useful performance model. These kinds of measurements can tell you how much you’ve improved, but not if you are where you should be: if I tell you that by tweaking some config files, I managed to double the number of requests served per second by the web server, that sounds great. It sounds less good if I give you the additional piece of information that with this fix deployed, we’re now at a whopping 1.5 requests per second; having an absolute scale of reference matters!&lt;/p&gt;
    &lt;p&gt;This goes especially for microbenchmarks. With microbenchmarks, like a trial lawyer during cross-examination, you should never ask a question you don’t know the answer to (or at least have a pretty good idea of what it is). Real-world systems are generally too complex and intertwined to understand from surface measurements alone. If you have no idea how a system works at all, you don’t know what the right questions are, nor how to ask them, and any answers you get will be opaque at best, if not outright garbage. Microbenchmarks are a useful tool to confirm that an existing model is a good approximation to reality, but not very helpful in building these models to begin with.&lt;/p&gt;
    &lt;head rend="h3"&gt;Machine models&lt;/head&gt;
    &lt;p&gt;So, if we want to go deeper than just squinting at C/C++ code and doing some hand-waving, we need to start looking at a somewhat lower abstraction level and define a machine model that is more sophisticated than “statements execute one by one”. If you’re only interested in a single specific processor, one option is to use whatever documentation and tools you can find for the chip in question and analyze your code in detail for that specific machine. And if you’re willing to go all-out on microarchitectural tweaking, that’s indeed the way to go, but it’s a giant step from looking at C++ code, and complete overkill in most cases.&lt;/p&gt;
    &lt;p&gt;Instead, what I’m going to do is use a simplified machine model that allows us to make quantitative predictions about the behavior of straightforward compute-bound loops, which is simple to describe but still gives us a lot of useful groundwork for more complex scenarios. Here’s what I’ll use:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We have an unlimited set of 64-bit integer general-purpose registers, which I’ll refer to by names like &lt;code&gt;rSomething&lt;/code&gt;. Any “identifiers” that aren’t prefixed with a lowercase r are either symbolic constants or things like labels.&lt;/item&gt;
      &lt;item&gt;We have the usual 64-bit integer arithmetic and logic operations. All operations can either be performed between two registers or a register and an immediate constant, and the result is placed in another register. All arithmetic uses two’s complement. For simplicity, all 64-bit values are permitted as immediate constants.&lt;/item&gt;
      &lt;item&gt;There’s a flat, byte-granular 64-bit address space, and pointers are just represented as integers.&lt;/item&gt;
      &lt;item&gt;All memory accesses require explicit load and store operations. Memory accesses are either 8, 16, 32, or 64 bits in size and can use (for my convenience) both little-endian or big-endian byte ordering, when requested. One of these is the default, but both are the same cost. Narrow stores store the least significant bits of the register in question; narrow loads zero-extend to 64 bits. Loads and stores have a few common addressing modes (that I’ll introduce as I use them). Unaligned loads and stores are supported.&lt;/item&gt;
      &lt;item&gt;There’s unconditional branches, which just jump to a given location, and conditional branches, which compare a register to either another register or an immediate constant, and branch to a given destination if the condition is true.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Code will be written in a pseudo-C form, at most one instruction per line. Here’s a brief example showing what kind of thing I have in mind:&lt;/p&gt;
    &lt;quote&gt;loop: // label rFoo = rBar | 1; // bitwise logical OR rFoo = lsl(rFoo, 3); // logical shift left rBar = asr(rBar, rBaz); // arithmetic shift right rMem = load64LE(rBase + rFoo); // little-endian load store16BE(rDest + 3, rMem); // big-endian store rCount = rCount - 1; // basic arithmetic if rCount != 0 goto loop; // branch&lt;/quote&gt;
    &lt;p&gt;Shifts use explicit mnemonics because there’s different types of right shifts and at this level of abstraction, registers are generally treated as untyped bags of bits. I’ll introduce other operations and addressing modes as we get to them. What we’ve seen so far is quite close to classic RISC instruction sets, although I’ll allow a larger set of addressing modes than some of the more minimalist designs, and require support for unaligned access on all loads and stores. It’s also close in spirit to an IR (Intermediate Representation) you’d expect to see early in the backend of a modern compiler: somewhat lower-level than LLVM IR, and comparable to early-stage LLVM Machine IR or GCC RTL.&lt;/p&gt;
    &lt;p&gt;This model requires us to make the distinction between values kept in registers and memory accesses explicit, and flattens down control flow to basic blocks connected by branches. But it’s still relatively easy to look at a small snippet of C++ and e.g. figure out how many arithmetic instructions it boils down to: just count the number of operations.&lt;/p&gt;
    &lt;p&gt;As a next step, we could now specify a virtual processor to go with our instruction set, but I don’t want to really get into that level of detail; instead of specifying the actual processor, I’ll work the same way actual architectures do: we require that the end result (eventual register and memory contents in our model) of running a program must be as if we had executed the instructions sequentially one by one (as-if rule). Beyond that, an aggressive implementation is free to cut corners as much as it wants provided it doesn’t get caught. We’ll assume we’re in an environment—the combination of compilers/tools and the processor itself—that uses pipelining and tries to extract instruction-level parallelism to achieve higher performance, in particular:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Instructions can launch independent from each other, and take some number of clock cycles to complete. For an instruction to start executing, all the operands it depends on need to have been computed. As long as the dependencies are respected, all reorderings are valid.&lt;/item&gt;
      &lt;item&gt;There is some limit W (“width”) on how many new instructions we can start per clock cycle. In-flight instructions don’t interfere with each other; as long as we have enough independent work, we can start W new instructions every cycle. We’re going to treat W as variable.&lt;/item&gt;
      &lt;item&gt;Memory operations have a latency of 4 cycles, meaning that the result of a load is available 4 cycles after the load issued, and a load reading the bytes written by a prior store can issue 4 cycles after the store. That’s a fairly typical latency for a load that hits in the L1 cache, in case you were wondering.&lt;/item&gt;
      &lt;item&gt;Branches (conditional or not) count as a single instruction, but their latency is variable. Unconditional branches or easily predicted branches such as the loop counter in along-running loop have an effective latency of 0 cycles, meaning the instructions being branched to can issue at the same time as the branch itself. Unpredictable branches have a nonzero cost that depends on how unpredictable they are—I won’t even try to be more precise here.&lt;/item&gt;
      &lt;item&gt;Every other instruction has a latency of 1 clock cycle, meaning the result is available in the next cycle.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This model can be understood as approximating either a dataflow architecture, an out-of-order machine with a very large issue window (and infinitely fast front-end), or a statically scheduled in-order machine running code compiled with a Sufficiently Smart Scheduler. (The kind that actually exists; e.g. a compiler implementing software pipelining).&lt;/p&gt;
    &lt;p&gt;Furthermore, I’m assuming that while there is explicit control flow (unlike a pure dataflow machine), there is a branch prediction mechanism in place that allows the machine to guess the control flow path taken arbitrarily far in advance. When these guesses are correct, the branches are effectively free other than still taking an instruction slot, during which time the machine checks whether its prediction was correct. When the guess was incorrect, the machine reverts all computations that were down the incorrectly guessed path, and takes some number of clock cycles to recover. If this idea of branch prediction is new to you, I’ll refer you to Dan Luu’s excellent article on the subject, which explains both how and why computers would be doing this.&lt;/p&gt;
    &lt;p&gt;The end result of these model assumptions is that while control flow exists, it’s on the sidelines: its only observable effect is that it sometimes causes us to throw away a bunch of work and take a brief pause to recover when we guessed wrong. Dataflow, on the other hand—the dependencies between instructions, and how long it takes for these dependencies to be satisfied—is front and center.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dataflow graphs&lt;/head&gt;
    &lt;p&gt;Why this emphasis? Because dataflow and data dependencies is because they can be viewed as the fundamental expression of the structure of a particular computation, whether it’s done on a small sequential machine, a larger superscalar out-of-order CPU, a GPU, or in hardware (be it a hand-soldered digital circuit, a FPGA, or an ASIC). Dataflow and keeping track of the shape of data dependencies is an organizing principle of both the machines themselves and the compilers that target them.&lt;/p&gt;
    &lt;p&gt;And these dependencies are naturally expressed in graph form, with individual operations being the nodes and data dependencies denoted by directed edges. In this post, I’ll have dependent operations point towards the operations they depend on, with the directed edges labeled with their latency. To reduce clutter, I’ll only write latency numbers when they’re not 1.&lt;/p&gt;
    &lt;p&gt;With all that covered, and to see what the point of this all is, let’s start with a simple, short toy program that just sums the 64-bit integers in some array delineated by two pointers stored in &lt;code&gt;rCurPtr&lt;/code&gt; (which starts pointing to the first element) and &lt;code&gt;rEndPtr&lt;/code&gt; (which points to one past the last element), idiomatic C++ iterator-style.&lt;/p&gt;
    &lt;quote&gt;loop: rCurInt = load64(rCurPtr); // Load rSum = rSum + rCurInt; // Sum rCurPtr = rCurPtr + 8; // Advance if rCurPtr != rEndPtr goto loop; // Done?&lt;/quote&gt;
    &lt;p&gt;We load a 64-bit integer from the current pointer, add it to our current running total in register &lt;code&gt;rSum&lt;/code&gt;, increment the pointer by 8 bytes (since we grabbed a 64-bit integer), and then loop until we’re done. Now let’s say we run this program for a short 6 iterations and draw the corresponding dataflow graph (click to see full-size version):&lt;/p&gt;
    &lt;p&gt;Note I group nodes into ranks by which cycle they can execute in, at the earliest, assuming we can issue as many instructions in parallel as we want, purely constrained by the data dependencies. The “Load” and “Advance” from the first iteration can execute immediately; the “Done?” check from the first iteration looks at the updated &lt;code&gt;rCurPtr&lt;/code&gt;, which is only known one cycle later; and “Sum” from the first iteration needs to wait for the load to finish, which means it can only start a full 4 cycles later.&lt;/p&gt;
    &lt;p&gt;As we can see, during the first four cycles, all we do is keep issuing more loads and advancing the pointer. It takes until cycle 4 for the results of the first load to become available, so we can actually do some summing. After that, one more load completes every cycle, allowing us to add one more integer to the running sum in turn. If we let this process continue for longer, all the middle iterations would look the way cycles 4 and 5 do: in our state state, we’re issuing a copy of all four instructions in the loop every cycle, but from different iterations.&lt;/p&gt;
    &lt;p&gt;There’s a few conclusions we can draw from this: first, we can see that this four-instruction loop achieves a steady-state throughput of one integer added to the sum in every clock cycle. We take a few cycles to get into the steady state, and then a few more cycles at the end to drain out the pipeline, but if we start in cycle 0 and keep running N iterations, then the final sum will be completed by cycle N+4. Second, even though I said that our model has infinite lookahead and is free to issue as many instructions per cycle as it wants, we “only” end up using at most 4 instructions per cycle. The limiter here ends up being the address increment (“Advance”); we increment the pointer after every load, per our cost model this increment takes a cycle of latency, and therefore the load in the next iteration of the loop (which wants to use the updated pointer) can start in the next cycle at the earliest.&lt;/p&gt;
    &lt;p&gt;This is a crucial point: the longest-latency instruction in this loop is definitely the load, at 4 cycles. But that’s not a limiting factor; we can schedule around the load and do the summing later. The actual problem here is with the pointer advance; every single instruction that comes after it in program order depends on it either directly or indirectly, and therefore, its 1 cycle of latency determines when the next loop iteration can start. We say it’s on the critical path. In loops specifically, we generally distinguish between intra-iteration dependencies (between instructions within the same iteration, say “Sum 0” depending on “Load 0”) and inter-iteration or loop-carried dependencies (say “Sum 1” depending on “Sum 0”, or “Load 1” depending on “Advance 0”). Intra-iteration dependencies may end up delaying instructions within that iteration quite a lot, but it’s inter-iteration dependencies that determine how soon we can start working on the next iteration of the loop, which is usually more important because it tends to open up more independent instructions to work on.&lt;/p&gt;
    &lt;p&gt;The good news is that W=4 is actually a fairly typical number of instructions decoded/retired per cycle in current (as of this writing in early 2018) out-of-order designs, and the instruction mixture here (1 load, 1 branch, 2 arithmetic instructions) is also one that is quite likely to be able to issue in parallel on a realistic 4-wide decode/retire design. While many machines can issue a lot more instructions than that in short bursts, a steady state of 4 instructions per cycle is definitely good. So even though we’re not making much of the infinite parallel computing power of our theoretical machine, in practical terms, we’re doing OK, although on real machines we might want to apply some more transforms to the loop; see below.&lt;/p&gt;
    &lt;p&gt;Because these real-world machines can’t start an arbitrary number of instructions at the same time, we have another concern: throughput. Say we’re running the same loop on a processor that has W=2, i.e. only two instructions can start every cycle. Because our loop has 4 instructions, that means that we can’t possibly start a new loop iteration more often than once every two clock cycles, and the limiter aren’t the data dependencies, but the number of instructions our imaginary processor can execute in a clock cycle; we’re throughput-bound. We would also be throughput-bound on a machine with W=3, with a steady state of 3 new instructions issued per clock cycle, where we can start working on a new iteration every 4/3≈1.33 cycles.&lt;/p&gt;
    &lt;head rend="h3"&gt;A different example&lt;/head&gt;
    &lt;p&gt;For the next example, we’re going to look at what’s turned into everyone’s favorite punching-bag of a data structure, the linked list. Let’s do the exact same task as before, only this time, the integers are stored in a singly-linked list instead of laid out as an array. We store first a 64-bit integer and then a 64-bit pointer to the next element, with the end of the list denoted by a special value stored in &lt;code&gt;rEndPtr&lt;/code&gt; as before. We also assume the list has at least 1 element. The corresponding program looks like this:&lt;/p&gt;
    &lt;quote&gt;loop: rCurInt = load64(rCurPtr); // LoadInt rSum = rSum + rCurInt; // Sum rCurPtr = load64(rCurPtr + 8); // LoadNext if rCurPtr != rEndPtr goto loop; // Done?&lt;/quote&gt;
    &lt;p&gt;Very similar to before, only this time, instead of incrementing the pointer, we do another load to grab the “next” pointer. And here’s what happens to the dataflow graph if we make this one-line change:&lt;/p&gt;
    &lt;p&gt;Switching from a contiguous array to a linked list means that we have to wait for the load to finish before we can start the next iteration. Because loads have a latency of 4 cycles in our model, that means we can’t start a new iteration any more often than once every 4 cycles. With our 4-instruction loop, we don’t even need any instruction-level parallelism to reach that target; we might as well just execute one instruction per cycle and still hit the same overall throughput.&lt;/p&gt;
    &lt;p&gt;Now, this example, with its short 4-instruction loop, is fairly extreme; if our loop had say a total of 12 instructions that worked out nicely, the same figure might well end up averaging 3 instructions per clock cycle, and that’s not so bad. But the underlying problem here is a nasty one: because our longest-latency instruction is on the critical path between iterations, it ends up determining the overall loop throughput.&lt;/p&gt;
    &lt;p&gt;In our model, we’re still primarily focused on compute-bound code, and memory access is very simple: there’s no memory hierarchy with different cache levels, all memory accesses take the same time. If we instead had a more realistic model, we would also have to deal with the fact that some memory accesses take a whole lot longer than 4 cycles to complete. For example, suppose we have three cache levels and, at the bottom, DRAM. Sticking with the powers-of-4 theme, let’s say that a L1 cache hit takes 4 cycles (i.e. our current memory access latency), a L2 hit takes 16 cycles, a L3 hit takes 64 cycles, and an actual memory access takes 256 cycles—for what it’s worth, all these numbers are roughly in the right ballpark for high-frequency desktop CPUs under medium memory subsystem load as of this writing.&lt;/p&gt;
    &lt;p&gt;Finding work to keep the machine otherwise occupied for the next 4 cycles (L1 hit) is usually not that big a deal, unless we have a very short loop with unfavorable dependency structure, as in the above example. Fully covering the 16 cycles for a L1 miss but L2 hit is a bit trickier and requires a larger out-of-order window, but current out-of-order CPUs have those, and as long as there’s enough other independent work and not too many hard-to-predict branches along the way, things will work out okay. With a L3 cache hit, we’ll generally be hard-pressed to find enough independent work to keep the core usefully busy during the wait for the result, and if we actually miss all the way to DRAM, then in our current model, the machine is all but guaranteed to stall; that is, to have many cycles with no instructions executed at all, just like the gaps in the diagram above.&lt;/p&gt;
    &lt;p&gt;Because linked lists have this nasty habit of putting memory access latencies on the critical path, they have a reputation of being slow “because they’re bad for the cache”. Now while it’s definitely true that most CPUs with a cache would much rather have you iterate sequentially over an array, we have to be careful how we think about it. To elaborate, suppose we have yet another sum kernel, this time processing an array of pointers to integers, to compute the sum of the pointed-to values.&lt;/p&gt;
    &lt;quote&gt;loop: rCurIntPtr = load64(rCurPtr); // LoadPtr rCurInt = load64(rCurIntPtr); // LoadInt rSum = rSum + rCurInt; // Sum rCurPtr = rCurPtr + 8; // Advance if rCurPtr != rEndPtr goto loop; // Done?&lt;/quote&gt;
    &lt;p&gt;And this time, I’ll prune the dataflow graph to show only the current iteration and its direct dependency relationships with earlier and later iterations, because otherwise these more complicated graphs will get cluttered and unreadable quickly:&lt;/p&gt;
    &lt;p&gt;A quick look over that graph shows us that copies of the same instruction from different iterations are all spaced 1 cycle apart; this means that in the steady state, we will again execute one iteration of the loop per clock cycle, this time issuing 5 instructions instead of 4 (because there are 5 instructions in the loop). Just like in the linked list case, the pointer indirection here allows us to jump all over memory (potentially incurring cache misses along the way) if we want to, but there’s a crucial difference: in this setup, we can keep setting up future iterations of the loop and get more loads started while we’re waiting for the first memory access to complete.&lt;/p&gt;
    &lt;p&gt;To explain what I mean, let’s pretend that every single of the “LoadInt”s misses the L1 cache, but hits in the L2 cache, so its actual latency is 16 cycles, not 4. But a latency of 16 cycles just means that it takes 16 cycles between issuing the load and getting the result; we can keep issuing other loads for the entire time. So the only thing that ends up happening is that the “Sum k” in the graph above happens 12 cycles later. We still start two new loads every clock cycle in the steady state; some of them end up taking longer, but that does not keep us from starting work on a new iteration of the loop in every cycle.&lt;/p&gt;
    &lt;p&gt;Both the linked list and the indirect-sum examples have the opportunity to skip all over memory if they want to; but in the linked-list case, we need to wait for the result of the previous load until we can get started on the next one, whereas in the indirect-sum case, we get to overlap the wait times from the different iterations nicely. As a result, in the indirect-sum case, the extra latency towards reaching the final sum is essentially determined by the worst single iteration we had, whereas in the linked-list case, every single cache miss makes our final result later (and costs us throughput).&lt;/p&gt;
    &lt;p&gt;The fundamental issue isn’t that the linked-list traversal might end up missing the cache a lot; while this isn’t ideal (and might cost us in other ways), the far more serious issue is that any such cache miss prevents us from making progress elsewhere. Having a lot of cache misses isn’t necessarily a problem if we get to overlap them; having long stretches of time were we can’t do anything else, because everything else we could do depends on that one cache-missing load, is.&lt;/p&gt;
    &lt;p&gt;In fact, when we hit this kind of problem, our best bet is to just switch to doing something else entirely. This is what CPUs with simultaneous multithreading/hardware threads (“hyperthreads”) and essentially all GPUs do: build the machine so that it can process instructions from multiple instruction streams (threads), and then if one of the threads isn’t really making progress right now because it’s waiting for something, just work on something else for a while. If we have enough threads, then we can hopefully fill those gaps and always have something useful to work on. This trade-off is worthwhile if we have many threads and aren’t really worried about the extra latency caused by time-slicing, which is why this approach is especially popular in throughput-centric architectures that don’t worry about slight latency increases.&lt;/p&gt;
    &lt;head rend="h3"&gt;Unrolling&lt;/head&gt;
    &lt;p&gt;But let’s get back to our original integer sum code for a second:&lt;/p&gt;
    &lt;quote&gt;loop: rCurInt = load64(rCurPtr); // Load rSum = rSum + rCurInt; // Sum rCurPtr = rCurPtr + 8; // Advance if rCurPtr != rEndPtr goto loop; // Done?&lt;/quote&gt;
    &lt;p&gt;We have a kernel with four instructions here. Out of these four, two (“Load” and “Sum”) do the actual work we want done, whereas “Advance” and “Done?” just implement the loop itself and are essentially overhead. This type of loop is a prime target for unrolling, where we collapse two or more iterations of the loop into one to decrease the overhead fraction. Let’s not worry about the setup or what to do when the number of elements in the array is odd right now, and only focus on the “meat” of the loop. Then a 2× unrolled version might look like this:&lt;/p&gt;
    &lt;quote&gt;loop: rCurInt = load64(rCurPtr); // LoadEven rSum = rSum + rCurInt; // SumEven rCurInt = load64(rCurPtr + 8); // LoadOdd rSum = rSum + rCurInt; // SumOdd rCurPtr = rCurPtr + 16; // Advance if rCurPtr != rEndPtr goto loop; // Done?&lt;/quote&gt;
    &lt;p&gt;which has this dataflow graph:&lt;/p&gt;
    &lt;p&gt;Note that even though I’m writing to &lt;code&gt;rCurInt&lt;/code&gt; twice in an iteration, which constitutes a write-after-write (WAW) or “output dependency”, there’s no actual dataflow between the loads and sums for the first and second version of &lt;code&gt;rCurInt&lt;/code&gt;, so the loads can issue in parallel just fine.&lt;/p&gt;
    &lt;p&gt;This isn’t bad: we now have two loads every iteration and spend 6N instructions to sum 2N integers, meaning we take 3 instructions per integer summed, whereas our original kernel took 4. That’s an improvement, and (among other things) means that while our original integer-summing loop needed a machine that sustained 4 instructions per clock cycle to hit full throughput, we can now hit the same throuhgput on a smaller machine that only does 3 instructions per clock. This is definitely progress.&lt;/p&gt;
    &lt;p&gt;However, there’s a problem: if we look at the diagram, we can see that we can indeed start a new pair of loads every clock cycle, but there’s a problem with the summing: we have two dependent adds in our loop, and as we can see from the relationship between “SumEven k” and “SumEven k+1”, the actual summing part of the computation still takes 2 cycles per iteration. On our idealized dataflow machine with infinite lookahead, that just means that all the loads will get front-loaded, and then the adds computing the final sum proceed at their own pace; the result will eventually be available, but it will still take a bit more than 2N cycles, no faster than we were in the original version of the loop. On a more realistic machine (which can only look ahead by a limited number of instructions), we would eventually stop being able to start new loop iterations until some of the old loop iterations have completed. No matter how we slice it, we’ve gone from adding one integer to the sum per cycle to adding two integers to the sum every two cycles. We might take fewer instructions to do so, which is a nice consolation prize, but this is not what we wanted!&lt;/p&gt;
    &lt;p&gt;What’s happened is that unrolling shifted the critical path. Before, the critical path between iterations went through the pointer advance (or, to be more precise, there were two critical paths, one through the pointer advance and one through the sum, and they were both the same length). Now that we do half the number of advances per item, that isn’t a problem anymore; but the fact that we’re summing these integers sequentially is now the limiter.&lt;/p&gt;
    &lt;p&gt;A working solution is to change the algorithm slightly: instead of keeping a single sum of all integers, we keep two separate sums. One for the integers at even-numbered array positions, and one for the integers at odd-numberd positions. Then we need to sum those two values at the end. This is the algorithm:&lt;/p&gt;
    &lt;quote&gt;loop: rCurInt = load64(rCurPtr); // LoadEven rSumEven = rSumEven + rCurInt; // SumEven rCurInt = load64(rCurPtr + 8); // LoadOdd rSumOdd = rSumOdd + rCurInt; // SumOdd rCurPtr = rCurPtr + 16; // Advance if rCurPtr != rEndPtr goto loop; // Done? rSum = rSumEven + rSumOdd; // FinalSum&lt;/quote&gt;
    &lt;p&gt;And the dataflow graph for the loop kernel looks as follows:&lt;/p&gt;
    &lt;p&gt;Where before all the summing was in what’s called the same dependency chain (the name should be self-explanatory by now, I hope), we have now split the summation into two dependency chains. And this is enough to make a sufficiently-wide machine that can sustain 6 instructions per cycle complete our integer-summing task in just slightly more than half a cycle per integer being summed. Progress!&lt;/p&gt;
    &lt;p&gt;On a somewhat narrower 4-wide design, we are now throughput-bound, and take around 6/4=1.5 cycles per two integers summed, or 0.75 cycles per integer. That’s still a good improvement from the 1 cycle per integer we would have gotten on the same machine from the non-unrolled version; this gain is purely from reduction the loop overhead fraction, and further unrolling could reduce it even further. (That said, unless your loop really is as tiny as our example, you don’t generally want to go overboard with unrolling.)&lt;/p&gt;
    &lt;head rend="h3"&gt;Tying it all together&lt;/head&gt;
    &lt;p&gt;In the introduction, I talked about the need for a model detailed enough to make quantitative, not just qualitative, predictions; and at least for very simple compute-bound loops, that is exactly what we have now. At this point, you should know enough to look at the dependency structure of simple loops, and have some idea for how much (or how little) latent parallelism there is, and be able to compute a coarse upper bound on their “speed of light” on various machines with different peak instructions/cycle rates.&lt;/p&gt;
    &lt;p&gt;Of course, there are many simplifications here, most of which have been already noted in the text; we’re mostly ignoring the effects of the memory hierarchy, we’re not worrying at all about where the decoded instructions come from and how fast they can possibly be delivered, we’ve been flat-out assuming that our branch prediction oracle is perfect, and we’ve been pretending that while there may be a limit on the total number of instructions we can issue per cycle, it doesn’t matter what these instructions are. None of these are true. And even if we’re still compute-bound, we need to worry at least about that latter constraint: sometimes it can make a noticeable difference to tweak the “instruction mix” so it matches better what the hardware can actually do in a given clock cycle.&lt;/p&gt;
    &lt;p&gt;But all these caveats aside, the basic concepts introduced here are very general, and even just sketching out the dependency graph of a loop like this and seeing it in front of you should give you useful ideas about what potential problems are and how you might address them. If you’re interested in performance optimization, it is definitely worth your time practicing this so you can look at loops and get a “feel” for how they execute, and how the shape of your algorithm (or your data structures, in the linked list case) aids or constrains the compiler and processor.&lt;/p&gt;
    &lt;p&gt;UPDATE: Some additional clarifications in answer to some questions: paraphrasing one, “if you have to first write C code, translate it to some pseudo-assembly, and then look at the graph, how can this possibly be a better process than just measuring the code in the first place?” Well, the trick here is that to measure anything, you actually need a working program. You don’t to draw a dataflow graph. For example, a common scenario is that there are many ways you could structure some task, and they all want their data structured differently. Actually implementing and testing multiple variants like this requires you to write a lot of plumbing to massage data from one format into another (all of which can be buggy). Drawing a graph can be done from a brief description of the inner loop alone, and you can leave out the parts that you don’t currently care about, or “dummy them out” by replacing them with a coarse approximation (“random work here, maybe 10 cycles latency?”). You only need to make these things precise when they become close to the critical path (or you’re throughput-bound).&lt;/p&gt;
    &lt;p&gt;The other thing I’ll say is that even though I’ve been talking about adding cycle estimates for compute-bound loops here, this technique works and is useful at pretty much any scale. It’s applicable in any system where work is started and then processed asynchronously, with the results arriving some time later. If you’re analyzing a tight, compute-bound loop, cycle-level granularity is the way to go. But you can zoom out and use the same technique to figure out how your decomposition of an algorithm into tasklets processed by a thread pool works out: do you actually have some meaningful overlap, or is there still one long serial dependency chain that dominates everything, and all you’re doing by splitting it into tasklets like that is adding overhead? Zooming out even further, it works to analyze RPCs you’re sending to a different machine, or queries to some database. Say you have a 30ms target response time, and each RPC takes about 2ms to return its results. In a system that takes 50 RPCs to produce a result, can you meet that deadline? The answer depends on how the dataflow between them looks. If they’re all in series, almost certainly not. If they’re in 5 “layers” that each fan out to 10 different machines then collect the results, you probably can. It certainly applies in project scheduling, and is one of the big reasons the “man-month” isn’t a very useful metric: adding manpower increases your available resources but does nothing to relax your dependencies. In fact, it often adds more of them, to bring new people up to speed. If the extra manpower ends up resulting in more work on the critical path towards finishing your project (for example to train new hires), then adding these extra people to the project made it finish later. And so forth. The point being, this is not just limited to cycle-by-cycle analysis, even though that’s the context I’ve been introducing it in. It’s far more general than that.&lt;/p&gt;
    &lt;p&gt;And I think that’s enough material for today. Next up, I’ll continue my “Reading bits in far too many ways” series with the third part, where I’ll be using these techniques to get some insight into what kind of difference the algorithm variants make. Until then!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45552590</guid><pubDate>Sat, 11 Oct 2025 20:52:21 +0000</pubDate></item><item><title>Three ways formally verified code can go wrong in practice</title><link>https://buttondown.com/hillelwayne/archive/three-ways-formally-verified-code-can-go-wrong-in/</link><description>&lt;doc fingerprint="2623409772eabd7c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Three ways formally verified code can go wrong in practice&lt;/head&gt;
    &lt;head rend="h2"&gt;"Correct" doesn't mean "correct" when correctly using "correct"&lt;/head&gt;
    &lt;head rend="h3"&gt;New Logic for Programmers Release!&lt;/head&gt;
    &lt;p&gt;v0.12 is now available! This should be the last major content release. The next few months are going to be technical review, copyediting and polishing, with a hopeful 1.0 release in March. Full release notes here.&lt;/p&gt;
    &lt;head rend="h1"&gt;Three ways formally verified code can go wrong in practice&lt;/head&gt;
    &lt;p&gt;I run this small project called Let's Prove Leftpad, where people submit formally verified proofs of the eponymous meme. Recently I read Breaking “provably correct” Leftpad, which argued that most (if not all) of the provably correct leftpads have bugs! The lean proof, for example, should render &lt;code&gt;leftpad('-', 9, אֳֽ֑)&lt;/code&gt; as &lt;code&gt;---------אֳֽ֑&lt;/code&gt;, but actually does &lt;code&gt;------אֳֽ֑&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;You can read the article for a good explanation of why this goes wrong (Unicode). The actual problem is that correct can mean two different things, and this leads to confusion about how much formal methods can actually guarantee us. So I see this as a great opportunity to talk about the nature of proof, correctness, and how "correct" code can still have bugs.&lt;/p&gt;
    &lt;head rend="h2"&gt;What we talk about when we talk about correctness&lt;/head&gt;
    &lt;p&gt;In most of the real world, correct means "no bugs". Except "bugs" isn't a very clear category. A bug is anything that causes someone to say "this isn't working right, there's a bug." Being too slow is a bug, a typo is a bug, etc. "correct" is a little fuzzy.&lt;/p&gt;
    &lt;p&gt;In formal methods, "correct" has a very specific and precise meaning: the code conforms to a specification (or "spec"). The spec is a higher-level description of what is supposed the code's properties, usually something we can't just directly implement. Let's look at the most popular kind of proven specification:&lt;/p&gt;
    &lt;code&gt;-- Haskell
inc :: Int -&amp;amp;gt; Int
inc x = x + 1
&lt;/code&gt;
    &lt;p&gt;The type signature &lt;code&gt;Int -&amp;gt; Int&lt;/code&gt; is a specification! It corresponds to the logical statement &lt;code&gt;all x in Int: inc(x) in Int&lt;/code&gt;. The Haskell type checker can automatically verify this for us. It cannot, however, verify properties like &lt;code&gt;all x in Int: inc(x) &amp;gt; x&lt;/code&gt;. Formal verification is concerned with verifying arbitrary properties beyond what is (easily) automatically verifiable. Most often, this takes the form of proof. A human manually writes a proof that the code conforms to its specification, and the prover checks that the proof is correct.&lt;/p&gt;
    &lt;p&gt;Even if we have a proof of "correctness", though, there's a few different ways the code can still have bugs.&lt;/p&gt;
    &lt;head rend="h3"&gt;1. The proof is invalid&lt;/head&gt;
    &lt;p&gt;For some reason the proof doesn't actually show the code matches the specification. This is pretty common in pencil-and-paper verification, where the proof is checked by someone saying "yep looks good to me". It's much rarer when doing formal verification but it can still happen in a couple of specific cases:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The theorem prover itself has a bug (in the code or introduced in the compiled binary) that makes it accept an incorrect proof. This is something people are really concerned about but it's so much rarer than every other way verified code goes wrong, so is only included for completeness.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;For convenience, most provers and FM languages have an "just accept this statement is true" feature. This helps you work on the big picture proof and fill in the details later. If you leave in a shortcut, and the compiler is configured to allow code-with-proof-assumptions to compile, then you can compile incorrect code that "passes the proof checker". You really should know better, though.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;2. The properties are wrong&lt;/head&gt;
    &lt;p&gt;This code is provably correct:&lt;/p&gt;
    &lt;code&gt;inc :: Int -&amp;amp;gt; Int
inc x = x-1
&lt;/code&gt;
    &lt;p&gt;The only specification I've given is the type signature &lt;code&gt;Int -&amp;gt; Int&lt;/code&gt;. At no point did I put the property &lt;code&gt;inc(x) &amp;gt; x&lt;/code&gt; in my specification, so it doesn't matter that it doesn't hold, the code is still "correct".&lt;/p&gt;
    &lt;p&gt;This is what "went wrong" with the leftpad proofs. They do not prove the property "&lt;code&gt;leftpad(c, n, s)&lt;/code&gt; will take up either &lt;code&gt;n&lt;/code&gt; spaces on the screen or however many characters &lt;code&gt;s&lt;/code&gt; takes up (if more than &lt;code&gt;n&lt;/code&gt;)". They prove the weaker property "&lt;code&gt;len(leftpad(c, n, s)) == max(n, len(s))&lt;/code&gt;, for however you want to define &lt;code&gt;len(string)&lt;/code&gt;". The second is a rough proxy for the first that works in most cases, but if someone really needs the former property they are liable to experience a bug.&lt;/p&gt;
    &lt;p&gt;Why don't we prove the stronger property? Sometimes it's because the code is meant to be used one way and people want to use it another way. This can lead to accusations that the developer is "misusing the provably correct code" but this should more often be seen as the verification expert failing to educate devs on was actually "proven".&lt;/p&gt;
    &lt;p&gt;Sometimes it's because the property is too hard to prove. "Outputs are visually aligned" is a proof about Unicode inputs, and the core Unicode specification is 1,243 pages long.&lt;/p&gt;
    &lt;p&gt;Sometimes it's because the property we want is too hard to express. How do you mathematically represent "people will perceive the output as being visually aligned"? Is it OS and font dependent? These two lines are exactly five characters but not visually aligned:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;|||||&lt;/p&gt;
      &lt;p&gt;MMMMM&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Or maybe they are aligned for you! I don't know, lots of people read email in a monospace font. "We can't express the property" comes up a lot when dealing with human/business concepts as opposed to mathematical/computational ones.&lt;/p&gt;
    &lt;p&gt;Finally, there's just the possibility of a brain fart. All of the proofs in Nearly All Binary Searches and Mergesorts are Broken are like this. They (informally) proved the correctness of binary search with unbound integers, forgetting that many programming languages use machine integers, where a large enough sum can overflow.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. The assumptions are wrong&lt;/head&gt;
    &lt;p&gt;This is arguably the most important and most subtle source of bugs. Most properties we prove aren't "&lt;code&gt;X&lt;/code&gt; is always true". They are "assuming &lt;code&gt;Y&lt;/code&gt; is true, &lt;code&gt;X&lt;/code&gt; is also true". Then if &lt;code&gt;Y&lt;/code&gt; is not true, the proof no longer guarantees &lt;code&gt;X&lt;/code&gt;. A good example of this is binary &lt;del&gt;sort&lt;/del&gt; search, which only correctly finds elements assuming the input list is sorted. If the list is not sorted, it will not work correctly.&lt;/p&gt;
    &lt;p&gt;Formal verification adds two more wrinkles. One: sometimes we need assumptions to make the property valid, but we can also add them to make the proof easier. So the code can be bug-free even if the assumptions used to verify it no longer hold! Even if a leftpad implements visual alignment for all Unicode glyphs, it will be a lot easier to prove visual alignment for just ASCII strings and padding.&lt;/p&gt;
    &lt;p&gt;Two: we need make a lot of environmental assumptions that are outside our control. Does the algorithm return output or use the stack? Need to assume that there's sufficient memory to store stuff. Does it use any variables? Need to assume nothing is concurrently modifying them. Does it use an external service? Need to assume the vendor doesn't change the API or response formats. You need to assume the compiler worked correctly, the hardware isn't faulty, and the OS doesn't mess with things, etc. Any of these could change well after the code is proven and deployed, meaning formal verification can't be a one-and-done thing.&lt;/p&gt;
    &lt;p&gt;You don't actually have to assume most of these, but each assumption drop makes the proof harder and the properties you can prove more restricted. Remember, the code might still be bug-free even if the environmental assumptions change, so there's a tradeoff in time spent proving vs doing other useful work.&lt;/p&gt;
    &lt;p&gt;Another common source of "assumptions" is when verified code depends on unverified code. The Rust compiler can prove that safe code doesn't have a memory bug assuming unsafe code does not have one either, but depends on the human to confirm that assumption. Liquid Haskell is verifiable but can also call regular Haskell libraries, which are unverified. We need to assume that code is correct (in the "conforms to spec") sense, and if it's not, our proof can be "correct" and still cause bugs.&lt;/p&gt;
    &lt;p&gt;These boundaries are fuzzy. I wrote that the "binary search" bug happened because they proved the wrong property, but you can just as well argue that it was a broken assumption (that integers could not overflow). What really matters is having a clear understanding of what "this code is proven correct" actually tells you. Where can you use it safely? When should you worry? How do you communicate all of this to your teammates?&lt;/p&gt;
    &lt;p&gt;Good lord it's already Friday&lt;/p&gt;
    &lt;p&gt;If you're reading this on the web, you can subscribe here. Updates are once a week. My main website is here.&lt;/p&gt;
    &lt;p&gt;My new book, Logic for Programmers, is now in early access! Get it here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45555727</guid><pubDate>Sun, 12 Oct 2025 06:17:52 +0000</pubDate></item><item><title>Show HN: I built a simple ambient sound app with no ads or subscriptions</title><link>https://ambisounds.app/</link><description>&lt;doc fingerprint="b7ff85d8f21c8a02"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Ambi&lt;/head&gt;
    &lt;head rend="h1"&gt;Ambi&lt;/head&gt;
    &lt;head rend="h1"&gt;Ambi&lt;/head&gt;
    &lt;head rend="h2"&gt;Relax, focus and sleep with ambient soundscapes&lt;/head&gt;
    &lt;head rend="h2"&gt;Relax, focus and sleep with &lt;lb/&gt;ambient soundscapes&lt;/head&gt;
    &lt;head rend="h2"&gt;Relax, focus and sleep with &lt;lb/&gt;ambient soundscapes&lt;/head&gt;
    &lt;head rend="h1"&gt;Mix your perfect soundscape&lt;/head&gt;
    &lt;head rend="h1"&gt;Mix your perfect soundscape&lt;/head&gt;
    &lt;head rend="h1"&gt;Mix your perfect soundscape&lt;/head&gt;
    &lt;head rend="h2"&gt;Blend multiple sounds together with volume control for each individual sound&lt;/head&gt;
    &lt;head rend="h2"&gt;Blend multiple sounds together with volume control for each individual sound&lt;/head&gt;
    &lt;head rend="h2"&gt;Blend multiple sounds together with volume control for each individual sound&lt;/head&gt;
    &lt;head rend="h1"&gt;Play for as long &lt;lb/&gt;as you like&lt;/head&gt;
    &lt;head rend="h1"&gt;Play for as long &lt;lb/&gt;as you like&lt;/head&gt;
    &lt;head rend="h1"&gt;Play for as long &lt;lb/&gt;as you like&lt;/head&gt;
    &lt;p&gt;Set a timer or let your mix play through the night&lt;/p&gt;
    &lt;p&gt;Set a timer or let your mix play through the night&lt;/p&gt;
    &lt;p&gt;Set a timer or let your mix play through the night&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45558611</guid><pubDate>Sun, 12 Oct 2025 14:49:50 +0000</pubDate></item><item><title>Oavif: Faster target quality image compression</title><link>https://giannirosato.com/blog/post/oavif/</link><description>&lt;doc fingerprint="15c0199215921a8a"&gt;
  &lt;main&gt;
    &lt;p&gt;oavif is a new approach to target quality encoding in image compression, designed around smarter convergence strategies and quicker scoring to be as fast as possible.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why?&lt;/head&gt;
    &lt;p&gt;Target quality encoding is one of the highest impact use cases for image compression. A target quality encoder framework aims to produce an image encoded at a particular quality set by the user according to some metric or visual quality index. This kind of encoder framework is useful for a variety of users, ranging from small website owners to content delivery networks pushing vast quantities of image data through the Web.&lt;/p&gt;
    &lt;p&gt;The value of target quality encoding is perceptual consistency. If I rely entirely on my encoder's internal quality index (often set with a "q" parameter), I may not get outputs of perfectly consistent quality when using the same "q" across different images. Relying on a metric that represents the viewer's experience is the solution to this; targeting a representative score within this metric will ensure you always receive an optimally encoded file that never undershoots and ruins image quality, and never overshoots and wastes data.&lt;/p&gt;
    &lt;p&gt;oavif is a tool to do target quality encoding extremely quickly. There are three core components to a target quality encoding framework: the metric, the encoder, and the convergence algorithm. oavif aims to leverage or improve the state of the art in all three categories.&lt;/p&gt;
    &lt;p&gt;I think this use case has been neglected because it sits in an awkward spot, stuck between encoder development and content deployment. Considering image encoders and powerful metrics are fast, it is easy to take them for granted and build inefficient frameworks around them. Slow frameworks waste valuable resources; processing images is expensive. I built oavif with the same approach I've adopted when building encoders, where every CPU cycle counts.&lt;/p&gt;
    &lt;head rend="h2"&gt;Metric&lt;/head&gt;
    &lt;p&gt;There are good metrics and bad metrics in the context of what humans care about in images. PSNR is a bad metric; targeting a PSNR score doesn't mean anything to users, because images at the same PSNR could look completely different. However, PSNR is very fast, and faster metrics lend themselves more favorably to target quality encoding.&lt;/p&gt;
    &lt;p&gt;SSIMULACRA2 correlates highly with subjective human quality ratings, but it is comparatively slow compared to simpler metrics. I set out to remedy this with fssimu2, a faster implementation that uses almost 40% less memory. This is what oavif uses, and it makes computing the in-loop metric much faster compared to the reference library.&lt;/p&gt;
    &lt;p&gt;Testing on a 4k test image against a distorted sample for an average time to score across 8 runs, Butteraugli (a perceptual metric from the libjxl project) took 2455ms, while the reference SSIMULACRA2 implementation took 1162ms. fssimu2 takes 631.9ms. Testing was done on my M2 MacBook Air using &lt;code&gt;hyperfine&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Encoder&lt;/head&gt;
    &lt;p&gt;AVIF is a capable Web image format. oavif uses libaom (via libavif) because it is the best open-source image encoder available relative to its speed. I worked on improving AVIF encoding in 2024 during my work on SVT-AV1-PSY. Google (with help from Julio Barba) later adopted this work and advanced it further in libaom. It is now used by some websites you may know, such as The Guardian.&lt;/p&gt;
    &lt;p&gt;Aside from speed, encoder consistency is valuable in the context of target quality encoding (I'll explain more about why later). In fact, a perfectly consistent encoder would be able to eliminate the need for targeting entirely since the encoder's user-configurable Q would map perfectly to some perceptual index. libaom has had engineering effort go into encoder consistency, which is a valuable thing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Convergence&lt;/head&gt;
    &lt;p&gt;A simple convergence loop looks like this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Decode input, pass to encoder&lt;/item&gt;
      &lt;item&gt;Decode encoder output &amp;amp; compare to input with metric&lt;/item&gt;
      &lt;item&gt;If we hit the target metric score, finish; otherwise, repeat prev. step with modified settings&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The most important part here is how we decide to modify our settings. This is the convergence algorithm that allows us to search for the best encoder Q. The easiest way to do this is with binary search, and some more recent implementations have utilized clever interpolation using past data to inform the next guess based on the fact that we know encoder Q and target score are likely correlated.&lt;/p&gt;
    &lt;p&gt;oavif takes inspiration from both of these, adding predictive modeling alongside error-informed search space correction to improve search times significantly. To understand why, we'll walk through each stage of the implementation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Binary Search&lt;/head&gt;
    &lt;p&gt;This testing was done using the Daala subset2 image dataset. Importantly, I only used this dataset for validation; oavif was not designed around this specific dataset in any way. When testing, the oavif configuration was left at defaults; only the convergence implementation was modified. The threshold for meeting the target score is ±2.0 in oavif by default, and the default target score is 80.0 as measured by fssimu2 because it is a reasonable "high fidelity" target.&lt;/p&gt;
    &lt;p&gt;Everyone with some algorithms background will start with binary search. Set your bounds for encoder Q to 0..100, and divide the range in half each time you test. In oavif, a pure binary search implementation at default settings yields the following results:&lt;/p&gt;
    &lt;code&gt;Average encoding time: 467.95 ms ± 94.64
Average passes: 3.20 ± 0.45 (max: 4 min: 2)
&lt;/code&gt;
    &lt;head rend="h3"&gt;Interpolation&lt;/head&gt;
    &lt;p&gt;Interpolation-based target quality searches by iteratively probing, measuring, and narrowing the search interval just like binary search. The difference is that it tries to model the score-vs-quantizer curve with interpolation (linear, quadratic, etc) as more data is accumulated. This should theoretically reduce the number of necessary encodes, and can start with standard binary search when there is not enough data to interpolate with.&lt;/p&gt;
    &lt;p&gt;Metric score vs encoder Q is generally (though not perfectly) a mostly monotonic curve. Interpolation-based inverse estimation uses the measured points to approximate that curve and solve for the quantizer that would produce the target score. Higher-order methods use more shape information and are thus theoretically more accurate. Adding linear and quadratic interpolation support to oavif, we see a small reduction in the average number of passes on subset2:&lt;/p&gt;
    &lt;code&gt;Average encoding time: 468.98 ms ± 97.28
Average passes: 3.12 ± 0.39 (max: 4 min: 2)
&lt;/code&gt;
    &lt;p&gt;This is a 2.5% improvement. We still need a minimum of two passes to accurately target.&lt;/p&gt;
    &lt;head rend="h3"&gt;Predictive Modeling&lt;/head&gt;
    &lt;p&gt;This feature uses an exponential curve trained on the gb82 image dataset with libaom (at speed 9, 10-bit, 4:4:4 chroma). The curve looks like this:&lt;/p&gt;
    &lt;p&gt;Based on this, we can write some very simple code to predict a Q value from the target score:&lt;/p&gt;
    &lt;code&gt;fn predictQFromScore(tgt: f64) u32 {
    const q = 6.83 * @exp(0.0282 * tgt);
    return @intFromFloat(@min(100.0, @round(q)));
}
&lt;/code&gt;
    &lt;p&gt;This yields the biggest average improvement in this testing so far, decreasing average pass count by 56.4% versus interpolation search and 57.5% versus binary search.&lt;/p&gt;
    &lt;code&gt;Average encoding time: 218.33 ms ± 114.16
Average passes: 1.36 ± 0.78 (max: 3 min: 1)
&lt;/code&gt;
    &lt;p&gt;The gb82 image set is fairly low-resolution mixed photographic content, while Daala subset2 is medium-resolution photographic content with less variation. The fact that the model generalizes so well is exciting.&lt;/p&gt;
    &lt;p&gt;It is at this stage that encoder consistency becomes important. A more consistent encoder will diverge from our model's predictions less frequently, and theoretically result in a faster target quality loop.&lt;/p&gt;
    &lt;head rend="h3"&gt;Error Bounds&lt;/head&gt;
    &lt;p&gt;Because our initial predictions tend to be so accurate, we can use them to aggressively narrow our search space without incurring too much risk of a search space collapse.&lt;/p&gt;
    &lt;p&gt;The basis of this is that utilizing plain binary search with prediction is often unreliable. Let's say we would like to target score=80, and our model predicts we need Q=65. We score 82.38. Now we are forced to search (0..65), which is worse than if we had just avoided prediction in the first place (our search space would be 50..100 in that case). This is in spite of the fact that our prediction was very close to the target.&lt;/p&gt;
    &lt;p&gt;oavif uses the distance from the target to its advantage:&lt;/p&gt;
    &lt;code&gt;const abs_err = @abs(e.t.score - o.score_tgt);
if (pass == 0) {
    const err_bound: u32 = @intFromFloat(@ceil(abs_err) * 4.0);
    if (e.t.score - o.score_tgt &amp;gt; 0) {
        hi_bound = e.q;
        lo_bound = if (e.q &amp;gt; err_bound) e.q - err_bound else 0;
    } else {
        lo_bound = e.q;
        hi_bound = @min(100, e.q + err_bound);
    }
}
&lt;/code&gt;
    &lt;p&gt;In this case, the error was 2.38; &lt;code&gt;@ceil()&lt;/code&gt; brings this to 3, and we multiply by 4 because the midpoint of the new range tends to be very close to the target value based on my testing. The performance improves in kind:&lt;/p&gt;
    &lt;code&gt;Average encoding time: 194.50 ms ± 69.89
Average passes: 1.18 ± 0.39 (max: 2 min: 1)
&lt;/code&gt;
    &lt;p&gt;This costs 13.2% fewer passes than interpolation + prediction, and 63.1% fewer passes than binary search.&lt;/p&gt;
    &lt;p&gt;The minimum number of passes necessary in a naive binary search or interpolation-informed search is now the maximum number of passes we need to converge on the target on Daala subset2. You'll also notice the standard deviation went down due to the fact that the ceiling has been lowered.&lt;/p&gt;
    &lt;head rend="h2"&gt;Architecture&lt;/head&gt;
    &lt;p&gt;We've made it work and we've made it good, so now we can make it fast. oavif is written in Zig, and uses available high-performance C decoder libraries for handling inputs and decoding AVIF in the convergence loop. All image I/O during convergence is done in memory, and a buffer is kept of our latest encode to write to a file if we meet the target in the search space.&lt;/p&gt;
    &lt;p&gt;Efforts have gone into making oavif comparable to libavif's &lt;code&gt;avifenc&lt;/code&gt; in terms of features as well. It supports high bit depth I/O, ICC profile handling for most formats, user-configurable encoder settings, and better defaults (until tune=iq becomes the libaom default in libavif).&lt;/p&gt;
    &lt;head rend="h2"&gt;Future Directions&lt;/head&gt;
    &lt;p&gt;Architecturally, it would be trivial to keep a history of buffers active and always pick from the history, even if our loop doesn't converge on the target. I opted to avoid this for now because it dramatically increases memory usage, but if I receive widespread feedback that memory is unimportant I'll consider an implementation. In its current state, we hit the in-loop buffer the vast majority of the time anyway.&lt;/p&gt;
    &lt;p&gt;I think the future of this kind of workflow is far more accurate predictive modeling. I believe it is possible to improve what I've done if we provide details about the source image as another term in the equation (like variance or entropy) and train our prediction mechanism on this additional data. I'm optimistically convinced this could result in a very high success rate for one-shot targeting.&lt;/p&gt;
    &lt;p&gt;I'm looking forward to seeing more target quality workflows taking advantage of smarter targeting. If you've made it this far, thanks for reading, and enjoy oavif!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45559372</guid><pubDate>Sun, 12 Oct 2025 16:21:59 +0000</pubDate></item><item><title>A years-long Turkish alphabet bug in the Kotlin compiler</title><link>https://sam-cooper.medium.com/the-country-that-broke-kotlin-84bdd0afb237</link><description>&lt;doc fingerprint="9f00b10bcb670219"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Country That Broke Kotlin&lt;/head&gt;
    &lt;head rend="h2"&gt;Logic vs language: how a Turkish alphabet bug played a years-long game of hide-and-seek inside the Kotlin compiler&lt;/head&gt;
    &lt;p&gt;When Turkish software engineer Mehmet Nuri Öztürk posted a short message on the Kotlin discussion forum in March of 2016, he had no idea he was reporting a dangerous standard library bug that would take five years to find and fix. All he knew was that his build didn’t work.&lt;/p&gt;
    &lt;p&gt;Kotlin 1.0 had been released to the world only a month earlier, promising to breathe much-needed fresh life into the twin worlds of Java and Android development. But for Mehmet Nuri, the new programming language was a frustrating dead end. His code simply wouldn’t build, and the compiler’s output gave him nothing to go on.&lt;/p&gt;
    &lt;p&gt;He pasted the impenetrable error into his forum post:&lt;/p&gt;
    &lt;code&gt;Compilation completed with 2 errors and 0 warnings in 10s 126ms&lt;lb/&gt;&lt;lb/&gt;Error:Kotlin: Unknown compiler message tag: INFO&lt;lb/&gt;Error:Kotlin: Unknown compiler message tag: LOGGING&lt;/code&gt;
    &lt;p&gt;The Kotlin team replied quickly, but they didn’t have much to go on either. “Did you see this error just once, or do you see it every time you compile your project?”&lt;/p&gt;
    &lt;p&gt;It was consistent, Mehmet Nuri replied, not just between builds but also across different machines and operating systems.&lt;/p&gt;
    &lt;p&gt;It was a full five months before the breakthrough discovery. Muhammed Demirbaş, another programmer working in Turkey, had been running into the same mysterious build failure message, and had started to do some investigation of his own.&lt;/p&gt;
    &lt;p&gt;“I suspect that the source of the error may be my locale or language,” wrote Muhammed, commenting on Mehmet Nuri’s post. Muhammed even pinpointed the exact line of code where he thought the problem might be. “Apparently this is a uppercase–lowercase Turkish &lt;code&gt;I&lt;/code&gt; problem in the &lt;code&gt;CompilerOutputParser.CATEGORIES&lt;/code&gt; map: &lt;code&gt;I -&amp;gt; ı&lt;/code&gt;, &lt;code&gt;İ -&amp;gt; i&lt;/code&gt;.”&lt;/p&gt;
    &lt;p&gt;This was proof that Mehmet Nuri’s problems weren’t confined to his particular project, but were a symptom of something more serious going on in the compiler itself. The Kotlin team, grateful for Muhammed’s new information, filed an issue report with a link to the forum post in their YouTrack bug tracker:&lt;/p&gt;
    &lt;p&gt;“Compilation fails on Turkish locale because of locale-sensitive uppercasing.” (KT-13631)&lt;/p&gt;
    &lt;p&gt;Muhammed Demirbaş couldn’t have been more spot on in his investigation and assessment of the compiler bug. Since Kotlin is open source, he was able to search the compiler’s code for the exact line of code where that “Unknown compiler message tag” string appears:&lt;/p&gt;
    &lt;code&gt;val qNameLowerCase = qName.toLowerCase()&lt;lb/&gt;var category: CompilerMessageSeverity? = CATEGORIES[qNameLowerCase]&lt;lb/&gt;if (category == null) {&lt;lb/&gt;    messageCollector.report(ERROR, "Unknown compiler message tag: $qName")&lt;lb/&gt;    category = INFO&lt;lb/&gt;}&lt;/code&gt;
    &lt;p&gt;So what does this code do, and why does it sometimes go wrong?&lt;/p&gt;
    &lt;p&gt;The code is part of a class named &lt;code&gt;CompilerOutputParser&lt;/code&gt;, and is responsible for reading XML files containing messages from the Kotlin compiler. Those files look something like this:&lt;/p&gt;
    &lt;code&gt;&amp;lt;MESSAGES&amp;gt;&lt;lb/&gt;  &amp;lt;INFO path="src/main/Kotlin/Example.kt" line="1" column="1"&amp;gt;&lt;lb/&gt;    This is a message from the compiler about a line of code.&lt;lb/&gt;  &amp;lt;/INFO&amp;gt;&lt;lb/&gt;&amp;lt;/MESSAGES&amp;gt;&lt;/code&gt;
    &lt;p&gt;At the time, the tags in this file were named in all-caps: &lt;code&gt;&amp;lt;INFO/&amp;gt;&lt;/code&gt;,&lt;code&gt;&amp;lt;ERROR/&amp;gt;&lt;/code&gt;, and so on (source: GitHub), like the HTML 1.0 webpages your grandpa used to write.&lt;/p&gt;
    &lt;p&gt;In the Kotlin code we just saw, &lt;code&gt;qName&lt;/code&gt; is the name of an XML tag that we’re parsing from this file. If we’re looking at an &lt;code&gt;&amp;lt;INFO/&amp;gt;&lt;/code&gt; tag, the &lt;code&gt;qName&lt;/code&gt; is “INFO.”&lt;/p&gt;
    &lt;p&gt;To determine what the message means, the &lt;code&gt;CompilerOutputParser&lt;/code&gt; next looks up that string in its &lt;code&gt;CATEGORIES&lt;/code&gt; map to find its corresponding &lt;code&gt;CompilerMessageSeverity&lt;/code&gt; enum entry. But wait: the keys in the &lt;code&gt;CATEGORIES&lt;/code&gt; map are lower case! (source: GitHub)&lt;/p&gt;
    &lt;code&gt;val categories = mapOf(&lt;lb/&gt;    "error" to CompilerMessageSeverity.ERROR,&lt;lb/&gt;    "info" to CompilerMessageSeverity.INFO,&lt;lb/&gt;    …&lt;lb/&gt;)&lt;/code&gt;
    &lt;p&gt;Instead of searching for “INFO,” we need to search for “info.” That’s why the code we looked at calls &lt;code&gt;qName.toLowerCase()&lt;/code&gt; before looking it up in the &lt;code&gt;CATEGORIES&lt;/code&gt; map. Here’s the code again, or at least the relevant lines:&lt;/p&gt;
    &lt;code&gt;val qNameLowerCase = qName.toLowerCase()&lt;lb/&gt;var category: CompilerMessageSeverity? = CATEGORIES[qNameLowerCase]&lt;/code&gt;
    &lt;p&gt;And that’s where the bug sneaks in.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If your computer is configured in English, &lt;code&gt;"INFO".toLowerCase()&lt;/code&gt;is&lt;code&gt;"info"&lt;/code&gt;, just like we wanted.&lt;/item&gt;
      &lt;item&gt;But if your computer is configured in Turkish, &lt;code&gt;"INFO".toLowerCase()&lt;/code&gt;turns out to be&lt;code&gt;"ınfo"&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Notice the difference? In the Turkish version, the lower case letter ‘ı’ has no dot above it.&lt;/p&gt;
    &lt;p&gt;The tiny discrepancy might be hard for a human to spot, but to a computer, these are two completely different strings. The dotless &lt;code&gt;"ınfo"&lt;/code&gt; string isn’t one of the keys in &lt;code&gt;CATEGORIES&lt;/code&gt; map, so the code fails to find the correct &lt;code&gt;CompilerMessageSeverity&lt;/code&gt; for our &lt;code&gt;&amp;lt;INFO/&amp;gt;&lt;/code&gt; tag, and complains that “INFO” must be a completely unknown category of message.&lt;/p&gt;
    &lt;p&gt;So why does calling &lt;code&gt;toLowerCase()&lt;/code&gt; on a Turkish computer produce this strange result?&lt;/p&gt;
    &lt;p&gt;Muhammed already provided part of the answer in his reply to Mehmet Nuri’s forum post. Turkic languages have two versions of the letter ‘i’:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;an ‘i’ with a dot, as in the word insan (human),&lt;/item&gt;
      &lt;item&gt;and a separate ‘ı’ without a dot, as in the word ırmak (river).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What’s more, the dotted/dotless distinction is also preserved in the upper case letters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;capital ‘i’ is ‘İ’, as in insan → İnsan,&lt;/item&gt;
      &lt;item&gt;and capital ‘ı’ is ‘I’, as in ırmak → Irmak.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That uppercase dotless ‘I’ is the same one we use in English. As a result, the single Unicode character &lt;code&gt;I&lt;/code&gt; (U+0049) has two different lower case forms: dotted &lt;code&gt;i&lt;/code&gt; (U+0069) in English, and dotless &lt;code&gt;ı&lt;/code&gt; (U+0131) in Turkish.&lt;/p&gt;
    &lt;p&gt;For Kotlin’s &lt;code&gt;toLowerCase()&lt;/code&gt; function, that’s a problem! When &lt;code&gt;toLowerCase()&lt;/code&gt; sees an &lt;code&gt;I&lt;/code&gt; character, which lower case form should it use? The lower case form of the Turkish word IRMAK should be ırmak, with no dot. But the lower case form of the English word INFO, which starts with exactly the same character, should be info, with a dot.&lt;/p&gt;
    &lt;p&gt;When you ask your computer to convert text to lower case, you should technically also specify the alphabet rules to use—English, Turkish, or something else entirely. But that’s a lot of hard work, so if you don’t specify, many systems — including, in those days, Kotlin’s &lt;code&gt;toLowerCase()&lt;/code&gt; function — will just use the language settings you chose when you set up your computer. That’s why &lt;code&gt;"INFO".toLowerCase()&lt;/code&gt; is &lt;code&gt;"ınfo"&lt;/code&gt; when you run it on a Turkish machine, and that’s why IntelliJ installations in Turkey couldn’t match the Kotlin compiler’s &lt;code&gt;&amp;lt;INFO/&amp;gt;&lt;/code&gt; messages to the lowercase &lt;code&gt;"info"&lt;/code&gt; string they were expecting to see.&lt;/p&gt;
    &lt;p&gt;But in 2016, all of that was still just a bug ticket waiting to be worked on. Muhammed Demirbaş had identified the right place to start the search, but the YouTrack issue linked to his findings was just one of hundreds of tickets in the Kotlin project backlog. With only a tiny number of people reporting that they were affected by the bug, a more thorough investigation was never a priority.&lt;/p&gt;
    &lt;p&gt;That would all change with the release of coroutines two years later, when the unassuming little bug wormed its way even deeper into the foundations of the Kotlin compiler.&lt;/p&gt;
    &lt;p&gt;October 2018 saw the release of Kotlin 1.3 — and with it, the first stable version of the new coroutines library, an innovative approach to asynchronous programming that promised to transform the Android app development experience. Coroutines had been in prerelease testing for over a year, and now that they were deemed ready for production use, Kotlin programmers of all kinds were ready to embrace them with enthusiasm.&lt;/p&gt;
    &lt;p&gt;To get the new tools, developers needed to upgrade their copy of the coroutines library from the prerelease 0.30.x version to the stable 1.0 release, at the same time as upgrading the Kotlin language and standard library to version 1.3.&lt;/p&gt;
    &lt;p&gt;Have you ever upgraded a dependency in a Kotlin or Java project? If so, you’ll know that making sure all your libraries remain compatible with one another can be a delicate juggling act. If your code references a function that’s been removed or changed in the new dependency, you’ll get a compilation error. But if the newly broken reference comes not from your own code but from another library, you won’t find out until you run the program. That’s because the code inside the library has already been compiled by its author, and isn’t being compiled or checked as part of your own build.&lt;/p&gt;
    &lt;p&gt;When one of your libraries tries to call a function that doesn’t exactly match what’s on offer in your freshly upgraded project’s new classpath, you’ll get a &lt;code&gt;NoSuchMethodError&lt;/code&gt;. As a result, when you’re upgrading dependencies—and especially if you’re upgrading several dependencies at once—the occasional &lt;code&gt;NoSuchMethodError&lt;/code&gt; is pretty much par for the course, until you figure out exactly which versions of each library are compatible with one another.&lt;/p&gt;
    &lt;p&gt;So when Kemal Atlı, an Android developer based in Turkey, ran into a &lt;code&gt;NoSuchMethodError&lt;/code&gt; while upgrading his app to use the shiny new coroutines library, it looked for all the world like just another dependency version mismatch. Kemal wasn’t having any luck fixing this one, though. Unsure if it might be a bug with the coroutines library itself, he opened a GitHub issue, pasting the stack trace from his crashed app:&lt;/p&gt;
    &lt;code&gt;java.lang.NoSuchMethodError: &lt;lb/&gt;  No static method boxİnt(I)Ljava/lang/Integer;&lt;lb/&gt;  in class Lkotlin/coroutines/jvm/internal/Boxing;&lt;/code&gt;
    &lt;p&gt;This exception already contained the vital clue—a tiny dot above the upper case letter ‘İ’ in &lt;code&gt;boxİnt()&lt;/code&gt;—but who’s going to spot that if they’re not looking for it? For now, nobody did.&lt;/p&gt;
    &lt;p&gt;“Does restarting your IDE and running a &lt;code&gt;clean build&lt;/code&gt; resolve the issue?” responded the coroutines library maintainers, immediately suspecting a version conflict. Kemal had said that the issue only happened on one of his two machines, which suggested the problem might just be an old incompatible dependency version hanging around in his build cache after the upgrade.&lt;/p&gt;
    &lt;p&gt;A week later, another bug report, from another Turkish developer seeing their app crash with the same exception. By now, the coroutines library maintainers were certain the problem could only be caused by a dependency version mismatch—a conclusion which, in any other circumstances, might have been entirely valid. They had no luck reproducing the issue on their end, which just provided further evidence that the problem was specific to the way those two issue reporters had configured and built their projects.&lt;/p&gt;
    &lt;p&gt;It took a month for someone to spot the smoking gun.&lt;/p&gt;
    &lt;p&gt;“It has to be a locale problem,” wrote Erel Özçakırlar, another Turkish software engineer, commenting on Kemal’s issue report in late December 2018. Erel pointed out what everyone had missed so far: the real function should be called &lt;code&gt;boxInt()&lt;/code&gt;, but the stack trace showed &lt;code&gt;boxİnt()&lt;/code&gt;. It wasn’t a simple case of trying to call an older version of an existing function. Instead, Kotlin had seemingly used the Turkish alphabet to invent a function name that had never existed in the first place. What’s more, Erel found he could fix the problem by running the code on a computer that used English as its system locale.&lt;/p&gt;
    &lt;p&gt;“I think Erel might have a point regarding system language settings,” replied Kemal, saying he’d look into it further. But figuring out why Kotlin’s compiler internals are suddenly inventing imaginary functions using Turkish characters — well, where would you even start? There was little for Kemal to offer beyond the original bug report, so the issue kept its “waiting for clarification” label, and was closed — along with the second similar bug report — in early 2019.&lt;/p&gt;
    &lt;p&gt;Once again, the bug was back in hiding. But this time, it had sunk its claws deep into Kotlin’s compiler internals. The misspelled &lt;code&gt;boxİnt()&lt;/code&gt; function wasn’t being called in Kemal’s own code, or even in a library he was using. Instead, the mistake was being added to his app by the compiler itself.&lt;/p&gt;
    &lt;p&gt;To understand why, we need to talk a little about how coroutines work.&lt;/p&gt;
    &lt;p&gt;Much of Kotlin’s coroutine magic takes places in the dedicated kotlinx.coroutines library. But there’s one core building block that’s more tightly integrated with the Kotlin language and its compiler: the &lt;code&gt;suspend&lt;/code&gt; keyword.&lt;/p&gt;
    &lt;p&gt;When you label a function with the &lt;code&gt;suspend&lt;/code&gt; keyword, the Kotlin compiler rewrites the function signature to make the function work asynchronously. For example, when you write a suspending function with two parameters, the corresponding output generated by the Kotlin compiler will actually include three parameters. The invisible third parameter is a &lt;code&gt;Continuation&lt;/code&gt;, which both stores the state of the function and acts as a callback to receive the function’s asynchronous result.&lt;/p&gt;
    &lt;p&gt;A &lt;code&gt;Continuation&lt;/code&gt; stores and sends all kinds of values, depending on the code inside your suspending function—and that’s where the mysterious &lt;code&gt;boxInt()&lt;/code&gt; function comes into play.&lt;/p&gt;
    &lt;p&gt;You might know that when you create an &lt;code&gt;Int&lt;/code&gt; in Kotlin, it can be stored as one of two different underlying Java types: a primitive &lt;code&gt;int&lt;/code&gt;, or an &lt;code&gt;Integer&lt;/code&gt; object. Kotlin makes the choice for you automatically, depending on how the value is used.&lt;/p&gt;
    &lt;p&gt;If you use an &lt;code&gt;Int&lt;/code&gt; in a coroutine, Kotlin will sometimes need to convert it from the primitive &lt;code&gt;int&lt;/code&gt; storage mechanism to an &lt;code&gt;Integer&lt;/code&gt; object, so that the value can pass through the generic coroutine continuation machinery. This conversion is called boxing. Java will happily perform the conversion automatically—but for the stable release of coroutines, the Kotlin team wanted to make sure the conversion was as efficient as possible.&lt;/p&gt;
    &lt;p&gt;To help the JVM optimize its execution of suspending functions, Kotlin 1.3 added a set of new functions for the compiler to use in its generated coroutine code: &lt;code&gt;boxBoolean()&lt;/code&gt;, &lt;code&gt;boxByte()&lt;/code&gt;, &lt;code&gt;boxShort()&lt;/code&gt;, &lt;code&gt;boxInt()&lt;/code&gt;, and so on (source: GitHub). Since the &lt;code&gt;suspend&lt;/code&gt; keyword is part of the core language, these functions must be available to all Kotlin programs, which is why they’re in the standard library, not the coroutines library—though they’re marked as &lt;code&gt;internal&lt;/code&gt;, and aren’t available for you to call directly.&lt;/p&gt;
    &lt;p&gt;The functions themselves aren’t the problem: they’re spelled correctly, and their implementations are so trivial that there’s nowhere for anything to go wrong. No, the bug happens when the compiler generates the code that calls these functions.&lt;/p&gt;
    &lt;p&gt;To correctly box a value, Kotlin needs to map the Java primitive type to its corresponding box function. A &lt;code&gt;boolean&lt;/code&gt; value must be passed to &lt;code&gt;boxBoolean()&lt;/code&gt;; a &lt;code&gt;byte&lt;/code&gt; value to &lt;code&gt;boxByte()&lt;/code&gt;, and so on.&lt;/p&gt;
    &lt;p&gt;There’s an obvious pattern there: capitalize the first letter of the primitive type, and then add “box” to the start. And that’s exactly what Kotlin 1.3 did, using the standard library’s &lt;code&gt;capitalize()&lt;/code&gt; function: (source: GitHub)&lt;/p&gt;
    &lt;code&gt;map[name] = "box${primitiveType.javaKeywordName.capitalize()}"&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;capitalize()&lt;/code&gt; function modifies only the first letter of a string—so &lt;code&gt;"boolean".capitalize()&lt;/code&gt; becomes &lt;code&gt;"Boolean"&lt;/code&gt;, &lt;code&gt;"int".capitalize()&lt;/code&gt; becomes &lt;code&gt;"Int"&lt;/code&gt;, and so on.&lt;/p&gt;
    &lt;p&gt;Unless you’re in Turkey.&lt;/p&gt;
    &lt;p&gt;Once again, the behaviour of &lt;code&gt;capitalize()&lt;/code&gt; can vary depending on your computer’s language settings. It’s that pesky letter ‘i’ again:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;With Turkish language settings, the upper case form of &lt;code&gt;i&lt;/code&gt;is&lt;code&gt;İ&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;whereas in English, the upper case form of &lt;code&gt;i&lt;/code&gt;is&lt;code&gt;I&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you’re working in Turkish, the correct result for &lt;code&gt;"int".capitalize()&lt;/code&gt; is &lt;code&gt;"İnt"&lt;/code&gt;. The &lt;code&gt;capitalize()&lt;/code&gt; function has no way of knowing that “int” is a special programming keyword that needs to be treated as English text rather than Turkish. So when the Kotlin 1.3 compiler, running on a machine with Turkish language settings, needs to box up a primitive Java &lt;code&gt;int&lt;/code&gt; inside a suspending function, it’s going to generate a call to a non-existent standard library function called &lt;code&gt;boxİnt()&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Oops!&lt;/p&gt;
    &lt;p&gt;It was Fatih Doğan, another Turkish programmer, who in September 2019 managed to put all the pieces together, filing the issue report that would finally lead to a fix. Fatih clearly pointed out the misplaced dot above the ‘İ’ in &lt;code&gt;boxİnt()&lt;/code&gt;, and, crucially, set up a GitHub repository with instructions to reliably reproduce the issue.&lt;/p&gt;
    &lt;p&gt;Within a day of this new detailed issue report, the Kotlin team had found the line of code that was causing the issue. Less than a week later, they had a fix ready: (source: GitHub)&lt;/p&gt;
    &lt;code&gt;map[name] = "box${primitiveType.javaKeywordName.capitalize(Locale.US)}"&lt;/code&gt;
    &lt;p&gt;Passing a specific &lt;code&gt;Locale&lt;/code&gt; to the &lt;code&gt;capitalize()&lt;/code&gt; function means it will always use the same language rules, no matter what machine you run it on. It’s an easy change: like all the case conversion functions in the Kotlin standard library, &lt;code&gt;capitalize()&lt;/code&gt; already accepted an optional &lt;code&gt;Locale&lt;/code&gt; argument. It only fell back to the system’s default &lt;code&gt;Locale&lt;/code&gt; if you didn’t specify your own.&lt;/p&gt;
    &lt;p&gt;The fix was released as part of Kotlin 1.3.6, in November 2019, finally giving Turkish developers a stable way to use suspending functions.&lt;/p&gt;
    &lt;p&gt;But that’s not the end of this story—far from it. Coroutines might be working again, but they had proved just how easy it was to fall disastrously foul of locale-sensitive case conversions. And that original build error from the start of the story still wasn’t fixed…&lt;/p&gt;
    &lt;p&gt;It took one more bug to demonstrate the true severity of the problem.&lt;/p&gt;
    &lt;p&gt;In September 2020, nearly a year after the coroutines bug had been fixed and forgotten, Muhittin Kaplan was just starting to learn Kotlin. He wrote a simple program to check his understanding of arrays:&lt;/p&gt;
    &lt;code&gt;fun main() {&lt;lb/&gt;    println("Hello, world!!!")&lt;lb/&gt;    val nums = intArrayOf(1, 2, 3, 4, 5)&lt;lb/&gt;    println(nums[2])&lt;lb/&gt;}&lt;/code&gt;
    &lt;p&gt;But when he ran the program, he saw a baffling error:&lt;/p&gt;
    &lt;code&gt;java.lang.NoSuchMethodError:&lt;lb/&gt;  'int[] kotlin.jvm.internal.Intrinsics$Kotlin.intArrayOf(int[])'&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;intArrayOf()&lt;/code&gt; function is one of the most basic tools in the Kotlin standard library—and it has existed in every Kotlin version since 1.0 and before. Even if it didn’t exist, or if it was being called incorrectly, the error should happen at compile time, not at runtime.&lt;/p&gt;
    &lt;p&gt;Muhittin knew something fishy was going on, and he filed a YouTrack issue describing what he was seeing.&lt;/p&gt;
    &lt;p&gt;“Hi from Türkiye,” he began.&lt;/p&gt;
    &lt;p&gt;This time, the Kotlin team knew what to look for. It wasn’t long before they had tracked down the faulty line of code in the compiler: (source: GitHub)&lt;/p&gt;
    &lt;code&gt;StringsKt.decapitalize(type.getArrayTypeName().asString()) + "Of"&lt;/code&gt;
    &lt;p&gt;Although it’s written in Java, this code is calling the &lt;code&gt;decapitalize()&lt;/code&gt; function from Kotlin’s own standard library. And once again, it’s relying on the system’s default language settings, instead of using a fixed &lt;code&gt;Locale&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The code is part of a procedure that’s responsible for configuring intrinsics—functions that don’t really have an implementation in the Kotlin standard library, but are instead replaced directly by the compiler with the corresponding Java instructions or even JVM bytecode. When you write &lt;code&gt;intArrayOf(1, 2, 3)&lt;/code&gt;, Kotlin doesn’t really call an &lt;code&gt;intArrayOf()&lt;/code&gt; function. Instead, it recognizes that &lt;code&gt;intArrayOf()&lt;/code&gt; was registered as an intrinsic, and just outputs the bytecode to create and populate an array.&lt;/p&gt;
    &lt;p&gt;Much like the &lt;code&gt;boxInt()&lt;/code&gt; function we saw before, &lt;code&gt;intArrayOf()&lt;/code&gt; is part of a wider family of functions: one array-builder function for each primitive type. The call to &lt;code&gt;type.getArrayTypeName()&lt;/code&gt; returns the name of the Kotlin class for each array—&lt;code&gt;IntArray&lt;/code&gt;, &lt;code&gt;BooleanArray&lt;/code&gt;, and so on. The corresponding function—&lt;code&gt;intArrayOf()&lt;/code&gt;, &lt;code&gt;booleanArrayOf()&lt;/code&gt;, and so on—should start with a lower case letter, so we need to call &lt;code&gt;decapitalize()&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;And there’s our bug.&lt;/p&gt;
    &lt;p&gt;On a machine with Turkish language settings, &lt;code&gt;"IntArray".decapitalize()&lt;/code&gt; (or &lt;code&gt;StringsKt.decapitalize("IntArray")&lt;/code&gt;, as it appears in Java) returns &lt;code&gt;"ıntArray"&lt;/code&gt;, with that all-too-familiar dotless lowercase ‘ı’. Add the &lt;code&gt;"Of"&lt;/code&gt; suffix, and we’ve just registered an intrinsic bytecode implementation for a function called &lt;code&gt;ıntArrayOf()&lt;/code&gt;—not the same as the &lt;code&gt;intArrayOf()&lt;/code&gt; function that the standard library is advertising!&lt;/p&gt;
    &lt;p&gt;When they came to fix this issue, the Kotlin team weren’t leaving anything to chance. They scoured the entire compiler codebase for case-conversion operations—calls to &lt;code&gt;capitalize()&lt;/code&gt;, &lt;code&gt;decapitalize()&lt;/code&gt;, &lt;code&gt;toLowerCase()&lt;/code&gt;, and &lt;code&gt;toUpperCase()&lt;/code&gt;—and replaced them with locale-invariant alternatives. 173 lines of code changed, across 53 files—including the compiler-output XML parser that had caused Mehmet Nuri Öztürk’s build to fail in Kotlin 1.0, all those years ago (source: GitHub).&lt;/p&gt;
    &lt;p&gt;The slew of fixes was released as part of a more general compiler upgrade project in Kotlin 1.5, in May 2021. After five years in the backlog, KT-13631 was finally closed.&lt;/p&gt;
    &lt;p&gt;Three different bugs—in compiler outputs, coroutines, and arrays—caused by three different functions—&lt;code&gt;toLowerCase()&lt;/code&gt;, &lt;code&gt;capitalize()&lt;/code&gt;, and &lt;code&gt;decapitalize()&lt;/code&gt;. Without a more foundational solution, Kotlin’s case-conversion trap was just waiting to claim its next victim.&lt;/p&gt;
    &lt;p&gt;Even before Kotlin 1.5 was released, the Kotlin team were hard at work on a project to make sure locale-sensitive case conversions would never crash another Kotlin program.&lt;/p&gt;
    &lt;p&gt;In October of 2020, they published KEEP-223, “Locale-agnostic case conversions by default”—a proposal to replace Kotlin’s case-conversion functions with a new set of functions that would ignore your system’s language settings and simply default to a fixed locale. The new &lt;code&gt;uppercase()&lt;/code&gt; and &lt;code&gt;lowercase()&lt;/code&gt; functions were added to the standard library in Kotlin 1.5, and as of Kotlin 2.1, using the older &lt;code&gt;toLowerCase()&lt;/code&gt; and &lt;code&gt;toUpperCase()&lt;/code&gt; functions generates an error.&lt;/p&gt;
    &lt;p&gt;What about &lt;code&gt;capitalize()&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;When KEEP-223 was being discussed, it gradually became clear that &lt;code&gt;capitalize()&lt;/code&gt; had more problems than just locale sensitivity. The function name itself is surprisingly ambiguous. Look up capitalize in almost any English dictionary and you’ll find two competing definitions. Here’s one example from Collins:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;6. to print or write (a word or words) in capital letters&lt;/p&gt;
      &lt;p&gt;7. to begin (a word) with a capital letter&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Kotlin’s &lt;code&gt;capitalize()&lt;/code&gt; function had always been designed to modify only the first letter of a string, and this was the perfect opportunity to clear up the confusion. If &lt;code&gt;capitalize()&lt;/code&gt; was an ambiguous name, what should its replacement be called? Can you think of a name that describes the function’s behaviour more clearly?&lt;/p&gt;
    &lt;p&gt;In the end, the Kotlin team chose not to provide a replacement at all. If the function doesn’t exist, it can’t cause confusion or bugs! In modern Kotlin, when you want to modify the first character of a string, you provide a custom lambda to &lt;code&gt;replaceFirstChar { … }&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Kotlin 2.1 was released in November 2024, drawing our story to a satisfying close.&lt;/p&gt;
    &lt;p&gt;What can we learn from it? I think the biggest lesson is just how much responsibility rests on a language’s standard library. It’s easy to think of the standard library as simply a starter pack of stock algorithms and data structures. But dig just a little deeper, and you’ll find that even the simplest string operations rely on a detailed digital model of the complexity and creativity of human culture.&lt;/p&gt;
    &lt;p&gt;Much of that digital model, by the way, is provided by the Unicode Common Locale Data Repository (CLDR), which documents language rules, date and time formats, measurement units, currencies, and much more. Unicode isn’t just for emojis!&lt;/p&gt;
    &lt;p&gt;Now, there’s just one question that’s still bugging me. Did Mehmet Nuri Öztürk ever get that app to build?&lt;/p&gt;
    &lt;p&gt;Thanks for reading!&lt;/p&gt;
    &lt;p&gt;I write books, too. If you want more Kotlin oddities and compiler quirks, check out my puzzle book, Kotlin Brain Teasers:&lt;/p&gt;
    &lt;p&gt;And if you enjoyed learning about suspending functions and coroutines, you might like Kotlin Coroutine Confidence:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45559767</guid><pubDate>Sun, 12 Oct 2025 17:02:09 +0000</pubDate></item><item><title>Wireguard FPGA</title><link>https://github.com/chili-chips-ba/wireguard-fpga</link><description>&lt;doc fingerprint="b5d9dc196105abe7"&gt;
  &lt;main&gt;
    &lt;p&gt;Virtual Private Networks (VPNs) are the central and indispensable component of Internet security. They comprise a set of technologies that connect geographically dispersed, heterogeneous networks through encrypted tunnels, creating the impression of a homogenous private network on the public shared physical medium.&lt;/p&gt;
    &lt;p&gt;With traditional solutions (such as OpenVPN / IPSec) starting to run out of steam, Wireguard is increasingly coming to the forefront as a modern, secure data tunneling and encryption method, one that's also easier to manage than the incumbents. Both software and hardware implementations of Wireguard already exist. However, the software performance is far below the speed of wire. Existing hardware approaches are both prohibitively expensive and based on proprietary, closed-source IP blocks and tools.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The intent of this project is to bridge these gaps with an FPGA open-source implementation of Wireguard, written in SystemVerilog HDL.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We have contributed to the Blackwire project, which is a 100Gbps hardware implementation of Wireguard switch based on AMD/Xilinx-proprietary AlveoU50 PC-accelerator card (SmartNIC form-factor), and implementable only with proprietary Vivado toolchain.&lt;/p&gt;
    &lt;p&gt;While working on the Blackwire, we have touched multiple sections, and focused on the novel algorithm for Balanced Binary Tree Search of IP tables. However, the Blackwire hardware platform is expensive and priced out of reach of most educational institutions. Its gateware is written in SpinalHDL, a nice and powerfull but a niche HDL, which has not taken roots in the industry. While Blackwire is now released to open-source, that decision came from their financial hardship -- It was originaly meant for sale. Moreover, the company behind it is subject to disputes and obligations that bring into question the legality of ownership over the codebase they "donated" to the open source community.&lt;/p&gt;
    &lt;p&gt;To make the hardware Wireguard truly accessible in the genuine spirit of open-source movement, this project implements it:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;for an inexpensive hardware platform with four 1000Base-T ports&lt;/item&gt;
      &lt;item&gt;in a self-sufficient way, i.e. w/o requiring PC host&lt;/item&gt;
      &lt;item&gt;using a commodity Artix7 FPGA&lt;/item&gt;
      &lt;item&gt;which is supported by open-source tools&lt;/item&gt;
      &lt;item&gt;and with all gateware written in the ubiquitous Verilog / System Verilog&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;[Ref1] Wireguard implementations in software:&lt;/p&gt;
    &lt;p&gt;[Ref2] 100Gbps Blackwire Wireguard&lt;/p&gt;
    &lt;p&gt;[Ref3] Corundum, open-source FPGA-NIC platform&lt;/p&gt;
    &lt;p&gt;[Ref4] ChaCha20-Poly1305 open-source Crypto RTL&lt;/p&gt;
    &lt;p&gt;[Ref5] Cookie Cutter SOC&lt;/p&gt;
    &lt;p&gt;[Ref6] RISC-V ISS&lt;/p&gt;
    &lt;p&gt;[Ref7] 10Gbps Ethernet Switch&lt;/p&gt;
    &lt;p&gt;[Ref8] OpenXC7 open-source tools for Xilinx Series7&lt;/p&gt;
    &lt;p&gt;[Ref9] Alex's Ethernet Stack&lt;/p&gt;
    &lt;p&gt;[Ref10] Amina's ADASEC-SDN&lt;/p&gt;
    &lt;p&gt;The Phase1 (This!) is primarily Proof of Concept, i.e. not full-featured, and definitely not a deployable product. It is envisoned as a mere on-ramp, a springboard for future build-up and optimizations.&lt;/p&gt;
    &lt;p&gt;The Phase2 continuation project is therefore also in the plans, to maximize efficiency and overall useability, such as by increasing the number of channels, facilitating management with GUI apps, or something else as identified by the community feedback.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;HW/SW partitioning, interface, interactions and workload distribution&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;While, contrary to Blackwire, we don’t rely on an external PC connected via PCIE, we will still have an on-chip RISC-V CPU with intricate hardware interface and significant Embedded Software component that controls the backbone of wire-speed datapath&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;HW/SW co-development, integration and debugging&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Standard simulation is impractical for the project of this size and complexity. We therefore intend to put to test and good use the very promissing new VProc ISS [Ref6]&lt;/item&gt;
          &lt;item&gt;It’s also impractical and expensive to provide full test systems with real traffic generators and checkers to all developers. We therefore plan to rent some space for a central lab that will host two test systems, then provide remote access to all developers&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Real-life, at-speed testing&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Extent of open-source tools support for SystemVerilog and all needed FPGA primitives and IP functions&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;QOR of the (still maturing) open-source tools&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Blackwire used commercial, AMD/Xilinx-proprietary Vivado toolchain, as well as high-end Alveo U50 FPGA silicon. Even then, they ran into multiple timing closure, utilization and routing congestion challenges.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Financial resources&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Given that this is a complex, multi-disciplinary dev effort, the available funding may not be sufficient to bring it to completion. Blackwire, despite a larger allocated budget, ended up with funding crisis and abrupt cessation of dev activities.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project is WIP at the moment. The checkmarks below indicate our status. Until all checkmarks are in place, anything you get from here is w/o guaranty -- Use at own risk, as you see fit, and don't blame us if it is not working 🌤️&lt;/p&gt;
    &lt;p&gt;Board bring up. In-depth review of Wireguard ecosystem and prior art. Design Blueprint&lt;/p&gt;
    &lt;p&gt;While the board we're using is low cost, it is also not particularly known in the open-source community. We certainly don’t have prior experience with it. In this opening take we will build a solid foundation for efficient project execution. Good preparation is crucial for a smooth run. We thus seek to first &lt;code&gt;understand and document what we will be designing: SOC Architecture, Datapath Microarchitecture, Hardware/Software Partitioning, DV and Validation Strategy&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Getting a good feel for our Fmax is also a goal of this take. Artix-7 does not support High-Performance (HP) I/O. Consequently, we cannot push its I/O beyond 600MHz, nor its core logic beyond 100 MHz.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Familiarization with HW platform&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Create our first FPGA program that blinks LEDs&lt;/item&gt;
          &lt;item&gt;Verify pinouts and connectivity using simple test routines&lt;/item&gt;
          &lt;item&gt;Generate a few Ethernet test patterns&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Familiarization with SW platform&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Initial bring up of embedded CPU within a cookie-cutter SOC, such as [Ref5]&lt;/item&gt;
          &lt;item&gt;Design and test a simple SW interface to rudimentary HW Ethernet datapath&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Detailed analysis and comparisons of:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Wireguard White Papers&lt;/item&gt;
          &lt;item&gt;existing implementations in software [Ref1]&lt;/item&gt;
          &lt;item&gt;vs. Blackwire hardware implementation [Ref2]&lt;/item&gt;
          &lt;item&gt;cryptographic algorithms used for Wireguard, esp. ChaCha20 for encryption, Poly1305 for authentication [Ref4] and, to a lesser extent, Curve25519 for key exchange and blake2 for hashing&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Identification and assimilation of prior art and building IP blocks, in particular Corundum [Ref3] and, to a lesser extent, 10GE Switch [Ref7]&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Architecture/uArch Design. HW/SW Partitioning. Verification Plan&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Creation of sufficient initial documentation for project divide-and-conquer across a multi-disciplinary team of half a dozen developers&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Implementation of a basic, statically pre-configured Wireguard link&lt;/p&gt;
    &lt;p&gt;It it in this take that we start creating hardware Datapath and hardening Wireguard encryption protocols, all using Vivado and Xilinx primitives.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Integration of collected RTL blocks into a coherent HW system that implements the basic Wireguard datapath for a handful of manually pre-configured channels.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Corundum FPGA-based NIC and platform for opensource Ethernet development [Ref3]&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;IP Core for ChaCha20-Poly1305 [Ref4] -- Definitely in hardware from the get-go&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Curve25519 module for key exchange -- Likely in software at this point&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;blake2 module for hashing (we'll most likely do it in software)&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Timing closure. Resolution of FPGA device utilization and routing congestion issues&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Creation of cocoTB DV in the CI/CD environmenT, and representative test cases for datapath simulation&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Development and integration of embedded management software (Control Plane)&lt;/p&gt;
    &lt;p&gt;This work package is about hardware/software codesign and integration. The firmware will run on a soft RISC V processor, inside the FPGA. Our vanilla SOC is at this point starting to be customized to Wireguard needs. This work can to some extent go on in parallel with hardware activities of Take2.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;SW design for on-chip processor (Part 1)&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Code is to be written in the bare-metal C with, as necessary, a few sections in Assembly&lt;/item&gt;
          &lt;item&gt;SW is responsible for configuration and management of hardware blocks&lt;/item&gt;
          &lt;item&gt;SW must not participate in the bulk datapath transfers&lt;/item&gt;
          &lt;item&gt;SW may however intercept the low-frequency management packets&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;SW design for on-chip processor (Part 2)&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;KMM function -- Key Management Module&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;HW/SW Integration&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;VPN Tunnel: Session initialization, maintenance, and secure closure&lt;/p&gt;
    &lt;p&gt;This is about managing the bring-up, maintenance and tear-down of VPN tunnels between two devices.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Session Initialization: Starting the handshake process to establish secure communication with another device&lt;/item&gt;
      &lt;item&gt;Session Maintenance: Keeping the session active through the regular exchange of control messages, which allows detection and recovery from problems such as connection interruptions&lt;/item&gt;
      &lt;item&gt;Session Closure: Securely close the VPN tunnel when communication is no longer needed, ensuring that all temporary keys and sensitive data are deleted&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Testing, Profiling and Porting to OpenXC7&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Functional testing on the real system. Does it work as intended? Bug fixes&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Performance testing. HW/SW profiling, updates and enhancements to ensure the design indeed operates at close to the wire speed on all preconfigured channels&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Porting to openXC7 [Ref8] using SV2V, in the GoCD CI/CD setting&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;This is challenging, as openXC7 has thus far been crashing for NES SV&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Timing closure with openXC7&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;This is definitely challenging, given that openXC7 is currently without accurate timing-driven STA&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Filing bug tickets with open source developers for issues found in their tools, supporting them all the way to the resolution&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Creation and maintenance of an attractive and well-documented Github repo, to entice community interest&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Ongoing documentation updates and CI/CD script maintenance to keep it valid in the light of inevitable design mutations compared to the original Design Blueprint.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Flow control module for efficient and stable VPN tunnel data management&lt;/p&gt;
    &lt;p&gt;The objective of this optional deliverable is to ensure stable and efficient links, thus taking this project one step closer to a deployable product.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Develop software components for management of data flow within VPN tunnels&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since the WireGuard node essentially functions as an IP router with WireGuard protocol support, we have decided to design the system according to a two-layer architecture: a control plane responsible for managing IP routing processes and executing the WireGuard protocol (managing remote peers, sessions, and keys), and a data plane that will perform IP routing and cryptography processes at wire speed. The control plane will be implemented as software running on a soft CPU, while the data plane will be fully implemented in RTL on an FPGA.&lt;/p&gt;
    &lt;p&gt;In the HW/SW partitioning diagram, we can observe two types of network traffic: control traffic, which originates from the control plane and goes toward the external network (and vice versa), and data traffic, which arrives from the external network and, after processing in the data plane, returns to the external network. Specifically, control traffic represents WireGuard protocol handshake messages, while data traffic consists of end-user traffic, either encrypted or in plaintext, depending on the perspective.&lt;/p&gt;
    &lt;p&gt;The hardware architecture essentially follows the HW/SW partitioning and consists of two domains: a soft CPU for the control plane and RTL for the data plane.&lt;/p&gt;
    &lt;p&gt;The soft CPU is equipped with a Boot ROM and a DDR3 SDRAM controller for interfacing with off-chip memory. External memory is exclusively used for control plane processes and does not store packets. The connection between the control and data planes is established through a CSR-based HAL.&lt;/p&gt;
    &lt;p&gt;The data plane consists of several IP cores, including data plane engine (DPE) and supporting components, which are listed and explained in the direction of network traffic propagation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PHY Controller - initial configuration of Realtek PHYs and monitoring link activity (link up/down events)&lt;/item&gt;
      &lt;item&gt;1G MAC - execution of the 1G Ethernet protocol (framing, flow control, FCS, etc.)&lt;/item&gt;
      &lt;item&gt;Rx FIFOs - clock domain crossing, bus width conversion, and store &amp;amp; forward packet handling&lt;/item&gt;
      &lt;item&gt;Per-Packet Round Robin Multiplexer - servicing Rx FIFOs on a per-packet basis using a round-robin algorithm&lt;/item&gt;
      &lt;item&gt;Header Parser - extraction of WireGuard-related information from packet headers (IP addresses, UDP ports, WireGuard message type, peer ID, etc.)&lt;/item&gt;
      &lt;item&gt;Wireguard/UDP Packet Disassembler - decapsulation of the payload from the Wireguard data packet for decryption of tunneled traffic&lt;/item&gt;
      &lt;item&gt;ChaCha20-Poly1305 Decryptor - decryption and authentication of tunneled traffic&lt;/item&gt;
      &lt;item&gt;IP Lookup Engine - routing/forwarding table lookup, mapping packets to the appropriate WireGuard peer, and making packet accept/reject decisions&lt;/item&gt;
      &lt;item&gt;ChaCha20-Poly1305 Encryptor - encryption and authentication of traffic to be tunneled&lt;/item&gt;
      &lt;item&gt;Wireguard/UDP Packet Assembler - encapsulation of the encrypted packet into a WireGuard data packet for tunneling to the remote peer&lt;/item&gt;
      &lt;item&gt;Per-Packet Demultiplexer - forwarding packets to Tx FIFOs based on packet type and destination&lt;/item&gt;
      &lt;item&gt;Tx FIFOs - clock domain crossing, bus width conversion, and store &amp;amp; forward packet handling&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;ChaCha20-Poly1305 Encryptor/Decryptor are using RFC7539's AEAD (Authenticated Encryption Authenticated Data) construction based on ChaCha20 for symmetric encryption and Poly1305 for authentication.&lt;/p&gt;
    &lt;p&gt;The details of hardware architecture can be found in the README.md in the &lt;code&gt;1.hw/&lt;/code&gt; directory.&lt;/p&gt;
    &lt;p&gt;The conceptual class diagram provides an overview of the components in the software part of the system without delving into implementation details. The focus is on the WireGuard Agent, which implements the protocol's handshake procedures, along with the following supplementary components:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Curve25519 - an ECDH algorithm implementation for establishing a shared secret using a public-private key pair between two remote parties connected via an insecure channel, such as the Internet&lt;/item&gt;
      &lt;item&gt;ChaCha20-Poly1305 - an AEAD algorithm implementation for encryption and authentication of static keys and nonce values to prevent replay attacks&lt;/item&gt;
      &lt;item&gt;XChaCha20-Poly1305 - a XAEAD algorithm implementation for encrypting and authenticating nonce values in Cookie Replay messages to mitigate potential DoS attacks&lt;/item&gt;
      &lt;item&gt;BLAKE2s - an implementation of the BLAKE2s hash function for MAC authentication and keyed hashing, per RFC7693&lt;/item&gt;
      &lt;item&gt;RNG - a random number generator used to initialize the DH key generator and generate peer identifiers&lt;/item&gt;
      &lt;item&gt;Timer - timers for rekey, retry, and keepalive procedures&lt;/item&gt;
      &lt;item&gt;HKDF - an implementation of the algorithm for expanding the ECDH result&lt;/item&gt;
      &lt;item&gt;RTC - a real-time clock used to generate the TAI64N timestamp&lt;/item&gt;
      &lt;item&gt;SipHash - a simple non-cryptographic function used for implementing a hashtable for fast lookup of decrypted static public keys of remote peers&lt;/item&gt;
      &lt;item&gt;Routing DB Updater - a subsystem for maintaining the cryptokey routing table content and deploying it to the data plane via the HAL/CSR interface&lt;/item&gt;
      &lt;item&gt;ICMP - implementing basic ICMP protocol functions (echo request/reply, TTL exceeded, etc.)&lt;/item&gt;
      &lt;item&gt;CLI - a USB/UART-based command-line interface for configuring the WireGuard node (setting the local IP address, remote peer IP addresses, network addresses, keys, etc.)&lt;/item&gt;
      &lt;item&gt;HAL/CSR Driver - a CSR-based abstraction for data plane components with an interface for reading/writing the corresponding registers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The details of software architecture can be found in the README.md in the &lt;code&gt;2.sw/&lt;/code&gt; directory.&lt;/p&gt;
    &lt;p&gt;To illustrate the operation of the system as a whole, we have prepared a step-by-step analysis of packets processing based on the capture of real WireGuard traffic. The experimental topology consists of four nodes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;10.10.0.2 - the end-user host at site A&lt;/item&gt;
      &lt;item&gt;10.9.0.1 - WireGuard peer A&lt;/item&gt;
      &lt;item&gt;10.9.0.2 - WireGuard peer B&lt;/item&gt;
      &lt;item&gt;10.10.0.1 - the end-user host at site B&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The detailed analysis can be found in the README.md in the &lt;code&gt;1.hw/&lt;/code&gt; directory.&lt;/p&gt;
    &lt;p&gt;The Wireguard FPGA test bench aims to have a flexible approach to simulation which allows a common test environment to be used whilst selecting between alternative CPU components, one of which uses the VProc virtual processor co-simulation element. This allows simulations to be fully HDL, with a RISC-V processor RTL implementation such as picoRV32, IBEX or EDUBOS5, or to co-simulate software using the virtual processor, with a significant speed up in simulation times. The test bench has the following features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A VProc virtual processor based &lt;code&gt;soc_cpu.VPROC&lt;/code&gt;component&lt;list rend="ul"&gt;&lt;item&gt;Selectable between this or an RTL softcore&lt;/item&gt;&lt;item&gt;Can run natively compiled test code&lt;/item&gt;&lt;item&gt;Can run the application compiled natively with the auto-generated co-sim HAL&lt;/item&gt;&lt;item&gt;Can run RISC-V compiled code using the rv32 RISC-V ISS model&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Uses a C sparse memory model &lt;list rend="ul"&gt;&lt;item&gt;An HDL component instantiated in logic gives logic access to this memory&lt;/item&gt;&lt;item&gt;An API is provided to VProc running code for direct access&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;The udpIpPg VIP is used to drive the logic's four ethernet ports in a four port &lt;code&gt;bfm_ethernet&lt;/code&gt;block.&lt;list rend="ul"&gt;&lt;item&gt;An MDIO slave interface is also provided that maps mem_model memory areas to the registers with instantiated mem_model components&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The figure below shows an oveview block diagram of the test bench HDL.&lt;/p&gt;
    &lt;p&gt;More details on the architecture and usage of the Wireguard test bench can be found in the README.md in the &lt;code&gt;4.sim&lt;/code&gt; directory.&lt;/p&gt;
    &lt;p&gt;The Wireguard control and status register harware abstraction layer (HAL) software is auto-generated, as is the CSR RTL, using &lt;code&gt;peakrdl&lt;/code&gt;. For co-simulation purposes an additional layer is auto-generated from the same SystemRDL specification using &lt;code&gt;systemrdl-compiler&lt;/code&gt; that accompanies the &lt;code&gt;peakrdl&lt;/code&gt; tools. This produces two header files that define a common API to the application layer for both the RISC-V platform and the VProc based co-simulation verification environment. The details of the HAL generation can be found in the README.md in the &lt;code&gt;3.build/&lt;/code&gt; directory.&lt;/p&gt;
    &lt;p&gt;TODO&lt;/p&gt;
    &lt;p&gt;WIP&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Verilator v5.024&lt;/item&gt;
      &lt;item&gt;VProc v1.12.2&lt;/item&gt;
      &lt;item&gt;Mem Model v1.0.0&lt;/item&gt;
      &lt;item&gt;rv32 ISS v1.1.4&lt;/item&gt;
      &lt;item&gt;udpIpPg v1.0.3&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;TODO&lt;/p&gt;
    &lt;p&gt;TODO&lt;/p&gt;
    &lt;p&gt;TODO&lt;/p&gt;
    &lt;p&gt;TODO&lt;/p&gt;
    &lt;p&gt;We are grateful to NLnet Foundation for their sponsorship of this development activity.&lt;/p&gt;
    &lt;p&gt;The wyvernSemi's wisdom and contribution made a great deal of difference -- Thank you, we are honored to have you on the project.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45559857</guid><pubDate>Sun, 12 Oct 2025 17:12:00 +0000</pubDate></item><item><title>Constraint satisfaction to optimize item selection for bundles in Minecraft</title><link>https://www.robw.fyi/2025/10/12/using-constraint-satisfaction-to-optimize-item-selection-for-bundles-in-minecraft/</link><description>&lt;doc fingerprint="fa6c0da3bdcf216"&gt;
  &lt;main&gt;
    &lt;p&gt;I read a blog post on how Many Hard Leetcode Problems are Easy Constraint Problems and was pleasantly surprised at how approachable MiniZinc was compared to other solver software I have been exposed to, and the examples given helped me to understand how to apply it to a domain I was already familiar with. I have always wanted to be able to get more familiar with using constraint satisfaction as a way to solve problems, so I decided to create a solver to help optimize storage space for Minecraft using constraint satisfaction to help learn how to use this tool. I will outline my thought process and how I reached the solution I came up with.&lt;/p&gt;
    &lt;head rend="h2"&gt;Game Mechanics&lt;/head&gt;
    &lt;p&gt;In Minecraft, your player inventory is limited. You have 27 inventory slots, 9 hotbar slots, 4 armor slots, 4 temporary crafting slots, 1 offhand slot, 1 temporary slot for the result of crafting, and you can hold 1 item in your cursor (thanks charcircuit).&lt;/p&gt;
    &lt;p&gt;Each slot can either contain a single item, a small stack of 16, or a full stack of 64 items, depending on the item.&lt;/p&gt;
    &lt;p&gt;Often when adventuring, you will come across many rare and powerful items and accumulate many items in your inventory slots, but they may be stacks of items that are not at maximum capacity for that inventory slot. Once your inventory is full, you cannot pick up new items. Using bundles, you can consolidate your inventory and pick up more items.&lt;/p&gt;
    &lt;p&gt;A bundle is an item that can store up to a stack’s worth of mixed item types within itself in a single inventory slot. Items can be individually selected and taken out of the bundle via the inventory menu.&lt;/p&gt;
    &lt;p&gt;Item stack sizes (top row) and the number of bundle slots they take up (middle row). Sticks stack to 64, so they take up one bundle slot; ender pearls stack to 16, so they take up four; and swords do not stack, so they take up the whole bundle. So, for instance, a bundle may have 32 sticks and 8 ender pearls inside, which take up a total of (32×1)+(8×4)=64 bundle slots.&lt;/p&gt;
    &lt;head rend="h2"&gt;Creating an Optimizer&lt;/head&gt;
    &lt;p&gt;Using the MiniZinc Playground, we can model our player’s inventory and the constraints such that we free up the maximum number of inventory slots. That way, our player can pick up the most new items!&lt;/p&gt;
    &lt;p&gt;First, let’s create an inventory and let’s only focus on items that have full stacks. We can maximize our free inventory slots for this case, and then we will progressively make our way toward supporting all of the item types.&lt;/p&gt;
    &lt;code&gt;array[int] of int: inventory = [
  % fullstack of dirt 
  64,
  % half of a fullstack of wood
  32,
  % half stack of a fullstack of  wool
  32,
  % quarter of fullstack of sticks
  16,
  % quarter of a fulstack of carrots
  16
];&lt;/code&gt;
    &lt;p&gt;If our optimizer was working, it would select three slots of our inventory to go in the bundle to maximize our free inventory space. We can model inventory selection by creating another array of integers of either 0 or 1, with 1 representing a selected slot in our inventory and 0 representing an unselected slot.&lt;/p&gt;
    &lt;code&gt;int: n = length(inventory);
array[1..n] of var 0..1: selected;&lt;/code&gt;
    &lt;p&gt;Once we have this array representing our selected slots, we can maximize the sum of our array in order to select the most slots possible.&lt;/p&gt;
    &lt;code&gt;solve maximize sum(i in 1..n)(selected[i]);&lt;/code&gt;
    &lt;p&gt;The full code now will look like this&lt;/p&gt;
    &lt;code&gt;% Use this editor as a MiniZinc scratch book
array[int] of int: inventory = [
  % fullstack of dirt 
  64,
  % half of a fullstack of wood
  32,
  % half stack of a fullstack of  wool
  32,
  % quarter of fullstack of sticks
  16,
  % quarter of a fulstack of carrots
  16
];

int: n = length(inventory);
array[1..n] of var 0..1: selected;

solve maximize sum(i in 1..n)(selected[i]);&lt;/code&gt;
    &lt;p&gt;if we ran this on the playground, the output is&lt;/p&gt;
    &lt;code&gt;Running Playground.mzn
selected = [0, 0, 0, 0, 0];
----------
selected = [1, 0, 0, 0, 0];
----------
selected = [1, 1, 0, 0, 0];
----------
selected = [1, 1, 1, 0, 0];
----------
selected = [1, 1, 1, 1, 0];
----------
selected = [1, 1, 1, 1, 1];
----------
==========
Finished in 215msec.&lt;/code&gt;
    &lt;p&gt;Our solver found the solution &lt;code&gt;selected = [1, 1, 1, 1, 1];&lt;/code&gt; to maximize our selected slots in our inventory, which means it selected every slot. This makes sense because we did not constrain it in any way. Let’s add some constraints so we can now make better-informed selections.&lt;/p&gt;
    &lt;p&gt;The constraint we are exceeding is our bundle capacity. If we include our bundle capacity in our model and constrain our selections to include only inventory items that do not exceed the bundle capacity, we can find a valid solution.&lt;/p&gt;
    &lt;code&gt;int: capacity = 64;
constraint sum(i in 1..n)(selected[i] * inventory[i]) &amp;lt;= capacity;&lt;/code&gt;
    &lt;p&gt;If we add this constraint to our model and run it again, we can see that it now only selects three inventory slots, and it selects slots that do not exceed the capacity of our bundle.&lt;/p&gt;
    &lt;code&gt;Running Playground.mzn
selected = [0, 0, 0, 0, 0];
----------
selected = [1, 0, 0, 0, 0];
----------
selected = [0, 1, 1, 0, 0];
----------
selected = [0, 1, 0, 1, 1];
----------
==========
Finished in 207msec.&lt;/code&gt;
    &lt;p&gt;Great! Our initial model works for items that all stack to 64, but Minecraft has items with different maximum stack sizes. As mentioned earlier:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Items that stack to 64 (like dirt, sticks) take up 1 bundle slot per item&lt;/item&gt;
      &lt;item&gt;Items that stack to 16 (like ender pearls, snowballs) take up 4 bundle slots per item&lt;/item&gt;
      &lt;item&gt;Unstackable items (like tools, armor) take up 64 bundle slots (the entire bundle)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To handle this in our model, we need to represent each item’s bundle cost accurately. We’ll use a scaled representation to avoid decimal arithmetic:&lt;/p&gt;
    &lt;code&gt;int: units = 16 * 64;  % Total bundle capacity (1024 units)
int: fullstack = 1 * 16;  % Cost per fullstack item (16 units per item)
int: smallstack = 1 * 64;  % Cost per smallstack item (64 units per item)
int: unstackable = 16 * 64 + 1;  % Cost exceeds capacity (1025 units)&lt;/code&gt;
    &lt;p&gt;Why scale by 16? This gives us clean integer values:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1 dirt item = 16 units (1 bundle slot × 16)&lt;/item&gt;
      &lt;item&gt;1 ender pearl = 64 units (4 bundle slots × 16)&lt;/item&gt;
      &lt;item&gt;Total capacity = 1024 units (64 bundle slots × 16)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now we can model a more realistic inventory:&lt;/p&gt;
    &lt;code&gt;array[int] of int: inventory = [
  % full stack of 64 dirt (takes entire bundle)
  64 * fullstack,  % 64 × 16 = 1024 units
  
  % full stack of 16 ender pearls (takes entire bundle)
  16 * smallstack,  % 16 × 64 = 1024 units
  
  % 1 pickaxe (can't fit with anything else)
  unstackable,  % 1025 units
  
  % half stack of 32 dirt (half a bundle)
  32 * fullstack,  % 32 × 16 = 512 units
  
  % half stack of 8 ender pearls (half a bundle)
  8 * smallstack,  % 8 × 64 = 512 units
];&lt;/code&gt;
    &lt;p&gt;and the rest of the model remains the same&lt;/p&gt;
    &lt;code&gt;int: n = length(inventory);
array[1..n] of var 0..1: selected;

constraint sum(i in 1..n)(selected[i] * inventory[i]) &amp;lt;= units;
solve maximize sum(i in 1..n)(selected[i]);&lt;/code&gt;
    &lt;p&gt;the full example:&lt;/p&gt;
    &lt;code&gt;int: units = 16 * 64;
int: fullstack = 1 * 16;
int: smallstack = 1 * 64;
int: unstackable = 16 * 64 + 1;

array[int] of int: inventory = [
  % stack of dirt 
  64 * fullstack,
  % stack of ender pearls 
  16 * smallstack,
  % pickaxe
  unstackable,
  % half stacks, should select these to maximize inventory space
  32 * fullstack,
  8 * smallstack,
];

int: n = length(inventory);
array[1..n] of var 0..1: selected;

constraint sum(i in 1..n)(selected[i] * inventory[i]) &amp;lt;= units;
solve maximize sum(i in 1..n)(selected[i]);&lt;/code&gt;
    &lt;p&gt;When we run this on the playground, we can see that it selects the last two slots, which correctly selects the maximum number of mixed items to store in a bundle.&lt;/p&gt;
    &lt;code&gt;Running Playground.mzn
selected = [0, 0, 0, 0, 0];
----------
selected = [1, 0, 0, 0, 0];
----------
selected = [0, 0, 0, 1, 1];
----------
==========
Finished in 169msec.&lt;/code&gt;
    &lt;p&gt;Using MiniZinc, we can represent our problem declaratively, making it easy to extend and modify as needed. If game mechanics change or we want to support other storage systems like shulker boxes, we can simply update our constraints. This project was an enjoyable introduction to constraint satisfaction problems, and the MiniZinc Playground made it accessible without requiring any local setup.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45560535</guid><pubDate>Sun, 12 Oct 2025 18:31:51 +0000</pubDate></item><item><title>Completing a BASIC language interpreter in 2025</title><link>https://nanochess.org/ecs_basic_2.html</link><description>&lt;doc fingerprint="9f546b51621a25d4"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;head rend="h1"&gt;Completing a BASIC language interpreter in 2025&lt;/head&gt;
        &lt;div&gt;&lt;p&gt; This is a follow-up to my previous article &lt;/p&gt;Developing a BASIC language in 2025&lt;p&gt;, where I describe how I got inspired to start coding a new BASIC interpreter for the 1983 Mattel ECS add-on for Intellivision. &lt;/p&gt;&lt;/div&gt;
        &lt;p&gt; Although my interpreter was already pretty fast and with enough statements to build games, I wasn't satisfied because it still missed one thing that the ECS BASIC implements: text strings. Only three, A$, B$, and C$, with SET, GET and PUT, for things like assigning a string, getting a name from the keyboard, or showing a name. Each string a maximum of 20 characters. &lt;/p&gt;
        &lt;p&gt; I thought about strings for four days, then I decided to code things like I know what I was doing. I added a string stack pointer bas_strptr where any created string is added. &lt;/p&gt;
        &lt;p&gt; The first thing to implement was an array for the string variables (A$-Z$) each element pointing to the current string contained (or zero if none). I modified the whole of the expression parser to insert the type in the Carry flag (Clear = it is a number, Set = it is a string), then I made the first string support in the language where it detects if a string name appears (letter plus the $ sign) and reads it and copies it to a new string on the stack, returning this pointer as expression value (and of course the carry flag set) &lt;/p&gt;
        &lt;p&gt; The next step was assigning string variables, it simply took the pointer and stored it into the respective string variable pointer. Of course, I was afraid that I was creating a monster because I wasn't planning for the garbage collector. &lt;/p&gt;
        &lt;p&gt; Then I went full-steam ahead and put support in INPUT, PRINT, and added the concatenation of strings using the plus operator, also the functions ASC, CHR$, LEN, LEFT$, RIGHT$, MID$, INSTR, VAL, and STR$. By the way, the original Mattel ECS BASIC has none of these! &lt;/p&gt;
        &lt;p&gt; Now I had string support in my BASIC language for the ECS, but at some point in the execution it would fill up the stack, and crash with an "Out of Memory" error. &lt;/p&gt;
        &lt;head rend="h2"&gt;Garbage collection&lt;/head&gt;
        &lt;p&gt; It was kind of crazy having a BASIC with string support but no garbage collection. I needed a way to copy strings into the respective variable, and delete the work-in-progress strings created as expressions are evaluated. &lt;/p&gt;
        &lt;p&gt; It would be easy when having a C memory management system as you only have to replace the pointer, and free the original. But any memory management comes with headers and linked lists, extra memory requirements, and slowness. Given the Intellivision CP1610 processor is already slow enough (894 khz), I decided against it. &lt;/p&gt;
        &lt;p&gt; However, I noticed that temporary strings are only created inside the expression parser. So what about a double stack? One stack for strings in variables, and one stack for temporary strings. &lt;/p&gt;
        &lt;p&gt; I added a secondary pointer bas_strbase (I like how it sounds like star base) &lt;/p&gt;
        &lt;p&gt; At the start of each statement, bas_strbase is copied to bas_strptr (thus effectively erasing the temporary strings) A problem needed to be solved: growing bas_strbase on each string assignment. &lt;/p&gt;
        &lt;p&gt; I was going to implement the most simple solution: go over the 26 string variables doing comparison and movement of pointers, and insert the new string in its place. &lt;/p&gt;
        &lt;p&gt; Just as I was coding this, I noticed I had an easier solution. As I was working with 16-bit words, not all values are used. I could use a value like 0xcafe to mark a non-used space, and boom! I had an idea. &lt;/p&gt;
        &lt;p&gt; When doing the assignment, delete the original string (fill it with 0xcafe words), now explore the strbase area to find a string of 0xcafe words big enough to save the new string. &lt;/p&gt;
        &lt;p&gt; The better part is when there is no space for the string, I simply copy the string pointer as the new bas_strbase pointer (effectively growing the base memory area), and all words between the end of the string and the previous bas_strbase pointer (ahead in memory) are filled with 0xcafe words. &lt;/p&gt;
        &lt;p&gt; Full string support with garbage collection at very small price of performance. Exactly what a CP1610 processor needs. &lt;/p&gt;
        &lt;quote&gt; STRING_TRASH: EQU $CAFE ; ; String assign. ; R1 = Pointer to string variable. ; R3 = New string. ; string_assign: PROC PSHR R5 MVII #STRING_TRASH,R4 ; ; Erase the used space of the stack. ; MOVR R3,R2 MVI@ R2,R0 INCR R2 ADDR R0,R2 MVI bas_strbase,R0 CMPR R0,R2 BC @@3 @@4: MVO@ R4,R2 INCR R2 CMPR R0,R2 BNC @@4 @@3: ; ; Erase the old string. ; MVI@ R1,R2 TSTR R2 BEQ @@1 MVI@ R2,R0 MVO@ R4,R2 INCR R2 TSTR R0 BEQ @@1 @@2: MVO@ R4,R2 INCR R2 DECR R0 BNE @@2 ; ; Search for space at higher-addresses. ; @@1: MVII #start_strings-1,R2 CMP bas_strbase,R2 ; All examined? BNC @@6 ; Yes, jump. @@5: CMP@ R2,R4 ; Space found? BNE @@7 ; No, keep searching. CLRR R5 @@8: INCR R5 DECR R2 CMP bas_strbase,R2 BNC @@9 CMP@ R2,R4 BEQ @@8 @@9: INCR R2 MVI@ R3,R0 INCR R0 CMPR R0,R5 ; The string fits? BNC @@7 ; ; The string fits in previous space. ; MOVR R3,R4 MOVR R2,R5 MVO@ R2,R1 ; New address. @@10: MVI@ R4,R2 MVO@ R2,R5 DECR R0 BNE @@10 PULR PC @@7: DECR R2 CMP bas_strbase,R2 BC @@5 ; ; No space available. ; @@6: MVO R3,bas_strbase ; Grow space for string variables. MVO@ R3,R1 PULR PC ENDP &lt;/quote&gt;
        &lt;div&gt;
          &lt;p&gt; Example of a parser for a text adventure game using string functions. &lt;/p&gt;
        &lt;/div&gt;
        &lt;head rend="h2"&gt;Going mathematic&lt;/head&gt;
        &lt;p&gt; Since my floating-point library was complete with the four operations, I had an ace under the sleeve: I already had tested sin and cos functions with it, but for some reason these had a bug. For sin(1Â°) the resulting value was 0.0172. &lt;/p&gt;
        &lt;div&gt;&lt;p&gt; These functions were ported from my &lt;/p&gt;Pascal compiler for transputer&lt;p&gt;. As Pascal happens to have exactly the same mathematical function set as a BASIC interpreter. &lt;/p&gt;&lt;/div&gt;
        &lt;p&gt; After a whole day examining the operation instruction-by-instruction (the jzintv debugger shines here), I discovered that I did a comparison in the wrong way and corrected it. &lt;/p&gt;
        &lt;p&gt; I was so happy that I went immediately to port the remaining mathematical functions (ATN, TAN, LOG, EXP, and derived SQR and the power-of ^ operator). There were no pitfalls along the way, except one, my BASIC has a mantissa with one extra bit of precision, and EXP(LOG(64)) returned 63.999999 &lt;/p&gt;
        &lt;p&gt; Both operations use a multiplication with a constant (log does it at the end, and exp in the start). I noticed that the value was misrounded for 25 bits of mantissa, so I calculated a better constant, and et voila! EXP(LOG(64)) returned 64. &lt;/p&gt;
        &lt;head rend="h2"&gt;Making it easier for the user&lt;/head&gt;
        &lt;p&gt; A lot of BASIC interpreters in the eighties didn't supported instructions for graphics. The Commodore 64 was particularly known for requiring POKE for almost anything, unless you had the somewhat expensive Simon BASIC cartridge. &lt;/p&gt;
        &lt;p&gt; However, in the Intellivision you have few graphics capabilities. In the Color Stack mode you have something called Colored Squares. This means each tile on the screen (20x12) can have four colors. This means a bloxel resolution of 40x24, and each bloxel can have one of eight colors (one being the background). &lt;/p&gt;
        &lt;p&gt; I implemented PLOT with these limitations, and also added PRINT AT (for putting text at any screen position), and TIMER to measure time. &lt;/p&gt;
        &lt;p&gt; One of the most difficult things was implementing the floating-point number parsing. I finally decided to approach it like parsing an integer, taking note of the number of digits parsed, and take note of the position of a period. Once it reaches the biggest number it can represent (9,999,999) then it starts ignoring any further digit (but it keeps counting them) &lt;/p&gt;
        &lt;p&gt; The final calculation step is to multiply it, or divide it taking in account the period position. Also taking in account any exponent present (for example, e+1 or e-3) &lt;/p&gt;
        &lt;p&gt; It wasn't so expensive in computation time. I added along a FRE(0) function to know how much space remains for writing programs. &lt;/p&gt;
      &lt;/div&gt;
      &lt;head rend="h2"&gt;It is the eighties&lt;/head&gt;
      &lt;p&gt; Let's suppose we are working to make this BASIC interpreter really useful for the Mattel ECS. We still need two things: cassette, and printer. &lt;/p&gt;
      &lt;p&gt; Fortunately, a lot of people at Atariage Forums have worked along the years to decipher the ECS hardware (thanks to intvnut, lathe26, and decle) &lt;/p&gt;
      &lt;p&gt; The ECS contains the hardware to interface to a cassette recorder/player at 300 bauds with FSK (Frequency-Shift Keying) of 2400/4800 hz (technically this is a modem) and it also includes a UART (Universal Asynchronous Receiver/Transmitter) patterned losely after a Motorola MC6850 chip, but the frequency selector is separated, allowing to turn on/off a relay (cassette remote control), and to switch between two ports (the cassete and the AUX port for the printer) &lt;/p&gt;
      &lt;p&gt; Now for the cassette, I was going to use 300 bauds, this means around 30 characters per second. Do you remember your 56K modem? It was 186 times faster! I needed to optimize my BASIC as I was using token numbers above $0100, so I moved them to the area $0080-$00ff. Now all the words are only used in the lower 8 bits, and the tokenized program can be saved as bytes. &lt;/p&gt;
      &lt;div&gt;&lt;p&gt; I coded the cassette routines based on code published by decle in his article &lt;/p&gt;ECS Text Editor written in IntyBASIC with tape support&lt;p&gt; and added LOAD, SAVE, and VERIFY. &lt;/p&gt;&lt;/div&gt;
      &lt;p&gt; I was very happy when these cassette routines worked in emulation, and I ordered cables from Amazon for trying to record and play in my cellphone. &lt;/p&gt;
      &lt;p&gt; For some reason probably related to audio levels and automatic compression, I could record audio from the ECS in my cellphone, but playing it back never resulted in anything. &lt;/p&gt;
      &lt;p&gt; I was tired, and I decided to try my PC. I connected the ECS to the Mic In, and Line Out, and same problem. Besides the Windows utilities make amazingly hard to change the source and playing line. I got the Audacity program, and it has the line input/output options easily selectable. Again no results. &lt;/p&gt;
      &lt;p&gt; I wrote a small program to read the UART continuously, and I couldn't see anything. I decided to try the Audacity's amplify effect, and et voila! My UART program started throwing decoded bytes. I stopped the program, and I tried the VERIFY command (remember I had just saved the same program), but it didn't worked. Worst, when I ran again my test program, I didn't got any data! &lt;/p&gt;
      &lt;p&gt; I revised my setup values for the UART, but nothing. I was mystified for some hours until I got memories of a chip that basically went nuts if you accessed it too fast. Could it be that? Is the CP1610 so fast? I added a delay after every access to the UART chip. &lt;/p&gt;
      &lt;p&gt; I typed again my test program, I did SAVE, recorded on the PC, amplified it, I RUN my program, and played the audio back. Ok, UART was reading things. Now I stopped the test program, I did VERIFY, and I played the audio back from my PC. The longest 20 seconds of my life. And it worked! &lt;/p&gt;
      &lt;p&gt; Immediately I resetted the ECS, losing the program, and I did LOAD (of course, playing back the audio), and again it worked! &lt;/p&gt;
      &lt;div&gt;
        &lt;p&gt; My UART test program after a successful LOAD statement. &lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt;Notice that although it saves BASIC programs, these programs aren't compatible with the original ECS BASIC because it is a completely different BASIC language.&lt;/p&gt;
      &lt;div&gt;&lt;p&gt; Break time: As I couldn't lose programs anymore, I decided to test my BASIC language with a "real" long program. So I took Tim Hartnell's Giant Book of Computer Games (Mexican edition), and I typed the Reversi game. I had to adapt it, because my BASIC doesn't allow for multidimensional arrays, and the screen positioning. I found a few bugs in my interpreter (&lt;/p&gt;INPUT W$&lt;p&gt; still wasn't written with the garbage-collector support, and the variables weren't deleted properly on &lt;/p&gt;RUN&lt;p&gt;), but it was amazing to watch the Reversi game playing against me. I've put a WAV file recording &lt;/p&gt;here&lt;p&gt;. &lt;/p&gt;&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt; Reversi game from the Tim Hartnell's Giant Book of Computer Games book working with my BASIC interpreter for the Mattel ECS. &lt;/p&gt;
      &lt;/div&gt;
      &lt;head rend="h2"&gt;The printer is in the room&lt;/head&gt;
      &lt;div&gt;&lt;p&gt; After reading &lt;/p&gt;Aquarius Printer Technical Info and Reverse Engineering&lt;p&gt; and the &lt;/p&gt;jzintv ECS document&lt;p&gt;, I decided using the printer was very easy, and I went to buy a Mattel Aquarius sourced locally because it included the printer and some thermal paper. &lt;/p&gt;&lt;/div&gt;
      &lt;p&gt; While the printer was in shipping process, I implemented LLIST, and LPRINT. I modified the core of both statements to access the output through an indirect function. So you only change the pointer to target the screen or the printer. I detected here a bug in jzintv that prevents it from outputting the printer data to a file. &lt;/p&gt;
      &lt;p&gt; I got the Mattel Aquarius along the printer a few days later. I had to clean it because it was pretty dusty. The printer doesn't have the top cover that protected the paper roll, but it included a paper roll, and fortunately it still had the cylinder that helps the papel to roll. &lt;/p&gt;
      &lt;div&gt;
        &lt;p&gt; Mattel Aquarius computer with expansion board, two games, cables, and the Aquarius printer. &lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; I adjusted the paper, powered on the printer, and verified I could advance the paper (having a working motor is 90% of the printer). &lt;/p&gt;
      &lt;p&gt; I built the serial cable with the instructions from lathe26's article, and the first time it didn't worked (I grounded the CTS cable accidentally), but after correcting it, I expected trash for my first print, instead, I got a pretty nice printing! &lt;/p&gt;
      &lt;p&gt; Of course, I couldn't resist printing some listings, and a sine wave. Pretty speedy for a 1200 bauds printer. &lt;/p&gt;
      &lt;div&gt;
        &lt;p&gt; The printed source of my UFO game. The paper roll is really old. &lt;/p&gt;
      &lt;/div&gt;
      &lt;head rend="h2"&gt;What remains to do?&lt;/head&gt;
      &lt;p&gt; I added the DRAW and CIRCLE statements, and POINT functions to complete the graphics support. These are enough to make some nice games without using sprites. I made a graphics demo for filling the screen with lines, and I noticed my pseudo random number generator didn't covered the screen, so I had to improve it. &lt;/p&gt;
      &lt;div&gt;
        &lt;p&gt; DRAW program for my ECS BASIC interpreter. &lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; Also I added the POS and LPOS functions to know the horizontal position of the cursor. The SPC and TAB functions for PRINT. Plus a HEX$ function to ease system programming. &lt;/p&gt;
      &lt;p&gt; In the tokenization table, I added placeholders to expand the language and don't break compatibility with any cassette tape being created. &lt;/p&gt;
      &lt;p&gt; With this it has become a full-fledged BASIC interpreter for the Mattel ECS that uses 19 kilowords, instead of the 24 kilowords of the slow and limited Mattel ECS BASIC interpreter. &lt;/p&gt;
      &lt;p&gt; I don't see anything more I could do in the near future, except maybe expanding the editor to be a full-screen editor. Currently, it is a line editor that reads its input from the screen. &lt;/p&gt;
      &lt;p&gt; At this point, it is a fun experience the process of typing BASIC programs in the ECS, and watch the results back. You can save the programs, or print it. And of course, you can only imagine the success that Mattel Electronics would have enjoyed if they put together a good BASIC with its Mattel ECS. &lt;/p&gt;
      &lt;p&gt; Small statistics of the assembler code: &lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;basic.asm: 5333 lines.&lt;/item&gt;
        &lt;item&gt;fplib.asm: 718 lines.&lt;/item&gt;
        &lt;item&gt;fpio.asm: 462 lines.&lt;/item&gt;
        &lt;item&gt;fpmath.asm: 516 lines.&lt;/item&gt;
        &lt;item&gt;uart.asm: 341 lines.&lt;/item&gt;
        &lt;item&gt;Total of 7370 lines of assembler code written between Sep/17 and Oct/12, around 300 lines written daily.&lt;/item&gt;
      &lt;/list&gt;
      &lt;div&gt;&lt;p&gt; The source code is released at &lt;/p&gt;https://github.com/nanochess/ecsbasic&lt;p&gt;. I tried to release it so early as possible, so you can get a glance of how it was growing in the commits. &lt;/p&gt;&lt;/div&gt;
      &lt;div&gt;&lt;p&gt; Enjoy it! &lt;/p&gt;Did you like this article? Invite me a coffee on ko-fi! &lt;/div&gt;
      &lt;head rend="h2"&gt;Related links&lt;/head&gt;
      &lt;p&gt;Last modified: Oct/12/2025&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45560974</guid><pubDate>Sun, 12 Oct 2025 19:19:38 +0000</pubDate></item><item><title>Ask HN: What are you working on? (October 2025)</title><link>https://news.ycombinator.com/item?id=45561428</link><description>&lt;doc fingerprint="3768985329a5bbf8"&gt;
  &lt;main&gt;
    &lt;p&gt;Last year, PlasticList found plastic chemicals in 86% of tested foods—including 100% of baby foods they tested. Around the same time, the EU lowered its “safe” BPA limit by 20,000×, while the FDA still allows levels roughly 100× higher than Europe’s new standard.&lt;/p&gt;
    &lt;p&gt;That seemed solvable.&lt;/p&gt;
    &lt;p&gt;Laboratory.love lets you crowdfund independent lab testing of the specific products you actually buy. Think Consumer Reports × Kickstarter, but focused on detecting endocrine disruptors in your yogurt, your kid’s snacks, or whatever you’re curious about.&lt;/p&gt;
    &lt;p&gt;Find a product (or suggest one), contribute to its testing fund, and get full lab results when testing completes. If a product doesn’t reach its goal within 365 days, you’re automatically refunded. All results are published publicly.&lt;/p&gt;
    &lt;p&gt;We use the same ISO 17025-accredited methodology as PlasticList.org, testing three separate production lots per product and detecting down to parts-per-billion. The entire protocol is open.&lt;/p&gt;
    &lt;p&gt;Since last month’s “What are you working on?” post:&lt;/p&gt;
    &lt;p&gt;- 4 more products have been fully funded (now 10 total!)&lt;/p&gt;
    &lt;p&gt;- That’s 30 individual samples (we do triplicate testing on different batches) and 60 total chemical panels (two separate tests for each sample, BPA/BPS/BPF and phthalates)&lt;/p&gt;
    &lt;p&gt;- 6 results published, 4 in progress&lt;/p&gt;
    &lt;p&gt;The goal is simple: make supply chains transparent enough that cleaner ones win. When consumers have real data, markets shift.&lt;/p&gt;
    &lt;p&gt;1. An example result is "https://laboratory.love/product/117", which is a list of chemicals and measurements. Is there a visualization of how these levels relate to regulations and expert recommendations? What about a visualization of how different products in the same category compare, so that consumers know which brand is supposedly "best"? Maybe a summary rating, as stars or color-coded threat level?&lt;/p&gt;
    &lt;p&gt;2. If you find regulation-violating (or otherwise serious) levels of undesirable chemicals, do you... (a) report it to FDA; (b) initiate a class-action lawsuit; (c) short the brand's stock and then news blitz; or (d) make a Web page with the test results for people to do with it what they will?&lt;/p&gt;
    &lt;p&gt;3. Is 3 tests enough? On the several product test results I clicked, there's often wide variation among the 3 samples. Or would the visualization/rating tell me that all 3 numbers are unacceptably bad, whether it's 635.8 or 6728.6?&lt;/p&gt;
    &lt;p&gt;4. If I know that plastic contamination is a widespread problem, can I secretly fund testing of my competitors' products, to generate bad press for them?&lt;/p&gt;
    &lt;p&gt;5. Could this project be shut down by a lawsuit? Could the labs be?&lt;/p&gt;
    &lt;p&gt;1. I'm still working to make results more digestible and actionable. This will include the %TDI toggle (total daily intake, for child vs adult and USA vs EU) as seen on PlasticList, but I'm also tinkering with an even more consumer-friendly 'chemical report card'. The final results page would have both the card and the detailed table of results.&lt;/p&gt;
    &lt;p&gt;2. I have not found any regulation-violating levels yet, so in some sense, I'll cross that bridge when I get there. Part of the issue here is that many believe the FDA levels are far too relaxed which is part of why demand for a service like laboratory.love exists.&lt;/p&gt;
    &lt;p&gt;3. This is part of the challenge that PlasticList faced, and additionally a lot of my thinking around the chemical report card are related to this. Some folks think a single test would be sufficient to catch major red flags. I think triplicate testing is a reasonable balance of statistically robust while not being completely cost-prohibitive.&lt;/p&gt;
    &lt;p&gt;4. Yes, I suppose one could do that, as long as the funded products can be acquired by laboratory.love anonymously through their normal consumer supply chains. Laboratory.love merely acquires three separate batches of a given product from different sources, tests them at an ISO/IEC 17025-accredited lab, and publishes the data.&lt;/p&gt;
    &lt;p&gt;5. I suppose any project can be shut down by a lawsuit, but laboratory.love is not currently breaking any laws as far as I'm aware.&lt;/p&gt;
    &lt;p&gt;The UK levels are more strict and generally more up to date, which I personally follow rather than FDA. Could be nice to show those violations as a comparison to FDA.&lt;/p&gt;
    &lt;p&gt;I'll add subscriptions as a more formal option on laboratory.love soon!&lt;/p&gt;
    &lt;p&gt;Disclaimer: I don't think I can have a 365-day refund with a recurring donations like this. The financial infrastructure would add too much complexity.&lt;/p&gt;
    &lt;p&gt;Serious question: around 1900 meat was often preserved using formaldehyde, and milk was adulterated with water and chalk, and sometimes with pureed calf brains to simulate cream.&lt;/p&gt;
    &lt;p&gt;I hope we can agree that we are better off than that now.&lt;/p&gt;
    &lt;p&gt;What I'm curious about is whether you think it's been a steady stream of improvements, and we just need to improve further? Or if you think there was some point between 1900 and now where food health and safety was maximized, greater than either 1900 or now, and we've regressed since then?&lt;/p&gt;
    &lt;p&gt;Trying to collapse high dimensional, complex phenomena onto a single axis usually gives one a fake sense of certainty. One should avoid it as much as possible.&lt;/p&gt;
    &lt;p&gt;Where are you? This project is not necessarily limited to products that are available in the United States. Anything that can be shipped to the United States is still testable.&lt;/p&gt;
    &lt;p&gt;this looks so cool! I wish it told me if the levels found for tested products were good/bad - I have no prior reference so the numbers meant nothing to me&lt;/p&gt;
    &lt;p&gt;Given the current reach of the project (read: still small!), I suspect for awhile yet the majority of successfully funded testing will be by concerned individuals with expendable income. It is cheaper and much faster to go through laboratory.love than it would be to partner with a lab as an individual (plus the added bonus that all data is published openly).&lt;/p&gt;
    &lt;p&gt;I've yet to have any product funded by a manufacturer. I'm open to this, but I would only publish data for products that were acquired through normal consumer supply chains anonymously.&lt;/p&gt;
    &lt;p&gt;Both of them do measurements and YouTube videos. Neither one has a particularly good index of their completed reviews, let alone tools to compare the data.&lt;/p&gt;
    &lt;p&gt;I wish I could subscribe to support a domain like “loud speaker spin tests” and then have my donation paid out to these reviewers based on them publishing new high quality reviews with good data that is published to a common store.&lt;/p&gt;
    &lt;p&gt;I'm playing around with sandboxing techniques on Mac so I can isolate AI tools and prevent them from interacting with files they shouldn't have access to -- like all my dotfiles, AWS credentials, and such.&lt;/p&gt;
    &lt;p&gt;Along the way I rolled my own git-multi-hook solution (https://github.com/webcoyote/git-multi-hook) to use git hooks for shellcheck-ing, ending files with blank lines, and avoid committing things that shouldn't be in source control.&lt;/p&gt;
    &lt;p&gt;Yes, I've used docker and podman. They're great. But I wanted to be able to run Xcode and IOS simulator, which requires macOS, so developed these solutions.&lt;/p&gt;
    &lt;p&gt;I am working on making ultra-low cost freeze-dried enzymes for synthetic biology.&lt;/p&gt;
    &lt;p&gt;For example, 1 PCR reaction (a common reaction used to amplify DNA) costs about $1 each, and we're doing tons every day. Since it is $1, nobody really tries to do anything about it - even if you do 20 PCRs in one day, eh it's not that expensive vs everything else you're doing in lab. But that calculus changes once you start scaling up with robots, and that's where I want to be.&lt;/p&gt;
    &lt;p&gt;Approximately $30 of culture media can produce &amp;gt;10,000,000 reactions worth of PCR enzyme, but you need the right strain and the right equipment. So, I'm producing the strain and I have the equipment! I'm working on automating the QC (usually very expensive if done by hand) and lyophilizing for super simple logistics.&lt;/p&gt;
    &lt;p&gt;My idea is that every day you can just put a tube on your robot and it can do however many PCR reactions you need that day, and when the next day, you just throw it out! Bring the price from $1 each to $0.01 + greatly simplify logistics!&lt;/p&gt;
    &lt;p&gt;Of course, you can't really make that much money off of this... but will still be fun and impactful :)&lt;/p&gt;
    &lt;p&gt;As a bio hobbyist, this is fantastic! I don't do enough volume of PCR to think of it as expensive, but your use case of high-volume/automatic sounds fantastic! (And so many other types of reagents and equipment are very expensive).&lt;/p&gt;
    &lt;p&gt;Some things that would be cool&lt;/p&gt;
    &lt;p&gt;- Along your lines: In general, cheap automated setups for PCR and gels - Cheap/automatic quantifiable gels. E.g. without needing a kV supply capillary, expensive QPCR machines etc. - Cheaper enzymes in general - More options for -80 freezers - Cheaper/more automated DNA quantification. I got a v1 Quibit which gets the job done, but new ones are very expensive, and reagent costs add up. - Cheaper shaking incubator options. You can get cheap shakers and baters, but not cheap combined ones... which you need for pretty much everything. Placing one in the other can work, but is sub-optimal due to size and power-cord considerations. - More centrifuges that can do 10kG... this is the minimum for many protocols. - Ability to buy pure ethanol without outrageous prices or hazardous shipping fees.&lt;/p&gt;
    &lt;p&gt;- Not sure if this is feasible but... reasonable cost machines to synthesize oglios?&lt;/p&gt;
    &lt;p&gt;That sounds really cool. I wouldn't agree you can't make money off this, you can make money off anything, just find people who need this and it seems you did find it.&lt;/p&gt;
    &lt;p&gt;Anyhow good luck. Would love to follow if you do anything with this in the future. Do you have a blog or anything?&lt;/p&gt;
    &lt;p&gt;I found a neat way to do high-quality "semantic soft joins" using embedding vectors[1] and the Hungarian algorithm[2] and I'm turning it into an open source Python package:&lt;/p&gt;
    &lt;p&gt;It hits a sweet spot by being easier to use than record linkage[3][4] while still giving really good matches, so I think there's something there that might gain traction.&lt;/p&gt;
    &lt;p&gt;I see you saved a spot to show how to use it with an alternative embedding model. It would be nice to be able to use the library without an OpenAI api key. Might even make sense to vendor a basic open source model in your package so it can work out of the box without remote dependencies.&lt;/p&gt;
    &lt;p&gt;Yes, I'm planning out-of-the-box support for nomic[1] which can run in-process, and ollama which runs as a local server and supports many free embedding models[2].&lt;/p&gt;
    &lt;p&gt;If you're adding more LLM integration, a cool feature might be sending the results of allow_many="left" off to an LLM completions API that supports structured outputs. Eg imagine N_left=1e5 and N_right=1e5 but they are different datasets. You could use jellyjoin to identify the top ~5 candidates in right for each left, reducing candidate matches from 1e10 to 5e5. Then you ship the 5e5 off to an LLM for final scoring/matching.&lt;/p&gt;
    &lt;p&gt;I've been working on a 3D voxel-based game engine for like 10 years in my spare time. The most recent big job has been to port the world gen and editor to the GPU, which has had some pretty cute knock-on effects. The most interesting is you can hot-reload the world gen shaders and out pop your changes on the screen, like a voxel version of shadertoy.&lt;/p&gt;
    &lt;p&gt;I also wrote a metaprogramming language which generates a lot of the editor UI for the engine. It's a bespoke C parser that supports a small subset of C++, which is exposed to the user through a 'scripting-like' language you embed directly in your source files. I wrote it as a replacement for C++ templates and in my completely unbiased opinion it is WAY better.&lt;/p&gt;
    &lt;p&gt;Microlandia, the brutally honest city builder. Posting this for a second time, because i’ve been working super hard on a steam release.&lt;/p&gt;
    &lt;p&gt;last month’s “what are you working on” thread impulsed me to upload this game to itch and 1 month later, i’ve got a small community, lots of feedback and iterations. It brought a whole new life to a project that was on the verge of abandoning.&lt;/p&gt;
    &lt;p&gt;I've become a bit addicted to online education. I finished my first masters degree in Computer Science in July, and I started a masters in Mathematics from The Open University at the beginning of October. I've wanted to really get into the weeds of obscure and arguably-useless math for about as long as I can remember, and I figure that getting a masters in it is as good a way to get that knowledge as any way else.&lt;/p&gt;
    &lt;p&gt;Other than that, I've been doing a lot of fixing of tech debt in my home network from the last six years. I've admittedly kind of half-assed a lot of the work with my home router and my server and my NAS and I want these things to be done correctly. (In fairness to me, I didn't know what I was doing back when I started, and I'd like to think I know a fair bit better now).&lt;/p&gt;
    &lt;p&gt;For example, when I first built my server, I didn't know about ZFS datasets, so everything was on the main /tank mount. This works but there are advantages to having different settings for different parts of the RAID and as such I've been dividing stuff into datasets (which has the added advantage of "defragging" because this RAID has grown by several orders of magnitude and as a result some of the initial files were fragmented).&lt;/p&gt;
    &lt;p&gt;My on again, off again life's work has been a foss dev stack for interactive tutoring systems. Something like a general purpose Math Academy, with mechanics to permit UGC and courses that are both adaptive (to the user's background and demonstrated skill) and inter-adaptive (to the userbase's expressed priorities).&lt;/p&gt;
    &lt;p&gt;I am using this stack now to build an early literacy app targeting kids aged 3-5ish at https://letterspractice.com (also pre-release state, although the email waitlist works I think!). LLM assisted edtech has a lot of promise, but I'm pretty confident I can get the unit cost for teaching someone to read down to 5 USD or less.&lt;/p&gt;
    &lt;p&gt;Not sure what the market is for something like this but it's something I've been thinking a lot about since stepping down as CEO of my previous company.&lt;/p&gt;
    &lt;p&gt;My goal is two-fold:&lt;/p&gt;
    &lt;p&gt;1. Help teams make better, faster decisions with all context populating a source-of-truth.&lt;/p&gt;
    &lt;p&gt;2. Help leaders stay eyes-on, and circumstantially hands-on, without slowing everything down. What I'd hope to be an effective version of "Founder Mode".&lt;/p&gt;
    &lt;p&gt;If anybody wants to play around with it, here's a link to my staging environment:&lt;/p&gt;
    &lt;p&gt;This is a good nudge to choose the grammatically correct option, thank you.&lt;/p&gt;
    &lt;p&gt;I originally had "less meetings" before an LLM corrected me into using "fewer meetings". Then when talking about Orgtools to a couple people I heard them say "less meetings" and switched back thinking that sounds slightly more natural (but incorrect).&lt;/p&gt;
    &lt;p&gt;- You can precisely tweak every shade/tint so you can incorporate your own brand colors. No AI or auto generation!&lt;/p&gt;
    &lt;p&gt;- It helps you build palettes that have simple to follow color contrast guarantees by design e.g. all grade 600 colors have 4.5:1 WCAG contrast (for body text) against all grade 50 colors, such as red-600 vs gray-50, or green-600 vs gray-50.&lt;/p&gt;
    &lt;p&gt;- There's export options for plain CSS, Tailwind, Figma, and Adobe.&lt;/p&gt;
    &lt;p&gt;- It uses HSLuv for the color picker, which makes it easier to explore accessible color combinations because only the lightness slider impacts the WCAG contrast. A lot of design tools still use HSL, where the WCAG contrast goes everywhere when you change any slider which makes finding contrasting colors much harder.&lt;/p&gt;
    &lt;p&gt;- Check out the included example open source palettes and what their hue, saturation and lightness curves look like to get some hints on designing your own palettes.&lt;/p&gt;
    &lt;p&gt;It's probably more for advanced users right now but I'm hoping to simplify it and add more handholding later.&lt;/p&gt;
    &lt;p&gt;Really open to any feedback, feature requests, and discussing challenges people have with creating accessible designs. :)&lt;/p&gt;
    &lt;p&gt;I get that you say it is for advanced users, but I think a "how to use this" link with a video in it that explained a few things would probably open it up to a lot more users.&lt;/p&gt;
    &lt;p&gt;There's so much more to do with tools like this, and I'm really glad to see it.&lt;/p&gt;
    &lt;p&gt;Thanks for the feedback! Yeah, I appreciate there's a lot of background here around color palette design, UI design, color spaces, and accessibility so I likely need something like a video or tutorial. Another route is to have the tool start in a less freeform mode that handholds you through the process more.&lt;/p&gt;
    &lt;p&gt;Continuing to work on a Low Power FM community radio station for the East San Fernando Valley of Los Angeles. We have started promoting and putting on local events and are trying to fund raise to build out the station. Raising money is hard! We did a big show in Burbank where several hundred people showed up but we only netted $800 after expenses. :(&lt;/p&gt;
    &lt;p&gt;Since this is hackernews, i'll add that i'm building the website and archiving system using haskell and htmx, but what is currently live is a temp static html site. https://github.com/solomon-b/kpbj.fm&lt;/p&gt;
    &lt;p&gt;This might be a naive question which you've probably been asked plenty of times before so I'm sorry of I'm being tedious here.&lt;/p&gt;
    &lt;p&gt;Is it really worth the effort and expense to have a real radio station these days? Wouldn't an online stream be just as effective if it was promoted well locally?&lt;/p&gt;
    &lt;p&gt;A few years ago a friend who was very much involved in a local community group which I was also somewhat interested in asked me if I wanted to help build a low power FM station. He asked me because I know something about radio since I was into ham radio etc.&lt;/p&gt;
    &lt;p&gt;I was skeptical that it was worth the effort. The nerdy part of me would have enjoyed doing it but I couldn't help thinking that an online stream would probably reach as many people without the hassle and expensive of a transmitter, antenna etc.&lt;/p&gt;
    &lt;p&gt;I know it's a toss up. Every car has an FM radio. Not everyone is going to have a phone plugged in to Android Auto or Apple Car Play and have a good data plan and have a solid connection.&lt;/p&gt;
    &lt;p&gt;I also pointed out that the technical effort is probably the small part compared to producing interesting content.&lt;/p&gt;
    &lt;p&gt;1. Radio is COOL. As a fellow ham I think you would agree with me on this one so I'll leave it at that.&lt;/p&gt;
    &lt;p&gt;2. Internet streaming gives you wider but far less localized audience. We will have an internet stream, but being radio first shifts the focus to local community and local content.&lt;/p&gt;
    &lt;p&gt;3. Internet streaming and radio have related but not entirely overlapping histories and contexts which impacts how people produce and consume their content. I love the traditional formats of radio and they are often completely missing in online radio which IMO models itself more often on mixtape and club DJ culture.&lt;/p&gt;
    &lt;p&gt;4. AI slop is ruining the world. I have this belief that as AI slop further conquers the internet we are going to get to a place where nobody trusts internet content. People will seek out novelty and authenticity (sort of how LLMs do lol) and I think there will be a return to local content and community.&lt;/p&gt;
    &lt;p&gt;5. Commercial radio sucks. The LPFM system is a wonderful opportunity to create a strong, community driven alternative to corporate media.&lt;/p&gt;
    &lt;p&gt;Radio is so much fun to learn. It’s liberating to learn for curiosity and joy rather than commercialization. The community is welcoming, and while not directly translatable for most paid work, it does teach general problem solving skills.&lt;/p&gt;
    &lt;p&gt;Taking a break from tech to work on a luxury fashion brand with my mum. She hand paints all the designs. I it first collection is a set of silk scarves and we’re moving into skirts and jackets soon.&lt;/p&gt;
    &lt;p&gt;Been a wonderful journey to connect with my mum in this way. And also to make something physical that I can actually touch. Tech seems so…ephemeral at times&lt;/p&gt;
    &lt;p&gt;this is super cool. congrats and best of luck with it! Love the mother &amp;amp; son backstory to the product. The scarves look like they could make a great gift as well. I'll bookmark your website.&lt;/p&gt;
    &lt;p&gt;I’ve been casually getting into thrifting and realized pretty quickly that Lens is super limited in its functionality and is mostly a shopping app. I put a site together that is like a supercharged version of Lens for thrifters where you can get info on price, demand, and condition. Share function is borked atm tho&lt;/p&gt;
    &lt;p&gt;It currently supports complex heatmaps based on travel time (e.g. close to work + close to friends + far from police precincts), and has a browser extension to display your heatmap over popular listing sites like Zillow.&lt;/p&gt;
    &lt;p&gt;I'm thinking of making it into an API to allow websites to integrate with it directly.&lt;/p&gt;
    &lt;p&gt;Living in hongkong for a few months, and absolutely love exploring the different neighborhoods. I’d love something like this or walkscore but for local guides to contribute.&lt;/p&gt;
    &lt;p&gt;Absolutely stellar! I've been looking for something like this for ages. Any chance you'll have some pre- defined options like grocery stores, libraries, airport, etc?&lt;/p&gt;
    &lt;p&gt;I’m currently working on https://www.dreamly.in - automated, personalized, and localized bedtime stories for kids.&lt;/p&gt;
    &lt;p&gt;My daughter loves stories, and I often struggled to come up with new ones every night. I remember enjoying local folk tales and Indian mythological stories from my childhood, and I wanted her to experience that too — while also learning new things like basic science concepts and morals through stories.&lt;/p&gt;
    &lt;p&gt;So I built Dreamly and opened it up to friends and families. Parents can set up their child’s profile once - name, age, favorite shows or characters, and preferred themes (e.g. morals, history, mythology, or school concepts). After that, personalized stories are automatically delivered to their inbox every night. No more scrambling to think of stories on the spot!&lt;/p&gt;
    &lt;p&gt;I've been vanlifing for a few months now. I tend to have long hours on the road where my mind wonders and I want to write code hands-free.&lt;/p&gt;
    &lt;p&gt;So, I built it.&lt;/p&gt;
    &lt;p&gt;Using ChatGPT's voice agents to generate Github issues tagging @claude to trigger Claude Code's Github Action, I created https://voicescri.pt that allows me to have discussions with the voice agent, having it create issues, pull requests, and logical diffs of the code generated all via voice, hands free, with my phone in my pocket.&lt;/p&gt;
    &lt;p&gt;Redesigning investment holdings for wider screens and leaning on hotwired turbo frames. Thankful for once-campfire as a reference for how to structure the backend. The lazy loading attribute works great with css media queries to display more on larger viewports.&lt;/p&gt;
    &lt;p&gt;Enjoying learning modern css in general. App uses tailwind, but did experiment with just css on the homepage. Letting the design emerge organically from using it daily, prototype with tailwind, then slim it back down with plain css.&lt;/p&gt;
    &lt;p&gt;Eidetica is a decentralized database project that I've been working on that is finally in a somewhat usable state. It basically wraps CRDTs into a close to normal Database interface, with decentralized authentication, background sync, etc.&lt;/p&gt;
    &lt;p&gt;I'm working on a DSL and browser-based playground for procedural 3D geometry called Geotoy: https://3d.ameo.design/geotoy&lt;/p&gt;
    &lt;p&gt;It's largely finished and functional, and I'm now focused on polish and adding additional builtin functions to expand its capabilities. I've been integrating different geometry libraries and kernels as well as writing some of my own.&lt;/p&gt;
    &lt;p&gt;I've been stress-testing it by building out different scenes from movies or little pieces of buildings on Google Maps street view - finding the sharp edges and missing pieces in the tool.&lt;/p&gt;
    &lt;p&gt;My hope is for Geotoy to be a relatively easy-to-learn tool and I've invested significantly in good docs, tutorials, and other resources. Now my goal is to ensure it's something worth using for other people.&lt;/p&gt;
    &lt;p&gt;I'm making an app for self-tracking. Combining elements from habit trackers, health logging and journaling. Built for rich customization and local-first. Want to be free of rigid structures of many existing apps while providing a better UX / usability than using a spreadhsheet.&lt;/p&gt;
    &lt;p&gt;I’m creating an electronic avionics sensor and display for experimental aircraft. I’m having a fantastic time learning about circuits and MCUs (I have a pure CS degree, zero background with EE stuff). I’ve been working on this in my off hours for over a year now, maybe someday it will be a product that people buy!&lt;/p&gt;
    &lt;p&gt;The current challenge is the display. I’ve struggled to learn about this part more than any other. After studying DVI and LVDS, and after trying to figure out what MIPI/DSI is all about, I think parallel RGB is the path forward, so I’ve just designed a test PCB for that, and ordered it from JLCPCB’s PCBA service.&lt;/p&gt;
    &lt;p&gt;I wanted to build my own speech-to-text transcription program [1] for Discord, similar to how zoom or google hangouts works. I built it so that I can record my group's DND sessions and build applications / tools for VTTs (Virtual TableTop gaming).&lt;/p&gt;
    &lt;p&gt;It can process a set of 3-hour audio files in ~20 mins.&lt;/p&gt;
    &lt;p&gt;I'm working on a open-source tool to create photo galleries from a folder of photos: https://simple.photo. It creates galleries as static sites that are easy to self-host.&lt;/p&gt;
    &lt;p&gt;I started this out of frustration that there is no good tool I could use to share photos from my travel and of my kids with friends and family. I wanted to have a beautiful web gallery that works on all devices, where I can add rich descriptions and that I could share with a simple link.&lt;/p&gt;
    &lt;p&gt;Turned out more people wanted this (got 200+ GitHub stars for the V1) so I recently released the V2 and I'm working on it with another dev. Down the road we plan a SaaS offer for people that don't want to fiddle with the CLI and self-host the gallery.&lt;/p&gt;
    &lt;p&gt;Like the layout tiles you have for the photo thumbnails. Will dig through and learn some css. Have struggled with different size content to create a compact masonry layout.&lt;/p&gt;
    &lt;p&gt;An open source website I built to explain tensor functions in PyTorch: https://whytorch.org&lt;/p&gt;
    &lt;p&gt;It makes tricky functions like torch.gather and torch.scatter more intuitive by showing element-level relationships between inputs and outputs.&lt;/p&gt;
    &lt;p&gt;For any function, you can click elements in the result to see where they came from, or elements in the inputs to see how they contribute to the result to see exactly how it contributes to the result. I found that visually tracing tensor operations clarifies indexing, slicing, and broadcasting in ways reading that the docs can't.&lt;/p&gt;
    &lt;p&gt;You can also jump straight to WhyTorch from the PyTorch docs pages by modifying the base URL directly.&lt;/p&gt;
    &lt;p&gt;I launched a week or two back and now have the top post of all time on r/pytorch, which has been pretty fun.&lt;/p&gt;
    &lt;p&gt;This really nice. For `torch.mul(x, y)`, it would be nice if it highlighted the entire row or column in the other matrix and result. Right now it shows only a single multiplication, which gives a misleading impression of how matrix multiply works. I wouldn't mention it, except that matrix multiplication is so important that it's worth showcasing. I've bookmarked the site and will share it at a pytorch training session I'm leading in a couple of weeks.&lt;/p&gt;
    &lt;p&gt;It's a sync infra product that is meant to cut down 6 months of development time, and years of maintenance of deep CRM sync for B2B SaaS.&lt;/p&gt;
    &lt;p&gt;Every Salesforce instance is a unique snowflake. I am moving that customization into configuration and building a resilient infrastructure for bi-directional sync.&lt;/p&gt;
    &lt;p&gt;I’m currently building YTVidHub—a tool that focuses on solving a very specific, repetitive workflow pain for researchers and content analysts.&lt;/p&gt;
    &lt;p&gt;The Pain Point: If you are analyzing a large YouTube channel (e.g., for language study, competitive analysis, or data modeling), you often need the subtitle files for 50, 100, or more videos. The current process is agonizing: copy-paste URL, click, download, repeat dozens of times. It's a massive time sink.&lt;/p&gt;
    &lt;p&gt;My Solution: YTVidHub is designed around bulk processing. The core feature is a clean interface where you can paste dozens of YouTube URLs at once, and the system intelligently extracts all available subtitles (including auto-generated ones) and packages them into a single, organized ZIP file for one-click download.&lt;/p&gt;
    &lt;p&gt;Target Users: Academic researchers needing data sets, content creators doing competitive keyword analysis, and language learners building large vocabulary corpora.&lt;/p&gt;
    &lt;p&gt;The architecture challenge right now is optimizing the backend queuing system for high-volume, concurrent requests to ensure we can handle large batches quickly and reliably without hitting rate limits.&lt;/p&gt;
    &lt;p&gt;It's still pre-launch, but I'd love any feedback on this specific problem space. Is this a pain point you've encountered? What's your current workaround?&lt;/p&gt;
    &lt;p&gt;How coincidental - I needed exactly this just a couple days ago. I ended up vibecoding a script to feed an individual URL into yt-dlp then pipe the downloaded audio through Whisper - not quite the same thing as it's not downloading the _actual_ subtitles but rather generating its own transcription, but similar. I've only run it on a single video to test, but it seemed to work satisfactorily.&lt;/p&gt;
    &lt;p&gt;I haven't upgraded to bulk processing yet, but I imagine I'd look for some API to get "all URLs for a channel" and then process them in parallel.&lt;/p&gt;
    &lt;p&gt;That is some fantastic validation, thank you! It’s cool to hear you already vibecoded a solution for this.&lt;/p&gt;
    &lt;p&gt;You've basically hit on the two main challenges:&lt;/p&gt;
    &lt;p&gt;Transcription Quality vs. Official Subtitles: The Whisper approach is brilliant for videos without captions, but the downside is potential errors, especially with specialized terminology. YTVidHub's core differentiator is leveraging the official (manual or auto-generated) captions provided by YouTube. When accuracy is crucial (like for research), getting that clean, time-synced file is essential.&lt;/p&gt;
    &lt;p&gt;The Bulk Challenge (Channel/Playlist Harvesting): You're spot on. We were just discussing that getting a full list of URLs for a channel is the biggest hurdle against API limits.&lt;/p&gt;
    &lt;p&gt;You actually mentioned the perfect workaround! We tap into that exact yt-dlp capability—passing the channel or playlist link to internally get all the video IDs. That's the most reliable way to create a large batch request. We then take that list of IDs and feed them into our own optimized, parallel extraction system to pull the subtitles only.&lt;/p&gt;
    &lt;p&gt;It's tricky to keep that pipeline stable against YouTube’s front-end changes, but using that list/channel parsing capability is definitely the right architectural starting point for handling bulk requests gracefully.&lt;/p&gt;
    &lt;p&gt;Quick question for you: For your analysis, is the SRT timestamp structure important (e.g., for aligning data), or would a plain TXT file suffice? We're optimizing the output options now and your use case is highly relevant.&lt;/p&gt;
    &lt;p&gt;Good luck with your script development! Let me know if you run into any other interesting architectural issues.&lt;/p&gt;
    &lt;p&gt;- Working on Kanji Palace (https://kanjipalace.com): We're going to publish the iOS app on the App Store and adding vocabulary. Currently, the app converts single Kanji (e.g., 生) into vivid mnemonic images. We aim to support vocabulary like 先生.&lt;/p&gt;
    &lt;p&gt;- Writing a book about Claude Code, not just for assisted programming, but as a general AI agent framework.&lt;/p&gt;
    &lt;p&gt;Building a tool that automatically generates living infrastructure diagrams from your IaC files and turns them into real-time incident dashboards. Think Figma meets Datadog - beautiful visualization that updates during outages to show you exactly what's failing and how to fix it.&lt;/p&gt;
    &lt;p&gt;The insight: your architecture diagram shouldn't be a stale PNG in Confluence. It should be your war room during incidents.&lt;/p&gt;
    &lt;p&gt;Going to be available as both web app and native desktop.&lt;/p&gt;
    &lt;p&gt;I’m building SPARK (Signal Processing Algorithms, Routines, and Kernels), an open-source library of modular, efficient DSP components for low-power embedded audio systems.&lt;/p&gt;
    &lt;p&gt;The goal is to make it straightforward to design and deploy small, composable audio graphs that fit on MCUs and similar hardware. The project is in its infancy, so there’s plenty of room for experimentation and contributions.&lt;/p&gt;
    &lt;p&gt;An experimental mesh network protocol, that is still very much pre alpha and missing some features.&lt;/p&gt;
    &lt;p&gt;The big thing I wanted to try is automatic global routing via MQTT.&lt;/p&gt;
    &lt;p&gt;Everything is globally routable. You can roam around between gateway nodes, as long as all the gateways are on the same MQTT server.&lt;/p&gt;
    &lt;p&gt;And there's a JavaScript implementation that connects directly to MQTT. So you can make a sensor, go to the web app, type the sensor's channel key, and see the data, without needing to create any accounts or activate or provision anything.&lt;/p&gt;
    &lt;p&gt;Nice! I recently built an invoice generator (not open sourced) for my own needs. I built mine because I needed something when I discontinued a SaaS that had provided it. Mine is written in C# and uses a JSON file to define the contents of the invoice. It's run from the command-line and just produces the PDF.&lt;/p&gt;
    &lt;p&gt;I wonder if you could just send invoices to Comcast for price increases to their Payable Accounts department and if they'd just pay them. Or just invoice companies for "inconvenience fees" of sorts when they actually create inconveniences.&lt;/p&gt;
    &lt;p&gt;Are you planning to turn this into a full-fledged CRM of some sort? Are you planning to add user login with templates/company fields auto-populated at one point? Looks very clean, congrats.&lt;/p&gt;
    &lt;p&gt;Why would you do something like this instead of using a cheap script from a codecanyon-type website (a true CRUD crm) where you can collect customer data and provide complete service in the long run? Just saying this because you said you built it for your own use.&lt;/p&gt;
    &lt;p&gt;I actually hadn’t heard of Codecanyon before! I used to use a paid invoicing service, but these days I just need a simple way to generate invoice PDFs - that’s really all I need.&lt;/p&gt;
    &lt;p&gt;You can use invoice generators that have complete control over your customers. Most scripts are php, and if you want something very detailed I'd go with Perfex. Codecanyon is the biggest code marketplace on the internet, owned by Envato.&lt;/p&gt;
    &lt;p&gt;It's an API that allows zero-knowledge proofs to be generated in a streaming fashion, meaning ZKPs that use way less RAM than normal.&lt;/p&gt;
    &lt;p&gt;The goal is to let people create ZKPs of any size on any device. ZKPs are very cool but have struggled to gain adoption due to the memory requirements. You usually need to pay for specialized hardware or massive server costs. Hoping to help fix the problem for devs&lt;/p&gt;
    &lt;p&gt;Fwiw: the website is brand new and very much in the "hot garbage" phase of development. I'm not a front-end guy, so critique is welcome from all - especially any bugs in the UX. I'm still actively uncovering them&lt;/p&gt;
    &lt;p&gt;I am working on a platform to help user to enrich their data by AI. so that AI can understand their Data more, especially for ChatGPT. Also it's easy to host a data and publish a MCP for ChatGPT.&lt;/p&gt;
    &lt;p&gt;The challenge is how ChatGPT can understand your "query" or say "prompts"? Raw data is not good enough - so I try to use a term called "AI Understanding Score" to measure it: https://senify.ai/ai-understanding-score. I think this index will help user to build more context so that AI can know more and answer with correct result.&lt;/p&gt;
    &lt;p&gt;This is very early work without every detail considered, really would like to have your feedback and suggestions.&lt;/p&gt;
    &lt;p&gt;- hiragana / katakana / number / time reading quizzes&lt;/p&gt;
    &lt;p&gt;- vocabulary quizzes based on wordlists you define and build&lt;/p&gt;
    &lt;p&gt;- learn and practice kanji anki-style (using FSRS algo)&lt;/p&gt;
    &lt;p&gt;- the coolest feature (imo) is a "reader": upload Japanese texts (light novels, children's books, etc), then translate them to your native language to practice your reading comprehension. Select text anywhere on the page (with your cursor) to instantly do a dictionary lookup. A LLM evaluates your translation accuracy (0..100%) and suggests other possible interpretations.&lt;/p&gt;
    &lt;p&gt;I just revamped the UI look and feel the other day after implementing some other user feedback! I'm now exploring ads as a way to monetize it.&lt;/p&gt;
    &lt;p&gt;- A front-end library that generates 10kb single-html-file artifacts using a Reagent-like API and a ClojureScript-like language. https://github.com/chr15m/eucalypt&lt;/p&gt;
    &lt;p&gt;- Beat Maker, an online drum machine. I'm adding sample uploads now with a content accessible storage API on the server. https://dopeloop.ai/beat-maker&lt;/p&gt;
    &lt;p&gt;- Tinkering with Nostr as a decentralized backend for simple web apps.&lt;/p&gt;
    &lt;p&gt;For context, I'm a UX Designer at a low-code company. LLMs are great at cranking out prototypes using well-known React component libraries. But lesser known low-code syntax takes more work. We made an MCP server that helps a lot, but what I'm working on now is a set of steering docs to generate components and prototypes that are "backwards compatible" with our bespoke front end language. This way our vibe prototyping has our default look out of the box and translates more directly to production code. https://github.com/pglevy/sail-zero&lt;/p&gt;
    &lt;p&gt;I'm trying to figure out how modern internal API management should work like and started https://www.appear.sh/.&lt;/p&gt;
    &lt;p&gt;After spending so much of my career dealing with APIs and building tooling for that I feel there's huge gap between what is needed and possible vs how the space generally works. There's a plethora of great tools that do one job really well, but when you want to use them the integration will kill you. When you want to get your existing system in them it takes forever. When you want to connect those tools that takes even longer.&lt;/p&gt;
    &lt;p&gt;The reality I'm seeing around myself and hearing from people we talk to is that most companies have many services in various stages of decay. Some brand new and healthy, some very old, written by people who left, acquired from different companies or in languages that were abandoned. And all of that software is still generating a lot of value for the company and to be able to leverage that value APIs are essential. But they are incredibly hard and slow to use, and the existing tools don't make it easier.&lt;/p&gt;
    &lt;p&gt;I'm attempting to work on a "spiritual successor" to Dramatica Story Expert, a crazy story theory/brainstorming program of days gone by. Technically, Dramatica is still around, but they never made a 64-bit version for Macs, and both the Mac and Windows version have been tenaciously clinging to the trailing edge of technology for decades. (The Mac version somehow never got retina fonts. I'm not sure how you even do that.)&lt;/p&gt;
    &lt;p&gt;I started my program in Swift and SwiftUI, although for various reasons I'm starting to look at Dart and Flutter (in part because being multiplatform would be beneficial, and in part because I am getting the distinct feeling this program is more ambitious than where SwiftUI is at currently). It isn't a direct port of Dramatica by any stretch, instead drawing on what I've learned writing my own novels, getting taught by master fiction writers, and being part of writing workshops. But no other program that I've seen uses Dramatica's neatest concepts, other than Subtxt, a web-based, AI-focused app which has recently been anointed Dramatica's official successor. (It's a neat concept, but it's very expensive compared to the original Dramatica or any other extant "fiction plotting" program. Also, there's a space for non-AI software here, I suspect: there are a lot of creatives who are adamantly opposed to it in any form whatsoever.)&lt;/p&gt;
    &lt;p&gt;I built a website that lets you browse Pokemon ENS (Ethereum Name Service) names, view their registration statuses and recent sales. It's a small but engaged niche&lt;/p&gt;
    &lt;p&gt;My personal website/webring. It's mostly a collection of ideas I've been mulling over and holding off on due to not being able to iterate on them fast enough. Nowadays thanks to AI, a lot of these a short errands so it's been a fun few weeks. I've also started chucking a few previous side projects under more unified domains. [1][2]&lt;/p&gt;
    &lt;p&gt;Also working on a youtube channel [3] for my climbing/travel videos, but the dreary state of that website has me wondering whether it's worth it, tbh. I haven't been able to change my channel name after trying for weeks. It's apparently the best place to archive edited GoPro footage at least.&lt;/p&gt;
    &lt;p&gt;It is a tool that lets you create whiteboard explainers.&lt;/p&gt;
    &lt;p&gt;You can prompt it with an idea or upload a document and it will create a video with illustrations and voiceover. All the design and animations are done by using AI apis, you dont need any design skills.&lt;/p&gt;
    &lt;p&gt;Here is a video explainer of the popular "Attention is all you need" paper.&lt;/p&gt;
    &lt;p&gt;I really like the idea! One issue though is that the content seems to "stream" much slower than what's being spoken. The result is that I'm sitting there waiting to see whats going to come, even though its already been said which makes it hard to focus on whatever new information is coming.&lt;/p&gt;
    &lt;p&gt;The animations / drawings themselves are solid too. I think there's more to play with wrt the dimensions and space of the background. It would be nice to see it zoom in and out for example.&lt;/p&gt;
    &lt;p&gt;https://lustroczynszowe.pl/ The aggregator of rental values in Poland. We want to increase the transparency of the real estate market, empowering consumers and enabling them to make fully informed financial decisions. We will also suggest savings on specific fields.&lt;/p&gt;
    &lt;p&gt;I've been working on https://booplet.com. It's like Lovable but for desktop apps and heavily inspired by Robin Sloan's home-cooked app essay [1][2]. The idea is to let anyone, especially non-technical folks, build and use personal apps. Instead of cloud deployment, we focused on a local-first setup so that users can fully own their apps and data.&lt;/p&gt;
    &lt;p&gt;Shipping pets and animals across borders is a big problem, and we are building the operating system to solve it at scale. If you are a vet (or work in the veterinary space), we would love to talk to you.&lt;/p&gt;
    &lt;p&gt;For fun, playing with Meshtastic https://meshtastic.org/ and contributing to the open source firmware and apps. They have something cool but need lots of help. I've patched 3 memory leaks and had a few other PRs merged already.&lt;/p&gt;
    &lt;p&gt;For work, https://heyoncall.com/ as the best tool for on-call alerting, website monitoring, cron job monitoring, especially for small teams and solo founders.&lt;/p&gt;
    &lt;p&gt;I guess they both fall under the category of "how do you build reliable systems out of unreliable distributed components" :)&lt;/p&gt;
    &lt;p&gt;Currently working on Note Cargo, basically self-hosted Markdown note-taking app, but I tried to not using database. So similar to Obsidian/Logseq but its web-based.&lt;/p&gt;
    &lt;p&gt;And currently working to make things shareable, also don't want to use database.&lt;/p&gt;
    &lt;p&gt;They’re always on. They log into real sites, click around, fill out forms, and adapt when pages change — no brittle scripts, no APIs needed. You can deploy one in minutes, host it yourself, and watch it do work like a human (but faster, cheaper, never tired).&lt;/p&gt;
    &lt;p&gt;Kind of like a “browser-use cloud,” except it’s yours — open, self-hostable, and way more capable.&lt;/p&gt;
    &lt;p&gt;i got a side project mirubato https://mirubato.com/, a web app tracking instrument practice logs. there were wild ideas such as enable AI training, grading, managing score, practice plans, and such, but in the end, i removed most of features. not only because it takes more time (i am only using a part of my free time to work on this) and effort, talent, plannings, but also because during vibe (yes, most of coding done by claude code) i realized that it still requires ultra deep thinking to design the minimal minimal UI i would like.&lt;/p&gt;
    &lt;p&gt;now the foundation is done, i've learnt a lot. i'm actually eating dog food by using it to track my own classical guitar practice everyday. i am pausing a while to process the requirements by ultra deep thinking to understand what would be helpful and how to shape the product.&lt;/p&gt;
    &lt;p&gt;LLMs such as codex and claude code definitely helped a lot, but I guess human beings' opinions would be more helpful - after all, the tool is made for humans instead of being used by claude code.&lt;/p&gt;
    &lt;p&gt;I would also like to hear when you start a project, if you know your audience are not super close to AI, would you still consider to enable the AI feature for them?&lt;/p&gt;
    &lt;p&gt;Having migraines on and off the past few months, I wanted a way to try and narrow down triggers. All the existing apps out there were overly complicated. So I built something simpler.&lt;/p&gt;
    &lt;p&gt;It’s an iOS app to help tracking events and stats about my day as simple dots. How many cups of coffee? Did I take my supplements? How did I sleep? Did I have a migraine? Think of it like a digital bullet journal.&lt;/p&gt;
    &lt;p&gt;Then visualizing all those dots together helps me see patterns and correlations. It’s helped me cut down my occurrence of migraines significantly. I’m still just in the public beta phase but looking forward to a full release fairly soon.&lt;/p&gt;
    &lt;p&gt;Would love to hear more feedback on how to improve the app!&lt;/p&gt;
    &lt;p&gt;This is great! I can see this useful across a variety of self-assessment things: - I’m tired often, are there certain patterns that align with that? - I’m feeling anxious, what events in a day (or other inputs) align with that?&lt;/p&gt;
    &lt;p&gt;I'm building Monadic DNA explorer, a tool to explore thousands of genetic traits from GWAS Catalog in your browser and plug in your own DNA data from 23andMe, Ancestry, etc. All processing happens locally on your machine and AI insights are run in a private LLM inside a TEE.&lt;/p&gt;
    &lt;p&gt;I'm working on Teletable (https://teletable.app), a macOS app that shows live football &amp;amp; F1 standings/results with a teletext interface (think BBC Ceefax). It's free and on the appstore:&lt;/p&gt;
    &lt;p&gt;Collecting public datasets for training visual AI models to track and target drones.&lt;/p&gt;
    &lt;p&gt;Drones are real bastards - there's a lot of startups working on anti drone systems and interceptors, but most of them are using synthetic data. The data I'm collecting is designed to augment the synthetic data, so anti drone systems are closer to field testing&lt;/p&gt;
    &lt;p&gt;I'm working on Veila, a privacy‑first AI chat service. I wanted something that prevents model providers from profiling users and linking information from chats to their identity.&lt;/p&gt;
    &lt;p&gt;I'm a robotics engineer by training, this is my first public launch of a web app.&lt;/p&gt;
    &lt;p&gt;- What it is: - Anonymous AI chat via a privacy proxy (provider sees our server, not your IP or account info) - End‑to‑end encrypted history, keys derived from password and never leave your device - Pay‑as‑you‑go; switch models mid‑chat (OpenAI now; Claude, Gemini and others planned) - Practical UX: sort chats into folders, Markdown, copyable code blocks, mobile‑friendly - Notes/limits: - Not self‑hosted: prompts go to third‑party APIs - If you include identifying info, upstream sees it - Prompts take a bit long sometimes, because reasoning is set to "medium" for now. Plan to make this adjustable in the future. - Looking for feedback: - What do you need to trust this? Open source? Independent audit? - Gaps in the threat model I'm missing - Which UI features and AI models you'd want next - Any UX rough edges (esp. mobile) - Learn more: - Compare Veila to ChatGPT, Claude, Gemini, etc. (best viewed on desktop): https://veila.ai/docs/compare.html - Discord: https://discord.gg/RcrbZ25ytb - More background: https://veila.ai/about.html&lt;/p&gt;
    &lt;p&gt;Hmm. I can't say for others, but I can tell you what would work for me given that I might meet some the criteria of desired audience for this.&lt;/p&gt;
    &lt;p&gt;In this space, it is more about trust and what you have done in the past more than anything else. Audits and whatnot are nice, but I need to be able to trust that your decisions will be sound. Think how Steam's Gabe gained his reputation. Not exactly easy feat these days.&lt;/p&gt;
    &lt;p&gt;Thanks for sharing this! Fully agree that trust is key, normally being on the user side of privacy-focussed services myself. Open source can help build this trust, but it would be ideal to have a way to make what is actually running on and being served by the servers transparent.&lt;/p&gt;
    &lt;p&gt;I'd love to hear your feedback if you get around to test Veila, e.g. on hey@veila.ai.&lt;/p&gt;
    &lt;p&gt;Hi HN, I am working on Circuitscript, a language based on python to describe electronic schematics: https://circuitscript.net/. A basic IDE (called the Bench) to try Circuitscript is available online: https://bench.circuitscript.net/&lt;/p&gt;
    &lt;p&gt;Since the last month, I have created a complete schematic with Circuitscript, exported the netlist to pcbnew and designed the PCB. The boards have been produced and currently waiting for them to be delivered to verify that it works. Quite excited since this will be the first design ever produced with Circuitscript as the schematic capture tool!&lt;/p&gt;
    &lt;p&gt;The motivation for creating Circuitscript is to describe schematics in terms of code rather than graphical UIs after using different CAD packages extensively (Allegro, Altium, KiCAD) in the past. I wanted to spend more time thinking about the schematic design itself rather than fiddling around with GUIs.&lt;/p&gt;
    &lt;p&gt;The main language goals are to be easy to write and reason, generated graphical schematics should be displayed according to how the designer wishes so (because this is also part of the design process) and to encourage code reuse.&lt;/p&gt;
    &lt;p&gt;Please check it out and I look forward to your feedback, especially from electronics designers/hobbyists. Thanks!&lt;/p&gt;
    &lt;p&gt;Our waitlist is open for https://flatm8.co.uk - the platform for anonymous reviews of Landlords and Estate Agents in Britain and Ireland.&lt;/p&gt;
    &lt;p&gt;We’re working directly with partner housing unions and charities in Britain and Ireland to build the first central database of rogue landlords and estate agents. Users can search an address and see if it’s marked as rogue/dangerous by the local union, as well as whether you can expect to see your deposit returned, maintenance, communication - etc.&lt;/p&gt;
    &lt;p&gt;After renting for close to a decade, it’s the same old problems with no accountability. We wanted to change this, and empower tenants to share their experiences freely and easily with one another.&lt;/p&gt;
    &lt;p&gt;We’re launching in November, and I’m very excited to announce our partner organisations! We know this relies on a network effect to work, and we’re hoping to run it as a social venture. I welcome any feedback.&lt;/p&gt;
    &lt;p&gt;I built https://invoicepad.app which is a free, completely in-browser tool for creating invoices, estimates, and quotes. Yes, similar apps have been posted here before, but none were built the way I envisioned, so I made my own. The key difference: all invoice data is stored in the URL hash, not the querystring. This is important because querystrings are sent to the server with every request, while hashes stay local to your browser. This means I can never see your invoice data, unlike other similar apps. The workflow is simple: use your browser's bookmark manager as your invoice filing system. Or if you want to keep it offline, just copy and paste invoice URLs into a text document for storage. I’ve also included helpful features like saved profiles to save on repeated data input. The next step is to finish working on a browser extension (v1 is being tested) to make bookmarking, editing, and saving changes even easier, that is if I ever stop being distracted by other side projects.&lt;/p&gt;
    &lt;p&gt;I'm working on 1:6 size furniture. There's not much woodworking I can do outside of the shop, so I've been trying to shrink full joinery techniques down to dollhouse size.&lt;/p&gt;
    &lt;p&gt;I am working on Lunch Flow (https://lunchflow.app), a tool that allows people to automatically sync their bank accounts to their favorite budgeting apps (Google Sheets, Lunch Money, Actual Budget, or use our API!)&lt;/p&gt;
    &lt;p&gt;I was motivated to build this as I found that many great personal finance and budget apps didn't offer integrations with the banks I used, which is understandable given the complexity and costs involved, so I wanted to tackle this problem and help build the missing open banking layer for personal finance apps, with very low costs (a few dollars a month) and a very simple api, or built-in integrations.&lt;/p&gt;
    &lt;p&gt;Still working on making this sustainable, but been quite a learning experience so far, and quite excited to see it already making a difference for so many people :)&lt;/p&gt;
    &lt;p&gt;I'm putting a bunch of security tools / data feeds together as a service. The goal is to help teams and individuals run scans/analysis/security project management for "freemium" (certain number of scans/projects for free each month, haven't locked in on how it'll pan out fully $$ wise).&lt;/p&gt;
    &lt;p&gt;I want to help lower the technical hurdles to running and maintaining security tools for teams and individuals. There are a ton of great open source tools out there, most people either don't know or don't have the time to do a technical deep dive into each. So I'm adding utilities and tools by the day to the platform.&lt;/p&gt;
    &lt;p&gt;Likewise, there's a built in expert platform for you to get help on your security problems built into the system. (Currently an expert team consisting of [me]). Longer term, I'm working on some AI plugins to help alert on CVEs custom to you, generate automated scans, and some other fun stuff.&lt;/p&gt;
    &lt;p&gt;I'm working on a design system. I'm a software eng not a designer, but I started one a long while back because I wanted to get a sense of what designers go through. I've dropped it and came back a half dozen times but now I'm finishing it up.&lt;/p&gt;
    &lt;p&gt;It's been a great project to understand how design depends on a consistent narrative and purpose. At first I put together elements I thought looked good but nothing seemed to "work" and it's only when I took a step back and considered what the purpose and philosophy of the design was that it started to feel cohesive and intentional.&lt;/p&gt;
    &lt;p&gt;I'll never be a designer but I often do side projects outside my wheelhouse so I can build empathy for my teammates and better speak their language.&lt;/p&gt;
    &lt;p&gt;I'm working on Plaid / Perplexity for business data.&lt;/p&gt;
    &lt;p&gt;The basic idea is that integrating business data into a B2B app or AI agent process is a pain. On one side there's web data providers (Clearbit, Apollo, ZoomInfo) then on the other, 150 year old legacy providers based on government data (D&amp;amp;B, Factset, Moody's, etc). You'd be surprised to learn how much manual work is still happening - teams of people just manually researching business entities all day.&lt;/p&gt;
    &lt;p&gt;At a high level, we're building out a series of composable deep research APIs. It's built on a business graph powered by integrations to global government registrars and a realtime web search index. Our government data index is 265M records so far.&lt;/p&gt;
    &lt;p&gt;We're still pretty early and working with enterprise design partners for finance and compliance use cases. Open to any thoughts or feedback.&lt;/p&gt;
    &lt;p&gt;I'm calling it a "Micro Functions as a Service" platform.&lt;/p&gt;
    &lt;p&gt;What it really is, is hosted Lua scripts that run in response to incoming HTTP requests to static URLs.&lt;/p&gt;
    &lt;p&gt;It's basically my version of the old https://webscript.io/ (that site is mostly the same as it was as long as you ignore the added SEO spam on the homepage). I used to subscribe to webscript and I'd been constantly missing it since it went away years ago, so I made my own.&lt;/p&gt;
    &lt;p&gt;I mostly just made this for myself, but since I'd put so much effort into it, I figure I'm going to try to put it out there and see if anyone wants to pay me to use it. Turns out there's a _lot_ of work that goes into abuse prevention when you're code from literally anyone on the internet, so it's not ready to actually take signups yet. But, there is a demo on the homepage.&lt;/p&gt;
    &lt;p&gt;I'm working on a DnD character sheet app! I spent last week implementing the core DnD SRD ruleset, but what I'm really excited about is ML integration. I want to add a self-hosted fine-tuned ML model that acts as a character and DM assistant. Obviously an LLM via API can do the job, but I'm really curious if it's possible to build smaller, cheaper, task-specific models. Plus, I've never integrated an ML model into a product before, and I'm curious to play with it. I'm thinking of it like clippy for DnD: "it looks like you're trying to cast fireball?"&lt;/p&gt;
    &lt;p&gt;Besides the LLM experimentation, this project has allowed me to dive into interesting new tech stacks. I'm working in Hono on Bun, writing server-side components in JSX and then updating the UI via htmx. I'm really happy with how it's coming together so far!&lt;/p&gt;
    &lt;p&gt;Drawing a lot of inspiration from interval.com. It was an amazing product but was a hosted SAAS. I'm exploring taking the idea to the .NET ecosystem and also making it a Nuget package that can be installed and served through any ASP.NET project.&lt;/p&gt;
    &lt;p&gt;Adding new transports and documentation to my Typescript logging library (MIT licensed), LogLayer (https://loglayer.dev). Just added documentation for Bun and Deno support added some new logging library transports (LogTape), and finishing up Logflare and Betterstack transports so you can send logs to their logging APIs.&lt;/p&gt;
    &lt;p&gt;I’m working on Reflect [0], it’s a private self discovery and self experimentation app. You can track metrics, set goals, get alerted to anomalies, view correlations, visualize your data, etc.&lt;/p&gt;
    &lt;p&gt;I'm trying to turn code into a design tool. Kind of like if you ask yourself - what if Cursor had been built for designers?&lt;/p&gt;
    &lt;p&gt;Currently it looks like this:&lt;/p&gt;
    &lt;p&gt;- code editor directly in the browser - writes to your local file system - UI-specific features built into the editor - ways to edit the CSS visually as well as using code - integrated AI chat&lt;/p&gt;
    &lt;p&gt;But I have tons of features I want to add. Asset management, image generation, collaborative editing, etc.&lt;/p&gt;
    &lt;p&gt;It's still a prototype, but I'm actively posting about it on twitter as I go. Soon, I'll probably start publishing versioned builds for people to play with: https://x.com/danielvaughn&lt;/p&gt;
    &lt;p&gt;A lot of people often ask questions like: - How do I lose body fat and build muscle? - How can I track progress over time? - How much exercise do I actually need? - What should my calorie and macro targets be?&lt;/p&gt;
    &lt;p&gt;I've been working on a tool called Materia[0] for managing Podman Quadlets on hosts; I released a new version last month (and posted it on the September thread) and just merged automatic volume data migration the other day. Next goal is to design a system for downloading and loading remote components, similar to ansible roles. Hopefully I can tie it into the new podman quadlet install/etc commands.&lt;/p&gt;
    &lt;p&gt;Currently I've been working on a CLI tool [1] for my WebASM UI library [2] with the idea that all the gluecode generating stuff is abstracted away in nice CLI wizards.&lt;/p&gt;
    &lt;p&gt;Essentially like yeoman back then, to bootstrap your webapp and all the necessary files more easily.&lt;/p&gt;
    &lt;p&gt;Currently I am somewhat stuck because of Go's type system, as the UI components require a specific interface for the Dataset or Data/Record entries.&lt;/p&gt;
    &lt;p&gt;For example, a Pie chart would require a map[string]number which could be a float, percentage string or an integer.&lt;/p&gt;
    &lt;p&gt;A Line chart would require a slice of map[string]number, where each slice index would represent a step in the timeline.&lt;/p&gt;
    &lt;p&gt;A table would require a slice of map[string]any where each slice index would represent a step in the culling, but the data types would require a custom rendering method or Stringifier(?) of sorts attached to the data type. So that it's possible to serialize or deserialize the properties (e.g. yes/no in the UI meaning true/false, etc).&lt;/p&gt;
    &lt;p&gt;As I want to provide UI components that can use whatever struct the developer provides, the Go way would be to use an interface. But that would imply that all data type structs on the backend side would have this type of clutter on them attached.&lt;/p&gt;
    &lt;p&gt;No idea if something like a Parser and Stringifier method definition would make more sense for the UI components here...or whether or not it's better to have something like a Render method attached per component that does all the stringifying on a per-property basis like a "func(dataset any, index int, column string) string" where the developer needs to do all the typecasting manually.&lt;/p&gt;
    &lt;p&gt;Manual typecasting like this would be pretty painful as components then cannot exist in pure HTML serialized form, which is essentially the core value proposition of my whole UI components framework.&lt;/p&gt;
    &lt;p&gt;An alternative would be offering a marshal/unmarshal API similar to how JSON does it, but that would require the reflect package which bloats up the runtime binary by several MB and wouldn't be tinygo compatible, so I heavily would wanna avoid that.&lt;/p&gt;
    &lt;p&gt;Currently looking for other libraries and best practices, as this issue is really bugging me a lot in the app I'm currently building [3] and it's a pretty annoying type system problem.&lt;/p&gt;
    &lt;p&gt;Feedback as to how it's solved in other frameworks or languages would be appreciated. Maybe there's an architectural convention I'm not aware of that could solve this.&lt;/p&gt;
    &lt;p&gt;The goal is to provide a fully typed nodeJS framework that allows you to write a typescript function once and then decide whether to wire it up to http, websocket, queues, scheduled tasks, mcp server, cli and other interactions.&lt;/p&gt;
    &lt;p&gt;You can switch between serverless and server deployments without any refactoring / completely agnostic to whatever platform your running it on&lt;/p&gt;
    &lt;p&gt;It also provides services, permissions, auth, eventhub, advanced tree shaking, middleware, schema generation and validation and more&lt;/p&gt;
    &lt;p&gt;The way it works is by scanning your project via the typescript compiler and generating a bootstrap file that imports everything you need (hence tree shaking), and allows you to filter down your backend to only the endpoints needed (great to pluck out individual entry points for serverless). It also generates types fetch, rpc, websocket and queue client files. Types is pretty much most of what pikku is about.&lt;/p&gt;
    &lt;p&gt;Think honoJS and nestJS sort of combined together and also decided to support most server standards / not just http.&lt;/p&gt;
    &lt;p&gt;Website needs love, currently working on a release to support CLI support and full tree shaking.&lt;/p&gt;
    &lt;p&gt;A “code index” tool that finds symbols in a codebase and creates a single table sqlite database for querying. It’s my second month using Claude Code, and I see a common pattern where Claude tries to guess patterns with grep, and often comes back with empty results. I’m writing the tool to prevent these fruitless searches. Using tree-sitter to parse the AST and add the symbols and what they are (function, class, argument, etc) to the db. I have it working with TypeScript, and am working on adding C and PHP.&lt;/p&gt;
    &lt;p&gt;Aider builds something it calls a "repo map" that I believe is for a similar purpose. Might be worth taking a look!&lt;/p&gt;
    &lt;p&gt;I haven't used Claude Code, but recently switched to OpenCode. My token usage and cost is a lot higher, I'm not sure why yet, but I suspect Aider's approach is much more lean.&lt;/p&gt;
    &lt;p&gt;This is why codepathfinder.dev is born. It underhood use tree-sitter to search functions, class, member variables and pulls code accurately instead of regex.&lt;/p&gt;
    &lt;p&gt;I started using it like tool call in Security scanning (think of something like claude-code for security scanning)&lt;/p&gt;
    &lt;p&gt;Currently building a Declarative Web Assembler of Html/Json using AI in multiple languages for the past 1 month: https://github.com/Srid68/Arshu.Assembler deployed to fly.io&lt;/p&gt;
    &lt;p&gt;The purpose is to find if can i build declarative software in multiple langauges (Rust, Go, Node.Js, PHP and Javascript) knowing only one language (C#) without understanding the implementation deeply.&lt;/p&gt;
    &lt;p&gt;Another purpose is validate AI models and their efficiency since development using AI is hard but highly productive and having a declarative rules to recreate the implementation may be used to validate models&lt;/p&gt;
    &lt;p&gt;Currently i am convinced it is possible to build, but now working on creating a solid foundation with tests of the two assembler engines, structure dumps, logging, logging outputs so that those can be used by the AI which it needs to fix issues iteratively.&lt;/p&gt;
    &lt;p&gt;Need to add more declarative rules and implement a full stack web assembler to see if AI will hit the technical debt which slows/stop progress. Only time will tell.&lt;/p&gt;
    &lt;p&gt;We just brought an IFR 2947a communications service monitor back from the dead. It's amazing how much functionality that you can pack into about 6U of rack space. I was testing it out, and detecting signals down to 0.1 uV on the spectrum analyzer.&lt;/p&gt;
    &lt;p&gt;I've been gathering up the supplies to set up a proper radio/computer repair workshop.&lt;/p&gt;
    &lt;p&gt;I’ve got a side project going that’s a browser extension (starting with Safari + Sign in with Apple) intended to add a comment layer to the internet as a whole. I’m calling it Chaffiti (https://chaffiti.com).&lt;/p&gt;
    &lt;p&gt;The idea is to enable a comment section on any webpage, right as you’re browsing. Viewing a Zillow listing? See what people are excited about with the property. Wonder what people think about a tourist attraction? It’ll be right there. Want to leave your referral or promo code on a checkout page for others? Post it.&lt;/p&gt;
    &lt;p&gt;Not sure what the business model will look like just yet. Just the kind of thing I wish existed compared to needing to venture out to a third party (traditional social media / forums etc) to see others’ thoughts on something I’m viewing online. I welcome any feedback!&lt;/p&gt;
    &lt;p&gt;Great idea but wouldn’t you run into storage issues pretty quickly without massive budget for large database clusters? The web is a big and constantly changing place. Covering it in useful comments seems prohibitively expensive.&lt;/p&gt;
    &lt;p&gt;This is built with Rust, egui and SQLite3. The app has a downloader for NSE India reports. These are the daily end of day stock prices. Out of the box the app is really fast, which is expected but still surprises me. I am going to work on improving the stocks chart. I also want to add an AI assisted stocks analyst. Since all the stocks data is on the SQLite3 DB, I should be able to express my stocks screening ideas as plain text and let an LLM generate the SQL and show me in my data grid.&lt;/p&gt;
    &lt;p&gt;It was really interesting to generate it within 3 days. I had just a few places where I had to copy from app (std) log and paste into my prompt. Most of the time just describing the features was enough. Rust compiler did most of the heavy lifting. I have used a mix of Claude Code and OpenCode (with either GLM 4.5 or Grok Code Fast 1).&lt;/p&gt;
    &lt;p&gt;I have been generating full-stack web apps. I built and launched https://github.com/brainless/letsorder (https://letsorder.app/). Building full-stack web apps is basically building 2 apps (at a minimum) so desktop apps are way better it seems.&lt;/p&gt;
    &lt;p&gt;In the long-term, I plan to build and help others generated apps. I am building a vibe coding platform (https://github.com/brainless/nocodo). I have a couple early stage founders I consult for who take my guidance to generate their products (web and mobile apps + backend).&lt;/p&gt;
    &lt;p&gt;Still working on cataloging a curated list of craft beer venues across the world at https://wheretodrink.beer Unsure what the plan is going forward with it, apart from adding more venues and more countries. As long as it's fun for me I'll just keep adding things.&lt;/p&gt;
    &lt;p&gt;Just added health inspection data from countries that have that in open datasets (UK and Denmark). If anyone know of others I'd be appreciative of hints.&lt;/p&gt;
    &lt;p&gt;Thinking of focusing on another idea for the rest of the year, have a rough idea for a map based ui to structure history by geofences or lat / lng points for small local museums&lt;/p&gt;
    &lt;p&gt;Building a donations powered marketplace, zero platform fee: https://shomp.co&lt;/p&gt;
    &lt;p&gt;Merchants who want to sell on Etsy or Shopify either have to pay a listing fee or pay per month just to keep an online store on the web. Our goal is to provide a perpetually free marketplace that is powered solely off donations. The only fees merchants pay are the Stripe fees, and it's possible that at some volume of usage we will be able to negotiate those down.&lt;/p&gt;
    &lt;p&gt;You can sell digital goods as well as physical goods. Right now in the "manual onboarding" phase for our first batch of sellers.&lt;/p&gt;
    &lt;p&gt;For digital goods, purchasers get a download link for files (hosted on R3).&lt;/p&gt;
    &lt;p&gt;For physical goods, once a purchase comes through, the seller gets an SMS notification and a shipping label gets created. The buyer gets notified of the tracking number and on status changes.&lt;/p&gt;
    &lt;p&gt;We use Stripe Connect to manage KYC (know your customer) identities so we don't store any of your sensitive details other than your name and email. Since we are in the process of incorporating as a 501(c)(3) nonprofit, we are only serving sellers based in the United States.&lt;/p&gt;
    &lt;p&gt;The mission of the company is to provide entrepreneurial training to people via our online platform, as well as educational materials to that aim.&lt;/p&gt;
    &lt;p&gt;Right now the API is nonexistent, relying entirely on people using the web interface to make listings, upload photos, and set prices. But if you would find this useful I can happily build it out. Our stack is Elixir and building APIs is very straightforward. Our code is open-source, too!&lt;/p&gt;
    &lt;p&gt;When you say "algorithmically driven print-on-demand" do you mean that prices would automatically adjust based on inventory? Or like, how do you mean.&lt;/p&gt;
    &lt;p&gt;Also, when you say "see them show up in a request on sale" — can you clarify? I interpret this to mean you want a webhook triggered when an order comes in.&lt;/p&gt;
    &lt;p&gt;Trying to get a new release of Video Hub App - my 7+ years passion project to browse videos from local storage in style. Maybe will finally finish the (optional!) facial recognition feature I started 5+ years ago.&lt;/p&gt;
    &lt;p&gt;Working on my lisp. I recently added delimited continuations, even wrote a blog post about it. Now I'm working on adding more control primitives. Just finished researching generators. I'm going to implement them as a separate interpreter of sorts.&lt;/p&gt;
    &lt;p&gt;I am playing at creating a FTP interface for all file transfer protocols (including the Dropbox API) so we can settle the argument of the infamous top comment of the Dropbox launch: https://github.com/mickael-kerjean/filestash&lt;/p&gt;
    &lt;p&gt;I just finished writing a small script that finds all optimally bad Wordle guesses. More precisely, on hard Wordle, where you must give a valid word (from the guesses list), and you must use yellows + greens, and must not use greys, what are all the combinations of answer + 6 guesses where there is only grey. This is equivalent to finding all answer + 6 guesses where no letters are in common between any pair.&lt;/p&gt;
    &lt;p&gt;This is basically a variation on bit-packing (which is NP-hard), but it's tractable if you prune the search space enough.&lt;/p&gt;
    &lt;p&gt;The goal is to catch vulnerabilities early in the SDLC by running agentic loop that autonomously hunt for security issues in codebases.Currently available as a CLI tool, VSCode extension.I've been actively using to scan WordPress, odoo plugins and found several privilege escalation vuln. I have documented as blog post here: https://codepathfinder.dev/blog/introducing-secureflow-cli-t...&lt;/p&gt;
    &lt;p&gt;Now that I can finally test on hardware, I completely rewrote input handling. I can now support original NES controllers, but also SNES and the Power Pad dance mat, for anyone crazy enough to try that. The hardest part was working around a particularly nasty hardware bug: if you try to read the input ports on even cycles while one of the sound channels is playing, the data becomes corrupted. Perform the exact same read on an odd cycle and it works every time.&lt;/p&gt;
    &lt;p&gt;The solution? Have the cartridge keep track of CPU parity (there's no simple way to do this with just the CPU), then check that, skip one cycle if needed... and very carefully cycle time the rest of the routine, making sure that your reads land on safe cycles, and your writes land in places that won't throw off the alignment.&lt;/p&gt;
    &lt;p&gt;But it works! It's quite reliable on every console revision I've thrown it at so far. Suuuper happy with that.&lt;/p&gt;
    &lt;p&gt;I’m working on a performance capture library for Python because I often need to know the performance of backend systems I maintain. I frequently build tooling to capture performance and save it for later analysis. I/O operations get costly when writing lots of data to disk and creating good real-time analytics tools takes a lot of my time. I wanted a library that captures real-time performance analytics from Python backends.&lt;/p&gt;
    &lt;p&gt;This is why I wrote kronicler to record performance metrics while being fast and simple to implement. I built my own columnar database in Rust to capture and analyze these logs.&lt;/p&gt;
    &lt;p&gt;To capture logs, `import kronicler` and add `@kronicler.capture` as a decorator to functions in Python. It will then start saving performance metrics to the custom database on disk.&lt;/p&gt;
    &lt;p&gt;You can then view these performance metrics by adding a route to your server called `/logs` where you return `DB.logs()`. You can paste your hosted URL into the settings of usekronicler.com (the online dashboard) and view your data with a couple charts. View the readme or the website for more details for how to do this.&lt;/p&gt;
    &lt;p&gt;I'm still working on features like concurrency and other overall improvements. I would love some feedback to help shape this product into something useful for you all.&lt;/p&gt;
    &lt;p&gt;This is going in fits and starts, but I'm working on a Win16 decompiler. The problems with existing decompiler tools for 16-bit code are a) support the NE file format is far less widespread; b) 16-bit code means geating to deal with segment registers, which are largely unmodelled for most binary tools; and c) turns out that you also have to get really good at recognizing "this is a 32-bit value being accessed entirely in 16-bit word chunks," which tends to be under-supported for most optimization toolchains.&lt;/p&gt;
    &lt;p&gt;I've been working on a browser plugin for Amazon that attempts to identify the brand and seller country: https://www.wheresthatfrom.org/&lt;/p&gt;
    &lt;p&gt;It's mostly where I want it to be now, but still need to automate the ingest of USPTO data. I'd really like it to show a country flag on the search results page next to each item, but inferring the brand name just from the item title would probably need some kind of natural language processing; if there's even a brand in the title.&lt;/p&gt;
    &lt;p&gt;No support for their mobile layout. Do many people buy from their phone?&lt;/p&gt;
    &lt;p&gt;I'm doing some experiments in LLM (historical) fiction writing. I feel like we can get pretty good writing out of an LLM (especially Sonnet) with enough prompting, reasoning, and guided thinking. Still with a human as producer and guidance.&lt;/p&gt;
    &lt;p&gt;I'm trying to use this to create stories that would be somewhat unreasonable to write otherwise. Branching stories (i.e., CYOA), multiperspective stories, some multimedia. I'm still trying to figure out the narrative structures that might work well.&lt;/p&gt;
    &lt;p&gt;LLMs can overproduce and write in different directions than is reasonable for a regular author. Though even then I'm finding branching hard to handle.&lt;/p&gt;
    &lt;p&gt;The big challenges are rhythm, pacing, following an arc. Those have been hard for LLMs all along.&lt;/p&gt;
    &lt;p&gt;I'm in The Hague right now at a digital democracy conference, where I was invited to present on my prototype that I've been building the past few months!&lt;/p&gt;
    &lt;p&gt;It's for doing realtime "human cartography", to make maps of who we are together in complex large-scale discourse (even messy protest).&lt;/p&gt;
    &lt;p&gt;It's for exploring human perspective data -- agree, disagree, pass reactions to dozens or hundreds of belief statements -- so we can read it as if it were Google Maps.&lt;/p&gt;
    &lt;p&gt;My operating assumption is that if a critical mass of us can understand culture and value clashes as mere shapes of discourse, and we can all see it together, the we can navigate them more dispassionately and with clear heads. Kinda like reading a map or watching the weather report -- islands that rise from oceans, or plate tectonics that move like currents over months, and terraform the human landscape -- maybe if we can see these things together, we'll act less out of fear of fun-house caricatures. (E.g., "Hey, dad, it seems like the peninsula you're on is becoming a land bridge toward the alt right corner. I feel a little bummed about that. How do you feel about it?")&lt;/p&gt;
    &lt;p&gt;(It builds on data and the mathematical primitives of a great tool called Pol.is, which I've worked with for almost a decade.)&lt;/p&gt;
    &lt;p&gt;I'm working on a compiler for WebAssembly. The idea is you use the raw wasm instructions like you’d use JSX in React, so you can make reusable components and compose them into higher abstractions. Inlining is just a function call.&lt;/p&gt;
    &lt;p&gt;It’s implemented in Elixir and uses its powerful macro system. This is paired with a philosophy of static &amp;amp; bump allocation, so I’m trying to find a happy medium of simplicity with a powerful-enough paradigm yet generate simple, compact code.&lt;/p&gt;
    &lt;p&gt;It is a modified version of Shopify's CEO Tobi try implementation[0]. It extends his implementation with sandboxing capabilities and designed with functional core, imperative shell in mind.&lt;/p&gt;
    &lt;p&gt;I had success using it to manage multiple coding agents at once.&lt;/p&gt;
    &lt;p&gt;Currently working on the web reader of WithAudio. Just add with.audio/ to begining of a public URL and get the text and audio in your browser. It runs the TTS in your browser so its free and unlimited.&lt;/p&gt;
    &lt;p&gt;I buit this to get some traffic to my main project's website using a free tool people might like. The main project: https://desktop.with.audio -&amp;gt; a one time payment text to speech app with text highlighting and export mp3 and other features on MacOS (ARM only) and Windows.&lt;/p&gt;
    &lt;p&gt;I have been building OpenRun, a declarative web app deployment platform https://github.com/openrundev/openrun. It is an open source alternative to Google Cloud Run and AWS App Runner, running on your own hardware.&lt;/p&gt;
    &lt;p&gt;OpenRun allows defining your web app configuration in a declarative config using Starlark (which is like a subset of Python). Setting up a full GitOps workflow is just one command:&lt;/p&gt;
    &lt;p&gt;This will set up a scheduled sync, which will look for new apps in the config and create them. It will also apply any config updates on existing apps and reload apps with the latest source code. After this, no further CLI operations are required, all updates are done declaratively. For containerized apps, OpenRun will directly talk to Docker/Podman to manage the container build and startup. There are lots of tools which simplify web app deployment. Most of them use a UI driven approach or an imperative CLI approach. That makes it difficult to recreate an environment. Managing these tools when multiple people need to coordinate changes is also difficult.&lt;/p&gt;
    &lt;p&gt;Any repo which has a Dockerfile can be deployed directly. For frameworks like Streamlit/Gradio/FastHTML/Shiny/Reflex/Flask/FastAPI, OpenRun supports zero-config deployments, there is no need to even have a Dockerfile. Domain based deployment is supported for all apps. Path based deployment is also supported for most frameworks, which makes DNS routing and certificate management easier.&lt;/p&gt;
    &lt;p&gt;OpenRun currently runs on a single machine with an embedded SQLite database or on multiple machines with an external Postgres database. I plan to support OpenRun as a service on top of Kubernetes, to support auto-scaling. OpenRun implements its own web server, instead of using Traefik/Nginx. That makes it possible to implement features like scaling down to zero and RBAC. The goal with OpenRun is to support declarative deployment for web apps while removing the complexity of maintaining multiple YAML config files. See https://github.com/openrundev/openrun/blob/main/examples/uti... for an example config, each app is just one or two lines of config.&lt;/p&gt;
    &lt;p&gt;OpenRun makes it easy to set up OAuth/OIDC/SAML based auth, with RBAC. See https://openrun.dev/docs/use-cases/ for a couple of use cases examples: sharing apps with family and sharing across a team. Outside of managed services, I have found it difficult to implement this type of RBAC with any other open source solution.&lt;/p&gt;
    &lt;p&gt;Would love to see the Red Cross partner with someone like you here in Australia. Not affiliated, just a donor. We're not financially incentivised like other countries but there's a big culture here about celebrating the free milkshake and/or sausage roll you get after donating.&lt;/p&gt;
    &lt;p&gt;I am working on Tailstream (https://tailstream.io/), turning logs into task time visual data streams. Built the web application, web site and a Go CLI agent (open source) and am now slightly pivoting into making it more log-focused.&lt;/p&gt;
    &lt;p&gt;Working on faceted search for logs and CLI client now and trying to share my progress on X.&lt;/p&gt;
    &lt;p&gt;I’m working on Leggen (https://github.com/elisiariocouto/leggen), a self hosted personal banking account management system. It started out as a CLI that syncs your bank account transactions and balances, saves them in a sqlite database and can alert you via Telegram or Discord if a transaction matches a filter. Recently I started refactoring the project with the help of Claude Code and Copilot Agent to include an API and a Web app to explore the data and configure it. The product is using GoCardless Bank Accout Data APIs to connect to banks via PSD2 but I found out recently that registering a new account is no longer possible so I’m currently looking into alternatives.&lt;/p&gt;
    &lt;p&gt;Check out Lunch Flow (https://lunchflow.app) for a global open banking API that's accessible for personal finance apps :) We integrate with Gocardless, among other global open banking providers.&lt;/p&gt;
    &lt;p&gt;The goal was to make the learning material very malleable, so all content can be viewed through different "lenses" (e.g. made simpler, more thorough, from first principles, etc.). A bit like Wikipedia it also allows for infinite depth/rabbit holing. Each document links to other documents, which link to other documents (...).&lt;/p&gt;
    &lt;p&gt;I'm also currently in the middle of adding interactive visualizations which actually work better than expected! Some demos:&lt;/p&gt;
    &lt;p&gt;A monster trainer game where you can _actually teach new, creative moves_ to your monsters: https://youtu.be/ThOCM9TK_yo&lt;/p&gt;
    &lt;p&gt;Basically, think of it as "Pokemon the anime, but for real". We allow you to use your voice to talk to, command, and train your monster. You and your monster are in this sandbox-y, dynamic environment where your actions have side effects.&lt;/p&gt;
    &lt;p&gt;You can train to fight or just to mess around.&lt;/p&gt;
    &lt;p&gt;Behind the scenes, we are converting player's voice into code in real time to give life to these monsters.&lt;/p&gt;
    &lt;p&gt;Working on https://videotobe.com a audio/video transcription service. VideoToBe started as a user friendly Whisper wrapper — but is evolving into a full pipeline that extracts, summarizes, and structures insights from multimedia content.&lt;/p&gt;
    &lt;p&gt;Working on https://practicecallai.com/ - simple saas that lets users run practice calls / role play against a custom AI partner. Goal is to make it the easiest to use &amp;amp; fastest to get started with in the market.&lt;/p&gt;
    &lt;p&gt;It’s been a fun, practical way to continuously evaluate the latest models two ways - via coding assistance &amp;amp; swapping between models to power the conversational AI voice partner. I’ve been trying to add one big new feature each time the model generation updates.&lt;/p&gt;
    &lt;p&gt;The next thing I want to add is a self improving feedback loop where it uses user ratings of the calls &amp;amp; evaluations to refine the prompts that generate them.&lt;/p&gt;
    &lt;p&gt;Working on: to teach myself Rust, I’ve been working on a NYT Letter Boxed solver, with some ambitions to turn it into a game by itself. I think this game could be made a lot more fun.&lt;/p&gt;
    &lt;p&gt;Thinking about: A new take on LinkedIn/web-of-trust, bootstrapped by in-person interactions with devices. It seems that the problem of proving who is actually human and getting a sense of how your community values you might be getting more important, and now devices have some new tools to bring that within reach.&lt;/p&gt;
    &lt;p&gt;Conductor is a LLM agnostic framework for building sophisticated AI applications using a subagent architecture. It provides a robust platform for orchestrating multiple specialized AI agents to accomplish complex tasks, with features like LLM-based planning, memory persistence, and dynamic tool use.&lt;/p&gt;
    &lt;p&gt;It provides a robust and flexible platform for orchestrating multiple specialized AI agents to accomplish complex tasks. This project is inspired by the concepts outlined in "The Rise of Subagents" by Phil Schmid at https://www.philschmid.de/the-rise-of-subagents and it aims to provide a practical implementation of this powerful architectural pattern.&lt;/p&gt;
    &lt;p&gt;I'm working on a set of TypeScript libraries to make it really really easy to spin up an agent, or an chatbot, or pretty much anything else you want to prototype. It's based around sensible interfaces, and while batteries are included, they're also meant to be removed when you've got something you want.&lt;/p&gt;
    &lt;p&gt;The idea is that a beginner should be able to wire up a personally useful agent (like a file-finder for your computer) in ten minutes by writing a simple prompt, some simple tools, and running it. Easy to plugin any kind of tracing, etc you want. Have three or four projects in prod which I'll be switching to use it just to make sure it fits all those use-cases.&lt;/p&gt;
    &lt;p&gt;But I want to be able to go from someone saying "can we build an agent to" to having the PoC done in a few minutes. Everything else I've looked at so far seems limited, or complicated, or insufficiently hackable for niche use-cases. Or, worse of all, in Python.&lt;/p&gt;
    &lt;p&gt;An implementation of statecharts. I'm working through core functionality using recursive algorithms.&lt;/p&gt;
    &lt;p&gt;I discovered that "least common ancestor" boils down to the intersection of 'root-path' sets, where you select the last item in the set as the 'first/least common ancestor'.&lt;/p&gt;
    &lt;p&gt;Plugging away with reviews of Genrative AI tech with detailed comparisons. I announced the launch on HN a while ago, thought I’d use this month’s for a status update.&lt;/p&gt;
    &lt;p&gt;I just took Qwen-Image and Google’s image AIs for a spin and I keep a side by side comparison of many of them.&lt;/p&gt;
    &lt;p&gt;Thanks, the 3D asset creators are very interesting. I'm working on LLM -&amp;gt; CAD tool (for 3d printing) and your post confirms that I should keep my focus, because there is so much other things to do (uv unwrapping!) if you are targeting games for example.&lt;/p&gt;
    &lt;p&gt;I'm expanding my computational biology toolkit in rust. Of recent interest is optimizing long-range molecular dynamics forces on GPU and SIMD, adding support to generate lipid membranes and LNPs, and a 3D small molecule editor with integrated dynamics.&lt;/p&gt;
    &lt;p&gt;Porting my binary &amp;amp; decimal palindromes[0] finding code[1] to CUDA, with which I had no experience before starting this project.&lt;/p&gt;
    &lt;p&gt;It's already working, and slightly faster than the CPU version, but that's far from an acceptable result. The occupancy (which is a term I first learned this week) is currently at a disappointing 50%, so there's a clear target for optimisation.&lt;/p&gt;
    &lt;p&gt;Once I'm satisfied with how the code runs on my modest GPU at home, the plan is to use some online GPU renting service to make it go brrrrrrrrrr and see how many new elements I can find in the series.&lt;/p&gt;
    &lt;p&gt;This is a job board for AI jobs and companies. The job market in AI is pretty hot right now, and there are a lot of cool AI companies out there. I'm hoping to connect job seekers with fast-growing AI companies.&lt;/p&gt;
    &lt;p&gt;-Many say they want to stop doomscrolling and clout-chasing but I don't know how many are actually willing to do so&lt;/p&gt;
    &lt;p&gt;-Individuals may move here but their friends won't. So the feed will be initially empty by design. Introducing any kind of reward is against our ethos so we are clueless about how to convince existing friend circles to move.&lt;/p&gt;
    &lt;p&gt;This may work in your favour - it's one of the reasons I enjoy Mastodon so much - friction is/was a little higher which kept my network small but focussed&lt;/p&gt;
    &lt;p&gt;I'm working on adding favicons support to listings on my website directory I recently launched: https://intrasti.com&lt;/p&gt;
    &lt;p&gt;I just released the changelog 5 minutes ago https://intrasti.com/changelog which I went with a directory based approach using the international date format YYYY-MM-DD so in the source code it's ./changelog/docs/YYYY/MM/DD.md - seems to do the trick and ready for pagination which I haven't implemented yet.&lt;/p&gt;
    &lt;p&gt;You provide your URL and an LLM browses your site and writes up feedback. Currently working on increasing the quality of the feedback. Trying to start with a narrower set of tests that give what I think is good feedback, then increase from there.&lt;/p&gt;
    &lt;p&gt;If a tool like this analyzed your website, what would you actually want it to tell you? What feedback would be most useful?&lt;/p&gt;
    &lt;p&gt;The main idea is to bring as many of the agentic tools and features into a single cohesive platform as much as possible so that we can unlock more useful AI use-cases.&lt;/p&gt;
    &lt;p&gt;I'm working on Debtmap - An open source Rust-based code complexity analyzer that tells you exactly which code to refactor and which code to test for maximum impact. Combines complexity metrics with test coverage data to identify the riskiest code in your codebase. Uses entropy analysis to reduce false positives by distinguishing genuinely complex code from repetitive patterns.&lt;/p&gt;
    &lt;p&gt;I am working on a paper about solving the Royal Game of Ur, one of the world's oldest board games. We solved it a while ago, and are now trying to get more formal about it (https://royalur.net/solved).&lt;/p&gt;
    &lt;p&gt;My partner and I are working on Supabird.io (https://supabird.io), a tool to help people grow on X in a more consistent and structured way. It analyzes viral posts within specific communities so users can learn what works and apply those insights to their own content.&lt;/p&gt;
    &lt;p&gt;My partner shares our journey on X (@hustle_fred), while I’ve been focused on building the product (yep, the techie here :). We’re excited to have onboarded 43 users in our first month, and we're looking forward to getting feedback from the HN community!&lt;/p&gt;
    &lt;p&gt;I'm currently working on building a local delivery service using electric cargo bikes in NYC: https://hudsonshipping.co. We are planning to launch our first pilot in early 2026 with our first customers in Brooklyn. We've built all of the tech in-house to manage the fleet, deliveries and optimize our routes. If you know of anyone that would like to be a part of the pilot program, feel free to reach out to me!&lt;/p&gt;
    &lt;p&gt;Haunted house trope, but it's a chatbot. Not done yet, but it's going well. The only real blocker is that I ran into the parental controls on the commercial models right away when trying to make gory images, so I had to spin up my own generators. (Compositing by hand definitely taking forever).&lt;/p&gt;
    &lt;p&gt;On-site surveys for eCommerce and SaaS. It's been an amazing ride leveling up back and forth between product, design, and marketing. Marketing is way more involved than most people on this site realize...&lt;/p&gt;
    &lt;p&gt;I'm building a mod for the game Subway Builder (http://subwaybuilder.com) that lets me undo/redo individual stations and tracks, instead of clearing all blueprints.&lt;/p&gt;
    &lt;p&gt;Lovely interface. This is quite impressive. I can't seem to get a terminal running though. Can I actually execute scripts here? I opened code and created a hello.py, terminal did not come up in Code either.&lt;/p&gt;
    &lt;p&gt;I'm working on a workout tracker that you can actually use for things like TRX and gymnastic rings. Along with normal workouts too. Let me know if there's anything you'd like on there. https://gravitygainsapp.com/&lt;/p&gt;
    &lt;p&gt;I've spent the last few months working on a custom RL model for coding tasks. The biggest headache has been the lack of good tooling for tuning the autorater's prompt. (That's the judge that gives the training feedback.) The process is like any other quality-focused task—running batch rating jobs and doing SxS evaluations—but the tooling really falls short. I think I'll have to build my own tools once I wrap up the current project&lt;/p&gt;
    &lt;p&gt;Hmm, a personal assistant of sorts that does evaluation of you to get to the bottom of who you really are. For very obvious reasons, it is a local only project and not exactly intended for consumption.&lt;/p&gt;
    &lt;p&gt;Beyond that, just regular random stuff that comes up here and there, but, for once, my hdd with sidelined projects is slowly being worked through.&lt;/p&gt;
    &lt;p&gt;A mobile app that checks my email to find and extract family-related events/activities. The kind of things that are buried in a 12-point bullet list with font 8, inside of one of 10 school email messages received during the week&lt;/p&gt;
    &lt;p&gt;It runs fully on-device, including email classification and event extraction&lt;/p&gt;
    &lt;p&gt;Headbang, a rhythm game that you play by bobbing your head while wearing Airpods while listening to music, is what I'm considering building next. The idea came from someone else using Airpods to create a racing game (RidePods).&lt;/p&gt;
    &lt;p&gt;I should also point out - if you download the current version, you should immediately apply the update that will pop up. And even then, you're results may be flakey.&lt;/p&gt;
    &lt;p&gt;While working on Shelvica, a personal library management service and reading tracker, I realized I needed a source of data for book information, and none of the solutions available provided all the data I needed. One might provide the series, the other might provide genres, and yet another might provide a cover with good dimensions, but none provided everything.&lt;/p&gt;
    &lt;p&gt;So I started working on Librario, an ISBN database that fetches information from several other services, such as Hardcover.app, Google Books, and ISBNDB, merges that information, and return something more complete than using them alone. It also saves that information in the database for future lookups.&lt;/p&gt;
    &lt;p&gt;You can see an example response here[1]. Pricing information for books is missing right now because I need to finish the extractor for those, genres need some work[2], and having a 5 months old baby make development a tad slow, but the service is almost ready for a preview.&lt;/p&gt;
    &lt;p&gt;The algorithm to decide what to merge is the hardest part, in my opinion, and very basic right now. It's based on a priority and score system for now, where different extractors have different priorities, and different fields have different scores. Eventually, I wanna try doing something with machine learning instead.&lt;/p&gt;
    &lt;p&gt;I'd also like to add book summaries to the data somehow, but I haven't figured out a way to do this legally yet. For books in the public domain I could feed the entire book to an LLM and ask them to write a spoiler-free summary of the book, but for other books, that'd land me in legal trouble.&lt;/p&gt;
    &lt;p&gt;Oh, and related books, and things of the sort. But I'd like to do that based on the information stored in the database itself instead of external sources, so it's something for the future.&lt;/p&gt;
    &lt;p&gt;Last time I posted about Shelvica some people showed interest in Librario instead, so I decided to make it something I can sell instead of just a service I use in Shelvica[3], hence why I'm focusing more on it these past two weeks.&lt;/p&gt;
    &lt;p&gt;[2]: In the example you'll see genres such as "English" and "Fiction In English", which is mostly noise. Also things like "Humor", "Humorous", and "Humorous Fiction" for the same book.&lt;/p&gt;
    &lt;p&gt;[3]: Which is nice, cause that way there are two possible sources of income for the project.&lt;/p&gt;
    &lt;p&gt;Working on https://JobBoardSearch.com a meta directory of job boards helping job site owners with their DR, visibility, jobs cross posting and promoting in general&lt;/p&gt;
    &lt;p&gt;Building a new layer of hyper-personalization over the web. Instead of generating more content, it helps you reformat and interact with what already exists, turning any page, paper, or YouTube video into a summary, mind-map, podcast, infographic or chat.&lt;/p&gt;
    &lt;p&gt;The broader idea is to make the web adaptive to how each person thinks and learns.&lt;/p&gt;
    &lt;p&gt;Working on an AI governance and security platform that gives security and GRC visibility into what AI tools people are actually using but also what is going into them.&lt;/p&gt;
    &lt;p&gt;It's a browser extension right now and the platform integrates with SSO providers and AI APIs, to help discover shadow AI, enforce policies and creates audit trails. Think observability for AI adoption but also Grammerly since we help coach endusers to better behavior/outcomes.&lt;/p&gt;
    &lt;p&gt;Early days but the problem is real, have a few design partners in the F500 already&lt;/p&gt;
    &lt;p&gt;An application that helps deaf and nonverbal individuals with daily interactions when they’re out and about.&lt;/p&gt;
    &lt;p&gt;My first career was in sales. And most of the time these interactions began with grabbing a sheet of paper and writing to one another. I think small LLMs can help here.&lt;/p&gt;
    &lt;p&gt;Currently making use of api’s but I think small models on phones will be good enough soon. Just completed my MVP.&lt;/p&gt;
    &lt;p&gt;AppGoblin is a free place to do app research for understanding which apps use which companies to monetize, track where data is sent and what kinds of ads are shown.&lt;/p&gt;
    &lt;p&gt;An open source campaign management app for TTRPGs. There are a ton out there, that are basically just fancy wikis. I'm working on one in Django for running my old school D&amp;amp;D game i'm starting back up this fall.&lt;/p&gt;
    &lt;p&gt;Camera Search (camerasearch.ai) is my iOS app for tradespeople and DIY users. It combines voice, video, image understanding, and chat—backed by tuned LLM API—to help diagnose issues and guide builds/repairs in realtime.&lt;/p&gt;
    &lt;p&gt;Currently running some finetuning experiments on non-verbal sounds to teach TTS how to laugh. I have had some success to add the necessary tags and tokens to multiple systems, but assembling the necessary dataset with sufficient quality is hard.&lt;/p&gt;
    &lt;p&gt;Working on securing software against backdoors and hidden exploits using a set of debloating tools. First one available here: github.com/negativa-ai/BLAFS&lt;/p&gt;
    &lt;p&gt;iOS/Mac app for learning Japanese by reading, all in one solution with optional Anki integration&lt;/p&gt;
    &lt;p&gt;I went full-time on this a couple years ago. I’m now doing a full iOS 26 redesign, just added kanji drawing, and am almost done adding a manga mode via Mokuro. I’m also preparing influencer UGC campaigns as I haven’t marketed it basically at all yet.&lt;/p&gt;
    &lt;p&gt;a tool to help California home owners to lower their property taxes. This works for people who bought in the past years low interest environment and are overpaying in taxes because of that.&lt;/p&gt;
    &lt;p&gt;Feel free to email me, if you have questions: phl.berner@gmail.com&lt;/p&gt;
    &lt;p&gt;I just tried your app and after providing my email the analysis I get is for a completely different address than what I entered. I tried twice just to make sure the address i entered was right.&lt;/p&gt;
    &lt;p&gt;https://revise.io - a new word processor with live collaboration, git-like revision history, and an AI agent like Cursor.&lt;/p&gt;
    &lt;p&gt;Basically, an agentic platform for working with rich text documents.&lt;/p&gt;
    &lt;p&gt;I’ve been building this solo since May and having so much fun with it. I created a canvas renderer and all of the word processor interactions from scratch so I can have maximum control over how things are display when it comes to features like AI suggestions and other more novel features I have planned for the future.&lt;/p&gt;
    &lt;p&gt;A little library to define functions in English (through LLM of course; for TypeScript initially) and use these functions like ordinary (async) functions (calling &amp;amp; be called). Agents as functions and multi-step concurrent orchestration of agents with event loops, if fanciness is in order.&lt;/p&gt;
    &lt;p&gt;And an agentic news digest service which scrapes a few sources (like HackerNews) for technical news and create a daily digest, which you can instruct and skew with words.&lt;/p&gt;
    &lt;p&gt;Any chance you'll take a look at power tools next?&lt;/p&gt;
    &lt;p&gt;There are some Amish people who rebuild Dewalt, Milwaukee etc battery packs. I'd like a repairable/sustainable platform where I can actually check the health of the battery packs and replace worn out cells as needed.&lt;/p&gt;
    &lt;p&gt;To give you an idea of the market, original batteries are about $149, and their knockoffs are around $100.&lt;/p&gt;
    &lt;p&gt;Very nice, looking forward to a deal with Décath' ;) How hard is it to make it compatible with the various motors when there is communication involved?&lt;/p&gt;
    &lt;p&gt;I've been wondering for a while if the display on ebikes could also be a more open and durable part of it.&lt;/p&gt;
    &lt;p&gt;trying to build some opportunity for the VR/XR community with https://vr.dev&lt;/p&gt;
    &lt;p&gt;right now, it’s a better way to showcase your really specific industry skills and portfolio of 3D assets (i.e., “LinkedIn for VR/XR) with hiring layered on&lt;/p&gt;
    &lt;p&gt;starting to add onto the current perf analysis tools and think more about how to get to a “lovable for VR/XR”&lt;/p&gt;
    &lt;p&gt;Working on https://run-phx.com ... a guide to trail running in the Valley of the Sun with notable routes, curated by actual human beings in the running community. (whoa)&lt;/p&gt;
    &lt;p&gt;Not earth shattering, but something that should exist.&lt;/p&gt;
    &lt;p&gt;I think getting a clear picture about what it is about yourself that needs work is actually a lot of the real work. Much of the rest of it is picking a direction and then living in that direction.&lt;/p&gt;
    &lt;p&gt;I have been trying to study Chinese on my own for a while now and found it very frustrating to spend half the time just looking for simple content to read and listen to. Apps and websites exist, but they usually only have very little content or they ramp up the difficulty too quickly.&lt;/p&gt;
    &lt;p&gt;Now that LLMs and TTS are quite good I wanted to try it out for languages learning. The goal is to create a vast number of short AI-generated stories to bridge the gap between knowing a few characters and reading real content in Chinese.&lt;/p&gt;
    &lt;p&gt;Curious to see if it is possible to automatically create stories which are comfortable to read for beginners, or if they sound too much like AI-slop.&lt;/p&gt;
    &lt;p&gt;I'm working on a card game for android, it's being built with Monogame and C#. It's just go fish at the moment, but I'm thinking of expanding it into a full suite of card games like solitaire and poker. The source is available on GitHub if anyone wants to poke around and perhaps collaborate. https://github.com/joshsiegl1/GoFishRefresh&lt;/p&gt;
    &lt;p&gt;A tool for threshold signing software releases that I eventually want to integrate with SigStore, etc. to help folks distribute their code-signing. https://github.com/soatok/freeon&lt;/p&gt;
    &lt;p&gt;-----&lt;/p&gt;
    &lt;p&gt;Want E2EE for Mastodon (and other ActivityPub-based software), so you can have encrypted Fediverse DMs? I've been working on the public key transparency aspect of this too.&lt;/p&gt;
    &lt;p&gt;It's an AI-webapp builder with a twist: I proxy all OpenAI API calls your webapp makes and charge 2x the token rate; so when you publish your webapp onto a subdomain, the users who use your webapp will be charged 2x on their token usage. Then you, the webapp creator, gets 80% of what's left over after I pay OpenAI (and I get 20%).&lt;/p&gt;
    &lt;p&gt;It's also a fun project because I'm making code changes a different way than most people are: I'm having the LLM write AST modification code; My site immediately runs the code spit out by the LLM in order to make the changes you requested in a ticket. I blogged about how this works here: https://codeplusequalsai.com/static/blog/prompting_llms_to_m...&lt;/p&gt;
    &lt;p&gt;I am still [0] working on trying to recover who I was before whatever -- a couple of years ago -- rendered me progressively unable to concentrate on anything.&lt;/p&gt;
    &lt;p&gt;Last month was an improvement. This month I can't concentrate for long and I distract very easily, but I seem to be able to do more with what I have, A small sense of ambition that I might be able to do bigger things, and might not need to drop out of tech and get a simple job, is returning.&lt;/p&gt;
    &lt;p&gt;I am trying to use this inhibited, fractured state to clarify thoughts about useless technology and distractions, and about what really matters, because (without wishing to sound haughty) I used to be unusually good at a lot of tech stuff, and now I am not. It is sobering but it is also an insight into what it might be like to be on the outside of technology bullshit, looking in.&lt;/p&gt;
    &lt;p&gt;I'm building an open source NAT traversal and networking framework called P2PD. Built from the ground up to allow things like multi-network interface applications, improved network programming in Python, and if people want it: an easy way to bypass NATs. The thing is: it depends on public servers for some of this which tends to change a lot, causing errors when they're all down.&lt;/p&gt;
    &lt;p&gt;What I'm building at the moment is a server monitoring solution for STUN, TURN, MQTT, and NTP servers. I wanted to allow the software for this to be portable. So I wrote a simple work queue myself. Python doesn't have linked-lists which is the data structure I'm using for the queues. They allow for O(1) deletes which you can't really get on many Python data structures. Important for work items when you're moving work between queues.&lt;/p&gt;
    &lt;p&gt;For the actual workers I keep things very simple. I make like 100 independent Python processes each with an event loop. This uses up a crap load of memory but the advantage is that you can parallel execution without any complexity. It would be extremely complex trying to do that with code alone and asyncio's event loop doesn't play well with parallelism. So you really only want one per process.&lt;/p&gt;
    &lt;p&gt;Result: simple, portable Python code that can easily manage monitoring hundreds of servers (sorry didnt mean for that to sound like chatgpt, lmao, incidental.) The DB for this is memory-based to avoid locking issues. I did use sqlite at first but even with optimizations there were locking issues. Now, I only use sqlite for import / export (checksums.)&lt;/p&gt;
    &lt;p&gt;yet another nvr (in python). also trying to make a switch for hpm style rocker light switches. these things are devilish. the switch requires a lot of force at a strange angle but i dont want to break it so knowing nothing about mechanical stuff ive had to learn about slip clutches, idling gears, worm gears, ratchet wheels. rack and pinions (ofc. from a hobbyist perspective). i know theres a switchbot and a fingerbot but neither of those will work with that type of switch unless you tape some sort of torque lever onto the light (which i dont want to do). its a rabbit hole :/&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45561428</guid><pubDate>Sun, 12 Oct 2025 20:09:14 +0000</pubDate></item><item><title>Emacs agent-shell (powered by ACP)</title><link>https://xenodium.com/introducing-agent-shell</link><description>&lt;doc fingerprint="35d78c5f8911c81d"&gt;
  &lt;main&gt;
    &lt;p&gt;Not long ago, I introduced acp.el, an Emacs lisp implementation of ACP (Agent Client Protocol), the agent protocol developed between Zed and Google folks.&lt;/p&gt;
    &lt;p&gt;While I've been happily accessing LLMs from my beloved text editor via chatgpt-shell (a multi-model package I built), I've been fairly slow on the AI agents uptake. Probably a severe case of old-man-shouts-at-cloud sorta thing, but hey I want well-integrated tools in my text editor. When I heard of ACP, I knew this was the thing I was waiting for to play around with agents.&lt;/p&gt;
    &lt;p&gt;With an early acp.el client library in place, I set out to build an Emacs-native agent integrationâ¦ Today, I have an initial version of agent-shell I can share.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;agent-shell&lt;/code&gt; is a native Emacs shell, powered by comint-mode (check out Mickey's comint article btw). As such, we don't have to dance between char and line modes to interact with things. &lt;code&gt;agent-shell&lt;/code&gt; is just a regular Emacs buffer like any other you're used to.&lt;/p&gt;
    &lt;p&gt;Thanks to ACP, we can now build agent-agnostic experiences by simply configuring our clients to communicate with their respective agents using a common protocol. As users, we benefit from a single, consistent experience, powered by any agent of our choice.&lt;/p&gt;
    &lt;p&gt;Configuring different agents from &lt;code&gt;agent-shell&lt;/code&gt; boils down which agent we want running in the comms process. Here's an example of Gemini CLI vs Claude Code configuration:&lt;/p&gt;
    &lt;code&gt;(defun agent-shell-start-gemini-agent ()
  "Start an interactive Gemini CLI agent shell."
  (interactive)
  (agent-shell--start
   :new-session t
   :mode-line-name "Gemini"
   :buffer-name "Gemini"
   :shell-prompt "Gemini&amp;gt; "
   :shell-prompt-regexp "Gemini&amp;gt; "
   :needs-authentication t
   :authenticate-request-maker (lambda ()
                                 (acp-make-authenticate-request :method-id "gemini-api-key"))
   :client-maker (lambda ()
                   (acp-make-client :command "gemini"
                                    :command-params '("--experimental-acp")
                                    :environment-variables (list (format "GEMINI_API_KEY=%s" (agent-shell-google-key)))))))
&lt;/code&gt;
    &lt;code&gt;(defun agent-shell-start-claude-code-agent ()
  "Start an interactive Claude Code agent shell."
  (interactive)
  (agent-shell--start
   :new-session t
   :mode-line-name "Claude Code"
   :buffer-name "Claude Code"
   :shell-prompt "Claude Code&amp;gt; "
   :shell-prompt-regexp "Claude Code&amp;gt; "
   :client-maker (lambda ()
                   (acp-make-client :command "claude-code-acp"
                                    :environment-variables (list (format "ANTHROPIC_API_KEY=%s" (agent-shell-anthropic-key)))))))
&lt;/code&gt;
    &lt;p&gt;I've yet to try other agents. If you get another agent running, I'd love to hear about it. Maybe submit a pull request?&lt;/p&gt;
    &lt;p&gt;While I've been relying on my acp.el client library, I'm still fairly new to the protocol. I often inspect traffic to see what's going on. After staring at json for far too long, I figured I may as well build some tooling around acp.el to make my life easier. I added a traffic buffer for that. From &lt;code&gt;agent-shell&lt;/code&gt;, you can invoke it via &lt;code&gt;M-x agent-shell-view-traffic&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Developing &lt;code&gt;agent-shell&lt;/code&gt; against paid agents got expensive quickly. Not only expensive, but my edit-compile-run cycle also became boringly slow waiting for agents. While I knew I wanted some sort of fake agent to work against, I didn't want to craft the fake traffic myself. Remember that traffic buffer I showed ya? Well, I can now save that traffic to disk and replay it later. This enabled me to run problematic sessions once and quickly replay multiple times to fix things. While re-playing has its quirks and limitations, it's done the job for now.&lt;/p&gt;
    &lt;p&gt;You can see a Claude Code session below, followed by its replayed counterpart via fake infrastructure.&lt;/p&gt;
    &lt;p&gt;Getting here took quite a bit of work. Having said that, it's only a start. I myself need to get more familiar with agent usage and evolve the package UX however it feels most natural within its new habitat. Lately, I've been experimenting with a quick diff buffer, driven by n/p keys, shown along the permission dialog.&lt;/p&gt;
    &lt;code&gt;#+ATTR_HTML: :width 99%
&lt;/code&gt;
    &lt;p&gt;While I've implemented enough parts of the Agent Client Protocol Schema to make the package useful, it's hardly complete. I've yet to fully familiarize myself with most protocol features.&lt;/p&gt;
    &lt;p&gt;Both of my new Emacs packages, agent-shell and acp.el, are now available on GitHub. As an agent user, go straight to agent-shell. If you're a package author and would like to build an ACP experience, then give acp.el a try. Both packages are brand new and may have rough edges. Be sure to file bugs or feature requests as needed.&lt;/p&gt;
    &lt;p&gt;I've been heads down, working on these packages for some time. If you're using cloud LLM services, you're likely already paying for tokens. If you find my work useful, please consider routing some of those coins to help fund it. Maybe my tools make you more productive at work? Ask your employer to support the work. These packages not only take time and effort, but also cost me money. Help fund the work.&lt;/p&gt;
    &lt;p&gt;powered by LMNO.lol&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45561672</guid><pubDate>Sun, 12 Oct 2025 20:37:16 +0000</pubDate></item><item><title>Edge AI for Beginners</title><link>https://github.com/microsoft/edgeai-for-beginners</link><description>&lt;doc fingerprint="2f0879180436a96a"&gt;
  &lt;main&gt;
    &lt;p&gt;Follow these steps to get started using these resources:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fork the Repository: Click&lt;/item&gt;
      &lt;item&gt;Clone the Repository: &lt;code&gt;git clone https://github.com/microsoft/edgeai-for-beginners.git&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Join The Azure AI Foundry Discord and meet experts and fellow developers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Arabic | Bengali | Bulgarian | Burmese (Myanmar) | Chinese (Simplified) | Chinese (Traditional, Hong Kong) | Chinese (Traditional, Macau) | Chinese (Traditional, Taiwan) | Croatian | Czech | Danish | Dutch | Estonian | Finnish | French | German | Greek | Hebrew | Hindi | Hungarian | Indonesian | Italian | Japanese | Korean | Lithuanian | Malay | Marathi | Nepali | Norwegian | Persian (Farsi) | Polish | Portuguese (Brazil) | Portuguese (Portugal) | Punjabi (Gurmukhi) | Romanian | Russian | Serbian (Cyrillic) | Slovak | Slovenian | Spanish | Swahili | Swedish | Tagalog (Filipino) | Tamil | Thai | Turkish | Ukrainian | Urdu | Vietnamese&lt;/p&gt;
    &lt;p&gt;If you wish to have additional translations languages supported are listed here&lt;/p&gt;
    &lt;p&gt;Welcome to EdgeAI for Beginners – your comprehensive journey into the transformative world of Edge Artificial Intelligence. This course bridges the gap between powerful AI capabilities and practical, real-world deployment on edge devices, empowering you to harness AI's potential directly where data is generated and decisions need to be made.&lt;/p&gt;
    &lt;p&gt;This course takes you from fundamental concepts to production-ready implementations, covering:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Small Language Models (SLMs) optimized for edge deployment&lt;/item&gt;
      &lt;item&gt;Hardware-aware optimization across diverse platforms&lt;/item&gt;
      &lt;item&gt;Real-time inference with privacy-preserving capabilities&lt;/item&gt;
      &lt;item&gt;Production deployment strategies for enterprise applications&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Edge AI represents a paradigm shift that addresses critical modern challenges:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Privacy &amp;amp; Security: Process sensitive data locally without cloud exposure&lt;/item&gt;
      &lt;item&gt;Real-time Performance: Eliminate network latency for time-critical applications&lt;/item&gt;
      &lt;item&gt;Cost Efficiency: Reduce bandwidth and cloud computing expenses&lt;/item&gt;
      &lt;item&gt;Resilient Operations: Maintain functionality during network outages&lt;/item&gt;
      &lt;item&gt;Regulatory Compliance: Meet data sovereignty requirements&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Edge AI refers to running AI algorithms and language models locally on hardware, close to where data is generated without relying on cloud resources for inference. It reduces latency, enhances privacy, and enables real-time decision-making.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;On-device inference: AI models run on edge devices (phones, routers, microcontrollers, industrial PCs)&lt;/item&gt;
      &lt;item&gt;Offline capability: Functions without persistent internet connectivity&lt;/item&gt;
      &lt;item&gt;Low latency: Immediate responses suited for real-time systems&lt;/item&gt;
      &lt;item&gt;Data sovereignty: Keeps sensitive data local, improving security and compliance&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SLMs like Phi-4, Mistral-7B, and Gemma are optimized versions of larger LLMs—trained or distilled for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reduced memory footprint: Efficient use of limited edge device memory&lt;/item&gt;
      &lt;item&gt;Lower compute demand: Optimized for CPU and edge GPU performance&lt;/item&gt;
      &lt;item&gt;Faster startup times: Quick initialization for responsive applications&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They unlock powerful NLP capabilities while meeting the constraints of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Embedded systems: IoT devices and industrial controllers&lt;/item&gt;
      &lt;item&gt;Mobile devices: Smartphones and tablets with offline capabilities&lt;/item&gt;
      &lt;item&gt;IoT Devices: Sensors and smart devices with limited resources&lt;/item&gt;
      &lt;item&gt;Edge servers: Local processing units with limited GPU resources&lt;/item&gt;
      &lt;item&gt;Personal Computers: Desktop and laptop deployment scenarios&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Module&lt;/cell&gt;
        &lt;cell role="head"&gt;Topic&lt;/cell&gt;
        &lt;cell role="head"&gt;Focus Area&lt;/cell&gt;
        &lt;cell role="head"&gt;Key Content&lt;/cell&gt;
        &lt;cell role="head"&gt;Level&lt;/cell&gt;
        &lt;cell role="head"&gt;Duration&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;📖 00&lt;/cell&gt;
        &lt;cell&gt;Introduction to EdgeAI&lt;/cell&gt;
        &lt;cell&gt;Foundation &amp;amp; Context&lt;/cell&gt;
        &lt;cell&gt;EdgeAI Overview • Industry Applications • SLM Introduction • Learning Objectives&lt;/cell&gt;
        &lt;cell&gt;Beginner&lt;/cell&gt;
        &lt;cell&gt;1-2 hrs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;📚 01&lt;/cell&gt;
        &lt;cell&gt;EdgeAI Fundamentals&lt;/cell&gt;
        &lt;cell&gt;Cloud vs Edge AI comparison&lt;/cell&gt;
        &lt;cell&gt;EdgeAI Fundamentals • Real World Case Studies • Implementation Guide • Edge Deployment&lt;/cell&gt;
        &lt;cell&gt;Beginner&lt;/cell&gt;
        &lt;cell&gt;3-4 hrs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;🧠 02&lt;/cell&gt;
        &lt;cell&gt;SLM Model Foundations&lt;/cell&gt;
        &lt;cell&gt;Model families &amp;amp; architecture&lt;/cell&gt;
        &lt;cell&gt;Phi Family • Qwen Family • Gemma Family • BitNET • μModel • Phi-Silica&lt;/cell&gt;
        &lt;cell&gt;Beginner&lt;/cell&gt;
        &lt;cell&gt;4-5 hrs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;🚀 03&lt;/cell&gt;
        &lt;cell&gt;SLM Deployment Practice&lt;/cell&gt;
        &lt;cell&gt;Local &amp;amp; cloud deployment&lt;/cell&gt;
        &lt;cell&gt;Advanced Learning • Local Environment • Cloud Deployment&lt;/cell&gt;
        &lt;cell&gt;Intermediate&lt;/cell&gt;
        &lt;cell&gt;4-5 hrs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;⚙️ 04&lt;/cell&gt;
        &lt;cell&gt;Model Optimization Toolkit&lt;/cell&gt;
        &lt;cell&gt;Cross-platform optimization&lt;/cell&gt;
        &lt;cell&gt;Introduction • Llama.cpp • Microsoft Olive • OpenVINO • Apple MLX • Workflow Synthesis&lt;/cell&gt;
        &lt;cell&gt;Intermediate&lt;/cell&gt;
        &lt;cell&gt;5-6 hrs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;🔧 05&lt;/cell&gt;
        &lt;cell&gt;SLMOps Production&lt;/cell&gt;
        &lt;cell&gt;Production operations&lt;/cell&gt;
        &lt;cell&gt;SLMOps Introduction • Model Distillation • Fine-tuning • Production Deployment&lt;/cell&gt;
        &lt;cell&gt;Advanced&lt;/cell&gt;
        &lt;cell&gt;5-6 hrs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;🤖 06&lt;/cell&gt;
        &lt;cell&gt;AI Agents &amp;amp; Function Calling&lt;/cell&gt;
        &lt;cell&gt;Agent frameworks &amp;amp; MCP&lt;/cell&gt;
        &lt;cell&gt;Agent Introduction • Function Calling • Model Context Protocol&lt;/cell&gt;
        &lt;cell&gt;Advanced&lt;/cell&gt;
        &lt;cell&gt;4-5 hrs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;💻 07&lt;/cell&gt;
        &lt;cell&gt;Platform Implementation&lt;/cell&gt;
        &lt;cell&gt;Cross-platform samples&lt;/cell&gt;
        &lt;cell&gt;AI Toolkit • Foundry Local • Windows Development&lt;/cell&gt;
        &lt;cell&gt;Advanced&lt;/cell&gt;
        &lt;cell&gt;3-4 hrs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;🏭 08&lt;/cell&gt;
        &lt;cell&gt;Foundry Local Toolkit&lt;/cell&gt;
        &lt;cell&gt;Production-ready samples&lt;/cell&gt;
        &lt;cell&gt;Sample applications (see details below)&lt;/cell&gt;
        &lt;cell&gt;Expert&lt;/cell&gt;
        &lt;cell&gt;8-10 hrs&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;01: REST Chat Quickstart&lt;/item&gt;
      &lt;item&gt;02: OpenAI SDK Integration&lt;/item&gt;
      &lt;item&gt;03: Model Discovery &amp;amp; Benchmarking&lt;/item&gt;
      &lt;item&gt;04: Chainlit RAG Application&lt;/item&gt;
      &lt;item&gt;05: Multi-Agent Orchestration&lt;/item&gt;
      &lt;item&gt;06: Models-as-Tools Router&lt;/item&gt;
      &lt;item&gt;07: Direct API Client&lt;/item&gt;
      &lt;item&gt;08: Windows 11 Chat App&lt;/item&gt;
      &lt;item&gt;09: Advanced Multi-Agent System&lt;/item&gt;
      &lt;item&gt;10: Foundry Tools Framework&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Comprehensive hands-on workshop materials with production-ready implementations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Workshop Guide - Complete learning objectives, outcomes, and resource navigation&lt;/item&gt;
      &lt;item&gt;Python Samples (6 sessions) - Updated with best practices, error handling, and comprehensive documentation&lt;/item&gt;
      &lt;item&gt;Jupyter Notebooks (8 interactive) - Step-by-step tutorials with benchmarks and performance monitoring&lt;/item&gt;
      &lt;item&gt;Session Guides - Detailed markdown guides for each workshop session&lt;/item&gt;
      &lt;item&gt;Validation Tools - Scripts to verify code quality and run smoke tests&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What You'll Build:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Local AI chat applications with streaming support&lt;/item&gt;
      &lt;item&gt;RAG pipelines with quality evaluation (RAGAS)&lt;/item&gt;
      &lt;item&gt;Multi-model benchmarking and comparison tools&lt;/item&gt;
      &lt;item&gt;Multi-agent orchestration systems&lt;/item&gt;
      &lt;item&gt;Intelligent model routing with task-based selection&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Total Duration: 36-45 hours&lt;/item&gt;
      &lt;item&gt;Beginner Path: Modules 01-02 (7-9 hours)&lt;/item&gt;
      &lt;item&gt;Intermediate Path: Modules 03-04 (9-11 hours)&lt;/item&gt;
      &lt;item&gt;Advanced Path: Modules 05-07 (12-15 hours)&lt;/item&gt;
      &lt;item&gt;Expert Path: Module 08 (8-10 hours)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Edge AI Architecture: Design local-first AI systems with cloud integration&lt;/item&gt;
      &lt;item&gt;Model Optimization: Quantize and compress models for edge deployment (85% speed boost, 75% size reduction)&lt;/item&gt;
      &lt;item&gt;Multi-Platform Deployment: Windows, mobile, embedded, and cloud-edge hybrid systems&lt;/item&gt;
      &lt;item&gt;Production Operations: Monitoring, scaling, and maintaining edge AI in production&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Foundry Local Chat Apps: Windows 11 native application with model switching&lt;/item&gt;
      &lt;item&gt;Multi-Agent Systems: Coordinator with specialist agents for complex workflows&lt;/item&gt;
      &lt;item&gt;RAG Applications: Local document processing with vector search&lt;/item&gt;
      &lt;item&gt;Model Routers: Intelligent selection between models based on task analysis&lt;/item&gt;
      &lt;item&gt;API Frameworks: Production-ready clients with streaming and health monitoring&lt;/item&gt;
      &lt;item&gt;Cross-Platform Tools: LangChain/Semantic Kernel integration patterns&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Manufacturing • Healthcare • Autonomous Vehicles • Smart Cities • Mobile Apps&lt;/p&gt;
    &lt;p&gt;Recommended Learning Path (20-30 hours total):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;📖 Introduction (Introduction.md): EdgeAI foundation + industry context + learning framework&lt;/item&gt;
      &lt;item&gt;📚 Foundation (Modules 01-02): EdgeAI concepts + SLM model families&lt;/item&gt;
      &lt;item&gt;⚙️ Optimization (Modules 03-04): Deployment + quantization frameworks&lt;/item&gt;
      &lt;item&gt;🚀 Production (Modules 05-06): SLMOps + AI agents + function calling&lt;/item&gt;
      &lt;item&gt;💻 Implementation (Modules 07-08): Platform samples + Foundry Local toolkit&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each module includes theory, hands-on exercises, and production-ready code samples.&lt;/p&gt;
    &lt;p&gt;Technical Roles: EdgeAI Solutions Architect • ML Engineer (Edge) • IoT AI Developer • Mobile AI Developer&lt;/p&gt;
    &lt;p&gt;Industry Sectors: Manufacturing 4.0 • Healthcare Tech • Autonomous Systems • FinTech • Consumer Electronics&lt;/p&gt;
    &lt;p&gt;Portfolio Projects: Multi-agent systems • Production RAG apps • Cross-platform deployment • Performance optimization&lt;/p&gt;
    &lt;code&gt;edgeai-for-beginners/
├── 📖 introduction.md  # Foundation: EdgeAI Overview &amp;amp; Learning Framework
├── 📚 Module01-04/     # Fundamentals → SLMs → Deployment → Optimization  
├── 🔧 Module05-06/     # SLMOps → AI Agents → Function Calling
├── 💻 Module07/        # Platform Samples (VS Code, Windows, Jetson, Mobile)
├── 🏭 Module08/        # Foundry Local Toolkit + 10 Comprehensive Samples
│   ├── samples/01-06/  # Foundation: REST, SDK, RAG, Agents, Routing
│   └── samples/07-10/  # Advanced: API Client, Windows App, Enterprise Agents, Tools
├── 🌐 translations/    # Multi-language support (8+ languages)
└── 📋 STUDY_GUIDE.md   # Structured learning paths &amp;amp; time allocation
&lt;/code&gt;
    &lt;p&gt;✅ Progressive Learning: Theory → Practice → Production deployment&lt;lb/&gt; ✅ Real Case Studies: Microsoft, Japan Airlines, enterprise implementations&lt;lb/&gt; ✅ Hands-on Samples: 50+ examples, 10 comprehensive Foundry Local demos&lt;lb/&gt; ✅ Performance Focus: 85% speed improvements, 75% size reductions&lt;lb/&gt; ✅ Multi-Platform: Windows, mobile, embedded, cloud-edge hybrid&lt;lb/&gt; ✅ Production Ready: Monitoring, scaling, security, compliance frameworks&lt;/p&gt;
    &lt;p&gt;📖 Study Guide Available: Structured 20-hour learning path with time allocation guidance and self-assessment tools.&lt;/p&gt;
    &lt;p&gt;EdgeAI represents the future of AI deployment: local-first, privacy-preserving, and efficient. Master these skills to build the next generation of intelligent applications.&lt;/p&gt;
    &lt;p&gt;Our team produces other courses! Check out:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;MCP for Beginners&lt;/item&gt;
      &lt;item&gt;AI Agents For Beginners&lt;/item&gt;
      &lt;item&gt;Generative AI for Beginners using .NET&lt;/item&gt;
      &lt;item&gt;Generative AI for Beginners using JavaScript&lt;/item&gt;
      &lt;item&gt;Generative AI for Beginners&lt;/item&gt;
      &lt;item&gt;ML for Beginners&lt;/item&gt;
      &lt;item&gt;Data Science for Beginners&lt;/item&gt;
      &lt;item&gt;AI for Beginners&lt;/item&gt;
      &lt;item&gt;Cybersecurity for Beginners&lt;/item&gt;
      &lt;item&gt;Web Dev for Beginners&lt;/item&gt;
      &lt;item&gt;IoT for Beginners&lt;/item&gt;
      &lt;item&gt;XR Development for Beginners&lt;/item&gt;
      &lt;item&gt;Mastering GitHub Copilot for AI Paired Programming&lt;/item&gt;
      &lt;item&gt;Mastering GitHub Copilot for C#/.NET Developers&lt;/item&gt;
      &lt;item&gt;Choose Your Own Copilot Adventure&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you get stuck or have any questions about building AI apps, join:&lt;/p&gt;
    &lt;p&gt;If you have product feedback or errors while building visit:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45561700</guid><pubDate>Sun, 12 Oct 2025 20:41:01 +0000</pubDate></item><item><title>MAML – A new configuration language</title><link>https://maml.dev/</link><description>&lt;doc fingerprint="bf40c47bc79692da"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;MAML&lt;/head&gt;
    &lt;p&gt;Minimal. Human-readable. Machine-parsable.&lt;/p&gt;
    &lt;p&gt;Minimal. Human-readable. Machine-parsable.&lt;/p&gt;
    &lt;code&gt;{
  project: "MAML"
  tags: [
    "minimal"
    "readable"
  ]

  # A simple nested object
  spec: {
    version: 1
    author: "Anton Medvedev"
  }

  # Array of objects
  examples: [
    { name: "JSON", born: 2001 }
    { name: "MAML", born: 2025 }    
  ]

  notes: """
This is a multiline strings.
Keeps formatting as-is.
"""
}&lt;/code&gt;
    &lt;p&gt;JSON is the most popular data-interchange format. But it isn't a very good configuration language.&lt;/p&gt;
    &lt;p&gt;MAML keeps JSON’s simplicity and adds only the needed bits for a good configuration language:&lt;/p&gt;
    &lt;p&gt;MAML is human-readable and easy to parse.&lt;/p&gt;
    &lt;p&gt;MAML is a brand-new language, so feel free to create new implementations following the specification.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45562056</guid><pubDate>Sun, 12 Oct 2025 21:24:39 +0000</pubDate></item><item><title>An initial investigation into WDDM on ReactOS</title><link>https://reactos.org/blogs/investigating-wddm/</link><description>&lt;doc fingerprint="4f899b575de63905"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;An initial investigation into WDDM on ReactOS&lt;/head&gt;&lt;p&gt;by The_DarkFire_ | October 7, 2025&lt;/p&gt;&lt;p&gt;The history of ReactOS spans a wider range than the lives of many of the people who work on it today. Incredible individuals have come and gone from the project with vastly different goals for what they want to see developed. In recent years, better hardware support has emerged as one of those goals. As ReactOS gazes towards the world of Vista and beyond, a few questions about how hardware works emerge. Vista introduced massive overhauls to how hardware drivers are written and maintained. Gradually weâre trying to handle many of these overhauls with great success. Today we talk about WDDM, or the Windows Display Driver Model.&lt;/p&gt;&lt;head rend="h3"&gt;The âOverhaulâ&lt;/head&gt;&lt;p&gt;Iâve always found the video driver documentation lacking for both architectures, enough to get started but not really enough to get deep into the world of 3D. Thankfully in recent years this has changed enough to where some open-source drivers have popped up. WDDM is a major overhaul that shifts responsibility of managing the GPU away from Win32k and gives better control over the GPU to the driver vendor. Dxgkrnl.sys, the DirectX graphics driver, talks to a miniport driver to provide varying levels of WDDM interfaces. The WDDM revision (WDDM 1.0, 1.1, 1.2â¦.) primarily describes how many of these interfaces are supported and implemented. This is different than the feature level that is described in DxDiag.&lt;/p&gt;&lt;head rend="h3"&gt;Waitâ¦ What happened to XDDM?&lt;/head&gt;&lt;p&gt;Officially starting with Windows 8, every GPU driver for the system had to be a WDDM driver. But all that was really dropped was the miniport driver. Vista and 7 could still load older GPU drivers using the XDDM architecture without complaint. Outside of the miniport driver, modern Windows still has XDDM remnants all over the place, including mechanisms for WDDM. The first example of this came when trying to load OpenGL installable client drivers (ICDs) from Vista, or loading our ICDs on Vista, 7, 8, and so on. For WDDM, the communication back to the miniport driver is more direct. Win32k only holds the syscall jump back into an interface thatâs filled in by Dxgkrnl. Itâs important to note the module that loads OpenGL ICDs hasnât changed very much.&lt;/p&gt;Interestingly Vulkan behaves similarly: OpenGL ICDs are one thing, but what about the display driver? Watching what Vista and later Win32k does while starting up shows some interesting behaviors. Thereâs two important display drivers we should talk about: TSDDD.dll and CDD.dll. TSDDD is manually loaded when session 0 starts. This is just a normal XDDM display driver that does nothing except write to blank memory. In NT5.x, or ReactOS at the time of writing, if your install canât get a valid display itâll bug check (BSoD) with a code meaning video initialization failure. Itâs possible to still make this happen with Vista and later but this failure no longer happens in practice due to CDD acting as the primary driver. In fact, if the video driver fails it’ll fall back to the basic display driver. CDD.dll is far more interesting: On one hand, it is an XDDM display driver. It also fires two I/O control codes (IOCTLs) which facilitates communication with WDDM. This driver is the only route which Dxgkrnl and Win32k use to talk to each other in any meaningful capacity. Technically, thereâs also some communication between watchdog, Win32k, and Dxgkrnl on initialization to fill in the interface to dispatch D3DKMT APIs for Dxgkrnl; but thatâs only during the initialization of Dxgkrnl itself. Itâs also worth noting, when this happens the query in Win32k to find a supported display adapter is always manually overwritten with cdd.dll. Once you start a WDDM driver you cannot run an XDDM driver at the same time! Alright that was a lot of information, but thereâs some important things to understand here. CDD.dll first and foremost IS an XDDM display driver, even if itâs translating from the old world Win32k to the new world WDDM stack itâs still stressing Win32k out. This means for ReactOS to be truly compatible with WDDM, our XDDM stack must be in great shape. This isnât the only Vista+ feature that has this requirement. DWM (which is worth its own blog post) does a lot of things that even the current ReactOS Win32k just isnât capable of handling quite yet. But we’re constantly improving.&lt;head rend="h3"&gt;Compiling WDDM drivers with ReactOS&lt;/head&gt;&lt;p&gt;One of the first components that needed to be understood was displib.lib, a component shipped with the WDK that allows for compiling WDDM drivers. https://learn.microsoft.com/en-us/windows-hardware/drivers/ddi/dispmprt/nf-dispmprt-dxgkinitializedisplayonlydriver In order to compile a WDDM driver, you need to link with a library that implements a function like the above to allow your driver to âstartâ Dxgkrnl. WDDM drivers donât link against Dxgkrnl at all! Instead, when you call an API like the one above, you pass data to Dxgkrnl, which then calls back into your miniportâs initialization routine. That callback is what provides the interfaces you need to continue communicating with Dxgkrnl. Win32k has nothing to do with the miniport driver starting up! This was relatively straightforward to make an alternative to, opening the door for ReactOS to import and compile WDDM drivers that work even on Windows!&lt;/p&gt;&lt;head rend="h3"&gt;WDDM! Technically..?&lt;/head&gt;&lt;p&gt;Now that we know how WDDM drivers start up, we can start thinking about how to support this architecture on ReactOS. The D3DKMT APIs are only used for DirectX and OpenGL acceleration so we can ignore that for now. What about just getting the display to work? ReactOS currently supports some 2D acceleration and is rapidly developing support for XDDM AMD GPUs so surely, we can get some kind of basis working? Dxgkrnl is a massive beast with a scheduler and many subsystems for managing these miniport drivers; but only two are really required for display management: VidPn (Video Presentation Network) and its hardware support. Starting with Windows 8 thereâs a built-in driver and style of WDDM miniports that drop 3D acceleration called KMDODâs. Theyâre a slightly different initialization type, but in general are easy to understand. Thankfully they donât use a lot of the interfaces DxgKrnl provides to miniport drivers to communicate with the hardware, so we can create a very simple Dxgkrnl for our experiment. All we need is to query the display modes from the VidPn network, pass it to CDD, load CDD when Dxgkrnl is active and … Ah hah!&lt;/p&gt;ReactOS can communicate with its first WDDM driver!&lt;head rend="h3"&gt;A more forgiving architecture&lt;/head&gt;&lt;p&gt;When I first got BasicDisplay.sys loading in ReactOS I noticed how forgiving WDDM truly was. I realized I could escalate this further than Iâd anticipated. It turned out these vendor drivers are very willing to accept being started just for their display for example.&lt;/p&gt;&lt;p&gt;I was quickly getting more drivers to show some kind of display out, allowing ReactOS to power modern monitors at their full resolutions and refresh rates. But I quickly was getting limited not by our implementation of Win32k but instead our support for hardware itself.&lt;/p&gt;&lt;head rend="h3"&gt;Why I wrote this and what’s to come&lt;/head&gt;&lt;p&gt;Thereâs two things I hoped to accomplish with this blog post. The first was to put out that ReactOS is indeed looking at trying to support later hardware, but that we canât just jump and ignore XDDM. XDDM is REQUIRED for WDDM, we need to continue to improve in this area. The other reason was because this is the first of a few blog posts I intend to write for the ReactOS project on this subject. Weâve been actively working to improve hardware support to unblock these future ideas and need more support to do so! To help support the ReactOS project, you can make a donation, contribute to the project on GitHub, or talk to your friends and family about us! We look forward to sharing more hardware support and WDDM blog posts.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45562188</guid><pubDate>Sun, 12 Oct 2025 21:40:43 +0000</pubDate></item><item><title>Free software hasn't won</title><link>https://dorotac.eu/posts/fosswon/</link><description>&lt;doc fingerprint="ee449113cc27a1bd"&gt;
  &lt;main&gt;
    &lt;p&gt;This is a translated version of a talk I gave at P.I.W.O in June, with cleanups and adjustments for the blog form.&lt;/p&gt;
    &lt;head rend="h1"&gt;# Free Software hasn't won&lt;/head&gt;
    &lt;p&gt;â¦that doesn't sound right. I made the slides in Inkscape, on a computer running KDE and Linux, I use Firefox regularly. But maybe that's just me. What about you, are you using Free Software? Hands up! [hands go up in the audience] Of course! What nonsense, "Free Software hasn't won". Someone replaced my slides, hey conference staff!&lt;/p&gt;
    &lt;p&gt;**Staff:** *The other folder.*&lt;/p&gt;
    &lt;p&gt;[Browsing to a directory named "other folder", opening file called "your slides dimwit.pdf"]&lt;/p&gt;
    &lt;p&gt;Now, those are finally my slides.&lt;/p&gt;
    &lt;p&gt;Hello audience, my name is Dorota, and I'm going to talk about how&lt;/p&gt;
    &lt;head rend="h1"&gt;# Open Source has won.&lt;/head&gt;
    &lt;p&gt;And that's not recent, the news has been out in 2008, and has been regularly repeated since by reputable press: ZDNET, Linux Journal, Wired, and so on.&lt;/p&gt;
    &lt;p&gt;Those press articles list a multitude of examples to prove it.&lt;/p&gt;
    &lt;p&gt;Linux, Ruby, Red Hat, uh, GitHub? Does that mean I can download GitHub and run it on my own server? Microsoft? Come on, that's some kind of a joke. Those slides are manipulated! So what else do they contain? Oh, this quote is all right:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;&amp;gt; Open source won. Itâs not that an enemy has been vanquished or that proprietary software is dead, thereâs not much regarding adopting open source to argue about anymore. After more than a decade of the low-cost, lean startup culture successfully developing on open source tools, itâs clearly a legitimate, mainstream option for technology tools and innovation.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Oh, the name of the quoted person is wrong. Looks like an attack on my reputation! Anyways.&lt;/p&gt;
    &lt;p&gt;The point is, if we want to build something new, using Free Software is not a hindrance. And thats super important, because software is eating the world. What does it mean? It means software keeps appearing in areas where there used to be no software before. That, in turn, means that we're slowly giving up control over more and more areas of life to those who made the software. After all, their software controls those areas of life from now on.&lt;/p&gt;
    &lt;p&gt;That's why it's great that there's always an alternative available, that we can select software that is free which grants control to us, and not just its manufacturer. So we have alternate operating systems made with Linux. There are very many programming languages to choose from. We can choose one of the open games, or graphics or audio creation software without resorting to closed software.&lt;/p&gt;
    &lt;p&gt;Similarly, we don't need closed software to print in 3D, or to build a mobile computer (also known as smartphone) or a smart watch. There are graphics cards which run completely free of closed firmware (Upon asking nouveau devs, they confirmed they wrote some firmware. Nvidia Kepler from 2012 is the last model where free firmware is allowed). There are such bicycles! Pretty much everyone owns one that rides without closed software. There are also sewing machines:&lt;/p&gt;
    &lt;p&gt;There are comms systems:&lt;/p&gt;
    &lt;p&gt;There are cars, and have been for a long time:&lt;/p&gt;
    &lt;p&gt;There are hard drives:&lt;/p&gt;
    &lt;p&gt;[the slides go blank]&lt;/p&gt;
    &lt;p&gt;There are wireless headphones, TVs... [slides remain blank] wait, something's wrong. There are phones! [Slides stay mockingly blank, noise of frantic clicking.]&lt;/p&gt;
    &lt;p&gt;[â¦]&lt;/p&gt;
    &lt;p&gt;[â¦]&lt;/p&gt;
    &lt;p&gt;[â¦]&lt;/p&gt;
    &lt;p&gt;Crap. This isn't right.&lt;/p&gt;
    &lt;p&gt;Oh, now I get it! The only kind of a phone that grants us openness is an analog phone. That reminds me of the time we were building the aforementioned Librem 5. There was a problem finding the modem for it. The reason is, one company controls the necessary patents necessary to connect to cellular networks. That company can and does impose arbitrary conditions on anyone using their integrated circuits. That made it very difficult to find modems that match our needs, and at the same time any reseller is willing to sell us. The resellers worried that by passing them on to us, they could break some distribution rule and not be able to get any modems in the future.&lt;/p&gt;
    &lt;p&gt;So that's a no. But I know what will be open for sure.&lt;/p&gt;
    &lt;p&gt;Richard Stallman started an important project called GNU in 1983. In one of his interviews, as he describes how he started the project, he mentions a certain device that his university bought, but which didn't work very well. He wanted to improve it, but no one wanted to share the device's sources with him. That was an offense! Why wouldn't anyone share the code?&lt;/p&gt;
    &lt;p&gt;What was that device?&lt;/p&gt;
    &lt;p&gt;That was a printer. Considering that the GNU project started in 1983 and that the story's from 1981, it works out to over 40 years of fighting for printer freedom. So let's reveal our open alternative.&lt;/p&gt;
    &lt;p&gt;Oh come on. This cannot be. Have you ever used a printer? If you even manage to find a driver, if you even manage to connect to the printer, then it's still going to print single-sided black-and-white when you asked double-sided color. And despite having to put up with that, Free Software people still haven't gotten frustrated enough to solve the problem once and for all? Unbelievable.&lt;/p&gt;
    &lt;p&gt;I have a theory. People who say "Open Source has won" are only taking into account a small part of what software is out there. Take a look at this list: it's a map showing which kinds of software force you into running something closed (bold) and which have open options available (italics).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Applications: *Blender, Firefox, KiCAD* â **Twitter, YouTube**&lt;/item&gt;
      &lt;item&gt;Operating System: *GCC, Apache, OpenSSL*&lt;/item&gt;
      &lt;item&gt;Kernel: *Linux, Zephyr, FreeRTOS*&lt;/item&gt;
      &lt;item&gt;Firmware: *Coreboot* â **modem, GPU**&lt;/item&gt;
      &lt;item&gt;Appliances: *Prusa 3D, Airgradient* â **washing machine, TV**&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What picture does this paint? Things programmers care about directly, like the OS and the kernel, are quite well covered. Whatever we need, there's an open version. Applications are also more or less fine. There's a Web browser, there's creativity software. The problem appears when you try to participate in social media. Sure, there are alternatives. But Mastodon, or PeerTube are separate networks from the closed ones, so they won't help much when trying to reach people who aren't yet using them.&lt;/p&gt;
    &lt;p&gt;Looking at the lower layers, like appliances or firmware, there seem to be options. But those options are limited to a couple niches, and with most things we buy, like a TV or a PC component â sorry, pal, there's simply no choice at all.&lt;/p&gt;
    &lt;head rend="h2"&gt;## All the firmwares in the average laptop&lt;/head&gt;
    &lt;p&gt;How many processors are there in a typical laptop? By "processor" I mean something that needs its own software. For example, a GPU has its own processor that needs software, or a hard drive, or a keyboard. Here's a diagram of my personal estimate of what separate components need software:&lt;/p&gt;
    &lt;p&gt;I estimate there are 10 to 15 separate processors on a typical laptop. Just the graphics card may host five of them.&lt;/p&gt;
    &lt;p&gt;What does that mean for free software? Normally, all that's open â Linux, drivers, applications â all of this is confined to the main CPU. Now imagine you want to use this operating system through some human-friendly interface, like the touch screen or the keyboard. Those are running closed software, so if you want to enter any sort of data on your average laptop, it's game over: you can't make a move without dependence on closed software.&lt;/p&gt;
    &lt;p&gt;Same story with the graphics card. You won't display anything without closed software. What a fail. Okay, let's ditch keyboards and displays because this is a server. But that's a fail, too: to communicate over a network card, you still need software that it's running and that hasn't been opened. Suppose that we managed to somehow solve this problem. We hit the wall anyway when we try to store data: SSDs as well as HDDs are running their own closed software. I haven't heard of a single case of open software ever running on a storage device!&lt;/p&gt;
    &lt;p&gt;But that's not even the worst. The peak of lameness is the processor inside the processor. Have you already heard of Secure Boot? It's a piece of BIOS that is loaded onto the processor inside the main processor before the main operating system. Secure Boot allows the manufacturer choose which software the user can run. A similar system exists on Android phones to lock them to a particular system. Manufacturers of Android-based phones are not shy about restricting what the user can run on their devices.&lt;/p&gt;
    &lt;head rend="h2"&gt;## That runs against user's freedom!&lt;/head&gt;
    &lt;p&gt;User freedom exists only when the Four Freedoms of Software are upheld:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;0: freedom to run the program for any purpose,&lt;/item&gt;
      &lt;item&gt;1: to study and change it,&lt;/item&gt;
      &lt;item&gt;2: to share copies of it,&lt;/item&gt;
      &lt;item&gt;3: to improve it and share the improvements.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;â¦but those are just words. Who cares about that? This theory is only ever going to be relevant to us computer experts, right?&lt;/p&gt;
    &lt;p&gt;Exceptâ¦ could it be that you're the family's tech support expert? Does your uncle/mum/grandma come to you carrying their malfunctioning Android phone, hoping for you to make everything right again?&lt;/p&gt;
    &lt;p&gt;And have you ever disappointed them? Has it already happened that their phone was simply too old and unsupported to be useful any more? Have you already told someone they need to pay up to replace a phone that seems perfectly functional?&lt;/p&gt;
    &lt;p&gt;Sadly, that's what the Android manufacturer support timelines say: typically after 4, exceptionally after 8 years, they will no longer release security updates. That makes devices too insecure to use, and turns them into e-waste.&lt;/p&gt;
    &lt;p&gt;What does Free Software have to do with it? I don't know, but my Lenovo laptop, 13 years since release of the hardware, the operating system is still receiving regular security updates. I suspect this has something to do with the lack of a boot loader lock and the openness of all the drivers. That's unlike Android. Even if there's no explicit lock, the drivers are so rarely open that the community rarely has the manpower to create a custom ROM for a given device.&lt;/p&gt;
    &lt;head rend="h2"&gt;## Rug pulls&lt;/head&gt;
    &lt;p&gt;The couple hundred bucks that your aunt might need to pony up to get a new phone pale in comparison to how much you need to pay for some cloud-only devices. Some cloud-enabled gadgets don't let the user choose an alternative provider for services the device requires. What happens when the company shuts down the online service? Of course, the device becomes an expensive brick. Imagine someone setting your 2300 bucks on fire just like that.&lt;/p&gt;
    &lt;p&gt;That's still nothing compared to what some other people have to deal with. Imagine you're a farmer, and your harvest is on the field, ready to get cut and brought in. T here's a storm brewing, so you jump into the combine harvester, and start the work. Oh no! The machine broke down! Not to worry, you're a resourceful farmer and you have the necessary spare part. You install it and start the machine, exceptâ¦ it tells you: "Unauthorized component. Please contact customer service". Now you're in real trouble because it could take 9 months for the customer service to solve the problem. You can't harvest the food worth tens of thousands of dollars, you're that much in the red. Game over, your farm is bankrupt. But that's not the end of the world, is it?&lt;/p&gt;
    &lt;p&gt;This is what a pacemaker looks like. Why would I mention those in a talk about software freedom? You see, a pacemaker is a complex device which must examine and diagnose the patient continuously, in real time, in order to perform its function. Its task is to detect a dangerous condition and perform a medical procedure in response to it. It needs software to do this complicated task. But if the device isn't perfect at diagnosing, that's a big problem. I'm not a medical expert, but getting your heart shocked when it's not necessary sounds dangerous in its own right. When it runs closed software that does not grant us the freedom to modify it, we have to resort to begging the manufacturer to fix it. And when we get no freedom to study it, we can't even avoid the circumstances that make it misfire!&lt;/p&gt;
    &lt;p&gt;But don't take my word for it. I only know of this problem because of Karen Sandler, whose involvement with Free Software is intertwined with this problem since the beginning.&lt;/p&gt;
    &lt;p&gt;The bottom line is, if we have people who have no other choice but to trust their own body to a piece of closed software and a single manufacturer, how could we possibly say that Open Source had won?&lt;/p&gt;
    &lt;head rend="h2"&gt;## Appliances and copyleft&lt;/head&gt;
    &lt;p&gt;Are you responsible for building an appliance? I bet you're using Open Source software in it, aren't you? Then licenses like the MIT require you to include a notice about the authors of the source code together with the software you distribute. There's a whole gallery of those on the curl website, ranging from cars to food processors. Are you feeling proud for releasing a device with Free Software in it? Not so quick! Can the user of your device study and modify the software you gave them? Have you actually granted them the Four Freedoms?&lt;/p&gt;
    &lt;p&gt;Permissive licenses like the MIT license are Free Software, so they let you do all that the Four Freedoms promise. But they also allow you to do another thing: to close the software again by never granting those freedoms regarding your own modifications. If that's what happened, then freedom for me, not for thee. You, the manufacturer, reaped the benefits, the user can't, sucks to be the user.&lt;/p&gt;
    &lt;p&gt;The responsibility to prevent this falls on us, computer experts. When we create software, we have the choice of license we want to release it under. And we should be using what's called "copyleft": it's a term that applies to licenses which prevent code once released under that license from being closed again. The most widespread copyleft license is the Gnu General Public License (GPL), and I recommend that you all use that one.&lt;/p&gt;
    &lt;head rend="h2"&gt;## Licenses and more&lt;/head&gt;
    &lt;p&gt;Licenses are not the only thing relevant for Free Software. There are other things to fight:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;patents, like in the case of cellular modems,&lt;/item&gt;
      &lt;item&gt;hardware locks, like Android's,&lt;/item&gt;
      &lt;item&gt;project management.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As for the last point, recently Google gave an amazing example by restricting access to sources in development to select manufacturers. Everyone else will not get continuous updates, but only once per major release. This illustrates how much influence over the practical usability of a project management decisions have. This is not a change in licensing, and it's also not a technical change, so it's not immediately visible under those lenses.&lt;/p&gt;
    &lt;p&gt;Instead, it's a consequence of who's in charge. In this case, it's not a community who controls the Android project, but a for-profit corporation. At the same time, it's regular people who are on the user side of the project. Is it any wonder that the goals of a corporation and those of regular people differ? Is it any wonder that the corporation is making changes that suit it even when they don't suit the community of users? When those are the conditions under which a project is developed, it can have deep consequences, even on an architectural level.&lt;/p&gt;
    &lt;p&gt;Take Debian as a point of comparison. The first statement on the web page already says "Debian is a Community of People!". The software is being developed and used by the same people. They won't make it harder to use. They provide a complete operating system, publish all the sources, and purge anything that isn't open enough. On the other hand, Android has long been replacing open components by closed ones, making AOSP (the open part of android) all but unuseable on its own.&lt;/p&gt;
    &lt;head rend="h2"&gt;## Why?&lt;/head&gt;
    &lt;p&gt;I suspect this situation has something to do with how computers and appliances have been developed historically. Computers have their roots in academia. When they were sold, they were always advertised as blank slates, general purpose devices, as opportunities to do what you choose to do with them. Not so much for appliances. Those have always had a single purpose. Except they kept getting complicated, until they entered a level of complexity where they needed to incorporate computers in order to perform their function. But they kept being manufactured as appliances, with only a handful of people being expected or allowed to exercise control over them. Incorporating computers didn't change the culture around them.&lt;/p&gt;
    &lt;p&gt;This is just a guess and I don't know how correct it is. For example Apple was always a computer manufacturer, but they are making computers now as if those were appliances.&lt;/p&gt;
    &lt;head rend="h2"&gt;## What now?&lt;/head&gt;
    &lt;p&gt;The responsibility is ours â computer nerds' â to make Free Software win. When we build a hardware device, we must publish the firmware sources. We must publish technical documentation â it often so happens that the device documentation needed to make open firmware is missing or incomplete (another war story from the Librem 5, camera sensors this time).&lt;/p&gt;
    &lt;p&gt;As users, or institutional customers, we should demand that the manufacturer provides open sources for any firmware they are shipping with their devices.&lt;/p&gt;
    &lt;p&gt;But there's one more way: political pressure. I expect this to be a more effective method than individual action. After all, EU managed to convince phone manufacturers to standardize on USB-C ports for charging, as well as to extend the warranty period. Perhaps they could also force computer manufacturers to not install boot loader locks. It would fit nicely into the Information Society Directive. It says things like:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;&amp;gt; Member States must provide legal protection against any person knowingly performing without authority any of the following acts:&lt;/p&gt;
      &lt;item&gt;&amp;gt; the removal or alteration of any electronic rights-management information;&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;â¦oh. So instead of jailing people who put locks on devices they no longer own, it enforces the jailing of those who remove them from their own devices. Great.&lt;/p&gt;
    &lt;p&gt;Dear European Comission, please always have someone with a clue in the room who can explain the consequences of your ideas in a way you can understand. You can do it, already did a couple times, like above. But work on consistency, okay? Pinky promise?&lt;/p&gt;
    &lt;p&gt;Here are some people with a clue: Free Software Foundation Europe with their Public Money Public Code open letter, the Right to Repair movement, as well as the European Pirate Party.&lt;/p&gt;
    &lt;p&gt;I recommend anyone who cares to join forces with them. But if you don't want to engage politically, there are also financial way of support. And I don't even mean (although I do encourage) donating. I mean supporting Free Software friendly manufacturers! Buy the Librem 5 from Purism, or a 3D printer from Prusa, or a smartwatch running Espruino. You see, it's expensive to manufacture any sort of hardware. It doesn't help that the markets are already saturated with closed products. Even if open source, hackable products are superior, it will take people at large a long time to realize that this is a superpower. Free Software thrived in culture of repair and modification. But this culture has been suffocated in the wider society with closed, throwaway items, so few people recognize its benefits. That unsustainable crowding out makes another obstacle for Open Source friendly products in the current markets.&lt;/p&gt;
    &lt;p&gt;There's a noble exception here. What makes it even more unusual is that it comes from Google. It's Chromebooks. Google has a set of requirements that all Chromebook manufacturers must fulfill, and one of them is having a completely open BIOS, together with the Embedded Controller firmware. All Chromebooks I'm aware of run Coreboot. They still contain some closed software, notably the RAM startup software, which, I believe, is present in all laptops, but! ARM-base Chromebooks are able to run with a completely open BIOS apart from that. So if anyone wants to take care of this together with me, I have this NLNet project to make it as easy as possible to run regular, mainline Linux on them. So please, contact me, if you're that person.&lt;/p&gt;
    &lt;head rend="h2"&gt;## The world&lt;/head&gt;
    &lt;p&gt;A short quiz: how many devices can you count around you which contain processors?&lt;/p&gt;
    &lt;p&gt;Some hints: TV, camera, toothbrush, oscilloscope, e-book reader, radio receiver, dishwasher, router, washing machine, vacuum cleaner, bathroom scales.&lt;/p&gt;
    &lt;p&gt;Now think wider. When I went to the supermarket, the vegetable section had a scales that printed labels with barcodes. They were equipped with touch screens. You bet there's a processor and a load of firmware in those. But shops are chock-full of processors in my part of the world. There are thousands of price labels in each of those stores, and they are all e-paper screens. I'm fairly sure you need software to drive those and receive wireless updates.&lt;/p&gt;
    &lt;p&gt;Keep going and you might realize that the software running in your car allows remote control. Or in your train. That snafu wouldn't have occured if the railways had access to sources of the train software.&lt;/p&gt;
    &lt;p&gt;What about other business uses? Car diagnostic stations? Medical equipment? Accounting software?&lt;/p&gt;
    &lt;p&gt;Software is really eating the world, and it's closed software which is *everywhere* around us, without free options. What's the regular person's role in this? They give up control over entire areas of their lives to others, others who often can't be supervised or replaced.&lt;/p&gt;
    &lt;p&gt;You know, we messed up. There's no other way to put it. We even let closed software sneak into our own home field: computers. Sure, the interfaces are open. There's SATA, there's PCI. We can swap parts if we want to, we can run Linux there, all is fine. Except it's not, because peripherals are as important as the core, and we, software people, lost control of the peripherals of our darlings already.&lt;/p&gt;
    &lt;head rend="h2"&gt;## Wasted potential&lt;/head&gt;
    &lt;p&gt;In theory, it's possible that someone opens a piece of software regardless of the wishes of the original authors. The whole game modding scene is about that. Here's an example of someone running Tetris on a pocket camera:&lt;/p&gt;
    &lt;p&gt;But going against the manufacturer is just wasted work. Imagine the difference between hacking it in and modifying the official sources. The potential, things we could achieve if we didn't have to break doors that are open! So here's a silly example: I have an action camera. Due to some stupid law, the camera breaks off every recording as soon as it reaches the 30 minutes mark. Now I have 20 years of coding experience. Having source code, I could have fixed the problem and went on with my life. Another example, another camera: I am making a time lapse from my window. Every day at 10:00, I take a picture from a camera that just sits there. But this camera has no time lapse feature, so I must go there in person every time. Why can't I fix this? Of course, no source code.&lt;/p&gt;
    &lt;head rend="h2"&gt;## Epilogue&lt;/head&gt;
    &lt;p&gt;There's now a new printer project that advertises itself as open source. But if you look at the details, it's actually not. Instead, it uses a source-available license which does not grant you Freedom 0 â you must not use the sources for commercial purposes. Better than nothing, I guess.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45562286</guid><pubDate>Sun, 12 Oct 2025 21:51:45 +0000</pubDate></item><item><title>Novelty Automation</title><link>https://www.novelty-automation.com/</link><description>&lt;doc fingerprint="ff25a85d0f9f504e"&gt;
  &lt;main&gt;
    &lt;p&gt;A collection of satirical home-made arcade machines (twinned with 'The Under The Pier Show', Southwold pier)&lt;/p&gt;
    &lt;p&gt;Only 5 minutes walk from Holborn tube station&lt;/p&gt;
    &lt;p&gt;August Bank Holiday open Monday Aug 25th 11-6&lt;/p&gt;
    &lt;p&gt;Normal opening hours: Open every day except Mondays, 11am to 6pm, with late opening on Thursday 12-8pm and 12-6pm on Sundays. 1a Princeton St London, WC1R 4AX&lt;/p&gt;
    &lt;p&gt;Next First Thursday bar evening Nov 6th, 5-9pm&lt;/p&gt;
    &lt;p&gt;Introduction&lt;/p&gt;
    &lt;p&gt;Machines on display&lt;/p&gt;
    &lt;p&gt;Latest machine&lt;/p&gt;
    &lt;p&gt;Video&lt;/p&gt;
    &lt;p&gt;Corporate &amp;amp; Party hire&lt;/p&gt;
    &lt;p&gt;In praise of coin operated machines&lt;/p&gt;
    &lt;p&gt;A short history of arcades&lt;/p&gt;
    &lt;p&gt;The perfect gift. Have a bag of gold delivered by post&lt;/p&gt;
    &lt;p&gt;Latest reviews in our visitors book&lt;/p&gt;
    &lt;p&gt;Prices, Directions &amp;amp; Disabled access&lt;/p&gt;
    &lt;p&gt;Reviews&lt;/p&gt;
    &lt;p&gt;Home&lt;/p&gt;
    &lt;p&gt;Links&lt;/p&gt;
    &lt;p&gt;Contact&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45563161</guid><pubDate>Sun, 12 Oct 2025 23:46:55 +0000</pubDate></item><item><title>John Searle has died</title><link>https://www.nytimes.com/2025/10/12/books/john-searle-dead.html</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45563627</guid><pubDate>Mon, 13 Oct 2025 00:57:49 +0000</pubDate></item><item><title>For centuries massive meals amazed visitors to Korea (2019)</title><link>https://www.atlasobscura.com/articles/history-of-korean-food</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45563900</guid><pubDate>Mon, 13 Oct 2025 01:46:18 +0000</pubDate></item><item><title>Despite what's happening in the USA, renewables are winning globally</title><link>https://thebulletin.org/2025/10/despite-whats-happening-in-the-usa-renewables-are-winning-globally/</link><description>&lt;doc fingerprint="283e815c2bb65d9a"&gt;
  &lt;main&gt;
    &lt;p&gt;By Zoya Teirstein | October 11, 2025&lt;/p&gt;
    &lt;p&gt;Editor’s note: This story was originally published by Grist. It appears here as part of the Climate Desk collaboration.&lt;/p&gt;
    &lt;p&gt;If you live in the United States, you could be forgiven for thinking that renewable energy is on the outs. In July, Congress voted to rapidly phase out longstanding tax credit support for wind and solar power, and the Trump administration has taken seemingly every step in its power to halt the development of individual wind and solar projects—even as domestic electricity demand rises and new sources of electricity become more important than ever.&lt;/p&gt;
    &lt;p&gt;But even as clean energy deployment hit roadblocks in the United States, the world overall set a new record for renewable energy investment over the first half of this year. Wind and solar power are meeting and even exceeding a global rise in energy demand. Indeed, electricity output from these sources is increasing faster than the world can use it, displacing some fossil fuel-generated power in the process. That’s according to a report published Tuesday by Ember, a global energy think tank, which mapped this year’s global power supply by analyzing monthly data from 88 countries that are responsible for more than 90 percent of global electricity demand.&lt;/p&gt;
    &lt;p&gt;“Overall—we’re talking globally—renewables overtook coal,” said Malgorzata Wiatros-Motyka, a senior electricity analyst at Ember and a co-author of the company’s report. “And I expect this to hold.” This year marks the first time that renewable energy sources have outpowered coal in the global energy mix. In fact, global use of fossil fuels for electricity actually declined slightly, compared to the same period in 2024.&lt;/p&gt;
    &lt;p&gt;Another report published this week by the International Energy Agency, or IEA, an intergovernmental energy research and policy organization, projects that the quantity of installed renewable power—meaning the maximum amount of energy that can be produced by systems like solar fields, hydroelectric dams, and wind turbines—will more than double by the end of this decade. National policies encouraging the development of green technology as well as astounding drops in the price of solar power—primarily driven by Chinese manufacturers, which build more than 80 percent of the world’s solar energy components—are largely driving the transition.&lt;/p&gt;
    &lt;p&gt;And even that projection may be conservative.&lt;/p&gt;
    &lt;p&gt;“The IEA has, consistently over the last couple of decades, way underestimated how fast renewables are growing,” said Robert Brecha, a senior climate and energy adviser at Climate Analytics, a global climate science and policy institute, who was not involved in either the Ember or IEA report. “I don’t see any reason to believe that renewables won’t double by 2030.”&lt;/p&gt;
    &lt;p&gt;The vast majority of the renewable energy projected to go online in the coming years will come from solar, which already met more than 80 percent of new global energy demand in the first six months of 2025, according to the Ember report. In China, the largest renewable energy growth market in the world, and in India, which is on pace to become the second-largest market, a major uptick in solar energy output is responsible for a historic global decline in coal-generated power.&lt;/p&gt;
    &lt;p&gt;In the United States and the European Union, however, fossil fuel generation rose in the first half of this year. In Europe, poor wind conditions and drought, rather than state policies, took a bite out of the bloc’s wind and hydroelectric production, leading to a 14-percent rise in gas-fired power. Across the Atlantic, US coal-fired power generation rose 17 percent.&lt;/p&gt;
    &lt;p&gt;The policy outlook for renewables in the United States is so bleak that the IEA lowered the country’s renewable capacity growth expectations by 50 percent compared to last year’s projections. That US-driven dip drags down the agency’s global projections for renewable energy growth by 5 percent. Overall, however, the IEA still expects renewable energy capacity to grow even faster between 2025 and 2030 than it did between 2020 and 2025.&lt;/p&gt;
    &lt;p&gt;“They can slow it down; they can do a lot more damage than I thought they could,” said Brecha, referring to the Trump administration’s efforts to slow the growth of renewable energy. “But they can’t stop it.”&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;The Bulletin elevates expert voices above the noise. But as an independent nonprofit organization, our operations depend on the support of readers like you. Help us continue to deliver quality journalism that holds leaders accountable. Your support of our work at any level is important. In return, we promise our coverage will be understandable, influential, vigilant, solution-oriented, and fair-minded. Together we can make a difference.&lt;/p&gt;
    &lt;p&gt;Keywords: Trump, climate change, climate crisis, fossil fuels, global warming, renewable energy, solar, wind&lt;lb/&gt; Topics: Climate Change&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45564077</guid><pubDate>Mon, 13 Oct 2025 02:15:25 +0000</pubDate></item><item><title>Countering Trusting Trust Through Diverse Double-Compiling (DDC)</title><link>https://dwheeler.com/trusting-trust/</link><description>&lt;doc fingerprint="a4c01b1900672730"&gt;
  &lt;main&gt;
    &lt;p&gt;Here’s information about my work to counter the “Trusting Trust” attack. The “Trusting Trust” attack is an incredibly nasty attack in computer security; up to now it’s been presumed to be the essential uncounterable attack. I’ve worried about it for a long time, essentially since Ken Thompson publicly described it. After all, if there’s a known attack that cannot be effectively countered, should we be using computers at all? Thankfully, I think there is an effective countermeasure, which I have named “Diverse Double-Compiling” (DDC).&lt;/p&gt;
    &lt;p&gt;This page notes my 2009 PhD dissertation and its preceding 2005 ACSAC paper, a little about citing my work, and detailed data (to duplicate the experiments), It then has sections on countering misconceptions, what about applying this to hardware?, Software patents and application programmer interface (API) copyrights, credit where credit is due, and who’s talking about it?. We then have a section on real-world application of DDC, specifically discussing GNU Mes. It includes a large section on some related material.&lt;/p&gt;
    &lt;p&gt;Fully Countering Trusting Trust through Diverse Double-Compiling (PDF version, HTML version, OpenDocument text version, Perma.cc link to PDF, arXiv:1004.5534 of PDF, GMU Mason Archival Repository Service (MARS)) is my 2009 PhD dissertation explaining how to counter the “trusting trust” attack by using the “Diverse Double-Compiling” (DDC) technique. This dissertation was accepted by my PhD committee on October 26, 2009.&lt;/p&gt;
    &lt;p&gt;The video of my official public defense is also available; this presentation was given on November 23, 2009, 1-3pm (podcast/RSS available). The presentation materials are also available in PDF and OpenDocument (ODP) formats. The public defense was held at George Mason University, Fairfax, Virginia, Innovation Hall, room 105 [location on campus] [Google map].&lt;/p&gt;
    &lt;p&gt;Here’s the abstract of the dissertation:&lt;/p&gt;
    &lt;quote&gt;An Air Force evaluation of Multics, and Ken Thompson’s Turing award lecture (“Reflections on Trusting Trust”), showed that compilers can be subverted to insert malicious Trojan horses into critical software, including themselves. If this “trusting trust” attack goes undetected, even complete analysis of a system’s source code will not find the malicious code that is running. Previously-known countermeasures have been grossly inadequate. If this attack cannot be countered, attackers can quietly subvert entire classes of computer systems, gaining complete control over financial, infrastructure, military, and/or business system infrastructures worldwide. This dissertation’s thesis is that the trusting trust attack can be detected and effectively countered using the “Diverse Double-Compiling” (DDC) technique, as demonstrated by (1) a formal proof that DDC can determine if source code and generated executable code correspond, (2) a demonstration of DDC with four compilers (a small C compiler, a small Lisp compiler, a small maliciously corrupted Lisp compiler, and a large industrial-strength C compiler, GCC), and (3) a description of approaches for applying DDC in various real-world scenarios. In the DDC technique, source code is compiled twice: once with a second (trusted) compiler (using the source code of the compiler’s parent), and then the compiler source code is compiled using the result of the first compilation. If the result is bit-for-bit identical with the untrusted executable, then the source code accurately represents the executable.&lt;/quote&gt;
    &lt;p&gt;The dissertation includes a section explaining how it extends my previous 2005 ACSAC paper. The dissertation generalizes the ACSAC paper (now compilers don’t need to self-parent), includes formal proofs, and includes demonstrations with GCC (to demonstrate scaleability) and with a malicious compiler.&lt;/p&gt;
    &lt;p&gt;If you read the dissertation you should also look at the dissertation errata (the errata are trivial and do not impact the fundamentals of anything in the dissertation).&lt;/p&gt;
    &lt;p&gt;My thanks go to the committee members, who were very helpful. A special thanks go to Dr. Ravi Sandhu; I wanted to do a PhD dissertation that was completely off the beaten path, and he was flexible enough to let me do it. He also had some great advice for getting through the process. Dr. Daniel A. Menascé asked me to demonstrate the approach with a malicious compiler (which I did). Dr. Jeff Offutt asked me about its relationship to N-version programming (so I added material about how this is different than N-version programming). Dr. Paul Ammann had some interesting comments about the N-version programming material; it turns out that he was personally involved in that landmark study! Dr. Yutao Zhong asked me about T-diagrams (so I added material about why I did not use them). Everyone on the committee asked good questions, especially in the private presentations before the public defense; thank you!&lt;/p&gt;
    &lt;p&gt;Here’s my 2005 paper, which was formally reviewed and published by ACSAC:&lt;/p&gt;
    &lt;p&gt;Countering Trusting Trust through Diverse Double-Compiling (DDC), David A. Wheeler, Proceedings of the Twenty-First Annual Computer Security Applications Conference (ACSAC), December 5-9, 2005, Tucson, Arizona, pp. 28-40, Los Alamitos: IEEE Computer Society. ISBN 0-7695-2461-3, ISSN 1063-9527, IEEE Computer Society Order Number P2461. If you cannot get that paper from ACSAC, here’s a local copy of Countering Trusting Trust through Diverse Double-Compiling (DDC) as posted by ACSAC. You can also get this alternative PDF of “Countering Trusting Trust through Diverse Double-Compiling (DDC)” and OpenDocument form of “Countering Trusting Trust through Diverse Double-Compiling (DDC)”. (I have the rights to publish it here as well.)&lt;/p&gt;
    &lt;p&gt;I’m honored to have been accepted by the ACSAC 2005 conference. They get lots of good submissions, yet in 2005 they rejected 77% of their submitted papers. One reason that I submitted to ACSAC is that I believe publication on the web is absolutely critical for widespread use of a result; ACSAC has been publishing on the web for a long time now, and is an open access conference.&lt;/p&gt;
    &lt;p&gt;There’s a minor change in notation between the ACSAC paper and the later dissertation:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Item&lt;/cell&gt;
        &lt;cell role="head"&gt;ACSAC (2005)&lt;/cell&gt;
        &lt;cell role="head"&gt;Dissertation (2009)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Trusted compiler&lt;/cell&gt;
        &lt;cell&gt;T&lt;/cell&gt;
        &lt;cell&gt;cT&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Compiler under test&lt;/cell&gt;
        &lt;cell&gt;A&lt;/cell&gt;
        &lt;cell&gt;cA&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Parent compiler&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;cP&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I have a presentation based on the ACSAC paper. I gave the original presentation at ACSAC; I’ve since updated it a little based on various feedback I’ve received.&lt;/p&gt;
    &lt;p&gt;You can get the presentation in:&lt;/p&gt;
    &lt;p&gt;Note: The ACSAC 2005 paper “Countering Trusting Trust through Diverse Double-Compiling” has a typo. In the last paragraph of section 4, just ahead of the figure, it says: “if c(sA,c(sA,T)), A, and c(sA,T) are identical, ...”. The “c(sA,T)” should be “c(sA,A)”; you can confirm this because the figure clearly shows “c(sA,A)” not “c(sA,T)”. My thanks to Ulf Dittmer for pointing this out to me!&lt;/p&gt;
    &lt;p&gt;If you cite my work, at least include my middle initial “A.”, and if at all possible please use “David A. Wheeler”. Please do not cite me as “David Wheeler” or “D. Wheeler” in any written work (including electronic media like the Internet). There are too many David Wheelers, so it’s like not giving me credit at all. If you are required by forces outside your control to use initials, at least use “D. A. Wheeler”. However, I would really appreciate it if you showed me the courtesy of using my name as I use it, instead of changing it. In general, please cite the names that people actually use; please don’t change them into someone else’s name. Thanks. This doesn't apply to talking in person, of course; usually there aren’t that many David Wheelers in the room that it’s confusing :-).&lt;/p&gt;
    &lt;p&gt;I strongly believe that scientific work must be repeatable. Sadly, much of the so-called computational sciences are no longer a science, because it is increasingly not possible to reproduce work. This problem is no secret; it is discussed in papers such as “Reproducible Research: Addressing the Need for Data and Code Sharing in Computational Science” by Victoria C. Stodden (Computing in Science &amp;amp; Engineering, 2010). It's not just computer science either; there is a widespread replication crisis in science. See also the blog post on the paper Why most of psychology is statistically unfalsifiable. "False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant" by Joseph P. Simmons, Leif D. Nelson, and Uri Simonsohn discusses some of dubious practices that allow practically anything to be "experimentally proven". Science is not the only source of truth, but if you're going to call it science, it needs to actually be science.&lt;/p&gt;
    &lt;p&gt;In contrast, I do provide the information necessary to reproduce this work. For the ACSAC paper, see my Tiny C Compiler (tcc) page for how to duplicate the ACSAC experiment, as well as other tcc-related work too. For the PhD dissertation, see the separate page on detailed data for the PhD dissertation. These provide enough information to repeat or extend the experiments.&lt;/p&gt;
    &lt;p&gt;Some misconceptions seems to be especially hard to shake, so let me counter them here (as well).&lt;/p&gt;
    &lt;p&gt;I say it in the ACSAC paper, and again in the dissertation, but somehow it does not sink in, so let me try again.&lt;/p&gt;
    &lt;p&gt;Both the ACSAC paper and dissertation do not assume that different compilers produce equal results. In fact, both specifically state that different compilers normally produce different results. In fact, as noted in the paper, it’s an improvement if the trusted compiler generates code for a different CPU architecture than the compiler under test (say, M68000 and 80x86). Clearly, if they’re generating code for different CPUs, the binary output of the two compilers cannot always be identical in the general case!&lt;/p&gt;
    &lt;p&gt;This approach does require that the trusted compiler be able to compile the source code of the parent of the compiler under test. You can’t use a Java compiler to directly compile C code.&lt;/p&gt;
    &lt;p&gt;For the pedants: Yes, sometimes it’s possible to write machine code that runs on multiple yet radically different CPU architectures, depending on the architectures. You may even be able to devise code that determines which architecture it’s running on, and then jumps to the “right” code for that architecture. These would exploit the exact values of various machine codes, and are certainly clever hacks. But if you want to do that, fat binaries with multiple segments (each for a different architecture) are a better approach — they’re designed to do that cleanly. In any case, that’s not the point; the point is that the compiler-under-test and the trusted compiler are not required to generate identical code as output.&lt;/p&gt;
    &lt;p&gt;DDC does require that the parent compiler must be deterministic when it compiles the compiler under test. That’s not the same as assuming that two different compilers always produce identical results. A compiler is deterministic if, when run twice on identical input (with all the same option flags, etc.), it produces the same output. You can use a random number generator, as long as you give the user control over the random number generator seed (gcc, for example, has a command line option for setting the seed). For example, on a Unix/Linux system, you should be able to do this:&lt;/p&gt;
    &lt;quote&gt;$ mycompiler input.c # Compile, store result in "a.out". $ mv a.out a.out.saved # Save old result. $ mycompiler input.c # Do it again $ cmp a.out a.out.saved # If always identical, it's determinstic.&lt;/quote&gt;
    &lt;p&gt;This is a relatively easy constraint, and one that most compiler authors want to be true anyway (since non-deterministic compilers are hard to debug). Compilers generally are deterministic, with the possible exception of embedded timestamps — and I discuss how to handle embedded timestamps in the paper. Sometimes you may need to use a flag (e.g., to set a random number generator seed as in the GCC C++ compiler).&lt;/p&gt;
    &lt;p&gt;The parent compiler may internally use constructs that are individually non-deterministic (such as threads with non-deterministic scheduling), but if it does it must use those mechanisms in a way that ensures that the output will be the same on each execution given the same input. Today’s underlying CPUs have all sorts of non-deterministic properties (e.g., from threading multiple cores, or timing variances); “modern CPUs are inherently random and a complex general purpose OS on top amplifies this inherent randomness substantially” [“Analysis of inherent randomness of the Linux kernel” by Nicholas Mc Guire, Peter Okech, and Georg Schiesser]. But if the CPU were so non-deterministic that you could not reliably write data in a particular order, you couldn’t get a compiler or any other program to run. So the parent compiler simply needs to be written in way that ensures that these effects will not impact its results. For example, the parent compiler could use locks to ensure that thread scheduling variation does not cause variation in the results. In practice, developers tend to do this anyway.&lt;/p&gt;
    &lt;p&gt;The trusted compiler (“compiler T” in the ACSAC paper, and “compiler cT” in the dissertation) doesn’t need to be deterministic.&lt;/p&gt;
    &lt;p&gt;See assumption sP_portable_and_deterministic in the dissertation if you want more details.&lt;/p&gt;
    &lt;p&gt;Some past approaches used a second compiler, but they basically just switched which compiler you had to trust completely. Indeed, you might make things worse, if you switch from an unsubverted compiler to a subverted compiler.&lt;/p&gt;
    &lt;p&gt;DDC, in contrast, uses additional compilers as a check on the first. This fundamentally changes things, because now an attacker must simultaneously subvert both the original compiler, and all of the compilers used in DDC. Subverting multiple compilers is much harder than subverting one, especially since the defender can choose which compilers to use in DDC and can choose the compilers used in DDC after the attack has been performed.&lt;/p&gt;
    &lt;p&gt;Using a different trusted compiler greatly increases the confidence that the compiler executable corresponds with its source code. When a second compiler is used as part of DDC, an attacker must subvert multiple executables and executable-generation processes to perform the “trusting trust” attack without detection. If you only used the trusted compiler, you’re back to the original problem, which I view as total trust on a single compiler executable without a viable verification process.&lt;/p&gt;
    &lt;p&gt;Also, as explained in section 4.6, there are many reasons the trusted compiler might not be suitable for general use. It may be slow, produce slow code, generate code for a different CPU architecture than desired, be costly, or have undesirable software license restrictions. It may lack many useful functions necessary for general-purpose use. In DDC, the trusted compiler only needs to be able to compile the parent; there is no need for it to provide other functions.&lt;/p&gt;
    &lt;p&gt;Finally, note that the “trusted” compiler(s) could be malicious and still work well well for DDC. We just need justified confidence that any triggers or payloads in a trusted compiler do not affect the DDC process when applied to the compiler-under-test. That is much, much easier to justify.&lt;/p&gt;
    &lt;p&gt;No, applying DDC by itself does not guarantee that the compiler isn't malicious, or that the compiler is not doing something surprising to you, or that the compiler has no defects. For example, in 2016 it was discovered that Microsoft Visual Studio 2015 Update 2 was quietly inserting telemetry calls into compiled programs by default, even though this was not well documented and could harm privacy. That's not the sort of thing that DDC could typically detect.&lt;/p&gt;
    &lt;p&gt;Passing the DDC test simply means that you can read compiler source code to see what the compiler does, instead of having to review executable (binary) code. But that's a difference that matters: Developers are used to looking at source code, since that's what they normally do. DDC turns an intractable challenge into a normal review process.&lt;/p&gt;
    &lt;p&gt;By “fully” I mean that “the trusting trust attack can be detected and effectively countered” (as I say in the thesis). A little background may help illustrate why I use the word “fully”.&lt;/p&gt;
    &lt;p&gt;First, complaining that people trust others is a waste of time. You must trust others in a modern world. No one grows all their own food, builds their own shelters from their own materials, and provides all their other needs by themselves; we all trust others. However, there is a serious systemic problem if you cannot independently verify what you trust. You should strive to “trust, but verify”.&lt;/p&gt;
    &lt;p&gt;I believe the fundamental problem caused by the trusting trust attack was that it was impractical to independently verify that what you depended on (the executable) corresponds to its human-readable representation (the source code). This is because program-handling programs can subvert the relationship between what humans review and what is actually used. Ken Thompson’s paper is not titled “Reflections on trust”; it is “Reflections on trusting trust”. Again, I believe problem was not trust, but the lack of a meaningful process for independent verification.&lt;/p&gt;
    &lt;p&gt;With DDC, we now have a practical process to independently verify that source code and executable correspond. DDC fully counters the problem that we lacked a practical independent verification process for program-handling programs (like compilers).&lt;/p&gt;
    &lt;p&gt;I believe it’s important that we understand the limitations of any result. Section 8.14 explains, in detail, how an attacker can subvert DDC. Because DDC has been proven using a formal mathematical proof, the only way to counter DDC is to falsify one of the proof assumptions. A defender can make such falsification very difficult. For example, the defender, not the attacker, gets to choose the compiler(s) used as the trusted compiler(s); the defender can even write one himself. It’s true that an unwise defender can depend on components that are not really diverse, but section 6 describes how to get that diversity. Once the defender knows that diversity is a goal, the defender can come up with all sorts of ways to provide it.&lt;/p&gt;
    &lt;p&gt;My goal was to create a process for independent verification. DDC provides an independent verification process, and one that can be practically applied. I applied the DDC process to four different compiler executables, and one of them was the widely-used gcc. Therefore, DDC fully meets the need for an independent verification process that can be practically applied.&lt;/p&gt;
    &lt;p&gt;So why did I put the word fully in the dissertation title at all? Well, I needed to find some way to diffentiate the titles of the ACSAC paper and the PhD dissertation. I realized that my older ACSAC paper had an important limitation: it only applied to self-parented compilers. Many compilers are not self-parented, and thus, the older ACSAC paper process could not apply to many compiler executables in use. In contrast, the 2009 dissertation can address all compilers, self-parenting or not. Thus, the dissertation “fully” provides a process for verifying compiler executables, whether they are self-parented or not. I should note that even if I wanted to, I cannot change the title now :-).&lt;/p&gt;
    &lt;p&gt;I mentioned applying this DDC approach to hardware in the dissertation and at the ACSAC conference. Obviously, if your software is okay, but the hardware is subverted, you’re still subverted. The ACSAC presentation and dissertation talk about this in more detail. DDC can be applied to hardware as well as software. As I also mentioned, there are two problem areas: legal and technical.&lt;/p&gt;
    &lt;p&gt;The legal problem is that increasingly chip designers and chip manufacturers cannot legally know what is supposed to be on the chip. For example, developers of the various “IP cores” used on chips typically forbid chip designers and manufactureres from obtaining or using this information.&lt;/p&gt;
    &lt;p&gt;The key technical problem is creating a meaningful “equality” test in hardware. I speculate that various techniques, such as scanning electron microscopes, could be used to help implement an equality test. Other hardware validation mecahnisms (e.g., see Semiconductor IP Validation Gets Faster), might also play a role. But it is fundamentally harder to implement equality tests for hardware (compared to software). I cited several papers in my dissertation about this. You can learn more about the challenge from papers pubished since then, such as “Stealthy Dopant-Level Hardware Trojans” by Georg T. Becker, Francesco Regazzoni, Christof Paar, and Wayne P. Burleson (Bruce Schneier briefly discusses this), as well as “Integrated Circuit Security Threats and Hardware Assurance Countermeasures” by Karen Mercedes Goertzel (CrossTalk, November/December 2013) [alternate URL]. "A2: Analog Malicious Hardware" by Kaiyuan Yang, Matthew Hicks, Qing Dong, Todd Austin, and Dennis Sylvester show "how a fabrication-time attacker can leverage analog circuits to create a hardware attack that is small (i.e., requires as little as one gate) and stealthy (i.e., requires an unlikely trigger sequence before effecting a chip’s functionality)." Researchers at University of Michigan demonstrated in 2016 a sabotaged processor called A2; it could be planted by a single employee at a chip factory.&lt;/p&gt;
    &lt;p&gt;However, while there are challenges, there is also hope. As noted in "X-Ray Tech Lays Chip Secrets Bare" by Samuel K. Moore (IEEE Spectrum, 2019-10-07, researchers in Switzerland and the U.S. have a non-destructive technique that has the poential to reverse engineer an entire chip without damaging it. This approach, called ptychographic X-ray laminography, could possibly "be used by integrated circuit designers to verify that manufactured chips match their designs."&lt;/p&gt;
    &lt;p&gt;Countering subverted hardware is definitely an area for potential future research. More generally, I think there's a growing problem in logic hardware verification. The Growing Semiconductor Design Problem is an especially easy-to-understand summary about the problem of hardware verification, aka the verification gap.&lt;/p&gt;
    &lt;p&gt;The approach described here only works when you can create alternative implementations of computer languages (compilers). There is no technical problem in doing so, but some organizations are trying to make it difficult to legally create alternative implementations.&lt;/p&gt;
    &lt;p&gt;Any limitation on creating or distributing alternative implementations of a computer languages creates a dangerous threat to any user of that computer language. It also creates a threat to any user of programs developed (directly or indirectly) with that language.&lt;/p&gt;
    &lt;p&gt;Computer application programmer interfaces (APIs) and languages are generally held to be outside the scope of copyright. Specific implementations and their documentation can be copyrighted, but APIs and languages are fundamentally ideas and not just fixed expressions. This was long understood, but many rulings in 2012 (in the US and Europe) make this even clearer... though there are some stormclouds that threaten this. The Oracle v. Google “Order RE Copyrightability of Certain Replicated Elements of the Java Application programming Interface” of 2012 found that “So long as the specific code used to implement a method is different, anyone is free under the Copyright Act to write his or her own code to carry out exactly the same function or specification of any methods used in the Java API. It does not matter that the declaration or method header lines are identical. Under the rules of Java, they must be identical to declare a method specifying the same functionality” even when the implementation is different. When there is only one way to express an idea or function, then everyone is free to do so and no one can monopolize that expression. And, while the Android method and class names could have been different from the names of their counterparts in Java and still have worked, copyright protection never extends to names or short phrases as a matter of law. ... This command structure is a system or method of operation under Section 102(b) of the Copyright Act and, therefore, cannot be copyrighted.” (Groklaw has this as text.) Similarly, the Court of Justice of the European Union found in SAS Institute v. World Programming Ltd., Judgment in Case C-406/10, that “The functionality of a computer program and the programming language cannot be protected by copyright.” (Here are the actual judgements of C-406/10.) Copyright, under U.S. law, specifically does not cover any “idea, procedure, process, system, method of operation, concept, principle, or discovery”; the history and justification of this (note that the list is much more than just ideas) is given in “Why Copyright Law Excludes Systems and Processes from the Scope of Its Protection” by Pamela Samuelson. However, on May 9, 2014, the Federal Circuit partially reversed the district court ruling, ruling in Oracle's favor on the copyrightability issue, and remanding the issue of fair use to the district court. I hope this will be construed narrowly; if broadly interpreted, then copyright might effectively prevent all future competition. As a practical matter, software must work with each other; if a a company can prevent compatible implementions, then that company can effectively prevent meaningful competition and verification measures like DDC. See Computer Scientists Ask Supreme Court to Rule APIs Can’t Be Copyrighted for more information about APIs and copyright.&lt;/p&gt;
    &lt;p&gt;Sadly, the risk from patents is still significant, as discussed in the dissertation. See my page on software patents for more.&lt;/p&gt;
    &lt;p&gt;I used the word “trusted” when referring to the “trusted compiler”. I should note that there is a big difference between the words “trusted” and “trustworthy”. Something is trustworthy if there is evidence that it is worthy of trust; something is trusted if someone trusts it (hopefully because they have determined that it is trustworthy). If you use DDC, you need to use a trusted compiler — since you are trusting its results, by definition it is trusted. You should choose a trustworthy compiler as the trusted compiler, however.&lt;/p&gt;
    &lt;p&gt;The good news is that you do not need to use a totally perfect, never-makes-a-mistake compiler; such compilers are rare. Instead, you just have to use a compiler that meets the conditions described in the paper, which are much easier conditions to meet.&lt;/p&gt;
    &lt;p&gt;I tried to summarize some lessons learned on how to use tools to prove things in my short paper “How to prove stuff automatically”.&lt;/p&gt;
    &lt;p&gt;After my paper was published I learned of another subverted compiler example (in the trusting trust sense) in Mike Stute's answer to "What is a coder's worst nightmare?". He tried to modify a program but found he couldn't do it successfully. After 15 days of work, "I suddenly realize it's in the compiler... every time you compile the original code and run it puts in the subliminal message code into the source code... Several days later.. we recompile the compiler from the source. That solves it... Except it didn't... The ex-grad student had poisoned the compiler to poison itself when it was recompiled... We also found that if /sbin/login is compiled it puts in a backdoor allowing anyone who uses a specific password to login in as the root user. This computer is accessible by modem and Tymnet. Finally this gets the computing center's attention. Genius! But put to a horrible cause."&lt;/p&gt;
    &lt;p&gt;As I clearly note in the paper, I didn’t come up with the original idea for the DDC countermeasure. The original idea was dreamed up by the amazingly bright Henry Spencer. However, he never pursued it; in fact over time he’d forgotten about it. I took his few sentences describing his idea and greatly expanded on it, including a much more detailed and analyzed description of it, as well as justifying and demonstrating it. For example, his original approach presumed self-parenting, a limitation my PhD dissertation removes. My thanks to him for his original idea, and for his helpful comments since.&lt;/p&gt;
    &lt;p&gt;I also want to credit those who made the world aware of the problem in the first place: Paul Karger, Roger Schell, and Ken Thompson. Paul Karger and Roger Schell’s groundbreaking analysis of Multics was the first time that this issue was identified. A key step in fixing a problem is knowing there’s a problem in the first place! I had several great conversations with Paul Karger, who was very enthusiastic about this work and provided several helpful comments. Sadly, Paul Karger died in 2010, and that is a loss for the world; the good news is that when he died, he knew about my solution and was quite happy about it. I also talked with Roger Schell about it. I also want to thank Ken Thompson, who (among his legion of accomplishments) demonstrated this attack and made far more people aware of the problem.&lt;/p&gt;
    &lt;p&gt;The first syllabus that included my ACSAC 2005 paper as required reading is CSC 593: Secure Software Engineering Seminar, a Spring 2006 class taught by Dr. James Walden at Northern Kentucky University. He paired my paper with Ken Thompson’s classic 1984 paper Reflections on Trusting Trust. It was also a subject of a class session at George Mason University (GMU)’s “Advanced Topics in Computer Security: Cyber-Identity, Authority and Trust” (IT962) taught by Ravi Sandhu. I had the honor of visiting for the day and giving the presentation myself for their Spring 2006 session. Technische Universitat Dortmund’s Lehrstuhl Informatik VI (Dr. Ulrich Flegel and Dr. Michael Meier) (WS 2007/2008) include it, too. It's specifically noted in Linux Luddites podcast #21 (August 2,2014) starting at 1:41 as well.&lt;/p&gt;
    &lt;p&gt;The ACSAC paper is cited in various places, including “Increasing Open Source Software Integration on the Department of Defense Unclassified Desktop” by Steven Anthony Schearer (June 2008), a Naval Postgraduate School (NPS) thesis, “How Practical Are Intrusion-Tolerant Distributed Systems?” by Obelheiro et al. (Sep 2006), Department of Informatics, University of Lisbon, and the PhD thesis “Tamper-resistant Peer-to-Peer Storage for File Integrity Checking” by Alexander Zangerl, Bond University, School of Information Technology (August 2006).&lt;/p&gt;
    &lt;p&gt;The ACSAC paper has been noted or discussed at many locations, including Bugtraq, comp.risks (the Risks digest), Bruce Schneier’s weblog (the source for Crypto-Gram), Lambda the ultimate, SC-L (the Secure Coding mailing list), LinuxSecurity.com, Chi Publishing’s Information Security Bulletin, Wikipedia’s “Backdoor” article, Open Web Application Security Project (OWASP) (mailing list), and others.&lt;/p&gt;
    &lt;p&gt;Bruce Schneier’s page in particular includes a lengthy commentary about it, and both his site and Lamba-the-Ultimate have various blog entries. The article Open Source is Securable discusses the paper and its ramifications -- in particular, it’s finally possible to make very strong claims through source code analysis.&lt;/p&gt;
    &lt;p&gt;BartK’s “Defeating the Trust Attack” summarized the PhD dissertation; this triggered a spirited reddit discussion in September 2013.&lt;/p&gt;
    &lt;p&gt;There was a lively discussion of the dissertation on Y Combinator's "Hacker News" in October 2016.&lt;/p&gt;
    &lt;p&gt;Diverse Double-Compiling to Harden Cryptocurrency Software by Niklas Rosencrantz, 2023, KTH, School of Electrical Engineering and Computer Science (EECS) ) is follow-on work. Its abstract is: "A trusting trust attack is a special case of a software supply-chain attack. The project in this report, named diverse double-compiling for cryptocurrency (DDC4CC), demonstrates and explains a defense for cryptocurrency software against trusting trust attacks. DDC4CC includes a case study that implements a trusting trust attack and the defense applied to a hypothetical theft of cryptocurrency on the Bitcoin blockchain. The motivation for such an attack is easy to understand: An adversary can acquire significant monetary funds by manipulating economic or decentralized financial systems. For a supply-chain attack in general, the outcome is potentially even more severe. An adversary can control entire organizations and even the systems belonging to the organization’s customers if the supply chain is compromised. Attacks are possible when targets are inherently vulnerable due to trust in their suppliers and trust in the supply chain, i.e., the hardware constructors and the software authors, the upstream development team, and the dependencies in the supply chain." This is a Master's thesis.&lt;/p&gt;
    &lt;p&gt;The video The Original Sin of Computing...that no one can fix by LaurieWired cites this work. I don't agree with the title - this is fixable, but always happy to get a citation!&lt;/p&gt;
    &lt;p&gt;Sure. In particular, this dissertation brings together technical areas that aren’t often combined. The practical demonstrations involved analyzing machine code (not just assembly code!) produced by C compilers, as well as S-expressions generated by Lisp. To prove that this really worked, I ended up using first-order predicate logic (a mathematical logic notation) and various tools to help automate its use. My mathematical models ended up having to account for stuff like different text encoding systems, because I wanted the models to accurately model the real world enough to really counter the attack. Some dissertations go deeply into the technical details of machine code, while others fly into the abstractions of mathematical proof; far fewer do both. Frankly, I think that unusual combination makes the result more interesting; I hope you do too.&lt;/p&gt;
    &lt;p&gt;A lot of people were sure that what I’m doing could not be done, so I did everything I could to prove it correct. I don’t just provide a mathematical proof; I provide a formal proof, where absolutely every step is spelled out (most proofs in math books “skip the details” but I do not). I presented the proof in Hilbert (3-column) style, giving justifications for absolutely every step. I directly used the output of a prover tool; I could have massaged it for clarity, but by using the output directly, I avoid the charge that I made a mistake in its transformation, and even more importantly I could use a separate tool (ivy) to double-check the proof. A lot of people do not have a background in this area of mathematics, so I give references to where the various steps come from, and I explain in detail each of the starting mathematical statements.&lt;/p&gt;
    &lt;p&gt;This section discusses the application of DDC in the real world. I know of at least one real-world application of DDC, specifically for the GNU Mes C compiler. So let's discuss that!&lt;/p&gt;
    &lt;p&gt;GNU Mes is "a Scheme interpreter and C compiler for bootstrapping the GNU System. Since version 0.22 it has again helped to halve the size of opaque, uninspectable binary seeds that are currently being used in the Reduced Binary Seed bootstrap of GNU Guix. The final goal is to help create a full source bootstrap as part of the bootstrappable builds effort for UNIX-like operating systems."&lt;/p&gt;
    &lt;p&gt;Rephrased differently, the idea is that GNU Mes, if trustworthy, can be used as a starting point to rebuild everything else from source code, giving you much greater confidence that the source code is an accurate representation. GNU Mes can in turn be used to bootstrap an entire distribution, and in particular is used to bootstrap GNU Guix (a complete operating system). But how can you be sure that the executable of GNU Mes is trustworthy? That seems like a chicken-and-egg kind of problem. One way to gain confidence is DDC.&lt;/p&gt;
    &lt;p&gt;The post Reproducible bootstrap of Mes C compiler describes their application of DDC to gain confidence in the Mes C compiler's binary. They used three different distributions (GNU Guix, Nix, and Debian) with three different major versions of GCC to recompile GNU Mes. They later used the tcc compiler as well (though details about that are sketchy). In all cases they recreated a bit-for-bit identical result of the GNU Mes C compiler!&lt;/p&gt;
    &lt;p&gt;As with all real-world uses there are limitations. DDC must be executed using only trusted processes and programs, and "trusted" in this case means that there is "justified confidence that it does not have triggers and payloads that would affect the results of DDC" (DDC dissertation section 6).&lt;/p&gt;
    &lt;p&gt;The application described here shows that several different distributions with different executables produce the same underlying result. However, three of these applications are using the same compiler, specifically GCC (albeit different versions). These tests use similar and highly related distributions; they even use many of the same underlying components like glibc, the Linux kernel, and so on (though again, with different versions).&lt;/p&gt;
    &lt;p&gt;So while this does use DDC, and it does increase confidence, it increases confidence so only to a limited extent because the checking systems are relatively similar. They hope to attempt to use an even more diverse set of compilers in the future, which would give even greater confidence.&lt;/p&gt;
    &lt;p&gt;This is an awesome application of DDC, and I believe it's the first publicly acknowledged use of DDC on a binary (beyond what I did). Basically, GNU Mes is designed to provide a safe starting point, and DDC can be used to verify that GNU Mes is providing a safe starting point. This application of DDC can be improved over time, and that is wonderful.&lt;/p&gt;
    &lt;p&gt;Many people have worked in related areas, in particular, to implement reproducible (deterministic) builds (which enable exact recreation of executables given source code) or proving that programs do what they say they do. I mention some of the issues, and counter-measures, in "Countering Development Environment Attacks" at the 2015 RSA Conference in San Francisco (this was a presentation by Dan Reddy and me).&lt;/p&gt;
    &lt;p&gt;Here are some pointers.&lt;/p&gt;
    &lt;p&gt;It's important to protect the development environment, including its development toolchain. Ken Thompson demonstrated an attack on the toolchain in the 1980s, and it was a full-blown "trusting trust" attack. My dissertation also discussed an attack on the Delphi compiler.&lt;/p&gt;
    &lt;p&gt;Ken Thompson Really Did Launch His "Trusting Trust" Trojan Attack in Real Life provides a copy of the full Usenet message discussing the subversion demonstrated by Ken Thompson. More sources available via mail-archive.com and Google Groups. It's important to note that "the compiler was never released) outside", but that was because Ken Thompson was ethical, not because it could not be done.&lt;/p&gt;
    &lt;p&gt;Perhaps more importantly, today the source code for Ken Thompson's attack is now available and annotated. "Running the 'Reflections on Trusting Trust' Compiler" by Russ Cox (2023-10-25) shares that source code, along with a detailed discussion of it. This version is the corrected version that didn't keep generating slightly longer versions of the generated compiler. You can experiment with this attack on a web-based simulator.&lt;/p&gt;
    &lt;p&gt;I also want to correct the record on a minor point about Thompson's work. At one time I thought the subverted compiler wasn't detected. I no evidence it was detected as being an attack, but due to a bug in how Ken Thompson implemented his trusting trust implementation, it did eventually fail. The post by Ken Thompson on 2021-09-20 titled "[TUHS] Thompson trojan put into practice" noted that every time it was recompiled it grew by another byte. They noted this oddity, and after that "they played with it until they broke the quine part" (how it worked). Ken noted that "the extra byte was my bug" and said "I'm not sure ... if they ever realized what was going on." This was discussed in Hacker News. It would be unwise to assume that future attacks would be buggy, especially since it would be easy to detect this kind of bug (make sure the subverted compiler generates the same bytes each time). The now-posted version of the attack doesn't include this bug.&lt;/p&gt;
    &lt;p&gt;"The Octopus Scanner Malware: Attacking the open source supply chain" by Alvaro Muñoz (2020-05-28) discusses the Octopus Scanner malware, "designed to enumerate and backdoor NetBeans projects, and which uses the build process and its resulting artifacts to spread itself."&lt;/p&gt;
    &lt;p&gt;Operation ShadowHammer was disclosed in March 2019 by Kaspersky Lab, and one thing it did was sabotage developer tools. As noted in "Operation ShadowHammer Exploited Weaknesses in the Software Pipeline" by Fahmida Rashid, IEEE Spectrum, 1 May 2019, "Attackers also targeted at least three gaming companies... the attackers made a one-line change to their targets’ integrated development environment (IDE), a software program that developers use to write code. The effect was that whenever Microsoft Visual Studio compiled code with a specific Microsoft-owned library, the IDE used a similarly-named library file instead. Compilers and development platforms are at the core of the software supply chain... One infected compiler on a few developers’ machines can result in thousands of Trojanized software applications installed on millions of end-user computers. “It’s a poisonous seed. Plant your poisonous seed in a safe place, and it will turn into the poisonous tree with fruit.” ... Since the compiler pulls in relevant pieces of code from linked libraries and other components, using the tampered library meant code the developer did not intend to include was added to the application. A source code review won’t find the issue because the problem isn’t anywhere in the original code and the developer doesn’t know about the alternate library. “When your compiler lies to you, your product always contains a backdoor, no matter what the source code is,” Kamluk said."&lt;/p&gt;
    &lt;p&gt;In 2015 it was revealed that over 4,000 Apple iOS applications were subverted and got into the Apple app store through an attack called XCodeGhost. This attack convinced developers to use a subverted version of Apple's XCode development environment. Many popular applications were infected via XCodeGhost, including Angry Birds 2 and WeChat. FireEye estimated that XcodeGhost added malicious code to over 4000 apps. CNBC reported that Apple was "cleaning up its iOS App Store to remove malicious iPhone and iPad programs identified in the first large-scale attack on the popular mobile software outlet. The company disclosed the effort after several cyber security firms reported finding a malicious program dubbed XcodeGhost that was embedded in hundreds of legitimate apps... The hackers embedded the malicious code in these apps by convincing developers of legitimate software to use a tainted, counterfeit version of Apple's software for creating iOS and Mac apps, which is known as Xcode, Apple said." Reuters carried a similar report. Wikipedia has an article on XcodeGhost Manish Goregaokar's "Reflections on Rusting Trust" demonstrates an implementation of the "trusting trust" attack in the Rust programming language. This isn't an attack, it's a demo of the attack, but it's a nice demo. There's a Hacker News and Reddit discussion of it.&lt;/p&gt;
    &lt;p&gt;"How a VC-funded company is undermining the open-source community" (The Outline, 2017) makes a number of claims about actions of Kite, a venture capital-funded startup. The article states that that Kite has been modifying developer tools for Kite's benefit. It's not the same as the trusting trust attack at all, but it does suggest that tools for developers are a potential target.&lt;/p&gt;
    &lt;p&gt;It's important to build software in a safe way.&lt;/p&gt;
    &lt;p&gt;Security challenges for the Qubes build process has an interesting discussion. They state their goals as:&lt;/p&gt;
    &lt;p&gt;To do this, they focus on these tasks:&lt;/p&gt;
    &lt;p&gt;They include this important footnote: "* Of course, one should understand that the mere fact that packages or sources are properly signed, even with key(s) we have decided to trust, doesn’t guarantee that the code has not been backdoored. This could happen if one of the developers turned out to be malicious or was somehow coerced to introduce a backdoor, e.g. via some kind of a warrant or blackmail, or if their laptop were somehow compromised. We would like to defend against such potential situations."&lt;/p&gt;
    &lt;p&gt;Creating reproducible builds (aka deterministic builds) is an excellent way to detect many development-time attacks, and is a precondition for applying DDC. The reproducible-builds.org web site has some great information on the topic. The video Reproducible builds: Two years in the trenches (2017) summarizes their work. They developed a tool called diffoscope that I wish I'd had! See below for more about the related topic semantically reproducible builds.&lt;/p&gt;
    &lt;p&gt;Here are a few pointers you may find useful.&lt;/p&gt;
    &lt;p&gt;The Tails Operating System image is reproducible. Verifying a Tails image for reproducibility describes the process to independently reproduce and verify an image.&lt;/p&gt;
    &lt;p&gt;"Preventing Supply Chain Attacks like SolarWinds" (Linux Foundation blog post) by David A. Wheeler (me!) discusses how to counter subverted build processes, like that in SolarWinds, by using verified reproducible builds. Basically, use reproducible builds to independently verify that your build result is correct. For a detailed technical discussion on how SolarWinds Orion was subverted, including a discussion of SUNSPOT (the malware that inserted the backdoor) and SUNBURST (the backdoor itself), see "SUNSPOT: An Implant in the Build Process" by the CrowdStrike Intelligence Team (2020-01-11).&lt;/p&gt;
    &lt;p&gt;Telegram supports reproducible builds so that others can verify that its open source code is the same as the code available in the Apple App Store and Google Play. As of early 2021 it's considered "somewhat experimental". Telegram notes that Reproducible Builds are especially hard for iOS due to Apple's current policies and MacOS limitations. As they say: (1) "Apple insists on using FairPlay encryption to “protect” even free apps from “app pirates” which makes obtaining the executable code of apps impossible without a jailbroken device." and (2) "macOS doesn't support containers like Docker." It's still possible, but challenging.&lt;/p&gt;
    &lt;p&gt;"Source and Executable" by Mike Lai (Microsoft) discussed reproducible builds and was given at the NIST/DHS Software and Supply Chain Assurance (SSCA) Forum in late 2019.&lt;/p&gt;
    &lt;p&gt;I've been told that the the Russian “Non-Documented Functionality” (NDF) certification regime may have specifically required demonstrating a reproducible build, and that at least at one time Microsoft did meet these NDF requirements. However, I have not been able to confirm this. One problem is that I don't speak Russian. Maybe someone can look at sites such as this Echelon site to confirm this?&lt;/p&gt;
    &lt;p&gt;"Improving Trust in Software through Diverse Double-Compiling and Reproducible Builds" by Yrjan Skrimstad (University of Oslo, 2018) is perhaps the closest paper to my work (since it builds on it). In my work I noted that it was possible to use more than one "trusted" compiler (that is, more than 2 grandparent compilers) to increase the difficulty for the attacker. This work by Yrjan Skrimstad expands that further, discussing implications when there are more than two such compilers. It demonstrates the trusting trust attack (using a quine) in the Go programming language, and then and demonstrates how to use DDC (with 3 grandparents) to detect the attack. It also discusses the relationship between DDC and reproducible builds. This work by Yrjan Skrimstad is demonstrated in the GitHub repo yrjan/untrustworthy_go.&lt;/p&gt;
    &lt;p&gt;The Tor project is very concerned about reproducible (deterministic) builds:&lt;/p&gt;
    &lt;p&gt;“Is that really the source code for this software?” by Jos van den Oever (2013-06-19) posts about the problems of trying to recreate executables from source code. Sometimes it’s not so bad, e.g., for Debian, “The binary package that was built from a Debian source package was not identical to the published binary package, but the differences are limited to timestamps and the build id in the executables.” But sometimes it’s very difficult, just as I had found years earlier, because you often need a lot more build information than you can get. You need much more than the source code and build script; you need to know the exact versions of all relevant build software, its configuration, and so on. But it is possible to record all that information, so that the process can be repeated... and you can repeat the process to make sure that you got it all. If you record that information, then you have the problem of “how do I know that my build tools are not malicious?” At that point, DDC comes to the rescue... because DDC can help you verify that.&lt;/p&gt;
    &lt;p&gt;"On business adoption and use of reproducible builds for open and closed source software" by Simon Butler, Jonas Gamalielsson, Björn Lundell, Christoffer Brax, Anders Mattsson, Tomas Gustavsson, Jonas Feist, Bengt Kvarnström &amp;amp; Erik Lönroth , 2022-11-29, Software Quality Journal discusses reproducible builds from a business point of view. "Through interviews with software practitioners and business managers, this study explores the utility of applying R-Bs in businesses in the primary and secondary software sectors and the business and technical reasons supporting their adoption. We find businesses use R-Bs in the safety-critical and security domains, and R-Bs are valuable for traceability and support collaborative software development. We also found that R-Bs are valued as engineering processes and are seen as a badge of software quality, but without a tangible value proposition. There are good engineering reasons to use R-Bs in industrial software development, and the principle of establishing correspondence between source code and binary offers opportunities for the development of further applications." This is an open access paper, so anyone can read it.&lt;/p&gt;
    &lt;p&gt;The Debian ReproducibleBuilds project has the goal of making it possible to reproduce, byte for byte, every build of every package in Debian. They have made a lot of progress, and I am really delighted to see it. Their Overview of known issues related to reproducible builds shows what commonly causes problems; these include embedded generated timestamps from various causes (this is a big one) and random/unsorted ordering. For example, SOURCE_DATE_EPOCH specification provides a simple mechanism to turn complicated timestamp issues into something simple. Also, the sources.debian.net site provides convenient browsing access to the Debian source code. How Debian Is Trying to Shut Down the CIA and Make Software Trustworthy Again also discusses this.&lt;/p&gt;
    &lt;p&gt;Reproducible Builds for Fedora is a similar project to deterministically reproduce the packages of Fedora.&lt;/p&gt;
    &lt;p&gt;F-Droid and The Guardian Project are working on reproducible builds for Android. For more information, see LWN.net, info on the first reproducible build by Guardian (a developers' tool), their success with the utility app Checkey.&lt;/p&gt;
    &lt;p&gt;How I compiled TrueCrypt 7.1a for Win32 and matched the official binaries describes a deterministic build (with explanable differences) was achieved for TrueCrypt. This is an encryption software capable of on-the-fly encryption on file-, partition- or disk-based virtual disks, yet its authors are anonymous, leading some to worry that the executables were backdoored. Note: Though its source code is visible, it does not use a standard OSS license and it imposes restrictions that probably mean is it is not OSS; it is not considered FLOSS by many major Linux distributions including Debian, Ubuntu, Fedora, openSUSE, and Gentoo. More recently, the TrueCrypt developers have stopped development, and its lack of a real OSS license may inhibit anyone else supporting it.&lt;/p&gt;
    &lt;p&gt;Gitian is a “secure source-control oriented software distribution method [so] you can download trusted binaries that are verified by multiple builders.”&lt;/p&gt;
    &lt;p&gt;Vagrant is designed to "create and configure lightweight, reproducible, and portable development environments". Seth Vargo discusses it briefly.&lt;/p&gt;
    &lt;p&gt;The paper "Reproducible Containers" by Omar S. Navarro Leija et al will be presented in March 2020 at the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS) 2020 (this is a conference of the Association for Computing Machinery (ACM)). This paper "describes DetTrace, a reproducible container abstraction for Linux implemented in user space." This looks really promising. The implementation is OSS (MIT license).&lt;/p&gt;
    &lt;p&gt;Buildroot is a simple mechanism for creating embedded Linux systems through cross-compilation.&lt;/p&gt;
    &lt;p&gt;Byzantine Askemos Language Layer (BALL) is an implementation of the Askemos Distributed Virtual Machine. It creates an “autonomous virtual execution environment for applications” which unlike traditional cloud environments is specifically designed to provide fault tolerance and to be tamper-proof. It executes the code on several different machines, runtime libraries, compilers, operating systems and so on in parallel and compares cryptographic signatures. Thus, this tries to counter subversion of various lower-level components.&lt;/p&gt;
    &lt;p&gt;Christophe Rhodes has discussed the problems of reproducing builds of Steel Bank Common Lisp (SBCL) on different systems in Still working on reproducible builds and Reproducible builds - a month ahead of schedule. While his notes are specific to SBCL, they illustrate more general issues. He notes that one of the reasons that SBCL separated from its parent CMCL was to "make the result of its build be independent of the compiler used to build it." His goal was not primarily to counter attack, but to eliminate hard-to-find bugs: "... how do we know there aren't odd differences that depend on the host compiler lurking, which will not obviously affect normal operation but will cause hard-to-debug trouble later? (In fact there were plenty of those, popping up at inopportune moments). I’ve been working intermittently on dealing with this, by attempting to make the Common Lisp code that SBCL!Compiler is written in sufficiently portable that executing it on different implementations generates bitwise-identical output. Because then, and only then, can we be confident that we are not depending in some unforseen way on a particular implementation-specific detail...". Here are some of the issues that he (and perhaps other the SBCL developers) found and fixed, as an example of what to look for:&lt;/p&gt;
    &lt;p&gt;The key thing to note is that creating compilers that can easily have reproducible (deterministic) builds on other compilers typically takes work in the real world... but it is very doable.&lt;/p&gt;
    &lt;p&gt;There are various tools that can help you create reproducible builds. For example, if build paths are embedded, you can force fixed directory values to make them reproducible. There are tools that enable this without requiring root permission, including my tools user-union and auto-destdir, as well as tools like proot.&lt;/p&gt;
    &lt;p&gt;LF-Edge EVE has worked hard on reproducibility. See EVE image reproducibility and EVE image sources for an interesting example of how to implement higher standards.&lt;/p&gt;
    &lt;p&gt;Reproducible builds are great for showing that a package really was built from some given source, but sometimes they're hard to do. A useful backoff is something called a "semantically equivalent build".&lt;/p&gt;
    &lt;p&gt;As explained in the documentation for the oss-reproducible tool (part of OSSGadget), "A project build is semantically equivalent if its build results can be either recreated exactly (a bit for bit reproducible build,or if the differences between the release package and a rebuilt package are not expected to produce functional differences in normal cases. For example, the rebuilt package might have different date/time stamps, or one might include files like .gitignore that are not in the other and would not change the execution of a program under normal circumstances."&lt;/p&gt;
    &lt;p&gt;A semantically equivalent build has very low risk of being a subverted build as long as it's verified to be semantically equivalent. Put another way, verifying that a package has a semantically equivalent build counters the risk where the putative source code isn't malicious, but where someone has tampered with the build or distribution process, resulting in a built package that is malicious. It's quite common for builds to produce different date/time stamps, or to add or remove "extra" files that would have no impact if the original source code was not malicious.&lt;/p&gt;
    &lt;p&gt;It's much easier (and lower cost) for software developers to create a semantically equivalent build instead of always creating a fully reproducible build. Fully reproducible builds are still a gold standard for verifying that a build has not been tampered with. However, creating fully reproducible builds often require that package creators change their build process, sometimes in substantive ways. In many cases a semantically equivalent build requires no changes, and even if changes are required, there are typically fewer changes required.&lt;/p&gt;
    &lt;p&gt;OSSGadget includes a tool that can determine if a given package is semantically equivalent. It's still helpful to work to make a package a fully reproducible build. A fully reproducible build is a somewhat stronger claim, and you don't need a complex tool to determine if the package is fully reproducible. Even given that, it's easier to first create a package that's semantically equivalent, and then work on the issues remaining to make it a fully reproducible build.&lt;/p&gt;
    &lt;p&gt;The intended use case for semantically equivalent builds (instead of fully reproducible builds) really to help people make risk decisions when they're thinking about bringing in external software. I'm primarily trying to deal with the case where the developer has decided to not provide a reproducible build, and I have to estimate the likelihood of it being maliciously built (presumably as a part of decideing whether or not the package is safe to install). I'm primarily thinking of applying this process to mostly-unmanaged repositories like npm, PyPI, and RubyGems, *not* to managed repositories like most Linux distributions' repositories (which have other mechanisms to counter malicious builds). The problem is that I cannot make the developer provide me a reproducible build (I can beg, but that's not the same thing). I'm trying to make good decisions with the information I have, not the information I *want* to have.&lt;/p&gt;
    &lt;p&gt;The threat model is a little different, too. The assumption isn't that "it is impossible for these differences to cause damage". The assumption is that "the original source code was benign, reasonably coded, and did not do damage". The question is, "is this non-reproducible package likely to have been generated from it, even though it's not a reproducible build?"&lt;/p&gt;
    &lt;p&gt;Here's an example that might clarify the threat model. It's possible that a program could look for the file ".gitignore" and run it if present. The source code repo might not have a .gitignore file, but the malicious package might add a .gitignore file and fill it with a malicious application. That would cause malicious code to be executed. However, it would also be *highly* suspicious for the source code to run a ".gitignore" file (that's *not* what they are for), so it's reasonable to assume that the source code didn't do that. If an attacker can insert a file that *would* cause malicious code to execute in a reasonably-coded app, then that *would* be a problem. "What's reasonable" is hard to truly write down, but ignoring date/time stamps and ignoring a whitelisted list of specific filenames seems like a reasonable place to start.&lt;/p&gt;
    &lt;p&gt;Sure, ideally everything would have a reproducible build. Since that day isn't here, what can we do to take piecemeal steps towards that?&lt;/p&gt;
    &lt;p&gt;In short, making packages at least semantically equivalent (and verifying this) is a great countermeasure against subverted builds.&lt;/p&gt;
    &lt;p&gt;Bootstrappable builds focuses on minimizing the amount of bootstrap binaries. They're not just interested in the direct "bootstrap" code to boot a computer, but also what is necessary to generate the direct bootstrap code. The problem bootstrappable builds is trying to address is a real one, namely, they are worried about subverted bootstrap code.&lt;/p&gt;
    &lt;p&gt;In the ideal sense they want to build up "compilers and interpreters and tools from nothing" - but of course you can't really build up from nothing - there has to be a starting point. DDC provides a great complement to bootstrappable builds - bootstrappable builds tries to limit the binaries you depend on, and DDC can be used to verify the those binaries that you depend on.&lt;/p&gt;
    &lt;p&gt;One example of this approach is stage0.&lt;/p&gt;
    &lt;p&gt;Another example is builder-hex0, a kernel "for bootstrapping compilers without having to trust a prebuilt binary". They are now able to bootstrap a POSIX kernel from Hex0 and use it to bootstrap a cross-platform C compiler with it.&lt;/p&gt;
    &lt;p&gt;Coq is being used by Xavier Leroy (main developer of OCaml) to write a certified compiler, compcert, that guarantees that semantics of a C source program is kept up to PowerPC assembly. The *specification* (unfortunately not the Coq proofs) of the compiler back-end is available as GPL software.&lt;/p&gt;
    &lt;p&gt;You might also be interested in the results of the MITRE Vlisp project. Vlisp README says: “The Verified Programming Language Implementation project has developed a formally verified implementation of the Scheme programming language, called Vlisp... An overview of the project is presented in the Vlisp Guide. More accessible PDFs about Vlisp are available too. Another paper that you may find interesting is Jonathan A. Rees. “A Security Kernel Based on the Lambda-Calculus”. PhD. Thesis. February 1995.&lt;/p&gt;
    &lt;p&gt;In my PhD dissertation I note the problem of code that intentionally written to look (to a human) like it does one thing, but actually does another. In my dissertation I called this "maliciously-misleading code"; more recently the term "underhanded code" seems to have become more common. Either way, it's not a good thing. There are contests such as the Obfuscated V Contest and Underhanded C Contest showing that it's very possible, and a 2003 attempted attack on the Linux kernel shows that it is not merely an academic issue.&lt;/p&gt;
    &lt;p&gt;If you are interested in the topic of underhanded code, I suggest looking at my paper “Initial Analysis of Underhanded Source Code” by David A. Wheeler, IDA Document D-13166, April 2020 (here's a Perma.cc link to the PDF of D-13166).&lt;/p&gt;
    &lt;p&gt;A related issue is developing (more) trusted hardware. Since it's much harder to determine if hardware is "equivalent" there are many ways an attacker can subvert hardware that aren't a trusting trust attack. It's especially easy to subvert an ASIC mask, and unfortunately it's hard to detect. Those who control foundries can easily insert attacks that are hard to detect. I can't cover this huge field, but I thought I should make a few notes about it.&lt;/p&gt;
    &lt;p&gt;First, it's well-known that it's possible to insert attacks on hardware that are hard to detect later. E.g., "A2: Analog Malicious Hardware” showed "how a fabrication-time attacker can leverage analog circuits to create a hardware attack that is small (i.e., requires as little as one gate) and stealthy (i.e., requires an unlikely trigger sequence before effecting a chip’s functionality)." It won “best paper” at the 37th IEEE Symposium on Security and Privacy and ComputerWorld had article about it.&lt;/p&gt;
    &lt;p&gt;There are some countermeasures. E.g., "Detecting Hardware Trojans with Gate-Level Information-Flow Tracking” (IEEE Computer August 2016) uses information flow tracking to discover hardware Trojans As noted in a UCSD article, it works by assigning a label to important data like a cryptographic key. That has some value, but it's not a good general-purpose technique for countering arbitrary Trojans - what if the Trojan isn't trying to violate that specific property?&lt;/p&gt;
    &lt;p&gt;Obviously, what we'd like to have is computers that we can really trust for important missions, either directly or for use as checks on "normal" computers. One approach I've suggested for decades is possibly using FPGAs to implement some computers. FPGAs can be subverted also, especially to add "kill switches", but at least the FPGA manufacturer has less information about the chip's use.&lt;/p&gt;
    &lt;p&gt;A really interesting set of work is ""A Trustworthy, Free (Libre), Linux Capable, Self-Hosting 64bit RISC-V Computer" by Gabriel L. Somlo (be sure to see the FOSDEM 2023 presentation "Self-Hosting (Almost) All The Way Down"). Gabriel L. Somlo has built an entire hardware and software system using an FPGA. He managed this by implementing RISC-V, an open instruction set that is supported by today's Linux systems. I'm especially delighted that he points to my DDC work (smile).&lt;/p&gt;
    &lt;p&gt;As of 2020 Somlo's FPGA-implemented system's speed was about 65MHz. That's obviously much slower than current systems which are measured in GHzr Still, this is a usable speed if you're patient; it's far faster than computers of decades past. I also believe that future versions could be much faster if resources were poured into it. This work used many off-the-shelf components; I expect that those components could be optimized to be much faster. Its speed is also seriously limited by the lack of capacity in current FPGAs (the 2020 implementation lacks hardware floating point and has a limited L1 cache). Future FPGA systems could be developed with more capacity (and more RAM) to improve its performance.&lt;/p&gt;
    &lt;p&gt;Attackers can exploit compiler bugs by intentionally writing code that triggers the bug in a way that subverts the program. This is yet another way to write maliciously-misleading programs (a general topic I discuss in my dissertation). This attack is not the same same as the "trusting trust" attack that DDC counters, but it is certainly related. The paper "Deniable Backdoors Using Compiler Bugs" by Scott Bauer, Pascal Cuoq, and John Regehr, Pastor Manul Laphroaig’s Export–Controlled Church Newsletter, June 20, 2015, demonstrates how a compiler defect (publicly known or found via techniques such as fuzzing) can be exploited. In their case, they demonstrated how sudo could be exploited via innocent-looking code. John Regehr's blog post about "Defending Against Compiler-Based Backdoors" (2015-06-21) points this out, noting that "the advantages of this kind of backdoor include subtlety, deniability, and target-specificity" - and he then makes some great points. In particular, he notes that compiler developers need to fix known miscompilation bugs as rapidly as possible and use fuzz tools; compilers are security-sensitive in a way that is often not appreciated. Maintainers of open source packages need to be suspicious of baroque patch submissions and consider rewriting patches. Such attacks are much more fragile than the traditional "trusting trust" attack, but they can occur in any program, are potentially very dangerous, and are currently difficult to detect. In the short term, it might be best to focus on detecting and eliminating defects in widely-used compilers. No one will complain about getting rid of compiler defects, we have lots of techniques today that we can use, and if compiler defects become more difficult to trigger these backdoor attempts would typically become more obvious. But that short-term strategy is not enough; I hope that people will develop longer-term strategies too.&lt;/p&gt;
    &lt;p&gt;“Some Remarks about Random Testing” by B A Wichmann, National Physical Laboratory, Teddington, Middlesex, TW11 0LW, UK, May 1998, discusses creating random tests for compilers.&lt;/p&gt;
    &lt;p&gt;Kegel’s building and testing gcc/glibc cross toolchains has lots of good information.&lt;/p&gt;
    &lt;p&gt;GCC explorer interactively shows the assembly output from GCC (given various inputs).&lt;/p&gt;
    &lt;p&gt;Compiler explorer interactively shows the assembly output from a variety of compilers and languages.&lt;/p&gt;
    &lt;p&gt;Aquine is a "computer program which takes no input and produces a copy of its own source code as its only output." Compilers written to attack themselves are highly related to quines (depending on your definition, you could consider them a special case of quines). "Quines (self-replicating programs)" by David Madore discusses quines in more detail. "How to write your first Quine program" by David Bertoldi (2019-07-26) also explains quines. Perhaps the most amazing quine mame quine relay, an Ouroborous quine that starts as a Ruby program to generate a Rust program, which generates a Scala program, which generates a Scheme program, and so on through 128 languages.&lt;/p&gt;
    &lt;p&gt;The RepRap Project is developing inexpensive 3D printer designs that will hopefully (eventually) be able to create themselves. Very interesting, and in the future, possibly quite relevant.&lt;/p&gt;
    &lt;p&gt;The Open proofs web site encourages the development of “open proofs”, where the implementation, proofs, and required tools are all open source software.&lt;/p&gt;
    &lt;p&gt;Mark Mitchell’s “Using C++ in GCC is OK” (Sun, 30 May 2010 17:26:16 -0700) officially reported that “the GCC Steering Committee and the FSF have approved the use of C++ in GCC itself. Of course, there’s no reason for us to use C++ features just because we can. The goal is a better compiler for users, not a C++ code base for its own sake.” Mark Mitchell later explains that he expects that GCC will use C++ cautiously. For DDC, this means that applying DDC to the GCC code base will require a C++ compiler (at least one that supports the parts that GCC uses), not just a C compiler. I used Intel’s icc, which was a C++ compiler anyway, so that would not have especially affected my example... and it certainly does not change the validity of the approach.&lt;/p&gt;
    &lt;p&gt;This paper has a number of connections back to the halting problem. Proof That Computers Can't Do Everything (The Halting Problem) is a delightful video that shows the traditional proof of the halting problem, but in a clever way. You might also want to see Beyond Computation: The P vs NP Problem - Michael Sipser.&lt;/p&gt;
    &lt;p&gt;Build tools like make are important for any large system. Improving make describes my efforts to improve the POSIX standard for make as well as make implementations, in particular to support the insights in Peter Miller’s Recursive Make Considered Harmful.&lt;/p&gt;
    &lt;p&gt;The Juniper backdoor is interesting - it appears that a crypto backdoor was itself backdoor'ed. There are interesting comments by Matthew Green and rpw.&lt;/p&gt;
    &lt;p&gt;"Hacking DNA: The Story of CRISPR, Ken Thompson, and the Gene Drive" by Geoff Ralston (April 3, 2017) discusses CRISPR, and in passing discusses Ken Thompson's work.&lt;/p&gt;
    &lt;p&gt;"Cory Doctorow: Demon-Haunted World" in Locus Magazine (2017-09-02) notes that "none of the world’s problems are ours to solve ... so long as the computers we rely on are sneaking around behind our backs, treating us as their enemies."&lt;/p&gt;
    &lt;p&gt;Compiler fuzzing, part 1 discusses some results from fuzzing compilers.&lt;/p&gt;
    &lt;p&gt;Breaking the Solidity Compiler with a Fuzzer discusses how they enhanced the American Fuzzy Lop (AFL) fuzzer to fuzz C-like languages and find defects in the Solidity compiler.&lt;/p&gt;
    &lt;p&gt;The Science Fiction story Coding Machines by Lawrence Kesteloot (January 2009) is based on these kinds of attacks.&lt;/p&gt;
    &lt;p&gt;The Open Trusted Technology Provider Standard (O-TTPS), Version 1.1: Mitigating Maliciously Tainted and Counterfeit Products from the Open Group may be of interest to you. Per its website description, "The O-TTPS is an open standard containing a set of organizational guidelines, requirements, and recommendations for integrators, providers, and component suppliers to enhance the security of the global supply chain and the integrity of commercial off-the-shelf (COTS) information and communication technology (ICT). This standard if properly adhered to will help assure against maliciously tainted and counterfeit products throughout the COTS ICT product life cycle encompassing the following phases: design, sourcing, build, fulfillment, distribution, sustainment, and disposal. The Open Group Trusted Technology Forum (OTTF) is a global initiative that invites industry, government, and other interested participants to work together to evolve this document and other OTTF deliverables."&lt;/p&gt;
    &lt;p&gt;TUF (The Update Framework) helps developers secure their new or existing software update systems. Between the system-level package managers, programming language specific package managers/repositories, and application-specific update systems, there are a lot of software updaters around.. and they all need to be secure.&lt;/p&gt;
    &lt;p&gt;This paper cites key previous papers, here is how to get those (I provide multiple links to increase the likelihood you can get them):&lt;/p&gt;
    &lt;p&gt;I used OpenOffice.org to write the dissertation, and it worked out very nicely. OpenOffice.org was a great program for writing larger documents. LibreOffice has replaced OpenOffice.org as its successor; LibreOffice started from the same code and has a far more active community. So definitely check out The Document Foundation’s LibreOffice Productivity Suite.&lt;/p&gt;
    &lt;p&gt;I developed an OpenDocument template for George Mason University (GMU) that did nearly all the formatting for me automatically. That made it easy to concentrate on the text instead of the formatting.&lt;/p&gt;
    &lt;p&gt;The most important rule for writing large documents using LibreOffice or any other word processor is to automate everything you can, and in particular, always use styles. Never set the font size, font type, etc., for a range of characters or a paragraph (one exception: using italics/bold to emphasize a word or phrase is okay). Instead, all formatting information should be attached to a paragraph style, and then make sure that each paragraph has the right paragraph style. Use “Text body” (not “Default”) for normal text, and the various “Heading 1”, “Heading 2”, and so on for headings. Similarly, use Insert &amp;gt; Cross-Reference to refer to other parts of the document; that way, the program can renumber things correctly.&lt;/p&gt;
    &lt;p&gt;OpenOffice.org gives you lots of control over how words break (or not) on a line; for more, see “Easy way to insert nonbreaking hyphen, etc. in OpenOffice.org Writer” (by Solveig Haugland). Basically, to get more control over hyphenation, go to Tools &amp;gt; Options &amp;gt; Language Settings &amp;gt; Languages and select the “Enabled for Complex Text Layout” option. Now you can use “Insert&amp;gt;Formatting Mark” menu to insert more control over formatting. The “no width no break” character, aka the “glue” character”, “glues” the characters it’s between to prevent line breaks there which would otherwise there. Similarly, the “no width optional break” character, when inserted, tells OpenOffice.org that it’s okay to insert a line break there where normally it would not do so. You can also insert non-breaking spaces, non-breaking hyphens, and optional hyphens.&lt;/p&gt;
    &lt;p&gt;In most cases, the paragraph styles should make paragraphs break across pages in the right way (e.g., the paragraph styles should have reasonable default “widow” and “orphan” settings, and header paragraph styles should have “keep with next paragraph” set). But in some cases the paragraphs won’t break across pages well because the program doesn’t “understand” text. For example, if you have text that leads into the next paragraph, you may need to right-click on that paragraph and set “keep with next paragraph”. In special cases you may want a different widow or orphan setting, too.&lt;/p&gt;
    &lt;p&gt;OpenOffice.org supports formulas, which I use quite a bit. Its “stack” and “matrix” options are sometimes very useful for multi-line formulas, for example. For in-line formulas, I recommend making formula borders 0. You can do this while editing formulas by selecting Format&amp;gt;Spacing, category Borders, and then making the all borders 0 (indeed, I suggest making this the default). Otherwise, there’s embedded extra space in formulas that looks odd when you try to combine formulas with punctuation.&lt;/p&gt;
    &lt;p&gt;For the final version, I used Tools &amp;gt; Update All (to update the table of contents, cross-references, etc.), moved to the beginning and saved, and then ran File &amp;gt; Export as PDF.&lt;/p&gt;
    &lt;p&gt;After doing endless numbers of tedious compiles, Xkcd’s cartoon about compiling made me smile. The big picture solution to the halting problem is also relevant :-).&lt;/p&gt;
    &lt;p&gt;Dilbert has mentioned long compiling times too: Dilbert 2013-06-22 Dilbert 2005-09-23 Dilbert 1998-06-21. Dilbert once noticed that “maybe there’s a bug in the compiler program itself!”. Dilbert also makes it clear why software single source strategies are a bad idea.&lt;/p&gt;
    &lt;p&gt;I gave a brief example of readable Lisp s-expressions; the readable Lisp s-expressions project has specifications and implementations for curly-infix-expresssions, neoteric-expressions, and sweet-expressions, which can make Lisp notation a lot easier to read.&lt;/p&gt;
    &lt;p&gt;Mortality.pvs is a short demo of how to express the “All men are mortal” example using PVS.&lt;/p&gt;
    &lt;p&gt;Here’s how to install gcc on SGI IRIX.&lt;/p&gt;
    &lt;p&gt;ERESI (ERESI Reverse Engineering Software Interface) is a “unified multi-architecture binary analysis framework targeting operating systems based on the Executable &amp;amp; Linking Format (ELF).”. developerworks has a nice article on ELF. Elfkickers was written by Brian Raiter, who also wrote A Whirlwind Tutorial on Creating Really Teensy ELF Executables for Linux and Albert Einstein’s Theory of Relativity: In Words of Four Letters or Less. This old article explains ELF’s advantages.&lt;/p&gt;
    &lt;p&gt;I have tried to make sure that this paper will stick around into the future. Here’s the GMU page for my dissertation “Fully Countering Trusting Trust through Diverse Double-Compiling”, as well as the arXiv.org copy of “Fully Countering Trusting Trust through Diverse Double-Compiling” and the UMI ProQuest copy of my PhD dissertation “Fully Countering Trusting Trust through Diverse Double-Compiling” (via ProQuest search). Archive.org also has a copy. These are just additional copies, with the same information. The PDF file, as I submitted it, has these properties:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Title&lt;/cell&gt;
        &lt;cell&gt;Fully Countering Trusting Trust through Diverse Double-Compiling&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Author&lt;/cell&gt;
        &lt;cell&gt;David A. Wheeler&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Date&lt;/cell&gt;
        &lt;cell&gt;Fall Semester 2009 (actually 2009-11-30)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Filename&lt;/cell&gt;
        &lt;cell&gt;wheeler-trusting-trust-ddc.pdf&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Length&lt;/cell&gt;
        &lt;cell&gt;1,971,698 bytes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Pages&lt;/cell&gt;
        &lt;cell&gt;199&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;MD5 hash&lt;/cell&gt;
        &lt;cell&gt;5320ff082ec060e7f58409b3877cb687&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SHA-1 hash&lt;/cell&gt;
        &lt;cell&gt;20c8b702dd4b7c6586f2 59eb98f577dbadd359dd&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SHA-256 hash&lt;/cell&gt;
        &lt;cell&gt;024bccc5254eaffe9466f12afe39f72b 154f63a6919f4e1add5d0513092b2052&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;SHA-512 hash&lt;/cell&gt;
        &lt;cell&gt;0004998431af5da486a87794969a5314 07cb607ffc411c966a23343a58636c20 72ceb85835ffe6eef727696ffc41b1dd d6d9e0fd090cbc85a33041c25acd2e55&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;An aside: At ACSAC 2005, Aleks Kissinger (from the University of Tulsa) also presented work that he and I had done on micro-tainting. Since that seems to have disappeared from the web, I thought I should briefly describe it here.&lt;/p&gt;
    &lt;p&gt;Aleks’ presentation was titled “Fine-Grained Taint Analysis using Regular Expressions,” which was part of the ACSA Works in Progress Basically, we noted that instead of assigning “taint” to a whole value, such as a string, you could assign taint on subcomponents, such as each character. Then you could assign rules that identified the input paths and what could come in -- typically zero or more tainted characters -- and rules on output paths. We concentrated on defining regular expressions for what is legal, though any other expression for patterns such as BNFs would be fine too. We noted that you could then check statically or dynamically. For the static case, when you work backwards, if the check “fails” you can even trivially derive the input patterns that cause security failures (and from that information it should be easy to figure out how to fix it). Aleks has recently made some good progress by transforming the regular expressions into DFAs. There was another ACSAC presentation on doing taint analysis with Java, but this was the traditional “whole variable” approach that is used in many languages, but through which many vulnerabilities slip by. We hope this micro-tainting approach will lead to improved tools for detecting security vulnerabilities in software, before that software is delivered to end-users.&lt;/p&gt;
    &lt;p&gt;There is related work that we know about that has been going on in the University of Virginia (UVA), though we only found out about it halfway through our work (via Usenix). More information about the UVA work is in “Automatically Hardening Web Applications Using Precise Tainting” by Anh Nguyen-Tuong, Salvatore Guarnieri, Doug Greene, Jeff Shirley, and David Evans. They focus on PHP, and only on the dynamic case; we were interested in both, but especially interested in the static case (where you can show that certain vulnerabilities never occur and thus don’t need any run-time overhead to deal with them).&lt;/p&gt;
    &lt;p&gt;Other related work includes the BRICS Java String Analyzer (GPL; uses the BSD-licensed dk.brics.automaton). Hampi might be able to implement this statically, which would be fantastic.&lt;/p&gt;
    &lt;p&gt;There is a long history of work on data flow, static typing, and security (such as work by Dennis Volpano et al). That’s good work, but not really focused on what we were looking at. Those works tend to view variables as a whole, while instead we’re tracking much smaller units of data. We’re also tracking sequences (like arrays) which contain data with different levels of security; most such works handled arrays like a single unit (a simplification that is fundamentally at odds with our approach).&lt;/p&gt;
    &lt;p&gt;You can also view my formal education timeline, my book on writing secure programs, FlawFinder, or my home page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45564196</guid><pubDate>Mon, 13 Oct 2025 02:39:25 +0000</pubDate></item></channel></rss>